<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>SSCNN&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#20013;&#24494;&#23376;&#26395;&#36828;&#38236;&#25968;&#25454;&#22788;&#29702;&#20013;&#31232;&#30095;&#12289;&#39640;&#32500;&#24230;&#21644;&#38750;&#35268;&#21017;&#20960;&#20309;&#24418;&#29366;&#31561;&#38382;&#39064;&#30340;&#32593;&#32476;&#65292;&#20854;&#20107;&#20214;&#37325;&#26500;&#24615;&#33021;&#20248;&#20110;&#20256;&#32479;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#36816;&#34892;&#36895;&#24230;&#22823;&#22823;&#25552;&#39640;&#65292;&#21487;&#29992;&#20110;&#25913;&#21892;&#20013;&#24494;&#23376;&#33021;&#37327;&#21644;&#26041;&#21521;&#30340;&#31532;&#19968;&#20272;&#35745;&#20197;&#25773;&#31181;&#26356;&#20808;&#36827;&#30340;&#37325;&#24314;&#12290;</title><link>http://arxiv.org/abs/2303.08812</link><description>&lt;p&gt;
&#22522;&#20110;&#31232;&#30095;&#23376;&#27969;&#24418;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20013;&#24494;&#23376;&#26395;&#36828;&#38236;&#35302;&#21457;&#20107;&#20214;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
Trigger-Level Event Reconstruction for Neutrino Telescopes Using Sparse Submanifold Convolutional Neural Networks. (arXiv:2303.08812v1 [hep-ex])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08812
&lt;/p&gt;
&lt;p&gt;
SSCNN&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#20013;&#24494;&#23376;&#26395;&#36828;&#38236;&#25968;&#25454;&#22788;&#29702;&#20013;&#31232;&#30095;&#12289;&#39640;&#32500;&#24230;&#21644;&#38750;&#35268;&#21017;&#20960;&#20309;&#24418;&#29366;&#31561;&#38382;&#39064;&#30340;&#32593;&#32476;&#65292;&#20854;&#20107;&#20214;&#37325;&#26500;&#24615;&#33021;&#20248;&#20110;&#20256;&#32479;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#36816;&#34892;&#36895;&#24230;&#22823;&#22823;&#25552;&#39640;&#65292;&#21487;&#29992;&#20110;&#25913;&#21892;&#20013;&#24494;&#23376;&#33021;&#37327;&#21644;&#26041;&#21521;&#30340;&#31532;&#19968;&#20272;&#35745;&#20197;&#25773;&#31181;&#26356;&#20808;&#36827;&#30340;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#31185;&#23398;&#25968;&#25454;&#20998;&#26512;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#21253;&#25324;&#22312;&#20013;&#24494;&#23376;&#26395;&#36828;&#38236;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#23454;&#39564;&#30340;&#25968;&#25454;&#23545;CNN&#25552;&#20986;&#20102;&#35768;&#22810;&#25361;&#25112;&#65292;&#22914;&#38750;&#35268;&#21017;&#20960;&#20309;&#24418;&#29366;&#12289;&#31232;&#30095;&#24615;&#21644;&#39640;&#32500;&#24230;&#12290;&#22240;&#27492;&#65292;CNN&#22312;&#20013;&#24494;&#23376;&#26395;&#36828;&#38236;&#25968;&#25454;&#19978;&#38750;&#24120;&#20302;&#25928;&#65292;&#24182;&#38656;&#35201;&#22823;&#37327;&#30340;&#39044;&#22788;&#29702;&#65292;&#23548;&#33268;&#20449;&#24687;&#25439;&#22833;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31232;&#30095;&#23376;&#27969;&#24418;&#21367;&#31215;&#65288;SSCNN&#65289;&#20316;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#26174;&#31034;SSCNN&#20107;&#20214;&#37325;&#26500;&#24615;&#33021;&#19982;&#20256;&#32479;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30456;&#27604;&#21487;&#27604;&#25110;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;SSCNN&#22312;GPU&#19978;&#30340;&#36816;&#34892;&#36895;&#24230;&#32422;&#20026;&#20256;&#32479;CNN&#30340;16&#20493;&#12290;&#30001;&#20110;&#36825;&#31181;&#21152;&#36895;&#65292;&#39044;&#35745;&#33021;&#22815;&#22788;&#29702;IceCube&#35268;&#27169;&#30340;&#20013;&#24494;&#23376;&#26395;&#36828;&#38236;&#30340;&#35302;&#21457;&#32423;&#20107;&#20214;&#36895;&#29575;&#12290;&#36825;&#20123;&#32593;&#32476;&#21487;&#29992;&#20110;&#25913;&#21892;&#20013;&#24494;&#23376;&#33021;&#37327;&#21644;&#26041;&#21521;&#30340;&#31532;&#19968;&#20272;&#35745;&#20197;&#25773;&#31181;&#26356;&#20808;&#36827;&#30340;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks (CNNs) have seen extensive applications in scientific data analysis, including in neutrino telescopes. However, the data from these experiments present numerous challenges to CNNs, such as non-regular geometry, sparsity, and high dimensionality. Consequently, CNNs are highly inefficient on neutrino telescope data, and require significant pre-processing that results in information loss. We propose sparse submanifold convolutions (SSCNNs) as a solution to these issues and show that the SSCNN event reconstruction performance is comparable to or better than traditional and machine learning algorithms. Additionally, our SSCNN runs approximately 16 times faster than a traditional CNN on a GPU. As a result of this speedup, it is expected to be capable of handling the trigger-level event rate of IceCube-scale neutrino telescopes. These networks could be used to improve the first estimation of the neutrino energy and direction to seed more advanced reconstructions,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#21512;&#34892;&#21160;&#39044;&#27979;&#21644;&#22810;&#23610;&#24230;&#32467;&#26500;&#20197;&#24314;&#31435;&#23616;&#37096;&#21644;&#20840;&#23616;&#21160;&#24577;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#22312;&#22810;&#26234;&#33021;&#20307;&#34892;&#20026;&#25361;&#25112;&#36187;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.08811</link><description>&lt;p&gt;
&#19981;&#29992;&#25285;&#24515;&#20320;&#24590;&#20040;&#21040;&#36798;&#30446;&#30340;&#22320;&#65306;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#22810;&#26102;&#38388;&#23610;&#24230;&#34892;&#20026;&#20998;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Relax, it doesn't matter how you get there: A new self-supervised approach for multi-timescale behavior analysis. (arXiv:2303.08811v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#21512;&#34892;&#21160;&#39044;&#27979;&#21644;&#22810;&#23610;&#24230;&#32467;&#26500;&#20197;&#24314;&#31435;&#23616;&#37096;&#21644;&#20840;&#23616;&#21160;&#24577;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#22312;&#22810;&#26234;&#33021;&#20307;&#34892;&#20026;&#25361;&#25112;&#36187;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#34892;&#20026;&#20855;&#26377;&#22797;&#26434;&#21644;&#19981;&#21487;&#39044;&#27979;&#30340;&#21160;&#24577;&#65292;&#29305;&#21035;&#26159;&#22312;&#23581;&#35797;&#39044;&#27979;&#26410;&#26469;&#35768;&#22810;&#27493;&#39588;&#26102;&#12290;&#23613;&#31649;&#22312;&#21463;&#38480;&#25110;&#31616;&#21270;&#30340;&#20219;&#21153;&#26465;&#20214;&#19979;&#26500;&#24314;&#34892;&#20026;&#34920;&#31034;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#25104;&#21151;&#65292;&#20294;&#20854;&#20013;&#35768;&#22810;&#27169;&#22411;&#19981;&#33021;&#24212;&#29992;&#20110;&#33258;&#30001;&#21644;&#33258;&#28982;&#30340;&#29615;&#22659;&#20013;&#65292;&#22240;&#20026;&#36825;&#20123;&#34892;&#20026;&#21464;&#24471;&#36234;&#26469;&#36234;&#38590;&#20197;&#24314;&#27169;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20026;&#34892;&#20026;&#24320;&#21457;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#20010;&#26032;&#39062;&#32452;&#20214;&#65306;&#65288;i&#65289;&#19968;&#20010;&#34892;&#21160;&#39044;&#27979;&#30446;&#26631;&#65292;&#26088;&#22312;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#27493;&#38271;&#30340;&#34892;&#21160;&#20998;&#24067;; &#65288;ii&#65289;&#19968;&#31181;&#22810;&#23610;&#24230;&#32467;&#26500;&#65292;&#24314;&#31435;&#29420;&#31435;&#30340;&#28508;&#22312;&#31354;&#38388;&#20197;&#36866;&#24212;&#30701;&#26399;&#21644;&#38271;&#26399;&#21160;&#24577;&#12290;&#22312;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#29615;&#22659;&#21644;&#22320;&#24418;&#20013;&#23545;&#30495;&#23454;&#26426;&#22120;&#20154;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#21160;&#24577;&#24314;&#27169;&#33021;&#21147;&#21518;&#65292;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#21040;MABe 2022&#22810;&#26234;&#33021;&#20307;&#34892;&#20026;&#25361;&#25112;&#36187;&#20013;&#65292;&#20854;&#20013;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#24635;&#20307;&#25490;&#21517;&#20013;&#25490;&#21517;&#31532;&#19968;&#65292;&#24182;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural behavior consists of dynamics that are complex and unpredictable, especially when trying to predict many steps into the future. While some success has been found in building representations of behavior under constrained or simplified task-based conditions, many of these models cannot be applied to free and naturalistic settings where behavior becomes increasingly hard to model. In this work, we develop a multi-task representation learning model for behavior that combines two novel components: (i) An action prediction objective that aims to predict the distribution of actions over future timesteps, and (ii) A multi-scale architecture that builds separate latent spaces to accommodate short- and long-term dynamics. After demonstrating the ability of the method to build representations of both local and global dynamics in realistic robots in varying environments and terrains, we apply our method to the MABe 2022 Multi-agent behavior challenge, where our model ranks 1st overall and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;Anchors&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2303.08806</link><description>&lt;p&gt;
&#29702;&#35299;&#20107;&#21518;&#35299;&#37322;&#22120;&#65306;&#20197;Anchors&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Understanding Post-hoc Explainers: The Case of Anchors. (arXiv:2303.08806v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;Anchors&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#26159;&#19968;&#39033;&#39640;&#24230;&#35201;&#27714;&#20294;&#38590;&#20197;&#23454;&#29616;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#30340;&#20010;&#20307;&#39044;&#27979;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#26412;&#22320;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20135;&#29983;&#35299;&#37322;&#30340;&#36807;&#31243;&#23545;&#20110;&#29992;&#25143;&#26469;&#35828;&#21487;&#33021;&#19982;&#35201;&#35299;&#37322;&#30340;&#39044;&#27979;&#19968;&#26679;&#31070;&#31192;&#12290;&#27492;&#22806;&#65292;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#32463;&#24120;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#19988;&#23427;&#20204;&#22312;&#31616;&#21333;&#27169;&#22411;&#19978;&#30340;&#34892;&#20026;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#12290;&#26412;&#25991;&#23545;Anchors&#65288;Ribeiro&#31561;&#20154;&#65292;2018&#65289;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65306;&#19968;&#31181;&#27969;&#34892;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#23427;&#24378;&#35843;&#19968;&#23567;&#32452;&#21333;&#35789;&#20197;&#35299;&#37322;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many scenarios, the interpretability of machine learning models is a highly required but difficult task. To explain the individual predictions of such models, local model-agnostic approaches have been proposed. However, the process generating the explanations can be, for a user, as mysterious as the prediction to be explained. Furthermore, interpretability methods frequently lack theoretical guarantees, and their behavior on simple models is frequently unknown. While it is difficult, if not impossible, to ensure that an explainer behaves as expected on a cutting-edge model, we can at least ensure that everything works on simple, already interpretable models. In this paper, we present a theoretical analysis of Anchors (Ribeiro et al., 2018): a popular rule-based interpretability method that highlights a small set of words to explain a text classifier's decision. After formalizing its algorithm and providing useful insights, we demonstrate mathematically that Anchors produces meaningf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;&#38543;&#26426;&#25554;&#20540;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#27969;&#21644;&#25193;&#25955;&#26041;&#27861;&#30340;&#32479;&#19968;&#12290;&#20316;&#32773;&#26500;&#24314;&#20102;&#19968;&#31867;&#24191;&#27867;&#30340;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#36807;&#31243;&#65292;&#29992;&#20110;&#23558;&#20004;&#20010;&#20219;&#24847;&#30340;&#23494;&#24230;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#31934;&#30830;&#22320;&#36830;&#25509;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#22522;&#20110;&#27010;&#29575;&#24494;&#20998;&#26041;&#31243;&#30340;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#29983;&#25104;&#27169;&#22411;&#30340;&#26500;&#24314;&#12290;</title><link>http://arxiv.org/abs/2303.08797</link><description>&lt;p&gt;
&#38543;&#26426;&#25554;&#20540;&#65306;&#27969;&#21644;&#25193;&#25955;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Stochastic Interpolants: A Unifying Framework for Flows and Diffusions. (arXiv:2303.08797v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;&#38543;&#26426;&#25554;&#20540;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#27969;&#21644;&#25193;&#25955;&#26041;&#27861;&#30340;&#32479;&#19968;&#12290;&#20316;&#32773;&#26500;&#24314;&#20102;&#19968;&#31867;&#24191;&#27867;&#30340;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#36807;&#31243;&#65292;&#29992;&#20110;&#23558;&#20004;&#20010;&#20219;&#24847;&#30340;&#23494;&#24230;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#31934;&#30830;&#22320;&#36830;&#25509;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#22522;&#20110;&#27010;&#29575;&#24494;&#20998;&#26041;&#31243;&#30340;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#29983;&#25104;&#27169;&#22411;&#30340;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31867;&#24314;&#31435;&#22312;&#38543;&#26426;&#25554;&#20540;&#26694;&#26550;&#19978;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#26694;&#26550;&#26159;&#22522;&#20110;Albergo&#65286;Vanden-Eijnden&#65288;2023&#65289;&#25552;&#20986;&#30340;&#65292;&#22312;&#27969;&#21644;&#25193;&#25955;&#26041;&#27861;&#19978;&#23454;&#29616;&#32479;&#19968;&#65292;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#22914;&#20309;&#26500;&#24314;&#19968;&#31867;&#24191;&#27867;&#30340;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#36807;&#31243;&#65292;&#20854;&#26102;&#38388;&#20381;&#36182;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#31934;&#30830;&#22320;&#36830;&#25509;&#20004;&#20010;&#20219;&#24847;&#30340;&#23494;&#24230;&#12290;&#36825;&#20123;&#8220;&#38543;&#26426;&#25554;&#20540;&#22120;&#8221;&#26159;&#36890;&#36807;&#23558;&#26469;&#33258;&#20004;&#20010;&#23494;&#24230;&#30340;&#25968;&#25454;&#19982;&#20854;&#20182;&#28508;&#22312;&#21464;&#37327;&#30456;&#32467;&#21512;&#26500;&#24314;&#30340;&#65292;&#24182;&#19988;&#26500;&#36896;&#30340;&#20855;&#20307;&#32454;&#33410;&#21487;&#20197;&#28789;&#27963;&#22320;&#22609;&#36896;&#23548;&#33268;&#30340;&#26102;&#38388;&#20381;&#36182;&#23494;&#24230;&#12290;&#28982;&#21518;&#25105;&#20204;&#23637;&#31034;&#20102;&#38543;&#26426;&#25554;&#20540;&#22120;&#30340;&#26102;&#38388;&#20381;&#36182;&#23494;&#24230;&#28385;&#36275;&#19968;&#38454;&#36755;&#36816;&#26041;&#31243;&#20197;&#21450;&#19968;&#31995;&#21015;&#20855;&#26377;&#21487;&#35843;&#25193;&#25955;&#30340;&#27491;&#21521;&#21644;&#21453;&#21521;Fokker-Planck&#26041;&#31243;&#26063;; &#22312;&#32771;&#34385;&#21333;&#20010;&#26679;&#26412;&#30340;&#26102;&#38388;&#28436;&#21270;&#26102;&#65292;&#36825;&#20010;&#35266;&#28857;&#31435;&#21363;&#23548;&#33268;&#20102;&#22522;&#20110;&#27010;&#29575;&#24494;&#20998;&#26041;&#31243;&#30340;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a class of generative models based on the stochastic interpolant framework proposed in Albergo &amp; Vanden-Eijnden (2023) that unifies flow-based and diffusion-based methods. We first show how to construct a broad class of continuous-time stochastic processes whose time-dependent probability density function bridges two arbitrary densities exactly in finite time. These `stochastic interpolants' are built by combining data from the two densities with an additional latent variable, and the specific details of the construction can be leveraged to shape the resulting time-dependent density in a flexible way. We then show that the time-dependent density of the stochastic interpolant satisfies a first-order transport equation as well as a family of forward and backward Fokker-Planck equations with tunable diffusion; upon consideration of the time evolution of an individual sample, this viewpoint immediately leads to both deterministic and stochastic generative models based on proba
&lt;/p&gt;</description></item><item><title>PLEX&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21033;&#29992;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#35270;&#35273;&#36816;&#21160;&#36712;&#36857;&#21644;&#22823;&#37327;&#30340;&#20219;&#21153;&#26465;&#20214;&#19979;&#30340;&#29289;&#20307;&#25805;&#20316;&#35270;&#39057;&#65292;&#22312;&#23398;&#20064;&#36890;&#29992;&#30340;&#25805;&#32437;&#20363;&#31243;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#35270;&#39057;&#28436;&#31034;&#23398;&#20064;&#22914;&#20309;&#22312;&#36825;&#20010;&#29305;&#24449;&#31354;&#38388;&#20013;&#35268;&#21010;&#21508;&#31181;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.08789</link><description>&lt;p&gt;
PLEX&#65306;&#21033;&#29992;&#21487;&#29992;&#25968;&#25454;&#36827;&#34892;&#26426;&#22120;&#20154;&#25805;&#32437;&#39044;&#35757;&#32451;&#30340;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
PLEX: Making the Most of the Available Data for Robotic Manipulation Pretraining. (arXiv:2303.08789v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08789
&lt;/p&gt;
&lt;p&gt;
PLEX&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21033;&#29992;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#35270;&#35273;&#36816;&#21160;&#36712;&#36857;&#21644;&#22823;&#37327;&#30340;&#20219;&#21153;&#26465;&#20214;&#19979;&#30340;&#29289;&#20307;&#25805;&#20316;&#35270;&#39057;&#65292;&#22312;&#23398;&#20064;&#36890;&#29992;&#30340;&#25805;&#32437;&#20363;&#31243;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#35270;&#39057;&#28436;&#31034;&#23398;&#20064;&#22914;&#20309;&#22312;&#36825;&#20010;&#29305;&#24449;&#31354;&#38388;&#20013;&#35268;&#21010;&#21508;&#31181;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20016;&#23500;&#30340;&#34920;&#24449;&#26159;&#23454;&#29616;&#26426;&#22120;&#20154;&#25805;&#32437;&#30340;&#20851;&#38190;&#65292;&#20294;&#29616;&#26377;&#30340;&#27169;&#22411;&#26550;&#26500;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#26469;&#23398;&#20064;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29702;&#24819;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#35757;&#32451;&#25968;&#25454;&#65292;&#21363;&#21508;&#31181;&#24050;&#27880;&#37322;&#20219;&#21153;&#30340;&#19987;&#23478;&#35270;&#35273;-&#21160;&#20316;&#28436;&#31034;&#65292;&#26159;&#31232;&#32570;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#26550;&#26500;PLEX&#65292;&#23427;&#26159;&#20174;&#20219;&#21153;&#19981;&#21487;&#30693;&#35270;&#35273;&#36816;&#21160;&#36712;&#36857;&#20013;&#23398;&#20064;&#30340;&#65292;&#20276;&#38543;&#30528;&#22823;&#37327;&#30340;&#20219;&#21153;&#26465;&#20214;&#19979;&#30340;&#29289;&#20307;&#25805;&#20316;&#35270;&#39057;&#8212;&#8212;&#36825;&#26159;&#19968;&#31181;&#25968;&#37327;&#21487;&#35266;&#30340;&#19982;&#26426;&#22120;&#20154;&#30456;&#20851;&#30340;&#25968;&#25454;&#12290;PLEX&#32972;&#21518;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#22312;&#35266;&#23519;&#21644;&#34892;&#21160;&#26041;&#38754;&#30340;&#36712;&#36857;&#19979;&#65292;&#26377;&#21161;&#20110;&#35825;&#23548;&#28508;&#22312;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#24182;&#35757;&#32451;&#26426;&#22120;&#20154;&#25191;&#34892;&#19982;&#20219;&#21153;&#19981;&#30456;&#20851;&#30340;&#25805;&#20316;&#20363;&#31243;&#65292;&#32780;&#22810;&#26679;&#21270;&#30340;&#20165;&#20026;&#35270;&#39057;&#28436;&#31034;&#20165;&#21487;&#20197;&#26377;&#25928;&#22320;&#25945;&#20250;&#26426;&#22120;&#20154;&#22914;&#20309;&#22312;&#36825;&#20010;&#29305;&#24449;&#31354;&#38388;&#20013;&#35268;&#21010;&#21508;&#31181;&#20219;&#21153;&#12290;&#19982;&#22823;&#22810;&#25968;&#26426;&#22120;&#20154;&#25805;&#32437;&#39044;&#22521;&#35757;&#20316;&#21697;&#19981;&#21516;&#65292;PLEX&#23398;&#20064;&#20102;&#19968;&#31181;&#21487;&#25512;&#24191;&#30340;&#24863;&#35273;&#36816;&#21160;&#22810;&#20219;&#21153;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
A rich representation is key to general robotic manipulation, but existing model architectures require a lot of data to learn it. Unfortunately, ideal robotic manipulation training data, which comes in the form of expert visuomotor demonstrations for a variety of annotated tasks, is scarce. In this work we propose PLEX, a transformer-based architecture that learns from task-agnostic visuomotor trajectories accompanied by a much larger amount of task-conditioned object manipulation videos -- a type of robotics-relevant data available in quantity. The key insight behind PLEX is that the trajectories with observations and actions help induce a latent feature space and train a robot to execute task-agnostic manipulation routines, while a diverse set of video-only demonstrations can efficiently teach the robot how to plan in this feature space for a wide variety of tasks. In contrast to most works on robotic manipulation pretraining, PLEX learns a generalizable sensorimotor multi-task polic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#20840;&#31070;&#32463;&#24418;&#24577;&#35270;&#35273;&#21040;&#25511;&#21046;&#30340;&#27969;&#31243;&#65292;&#20351;&#26080;&#20154;&#26426;&#20855;&#22791;&#20102;&#25191;&#34892;&#33258;&#20027;&#22522;&#20110;&#35270;&#35273;&#30340;&#39134;&#34892;&#25152;&#38656;&#30340;&#33021;&#21147;&#65292;&#20026;&#26426;&#22120;&#20154;&#24863;&#30693;&#21644;&#34892;&#21160;&#25552;&#20379;&#20102;&#20302;&#24310;&#36831;&#21644;&#33021;&#37327;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.08778</link><description>&lt;p&gt;
&#20840;&#31070;&#32463;&#24418;&#24577;&#35270;&#35273;&#21644;&#25511;&#21046;&#30340;&#33258;&#20027;&#39134;&#34892;&#25191;&#29031;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fully neuromorphic vision and control for autonomous drone flight. (arXiv:2303.08778v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#20840;&#31070;&#32463;&#24418;&#24577;&#35270;&#35273;&#21040;&#25511;&#21046;&#30340;&#27969;&#31243;&#65292;&#20351;&#26080;&#20154;&#26426;&#20855;&#22791;&#20102;&#25191;&#34892;&#33258;&#20027;&#22522;&#20110;&#35270;&#35273;&#30340;&#39134;&#34892;&#25152;&#38656;&#30340;&#33021;&#21147;&#65292;&#20026;&#26426;&#22120;&#20154;&#24863;&#30693;&#21644;&#34892;&#21160;&#25552;&#20379;&#20102;&#20302;&#24310;&#36831;&#21644;&#33021;&#37327;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#24863;&#30693;&#21644;&#22788;&#29702;&#26159;&#24322;&#27493;&#21644;&#31232;&#30095;&#30340;&#65292;&#23548;&#33268;&#20302;&#24310;&#36831;&#21644;&#33021;&#37327;&#26377;&#25928;&#30340;&#24863;&#30693;&#21644;&#34892;&#21160;&#12290;&#22312;&#26426;&#22120;&#20154;&#23398;&#20013;&#65292;&#20351;&#29992;&#38754;&#21521;&#20107;&#20214;&#30340;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#21644;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#25215;&#35834;&#20855;&#26377;&#31867;&#20284;&#30340;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24403;&#21069;&#23884;&#20837;&#24335;&#31070;&#32463;&#24418;&#24577;&#22788;&#29702;&#22120;&#20013;&#21463;&#38480;&#30340;&#32593;&#32476;&#35268;&#27169;&#20197;&#21450;&#35757;&#32451;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#30340;&#22256;&#38590;&#65292;&#26426;&#22120;&#20154;&#23454;&#29616;&#20165;&#38480;&#20110;&#20855;&#26377;&#20302;&#32500;&#24863;&#23448;&#36755;&#20837;&#21644;&#36816;&#21160;&#25191;&#34892;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20840;&#31070;&#32463;&#24418;&#24577;&#35270;&#35273;&#21040;&#25511;&#21046;&#30340;&#27969;&#31243;&#65292;&#20197;&#25511;&#21046;&#33258;&#30001;&#39134;&#34892;&#30340;&#26080;&#20154;&#26426;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#31070;&#32463;&#32593;&#32476;&#25509;&#21463;&#39640;&#32500;&#21407;&#22987;&#20107;&#20214;&#30456;&#26426;&#25968;&#25454;&#24182;&#36755;&#20986;&#25191;&#34892;&#33258;&#20027;&#22522;&#20110;&#35270;&#35273;&#30340;&#39134;&#34892;&#25152;&#38656;&#30340;&#20302;&#32423;&#25511;&#21046;&#21160;&#20316;&#12290;&#32593;&#32476;&#30340;&#35270;&#35273;&#37096;&#20998;&#30001;&#20116;&#23618;&#21644; 28.8k &#31070;&#32463;&#20803;&#32452;&#25104;&#65292;&#23558;&#20256;&#20837;&#30340;&#21407;&#22987;&#20107;&#20214;&#26144;&#23556;&#21040;&#33258;&#25105;&#36816;&#21160;&#20272;&#35745;&#19978;&#65292;&#24182;&#20351;&#29992;&#30495;&#23454;&#29615;&#22659;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biological sensing and processing is asynchronous and sparse, leading to low-latency and energy-efficient perception and action. In robotics, neuromorphic hardware for event-based vision and spiking neural networks promises to exhibit similar characteristics. However, robotic implementations have been limited to basic tasks with low-dimensional sensory inputs and motor actions due to the restricted network size in current embedded neuromorphic processors and the difficulties of training spiking neural networks. Here, we present the first fully neuromorphic vision-to-control pipeline for controlling a freely flying drone. Specifically, we train a spiking neural network that accepts high-dimensional raw event-based camera data and outputs low-level control actions for performing autonomous vision-based flight. The vision part of the network, consisting of five layers and 28.8k neurons, maps incoming raw events to ego-motion estimates and is trained with self-supervised learning on real e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#27169;&#22411;&#36873;&#25321;&#21644;&#20132;&#21449;&#39564;&#35777;&#39118;&#38505;&#20272;&#35745;&#26469;&#23398;&#20064;&#30340;&#19968;&#33324;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#26080;&#20998;&#24067;&#20559;&#24046;&#30028;&#65292;&#27604;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26041;&#27861;&#26356;&#32039;&#23494;&#65292;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2303.08777</link><description>&lt;p&gt;
&#27169;&#22411;&#36873;&#25321;&#37197;&#21512;&#20132;&#21449;&#39564;&#35777;&#39118;&#38505;&#20272;&#35745;&#30340;&#26080;&#20998;&#24067;&#20559;&#24046;&#30028;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Distribution-free Deviation Bounds of Learning via Model Selection with Cross-validation Risk Estimation. (arXiv:2303.08777v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#27169;&#22411;&#36873;&#25321;&#21644;&#20132;&#21449;&#39564;&#35777;&#39118;&#38505;&#20272;&#35745;&#26469;&#23398;&#20064;&#30340;&#19968;&#33324;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#26080;&#20998;&#24067;&#20559;&#24046;&#30028;&#65292;&#27604;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26041;&#27861;&#26356;&#32039;&#23494;&#65292;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#30340;&#39118;&#38505;&#20272;&#35745;&#21644;&#27169;&#22411;&#36873;&#25321;&#22312;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#36890;&#36807;&#27169;&#22411;&#36873;&#25321;&#19982;&#20132;&#21449;&#39564;&#35777;&#39118;&#38505;&#20272;&#35745;&#30340;&#29702;&#35770;&#24615;&#36136;&#30340;&#29702;&#35299;&#22312;&#20854;&#24191;&#27867;&#20351;&#29992;&#38754;&#21069;&#30456;&#24403;&#32570;&#20047;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#26412;&#25991;&#23558;&#23398;&#20064;&#36890;&#36807;&#27169;&#22411;&#36873;&#25321;&#19982;&#20132;&#21449;&#39564;&#35777;&#39118;&#38505;&#20272;&#35745;&#20316;&#20026;&#19968;&#31181;&#32463;&#20856;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#19968;&#33324;&#31995;&#32479;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#24314;&#31435;&#20102;&#22522;&#20110;VC&#32500;&#30340;&#26080;&#20998;&#24067;&#20559;&#24046;&#36793;&#30028;&#65292;&#32473;&#20986;&#20102;&#32467;&#26524;&#30340;&#35814;&#32454;&#35777;&#26126;&#65292;&#24182;&#32771;&#34385;&#20102;&#26377;&#30028;&#21644;&#26080;&#30028;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#25512;&#23548;&#20986;&#22312;&#25972;&#20010;&#20551;&#35774;&#31354;&#38388;&#20013;&#65292;&#23398;&#20064;&#36890;&#36807;&#27169;&#22411;&#36873;&#25321;&#30340;&#20559;&#24046;&#30028;&#27604;&#36890;&#36807;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#23398;&#20064;&#30340;&#20559;&#24046;&#30028;&#26356;&#32039;&#23494;&#30340;&#26465;&#20214;&#65292;&#25903;&#25345;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#32463;&#39564;&#19978;&#35266;&#23519;&#21040;&#30340;&#27169;&#22411;&#36873;&#25321;&#26694;&#26550;&#30340;&#26356;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-validation techniques for risk estimation and model selection are widely used in statistics and machine learning. However, the understanding of the theoretical properties of learning via model selection with cross-validation risk estimation is quite low in face of its widespread use. In this context, this paper presents learning via model selection with cross-validation risk estimation as a general systematic learning framework within classical statistical learning theory and establishes distribution-free deviation bounds in terms of VC dimension, giving detailed proofs of the results and considering both bounded and unbounded loss functions. We also deduce conditions under which the deviation bounds of learning via model selection are tighter than that of learning via empirical risk minimization in the whole hypotheses space, supporting the better performance of model selection frameworks observed empirically in some instances.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#36890;&#36807;&#26500;&#24314;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#24335;&#65292;&#20197;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#25968;&#25454;&#38598;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;ANN&#36827;&#34892;&#26399;&#26435;&#23450;&#20215;&#27169;&#22411;&#21442;&#25968;&#26657;&#20934;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;ANN&#26041;&#27861;&#30340;&#24615;&#33021;&#20248;&#20110;MCS&#26041;&#27861;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#30701;&#30340;&#35745;&#31639;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2303.08760</link><description>&lt;p&gt;
&#12298;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26399;&#26435;&#23450;&#20215;&#27169;&#22411;&#24615;&#33021;&#27604;&#36739;&#30340;&#28145;&#23618;&#26657;&#20934;&#12299;
&lt;/p&gt;
&lt;p&gt;
Deep Calibration With Artificial Neural Network: A Performance Comparison on Option Pricing Models. (arXiv:2303.08760v1 [q-fin.MF])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08760
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#36890;&#36807;&#26500;&#24314;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#24335;&#65292;&#20197;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#25968;&#25454;&#38598;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;ANN&#36827;&#34892;&#26399;&#26435;&#23450;&#20215;&#27169;&#22411;&#21442;&#25968;&#26657;&#20934;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;ANN&#26041;&#27861;&#30340;&#24615;&#33021;&#20248;&#20110;MCS&#26041;&#27861;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#30701;&#30340;&#35745;&#31639;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#20316;&#20026;&#26080;&#27169;&#22411;&#35299;&#20915;&#26399;&#26435;&#23450;&#20215;&#27169;&#22411;&#26657;&#20934;&#31639;&#27861;&#30340;&#26041;&#26696;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;ANN&#26469;&#26657;&#20934;&#20004;&#31181;&#30528;&#21517;&#30340;GARCH&#22411;&#26399;&#26435;&#23450;&#20215;&#27169;&#22411;&#30340;&#21442;&#25968;&#65306;Duan&#30340;GARCH&#21644;&#32463;&#20856;&#28201;&#21644;&#31283;&#23450;GARCH&#65292;&#36825;&#20004;&#20010;&#27169;&#22411;&#26174;&#33879;&#25913;&#36827;&#20102;Black-Scholes&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#20294;&#21463;&#21040;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#25216;&#26415;&#22256;&#38590;&#65292;&#25105;&#20204;&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;&#27169;&#25311;&#65288;MCS&#65289;&#26041;&#27861;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#23545;ANNS&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#26657;&#20934;&#26368;&#20339;&#21442;&#25968;&#12290;&#24615;&#33021;&#32467;&#26524;&#34920;&#26126;&#65292;ANN&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;MCS&#65292;&#24182;&#19988;&#19968;&#26086;&#32463;&#36807;&#35757;&#32451;&#65292;&#36824;&#21487;&#20197;&#21033;&#29992;&#26356;&#24555;&#30340;&#35745;&#31639;&#26102;&#38388;&#12290;&#36824;&#35752;&#35770;&#20102;&#26399;&#26435;&#30340;&#24076;&#33098;&#23383;&#27597;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores Artificial Neural Network (ANN) as a model-free solution for a calibration algorithm of option pricing models. We construct ANNs to calibrate parameters for two well-known GARCH-type option pricing models: Duan's GARCH and the classical tempered stable GARCH that significantly improve upon the limitation of the Black-Scholes model but have suffered from computation complexity. To mitigate this technical difficulty, we train ANNs with a dataset generated by Monte Carlo Simulation (MCS) method and apply them to calibrate optimal parameters. The performance results indicate that the ANN approach consistently outperforms MCS and takes advantage of faster computation times once trained. The Greeks of options are also discussed.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22235;&#32500;CTP&#20840;&#38754;&#21033;&#29992;&#26102;&#31354;&#20449;&#24687;&#65292;&#20197;&#20998;&#21106;&#30097;&#20284;&#24613;&#24615;&#32570;&#34880;&#24615;&#21330;&#20013;&#24739;&#32773;&#30340;&#26775;&#27515;&#21306;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#26377;&#28508;&#21147;&#20026;&#25913;&#21892;AIS&#24739;&#32773;&#30340;&#26089;&#26399;&#35786;&#26029;&#21644;&#27835;&#30103;&#35268;&#21010;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2303.08757</link><description>&lt;p&gt;
&#21033;&#29992;&#22235;&#32500;CT&#28748;&#27880;&#25104;&#20687;&#23545;&#30097;&#20284;&#24613;&#24615;&#32570;&#34880;&#24615;&#21330;&#20013;&#24739;&#32773;&#30340;&#26775;&#27515;&#21306;&#36827;&#34892;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Exploiting 4D CT Perfusion for segmenting infarcted areas in patients with suspected acute ischemic stroke. (arXiv:2303.08757v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08757
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22235;&#32500;CTP&#20840;&#38754;&#21033;&#29992;&#26102;&#31354;&#20449;&#24687;&#65292;&#20197;&#20998;&#21106;&#30097;&#20284;&#24613;&#24615;&#32570;&#34880;&#24615;&#21330;&#20013;&#24739;&#32773;&#30340;&#26775;&#27515;&#21306;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#26377;&#28508;&#21147;&#20026;&#25913;&#21892;AIS&#24739;&#32773;&#30340;&#26089;&#26399;&#35786;&#26029;&#21644;&#27835;&#30103;&#35268;&#21010;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#12289;&#24555;&#36895;&#30340;&#24613;&#24615;&#32570;&#34880;&#24615;&#21330;&#20013;&#65288;AIS&#65289;&#24739;&#32773;&#32570;&#34880;&#21306;&#65288;&#26680;&#24515;&#21644;&#21322;&#24433;&#21306;&#65289;&#39044;&#27979;&#26041;&#27861;&#23545;&#20110;&#25913;&#36827;&#35786;&#26029;&#21644;&#27835;&#30103;&#35268;&#21010;&#20855;&#26377;&#37325;&#35201;&#30340;&#20020;&#24202;&#24847;&#20041;&#12290;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#26159;&#30097;&#20284;AIS&#24739;&#32773;&#26089;&#26399;&#35780;&#20272;&#30340;&#20027;&#35201;&#27169;&#24335;&#20043;&#19968;&#12290;CT&#28748;&#27880;&#25104;&#20687;&#65288;CTP&#65289;&#36890;&#24120;&#29992;&#20316;&#20027;&#35201;&#35780;&#20272;&#25163;&#27573;&#65292;&#20197;&#30830;&#23450;&#21330;&#20013;&#20301;&#32622;&#12289;&#20005;&#37325;&#31243;&#24230;&#21644;&#32570;&#34880;&#24615;&#30149;&#28790;&#20307;&#31215;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;CTP&#33258;&#21160;&#20998;&#21106;&#26041;&#27861;&#37117;&#20351;&#29992;&#24050;&#32463;&#22788;&#29702;&#36807;&#30340;&#19977;&#32500;&#24425;&#33394;&#22320;&#22270;&#20316;&#20026;&#25918;&#23556;&#31185;&#21307;&#24072;&#24120;&#35268;&#35270;&#35273;&#35780;&#20272;&#30340;&#36755;&#20837;&#12290;&#25110;&#32773;&#65292;&#22522;&#20110;&#20999;&#29255;&#30340;&#20108;&#32500;+&#26102;&#38388;&#36755;&#20837;&#20351;&#29992;&#21407;&#22987;CTP&#25968;&#25454;&#65292;&#20854;&#20013;&#24573;&#30053;&#20102;&#22312;&#20307;&#31215;&#19978;&#30340;&#31354;&#38388;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#19981;&#21516;&#26041;&#27861;&#26469;&#21033;&#29992;&#25972;&#20010;&#22235;&#32500;CTP&#20316;&#20026;&#36755;&#20837;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#26102;&#31354;&#20449;&#24687;&#12290;&#36825;&#20351;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;4D&#21367;&#31215;&#23618;&#12290;&#25105;&#20204;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#20840;&#38754;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#26377;&#28508;&#21147;&#20026;&#25913;&#21892;AIS&#24739;&#32773;&#30340;&#26089;&#26399;&#35786;&#26029;&#21644;&#27835;&#30103;&#35268;&#21010;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precise and fast prediction methods for ischemic areas (core and penumbra) in acute ischemic stroke (AIS) patients are of significant clinical interest: they play an essential role in improving diagnosis and treatment planning. Computed Tomography (CT) scan is one of the primary modalities for early assessment in patients with suspected AIS. CT Perfusion (CTP) is often used as a primary assessment to determine stroke location, severity, and volume of ischemic lesions. Current automatic segmentation methods for CTP mostly use already processed 3D color maps conventionally used for visual assessment by radiologists as input. Alternatively, the raw CTP data is used on a slice-by-slice basis as 2D+time input, where the spatial information over the volume is ignored. In this paper, we investigate different methods to utilize the entire 4D CTP as input to fully exploit the spatio-temporal information. This leads us to propose a novel 4D convolution layer. Our comprehensive experiments on a l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;GENEA Challenge 2022&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#35813;&#27604;&#36187;&#26088;&#22312;&#22522;&#20934;&#27979;&#35797;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#33258;&#21160;&#20849;&#21516;&#35821;&#35328;&#25163;&#21183;&#29983;&#25104;&#12290;&#20351;&#29992;&#20855;&#26377;&#30456;&#21516;&#35821;&#38899;&#21644;&#21160;&#20316;&#30340;&#25968;&#25454;&#38598;&#65292;&#20247;&#22810;&#21442;&#36187;&#22242;&#38431;&#30340;&#25163;&#21183;&#29983;&#25104;&#31995;&#32479;&#22312;&#20960;&#20010;&#22823;&#22411;&#29992;&#25143;&#30740;&#31350;&#20013;&#24471;&#21040;&#20102;&#35780;&#20272;&#65292;&#22240;&#27492;&#33021;&#22815;&#36827;&#34892;&#30452;&#25509;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2303.08737</link><description>&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#24320;&#25918;&#25361;&#25112;&#20013;&#35780;&#20272;&#25163;&#21183;&#29983;&#25104;&#65306;GENEA Challenge 2022&#30340;&#30740;&#31350;&#25253;&#21578;(arXiv:2303.08737v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
Evaluating gesture-generation in a large-scale open challenge: The GENEA Challenge 2022. (arXiv:2303.08737v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;GENEA Challenge 2022&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#35813;&#27604;&#36187;&#26088;&#22312;&#22522;&#20934;&#27979;&#35797;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#33258;&#21160;&#20849;&#21516;&#35821;&#35328;&#25163;&#21183;&#29983;&#25104;&#12290;&#20351;&#29992;&#20855;&#26377;&#30456;&#21516;&#35821;&#38899;&#21644;&#21160;&#20316;&#30340;&#25968;&#25454;&#38598;&#65292;&#20247;&#22810;&#21442;&#36187;&#22242;&#38431;&#30340;&#25163;&#21183;&#29983;&#25104;&#31995;&#32479;&#22312;&#20960;&#20010;&#22823;&#22411;&#29992;&#25143;&#30740;&#31350;&#20013;&#24471;&#21040;&#20102;&#35780;&#20272;&#65292;&#22240;&#27492;&#33021;&#22815;&#36827;&#34892;&#30452;&#25509;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25253;&#36947;&#20102;&#31532;&#20108;&#23626;GENEA Challenge&#65292;&#23545;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#33258;&#21160;&#20849;&#21516;&#35821;&#35328;&#25163;&#21183;&#29983;&#25104;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#21442;&#36187;&#22242;&#38431;&#20351;&#29992;&#30456;&#21516;&#30340;&#35821;&#38899;&#21644;&#36816;&#21160;&#25968;&#25454;&#38598;&#26500;&#24314;&#25163;&#21183;&#29983;&#25104;&#31995;&#32479;&#12290;&#36825;&#20123;&#31995;&#32479;&#29983;&#25104;&#30340;&#21160;&#20316;&#20351;&#29992;&#26631;&#20934;&#21270;&#30340;&#21487;&#35270;&#21270;&#31649;&#36947;&#28210;&#26579;&#20026;&#35270;&#39057;&#65292;&#24182;&#22312;&#20960;&#20010;&#22823;&#22411;&#20247;&#21253;&#29992;&#25143;&#30740;&#31350;&#20013;&#36827;&#34892;&#35780;&#20272;&#12290;&#19982;&#27604;&#36739;&#19981;&#21516;&#30740;&#31350;&#35770;&#25991;&#26102;&#19981;&#21516;&#30340;&#26159;&#65292;&#36825;&#37324;&#30340;&#32467;&#26524;&#24046;&#24322;&#20165;&#30001;&#20110;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#31995;&#32479;&#20043;&#38388;&#30340;&#30452;&#25509;&#27604;&#36739;&#12290;&#35813;&#25968;&#25454;&#38598;&#22522;&#20110;18&#23567;&#26102;&#30340;&#20840;&#36523;&#21160;&#20316;&#25429;&#25417;&#65292;&#21253;&#25324;&#25163;&#25351;&#65292;&#24182;&#35760;&#24405;&#20102;&#19981;&#21516;&#20154;&#29289;&#21442;&#19982;&#21452;&#20154; &#23545;&#35805;&#12290;&#21313;&#20010;&#22242;&#38431;&#21442;&#21152;&#20102;&#20998;&#20026;&#20840;&#36523;&#21644;&#19978;&#21322;&#36523;&#32930;&#20307;&#34920;&#36798;&#30340;&#20004;&#20010;&#23618;&#27425;&#30340;&#25361;&#25112;&#12290;&#23545;&#20110;&#27599;&#20010;&#23618;&#27425;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25163;&#21183;&#36816;&#21160;&#30340;&#20154;&#31867;&#30456;&#20284;&#24230;&#21644;&#20854;&#23545;&#29305;&#23450;&#35821;&#38899;&#20449;&#21495;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#23558;&#20154;&#31867;&#30456;&#20284;&#24230;&#19982;&#25163;&#21183;&#36866;&#29992;&#24615;&#35299;&#34261;&#24320;&#65292;&#36825;&#19968;&#28857;&#19968;&#30452;&#26159;&#22256;&#38590;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper reports on the second GENEA Challenge to benchmark data-driven automatic co-speech gesture generation. Participating teams used the same speech and motion dataset to build gesture-generation systems. Motion generated by all these systems was rendered to video using a standardised visualisation pipeline and evaluated in several large, crowdsourced user studies. Unlike when comparing different research papers, differences in results are here only due to differences between methods, enabling direct comparison between systems. The dataset was based on 18 hours of full-body motion capture, including fingers, of different persons engaging in a dyadic conversation. Ten teams participated in the challenge across two tiers: full-body and upper-body gesticulation. For each tier, we evaluated both the human-likeness of the gesture motion and its appropriateness for the specific speech signal. Our evaluations decouple human-likeness from gesture appropriateness, which has been a difficu
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;SALAMA&#29992;&#20110;&#39044;&#27979;&#38647;&#26292;&#21457;&#29983;&#24773;&#20917;&#65292;&#21487;&#20197;&#22312;&#38271;&#36798;11&#23567;&#26102;&#30340;&#26102;&#38388;&#20869;&#36827;&#34892;&#39044;&#27979;&#65292;&#39044;&#27979;&#25216;&#33021;&#20248;&#20110;&#20256;&#32479;&#26041;&#26696;&#65292;&#19988;&#39044;&#27979;&#26102;&#38388;&#23610;&#24230;&#38543;&#30528;&#39044;&#25253;&#30340;&#31354;&#38388;&#23610;&#24230;&#21576;&#32447;&#24615;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2303.08736</link><description>&lt;p&gt;
&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#36890;&#36807;&#21518;&#22788;&#29702;&#27169;&#25311;&#25968;&#25454;&#39044;&#27979;&#38647;&#26292;&#22825;&#27668;(arXiv:2303.08736v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
A machine-learning approach to thunderstorm forecasting through post-processing of simulation data. (arXiv:2303.08736v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08736
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;SALAMA&#29992;&#20110;&#39044;&#27979;&#38647;&#26292;&#21457;&#29983;&#24773;&#20917;&#65292;&#21487;&#20197;&#22312;&#38271;&#36798;11&#23567;&#26102;&#30340;&#26102;&#38388;&#20869;&#36827;&#34892;&#39044;&#27979;&#65292;&#39044;&#27979;&#25216;&#33021;&#20248;&#20110;&#20256;&#32479;&#26041;&#26696;&#65292;&#19988;&#39044;&#27979;&#26102;&#38388;&#23610;&#24230;&#38543;&#30528;&#39044;&#25253;&#30340;&#31354;&#38388;&#23610;&#24230;&#21576;&#32447;&#24615;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38647;&#26292;&#23545;&#31038;&#20250;&#21644;&#32463;&#27982;&#26500;&#25104;&#37325;&#22823;&#23041;&#32961;&#65292;&#38656;&#35201;&#21487;&#38752;&#30340;&#38647;&#26292;&#39044;&#27979;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SALAMA&#65292;&#19968;&#31181;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#35782;&#21035;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#65288;NWP&#65289;&#25968;&#25454;&#20013;&#30340;&#38647;&#26292;&#21457;&#29983;&#24773;&#20917;&#12290;&#35813;&#27169;&#22411;&#22312;&#20013;&#27431;&#23545;&#27969;&#23618;&#35299;&#26512;&#38598;&#21512;&#39044;&#25253;&#21644;&#38647;&#30005;&#35266;&#27979;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#20165;&#32473;&#20986;&#20174;NWP&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#19982;&#38647;&#26292;&#21457;&#23637;&#30456;&#20851;&#30340;&#20687;&#32032;&#36755;&#20837;&#21442;&#25968;&#38598;&#65292;SALAMA&#20197;&#21487;&#38752;&#30340;&#26657;&#20934;&#26041;&#24335;&#25512;&#26029;&#38647;&#26292;&#21457;&#29983;&#30340;&#27010;&#29575;&#12290;&#23545;&#20110;&#38271;&#36798;11&#23567;&#26102;&#30340;&#21069;&#32622;&#26102;&#38388;&#65292;&#25105;&#20204;&#21457;&#29616;&#20854;&#39044;&#27979;&#25216;&#33021;&#20248;&#20110;&#20165;&#22522;&#20110;&#23545;&#27969;&#26377;&#25928;&#20301;&#33021;&#30340;&#20998;&#31867;&#12290;&#36890;&#36807;&#25913;&#21464;&#23558;&#38378;&#30005;&#35266;&#27979;&#19982;NWP&#25968;&#25454;&#30456;&#20851;&#32852;&#30340;&#26102;&#31354;&#26631;&#20934;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29087;&#32451;&#30340;&#38647;&#26292;&#39044;&#27979;&#26102;&#38388;&#23610;&#24230;&#38543;&#30528;&#39044;&#25253;&#30340;&#31354;&#38388;&#23610;&#24230;&#30340;&#32447;&#24615;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Thunderstorms pose a major hazard to society and economy, which calls for reliable thunderstorm forecasts. In this work, we introduce SALAMA, a feedforward neural network model for identifying thunderstorm occurrence in numerical weather prediction (NWP) data. The model is trained on convection-resolving ensemble forecasts over Central Europe and lightning observations. Given only a set of pixel-wise input parameters that are extracted from NWP data and related to thunderstorm development, SALAMA infers the probability of thunderstorm occurrence in a reliably calibrated manner. For lead times up to eleven hours, we find a forecast skill superior to classification based only on convective available potential energy. Varying the spatiotemporal criteria by which we associate lightning observations with NWP data, we show that the time scale for skillful thunderstorm predictions increases linearly with the spatial scale of the forecast.
&lt;/p&gt;</description></item><item><title>DACOS&#26159;&#19968;&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#21253;&#21547;&#19977;&#31181;&#19981;&#21516;&#31890;&#24230;&#30340;&#20195;&#30721;&#24322;&#21619;&#65288;&#22810;&#38754;&#25277;&#35937;&#12289;&#22797;&#26434;&#26041;&#27861;&#21644;&#38271;&#21442;&#25968;&#21015;&#34920;&#65289;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#26816;&#27979;&#20195;&#30721;&#24322;&#21619;&#30340;&#35757;&#32451;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2303.08729</link><description>&lt;p&gt;
DACOS-&#19968;&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#20195;&#30721;&#24322;&#21619;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DACOS-A Manually Annotated Dataset of Code Smells. (arXiv:2303.08729v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08729
&lt;/p&gt;
&lt;p&gt;
DACOS&#26159;&#19968;&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#21253;&#21547;&#19977;&#31181;&#19981;&#21516;&#31890;&#24230;&#30340;&#20195;&#30721;&#24322;&#21619;&#65288;&#22810;&#38754;&#25277;&#35937;&#12289;&#22797;&#26434;&#26041;&#27861;&#21644;&#38271;&#21442;&#25968;&#21015;&#34920;&#65289;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#26816;&#27979;&#20195;&#30721;&#24322;&#21619;&#30340;&#35757;&#32451;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#26816;&#27979;&#20195;&#30721;&#24322;&#21619;&#65292;&#20197;&#25269;&#28040;&#35768;&#22810;&#20195;&#30721;&#24322;&#21619;&#30340;&#20027;&#35266;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#38656;&#35201;&#19968;&#20010;&#22823;&#22411;&#12289;&#25163;&#21160;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;&#29616;&#26377;&#25991;&#29486;&#25552;&#20379;&#20102;&#19968;&#20123;&#25968;&#25454;&#38598;&#65292;&#20294;&#23427;&#20204;&#35268;&#27169;&#36739;&#23567;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#23427;&#20204;&#27809;&#26377;&#20851;&#27880;&#20027;&#35266;&#20195;&#30721;&#29255;&#27573;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DACOS&#65292;&#19968;&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;10,267&#20010;&#23545;5,192&#20010;&#20195;&#30721;&#29255;&#27573;&#30340;&#27880;&#37322;&#12290;&#35813;&#25968;&#25454;&#38598;&#38024;&#23545;&#19981;&#21516;&#31890;&#24230;&#30340;&#19977;&#31181;&#20195;&#30721;&#24322;&#21619;&#65306;&#22810;&#38754;&#25277;&#35937;&#12289;&#22797;&#26434;&#26041;&#27861;&#21644;&#38271;&#21442;&#25968;&#21015;&#34920;&#12290;&#25968;&#25454;&#38598;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#21019;&#24314;&#12290;&#31532;&#19968;&#38454;&#27573;&#36890;&#36807;&#30830;&#23450;&#29992;&#20110;&#26816;&#27979;&#27668;&#21619;&#30340;&#24230;&#37327;&#26631;&#20934;&#30340;&#38408;&#20540;&#26469;&#24110;&#21161;&#25105;&#20204;&#35782;&#21035;&#21487;&#33021;&#20855;&#26377;&#20027;&#35266;&#24615;&#30340;&#20195;&#30721;&#29255;&#27573;&#12290;&#31532;&#20108;&#38454;&#27573;&#25910;&#38598;&#21487;&#33021;&#20855;&#26377;&#20027;&#35266;&#24615;&#30340;&#20195;&#30721;&#29255;&#27573;&#30340;&#27880;&#37322;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#25193;&#23637;&#25968;&#25454;&#38598;DACOSX&#65292;&#23427;&#20351;&#29992;&#38408;&#20540;&#21253;&#21547;&#26126;&#30830;&#30340;&#33391;&#24615;&#20195;&#30721;&#29255;&#27573;&#21644;&#26126;&#30830;&#30340;&#26377;&#24322;&#21619;&#30340;&#20195;&#30721;&#29255;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers apply machine-learning techniques for code smell detection to counter the subjectivity of many code smells. Such approaches need a large, manually annotated dataset for training and benchmarking. Existing literature offers a few datasets; however, they are small in size and, more importantly, do not focus on the subjective code snippets. In this paper, we present DACOS, a manually annotated dataset containing 10,267 annotations for 5,192 code snippets. The dataset targets three kinds of code smells at different granularity: multifaceted abstraction, complex method, and long parameter list. The dataset is created in two phases. The first phase helps us identify the code snippets that are potentially subjective by determining the thresholds of metrics used to detect a smell. The second phase collects annotations for potentially subjective snippets. We also offer an extended dataset DACOSX that includes definitely benign and definitely smelly snippets by using the thresholds i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;AI&#39537;&#21160;&#30340;&#35828;&#26381;&#26410;&#26469;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21253;&#25324;&#31227;&#21160;&#35828;&#26381;&#21147;&#24179;&#34913;&#30340;&#26041;&#24335;&#12289;&#23450;&#21046;&#21270;&#30340;&#35828;&#26381;&#12289;&#34394;&#20551;&#20449;&#24687;&#30340;&#24102;&#21160;&#20197;&#21450;&#25913;&#21464;&#20154;&#31867;&#33258;&#36523;&#35328;&#35770;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#35686;&#21578;&#23384;&#22312;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#21628;&#21505;&#21152;&#24378;&#23545;&#20854;&#24320;&#21457;&#21644;&#20351;&#29992;&#30340;&#30417;&#31649;&#12290;</title><link>http://arxiv.org/abs/2303.08721</link><description>&lt;p&gt;
&#20154;&#24037;&#24433;&#21709;: AI&#39537;&#21160;&#30340;&#35828;&#26381;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Artificial Influence: An Analysis Of AI-Driven Persuasion. (arXiv:2303.08721v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;AI&#39537;&#21160;&#30340;&#35828;&#26381;&#26410;&#26469;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21253;&#25324;&#31227;&#21160;&#35828;&#26381;&#21147;&#24179;&#34913;&#30340;&#26041;&#24335;&#12289;&#23450;&#21046;&#21270;&#30340;&#35828;&#26381;&#12289;&#34394;&#20551;&#20449;&#24687;&#30340;&#24102;&#21160;&#20197;&#21450;&#25913;&#21464;&#20154;&#31867;&#33258;&#36523;&#35328;&#35770;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#35686;&#21578;&#23384;&#22312;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#21628;&#21505;&#21152;&#24378;&#23545;&#20854;&#24320;&#21457;&#21644;&#20351;&#29992;&#30340;&#30417;&#31649;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35828;&#26381;&#26159;&#20154;&#31867;&#30340;&#37325;&#35201;&#29305;&#24449;&#20043;&#19968;&#65292;&#26159;&#21830;&#19994;&#12289;&#25919;&#27835;&#31561;&#20107;&#19994;&#30340;&#26680;&#24515;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#36827;&#27493;&#24050;&#32463;&#20135;&#29983;&#20102;&#33021;&#22815;&#35828;&#26381;&#20154;&#31867;&#36141;&#20080;&#20135;&#21697;&#12289;&#35266;&#30475;&#35270;&#39057;&#12289;&#28857;&#20987;&#25628;&#32034;&#32467;&#26524;&#31561;&#30340;AI&#31995;&#32479;&#12290;&#21363;&#20351;&#27809;&#26377;&#26126;&#30830;&#35774;&#35745;&#20026;&#35828;&#26381;&#30340;&#31995;&#32479;&#65292;&#22312;&#23454;&#36341;&#20013;&#20063;&#21487;&#33021;&#20250;&#36825;&#26679;&#20570;&#12290;&#26410;&#26469;&#65292;&#36234;&#26469;&#36234;&#20855;&#26377;&#20154;&#24418;&#29305;&#24449;&#30340;AI&#31995;&#32479;&#21487;&#33021;&#20250;&#19982;&#29992;&#25143;&#24418;&#25104;&#25345;&#32493;&#30340;&#20851;&#31995;&#65292;&#25552;&#39640;&#23427;&#20204;&#30340;&#35828;&#26381;&#21147;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;AI&#31995;&#32479;&#30340;&#35828;&#26381;&#33021;&#21147;&#26410;&#26469;&#12290;&#25105;&#20204;&#32771;&#34385;&#21040;AI&#22914;&#20309;&#22312;&#31227;&#21160;&#35828;&#26381;&#21147;&#24179;&#34913;&#30340;&#22522;&#30784;&#19978;&#65292;&#23454;&#29616;&#23450;&#21046;&#21270;&#30340;&#35828;&#26381;&#65292;&#20026;&#34394;&#20551;&#20449;&#24687;&#24102;&#26469;&#21160;&#21147;&#20197;&#21450;&#25913;&#21464;&#20154;&#31867;&#22609;&#36896;&#33258;&#36523;&#35328;&#35770;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#32771;&#34385;AI&#39537;&#21160;&#30340;&#35828;&#26381;&#26041;&#24335;&#21487;&#33021;&#19982;&#20154;&#31867;&#39537;&#21160;&#30340;&#26041;&#24335;&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#35686;&#21578;&#35828;&#65292;&#26222;&#36941;&#23384;&#22312;&#39640;&#24230;&#35828;&#26381;&#21147;&#30340;AI&#31995;&#32479;&#21487;&#33021;&#23545;&#20154;&#31867;&#30340;&#33258;&#20027;&#26435;&#21644;&#31119;&#31049;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#21628;&#21505;&#21152;&#24378;&#20851;&#20110;&#20854;&#24320;&#21457;&#21644;&#20351;&#29992;&#30340;&#23545;&#35805;&#21644;&#30417;&#31649;&#12290;
&lt;/p&gt;
&lt;p&gt;
Persuasion is a key aspect of what it means to be human, and is central to business, politics, and other endeavors. Advancements in artificial intelligence (AI) have produced AI systems that are capable of persuading humans to buy products, watch videos, click on search results, and more. Even systems that are not explicitly designed to persuade may do so in practice. In the future, increasingly anthropomorphic AI systems may form ongoing relationships with users, increasing their persuasive power. This paper investigates the uncertain future of persuasive AI systems. We examine ways that AI could qualitatively alter our relationship to and views regarding persuasion by shifting the balance of persuasive power, allowing personalized persuasion to be deployed at scale, powering misinformation campaigns, and changing the way humans can shape their own discourse. We consider ways AI-driven persuasion could differ from human-driven persuasion. We warn that ubiquitous highlypersuasive AI sy
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#35780;&#20272;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#26377;&#28508;&#21147;&#28385;&#36275;&#25105;&#20204;&#35201;&#27714;&#30340;&#22495;&#36866;&#24212;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#30028;&#38480;&#65292;&#21457;&#29616;&#25152;&#26377;&#30028;&#38480;&#37117;&#26159;&#31354;&#27867;&#30340;&#65292;&#26679;&#26412;&#27867;&#21270;&#26415;&#35821;&#21344;&#25454;&#20102;&#35266;&#23519;&#21040;&#30340;&#26494;&#24347;&#31243;&#24230;&#30340;&#24456;&#22823;&#37096;&#20998;&#65292;&#29305;&#21035;&#26159;&#24403;&#36825;&#20123;&#26415;&#35821;&#19982;&#22495;&#30340;&#24230;&#37327;&#20114;&#21160;&#26102;&#12290;</title><link>http://arxiv.org/abs/2303.08720</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#30340;&#27867;&#21270;&#20445;&#35777;&#30340;&#23454;&#29992;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Practicality of generalization guarantees for unsupervised domain adaptation with neural networks. (arXiv:2303.08720v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08720
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35780;&#20272;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#26377;&#28508;&#21147;&#28385;&#36275;&#25105;&#20204;&#35201;&#27714;&#30340;&#22495;&#36866;&#24212;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#30028;&#38480;&#65292;&#21457;&#29616;&#25152;&#26377;&#30028;&#38480;&#37117;&#26159;&#31354;&#27867;&#30340;&#65292;&#26679;&#26412;&#27867;&#21270;&#26415;&#35821;&#21344;&#25454;&#20102;&#35266;&#23519;&#21040;&#30340;&#26494;&#24347;&#31243;&#24230;&#30340;&#24456;&#22823;&#37096;&#20998;&#65292;&#29305;&#21035;&#26159;&#24403;&#36825;&#20123;&#26415;&#35821;&#19982;&#22495;&#30340;&#24230;&#37327;&#20114;&#21160;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#27867;&#21270;&#23545;&#20110;&#33258;&#20449;&#22320;&#35774;&#35745;&#21644;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#24403;&#37096;&#32626;&#24847;&#21619;&#30528;&#25968;&#25454;&#22495;&#30340;&#36716;&#31227;&#26102;&#12290;&#23545;&#20110;&#36825;&#26679;&#30340;&#22495;&#36866;&#24212;&#38382;&#39064;&#65292;&#25105;&#20204;&#23547;&#27714;&#21487;&#35745;&#31639;&#21644;&#32039;&#23494;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#22914;&#26524;&#21487;&#20197;&#23454;&#29616;&#36825;&#20123;&#35201;&#27714;&#65292;&#36825;&#20123;&#30028;&#38480;&#21487;&#20197;&#20316;&#20026;&#37096;&#32626;&#20013;&#20805;&#36275;&#24615;&#33021;&#30340;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#39318;&#36873;&#27169;&#22411;&#30340;&#24212;&#29992;&#20013;&#65292;&#25512;&#23548;&#20986;&#28385;&#36275;&#36825;&#20123;&#35201;&#27714;&#30340;&#32467;&#26524;&#20173;&#26159;&#19968;&#39033;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#65307;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#30028;&#38480;&#35201;&#20040;&#26159;&#31354;&#27867;&#30340;&#65292;&#35201;&#20040;&#26377;&#19981;&#21487;&#20272;&#35745;&#30340;&#26415;&#35821;&#65292;&#21363;&#20351;&#22312;&#26377;&#21033;&#26465;&#20214;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#26377;&#28508;&#21147;&#28385;&#36275;&#25105;&#20204;&#35201;&#27714;&#30340;&#22495;&#36866;&#24212;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#30028;&#38480;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#39318;&#36873;&#12290;&#25105;&#20204;&#21457;&#29616;&#25152;&#26377;&#30028;&#38480;&#37117;&#26159;&#31354;&#27867;&#30340;&#65292;&#24182;&#19988;&#26679;&#26412;&#27867;&#21270;&#26415;&#35821;&#21344;&#25454;&#20102;&#35266;&#23519;&#21040;&#30340;&#26494;&#24347;&#31243;&#24230;&#30340;&#24456;&#22823;&#37096;&#20998;&#65292;&#29305;&#21035;&#26159;&#24403;&#36825;&#20123;&#26415;&#35821;&#19982;&#22495;&#30340;&#24230;&#37327;&#20114;&#21160;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding generalization is crucial to confidently engineer and deploy machine learning models, especially when deployment implies a shift in the data domain. For such domain adaptation problems, we seek generalization bounds which are tractably computable and tight. If these desiderata can be reached, the bounds can serve as guarantees for adequate performance in deployment. However, in applications where deep neural networks are the models of choice, deriving results which fulfill these remains an unresolved challenge; most existing bounds are either vacuous or has non-estimable terms, even in favorable conditions. In this work, we evaluate existing bounds from the literature with potential to satisfy our desiderata on domain adaptation image classification tasks, where deep neural networks are preferred. We find that all bounds are vacuous and that sample generalization terms account for much of the observed looseness, especially when these terms interact with measures of domain
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;SSBM&#65292;&#23427;&#21482;&#38656;&#35201;&#20108;&#36827;&#21046;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#25506;&#32034;&#20102;&#20174;&#19981;&#23436;&#25972;&#30340;&#20108;&#36827;&#21046;&#35266;&#23519;&#20013;&#23398;&#20064;&#30340;&#26497;&#31471;&#24773;&#20917;&#12290;&#36825;&#20026;&#20174;&#20108;&#36827;&#21046;&#27979;&#37327;&#20013;&#24674;&#22797;&#20449;&#21495;&#25552;&#20379;&#20102;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;SSBM&#30340;&#21331;&#36234;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.08691</link><description>&lt;p&gt;
&#20174;&#20108;&#36827;&#21046;&#27979;&#37327;&#20013;&#23398;&#20064;&#20449;&#21495;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
Learning to Reconstruct Signals From Binary Measurements. (arXiv:2303.08691v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08691
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;SSBM&#65292;&#23427;&#21482;&#38656;&#35201;&#20108;&#36827;&#21046;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#25506;&#32034;&#20102;&#20174;&#19981;&#23436;&#25972;&#30340;&#20108;&#36827;&#21046;&#35266;&#23519;&#20013;&#23398;&#20064;&#30340;&#26497;&#31471;&#24773;&#20917;&#12290;&#36825;&#20026;&#20174;&#20108;&#36827;&#21046;&#27979;&#37327;&#20013;&#24674;&#22797;&#20449;&#21495;&#25552;&#20379;&#20102;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;SSBM&#30340;&#21331;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#31361;&#20986;&#20102;&#20165;&#20174;&#22122;&#22768;&#21644;&#19981;&#23436;&#25972;&#30340;&#32447;&#24615;&#27979;&#37327;&#20013;&#23398;&#20064;&#20449;&#21495;&#37325;&#26500;&#30340;&#21487;&#33021;&#24615;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#21307;&#23398;&#21644;&#31185;&#23398;&#25104;&#20687;&#20197;&#21450;&#20256;&#24863;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#65292;&#20854;&#20013;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#32463;&#24120;&#31232;&#32570;&#25110;&#38590;&#20197;&#33719;&#24471;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#27979;&#37327;&#19981;&#20165;&#22122;&#22768;&#21644;&#19981;&#23436;&#25972;&#65292;&#32780;&#19988;&#36824;&#34987;&#37327;&#21270;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#32034;&#20174;&#20108;&#36827;&#21046;&#35266;&#23519;&#20013;&#23398;&#20064;&#30340;&#26497;&#31471;&#24773;&#20917;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#20174;&#19981;&#23436;&#25972;&#20108;&#36827;&#21046;&#25968;&#25454;&#20013;&#35782;&#21035;&#19968;&#32452;&#20449;&#21495;&#25152;&#38656;&#30340;&#27979;&#37327;&#25968;&#37327;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#23545;&#20174;&#20108;&#36827;&#21046;&#27979;&#37327;&#20013;&#20449;&#21495;&#24674;&#22797;&#29616;&#26377;&#30028;&#38480;&#30340;&#34917;&#20805;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;&#8220;SSBM&#8221;&#65292;&#23427;&#20165;&#38656;&#35201;&#20108;&#36827;&#21046;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;SSBM&#19982;&#30417;&#30563;&#23398;&#20064;&#30456;&#24403;&#65292;&#24182;&#20248;&#20110;&#31232;&#30095;&#37325;&#26500;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in unsupervised learning have highlighted the possibility of learning to reconstruct signals from noisy and incomplete linear measurements alone. These methods play a key role in medical and scientific imaging and sensing, where ground truth data is often scarce or difficult to obtain. However, in practice, measurements are not only noisy and incomplete but also quantized. Here we explore the extreme case of learning from binary observations and provide necessary and sufficient conditions on the number of measurements required for identifying a set of signals from incomplete binary data. Our results are complementary to existing bounds on signal recovery from binary measurements. Furthermore, we introduce a novel self-supervised learning approach, which we name SSBM, that only requires binary data for training. We demonstrate in a series of experiments with real datasets that SSBM performs on par with supervised learning and outperforms sparse reconstruction methods wit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#23616;&#37096;&#36951;&#24536;&#30340;&#26032;&#22411;&#37325;&#25918;&#32531;&#20914;&#21306;&#65292;&#21487;&#20197;&#22312;&#29366;&#24577;&#31354;&#38388;&#30340;&#30456;&#20851;&#37096;&#20998;&#24555;&#36895;&#36951;&#24536;&#36807;&#26102;&#30340;&#32463;&#39564;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#33258;&#36866;&#24212;&#28145;&#24230;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#23545;&#29615;&#22659;&#21464;&#21270;&#30340;&#36866;&#24212;&#33021;&#21147;&#65292;&#21152;&#36895;&#20102;&#23398;&#20064;&#36895;&#24230;&#24182;&#25913;&#21892;&#20102;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2303.08690</link><description>&lt;p&gt;
&#24102;&#26377;&#23616;&#37096;&#36951;&#24536;&#30340;&#37325;&#25918;&#32531;&#20914;&#21306;&#29992;&#20110;&#33258;&#36866;&#24212;&#28145;&#24230;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Replay Buffer With Local Forgetting for Adaptive Deep Model-Based Reinforcement Learning. (arXiv:2303.08690v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08690
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#23616;&#37096;&#36951;&#24536;&#30340;&#26032;&#22411;&#37325;&#25918;&#32531;&#20914;&#21306;&#65292;&#21487;&#20197;&#22312;&#29366;&#24577;&#31354;&#38388;&#30340;&#30456;&#20851;&#37096;&#20998;&#24555;&#36895;&#36951;&#24536;&#36807;&#26102;&#30340;&#32463;&#39564;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#33258;&#36866;&#24212;&#28145;&#24230;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#23545;&#29615;&#22659;&#21464;&#21270;&#30340;&#36866;&#24212;&#33021;&#21147;&#65292;&#21152;&#36895;&#20102;&#23398;&#20064;&#36895;&#24230;&#24182;&#25913;&#21892;&#20102;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31185;&#23398;&#20013;&#29992;&#20110;&#30830;&#23450;&#25152;&#30740;&#31350;&#30340;&#23545;&#35937;&#65288;&#26080;&#35770;&#26159;&#21870;&#40831;&#21160;&#29289;&#36824;&#26159;&#20154;&#31867;&#65289;&#26159;&#21542;&#34920;&#29616;&#20986;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#23398;&#20064;&#30340;&#20851;&#38190;&#34892;&#20026;&#29305;&#24449;&#20043;&#19968;&#26159;&#23545;&#29615;&#22659;&#20013;&#23616;&#37096;&#21464;&#21270;&#30340;&#26377;&#25928;&#36866;&#24212;&#12290;&#28982;&#32780;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#20195;&#28145;&#24230;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#65288;MBRL&#65289;&#26041;&#27861;&#36739;&#38590;&#36866;&#24212;&#36825;&#31181;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37325;&#25918;&#32531;&#20914;&#21306;&#65292;&#24102;&#26377;&#23616;&#37096;&#36951;&#24536;&#65292;&#21487;&#20197;&#24555;&#36895;&#22320;&#22312;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#30456;&#20851;&#37096;&#20998;&#36951;&#24536;&#36807;&#26102;&#30340;&#32463;&#39564;&#32780;&#22312;&#20854;&#20182;&#22320;&#26041;&#20445;&#30041;&#26087;&#25968;&#25454;&#12290;&#36890;&#36807;&#22312;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23548;&#33322;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#25913;&#21892;&#20102;&#23545;&#29615;&#22659;&#21464;&#21270;&#30340;&#36866;&#24212;&#33021;&#21147;&#65292;&#21152;&#24555;&#20102;&#23398;&#20064;&#36895;&#24230;&#24182;&#25913;&#21892;&#20102;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the key behavioral characteristics used in neuroscience to determine whether the subject of study -- be it a rodent or a human -- exhibits model-based learning is effective adaptation to local changes in the environment. In reinforcement learning, however, recent work has shown that modern deep model-based reinforcement-learning (MBRL) methods adapt poorly to such changes. An explanation for this mismatch is that MBRL methods are typically designed with sample-efficiency on a single task in mind and the requirements for effective adaptation are substantially higher, both in terms of the learned world model and the planning routine. One particularly challenging requirement is that the learned world model has to be sufficiently accurate throughout relevant parts of the state-space. This is challenging for deep-learning-based world models due to catastrophic forgetting. And while a replay buffer can mitigate the effects of catastrophic forgetting, the traditional first-in-first-out
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Semantic Token ViT (STViT)&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20840;&#23616;&#21644;&#26412;&#22320;&#35270;&#35273;Transformer&#30340;&#39640;&#25928;&#24615;&#33021;&#65292;&#21516;&#26102;&#21487;&#29992;&#20316;&#19979;&#28216;&#20219;&#21153;&#30340;&#20027;&#24178;&#39592;&#24178;&#12290;&#20854;&#36890;&#36807;&#32858;&#31867;&#20013;&#24515;&#30340;&#35821;&#20041;&#20196;&#29260;&#20195;&#34920;&#26469;&#20195;&#26367;&#22270;&#20687;&#20196;&#29260;&#65292;&#23454;&#29616;&#36739;&#23569;&#30340;&#35821;&#20041;&#20196;&#29260;&#21363;&#21487;&#36798;&#21040;&#21516;&#26679;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.08685</link><description>&lt;p&gt;
&#22522;&#20110;&#20196;&#29260;&#31232;&#30095;&#21270;&#35270;&#35282;&#30340;&#35270;&#35273;Transformer&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Making Vision Transformers Efficient from A Token Sparsification View. (arXiv:2303.08685v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Semantic Token ViT (STViT)&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20840;&#23616;&#21644;&#26412;&#22320;&#35270;&#35273;Transformer&#30340;&#39640;&#25928;&#24615;&#33021;&#65292;&#21516;&#26102;&#21487;&#29992;&#20316;&#19979;&#28216;&#20219;&#21153;&#30340;&#20027;&#24178;&#39592;&#24178;&#12290;&#20854;&#36890;&#36807;&#32858;&#31867;&#20013;&#24515;&#30340;&#35821;&#20041;&#20196;&#29260;&#20195;&#34920;&#26469;&#20195;&#26367;&#22270;&#20687;&#20196;&#29260;&#65292;&#23454;&#29616;&#36739;&#23569;&#30340;&#35821;&#20041;&#20196;&#29260;&#21363;&#21487;&#36798;&#21040;&#21516;&#26679;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer (ViTs)&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#38543;&#30528;&#20196;&#29260;&#25968;&#37327;&#21576;&#20108;&#27425;&#22686;&#38271;&#65292;&#38480;&#21046;&#20102;&#20854;&#23454;&#38469;&#24212;&#29992;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#25928;&#30340;ViT&#65292;&#24050;&#26377;&#22810;&#31181;&#26041;&#27861;&#36890;&#36807;&#20462;&#21098;&#20887;&#20313;&#20196;&#29260;&#26469;&#36798;&#21040;&#30446;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#23384;&#22312;&#20197;&#19979;&#38382;&#39064;&#65306;(i) &#26174;&#33879;&#30340;&#31934;&#24230;&#19979;&#38477;&#65292;(ii) &#26080;&#27861;&#24212;&#29992;&#20110;&#26412;&#22320;&#35270;&#35273;Transformer&#20013;&#65292;&#20197;&#21450; (iii) &#26080;&#27861;&#36890;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#32593;&#32476;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#20041;&#20196;&#29260;ViT&#65288;STViT&#65289;&#65292;&#29992;&#20110;&#23454;&#29616;&#20840;&#23616;&#21644;&#26412;&#22320;&#35270;&#35273;Transformer&#30340;&#39640;&#25928;&#24615;&#33021;&#65292;&#24182;&#21487;&#20316;&#20026;&#19979;&#28216;&#20219;&#21153;&#30340;&#20027;&#24178;&#39592;&#24178;&#36827;&#34892;&#20462;&#35746;&#12290;&#35821;&#20041;&#20196;&#29260;&#20195;&#34920;&#32858;&#31867;&#20013;&#24515;&#65292;&#20854;&#36890;&#36807;&#31354;&#38388;&#20869;&#30340;&#22270;&#20687;&#20196;&#29260;&#27719;&#38598;&#26469;&#21021;&#22987;&#21270;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#32452;&#20214;&#36827;&#34892;&#24674;&#22797;&#65292;&#33258;&#36866;&#24212;&#22320;&#34920;&#31034;&#20840;&#23616;&#25110;&#26412;&#22320;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#30001;&#20110;&#20854;&#32858;&#31867;&#24615;&#36136;&#65292;&#23569;&#37327;&#30340;&#35821;&#20041;&#20196;&#29260;&#21363;&#21487;&#23454;&#29616;&#19982;&#20247;&#22810;&#22270;&#20687;&#20196;&#29260;&#30456;&#21516;&#30340;&#25928;&#26524;&#65292;&#36866;&#29992;&#20110;&#20840;&#23616;&#21644;&#26412;&#22320;&#35270;&#35273;Transformer&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;DeiT-(Tiny, Small, Base)&#65292;&#20165;&#38656;16&#20010;&#35821;&#20041;&#20196;&#29260;&#21363;&#21487;&#36798;&#21040;&#30456;&#21516;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quadratic computational complexity to the number of tokens limits the practical applications of Vision Transformers (ViTs). Several works propose to prune redundant tokens to achieve efficient ViTs. However, these methods generally suffer from (i) dramatic accuracy drops, (ii) application difficulty in the local vision transformer, and (iii) non-general-purpose networks for downstream tasks. In this work, we propose a novel Semantic Token ViT (STViT), for efficient global and local vision transformers, which can also be revised to serve as backbone for downstream tasks. The semantic tokens represent cluster centers, and they are initialized by pooling image tokens in space and recovered by attention, which can adaptively represent global or local semantic information. Due to the cluster properties, a few semantic tokens can attain the same effect as vast image tokens, for both global and local vision transformers. For instance, only 16 semantic tokens on DeiT-(Tiny,Small,Base) can 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#20154;&#26426;&#36741;&#21161;&#32593;&#32476;&#20013;&#22810;&#26234;&#33021;&#20307;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#30340;&#25968;&#25454;&#23454;&#26102;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#26080;&#20154;&#26426;&#30340;&#36712;&#36857;&#35774;&#35745;&#21644;&#23545;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#35775;&#38382;&#65292;&#24182;&#25104;&#21151;&#22320;&#26368;&#23567;&#21270;&#20102;&#20840;&#23616;&#26356;&#26032;&#24180;&#40836;&#65288;AoU&#65289;&#12290;</title><link>http://arxiv.org/abs/2303.08680</link><description>&lt;p&gt;
&#26080;&#20154;&#26426;&#36741;&#21161;&#32593;&#32476;&#20013;&#30340;&#25968;&#25454;&#23454;&#26102;&#24615;&#22810;&#26234;&#33021;&#20307;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Muti-Agent Proximal Policy Optimization For Data Freshness in UAV-assisted Networks. (arXiv:2303.08680v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#20154;&#26426;&#36741;&#21161;&#32593;&#32476;&#20013;&#22810;&#26234;&#33021;&#20307;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#30340;&#25968;&#25454;&#23454;&#26102;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#26080;&#20154;&#26426;&#30340;&#36712;&#36857;&#35774;&#35745;&#21644;&#23545;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#35775;&#38382;&#65292;&#24182;&#25104;&#21151;&#22320;&#26368;&#23567;&#21270;&#20102;&#20840;&#23616;&#26356;&#26032;&#24180;&#40836;&#65288;AoU&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20154;&#26426;&#34987;&#35270;&#20026;&#22312;&#26080;&#32447;&#36890;&#20449;&#32593;&#32476;&#20013;&#25191;&#34892;&#24191;&#27867;&#20219;&#21153;&#30340;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#12290;&#26412;&#25991;&#32771;&#34385;&#37096;&#32626;&#19968;&#32452;&#26080;&#20154;&#26426;&#26469;&#25910;&#38598;&#29289;&#32852;&#32593;&#35774;&#22791;&#29983;&#25104;&#30340;&#25968;&#25454;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#25910;&#38598;&#30340;&#25968;&#25454;&#20855;&#26377;&#23454;&#26102;&#24615;&#30340;&#24773;&#20917;&#65292;&#24182;&#19988;&#20445;&#25345;&#20854;&#21450;&#26102;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#26368;&#20248;&#22320;&#35774;&#35745;&#26080;&#20154;&#26426;&#36712;&#36857;&#21644;&#35775;&#38382;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#23376;&#38598;&#65292;&#20197;&#26368;&#23567;&#21270;&#20840;&#23616;&#26356;&#26032;&#24180;&#40836;&#65288;Age-of-Updates, AoU&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#30740;&#31350;&#23545;&#35937;&#30340;&#38382;&#39064;&#21046;&#23450;&#20026;&#19968;&#31181;&#28151;&#21512;&#25972;&#25968;&#38750;&#32447;&#24615;&#35268;&#21010;&#65288;MINLP&#65289;&#65292;&#24182;&#22312;&#26102;&#38388;&#21644;&#26381;&#21153;&#36136;&#37327;&#38480;&#21046;&#19979;&#36827;&#34892;&#20248;&#21270;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#35299;&#20915;&#25152;&#24471;&#21040;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#34892;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;Policy Proximal Optimization, PPO&#65289;&#31639;&#27861;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#38598;&#20013;&#24335;&#35757;&#32451;&#21644;&#20998;&#25955;&#25191;&#34892;&#30340;RL&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unmanned aerial vehicles (UAVs) are seen as a promising technology to perform a wide range of tasks in wireless communication networks. In this work, we consider the deployment of a group of UAVs to collect the data generated by IoT devices. Specifically, we focus on the case where the collected data is time-sensitive, and it is critical to maintain its timeliness. Our objective is to optimally design the UAVs' trajectories and the subsets of visited IoT devices such as the global Age-of-Updates (AoU) is minimized. To this end, we formulate the studied problem as a mixed-integer nonlinear programming (MINLP) under time and quality of service constraints. To efficiently solve the resulting optimization problem, we investigate the cooperative Multi-Agent Reinforcement Learning (MARL) framework and propose an RL approach based on the popular on-policy Reinforcement Learning (RL) algorithm: Policy Proximal Optimization (PPO). Our approach leverages the centralized training decentralized ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#25552;&#31034;&#30340;PFL&#26694;&#26550;pFedPT&#65292;&#36890;&#36807;&#21033;&#29992;&#20010;&#24615;&#21270;&#30340;&#35270;&#35273;&#25552;&#31034;&#38544;&#21547;&#22320;&#34920;&#31034;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#20449;&#24687;&#65292;&#24182;&#23558;&#35813;&#20449;&#24687;&#25552;&#20379;&#32473;&#32858;&#21512;&#27169;&#22411;&#26469;&#24110;&#21161;&#20998;&#31867;&#20219;&#21153;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#31639;&#27861;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.08678</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#25552;&#31034;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Visual Prompt Based Personalized Federated Learning. (arXiv:2303.08678v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#25552;&#31034;&#30340;PFL&#26694;&#26550;pFedPT&#65292;&#36890;&#36807;&#21033;&#29992;&#20010;&#24615;&#21270;&#30340;&#35270;&#35273;&#25552;&#31034;&#38544;&#21547;&#22320;&#34920;&#31034;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#20449;&#24687;&#65292;&#24182;&#23558;&#35813;&#20449;&#24687;&#25552;&#20379;&#32473;&#32858;&#21512;&#27169;&#22411;&#26469;&#24110;&#21161;&#20998;&#31867;&#20219;&#21153;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#31639;&#27861;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;PFL&#65289;&#20316;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#33539;&#24335;&#65292;&#20801;&#35768;&#20010;&#24615;&#21270;&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#25152;&#26377;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#30340;&#30693;&#35782;&#26469;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;PFL&#31639;&#27861;&#37319;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20010;&#24615;&#21270;&#38382;&#39064;&#65292;&#20363;&#22914;&#20010;&#24615;&#21270;&#30340;&#23618;&#21010;&#20998;&#12289;&#27169;&#22411;&#27491;&#21017;&#21270;&#21644;&#27169;&#22411;&#25554;&#20540;&#65292;&#36825;&#20123;&#26041;&#27861;&#37117;&#26080;&#27861;&#32771;&#34385;&#21040;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#29305;&#24449;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;PFL&#26694;&#26550;pFedPT&#65292;&#23427;&#21033;&#29992;&#20010;&#24615;&#21270;&#30340;&#35270;&#35273;&#25552;&#31034;&#26469;&#38544;&#21547;&#22320;&#34920;&#31034;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#20449;&#24687;&#65292;&#24182;&#23558;&#35813;&#20449;&#24687;&#25552;&#20379;&#32473;&#32858;&#21512;&#27169;&#22411;&#26469;&#24110;&#21161;&#20998;&#31867;&#20219;&#21153;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;pFedPT&#35757;&#32451;&#30340;&#27599;&#19968;&#36718;&#20013;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#37117;&#20250;&#29983;&#25104;&#19982;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#30456;&#20851;&#30340;&#20010;&#24615;&#21270;&#26412;&#22320;&#25552;&#31034;&#65292;&#28982;&#21518;&#22312;&#30001;&#21407;&#22987;&#25968;&#25454;&#21644;&#35270;&#35273;&#25552;&#31034;&#32452;&#25104;&#30340;&#36755;&#20837;&#19978;&#35757;&#32451;&#26412;&#22320;&#27169;&#22411;&#20197;&#23398;&#20064;&#20998;&#24067;&#24335;&#25968;&#25454;&#29305;&#24449;&#65292;&#26368;&#21518;&#23558;&#35270;&#35273;&#25552;&#31034;&#21457;&#36865;&#21040;&#38598;&#20013;&#24335;&#32858;&#21512;&#22120;&#20197;&#26356;&#26032;&#20840;&#23616;&#27169;&#22411;&#12290;&#22312;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;PFL&#31639;&#27861;&#65292;pFedPT&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a popular paradigm of distributed learning, personalized federated learning (PFL) allows personalized models to improve generalization ability and robustness by utilizing knowledge from all distributed clients. Most existing PFL algorithms tackle personalization in a model-centric way, such as personalized layer partition, model regularization, and model interpolation, which all fail to take into account the data characteristics of distributed clients. In this paper, we propose a novel PFL framework for image classification tasks, dubbed pFedPT, that leverages personalized visual prompts to implicitly represent local data distribution information of clients and provides that information to the aggregation model to help with classification tasks. Specifically, in each round of pFedPT training, each client generates a local personalized prompt related to local data distribution. Then, the local model is trained on the input composed of raw data and a visual prompt to learn the distrib
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23558;&#22270;&#20687;&#20998;&#21106;&#65292;&#24182;&#20026;&#26102;&#23578;&#20154;&#21592;&#24314;&#35758;&#19982;&#36755;&#20837;&#22270;&#20687;&#30456;&#20284;&#30340;&#25670;&#23039;&#65292;&#21516;&#26102;&#25193;&#23637;&#20102;&#24037;&#20316;&#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2303.08660</link><description>&lt;p&gt;
&#29992;&#26426;&#22120;&#23398;&#20064;&#29983;&#25104;&#26102;&#23578;&#27169;&#29305;&#30340;&#25670;&#23039;&#24314;&#35758;&#21644;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Fashion-model pose recommendation and generation using Machine Learning. (arXiv:2303.08660v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23558;&#22270;&#20687;&#20998;&#21106;&#65292;&#24182;&#20026;&#26102;&#23578;&#20154;&#21592;&#24314;&#35758;&#19982;&#36755;&#20837;&#22270;&#20687;&#30456;&#20284;&#30340;&#25670;&#23039;&#65292;&#21516;&#26102;&#25193;&#23637;&#20102;&#24037;&#20316;&#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#23578;&#27169;&#29305;&#30340;&#25670;&#23039;&#26159;&#26102;&#23578;&#34892;&#19994;&#20013;&#24456;&#37325;&#35201;&#30340;&#23646;&#24615;&#12290;&#21019;&#24847;&#24635;&#30417;&#12289;&#27169;&#29305;&#21046;&#20316;&#20844;&#21496;&#21644;&#39030;&#23574;&#25668;&#24433;&#24072;&#24635;&#26159;&#22312;&#23547;&#25214;&#33021;&#22815;&#27491;&#30830;&#25670;&#23039;&#30340;&#19987;&#19994;&#27169;&#29305;&#65292;&#27809;&#26377;&#27491;&#30830;&#25670;&#23039;&#30340;&#25216;&#33021;&#65292;&#20182;&#20204;&#23601;&#24456;&#38590;&#33719;&#24471;&#19987;&#19994;&#30340;&#27169;&#29305;&#24037;&#20316;&#12290;&#26412;&#30740;&#31350;&#38598;&#20013;&#20110;&#20026;&#26102;&#23578;&#20154;&#21592;&#24314;&#35758;&#19968;&#31995;&#21015;&#19982;&#36755;&#20837;&#22270;&#20687;&#30456;&#20284;&#30340;&#22270;&#20687;&#12290;&#35813;&#22270;&#20687;&#34987;&#20998;&#21106;&#25104;&#19981;&#21516;&#30340;&#37096;&#20998;&#65292;&#24182;&#20026;&#29992;&#25143;&#24314;&#35758;&#30456;&#20284;&#30340;&#22270;&#20687;&#12290;&#36825;&#26159;&#36890;&#36807;&#35745;&#31639;&#36755;&#20837;&#22270;&#20687;&#30340;&#39068;&#33394;&#30452;&#26041;&#22270;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#25152;&#26377;&#22270;&#20687;&#24182;&#27604;&#36739;&#30452;&#26041;&#22270;&#26469;&#23454;&#29616;&#30340;&#12290;&#20026;&#20102;&#36991;&#20813;&#38544;&#31169;&#38382;&#39064;&#21644;&#20811;&#26381;&#25668;&#24433;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#65292;&#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#36824;&#25193;&#23637;&#20102;&#20174;&#24314;&#35758;&#20013;&#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fashion-model pose is an important attribute in the fashion industry. Creative directors, modeling production houses, and top photographers always look for professional models able to pose. without the skill to correctly pose, their chances of landing professional modeling employment are regrettably quite little. There are occasions when models and photographers are unsure of the best pose to strike while taking photographs. This research concentrates on suggesting the fashion personnel a series of similar images based on the input image. The image is segmented into different parts and similar images are suggested for the user. This was achieved by calculating the color histogram of the input image and applying the same for all the images in the dataset and comparing the histograms. Synthetic images have become popular to avoid privacy concerns and to overcome the high cost of photoshoots. Hence, this paper also extends the work of generating synthetic images from the recommendation en
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22235;&#31181;&#23646;&#24615;&#26041;&#27861;&#26469;&#35299;&#37322;&#21307;&#23398;&#21333;&#32454;&#32990;&#22270;&#20687;&#22810;&#23454;&#20363;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#22312;&#34880;&#30284;&#24739;&#32773;&#30340;&#34880;&#28082;&#28034;&#29255;&#20013;&#25512;&#23548;&#20986;&#20687;&#32032;&#32423;&#21035;&#30340;&#34880;&#30284;&#35786;&#26029;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2303.08632</link><description>&lt;p&gt;
&#21307;&#23398;&#21333;&#32454;&#32990;&#22270;&#20687;&#22810;&#23454;&#20363;&#23398;&#20064;&#27169;&#22411;&#30340;&#20687;&#32032;&#32423;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Pixel-Level Explanation of Multiple Instance Learning Models in Biomedical Single Cell Images. (arXiv:2303.08632v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22235;&#31181;&#23646;&#24615;&#26041;&#27861;&#26469;&#35299;&#37322;&#21307;&#23398;&#21333;&#32454;&#32990;&#22270;&#20687;&#22810;&#23454;&#20363;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#22312;&#34880;&#30284;&#24739;&#32773;&#30340;&#34880;&#28082;&#28034;&#29255;&#20013;&#25512;&#23548;&#20986;&#20687;&#32032;&#32423;&#21035;&#30340;&#34880;&#30284;&#35786;&#26029;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#24615;&#26159;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20851;&#38190;&#35201;&#27714;&#12290;&#22810;&#23454;&#20363;&#23398;&#20064;&#25552;&#20379;&#23454;&#20363;&#32423;&#35299;&#37322;&#65292;&#20294;&#35768;&#22810;&#20020;&#24202;&#24212;&#29992;&#38656;&#35201;&#26356;&#28145;&#20837;&#30340;&#20687;&#32032;&#32423;&#35299;&#37322;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#23578;&#26410;&#23454;&#29616;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22235;&#31181;&#23646;&#24615;&#26041;&#27861;&#65292;&#21363;GradCAM&#12289;LRP&#12289;IBA&#21644;InputIBA&#65292;&#20197;&#35299;&#37322;&#22810;&#23454;&#20363;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#36825;&#20123;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#24739;&#32773;&#30340;&#34880;&#28082;&#28034;&#29255;&#20013;&#25512;&#23548;&#20986;&#20687;&#32032;&#32423;&#21035;&#30340;&#34880;&#30284;&#35786;&#26029;&#35299;&#37322;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#32452;&#24613;&#24615;&#39635;&#24615;&#30333;&#34880;&#30149;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;100,000&#20010;&#21333;&#20010;&#32454;&#32990;&#22270;&#20687;&#65292;&#24182;&#35266;&#23519;&#27599;&#31181;&#23646;&#24615;&#26041;&#27861;&#22312;&#20851;&#27880;&#30333;&#32454;&#32990;&#21333;&#20010;&#32454;&#32990;&#30340;&#19981;&#21516;&#23646;&#24615;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#26550;&#26500;&#19978;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#23646;&#24615;&#22270;&#19982;&#21307;&#23398;&#19987;&#23478;&#30340;&#27880;&#37322;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#20102;&#35299;&#22235;&#31181;&#23646;&#24615;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainability is a key requirement for computer-aided diagnosis systems in clinical decision-making. Multiple instance learning with attention pooling provides instance-level explainability, however for many clinical applications a deeper, pixel-level explanation is desirable, but missing so far. In this work, we investigate the use of four attribution methods to explain a multiple instance learning models: GradCAM, Layer-Wise Relevance Propagation (LRP), Information Bottleneck Attribution (IBA), and InputIBA. With this collection of methods, we can derive pixel-level explanations on for the task of diagnosing blood cancer from patients' blood smears. We study two datasets of acute myeloid leukemia with over 100 000 single cell images and observe how each attribution method performs on the multiple instance learning architecture focusing on different properties of the white blood single cells. Additionally, we compare attribution maps with the annotations of a medical expert to see ho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#32531;&#35299;Q&#23398;&#20064;&#31639;&#27861;&#39640;&#20272;&#38382;&#39064;&#30340;&#24179;&#28369;Q&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.08631</link><description>&lt;p&gt;
&#24179;&#28369;Q&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Smoothed Q-learning. (arXiv:2303.08631v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#32531;&#35299;Q&#23398;&#20064;&#31639;&#27861;&#39640;&#20272;&#38382;&#39064;&#30340;&#24179;&#28369;Q&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;Q&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#35777;&#26126;&#25910;&#25947;&#21040;&#26368;&#20248;&#35299;&#12290;&#28982;&#32780;&#65292;&#27491;&#22914;&#20854;&#20182;&#20154;&#25152;&#35777;&#26126;&#30340;&#37027;&#26679;&#65292;Q&#23398;&#20064;&#20063;&#21487;&#33021;&#39640;&#20272;&#20215;&#20540;&#65292;&#22240;&#27492;&#33457;&#36153;&#22826;&#38271;&#26102;&#38388;&#25506;&#32034;&#26080;&#29992;&#29366;&#24577;&#12290;&#21452;Q&#23398;&#20064;&#26159;&#19968;&#31181;&#21487;&#35777;&#26126;&#25910;&#25947;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21487;&#20197;&#32531;&#35299;&#19968;&#20123;&#39640;&#20272;&#38382;&#39064;&#65292;&#20294;&#26377;&#26102;&#20197;&#26356;&#24930;&#30340;&#25910;&#25947;&#20195;&#20215;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26367;&#20195;&#31639;&#27861;&#65292;&#29992;&#24179;&#22343;&#20540;&#26367;&#25442;&#20102;&#26368;&#22823;&#21270;&#25805;&#20316;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#31181;&#21487;&#35777;&#26126;&#25910;&#25947;&#65292;&#19988;&#33021;&#32531;&#35299;&#39640;&#20272;&#38382;&#39064;&#30340;&#31163;&#32447;&#31639;&#27861;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#26631;&#20934;Q&#23398;&#20064;&#31867;&#20284;&#30340;&#25910;&#25947;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Reinforcement Learning the Q-learning algorithm provably converges to the optimal solution. However, as others have demonstrated, Q-learning can also overestimate the values and thereby spend too long exploring unhelpful states. Double Q-learning is a provably convergent alternative that mitigates some of the overestimation issues, though sometimes at the expense of slower convergence. We introduce an alternative algorithm that replaces the max operation with an average, resulting also in a provably convergent off-policy algorithm which can mitigate overestimation yet retain similar convergence as standard Q-learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20559;&#24577;&#30340;&#26143;&#31995;&#24418;&#24577;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#22495;&#36866;&#24212;&#23454;&#29616;&#20302;&#32500;&#24230;&#34920;&#31034;&#65292;40&#32500;&#28508;&#22312;&#21464;&#37327;&#33021;&#22815;&#26377;&#25928;&#20877;&#29616;&#26143;&#31995;&#22270;&#20687;&#20013;&#30340;&#22823;&#22810;&#25968;&#24418;&#24577;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#32463;&#20856;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#23454;&#29616;&#20102;&#35814;&#32454;&#30340;&#24418;&#24577;&#29305;&#24449;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26080;&#20559;&#22320;&#24212;&#29992;&#20110;&#20004;&#20010;&#19981;&#21516;&#30340;&#26143;&#31995;&#35843;&#26597;&#20013;&#12290;</title><link>http://arxiv.org/abs/2303.08627</link><description>&lt;p&gt;
&#36890;&#36807;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#22495;&#36866;&#24212;&#23454;&#29616;&#26080;&#20559;&#24577;&#24418;&#24577;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
From Images to Features: Unbiased Morphology Classification via Variational Auto-Encoders and Domain Adaptation. (arXiv:2303.08627v1 [astro-ph.GA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20559;&#24577;&#30340;&#26143;&#31995;&#24418;&#24577;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#22495;&#36866;&#24212;&#23454;&#29616;&#20302;&#32500;&#24230;&#34920;&#31034;&#65292;40&#32500;&#28508;&#22312;&#21464;&#37327;&#33021;&#22815;&#26377;&#25928;&#20877;&#29616;&#26143;&#31995;&#22270;&#20687;&#20013;&#30340;&#22823;&#22810;&#25968;&#24418;&#24577;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#32463;&#20856;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#23454;&#29616;&#20102;&#35814;&#32454;&#30340;&#24418;&#24577;&#29305;&#24449;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26080;&#20559;&#22320;&#24212;&#29992;&#20110;&#20004;&#20010;&#19981;&#21516;&#30340;&#26143;&#31995;&#35843;&#26597;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#21644;&#22495;&#36866;&#24212;&#65288;DA&#65289;&#30340;&#32452;&#21512;&#26469;&#38477;&#20302;&#26143;&#31995;&#22270;&#20687;&#30340;&#32500;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;Galaxy-Zoo DECaLS&#39033;&#30446;&#20013;&#20855;&#26377;&#35814;&#32454;&#24418;&#24577;&#31867;&#22411;&#26631;&#31614;&#30340;&#20302;&#32418;&#31227;&#26143;&#31995;&#30340;&#26679;&#26412;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;40&#32500;&#28508;&#22312;&#21464;&#37327;&#33021;&#22815;&#26377;&#25928;&#22320;&#20877;&#29616;&#26143;&#31995;&#22270;&#20687;&#20013;&#30340;&#22823;&#22810;&#25968;&#24418;&#24577;&#29305;&#24449;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#39564;&#35777;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#21033;&#29992;40&#32500;&#28508;&#22312;&#21464;&#37327;&#19978;&#30340;&#32463;&#20856;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#20998;&#31867;&#22120;&#26469;&#23454;&#29616;&#35814;&#32454;&#30340;&#24418;&#24577;&#29305;&#24449;&#20998;&#31867;&#12290;&#36825;&#31181;&#26041;&#27861;&#19982;&#30452;&#25509;&#24212;&#29992;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#25928;&#26524;&#30456;&#20284;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#20351;&#29992;DECaLS&#21644;BASS+MzLS&#37325;&#21472;&#21306;&#22495;&#20013;&#30340;&#26143;&#31995;&#26469;&#35843;&#25972;VAE&#32593;&#32476;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#26080;&#20559;&#22320;&#24212;&#29992;&#20110;&#36825;&#20004;&#20010;&#35843;&#26597;&#20013;&#30340;&#26143;&#31995;&#22270;&#20687;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;DA&#26399;&#38388;&#65292;&#22122;&#22768;&#24471;&#21040;&#20102;&#26377;&#25928;&#25233;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach for the dimensionality reduction of galaxy images by leveraging a combination of variational auto-encoders (VAE) and domain adaptation (DA). We demonstrate the effectiveness of this approach using a sample of low redshift galaxies with detailed morphological type labels from the Galaxy-Zoo DECaLS project. We show that 40-dimensional latent variables can effectively reproduce most morphological features in galaxy images. To further validate the effectiveness of our approach, we utilised a classical random forest (RF) classifier on the 40-dimensional latent variables to make detailed morphology feature classifications. This approach performs similarly to a direct neural network application on galaxy images. We further enhance our model by tuning the VAE network via DA using galaxies in the overlapping footprint of DECaLS and BASS+MzLS, enabling the unbiased application of our model to galaxy images in both surveys. We observed that noise suppression during DA 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#30697;&#24418;&#30340;&#21487;&#35299;&#37322;&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#23427;&#23558;&#22343;&#21248;&#29983;&#25104;&#30340;&#36724;&#23545;&#40784;&#36229;&#30697;&#24418;&#20316;&#20026;&#22522;&#27169;&#22411;&#65292;&#24182;&#25104;&#21151;&#22320;&#36991;&#20813;&#20102;&#36807;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2303.08625</link><description>&lt;p&gt;
&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#30340;&#36229;&#30697;&#24418;&#21487;&#35299;&#37322;&#38598;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Interpretable Ensembles of Hyper-Rectangles as Base Models. (arXiv:2303.08625v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#30697;&#24418;&#30340;&#21487;&#35299;&#37322;&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#23427;&#23558;&#22343;&#21248;&#29983;&#25104;&#30340;&#36724;&#23545;&#40784;&#36229;&#30697;&#24418;&#20316;&#20026;&#22522;&#27169;&#22411;&#65292;&#24182;&#25104;&#21151;&#22320;&#36991;&#20813;&#20102;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31616;&#21333;&#38598;&#25104;&#27169;&#22411;&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#20351;&#29992;&#22343;&#21248;&#29983;&#25104;&#30340;&#36724;&#23545;&#40784;&#36229;&#30697;&#24418;&#20316;&#20026;&#22522;&#27169;&#22411;&#65288;HRBM&#65289;&#12290;&#30740;&#31350;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;HRBM&#65306;&#23553;&#38381;&#30697;&#24418;&#21644;&#35282;&#33853;&#12290;HRBM&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#32771;&#34385;&#24182;&#35745;&#31639;&#27599;&#20010;&#30697;&#24418;&#20869;&#22806;&#30340;&#35757;&#32451;&#26679;&#20363;&#25968;&#37327;&#12290;&#25552;&#20986;&#23558;HRBM&#32435;&#20837;&#26799;&#24230;&#25552;&#21319;&#26426;&#65288;GBM&#65289;&#20013;&#12290;&#23613;&#31649;HRBM&#24456;&#31616;&#21333;&#65292;&#20294;&#36825;&#20123;&#31616;&#21333;&#30340;&#22522;&#30784;&#27169;&#22411;&#20801;&#35768;&#25105;&#20204;&#26500;&#24314;&#26377;&#25928;&#30340;&#38598;&#25104;&#27169;&#22411;&#24182;&#36991;&#20813;&#36807;&#25311;&#21512;&#12290;&#32771;&#34385;&#20102;&#19968;&#31181;&#35745;&#31639;&#38598;&#25104;&#27169;&#22411;&#30340;&#26368;&#20248;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;GBM&#30340;&#27599;&#27425;&#36845;&#20195;&#20013;&#20197;&#26174;&#24335;&#26041;&#24335;&#20462;&#25913;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#65292;&#31216;&#20026;&#8220;&#38454;&#26799;&#39640;&#24230;&#24809;&#32602;&#8221;&#65292;&#38500;&#20102;&#26631;&#20934;&#30340;L1&#21644;L2&#27491;&#21017;&#21270;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#24120;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#20351;&#29992;&#33879;&#21517;&#30340;SHAP&#26041;&#27861;&#23545;&#25152;&#25552;&#20986;&#30340;&#38598;&#25104;&#27169;&#22411;&#39044;&#27979;&#36827;&#34892;&#35299;&#37322;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;G
&lt;/p&gt;
&lt;p&gt;
A new extremely simple ensemble-based model with the uniformly generated axis-parallel hyper-rectangles as base models (HRBM) is proposed. Two types of HRBMs are studied: closed rectangles and corners. The main idea behind HRBM is to consider and count training examples inside and outside each rectangle. It is proposed to incorporate HRBMs into the gradient boosting machine (GBM). Despite simplicity of HRBMs, it turns out that these simple base models allow us to construct effective ensemble-based models and avoid overfitting. A simple method for calculating optimal regularization parameters of the ensemble-based model, which can be modified in the explicit way at each iteration of GBM, is considered. Moreover, a new regularization called the "step height penalty" is studied in addition to the standard L1 and L2 regularizations. An extremely simple approach to the proposed ensemble-based model prediction interpretation by using the well-known method SHAP is proposed. It is shown that G
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#39118;&#26684;&#36801;&#31227;&#20013;&#30340;&#38646;&#26679;&#26412;&#23545;&#27604;&#25439;&#22833;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#20855;&#26377;&#30456;&#21516;&#35821;&#20041;&#20869;&#23481;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2303.08622</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#23545;&#27604;&#25439;&#22833;&#29992;&#20110;&#25991;&#26412;&#24341;&#23548;&#25193;&#25955;&#22270;&#20687;&#39118;&#26684;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Contrastive Loss for Text-Guided Diffusion Image Style Transfer. (arXiv:2303.08622v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#39118;&#26684;&#36801;&#31227;&#20013;&#30340;&#38646;&#26679;&#26412;&#23545;&#27604;&#25439;&#22833;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#20855;&#26377;&#30456;&#21516;&#35821;&#20041;&#20869;&#23481;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#39118;&#26684;&#36801;&#31227;&#20013;&#34920;&#29616;&#20986;&#26497;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#30001;&#20110;&#20854;&#38543;&#26426;&#24615;&#32780;&#23384;&#22312;&#39118;&#26684;&#36716;&#25442;&#21644;&#20869;&#23481;&#20445;&#25252;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#35745;&#31639;&#23494;&#38598;&#30340;&#25193;&#25955;&#27169;&#22411;&#24494;&#35843;&#25110;&#38468;&#21152;&#31070;&#32463;&#32593;&#32476;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#23545;&#27604;&#25439;&#22833;&#65292;&#23427;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#24494;&#35843;&#25110;&#36741;&#21161;&#32593;&#32476;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#29983;&#25104;&#26679;&#26412;&#21644;&#21407;&#22987;&#22270;&#20687;&#23884;&#20837;&#20043;&#38388;&#30340;&#22270;&#22359;&#23545;&#27604;&#25439;&#22833;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20197;&#38646;&#26679;&#26412;&#30340;&#26041;&#24335;&#29983;&#25104;&#20855;&#26377;&#19982;&#28304;&#22270;&#20687;&#30456;&#21516;&#35821;&#20041;&#20869;&#23481;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20445;&#30041;&#20869;&#23481;&#19988;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#30340;&#21516;&#26102;&#65292;&#22312;&#22270;&#20687;&#39118;&#26684;&#36801;&#31227;&#12289;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#21644;&#25805;&#20316;&#20013;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have shown great promise in text-guided image style transfer, but there is a trade-off between style transformation and content preservation due to their stochastic nature. Existing methods require computationally expensive fine-tuning of diffusion models or additional neural network. To address this, here we propose a zero-shot contrastive loss for diffusion models that doesn't require additional fine-tuning or auxiliary networks. By leveraging patch-wise contrastive loss between generated samples and original image embeddings in the pre-trained diffusion model, our method can generate images with the same semantic content as the source image in a zero-shot manner. Our approach outperforms existing methods while preserving content and requiring no additional training, not only for image style transfer but also for image-to-image translation and manipulation. Our experimental results validate the effectiveness of our proposed method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#26679;&#26412;&#39640;&#25928;&#31639;&#27861;&#65292;&#23558; UCB &#31639;&#27861;&#65288;Auer&#31561;&#20154;&#65292;2002&#65289;&#24212;&#29992;&#20110;&#22996;&#25176;&#20195;&#29702;&#27169;&#22411;&#30340;&#22312;&#32447;&#35774;&#32622;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#36890;&#36807;&#19982;&#31574;&#30053;&#20195;&#29702;&#22810;&#27425;&#20114;&#21160;&#26469;&#35774;&#35745;&#26368;&#20248;&#30340;&#35745;&#20998;&#35268;&#21017;&#65292;&#24182;&#23454;&#29616;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.08613</link><description>&lt;p&gt;
&#23398;&#20064;&#22870;&#21169;&#20449;&#24687;&#33719;&#21462;&#65306;&#27491;&#30830;&#35745;&#20998;&#35268;&#21017;&#36935;&#21040;&#22996;&#25176;&#20195;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning to Incentivize Information Acquisition: Proper Scoring Rules Meet Principal-Agent Model. (arXiv:2303.08613v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#26679;&#26412;&#39640;&#25928;&#31639;&#27861;&#65292;&#23558; UCB &#31639;&#27861;&#65288;Auer&#31561;&#20154;&#65292;2002&#65289;&#24212;&#29992;&#20110;&#22996;&#25176;&#20195;&#29702;&#27169;&#22411;&#30340;&#22312;&#32447;&#35774;&#32622;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#36890;&#36807;&#19982;&#31574;&#30053;&#20195;&#29702;&#22810;&#27425;&#20114;&#21160;&#26469;&#35774;&#35745;&#26368;&#20248;&#30340;&#35745;&#20998;&#35268;&#21017;&#65292;&#24182;&#23454;&#29616;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22996;&#25176;&#20195;&#29702;&#27169;&#22411;&#20013;&#30340;&#28608;&#21169;&#20449;&#24687;&#33719;&#21462;&#38382;&#39064;&#12290;&#27492;&#38382;&#39064;&#34987;&#24314;&#27169;&#20026;&#22996;&#25176;&#26041;&#21644;&#20195;&#29702;&#26041;&#20043;&#38388;&#30340; Stackelberg &#21338;&#24328;&#65292;&#20854;&#20013;&#22996;&#25176;&#20154;&#23459;&#24067;&#20102;&#19968;&#26465;&#24471;&#20998;&#35268;&#21017;&#26469;&#25351;&#23450;&#20184;&#27454;&#65292;&#28982;&#21518;&#20195;&#29702;&#26041;&#36873;&#25321;&#26368;&#22823;&#21270;&#20854;&#33258;&#36523;&#21033;&#28070;&#21644;&#25253;&#21578;&#20449;&#24687;&#30340;&#21162;&#21147;&#27700;&#24179;&#12290;&#25105;&#20204;&#20174;&#22996;&#25176;&#26041;&#30340;&#35282;&#24230;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#30340;&#22312;&#32447;&#35774;&#32622;&#65292;&#21363;&#36890;&#36807;&#19982;&#31574;&#30053;&#20195;&#29702;&#22810;&#27425;&#20132;&#20114;&#26469;&#35774;&#35745;&#26368;&#20248;&#35745;&#20998;&#35268;&#21017;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#30340;&#26679;&#26412;&#39640;&#25928;&#31639;&#27861;&#65292;&#23558; UCB &#31639;&#27861; (Auer et al., 2002) &#37327;&#36523;&#23450;&#21046;&#21040;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#65292;&#20854;&#22312; T &#27425;&#36845;&#20195;&#21518;&#23454;&#29616;&#20102;&#27425;&#32447;&#24615; $T^{2/3}$-&#36951;&#25022;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20855;&#26377;&#23545;&#22996;&#25176;&#26041;&#26368;&#20248;&#21033;&#28070;&#36827;&#34892;&#31934;&#32454;&#20272;&#35745;&#30340;&#36807;&#31243;&#20197;&#21450;&#20445;&#23432;&#32416;&#27491;&#26041;&#26696;&#65292;&#20197;&#30830;&#20445;&#20195;&#29702;&#26041;&#30340;&#34892;&#21160;&#24471;&#21040;&#26377;&#25928;&#28608;&#21169;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#36951;&#25022;&#30028;&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#24449;&#26159;&#23427;&#26159;&#28176;&#36827;&#26368;&#23567;&#21487;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the incentivized information acquisition problem, where a principal hires an agent to gather information on her behalf. Such a problem is modeled as a Stackelberg game between the principal and the agent, where the principal announces a scoring rule that specifies the payment, and then the agent then chooses an effort level that maximizes her own profit and reports the information. We study the online setting of such a problem from the principal's perspective, i.e., designing the optimal scoring rule by repeatedly interacting with the strategic agent. We design a provably sample efficient algorithm that tailors the UCB algorithm (Auer et al., 2002) to our model, which achieves a sublinear $T^{2/3}$-regret after $T$ iterations. Our algorithm features a delicate estimation procedure for the optimal profit of the principal, and a conservative correction scheme that ensures the desired agent's actions are incentivized. Furthermore, a key feature of our regret bound is that it is i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#33258;&#36866;&#24212;&#12289;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32467;&#26500;&#21270;&#21098;&#26525;&#26041;&#27861;&#65292;&#20197;&#33258;&#21160;&#20135;&#29983;&#23567;&#12289;&#20934;&#30830;&#21644;&#30828;&#20214;&#26377;&#25928;&#30340;&#27169;&#22411;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2303.08595</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#27880;&#24847;&#21147;&#35009;&#21098;&#65306;&#20351;&#29992;&#27880;&#24847;&#21147;&#25913;&#36827;&#21644;&#33258;&#21160;&#21270;&#27169;&#22411;&#35009;&#21098;
&lt;/p&gt;
&lt;p&gt;
Automatic Attention Pruning: Improving and Automating Model Pruning using Attentions. (arXiv:2303.08595v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#33258;&#36866;&#24212;&#12289;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32467;&#26500;&#21270;&#21098;&#26525;&#26041;&#27861;&#65292;&#20197;&#33258;&#21160;&#20135;&#29983;&#23567;&#12289;&#20934;&#30830;&#21644;&#30828;&#20214;&#26377;&#25928;&#30340;&#27169;&#22411;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35009;&#21098;&#26159;&#19968;&#31181;&#21387;&#32553;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20197;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#23427;&#20204;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#21098;&#26525;&#35299;&#20915;&#26041;&#26696;&#22522;&#20110;&#38750;&#32467;&#26500;&#21270;&#21098;&#26525;&#65292;&#23548;&#33268;&#27169;&#22411;&#22312;&#21830;&#29992;&#30828;&#20214;&#19978;&#26080;&#27861;&#39640;&#25928;&#36816;&#34892;&#65307;&#32780;&#19988;&#23427;&#20204;&#32463;&#24120;&#38656;&#35201;&#29992;&#25143;&#25163;&#21160;&#25506;&#32034;&#21644;&#35843;&#25972;&#21098;&#26525;&#36807;&#31243;&#65292;&#36825;&#26159;&#32791;&#36153;&#26102;&#38388;&#19988;&#24120;&#24120;&#23548;&#33268;&#27425;&#20248;&#32467;&#26524;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#21160;&#21270;&#27880;&#24847;&#21147;&#35009;&#21098;&#65288;AAP&#65289;&#65292;&#19968;&#31181;&#33258;&#36866;&#24212;&#12289;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32467;&#26500;&#21270;&#21098;&#26525;&#26041;&#27861;&#65292;&#20197;&#33258;&#21160;&#20135;&#29983;&#23567;&#12289;&#20934;&#30830;&#21644;&#30828;&#20214;&#26377;&#25928;&#30340;&#27169;&#22411;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#30446;&#26631;&#12290;&#39318;&#20808;&#65292;&#23427;&#25552;&#20986;&#20102;&#20351;&#29992;&#22522;&#20110;&#28608;&#27963;&#30340;&#27880;&#24847;&#21147;&#26144;&#23556;&#36827;&#34892;&#36845;&#20195;&#32467;&#26500;&#21270;&#21098;&#26525;&#26469;&#26377;&#25928;&#22320;&#35782;&#21035;&#21644;&#20462;&#21098;&#19981;&#37325;&#35201;&#30340;&#28388;&#27874;&#22120;&#12290;&#28982;&#21518;&#65292;&#23427;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#21098;&#26525;&#31574;&#30053;&#65292;&#20197;&#33258;&#21160;&#28385;&#36275;&#31934;&#24230;&#20851;&#38190;&#12289;&#20869;&#23384;&#21463;&#38480;&#21644;&#24310;&#36831;&#25935;&#24863;&#20219;&#21153;&#30340;&#21098;&#26525;&#30446;&#26631;&#12290;&#20840;&#38754;&#30340;&#35780;&#20272;&#26174;&#31034;&#20102; AAP &#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pruning is a promising approach to compress deep learning models in order to deploy them on resource-constrained edge devices. However, many existing pruning solutions are based on unstructured pruning, which yields models that cannot efficiently run on commodity hardware; and they often require users to manually explore and tune the pruning process, which is time-consuming and often leads to sub-optimal results. To address these limitations, this paper presents Automatic Attention Pruning (AAP), an adaptive, attention-based, structured pruning approach to automatically generate small, accurate, and hardware-efficient models that meet user objectives. First, it proposes iterative structured pruning using activation-based attention maps to effectively identify and prune unimportant filters. Then, it proposes adaptive pruning policies for automatically meeting the pruning objectives of accuracy-critical, memory-constrained, and latency-sensitive tasks. A comprehensive evaluation shows th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#24310;&#36831;&#24494;&#20998;&#26041;&#31243;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;Delay-SDE-net&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#24314;&#27169;&#20855;&#26377;&#35760;&#24518;&#25928;&#24212;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#19988;&#33021;&#22815;&#23545;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#23454;&#26102;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2303.08587</link><description>&lt;p&gt;
Delay-SDE-net&#65306;&#19968;&#31181;&#29992;&#20110;&#20855;&#26377;&#35760;&#24518;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Delay-SDE-net: A deep learning approach for time series modelling with memory and uncertainty estimates. (arXiv:2303.08587v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#24310;&#36831;&#24494;&#20998;&#26041;&#31243;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;Delay-SDE-net&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#24314;&#27169;&#20855;&#26377;&#35760;&#24518;&#25928;&#24212;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#19988;&#33021;&#22815;&#23545;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#23454;&#26102;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#65292;&#20934;&#30830;&#22320;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#38750;&#24120;&#37325;&#35201;&#12290;&#30001;&#20110;&#19990;&#30028;&#36890;&#24120;&#36807;&#20110;&#22797;&#26434;&#20197;&#33267;&#26080;&#27861;&#20934;&#30830;&#22320;&#24314;&#27169;&#65292;&#22240;&#27492;&#35780;&#20272;&#21160;&#24577;&#31995;&#32479;&#22788;&#20110;&#29305;&#23450;&#29366;&#24577;&#30340;&#27010;&#29575;&#24120;&#24120;&#26377;&#24847;&#20041;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Delay-SDE-net&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#24310;&#36831;&#24494;&#20998;&#26041;&#31243;&#65288;SDDEs&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#20351;&#29992;&#20855;&#26377;&#22810;&#20010;&#24310;&#36831;&#30340;SDDE&#20316;&#20026;&#24314;&#27169;&#26694;&#26550;&#65292;&#20351;&#20854;&#25104;&#20026;&#20855;&#26377;&#35760;&#24518;&#25928;&#24212;&#30340;&#26102;&#38388;&#24207;&#21015;&#30340;&#21512;&#36866;&#27169;&#22411;&#65292;&#22240;&#20026;&#23427;&#36890;&#36807;&#31995;&#32479;&#20043;&#21069;&#30340;&#29366;&#24577;&#21253;&#25324;&#35760;&#24518;&#12290; Delay-SDE-net&#30340;&#38543;&#26426;&#37096;&#20998;&#25552;&#20379;&#20102;&#20272;&#35745;&#24314;&#27169;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#22522;&#30784;&#65292;&#24182;&#34987;&#20998;&#25104;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#20197;&#35299;&#37322;&#20808;&#39564;&#24615;&#21644;&#21518;&#39564;&#24615;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#26159;&#23454;&#26102;&#25552;&#20379;&#30340;&#65292;&#20351;&#24471;&#35813;&#27169;&#22411;&#36866;&#29992;&#20110;&#26102;&#38388;&#21294;&#20047;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;Delay-SDE-net&#30340;&#29702;&#35770;&#35823;&#24046;&#65292;&#24182;&#36827;&#34892;&#20102;&#25968;&#20540;&#25910;&#25947;&#29575;&#20998;&#26512;&#12290;&#22312;&#19982;&#31867;&#20284;&#27169;&#22411;&#30340;&#27604;&#36739;&#20013;&#65292;Delay-SDE-net&#26174;&#31034;&#20986;&#26356;&#21152;&#31283;&#23450;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
To model time series accurately is important within a wide range of fields. As the world is generally too complex to be modelled exactly, it is often meaningful to assess the probability of a dynamical system to be in a specific state. This paper presents the Delay-SDE-net, a neural network model based on stochastic delay differential equations (SDDEs). The use of SDDEs with multiple delays as modelling framework makes it a suitable model for time series with memory effects, as it includes memory through previous states of the system. The stochastic part of the Delay-SDE-net provides a basis for estimating uncertainty in modelling, and is split into two neural networks to account for aleatoric and epistemic uncertainty. The uncertainty is provided instantly, making the model suitable for applications where time is sparse. We derive the theoretical error of the Delay-SDE-net and analyze the convergence rate numerically. At comparisons with similar models, the Delay-SDE-net has consisten
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#27604;&#36739;&#20102;&#20004;&#31867;&#39640;&#32500;&#22810;&#35270;&#35282;&#32858;&#31867;&#26041;&#27861;&#65288;&#22522;&#20110;&#22270;&#21644;&#22522;&#20110;&#23376;&#31354;&#38388;&#65289;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#22914;&#20309;&#22788;&#29702;&#39640;&#38454;&#30456;&#20851;&#24615;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2303.08582</link><description>&lt;p&gt;
&#39640;&#32500;&#22810;&#35270;&#35282;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
High-dimensional multi-view clustering methods. (arXiv:2303.08582v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#27604;&#36739;&#20102;&#20004;&#31867;&#39640;&#32500;&#22810;&#35270;&#35282;&#32858;&#31867;&#26041;&#27861;&#65288;&#22522;&#20110;&#22270;&#21644;&#22522;&#20110;&#23376;&#31354;&#38388;&#65289;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#22914;&#20309;&#22788;&#29702;&#39640;&#38454;&#30456;&#20851;&#24615;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#30456;&#27604;&#20110;&#21333;&#35270;&#35282;&#32858;&#31867;&#65292;&#22810;&#35270;&#35282;&#32858;&#31867;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#25968;&#25454;&#20998;&#26512;&#20013;&#12290;&#23427;&#21487;&#20197;&#25552;&#20379;&#26356;&#22810;&#30340;&#25968;&#25454;&#20449;&#24687;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#22914;&#20309;&#32452;&#21512;&#36825;&#20123;&#35270;&#35282;&#25110;&#29305;&#24449;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#24352;&#37327;&#34920;&#31034;&#19978;&#65292;&#32780;&#19981;&#26159;&#23558;&#25968;&#25454;&#35270;&#20026;&#31616;&#21333;&#30340;&#30697;&#38453;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#25968;&#25454;&#20043;&#38388;&#30340;&#39640;&#38454;&#30456;&#20851;&#24615;&#65292;&#32780;&#22522;&#20110;&#30697;&#38453;&#30340;&#26041;&#27861;&#21017;&#38590;&#20197;&#25429;&#25417;&#36825;&#31181;&#30456;&#20851;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#30740;&#31350;&#21644;&#27604;&#36739;&#36825;&#20123;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#22270;&#30340;&#32858;&#31867;&#21644;&#23376;&#31354;&#38388;&#32858;&#31867;&#65292;&#20197;&#21450;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-view clustering has been widely used in recent years in comparison to single-view clustering, for clear reasons, as it offers more insights into the data, which has brought with it some challenges, such as how to combine these views or features. Most of recent work in this field focuses mainly on tensor representation instead of treating the data as simple matrices. This permits to deal with the high-order correlation between the data which the based matrix approach struggles to capture. Accordingly, we will examine and compare these approaches, particularly in two categories, namely graph-based clustering and subspace-based clustering. We will conduct and report experiments of the main clustering methods over a benchmark datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#37325;&#26032;&#21019;&#24314;&#19968;&#31181;&#26032;&#30340;GAN&#21464;&#20307;GANformer&#65292;&#24182;&#23545;&#20316;&#32773;&#30340;&#22768;&#26126;&#36827;&#34892;&#35780;&#35770;&#65292;&#20197;&#25506;&#31350;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.08577</link><description>&lt;p&gt;
&#25506;&#31350;GANsformer&#65306;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#22797;&#21046;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigating GANsformer: A Replication Study of a State-of-the-Art Image Generation Model. (arXiv:2303.08577v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#37325;&#26032;&#21019;&#24314;&#19968;&#31181;&#26032;&#30340;GAN&#21464;&#20307;GANformer&#65292;&#24182;&#23545;&#20316;&#32773;&#30340;&#22768;&#26126;&#36827;&#34892;&#35780;&#35770;&#65292;&#20197;&#25506;&#31350;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20170;&#24191;&#27867;&#35752;&#35770;&#30340;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#21487;&#20197;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#65292;&#20363;&#22914;&#21319;&#32423;&#29616;&#26377;&#22270;&#20687;&#65292;&#21019;&#24314;&#19981;&#23384;&#22312;&#30340;&#29289;&#20307;&#65288;&#22914;&#23460;&#20869;&#35774;&#35745;&#22330;&#26223;&#12289;&#20135;&#21697;&#29978;&#33267;&#20154;&#33080;&#65289;&#65292;&#24182;&#23454;&#29616;&#20256;&#36755;&#23398;&#20064;&#36807;&#31243;&#12290;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#30740;&#31350;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#19978;&#36848;&#30446;&#26631;&#12290;&#26412;&#25991;&#22797;&#21046;&#35780;&#20272;&#20102;&#21407;&#22987;GAN&#32593;&#32476;&#30340;&#19968;&#31181;&#26032;&#21464;&#20307;GANformer&#65292;&#35813;&#21464;&#20307;&#26159;Hudson&#21644;Zitnick&#22312;&#8220;&#29983;&#25104;&#23545;&#25239;Transformer&#8221;&#20013;&#25552;&#20986;&#30340;&#12290;&#30001;&#20110;&#36164;&#28304;&#21644;&#26102;&#38388;&#38480;&#21046;&#65292;&#25105;&#20204;&#19981;&#24471;&#19981;&#38480;&#21046;&#32593;&#32476;&#30340;&#35757;&#32451;&#26102;&#38388;&#12289;&#25968;&#25454;&#38598;&#31867;&#22411;&#21644;&#22823;&#23567;&#12290;&#30740;&#31350;&#25104;&#21151;&#22320;&#37325;&#26032;&#21019;&#24314;&#20102;&#21407;&#22987;&#32467;&#26524;&#24182;&#23545;&#20316;&#32773;&#30340;&#22768;&#26126;&#36827;&#34892;&#20102;&#35780;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of image generation through generative modelling is abundantly discussed nowadays. It can be used for various applications, such as up-scaling existing images, creating non-existing objects, such as interior design scenes, products or even human faces, and achieving transfer-learning processes. In this context, Generative Adversarial Networks (GANs) are a class of widely studied machine learning frameworks first appearing in the paper "Generative adversarial nets" by Goodfellow et al. that achieve the goal above. In our work, we reproduce and evaluate a novel variation of the original GAN network, the GANformer, proposed in "Generative Adversarial Transformers" by Hudson and Zitnick. This project aimed to recreate the methods presented in this paper to reproduce the original results and comment on the authors' claims. Due to resources and time limitations, we had to constrain the network's training times, dataset types, and sizes. Our research successfully recreated both vari
&lt;/p&gt;</description></item><item><title>WikiCoder&#26159;&#19968;&#31181;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#31243;&#24207;&#21512;&#25104;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#20174;&#36755;&#20837;&#36755;&#20986;&#20363;&#23376;&#20013;&#23398;&#20064;&#20195;&#30721;&#65292;&#29992;&#20110;&#35299;&#20915;&#38656;&#35201;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#35299;&#20915;&#20197;&#21069;&#26080;&#27861;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.08574</link><description>&lt;p&gt;
WikiCoder&#65306;&#23398;&#20064;&#32534;&#20889;&#30693;&#35782;&#22686;&#24378;&#20195;&#30721;
&lt;/p&gt;
&lt;p&gt;
WikiCoder: Learning to Write Knowledge-Powered Code. (arXiv:2303.08574v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08574
&lt;/p&gt;
&lt;p&gt;
WikiCoder&#26159;&#19968;&#31181;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#31243;&#24207;&#21512;&#25104;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#20174;&#36755;&#20837;&#36755;&#20986;&#20363;&#23376;&#20013;&#23398;&#20064;&#20195;&#30721;&#65292;&#29992;&#20110;&#35299;&#20915;&#38656;&#35201;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#35299;&#20915;&#20197;&#21069;&#26080;&#27861;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#20174;&#19968;&#20123;&#36755;&#20837;&#36755;&#20986;&#30340;&#20363;&#23376;&#20013;&#33258;&#21160;&#29983;&#25104;&#35745;&#31639;&#26426;&#31243;&#24207;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#30340;&#20986;&#21457;&#28857;&#26159;&#35266;&#23519;&#21040;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#35299;&#20915;&#26041;&#26696;&#31243;&#24207;&#24517;&#39035;&#20351;&#29992;&#36755;&#20837;&#36755;&#20986;&#20363;&#23376;&#20013;&#19981;&#23384;&#22312;&#30340;&#22806;&#37096;&#30693;&#35782;&#65306;&#25105;&#20204;&#31216;&#36825;&#26679;&#30340;&#31243;&#24207;&#20026;&#30693;&#35782;&#22686;&#24378;&#31243;&#24207;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#21442;&#32771;&#20174;&#30693;&#35782;&#22270;&#35889;&#65288;&#20363;&#22914;&#32500;&#22522;&#30334;&#31185;&#65289;&#20013;&#25910;&#38598;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#36808;&#20986;&#20102;&#21521;&#30693;&#35782;&#22686;&#24378;&#31243;&#24207;&#21512;&#25104;&#30340;&#31532;&#19968;&#27493;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; WikiCoder&#65292;&#35813;&#31995;&#32479;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31243;&#24207;&#21512;&#25104;&#22120;&#65292;&#24182;&#38598;&#25104;&#20102;&#30693;&#35782;&#22270;&#35889;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#23427;&#20197;&#23637;&#31034;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#23616;&#38480;&#24615;&#12290;WikiCoder&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#35299;&#20915;&#20102;&#20197;&#21069;&#27809;&#26377;&#31243;&#24207;&#21512;&#25104;&#22120;&#33021;&#22815;&#35299;&#20915;&#30340;&#20219;&#21153;&#65292;&#24182;&#19982;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#22823;&#35268;&#27169;&#36816;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
We tackle the problem of automatic generation of computer programs from a few pairs of input-output examples. The starting point of this work is the observation that in many applications a solution program must use external knowledge not present in the examples: we call such programs knowledge-powered since they can refer to information collected from a knowledge graph such as Wikipedia. This paper makes a first step towards knowledge-powered program synthesis. We present WikiCoder, a system building upon state of the art machine-learned program synthesizers and integrating knowledge graphs. We evaluate it to show its wide applicability over different domains and discuss its limitations. WikiCoder solves tasks that no program synthesizers were able to solve before thanks to the use of knowledge graphs, while integrating with recent developments in the field to operate at scale.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#22343;&#21248;&#36890;&#36947;&#27169;&#22411;&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21306;&#20998;&#20998;&#31867;&#25968;&#25454;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#35813;&#26041;&#27861;&#23558;&#26465;&#20214;&#27010;&#29575;&#36136;&#37327;&#20989;&#25968;&#35270;&#20026;&#31163;&#25955;&#26080;&#35760;&#24518;&#36890;&#36947;&#65292;&#24182;&#36873;&#25321;&#26368;&#21487;&#33021;&#30340;&#22240;&#26524;&#26041;&#21521;&#20351;&#24471;&#26465;&#20214;&#27010;&#29575;&#36136;&#37327;&#20989;&#25968;&#26356;&#25509;&#36817;&#20110;&#22343;&#21248;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2303.08572</link><description>&lt;p&gt;
&#21306;&#20998;&#20998;&#31867;&#25968;&#25454;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#65306;&#22343;&#21248;&#36890;&#36947;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Distinguishing Cause from Effect on Categorical Data: The Uniform Channel Model. (arXiv:2303.08572v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#22343;&#21248;&#36890;&#36947;&#27169;&#22411;&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21306;&#20998;&#20998;&#31867;&#25968;&#25454;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#35813;&#26041;&#27861;&#23558;&#26465;&#20214;&#27010;&#29575;&#36136;&#37327;&#20989;&#25968;&#35270;&#20026;&#31163;&#25955;&#26080;&#35760;&#24518;&#36890;&#36947;&#65292;&#24182;&#36873;&#25321;&#26368;&#21487;&#33021;&#30340;&#22240;&#26524;&#26041;&#21521;&#20351;&#24471;&#26465;&#20214;&#27010;&#29575;&#36136;&#37327;&#20989;&#25968;&#26356;&#25509;&#36817;&#20110;&#22343;&#21248;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21306;&#20998;&#22240;&#26524;&#20851;&#31995;&#26159;&#22240;&#26524;&#21457;&#29616;&#20013;&#30340;&#26680;&#24515;&#38382;&#39064;&#12290;&#22823;&#22810;&#25968;&#29992;&#20110;&#27492;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#21363;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#65288;ANM&#65289;&#65292;&#20165;&#36866;&#29992;&#20110;&#23450;&#37327;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#20998;&#31867;&#21464;&#37327;&#65288;&#23646;&#20110;&#27809;&#26377;&#26377;&#24847;&#20041;&#39034;&#24207;&#30340;&#38598;&#21512;&#65289;&#19978;&#35299;&#20915;&#22240;&#26524;-&#25928;&#24212;&#38382;&#39064;&#30340;&#26631;&#20934;&#65292;&#28789;&#24863;&#26469;&#28304;&#20110;&#23558;&#26465;&#20214;&#27010;&#29575;&#36136;&#37327;&#20989;&#25968;&#65288;pmf&#65289;&#35270;&#20026;&#31163;&#25955;&#26080;&#35760;&#24518;&#36890;&#36947;&#12290;&#25105;&#20204;&#36873;&#25321;&#26368;&#21487;&#33021;&#30340;&#22240;&#26524;&#26041;&#21521;&#65292;&#20854;&#20013;&#26465;&#20214;pmf&#26356;&#25509;&#36817;&#22343;&#21248;&#36890;&#36947;&#65288;UC&#65289;&#12290;&#21407;&#29702;&#26159;&#65292;&#22312;UC&#20013;&#65292;&#27491;&#22914;&#22312;ANM&#20013;&#19968;&#26679;&#65292;&#65288;&#32473;&#23450;&#22240;&#26524;&#20851;&#31995;&#30340;&#25928;&#24212;&#65289;&#26465;&#20214;&#29109;&#29420;&#31435;&#20110;&#22240;&#26524;&#20998;&#24067;&#65292;&#31526;&#21512;&#22240;&#26524;&#21644;&#26426;&#21046;&#29420;&#31435;&#24615;&#21407;&#21017;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;&#22343;&#21248;&#36890;&#36947;&#27169;&#22411;&#65288;UCM&#65289;&#65292;&#25193;&#23637;&#20102;ANM&#29702;&#35770;&#21040;&#20998;&#31867;&#21464;&#37327;&#12290;&#20026;&#20102;&#35780;&#20272;&#20174;&#25968;&#25454;&#20272;&#35745;&#30340;&#26465;&#20214;pmf&#19982;UC&#30340;&#25509;&#36817;&#31243;&#24230;&#65292;&#25105;&#20204;&#20351;&#29992;s
&lt;/p&gt;
&lt;p&gt;
Distinguishing cause from effect using observations of a pair of random variables is a core problem in causal discovery. Most approaches proposed for this task, namely additive noise models (ANM), are only adequate for quantitative data. We propose a criterion to address the cause-effect problem with categorical variables (living in sets with no meaningful order), inspired by seeing a conditional probability mass function (pmf) as a discrete memoryless channel. We select as the most likely causal direction the one in which the conditional pmf is closer to a uniform channel (UC). The rationale is that, in a UC, as in an ANM, the conditional entropy (of the effect given the cause) is independent of the cause distribution, in agreement with the principle of independence of cause and mechanism. Our approach, which we call the uniform channel model (UCM), thus extends the ANM rationale to categorical variables. To assess how close a conditional pmf (estimated from data) is to a UC, we use s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25935;&#24863;&#24230;&#24863;&#30693;&#30340;&#35270;&#35273;&#21442;&#25968;&#20302;&#25928;&#35843;&#25972;&#65288;SPT&#65289;&#26041;&#26696;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#23558;&#21487;&#35757;&#32451;&#21442;&#25968;&#20998;&#37197;&#21040;&#20219;&#21153;&#29305;&#23450;&#30340;&#37325;&#35201;&#20301;&#32622;&#65292;&#20197;&#25552;&#39640;&#34920;&#31034;&#33021;&#21147;&#65292;&#36866;&#24212;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#21040;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.08566</link><description>&lt;p&gt;
&#25935;&#24863;&#24230;&#24863;&#30693;&#30340;&#35270;&#35273;&#21442;&#25968;&#20302;&#25928;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Sensitivity-Aware Visual Parameter-Efficient Tuning. (arXiv:2303.08566v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25935;&#24863;&#24230;&#24863;&#30693;&#30340;&#35270;&#35273;&#21442;&#25968;&#20302;&#25928;&#35843;&#25972;&#65288;SPT&#65289;&#26041;&#26696;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#23558;&#21487;&#35757;&#32451;&#21442;&#25968;&#20998;&#37197;&#21040;&#20219;&#21153;&#29305;&#23450;&#30340;&#37325;&#35201;&#20301;&#32622;&#65292;&#20197;&#25552;&#39640;&#34920;&#31034;&#33021;&#21147;&#65292;&#36866;&#24212;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#21040;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21442;&#25968;&#20302;&#25928;&#35843;&#25972;&#65288;VPET&#65289;&#24050;&#25104;&#20026;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#21040;&#19979;&#28216;&#20219;&#21153;&#30340;&#24378;&#21170;&#26367;&#20195;&#26041;&#27861;&#12290;&#29616;&#26377;VPET&#26041;&#27861;&#26681;&#25454;&#20154;&#24037;&#21551;&#21457;&#24335;&#26041;&#27861;&#23558;&#21487;&#35757;&#32451;&#21442;&#25968;&#24341;&#20837;&#19981;&#21516;&#20219;&#21153;&#30340;&#30456;&#21516;&#20301;&#32622;&#65292;&#24573;&#30053;&#39046;&#22495;&#24046;&#24322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25935;&#24863;&#24230;&#24863;&#30693;&#30340;&#35270;&#35273;&#21442;&#25968;&#20302;&#25928;&#35843;&#25972;&#65288;SPT&#65289;&#26041;&#26696;&#65292;&#20197;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#20998;&#37197;&#21487;&#35757;&#32451;&#21442;&#25968;&#21040;&#20219;&#21153;&#29305;&#23450;&#30340;&#37325;&#35201;&#20301;&#32622;&#65292;&#32473;&#23450;&#25152;&#38656;&#30340;&#21487;&#35843;&#21442;&#25968;&#39044;&#31639;&#12290;&#26412;&#25991;&#39318;&#20808;&#20381;&#25454;&#25968;&#25454;&#30340;&#30456;&#20851;&#24615;&#24555;&#36895;&#35782;&#21035;&#29305;&#23450;&#20219;&#21153;&#25152;&#38656;&#35843;&#25972;&#30340;&#25935;&#24863;&#21442;&#25968;&#65292;&#28982;&#21518;&#25552;&#21319;&#34920;&#31034;&#33021;&#21147;&#65292;&#22686;&#22823;&#37325;&#35201;&#30340;&#26435;&#37325;&#30697;&#38453;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Parameter-Efficient Tuning (VPET) has become a powerful alternative for full fine-tuning so as to adapt pre-trained vision models to downstream tasks, which only tunes a small number of parameters while freezing the vast majority ones to ease storage burden and optimization difficulty. However, existing VPET methods introduce trainable parameters to the same positions across different tasks depending solely on human heuristics and neglect the domain gaps. To this end, we study where to introduce and how to allocate trainable parameters by proposing a novel Sensitivity-aware visual Parameter-efficient Tuning (SPT) scheme, which adaptively allocates trainable parameters to task-specific important positions given a desired tunable parameter budget. Specifically, our SPT first quickly identifies the sensitive parameters that require tuning for a given task in a data-dependent way. Next, our SPT further boosts the representational capability for the weight matrices whose number of se
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#23398;&#20064;&#22270;&#24418;&#65292;&#24182;&#20135;&#29983;&#20102;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#20026;&#31232;&#30095;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.08552</link><description>&lt;p&gt;
&#32852;&#21512;&#22270;&#24418;&#21644;&#39030;&#28857;&#37325;&#35201;&#24615;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Joint Graph and Vertex Importance Learning. (arXiv:2303.08552v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#23398;&#20064;&#22270;&#24418;&#65292;&#24182;&#20135;&#29983;&#20102;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#20026;&#31232;&#30095;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#19981;&#35268;&#21017;&#24863;&#30693;&#22270;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#22270;&#24418;&#23398;&#20064;&#30340;&#35805;&#39064;&#65292;&#26088;&#22312;&#23398;&#20064;&#22270;&#20449;&#21495;&#31354;&#38388;&#20869;&#31215;&#20197;&#26356;&#22909;&#22320;&#24314;&#27169;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#27604;&#32452;&#21512;&#25289;&#26222;&#25289;&#26031;&#26041;&#27861;&#20855;&#26377;&#26356;&#23567;&#30340;&#36793;&#32536;&#26435;&#37325;&#19978;&#30028;&#30340;&#22270;&#24418;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#32452;&#21512;&#25289;&#26222;&#25289;&#26031;&#26041;&#27861;&#20135;&#29983;&#26356;&#31232;&#30095;&#30340;&#22270;&#24418;&#65292;&#24182;&#20855;&#26377;&#26356;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore the topic of graph learning from the perspective of the Irregularity-Aware Graph Fourier Transform, with the goal of learning the graph signal space inner product to better model data. We propose a novel method to learn a graph with smaller edge weight upper bounds compared to combinatorial Laplacian approaches. Experimentally, our approach yields much sparser graphs compared to a combinatorial Laplacian approach, with a more interpretable model.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#26230;Zr&#24494;&#32467;&#26500;&#20013;&#32447;&#24615;&#24377;&#24615;&#24212;&#21147;&#20272;&#35745;&#30340;U-Net&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#32593;&#32476;&#30340;&#24615;&#33021;&#19981;&#21463;&#26230;&#31890;&#32467;&#26500;&#35268;&#21017;&#24615;&#25110;&#32441;&#29702;&#24433;&#21709;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#20219;&#24847;Zr&#26230;&#20307;&#32467;&#26500;&#65292;&#19982;&#26377;&#38480;&#20803;&#20998;&#26512;&#30456;&#27604;&#65292;&#20855;&#26377;&#26356;&#24555;&#30340;&#36895;&#24230;&#65288;&#22823;&#32422;200x&#33267;6000x&#65289;&#21644;&#36739;&#23567;&#30340;&#20869;&#23384;&#24320;&#38144;&#65292;&#20294;&#26368;&#39640;&#31934;&#24230;&#21487;&#33021;&#20250;&#38477;&#20302;10&#65285;&#12290;</title><link>http://arxiv.org/abs/2303.08541</link><description>&lt;p&gt;
&#22312;&#22810;&#26230;Zr&#24494;&#32467;&#26500;&#20013;&#65292;&#23558;U-Net&#29992;&#20110;&#32447;&#24615;&#24377;&#24615;&#24212;&#21147;&#20272;&#35745;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adapting U-Net for linear elastic stress estimation in polycrystal Zr microstructures. (arXiv:2303.08541v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08541
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#26230;Zr&#24494;&#32467;&#26500;&#20013;&#32447;&#24615;&#24377;&#24615;&#24212;&#21147;&#20272;&#35745;&#30340;U-Net&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#32593;&#32476;&#30340;&#24615;&#33021;&#19981;&#21463;&#26230;&#31890;&#32467;&#26500;&#35268;&#21017;&#24615;&#25110;&#32441;&#29702;&#24433;&#21709;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#20219;&#24847;Zr&#26230;&#20307;&#32467;&#26500;&#65292;&#19982;&#26377;&#38480;&#20803;&#20998;&#26512;&#30456;&#27604;&#65292;&#20855;&#26377;&#26356;&#24555;&#30340;&#36895;&#24230;&#65288;&#22823;&#32422;200x&#33267;6000x&#65289;&#21644;&#36739;&#23567;&#30340;&#20869;&#23384;&#24320;&#38144;&#65292;&#20294;&#26368;&#39640;&#31934;&#24230;&#21487;&#33021;&#20250;&#38477;&#20302;10&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;U-Net&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#21464;&#20307;&#65292;&#29992;&#20110;&#20272;&#35745;a-Zr&#65288;hcp&#65289;&#22810;&#26230;&#26230;&#31890;&#32467;&#26500;&#20013;&#30340;&#32447;&#24615;&#24377;&#24615;&#20860;&#23481;&#24212;&#21147;&#12290;&#20351;&#29992;VGrain&#36719;&#20214;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#26230;&#31890;&#32467;&#26500;&#30340;&#27491;&#21017;&#24615;&#945;&#20026;0.73&#65292;&#19988;&#20855;&#26377;&#22343;&#21248;&#38543;&#26426;&#26041;&#21521;&#65292;&#20351;&#29992;ABAQUS&#23545;&#24212;&#21147;&#28938;&#32541;&#36827;&#34892;&#26377;&#38480;&#20803;&#26041;&#27861;&#35780;&#20272;&#12290;&#21021;&#22987;&#25968;&#25454;&#38598;&#21253;&#21547;200&#20010;&#26679;&#26412;&#65292;&#20854;&#20013;20&#20010;&#29992;&#20110;&#39564;&#35777;&#30340;&#20445;&#30041;&#12290;&#19982;&#26377;&#38480;&#20803;&#20998;&#26512;&#30456;&#27604;&#65292;&#32593;&#32476;&#22312;CPU&#25110;GPU&#19978;&#21487;&#20197;&#21152;&#36895;&#32422;200&#20493;&#33267;6000&#20493;&#65292;&#24182;&#20855;&#26377;&#26174;&#33879;&#30340;&#20869;&#23384;&#33410;&#30465;&#65292;&#20934;&#30830;&#24230;&#30053;&#24494;&#38477;&#20302;&#26368;&#22810;10&#65285;&#12290;&#32593;&#32476;&#24615;&#33021;&#19982;&#26230;&#31890;&#32467;&#26500;&#30340;&#35268;&#21017;&#24615;&#25110;&#32441;&#29702;&#26080;&#20851;&#65292;&#34920;&#26126;&#32593;&#32476;&#21487;&#20197;&#25512;&#24191;&#21040;&#20219;&#24847;&#30340;Zr&#26230;&#20307;&#32467;&#26500;&#12290;&#27979;&#37327;&#20102;&#20351;&#29992;200&#21644;400&#20010;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#26102;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#24403;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#22686;&#21152;&#19968;&#20493;&#26102;&#65292;&#20934;&#30830;&#24230;&#32422;&#25552;&#39640;10&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
A variant of the U-Net convolutional neural network architecture is proposed to estimate linear elastic compatibility stresses in a-Zr (hcp) polycrystalline grain structures. Training data was generated using VGrain software with a regularity alpha of 0.73 and uniform random orientation for the grain structures and ABAQUS to evaluate the stress welds using the finite element method. The initial dataset contains 200 samples with 20 held from training for validation. The network gives speedups of around 200x to 6000x using a CPU or GPU, with signifcant memory savings, compared to finite element analysis with a modest reduction in accuracy of up to 10%. Network performance is not correlated with grain structure regularity or texture, showing generalisation of the network beyond the training set to arbitrary Zr crystal structures. Performance when trained with 200 and 400 samples was measured, finding an improvement in accuracy of approximately 10% when the size of the dataset was doubled.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Diamond Stacked&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#38598;&#25104;&#27169;&#22411;&#30340;&#20581;&#24247;&#30417;&#27979;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29305;&#24449;&#23884;&#20837;&#22534;&#21472;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#65288;FSSAE&#65289;&#36827;&#34892;&#29305;&#24449;&#25193;&#23637;&#21644;&#20351;&#29992;Diamond&#23618;&#32467;&#26500;&#21435;&#38500;&#20887;&#20313;&#21644;&#38750;&#20449;&#24687;&#29305;&#24449;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#36816;&#21160;&#38556;&#30861;&#24739;&#32773;&#30340;&#35786;&#26029;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.08538</link><description>&lt;p&gt;
&#22522;&#20110;Diamond Stacked&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#38598;&#25104;&#27169;&#22411;&#30340;&#36816;&#21160;&#38556;&#30861;&#24739;&#32773;&#30340;&#20581;&#24247;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
Health Monitoring of Movement Disorder Subject based on Diamond Stacked Sparse Autoencoder Ensemble Model. (arXiv:2303.08538v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Diamond Stacked&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#38598;&#25104;&#27169;&#22411;&#30340;&#20581;&#24247;&#30417;&#27979;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29305;&#24449;&#23884;&#20837;&#22534;&#21472;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#65288;FSSAE&#65289;&#36827;&#34892;&#29305;&#24449;&#25193;&#23637;&#21644;&#20351;&#29992;Diamond&#23618;&#32467;&#26500;&#21435;&#38500;&#20887;&#20313;&#21644;&#38750;&#20449;&#24687;&#29305;&#24449;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#36816;&#21160;&#38556;&#30861;&#24739;&#32773;&#30340;&#35786;&#26029;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#38556;&#30861;&#24739;&#32773;&#30340;&#20581;&#24247;&#30417;&#27979;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#20182;&#20204;&#30340;&#27963;&#21160;&#33021;&#21147;&#26377;&#38480;&#65292;&#24930;&#24615;&#30149;&#30340;&#25345;&#32493;&#26102;&#38388;&#24456;&#38271;&#12290;&#30446;&#21069;&#65292;&#21033;&#29992;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25910;&#38598;&#36816;&#21160;&#38556;&#30861;&#24739;&#32773;&#30340;&#25968;&#25454;&#24182;&#36827;&#34892;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22788;&#29702;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#20581;&#24247;&#30417;&#27979;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#31995;&#32479;&#38590;&#20197;&#33719;&#24471;&#39640;&#36136;&#37327;&#21644;&#22823;&#37327;&#25968;&#25454;&#65292;&#26080;&#27861;&#28385;&#36275;&#35786;&#26029;&#31934;&#24230;&#30340;&#35201;&#27714;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#19981;&#33021;&#24456;&#22909;&#22320;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Diamond Stacked&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#38598;&#25104;&#27169;&#22411;&#65288;DsaeEM&#65289;&#30340;&#36816;&#21160;&#38556;&#30861;&#24739;&#32773;&#20581;&#24247;&#30417;&#27979;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#26377;&#20004;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#29305;&#24449;&#23884;&#20837;&#22534;&#21472;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#65288;FSSAE&#65289;&#35774;&#35745;&#29305;&#24449;&#25193;&#23637;&#12290;&#20854;&#27425;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#29305;&#24449;&#36824;&#21407;&#26426;&#21046;&#65292;&#20351;&#29992;Diamond&#23618;&#32467;&#26500;&#21435;&#38500;&#20887;&#20313;&#21644;&#38750;&#20449;&#24687;&#29305;&#24449;&#12290;&#35813;&#27169;&#22411;&#22312;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#33719;&#24471;&#30340;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#35786;&#26029;&#31934;&#24230;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
The health monitoring of chronic diseases is very important for people with movement disorders because of their limited mobility and long duration of chronic diseases. Machine learning-based processing of data collected from the human with movement disorders using wearable sensors is an effective method currently available for health monitoring. However, wearable sensor systems are difficult to obtain high-quality and large amounts of data, which cannot meet the requirement for diagnostic accuracy. Moreover, existing machine learning methods do not handle this problem well. Feature learning is key to machine learning. To solve this problem, a health monitoring of movement disorder subject based on diamond stacked sparse autoencoder ensemble model (DsaeEM) is proposed in this paper. This algorithm has two major components. First, feature expansion is designed using feature-embedded stacked sparse autoencoder (FSSAE). Second, a feature reduction mechanism is designed to remove the redund
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38899;&#35270;&#39057;&#35821;&#38899;&#35782;&#21035;&#22312;&#22810;&#27169;&#24577;&#36755;&#20837;&#25439;&#22351;&#24773;&#20917;&#19979;&#30340;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#38899;&#35270;&#39057;&#21487;&#38752;&#24615;&#35780;&#20998;&#27169;&#22359;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#38887;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.08536</link><description>&lt;p&gt;
&#35266;&#30475;&#25110;&#21548;&#21462;&#65306;&#20855;&#26377;&#35270;&#35273;&#25439;&#22351;&#24314;&#27169;&#21644;&#21487;&#38752;&#24615;&#35780;&#20998;&#30340;&#24378;&#38887;&#38899;&#35270;&#39057;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Watch or Listen: Robust Audio-Visual Speech Recognition with Visual Corruption Modeling and Reliability Scoring. (arXiv:2303.08536v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38899;&#35270;&#39057;&#35821;&#38899;&#35782;&#21035;&#22312;&#22810;&#27169;&#24577;&#36755;&#20837;&#25439;&#22351;&#24773;&#20917;&#19979;&#30340;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#38899;&#35270;&#39057;&#21487;&#38752;&#24615;&#35780;&#20998;&#27169;&#22359;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#38899;&#35270;&#39057;&#35821;&#38899;&#35782;&#21035;&#65288;AVSR&#65289;&#22312;&#22810;&#27169;&#24577;&#36755;&#20837;&#25439;&#22351;&#24773;&#20917;&#19979;&#36827;&#34892;&#30740;&#31350;&#65292;&#20854;&#20013;&#38899;&#39057;&#36755;&#20837;&#21644;&#35270;&#35273;&#36755;&#20837;&#22343;&#21463;&#25439;&#65292;&#36825;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#26041;&#21521;&#20013;&#27809;&#26377;&#24471;&#21040;&#24456;&#22909;&#30340;&#35299;&#20915;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#22914;&#20309;&#29992;&#28165;&#26224;&#30340;&#35270;&#35273;&#36755;&#20837;&#26469;&#34917;&#20805;&#21463;&#25439;&#30340;&#38899;&#39057;&#36755;&#20837;&#65292;&#20551;&#35774;&#21487;&#29992;&#28165;&#26224;&#30340;&#35270;&#35273;&#36755;&#20837;&#65292;&#20294;&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#65292;&#28165;&#26224;&#30340;&#35270;&#35273;&#36755;&#20837;&#24182;&#19981;&#24635;&#26159;&#21487;&#29992;&#30340;&#65292;&#29978;&#33267;&#21487;&#33021;&#34987;&#36974;&#25377;&#30340;&#21767;&#37096;&#21306;&#22495;&#25110;&#22122;&#38899;&#25152;&#25439;&#22351;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper deals with Audio-Visual Speech Recognition (AVSR) under multimodal input corruption situations where audio inputs and visual inputs are both corrupted, which is not well addressed in previous research directions. Previous studies have focused on how to complement the corrupted audio inputs with the clean visual inputs with the assumption of the availability of clean visual inputs. However, in real life, clean visual inputs are not always accessible and can even be corrupted by occluded lip regions or noises. Thus, we firstly analyze that the previous AVSR models are not indeed robust to the corruption of multimodal input streams, the audio and the visual inputs, compared to uni-modal models. Then, we design multimodal input corruption modeling to develop robust AVSR models. Lastly, we propose a novel AVSR framework, namely Audio-Visual Reliability Scoring module (AV-RelScore), that is robust to the corrupted multimodal inputs. The AV-RelScore can determine which input modal 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#20010;&#22312;&#30418;&#23376;&#20013;&#31227;&#21160;&#30340;&#31890;&#23376;&#30340;&#33945;&#29305;&#21345;&#32599;&#31639;&#27861;&#30340;&#24347;&#35947;&#26412;&#24449;&#27169;&#24335;&#65292;&#21457;&#29616;&#24403;&#36339;&#36291;&#38271;&#24230;&#19982;&#30418;&#23376;&#22823;&#23567;&#30456;&#24403;&#26102;&#65292;&#24347;&#35947;&#26412;&#24449;&#27169;&#24335;&#25968;&#37327;&#21487;&#33021;&#38750;&#24120;&#23569;&#65292;&#21021;&#22987;&#26465;&#20214;&#30340;&#23545;&#31216;&#24615;&#30340;&#21512;&#36866;&#36873;&#25321;&#21487;&#20197;&#20351;&#24471;&#21521;&#24179;&#34913;&#29366;&#24577;&#30340;&#23616;&#37096;&#21270;&#34928;&#20943;&#12290;</title><link>http://arxiv.org/abs/2303.08535</link><description>&lt;p&gt;
&#24102;&#26377; Metropolis Monte Carlo &#21160;&#21147;&#23398;&#30340;&#30418;&#23376;&#20013;&#38543;&#26426;&#34892;&#36208;&#30340;&#22855;&#24322;&#24347;&#35947;
&lt;/p&gt;
&lt;p&gt;
Singular relaxation of a random walk in a box with a Metropolis Monte Carlo dynamics. (arXiv:2303.08535v1 [cond-mat.stat-mech])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#20010;&#22312;&#30418;&#23376;&#20013;&#31227;&#21160;&#30340;&#31890;&#23376;&#30340;&#33945;&#29305;&#21345;&#32599;&#31639;&#27861;&#30340;&#24347;&#35947;&#26412;&#24449;&#27169;&#24335;&#65292;&#21457;&#29616;&#24403;&#36339;&#36291;&#38271;&#24230;&#19982;&#30418;&#23376;&#22823;&#23567;&#30456;&#24403;&#26102;&#65292;&#24347;&#35947;&#26412;&#24449;&#27169;&#24335;&#25968;&#37327;&#21487;&#33021;&#38750;&#24120;&#23569;&#65292;&#21021;&#22987;&#26465;&#20214;&#30340;&#23545;&#31216;&#24615;&#30340;&#21512;&#36866;&#36873;&#25321;&#21487;&#20197;&#20351;&#24471;&#21521;&#24179;&#34913;&#29366;&#24577;&#30340;&#23616;&#37096;&#21270;&#34928;&#20943;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#19968;&#20010;&#31616;&#21333;&#33945;&#29305;&#21345;&#32599;&#31639;&#27861;&#30340;&#24347;&#35947;&#26412;&#24449;&#27169;&#24335;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#35813;&#31639;&#27861;&#23545;&#24212;&#20110;&#19968;&#20010;&#22312;&#30418;&#23376;&#20013;&#31227;&#21160;&#30340;&#31890;&#23376;&#65292;&#20854;&#36890;&#36807;&#22343;&#21248;&#38543;&#26426;&#36339;&#36291;&#36827;&#34892;&#36816;&#21160;&#12290;&#30418;&#23376;&#22806;&#30340;&#36339;&#36291;&#34987;&#25298;&#32477;&#12290;&#38271;&#26102;&#38388;&#21518;&#65292;&#31995;&#32479;&#25509;&#36817;&#24179;&#34913;&#27010;&#29575;&#23494;&#24230;&#65292;&#22312;&#30418;&#23376;&#20869;&#37096;&#26159;&#22343;&#21248;&#30340;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#31181;&#24179;&#34913;&#29366;&#24577;&#19979;&#30340;&#24347;&#35947;&#26159;&#19981;&#23547;&#24120;&#30340;&#65306;&#23545;&#20110;&#36339;&#36291;&#38271;&#24230;&#19982;&#30418;&#23376;&#22823;&#23567;&#30456;&#24403;&#30340;&#24773;&#20917;&#65292;&#24347;&#35947;&#26412;&#24449;&#27169;&#24335;&#30340;&#25968;&#37327;&#21487;&#33021;&#38750;&#24120;&#23569;&#65292;&#20026;&#19968;&#25110;&#20004;&#20010;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23436;&#25972;&#30340;&#20998;&#26512;&#24615;&#25551;&#36848;&#65292;&#26469;&#25551;&#36848;&#20004;&#31181;&#24773;&#20917;&#20043;&#38388;&#30340;&#36716;&#21464;&#12290;&#24403;&#21482;&#23384;&#22312;&#21333;&#20010;&#24347;&#35947;&#26412;&#24449;&#27169;&#24335;&#26102;&#65292;&#21021;&#22987;&#26465;&#20214;&#30340;&#23545;&#31216;&#24615;&#30340;&#21512;&#36866;&#36873;&#25321;&#20351;&#24471;&#21521;&#24179;&#34913;&#29366;&#24577;&#30340;&#23616;&#37096;&#21270;&#34928;&#20943;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20559;&#31163;&#24179;&#34913;&#30340;&#38598;&#20013;&#21040;&#30418;&#23376;&#36793;&#32536;&#65292;&#36825;&#37324;&#25298;&#32477;&#27010;&#29575;&#26368;&#22823;&#12290;&#26368;&#21518;&#65292;&#38500;&#20102;&#20027;&#26041;&#31243;&#30340;&#26494;&#24347;&#20998;&#26512;&#22806;&#65292;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#20027;&#26041;&#31243;&#30340;&#23436;&#25972;&#26412;&#24449;&#35889;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study analytically the relaxation eigenmodes of a simple Monte Carlo algorithm, corresponding to a particle in a box which moves by uniform random jumps. Moves outside of the box are rejected. At long times, the system approaches the equilibrium probability density, which is uniform inside the box. We show that the relaxation towards this equilibrium is unusual: for a jump length comparable to the size of the box, the number of relaxation eigenmodes can be surprisingly small, one or two. We provide a complete analytic description of the transition between these two regimes. When only a single relaxation eigenmode is present, a suitable choice of the symmetry of the initial conditions gives a localizing decay to equilibrium. In this case, the deviation from equilibrium concentrates at the edges of the box where the rejection probability is maximal. Finally, in addition to the relaxation analysis of the master equation, we also describe the full eigen-spectrum of the master equation i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#38750;&#33258;&#21161;&#23398;&#20064;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24179;&#38750;&#33258;&#21161;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#20559;&#35265;&#21487;&#33021;&#23384;&#22312;&#30340;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#20915;&#31574;&#35268;&#21017;&#12290;</title><link>http://arxiv.org/abs/2303.08516</link><description>&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23454;&#29616;&#20844;&#24179;&#30340;&#38750;&#33258;&#21161;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fair Off-Policy Learning from Observational Data. (arXiv:2303.08516v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#38750;&#33258;&#21161;&#23398;&#20064;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24179;&#38750;&#33258;&#21161;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#20559;&#35265;&#21487;&#33021;&#23384;&#22312;&#30340;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#20915;&#31574;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20225;&#19994;&#21644;&#32452;&#32455;&#24517;&#39035;&#30830;&#20445;&#20854;&#31639;&#27861;&#20915;&#31574;&#30340;&#20844;&#24179;&#24615;&#65292;&#20197;&#28385;&#36275;&#31435;&#27861;&#12289;&#36947;&#24503;&#21644;&#31038;&#20250;&#35201;&#27714;&#12290;&#26412;&#25991;&#38024;&#23545;&#38750;&#33258;&#21161;&#23398;&#20064;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24179;&#38750;&#33258;&#21161;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#20915;&#31574;&#35268;&#21017;&#65292;&#20197;&#19981;&#21516;&#30340;&#20844;&#24179;&#27010;&#24565;&#26126;&#30830;&#20551;&#23450;&#35266;&#27979;&#25968;&#25454;&#26159;&#22312;&#19981;&#21516;&#65288;&#28508;&#22312;&#20559;&#35265;&#30340;&#65289;&#34892;&#20026;&#31574;&#30053;&#19979;&#25910;&#38598;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Businesses and organizations must ensure that their algorithmic decision-making is fair in order to meet legislative, ethical, and societal demands. For example, decision-making in automated hiring must not discriminate with respect to gender or race. To achieve this, prior research has contributed approaches that ensure algorithmic fairness in machine learning predictions, while comparatively little effort has focused on algorithmic fairness in decision models, specifically off-policy learning. In this paper, we propose a novel framework for fair off-policy learning: we learn decision rules from observational data under different notions of fairness, where we explicitly assume that observational data were collected under a different -- potentially biased -- behavioral policy. For this, we first formalize different fairness notions for off-policy learning. We then propose a machine learning approach to learn optimal policies under these fairness notions. Specifically, we reformulate th
&lt;/p&gt;</description></item><item><title>&#20445;&#25252;&#20010;&#20154;&#38544;&#31169;&#20449;&#24687;&#26159;&#24456;&#37325;&#35201;&#30340;&#65292;&#20294;&#35268;&#36991;&#25193;&#25955;&#27169;&#22411;&#20013;&#28155;&#21152;&#30340;&#22122;&#22768;&#23545;&#25968;&#25454;&#36827;&#34892;&#20445;&#25252;&#23384;&#22312;&#25361;&#25112;&#12290;AVATAR&#31639;&#27861;&#20511;&#21161;&#25193;&#25955;&#27169;&#22411;&#30340;&#23041;&#21147;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#31934;&#24515;&#35774;&#35745;&#30340;&#21435;&#22122;&#36807;&#31243;&#26469;&#28040;&#38500;&#25968;&#25454;&#20445;&#25252;&#25200;&#21160;&#30340;&#24433;&#21709;&#65292;&#24182;&#33719;&#24471;&#20102;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.08500</link><description>&lt;p&gt;
&#35268;&#36991;&#25193;&#25955;&#27169;&#22411;&#20013;&#28155;&#21152;&#30340;&#22122;&#22768;&#23545;&#25968;&#25454;&#36827;&#34892;&#20445;&#25252;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
The Devil's Advocate: Shattering the Illusion of Unexploitable Data using Diffusion Models. (arXiv:2303.08500v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08500
&lt;/p&gt;
&lt;p&gt;
&#20445;&#25252;&#20010;&#20154;&#38544;&#31169;&#20449;&#24687;&#26159;&#24456;&#37325;&#35201;&#30340;&#65292;&#20294;&#35268;&#36991;&#25193;&#25955;&#27169;&#22411;&#20013;&#28155;&#21152;&#30340;&#22122;&#22768;&#23545;&#25968;&#25454;&#36827;&#34892;&#20445;&#25252;&#23384;&#22312;&#25361;&#25112;&#12290;AVATAR&#31639;&#27861;&#20511;&#21161;&#25193;&#25955;&#27169;&#22411;&#30340;&#23041;&#21147;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#31934;&#24515;&#35774;&#35745;&#30340;&#21435;&#22122;&#36807;&#31243;&#26469;&#28040;&#38500;&#25968;&#25454;&#20445;&#25252;&#25200;&#21160;&#30340;&#24433;&#21709;&#65292;&#24182;&#33719;&#24471;&#20102;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#25252;&#20010;&#20154;&#25968;&#25454;&#20813;&#21463;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21033;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#21487;&#29992;&#24615;&#25915;&#20987;&#23637;&#29616;&#20986;&#25552;&#20379;&#39069;&#22806;&#20445;&#25252;&#25514;&#26045;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20197;&#38450;&#27490;&#26410;&#32463;&#25480;&#26435;&#22320;&#20351;&#29992;&#25968;&#25454;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#20123;&#26041;&#27861;&#26088;&#22312;&#21521;&#24178;&#20928;&#25968;&#25454;&#28155;&#21152;&#38590;&#20197;&#23519;&#35273;&#30340;&#22122;&#22768;&#65292;&#20351;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#20174;&#21463;&#20445;&#25252;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#27169;&#24335;&#65292;&#22768;&#31216;&#21487;&#20197;&#20351;&#20010;&#20154;&#25968;&#25454;&#8220;&#26080;&#27861;&#21033;&#29992;&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#23545;&#25239;&#25514;&#26045;&#65292;&#34920;&#26126;&#19981;&#21487;&#21033;&#29992;&#30340;&#25968;&#25454;&#21487;&#33021;&#21482;&#26159;&#19968;&#31181;&#24187;&#35273;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#23041;&#21147;&#65292;&#24182;&#23637;&#31034;&#31934;&#24515;&#35774;&#35745;&#30340;&#21435;&#22122;&#36807;&#31243;&#21487;&#20197;&#28040;&#38500;&#25968;&#25454;&#20445;&#25252;&#25200;&#21160;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20005;&#35880;&#22320;&#20998;&#26512;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25152;&#38656;&#21435;&#22122;&#30340;&#37327;&#30452;&#25509;&#19982;&#25968;&#25454;&#20445;&#25252;&#25200;&#21160;&#30340;&#25968;&#37327;&#25104;&#27491;&#27604;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;AVATAR&#65292;&#22312;&#21253;&#25324;CelebA&#25968;&#25454;&#38598;&#22312;&#20869;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#23427;&#20197;&#24040;&#22823;&#30340;&#20248;&#21183;&#32988;&#20986;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protecting personal data against the exploitation of machine learning models is of paramount importance. Recently, availability attacks have shown great promise to provide an extra layer of protection against the unauthorized use of data to train neural networks. These methods aim to add imperceptible noise to clean data so that the neural networks cannot extract meaningful patterns from the protected data, claiming that they can make personal data "unexploitable." In this paper, we provide a strong countermeasure against such approaches, showing that unexploitable data might only be an illusion. In particular, we leverage the power of diffusion models and show that a carefully designed denoising process can defuse the ramifications of the data-protecting perturbations. We rigorously analyze our algorithm, and theoretically prove that the amount of required denoising is directly related to the magnitude of the data-protecting perturbations. Our approach, called AVATAR, delivers state-o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39046;&#22495;&#19981;&#21464;&#22330;&#26223;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21512;&#25104;&#22330;&#26223;&#22270;&#30452;&#25509;&#21512;&#25104;&#36924;&#30495;&#30340;&#20132;&#36890;&#22330;&#26223;&#65292;&#25552;&#39640;&#20102;&#21512;&#25104;&#22330;&#26223;&#22270;&#30340;&#31354;&#38388;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#22330;&#26223;&#25805;&#20316;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.08473</link><description>&lt;p&gt;
&#29992;&#21512;&#25104;&#30340;3D&#22330;&#26223;&#22270;&#29983;&#25104;&#26080;&#20154;&#39550;&#39542;&#20132;&#36890;&#22330;&#26223;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Traffic Scene Generation with Synthetic 3D Scene Graphs. (arXiv:2303.08473v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39046;&#22495;&#19981;&#21464;&#22330;&#26223;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21512;&#25104;&#22330;&#26223;&#22270;&#30452;&#25509;&#21512;&#25104;&#36924;&#30495;&#30340;&#20132;&#36890;&#22330;&#26223;&#65292;&#25552;&#39640;&#20102;&#21512;&#25104;&#22330;&#26223;&#22270;&#30340;&#31354;&#38388;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#22330;&#26223;&#25805;&#20316;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#39537;&#21160;&#30340;&#22270;&#20687;&#21512;&#25104;&#24050;&#32463;&#36798;&#21040;&#20102;&#26174;&#33879;&#30340;&#36924;&#30495;&#24230;&#65292;&#28982;&#32780;&#36825;&#31181;&#26041;&#24335;&#29983;&#25104;&#30340;&#21512;&#25104;&#22270;&#20687;&#19982;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#20043;&#38388;&#23384;&#22312;&#30528;&#26174;&#33879;&#30340;&#39046;&#22495;&#24046;&#36317;&#12290;&#36825;&#22312;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#20013;&#23588;&#20026;&#26126;&#26174;&#65292;&#32780;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#21448;&#26159;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26368;&#20026;&#20851;&#38190;&#30340;&#26041;&#38754;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39046;&#22495;&#19981;&#21464;&#22330;&#26223;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#30452;&#25509;&#21512;&#25104;&#20132;&#36890;&#22330;&#26223;&#22270;&#20687;&#32780;&#19981;&#36827;&#34892;&#28210;&#26579;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20381;&#36182;&#20110;&#21512;&#25104;&#22330;&#26223;&#22270;&#20316;&#20026;&#25105;&#20204;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26469;&#23454;&#29616;&#36924;&#30495;&#30340;&#20132;&#36890;&#22330;&#26223;&#21512;&#25104;&#12290;&#25105;&#20204;&#21033;&#29992;&#31354;&#38388;&#22330;&#26223;&#20449;&#24687;&#22686;&#24378;&#20102;&#21512;&#25104;&#22330;&#26223;&#22270;&#65292;&#24182;&#36890;&#36807;&#22330;&#26223;&#25805;&#20316;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image synthesis driven by computer graphics achieved recently a remarkable realism, yet synthetic image data generated this way reveals a significant domain gap with respect to real-world data. This is especially true in autonomous driving scenarios, which represent a critical aspect for overcoming utilizing synthetic data for training neural networks. We propose a method based on domain-invariant scene representation to directly synthesize traffic scene imagery without rendering. Specifically, we rely on synthetic scene graphs as our internal representation and introduce an unsupervised neural network architecture for realistic traffic scene synthesis. We enhance synthetic scene graphs with spatial information about the scene and demonstrate the effectiveness of our approach through scene manipulation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#20809;&#20239;&#30005;&#27744;&#32452;&#28151;&#21512;&#29289;&#29702;&#27010;&#29575;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#25968;&#20540;&#22825;&#27668;&#39044;&#27979;&#32467;&#26524;&#20316;&#20026;&#21327;&#21464;&#37327;&#65292;&#25913;&#21892;&#20102;&#20809;&#20239;&#31995;&#32479;&#21151;&#29575;&#36755;&#20986;&#30340;&#20934;&#30830;&#24615;&#65292;&#26368;&#32456;&#21487;&#20197;&#36798;&#21040;7.54&#65285;&#30340;&#25216;&#33021;&#35780;&#20998;&#12290;</title><link>http://arxiv.org/abs/2303.08459</link><description>&lt;p&gt;
&#22522;&#20110;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#20809;&#20239;&#30005;&#27744;&#32452;&#30340;&#28151;&#21512;&#29289;&#29702;&#27010;&#29575;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Hybrid-Physical Probabilistic Forecasting for a Set of Photovoltaic Systems using Recurrent Neural Networks. (arXiv:2303.08459v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#20809;&#20239;&#30005;&#27744;&#32452;&#28151;&#21512;&#29289;&#29702;&#27010;&#29575;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#25968;&#20540;&#22825;&#27668;&#39044;&#27979;&#32467;&#26524;&#20316;&#20026;&#21327;&#21464;&#37327;&#65292;&#25913;&#21892;&#20102;&#20809;&#20239;&#31995;&#32479;&#21151;&#29575;&#36755;&#20986;&#30340;&#20934;&#30830;&#24615;&#65292;&#26368;&#32456;&#21487;&#20197;&#36798;&#21040;7.54&#65285;&#30340;&#25216;&#33021;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#20809;&#20239;&#31995;&#32479;&#30340;&#21151;&#29575;&#36755;&#20986;&#23545;&#20110;&#25913;&#21892;&#33021;&#28304;&#20998;&#24067;&#32593;&#32476;&#30340;&#36816;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;-&#29289;&#29702;&#27169;&#22411;&#65292;&#22312;&#25968;&#20540;&#22825;&#27668;&#39044;&#27979;&#30340;&#24110;&#21161;&#19979;&#65292;&#36890;&#36807;&#20351;&#29992;&#20854;&#20316;&#20026;&#21327;&#21464;&#37327;&#30340;PV&#24615;&#33021;&#27169;&#22411;&#21644;&#33258;&#22238;&#24402;&#36882;&#24402;&#31070;&#32463;&#27169;&#22411;&#26469;&#25913;&#36827;&#30830;&#23450;&#24615;&#30340;&#30701;&#26102;&#39044;&#27979;&#12290;&#25105;&#20204;&#37325;&#26032;&#35774;&#35745;&#20102;&#26368;&#21021;&#29992;&#20110;&#38646;&#21806;&#39046;&#22495;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#25581;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#25130;&#26029;&#39640;&#26031;&#36755;&#20986;&#20998;&#24067;&#12290;&#25105;&#20204;&#23558;&#35768;&#22810;&#27169;&#22411;&#21464;&#37327;&#19982;&#25991;&#29486;&#20013;&#30340;&#26367;&#20195;&#26041;&#26696;&#36827;&#34892;&#20102;&#23454;&#39564;&#27604;&#36739;&#65292;&#24182;&#19988;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#26368;&#20339;&#24615;&#33021;&#21464;&#20307;&#20013;&#30340;&#32452;&#20214;&#21327;&#21516;&#24037;&#20316;&#20197;&#36798;&#21040;&#19982;NWP&#39537;&#21160;&#30340;PV&#24615;&#33021;&#27169;&#22411;&#22522;&#32447;&#30456;&#27604;&#30340;&#25216;&#33021;&#35780;&#20998;&#20026;7.54&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate intra-day forecasts of the power output by PhotoVoltaic (PV) systems are critical to improve the operation of energy distribution grids. We describe a hybrid-physical model, which aims at improving deterministic intra-day forecasts, issued by a PV performance model fed by Numerical Weather Predictions (NWP), by using them as covariates in the context of an autoregressive recurrent neural model. Our proposal repurposes a neural model initially used in the retail sector, and discloses a novel truncated Gaussian output distribution. We experimentally compare many model variants to alternatives from the literature, and an ablation study shows that the components in the best performing variant work synergistically to reach a skill score of 7.54% with respect to the NWP-driven PV performance model baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#25968;&#25454;&#19981;&#21487;&#36991;&#20813;&#24102;&#26377;&#22122;&#22768;&#30340;&#24773;&#20917;&#65292;&#30740;&#31350;&#20102;DEPINN&#22312;&#27714;&#35299;&#20013;&#23376;&#25193;&#25955;&#26412;&#24449;&#20540;&#38382;&#39064;&#26041;&#38754;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#21019;&#26032;&#30340;&#21306;&#38388;&#25439;&#22833;&#20989;&#25968;&#29992;&#20110;&#20943;&#23569;&#22122;&#22768;&#24433;&#21709;&#21644;&#25552;&#39640;&#20808;&#39564;&#25968;&#25454;&#21033;&#29992;&#29575;&#65292;&#27492;&#26041;&#27861;&#22312;&#20004;&#20010;&#22522;&#20934;&#38382;&#39064;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2303.08455</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#20013;&#23376;&#25193;&#25955;&#26412;&#24449;&#20540;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
On the uncertainty analysis of the data-enabled physics-informed neural network for solving neutron diffusion eigenvalue problem. (arXiv:2303.08455v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#25968;&#25454;&#19981;&#21487;&#36991;&#20813;&#24102;&#26377;&#22122;&#22768;&#30340;&#24773;&#20917;&#65292;&#30740;&#31350;&#20102;DEPINN&#22312;&#27714;&#35299;&#20013;&#23376;&#25193;&#25955;&#26412;&#24449;&#20540;&#38382;&#39064;&#26041;&#38754;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#21019;&#26032;&#30340;&#21306;&#38388;&#25439;&#22833;&#20989;&#25968;&#29992;&#20110;&#20943;&#23569;&#22122;&#22768;&#24433;&#21709;&#21644;&#25552;&#39640;&#20808;&#39564;&#25968;&#25454;&#21033;&#29992;&#29575;&#65292;&#27492;&#26041;&#27861;&#22312;&#20004;&#20010;&#22522;&#20934;&#38382;&#39064;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24037;&#31243;&#23454;&#39564;&#20013;&#65292;&#36890;&#36807;&#25506;&#27979;&#22120;&#33719;&#24471;&#30340;&#25968;&#25454;&#19981;&#21487;&#36991;&#20813;&#22320;&#24102;&#26377;&#22122;&#22768;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24403;&#20808;&#39564;&#25968;&#25454;&#21253;&#21547;&#19981;&#21516;&#31867;&#22411;&#22122;&#22768;&#26102;&#65292;&#24050;&#32463;&#25552;&#20986;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#65288;DEPINN&#65289;&#22312;&#35745;&#31639;&#20013;&#23376;&#25193;&#25955;&#26412;&#24449;&#20540;&#38382;&#39064;&#26102;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20943;&#23569;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#22122;&#22768;&#20808;&#39564;&#25968;&#25454;&#30340;&#21033;&#29992;&#29575;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#21019;&#26032;&#30340;&#21306;&#38388;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#20123;&#20005;&#26684;&#30340;&#25968;&#23398;&#35777;&#26126;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#20540;&#32467;&#26524;&#65292;&#26412;&#25991;&#22312;&#20004;&#20010;&#20856;&#22411;&#30340;&#22522;&#20934;&#38382;&#39064;&#19978;&#26816;&#39564;&#20102;DEPINN&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#21306;&#38388;&#25439;&#22833;&#20989;&#25968;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#25991;&#30830;&#35748;&#20102;&#25913;&#36827;&#30340;DEPINN&#22312;&#26680;&#21453;&#24212;&#22534;&#29289;&#29702;&#23454;&#38469;&#24037;&#31243;&#24212;&#29992;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In practical engineering experiments, the data obtained through detectors are inevitably noisy. For the already proposed data-enabled physics-informed neural network (DEPINN) \citep{DEPINN}, we investigate the performance of DEPINN in calculating the neutron diffusion eigenvalue problem from several perspectives when the prior data contain different scales of noise. Further, in order to reduce the effect of noise and improve the utilization of the noisy prior data, we propose innovative interval loss functions and give some rigorous mathematical proofs. The robustness of DEPINN is examined on two typical benchmark problems through a large number of numerical results, and the effectiveness of the proposed interval loss function is demonstrated by comparison. This paper confirms the feasibility of the improved DEPINN for practical engineering applications in nuclear reactor physics.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861; PHANES&#65292;&#23427;&#21487;&#20197;&#23558;&#20581;&#24247;&#32452;&#32455;&#20445;&#30041;&#24182;&#29992;&#20266;&#20581;&#24247;&#37325;&#26500;&#26367;&#25442;&#24322;&#24120;&#21306;&#22495;&#65292;&#26377;&#25928;&#26816;&#27979;&#20013;&#39118;&#25439;&#20260;&#12290;</title><link>http://arxiv.org/abs/2303.08452</link><description>&lt;p&gt;
&#32763;&#36716;&#24322;&#24120;&#65306;&#29992;&#34394;&#20551;&#20581;&#24247;&#29983;&#25104;&#32593;&#32476;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection. (arXiv:2303.08452v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08452
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861; PHANES&#65292;&#23427;&#21487;&#20197;&#23558;&#20581;&#24247;&#32452;&#32455;&#20445;&#30041;&#24182;&#29992;&#20266;&#20581;&#24247;&#37325;&#26500;&#26367;&#25442;&#24322;&#24120;&#21306;&#22495;&#65292;&#26377;&#25928;&#26816;&#27979;&#20013;&#39118;&#25439;&#20260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21450;&#26089;&#12289;&#20934;&#30830;&#22320;&#26816;&#27979;&#30142;&#30149;&#23545;&#20110;&#24739;&#32773;&#31649;&#29702;&#21644;&#25104;&#21151;&#27835;&#30103;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20013;&#33258;&#21160;&#35782;&#21035;&#24322;&#24120;&#21487;&#20197;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20256;&#32479;&#26041;&#27861;&#20381;&#36182;&#20110;&#38590;&#20197;&#33719;&#21462;&#30340;&#22823;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#31216;&#20026;PHANES&#65288;&#29992;&#20110;&#24322;&#24120;&#20998;&#21106;&#30340;&#34394;&#20551;&#20581;&#24247;&#29983;&#25104;&#32593;&#32476;&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#32763;&#36716;&#24322;&#24120;&#30340;&#33021;&#21147;&#65292;&#21363;&#20445;&#30041;&#20581;&#24247;&#32452;&#32455;&#24182;&#29992;&#20266;&#20581;&#24247;&#37325;&#26500;&#26367;&#25442;&#24322;&#24120;&#21306;&#22495;&#12290;&#19982;&#26368;&#36817;&#30340;&#25193;&#25955;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#23398;&#20064;&#22122;&#22768;&#20998;&#24067;&#65292;&#20063;&#19981;&#20250;&#24341;&#20837;&#38543;&#26426;&#21464;&#21270;&#21040;&#25972;&#20010;&#22270;&#20687;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#20351;&#29992;&#28508;&#22312;&#30340;&#29983;&#25104;&#32593;&#32476;&#26469;&#21019;&#24314;&#21487;&#33021;&#24322;&#24120;&#30340;&#25513;&#33180;&#65292;&#28982;&#21518;&#20351;&#29992;&#20462;&#34917;&#29983;&#25104;&#32593;&#32476;&#36827;&#34892;&#32454;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;PHANES&#22312;T1w&#33041;MRI&#25968;&#25454;&#38598;&#20013;&#26816;&#27979;&#20013;&#39118;&#25439;&#20260;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early and accurate disease detection is crucial for patient management and successful treatment outcomes. However, the automatic identification of anomalies in medical images can be challenging. Conventional methods rely on large labeled datasets which are difficult to obtain. To overcome these limitations, we introduce a novel unsupervised approach, called PHANES (Pseudo Healthy generative networks for ANomaly Segmentation). Our method has the capability of reversing anomalies, i.e., preserving healthy tissue and replacing anomalous regions with pseudo-healthy (PH) reconstructions. Unlike recent diffusion models, our method does not rely on a learned noise distribution nor does it introduce random alterations to the entire image. Instead, we use latent generative networks to create masks around possible anomalies, which are refined using inpainting generative networks. We demonstrate the effectiveness of PHANES in detecting stroke lesions in T1w brain MRI datasets and show significant
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#20083;&#33146;&#30284;&#34920;&#22411;&#25552;&#21462;&#20219;&#21153;&#30340;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#22522;&#20110;BERT&#30340;&#20020;&#24202;NLP&#27169;&#22411;&#22312;&#19981;&#21516;&#20020;&#24202;&#29615;&#22659;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#20351;&#29992;&#36716;&#31227;&#23398;&#20064;&#24320;&#21457;&#24191;&#20041;&#20020;&#24202;NLP&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.08448</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#30005;&#23376;&#30149;&#21382;&#30340;&#20083;&#33146;&#30284;&#34920;&#22411;NLP&#31639;&#27861;&#36827;&#34892;&#36328;&#26426;&#26500;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Cross-institutional Evaluation on Breast Cancer Phenotyping NLP Algorithms on Electronic Health Records. (arXiv:2303.08448v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#20083;&#33146;&#30284;&#34920;&#22411;&#25552;&#21462;&#20219;&#21153;&#30340;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#22522;&#20110;BERT&#30340;&#20020;&#24202;NLP&#27169;&#22411;&#22312;&#19981;&#21516;&#20020;&#24202;&#29615;&#22659;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#20351;&#29992;&#36716;&#31227;&#23398;&#20064;&#24320;&#21457;&#24191;&#20041;&#20020;&#24202;NLP&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#22312;&#27169;&#22411;&#24320;&#21457;&#36807;&#31243;&#20013;&#65292;&#36890;&#24120;&#24573;&#30053;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20083;&#33146;&#30284;&#34920;&#22411;&#25552;&#21462;&#20219;&#21153;&#65292;&#35780;&#20272;&#20102;&#22522;&#20110;BERT&#30340;&#20020;&#24202;NLP&#27169;&#22411;&#22312;&#19981;&#21516;&#20020;&#24202;&#29615;&#22659;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26041;&#27861;&#65306;&#20174;&#26126;&#23612;&#33487;&#36798;&#22823;&#23398;&#21644;&#26757;&#22885;&#35786;&#25152;&#30340;&#30005;&#23376;&#30149;&#21382;&#20013;&#25910;&#38598;&#20102;&#20004;&#31181;&#20083;&#33146;&#30284;&#24739;&#32773;&#30340;&#20020;&#24202;&#35821;&#26009;&#24211;&#65292;&#24182;&#25353;&#29031;&#21516;&#19968;&#25351;&#21335;&#36827;&#34892;&#27880;&#37322;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;NLP&#27169;&#22411;&#65288;&#26465;&#20214;&#38543;&#26426;&#22330;&#12289;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#21644;CancerBERT&#65289;&#65292;&#20174;&#20020;&#24202;&#25991;&#26412;&#20013;&#25552;&#21462;&#30284;&#30151;&#34920;&#22411;&#12290;&#20351;&#29992;&#19981;&#21516;&#30340;&#23398;&#20064;&#31574;&#30053;&#65288;&#27169;&#22411;&#36716;&#31227;&#19982;&#26412;&#22320;&#35757;&#32451;&#65289;&#23545;&#27169;&#22411;&#22312;&#19981;&#21516;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#27867;&#21270;&#33021;&#21147;&#35780;&#20272;&#12290;&#35780;&#20272;&#23454;&#20307;&#35206;&#30422;&#29575;&#19982;&#27169;&#22411;&#24615;&#33021;&#30340;&#30456;&#20851;&#24615;&#24471;&#20998;&#12290;&#32467;&#26524;&#65306;&#22312;UMN&#21644;MC&#25163;&#21160;&#27880;&#37322;&#20102;200&#21644;161&#20221;&#20020;&#24202;&#25991;&#26723;&#12290;CancerBERT&#27169;&#22411;&#36798;&#21040;&#20102;&#26368;&#39640;&#30340;F1&#20998;&#25968;&#65288;0.896&#65289;&#21644;&#23454;&#20307;&#35206;&#30422;&#29575;&#65288;98.8%&#65289;&#65292;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#27169;&#22411;&#36716;&#31227;&#26041;&#27861;&#22312;&#20004;&#20010;&#26426;&#26500;&#20013;&#20135;&#29983;&#20102;&#31867;&#20284;&#20110;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#30340;&#32467;&#26524;&#65292;&#34920;&#26126;&#36328;&#26426;&#26500;&#23384;&#22312;&#28508;&#22312;&#30340;&#27867;&#21270;&#24615;&#12290;&#32467;&#35770;&#65306;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#20020;&#24202;&#29615;&#22659;&#20013;&#35780;&#20272;NLP&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#20351;&#29992;&#36716;&#31227;&#23398;&#20064;&#24320;&#21457;&#24191;&#20041;&#20020;&#24202;NLP&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: The generalizability of clinical large language models is usually ignored during the model development process. This study evaluated the generalizability of BERT-based clinical NLP models across different clinical settings through a breast cancer phenotype extraction task.  Materials and Methods: Two clinical corpora of breast cancer patients were collected from the electronic health records from the University of Minnesota and the Mayo Clinic, and annotated following the same guideline. We developed three types of NLP models (i.e., conditional random field, bi-directional long short-term memory and CancerBERT) to extract cancer phenotypes from clinical texts. The models were evaluated for their generalizability on different test sets with different learning strategies (model transfer vs. locally trained). The entity coverage score was assessed with their association with the model performances.  Results: We manually annotated 200 and 161 clinical documents at UMN and MC, re
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#31649;&#29702;&#24494;&#30005;&#32593;&#20013;&#30340;&#33021;&#28304;&#20132;&#26131;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#26368;&#23567;&#21270;&#30899;&#36275;&#36857;&#65292;&#21516;&#26102;&#24179;&#34913;&#21487;&#20877;&#29983;&#33021;&#28304;&#21644;&#20256;&#32479;&#33021;&#28304;&#30340;&#28040;&#36153;&#21644;&#29983;&#20135;&#65292;&#24182;&#32771;&#34385;&#33021;&#28304;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2303.08447</link><description>&lt;p&gt;
MAHTM&#65306;&#20998;&#23618;&#21487;&#20132;&#26131;&#24494;&#30005;&#32593;&#30340;&#22810;&#26234;&#33021;&#20307;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MAHTM: A Multi-Agent Framework for Hierarchical Transactive Microgrids. (arXiv:2303.08447v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08447
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#31649;&#29702;&#24494;&#30005;&#32593;&#20013;&#30340;&#33021;&#28304;&#20132;&#26131;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#26368;&#23567;&#21270;&#30899;&#36275;&#36857;&#65292;&#21516;&#26102;&#24179;&#34913;&#21487;&#20877;&#29983;&#33021;&#28304;&#21644;&#20256;&#32479;&#33021;&#28304;&#30340;&#28040;&#36153;&#21644;&#29983;&#20135;&#65292;&#24182;&#32771;&#34385;&#33021;&#28304;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#21487;&#21464;&#30340;&#21487;&#20877;&#29983;&#33021;&#28304;&#24182;&#20837;&#30005;&#32593;&#32473;&#31995;&#32479;&#36816;&#33829;&#21830;&#24102;&#26469;&#25361;&#25112;&#65292;&#20351;&#24471;&#22312;&#33021;&#28304;&#21487;&#29992;&#24615;&#12289;&#25104;&#26412;&#25215;&#21463;&#33021;&#21147;&#21644;&#27745;&#26579;&#21487;&#25511;&#24615;&#20043;&#38388;&#21462;&#24471;&#26368;&#20248;&#25240;&#34935;&#25104;&#20026;&#19968;&#39033;&#38590;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31649;&#29702;&#24494;&#30005;&#32593;&#33021;&#28304;&#20132;&#26131;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#19978;&#36848;&#25361;&#25112;&#65306;&#26088;&#22312;&#36890;&#36807;&#26368;&#23567;&#21270;&#30899;&#36275;&#36857;&#30340;&#26041;&#24335;&#20248;&#21270;&#21487;&#29992;&#36164;&#28304;&#30340;&#21033;&#29992;&#65292;&#24182;&#20351;&#25152;&#26377;&#21033;&#30410;&#30456;&#20851;&#26041;&#21463;&#30410;&#12290;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#30001;&#19977;&#23618;&#20195;&#29702;&#32452;&#25104;&#65292;&#27599;&#23618;&#20195;&#29702;&#36861;&#27714;&#19981;&#21516;&#30340;&#30446;&#26631;&#12290;&#31532;&#19968;&#23618;&#30001;&#29983;&#20135;&#32773;&#21644;&#28040;&#36153;&#32773;&#32452;&#25104;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#24635;&#33021;&#28304;&#25104;&#26412;&#12290;&#20854;&#20182;&#20004;&#23618;&#25511;&#21046;&#33021;&#28304;&#20215;&#26684;&#65292;&#20197;&#20943;&#23569;&#30899;&#36275;&#36857;&#65292;&#21516;&#26102;&#24179;&#34913;&#21487;&#20877;&#29983;&#33021;&#28304;&#21644;&#20256;&#32479;&#33021;&#28304;&#30340;&#28040;&#36153;&#21644;&#29983;&#20135;&#12290;&#35813;&#26694;&#26550;&#36824;&#32771;&#34385;&#20102;&#33021;&#28304;&#38656;&#27714;&#21644;&#20379;&#24212;&#30340;&#27874;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating variable renewable energy into the grid has posed challenges to system operators in achieving optimal trade-offs among energy availability, cost affordability, and pollution controllability. This paper proposes a multi-agent reinforcement learning framework for managing energy transactions in microgrids. The framework addresses the challenges above: it seeks to optimize the usage of available resources by minimizing the carbon footprint while benefiting all stakeholders. The proposed architecture consists of three layers of agents, each pursuing different objectives. The first layer, comprised of prosumers and consumers, minimizes the total energy cost. The other two layers control the energy price to decrease the carbon impact while balancing the consumption and production of both renewable and conventional energy. This framework also takes into account fluctuations in energy demand and supply.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20004;&#20010;&#39044;&#35757;&#32451;&#30340;&#22402;&#30452;&#20108;&#32500;&#25193;&#25955;&#27169;&#22411;&#26469;&#35299;&#20915;&#19977;&#32500;&#36870;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#23558;&#19977;&#32500;&#25968;&#25454;&#20998;&#24067;&#24314;&#27169;&#20026;&#22312;&#19981;&#21516;&#26041;&#21521;&#19978;&#20999;&#29255;&#30340;&#20108;&#32500;&#20998;&#24067;&#30340;&#20056;&#31215;&#65292;&#35299;&#20915;&#20102;&#32500;&#25968;&#30340;&#28798;&#38590;&#24615;&#38382;&#39064;&#65292;&#24182;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#37325;&#24314;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.08440</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22402;&#30452;&#20108;&#32500;&#25193;&#25955;&#27169;&#22411;&#25913;&#36827;&#19977;&#32500;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
Improving 3D Imaging with Pre-Trained Perpendicular 2D Diffusion Models. (arXiv:2303.08440v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20004;&#20010;&#39044;&#35757;&#32451;&#30340;&#22402;&#30452;&#20108;&#32500;&#25193;&#25955;&#27169;&#22411;&#26469;&#35299;&#20915;&#19977;&#32500;&#36870;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#23558;&#19977;&#32500;&#25968;&#25454;&#20998;&#24067;&#24314;&#27169;&#20026;&#22312;&#19981;&#21516;&#26041;&#21521;&#19978;&#20999;&#29255;&#30340;&#20108;&#32500;&#20998;&#24067;&#30340;&#20056;&#31215;&#65292;&#35299;&#20915;&#20102;&#32500;&#25968;&#30340;&#28798;&#38590;&#24615;&#38382;&#39064;&#65292;&#24182;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#37325;&#24314;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#20247;&#22810;&#30340;&#20248;&#28857;&#65292;&#25193;&#25955;&#27169;&#22411;&#24050;&#25104;&#20026;&#22270;&#20687;&#29983;&#25104;&#21644;&#37325;&#26500;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290; &#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;&#25193;&#25955;&#30340;&#36870;&#38382;&#39064;&#27714;&#35299;&#26041;&#27861;&#20165;&#22788;&#29702;&#20108;&#32500;&#22270;&#20687;&#65292;&#21363;&#20351;&#26159;&#26368;&#36817;&#21457;&#24067;&#30340;&#19977;&#32500;&#26041;&#27861;&#20063;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#19977;&#32500;&#20808;&#39564;&#20998;&#24067;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#20004;&#20010;&#30456;&#20114;&#22402;&#30452;&#30340;&#39044;&#35757;&#32451;&#20108;&#32500;&#25193;&#25955;&#27169;&#22411;&#26469;&#35299;&#20915;&#19977;&#32500;&#36870;&#38382;&#39064;&#12290; &#36890;&#36807;&#23558;&#19977;&#32500;&#25968;&#25454;&#20998;&#24067;&#24314;&#27169;&#20026;&#22312;&#19981;&#21516;&#26041;&#21521;&#19978;&#20999;&#29255;&#30340;&#20108;&#32500;&#20998;&#24067;&#30340;&#20056;&#31215;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#32500;&#25968;&#30340;&#28798;&#38590;&#24615;&#38382;&#39064;&#12290; &#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#37325;&#24314;&#20219;&#21153;&#38750;&#24120;&#26377;&#25928;&#65292;&#21253;&#25324;MRI Z&#36724;&#36229;&#20998;&#36776;&#29575;&#65292;&#21387;&#32553;&#24863;&#30693;MRI&#21644;&#31232;&#30095;&#35270;&#22270;CT&#12290; &#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#36866;&#29992;&#20110;&#21307;&#23398;&#24212;&#29992;&#30340;&#39640;&#36136;&#37327;&#20307;&#32032;&#20307;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have become a popular approach for image generation and reconstruction due to their numerous advantages. However, most diffusion-based inverse problem-solving methods only deal with 2D images, and even recently published 3D methods do not fully exploit the 3D distribution prior. To address this, we propose a novel approach using two perpendicular pre-trained 2D diffusion models to solve the 3D inverse problem. By modeling the 3D data distribution as a product of 2D distributions sliced in different directions, our method effectively addresses the curse of dimensionality. Our experimental results demonstrate that our method is highly effective for 3D medical image reconstruction tasks, including MRI Z-axis super-resolution, compressed sensing MRI, and sparse-view CT. Our method can generate high-quality voxel volumes suitable for medical applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20809;&#21051;&#27169;&#22411;&#33539;&#24335;&#65292;&#36890;&#36807;&#20248;&#21270;&#22797;&#20540;&#31070;&#32463;&#22330;&#25191;&#34892;&#20809;&#23398;&#26680;&#22238;&#24402;&#24182;&#23558;&#20809;&#21051;&#31995;&#32479;&#25286;&#35299;&#20026;&#38750;&#21442;&#25968;&#25513;&#27169;&#25805;&#20316;&#21644;&#21253;&#21547;&#34892;&#21015;&#24335;&#28304;&#12289;&#30643;&#23380;&#21644;&#20809;&#21051;&#20449;&#24687;&#30340;&#23398;&#20064;&#20809;&#23398;&#26680;&#65292;&#20351;&#29992;&#23567;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.08435</link><description>&lt;p&gt;
&#20351;&#29992;&#22797;&#20540;&#31070;&#32463;&#22330;&#30340;&#29289;&#29702;&#20449;&#24687;&#20809;&#23398;&#26680;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Optical Kernel Regression Using Complex-valued Neural Fields. (arXiv:2303.08435v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20809;&#21051;&#27169;&#22411;&#33539;&#24335;&#65292;&#36890;&#36807;&#20248;&#21270;&#22797;&#20540;&#31070;&#32463;&#22330;&#25191;&#34892;&#20809;&#23398;&#26680;&#22238;&#24402;&#24182;&#23558;&#20809;&#21051;&#31995;&#32479;&#25286;&#35299;&#20026;&#38750;&#21442;&#25968;&#25513;&#27169;&#25805;&#20316;&#21644;&#21253;&#21547;&#34892;&#21015;&#24335;&#28304;&#12289;&#30643;&#23380;&#21644;&#20809;&#21051;&#20449;&#24687;&#30340;&#23398;&#20064;&#20809;&#23398;&#26680;&#65292;&#20351;&#29992;&#23567;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#21051;&#26159;&#38598;&#25104;&#30005;&#36335;&#21046;&#36896;&#30340;&#22522;&#30784;&#65292;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#12290;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20809;&#21051;&#27169;&#22411;&#30340;&#21457;&#23637;&#32531;&#35299;&#20102;&#21046;&#36896;&#36807;&#31243;&#24320;&#38144;&#21644;&#33021;&#21147;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#20197;&#21069;&#30340;&#26041;&#27861;&#37117;&#23558;&#20809;&#21051;&#31995;&#32479;&#35270;&#20026;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#40657;&#30418;&#26144;&#23556;&#65292;&#21033;&#29992;&#32593;&#32476;&#21442;&#25968;&#36890;&#36807;&#27515;&#35760;&#30828;&#32972;&#26144;&#23556;&#22823;&#37327;&#30340;&#25513;&#27169;&#21040;&#31354;&#20013;&#25110;&#25513;&#27169;&#21040;&#30005;&#38459;&#22270;&#20687;&#23545;&#65292;&#23548;&#33268;&#25512;&#24191;&#33021;&#21147;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33539;&#24335;&#65292;&#23558;&#20005;&#26684;&#30340;&#20809;&#21051;&#27169;&#22411;&#25286;&#35299;&#20026;&#38750;&#21442;&#25968;&#25513;&#27169;&#25805;&#20316;&#21644;&#21253;&#21547;&#34892;&#21015;&#24335;&#28304;&#12289;&#30643;&#23380;&#21644;&#20809;&#21051;&#20449;&#24687;&#30340;&#23398;&#20064;&#20809;&#23398;&#26680;&#12290;&#36890;&#36807;&#20248;&#21270;&#22797;&#20540;&#31070;&#32463;&#22330;&#20197;&#25191;&#34892;&#20809;&#23398;&#26680;&#22238;&#24402;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#22320;&#24674;&#22797;&#20809;&#21051;&#31995;&#32479;&#65292;&#21516;&#26102;&#20351;&#29992;&#36739;&#23569;&#30340;&#21442;&#25968;&#36827;&#34892;&#23567;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lithography is fundamental to integrated circuit fabrication, necessitating large computation overhead. The advancement of machine learning (ML)-based lithography models alleviates the trade-offs between manufacturing process expense and capability. However, all previous methods regard the lithography system as an image-to-image black box mapping, utilizing network parameters to learn by rote mappings from massive mask-to-aerial or mask-to-resist image pairs, resulting in poor generalization capability. In this paper, we propose a new ML-based paradigm disassembling the rigorous lithographic model into non-parametric mask operations and learned optical kernels containing determinant source, pupil, and lithography information. By optimizing complex-valued neural fields to perform optical kernel regression from coordinates, our method can accurately restore lithography system using a small-scale training dataset with fewer parameters, demonstrating superior generalization capability as w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#29305;&#23450;&#31867;&#22411;&#30149;&#21464;&#30340;&#28145;&#24230;&#26377;&#21521;&#32047;&#21152;&#22120; (DeDA) &#23545;&#31070;&#32463;&#32593;&#32476;&#27880;&#20837;&#39046;&#22495;&#29305;&#23450;&#30340;&#24402;&#32435;&#20559;&#24046;&#23454;&#29616;&#23545;&#35813;&#31867;&#22411;&#30149;&#21464;&#30340;&#31934;&#30830;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2303.08434</link><description>&lt;p&gt;
DeDA: &#28145;&#24230;&#26377;&#21521;&#32047;&#21152;&#22120;
&lt;/p&gt;
&lt;p&gt;
DeDA: Deep Directed Accumulator. (arXiv:2303.08434v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#29305;&#23450;&#31867;&#22411;&#30149;&#21464;&#30340;&#28145;&#24230;&#26377;&#21521;&#32047;&#21152;&#22120; (DeDA) &#23545;&#31070;&#32463;&#32593;&#32476;&#27880;&#20837;&#39046;&#22495;&#29305;&#23450;&#30340;&#24402;&#32435;&#20559;&#24046;&#23454;&#29616;&#23545;&#35813;&#31867;&#22411;&#30149;&#21464;&#30340;&#31934;&#30830;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24930;&#24615;&#27963;&#21160;&#24615;&#30340;&#22810;&#21457;&#24615;&#30828;&#21270;&#30151;&#29366; (rim+ &#25439;&#20260;) &#21487;&#20197;&#36890;&#36807;&#23450;&#37327;&#30913;&#24863;&#24212;&#36896;&#24433; (quantitative susceptibility maps) &#26469;&#34920;&#24449;&#65292;&#20854;&#36793;&#32536;&#22788;&#21576;&#29616;&#39640;&#20449;&#21495;&#24378;&#24230;&#36793;&#32536; (hyperintense rim)&#12290;&#36825;&#20123; rim+ &#25439;&#20260;&#20855;&#26377;&#20960;&#20309;&#31616;&#21333;&#30340;&#32467;&#26500;&#65292;&#20854;&#20013;&#25439;&#20260;&#36793;&#32536;&#22788;&#30340;&#26799;&#24230;&#26159;&#20174;&#20013;&#24515;&#21521;&#36793;&#32536;&#36752;&#23556;&#26041;&#21521;&#20998;&#24067;&#30340;&#65292;&#19988;&#26799;&#24230;&#30340;&#24133;&#24230;&#27604; rim- (&#38750; rim+) &#25439;&#20260;&#26356;&#22823;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#30001;&#20110;&#25968;&#25454;&#26377;&#38480;&#21644;&#39640;&#24230;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#36825;&#31867;&#25439;&#20260;&#30340;&#35782;&#21035;&#34920;&#29616;&#20173;&#19981;&#23613;&#22914;&#20154;&#24847;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22270;&#20687;&#22788;&#29702;&#25805;&#20316;&#65292;&#31216;&#20026; deep directed accumulator (DeDA)&#65292;&#23427;&#20026; rim+ &#25439;&#20260;&#30340;&#35782;&#21035;&#27880;&#20837;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;&#24402;&#32435;&#20559;&#24046; (priors)&#12290;&#32473;&#23450;&#19968;&#20010;&#29305;&#24449;&#26144;&#23556;&#21644;&#19968;&#32452;&#37319;&#26679;&#32593;&#26684;&#65292;DeDA &#23558;&#21019;&#24314;&#24182;&#37327;&#21270;&#19968;&#20010;&#32047;&#21152;&#22120;&#31354;&#38388;&#21040;&#26377;&#38480;&#30340;&#38388;&#38548;&#65292;&#24182;&#30456;&#24212;&#22320;&#32047;&#31215;&#29305;&#24449;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chronic active multiple sclerosis lesions, also termed as rim+ lesions, can be characterized by a hyperintense rim at the edge of the lesion on quantitative susceptibility maps. These rim+ lesions exhibit a geometrically simple structure, where gradients at the lesion edge are radially oriented and a greater magnitude of gradients is observed in contrast to rim- (non rim+) lesions. However, recent studies have shown that the identification performance of such lesions remains unsatisfied due to the limited amount of data and high class imbalance. In this paper, we propose a simple yet effective image processing operation, deep directed accumulator (DeDA), that provides a new perspective for injecting domain-specific inductive biases (priors) into neural networks for rim+ lesion identification. Given a feature map and a set of sampling grids, DeDA creates and quantizes an accumulator space into finite intervals, and accumulates feature values accordingly. This DeDA operation is a general
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;Mixup&#23545;&#20110;&#29305;&#24449;&#23398;&#20064;&#30340;&#30410;&#22788;&#12290;&#28151;&#21512;&#35757;&#32451;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#28151;&#21512;&#25968;&#25454;&#20013;&#23398;&#20064;&#32597;&#35265;&#29305;&#24449;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#26631;&#20934;&#35757;&#32451;&#21487;&#33021;&#20250;&#28431;&#25481;&#36825;&#20123;&#32597;&#35265;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2303.08433</link><description>&lt;p&gt;
&#28151;&#21512;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;Mixup&#23545;&#20110;&#29305;&#24449;&#23398;&#20064;&#30340;&#30410;&#22788;
&lt;/p&gt;
&lt;p&gt;
The Benefits of Mixup for Feature Learning. (arXiv:2303.08433v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;Mixup&#23545;&#20110;&#29305;&#24449;&#23398;&#20064;&#30340;&#30410;&#22788;&#12290;&#28151;&#21512;&#35757;&#32451;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#28151;&#21512;&#25968;&#25454;&#20013;&#23398;&#20064;&#32597;&#35265;&#29305;&#24449;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#26631;&#20934;&#35757;&#32451;&#21487;&#33021;&#20250;&#28431;&#25481;&#36825;&#20123;&#32597;&#35265;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Mixup&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#32447;&#24615;&#25554;&#20540;&#38543;&#26426;&#28151;&#21512;&#20004;&#20010;&#25968;&#25454;&#28857;&#65292;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#27867;&#21270;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#20854;&#26377;&#25928;&#24615;&#30340;&#29702;&#35770;&#22522;&#30784;&#23578;&#26410;&#23436;&#20840;&#34987;&#29702;&#35299;&#12290;&#26412;&#25991;&#26088;&#22312;&#23547;&#27714;&#23545;Mixup&#30410;&#22788;&#30340;&#22522;&#26412;&#29702;&#35299;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;Mixup&#22312;&#29305;&#24449;&#21644;&#26631;&#31614;&#20351;&#29992;&#19981;&#21516;&#30340;&#32447;&#24615;&#25554;&#20540;&#21442;&#25968;&#26102;&#20173;&#21487;&#23454;&#29616;&#31867;&#20284;&#20110;&#26631;&#20934;Mixup&#30340;&#24615;&#33021;&#12290;&#36825;&#34920;&#26126;&#65292;Zhang&#31561;&#20154;&#65288;2018&#65289;&#25552;&#20986;&#30340;&#30452;&#35266;&#32447;&#24615;&#35299;&#37322;&#21487;&#33021;&#24182;&#19981;&#33021;&#23436;&#20840;&#35299;&#37322;Mixup&#30340;&#25104;&#21151;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#29305;&#24449;&#23398;&#20064;&#30340;&#35282;&#24230;&#23545;Mixup&#36827;&#34892;&#29702;&#35770;&#30740;&#31350;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#29305;&#24449;&#22122;&#22768;&#25968;&#25454;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;Mixup&#35757;&#32451;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#20854;&#19982;&#24120;&#35265;&#29305;&#24449;&#65288;&#20986;&#29616;&#22312;&#22823;&#37096;&#20998;&#25968;&#25454;&#20013;&#65289;&#28151;&#21512;&#20013;&#23398;&#20064;&#32597;&#35265;&#29305;&#24449;&#65288;&#20986;&#29616;&#22312;&#23569;&#37096;&#20998;&#25968;&#25454;&#20013;&#65289;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26631;&#20934;&#35757;&#32451;&#21487;&#33021;&#20250;&#28431;&#25481;&#36825;&#20123;&#32597;&#35265;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixup, a simple data augmentation method that randomly mixes two data points via linear interpolation, has been extensively applied in various deep learning applications to gain better generalization. However, the theoretical underpinnings of its efficacy are not yet fully understood. In this paper, we aim to seek a fundamental understanding of the benefits of Mixup. We first show that Mixup using different linear interpolation parameters for features and labels can still achieve similar performance to the standard Mixup. This indicates that the intuitive linearity explanation in Zhang et al., (2018) may not fully explain the success of Mixup. Then we perform a theoretical study of Mixup from the feature learning perspective. We consider a feature-noise data model and show that Mixup training can effectively learn the rare features (appearing in a small fraction of data) from its mixture with the common features (appearing in a large fraction of data). In contrast, standard training ca
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#20960;&#20046;&#32447;&#24615;&#20108;&#27425;&#22411;&#35843;&#33410;&#22120;&#31995;&#32479;&#20013;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#21487;&#20197;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2303.08431</link><description>&lt;p&gt;
&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#25910;&#25947;&#20110;&#20960;&#20046;&#32447;&#24615;&#20108;&#27425;&#22411;&#35843;&#33410;&#22120;&#30340;&#20840;&#23616;&#26368;&#20248;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Policy Gradient Converges to the Globally Optimal Policy for Nearly Linear-Quadratic Regulators. (arXiv:2303.08431v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#20960;&#20046;&#32447;&#24615;&#20108;&#27425;&#22411;&#35843;&#33410;&#22120;&#31995;&#32479;&#20013;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#21487;&#20197;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#32773;&#21482;&#33719;&#24471;&#20102;&#38750;&#23436;&#25972;&#20449;&#24687;&#30340;&#38750;&#32447;&#24615;&#25511;&#21046;&#31995;&#32479;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#25214;&#21040;&#20960;&#20046;&#32447;&#24615;&#20108;&#27425;&#22411;&#35843;&#33410;&#22120;&#31995;&#32479;&#20013;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#21160;&#24577;&#31995;&#32479;&#65292;&#32467;&#21512;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#30001;&#30456;&#21516;&#32467;&#26500;&#30340;&#31574;&#30053;&#36827;&#34892;&#31649;&#29702;&#12290;&#22312;&#20551;&#35774;&#38750;&#32447;&#24615;&#32452;&#25104;&#37096;&#20998;&#21253;&#21547;&#20855;&#26377;&#23567;&#22411;Lipschitz&#31995;&#25968;&#30340;&#20869;&#26680;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23545;&#25104;&#26412;&#20989;&#25968;&#30340;&#20248;&#21270;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#34429;&#28982;&#25104;&#26412;&#20989;&#25968;&#36890;&#24120;&#26159;&#38750;&#20984;&#30340;&#65292;&#20294;&#25105;&#20204;&#30830;&#31435;&#20102;&#20840;&#23616;&#26368;&#20248;&#35299;&#38468;&#36817;&#23616;&#37096;&#30340;&#24378;&#20984;&#24615;&#21644;&#20809;&#28369;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21021;&#22987;&#21270;&#26426;&#21046;&#65292;&#20197;&#21033;&#29992;&#36825;&#20123;&#23646;&#24615;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#21487;&#20197;&#20445;&#35777;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nonlinear control systems with partial information to the decision maker are prevalent in a variety of applications. As a step toward studying such nonlinear systems, this work explores reinforcement learning methods for finding the optimal policy in the nearly linear-quadratic regulator systems. In particular, we consider a dynamic system that combines linear and nonlinear components, and is governed by a policy with the same structure. Assuming that the nonlinear component comprises kernels with small Lipschitz coefficients, we characterize the optimization landscape of the cost function. Although the cost function is nonconvex in general, we establish the local strong convexity and smoothness in the vicinity of the global optimizer. Additionally, we propose an initialization mechanism to leverage these properties. Building on the developments, we design a policy gradient algorithm that is guaranteed to converge to the globally optimal policy with a linear rate.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#27169;&#22411;DualFair&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#23545;&#27604;&#25439;&#22833;&#26469;&#29983;&#25104;&#23545;&#27599;&#20010;&#21463;&#20445;&#25252;&#32676;&#20307;&#19981;&#21487;&#21306;&#20998;&#30340;&#23884;&#20837;&#65292;&#24182;&#32852;&#21512;&#20248;&#21270;&#20102;&#32676;&#20307;&#20844;&#24179;&#24615;&#21644;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#20004;&#20010;&#20844;&#24179;&#24615;&#26631;&#20934;&#65292;&#20351;&#24471;&#22312;&#32676;&#20307;&#21644;&#20010;&#20307;&#32423;&#21035;&#19978;&#20570;&#20986;&#26356;&#20844;&#27491;&#30340;&#39044;&#27979;&#65292;&#35813;&#27169;&#22411;&#22312;&#20844;&#24179;&#26234;&#33021;&#39046;&#22495;&#20013;&#20855;&#26377;&#28508;&#22312;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2303.08403</link><description>&lt;p&gt;
&#21452;&#37325;&#20844;&#24179;&#24615;&#65306;&#36890;&#36807;&#23545;&#27604;&#33258;&#30417;&#30563;&#23454;&#29616;&#32676;&#20307;&#21644;&#20010;&#20307;&#32423;&#21035;&#30340;&#20844;&#24179;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DualFair: Fair Representation Learning at Both Group and Individual Levels via Contrastive Self-supervision. (arXiv:2303.08403v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#27169;&#22411;DualFair&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#23545;&#27604;&#25439;&#22833;&#26469;&#29983;&#25104;&#23545;&#27599;&#20010;&#21463;&#20445;&#25252;&#32676;&#20307;&#19981;&#21487;&#21306;&#20998;&#30340;&#23884;&#20837;&#65292;&#24182;&#32852;&#21512;&#20248;&#21270;&#20102;&#32676;&#20307;&#20844;&#24179;&#24615;&#21644;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#20004;&#20010;&#20844;&#24179;&#24615;&#26631;&#20934;&#65292;&#20351;&#24471;&#22312;&#32676;&#20307;&#21644;&#20010;&#20307;&#32423;&#21035;&#19978;&#20570;&#20986;&#26356;&#20844;&#27491;&#30340;&#39044;&#27979;&#65292;&#35813;&#27169;&#22411;&#22312;&#20844;&#24179;&#26234;&#33021;&#39046;&#22495;&#20013;&#20855;&#26377;&#28508;&#22312;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#20844;&#24179;&#24615;&#24050;&#25104;&#20026;&#37325;&#35201;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20851;&#38190;&#20219;&#21153;&#30340;Web&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#27169;&#22411;DualFair&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20351;&#23398;&#20064;&#34920;&#31034;&#19981;&#24102;&#26377;&#24615;&#21035;&#21644;&#31181;&#26063;&#31561;&#25935;&#24863;&#23646;&#24615;&#12290;&#19982;&#29616;&#26377;&#30340;&#38024;&#23545;&#21333;&#19968;&#20844;&#24179;&#24615;&#31867;&#22411;&#30340;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#32852;&#21512;&#20248;&#21270;&#20102;&#20004;&#20010;&#20844;&#24179;&#24615;&#26631;&#20934;&#65292;&#21363;&#32676;&#20307;&#20844;&#24179;&#24615;&#21644;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#65292;&#20174;&#32780;&#22312;&#32676;&#20307;&#21644;&#20010;&#20307;&#32423;&#21035;&#19978;&#20570;&#20986;&#26356;&#20844;&#27491;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#23545;&#27604;&#25439;&#22833;&#26469;&#29983;&#25104;&#23545;&#27599;&#20010;&#21463;&#20445;&#25252;&#32676;&#20307;&#19981;&#21487;&#21306;&#20998;&#30340;&#23884;&#20837;&#65292;&#21516;&#26102;&#36843;&#20351;&#21453;&#20107;&#23454;&#23545;&#30340;&#23884;&#20837;&#30456;&#20284;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#25105;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#26469;&#32500;&#25252;&#19979;&#28216;&#20219;&#21153;&#30340;&#34920;&#31034;&#36136;&#37327;&#12290;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#20998;&#26512;&#35777;&#23454;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#32852;&#21512;&#22788;&#29702;&#20004;&#20010;&#20844;&#24179;&#24615;&#26631;&#20934;&#30340;&#21327;&#21516;&#25928;&#24212;&#65292;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#20844;&#24179;&#26234;&#33021;&#39046;&#22495;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic fairness has become an important machine learning problem, especially for mission-critical Web applications. This work presents a self-supervised model, called DualFair, that can debias sensitive attributes like gender and race from learned representations. Unlike existing models that target a single type of fairness, our model jointly optimizes for two fairness criteria - group fairness and counterfactual fairness - and hence makes fairer predictions at both the group and individual levels. Our model uses contrastive loss to generate embeddings that are indistinguishable for each protected group, while forcing the embeddings of counterfactual pairs to be similar. It then uses a self-knowledge distillation method to maintain the quality of representation for the downstream tasks. Extensive analysis over multiple datasets confirms the model's validity and further shows the synergy of jointly addressing two fairness criteria, suggesting the model's potential value in fair int
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#38050;&#29748;&#21367;&#24088;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21327;&#35843;&#12289;&#29983;&#25104;&#12289;&#23436;&#21892;&#38899;&#20048;&#65307;&#20195;&#30721;&#24050;&#20844;&#24320;&#20849;&#20139;&#12290;</title><link>http://arxiv.org/abs/2303.08385</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#31526;&#21495;&#38899;&#20048;
&lt;/p&gt;
&lt;p&gt;
Generating symbolic music using diffusion models. (arXiv:2303.08385v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#38050;&#29748;&#21367;&#24088;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21327;&#35843;&#12289;&#29983;&#25104;&#12289;&#23436;&#21892;&#38899;&#20048;&#65307;&#20195;&#30721;&#24050;&#20844;&#24320;&#20849;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#24050;&#25104;&#20026;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#19982;&#20854;&#20182;&#29983;&#25104;&#27169;&#22411;&#19981;&#21516;&#65292;&#25193;&#25955;&#27169;&#22411;&#19981;&#20250;&#20986;&#29616;&#27169;&#24335;&#23849;&#28291;&#65292;&#20063;&#19981;&#38656;&#35201;&#36776;&#21035;&#22120;&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#26679;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20108;&#39033;&#20808;&#39564;&#20998;&#24067;&#26469;&#29983;&#25104;&#38050;&#29748;&#21367;&#24088;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#27169;&#22411;&#21644;&#29983;&#25104;&#26679;&#26412;&#12290;&#29983;&#25104;&#30340;&#38899;&#20048;&#20855;&#26377;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#19968;&#33268;&#24615;&#65292;&#21487;&#20197;&#36798;&#21040;&#35757;&#32451;&#38050;&#29748;&#21367;&#24088;&#27573;&#30340;&#38271;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#26679;&#19968;&#20010;&#27169;&#22411;&#26159;&#22914;&#20309;&#22312;&#36755;&#20837;&#30340;&#26465;&#20214;&#19979;&#36827;&#34892;&#24037;&#20316;&#65292;&#24182;&#21487;&#29992;&#20110;&#21327;&#35843;&#32473;&#23450;&#30340;&#26059;&#24459;&#65292;&#23436;&#25104;&#19981;&#23436;&#25972;&#30340;&#38050;&#29748;&#21367;&#24088;&#25110;&#29983;&#25104;&#32473;&#23450;&#20048;&#26354;&#30340;&#21464;&#21270;&#12290;&#20195;&#30721;&#26159;&#20844;&#24320;&#20849;&#20139;&#30340;&#65292;&#20197;&#40723;&#21169;&#31038;&#21306;&#20351;&#29992;&#21644;&#24320;&#21457;&#35813;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Denoising Diffusion models have emerged as simple yet very powerful generative models. Diffusion models unlike other generative models do not suffer from mode collapse nor require a discriminator to generate high quality samples. In this paper, we propose a diffusion model that uses a binomial prior distribution to generate piano-rolls. The paper also proposes an efficient method to train the model and generate samples. The generated music has coherence at time scales up to the length of the training piano-roll segments. We show how such a model is conditioned on the input and can be used to harmonize a given melody, complete an incomplete piano-roll or generate a variation of a given piece. The code is shared publicly to encourage the use and development of the method by the community.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;MCR-DL&#65292;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21487;&#25193;&#23637;&#28151;&#21512;&#36890;&#20449;&#26694;&#26550;&#65292;&#33021;&#22815;&#25903;&#25345;&#21508;&#31181;&#38598;&#21512;&#21644;&#28857;&#23545;&#28857;&#25805;&#20316;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#21160;&#24577;&#22320;&#28151;&#21512;&#21644;&#21305;&#37197;&#36890;&#20449;&#21518;&#31471;&#20197;&#36827;&#34892;&#32473;&#23450;&#25805;&#20316;&#65292;&#20174;&#32780;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.08374</link><description>&lt;p&gt;
MCR-DL: &#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#28151;&#21512;&#36890;&#20449;&#36816;&#34892;&#26102;
&lt;/p&gt;
&lt;p&gt;
MCR-DL: Mix-and-Match Communication Runtime for Deep Learning. (arXiv:2303.08374v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08374
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;MCR-DL&#65292;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21487;&#25193;&#23637;&#28151;&#21512;&#36890;&#20449;&#26694;&#26550;&#65292;&#33021;&#22815;&#25903;&#25345;&#21508;&#31181;&#38598;&#21512;&#21644;&#28857;&#23545;&#28857;&#25805;&#20316;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#21160;&#24577;&#22320;&#28151;&#21512;&#21644;&#21305;&#37197;&#36890;&#20449;&#21518;&#31471;&#20197;&#36827;&#34892;&#32473;&#23450;&#25805;&#20316;&#65292;&#20174;&#32780;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#38656;&#27714;&#36229;&#20986;&#20102;&#21333;&#20010;&#22788;&#29702;&#22120;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#33021;&#21147;&#65292;&#24182;&#38656;&#35201;&#22312;&#22810;&#20010;&#22788;&#29702;&#22120;&#20043;&#38388;&#20998;&#37197;&#12290;&#35757;&#32451;&#36825;&#26679;&#30340;&#22823;&#22411;&#27169;&#22411;&#38656;&#35201;&#20808;&#36827;&#30340;&#24182;&#34892;&#31574;&#30053;&#20197;&#20445;&#25345;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20998;&#24067;&#24335;&#30340;&#28145;&#24230;&#23398;&#20064;&#24182;&#34892;&#31574;&#30053;&#38656;&#35201;&#22312;&#21508;&#31181;&#28040;&#24687;&#22823;&#23567;&#21644;&#35268;&#27169;&#19979;&#36827;&#34892;&#21508;&#31181;&#38598;&#21512;&#21644;&#28857;&#23545;&#28857;&#36890;&#20449;&#25805;&#20316;&#30340;&#28151;&#21512;&#12290;&#20351;&#29992;&#39640;&#32423;&#24182;&#34892;&#31574;&#30053;&#30340;&#27169;&#22411;&#21253;&#25324;&#65306;&#28145;&#24230;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411;&#65288;DLRM&#65289;&#21644;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#12290;&#36890;&#20449;&#24211;&#22312;&#19981;&#21516;&#30340;&#36890;&#20449;&#25805;&#20316;&#12289;&#35268;&#27169;&#21644;&#28040;&#24687;&#22823;&#23567;&#19979;&#30340;&#24615;&#33021;&#24046;&#24322;&#24040;&#22823;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; MCR-DL&#65306;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#28145;&#24230;&#23398;&#20064;&#36890;&#20449;&#26694;&#26550;&#65292;&#25903;&#25345;&#25152;&#26377;&#28857;&#23545;&#28857;&#21644;&#38598;&#20307;&#25805;&#20316;&#65292;&#21516;&#26102;&#20351;&#29992;&#25143;&#33021;&#22815;&#21160;&#24577;&#22320;&#28151;&#21512;&#21644;&#21305;&#37197;&#36890;&#20449;&#21518;&#31471;&#20197;&#36827;&#34892;&#32473;&#23450;&#25805;&#20316;&#32780;&#19981;&#20250;&#20986;&#29616;&#27515;&#38145;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the training requirements of many state-of-the-art Deep Learning (DL) models have scaled beyond the compute and memory capabilities of a single processor, and necessitated distribution among processors. Training such massive models necessitates advanced parallelism strategies to maintain efficiency. However, such distributed DL parallelism strategies require a varied mixture of collective and point-to-point communication operations across a broad range of message sizes and scales. Examples of models using advanced parallelism strategies include Deep Learning Recommendation Models (DLRM) and Mixture-of-Experts (MoE). Communication libraries' performance varies wildly across different communication operations, scales, and message sizes. We propose MCR-DL: an extensible DL communication framework that supports all point-to-point and collective operations while enabling users to dynamically mix-and-match communication backends for a given operation without deadlocks. MCR-D
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;CNN&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;Mel&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;&#65288;MFCCs&#65289;&#36827;&#34892;&#20998;&#31867;&#65292;&#23454;&#29616;&#23545;&#32954;&#37096;&#21628;&#21560;&#38899;&#30340;&#38750;&#20405;&#20837;&#24615;&#26816;&#27979;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;95%&#12290;</title><link>http://arxiv.org/abs/2303.08362</link><description>&lt;p&gt;
&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;&#32954;&#38899;&#24322;&#24120;&#26816;&#27979;&#19982;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning Based Diagnosis and Analysis of Lung Sound Aberrations. (arXiv:2303.08362v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;CNN&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;Mel&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;&#65288;MFCCs&#65289;&#36827;&#34892;&#20998;&#31867;&#65292;&#23454;&#29616;&#23545;&#32954;&#37096;&#21628;&#21560;&#38899;&#30340;&#38750;&#20405;&#20837;&#24615;&#26816;&#27979;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;95%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33021;&#22815;&#25910;&#38598;&#21644;&#20998;&#26512;&#22823;&#37327;&#25968;&#25454;&#30340;&#35745;&#31639;&#26426;&#31995;&#32479;&#30340;&#21457;&#23637;&#65292;&#21307;&#23398;&#30028;&#27491;&#22312;&#24314;&#31435;&#20960;&#31181;&#38750;&#20405;&#20837;&#24615;&#24037;&#20855;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#19968;&#31181;&#38750;&#20405;&#20837;&#24615;&#25216;&#26415;&#65292;&#29992;&#20110;&#35782;&#21035;&#30001;&#21548;&#35786;&#22120;&#21644;&#35821;&#38899;&#24405;&#21046;&#36719;&#20214;&#33719;&#21462;&#30340;&#21628;&#21560;&#38899;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#36807;&#35757;&#32451;&#21644;&#39564;&#35777;&#30340;&#22522;&#20110; CNN &#30340;&#26041;&#27861;&#26469;&#23545;&#21628;&#21560;&#38899;&#36827;&#34892;&#20998;&#31867;&#12290;&#36890;&#36807;&#26500;&#24314;&#27599;&#20010;&#38899;&#39057;&#26679;&#26412;&#30340;&#35270;&#35273;&#34920;&#31034;&#65292;&#21487;&#20197;&#20351;&#29992;&#31867;&#20284;&#20110;&#26377;&#25928;&#25551;&#36848;&#35270;&#35273;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#36164;&#28304;&#20197;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#31216;&#20026; Mel &#39057;&#29575;&#20498;&#35889;&#31995;&#25968; (MFCCs) &#30340;&#25216;&#26415;&#12290;&#36825;&#37324;&#65292;&#36890;&#36807; VGG16 (&#36801;&#31227;&#23398;&#20064;) &#26816;&#32034;&#21644;&#20998;&#31867;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992; 5 &#25240;&#20132;&#21449;&#39564;&#35777;&#36827;&#34892;&#39044;&#27979;&#12290;&#37319;&#29992;&#21508;&#31181;&#25968;&#25454;&#20998;&#21106;&#25216;&#26415;&#65292;&#21628;&#21560;&#38899;&#25968;&#25454;&#24211;&#33719;&#24471;&#20102;&#23574;&#31471;&#32467;&#26524;&#65292;&#21253;&#25324; 95% &#30340;&#20934;&#30830;&#29575;&#12289;88% &#30340;&#31934;&#30830;&#24230;&#12289;86% &#30340;&#21484;&#22238;&#29575;&#21644; 87% &#30340; F1 &#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of computer -systems that can collect and analyze enormous volumes of data, the medical profession is establishing several non-invasive tools. This work attempts to develop a non-invasive technique for identifying respiratory sounds acquired by a stethoscope and voice recording software via machine learning techniques. This study suggests a trained and proven CNN-based approach for categorizing respiratory sounds. A visual representation of each audio sample is constructed, allowing resource identification for classification using methods like those used to effectively describe visuals. We used a technique called Mel Frequency Cepstral Coefficients (MFCCs). Here, features are retrieved and categorized via VGG16 (transfer learning) and prediction is accomplished using 5-fold cross-validation. Employing various data splitting techniques, Respiratory Sound Database obtained cutting-edge results, including accuracy of 95%, precision of 88%, recall score of 86%, and F1 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#20316;&#24335;&#32852;&#37030;&#23398;&#20064;(CFL)&#33539;&#24335;&#65292;&#36890;&#36807;&#35774;&#22791;&#38388;&#30340;&#21512;&#20316;&#26469;&#25269;&#28040;&#36793;&#32536;/&#38654;&#32593;&#32476;&#20013;&#30340;&#24322;&#26500;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#36136;&#37327;&#21644;&#32593;&#32476;&#36164;&#28304;&#21033;&#29992;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.08361</link><description>&lt;p&gt;
&#38754;&#21521;&#24322;&#26500;&#36793;&#32536;/&#38654;&#35745;&#31639;&#32593;&#32476;&#30340;&#21512;&#20316;&#24335;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Cooperative Federated Learning over Heterogeneous Edge/Fog Networks. (arXiv:2303.08361v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08361
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#20316;&#24335;&#32852;&#37030;&#23398;&#20064;(CFL)&#33539;&#24335;&#65292;&#36890;&#36807;&#35774;&#22791;&#38388;&#30340;&#21512;&#20316;&#26469;&#25269;&#28040;&#36793;&#32536;/&#38654;&#32593;&#32476;&#20013;&#30340;&#24322;&#26500;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#36136;&#37327;&#21644;&#32593;&#32476;&#36164;&#28304;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#22312;&#36793;&#32536;/&#38654;&#35745;&#31639;&#32593;&#32476;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#27969;&#34892;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#23454;&#29616;&#24573;&#30053;&#20102;&#32593;&#32476;&#20043;&#38388;&#30340;&#28508;&#22312;&#21512;&#20316;&#65292;&#23558;&#21442;&#19982;&#26426;&#22120;&#23398;&#20064;&#30340;&#36793;&#32536;/&#38654;&#35774;&#22791;&#21644;&#20854;&#20182;&#22522;&#30784;&#35774;&#26045;&#35270;&#20026;&#29420;&#31435;&#30340;&#22788;&#29702;&#21333;&#20803;&#12290;&#22240;&#27492;&#65292;&#32852;&#37030;&#23398;&#20064;&#23481;&#26131;&#21463;&#21040;&#32593;&#32476;&#24322;&#26500;&#24615;&#30340;&#24433;&#21709;&#65292;&#22914;&#35745;&#31639;&#33021;&#21147;&#12289;&#36890;&#20449;&#36164;&#28304;&#12289;&#25968;&#25454;&#36136;&#37327;&#21644;&#38544;&#31169;&#38656;&#27714;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#20316;&#24335;&#32852;&#37030;&#23398;&#20064;(CFL)&#33539;&#24335;&#65292;&#24314;&#31435;&#22312;&#35774;&#22791;&#23545;&#35774;&#22791;(D2D)&#21644;&#35774;&#22791;&#23545;&#26381;&#21153;&#22120;(D2S)&#20132;&#20114;&#20043;&#19978;&#12290;&#36890;&#36807;D2D&#21644;D2S&#21512;&#20316;&#65292;CFL&#36890;&#36807;&#21551;&#29992;&#27169;&#22411;/&#25968;&#25454;/&#36164;&#28304;&#27719;&#38598;&#26426;&#21046;&#26469;&#25269;&#28040;&#36793;&#32536;/&#38654;&#32593;&#32476;&#20013;&#30340;&#24322;&#26500;&#24615;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#36136;&#37327;&#21644;&#32593;&#32476;&#36164;&#28304;&#21033;&#29992;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#26680;&#24515;&#26041;&#27861;&#65292;&#26500;&#25104;&#20102;CFL&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has been promoted as a popular technique for training machine learning (ML) models over edge/fog networks. Traditional implementations of FL have largely neglected the potential for inter-network cooperation, treating edge/fog devices and other infrastructure participating in ML as separate processing elements. Consequently, FL has been vulnerable to several dimensions of network heterogeneity, such as varying computation capabilities, communication resources, data qualities, and privacy demands. We advocate for cooperative federated learning (CFL), a cooperative edge/fog ML paradigm built on device-to-device (D2D) and device-to-server (D2S) interactions. Through D2D and D2S cooperation, CFL counteracts network heterogeneity in edge/fog networks through enabling a model/data/resource pooling mechanism, which will yield substantial improvements in ML model training quality and network resource consumption. We propose a set of core methodologies that form the foun
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37329;&#34701;&#24212;&#29992;&#30340;&#39640;&#25928;&#23433;&#20840;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;Top-K&#26799;&#24230;&#31232;&#30095;&#21270;&#21644;&#22522;&#20110;Delta&#30340;&#31232;&#30095;&#21270;&#65292;&#20197;&#21450;&#19968;&#31181;&#21487;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#32858;&#21512;&#26694;&#26550;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#22823;&#24133;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#24182;&#20445;&#35777;&#39044;&#27979;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.08355</link><description>&lt;p&gt;
&#37329;&#34701;&#24212;&#29992;&#30340;&#39640;&#25928;&#23433;&#20840;&#32852;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient and Secure Federated Learning for Financial Applications. (arXiv:2303.08355v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37329;&#34701;&#24212;&#29992;&#30340;&#39640;&#25928;&#23433;&#20840;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;Top-K&#26799;&#24230;&#31232;&#30095;&#21270;&#21644;&#22522;&#20110;Delta&#30340;&#31232;&#30095;&#21270;&#65292;&#20197;&#21450;&#19968;&#31181;&#21487;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#32858;&#21512;&#26694;&#26550;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#22823;&#24133;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#24182;&#20445;&#35777;&#39044;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#19982;&#22806;&#37096;&#24449;&#20449;&#23616;&#20849;&#20139;&#23458;&#25143;&#30340;&#25935;&#24863;&#20449;&#24687;&#26469;&#29983;&#25104;&#39044;&#27979;&#27169;&#22411;&#65292;&#20174;&#32780;&#23548;&#33268;&#38544;&#31169;&#27844;&#38706;&#30340;&#39118;&#38505;&#12290;&#32852;&#21512;&#23398;&#20064;&#26159;&#19968;&#31181;&#21487;&#20197;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#35774;&#32622;&#65292;&#20294;&#39640;&#36890;&#20449;&#25104;&#26412;&#32463;&#24120;&#25104;&#20026;&#32852;&#21512;&#31995;&#32479;&#30340;&#29942;&#39048;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#32780;&#35328;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31232;&#30095;&#21270;&#26041;&#27861;&#26469;&#38477;&#20302;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#25104;&#26412;&#65292;&#21363;Top-K&#26799;&#24230;&#31232;&#30095;&#21270;&#21644;&#22522;&#20110;Delta&#30340;&#31232;&#30095;&#21270;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#21644;&#23433;&#20840;&#30340;&#32858;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#32858;&#21512;&#26799;&#24230;&#26356;&#26032;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#22312;&#36827;&#34892;&#32858;&#21512;&#26102;&#20445;&#25252;&#27599;&#20010;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#38544;&#31169;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20004;&#31181;&#31232;&#30095;&#21270;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25345;&#26399;&#26395;&#30340;&#25910;&#25947;&#29575;&#21644;&#39044;&#27979;&#31934;&#24230;&#30340;&#21516;&#26102;&#65292;&#20943;&#23569;&#22810;&#36798;90%&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
The conventional machine learning (ML) and deep learning approaches need to share customers' sensitive information with an external credit bureau to generate a prediction model that opens the door to privacy leakage. This leakage risk makes financial companies face an enormous challenge in their cooperation. Federated learning is a machine learning setting that can protect data privacy, but the high communication cost is often the bottleneck of the federated systems, especially for large neural networks. Limiting the number and size of communications is necessary for the practical training of large neural structures. Gradient sparsification has received increasing attention as a method to reduce communication cost, which only updates significant gradients and accumulates insignificant gradients locally. However, the secure aggregation framework cannot directly use gradient sparsification. This article proposes two sparsification methods to reduce communication cost in federated learnin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#31209;&#24352;&#37327;&#20998;&#20139;&#30340;&#27169;&#22411;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#32553;&#23567;&#33267;5M&#21442;&#25968;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#22312;&#20302;&#20869;&#23384;&#31070;&#32463;&#22788;&#29702;&#22120;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#22987;&#32456;&#22788;&#20110;&#36816;&#34892;&#29366;&#24577;&#30340;&#35821;&#38899;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2303.08343</link><description>&lt;p&gt;
&#22522;&#20110;&#20302;&#31209;&#24352;&#37327;&#20998;&#20139;&#30340;Tiny Ambient Speech Recognition&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Sharing Low Rank Conformer Weights for Tiny Always-On Ambient Speech Recognition Models. (arXiv:2303.08343v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#31209;&#24352;&#37327;&#20998;&#20139;&#30340;&#27169;&#22411;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#32553;&#23567;&#33267;5M&#21442;&#25968;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#22312;&#20302;&#20869;&#23384;&#31070;&#32463;&#22788;&#29702;&#22120;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#22987;&#32456;&#22788;&#20110;&#36816;&#34892;&#29366;&#24577;&#30340;&#35821;&#38899;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#25345;&#32493;&#25913;&#36827;&#20026;&#20351;&#29992;&#26356;&#22823;&#30340;&#27169;&#22411;&#21644;&#26356;&#22823;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#26032;&#26426;&#20250;&#12290;&#20294;&#26159;&#65292;&#22312;&#20165;&#26377;&#20302;&#20869;&#23384;&#30340;&#26234;&#33021;&#25163;&#26426;&#12289;&#21487;&#31359;&#25140;&#35774;&#22791;&#21644;&#20854;&#20182;&#23884;&#20837;&#24335;&#29615;&#22659;&#31561;&#20302;&#21151;&#32791;&#35774;&#22791;&#19978;&#25552;&#20379;&#36825;&#20123;&#26032;&#21151;&#33021;&#30340;&#38656;&#27714;&#19982;&#26085;&#20465;&#22686;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#20943;&#23567;&#22522;&#20110;Conformer&#30340;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#27169;&#22411;&#22823;&#23567;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#20110;100M&#20010;&#21442;&#25968;&#30340;&#27169;&#22411;&#65292;&#23558;&#20854;&#32553;&#23567;&#21040;&#20165;$5$M&#20010;&#21442;&#25968;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23545;&#27169;&#22411;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#20855;&#26377;&#20302;&#20869;&#23384;&#31070;&#32463;&#22788;&#29702;&#22120;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#22987;&#32456;&#22788;&#20110;&#36816;&#34892;&#29366;&#24577;&#30340;&#35821;&#38899;&#35782;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#22312;&#27169;&#22411;&#26550;&#26500;&#20013;&#30340;&#19981;&#21516;&#23618;&#27425;&#19978;&#37325;&#22797;&#20351;&#29992;&#27169;&#22411;&#26435;&#37325;: (i) &#37325;&#22797;&#25972;&#20010;Conformer&#22359;&#23618;&#65292;(ii) &#22312;&#23618;&#20043;&#38388;&#20849;&#20139;&#29305;&#23450;&#30340;Conformer&#27169;&#22359;&#65292;(iii) &#22312;Conformer&#27169;&#22359;&#20013;&#20849;&#20139;&#23376;&#37096;&#20214;&#65292;(iv) &#22312;&#20302;&#31209;&#24352;&#37327;&#20998;&#35299;&#21518;&#20849;&#20139;&#20998;&#35299;&#30340;&#23376;&#37096;&#20214;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continued improvements in machine learning techniques offer exciting new opportunities through the use of larger models and larger training datasets. However, there is a growing need to offer these new capabilities on-board low-powered devices such as smartphones, wearables and other embedded environments where only low memory is available. Towards this, we consider methods to reduce the model size of Conformer-based speech recognition models which typically require models with greater than 100M parameters down to just $5$M parameters while minimizing impact on model quality. Such a model allows us to achieve always-on ambient speech recognition on edge devices with low-memory neural processors. We propose model weight reuse at different levels within our model architecture: (i) repeating full conformer block layers, (ii) sharing specific conformer modules across layers, (iii) sharing sub-components per conformer module, and (iv) sharing decomposed sub-component weights after low-rank 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31354;&#38388;&#26102;&#38388;&#20449;&#24687;&#26469;&#25552;&#39640;&#35270;&#39057;&#36229;&#20998;&#36776;&#29575;&#30340;&#26032;&#26041;&#27861;&#65292;&#37319;&#29992;&#39640;&#32500;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;&#39044;&#27979;&#24182;&#24212;&#29992;&#26102;&#38388;&#27880;&#24847;&#26426;&#21046;&#20197;&#21435;&#38500;&#20887;&#20313;&#20449;&#24687;&#24182;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.08331</link><description>&lt;p&gt;
&#36890;&#36807;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#36807;&#25311;&#21512;&#23454;&#29616;&#39640;&#36136;&#37327;&#39640;&#25928;&#30340;&#35270;&#39057;&#36229;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
Towards High-Quality and Efficient Video Super-Resolution via Spatial-Temporal Data Overfitting. (arXiv:2303.08331v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31354;&#38388;&#26102;&#38388;&#20449;&#24687;&#26469;&#25552;&#39640;&#35270;&#39057;&#36229;&#20998;&#36776;&#29575;&#30340;&#26032;&#26041;&#27861;&#65292;&#37319;&#29992;&#39640;&#32500;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;&#39044;&#27979;&#24182;&#24212;&#29992;&#26102;&#38388;&#27880;&#24847;&#26426;&#21046;&#20197;&#21435;&#38500;&#20887;&#20313;&#20449;&#24687;&#24182;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(DNN)&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#21508;&#20010;&#39046;&#22495;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#21033;&#29992;DNN&#30340;&#36807;&#25311;&#21512;&#33021;&#21147;&#23454;&#29616;&#35270;&#39057;&#20998;&#36776;&#29575;&#30340;&#25552;&#21319;&#24050;&#32463;&#25104;&#20026;&#29616;&#20195;&#35270;&#39057;&#20256;&#36755;&#31995;&#32479;&#30340;&#26032;&#36235;&#21183;&#12290;&#23558;&#35270;&#39057;&#20998;&#20026;&#22359;&#24182;&#23558;&#27599;&#20010;&#22359;&#19982;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;&#36807;&#25311;&#21512;&#65292;&#20174;&#32780;&#22312;&#20256;&#36755;&#32473;&#23458;&#25143;&#31471;&#20043;&#21069;&#23545;&#35270;&#39057;&#36827;&#34892;&#32534;&#30721;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#35270;&#39057;&#36136;&#37327;&#21644;&#20256;&#36755;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20445;&#35777;&#33391;&#22909;&#30340;&#36807;&#25311;&#21512;&#36136;&#37327;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#22359;&#65292;&#36825;&#20250;&#22823;&#22823;&#22686;&#21152;&#23384;&#20648;&#37327;&#21644;&#28040;&#32791;&#26356;&#22810;&#24102;&#23485;&#36164;&#28304;&#36827;&#34892;&#25968;&#25454;&#20256;&#36755;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36890;&#36807;&#35757;&#32451;&#20248;&#21270;&#25216;&#26415;&#20943;&#23569;&#22359;&#30340;&#25968;&#37327;&#36890;&#24120;&#38656;&#35201;&#39640;&#27169;&#22411;&#23481;&#37327;&#65292;&#36825;&#20250;&#26174;&#33879;&#38477;&#20302;&#25191;&#34892;&#36895;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23436;&#25104;&#39640;&#36136;&#37327;&#21644;&#39640;&#25928;&#30340;&#35270;&#39057;&#20998;&#36776;&#29575;&#21319;&#32423;&#20219;&#21153;&#65292;&#21033;&#29992;&#31354;&#38388;&#26102;&#38388;&#20449;&#24687;&#26469;&#20934;&#30830;&#25429;&#25417;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#30340;&#35270;&#39057;&#22359;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#35270;&#39057;&#22359;&#30340;&#31354;&#38388;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#20351;&#29992;&#39640;&#32500;&#21367;&#31215;&#32593;&#32476;&#25913;&#36827;&#27599;&#20010;&#22359;&#30340;&#39044;&#27979;&#65292;&#24182;&#36827;&#19968;&#27493;&#24212;&#29992;&#26102;&#38388;&#27880;&#24847;&#26426;&#21046;&#20197;&#21435;&#38500;&#20887;&#20313;&#20449;&#24687;&#24182;&#20419;&#36827;&#20256;&#36755;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35270;&#35273;&#36136;&#37327;&#21644;&#25928;&#29575;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep convolutional neural networks (DNNs) are widely used in various fields of computer vision, leveraging the overfitting ability of the DNN to achieve video resolution upscaling has become a new trend in the modern video delivery system. By dividing videos into chunks and overfitting each chunk with a super-resolution model, the server encodes videos before transmitting them to the clients, thus achieving better video quality and transmission efficiency. However, a large number of chunks are expected to ensure good overfitting quality, which substantially increases the storage and consumes more bandwidth resources for data transmission. On the other hand, decreasing the number of chunks through training optimization techniques usually requires high model capacity, which significantly slows down execution speed. To reconcile such, we propose a novel method for high-quality and efficient video resolution upscaling tasks, which leverages the spatial-temporal information to accurately
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FairAdaBN&#65292;&#23558;&#25209;&#24402;&#19968;&#21270;&#36866;&#24212;&#25935;&#24863;&#23646;&#24615;&#65292;&#21487;&#20197;&#23558;&#20854;&#31616;&#21333;&#32780;&#26377;&#25928;&#22320;&#24212;&#29992;&#21040;&#21407;&#26412;&#19981;&#20102;&#35299;&#20844;&#24179;&#24615;&#30340;&#22810;&#20010;&#20998;&#31867;&#20027;&#24178;&#20013;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20943;&#23569;&#19981;&#20844;&#24179;&#24615;&#65292;&#24182;&#23454;&#29616;&#27169;&#22411;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2303.08325</link><description>&lt;p&gt;
FairAdaBN&#65306;&#33258;&#36866;&#24212;&#25209;&#24402;&#19968;&#21270;&#20943;&#23569;&#19981;&#20844;&#24179;&#24615;&#21450;&#20854;&#22312;&#30382;&#32932;&#30149;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
FairAdaBN: Mitigating unfairness with adaptive batch normalization and its application to dermatological disease classification. (arXiv:2303.08325v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FairAdaBN&#65292;&#23558;&#25209;&#24402;&#19968;&#21270;&#36866;&#24212;&#25935;&#24863;&#23646;&#24615;&#65292;&#21487;&#20197;&#23558;&#20854;&#31616;&#21333;&#32780;&#26377;&#25928;&#22320;&#24212;&#29992;&#21040;&#21407;&#26412;&#19981;&#20102;&#35299;&#20844;&#24179;&#24615;&#30340;&#22810;&#20010;&#20998;&#31867;&#20027;&#24178;&#20013;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20943;&#23569;&#19981;&#20844;&#24179;&#24615;&#65292;&#24182;&#23454;&#29616;&#27169;&#22411;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27491;&#22312;&#21307;&#23398;&#30740;&#31350;&#21644;&#24212;&#29992;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#21516;&#26102;&#28041;&#21450;&#25935;&#24863;&#20449;&#24687;&#65292;&#29978;&#33267;&#21253;&#25324;&#20851;&#38190;&#30340;&#35786;&#26029;&#20915;&#31574;&#12290;&#30740;&#31350;&#20154;&#21592;&#35266;&#23519;&#21040;&#19981;&#21516;&#20154;&#21475;&#23646;&#24615;&#23376;&#32452;&#20043;&#38388;&#30340;&#26174;&#33879;&#24615;&#33021;&#24046;&#24322;&#65292;&#31216;&#20026;&#27169;&#22411;&#19981;&#20844;&#24179;&#24615;&#65292;&#24182;&#33268;&#21147;&#20110;&#31934;&#24515;&#35774;&#35745;&#20248;&#38597;&#30340;&#20307;&#31995;&#32467;&#26500;&#20197;&#35299;&#20915;&#19981;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#36825;&#24102;&#26469;&#20102;&#27785;&#37325;&#30340;&#35757;&#32451;&#36127;&#25285;&#12289;&#36739;&#24046;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FairAdaBN&#65292;&#36890;&#36807;&#20351;&#25209;&#24402;&#19968;&#21270;&#36866;&#24212;&#25935;&#24863;&#23646;&#24615;&#65292;&#21487;&#20197;&#23558;&#20854;&#31616;&#21333;&#32780;&#26377;&#25928;&#22320;&#24212;&#29992;&#21040;&#21407;&#26412;&#19981;&#20102;&#35299;&#20844;&#24179;&#24615;&#30340;&#22810;&#20010;&#20998;&#31867;&#20027;&#24178;&#20013;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#38480;&#21046;&#23567;&#25209;&#37327;&#23376;&#32452;&#20043;&#38388;&#30340;&#32479;&#35745;&#24179;&#34913;&#65292;&#40723;&#21169;&#27169;&#22411;&#20197;&#30456;&#24403;&#20844;&#24179;&#30340;&#26041;&#24335;&#25910;&#25947;&#12290;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#25105;&#20204;&#22312;HAM10000&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#24320;&#25918;&#33719;&#21462;&#30382;&#32932;&#30149;&#25968;&#25454;&#24211;&#65292;&#29992;&#20110;&#20998;&#31867;&#19971;&#31181;&#24120;&#35265;&#30340;&#30382;&#32932;&#30149;&#30149;&#21464;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;FairAdaBN&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#19981;&#20844;&#24179;&#24615;&#65292;&#24182;&#23454;&#29616;&#27169;&#22411;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#24320;&#38144;&#21487;&#24573;&#30053;&#19981;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning is becoming increasingly ubiquitous in medical research and applications while involving sensitive information and even critical diagnosis decisions. Researchers observe a significant performance disparity among subgroups with different demographic attributes, which is called model unfairness, and put lots of effort into carefully designing elegant architectures to address unfairness, which poses heavy training burden, brings poor generalization, and reveals the trade-off between model performance and fairness. To tackle these issues, we propose FairAdaBN by making batch normalization adaptive to sensitive attribute. This simple but effective design can be adopted to several classification backbones that are originally unaware of fairness. Additionally, we derive a novel loss function that restrains statistical parity between subgroups on mini-batches, encouraging the model to converge with considerable fairness. In order to evaluate the trade-off between model performanc
&lt;/p&gt;</description></item><item><title>&#32852;&#37030;&#23398;&#20064;&#26159;&#23454;&#29616;6G&#32593;&#32476;&#20154;&#24037;&#26234;&#33021;&#26222;&#21450;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#20294;&#22312;6G&#32593;&#32476;&#20013;&#65292;&#23384;&#22312;&#19968;&#20123;&#31995;&#32479;&#21644;&#32479;&#35745;&#24322;&#26500;&#24615;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#36825;&#20123;&#24322;&#26500;&#24615;&#38382;&#39064;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.08322</link><description>&lt;p&gt;
&#24322;&#26500;6G&#32593;&#32476;&#20013;&#32852;&#37030;&#23398;&#20064;&#30340;&#20248;&#21270;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Optimization Design for Federated Learning in Heterogeneous 6G Networks. (arXiv:2303.08322v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08322
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#23454;&#29616;6G&#32593;&#32476;&#20154;&#24037;&#26234;&#33021;&#26222;&#21450;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#20294;&#22312;6G&#32593;&#32476;&#20013;&#65292;&#23384;&#22312;&#19968;&#20123;&#31995;&#32479;&#21644;&#32479;&#35745;&#24322;&#26500;&#24615;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#36825;&#20123;&#24322;&#26500;&#24615;&#38382;&#39064;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;5G&#32593;&#32476;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20159;&#19975;&#26234;&#33021;&#29289;&#32852;&#32593;&#35774;&#22791;&#21644;&#24222;&#22823;&#30340;&#25968;&#25454;&#22312;&#32593;&#32476;&#36793;&#32536;&#29983;&#25104;&#12290;&#23613;&#31649;&#20173;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#65292;&#20294;&#39044;&#35745;&#27491;&#22312;&#21457;&#23637;&#20013;&#30340;6G&#32593;&#32476;&#23558;&#37319;&#29992;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#25910;&#38598;&#12289;&#20256;&#36755;&#21644;&#23398;&#20064;&#36825;&#20123;&#23453;&#36149;&#30340;&#25968;&#25454;&#65292;&#20197;&#23454;&#29616;&#21019;&#26032;&#24212;&#29992;&#21644;&#26234;&#33021;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#23558;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#21040;&#25968;&#25454;&#20013;&#24515;&#25110;&#20113;&#20013;&#65292;&#24341;&#21457;&#20102;&#20005;&#37325;&#30340;&#29992;&#25143;&#38544;&#31169;&#38382;&#39064;&#12290;&#20316;&#20026;&#19968;&#31181;&#26032;&#20852;&#30340;&#20998;&#24067;&#24335;&#20154;&#24037;&#26234;&#33021;&#33539;&#20363;&#65292;&#20855;&#26377;&#38544;&#31169;&#20445;&#25252;&#24615;&#36136;&#30340;&#32852;&#37030;&#23398;&#20064;&#34987;&#35270;&#20026;&#23454;&#29616;6G&#32593;&#32476;&#26222;&#36941;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;6G&#32593;&#32476;&#20013;&#26377;&#25928;&#12289;&#39640;&#25928;&#22320;&#23454;&#29616;&#32852;&#37030;&#23398;&#20064;&#20173;&#23384;&#22312;&#19968;&#20123;&#31995;&#32479;&#21644;&#32479;&#35745;&#24322;&#26500;&#24615;&#25361;&#25112;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#36825;&#20123;&#24322;&#26500;&#24615;&#38382;&#39064;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid advancement of 5G networks, billions of smart Internet of Things (IoT) devices along with an enormous amount of data are generated at the network edge. While still at an early age, it is expected that the evolving 6G network will adopt advanced artificial intelligence (AI) technologies to collect, transmit, and learn this valuable data for innovative applications and intelligent services. However, traditional machine learning (ML) approaches require centralizing the training data in the data center or cloud, raising serious user-privacy concerns. Federated learning, as an emerging distributed AI paradigm with privacy-preserving nature, is anticipated to be a key enabler for achieving ubiquitous AI in 6G networks. However, there are several system and statistical heterogeneity challenges for effective and efficient FL implementation in 6G networks. In this article, we investigate the optimization approaches that can effectively address the challenging heterogeneity issues
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SegPrompt&#65292;&#21487;&#20197;&#20351;&#29992;&#20998;&#21106;&#22270;&#21152;&#24378;&#20998;&#31867;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#21516;&#26102;&#20351;&#29992;&#20998;&#21106;&#22270;&#26469;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#27169;&#22411;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20998;&#31867;&#24615;&#33021;&#65292;&#21516;&#26102;&#38656;&#35201;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#23569;&#24471;&#22810;&#30340;&#27880;&#37322;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2303.08303</link><description>&lt;p&gt;
SegPrompt: &#20351;&#29992;&#20998;&#21106;&#22270;&#20316;&#20026;&#26356;&#22909;&#30340;&#25552;&#31034;&#26469;&#24494;&#35843;&#28145;&#24230;&#27169;&#22411;&#29992;&#20110;&#32958;&#32467;&#30707;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
SegPrompt: Using Segmentation Map as a Better Prompt to Finetune Deep Models for Kidney Stone Classification. (arXiv:2303.08303v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SegPrompt&#65292;&#21487;&#20197;&#20351;&#29992;&#20998;&#21106;&#22270;&#21152;&#24378;&#20998;&#31867;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#21516;&#26102;&#20351;&#29992;&#20998;&#21106;&#22270;&#26469;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#27169;&#22411;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20998;&#31867;&#24615;&#33021;&#65292;&#21516;&#26102;&#38656;&#35201;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#23569;&#24471;&#22810;&#30340;&#27880;&#37322;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#22312;&#20351;&#29992;&#20869;&#31397;&#38236;&#22270;&#20687;&#36827;&#34892;&#32958;&#32467;&#30707;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#27880;&#37322;&#30340;&#35757;&#32451;&#25968;&#25454;&#23545;&#20110;&#25913;&#21892;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#26500;&#25104;&#20102;&#20005;&#37325;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#20805;&#20998;&#21033;&#29992;&#25163;&#22836;&#26377;&#38480;&#30340;&#25968;&#25454;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SegPrompt&#65292;&#36890;&#36807;&#20174;&#20004;&#20010;&#26041;&#38754;&#21033;&#29992;&#20998;&#21106;&#22270;&#26469;&#32531;&#35299;&#25968;&#25454;&#30701;&#32570;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;SegPrompt&#25972;&#21512;&#20102;&#20998;&#21106;&#22270;&#20197;&#20415;&#20110;&#20998;&#31867;&#35757;&#32451;&#65292;&#20351;&#20998;&#31867;&#27169;&#22411;&#33021;&#22815;&#24847;&#35782;&#21040;&#24863;&#20852;&#36259;&#30340;&#21306;&#22495;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20801;&#35768;&#22270;&#20687;&#21644;&#20998;&#21106;&#20196;&#29260;&#30456;&#20114;&#20132;&#20114;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#20998;&#21106;&#22270;&#20449;&#24687;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#20998;&#21106;&#22270;&#20316;&#20026;&#25552;&#31034;&#26469;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#27169;&#22411;&#65292;&#32467;&#26524;&#21487;&#35757;&#32451;&#21442;&#25968;&#27604;&#39321;&#33609;&#24494;&#35843;&#23569;&#24471;&#22810;&#12290;&#25105;&#20204;&#22312;&#25910;&#38598;&#30340;&#32958;&#32467;&#30707;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;SegPrompt&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#65292;&#21516;&#26102;&#38656;&#35201;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#23569;&#24471;&#22810;&#30340;&#27880;&#37322;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, deep learning has produced encouraging results for kidney stone classification using endoscope images. However, the shortage of annotated training data poses a severe problem in improving the performance and generalization ability of the trained model. It is thus crucial to fully exploit the limited data at hand. In this paper, we propose SegPrompt to alleviate the data shortage problems by exploiting segmentation maps from two aspects. First, SegPrompt integrates segmentation maps to facilitate classification training so that the classification model is aware of the regions of interest. The proposed method allows the image and segmentation tokens to interact with each other to fully utilize the segmentation map information. Second, we use the segmentation maps as prompts to tune the pretrained deep model, resulting in much fewer trainable parameters than vanilla finetuning. We perform extensive experiments on the collected kidney stone dataset. The results show that SegPromp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#25968;&#19975;&#20010;&#38646;-shot&#23454;&#39564;&#23545;&#22522;&#20110;&#21518;&#35757;&#32451;&#37327;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#21516;&#37327;&#21270;&#32452;&#20214;&#36827;&#34892;&#20102;&#32508;&#21512;&#30740;&#31350;&#65292;&#32467;&#26524;&#21457;&#29616;&#32454;&#31890;&#24230;&#37327;&#21270;&#21644;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#24456;&#37325;&#35201;&#65292;&#29992;&#31895;&#31890;&#24230;&#37327;&#21270;&#30340;&#26356;&#39640;&#20301;&#25968;&#27604;&#29992;&#38750;&#24120;&#32454;&#31890;&#24230;&#30340;&#26356;&#20302;&#20301;&#25968;&#26356;&#24378;&#22823;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#22914;&#20309;&#20026;&#19981;&#21516;&#22823;&#23567;&#30340;\llms&#21033;&#29992;&#37327;&#21270;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2303.08302</link><description>&lt;p&gt;
&#22522;&#20110;&#21518;&#35757;&#32451;&#37327;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#21512;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study on Post-Training Quantization for Large Language Models. (arXiv:2303.08302v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#25968;&#19975;&#20010;&#38646;-shot&#23454;&#39564;&#23545;&#22522;&#20110;&#21518;&#35757;&#32451;&#37327;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#21516;&#37327;&#21270;&#32452;&#20214;&#36827;&#34892;&#20102;&#32508;&#21512;&#30740;&#31350;&#65292;&#32467;&#26524;&#21457;&#29616;&#32454;&#31890;&#24230;&#37327;&#21270;&#21644;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#24456;&#37325;&#35201;&#65292;&#29992;&#31895;&#31890;&#24230;&#37327;&#21270;&#30340;&#26356;&#39640;&#20301;&#25968;&#27604;&#29992;&#38750;&#24120;&#32454;&#31890;&#24230;&#30340;&#26356;&#20302;&#20301;&#25968;&#26356;&#24378;&#22823;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#22914;&#20309;&#20026;&#19981;&#21516;&#22823;&#23567;&#30340;\llms&#21033;&#29992;&#37327;&#21270;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;&#26159;&#19968;&#31181;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#23384;&#28040;&#32791;&#21644;/&#25110;&#35745;&#31639;&#25104;&#26412;&#30340;&#26435;&#34913;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#19981;&#21516;&#37327;&#21270;&#26041;&#26696;&#12289;&#19981;&#21516;&#27169;&#22411;&#26063;&#12289;&#19981;&#21516;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#12289;&#19981;&#21516;&#37327;&#21270;&#20301;&#31934;&#24230;&#31561;&#30340;&#24433;&#21709;&#30340;&#20840;&#38754;&#30740;&#31350;&#20173;&#32570;&#22833;&#12290;&#26412;&#25991;&#36890;&#36807;&#25968;&#19975;&#20010;&#38646;-shot&#23454;&#39564;&#23545;&#36825;&#20123;&#32452;&#20214;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65306;(1)&#32454;&#31890;&#24230;&#37327;&#21270;&#21644;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;(&#32780;&#19981;&#26159;&#26420;&#32032;&#30340;&#26368;&#36817;&#33293;&#20837;&#37327;&#21270;)&#26159;&#23454;&#29616;&#33391;&#22909;&#31934;&#24230;&#30340;&#24517;&#35201;&#26465;&#20214;&#65307;(2) &#29992;&#31895;&#31890;&#24230;&#37327;&#21270;&#30340;&#26356;&#39640;&#20301;&#25968;&#65288;&#22914;5&#20301;&#65289;&#27604;&#29992;&#38750;&#24120;&#32454;&#31890;&#24230;&#30340;&#26356;&#20302;&#20301;&#25968;&#65288;&#22914;4&#20301;&#65289;&#65288;&#20854;&#26377;&#25928;&#20301;&#25968;&#19982;5&#20301;&#30456;&#20284;&#65289;&#26356;&#24378;&#22823;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#22914;&#20309;&#20026;&#19981;&#21516;&#22823;&#23567;&#30340;\llms&#21033;&#29992;&#37327;&#21270;&#30340;&#24314;&#35758;&#65292;&#24182;&#30041;&#19979;&#26410;&#26469;&#26426;&#20250;&#21644;&#31995;&#32479;&#24037;&#20316;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization (\ptq) had been recently shown as a compromising method to reduce the memory consumption and/or compute cost for large language models. However, a comprehensive study about the effect of different quantization schemes, different model families, different \ptq methods, different quantization bit precision, etc, is still missing. In this work, we provide an extensive study on those components over tens of thousands of zero-shot experiments. Our results show that (1) Fine-grained quantization and \ptq methods (instead of naive round-to-nearest quantization) are necessary to achieve good accuracy and (2) Higher bits (e.g., 5 bits) with coarse-grained quantization is more powerful than lower bits (e.g., 4 bits) with very fine-grained quantization (whose effective bits is similar to 5-bits). We also present recommendations about how to utilize quantization for \llms with different sizes, and leave suggestions of future opportunities and system work that are not res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#29305;&#24449;&#24037;&#31243;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#32593;&#32476;&#29289;&#29702;&#30005;&#21147;&#31995;&#32479;&#25968;&#25454;&#36136;&#37327;&#12289;&#35745;&#31639;&#25104;&#26412;&#21644;&#20887;&#20313;&#27979;&#37327;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25925;&#38556;&#35786;&#26029;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.08300</link><description>&lt;p&gt;
&#23398;&#20064;&#39640;&#32500;&#32593;&#32476;&#29289;&#29702;&#25968;&#25454;&#27969;&#29992;&#20110;&#26234;&#33021;&#30005;&#32593;&#30340;&#25925;&#38556;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Learning From High-Dimensional Cyber-Physical Data Streams for Diagnosing Faults in Smart Grids. (arXiv:2303.08300v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#29305;&#24449;&#24037;&#31243;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#32593;&#32476;&#29289;&#29702;&#30005;&#21147;&#31995;&#32479;&#25968;&#25454;&#36136;&#37327;&#12289;&#35745;&#31639;&#25104;&#26412;&#21644;&#20887;&#20313;&#27979;&#37327;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25925;&#38556;&#35786;&#26029;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25925;&#38556;&#35786;&#26029;&#31995;&#32479;&#30340;&#24615;&#33021;&#21463;&#32593;&#32476;&#29289;&#29702;&#30005;&#21147;&#31995;&#32479;&#25968;&#25454;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#31995;&#32479;&#20135;&#29983;&#22823;&#37327;&#25968;&#25454;&#65292;&#20351;&#31995;&#32479;&#25215;&#21463;&#36807;&#22810;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#21478;&#19968;&#20010;&#38382;&#39064;&#26159;&#35760;&#24405;&#27979;&#37327;&#20013;&#23384;&#22312;&#22122;&#22768;&#65292;&#36825;&#21487;&#20197;&#38450;&#27490;&#26500;&#24314;&#31934;&#30830;&#30340;&#20915;&#31574;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#35786;&#26029;&#27169;&#22411;&#36890;&#24120;&#37197;&#22791;&#20102;&#19968;&#32452;&#20887;&#20313;&#27979;&#37327;&#25968;&#25454;&#65292;&#21487;&#33021;&#20559;&#31163;&#27491;&#24120;&#21644;&#25925;&#38556;&#20998;&#24067;&#30340;&#23398;&#20064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#29305;&#24449;&#24037;&#31243;&#23545;&#20110;&#32531;&#35299;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#20013;&#19978;&#36848;&#25361;&#25112;&#30340;&#24433;&#21709;&#12290;&#29305;&#24449;&#36873;&#25321;&#21644;&#38477;&#32500;&#26041;&#27861;&#19982;&#20915;&#31574;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#27169;&#25311;&#20102;&#23545;118&#20010;&#24635;&#32447;&#30005;&#21147;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#39537;&#21160;&#25925;&#38556;&#35786;&#26029;&#12290;&#22240;&#27492;&#65292;&#24320;&#23637;&#20102;&#27604;&#36739;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#20004;&#20010;&#39046;&#22495;&#20013;&#30340;&#20960;&#31181;&#20808;&#36827;&#25216;&#26415;&#12290;&#21516;&#26102;&#27604;&#36739;&#20102;&#38477;&#32500;&#21644;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of fault diagnosis systems is highly affected by data quality in cyber-physical power systems. These systems generate massive amounts of data that overburden the system with excessive computational costs. Another issue is the presence of noise in recorded measurements, which prevents building a precise decision model. Furthermore, the diagnostic model is often provided with a mixture of redundant measurements that may deviate it from learning normal and fault distributions. This paper presents the effect of feature engineering on mitigating the aforementioned challenges in cyber-physical systems. Feature selection and dimensionality reduction methods are combined with decision models to simulate data-driven fault diagnosis in a 118-bus power system. A comparative study is enabled accordingly to compare several advanced techniques in both domains. Dimensionality reduction and feature selection methods are compared both jointly and separately. Finally, experiments are con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#23558;&#22238;&#25910;&#20877;&#21033;&#29992;&#26448;&#26009;&#29992;&#20110;&#25935;&#25463;&#21046;&#36896;&#65292;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#20998;&#26512;&#65292;&#20197;&#20915;&#31574;&#25903;&#25345;&#23454;&#29616;&#29615;&#22659;&#21487;&#25345;&#32493;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.08291</link><description>&lt;p&gt;
&#22312;&#22238;&#25910;&#26448;&#26009;&#21487;&#25345;&#32493;&#21270;&#30340;&#25935;&#25463;&#21046;&#36896;&#20013;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;(arXiv:2303.08291v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
Machine Learning Approaches in Agile Manufacturing with Recycled Materials for Sustainability. (arXiv:2303.08291v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#23558;&#22238;&#25910;&#20877;&#21033;&#29992;&#26448;&#26009;&#29992;&#20110;&#25935;&#25463;&#21046;&#36896;&#65292;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#20998;&#26512;&#65292;&#20197;&#20915;&#31574;&#25903;&#25345;&#23454;&#29616;&#29615;&#22659;&#21487;&#25345;&#32493;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26448;&#26009;&#31185;&#23398;&#21644;&#21046;&#36896;&#19994;&#20013;&#24320;&#21457;&#21487;&#25345;&#32493;&#30340;&#12289;&#29615;&#20445;&#30340;&#36807;&#31243;&#38750;&#24120;&#37325;&#35201;&#12290;&#20154;&#24037;&#26234;&#33021;&#22312;&#20915;&#31574;&#25903;&#25345;&#26041;&#38754;&#21487;&#20197;&#36215;&#21040;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#36825;&#19968;&#28857;&#24050;&#32463;&#22312;&#25105;&#20204;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#24471;&#20197;&#35777;&#26126;&#65292;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#24320;&#21457;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#29992;&#20110;&#35745;&#31639;&#20272;&#35745;&#21644;&#19987;&#23478;&#31995;&#32479;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20915;&#31574;&#25903;&#25345;&#23454;&#29616;&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#29615;&#22659;&#21487;&#25345;&#32493;&#24615;&#65292;&#24212;&#29992;&#22238;&#25910;&#21644;&#20877;&#29983;&#26448;&#26009;&#36827;&#34892;&#25935;&#25463;&#21046;&#36896;&#65292;&#36825;&#26159;&#19968;&#31181;&#23433;&#20840;&#12289;&#36127;&#36131;&#20219;&#30340;&#23558;&#29305;&#23450;&#24223;&#24323;&#29289;&#36716;&#21464;&#25104;&#22686;&#20540;&#20135;&#21697;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;AI&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#20998;&#26512;&#65292;&#20197;&#25351;&#23548;&#21046;&#36896;&#20013;&#30340;&#20915;&#31574;&#25903;&#25345;&#65292;&#21253;&#25324;&#21033;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#24433;&#21709;&#26448;&#26009;&#28909;&#22788;&#29702;&#21450;&#20854;&#24615;&#33021;&#30340;&#21442;&#25968;&#65292;&#20197;&#21450;&#36890;&#36807;&#20808;&#36827;&#25216;&#26415;&#65288;&#22914;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65289;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#65292;&#26469;&#25506;&#32034;&#31890;&#24230;&#22823;&#23567;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is important to develop sustainable processes in materials science and manufacturing that are environmentally friendly. AI can play a significant role in decision support here as evident from our earlier research leading to tools developed using our proposed machine learning based approaches. Such tools served the purpose of computational estimation and expert systems. This research addresses environmental sustainability in materials science via decision support in agile manufacturing using recycled and reclaimed materials. It is a safe and responsible way to turn a specific waste stream to value-added products. We propose to use data-driven methods in AI by applying machine learning models for predictive analysis to guide decision support in manufacturing. This includes harnessing artificial neural networks to study parameters affecting heat treatment of materials and impacts on their properties; deep learning via advances such as convolutional neural networks to explore grain size
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#65292;CNN&#22312;&#20581;&#24247;&#35760;&#24405;&#25991;&#26412;&#32534;&#30721;&#26041;&#38754;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#38544;&#21547;&#23618;&#27425;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN&#30340;&#32534;&#30721;&#22120;&#26469;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;EHR&#29305;&#24449;&#65292;&#24182;&#22312;&#20020;&#24202;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.08290</link><description>&lt;p&gt;
&#37325;&#26032;&#21457;&#29616;CNN&#22312;&#21407;&#22987;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25991;&#26412;&#32534;&#30721;&#20013;&#30340;&#22810;&#21151;&#33021;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rediscovery of CNN's Versatility for Text-based Encoding of Raw Electronic Health Records. (arXiv:2303.08290v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#65292;CNN&#22312;&#20581;&#24247;&#35760;&#24405;&#25991;&#26412;&#32534;&#30721;&#26041;&#38754;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#38544;&#21547;&#23618;&#27425;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN&#30340;&#32534;&#30721;&#22120;&#26469;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;EHR&#29305;&#24449;&#65292;&#24182;&#22312;&#20020;&#24202;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20805;&#20998;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#20016;&#23500;&#30340;&#20449;&#24687;&#27491;&#36880;&#28176;&#25104;&#20026;&#21307;&#23398;&#39046;&#22495;&#30340;&#37325;&#35201;&#35805;&#39064;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#22312;&#19981;&#32771;&#34385;&#20854;&#26684;&#24335;&#21644;&#21307;&#23398;&#32534;&#30721;&#26631;&#20934;&#30340;&#24773;&#20917;&#19979;&#23884;&#20837;&#21407;&#22987;EHR&#25968;&#25454;&#30340;&#25972;&#20010;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#35813;&#26694;&#26550;&#20165;&#20391;&#37325;&#20110;&#23545;EHR&#36827;&#34892;&#26368;&#23567;&#30340;&#39044;&#22788;&#29702;&#65292;&#26410;&#32771;&#34385;&#22914;&#20309;&#23398;&#20064;&#39640;&#25928;&#30340;EHR&#34920;&#31034;&#65292;&#21253;&#25324;&#35745;&#31639;&#21644;&#20869;&#23384;&#20351;&#29992;&#31561;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23547;&#25214;&#19968;&#31181;&#22810;&#21151;&#33021;&#30340;&#32534;&#30721;&#22120;&#65292;&#19981;&#20165;&#23558;&#22823;&#37327;&#25968;&#25454;&#32553;&#23567;&#21040;&#21487;&#31649;&#29702;&#30340;&#22823;&#23567;&#65292;&#36824;&#33021;&#24456;&#22909;&#22320;&#20445;&#30041;&#24739;&#32773;&#30340;&#26680;&#24515;&#20449;&#24687;&#65292;&#20197;&#25191;&#34892;&#21508;&#31181;&#20020;&#24202;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20855;&#26377;&#20998;&#23618;&#32467;&#26500;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#65288;&#22914;&#37325;&#24314;&#65292;&#39044;&#27979;&#21644;&#29983;&#25104;&#65289;&#20013;&#32463;&#24120;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#21363;&#20351;&#21442;&#25968;&#36739;&#23569;&#19988;&#35757;&#32451;&#26102;&#38388;&#36739;&#30701;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;EHR&#25968;&#25454;&#30340;&#22266;&#26377;&#23618;&#27425;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;CNN&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#20123;&#21457;&#29616;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN&#30340;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;EHR&#29305;&#24449;&#65292;&#24182;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#20960;&#31181;&#20020;&#24202;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Making the most use of abundant information in electronic health records (EHR) is rapidly becoming an important topic in the medical domain. Recent work presented a promising framework that embeds entire features in raw EHR data regardless of its form and medical code standards. The framework, however, only focuses on encoding EHR with minimal preprocessing and fails to consider how to learn efficient EHR representation in terms of computation and memory usage. In this paper, we search for a versatile encoder not only reducing the large data into a manageable size but also well preserving the core information of patients to perform diverse clinical tasks. We found that hierarchically structured Convolutional Neural Network (CNN) often outperforms the state-of-the-art model on diverse tasks such as reconstruction, prediction, and generation, even with fewer parameters and less training time. Moreover, it turns out that making use of the inherent hierarchy of EHR data can boost the perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#35282;&#24230;-AT&#65292;&#32467;&#21512;&#36229;&#29699;&#23884;&#20837;&#21644;&#22522;&#20110;&#35282;&#24230;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#20197;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.08289</link><description>&lt;p&gt;
&#36890;&#36807;&#36229;&#29699;&#23884;&#20837;&#21644;&#22522;&#20110;&#35282;&#24230;&#30340;&#27491;&#21017;&#21270;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Adversarial Robustness with Hypersphere Embedding and Angular-based Regularizations. (arXiv:2303.08289v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#35282;&#24230;-AT&#65292;&#32467;&#21512;&#36229;&#29699;&#23884;&#20837;&#21644;&#22522;&#20110;&#35282;&#24230;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#20197;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#25915;&#20987;&#20855;&#26377;&#19968;&#23450;&#25928;&#26524;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;AT&#30340;&#21464;&#20307;&#26469;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290; Pang&#31561;&#20154;&#26368;&#36817;&#34920;&#26126;&#65292;&#23558;&#36229;&#29699;&#23884;&#20837;&#65288;HE&#65289;&#32435;&#20837;&#29616;&#26377;&#30340;AT&#36807;&#31243;&#20013;&#21487;&#20197;&#25552;&#39640;&#23545;&#25239;&#24615;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#29616;&#26377;&#30340;AT&#36807;&#31243;&#24182;&#19981;&#26159;&#20026;HE&#26694;&#26550;&#35774;&#35745;&#30340;&#65292;&#22240;&#27492;&#26410;&#33021;&#20805;&#20998;&#23398;&#20064;HE&#26694;&#26550;&#20013;&#30340;&#35282;&#24230;&#21028;&#21035;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;HE&#19982;&#21033;&#29992;HE&#26694;&#26550;&#20013;&#30340;&#20016;&#23500;&#35282;&#24230;&#20449;&#24687;&#30340;&#27491;&#21017;&#21270;&#39033;&#30446;&#30456;&#32467;&#21512;&#65292;&#23558;&#20854;&#38598;&#25104;&#21040;AT&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#31216;&#20026;&#35282;&#24230;AT&#65292;&#23558;&#27491;&#21017;&#21270;&#39033;&#30446;&#28155;&#21152;&#21040;AT&#20013;&#65292;&#24182;&#36890;&#36807;&#35282;&#24230;&#29305;&#24449;&#26126;&#30830;&#24378;&#21046;&#26435;&#37325;&#29305;&#24449;&#32039;&#20945;&#24615;&#21644;&#31867;&#38388;&#20998;&#38548;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35282;&#24230;AT&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training (AT) methods have been found to be effective against adversarial attacks on deep neural networks. Many variants of AT have been proposed to improve its performance. Pang et al. [1] have recently shown that incorporating hypersphere embedding (HE) into the existing AT procedures enhances robustness. We observe that the existing AT procedures are not designed for the HE framework, and thus fail to adequately learn the angular discriminative information available in the HE framework. In this paper, we propose integrating HE into AT with regularization terms that exploit the rich angular information available in the HE framework. Specifically, our method, termed angular-AT, adds regularization terms to AT that explicitly enforce weight-feature compactness and inter-class separation; all expressed in terms of angular features. Experimental results show that angular-AT further improves adversarial robustness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;Transformer&#20013;&#26631;&#35760;&#21487;&#33021;&#24615;&#21644;&#27880;&#24847;&#21147;&#20540;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#25581;&#31034;&#20102;&#22312;&#36935;&#21040;&#24847;&#22806;&#26631;&#35760;&#26102;&#27169;&#22411;&#20851;&#27880;&#36739;&#23569;&#30340;&#20449;&#24687;&#65292;&#23545;&#20110;&#35780;&#20272;LLMs&#22312;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#20855;&#26377;&#26377;&#20215;&#20540;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.08288</link><description>&lt;p&gt;
Transformer&#20013;&#30340;&#27880;&#24847;&#21147;-&#21487;&#33021;&#24615;&#20851;&#31995;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Attention-likelihood relationship in transformers. (arXiv:2303.08288v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;Transformer&#20013;&#26631;&#35760;&#21487;&#33021;&#24615;&#21644;&#27880;&#24847;&#21147;&#20540;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#25581;&#31034;&#20102;&#22312;&#36935;&#21040;&#24847;&#22806;&#26631;&#35760;&#26102;&#27169;&#22411;&#20851;&#27880;&#36739;&#23569;&#30340;&#20449;&#24687;&#65292;&#23545;&#20110;&#35780;&#20272;LLMs&#22312;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#20855;&#26377;&#26377;&#20215;&#20540;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;&#20309;&#34920;&#31034;&#19978;&#19979;&#25991;&#20043;&#22806;&#30340;&#21333;&#35789;&#65292;&#24182;&#35843;&#26597;&#23427;&#20204;&#23545;&#32473;&#23450;&#19978;&#19979;&#25991;&#26469;&#25429;&#25417;&#35821;&#20041;&#30340;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#30340;&#21487;&#33021;&#24615;&#24341;&#23548;&#30340;&#25991;&#26412;&#25200;&#21160;&#25581;&#31034;&#20102;&#22522;&#20110;transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#26631;&#35760;&#21487;&#33021;&#24615;&#21644;&#27880;&#24847;&#21147;&#20540;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#21457;&#29616;&#65292;&#22312;&#26356;&#39640;&#23618;&#29305;&#21035;&#26159;&#36935;&#21040;&#24847;&#22806;&#30340;&#26631;&#35760;&#26102;&#65292;&#27169;&#22411;&#20250;&#20851;&#27880;&#36739;&#23569;&#30340;&#26469;&#33258;&#33258;&#36523;&#30340;&#20449;&#24687;&#26469;&#35745;&#31639;&#23427;&#20204;&#30340;&#34920;&#31034;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#35780;&#20272;LLMs&#22312;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#20855;&#26377;&#26377;&#20215;&#20540;&#30340;&#24433;&#21709;&#12290;&#22312;https://github.com/Flegyas/AttentionLikelihood&#20013;&#26377;&#23436;&#20840;&#21487;&#37325;&#29616;&#30340;&#20195;&#30721;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze how large language models (LLMs) represent out-of-context words, investigating their reliance on the given context to capture their semantics. Our likelihood-guided text perturbations reveal a correlation between token likelihood and attention values in transformer-based language models. Extensive experiments reveal that unexpected tokens cause the model to attend less to the information coming from themselves to compute their representations, particularly at higher layers. These findings have valuable implications for assessing the robustness of LLMs in real-world scenarios. Fully reproducible codebase at https://github.com/Flegyas/AttentionLikelihood.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20511;&#21161;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25506;&#31350;&#26367;&#20195;&#29123;&#26009;&#27773;&#36710;&#30340;&#26222;&#21450;&#65292;&#21516;&#26102;&#23558;&#20854;&#19982;&#28040;&#36153;&#32773;&#30340;&#31038;&#20250;&#32463;&#27982;&#22320;&#20301;&#21644;&#31354;&#27668;&#36136;&#37327;&#25351;&#25968;&#36827;&#34892;&#20851;&#32852;&#65292;&#20174;&#32780;&#21046;&#23450;&#21512;&#36866;&#30340;&#25919;&#31574;&#12290;</title><link>http://arxiv.org/abs/2303.08286</link><description>&lt;p&gt;
&#23558;&#26367;&#20195;&#29123;&#26009;&#27773;&#36710;&#30340;&#37319;&#29992;&#19982;&#31038;&#20250;&#32463;&#27982;&#22320;&#20301;&#21644;&#31354;&#27668;&#36136;&#37327;&#25351;&#25968;&#32852;&#31995;&#36215;&#26469;
&lt;/p&gt;
&lt;p&gt;
Linking Alternative Fuel Vehicles Adoption with Socioeconomic Status and Air Quality Index. (arXiv:2303.08286v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08286
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20511;&#21161;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25506;&#31350;&#26367;&#20195;&#29123;&#26009;&#27773;&#36710;&#30340;&#26222;&#21450;&#65292;&#21516;&#26102;&#23558;&#20854;&#19982;&#28040;&#36153;&#32773;&#30340;&#31038;&#20250;&#32463;&#27982;&#22320;&#20301;&#21644;&#31354;&#27668;&#36136;&#37327;&#25351;&#25968;&#36827;&#34892;&#20851;&#32852;&#65292;&#20174;&#32780;&#21046;&#23450;&#21512;&#36866;&#30340;&#25919;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#39033;&#30740;&#31350;&#65292;&#30740;&#31350;&#26367;&#20195;&#29123;&#26009;&#27773;&#36710;&#30340;&#28508;&#22312;&#24191;&#27867;&#20351;&#29992;&#65292;&#23558;&#23427;&#20204;&#19982;&#30456;&#24212;&#28040;&#36153;&#32773;&#30340;&#31038;&#20250;&#32463;&#27982;&#22320;&#20301;&#20197;&#21450;&#23545; resulting &#31354;&#27668;&#36136;&#37327;&#25351;&#25968;&#30340;&#24433;&#21709;&#32852;&#31995;&#36215;&#26469;&#12290;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20197;&#20419;&#36827;&#36866;&#24403;&#30340;&#25919;&#31574;&#65292;&#25512;&#24191;&#26367;&#20195;&#29123;&#26009;&#27773;&#36710;&#30340;&#26222;&#21450;&#65292;&#20363;&#22914;&#30005;&#21160;&#27773;&#36710;&#65292;&#35201;&#20844;&#27491;&#23545;&#24453;&#19981;&#21516;&#30340;&#20154;&#32676;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992; Pearson &#30456;&#20851;&#31995;&#25968;&#23545;&#31038;&#20250;&#32463;&#27982;&#25968;&#25454;&#12289;&#31354;&#27668;&#36136;&#37327;&#25351;&#25968;&#21644;&#26367;&#20195;&#29123;&#26009;&#27773;&#36710;&#30340;&#25968;&#25454;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;&#22522;&#20110;&#31038;&#20250;&#32463;&#27982;&#22240;&#32032;&#65292;&#20351;&#29992;&#32447;&#24615;&#22238;&#24402;&#23545;&#31354;&#27668;&#36136;&#37327;&#25351;&#25968;&#36827;&#34892;&#39044;&#27979;&#24314;&#27169;&#65292;&#20197;&#26681;&#25454;&#26367;&#20195;&#29123;&#26009;&#27773;&#36710;&#30340;&#37319;&#29992;&#24773;&#20917;&#36827;&#34892;&#35843;&#25972;&#12290;&#36825;&#39033;&#24037;&#20316;&#35777;&#26126;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#31038;&#20250;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
This is a study on the potential widespread usage of alternative fuel vehicles, linking them with the socio-economic status of the respective consumers as well as the impact on the resulting air quality index. Research in this area aims to leverage machine learning techniques in order to promote appropriate policies for the proliferation of alternative fuel vehicles such as electric vehicles with due justice to different population groups. Pearson correlation coefficient is deployed in the modeling the relationships between socio-economic data, air quality index and data on alternative fuel vehicles. Linear regression is used to conduct predictive modeling on air quality index as per the adoption of alternative fuel vehicles, based on socio-economic factors. This work exemplifies artificial intelligence for social good.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22312;UAE&#37096;&#32626;&#30340;&#30140;&#30171;&#32423;&#21035;&#26816;&#27979;&#26694;&#26550;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#33021;&#22815;&#20934;&#30830;&#22320;&#35782;&#21035;&#30140;&#30171;&#32423;&#21035;&#65292;&#26377;&#21161;&#20110;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#21307;&#30103;&#35745;&#21010;&#12290;</title><link>http://arxiv.org/abs/2303.08273</link><description>&lt;p&gt;
&#38754;&#21521;UAE&#30340;&#28145;&#24230;&#23398;&#20064;&#30140;&#30171;&#32423;&#21035;&#26816;&#27979;&#37096;&#32626;&#65306;&#20026;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#30140;&#30171;&#31649;&#29702;&#21644;&#35786;&#26029;&#25903;&#25345;&#22880;&#23450;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Towards a Deep Learning Pain-Level Detection Deployment at UAE for Patient-Centric-Pain Management and Diagnosis Support: Framework and Performance Evaluation. (arXiv:2303.08273v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22312;UAE&#37096;&#32626;&#30340;&#30140;&#30171;&#32423;&#21035;&#26816;&#27979;&#26694;&#26550;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#33021;&#22815;&#20934;&#30830;&#22320;&#35782;&#21035;&#30140;&#30171;&#32423;&#21035;&#65292;&#26377;&#21161;&#20110;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#21307;&#30103;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#22823;&#27969;&#34892;&#29190;&#21457;&#25581;&#31034;&#20102;&#21450;&#26102;&#24178;&#39044;&#30340;&#37325;&#35201;&#24615;&#65292;&#32780;&#30701;&#32570;&#30340;&#21307;&#30103;&#20154;&#21592;&#21644;&#35774;&#22791;&#20351;&#24773;&#20917;&#26356;&#21152;&#24694;&#21270;&#12290;&#26816;&#27979;&#30140;&#30171;&#32423;&#21035;&#26159;&#30830;&#23450;&#24739;&#32773;&#30149;&#24773;&#20005;&#37325;&#31243;&#24230;&#30340;&#39318;&#35201;&#27493;&#39588;&#12290;&#33258;&#21160;&#35782;&#21035;&#29366;&#24577;&#21644;&#24863;&#21463;&#26377;&#21161;&#20110;&#35782;&#21035;&#24739;&#32773;&#30340;&#30151;&#29366;&#65292;&#20197;&#21450;&#37319;&#21462;&#31435;&#21363;&#24688;&#24403;&#30340;&#34892;&#21160;&#65292;&#24182;&#25552;&#20379;&#29305;&#23450;&#20110;&#24739;&#32773;&#29366;&#24577;&#30340;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#21307;&#30103;&#35745;&#21010;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30140;&#30171;&#32423;&#21035;&#26816;&#27979;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#38463;&#25289;&#20271;&#32852;&#21512;&#37195;&#38271;&#22269;&#37096;&#32626;&#65292;&#24182;&#20351;&#29992;&#25991;&#29486;&#20013;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#37096;&#32626;&#30140;&#30171;&#32423;&#21035;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#26694;&#26550;&#22312;&#20934;&#30830;&#35782;&#21035;&#30140;&#30171;&#32423;&#21035;&#26041;&#38754;&#26159;&#26377;&#21069;&#36884;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The outbreak of the COVID-19 pandemic revealed the criticality of timely intervention in a situation exacerbated by a shortage in medical staff and equipment. Pain-level screening is the initial step toward identifying the severity of patient conditions. Automatic recognition of state and feelings help in identifying patient symptoms to take immediate adequate action and providing a patient-centric medical plan tailored to a patient's state. In this paper, we propose a framework for pain-level detection for deployment in the United Arab Emirates and assess its performance using the most used approaches in the literature. Our results show that a deployment of a pain-level deep learning detection framework is promising in identifying the pain level accurately.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#33258;&#21160;&#21270;&#31649;&#36947;&#65292;&#20351;&#29992;&#19987;&#21033;&#25968;&#25454;&#28304;&#35757;&#32451;&#39046;&#22495;&#29305;&#23450;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21033;&#29992;&#19987;&#21033;&#20013;&#30340;&#24369;&#26631;&#35760;&#24212;&#29992;&#31867;&#21035;&#20013;&#23613;&#21487;&#33021;&#22810;&#30340;&#20449;&#24687;&#23454;&#29616;&#21270;&#23398;&#31354;&#38388;&#20869;&#29983;&#25104;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2303.08272</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#19987;&#21033;&#25552;&#21462;&#25903;&#25345;&#32858;&#28966;&#21270;&#23398;&#31354;&#38388;&#20869;&#30340;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Automated patent extraction powers generative modeling in focused chemical spaces. (arXiv:2303.08272v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#33258;&#21160;&#21270;&#31649;&#36947;&#65292;&#20351;&#29992;&#19987;&#21033;&#25968;&#25454;&#28304;&#35757;&#32451;&#39046;&#22495;&#29305;&#23450;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21033;&#29992;&#19987;&#21033;&#20013;&#30340;&#24369;&#26631;&#35760;&#24212;&#29992;&#31867;&#21035;&#20013;&#23613;&#21487;&#33021;&#22810;&#30340;&#20449;&#24687;&#23454;&#29616;&#21270;&#23398;&#31354;&#38388;&#20869;&#29983;&#25104;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#24050;&#25104;&#20026;&#21453;&#21521;&#20998;&#23376;&#35774;&#35745;&#30340;&#19968;&#31181;&#20196;&#20154;&#20852;&#22859;&#30340;&#25163;&#27573;&#65292;&#20854;&#36827;&#23637;&#26469;&#33258;&#20110;&#35757;&#32451;&#31639;&#27861;&#21644;&#20998;&#23376;&#34920;&#31034;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#24212;&#29992;&#20110;&#26448;&#26009;&#31185;&#23398;&#21644;&#21270;&#23398;&#39046;&#22495;&#26102;&#65292;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#32570;&#20047;&#20855;&#26377;&#23646;&#24615;&#26631;&#31614;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#24050;&#21457;&#24067;&#30340;&#19987;&#21033;&#21253;&#21547;&#22312;&#20854;&#22312;&#26399;&#21002;&#19978;&#21457;&#34920;&#20043;&#21069;&#25259;&#38706;&#26032;&#26448;&#26009;&#30340;&#20449;&#24687;&#65292;&#26159;&#19968;&#31181;&#30456;&#23545;&#26410;&#34987;&#20805;&#20998;&#21033;&#29992;&#30340;&#31185;&#23398;&#30693;&#35782;&#24191;&#27867;&#26469;&#28304;&#12290;&#30001;&#20110;&#19987;&#21033;&#34987;&#25552;&#20132;&#26159;&#20026;&#20102;&#20445;&#25252;&#29305;&#23450;&#29992;&#36884;&#65292;&#22240;&#27492;&#19987;&#21033;&#20013;&#30340;&#20998;&#23376;&#21487;&#20197;&#34987;&#35270;&#20026;&#24369;&#26631;&#35760;&#30340;&#24212;&#29992;&#31867;&#21035;&#12290;&#27492;&#22806;&#65292;&#30001;&#32654;&#22269;&#19987;&#21033;&#19982;&#21830;&#26631;&#23616;&#65288;USPTO&#65289;&#21457;&#24067;&#30340;&#19987;&#21033;&#20855;&#26377;&#21487;&#19979;&#36733;&#30340;&#26426;&#22120;&#21487;&#35835;&#25991;&#26412;&#21644;&#20998;&#23376;&#32467;&#26500;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#33258;&#21160;&#21270;&#31649;&#36947;&#65292;&#20351;&#29992;&#19987;&#21033;&#25968;&#25454;&#28304;&#35757;&#32451;&#39046;&#22495;&#29305;&#23450;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models have emerged as an exciting avenue for inverse molecular design, with progress coming from the interplay between training algorithms and molecular representations. One of the key challenges in their applicability to materials science and chemistry has been the lack of access to sizeable training datasets with property labels. Published patents contain the first disclosure of new materials prior to their publication in journals, and are a vast source of scientific knowledge that has remained relatively untapped in the field of data-driven molecular design. Because patents are filed seeking to protect specific uses, molecules in patents can be considered to be weakly labeled into application classes. Furthermore, patents published by the US Patent and Trademark Office (USPTO) are downloadable and have machine-readable text and molecular structures. In this work, we train domain-specific generative models using patent data sources by developing an automated pipeline
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#20110;&#20195;&#29702;&#26377;&#30452;&#25509;&#25511;&#21046;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#25910;&#38598;&#20449;&#24687;&#30340;&#33021;&#21147;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#34892;&#21160;&#21518;&#27979;&#37327; (ATM) &#31574;&#30053;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110; ATM &#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22810;&#20010;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.08271</link><description>&lt;p&gt;
Act-Then-Measure: &#24102;&#20027;&#21160;&#27979;&#37327;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Act-Then-Measure: Reinforcement Learning for Partially Observable Environments with Active Measuring. (arXiv:2303.08271v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#20110;&#20195;&#29702;&#26377;&#30452;&#25509;&#25511;&#21046;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#25910;&#38598;&#20449;&#24687;&#30340;&#33021;&#21147;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#34892;&#21160;&#21518;&#27979;&#37327; (ATM) &#31574;&#30053;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110; ATM &#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22810;&#20010;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243; (MDPs)&#65292;&#20854;&#20013;&#20195;&#29702;&#26377;&#30452;&#25509;&#25511;&#21046;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#25910;&#38598;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#22914; action-contingent noiselessly observable MDPs (ACNO-MPDs) &#25152;&#24418;&#24335;&#21270;&#12290;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#65292;&#21160;&#20316;&#30001;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#24433;&#21709;&#29615;&#22659;&#30340;&#25511;&#21046;&#21160;&#20316;&#21644;&#24433;&#21709;&#20195;&#29702;&#21487;&#20197;&#35266;&#23519;&#21040;&#20160;&#20040;&#30340;&#27979;&#37327;&#21160;&#20316;&#12290;&#20026;&#20102;&#35299;&#20915; ACNO-MDPs&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#34892;&#21160;&#21518;&#27979;&#37327; (ATM) &#31574;&#30053;&#65292;&#23427;&#20551;&#35774;&#22312;&#36873;&#25321;&#25511;&#21046;&#21160;&#20316;&#26102;&#21487;&#20197;&#24573;&#30053;&#26410;&#26469;&#29366;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36981;&#24490;&#27492;&#31574;&#30053;&#21487;&#33021;&#23548;&#33268;&#36739;&#30701;&#30340;&#31574;&#30053;&#35745;&#31639;&#26102;&#38388;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#21551;&#21457;&#24335;&#26041;&#27861;&#24341;&#36215;&#30340;&#24615;&#33021;&#20007;&#22833;&#30340;&#30028;&#38480;&#12290;&#20026;&#20102;&#30830;&#23450;&#26159;&#21542;&#37319;&#21462;&#27979;&#37327;&#34892;&#21160;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#27979;&#37327;&#20215;&#20540;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#22522;&#20110; ATM &#21551;&#21457;&#24335;&#26041;&#27861;&#24320;&#21457;&#20102;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#38024;&#23545;&#37096;&#20998;&#21487;&#35266;&#23519;&#22495;&#30340; Dyna-Q &#21464;&#20307;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#22810;&#20010;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study Markov decision processes (MDPs), where agents have direct control over when and how they gather information, as formalized by action-contingent noiselessly observable MDPs (ACNO-MPDs). In these models, actions consist of two components: a control action that affects the environment, and a measurement action that affects what the agent can observe. To solve ACNO-MDPs, we introduce the act-then-measure (ATM) heuristic, which assumes that we can ignore future state uncertainty when choosing control actions. We show how following this heuristic may lead to shorter policy computation times and prove a bound on the performance loss incurred by the heuristic. To decide whether or not to take a measurement action, we introduce the concept of measuring value. We develop a reinforcement learning algorithm based on the ATM heuristic, using a Dyna-Q variant adapted for partially observable domains, and showcase its superior performance compared to prior methods on a number of partially-o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#12289;&#38754;&#21521;&#29305;&#23450;&#20307;&#31995;&#32467;&#26500;&#30340;&#36328;&#23618;&#36924;&#36817;&#26694;&#26550;&#65292;&#33021;&#22312;PE&#20013;&#23454;&#29616;&#22797;&#26434;&#30340;ML&#27169;&#22411;&#65292;&#21387;&#32553;&#30005;&#36335;&#22823;&#23567;&#39640;&#36798;95%&#65292;&#20998;&#31867;&#31934;&#24230;&#25439;&#22833;&#24179;&#22343;&#22312;4%&#20869;&#12290;</title><link>http://arxiv.org/abs/2303.08255</link><description>&lt;p&gt;
&#21360;&#21047;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#27169;&#22411;&#21040;&#30005;&#36335;&#30340;&#20132;&#21449;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Model-to-Circuit Cross-Approximation For Printed Machine Learning Classifiers. (arXiv:2303.08255v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#12289;&#38754;&#21521;&#29305;&#23450;&#20307;&#31995;&#32467;&#26500;&#30340;&#36328;&#23618;&#36924;&#36817;&#26694;&#26550;&#65292;&#33021;&#22312;PE&#20013;&#23454;&#29616;&#22797;&#26434;&#30340;ML&#27169;&#22411;&#65292;&#21387;&#32553;&#30005;&#36335;&#22823;&#23567;&#39640;&#36798;95%&#65292;&#20998;&#31867;&#31934;&#24230;&#25439;&#22833;&#24179;&#22343;&#22312;4%&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21360;&#21047;&#30005;&#23376;&#65288;PE&#65289;&#33021;&#22815;&#25353;&#38656;&#21046;&#36896;&#65292;&#20855;&#26377;&#20302;&#19981;&#37325;&#22797;&#24037;&#31243;&#25104;&#26412;&#21644;&#20122;&#20998;&#20043;&#19968;&#30340;&#21046;&#36896;&#25104;&#26412;&#12290;&#23427;&#36824;&#20801;&#35768;&#39640;&#24230;&#23450;&#21046;&#65292;&#36825;&#22312;&#30789;&#29255;&#19978;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#32780;&#19988;&#29305;&#27530;&#30340;&#20307;&#31995;&#32467;&#26500;&#20063;&#30427;&#34892;&#20110;&#26032;&#20852;&#30340;PE&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24212;&#29992;&#20013;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;PE&#20013;&#30340;&#22823;&#29305;&#24449;&#23610;&#23544;&#38459;&#27490;&#20102;&#22797;&#26434;&#30340;ML&#27169;&#22411;&#30340;&#23454;&#29616;&#65292;&#21363;&#20351;&#20351;&#29992;&#29305;&#27530;&#30340;&#20307;&#31995;&#32467;&#26500;&#20063;&#26159;&#22914;&#27492;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#65292;&#36328;&#23618;&#36924;&#36817;&#26694;&#26550;&#65292;&#19987;&#20026;&#29305;&#23450;&#30340;&#20307;&#31995;&#32467;&#26500;&#23450;&#21046;&#65292;&#21487;&#20197;&#22312;PE&#20013;&#23454;&#29616;&#22797;&#26434;&#30340;ML&#27169;&#22411;&#65292;&#22914;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLPs&#65289;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVMs&#65289;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#31639;&#27861;&#32423;&#21035;&#37319;&#29992;&#20102;&#30828;&#20214;&#39537;&#21160;&#30340;&#31995;&#25968;&#36924;&#36817;&#12289;&#36923;&#36753;&#32423;&#21035;&#30340;&#30005;&#36335;&#21015;&#34920;&#20462;&#21098;&#21644;&#30005;&#36335;&#32423;&#21035;&#30340;&#30005;&#21387;&#36229;&#26631;&#65292;&#20840;&#38754;&#21512;&#20316;&#12290;&#23545;12&#20010;MLP&#21644;12&#20010;SVM&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#20197;&#21450;6000&#22810;&#20010;&#36817;&#20284;&#21644;&#31934;&#30830;&#35774;&#35745;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#21040;&#30005;&#36335;&#30340;&#20132;&#21449;&#36924;&#36817;&#26694;&#26550;&#21487;&#20197;&#23558;&#30005;&#36335;&#22823;&#23567;&#21387;&#32553;&#39640;&#36798;95%&#65292;&#21516;&#26102;&#20445;&#35777;&#20998;&#31867;&#31934;&#24230;&#25439;&#22833;&#24179;&#22343;&#22312;4%&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;
Printed electronics (PE) promises on-demand fabrication, low non-recurring engineering costs, and sub-cent fabrication costs. It also allows for high customization that would be infeasible in silicon, and bespoke architectures prevail to improve the efficiency of emerging PE machine learning (ML) applications. Nevertheless, large feature sizes in PE prohibit the realization of complex ML models in PE, even with bespoke architectures. In this work, we present an automated, cross-layer approximation framework tailored to bespoke architectures that enable complex ML models, such as Multi-Layer Perceptrons (MLPs) and Support Vector Machines (SVMs), in PE. Our framework adopts cooperatively a hardware-driven coefficient approximation of the ML model at algorithmic level, a netlist pruning at logic level, and a voltage over-scaling at the circuit level. Extensive experimental evaluation on 12 MLPs and 12 SVMs and more than 6000 approximate and exact designs demonstrates that our model-to-cir
&lt;/p&gt;</description></item><item><title>R^2&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#38388;&#27491;&#21017;&#21270;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#26377;&#25928;&#30340;&#26368;&#23567;&#20540;&#21644;&#26368;&#22823;&#20540;&#35843;&#25972;&#26435;&#37325;&#20998;&#24067;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#21387;&#32553;&#21644;&#37327;&#21270;&#25216;&#26415;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#20854;&#25968;&#20540;&#34920;&#31034;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#20248;&#21270;&#30340;&#36136;&#37327;&#65292;&#23588;&#20854;&#26159;&#22312;&#36739;&#20302;&#20301;&#19978;&#12290;</title><link>http://arxiv.org/abs/2303.08253</link><description>&lt;p&gt;
R^2: &#22522;&#20110;&#21306;&#38388;&#27491;&#21017;&#21270;&#30340;&#27169;&#22411;&#21387;&#32553;&#19982;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
R^2: Range Regularization for Model Compression and Quantization. (arXiv:2303.08253v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08253
&lt;/p&gt;
&lt;p&gt;
R^2&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#38388;&#27491;&#21017;&#21270;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#26377;&#25928;&#30340;&#26368;&#23567;&#20540;&#21644;&#26368;&#22823;&#20540;&#35843;&#25972;&#26435;&#37325;&#20998;&#24067;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#21387;&#32553;&#21644;&#37327;&#21270;&#25216;&#26415;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#20854;&#25968;&#20540;&#34920;&#31034;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#20248;&#21270;&#30340;&#36136;&#37327;&#65292;&#23588;&#20854;&#26159;&#22312;&#36739;&#20302;&#20301;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#27491;&#21017;&#21270;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#30340;&#25216;&#26415;&#65292;&#20063;&#21487;&#29992;&#20110;&#35843;&#25972;&#26435;&#37325;&#20998;&#24067;&#20197;&#36798;&#21040;&#21508;&#31181;&#30446;&#30340;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#26435;&#37325;&#27491;&#21017;&#21270;&#26469;&#36741;&#21161;&#27169;&#22411;&#37327;&#21270;&#21644;&#21387;&#32553;&#25216;&#26415;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#21306;&#38388;&#27491;&#21017;&#21270;(R^2)&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#20248;&#21270;&#30340;&#36136;&#37327;&#65292;&#37325;&#28857;&#25918;&#22312;&#38450;&#27490;&#24322;&#24120;&#20540;&#26041;&#38754;&#12290;&#36890;&#36807;&#26377;&#25928;&#22320;&#35843;&#25972;&#20998;&#24067;&#20013;&#30340;&#26368;&#23567;&#20540;&#21644;&#26368;&#22823;&#20540;&#65292;&#23558;&#25972;&#20010;&#20998;&#24067;&#22609;&#36896;&#25104;&#32039;&#20945;&#30340;&#24418;&#29366;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#21387;&#32553;&#21644;&#37327;&#21270;&#25216;&#26415;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#23427;&#20204;&#26377;&#38480;&#30340;&#25968;&#20540;&#34920;&#31034;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;L-inf&#27491;&#21017;&#21270;&#65292;&#20854;&#25193;&#23637;&#38388;&#38548;&#27491;&#21017;&#21270;&#21644;&#26032;&#30340;soft-min-max&#27491;&#21017;&#21270;&#65292;&#20316;&#20026;&#20840;&#31934;&#24230;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#30340;&#27491;&#21017;&#21270;&#25439;&#22833;&#12290;&#32467;&#21512;&#26368;&#20808;&#36827;&#30340;&#37327;&#21270;&#21644;&#21387;&#32553;&#25216;&#26415;&#65292;&#21033;&#29992;R^2&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#24179;&#22343;&#27700;&#24179;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#23588;&#20854;&#26159;&#22312;&#36739;&#20302;&#20301;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model parameter regularization is a widely used technique to improve generalization, but also can be used to shape the weight distributions for various purposes. In this work, we shed light on how weight regularization can assist model quantization and compression techniques, and then propose range regularization (R^2) to further boost the quality of model optimization by focusing on the outlier prevention. By effectively regulating the minimum and maximum weight values from a distribution, we mold the overall distribution into a tight shape so that model compression and quantization techniques can better utilize their limited numeric representation powers. We introduce L-inf regularization, its extension margin regularization and a new soft-min-max regularization to be used as a regularization loss during full-precision model training. Coupled with state-of-the-art quantization and compression techniques, models trained with R^2 perform better on an average, specifically at lower bit 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Vision Transformer&#20013;&#23398;&#20064;&#25104;&#38271;&#20154;&#24037;&#28023;&#39532;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#24377;&#24615;&#32456;&#36523;&#23398;&#20064;&#12290;&#36890;&#36807;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#36827;&#34892;&#32500;&#25252;&#65292;&#36873;&#21462;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#22359;&#20013;&#30340;&#26368;&#32456;&#32447;&#24615;&#25237;&#24433;&#23618;&#36827;&#34892;ArtiHippo&#30340;&#23454;&#29616;&#21644;&#25104;&#38271;&#12290;</title><link>http://arxiv.org/abs/2303.08250</link><description>&lt;p&gt;
&#22312;&#35270;&#35273;Transformer&#20013;&#23398;&#20064;&#25104;&#38271;&#20154;&#24037;&#28023;&#39532;&#65292;&#23454;&#29616;&#24377;&#24615;&#32456;&#36523;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning to Grow Artificial Hippocampi in Vision Transformers for Resilient Lifelong Learning. (arXiv:2303.08250v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Vision Transformer&#20013;&#23398;&#20064;&#25104;&#38271;&#20154;&#24037;&#28023;&#39532;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#24377;&#24615;&#32456;&#36523;&#23398;&#20064;&#12290;&#36890;&#36807;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#36827;&#34892;&#32500;&#25252;&#65292;&#36873;&#21462;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#22359;&#20013;&#30340;&#26368;&#32456;&#32447;&#24615;&#25237;&#24433;&#23618;&#36827;&#34892;ArtiHippo&#30340;&#23454;&#29616;&#21644;&#25104;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32456;&#36523;&#23398;&#20064;&#38656;&#35201;&#25317;&#26377;&#20154;&#31867;&#26234;&#33021;&#30340;&#38887;&#24615;&#65292;&#21363;&#19981;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36825;&#31181;&#38887;&#24615;&#19982;&#22823;&#33041;&#20013;&#22797;&#26434;&#30340;&#35760;&#24518;&#26426;&#21046;&#65292;&#23588;&#20854;&#26159;&#28023;&#39532;&#32500;&#25252;&#30340;&#38271;&#26399;&#35760;&#24518;&#65288;LM&#65289;&#32039;&#23494;&#30456;&#20851;&#12290;Transformer&#24050;&#32463;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#8220;&#22823;&#33041;&#8221;&#30340;&#23545;&#24212;&#20307;&#65292;&#20294;LM&#32452;&#20214;&#22312;&#32456;&#36523;&#23398;&#20064;&#20013;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Vision Transformer&#20013;&#23398;&#20064;&#25104;&#38271;&#20154;&#24037;&#28023;&#39532;&#65288;ArtiHippo&#65289;&#20197;&#23454;&#29616;&#24377;&#24615;&#32456;&#36523;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20840;&#38754;&#28040;&#34701;&#23454;&#39564;&#65292;&#36873;&#23450;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#65288;MHSA&#65289;&#22359;&#20013;&#30340;&#26368;&#32456;&#32447;&#24615;&#25237;&#24433;&#23618;&#26469;&#23454;&#29616;&#21644;&#25104;&#38271;ArtiHippo&#12290;ArtiHippo&#30001;&#19987;&#23478;&#28151;&#21512;&#65288;MoEs&#65289;&#34920;&#31034;&#12290;&#27599;&#20010;&#19987;&#23478;&#32452;&#20214;&#26159;&#32447;&#24615;&#25237;&#24433;&#23618;&#30340;&#29616;&#22330;&#21464;&#20307;&#65292;&#36890;&#36807;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#36827;&#34892;&#32500;&#25252;&#65292;&#25628;&#32034;&#31354;&#38388;&#30001;&#22235;&#20010;&#22522;&#26412;&#25104;&#38271;&#25805;&#20316;&#65288;&#36339;&#36807;&#12289;&#37325;&#29992;&#12289;&#36866;&#24212;&#21644;&#26032;&#65289;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lifelong learning without catastrophic forgetting (i.e., resiliency) possessed by human intelligence is entangled with sophisticated memory mechanisms in the brain, especially the long-term memory (LM) maintained by Hippocampi. To a certain extent, Transformers have emerged as the counterpart ``Brain" of Artificial Intelligence (AI), and yet leave the LM component under-explored for lifelong learning settings. This paper presents a method of learning to grow Artificial Hippocampi (ArtiHippo) in Vision Transformers (ViTs) for resilient lifelong learning. With a comprehensive ablation study, the final linear projection layer in the multi-head self-attention (MHSA) block is selected in realizing and growing ArtiHippo. ArtiHippo is represented by a mixture of experts (MoEs). Each expert component is an on-site variant of the linear projection layer, maintained via neural architecture search (NAS) with the search space defined by four basic growing operations -- skip, reuse, adapt, and new 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#23398;&#20064;&#24050;&#25506;&#32034;&#25968;&#25454;&#31354;&#38388;&#65292;&#36319;&#36394;&#21442;&#25968;&#25506;&#32034;&#29366;&#24577;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#27491;&#30340;&#40065;&#26834;&#38543;&#26426;&#20999;&#21106;&#26862;&#26519;&#21644;&#21551;&#21457;&#24335;&#26041;&#27861;&#22312;&#20108;&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2303.08249</link><description>&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#23398;&#20064;&#25506;&#27979;&#31354;&#38388;&#30340;&#31995;&#32479;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Systematic design space exploration by learning the explored space using Machine Learning. (arXiv:2303.08249v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#23398;&#20064;&#24050;&#25506;&#32034;&#25968;&#25454;&#31354;&#38388;&#65292;&#36319;&#36394;&#21442;&#25968;&#25506;&#32034;&#29366;&#24577;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#27491;&#30340;&#40065;&#26834;&#38543;&#26426;&#20999;&#21106;&#26862;&#26519;&#21644;&#21551;&#21457;&#24335;&#26041;&#27861;&#22312;&#20108;&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#21442;&#25968;&#31354;&#38388;&#25506;&#32034;&#30340;&#23454;&#36341;&#20027;&#35201;&#30001;&#38543;&#26426;&#25277;&#26679;&#25110;&#35797;&#39564;&#35774;&#35745;&#26041;&#27861;&#20027;&#23548;&#12290;&#36825;&#20123;&#26041;&#27861;&#26368;&#22823;&#30340;&#38382;&#39064;&#26159;&#26080;&#27861;&#36319;&#36394;&#21442;&#25968;&#31354;&#38388;&#30340;&#21738;&#19968;&#37096;&#20998;&#24050;&#34987;&#25506;&#32034;&#65292;&#21738;&#19968;&#37096;&#20998;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21033;&#29992;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20960;&#20309;&#23398;&#20064;&#24050;&#25506;&#32034;&#25968;&#25454;&#31354;&#38388;&#65292;&#20197;&#36319;&#36394;&#24050;&#32463;&#25506;&#32034;&#36807;&#30340;&#21306;&#22495;&#65292;&#24182;&#20174;&#26410;&#25506;&#32034;&#30340;&#21306;&#22495;&#20013;&#25552;&#21462;&#26679;&#26412;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20462;&#25913;&#29256;&#30340;&#40065;&#26834;&#38543;&#26426;&#20999;&#21106;&#26862;&#26519;&#20197;&#21450;&#20854;&#20182;&#22522;&#20110;&#21551;&#21457;&#24335;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21450;&#20854;&#22312;&#20108;&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#21487;&#20197;&#25193;&#23637;&#21040;&#20219;&#20309;&#32500;&#24230;&#65292;&#22240;&#20026;&#24213;&#23618;&#26041;&#27861;&#26159;&#36890;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current practice in parameter space exploration in euclidean space is dominated by randomized sampling or design of experiment methods. The biggest issue with these methods is not keeping track of what part of parameter space has been explored and what has not. In this context, we utilize the geometric learning of explored data space using modern machine learning methods to keep track of already explored regions and samples from the regions that are unexplored. For this purpose, we use a modified version of a robust random-cut forest along with other heuristic-based approaches. We demonstrate our method and its progression in two-dimensional Euclidean space but it can be extended to any dimension since the underlying method is generic.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#38024;&#23545;&#22810;&#32500;&#27969;&#24335;&#26102;&#38388;&#24207;&#21015;&#30340;&#26368;&#20248;&#25277;&#26679;&#35774;&#35745;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#20110;&#39640;&#36895;&#30005;&#21147;&#28040;&#32791;&#25968;&#25454;&#30340;&#20302;&#25104;&#26412;&#23454;&#26102;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.08242</link><description>&lt;p&gt;
&#22810;&#32500;&#27969;&#24335;&#26102;&#38388;&#24207;&#21015;&#30340;&#26368;&#20248;&#25277;&#26679;&#35774;&#35745;&#21450;&#22312;&#30005;&#21147;&#31995;&#32479;&#30417;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Optimal Sampling Designs for Multi-dimensional Streaming Time Series with Application to Power Grid Sensor Data. (arXiv:2303.08242v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#38024;&#23545;&#22810;&#32500;&#27969;&#24335;&#26102;&#38388;&#24207;&#21015;&#30340;&#26368;&#20248;&#25277;&#26679;&#35774;&#35745;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#20110;&#39640;&#36895;&#30005;&#21147;&#28040;&#32791;&#25968;&#25454;&#30340;&#20302;&#25104;&#26412;&#23454;&#26102;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#31995;&#32479;&#20135;&#29983;&#20102;&#22823;&#37327;&#39640;&#36895;&#26102;&#38388;&#30456;&#20851;&#30340;&#27969;&#24335;&#25968;&#25454;&#65292;&#24182;&#32463;&#24120;&#19982;&#35745;&#31639;&#25110;&#33021;&#28304;&#32422;&#26463;&#19979;&#30340;&#22312;&#32447;&#25512;&#26029;&#20219;&#21153;&#30456;&#36830;&#12290;&#23545;&#36825;&#20123;&#27969;&#24335;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22312;&#32447;&#20998;&#26512;&#32463;&#24120;&#38754;&#20020;&#32479;&#35745;&#25928;&#29575;&#21644;&#35745;&#31639;&#25104;&#26412;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#35299;&#20915;&#36825;&#31181;&#26435;&#34913;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#27861;&#26159;&#25277;&#26679;&#65292;&#20165;&#36873;&#25321;&#19968;&#23567;&#37096;&#20998;&#26679;&#26412;&#36827;&#34892;&#27169;&#22411;&#25311;&#21512;&#21644;&#26356;&#26032;&#12290;&#20026;&#20102;&#28385;&#36275;&#29289;&#32852;&#32593;&#31995;&#32479;&#21160;&#24577;&#20851;&#31995;&#20998;&#26512;&#30340;&#38656;&#27714;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#21521;&#22810;&#32500;&#27969;&#24335;&#26102;&#38388;&#24207;&#21015;&#30340;&#25968;&#25454;&#20381;&#36182;&#25277;&#26679;&#36873;&#25321;&#21644;&#22312;&#32447;&#25512;&#26029;&#38382;&#39064;&#65292;&#26088;&#22312;&#25552;&#20379;&#39640;&#36895;&#30005;&#21147;&#28040;&#32791;&#25968;&#25454;&#30340;&#20302;&#25104;&#26412;&#23454;&#26102;&#20998;&#26512;&#12290;&#21463;&#23454;&#39564;&#35774;&#35745;&#20013;D-&#25928;&#24212;&#20934;&#21017;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31867;&#22312;&#32447;&#25968;&#25454;&#32553;&#20943;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#20248;&#25277;&#26679;&#20934;&#21017;&#65292;&#24182;&#25552;&#39640;&#20102;&#22312;&#32447;&#20998;&#26512;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
The Internet of Things (IoT) system generates massive high-speed temporally correlated streaming data and is often connected with online inference tasks under computational or energy constraints. Online analysis of these streaming time series data often faces a trade-off between statistical efficiency and computational cost. One important approach to balance this trade-off is sampling, where only a small portion of the sample is selected for the model fitting and update. Motivated by the demands of dynamic relationship analysis of IoT system, we study the data-dependent sample selection and online inference problem for a multi-dimensional streaming time series, aiming to provide low-cost real-time analysis of high-speed power grid electricity consumption data. Inspired by D-optimality criterion in design of experiments, we propose a class of online data reduction methods that achieve an optimal sampling criterion and improve the computational efficiency of the online analysis. We show 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Beta-Bernoulli&#36807;&#31243;&#21644;&#38750;&#21442;&#25968;&#36845;&#20195;&#31639;&#27861;&#30340;&#28145;&#24230;&#31232;&#30095;&#32534;&#30721;&#27169;&#22411;&#65292;&#26088;&#22312;&#23398;&#20064;&#20855;&#26377;&#23610;&#24230;&#19981;&#21464;&#24615;&#30340;&#31163;&#25955;&#29305;&#24449;&#65292;&#24182;&#40723;&#21169;&#34920;&#31034;&#30340;&#31232;&#30095;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.08230</link><description>&lt;p&gt;
&#22522;&#20110;Beta-Bernoulli&#36807;&#31243;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36125;&#21494;&#26031;&#31232;&#30095;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Bayesian Beta-Bernoulli Process Sparse Coding with Deep Neural Networks. (arXiv:2303.08230v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Beta-Bernoulli&#36807;&#31243;&#21644;&#38750;&#21442;&#25968;&#36845;&#20195;&#31639;&#27861;&#30340;&#28145;&#24230;&#31232;&#30095;&#32534;&#30721;&#27169;&#22411;&#65292;&#26088;&#22312;&#23398;&#20064;&#20855;&#26377;&#23610;&#24230;&#19981;&#21464;&#24615;&#30340;&#31163;&#25955;&#29305;&#24449;&#65292;&#24182;&#40723;&#21169;&#34920;&#31034;&#30340;&#31232;&#30095;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#28145;&#24230;&#31163;&#25955;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#36817;&#20284;&#25512;&#26029;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#32463;&#20856;&#31232;&#30095;&#32534;&#30721;&#27169;&#22411;&#20013;&#25104;&#21151;&#24212;&#29992;&#30340;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#22312;&#28145;&#24230;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#20013;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#36845;&#20195;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#27492;&#31867;&#28145;&#24230;&#27169;&#22411;&#20013;&#30340;&#31163;&#25955;&#28508;&#22312;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23398;&#20064;&#20855;&#26377;&#23610;&#24230;&#19981;&#21464;&#24615;&#30340;&#31163;&#25955;&#29305;&#24449;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26412;&#22320;&#25968;&#25454;&#32553;&#25918;&#21464;&#37327;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#22312;&#25105;&#20204;&#30340;&#34920;&#31034;&#20013;&#40723;&#21169;&#31232;&#30095;&#24615;&#65292;&#25105;&#20204;&#22312;&#28508;&#22312;&#22240;&#23376;&#19978;&#25552;&#20986;&#20102;Beta-Bernoulli&#36807;&#31243;&#20808;&#39564;&#12290;&#25105;&#20204;&#23545;&#32806;&#21512;&#19981;&#21516;&#20284;&#28982;&#27169;&#22411;&#30340;&#31232;&#30095;&#32534;&#30721;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#32467;&#26524;&#19982;&#24403;&#21069;&#30340;&#25674;&#38144;&#36817;&#20284;&#25512;&#26029;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several approximate inference methods have been proposed for deep discrete latent variable models. However, non-parametric methods which have previously been successfully employed for classical sparse coding models have largely been unexplored in the context of deep models. We propose a non-parametric iterative algorithm for learning discrete latent representations in such deep models. Additionally, to learn scale invariant discrete features, we propose local data scaling variables. Lastly, to encourage sparsity in our representations, we propose a Beta-Bernoulli process prior on the latent factors. We evaluate our spare coding model coupled with different likelihood models. We evaluate our method across datasets with varying characteristics and compare our results to current amortized approximate inference methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20511;&#21161;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#38669;&#23572;&#25928;&#24212;&#25512;&#36827;&#22120;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#36731;&#26494;&#33719;&#24471;&#25152;&#38656;&#29305;&#24615;&#30340;&#35774;&#35745;&#65292;&#20351;&#29992;&#30340;&#35745;&#31639;&#36164;&#28304;&#26356;&#23569;&#65292;&#27604;&#36890;&#24120;&#35774;&#35745;&#26041;&#27861;&#26356;&#20026;&#28789;&#27963;&#12290;</title><link>http://arxiv.org/abs/2303.08227</link><description>&lt;p&gt;
&#20511;&#21161;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22686;&#26448;&#21046;&#36896;&#30340;&#38669;&#23572;&#25928;&#24212;&#25512;&#36827;&#22120;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Hall effect thruster design via deep neural network for additive manufacturing. (arXiv:2303.08227v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20511;&#21161;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#38669;&#23572;&#25928;&#24212;&#25512;&#36827;&#22120;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#36731;&#26494;&#33719;&#24471;&#25152;&#38656;&#29305;&#24615;&#30340;&#35774;&#35745;&#65292;&#20351;&#29992;&#30340;&#35745;&#31639;&#36164;&#28304;&#26356;&#23569;&#65292;&#27604;&#36890;&#24120;&#35774;&#35745;&#26041;&#27861;&#26356;&#20026;&#28789;&#27963;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38669;&#23572;&#25928;&#24212;&#25512;&#36827;&#22120;&#26159;&#22826;&#31354;&#24212;&#29992;&#20013;&#26368;&#36890;&#29992;&#21644;&#27969;&#34892;&#30340;&#30005;&#25512;&#36827;&#31995;&#32479;&#20043;&#19968;&#12290;&#38543;&#30528;&#24037;&#19994;&#36235;&#21183;&#21521;&#26143;&#38469;&#20219;&#21153;&#21457;&#23637;&#65292;&#23545;&#36825;&#31181;&#25512;&#36827;&#31995;&#32479;&#30340;&#35774;&#35745;&#24320;&#21457;&#26041;&#26696;&#19981;&#26029;&#26356;&#26032;&#65292;&#32780;&#27491;&#30830;&#30340;&#25918;&#30005;&#36890;&#36947;&#23610;&#23544;&#23545;&#38669;&#23572;&#25928;&#24212;&#25512;&#36827;&#22120;&#30340;&#24615;&#33021;&#26377;&#24456;&#22823;&#24433;&#21709;&#12290;&#30001;&#20110;&#36825;&#31181;&#25512;&#36827;&#31995;&#32479;&#30340;&#23436;&#25972;&#29289;&#29702;&#27169;&#22411;&#23578;&#26410;&#32463;&#36807;&#24555;&#36895;&#35745;&#31639;&#21644;&#35774;&#35745;&#36845;&#20195;&#30340;&#20248;&#21270;&#65292;&#22823;&#22810;&#25968;&#25512;&#36827;&#22120;&#37117;&#26159;&#20351;&#29992;&#25152;&#35859;&#30340;&#27604;&#20363;&#23450;&#24459;&#36827;&#34892;&#35774;&#35745;&#12290;&#20294;&#36825;&#39033;&#24037;&#20316;&#20391;&#37325;&#20110;&#20351;&#29992;&#28145;&#24230;&#26426;&#22120;&#23398;&#20064;&#26469;&#21019;&#24314;&#39044;&#27979;&#24615;&#33021;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#29992;&#20110;&#36731;&#26494;&#33719;&#24471;&#25152;&#38656;&#29305;&#24615;&#30340;&#38669;&#23572;&#25512;&#36827;&#22120;&#30340;&#35774;&#35745;&#65292;&#19988;&#20351;&#29992;&#30340;&#35745;&#31639;&#36164;&#28304;&#27604;&#20174;&#22836;&#35774;&#35745;&#30340;&#35201;&#23569;&#24471;&#22810;&#65292;&#27604;&#36890;&#24120;&#30340;&#27604;&#20363;&#35774;&#35745;&#26041;&#27861;&#26356;&#21152;&#28789;&#27963;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hall effect thrusters are one of the most versatile and popular electric propulsion systems for space use. Industry trends towards interplanetary missions arise advances in design development of such propulsion systems. It is understood that correct sizing of discharge channel in Hall effect thruster impact performance greatly. Since the complete physics model of such propulsion system is not yet optimized for fast computations and design iterations, most thrusters are being designed using so-called scaling laws. But this work focuses on rather novel approach, which is outlined less frequently than ordinary scaling design approach in literature. Using deep machine learning it is possible to create predictive performance model, which can be used to effortlessly get design of required hall thruster with required characteristics using way less computational power than design from scratch and way more flexible than usual scaling approach.
&lt;/p&gt;</description></item><item><title>DeepAxe&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;DNN&#21152;&#36895;&#22120;&#30340;&#35774;&#35745;&#31354;&#38388;&#20013;&#32771;&#34385;&#36817;&#20284;&#21644;&#21487;&#38752;&#24615;&#26435;&#34913;&#30340;&#26694;&#26550;&#65292;&#36924;&#36817;&#21487;&#38752;&#24615;&#20851;&#38190;&#30340;DNN&#65292;&#24182;&#25552;&#20379;&#19968;&#32452;Pareto&#26368;&#20248;&#30340;DNN&#23454;&#29616;&#35774;&#35745;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2303.08226</link><description>&lt;p&gt;
DeepAxe: &#19968;&#31181;&#29992;&#20110;&#25506;&#32034;DNN&#21152;&#36895;&#22120;&#30340;&#36817;&#20284;&#21644;&#21487;&#38752;&#24615;&#26435;&#34913;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DeepAxe: A Framework for Exploration of Approximation and Reliability Trade-offs in DNN Accelerators. (arXiv:2303.08226v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08226
&lt;/p&gt;
&lt;p&gt;
DeepAxe&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;DNN&#21152;&#36895;&#22120;&#30340;&#35774;&#35745;&#31354;&#38388;&#20013;&#32771;&#34385;&#36817;&#20284;&#21644;&#21487;&#38752;&#24615;&#26435;&#34913;&#30340;&#26694;&#26550;&#65292;&#36924;&#36817;&#21487;&#38752;&#24615;&#20851;&#38190;&#30340;DNN&#65292;&#24182;&#25552;&#20379;&#19968;&#32452;Pareto&#26368;&#20248;&#30340;DNN&#23454;&#29616;&#35774;&#35745;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;Deep Neural Networks&#65288;DNNs&#65289;&#22312;&#24191;&#27867;&#30340;&#23433;&#20840;&#20851;&#38190;&#22411;&#24212;&#29992;&#20013;&#30340;&#20316;&#29992;&#27491;&#22312;&#25193;&#22823;&#65292;&#26032;&#20852;&#30340;DNN&#32463;&#21382;&#20102;&#35745;&#31639;&#33021;&#21147;&#26041;&#38754;&#30340;&#24040;&#22823;&#22686;&#38271;&#12290;&#36825;&#22686;&#21152;&#20102;&#25552;&#39640;DNN&#21152;&#36895;&#22120;&#21487;&#38752;&#24615;&#30340;&#24517;&#35201;&#24615;&#65292;&#21516;&#26102;&#38477;&#20302;&#30828;&#20214;&#24179;&#21488;&#19978;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#21363;&#38477;&#20302;&#33021;&#32791;&#21644;&#25191;&#34892;&#26102;&#38388;&#65292;&#25552;&#39640;DNN&#21152;&#36895;&#22120;&#30340;&#25928;&#29575;&#12290;&#22240;&#27492;&#65292;&#30828;&#20214;&#24615;&#33021;&#65288;&#21363;&#21306;&#22495;&#12289;&#21151;&#29575;&#21644;&#24310;&#36831;&#65289;&#19982;DNN&#21152;&#36895;&#22120;&#23454;&#29616;&#30340;&#21487;&#38752;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#38656;&#35201;&#24037;&#20855;&#36827;&#34892;&#20998;&#26512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;DeepAxe&#65292;&#29992;&#20110;&#22312;&#32771;&#34385;&#24212;&#29992;&#21151;&#33021;&#36817;&#20284;&#23545;&#20934;&#30830;&#24230;&#12289;&#21487;&#38752;&#24615;&#21644;&#30828;&#20214;&#24615;&#33021;&#30340;&#19977;&#26041;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#22522;&#20110;FPGA&#30340;DNN&#23454;&#29616;&#36827;&#34892;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#12290;&#35813;&#26694;&#26550;&#20351;&#24471;&#23545;&#20110;&#20851;&#38190;&#21487;&#38752;&#24615;&#30340;DNN&#36827;&#34892;&#26377;&#36873;&#25321;&#30340;&#36924;&#36817;&#65292;&#25552;&#20379;&#20102;&#19968;&#32452;Pareto&#26368;&#20248;&#30340;DNN&#23454;&#29616;&#35774;&#35745;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the role of Deep Neural Networks (DNNs) in a wide range of safety-critical applications is expanding, emerging DNNs experience massive growth in terms of computation power. It raises the necessity of improving the reliability of DNN accelerators yet reducing the computational burden on the hardware platforms, i.e. reducing the energy consumption and execution time as well as increasing the efficiency of DNN accelerators. Therefore, the trade-off between hardware performance, i.e. area, power and delay, and the reliability of the DNN accelerator implementation becomes critical and requires tools for analysis. In this paper, we propose a framework DeepAxe for design space exploration for FPGA-based implementation of DNNs by considering the trilateral impact of applying functional approximation on accuracy, reliability and hardware performance. The framework enables selective approximation of reliability-critical DNNs, providing a set of Pareto-optimal DNN implementation design spac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#30340;&#22330;&#22320;&#19981;&#21487;&#30693;&#20803;&#23398;&#20064;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#65292;&#20351;&#29992;&#22312;&#22810;&#20010;&#31449;&#28857;&#30340;MRI&#25968;&#25454;&#35757;&#32451;&#20986;&#30340;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#30149;&#20154;&#21644;&#27491;&#24120;&#20154;&#30340;&#24555;&#36895;&#35782;&#21035;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2303.08224</link><description>&lt;p&gt;
&#20351;&#29992;&#22330;&#22320;&#19981;&#21487;&#30693;&#20803;&#23398;&#20064;&#21644;&#33041;&#37096;MRI&#36827;&#34892;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#30340;&#23569;&#26679;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Classification of Autism Spectrum Disorder using Site-Agnostic Meta-Learning and Brain MRI. (arXiv:2303.08224v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#30340;&#22330;&#22320;&#19981;&#21487;&#30693;&#20803;&#23398;&#20064;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#65292;&#20351;&#29992;&#22312;&#22810;&#20010;&#31449;&#28857;&#30340;MRI&#25968;&#25454;&#35757;&#32451;&#20986;&#30340;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#30149;&#20154;&#21644;&#27491;&#24120;&#20154;&#30340;&#24555;&#36895;&#35782;&#21035;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#21487;&#29992;&#24615;&#26377;&#38480;&#65292;&#35774;&#35745;&#38024;&#23545;&#32454;&#24494;&#30149;&#24773;&#65288;&#22914;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#65289;&#30340;&#25918;&#23556;&#23398;&#20998;&#31867;&#22120;&#21463;&#21040;&#38459;&#30861;&#12290;&#36801;&#31227;&#23398;&#20064;&#26159;&#35299;&#20915;&#20302;&#35757;&#32451;&#25968;&#25454;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#26412;&#25991;&#25506;&#35752;&#22312;&#20855;&#26377;&#22810;&#20010;&#31449;&#28857;&#30340;&#20808;&#21069;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#20803;&#23398;&#20064;&#30340;&#20351;&#29992;&#65292;&#21363;&#25105;&#20204;&#31216;&#20043;&#20026;&#22330;&#22320;&#19981;&#21487;&#30693;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38750;&#24120;&#20302;&#25968;&#25454;&#21046;&#24230;&#30340;&#24773;&#20917;&#12290;&#21463;&#21040;&#20803;&#23398;&#20064;&#22312;&#20248;&#21270;&#27169;&#22411;&#36328;&#22810;&#20010;&#20219;&#21153;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20351;&#20854;&#36866;&#24212;&#20110;&#19981;&#21516;&#31449;&#28857;&#20043;&#38388;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;38&#20010;&#24433;&#20687;&#31449;&#28857;&#30340;2,201&#20010;T1&#21152;&#26435;&#65288;T1-w&#65289;MRI&#25195;&#25551;&#20013;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#36825;&#20123;&#25968;&#25454;&#26159;&#20316;&#20026;&#33258;&#38381;&#30151;&#33041;&#37096;&#24433;&#20687;&#25968;&#25454;&#20132;&#25442;&#65288;ABIDE&#65289;&#30340;&#19968;&#37096;&#20998;&#25910;&#38598;&#30340;&#65288;&#24180;&#40836;&#65306;5.2-64.0&#23681;&#65289;&#12290;&#35813;&#26041;&#27861;&#34987;&#35757;&#32451;&#20026;&#26597;&#25214;&#19968;&#20010;&#33391;&#22909;&#30340;&#21021;&#22987;&#29366;&#24577;&#65292;&#20197;&#20415;&#24555;&#36895;&#36866;&#24212;&#26032;&#30340;&#26410;&#35265;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
For machine learning applications in medical imaging, the availability of training data is often limited, which hampers the design of radiological classifiers for subtle conditions such as autism spectrum disorder (ASD). Transfer learning is one method to counter this problem of low training data regimes. Here we explore the use of meta-learning for very low data regimes in the context of having prior data from multiple sites - an approach we term site-agnostic meta-learning. Inspired by the effectiveness of meta-learning for optimizing a model across multiple tasks, here we propose a framework to adapt it to learn across multiple sites. We tested our meta-learning model for classifying ASD versus typically developing controls in 2,201 T1-weighted (T1-w) MRI scans collected from 38 imaging sites as part of Autism Brain Imaging Data Exchange (ABIDE) [age: 5.2-64.0 years]. The method was trained to find a good initialization state for our model that can quickly adapt to data from new uns
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23616;&#37096;&#26368;&#20248;&#30340;&#38598;&#21512;&#20998;&#21106;&#38382;&#39064;&#29256;&#26412;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;$O(N^2)$&#26102;&#38388;&#30340;2-opt&#31639;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#20219;&#24847;&#36755;&#20837;&#31934;&#24230;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2303.08219</link><description>&lt;p&gt;
&#19968;&#20010;&#23616;&#37096;&#26368;&#20248;&#38598;&#21512;&#20998;&#21106;&#20248;&#21270;&#30340;2-opt&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A 2-opt Algorithm for Locally Optimal Set Partition Optimization. (arXiv:2303.08219v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08219
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23616;&#37096;&#26368;&#20248;&#30340;&#38598;&#21512;&#20998;&#21106;&#38382;&#39064;&#29256;&#26412;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;$O(N^2)$&#26102;&#38388;&#30340;2-opt&#31639;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#20219;&#24847;&#36755;&#20837;&#31934;&#24230;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#28041;&#21450;&#38598;&#21512;&#20998;&#21106;&#38382;&#39064;&#30340;&#20248;&#21270;&#29256;&#26412;&#65292;&#30446;&#26631;&#26159;&#23558;&#20004;&#20010;&#19981;&#30456;&#20132;&#20998;&#21306;&#30340;&#21644;&#30340;&#32477;&#23545;&#24046;&#26368;&#23567;&#21270;&#12290;&#34429;&#28982;&#36825;&#20010;&#38382;&#39064;&#24050;&#30693;&#20026;NP&#38590;&#38382;&#39064;&#65292;&#38656;&#35201;&#25351;&#25968;&#26102;&#38388;&#26469;&#35299;&#20915;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#38382;&#39064;&#29256;&#26412;&#65292;&#21363;&#23547;&#25214;&#19968;&#20010;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#32771;&#34385;&#21040;&#33267;&#22810;&#31227;&#21160;&#20004;&#20010;&#20803;&#32032;&#30340;&#23616;&#37096;&#26368;&#20248;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;$O(N^2)$&#26102;&#38388;&#21644;$O(N)$&#31354;&#38388;&#20869;&#29983;&#25104;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#22788;&#29702;&#20219;&#24847;&#36755;&#20837;&#31934;&#24230;&#65292;&#19981;&#38656;&#35201;&#27491;&#25968;&#25110;&#25972;&#25968;&#36755;&#20837;&#12290;&#22240;&#27492;&#65292;&#23427;&#21487;&#20197;&#36731;&#26494;&#22320;&#24212;&#29992;&#20110;&#21508;&#31181;&#38382;&#39064;&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our research deals with the optimization version of the set partition problem, where the objective is to minimize the absolute difference between the sums of the two disjoint partitions. Although this problem is known to be NP-hard and requires exponential time to solve, we propose a less demanding version of this problem where the goal is to find a locally optimal solution. In our approach, we consider the local optimality in respect to any movement of at most two elements. To accomplish this, we developed an algorithm that can generate a locally optimal solution in at most $O(N^2)$ time and $O(N)$ space. Our algorithm can handle arbitrary input precisions and does not require positive or integer inputs. Hence, it can be applied in various problem scenarios with ease.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23581;&#35797;&#20351;&#29992;Vision Transformers&#23545;&#22522;&#20110;MRI&#25195;&#25551;&#30340;&#24615;&#21035;&#21644;AD&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#20854;&#20013;&#20004;&#31181;ViT&#26550;&#26500;&#21464;&#20307;&#20998;&#21035;&#23454;&#29616;&#20102;0.987&#30340;&#24615;&#21035;&#20998;&#31867;AUC&#21644;0.892&#30340;AD&#20998;&#31867;AUC&#12290;&#35813;&#26041;&#27861;&#21487;&#20026;&#22823;&#35268;&#27169;&#31070;&#32463;&#24433;&#20687;&#23398;&#35782;&#21035;&#25552;&#20379;&#39640;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.08216</link><description>&lt;p&gt;
&#29992;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26816;&#27979;&#30340;&#32467;&#26500;&#24615;MRI&#25195;&#25551;&#30340;Vision Transformers&#39640;&#25928;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Efficiently Training Vision Transformers on Structural MRI Scans for Alzheimer's Disease Detection. (arXiv:2303.08216v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23581;&#35797;&#20351;&#29992;Vision Transformers&#23545;&#22522;&#20110;MRI&#25195;&#25551;&#30340;&#24615;&#21035;&#21644;AD&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#20854;&#20013;&#20004;&#31181;ViT&#26550;&#26500;&#21464;&#20307;&#20998;&#21035;&#23454;&#29616;&#20102;0.987&#30340;&#24615;&#21035;&#20998;&#31867;AUC&#21644;0.892&#30340;AD&#20998;&#31867;AUC&#12290;&#35813;&#26041;&#27861;&#21487;&#20026;&#22823;&#35268;&#27169;&#31070;&#32463;&#24433;&#20687;&#23398;&#35782;&#21035;&#25552;&#20379;&#39640;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#20154;&#32676;&#31070;&#32463;&#24433;&#20687;&#23398;&#23545;&#20110;&#35782;&#21035;&#20419;&#36827;&#25110;&#25269;&#25239;&#33041;&#30142;&#30149;&#30340;&#22240;&#32032;&#20197;&#21450;&#21327;&#21161;&#35786;&#26029;&#12289;&#20122;&#22411;&#20998;&#31867;&#21644;&#39044;&#21518;&#37117;&#20855;&#26377;&#20215;&#20540;&#12290;&#26412;&#25991;&#23581;&#35797;&#20351;&#29992;Vision Transformers(ViT)&#23545;&#22522;&#20110;&#38590;&#24230;&#35843;&#25972;&#30340;&#19968;&#31995;&#21015;&#31070;&#32463;&#24433;&#20687;&#23398;&#20219;&#21153;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21253;&#25324;&#24615;&#21035;&#21644;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;(AD)&#22522;&#20110;3D&#22823;&#33041;MRI&#30340;&#20998;&#31867;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#20004;&#31181;ViT&#26550;&#26500;&#21464;&#20307;&#20998;&#21035;&#23454;&#29616;&#20102;0.987&#30340;&#24615;&#21035;&#20998;&#31867;AUC&#21644;0.892&#30340;AD&#20998;&#31867;AUC&#12290;&#25105;&#20204;&#29420;&#31435;&#35780;&#20272;&#20102;&#27169;&#22411;&#22312;&#20004;&#20010;&#22522;&#20934;AD&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#33719;&#24471;&#20102;5%&#21644;9-10%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neuroimaging of large populations is valuable to identify factors that promote or resist brain disease, and to assist diagnosis, subtyping, and prognosis. Data-driven models such as convolutional neural networks (CNNs) have increasingly been applied to brain images to perform diagnostic and prognostic tasks by learning robust features. Vision transformers (ViT) - a new class of deep learning architectures - have emerged in recent years as an alternative to CNNs for several computer vision applications. Here we tested variants of the ViT architecture for a range of desired neuroimaging downstream tasks based on difficulty, in this case for sex and Alzheimer's disease (AD) classification based on 3D brain MRI. In our experiments, two vision transformer architecture variants achieved an AUC of 0.987 for sex and 0.892 for AD classification, respectively. We independently evaluated our models on data from two benchmark AD datasets. We achieved a performance boost of 5% and 9-10% upon fine-t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#23545;&#20110;&#25345;&#32493;&#23398;&#20064;&#20219;&#21153;&#26469;&#35828;&#65292;&#36951;&#24536;&#19981;&#26159;&#19968;&#31181;&#33391;&#22909;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#27809;&#26377;&#32771;&#34385;&#21040;&#21069;&#21521;&#36801;&#31227;&#30340;&#37327;&#24230;&#26041;&#24335;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;&#24230;&#26041;&#24335;&#65292;&#21457;&#29616;&#36739;&#19981;&#36951;&#24536;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.08207</link><description>&lt;p&gt;
&#36951;&#24536;&#26159;&#21542;&#26159;&#21069;&#21521;&#36801;&#31227;&#30340;&#33391;&#22909;&#24402;&#32435;&#20559;&#24046;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is forgetting less a good inductive bias for forward transfer?. (arXiv:2303.08207v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#23545;&#20110;&#25345;&#32493;&#23398;&#20064;&#20219;&#21153;&#26469;&#35828;&#65292;&#36951;&#24536;&#19981;&#26159;&#19968;&#31181;&#33391;&#22909;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#27809;&#26377;&#32771;&#34385;&#21040;&#21069;&#21521;&#36801;&#31227;&#30340;&#37327;&#24230;&#26041;&#24335;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;&#24230;&#26041;&#24335;&#65292;&#21457;&#29616;&#36739;&#19981;&#36951;&#24536;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#30340;&#20027;&#35201;&#21160;&#26426;&#20043;&#19968;&#26159;&#65292;&#35813;&#38382;&#39064;&#35774;&#32622;&#20801;&#35768;&#27169;&#22411;&#20174;&#36807;&#21435;&#30340;&#20219;&#21153;&#20013;&#31215;&#32047;&#30693;&#35782;&#20197;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#25152;&#20248;&#21270;&#30340;&#20851;&#38190;&#25351;&#26631;&#65292;&#21363;&#20943;&#23569;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#19981;&#19982;&#21069;&#21521;&#30693;&#35782;&#36801;&#31227;&#30456;&#20851;&#12290;&#25105;&#20204;&#35748;&#20026;&#20043;&#21069;&#30340;&#30740;&#31350;&#32467;&#35770;&#26159;&#30001;&#20110;&#20182;&#20204;&#34913;&#37327;&#21069;&#21521;&#36801;&#31227;&#30340;&#26041;&#24335;&#25152;&#33268;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#34913;&#37327;&#19968;&#20010;&#20219;&#21153;&#30340;&#21069;&#21521;&#36801;&#31227;&#19981;&#24212;&#21463;&#21040;&#20026;&#20445;&#30041;&#20808;&#21069;&#20219;&#21153;&#30693;&#35782;&#32780;&#23545;&#25345;&#32493;&#23398;&#20064;&#22120;&#26045;&#21152;&#30340;&#38480;&#21046;&#30340;&#24433;&#21709;&#12290;&#30456;&#21453;&#65292;&#21069;&#21521;&#36801;&#31227;&#24212;&#35813;&#36890;&#36807;&#25345;&#32493;&#23398;&#20064;&#20135;&#29983;&#30340;&#19968;&#32452;&#34920;&#31034;&#26469;&#35780;&#20272;&#32473;&#23450;&#19968;&#20010;&#26032;&#20219;&#21153;&#26377;&#22810;&#23481;&#26131;&#23398;&#20064;&#12290;&#22312;&#36825;&#31181;&#21069;&#21521;&#36801;&#31227;&#27010;&#24565;&#19979;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#22312;&#21508;&#31181;&#22270;&#20687;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36739;&#19981;&#36951;&#24536;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#24403;&#35757;&#32451;&#25968;&#25454;&#25968;&#37327;&#23569;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the main motivations of studying continual learning is that the problem setting allows a model to accrue knowledge from past tasks to learn new tasks more efficiently. However, recent studies suggest that the key metric that continual learning algorithms optimize, reduction in catastrophic forgetting, does not correlate well with the forward transfer of knowledge. We believe that the conclusion previous works reached is due to the way they measure forward transfer. We argue that the measure of forward transfer to a task should not be affected by the restrictions placed on the continual learner in order to preserve knowledge of previous tasks. Instead, forward transfer should be measured by how easy it is to learn a new task given a set of representations produced by continual learning on previous tasks. Under this notion of forward transfer, we evaluate different continual learning algorithms on a variety of image classification benchmarks. Our results indicate that less forgetf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#35780;&#20272;&#25968;&#25454;&#31435;&#26041;&#20307;&#20013;&#40065;&#26834;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#26694;&#26550;RODD&#65292;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#38543;&#26426;&#26862;&#26519;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;RODD-RF&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#20013;&#65292;&#32467;&#26524;&#34920;&#26126;RODD-RF&#21487;&#20197;&#23548;&#33268;&#26356;&#22909;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2303.08193</link><description>&lt;p&gt;
RODD: &#25968;&#25454;&#31435;&#26041;&#20307;&#20013;&#30340;&#40065;&#26834;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
RODD: Robust Outlier Detection in Data Cubes. (arXiv:2303.08193v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#35780;&#20272;&#25968;&#25454;&#31435;&#26041;&#20307;&#20013;&#40065;&#26834;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#26694;&#26550;RODD&#65292;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#38543;&#26426;&#26862;&#26519;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;RODD-RF&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#20013;&#65292;&#32467;&#26524;&#34920;&#26126;RODD-RF&#21487;&#20197;&#23548;&#33268;&#26356;&#22909;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31435;&#26041;&#20307;&#26159;&#22810;&#32500;&#25968;&#25454;&#24211;&#65292;&#36890;&#24120;&#30001;&#20960;&#20010;&#29420;&#31435;&#30340;&#25968;&#25454;&#24211;&#26500;&#24314;&#32780;&#25104;&#65292;&#21487;&#20316;&#20026;&#28789;&#27963;&#30340;&#25968;&#25454;&#20998;&#26512;&#22522;&#30784;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25968;&#25454;&#31435;&#26041;&#20307;&#19978;&#30340;&#24322;&#24120;&#26816;&#27979;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#35780;&#20272;&#25968;&#25454;&#31435;&#26041;&#20307;&#20013;&#40065;&#26834;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#26694;&#26550;&#65288;RODD&#65289;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#38543;&#26426;&#26862;&#26519;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65288;RODD-RF&#65289;&#65292;&#24182;&#23558;&#20854;&#19982;&#22522;&#20110;&#40065;&#26834;&#20301;&#32622;&#20272;&#35745;&#30340;&#20256;&#32479;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#33324;&#31867;&#22411;&#30340;&#27979;&#35797;&#25968;&#25454;&#65292;&#24182;&#22312;&#27169;&#25311;&#30740;&#31350;&#20013;&#30740;&#31350;&#20102;&#25152;&#26377;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;ROOD-RF&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;RODD-RF&#21487;&#20197;&#23548;&#33268;&#26356;&#22909;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data cubes are multidimensional databases, often built from several separate databases, that serve as flexible basis for data analysis. Surprisingly, outlier detection on data cubes has not yet been treated extensively. In this work, we provide the first framework to evaluate robust outlier detection methods in data cubes (RODD). We introduce a novel random forest-based outlier detection approach (RODD-RF) and compare it with more traditional methods based on robust location estimators. We propose a general type of test data and examine all methods in a simulation study. Moreover, we apply ROOD-RF to real world data. The results show that RODD-RF can lead to improved outlier detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#20102;&#36710;&#36742;&#27178;&#21521;&#25511;&#21046;&#22120;&#65292;&#22312;&#27169;&#25311;&#22120;&#19978;&#35757;&#32451;&#27169;&#22411;&#24182;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#39044;&#27979;&#32622;&#20449;&#24230;/&#19981;&#30830;&#23450;&#24615;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#33258;&#21160;&#24341;&#23548;&#36710;&#36742;&#65292;&#20855;&#26377;&#38750;&#24120;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.08187</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#24341;&#23548;&#20013;&#30340;&#20391;&#21521;&#25511;&#21046;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Vehicle lateral control using Machine Learning for automated vehicle guidance. (arXiv:2303.08187v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#20102;&#36710;&#36742;&#27178;&#21521;&#25511;&#21046;&#22120;&#65292;&#22312;&#27169;&#25311;&#22120;&#19978;&#35757;&#32451;&#27169;&#22411;&#24182;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#39044;&#27979;&#32622;&#20449;&#24230;/&#19981;&#30830;&#23450;&#24615;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#33258;&#21160;&#24341;&#23548;&#36710;&#36742;&#65292;&#20855;&#26377;&#38750;&#24120;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#23545;&#20110;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#36816;&#34892;&#30340;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20013;&#20351;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#23433;&#20840;&#25805;&#20316;&#65292;&#20248;&#38597;&#22320;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#20102;&#36710;&#36742;&#30340;&#27178;&#21521;&#25511;&#21046;&#22120;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#21644;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#30001;&#20110;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#20013;&#30340;&#38598;&#25104;&#65292;&#25105;&#20204;&#21487;&#20197;&#39044;&#27979;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;/&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#27169;&#25311;&#22120;&#30340;&#19968;&#26465;&#36187;&#36947;&#19978;&#36816;&#34892;&#27773;&#36710;&#20135;&#29983;&#30340;&#25968;&#25454;&#23545;&#25511;&#21046;&#22120;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#20854;&#20182;&#36187;&#36947;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#30001;&#20110;&#21487;&#20449;&#24230;&#30340;&#39044;&#27979;&#65292;&#25105;&#20204;&#21487;&#20197;&#20915;&#23450;&#25511;&#21046;&#22120;&#22312;&#39044;&#27979;&#19981;&#30830;&#23450;&#24230;&#36739;&#23567;&#26102;&#20309;&#26102;&#38656;&#35201;&#25509;&#31649;&#25511;&#21046;&#12290;&#25105;&#20204;&#26377;&#20004;&#20010;&#32467;&#26524;&#35201;&#20998;&#20139;&#65306;&#39318;&#20808;&#65292;&#21363;&#20351;&#21482;&#26377;&#24456;&#23569;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;&#38543;&#26426;&#26862;&#26519;&#30340;&#22238;&#24402;&#22120;&#30456;&#27604;&#65292;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#20855;&#26377;&#38750;&#24120;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty in decision-making is crucial in the machine learning model used for a safety-critical system that operates in the real world. Therefore, it is important to handle uncertainty in a graceful manner for the safe operation of the CPS. In this work, we design a vehicle's lateral controller using a machine-learning model. To this end, we train a random forest model that is an ensemble model and a deep neural network model. Due to the ensemble in the random forest model, we can predict the confidence/uncertainty in the prediction. We train our controller on data generated from running the car on one track in the simulator and tested it on other tracks. Due to prediction in confidence, we could decide when the controller is less confident in prediction and takes control if needed. We have two results to share: first, even on a very small number of labeled data, a very good generalization capability of the random forest-based regressor in comparison with a deep neural network and a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#39640;&#25928;&#22320;&#25429;&#25417;&#35270;&#35273;&#33402;&#26415;&#30340;&#20803;&#32032;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#23398;&#20064;&#25216;&#26415;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#33402;&#26415;&#21697;&#25512;&#33616;&#65292;&#32467;&#26524;&#26174;&#31034;&#20004;&#32773;&#30340;&#32467;&#21512;&#21487;&#20197;&#25429;&#25417;&#26368;&#21512;&#36866;&#30340;&#38544;&#34255;&#35821;&#20041;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2303.08182</link><description>&lt;p&gt;
&#35270;&#35273;&#33402;&#26415;&#25512;&#33616;&#30340;&#35201;&#32032;&#65306;&#23398;&#20064;&#30011;&#20316;&#30340;&#28508;&#22312;&#35821;&#20041;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
The Elements of Visual Art Recommendation: Learning Latent Semantic Representations of Paintings. (arXiv:2303.08182v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#39640;&#25928;&#22320;&#25429;&#25417;&#35270;&#35273;&#33402;&#26415;&#30340;&#20803;&#32032;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#23398;&#20064;&#25216;&#26415;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#33402;&#26415;&#21697;&#25512;&#33616;&#65292;&#32467;&#26524;&#26174;&#31034;&#20004;&#32773;&#30340;&#32467;&#21512;&#21487;&#20197;&#25429;&#25417;&#26368;&#21512;&#36866;&#30340;&#38544;&#34255;&#35821;&#20041;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33402;&#26415;&#21697;&#25512;&#33616;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#29702;&#35299;&#29992;&#25143;&#22914;&#20309;&#19982;&#39640;&#24230;&#20027;&#35266;&#30340;&#20869;&#23481;&#20114;&#21160;&#65292;&#33402;&#26415;&#21697;&#20013;&#23884;&#20837;&#30340;&#27010;&#24565;&#30340;&#22797;&#26434;&#24615;&#65292;&#20197;&#21450;&#23427;&#20204;&#21487;&#33021;&#24341;&#36215;&#29992;&#25143;&#30340;&#24773;&#24863;&#21644;&#35748;&#30693;&#21453;&#24212;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#22914;&#20309;&#39640;&#25928;&#22320;&#25429;&#25417;&#35270;&#35273;&#33402;&#26415;&#30340;&#20803;&#32032;&#65288;&#21363;&#28508;&#22312;&#35821;&#20041;&#20851;&#31995;&#65289;&#65292;&#20197;&#36827;&#34892;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#22522;&#20110;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#23398;&#20064;&#25216;&#26415;&#20197;&#21450;&#23427;&#20204;&#30340;&#32452;&#21512;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;&#25105;&#20204;&#23545;&#25512;&#33616;&#36136;&#37327;&#36827;&#34892;&#20102;&#23567;&#35268;&#27169;&#21644;&#22823;&#35268;&#27169;&#30340;&#29992;&#25143;&#20013;&#24515;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25991;&#26412;&#29305;&#24449;&#27604;&#35270;&#35273;&#29305;&#24449;&#34920;&#29616;&#26356;&#22909;&#65292;&#32780;&#20004;&#32773;&#30340;&#32467;&#21512;&#21487;&#20197;&#25429;&#25417;&#33402;&#26415;&#21697;&#25512;&#33616;&#26368;&#21512;&#36866;&#30340;&#38544;&#34255;&#35821;&#20041;&#20851;&#31995;&#12290;&#26368;&#32456;&#65292;&#26412;&#25991;&#26377;&#21161;&#20110;&#29702;&#35299;&#22914;&#20309;&#25552;&#20379;&#36866;&#21512;&#29992;&#25143;&#20852;&#36259;&#21644;&#24863;&#30693;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artwork recommendation is challenging because it requires understanding how users interact with highly subjective content, the complexity of the concepts embedded within the artwork, and the emotional and cognitive reflections they may trigger in users. In this paper, we focus on efficiently capturing the elements (i.e., latent semantic relationships) of visual art for personalized recommendation. We propose and study recommender systems based on textual and visual feature learning techniques, as well as their combinations. We then perform a small-scale and a large-scale user-centric evaluation of the quality of the recommendations. Our results indicate that textual features compare favourably with visual ones, whereas a fusion of both captures the most suitable hidden semantic relationships for artwork recommendation. Ultimately, this paper contributes to our understanding of how to deliver content that suitably matches the user's interests and how they are perceived.
&lt;/p&gt;</description></item><item><title>Allegro-Legato &#26159;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;NNQMD&#27169;&#22411;&#65292;&#20351;&#29992; Sharpness-Aware Minimization &#35299;&#20915;&#20102;&#35745;&#31639;&#26426;&#22810;&#26680;&#24515;&#22788;&#29702;&#22120;&#26550;&#26500;&#19979;&#30340;&#31934;&#24230;&#25193;&#23637;&#38382;&#39064;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.08169</link><description>&lt;p&gt;
Allegro-Legato: &#22522;&#20110; Sharpness-Aware Minimization &#30340;&#22823;&#35268;&#27169;&#19988;&#24555;&#36895;&#30340;&#31070;&#32463;&#32593;&#32476;&#37327;&#23376;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Allegro-Legato: Scalable, Fast, and Robust Neural-Network Quantum Molecular Dynamics via Sharpness-Aware Minimization. (arXiv:2303.08169v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08169
&lt;/p&gt;
&lt;p&gt;
Allegro-Legato &#26159;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;NNQMD&#27169;&#22411;&#65292;&#20351;&#29992; Sharpness-Aware Minimization &#35299;&#20915;&#20102;&#35745;&#31639;&#26426;&#22810;&#26680;&#24515;&#22788;&#29702;&#22120;&#26550;&#26500;&#19979;&#30340;&#31934;&#24230;&#25193;&#23637;&#38382;&#39064;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#37327;&#23376;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#65288;NNQMD&#65289;&#27491;&#22312;&#36890;&#36807;&#25552;&#20379;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#36895;&#24230;&#26469;&#24443;&#24213;&#25913;&#21464;&#26448;&#26009;&#21407;&#23376;&#32423;&#27169;&#25311;&#12290;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;NNQMD&#27169;&#22411;&#22522;&#20110;&#32676;&#35770;&#30340;&#26059;&#36716;&#31561;&#21464;&#24615;&#29305;&#24449;&#21644;&#23616;&#37096;&#25551;&#36848;&#31526;&#65292;&#31216;&#20026; Allegro&#65288;&#24847;&#20026;&#24555;&#36895;&#65289;&#65292;&#25552;&#20379;&#20102;&#27604;&#20197;&#21069;&#26356;&#39640;&#30340;&#20934;&#30830;&#24230;&#21644;&#36895;&#24230;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#24615;&#33021;&#36229;&#32423;&#35745;&#31639;&#26426;&#19978;&#65292;&#23427;&#38754;&#20020;&#19968;&#20010;&#31934;&#24230;&#25193;&#23637;&#30340;&#38382;&#39064;&#65292;&#21363;&#19981;&#21512;&#29702;&#30340;&#39044;&#27979;&#38543;&#30528;&#21407;&#23376;&#25968;&#21644;&#27169;&#25311;&#26102;&#38388;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558; Allegro &#27169;&#22411;&#19982; Sharpeness aware minimization&#65288;SAM&#65289;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#25439;&#22833;&#20989;&#25968;&#34920;&#38754;&#30340;&#24179;&#28369;&#24615;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#20135;&#29983;&#30340; Allegro-Legato &#27169;&#22411;&#23637;&#31034;&#20102;&#26356;&#38271;&#26102;&#38388;&#21644;&#26356;&#22810;&#21407;&#23376;&#25968;&#30340;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural-network quantum molecular dynamics (NNQMD) simulations based on machine learning are revolutionizing atomistic simulations of materials by providing quantum-mechanical accuracy but orders-of-magnitude faster, illustrated by ACM Gordon Bell prize (2020) and finalist (2021). State-of-the-art (SOTA) NNQMD model founded on group theory featuring rotational equivariance and local descriptors has provided much higher accuracy and speed than those models, thus named Allegro (meaning fast). On massively parallel supercomputers, however, it suffers a fidelity-scaling problem, where growing number of unphysical predictions of interatomic forces prohibits simulations involving larger numbers of atoms for longer times. Here, we solve this problem by combining the Allegro model with sharpness aware minimization (SAM) for enhancing the robustness of model through improved smoothness of the loss landscape. The resulting Allegro-Legato (meaning fast and "smooth") model was shown to elongate the
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#36807;&#28388;&#22120;&#24863;&#30693;&#30340;&#36890;&#29992;&#36817;&#20284;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#23450;&#20041;&#20102;&#21512;&#36866;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#36816;&#34892;&#26102;&#35757;&#32451;&#20197;&#28385;&#36275;&#32479;&#35745;&#24179;&#31561;&#32422;&#26463;&#65292;&#21516;&#26102;&#26368;&#23567;&#31243;&#24230;&#25200;&#21160;&#21407;&#22987;&#21518;&#39564;&#24773;&#20917;&#19979;&#23454;&#29616;&#27492;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2303.08157</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20844;&#24179;&#22270;&#36807;&#28388;&#26367;&#20195;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network Surrogates of Fair Graph Filtering. (arXiv:2303.08157v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08157
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#36807;&#28388;&#22120;&#24863;&#30693;&#30340;&#36890;&#29992;&#36817;&#20284;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#23450;&#20041;&#20102;&#21512;&#36866;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#36816;&#34892;&#26102;&#35757;&#32451;&#20197;&#28385;&#36275;&#32479;&#35745;&#24179;&#31561;&#32422;&#26463;&#65292;&#21516;&#26102;&#26368;&#23567;&#31243;&#24230;&#25200;&#21160;&#21407;&#22987;&#21518;&#39564;&#24773;&#20917;&#19979;&#23454;&#29616;&#27492;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36793;&#20256;&#25773;&#23558;&#20808;&#21069;&#30340;&#33410;&#28857;&#20540;&#36716;&#25442;&#20026;&#21518;&#26469;&#30340;&#20998;&#25968;&#30340;&#22270;&#28388;&#27874;&#22120;&#36890;&#24120;&#25903;&#25345;&#24433;&#21709;&#20154;&#31867;&#30340;&#22270;&#25366;&#25496;&#20219;&#21153;&#65292;&#20363;&#22914;&#25512;&#33616;&#21644;&#25490;&#21517;&#12290;&#22240;&#27492;&#65292;&#37325;&#35201;&#30340;&#26159;&#22312;&#28385;&#36275;&#33410;&#28857;&#32452;&#20043;&#38388;&#30340;&#32479;&#35745;&#24179;&#31561;&#32422;&#26463;&#26041;&#38754;&#20351;&#23427;&#20204;&#20844;&#24179;&#65288;&#20363;&#22914;&#65292;&#25353;&#20854;&#20195;&#34920;&#24615;&#23558;&#20998;&#25968;&#36136;&#37327;&#22312;&#24615;&#21035;&#20043;&#38388;&#22343;&#34913;&#20998;&#37197;&#65289;&#12290;&#20026;&#20102;&#22312;&#26368;&#23567;&#31243;&#24230;&#22320;&#25200;&#21160;&#21407;&#22987;&#21518;&#39564;&#24773;&#20917;&#19979;&#23454;&#29616;&#27492;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36807;&#28388;&#22120;&#24863;&#30693;&#30340;&#36890;&#29992;&#36817;&#20284;&#26694;&#26550;&#65292;&#29992;&#20110;&#21518;&#39564;&#30446;&#26631;&#12290;&#36825;&#23450;&#20041;&#20102;&#36866;&#24403;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#22312;&#36816;&#34892;&#26102;&#35757;&#32451;&#65292;&#31867;&#20284;&#20110;&#36807;&#28388;&#22120;&#65292;&#20294;&#20063;&#22312;&#26412;&#22320;&#20248;&#21270;&#21253;&#25324;&#20844;&#24179;&#24863;&#30693;&#22312;&#20869;&#30340;&#22823;&#31867;&#30446;&#26631;&#12290;&#22312;&#19968;&#32452;8&#20010;&#36807;&#28388;&#22120;&#21644;5&#20010;&#22270;&#24418;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#28385;&#36275;&#32479;&#35745;&#24179;&#31561;&#32422;&#26463;&#26041;&#38754;&#34920;&#29616;&#24471;&#19981;&#20122;&#20110;&#26367;&#20195;&#21697;&#65292;&#21516;&#26102;&#20445;&#30041;&#22522;&#20110;&#20998;&#25968;&#30340;&#31038;&#21306;&#25104;&#21592;&#25512;&#33616;&#30340;AUC&#24182;&#22312;&#20256;&#25773;&#20808;&#21069;&#33410;&#25293;&#26102;&#21019;&#24314;&#26368;&#23567;&#23454;&#29992;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph filters that transform prior node values to posterior scores via edge propagation often support graph mining tasks affecting humans, such as recommendation and ranking. Thus, it is important to make them fair in terms of satisfying statistical parity constraints between groups of nodes (e.g., distribute score mass between genders proportionally to their representation). To achieve this while minimally perturbing the original posteriors, we introduce a filter-aware universal approximation framework for posterior objectives. This defines appropriate graph neural networks trained at runtime to be similar to filters but also locally optimize a large class of objectives, including fairness-aware ones. Experiments on a collection of 8 filters and 5 graphs show that our approach performs equally well or better than alternatives in meeting parity constraints while preserving the AUC of score-based community member recommendation and creating minimal utility loss in prior diffusion.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24615;&#33021;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#23376;&#31243;&#24207;&#30340;&#23884;&#20837;&#31354;&#38388;&#26469;&#23454;&#29616;&#24615;&#33021;&#20248;&#21270;&#30340;&#30452;&#25509;&#30693;&#35782;&#20256;&#36755;&#12290;&#20256;&#36755;&#35843;&#25972;&#23558;&#25628;&#32034;&#22797;&#26434;&#24230;&#38477;&#20302;&#20102;&#22810;&#36798;&#22235;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#22312;&#31232;&#30095;-&#23494;&#38598;&#30697;&#38453;&#20056;&#27861;&#20013;&#20248;&#20110;MKL&#24211;&#12290;</title><link>http://arxiv.org/abs/2303.08142</link><description>&lt;p&gt;
&#24615;&#33021;&#23884;&#20837;&#65306;&#19968;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#33258;&#21160;&#24615;&#33021;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Performance Embeddings: A Similarity-based Approach to Automatic Performance Optimization. (arXiv:2303.08142v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08142
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24615;&#33021;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#23376;&#31243;&#24207;&#30340;&#23884;&#20837;&#31354;&#38388;&#26469;&#23454;&#29616;&#24615;&#33021;&#20248;&#21270;&#30340;&#30452;&#25509;&#30693;&#35782;&#20256;&#36755;&#12290;&#20256;&#36755;&#35843;&#25972;&#23558;&#25628;&#32034;&#22797;&#26434;&#24230;&#38477;&#20302;&#20102;&#22810;&#36798;&#22235;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#22312;&#31232;&#30095;-&#23494;&#38598;&#30697;&#38453;&#20056;&#27861;&#20013;&#20248;&#20110;MKL&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24615;&#33021;&#20248;&#21270;&#26159;&#19968;&#39033;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#32463;&#24120;&#37325;&#22797;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#27599;&#20010;&#24179;&#21488;&#37117;&#26377;&#20854;&#29420;&#29305;&#20043;&#22788;&#65292;&#20294;&#22522;&#20110;&#25968;&#25454;&#31227;&#21160;&#21644;&#35745;&#31639;&#29305;&#24615;&#30340;&#24213;&#23618;&#20195;&#30721;&#36716;&#25442;&#22312;&#24212;&#29992;&#31243;&#24207;&#20013;&#20250;&#21453;&#22797;&#20986;&#29616;&#12290;&#26412;&#25991;&#25552;&#35758;&#21033;&#29992;&#36825;&#20123;&#30456;&#20284;&#24615;&#36890;&#36807;&#26500;&#24314;&#23376;&#31243;&#24207;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;&#35813;&#36830;&#32493;&#31354;&#38388;&#36890;&#36807;&#31526;&#21495;&#20195;&#30721;&#20998;&#26512;&#21644;&#24615;&#33021;&#20998;&#26512;&#25429;&#25417;&#24490;&#29615;&#23884;&#22871;&#30340;&#38745;&#24577;&#21644;&#21160;&#24577;&#29305;&#24615;&#12290;&#24615;&#33021;&#23884;&#20837;&#20351;&#24615;&#33021;&#35843;&#25972;&#30340;&#30452;&#25509;&#30693;&#35782;&#20256;&#36755;&#25104;&#20026;&#21487;&#33021;&#65292;&#36825;&#21487;&#20197;&#26469;&#33258;&#33258;&#21160;&#35843;&#25972;&#25110;&#37327;&#36523;&#23450;&#21046;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12289;&#23494;&#38598;&#21644;&#31232;&#30095;&#32447;&#24615;&#20195;&#25968;&#32452;&#21512;&#20197;&#21450;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#27169;&#26495;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#36825;&#31181;&#20256;&#36755;&#35843;&#25972;&#26041;&#27861;&#12290;&#20256;&#36755;&#35843;&#25972;&#23558;&#25628;&#32034;&#22797;&#26434;&#24230;&#38477;&#20302;&#20102;&#22810;&#36798;&#22235;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#22312;&#31232;&#30095;-&#23494;&#38598;&#30697;&#38453;&#20056;&#27861;&#20013;&#20248;&#20110;MKL&#24211;&#12290;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Performance optimization is an increasingly challenging but often repetitive task. While each platform has its quirks, the underlying code transformations rely on data movement and computational characteristics that recur across applications. This paper proposes to leverage those similarities by constructing an embedding space for subprograms. The continuous space captures both static and dynamic properties of loop nests via symbolic code analysis and performance profiling, respectively. Performance embeddings enable direct knowledge transfer of performance tuning between applications, which can result from autotuning or tailored improvements. We demonstrate this transfer tuning approach on case studies in deep neural networks, dense and sparse linear algebra compositions, and numerical weather prediction stencils. Transfer tuning reduces the search complexity by up to four orders of magnitude and outperforms the MKL library in sparse-dense matrix multiplication. The results exhibit cl
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#30340;&#29983;&#29289;&#26579;&#33394;&#21327;&#35758;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#65292;&#25968;&#23383;&#26579;&#33394;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#23558;&#20809;&#23398;&#23545;&#27604;&#36716;&#21270;&#20026;&#23454;&#38469;&#26579;&#33394;&#30340;&#22522;&#30784;&#23545;&#27604;&#25104;&#20026;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.08140</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#20809;&#23398;&#26174;&#24494;&#38236;&#20013;&#30340;&#25968;&#23383;&#26579;&#33394; - &#19968;&#31687;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Digital staining in optical microscopy using deep learning -- a review. (arXiv:2303.08140v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08140
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#29983;&#29289;&#26579;&#33394;&#21327;&#35758;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#65292;&#25968;&#23383;&#26579;&#33394;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#23558;&#20809;&#23398;&#23545;&#27604;&#36716;&#21270;&#20026;&#23454;&#38469;&#26579;&#33394;&#30340;&#22522;&#30784;&#23545;&#27604;&#25104;&#20026;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#21040;&#26368;&#36817;&#65292;&#20256;&#32479;&#30340;&#29983;&#29289;&#26579;&#33394;&#19968;&#30452;&#34987;&#35270;&#20026;&#19982;&#20020;&#24202;&#35786;&#26029;&#12289;&#22522;&#30784;&#30740;&#31350;&#21644;&#29983;&#29289;&#25216;&#26415;&#30456;&#20851;&#30340;&#22823;&#22810;&#25968;&#29983;&#29289;&#21307;&#23398;&#38382;&#39064;&#30340;&#19994;&#24050;&#30830;&#31435;&#30340;&#22522;&#20934;&#12290;&#23613;&#31649;&#20316;&#20026;&#40644;&#37329;&#26631;&#20934;&#30340;&#35282;&#33394;&#65292;&#20294;&#26579;&#33394;&#21327;&#35758;&#38754;&#20020;&#30528;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#38656;&#35201;&#23545;&#26679;&#21697;&#36827;&#34892;&#24191;&#27867;&#12289;&#25163;&#21160;&#22788;&#29702;&#12289;&#38271;&#26102;&#38388;&#24310;&#36831;&#12289;&#25913;&#21464;&#32452;&#32455;&#31283;&#24577;&#12289;&#23545;&#20110;&#32473;&#23450;&#26679;&#21697;&#30340;&#23545;&#27604;&#21058;&#36873;&#25321;&#26377;&#38480;&#65292;&#21482;&#33021;&#36827;&#34892;&#20108;&#32500;&#25104;&#20687;&#32780;&#38750;&#19977;&#32500;&#26029;&#23618;&#25195;&#25551;&#31561;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#19981;&#38656;&#35201;&#22806;&#28304;&#24615;&#21644;&#20154;&#24037;&#26631;&#35760;&#29289;&#30340;&#26080;&#26631;&#35760;&#20809;&#23398;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#20869;&#22312;&#30340;&#20809;&#23398;&#23545;&#27604;&#26426;&#21046;&#65292;&#20854;&#29305;&#24322;&#24615;&#36890;&#24120;&#23545;&#20154;&#31867;&#35266;&#23519;&#32773;&#19981;&#22826;&#26126;&#26174;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#25968;&#23383;&#26579;&#33394;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#27010;&#24565;&#65292;&#21033;&#29992;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#23558;&#20809;&#23398;&#23545;&#27604;&#36716;&#21270;&#20026;&#23454;&#38469;&#26579;&#33394;&#30340;&#22522;&#30784;&#23545;&#27604;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#24403;&#21069;&#25968;&#23383;&#26579;&#33394;&#26041;&#27861;&#21644;&#25216;&#26415;&#30340;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Until recently, conventional biochemical staining had the undisputed status as well-established benchmark for most biomedical problems related to clinical diagnostics, fundamental research and biotechnology. Despite this role as gold-standard, staining protocols face several challenges, such as a need for extensive, manual processing of samples, substantial time delays, altered tissue homeostasis, limited choice of contrast agents for a given sample, 2D imaging instead of 3D tomography and many more. Label-free optical technologies, on the other hand, do not rely on exogenous and artificial markers, by exploiting intrinsic optical contrast mechanisms, where the specificity is typically less obvious to the human observer. Over the past few years, digital staining has emerged as a promising concept to use modern deep learning for the translation from optical contrast to established biochemical contrast of actual stainings. In this review article, we provide an in-depth analysis of the cu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22266;&#23450;&#19987;&#23478;&#24314;&#35758;&#19979;&#30340;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#36951;&#25022;&#30028;&#38480;&#65292;&#21487;&#20197;&#20351;&#24471;&#26576;&#20123;&#31639;&#27861;&#30340;&#36951;&#25022;&#26080;&#38480;&#25509;&#36817;&#20110;&#38646;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;KL&#25955;&#24230;&#26469;&#25551;&#36848;&#19987;&#23478;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#30028;&#38480;&#65292;&#24182;&#32473;&#20986;&#20102;&#19979;&#38480;&#35777;&#26126;&#31639;&#27861;&#30340;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.08102</link><description>&lt;p&gt;
&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#30340;&#22266;&#23450;&#19987;&#23478;&#24314;&#35758;&#19979;&#36172;&#21338;&#26426;&#30340;&#36951;&#25022;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Information-Theoretic Regret Bounds for Bandits with Fixed Expert Advice. (arXiv:2303.08102v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22266;&#23450;&#19987;&#23478;&#24314;&#35758;&#19979;&#30340;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#36951;&#25022;&#30028;&#38480;&#65292;&#21487;&#20197;&#20351;&#24471;&#26576;&#20123;&#31639;&#27861;&#30340;&#36951;&#25022;&#26080;&#38480;&#25509;&#36817;&#20110;&#38646;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;KL&#25955;&#24230;&#26469;&#25551;&#36848;&#19987;&#23478;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#30028;&#38480;&#65292;&#24182;&#32473;&#20986;&#20102;&#19979;&#38480;&#35777;&#26126;&#31639;&#27861;&#30340;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#19987;&#23478;&#26159;&#22266;&#23450;&#21644;&#24050;&#30693;&#30340;&#24773;&#20917;&#19979;&#65292;&#36172;&#21338;&#26426;&#19982;&#19987;&#23478;&#24314;&#35758;&#30340;&#38382;&#39064;&#65292;&#36825;&#20123;&#19987;&#23478;&#26159;&#34892;&#21160;&#22266;&#23450;&#21644;&#24050;&#30693;&#20998;&#24067;&#12290;&#30456;&#27604;&#20197;&#21069;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#24773;&#20917;&#19979;&#36951;&#25022;&#26159;&#30001;&#34913;&#37327;&#19987;&#23478;&#20043;&#38388;&#30456;&#20284;&#24615;&#30340;&#20449;&#24687;&#35770;&#37327;&#25152;&#25511;&#21046;&#30340;&#12290;&#22312;&#19968;&#20123;&#33258;&#28982;&#29305;&#27530;&#24773;&#20917;&#19979;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#33719;&#24471;EXP4&#30340;&#31532;&#19968;&#20010;&#36951;&#25022;&#30028;&#38480;&#65292;&#22914;&#26524;&#19987;&#23478;&#36275;&#22815;&#30456;&#20284;&#65292;&#21017;&#21487;&#20197;&#26080;&#38480;&#25509;&#36817;&#20110;&#38646;&#12290;&#20026;&#21478;&#19968;&#31181;&#31639;&#27861;&#25552;&#20379;&#20102;&#21487;&#20197;&#29992;KL&#25955;&#24230;&#26469;&#25551;&#36848;&#19987;&#23478;&#20043;&#38388;&#30456;&#20284;&#24615;&#30340;&#21478;&#19968;&#31181;&#30028;&#38480;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#30028;&#38480;&#21487;&#20197;&#27604;EXP4&#26356;&#23567;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#26576;&#20123;&#19987;&#23478;&#31867;&#21035;&#25552;&#20379;&#20102;&#19979;&#38480;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#20998;&#26512;&#30340;&#31639;&#27861;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;&#20960;&#20046;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the problem of bandits with expert advice when the experts are fixed and known distributions over the actions. Improving on previous analyses, we show that the regret in this setting is controlled by information-theoretic quantities that measure the similarity between experts. In some natural special cases, this allows us to obtain the first regret bound for EXP4 that can get arbitrarily close to zero if the experts are similar enough. While for a different algorithm, we provide another bound that describes the similarity between the experts in terms of the KL-divergence, and we show that this bound can be smaller than the one of EXP4 in some cases. Additionally, we provide lower bounds for certain classes of experts showing that the algorithms we analyzed are nearly optimal in some cases.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#23610;&#24230;&#31354;&#38388;&#30740;&#31350;&#30340;&#35282;&#24230;&#30740;&#31350;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#22312;&#28436;&#21270;&#27010;&#29575;&#20998;&#24067;&#19978;&#28385;&#36275;&#24191;&#20041;&#23610;&#24230;&#31354;&#38388;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.07900</link><description>&lt;p&gt;
&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#30340;&#24191;&#20041;&#23610;&#24230;&#31354;&#38388;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Generalised Scale-Space Properties for Probabilistic Diffusion Models. (arXiv:2303.07900v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#23610;&#24230;&#31354;&#38388;&#30740;&#31350;&#30340;&#35282;&#24230;&#30740;&#31350;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#22312;&#28436;&#21270;&#27010;&#29575;&#20998;&#24067;&#19978;&#28385;&#36275;&#24191;&#20041;&#23610;&#24230;&#31354;&#38388;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#22312;&#28145;&#24230;&#23398;&#20064;&#31038;&#21306;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#23427;&#20204;&#29983;&#25104;&#20174;&#23398;&#20064;&#22270;&#20687;&#20998;&#24067;&#30340;&#20196;&#20154;&#20449;&#26381;&#30340;&#26679;&#26412;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#36825;&#20123;&#26041;&#27861;&#26368;&#21021;&#26159;&#21463;&#28418;&#31227;-&#25193;&#25955;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#20294;&#22312;&#36817;&#26399;&#30340;&#23454;&#36341;&#23548;&#21521;&#30340;&#20986;&#29256;&#29289;&#20013;&#65292;&#36825;&#20123;&#36215;&#28304;&#24471;&#21040;&#20102;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#20174;&#23610;&#24230;&#31354;&#38388;&#30740;&#31350;&#30340;&#35282;&#24230;&#30740;&#31350;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#22312;&#28436;&#21270;&#27010;&#29575;&#20998;&#24067;&#19978;&#28385;&#36275;&#24191;&#20041;&#23610;&#24230;&#31354;&#38388;&#29305;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#19990;&#30028;&#20013;&#28418;&#31227;-&#25193;&#25955;&#29289;&#29702;&#26680;&#24515;&#27010;&#24565;&#35299;&#37322;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#32771;&#23519;&#20102;&#27010;&#29575;&#25193;&#25955;&#19982;&#28183;&#36879;&#28388;&#27874;&#22120;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic diffusion models enjoy increasing popularity in the deep learning community. They generate convincing samples from a learned distribution of input images with a wide field of practical applications. Originally, these approaches were motivated from drift-diffusion processes, but these origins find less attention in recent, practice-oriented publications.  We investigate probabilistic diffusion models from the viewpoint of scale-space research and show that they fulfil generalised scale-space properties on evolving probability distributions. Moreover, we discuss similarities and differences between interpretations of the physical core concept of drift-diffusion in the deep learning and model-based world. To this end, we examine relations of probabilistic diffusion to osmosis filters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#36817;&#24180;&#26469;&#26426;&#22120;&#23398;&#20064;&#22312;&#23454;&#39564;&#22266;&#20307;&#21147;&#23398;&#39046;&#22495;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#29289;&#29702;&#24863;&#30693;&#21644;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#28085;&#30422;&#20102;&#26029;&#35010;&#21147;&#23398;&#12289;&#29983;&#29289;&#21147;&#23398;&#12289;&#32435;&#31859;&#21644;&#24494;&#35266;&#21147;&#23398;&#12289;&#26500;&#24314;&#26448;&#26009;&#21644;&#20108;&#32500;&#26448;&#26009;&#31561;&#20256;&#32479;&#21644;&#26032;&#20852;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2303.07647</link><description>&lt;p&gt;
&#23454;&#39564;&#22266;&#20307;&#21147;&#23398;&#20013;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#19982;&#24212;&#29992;&#65306;&#19968;&#31687;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Recent Advances and Applications of Machine Learning in Experimental Solid Mechanics: A Review. (arXiv:2303.07647v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#36817;&#24180;&#26469;&#26426;&#22120;&#23398;&#20064;&#22312;&#23454;&#39564;&#22266;&#20307;&#21147;&#23398;&#39046;&#22495;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#29289;&#29702;&#24863;&#30693;&#21644;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#28085;&#30422;&#20102;&#26029;&#35010;&#21147;&#23398;&#12289;&#29983;&#29289;&#21147;&#23398;&#12289;&#32435;&#31859;&#21644;&#24494;&#35266;&#21147;&#23398;&#12289;&#26500;&#24314;&#26448;&#26009;&#21644;&#20108;&#32500;&#26448;&#26009;&#31561;&#20256;&#32479;&#21644;&#26032;&#20852;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#26469;&#65292;&#23454;&#39564;&#22266;&#20307;&#21147;&#23398;&#22312;&#34920;&#24449;&#21644;&#29702;&#35299;&#22825;&#28982;&#21644;&#26032;&#26448;&#26009;&#30340;&#21147;&#23398;&#24615;&#36136;&#26041;&#38754;&#21457;&#25381;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#20026;&#35813;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#36935;&#65292;&#21253;&#25324;&#23454;&#39564;&#35774;&#35745;&#12289;&#25968;&#25454;&#20998;&#26512;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#21453;&#38382;&#39064;&#12290;&#30001;&#20110;&#36817;&#24180;&#26469;&#35813;&#26032;&#20852;&#39046;&#22495;&#21457;&#34920;&#30340;&#35770;&#25991;&#25968;&#37327;&#36805;&#36895;&#22686;&#21152;&#65292;&#22240;&#27492;&#21450;&#26102;&#36827;&#34892;&#20840;&#38754;&#21644;&#26356;&#26032;&#30340;&#32508;&#36848;&#65292;&#23545;&#20110;&#26368;&#36817;&#26426;&#22120;&#23398;&#20064;&#22312;&#23454;&#39564;&#22266;&#20307;&#21147;&#23398;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;&#19982;&#35813;&#32508;&#36848;&#30456;&#20851;&#30340;&#24120;&#35265;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#26415;&#35821;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#22522;&#20110;&#29289;&#29702;&#23398;&#21644;&#29289;&#29702;&#24863;&#30693;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20840;&#38754;&#28085;&#30422;&#20102;&#23454;&#39564;&#21147;&#23398;&#20256;&#32479;&#21644;&#26032;&#20852;&#39046;&#22495;&#20013;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#24212;&#29992;&#65292;&#21253;&#25324;&#26029;&#35010;&#21147;&#23398;&#12289;&#29983;&#29289;&#21147;&#23398;&#12289;&#32435;&#31859;&#21644;&#24494;&#35266;&#21147;&#23398;&#12289;&#26500;&#24314;&#26448;&#26009;&#21644;&#20108;&#32500;&#26448;&#26009;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#24403;&#21069;&#27963;&#36291;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#39046;&#22495;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#26368;&#21518;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#23454;&#39564;&#22266;&#20307;&#21147;&#23398;&#26410;&#26469;&#30340;&#28508;&#22312;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
For many decades, experimental solid mechanics has played a crucial role in characterizing and understanding the mechanical properties of natural and novel materials. Recent advances in machine learning (ML) provide new opportunities for the field, including experimental design, data analysis, uncertainty quantification, and inverse problems. As the number of papers published in recent years in this emerging field is exploding, it is timely to conduct a comprehensive and up-to-date review of recent ML applications in experimental solid mechanics. Here, we first provide an overview of common ML algorithms and terminologies that are pertinent to this review, with emphasis placed on physics-informed and physics-based ML methods. Then, we provide thorough coverage of recent ML applications in traditional and emerging areas of experimental mechanics, including fracture mechanics, biomechanics, nano- and micro-mechanics, architected materials, and 2D material. Finally, we highlight some curr
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#65292;&#23558;&#24515;&#34880;&#31649;&#23454;&#39564;&#23460;&#25351;&#26631;&#30340;&#24739;&#32773;&#36827;&#23637;&#36235;&#21183;&#20174;&#24120;&#35265;&#24773;&#20917;&#36716;&#31227;&#21040;&#32597;&#35265;&#25110;&#29305;&#23450;&#24515;&#34880;&#31649;&#20107;&#20214;&#26816;&#27979;&#20013;&#65292;&#20197;&#21327;&#21161;&#26816;&#27979;&#32463;&#30382;&#20896;&#29366;&#21160;&#33033;&#20171;&#20837;&#27835;&#30103;&#24739;&#32773;&#30340;&#38774;&#34880;&#31649;&#37325;&#24314;&#12290;</title><link>http://arxiv.org/abs/2303.06980</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#30340;&#24515;&#34880;&#31649;&#20107;&#20214;&#26816;&#27979;&#36890;&#29992;&#23454;&#39564;&#23460;&#36827;&#23637;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Self-supervised based general laboratory progress pretrained model for cardiovascular event detection. (arXiv:2303.06980v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06980
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#65292;&#23558;&#24515;&#34880;&#31649;&#23454;&#39564;&#23460;&#25351;&#26631;&#30340;&#24739;&#32773;&#36827;&#23637;&#36235;&#21183;&#20174;&#24120;&#35265;&#24773;&#20917;&#36716;&#31227;&#21040;&#32597;&#35265;&#25110;&#29305;&#23450;&#24515;&#34880;&#31649;&#20107;&#20214;&#26816;&#27979;&#20013;&#65292;&#20197;&#21327;&#21161;&#26816;&#27979;&#32463;&#30382;&#20896;&#29366;&#21160;&#33033;&#20171;&#20837;&#27835;&#30103;&#24739;&#32773;&#30340;&#38774;&#34880;&#31649;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23450;&#26399;&#30417;&#27979;&#26159;&#31649;&#29702;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#24517;&#35201;&#26041;&#38754;&#12290;&#30001;&#20110;&#32597;&#35265;&#25110;&#29305;&#23450;&#30142;&#30149;&#30340;&#24739;&#32773;&#35268;&#27169;&#36739;&#23567;&#65292;&#35266;&#23519;&#20063;&#26159;&#38388;&#27463;&#24615;&#30340;&#65292;&#22240;&#27492;&#20854;&#25307;&#21215;&#24120;&#24120;&#21463;&#21040;&#38480;&#21046;&#65292;&#32780;&#24120;&#35265;&#24773;&#20917;&#30001;&#20110;&#23450;&#26399;&#38543;&#35775;&#32780;&#26356;&#23481;&#26131;&#32047;&#31215;&#32437;&#21521;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#20197;&#20854;&#26080;&#35268;&#24459;&#24615;&#12289;&#26102;&#38388;&#24615;&#12289;&#32570;&#24109;&#24615;&#21644;&#31232;&#30095;&#24615;&#32780;&#38395;&#21517;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#26469;&#20811;&#26381;&#19978;&#36848;&#38556;&#30861;&#65292;&#23558;&#24515;&#34880;&#31649;&#23454;&#39564;&#23460;&#25351;&#26631;&#30340;&#24739;&#32773;&#36827;&#23637;&#36235;&#21183;&#20174;&#24120;&#35265;&#24773;&#20917;&#36716;&#31227;&#21040;&#32597;&#35265;&#25110;&#29305;&#23450;&#24515;&#34880;&#31649;&#20107;&#20214;&#26816;&#27979;&#20013;&#12290;&#25105;&#20204;&#20351;&#29992;&#39640;&#34880;&#21387;&#24739;&#32773;&#65288;&#23578;&#26410;&#24739;&#31958;&#23615;&#30149;&#65289;&#36827;&#34892;&#20102;&#19968;&#33324;&#23454;&#39564;&#23460;&#36827;&#23637;&#65288;GLP&#65289;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#65292;&#24182;&#23558;&#20854;&#23454;&#39564;&#23460;&#36827;&#23637;&#36235;&#21183;&#36716;&#31227;&#65292;&#20197;&#21327;&#21161;&#26816;&#27979;&#32463;&#30382;&#20896;&#29366;&#21160;&#33033;&#20171;&#20837;&#27835;&#30103;&#24739;&#32773;&#30340;&#38774;&#34880;&#31649;&#37325;&#24314;&#65288;TVR&#65289;&#12290;GLP&#37319;&#29992;&#20102;&#20004;&#20010;&#38454;&#27573;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#21253;&#25324;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regular surveillance is an indispensable aspect of managing cardiovascular disorders. Patient recruitment for rare or specific diseases is often limited due to their small patient size and episodic observations, whereas prevalent cases accumulate longitudinal data easily due to regular follow-ups. These data, however, are notorious for their irregularity, temporality, absenteeism, and sparsity. In this study, we leveraged self-supervised learning (SSL) and transfer learning to overcome the above-mentioned barriers, transferring patient progress trends in cardiovascular laboratory parameters from prevalent cases to rare or specific cardiovascular events detection. We pretrained a general laboratory progress (GLP) pretrain model using hypertension patients (who were yet to be diabetic), and transferred their laboratory progress trend to assist in detecting target vessel revascularization (TVR) in percutaneous coronary intervention patients. GLP adopted a two-stage training process that u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#20808;&#39564;&#26657;&#20934;&#30340;softmax&#20989;&#25968;&#26469;&#35745;&#31639;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;&#22522;&#20110;&#21407;&#22411;&#30340;&#29305;&#24449;&#25552;&#21462;&#26469;&#32500;&#25252;&#19968;&#20010;&#24179;&#34913;&#30340;&#20998;&#31867;&#22120;&#22836;&#65292;&#20197;&#31283;&#23450;&#21644;&#25913;&#36827;&#32852;&#37030;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2303.06314</link><description>&lt;p&gt;
&#22312;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#36890;&#36807;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#21644;&#23458;&#25143;&#31471;dropout&#26469;&#31283;&#23450;&#21644;&#25913;&#36827;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Stabilizing and Improving Federated Learning with Non-IID Data and Client Dropout in IoT Systems. (arXiv:2303.06314v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#20808;&#39564;&#26657;&#20934;&#30340;softmax&#20989;&#25968;&#26469;&#35745;&#31639;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;&#22522;&#20110;&#21407;&#22411;&#30340;&#29305;&#24449;&#25552;&#21462;&#26469;&#32500;&#25252;&#19968;&#20010;&#24179;&#34913;&#30340;&#20998;&#31867;&#22120;&#22836;&#65292;&#20197;&#31283;&#23450;&#21644;&#25913;&#36827;&#32852;&#37030;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a simple yet effective framework to stabilize and improve federated learning by introducing a prior-calibrated softmax function for computing the cross-entropy loss and a prototype-based feature extraction to maintain a balanced classifier head.
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#22312;&#19981;&#26292;&#38706;&#31169;&#26377;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#22312;&#20998;&#25955;&#30340;&#23458;&#25143;&#31471;&#19978;&#35757;&#32451;&#28145;&#24230;&#27169;&#22411;&#65292;&#28982;&#32780;&#23427;&#21463;&#21040;&#26631;&#31614;&#20998;&#24067;&#20559;&#26012;&#30340;&#24433;&#21709;&#65292;&#36890;&#24120;&#23548;&#33268;&#25910;&#25947;&#32531;&#24930;&#21644;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#12290;&#24403;&#21442;&#19982;&#30340;&#23458;&#25143;&#31471;&#22788;&#20110;&#19981;&#31283;&#23450;&#30340;&#29615;&#22659;&#24182;&#32463;&#24120;&#25481;&#32447;&#26102;&#65292;&#36825;&#20010;&#25361;&#25112;&#21487;&#33021;&#26356;&#21152;&#20005;&#37325;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#20808;&#39564;&#26657;&#20934;&#30340;softmax&#20989;&#25968;&#26469;&#35745;&#31639;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;&#22522;&#20110;&#21407;&#22411;&#30340;&#29305;&#24449;&#25552;&#21462;&#26469;&#32500;&#25252;&#19968;&#20010;&#24179;&#34913;&#30340;&#20998;&#31867;&#22120;&#22836;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is an emerging technique for training deep models over decentralized clients without exposing private data, which however suffers from label distribution skew and usually results in slow convergence and degraded model performance. This challenge could be more serious when the participating clients are in unstable circumstances and dropout frequently. Previous work and our empirical observations demonstrate that the classifier head for classification task is more sensitive to label skew and the unstable performance of FedAvg mainly lies in the imbalanced training samples across different classes. The biased classifier head will also impact the learning of feature representations. Therefore, maintaining a balanced classifier head is of significant importance for building a better global model. To tackle this issue, we propose a simple yet effective framework by introducing a prior-calibrated softmax function for computing the cross-entropy loss and a prototype-based fe
&lt;/p&gt;</description></item><item><title>STAIR&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#24322;&#24120;&#20540;&#27719;&#24635;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#32039;&#20945;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#35268;&#21017;&#65292;&#20197;&#27719;&#24635;&#21644;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#32467;&#26524;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#20934;&#30830;&#22320;&#24635;&#32467;&#26816;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.06261</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#24322;&#24120;&#20540;&#27719;&#24635;
&lt;/p&gt;
&lt;p&gt;
Interpretable Outlier Summarization. (arXiv:2303.06261v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06261
&lt;/p&gt;
&lt;p&gt;
STAIR&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#24322;&#24120;&#20540;&#27719;&#24635;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#32039;&#20945;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#35268;&#21017;&#65292;&#20197;&#27719;&#24635;&#21644;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#32467;&#26524;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#20934;&#30830;&#22320;&#24635;&#32467;&#26816;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
STAIR proposes an interpretable outlier summarization method by learning a compact set of human understandable rules to summarize and explain the anomaly detection results, which has strong interpretability to accurately summarize the detection results.
&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#20540;&#26816;&#27979;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20197;&#38450;&#27490;&#37329;&#34701;&#27450;&#35784;&#12289;&#38450;&#24481;&#32593;&#32476;&#20837;&#20405;&#25110;&#26816;&#27979;&#21363;&#23558;&#21457;&#29983;&#30340;&#35774;&#22791;&#25925;&#38556;&#12290;&#20026;&#20102;&#20943;&#23569;&#20154;&#21147;&#35780;&#20272;&#24322;&#24120;&#20540;&#26816;&#27979;&#32467;&#26524;&#30340;&#24037;&#20316;&#37327;&#65292;&#24182;&#26377;&#25928;&#22320;&#23558;&#24322;&#24120;&#20540;&#36716;&#21270;&#20026;&#21487;&#25805;&#20316;&#30340;&#35265;&#35299;&#65292;&#29992;&#25143;&#36890;&#24120;&#24076;&#26395;&#31995;&#32479;&#33258;&#21160;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#24322;&#24120;&#20540;&#26816;&#27979;&#32467;&#26524;&#30340;&#23376;&#32452;&#30340;&#27719;&#24635;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#27809;&#26377;&#36825;&#26679;&#30340;&#31995;&#32479;&#23384;&#22312;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;STAIR&#65292;&#23427;&#23398;&#20064;&#20102;&#19968;&#32452;&#32039;&#20945;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#35268;&#21017;&#65292;&#20197;&#27719;&#24635;&#21644;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#32467;&#26524;&#12290;STAIR&#19981;&#20351;&#29992;&#32463;&#20856;&#30340;&#20915;&#31574;&#26641;&#31639;&#27861;&#26469;&#20135;&#29983;&#36825;&#20123;&#35268;&#21017;&#65292;&#32780;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#20197;&#20135;&#29983;&#23569;&#37327;&#35268;&#21017;&#65292;&#20855;&#26377;&#26368;&#23567;&#30340;&#22797;&#26434;&#24615;&#65292;&#22240;&#27492;&#20855;&#26377;&#24378;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#20934;&#30830;&#22320;&#24635;&#32467;&#26816;&#27979;&#32467;&#26524;&#12290;STAIR&#30340;&#23398;&#20064;&#31639;&#27861;&#36890;&#36807;&#36845;&#20195;&#20998;&#21106;&#22823;&#35268;&#21017;&#26469;&#20135;&#29983;&#35268;&#21017;&#38598;&#65292;&#24182;&#22312;&#27599;&#20010;i&#20013;&#26368;&#22823;&#21270;&#36825;&#20010;&#30446;&#26631;&#65292;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Outlier detection is critical in real applications to prevent financial fraud, defend network intrusions, or detecting imminent device failures. To reduce the human effort in evaluating outlier detection results and effectively turn the outliers into actionable insights, the users often expect a system to automatically produce interpretable summarizations of subgroups of outlier detection results. Unfortunately, to date no such systems exist. To fill this gap, we propose STAIR which learns a compact set of human understandable rules to summarize and explain the anomaly detection results. Rather than use the classical decision tree algorithms to produce these rules, STAIR proposes a new optimization objective to produce a small number of rules with least complexity, hence strong interpretability, to accurately summarize the detection results. The learning algorithm of STAIR produces a rule set by iteratively splitting the large rules and is optimal in maximizing this objective in each i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#25511;&#21046;&#21464;&#37327;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#26377;&#38480;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20943;&#23567;&#33945;&#29305;&#21345;&#32599;&#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#65292;&#24182;&#23545;&#22810;&#20010;&#20219;&#21153;&#36827;&#34892;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2303.04756</link><description>&lt;p&gt;
&#20803;&#23398;&#20064;&#25511;&#21046;&#21464;&#37327;&#65306;&#26377;&#38480;&#25968;&#25454;&#20013;&#26041;&#24046;&#32553;&#20943;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Meta-learning Control Variates: Variance Reduction with Limited Data. (arXiv:2303.04756v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04756
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#25511;&#21046;&#21464;&#37327;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#26377;&#38480;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20943;&#23567;&#33945;&#29305;&#21345;&#32599;&#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#65292;&#24182;&#23545;&#22810;&#20010;&#20219;&#21153;&#36827;&#34892;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25511;&#21046;&#21464;&#37327;&#26159;&#20943;&#23567;&#33945;&#29305;&#21345;&#32599;&#20272;&#35745;&#22120;&#26041;&#24046;&#30340;&#26377;&#21147;&#24037;&#20855;&#65292;&#20294;&#22312;&#26679;&#26412;&#25968;&#37327;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#26500;&#24314;&#26377;&#25928;&#30340;&#25511;&#21046;&#21464;&#37327;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#24403;&#38656;&#35201;&#35745;&#31639;&#22823;&#37327;&#30456;&#20851;&#31215;&#20998;&#26102;&#65292;&#21363;&#20351;&#27599;&#20010;&#20219;&#21153;&#30340;&#26679;&#26412;&#25968;&#24456;&#23569;&#65292;&#20063;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#31215;&#20998;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#20803;&#23398;&#20064;CV&#65288;Meta-CVs&#65289;&#26041;&#27861;&#21487;&#29992;&#20110;&#22788;&#29702;&#25968;&#30334;&#20010;&#25110;&#25968;&#21315;&#20010;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;Meta-CVs&#21487;&#20197;&#26174;&#33879;&#20943;&#23567;&#26041;&#24046;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#30830;&#23450;&#20102;Meta-CVs&#25104;&#21151;&#35757;&#32451;&#30340;&#19968;&#33324;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Control variates can be a powerful tool to reduce the variance of Monte Carlo estimators, but constructing effective control variates can be challenging when the number of samples is small. In this paper, we show that when a large number of related integrals need to be computed, it is possible to leverage the similarity between these integration tasks to improve performance even when the number of samples per task is very small. Our approach, called meta learning CVs (Meta-CVs), can be used for up to hundreds or thousands of tasks. Our empirical assessment indicates that Meta-CVs can lead to significant variance reduction in such settings, and our theoretical analysis establishes general conditions under which Meta-CVs can be successfully trained.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#21521;&#37327;&#37327;&#21270;&#25216;&#26415;&#21644;&#21452;&#21521;&#21464;&#21387;&#22120;&#27169;&#22411;&#26469;&#29983;&#25104;&#36136;&#37327;&#26356;&#22909;&#12289;&#27169;&#22359;&#21270;&#21464;&#21270;&#26356;&#24555;&#30340;&#21512;&#25104;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2303.04743</link><description>&lt;p&gt;
&#24102;&#26377;&#21452;&#21521;&#20808;&#39564;&#27169;&#22411;&#30340;&#21521;&#37327;&#37327;&#21270;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Vector Quantized Time Series Generation with a Bidirectional Prior Model. (arXiv:2303.04743v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#21521;&#37327;&#37327;&#21270;&#25216;&#26415;&#21644;&#21452;&#21521;&#21464;&#21387;&#22120;&#27169;&#22411;&#26469;&#29983;&#25104;&#36136;&#37327;&#26356;&#22909;&#12289;&#27169;&#22359;&#21270;&#21464;&#21270;&#26356;&#24555;&#30340;&#21512;&#25104;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#19982;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21464;&#20307;&#30456;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451; GAN &#30340;&#22522;&#26412;&#38480;&#21046;&#21644;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#12290;&#27492;&#22806;&#65292;RNN&#26063;&#36890;&#24120;&#22312;&#36828;&#31243;&#26102;&#38388;&#27493;&#20043;&#38388;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#21463;&#21040;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986; TimeVQVAE&#65292;&#36825;&#26159;&#25105;&#20204;&#25152;&#30693;&#36947;&#30340;&#31532;&#19968;&#20010;&#20351;&#29992;&#21521;&#37327;&#37327;&#21270;&#65288;VQ&#65289;&#25216;&#26415;&#35299;&#20915; TSG &#38382;&#39064;&#30340;&#24037;&#20316;&#12290;&#27492;&#22806;&#65292;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#30340;&#20808;&#39564;&#20351;&#29992;&#21452;&#21521;&#21464;&#21387;&#22120;&#27169;&#22411;&#36827;&#34892;&#23398;&#20064;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#20840;&#23616;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#22312;&#26102;&#38388; - &#39057;&#29575;&#22495;&#20013;&#36827;&#34892; VQ &#24314;&#27169;&#65292;&#20998;&#20026;&#20302;&#39057;&#65288;LF&#65289;&#21644;&#39640;&#39057;&#65288;HF&#65289;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20445;&#30041;&#26102;&#38388;&#24207;&#21015;&#30340;&#37325;&#35201;&#29305;&#24449;&#65292;&#24182;&#29983;&#25104;&#36136;&#37327;&#26356;&#22909;&#12289;&#27169;&#22359;&#24615;&#21464;&#21270;&#26356;&#24555;&#30340;&#26032;&#21512;&#25104;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series generation (TSG) studies have mainly focused on the use of Generative Adversarial Networks (GANs) combined with recurrent neural network (RNN) variants. However, the fundamental limitations and challenges of training GANs still remain. In addition, the RNN-family typically has difficulties with temporal consistency between distant timesteps. Motivated by the successes in the image generation (IMG) domain, we propose TimeVQVAE, the first work, to our knowledge, that uses vector quantization (VQ) techniques to address the TSG problem. Moreover, the priors of the discrete latent spaces are learned with bidirectional transformer models that can better capture global temporal consistency. We also propose VQ modeling in a time-frequency domain, separated into low-frequency (LF) and high-frequency (HF). This allows us to retain important characteristics of the time series and, in turn, generate new synthetic signals that are of better quality, with sharper changes in modularity, t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#27491;&#26679;&#26412;&#26410;&#26631;&#27880;&#23398;&#20064;&#30340;&#25913;&#36827;&#38750;&#27861;&#33410;&#28857;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#35752;&#38544;&#34255;&#27491;&#26679;&#26412;&#30340;&#26631;&#31614;&#26426;&#21046;&#20551;&#35774;&#65292;&#19982;&#19968;&#31995;&#21015;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26356;&#21487;&#38752;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.02462</link><description>&lt;p&gt;
&#22522;&#20110;&#27491;&#26679;&#26412;&#26410;&#26631;&#27880;&#23398;&#20064;&#30340;&#25552;&#39640;&#38750;&#27861;&#33410;&#28857;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Improved Illicit Node Detection with Positive-Unlabelled Learning. (arXiv:2303.02462v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#27491;&#26679;&#26412;&#26410;&#26631;&#27880;&#23398;&#20064;&#30340;&#25913;&#36827;&#38750;&#27861;&#33410;&#28857;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#35752;&#38544;&#34255;&#27491;&#26679;&#26412;&#30340;&#26631;&#31614;&#26426;&#21046;&#20551;&#35774;&#65292;&#19982;&#19968;&#31995;&#21015;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26356;&#21487;&#38752;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21306;&#22359;&#38142;&#32593;&#32476;&#19978;&#26816;&#27979;&#38750;&#27861;&#33410;&#28857;&#26159;&#21152;&#24378;&#26410;&#26469;&#30417;&#31649;&#30340;&#19968;&#39033;&#26377;&#20215;&#20540;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#65292;&#36825;&#20123;&#26041;&#27861;&#20351;&#29992;&#20102;&#19968;&#20123;&#21306;&#22359;&#38142;&#20132;&#26131;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#26377;&#19968;&#23567;&#37096;&#20998;&#26631;&#35760;&#20026;&#27491;&#26679;&#26412;&#65292;&#20854;&#20313;&#26410;&#26631;&#35760;&#65288;PU&#65289;&#12290;&#34429;&#28982;&#19968;&#20123;&#30740;&#31350;&#20351;&#29992;&#20102;&#38543;&#26426;&#26679;&#26412;&#20551;&#23450;&#26410;&#26631;&#35760;&#33410;&#28857;&#26159;&#27491;&#24120;&#33410;&#28857;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#20540;&#24471;&#32771;&#34385;&#38544;&#34255;&#27491;&#26679;&#26412;&#26631;&#31614;&#30340;&#26631;&#31614;&#26426;&#21046;&#20551;&#35774;&#21450;&#20854;&#23545;&#35780;&#20272;&#25351;&#26631;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#22788;&#29702;&#28508;&#22312;&#38544;&#34255;&#27491;&#26679;&#26412;&#30340;PU&#20998;&#31867;&#22120;&#19982;&#24120;&#35268;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#21487;&#20197;&#20855;&#26377;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;PU&#20998;&#31867;&#22120;&#19982;&#19968;&#31995;&#21015;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#65292;&#20197;&#33719;&#24471;&#30456;&#21516;&#25968;&#25454;&#30340;&#19981;&#21516;&#29305;&#24449;&#20998;&#24067;&#65292;&#20197;&#33719;&#24471;&#26356;&#21487;&#38752;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting illicit nodes on blockchain networks is a valuable task for strengthening future regulation. Recent machine learning-based methods proposed to tackle the tasks are using some blockchain transaction datasets with a small portion of samples labeled positive and the rest unlabelled (PU). Albeit the assumption that a random sample of unlabeled nodes are normal nodes is used in some works, we discuss that the label mechanism assumption for the hidden positive labels and its effect on the evaluation metrics is worth considering. We further explore that PU classifiers dealing with potential hidden positive labels can have improved performance compared to regular machine learning models. We test the PU classifiers with a list of graph representation learning methods for obtaining different feature distributions for the same data to have more reliable results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#20027;&#27700;&#19979;&#36733;&#20307;&#30340;&#20248;&#21270;&#35774;&#35745;&#38382;&#39064;&#65292;&#36890;&#36807;&#38598;&#25104;FreeCAD&#21644;OpenFoam&#31561;&#24037;&#20855;&#36827;&#34892;&#33258;&#21160;&#21270;&#35774;&#35745;&#35780;&#20272;&#65292;&#24182;&#37319;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#35299;&#20915;&#20102;&#20248;&#21270;&#20013;&#26679;&#26412;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.14732</link><description>&lt;p&gt;
&#33258;&#20027;&#27700;&#19979;&#36710;&#20307;&#35774;&#35745;&#30340;&#32422;&#26463;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Constrained Bayesian Optimization for Automatic Underwater Vehicle Hull Design. (arXiv:2302.14732v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#20027;&#27700;&#19979;&#36733;&#20307;&#30340;&#20248;&#21270;&#35774;&#35745;&#38382;&#39064;&#65292;&#36890;&#36807;&#38598;&#25104;FreeCAD&#21644;OpenFoam&#31561;&#24037;&#20855;&#36827;&#34892;&#33258;&#21160;&#21270;&#35774;&#35745;&#35780;&#20272;&#65292;&#24182;&#37319;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#35299;&#20915;&#20102;&#20248;&#21270;&#20013;&#26679;&#26412;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#27700;&#19979;&#36710;&#20307;&#35774;&#35745;&#20248;&#21270;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#24037;&#31243;&#36807;&#31243;&#65292;&#26088;&#22312;&#28385;&#36275;&#32473;&#23450;&#35201;&#27714;&#29983;&#25104;&#20855;&#26377;&#20248;&#21270;&#29305;&#24615;&#30340;UUV&#36733;&#20307;&#12290;&#39318;&#20808;&#65292;&#23427;&#28041;&#21450;&#38598;&#25104;&#30456;&#20851;&#30340;&#22797;&#26434;&#24037;&#31243;&#20223;&#30495;&#24037;&#20855;&#12290;&#20854;&#27425;&#65292;&#23427;&#38656;&#35201;&#23558;&#26679;&#26412;&#26377;&#25928;&#30340;&#20248;&#21270;&#26694;&#26550;&#19982;&#38598;&#25104;&#24037;&#20855;&#38142;&#30456;&#32467;&#21512;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#31216;&#20026;FreeCAD&#30340;CAD&#24037;&#20855;&#19982;CFD&#24037;&#20855;openFoam&#38598;&#25104;&#65292;&#20197;&#36827;&#34892;&#33258;&#21160;&#21270;&#35774;&#35745;&#35780;&#20272;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#36827;&#34892;&#20248;&#21270;&#65292;&#36825;&#26159;&#19968;&#31181;&#20026;&#20248;&#21270;&#32791;&#26102;&#26114;&#36149;&#30340;&#24037;&#31243;&#27169;&#25311;&#32780;&#24320;&#21457;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#25216;&#26415;&#65292;&#22312;&#22810;&#31181;&#38382;&#39064;&#20013;&#24050;&#35777;&#26126;&#20855;&#26377;&#38750;&#24120;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#21253;&#25324;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#23454;&#39564;&#35774;&#35745;&#12290;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#19981;&#21487;&#34892;&#35774;&#35745;&#20316;&#20026;&#32422;&#26463;&#38598;&#25104;&#21040;&#20248;&#21270;&#36807;&#31243;&#20013;&#12290;&#36890;&#36807;&#23558;&#39046;&#22495;&#19987;&#29992;&#24037;&#20855;&#38142;&#19982;&#22522;&#20110;AI&#30340;&#20248;&#21270;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#25191;&#34892;&#20102;&#33258;&#21160;&#35774;&#35745;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic underwater vehicle hull Design optimization is a complex engineering process for generating a UUV hull with optimized properties on a given requirement. First, it involves the integration of involved computationally complex engineering simulation tools. Second, it needs integration of a sample efficient optimization framework with the integrated toolchain. To this end, we integrated the CAD tool called FreeCAD with CFD tool openFoam for automatic design evaluation. For optimization, we chose Bayesian optimization (BO), which is a well-known technique developed for optimizing time-consuming expensive engineering simulations and has proven to be very sample efficient in a variety of problems, including hyper-parameter tuning and experimental design. During the optimization process, we can handle infeasible design as constraints integrated into the optimization process. By integrating domain-specific toolchain with AI-based optimization, we executed the automatic design optimiza
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GradMA&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#26799;&#24230;&#20869;&#23384;&#21152;&#36895;&#32852;&#37030;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#25345;&#32493;&#23398;&#20064;&#26469;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#23454;&#39564;&#34920;&#26126;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#12289;&#36890;&#20449;&#25928;&#29575;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#32531;&#35299;&#26041;&#38754;&#37117;&#26377;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2302.14307</link><description>&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#20869;&#23384;&#30340;&#21152;&#36895;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;GradMA&#24182;&#32531;&#35299;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
GradMA: A Gradient-Memory-based Accelerated Federated Learning with Alleviated Catastrophic Forgetting. (arXiv:2302.14307v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14307
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GradMA&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#26799;&#24230;&#20869;&#23384;&#21152;&#36895;&#32852;&#37030;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#25345;&#32493;&#23398;&#20064;&#26469;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#23454;&#39564;&#34920;&#26126;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#12289;&#36890;&#20449;&#25928;&#29575;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#32531;&#35299;&#26041;&#38754;&#37117;&#26377;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#21463;&#21040;&#20102;&#31038;&#21306;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22240;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#37096;&#20998;&#21442;&#19982;&#23548;&#33268;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#32473;&#32852;&#37030;&#23398;&#20064;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#65292;&#24433;&#21709;&#20102;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65288;&#21517;&#20026;GradMA&#65289;&#65292;&#23427;&#20174;&#25345;&#32493;&#23398;&#20064;&#20013;&#33719;&#24471;&#21551;&#31034;&#65292;&#21516;&#26102;&#32416;&#27491;&#26381;&#21153;&#22120;&#31471;&#21644;&#24037;&#20316;&#31471;&#30340;&#26356;&#26032;&#26041;&#21521;&#65292;&#24182;&#20805;&#20998;&#21033;&#29992;&#26381;&#21153;&#22120;&#30340;&#20016;&#23500;&#35745;&#31639;&#21644;&#20869;&#23384;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#38416;&#36848;&#20102;&#19968;&#31181;&#20869;&#23384;&#32553;&#20943;&#31574;&#30053;&#65292;&#20351;GradMA&#33021;&#22815;&#36866;&#24212;&#20855;&#26377;&#22823;&#37327;&#24037;&#20316;&#32773;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#20809;&#28369;&#38750;&#20984;&#29615;&#22659;&#19979;&#29702;&#35770;&#20998;&#26512;&#20102;GradMA&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#25910;&#25947;&#36895;&#24230;&#38543;&#30528;&#37319;&#26679;&#27963;&#36291;&#24037;&#20316;&#32773;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#23454;&#29616;&#32447;&#24615;&#21152;&#36895;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;GradMA&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#12289;&#36890;&#20449;&#25928;&#29575;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#32531;&#35299;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has emerged as a de facto machine learning area and received rapid increasing research interests from the community. However, catastrophic forgetting caused by data heterogeneity and partial participation poses distinctive challenges for FL, which are detrimental to the performance. To tackle the problems, we propose a new FL approach (namely GradMA), which takes inspiration from continual learning to simultaneously correct the server-side and worker-side update directions as well as take full advantage of server's rich computing and memory resources. Furthermore, we elaborate a memory reduction strategy to enable GradMA to accommodate FL with a large scale of workers. We then analyze convergence of GradMA theoretically under the smooth non-convex setting and show that its convergence rate achieves a linear speed up w.r.t the increasing number of sampled active workers. At last, our extensive experiments on various image classification tasks show that GradMA ach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Fisher&#32447;&#24615;&#21028;&#21035;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26159;&#20004;&#20010;&#20551;&#35774;&#30340;&#20984;&#32452;&#21512;&#65292;&#21487;&#20197;&#22312;&#19981;&#35775;&#38382;&#20219;&#20309;&#21333;&#20010;&#28304;&#20219;&#21153;&#30340;&#30452;&#25509;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#35745;&#31639;&#26368;&#20248;&#20998;&#31867;&#22120;&#65292;&#24182;&#22312;&#22522;&#20110;EEG&#21644;ECG&#30340;&#20998;&#31867;&#35774;&#32622;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.14186</link><description>&lt;p&gt;
&#24102;&#26377;Fisher&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#30340;&#36817;&#20284;&#26368;&#20248;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Approximately optimal domain adaptation with Fisher's Linear Discriminant Analysis. (arXiv:2302.14186v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Fisher&#32447;&#24615;&#21028;&#21035;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26159;&#20004;&#20010;&#20551;&#35774;&#30340;&#20984;&#32452;&#21512;&#65292;&#21487;&#20197;&#22312;&#19981;&#35775;&#38382;&#20219;&#20309;&#21333;&#20010;&#28304;&#20219;&#21153;&#30340;&#30452;&#25509;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#35745;&#31639;&#26368;&#20248;&#20998;&#31867;&#22120;&#65292;&#24182;&#22312;&#22522;&#20110;EEG&#21644;ECG&#30340;&#20998;&#31867;&#35774;&#32622;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31867;&#22522;&#20110;Fisher&#32447;&#24615;&#21028;&#21035;&#65288;FLD&#65289;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;&#35813;&#31867;&#27169;&#22411;&#26159;&#20004;&#20010;&#20551;&#35774;&#30340;&#20984;&#32452;&#21512;&#65306;i&#65289;&#20195;&#34920;&#20808;&#21069;&#30475;&#21040;&#30340;&#28304;&#20219;&#21153;&#30340;&#24179;&#22343;&#20551;&#35774;&#21644;ii&#65289;&#22312;&#26032;&#30340;&#30446;&#26631;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#20551;&#35774;&#12290;&#23545;&#20110;&#29305;&#23450;&#30340;&#29983;&#25104;&#35774;&#32622;&#65292;&#25105;&#20204;&#22312;0-1&#25439;&#22833;&#19979;&#23548;&#20986;&#20102;&#20004;&#31181;&#27169;&#22411;&#30340;&#26368;&#20248;&#20984;&#32452;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35745;&#31639;&#30340;&#36924;&#36817;&#65292;&#24182;&#30740;&#31350;&#20102;&#21508;&#31181;&#21442;&#25968;&#35774;&#32622;&#23545;&#26368;&#20248;&#20551;&#35774;&#12289;&#20551;&#35774;i&#65289;&#21644;&#20551;&#35774;ii&#65289;&#20043;&#38388;&#30456;&#23545;&#39118;&#38505;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26368;&#20248;&#20998;&#31867;&#22120;&#22312;&#22522;&#20110;EEG&#21644;ECG&#30340;&#20998;&#31867;&#35774;&#32622;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35748;&#20026;&#21487;&#20197;&#22312;&#19981;&#35775;&#38382;&#20219;&#20309;&#21333;&#20010;&#28304;&#20219;&#21153;&#30340;&#30452;&#25509;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#35745;&#31639;&#26368;&#20248;&#20998;&#31867;&#22120;&#12290;&#26368;&#21518;&#25105;&#20204;&#35752;&#35770;&#20102;&#36827;&#19968;&#27493;&#30340;&#24212;&#29992;&#12289;&#38480;&#21046;&#21644;&#21487;&#33021;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a class of models based on Fisher's Linear Discriminant (FLD) in the context of domain adaptation. The class is the convex combination of two hypotheses: i) an average hypothesis representing previously seen source tasks and ii) a hypothesis trained on a new target task. For a particular generative setting we derive the optimal convex combination of the two models under 0-1 loss, propose a computable approximation, and study the effect of various parameter settings on the relative risks between the optimal hypothesis, hypothesis i), and hypothesis ii). We demonstrate the effectiveness of the proposed optimal classifier in the context of EEG- and ECG-based classification settings and argue that the optimal classifier can be computed without access to direct information from any of the individual source tasks. We conclude by discussing further applications, limitations, and possible future directions.
&lt;/p&gt;</description></item><item><title>FTM &#26159;&#19968;&#31181;&#24103;&#32423;&#26102;&#38388;&#32447;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#21516;&#26102;&#25429;&#33719;&#26102;&#38388;&#22270;&#20013;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#29305;&#24449;&#65292;&#24182;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#27604;&#20854;&#20182;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.11814</link><description>&lt;p&gt;
FTM: &#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#24103;&#32423;&#26102;&#38388;&#32447;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FTM: A Frame-level Timeline Modeling Method for Temporal Graph Representation Learning. (arXiv:2302.11814v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11814
&lt;/p&gt;
&lt;p&gt;
FTM &#26159;&#19968;&#31181;&#24103;&#32423;&#26102;&#38388;&#32447;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#21516;&#26102;&#25429;&#33719;&#26102;&#38388;&#22270;&#20013;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#29305;&#24449;&#65292;&#24182;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#27604;&#20854;&#20182;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22270;&#24418;&#32467;&#26500;&#25968;&#25454;&#30340;&#34920;&#31034;&#23545;&#20110;&#22270;&#24418;&#20998;&#26512;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#38745;&#24577;&#22270;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#20020;&#26102;&#22270;&#30340;&#30740;&#31350;&#20173;&#22788;&#20110;&#21021;&#27493;&#38454;&#27573;&#12290;&#26102;&#38388;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#30340; bottleneck &#26159;&#37051;&#22495;&#32858;&#21512;&#31574;&#30053;&#65292;&#22522;&#20110;&#27492;&#22270;&#23646;&#24615;&#26126;&#30830;&#20998;&#20139;&#21644;&#25910;&#38598;&#20449;&#24687;&#12290;&#29616;&#26377;&#30340;&#37051;&#22495;&#32858;&#21512;&#31574;&#30053;&#26080;&#27861;&#25429;&#33719;&#26102;&#38388;&#22270;&#23646;&#24615;&#30340;&#30701;&#26399;&#29305;&#24449;&#25110;&#38271;&#26399;&#29305;&#24449;&#65292;&#23548;&#33268;&#27169;&#22411;&#24615;&#33021;&#19981;&#23481;&#20048;&#35266;&#65292;&#29978;&#33267;&#24433;&#21709;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#39046;&#22495;&#36890;&#29992;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24103;&#32423;&#26102;&#38388;&#32447;&#24314;&#27169;&#65288;FTM&#65289;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#25429;&#33719;&#30701;&#26399;&#21644;&#38271;&#26399;&#29305;&#24449;&#65292;&#20174;&#32780;&#22312;&#26102;&#38388;&#22270;&#19978;&#23398;&#20064;&#26356;&#26377;&#20449;&#24687;&#30340;&#34920;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#38142;&#25509;&#30340;&#26694;&#26550;&#25216;&#26415;&#26469;&#20445;&#30041;&#30701;&#26399;&#29305;&#24449;&#65292;&#28982;&#21518;&#23558;&#20854;&#38598;&#25104;&#21040;&#26102;&#38388;&#32447;&#24314;&#27169;&#26694;&#26550;&#20013;&#20197;&#25429;&#33719;&#38271;&#26399;&#27169;&#24335;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340; FTM &#26041;&#27861;&#22312;&#19977;&#20010;&#19981;&#21516;&#20219;&#21153;&#21644;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning representations for graph-structured data is essential for graph analytical tasks. While remarkable progress has been made on static graphs, researches on temporal graphs are still in its beginning stage. The bottleneck of the temporal graph representation learning approach is the neighborhood aggregation strategy, based on which graph attributes share and gather information explicitly. Existing neighborhood aggregation strategies fail to capture either the short-term features or the long-term features of temporal graph attributes, leading to unsatisfactory model performance and even poor robustness and domain generality of the representation learning method. To address this problem, we propose a Frame-level Timeline Modeling (FTM) method that helps to capture both short-term and long-term features and thus learns more informative representations on temporal graphs. In particular, we present a novel link-based framing technique to preserve the short-term features and then inco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;Fine-grained Two-stage&#35757;&#32451;&#26694;&#26550;&#65288;FiTs&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#30693;&#35782;&#24863;&#30693;&#38382;&#31572;&#65288;KAQA&#65289;&#20013;&#65292;&#20174;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#20013;&#33719;&#24471;&#30340;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#30693;&#35782;&#22312;&#34920;&#31034;&#19978;&#30340;&#24046;&#24322;&#21644;&#32852;&#21512;&#25512;&#29702;&#30340;&#22256;&#38590;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.11799</link><description>&lt;p&gt;
FiTs:&#32454;&#31890;&#24230;&#20004;&#38454;&#27573;&#35757;&#32451;&#29992;&#20110;&#30693;&#35782;&#24863;&#30693;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
FiTs: Fine-grained Two-stage Training for Knowledge-aware Question Answering. (arXiv:2302.11799v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;Fine-grained Two-stage&#35757;&#32451;&#26694;&#26550;&#65288;FiTs&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#30693;&#35782;&#24863;&#30693;&#38382;&#31572;&#65288;KAQA&#65289;&#20013;&#65292;&#20174;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#20013;&#33719;&#24471;&#30340;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#30693;&#35782;&#22312;&#34920;&#31034;&#19978;&#30340;&#24046;&#24322;&#21644;&#32852;&#21512;&#25512;&#29702;&#30340;&#22256;&#38590;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24863;&#30693;&#38382;&#31572;&#65288;KAQA&#65289;&#38656;&#35201;&#27169;&#22411;&#22312;&#30693;&#35782;&#24211;&#20013;&#22238;&#31572;&#38382;&#39064;&#65292;&#36825;&#23545;&#20110;&#24320;&#25918;&#22495;QA&#21644;&#29305;&#23450;&#39046;&#22495;QA&#37117;&#26159;&#24517;&#35201;&#30340;&#65292;&#23588;&#20854;&#26159;&#24403;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#25552;&#20379;&#25152;&#38656;&#30340;&#25152;&#26377;&#30693;&#35782;&#26102;&#12290;&#26368;&#36817;KAQA&#31995;&#32479;&#34701;&#21512;&#20102;&#20174;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#21644;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#20013;&#33719;&#24471;&#30340;&#35821;&#35328;&#30693;&#35782;&#21644;&#20107;&#23454;&#30693;&#35782;&#20197;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#20294;&#26159;&#23384;&#22312;&#22256;&#38590;&#65292;&#21363;&#26377;&#25928;&#22320;&#34701;&#21512;&#26469;&#33258;PLMs&#21644;KGs&#30340;&#34920;&#31034;&#65292;&#22240;&#20026;&#65288;i&#65289;&#23427;&#20204;&#20043;&#38388;&#23384;&#22312;&#35821;&#20041;&#21644;&#20998;&#24067;&#24046;&#24322;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#38590;&#20197;&#32852;&#21512;&#25512;&#29702;&#25552;&#20379;&#30340;&#20004;&#31867;&#30693;&#35782;&#12290;&#38024;&#23545;&#19978;&#36848;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;Fine-grained Two-stage&#35757;&#32451;&#26694;&#26550;&#65288;FiTs&#65289;&#65292;&#26088;&#22312;&#25552;&#39640;KAQA&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#31532;&#19968;&#38454;&#27573;&#26088;&#22312;&#36890;&#36807;&#30693;&#35782;&#36866;&#24212;&#21518;&#35757;&#32451;&#26469;&#23545;&#40784;&#26469;&#33258;PLM&#21644;KG&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#24357;&#21512;&#23427;&#20204;&#20043;&#38388;&#30340;&#27169;&#24577;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge-aware question answering (KAQA) requires the model to answer questions over a knowledge base, which is essential for both open-domain QA and domain-specific QA, especially when language models alone cannot provide all the knowledge needed. Despite the promising result of recent KAQA systems which tend to integrate linguistic knowledge from pre-trained language models (PLM) and factual knowledge from knowledge graphs (KG) to answer complex questions, a bottleneck exists in effectively fusing the representations from PLMs and KGs because of (i) the semantic and distributional gaps between them, and (ii) the difficulties in joint reasoning over the provided knowledge from both modalities. To address the above two problems, we propose a Fine-grained Two-stage training framework (FiTs) to boost the KAQA system performance: The first stage aims at aligning representations from the PLM and the KG, thus bridging the modality gaps between them, named knowledge adaptive post-training. 
&lt;/p&gt;</description></item><item><title>DrasCLR&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20986;&#20004;&#31181;&#39046;&#22495;&#29305;&#23450;&#30340;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#26469;&#23398;&#20064;&#30142;&#30149;&#30456;&#20851;&#21644;&#35299;&#21078;&#29305;&#24322;&#24615;&#34920;&#31034;&#65292;&#29305;&#21035;&#26159;&#35299;&#20915;&#20102;&#21306;&#20998;&#30142;&#30149;&#27169;&#24335;&#21644;&#35299;&#21078;&#29305;&#24449;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2302.10390</link><description>&lt;p&gt;
DrasCLR: &#19968;&#20010;&#29992;&#20110;&#23398;&#20064;&#30142;&#30149;&#30456;&#20851;&#21644;&#35299;&#21078;&#29305;&#24322;&#24615;&#34920;&#31034;&#30340;&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
DrasCLR: A Self-supervised Framework of Learning Disease-related and Anatomy-specific Representation for 3D Medical Images. (arXiv:2302.10390v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10390
&lt;/p&gt;
&lt;p&gt;
DrasCLR&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20986;&#20004;&#31181;&#39046;&#22495;&#29305;&#23450;&#30340;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#26469;&#23398;&#20064;&#30142;&#30149;&#30456;&#20851;&#21644;&#35299;&#21078;&#29305;&#24322;&#24615;&#34920;&#31034;&#65292;&#29305;&#21035;&#26159;&#35299;&#20915;&#20102;&#21306;&#20998;&#30142;&#30149;&#27169;&#24335;&#21644;&#35299;&#21078;&#29305;&#24449;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#35268;&#27169;&#24102;&#27880;&#37322;&#30340;&#20307;&#31215;&#21307;&#23398;&#22270;&#20687;&#30340;&#33719;&#21462;&#25104;&#26412;&#39640;&#26114;&#65292;&#22240;&#27492;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20026;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#39044;&#35757;&#32451;&#21644;&#29305;&#24449;&#25552;&#21462;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#20165;&#20351;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#23454;&#20363;&#21306;&#20998;&#30340;SSL&#26041;&#27861;&#22312;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#21464;&#24471;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;SSL&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#21487;&#33021;&#20351;&#29992;&#35768;&#22810;&#22270;&#20687;&#32447;&#32034;&#26469;&#21306;&#20998;&#19968;&#20010;&#23454;&#20363;&#65292;&#32780;&#36825;&#20123;&#32447;&#32034;&#19981;&#19968;&#23450;&#19982;&#30142;&#30149;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#30149;&#29702;&#27169;&#24335;&#24120;&#24120;&#24456;&#24494;&#22937;&#21644;&#24322;&#36136;&#65292;&#38656;&#35201;&#25152;&#38656;&#26041;&#27861;&#33021;&#22815;&#34920;&#31034;&#23545;&#19981;&#21516;&#36523;&#20307;&#37096;&#20301;&#20013;&#30340;&#24322;&#24120;&#21464;&#21270;&#25935;&#24863;&#30340;&#35299;&#21078;&#29305;&#24322;&#24615;&#29305;&#24449;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DrasCLR&#30340;&#26032;&#30340;SSL&#26694;&#26550;&#65292;&#29992;&#20110;&#19977;&#32500;&#21307;&#23398;&#25104;&#20687;&#65292;&#20197;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#39046;&#22495;&#29305;&#23450;&#30340;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65306;&#19968;&#31181;&#26088;&#22312;&#25429;&#25417;&#23616;&#37096;&#35299;&#21078;&#21306;&#22495;&#20869;&#24494;&#22937;&#30142;&#30149;&#27169;&#24335;&#65292;&#21478;&#19968;&#31181;&#26088;&#22312;&#35782;&#21035;&#20855;&#26377;&#19981;&#21516;&#25361;&#25112;&#24615;&#30340;&#35299;&#21078;&#21306;&#22495;&#38388;&#30340;&#30142;&#30149;&#30456;&#20851;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale volumetric medical images with annotation are rare, costly, and time prohibitive to acquire. Self-supervised learning (SSL) offers a promising pre-training and feature extraction solution for many downstream tasks, as it only uses unlabeled data. Recently, SSL methods based on instance discrimination have gained popularity in the medical imaging domain. However, SSL pre-trained encoders may use many clues in the image to discriminate an instance that are not necessarily disease-related. Moreover, pathological patterns are often subtle and heterogeneous, requiring the ability of the desired method to represent anatomy-specific features that are sensitive to abnormal changes in different body parts. In this work, we present a novel SSL framework, named DrasCLR, for 3D medical imaging to overcome these challenges. We propose two domain-specific contrastive learning strategies: one aims to capture subtle disease patterns inside a local anatomical region, and the other aims to r
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#26680;&#23725;&#22238;&#24402;&#30340;&#21327;&#21464;&#37327;&#36716;&#31227;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;&#29992;&#20266;&#26631;&#31614;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#65292;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#29305;&#24449;&#20998;&#24067;&#19979;&#30340;&#23398;&#20064;&#65292;&#23454;&#29616;&#22343;&#26041;&#35823;&#24046;&#26368;&#23567;&#21270;&#12290;</title><link>http://arxiv.org/abs/2302.10160</link><description>&lt;p&gt;
&#26680;&#23725;&#22238;&#24402;&#19979;&#20266;&#26631;&#31614;&#30340;&#21327;&#21464;&#37327;&#36716;&#31227;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Pseudo-Labeling for Kernel Ridge Regression under Covariate Shift. (arXiv:2302.10160v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10160
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#26680;&#23725;&#22238;&#24402;&#30340;&#21327;&#21464;&#37327;&#36716;&#31227;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;&#29992;&#20266;&#26631;&#31614;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#65292;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#29305;&#24449;&#20998;&#24067;&#19979;&#30340;&#23398;&#20064;&#65292;&#23454;&#29616;&#22343;&#26041;&#35823;&#24046;&#26368;&#23567;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#22522;&#20110;&#21327;&#21464;&#37327;&#36716;&#31227;&#30340;&#26680;&#23725;&#22238;&#24402;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#30446;&#26631;&#20998;&#24067;&#19978;&#23398;&#20064;&#19968;&#20010;&#22343;&#26041;&#35823;&#24046;&#26368;&#23567;&#30340;&#22238;&#24402;&#20989;&#25968;&#65292;&#22522;&#20110;&#20174;&#30446;&#26631;&#20998;&#24067;&#37319;&#26679;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#20998;&#24067;&#30340;&#24050;&#26631;&#35760;&#25968;&#25454;&#12290;&#25105;&#20204;&#23558;&#24050;&#26631;&#35760;&#25968;&#25454;&#20998;&#25104;&#20004;&#20010;&#23376;&#38598;&#65292;&#24182;&#20998;&#21035;&#36827;&#34892;&#26680;&#23725;&#22238;&#24402;&#65292;&#20197;&#33719;&#24471;&#20505;&#36873;&#27169;&#22411;&#38598;&#21512;&#21644;&#19968;&#20010;&#22635;&#20805;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#21518;&#32773;&#22635;&#20805;&#32570;&#22833;&#30340;&#26631;&#31614;&#65292;&#28982;&#21518;&#30456;&#24212;&#22320;&#36873;&#25321;&#26368;&#20339;&#30340;&#20505;&#36873;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#38750;&#28176;&#36817;&#24615;&#36807;&#37327;&#39118;&#38505;&#30028;&#34920;&#26126;&#65292;&#22312;&#30456;&#24403;&#19968;&#33324;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#33021;&#22815;&#36866;&#24212;&#30446;&#26631;&#20998;&#24067;&#20197;&#21450;&#21327;&#21464;&#37327;&#36716;&#31227;&#30340;&#32467;&#26500;&#12290;&#23427;&#33021;&#22815;&#23454;&#29616;&#28176;&#36817;&#27491;&#24577;&#35823;&#24046;&#29575;&#30452;&#21040;&#23545;&#25968;&#22240;&#23376;&#30340;&#26368;&#23567;&#26497;&#38480;&#20248;&#21270;&#12290;&#22312;&#27169;&#22411;&#36873;&#25321;&#20013;&#20351;&#29992;&#20266;&#26631;&#31614;&#19981;&#20250;&#20135;&#29983;&#20027;&#35201;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop and analyze a principled approach to kernel ridge regression under covariate shift. The goal is to learn a regression function with small mean squared error over a target distribution, based on unlabeled data from there and labeled data that may have a different feature distribution. We propose to split the labeled data into two subsets and conduct kernel ridge regression on them separately to obtain a collection of candidate models and an imputation model. We use the latter to fill the missing labels and then select the best candidate model accordingly. Our non-asymptotic excess risk bounds show that in quite general scenarios, our estimator adapts to the structure of the target distribution as well as the covariate shift. It achieves the minimax optimal error rate up to a logarithmic factor. The use of pseudo-labels in model selection does not have major negative impacts.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#33324;&#31867;&#30340;&#38750;&#32447;&#24615;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#22810;&#27169;&#24335;&#23545;&#27604;&#23398;&#20064;&#65292;&#25581;&#31034;&#20102;&#20854;&#19982;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#32852;&#31995;&#12290;&#24182;&#35777;&#26126;&#22312;&#38169;&#35823;&#21305;&#37197;&#30340;&#24773;&#20917;&#19979;&#65292;&#22810;&#27169;&#24335;&#23545;&#27604;&#23398;&#20064;&#21487;&#20197;&#27604;&#21333;&#27169;&#24335;&#23545;&#27604;&#23398;&#20064;&#34920;&#29616;&#26356;&#20339;&#65292;&#34920;&#29616;&#20102;&#20854;&#23545;&#22024;&#26434;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.06232</link><description>&lt;p&gt;
&#29702;&#35299;&#22810;&#27169;&#24335;&#23545;&#27604;&#23398;&#20064;&#21450;&#25972;&#21512;&#38750;&#37197;&#23545;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Understanding Multimodal Contrastive Learning and Incorporating Unpaired Data. (arXiv:2302.06232v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#33324;&#31867;&#30340;&#38750;&#32447;&#24615;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#22810;&#27169;&#24335;&#23545;&#27604;&#23398;&#20064;&#65292;&#25581;&#31034;&#20102;&#20854;&#19982;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#32852;&#31995;&#12290;&#24182;&#35777;&#26126;&#22312;&#38169;&#35823;&#21305;&#37197;&#30340;&#24773;&#20917;&#19979;&#65292;&#22810;&#27169;&#24335;&#23545;&#27604;&#23398;&#20064;&#21487;&#20197;&#27604;&#21333;&#27169;&#24335;&#23545;&#27604;&#23398;&#20064;&#34920;&#29616;&#26356;&#20339;&#65292;&#34920;&#29616;&#20102;&#20854;&#23545;&#22024;&#26434;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35821;&#35328;&#30417;&#30563;&#30340;&#35270;&#35273;&#27169;&#22411;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26500;&#24314;&#36825;&#31181;&#27169;&#22411;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#22312;&#20004;&#31181;&#27169;&#24577;&#20043;&#38388;&#23545;&#37197;&#23545;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#65292;&#20363;&#22914;&#23545;&#27604;&#35821;&#35328; - &#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#32447;&#24615;&#34920;&#31034;&#35774;&#32622;&#19979;&#65292;&#65288;i&#65289;&#21551;&#21160;&#20102;&#19968;&#20010;&#20851;&#20110;&#22810;&#27169;&#24335;&#23545;&#27604;&#23398;&#20064;&#65288;MMCL&#65289;&#30340;&#19968;&#33324;&#38750;&#32447;&#24615;Loss&#20989;&#25968;&#30340;&#35843;&#26597;&#65292;&#21253;&#25324;CLIP Loss&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#19982;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#21363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26799;&#24230;&#19979;&#38477;&#27599;&#19968;&#27493;Loss&#26368;&#23567;&#21270;&#21487;&#20197;&#34987;&#35270;&#20026;&#23545;&#19968;&#20010;&#23545;&#27604;&#24615;&#21327;&#26041;&#24046;&#30697;&#38453;&#36827;&#34892;SVD&#12290;&#22522;&#20110;&#36825;&#20010;&#27934;&#23519;&#65292;&#65288;ii&#65289;&#25105;&#20204;&#20998;&#26512;&#20102;MMCL&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23450;&#37327;&#22320;&#34920;&#26126;&#65292;&#22312;&#38169;&#35823;&#21305;&#37197;&#30340;&#24773;&#20917;&#19979;&#65292;MMCL&#30340;&#29305;&#24449;&#23398;&#20064;&#33021;&#21147;&#21487;&#20197;&#27604;&#21333;&#27169;&#24335;&#23545;&#27604;&#23398;&#20064;&#24212;&#29992;&#20110;&#27599;&#31181;&#27169;&#24335;&#26356;&#22909;&#12290;&#36825;&#34920;&#24449;&#20102;MMCL&#23545;&#22024;&#26434;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language-supervised vision models have recently attracted great attention in computer vision. A common approach to build such models is to use contrastive learning on paired data across the two modalities, as exemplified by Contrastive Language-Image Pre-Training (CLIP). In this paper, under linear representation settings, (i) we initiate the investigation of a general class of nonlinear loss functions for multimodal contrastive learning (MMCL) including CLIP loss and show its connection to singular value decomposition (SVD). Namely, we show that each step of loss minimization by gradient descent can be seen as performing SVD on a contrastive cross-covariance matrix. Based on this insight, (ii) we analyze the performance of MMCL. We quantitatively show that the feature learning ability of MMCL can be better than that of unimodal contrastive learning applied to each modality even under the presence of wrongly matched pairs. This characterizes the robustness of MMCL to noisy data. Furthe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;DBS&#27835;&#30103;&#24085;&#37329;&#26862;&#27663;&#30151;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#30340;&#38381;&#29615;&#28145;&#24230;&#33041;&#30005;&#21050;&#28608;&#25511;&#21046;&#22120;&#65292;&#20197;&#21160;&#24577;&#35843;&#25972;&#27835;&#30103;&#24133;&#24230;&#65292;&#20943;&#23569;&#33021;&#37327;&#20351;&#29992;&#65292;&#24182;&#26816;&#27979;&#20854;&#23433;&#20840;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.02477</link><description>&lt;p&gt;
&#38024;&#23545;&#24085;&#37329;&#26862;&#30149;&#27835;&#30103;&#30340;&#38381;&#29615;&#28145;&#24230;&#33041;&#30005;&#21050;&#28608;&#25511;&#21046;&#22120;&#30340;&#31163;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Offline Learning of Closed-Loop Deep Brain Stimulation Controllers for Parkinson Disease Treatment. (arXiv:2302.02477v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;DBS&#27835;&#30103;&#24085;&#37329;&#26862;&#27663;&#30151;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#30340;&#38381;&#29615;&#28145;&#24230;&#33041;&#30005;&#21050;&#28608;&#25511;&#21046;&#22120;&#65292;&#20197;&#21160;&#24577;&#35843;&#25972;&#27835;&#30103;&#24133;&#24230;&#65292;&#20943;&#23569;&#33021;&#37327;&#20351;&#29992;&#65292;&#24182;&#26816;&#27979;&#20854;&#23433;&#20840;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#33041;&#30005;&#21050;&#28608;&#65288;DBS&#65289;&#36890;&#36807;&#21521;&#33041;&#30340;&#22522;&#24213;&#31070;&#32463;&#33410;&#21306;&#22495;&#20256;&#36882;&#30005;&#33033;&#20914;&#65292;&#26174;&#31034;&#20986;&#27835;&#30103;&#24085;&#37329;&#26862;&#27663;&#30151;&#65288;PD&#65289;&#24341;&#36215;&#30340;&#36816;&#21160;&#30151;&#29366;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24471;&#21040;&#32654;&#22269;&#39135;&#21697;&#21644;&#33647;&#29289;&#31649;&#29702;&#23616;&#65288;FDA&#65289;&#25209;&#20934;&#30340;DBS&#20165;&#33021;&#20197;&#22266;&#23450;&#24133;&#24230;&#25552;&#20379;&#25345;&#32493;DBS&#65288;cDBS&#65289;&#21050;&#28608;&#65307;&#36825;&#31181;&#33021;&#37327;&#20302;&#25928;&#30340;&#25805;&#20316;&#38477;&#20302;&#20102;&#35774;&#22791;&#30340;&#30005;&#27744;&#23551;&#21629;&#65292;&#19981;&#33021;&#26681;&#25454;&#27963;&#21160;&#24773;&#20917;&#21160;&#24577;&#35843;&#25972;&#27835;&#30103;&#65292;&#21487;&#33021;&#20250;&#24341;&#36215;&#26174;&#33879;&#30340;&#21103;&#20316;&#29992;&#65288;&#22914;&#27493;&#24577;&#38556;&#30861;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#65292;&#20801;&#35768;&#21033;&#29992;&#36807;&#21435;&#30340;&#20020;&#24202;&#25968;&#25454;&#26469;&#35757;&#32451;&#19968;&#20010;RL&#31574;&#30053;&#65292;&#20174;&#32780;&#22312;&#23454;&#26102;&#35843;&#25972;&#21050;&#28608;&#24133;&#24230;&#30340;&#21516;&#26102;&#65292;&#20197;&#20943;&#23569;&#33021;&#37327;&#20351;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;cDBS&#30456;&#21516;&#27700;&#24179;&#30340;&#27835;&#30103;&#25928;&#21147;&#65288;&#21363;&#25511;&#21046;&#65289;&#12290;&#27492;&#22806;&#65292;&#20020;&#24202;&#21327;&#35758;&#35201;&#27714;&#22312;&#23558;&#36825;&#31181;RL&#25511;&#21046;&#22120;&#37096;&#32626;&#21040;&#24739;&#32773;&#20043;&#21069;&#65292;&#38656;&#35777;&#26126;&#20854;&#23433;&#20840;&#24615;&#21644;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#22522;&#20110;&#27169;&#25311;&#25216;&#26415;&#30340;&#23433;&#20840;&#26816;&#27979;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep brain stimulation (DBS) has shown great promise toward treating motor symptoms caused by Parkinson's disease (PD), by delivering electrical pulses to the Basal Ganglia (BG) region of the brain. However, DBS devices approved by the U.S. Food and Drug Administration (FDA) can only deliver continuous DBS (cDBS) stimuli at a fixed amplitude; this energy inefficient operation reduces battery lifetime of the device, cannot adapt treatment dynamically for activity, and may cause significant side-effects (e.g., gait impairment). In this work, we introduce an offline reinforcement learning (RL) framework, allowing the use of past clinical data to train an RL policy to adjust the stimulation amplitude in real time, with the goal of reducing energy use while maintaining the same level of treatment (i.e., control) efficacy as cDBS. Moreover, clinical protocols require the safety and performance of such RL controllers to be demonstrated ahead of deployments in patients. Thus, we also introduce
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;BabelCode&#26694;&#26550;&#21644;Translating Python Programming Puzzles&#65288;TP3&#65289;&#22522;&#20934;&#27979;&#35797;&#65292;&#25506;&#35752;&#20102;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;14&#31181;&#32534;&#31243;&#35821;&#35328;&#20998;&#24067;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#24179;&#34913;&#20998;&#24067;&#26377;&#21161;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2302.01973</link><description>&lt;p&gt;
&#27979;&#37327;&#32534;&#31243;&#35821;&#35328;&#20998;&#24067;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Measuring The Impact Of Programming Language Distribution. (arXiv:2302.01973v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01973
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;BabelCode&#26694;&#26550;&#21644;Translating Python Programming Puzzles&#65288;TP3&#65289;&#22522;&#20934;&#27979;&#35797;&#65292;&#25506;&#35752;&#20102;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;14&#31181;&#32534;&#31243;&#35821;&#35328;&#20998;&#24067;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#24179;&#34913;&#20998;&#24067;&#26377;&#21161;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#29992;&#20110;&#35780;&#20272;&#31070;&#32463;&#20195;&#30721;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#21482;&#38598;&#20013;&#22312;&#24456;&#23569;&#30340;&#19968;&#37096;&#20998;&#32534;&#31243;&#35821;&#35328;&#19978;&#65292;&#19981;&#21253;&#25324;&#35768;&#22810;&#27969;&#34892;&#30340;&#35821;&#35328;&#65292;&#20363;&#22914;Go&#25110;Rust&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BabelCode&#26694;&#26550;&#65292;&#29992;&#20110;&#22522;&#20110;&#25191;&#34892;&#30340;&#35780;&#20272;&#20219;&#20309;&#35821;&#35328;&#20013;&#30340;&#20219;&#20309;&#22522;&#20934;&#27979;&#35797;&#12290;BabelCode&#20351;&#24471;&#21487;&#20197;&#23545;&#27169;&#22411;&#30340;&#20869;&#23384;&#12289;&#36816;&#34892;&#26102;&#38388;&#21644;&#21333;&#20010;&#27979;&#35797;&#26696;&#20363;&#32467;&#26524;&#36827;&#34892;&#26032;&#30340;&#23450;&#24615;&#24615;&#33021;&#35843;&#26597;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;Translating Python Programming Puzzles&#65288;TP3&#65289;&#30340;&#26032;&#20195;&#30721;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26469;&#33258;Python Programming Puzzles&#65288;Schuster&#31561;&#20154;&#65292;2021&#65289;&#22522;&#20934;&#27979;&#35797;&#65292;&#28041;&#21450;&#23558;&#19987;&#23478;&#32423;Python&#20989;&#25968;&#32763;&#35793;&#25104;&#20219;&#20309;&#35821;&#35328;&#12290;&#36890;&#36807;&#23545;BabelCode&#21644;TP3&#22522;&#20934;&#27979;&#35797;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#24179;&#34913;14&#31181;&#35821;&#35328;&#30340;&#20998;&#24067;&#26159;&#21542;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#12290;&#22312;&#24179;&#34913;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#24179;&#22343;&#32780;&#35328;&#65292;&#30456;&#23545;&#20110;&#19981;&#24179;&#34913;&#20998;&#24067;&#30340;&#24773;&#20917;&#65292;&#25152;&#26377;&#20219;&#21153;&#21644;&#35821;&#35328;&#30340;$pass@k$&#32467;&#26524;&#24179;&#22343;&#25552;&#39640;&#20102;12.34%&#12290;
&lt;/p&gt;
&lt;p&gt;
Current benchmarks for evaluating neural code models focus on only a small subset of programming languages, excluding many popular languages such as Go or Rust. To ameliorate this issue, we present the BabelCode framework for execution-based evaluation of any benchmark in any language. BabelCode enables new investigations into the qualitative performance of models' memory, runtime, and individual test case results. Additionally, we present a new code translation dataset called Translating Python Programming Puzzles (TP3) from the Python Programming Puzzles (Schuster et al. 2021) benchmark that involves translating expert-level python functions to any language. With both BabelCode and the TP3 benchmark, we investigate if balancing the distributions of 14 languages in a training dataset improves a large language model's performance on low-resource languages. Training a model on a balanced corpus results in, on average, 12.34% higher $pass@k$ across all tasks and languages compared to the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#30340;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#65292;&#24182;&#22312;&#21463;&#27745;&#26579;&#30340;&#25968;&#25454;&#27969;&#20013;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#34920;&#29616;&#20248;&#24322;&#65292;&#21516;&#26102;&#30830;&#20445;&#20102;&#31283;&#23450;&#24615;&#24182;&#20943;&#23569;&#24322;&#24120;&#20540;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2302.00422</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Robust online active learning. (arXiv:2302.00422v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#30340;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#65292;&#24182;&#22312;&#21463;&#27745;&#26579;&#30340;&#25968;&#25454;&#27969;&#20013;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#34920;&#29616;&#20248;&#24322;&#65292;&#21516;&#26102;&#30830;&#20445;&#20102;&#31283;&#23450;&#24615;&#24182;&#20943;&#23569;&#24322;&#24120;&#20540;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24037;&#19994;&#24212;&#29992;&#20013;&#65292;&#33719;&#24471;&#26631;&#35760;&#30340;&#35266;&#27979;&#25968;&#25454;&#24182;&#19981;&#31616;&#21333;&#65292;&#36890;&#24120;&#38656;&#35201;&#20154;&#24037;&#19987;&#23478;&#24178;&#39044;&#25110;&#20351;&#29992;&#26114;&#36149;&#30340;&#27979;&#35797;&#35774;&#22791;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20027;&#21160;&#23398;&#20064;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#25311;&#21512;&#27169;&#22411;&#26102;&#26368;&#20449;&#24687;&#25968;&#25454;&#28857;&#30340;&#24314;&#35758;&#12290;&#20943;&#23569;&#27169;&#22411;&#24320;&#21457;&#25152;&#38656;&#30340;&#35266;&#27979;&#25968;&#25454;&#25968;&#37327;&#21487;&#20197;&#20943;&#36731;&#35757;&#32451;&#25152;&#38656;&#30340;&#35745;&#31639;&#36127;&#25285;&#21644;&#26631;&#35760;&#30456;&#20851;&#30340;&#25805;&#20316;&#25903;&#20986;&#12290;&#29305;&#21035;&#26159;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#65292;&#22312;&#38656;&#35201;&#22312;&#26497;&#30701;&#26102;&#38388;&#20869;&#20915;&#23450;&#26159;&#21542;&#33719;&#21462;&#25968;&#25454;&#28857;&#26631;&#35760;&#30340;&#39640;&#23481;&#37327;&#29983;&#20135;&#36807;&#31243;&#20013;&#38750;&#24120;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26368;&#36817;&#33268;&#21147;&#20110;&#24320;&#21457;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#20294;&#22312;&#23384;&#22312;&#24322;&#24120;&#20540;&#30340;&#24773;&#20917;&#19979;&#36825;&#20123;&#26041;&#27861;&#30340;&#34892;&#20026;&#20173;&#26410;&#24471;&#21040;&#24443;&#24213;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22312;&#32447;&#20027;&#21160;&#32447;&#24615;&#22238;&#24402;&#22312;&#21463;&#27745;&#26579;&#30340;&#25968;&#25454;&#27969;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#30340;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#65292;&#21516;&#26102;&#20445;&#35777;&#31283;&#23450;&#24615;&#24182;&#20943;&#23569;&#24322;&#24120;&#20540;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many industrial applications, obtaining labeled observations is not straightforward as it often requires the intervention of human experts or the use of expensive testing equipment. In these circumstances, active learning can be highly beneficial in suggesting the most informative data points to be used when fitting a model. Reducing the number of observations needed for model development alleviates both the computational burden required for training and the operational expenses related to labeling. Online active learning, in particular, is useful in high-volume production processes where the decision about the acquisition of the label for a data point needs to be taken within an extremely short time frame. However, despite the recent efforts to develop online active learning strategies, the behavior of these methods in the presence of outliers has not been thoroughly examined. In this work, we investigate the performance of online active linear regression in contaminated data strea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#28304;&#22495;&#36866;&#24212;&#30340;&#23454;&#29992;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#21407;&#22987;&#29305;&#24449;&#31354;&#38388;&#32858;&#31867;&#65292;&#26500;&#24314;&#30495;&#27491;&#22256;&#38590;&#30340;&#36127;&#23545;&#65292;&#32467;&#21512;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#29702;&#35770;&#65292;&#23398;&#20064;&#19968;&#20010;&#22495;&#19981;&#21464;&#30340;&#29305;&#24449;&#26469;&#35299;&#20915;&#22495;&#19978;&#30340;&#24046;&#24322;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#19977;&#20010;&#24120;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26377;&#25928;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.13428</link><description>&lt;p&gt;
&#23545;&#27604;&#19982;&#32858;&#31867;&#65306;&#23398;&#20064;&#37051;&#22495;&#23545;&#34920;&#31034;&#29992;&#20110;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Contrast and Clustering: Learning Neighborhood Pair Representation for Source-free Domain Adaptation. (arXiv:2301.13428v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#28304;&#22495;&#36866;&#24212;&#30340;&#23454;&#29992;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#21407;&#22987;&#29305;&#24449;&#31354;&#38388;&#32858;&#31867;&#65292;&#26500;&#24314;&#30495;&#27491;&#22256;&#38590;&#30340;&#36127;&#23545;&#65292;&#32467;&#21512;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#29702;&#35770;&#65292;&#23398;&#20064;&#19968;&#20010;&#22495;&#19981;&#21464;&#30340;&#29305;&#24449;&#26469;&#35299;&#20915;&#22495;&#19978;&#30340;&#24046;&#24322;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#19977;&#20010;&#24120;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26377;&#25928;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#21033;&#29992;&#26469;&#33258;&#19981;&#21516;&#20998;&#24067;&#30340;&#28304;&#25968;&#25454;&#35299;&#20915;&#20174;&#26410;&#26631;&#35760;&#30446;&#26631;&#22495;&#20998;&#31867;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26041;&#27861;&#38656;&#35201;&#35775;&#38382;&#28304;&#25968;&#25454;&#65292;&#36825;&#32463;&#24120;&#24341;&#36215;&#25968;&#25454;&#38544;&#31169;&#26041;&#38754;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26356;&#21152;&#23454;&#38469;&#20294;&#20805;&#28385;&#25361;&#25112;&#30340;&#35774;&#32622;&#65292;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#28304;&#22495;&#25968;&#25454;&#19981;&#21487;&#29992;&#65292;&#30446;&#26631;&#22495;&#25968;&#25454;&#26410;&#26631;&#35760;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#23545;&#27604;&#23398;&#20064;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;&#22495;&#38388;&#24046;&#24322;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#23398;&#20064;&#19968;&#20010;&#22495;&#19981;&#21464;&#30340;&#29305;&#24449;:1)&#22312;&#21407;&#22987;&#29305;&#24449;&#31354;&#38388;&#20013;&#30452;&#25509;&#25191;&#34892;&#20855;&#26377;&#26368;&#36817;&#37051;&#23621;&#30340;&#32858;&#31867;&#65307;2)&#36890;&#36807;&#25193;&#23637;&#37051;&#23621;&#26500;&#36896;&#30495;&#27491;&#22256;&#38590;&#30340;&#36127;&#23545;&#65292;&#32780;&#19981;&#24341;&#20837;&#39069;&#22806;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65307;3)&#32467;&#21512;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#29702;&#35770;&#20197;&#33719;&#24471;&#35745;&#31639;&#20248;&#21183;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#24120;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;VisDA&#12289;Office-Home&#21644;Office-31&#19978;&#36827;&#34892;&#20102;&#20180;&#32454;&#30340;&#28040;&#34701;&#30740;&#31350;&#21644;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised domain adaptation uses source data from different distributions to solve the problem of classifying data from unlabeled target domains. However, conventional methods require access to source data, which often raise concerns about data privacy. In this paper, we consider a more practical but challenging setting where the source domain data is unavailable and the target domain data is unlabeled. Specifically, we address the domain discrepancy problem from the perspective of contrastive learning. The key idea of our work is to learn a domain-invariant feature by 1) performing clustering directly in the original feature space with nearest neighbors; 2) constructing truly hard negative pairs by extended neighbors without introducing additional computational complexity; and 3) combining noise-contrastive estimation theory to gain computational advantage. We conduct careful ablation studies and extensive experiments on three common benchmarks: VisDA, Office-Home, and Office-31. T
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#8212;Flex-Net&#65292;&#29992;&#20110;&#35299;&#20915;&#28789;&#27963;&#21452;&#24037;&#32593;&#32476;&#20013;&#36164;&#28304;&#31649;&#29702;&#38382;&#39064;&#65292;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#36890;&#20449;&#26041;&#21521;&#21644;&#20256;&#36755;&#21151;&#29575;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#21644;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2301.11166</link><description>&lt;p&gt;
&#28789;&#27963;&#21452;&#24037;&#32593;&#32476;&#20013;&#36164;&#28304;&#31649;&#29702;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Flex-Net: A Graph Neural Network Approach to Resource Management in Flexible Duplex Networks. (arXiv:2301.11166v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#8212;Flex-Net&#65292;&#29992;&#20110;&#35299;&#20915;&#28789;&#27963;&#21452;&#24037;&#32593;&#32476;&#20013;&#36164;&#28304;&#31649;&#29702;&#38382;&#39064;&#65292;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#36890;&#20449;&#26041;&#21521;&#21644;&#20256;&#36755;&#21151;&#29575;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#21644;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28789;&#27963;&#21452;&#24037;&#32593;&#32476;&#20801;&#35768;&#29992;&#25143;&#22312;&#27809;&#26377;&#38745;&#24577;&#26102;&#38388;&#35843;&#24230;&#30340;&#24773;&#20917;&#19979;&#21160;&#24577;&#20351;&#29992;&#19978;&#34892;&#21644;&#19979;&#34892;&#20449;&#36947;&#65292;&#20174;&#32780;&#26377;&#25928;&#21033;&#29992;&#32593;&#32476;&#36164;&#28304;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#28789;&#27963;&#21452;&#24037;&#32593;&#32476;&#30340;&#24635;&#36895;&#29575;&#26368;&#22823;&#21270;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20855;&#26377;&#19968;&#23545;&#19968;&#36890;&#20449;&#38142;&#25509;&#30340;&#32593;&#32476;&#12290;&#30456;&#24212;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#26159;&#38750;&#30830;&#23450;&#22810;&#39033;&#24335;&#65288;NP&#65289;&#38590;&#39064;&#65292;&#27809;&#26377;&#38381;&#21512;&#24418;&#24335;&#30340;&#35299;&#27861;&#12290;&#22240;&#27492;&#65292;&#29616;&#26377;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#22312;&#22823;&#22411;&#32593;&#32476;&#20013;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#35745;&#31639;&#22797;&#26434;&#24230;&#24456;&#39640;&#12290;&#21463;&#36817;&#24180;&#26469;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#35299;&#20915;NP&#38590;&#30340;&#26080;&#32447;&#36164;&#28304;&#31649;&#29702;&#38382;&#39064;&#26041;&#38754;&#30340;&#25104;&#21151;&#25152;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GNN&#26550;&#26500;&#65292;&#21517;&#20026;Flex-Net&#65292;&#20197;&#32852;&#21512;&#20248;&#21270;&#36890;&#20449;&#26041;&#21521;&#21644;&#20256;&#36755;&#21151;&#29575;&#12290;&#25152;&#25552;&#20986;&#30340;GNN&#21516;&#26102;&#20445;&#25345;&#20102;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#25509;&#36817;&#26368;&#20248;&#24615;&#33021;&#65292;&#30456;&#36739;&#20110;&#26368;&#24120;&#29992;&#30340;&#25216;&#26415;&#32780;&#35328;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#31574;&#30053;&#20013;&#65292;Flex-Net&#30340;&#24179;&#22343;&#24615;&#33021;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Flexible duplex networks allow users to dynamically employ uplink and downlink channels without static time scheduling, thereby utilizing the network resources efficiently. This work investigates the sum-rate maximization of flexible duplex networks. In particular, we consider a network with pairwise-fixed communication links. Corresponding combinatorial optimization is a non-deterministic polynomial (NP)-hard without a closed-form solution. In this respect, the existing heuristics entail high computational complexity, raising a scalability issue in large networks. Motivated by the recent success of Graph Neural Networks (GNNs) in solving NP-hard wireless resource management problems, we propose a novel GNN architecture, named Flex-Net, to jointly optimize the communication direction and transmission power. The proposed GNN produces near-optimal performance meanwhile maintaining a low computational complexity compared to the most commonly used techniques. Furthermore, our numerical res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SoftMatch&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#20445;&#25345;&#39640;&#25968;&#37327;&#21644;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#26469;&#20811;&#26381;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#25968;&#37327;-&#36136;&#37327;&#26435;&#34913;&#38382;&#39064;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#26410;&#26631;&#27880;&#30340;&#25968;&#25454;&#12290; &#22312;&#23454;&#39564;&#20013;&#65292;SoftMatch&#22312;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#24433;&#29255;&#31561;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#37117;&#26174;&#31034;&#20102;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2301.10921</link><description>&lt;p&gt;
SoftMatch&#65306;&#35299;&#20915;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#25968;&#37327;-&#36136;&#37327;&#26435;&#34913;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning. (arXiv:2301.10921v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10921
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SoftMatch&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#20445;&#25345;&#39640;&#25968;&#37327;&#21644;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#26469;&#20811;&#26381;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#25968;&#37327;-&#36136;&#37327;&#26435;&#34913;&#38382;&#39064;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#26410;&#26631;&#27880;&#30340;&#25968;&#25454;&#12290; &#22312;&#23454;&#39564;&#20013;&#65292;SoftMatch&#22312;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#24433;&#29255;&#31561;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#37117;&#26174;&#31034;&#20102;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#22914;&#20309;&#26377;&#25928;&#21033;&#29992;&#26377;&#38480;&#30340;&#26631;&#27880;&#25968;&#25454;&#21644;&#22823;&#37327;&#30340;&#26410;&#26631;&#27880;&#25968;&#25454;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#26412;&#25991;&#39318;&#20808;&#36890;&#36807;&#32479;&#19968;&#30340;&#26679;&#26412;&#21152;&#26435;&#20844;&#24335;&#37325;&#26032;&#23457;&#35270;&#20102;&#27969;&#34892;&#30340;&#20266;&#26631;&#31614;&#26041;&#27861;&#65292;&#24182;&#28436;&#31034;&#20102;&#20266;&#26631;&#31614;&#38408;&#20540;&#27861;&#22266;&#26377;&#30340;&#25968;&#37327;-&#36136;&#37327;&#26435;&#34913;&#38382;&#39064;&#65292;&#21487;&#33021;&#20250;&#38459;&#30861;&#23398;&#20064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SoftMatch&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#20445;&#25345;&#20266;&#26631;&#31614;&#30340;&#39640;&#25968;&#37327;&#21644;&#39640;&#36136;&#37327;&#26469;&#20811;&#26381;&#36825;&#31181;&#26435;&#34913;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#26410;&#26631;&#27880;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#20010;&#25130;&#26029;&#30340;&#39640;&#26031;&#20989;&#25968;&#26469;&#26681;&#25454;&#26679;&#26412;&#30340;&#32622;&#20449;&#24230;&#23545;&#26679;&#26412;&#36827;&#34892;&#21152;&#26435;&#65292;&#36825;&#21487;&#20197;&#30475;&#20316;&#26159;&#32622;&#20449;&#24230;&#38408;&#20540;&#30340;&#36719;&#29256;&#26412;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#25552;&#20986;&#32479;&#19968;&#30340;&#23545;&#40784;&#26041;&#27861;&#26469;&#22686;&#24378;&#23545;&#24369;&#23398;&#20064;&#31867;&#30340;&#21033;&#29992;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;SoftMatch&#22312;&#21253;&#25324;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#24433;&#29255;&#31561;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#20102;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The critical challenge of Semi-Supervised Learning (SSL) is how to effectively leverage the limited labeled data and massive unlabeled data to improve the model's generalization performance. In this paper, we first revisit the popular pseudo-labeling methods via a unified sample weighting formulation and demonstrate the inherent quantity-quality trade-off problem of pseudo-labeling with thresholding, which may prohibit learning. To this end, we propose SoftMatch to overcome the trade-off by maintaining both high quantity and high quality of pseudo-labels during training, effectively exploiting the unlabeled data. We derive a truncated Gaussian function to weight samples based on their confidence, which can be viewed as a soft version of the confidence threshold. We further enhance the utilization of weakly-learned classes by proposing a uniform alignment approach. In experiments, SoftMatch shows substantial improvements across a wide variety of benchmarks, including image, text, and im
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20219;&#21153;&#23548;&#21521;&#36890;&#20449;&#20013;&#36890;&#36807;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#23545;&#26469;&#25191;&#34892;&#20219;&#21153;&#30340;&#20449;&#24687;&#26102;&#20195;&#38382;&#39064;&#65292;&#36890;&#36807;&#20449;&#36947;&#21033;&#29992;&#29575;&#30340;&#22686;&#21152;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#38656;&#35201;&#26356;&#38271;&#30340;&#26381;&#21153;&#26102;&#38388;&#65292;&#24341;&#20837;&#20219;&#21153;&#20449;&#24687;&#30340;&#26368;&#22823;&#26102;&#20195;&#65288;PAoTI&#65289;&#26469;&#23545;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#36827;&#34892;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2301.04298</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#39537;&#21160;&#30340;&#20219;&#21153;&#23548;&#21521;&#36890;&#20449;&#20013;&#30340;&#20449;&#24687;&#26102;&#20195;
&lt;/p&gt;
&lt;p&gt;
Age of Information in Deep Learning-Driven Task-Oriented Communications. (arXiv:2301.04298v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20219;&#21153;&#23548;&#21521;&#36890;&#20449;&#20013;&#36890;&#36807;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#23545;&#26469;&#25191;&#34892;&#20219;&#21153;&#30340;&#20449;&#24687;&#26102;&#20195;&#38382;&#39064;&#65292;&#36890;&#36807;&#20449;&#36947;&#21033;&#29992;&#29575;&#30340;&#22686;&#21152;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#38656;&#35201;&#26356;&#38271;&#30340;&#26381;&#21153;&#26102;&#38388;&#65292;&#24341;&#20837;&#20219;&#21153;&#20449;&#24687;&#30340;&#26368;&#22823;&#26102;&#20195;&#65288;PAoTI&#65289;&#26469;&#23545;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#36827;&#34892;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20219;&#21153;&#23548;&#21521;&#36890;&#20449;&#20013;&#30340;&#20449;&#24687;&#26102;&#20195;&#27010;&#24565;&#65292;&#26088;&#22312;&#21033;&#29992;&#21457;&#23556;&#26426;&#22788;&#30340;&#25968;&#25454;&#22312;&#25509;&#25910;&#22120;&#22788;&#25191;&#34892;&#20219;&#21153;&#12290;&#21457;&#23556;&#26426;&#21644;&#25509;&#25910;&#26426;&#20043;&#38388;&#30340;&#25805;&#20316;&#34987;&#24314;&#27169;&#20026;&#19968;&#20010;&#32852;&#21512;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#23545;&#65292;&#24182;&#32771;&#34385;&#20102;&#20449;&#36947;&#25928;&#24212;&#12290;&#32534;&#30721;&#22120;&#23558;&#25968;&#25454;&#26679;&#26412;&#36716;&#25442;&#20026;&#23567;&#32500;&#24230;&#30340;&#29305;&#24449;&#21521;&#37327;&#65292;&#24182;&#20351;&#29992;&#23569;&#37327;&#30340;&#20449;&#36947;&#29992;&#20110;&#20256;&#36755;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#20256;&#36755;&#27425;&#25968;&#21644;&#24310;&#36831;&#12290;&#35299;&#30721;&#22120;&#19981;&#20877;&#37325;&#26500;&#36755;&#20837;&#26679;&#26412;&#65292;&#32780;&#26159;&#23545;&#25509;&#25910;&#21040;&#30340;&#20449;&#21495;&#25191;&#34892;&#20219;&#21153;&#65292;&#20363;&#22914;&#20998;&#31867;&#12290;&#36890;&#36807;&#22312;MNIST&#21644;CIFAR-10&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#19981;&#21516;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#26174;&#31034;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#24615;&#38543;&#30528;&#20449;&#36947;&#21033;&#29992;&#29575;&#30340;&#22686;&#21152;&#32780;&#25552;&#39640;&#65292;&#20294;&#38656;&#35201;&#26356;&#38271;&#30340;&#26381;&#21153;&#26102;&#38388;&#12290;&#24341;&#20837;&#20219;&#21153;&#20449;&#24687;&#30340;&#26368;&#22823;&#26102;&#20195;&#65288;PAoTI&#65289;&#26469;&#20998;&#26512;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#26435;&#34913;&#30340;&#26102;&#26426;&#65292;&#24403;&#20449;&#24687;&#24180;&#40836;&#22686;&#38271;&#26102;&#65292;&#38500;&#38750;&#25509;&#25910;&#21040;&#30340;&#20449;&#21495;&#34987;&#27491;&#30830;&#20998;&#31867;&#12290;&#36890;&#36807;&#32467;&#21512;&#36890;&#36947;&#21644;&#28304;&#30340;&#24433;&#21709;&#23545;PAoTI&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#32467;&#26524;&#34920;&#26126;PAoTI&#19982;&#36890;&#36947;&#30340;&#24433;&#21709;&#36235;&#21183;&#19968;&#33268;&#65292;&#20294;&#27010;&#36848;&#35823;&#24046;&#21487;&#33021;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the notion of age in task-oriented communications that aims to execute a task at a receiver utilizing the data at its transmitter. The transmitter-receiver operations are modeled as an encoder-decoder pair that is jointly trained while considering channel effects. The encoder converts data samples into feature vectors of small dimension and transmits them with a small number of channel uses thereby reducing the number of transmissions and latency. Instead of reconstructing input samples, the decoder performs a task, e.g., classification, on the received signals. Applying different deep neural networks of encoder-decoder pairs on MNIST and CIFAR-10 image datasets, the classifier accuracy is shown to increase with the number of channel uses at the expense of longer service time. The peak age of task information (PAoTI) is introduced to analyze this accuracy-latency tradeoff when the age grows unless a received signal is classified correctly. By incorporating channel an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24335;&#27468;&#35789;-&#33410;&#22863;&#21305;&#37197;&#26041;&#27861;&#65292;&#37319;&#29992;&#38899;&#39057;&#32780;&#19981;&#26159;&#26377;&#21487;&#29992;&#20803;&#25968;&#25454;&#30340;&#35889;&#38754;&#65292;&#21487;&#23558;&#27468;&#35789;&#21644;&#38899;&#20048;&#30340;&#20851;&#38190;&#37096;&#20998;&#30456;&#20114;&#21305;&#37197;&#65292;&#21253;&#25324;&#38899;&#20048;&#30340;&#24378;&#33410;&#25293;&#12289;&#27468;&#35789;&#38899;&#33410;&#21644;&#27468;&#35789;&#20851;&#38190;&#35789;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.02732</link><description>&lt;p&gt;
&#22810;&#27169;&#24335;&#27468;&#35789;-&#33410;&#22863;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Multimodal Lyrics-Rhythm Matching. (arXiv:2301.02732v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24335;&#27468;&#35789;-&#33410;&#22863;&#21305;&#37197;&#26041;&#27861;&#65292;&#37319;&#29992;&#38899;&#39057;&#32780;&#19981;&#26159;&#26377;&#21487;&#29992;&#20803;&#25968;&#25454;&#30340;&#35889;&#38754;&#65292;&#21487;&#23558;&#27468;&#35789;&#21644;&#38899;&#20048;&#30340;&#20851;&#38190;&#37096;&#20998;&#30456;&#20114;&#21305;&#37197;&#65292;&#21253;&#25324;&#38899;&#20048;&#30340;&#24378;&#33410;&#25293;&#12289;&#27468;&#35789;&#38899;&#33410;&#21644;&#27468;&#35789;&#20851;&#38190;&#35789;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#24180;&#26469;&#20154;&#24037;&#26234;&#33021;&#22312;&#38899;&#20048;&#39046;&#22495;&#30340;&#30740;&#31350;&#22686;&#21152;&#20102;&#65292;&#20294;&#20851;&#20110;&#27468;&#35789;&#21644;&#33410;&#22863;&#36825;&#20004;&#20010;&#20851;&#38190;&#37096;&#20998;&#30340;&#20027;&#35201;&#30456;&#20851;&#24615;&#65292;&#22914;&#20851;&#38190;&#35789;&#12289;&#37325;&#38899;&#38899;&#33410;&#21644;&#24378;&#33410;&#25293;&#31561;&#65292;&#19981;&#32463;&#24120;&#34987;&#30740;&#31350;&#12290;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#38899;&#39057;&#38169;&#20301;&#12289;&#38899;&#33410;&#35782;&#21035;&#30340;&#19981;&#20934;&#30830;&#24615;&#20197;&#21450;&#36328;&#23398;&#31185;&#30693;&#35782;&#38656;&#27714;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#19981;&#36275;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24335;&#27468;&#35789;-&#33410;&#22863;&#21305;&#37197;&#26041;&#27861;&#65292;&#20854;&#29305;&#21035;&#26159;&#22312;&#27809;&#26377;&#20219;&#20309;&#35821;&#35328;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#27468;&#35789;&#21644;&#38899;&#20048;&#30340;&#20851;&#38190;&#37096;&#20998;&#30456;&#20114;&#21305;&#37197;&#12290;&#25105;&#20204;&#20351;&#29992;&#38899;&#39057;&#32780;&#19981;&#26159;&#26377;&#21487;&#29992;&#20803;&#25968;&#25454;&#30340;&#35889;&#38754;&#65292;&#36825;&#22686;&#21152;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24212;&#29992;&#28789;&#27963;&#24615;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#26356;&#22810;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21019;&#36896;&#24615;&#22320;&#29983;&#25104;&#20102;&#20960;&#31181;&#21253;&#21547;&#21508;&#31181;&#22810;&#27169;&#24577;&#30340;&#27169;&#24335;&#65292;&#21253;&#25324;&#38899;&#20048;&#30340;&#24378;&#33410;&#25293;&#12289;&#27468;&#35789;&#38899;&#33410;&#12289;&#27468;&#25163;&#21457;&#38899;&#30340;&#21548;&#35273;&#21464;&#21270;&#20197;&#21450;&#23588;&#20854;&#26159;&#27468;&#35789;&#20851;&#38190;&#35789;&#65292;&#23427;&#20204;&#36890;&#24120;&#25351;&#31034;&#19968;&#39318;&#27468;&#30340;&#20013;&#24515;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19968;&#20010;&#21253;&#21547;100&#39318;&#27468;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#23427;&#20204;&#26377;&#27468;&#35789;&#27880;&#37322;&#21644;MIDI&#25991;&#20214;&#65292;&#34920;&#29616;&#20986;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent increase in research on artificial intelligence for music, prominent correlations between key components of lyrics and rhythm such as keywords, stressed syllables, and strong beats are not frequently studied. This is likely due to challenges such as audio misalignment, inaccuracies in syllabic identification, and most importantly, the need for cross-disciplinary knowledge. To address this lack of research, we propose a novel multimodal lyrics-rhythm matching approach in this paper that specifically matches key components of lyrics and music with each other without any language limitations. We use audio instead of sheet music with readily available metadata, which creates more challenges yet increases the application flexibility of our method. Furthermore, our approach creatively generates several patterns involving various multimodalities, including music strong beats, lyrical syllables, auditory changes in a singer's pronunciation, and especially lyrical keywords, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21322;&#30417;&#30563;&#33258;&#32534;&#30721;&#22120;&#20197;&#21450;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#23613;&#21487;&#33021;&#23569;&#30340;&#26631;&#35760;&#26679;&#26412;&#26469;&#24320;&#21457;&#36719;&#27979;&#37327;&#20256;&#24863;&#22120;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#20102;&#25104;&#26412;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#20316;&#32773;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#21462;&#24471;&#22909;&#30340;&#39044;&#27979;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.13067</link><description>&lt;p&gt;
&#20351;&#29992;&#21322;&#30417;&#30563;&#33258;&#32534;&#30721;&#22120;&#30340;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#36827;&#34892;&#36719;&#27979;&#37327;&#24320;&#21457;
&lt;/p&gt;
&lt;p&gt;
Online Active Learning for Soft Sensor Development using Semi-Supervised Autoencoders. (arXiv:2212.13067v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21322;&#30417;&#30563;&#33258;&#32534;&#30721;&#22120;&#20197;&#21450;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#23613;&#21487;&#33021;&#23569;&#30340;&#26631;&#35760;&#26679;&#26412;&#26469;&#24320;&#21457;&#36719;&#27979;&#37327;&#20256;&#24863;&#22120;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#20102;&#25104;&#26412;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#20316;&#32773;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#21462;&#24471;&#22909;&#30340;&#39044;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#36719;&#27979;&#37327;&#22312;&#24037;&#19994;&#21644;&#21270;&#23398;&#36807;&#31243;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20197;&#39044;&#27979;&#38590;&#20197;&#27979;&#37327;&#30340;&#36807;&#31243;&#21464;&#37327;&#12290;&#36825;&#20123;&#20256;&#24863;&#22120;&#20351;&#29992;&#30340;&#22238;&#24402;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#30340;&#26679;&#26412;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#36136;&#37327;&#26816;&#26597;&#38656;&#35201;&#39640;&#26114;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#65292;&#33719;&#21462;&#26631;&#31614;&#20449;&#24687;&#21487;&#33021;&#38750;&#24120;&#26114;&#36149;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#21487;&#33021;&#38750;&#24120;&#26377;&#30410;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#24314;&#35758;&#26597;&#35810;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#20026;&#22238;&#24402;&#25552;&#20986;&#30340;&#22823;&#22810;&#25968;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#37117;&#38598;&#20013;&#22312;&#31163;&#32447;&#22330;&#26223;&#12290;&#26412;&#25991;&#23558;&#20854;&#20013;&#19968;&#20123;&#26041;&#27861;&#36866;&#24212;&#20110;&#27969;&#24335;&#22330;&#26223;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#22522;&#20110;&#27491;&#20132;&#33258;&#32534;&#30721;&#22120;&#30340;&#21322;&#30417;&#30563;&#26550;&#26500;&#23398;&#20064;&#20302;&#32500;&#31354;&#38388;&#20013;&#30340;&#26174;&#33879;&#29305;&#24449;&#12290;&#25105;&#20204;&#20063;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#30000;&#32435;&#35199;&#19996;&#26364;&#36807;&#31243;&#27604;&#36739;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven soft sensors are extensively used in industrial and chemical processes to predict hard-to-measure process variables whose real value is difficult to track during routine operations. The regression models used by these sensors often require a large number of labeled examples, yet obtaining the label information can be very expensive given the high time and cost required by quality inspections. In this context, active learning methods can be highly beneficial as they can suggest the most informative labels to query. However, most of the active learning strategies proposed for regression focus on the offline setting. In this work, we adapt some of these approaches to the stream-based scenario and show how they can be used to select the most informative data points. We also demonstrate how to use a semi-supervised architecture based on orthogonal autoencoders to learn salient features in a lower dimensional space. The Tennessee Eastman Process is used to compare the predictive 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#32479;&#19968;&#20132;&#21472;&#20551;&#35774;&#65292;&#32780;&#26159;&#21033;&#29992;&#20215;&#20540;&#30340;&#19979;&#38480;&#32622;&#20449;&#21306;&#38388;&#65288;LCBs&#65289;&#20248;&#21270;&#31574;&#30053;&#65292;&#22240;&#27492;&#33021;&#22815;&#36866;&#24212;&#20801;&#35768;&#34892;&#20026;&#31574;&#30053;&#28436;&#21464;&#21644;&#20542;&#21521;&#24615;&#20943;&#24369;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2212.09900</link><description>&lt;p&gt;
&#26080;&#20132;&#21472;&#31574;&#30053;&#23398;&#20064;&#65306;&#24754;&#35266;&#21644;&#24191;&#20041;&#32463;&#39564;Bernstein&#19981;&#31561;&#24335;
&lt;/p&gt;
&lt;p&gt;
Policy learning "without'' overlap: Pessimism and generalized empirical Bernstein's inequality. (arXiv:2212.09900v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#32479;&#19968;&#20132;&#21472;&#20551;&#35774;&#65292;&#32780;&#26159;&#21033;&#29992;&#20215;&#20540;&#30340;&#19979;&#38480;&#32622;&#20449;&#21306;&#38388;&#65288;LCBs&#65289;&#20248;&#21270;&#31574;&#30053;&#65292;&#22240;&#27492;&#33021;&#22815;&#36866;&#24212;&#20801;&#35768;&#34892;&#20026;&#31574;&#30053;&#28436;&#21464;&#21644;&#20542;&#21521;&#24615;&#20943;&#24369;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#65292;&#26088;&#22312;&#21033;&#29992;&#20808;&#21069;&#25910;&#38598;&#21040;&#30340;&#35266;&#27979;&#65288;&#26469;&#33258;&#20110;&#22266;&#23450;&#30340;&#25110;&#26159;&#36866;&#24212;&#28436;&#21464;&#30340;&#34892;&#20026;&#31574;&#30053;&#65289;&#26469;&#23398;&#20064;&#32473;&#23450;&#31867;&#21035;&#20013;&#30340;&#26368;&#20248;&#20010;&#24615;&#21270;&#20915;&#31574;&#35268;&#21017;&#12290;&#29616;&#26377;&#30340;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#32479;&#19968;&#20132;&#21472;&#20551;&#35774;&#65292;&#21363;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#25506;&#32034;&#25152;&#26377;&#20010;&#24615;&#21270;&#29305;&#24449;&#30340;&#25152;&#26377;&#21160;&#20316;&#30340;&#20542;&#21521;&#24615;&#19979;&#30028;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#21462;&#20915;&#20110;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#26368;&#22351;&#30340;&#20542;&#21521;&#24615;&#12290;&#30001;&#20110;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#19981;&#21463;&#25511;&#21046;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#20551;&#35774;&#21487;&#33021;&#19981;&#22826;&#29616;&#23454;&#65292;&#29305;&#21035;&#26159;&#24403;&#20801;&#35768;&#34892;&#20026;&#31574;&#30053;&#38543;&#26102;&#38388;&#28436;&#21464;&#24182;&#19988;&#20542;&#21521;&#24615;&#20943;&#24369;&#26102;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#23427;&#20248;&#21270;&#31574;&#30053;&#20215;&#20540;&#30340;&#19979;&#38480;&#32622;&#20449;&#21306;&#38388;&#65288;LCBs&#65289;&#8212;&#8212;&#32780;&#19981;&#26159;&#28857;&#20272;&#35745;&#12290;LCBs&#36890;&#36807;&#37327;&#21270;&#22686;&#24378;&#20498;&#25968;&#20542;&#21521;&#26435;&#37325;&#30340;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#26469;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies offline policy learning, which aims at utilizing observations collected a priori (from either fixed or adaptively evolving behavior policies) to learn the optimal individualized decision rule in a given class. Existing policy learning methods rely on a uniform overlap assumption, i.e., the propensities of exploring all actions for all individual characteristics are lower bounded in the offline dataset. In other words, the performance of these methods depends on the worst-case propensity in the offline dataset. As one has no control over the data collection process, this assumption can be unrealistic in many situations, especially when the behavior policies are allowed to evolve over time with diminishing propensities.  In this paper, we propose a new algorithm that optimizes lower confidence bounds (LCBs) -- instead of point estimates -- of the policy values. The LCBs are constructed by quantifying the estimation uncertainty of the augmented inverse propensity weight
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#21453;&#39304;&#30340;&#31574;&#30053;&#36866;&#24212;&#65288;PAFF&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#35753;&#31574;&#30053;&#20351;&#29992;&#38543;&#26426;&#29983;&#25104;&#30340;&#25351;&#20196;&#36827;&#34892;&#28436;&#31034;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#25552;&#20379;&#21453;&#39304;&#26469;&#37325;&#26032;&#26631;&#35760;&#28436;&#31034;&#65292;&#33258;&#21160;&#25552;&#20379;&#26032;&#30340;&#28436;&#31034;-&#25351;&#20196;&#25968;&#25454;&#23545;&#36827;&#34892;&#31574;&#30053;&#24494;&#35843;&#65292;&#20197;&#23454;&#29616;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#27867;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PAFF&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.07398</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#21453;&#39304;&#30340;&#31574;&#30053;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Policy Adaptation from Foundation Model Feedback. (arXiv:2212.07398v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#21453;&#39304;&#30340;&#31574;&#30053;&#36866;&#24212;&#65288;PAFF&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#35753;&#31574;&#30053;&#20351;&#29992;&#38543;&#26426;&#29983;&#25104;&#30340;&#25351;&#20196;&#36827;&#34892;&#28436;&#31034;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#25552;&#20379;&#21453;&#39304;&#26469;&#37325;&#26032;&#26631;&#35760;&#28436;&#31034;&#65292;&#33258;&#21160;&#25552;&#20379;&#26032;&#30340;&#28436;&#31034;-&#25351;&#20196;&#25968;&#25454;&#23545;&#36827;&#34892;&#31574;&#30053;&#24494;&#35843;&#65292;&#20197;&#23454;&#29616;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#27867;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PAFF&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#26041;&#38754;&#30340;&#36827;&#23637;&#20026;&#26500;&#24314;&#36890;&#29992;&#26426;&#22120;&#20154;&#24102;&#26469;&#20102;&#26174;&#33879;&#36827;&#27493;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#23558;&#22330;&#26223;&#21644;&#25351;&#20196;&#32534;&#30721;&#20026;&#20915;&#31574;&#36755;&#20837;&#65292;&#25351;&#20196;&#26465;&#20214;&#21270;&#31574;&#30053;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#23545;&#35937;&#21644;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#27867;&#21270;&#12290;&#23613;&#31649;&#36825;&#26159;&#20196;&#20154;&#40723;&#33310;&#30340;&#65292;&#20294;&#31574;&#30053;&#22312;&#36935;&#21040;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#25110;&#29615;&#22659;&#26102;&#20173;&#28982;&#22833;&#36133;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#21453;&#39304;&#30340;&#31574;&#30053;&#36866;&#24212;&#65288;PAFF&#65289;&#12290;&#24403;&#23558;&#35757;&#32451;&#22909;&#30340;&#31574;&#30053;&#37096;&#32626;&#21040;&#26032;&#20219;&#21153;&#25110;&#26032;&#29615;&#22659;&#26102;&#65292;&#25105;&#20204;&#39318;&#20808;&#35753;&#31574;&#30053;&#20351;&#29992;&#38543;&#26426;&#29983;&#25104;&#30340;&#25351;&#20196;&#36827;&#34892;&#28436;&#31034;&#12290;&#34429;&#28982;&#25191;&#34892;&#21487;&#33021;&#20986;&#29616;&#38169;&#35823;&#65292;&#20294;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#25552;&#20379;&#21453;&#39304;&#26469;&#37325;&#26032;&#26631;&#35760;&#28436;&#31034;&#12290;&#36825;&#33258;&#21160;&#20026;&#31574;&#30053;&#24494;&#35843;&#25552;&#20379;&#20102;&#26032;&#30340;&#28436;&#31034;-&#25351;&#20196;&#25968;&#25454;&#23545;&#12290;&#25105;&#20204;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#35774;&#32622;&#20013;&#36827;&#34892;&#20102;&#21508;&#31181;&#23454;&#39564;&#30340;&#35780;&#20272;&#65292;&#37325;&#28857;&#26159;&#22312;&#26410;&#35265;&#36807;&#30340;&#23545;&#35937;&#12289;&#20219;&#21153;&#21644;&#26410;&#35266;&#23519;&#21040;&#30340;&#29615;&#22659;&#20013;&#30340;&#27867;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PAFF&#22312;&#26368;&#32456;&#20219;&#21153;&#25104;&#21151;&#29575;&#21644;&#35757;&#32451;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress on vision-language foundation models have brought significant advancement to building general-purpose robots. By using the pre-trained models to encode the scene and instructions as inputs for decision making, the instruction-conditioned policy can generalize across different objects and tasks. While this is encouraging, the policy still fails in most cases given an unseen task or environment. In this work, we propose Policy Adaptation from Foundation model Feedback (PAFF). When deploying the trained policy to a new task or a new environment, we first let the policy play with randomly generated instructions to record the demonstrations. While the execution could be wrong, we can use the pre-trained foundation models to provide feedback to relabel the demonstrations. This automatically provides new pairs of demonstration-instruction data for policy fine-tuning. We evaluate our method on a broad range of experiments with the focus on generalization on unseen objects, unse
&lt;/p&gt;</description></item><item><title>FretNet&#26159;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#21270;&#21513;&#20182;&#35889;&#36716;&#24405;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#36830;&#32493;&#20540;&#38899;&#39640;&#36718;&#24275;&#27969;&#21644;&#21508;&#31181;&#21513;&#20182;&#25216;&#24039;&#30340;&#20272;&#35745;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#27604;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#24179;&#22343;&#31934;&#24230;&#21644;&#26356;&#23569;&#30340;&#27169;&#22411;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2212.03023</link><description>&lt;p&gt;
FretNet&#65306;&#36830;&#32493;&#20540;&#38899;&#39640;&#36718;&#24275;&#27969;&#25216;&#26415;&#22312;&#22810;&#22768;&#37096;&#21513;&#20182;&#35889;&#36716;&#24405;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
FretNet: Continuous-Valued Pitch Contour Streaming for Polyphonic Guitar Tablature Transcription. (arXiv:2212.03023v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03023
&lt;/p&gt;
&lt;p&gt;
FretNet&#26159;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#21270;&#21513;&#20182;&#35889;&#36716;&#24405;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#36830;&#32493;&#20540;&#38899;&#39640;&#36718;&#24275;&#27969;&#21644;&#21508;&#31181;&#21513;&#20182;&#25216;&#24039;&#30340;&#20272;&#35745;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#27604;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#24179;&#22343;&#31934;&#24230;&#21644;&#26356;&#23569;&#30340;&#27169;&#22411;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#21160;&#38899;&#20048;&#36716;&#24405;&#65288;AMT&#65289;&#30340;&#20219;&#21153;&#21463;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20854;&#20013;&#21253;&#25324;&#20174;&#38899;&#39057;&#20013;&#20272;&#35745;&#21508;&#31181;&#38899;&#31526;&#23646;&#24615;&#12290;&#21516;&#26102;&#65292;&#22810;&#38899;&#39640;&#20272;&#35745;&#65288;MPE&#65289;&#30456;&#20851;&#20219;&#21153;&#20173;&#28982;&#26159;&#20960;&#20046;&#25152;&#26377;AMT&#26041;&#27861;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#24517;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#21363;&#20351;&#21482;&#26159;&#38544;&#21547;&#22320;&#12290;&#22312;AMT&#19978;&#19979;&#25991;&#20013;&#65292;&#38899;&#39640;&#20449;&#24687;&#36890;&#24120;&#34987;&#37327;&#21270;&#20026;&#35199;&#26041;&#38899;&#20048;&#38899;&#38454;&#30340;&#21517;&#20041;&#38899;&#39640;&#12290;&#21363;&#20351;&#22312;&#26356;&#19968;&#33324;&#30340;&#24773;&#20917;&#19979;&#65292;MPE&#31995;&#32479;&#36890;&#24120;&#20063;&#20250;&#20135;&#29983;&#20855;&#26377;&#26576;&#31181;&#37327;&#21270;&#31243;&#24230;&#30340;&#38899;&#39640;&#39044;&#27979;&#12290;&#22312;GT&#36716;&#24405;&#31561;&#26576;&#20123;AMT&#24212;&#29992;&#20013;&#65292;&#20272;&#35745;&#36830;&#32493;&#20540;&#38899;&#39640;&#36718;&#24275;&#26356;&#26377;&#24847;&#20041;&#12290;&#21513;&#20182;&#35889;&#26377;&#33021;&#21147;&#34920;&#31034;&#21508;&#31181;&#28436;&#22863;&#25216;&#24039;&#65292;&#20854;&#20013;&#19968;&#20123;&#28041;&#21450;&#38899;&#39640;&#35843;&#21046;&#12290;&#30446;&#21069;&#30340;AMT&#26041;&#27861;&#27809;&#26377;&#20805;&#20998;&#35299;&#20915;&#38899;&#39640;&#35843;&#21046;&#38382;&#39064;&#65292;&#24182;&#19988;&#21482;&#25552;&#20379;&#26356;&#23569;&#30340;&#37327;&#21270;&#26041;&#27861;&#20197;&#25442;&#21462;&#26356;&#22810;&#30340;&#27169;&#22411;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FretNet&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#23454;&#29616;&#36830;&#32493;&#20540;&#38899;&#39640;&#36718;&#24275;&#27969;&#65292;&#24182;&#20272;&#35745;&#22312;&#21508;&#20010;&#26102;&#38388;&#28857;&#19978;&#30340;&#38899;&#39640;&#12289;&#24310;&#38899;&#21644;&#28369;&#38899;&#31561;&#21513;&#20182;&#25216;&#24039;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;FretNet&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#25552;&#20379;&#26356;&#39640;&#30340;&#24179;&#22343;&#31934;&#24230;&#21644;&#26356;&#23569;&#30340;&#27169;&#22411;&#22797;&#26434;&#24230;&#12290;&#22312;&#21508;&#31181;GT&#20219;&#21153;&#20013;&#65292;&#21253;&#25324;&#36716;&#25442;&#27665;&#35875;&#21513;&#20182;&#12289;&#27969;&#34892;&#21513;&#20182;&#21644;&#30005;&#21488;&#21513;&#20182;&#31561;&#21513;&#20182;&#31867;&#22411;&#65292;FretNet&#37117;&#34920;&#29616;&#20986;&#33394;&#65292;&#36825;&#34920;&#26126;&#20102;FretNet&#22312;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the task of Automatic Music Transcription (AMT), whereby various attributes of music notes are estimated from audio, has received increasing attention. At the same time, the related task of Multi-Pitch Estimation (MPE) remains a challenging but necessary component of almost all AMT approaches, even if only implicitly. In the context of AMT, pitch information is typically quantized to the nominal pitches of the Western music scale. Even in more general contexts, MPE systems typically produce pitch predictions with some degree of quantization. In certain applications of AMT, such as Guitar Tablature Transcription (GTT), it is more meaningful to estimate continuous-valued pitch contours. Guitar tablature has the capacity to represent various playing techniques, some of which involve pitch modulation. Contemporary approaches to AMT do not adequately address pitch modulation, and offer only less quantization at the expense of more model complexity. In this paper, we present
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HetMed&#30340;&#24322;&#26500;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#34701;&#21512;&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#20020;&#24202;&#20915;&#31574;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.15158</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#20998;&#26512;&#30340;&#24322;&#26500;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Graph Learning for Multi-modal Medical Data Analysis. (arXiv:2211.15158v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15158
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HetMed&#30340;&#24322;&#26500;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#34701;&#21512;&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#20020;&#24202;&#20915;&#31574;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30149;&#20154;&#30340;&#24120;&#35268;&#20020;&#24202;&#35775;&#38382;&#19981;&#20165;&#20250;&#20135;&#29983;&#22270;&#20687;&#25968;&#25454;&#65292;&#36824;&#20250;&#21253;&#21547;&#26377;&#20851;&#30149;&#20154;&#30340;&#20020;&#24202;&#20449;&#24687;&#31561;&#38750;&#22270;&#20687;&#25968;&#25454;&#65292;&#21363;&#21307;&#23398;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#24615;&#36136;&#12290;&#36825;&#26679;&#30340;&#24322;&#26500;&#27169;&#24577;&#25552;&#20379;&#20102;&#19981;&#21516;&#21644;&#20114;&#34917;&#30340;&#30149;&#20154;&#35270;&#35282;&#65292;&#24403;&#23427;&#20204;&#34987;&#27491;&#30830;&#22320;&#32452;&#21512;&#26102;&#65292;&#21487;&#20197;&#23548;&#33268;&#26356;&#20934;&#30830;&#30340;&#20020;&#24202;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20854;&#37325;&#35201;&#24615;&#65292;&#22914;&#20309;&#23558;&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#26377;&#25928;&#22320;&#34701;&#21512;&#21040;&#32479;&#19968;&#26694;&#26550;&#20013;&#21364;&#21463;&#21040;&#20102;&#30456;&#23545;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HetMed&#65288;&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#20998;&#26512;&#30340;&#24322;&#26500;&#22270;&#23398;&#20064;&#65289;&#30340;&#26377;&#25928;&#22270;&#24418;&#26694;&#26550;&#65292;&#29992;&#20110;&#34701;&#21512;&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#19968;&#20010;&#21253;&#25324;&#22810;&#31181;&#30149;&#20154;&#38750;&#22270;&#20687;&#29305;&#24449;&#30340;&#22810;&#37325;&#32593;&#32476;&#65292;&#20197;&#31995;&#32479;&#21270;&#26041;&#24335;&#25429;&#33719;&#30149;&#20154;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#20174;&#32780;&#23548;&#33268;&#26356;&#20934;&#30830;&#30340;&#20020;&#24202;&#20915;&#31574;&#12290;&#22823;&#37327;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Routine clinical visits of a patient produce not only image data, but also non-image data containing clinical information regarding the patient, i.e., medical data is multi-modal in nature. Such heterogeneous modalities offer different and complementary perspectives on the same patient, resulting in more accurate clinical decisions when they are properly combined. However, despite its significance, how to effectively fuse the multi-modal medical data into a unified framework has received relatively little attention. In this paper, we propose an effective graph-based framework called HetMed (Heterogeneous Graph Learning for Multi-modal Medical Data Analysis) for fusing the multi-modal medical data. Specifically, we construct a multiplex network that incorporates multiple types of non-image features of patients to capture the complex relationship between patients in a systematic way, which leads to more accurate clinical decisions. Extensive experiments on various real-world datasets dem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#29983;&#29289;&#21551;&#31034;&#30340;&#26465;&#20214;&#26102;&#38388;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(BI-CTVAE)&#27169;&#22411;&#65292;&#36890;&#36807;&#25345;&#32493;&#23398;&#20064;&#29983;&#25104;(CL2Gen)&#22330;&#26223;&#65292;&#21487;&#20197;&#23545;&#19981;&#21516;&#31867;&#21035;&#30340;&#36816;&#21160;&#24207;&#21015;&#36827;&#34892;&#29983;&#25104;&#65292;&#24182;&#22312;&#19968;&#32452;&#20219;&#21153;&#19978;&#24471;&#21040;&#36739;&#39640;&#30340;&#29983;&#25104;&#20934;&#30830;&#24615;&#21644;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.05231</link><description>&lt;p&gt;
&#21463;&#29983;&#29289;&#21551;&#31034;&#30340;&#20154;&#31867;&#36816;&#21160;&#24207;&#21015;&#30340;&#25345;&#32493;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Biologically-Inspired Continual Learning of Human Motion Sequences. (arXiv:2211.05231v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#29983;&#29289;&#21551;&#31034;&#30340;&#26465;&#20214;&#26102;&#38388;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(BI-CTVAE)&#27169;&#22411;&#65292;&#36890;&#36807;&#25345;&#32493;&#23398;&#20064;&#29983;&#25104;(CL2Gen)&#22330;&#26223;&#65292;&#21487;&#20197;&#23545;&#19981;&#21516;&#31867;&#21035;&#30340;&#36816;&#21160;&#24207;&#21015;&#36827;&#34892;&#29983;&#25104;&#65292;&#24182;&#22312;&#19968;&#32452;&#20219;&#21153;&#19978;&#24471;&#21040;&#36739;&#39640;&#30340;&#29983;&#25104;&#20934;&#30830;&#24615;&#21644;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25345;&#32493;&#23398;&#20064;&#27169;&#22411;&#65292;&#19987;&#27880;&#20110;&#28041;&#21450;&#26102;&#38388;&#24207;&#21015;&#8212;&#8212;&#20855;&#20307;&#32780;&#35328;&#65292;&#26159;&#20154;&#30340;&#36816;&#21160;&#12290;&#35813;&#27169;&#22411;&#25913;&#36827;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#31867;&#20284;&#20110;&#22823;&#33041;&#30340;&#37325;&#25918;&#27169;&#22411;(BI-R)&#65292;&#23427;&#24314;&#31435;&#20102;&#19968;&#20010;&#21463;&#29983;&#29289;&#21551;&#31034;&#30340;&#26465;&#20214;&#26102;&#38388;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(BI-CTVAE)&#65292;&#20854;&#23454;&#20363;&#21270;&#20102;&#19968;&#20010;&#39640;&#26031;&#20989;&#25968;&#28151;&#21512;&#20307;&#26469;&#34920;&#31034;&#31867;&#21035;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25345;&#32493;&#23398;&#20064;&#29983;&#25104;(CL2Gen)&#22330;&#26223;&#65292;&#20854;&#20013;&#27169;&#22411;&#29983;&#25104;&#19981;&#21516;&#31867;&#21035;&#30340;&#36816;&#21160;&#24207;&#21015;&#12290;&#27169;&#22411;&#30340;&#29983;&#25104;&#20934;&#30830;&#24615;&#22312;&#19968;&#32452;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#22312;&#25353;&#39034;&#24207;&#23398;&#20064;&#25152;&#26377;&#21160;&#20316;&#31867;&#21035;&#20043;&#21518;&#65292;BI-CTVAE&#22312;&#19968;&#20010;&#20154;&#31867;&#36816;&#21160;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#32456;&#20998;&#31867;&#20934;&#30830;&#24615;&#36798;&#21040;&#20102;78&#65285;&#65292;&#27604;&#19981;&#20351;&#29992;&#37325;&#25918;&#39640;63&#65285;&#65292;&#27604;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;&#35757;&#32451;GRU&#27169;&#22411;&#20302;5.4&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes a model for continual learning on tasks involving temporal sequences, specifically, human motions. It improves on a recently proposed brain-inspired replay model (BI-R) by building a biologically-inspired conditional temporal variational autoencoder (BI-CTVAE), which instantiates a latent mixture-of-Gaussians for class representation. We investigate a novel continual-learning-to-generate (CL2Gen) scenario where the model generates motion sequences of different classes. The generative accuracy of the model is tested over a set of tasks. The final classification accuracy of BI-CTVAE on a human motion dataset after sequentially learning all action classes is 78%, which is 63% higher than using no-replay, and only 5.4% lower than a state-of-the-art offline trained GRU model.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;AC-OSELM&#30340;&#39640;&#25928;RL&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#20272;&#35745;&#21512;&#36866;&#30340;&#21387;&#32553;&#27604;&#65292;&#20174;&#32780;&#23454;&#29616;&#25968;&#25454;&#30340;&#39640;&#25928;&#21387;&#32553;&#65292;&#21516;&#26102;&#20445;&#25345;&#37325;&#26500;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.04284</link><description>&lt;p&gt;
&#22522;&#20110;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#36793;&#32536;&#35745;&#31639;&#39640;&#25928;&#21387;&#32553;&#27604;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Efficient Compressed Ratio Estimation using Online Sequential Learning for Edge Computing. (arXiv:2211.04284v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;AC-OSELM&#30340;&#39640;&#25928;RL&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#20272;&#35745;&#21512;&#36866;&#30340;&#21387;&#32553;&#27604;&#65292;&#20174;&#32780;&#23454;&#29616;&#25968;&#25454;&#30340;&#39640;&#25928;&#21387;&#32553;&#65292;&#21516;&#26102;&#20445;&#25345;&#37325;&#26500;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29289;&#32852;&#32593;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#22823;&#37327;&#30340;&#20256;&#24863;&#22120;&#20449;&#24687;&#27491;&#22312;&#23454;&#26102;&#37319;&#38598;&#12290;&#22240;&#27492;&#65292;&#20174;&#36793;&#32536;&#35774;&#22791;&#20256;&#36755;&#25968;&#25454;&#30340;&#36890;&#35759;&#25104;&#26412;&#19981;&#26029;&#22686;&#21152;&#12290;&#21387;&#32553;&#24863;&#30693;&#65288;CS&#65289;&#26159;&#19968;&#31181;&#21487;&#29992;&#20110;&#36793;&#32536;&#35774;&#22791;&#30340;&#25968;&#25454;&#21387;&#32553;&#26041;&#27861;&#65292;&#24182;&#22240;&#20854;&#21487;&#33410;&#30465;&#36890;&#35759;&#25104;&#26412;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#22312;&#21387;&#32553;&#24863;&#30693;&#20013;&#65292;&#20272;&#35745;&#21512;&#36866;&#30340;&#21387;&#32553;&#27604;&#26159;&#37325;&#35201;&#30340;&#12290;&#29616;&#26377;&#30340;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#33258;&#36866;&#24212;&#20272;&#35745;&#33719;&#24471;&#25968;&#25454;&#30340;&#21387;&#32553;&#27604;&#30340;&#26041;&#27861;&#65292;&#20854;&#35745;&#31639;&#25104;&#26412;&#24120;&#24120;&#24456;&#39640;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#38024;&#23545;&#36793;&#32536;&#35774;&#22791;&#30340;&#39640;&#25928;RL&#26041;&#27861;&#65292;&#31216;&#20026;actor-critic&#22312;&#32447;&#36830;&#32493;&#26497;&#38480;&#23398;&#20064;&#26426;&#65288;AC-OSELM&#65289;&#65292;&#24182;&#21033;&#29992;AC-OSELM&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#36890;&#36807;&#20272;&#35745;&#36866;&#24403;&#30340;&#21387;&#32553;&#27604;&#21387;&#32553;&#25968;&#25454;&#30340;&#31995;&#32479;&#12290;&#36890;&#36807;&#20351;&#29992;&#23454;&#38469;&#20256;&#24863;&#22120;&#25968;&#25454;&#36827;&#34892;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#22312;&#20272;&#35745;&#21387;&#32553;&#27604;&#21644;&#37325;&#26500;&#21387;&#32553;&#25968;&#25454;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;&#37325;&#26500;&#21387;&#32553;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#20102;&#27604;&#24378;&#21270;&#23398;&#20064;&#21644;&#20256;&#32479;&#26041;&#27861;&#26356;&#39640;&#25928;&#30340;&#21387;&#32553;&#27604;&#20272;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Owing to the widespread adoption of the Internet of Things, a vast amount of sensor information is being acquired in real time. Accordingly, the communication cost of data from edge devices is increasing. Compressed sensing (CS), a data compression method that can be used on edge devices, has been attracting attention as a method to reduce communication costs. In CS, estimating the appropriate compression ratio is important. There is a method to adaptively estimate the compression ratio for the acquired data using reinforcement learning (RL). However, the computational costs associated with existing RL methods that can be utilized on edges are often high. In this study, we developed an efficient RL method for edge devices, referred to as the actor--critic online sequential extreme learning machine (AC-OSELM), and a system to compress data by estimating an appropriate compression ratio on the edge using AC-OSELM. The performance of the proposed method in estimating the compression ratio
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#20110;&#36866;&#24212;&#19981;&#21516;&#31867;&#22411;&#30340;&#26292;&#39118;&#38632;&#39044;&#27979;&#65292;&#21033;&#29992;&#22810;&#31181;&#25968;&#25454;&#28304;&#36827;&#34892;&#25968;&#25454;&#34701;&#21512;&#65292;&#24182;&#33021;&#39044;&#27979;&#38647;&#30005;&#12289;&#20912;&#38649;&#21644;&#26292;&#38632;&#30340;&#27010;&#29575;&#65292;&#20854;&#20013;&#22825;&#27668;&#38647;&#36798;&#20135;&#21697;&#26159;&#26368;&#37325;&#35201;&#30340;&#39044;&#27979;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2211.01001</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#26292;&#39118;&#38632;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#31181;&#22810;&#21361;&#38505;&#25968;&#25454;&#34701;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Thunderstorm nowcasting with deep learning: a multi-hazard data fusion model. (arXiv:2211.01001v2 [physics.ao-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01001
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#20110;&#36866;&#24212;&#19981;&#21516;&#31867;&#22411;&#30340;&#26292;&#39118;&#38632;&#39044;&#27979;&#65292;&#21033;&#29992;&#22810;&#31181;&#25968;&#25454;&#28304;&#36827;&#34892;&#25968;&#25454;&#34701;&#21512;&#65292;&#24182;&#33021;&#39044;&#27979;&#38647;&#30005;&#12289;&#20912;&#38649;&#21644;&#26292;&#38632;&#30340;&#27010;&#29575;&#65292;&#20854;&#20013;&#22825;&#27668;&#38647;&#36798;&#20135;&#21697;&#26159;&#26368;&#37325;&#35201;&#30340;&#39044;&#27979;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#65292;&#22914;&#31532;&#19968;&#21453;&#24212;&#32773;&#12289;&#22522;&#30784;&#35774;&#26045;&#31649;&#29702;&#21644;&#33322;&#31354;&#31561;&#39046;&#22495;&#65292;&#37117;&#38656;&#35201;&#39044;&#27979;&#19982;&#38647;&#26292;&#30456;&#20851;&#30340;&#21361;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#21361;&#38505;&#31867;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#28304;&#65307;&#25105;&#20204;&#20351;&#29992;&#20102;&#26469;&#33258;&#22825;&#27668;&#38647;&#36798;&#12289;&#38378;&#30005;&#25506;&#27979;&#12289;&#21355;&#26143;&#21487;&#35265;/&#32418;&#22806;&#22270;&#20687;&#12289;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#21644;&#25968;&#23383;&#39640;&#31243;&#27169;&#22411;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#39044;&#27979;&#38378;&#30005;&#12289;&#20912;&#38649;&#21644;&#26292;&#38632;&#30340;&#33021;&#21147;&#65292;&#29992;1 km&#20998;&#36776;&#29575;&#32593;&#26684;&#21644;5&#20998;&#38047;&#26102;&#38388;&#20998;&#36776;&#29575;&#65292;&#39044;&#27979;&#36229;&#36807;60&#20998;&#38047;&#12290;Shapley&#20540;&#37327;&#21270;&#20102;&#19981;&#21516;&#25968;&#25454;&#26469;&#28304;&#30340;&#37325;&#35201;&#24615;&#65292;&#26174;&#31034;&#20986;&#22825;&#27668;&#38647;&#36798;&#20135;&#21697;&#26159;&#25152;&#26377;&#19977;&#31181;&#21361;&#38505;&#31867;&#22411;&#30340;&#26368;&#37325;&#35201;&#39044;&#27979;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictions of thunderstorm-related hazards are needed in several sectors, including first responders, infrastructure management and aviation. To address this need, we present a deep learning model that can be adapted to different hazard types. The model can utilize multiple data sources; we use data from weather radar, lightning detection, satellite visible/infrared imagery, numerical weather prediction and digital elevation models. We demonstrate the ability of the model to predict lightning, hail and heavy precipitation probabilistically on a 1 km resolution grid, with a temporal resolution of 5 min and lead times up to 60 min. Shapley values quantify the importance of the different data sources, showing that the weather radar products are the most important predictors for all three hazard types.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#29992;&#20110;&#30452;&#25509;&#20174;&#35266;&#27979;&#36755;&#20837;&#21040;&#25152;&#38656;&#30340;&#20272;&#35745;&#22120;&#32479;&#35745;&#37327;&#23398;&#20064;&#36882;&#24402;&#26144;&#23556;&#65292;&#21487;&#20197;&#36817;&#20284;&#20272;&#35745;&#28508;&#22312;&#26102;&#38388;&#24207;&#21015;&#20449;&#21495;&#30340;&#26465;&#20214;&#32479;&#35745;&#37327;&#65292;&#22312;&#38750;&#32039;&#33268;&#22495;&#20013;&#26377;&#35823;&#24046;&#30028;&#38480;&#65292;&#22312;&#38271;&#26102;&#38388;&#19978;&#26377;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.00335</link><description>&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#36125;&#21494;&#26031;&#28388;&#27874;&#22120;&#30340;&#36890;&#29992;&#36924;&#36817;&#24615;
&lt;/p&gt;
&lt;p&gt;
Recurrent Neural Networks and Universal Approximation of Bayesian Filters. (arXiv:2211.00335v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#29992;&#20110;&#30452;&#25509;&#20174;&#35266;&#27979;&#36755;&#20837;&#21040;&#25152;&#38656;&#30340;&#20272;&#35745;&#22120;&#32479;&#35745;&#37327;&#23398;&#20064;&#36882;&#24402;&#26144;&#23556;&#65292;&#21487;&#20197;&#36817;&#20284;&#20272;&#35745;&#28508;&#22312;&#26102;&#38388;&#24207;&#21015;&#20449;&#21495;&#30340;&#26465;&#20214;&#32479;&#35745;&#37327;&#65292;&#22312;&#38750;&#32039;&#33268;&#22495;&#20013;&#26377;&#35823;&#24046;&#30028;&#38480;&#65292;&#22312;&#38271;&#26102;&#38388;&#19978;&#26377;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#36125;&#21494;&#26031;&#26368;&#20248;&#28388;&#27874;&#38382;&#39064;&#65292;&#21363;&#20174;&#35266;&#27979;&#24207;&#21015;&#20013;&#20272;&#35745;&#28508;&#22312;&#26102;&#38388;&#24207;&#21015;&#20449;&#21495;&#30340;&#26465;&#20214;&#32479;&#35745;&#37327;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#20551;&#23450;&#25110;&#20272;&#35745;&#30340;&#36716;&#31227;&#21644;&#35266;&#27979;&#27169;&#22411;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#24182;&#35797;&#22270;&#30452;&#25509;&#20174;&#35266;&#27979;&#36755;&#20837;&#21040;&#25152;&#38656;&#30340;&#20272;&#35745;&#22120;&#32479;&#35745;&#37327;&#23398;&#20064;&#36882;&#24402;&#26144;&#23556;&#12290;&#26412;&#25991;&#30340;&#37325;&#28857;&#26159;&#27492;&#26694;&#26550;&#30340;&#36924;&#36817;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#33324;&#38750;&#32039;&#33268;&#22495;&#30340;&#28388;&#27874;&#36924;&#36817;&#35823;&#24046;&#30028;&#38480;&#12290;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#24378;&#26102;&#38388;&#19968;&#33268;&#30340;&#36924;&#36817;&#35823;&#24046;&#30028;&#38480;&#65292;&#20445;&#35777;&#33391;&#22909;&#30340;&#38271;&#26399;&#24615;&#33021;&#12290;&#25105;&#20204;&#35752;&#35770;&#21644;&#35828;&#26126;&#20102;&#36825;&#20123;&#32467;&#26524;&#30340;&#35768;&#22810;&#23454;&#38469;&#20851;&#27880;&#28857;&#21644;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the Bayesian optimal filtering problem: i.e. estimating some conditional statistics of a latent time-series signal from an observation sequence. Classical approaches often rely on the use of assumed or estimated transition and observation models. Instead, we formulate a generic recurrent neural network framework and seek to learn directly a recursive mapping from observational inputs to the desired estimator statistics. The main focus of this article is the approximation capabilities of this framework. We provide approximation error bounds for filtering in general non-compact domains. We also consider strong time-uniform approximation error bounds that guarantee good long-time performance. We discuss and illustrate a number of practical concerns and implications of these results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#36880;&#27493;&#35745;&#31639;&#26816;&#27979;&#32479;&#35745;&#37327;&#30340;&#32047;&#31215;&#21644;&#26469;&#26816;&#27979;&#21464;&#28857;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.17312</link><description>&lt;p&gt;
&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26102;&#24207;&#21464;&#28857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Training Neural Networks for Sequential Change-point Detection. (arXiv:2210.17312v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#36880;&#27493;&#35745;&#31639;&#26816;&#27979;&#32479;&#35745;&#37327;&#30340;&#32047;&#31215;&#21644;&#26469;&#26816;&#27979;&#21464;&#28857;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#25968;&#25454;&#27969;&#20013;&#30340;&#31361;&#21464;&#20998;&#24067;&#36716;&#25442;&#65292;&#21363;&#25152;&#35859;&#30340;&#21464;&#28857;&#26816;&#27979;&#65292;&#26159;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#36880;&#27493;&#35745;&#31639;&#26816;&#27979;&#32479;&#35745;&#37327;&#30340;&#32047;&#31215;&#21644;&#65292;&#24403;&#21457;&#29983;&#21464;&#28857;&#26102;&#65292;&#35813;&#37327;&#20250;&#26174;&#33879;&#21464;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#26816;&#27979;&#21464;&#28857;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting an abrupt distributional shift of a data stream, known as change-point detection, is a fundamental problem in statistics and machine learning. We introduce a novel approach for online change-point detection using neural networks. To be specific, our approach is training neural networks to compute the cumulative sum of a detection statistic sequentially, which exhibits a significant change when a change-point occurs. We demonstrated the superiority and potential of the proposed method in detecting change-point using both synthetic and real-world data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#23545;&#25239;&#25915;&#20987;&#20256;&#36882;&#24230;&#37327;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#37327;&#21270;&#19988;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#26550;&#26500;&#30456;&#20284;&#24230;&#20989;&#25968;&#65292;&#20998;&#26512;&#20102;69&#20010;&#26368;&#20808;&#36827;&#30340;ImageNet&#20998;&#31867;&#22120;&#65292;&#21457;&#29616;&#22810;&#26679;&#21270;&#30340;&#31070;&#32463;&#26550;&#26500;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#38598;&#21512;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.11407</link><description>&lt;p&gt;
&#22522;&#20110;&#36755;&#20837;&#26799;&#24230;&#20256;&#36882;&#30340;&#31070;&#32463;&#26550;&#26500;&#30456;&#20284;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Similarity of Neural Architectures Based on Input Gradient Transferability. (arXiv:2210.11407v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#23545;&#25239;&#25915;&#20987;&#20256;&#36882;&#24230;&#37327;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#37327;&#21270;&#19988;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#26550;&#26500;&#30456;&#20284;&#24230;&#20989;&#25968;&#65292;&#20998;&#26512;&#20102;69&#20010;&#26368;&#20808;&#36827;&#30340;ImageNet&#20998;&#31867;&#22120;&#65292;&#21457;&#29616;&#22810;&#26679;&#21270;&#30340;&#31070;&#32463;&#26550;&#26500;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#38598;&#21512;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20026;&#22270;&#20687;&#20998;&#31867;&#32780;&#24320;&#21457;&#20102;&#22823;&#37327;&#30340;&#28145;&#24230;&#31070;&#32463;&#26550;&#26500;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#30456;&#20284;&#25110;&#19981;&#21516;&#65292;&#20197;&#21450;&#20160;&#20040;&#22240;&#32032;&#24433;&#21709;&#23427;&#20204;&#30340;&#30456;&#20284;&#24615;&#25110;&#19981;&#21516;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#26088;&#22312;&#35774;&#35745;&#19968;&#20010;&#37327;&#21270;&#19988;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#26550;&#26500;&#30456;&#20284;&#24230;&#20989;&#25968;&#20197;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21033;&#29992;&#23545;&#25239;&#25915;&#20987;&#20256;&#36882;&#24230;&#37327;&#65292;&#35813;&#24230;&#37327;&#20855;&#26377;&#19982;&#36755;&#20837;&#26799;&#24230;&#21644;&#20915;&#31574;&#36793;&#30028;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#34987;&#24191;&#27867;&#29992;&#20110;&#29702;&#35299;&#27169;&#22411;&#34892;&#20026;&#12290;&#25105;&#20204;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#30456;&#20284;&#24230;&#20989;&#25968;&#23545;69&#20010;&#26368;&#20808;&#36827;&#30340;ImageNet&#20998;&#31867;&#22120;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#20998;&#26512;&#65292;&#20174;&#32780;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19982;&#31070;&#32463;&#26550;&#26500;&#30456;&#20851;&#30340;&#29616;&#35937;&#65292;&#21363;&#27169;&#22411;&#22810;&#26679;&#24615;&#21487;&#20197;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#23545;&#27169;&#22411;&#38598;&#21512;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#24615;&#33021;&#26377;&#25152;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#20026;&#20160;&#20040;&#24320;&#21457;&#20855;&#26377;&#19981;&#21516;&#32452;&#20214;&#30340;&#22810;&#26679;&#21270;&#31070;&#32463;&#26550;&#26500;&#26159;&#24517;&#35201;&#30340;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, a huge amount of deep neural architectures have been developed for image classification. It remains curious whether these models are similar or different and what factors contribute to their similarities or differences. To address this question, we aim to design a quantitative and scalable similarity function between neural architectures. We utilize adversarial attack transferability, which has information related to input gradients and decision boundaries that are widely used to understand model behaviors. We conduct a large-scale analysis on 69 state-of-the-art ImageNet classifiers using our proposed similarity function to answer the question. Moreover, we observe neural architecture-related phenomena using model similarity that model diversity can lead to better performance on model ensembles and knowledge distillation under specific conditions. Our results provide insights into why the development of diverse neural architectures with distinct components is necessar
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#19977;&#32500;&#34920;&#31034;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20004;&#20010;&#38454;&#27573;&#65292;&#30456;&#27604;&#20110;&#20108;&#32500;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25805;&#20316;&#20219;&#21153;&#20013;&#20855;&#26377;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#24182;&#19988;&#26356;&#22909;&#22320;&#27867;&#21270;&#21040;&#20854;&#20182;&#24773;&#20917;&#19979;&#30340;RL&#12290;</title><link>http://arxiv.org/abs/2210.07241</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#30417;&#30563;&#19977;&#32500;&#34920;&#31034;&#30340;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Visual Reinforcement Learning with Self-Supervised 3D Representations. (arXiv:2210.07241v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07241
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#19977;&#32500;&#34920;&#31034;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20004;&#20010;&#38454;&#27573;&#65292;&#30456;&#27604;&#20110;&#20108;&#32500;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25805;&#20316;&#20219;&#21153;&#20013;&#20855;&#26377;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#24182;&#19988;&#26356;&#22909;&#22320;&#27867;&#21270;&#21040;&#20854;&#20182;&#24773;&#20917;&#19979;&#30340;RL&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#27861;&#26159;&#20351;&#29992;&#33258;&#30417;&#30563;&#26041;&#27861;&#23398;&#20064;&#20869;&#37096;&#29366;&#24577;&#34920;&#31034;&#65292;&#36825;&#20855;&#26377;&#36890;&#36807;&#39069;&#22806;&#23398;&#20064;&#20449;&#21495;&#21644;&#24402;&#32435;&#20559;&#24046;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#30495;&#23454;&#19990;&#30028;&#26412;&#36136;&#19978;&#26159;&#19977;&#32500;&#30340;&#65292;&#20294;&#20043;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#20110;&#21033;&#29992;&#20108;&#32500;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#20316;&#20026;&#36741;&#21161;&#33258;&#30417;&#30563;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#19977;&#32500;&#34920;&#31034;&#29992;&#20110;&#36816;&#21160;&#25511;&#21046;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#20854;&#20013;&#23545;&#28145;&#24230;&#20307;&#32032;&#19977;&#32500;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#30446;&#26631;&#20026;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#65292;&#20197;&#21450;&#24494;&#35843;&#38454;&#27573;&#65292;&#22312;&#35813;&#38454;&#27573;&#65292;&#34920;&#31034;&#19982;RL&#19968;&#36215;&#22312;&#39046;&#22495;&#20869;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#34920;&#26126;&#65292;&#19982;&#20108;&#32500;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27169;&#25311;&#25805;&#20316;&#20219;&#21153;&#20013;&#20855;&#26377;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290; &#27492;&#22806;&#65292;&#25105;&#20204;&#23398;&#20064;&#30340;&#31574;&#30053;&#20351;&#20854;&#19982;&#20256;&#32479;&#20108;&#32500;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#27867;&#21270;&#21040;&#20854;&#20182;&#24773;&#20917;&#19979;&#30340;RL&#12290;
&lt;/p&gt;
&lt;p&gt;
A prominent approach to visual Reinforcement Learning (RL) is to learn an internal state representation using self-supervised methods, which has the potential benefit of improved sample-efficiency and generalization through additional learning signal and inductive biases. However, while the real world is inherently 3D, prior efforts have largely been focused on leveraging 2D computer vision techniques as auxiliary self-supervision. In this work, we present a unified framework for self-supervised learning of 3D representations for motor control. Our proposed framework consists of two phases: a pretraining phase where a deep voxel-based 3D autoencoder is pretrained on a large object-centric dataset, and a finetuning phase where the representation is jointly finetuned together with RL on in-domain data. We empirically show that our method enjoys improved sample efficiency in simulated manipulation tasks compared to 2D representation learning methods. Additionally, our learned policies tra
&lt;/p&gt;</description></item><item><title>MAPL&#20351;&#29992;&#23545;&#40784;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#23398;&#20064;&#21333;&#27169;&#24577;&#27169;&#22411;&#34920;&#31034;&#31354;&#38388;&#20043;&#38388;&#30340;&#36731;&#37327;&#32423;&#26144;&#23556;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#38754;&#21521;&#35270;&#35273;-&#35821;&#35328;&#23569;&#26679;&#26412;&#20219;&#21153;&#30340;&#22522;&#20110;&#21442;&#25968;&#25928;&#29575;&#30340;&#36866;&#24212;&#65292;&#24182;&#22312;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2210.07179</link><description>&lt;p&gt;
MAPL: &#22522;&#20110;&#21442;&#25968;&#25928;&#29575;&#30340;&#21333;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#35270;&#35273;-&#35821;&#35328;&#23569;&#26679;&#26412;&#20219;&#21153;&#20013;&#30340;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
MAPL: Parameter-Efficient Adaptation of Unimodal Pre-Trained Models for Vision-Language Few-Shot Prompting. (arXiv:2210.07179v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07179
&lt;/p&gt;
&lt;p&gt;
MAPL&#20351;&#29992;&#23545;&#40784;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#23398;&#20064;&#21333;&#27169;&#24577;&#27169;&#22411;&#34920;&#31034;&#31354;&#38388;&#20043;&#38388;&#30340;&#36731;&#37327;&#32423;&#26144;&#23556;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#38754;&#21521;&#35270;&#35273;-&#35821;&#35328;&#23569;&#26679;&#26412;&#20219;&#21153;&#30340;&#22522;&#20110;&#21442;&#25968;&#25928;&#29575;&#30340;&#36866;&#24212;&#65292;&#24182;&#22312;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21333;&#27169;&#24577;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#65292;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#20986;&#33394;&#30340;&#38646;&#26679;&#26412;&#21644;&#65288;&#22522;&#20110;&#25552;&#31034;&#30340;&#65289;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MAPL&#65292;&#19968;&#31181;&#31616;&#21333;&#19988;&#21442;&#25968;&#25928;&#29575;&#39640;&#30340;&#26041;&#27861;&#65292;&#23427;&#37325;&#29992;&#20923;&#32467;&#30340;&#21333;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#22312;&#22810;&#27169;&#24577;&#35270;&#35273;-&#35821;&#35328;&#65288;VL&#65289;&#22330;&#26223;&#20013;&#30340;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#12290;MAPL&#20351;&#29992;&#23545;&#40784;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#23398;&#20064;&#20102;&#21333;&#27169;&#24577;&#27169;&#22411;&#34920;&#31034;&#31354;&#38388;&#20043;&#38388;&#30340;&#36731;&#37327;&#32423;&#26144;&#23556;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#20165;&#26377;&#23569;&#37327;&#19978;&#19979;&#25991;&#31034;&#20363;&#23601;&#25512;&#24191;&#21040;&#30475;&#19981;&#35265;&#30340;VL&#20219;&#21153;&#12290;MAPL&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#24456;&#23569;&#65292;&#20351;&#24471;&#23427;&#22312;&#20302;&#25968;&#25454;&#21644;&#22495;&#20869;&#23398;&#20064;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;MAPL&#30340;&#27169;&#22359;&#21270;&#20351;&#24471;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#20854;&#20182;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#22312;&#20960;&#20010;&#35270;&#35273;&#38382;&#31572;&#21644;&#22270;&#20687;&#26631;&#39064;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;MAPL&#30456;&#23545;&#20110;&#31867;&#20284;&#26041;&#27861;&#22312;&#35757;&#32451;&#23569;&#24471;&#22810;&#30340;&#21442;&#25968;&#26102;&#23454;&#29616;&#20102;&#20248;&#36234;&#25110;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;MAPL&#21487;&#20197;&#22312;&#20960;&#23567;&#26102;&#20869;&#20351;&#29992;&#36866;&#24230;&#30340;&#35745;&#31639;&#36164;&#28304;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained models have proved to be remarkable zero- and (prompt-based) few-shot learners in unimodal vision and language tasks. We propose MAPL, a simple and parameter-efficient method that reuses frozen pre-trained unimodal models and leverages their strong generalization capabilities in multimodal vision-language (VL) settings. MAPL learns a lightweight mapping between the representation spaces of unimodal models using aligned image-text data, and can generalize to unseen VL tasks from just a few in-context examples. The small number of trainable parameters makes MAPL effective at low-data and in-domain learning. Moreover, MAPL's modularity enables easy extension to other pre-trained models. Extensive experiments on several visual question answering and image captioning benchmarks show that MAPL achieves superior or competitive performance compared to similar methods while training orders of magnitude fewer parameters. MAPL can be trained in just a few hours using modest comp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#38646;&#20551;&#35774;&#26816;&#39564;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#25490;&#38500;&#20165;&#32972;&#26223;&#20551;&#35774;&#65292;&#20381;&#36182;&#20110;&#29305;&#24449;&#21644;&#21306;&#22495;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#20551;&#35774;&#65292;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#20449;&#21495;&#20998;&#25968;&#24773;&#20917;&#19979;&#30340;&#25968;&#25454;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2210.02226</link><description>&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#30340;&#38646;&#20551;&#35774;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Null Hypothesis Test for Anomaly Detection. (arXiv:2210.02226v3 [hep-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#38646;&#20551;&#35774;&#26816;&#39564;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#25490;&#38500;&#20165;&#32972;&#26223;&#20551;&#35774;&#65292;&#20381;&#36182;&#20110;&#29305;&#24449;&#21644;&#21306;&#22495;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#20551;&#35774;&#65292;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#20449;&#21495;&#20998;&#25968;&#24773;&#20917;&#19979;&#30340;&#25968;&#25454;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25193;&#23637;&#20102;&#20351;&#29992;&#26080;&#26631;&#31614;&#20998;&#31867;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#25490;&#38500;&#20165;&#32972;&#26223;&#20551;&#35774;&#30340;&#20551;&#35774;&#26816;&#39564;&#12290;&#36890;&#36807;&#27979;&#35797;&#20004;&#20010;&#21306;&#22495;&#30340;&#24046;&#24322;&#25968;&#25454;&#30340;&#32479;&#35745;&#29420;&#31435;&#24615;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#19981;&#20381;&#36182;&#22266;&#23450;&#24322;&#24120;&#24471;&#20998;&#38408;&#20540;&#25110;&#22312;&#21306;&#22495;&#20043;&#38388;&#22806;&#25512;&#32972;&#26223;&#20272;&#35745;&#30340;&#24773;&#20917;&#19979;&#65292;&#25490;&#38500;&#20165;&#32972;&#26223;&#20551;&#35774;&#12290;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#24322;&#24120;&#24471;&#20998;&#29305;&#24449;&#21644;&#25968;&#25454;&#38598;&#21306;&#22495;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#20551;&#35774;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#29616;&#26377;&#30340;&#21435;&#30456;&#20851;&#25216;&#26415;&#26469;&#20445;&#35777;&#12290;&#20316;&#20026;&#22522;&#20934;&#20363;&#23376;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;LHC&#22885;&#26519;&#21305;&#20811;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#20114;&#20449;&#24687;&#34920;&#31034;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#32479;&#35745;&#29420;&#31435;&#24615;&#26816;&#39564;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#20449;&#21495;&#20998;&#25968;&#19979;&#23637;&#29616;&#20102;&#20986;&#33394;&#19988;&#31283;&#20581;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#22312;&#23384;&#22312;&#23454;&#38469;&#29305;&#24449;&#30456;&#20851;&#24615;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
We extend the use of Classification Without Labels for anomaly detection with a hypothesis test designed to exclude the background-only hypothesis. By testing for statistical independence of the two discriminating dataset regions, we are able to exclude the background-only hypothesis without relying on fixed anomaly score cuts or extrapolations of background estimates between regions. The method relies on the assumption of conditional independence of anomaly score features and dataset regions, which can be ensured using existing decorrelation techniques. As a benchmark example, we consider the LHC Olympics dataset where we show that mutual information represents a suitable test for statistical independence and our method exhibits excellent and robust performance at different signal fractions even in presence of realistic feature correlations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#33258;&#30001;&#26694;&#26550;&#65292;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#23454;&#29616;&#22797;&#26434;&#39640;&#32423;&#20219;&#21153;&#30340;&#30446;&#26631;&#39537;&#21160;&#23548;&#33322;&#12290;&#36890;&#36807;&#23558;&#20808;&#21069;&#30340;&#22810;&#30446;&#26631;DRL&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#21333;&#19968;&#30446;&#26631;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#37319;&#26679;&#30340;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;&#26469;&#25351;&#23548;DRL&#26234;&#33021;&#20307;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#28385;&#36275;&#19981;&#21487;&#34892;&#30340;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#20219;&#21153;&#24182;&#23613;&#21487;&#33021;&#20943;&#23569;&#36829;&#35268;&#12290;</title><link>http://arxiv.org/abs/2210.01162</link><description>&lt;p&gt;
&#23398;&#20064;&#26368;&#23567;&#36829;&#21453;&#36830;&#32493;&#25511;&#21046;&#20197;&#23454;&#29616;&#19981;&#21487;&#34892;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#35268;&#33539;
&lt;/p&gt;
&lt;p&gt;
Learning Minimally-Violating Continuous Control for Infeasible Linear Temporal Logic Specifications. (arXiv:2210.01162v4 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#33258;&#30001;&#26694;&#26550;&#65292;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#23454;&#29616;&#22797;&#26434;&#39640;&#32423;&#20219;&#21153;&#30340;&#30446;&#26631;&#39537;&#21160;&#23548;&#33322;&#12290;&#36890;&#36807;&#23558;&#20808;&#21069;&#30340;&#22810;&#30446;&#26631;DRL&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#21333;&#19968;&#30446;&#26631;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#37319;&#26679;&#30340;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;&#26469;&#25351;&#23548;DRL&#26234;&#33021;&#20307;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#28385;&#36275;&#19981;&#21487;&#34892;&#30340;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#20219;&#21153;&#24182;&#23613;&#21487;&#33021;&#20943;&#23569;&#36829;&#35268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#26102;&#38388;&#25511;&#21046;&#32508;&#21512;&#65292;&#20197;&#23454;&#29616;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;(LTL)&#34920;&#36798;&#30340;&#22797;&#26434;&#39640;&#32423;&#20219;&#21153;&#30340;&#30446;&#26631;&#39537;&#21160;&#23548;&#33322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#33258;&#30001;&#26694;&#26550;&#65292;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#65292;&#20854;&#20013;&#24213;&#23618;&#21160;&#24577;&#31995;&#32479;&#26410;&#30693;&#65288;&#36879;&#26126;&#30418;&#23376;&#65289;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#26412;&#25991;&#32771;&#34385;&#20102;&#32473;&#23450;&#30340;LTL&#35268;&#33539;&#21487;&#33021;&#26159;&#19981;&#21487;&#34892;&#30340;&#24773;&#20917;&#65292;&#22240;&#27492;&#26080;&#27861;&#20840;&#23616;&#23436;&#25104;&#12290;&#25105;&#20204;&#19981;&#20462;&#25913;&#32473;&#23450;&#30340;LTL&#20844;&#24335;&#65292;&#32780;&#26159;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;DRL&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#36829;&#35268;&#28385;&#36275;&#23427;&#12290;&#20026;&#20102;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#23558;&#20808;&#21069;&#30340;&#22810;&#30446;&#26631;DRL&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#21333;&#19968;&#30446;&#26631;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#35201;&#27714;&#21516;&#26102;&#23454;&#29616;&#33258;&#21160;&#26426;&#28385;&#36275;&#21644;&#26368;&#23567;&#36829;&#35268;&#20195;&#20215;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#37319;&#26679;&#30340;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;&#26469;&#25351;&#23548;&#21487;&#33021;&#19981;&#21487;&#34892;&#30340;LTL&#20219;&#21153;&#30340;DRL&#26234;&#33021;&#20307;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20943;&#36731;&#20102;DRL&#30340;&#36817;&#35270;&#20542;&#21521;&#65292;&#36825;&#22312;&#23398;&#20064;&#21487;&#20197;&#20855;&#26377;&#38271;&#25110;&#26080;&#38480;&#25345;&#32493;&#26102;&#38388;&#30340;&#19968;&#33324;LTL&#20219;&#21153;&#26102;&#32463;&#24120;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores continuous-time control synthesis for target-driven navigation to satisfy complex high-level tasks expressed as linear temporal logic (LTL). We propose a model-free framework using deep reinforcement learning (DRL) where the underlying dynamic system is unknown (an opaque box). Unlike prior work, this paper considers scenarios where the given LTL specification might be infeasible and therefore cannot be accomplished globally. Instead of modifying the given LTL formula, we provide a general DRL-based approach to satisfy it with minimal violation. To do this, we transform a previously multi-objective DRL problem, which requires simultaneous automata satisfaction and minimum violation cost, into a single objective. By guiding the DRL agent with a sampling-based path planning algorithm for the potentially infeasible LTL task, the proposed approach mitigates the myopic tendencies of DRL, which are often an issue when learning general LTL tasks that can have long or infin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20108;&#23618;&#20248;&#21270;&#38382;&#39064;&#30340;&#19968;&#38454;&#65288;&#22522;&#20110;&#26799;&#24230;&#30340;&#65289;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#27867;&#21270;&#20998;&#26512;&#65292;&#24314;&#31435;&#20102;&#31639;&#27861;&#31283;&#23450;&#24615;&#19982;&#27867;&#21270;&#35823;&#24046;&#20043;&#38388;&#30340;&#22522;&#26412;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#39640;&#27010;&#29575;&#27867;&#21270;&#30028;&#65292;&#23558;&#20854;&#20174;$ \bigO(\sqrt{n}) $&#25913;&#21892;&#20026;$ \bigO(\log n) $&#65292;&#21516;&#26102;&#20063;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#27867;&#21270;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2210.01063</link><description>&lt;p&gt;
&#35770;&#20108;&#23618;&#20248;&#21270;&#38382;&#39064;&#30340;&#31283;&#23450;&#24615;&#21450;&#20854;&#27867;&#21270;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
On Stability and Generalization of Bilevel Optimization Problem. (arXiv:2210.01063v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20108;&#23618;&#20248;&#21270;&#38382;&#39064;&#30340;&#19968;&#38454;&#65288;&#22522;&#20110;&#26799;&#24230;&#30340;&#65289;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#27867;&#21270;&#20998;&#26512;&#65292;&#24314;&#31435;&#20102;&#31639;&#27861;&#31283;&#23450;&#24615;&#19982;&#27867;&#21270;&#35823;&#24046;&#20043;&#38388;&#30340;&#22522;&#26412;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#39640;&#27010;&#29575;&#27867;&#21270;&#30028;&#65292;&#23558;&#20854;&#20174;$ \bigO(\sqrt{n}) $&#25913;&#21892;&#20026;$ \bigO(\log n) $&#65292;&#21516;&#26102;&#20063;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#27867;&#21270;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#65288;&#38543;&#26426;&#65289;&#20108;&#23618;&#20248;&#21270;&#38382;&#39064;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#32463;&#24120;&#36935;&#21040;&#30340;&#38382;&#39064;&#65292;&#20855;&#26377;&#20803;&#23398;&#20064;&#12289;&#36229;&#21442;&#25968;&#20248;&#21270;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#24191;&#27867;&#24212;&#29992;&#12290;&#29616;&#26377;&#30740;&#31350;&#22823;&#22810;&#20851;&#27880;&#20110;&#20998;&#26512;&#35813;&#38382;&#39064;&#30340;&#25910;&#25947;&#24615;&#25110;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#19987;&#27880;&#20110;&#29702;&#35299;&#20854;&#27867;&#21270;&#34892;&#20026;&#12290;&#26412;&#25991;&#38024;&#23545;&#20108;&#23618;&#20248;&#21270;&#38382;&#39064;&#30340;&#19968;&#38454;&#65288;&#22522;&#20110;&#26799;&#24230;&#30340;&#65289;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#27867;&#21270;&#20998;&#26512;&#12290;&#39318;&#20808;&#22312;&#19981;&#21516;&#24418;&#24335;&#19978;&#24314;&#31435;&#20102;&#31639;&#27861;&#31283;&#23450;&#24615;&#19982;&#27867;&#21270;&#35823;&#24046;&#20043;&#38388;&#30340;&#22522;&#26412;&#32852;&#31995;&#65292;&#24182;&#32473;&#20986;&#20102;&#39640;&#27010;&#29575;&#27867;&#21270;&#30028;&#65292;&#23558;&#20854;&#20174;$ \bigO(\sqrt{n}) $&#25913;&#21892;&#20026;$ \bigO(\log n) $&#65292;&#20854;&#20013;$ n $&#26159;&#26679;&#26412;&#37327;&#12290;&#20854;&#27425;&#65292;&#23545;&#20110;&#21442;&#25968;&#25345;&#32493;&#26356;&#26032;&#30340;&#20869;&#37096;&#23618;&#19982;&#22806;&#37096;&#23618;&#36890;&#29992;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31283;&#23450;&#24615;&#30028;&#38480;&#65292;&#32780;&#29616;&#26377;&#30340;&#24037;&#20316;&#20165;&#36866;&#29992;&#20110;&#29305;&#27530;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
(Stochastic) bilevel optimization is a frequently encountered problem in machine learning with a wide range of applications such as meta-learning, hyper-parameter optimization, and reinforcement learning. Most of the existing studies on this problem only focused on analyzing the convergence or improving the convergence rate, while little effort has been devoted to understanding its generalization behaviors. In this paper, we conduct a thorough analysis on the generalization of first-order (gradient-based) methods for the bilevel optimization problem. We first establish a fundamental connection between algorithmic stability and generalization error in different forms and give a high probability generalization bound which improves the previous best one from $\bigO(\sqrt{n})$ to $\bigO(\log n)$, where $n$ is the sample size. We then provide the first stability bounds for the general case where both inner and outer level parameters are subject to continuous update, while existing work allo
&lt;/p&gt;</description></item><item><title>G2&#26159;&#19968;&#31181;&#21033;&#29992;&#26799;&#24230;&#38376;&#25511;&#26426;&#21046;&#30340;&#26032;&#22411;GNN&#26694;&#26550;&#65292;&#21487;&#32531;&#35299;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#21508;&#31181;&#22270;&#23398;&#20064;&#20219;&#21153;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.00513</link><description>&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#38376;&#25511;&#26426;&#21046;&#30340;&#22270;&#28145;&#24230;&#22810;&#36895;&#29575;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Gradient Gating for Deep Multi-Rate Learning on Graphs. (arXiv:2210.00513v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00513
&lt;/p&gt;
&lt;p&gt;
G2&#26159;&#19968;&#31181;&#21033;&#29992;&#26799;&#24230;&#38376;&#25511;&#26426;&#21046;&#30340;&#26032;&#22411;GNN&#26694;&#26550;&#65292;&#21487;&#32531;&#35299;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#21508;&#31181;&#22270;&#23398;&#20064;&#20219;&#21153;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Gradient Gating (G$^2$) &#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#21892;&#22270;&#31070;&#32463;&#32593;&#32476; (GNNs) &#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22522;&#20110;&#23545; GNN &#23618;&#30340;&#36755;&#20986;&#36827;&#34892;&#38376;&#25511;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#19968;&#31181;&#36328;&#26412;&#36136;&#22270;&#33410;&#28857;&#30340;&#28040;&#24687;&#20256;&#36882;&#20449;&#24687;&#30340;&#22810;&#36895;&#29575;&#27969;&#26426;&#21046;&#12290;&#26412;&#22320;&#26799;&#24230;&#34987;&#21033;&#29992;&#26469;&#36827;&#19968;&#27493;&#35843;&#21046;&#28040;&#24687;&#20256;&#36882;&#30340;&#26356;&#26032;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#28789;&#27963;&#22320;&#20801;&#35768;&#20351;&#29992;&#20219;&#20309;&#22522;&#26412;&#30340; GNN &#23618;&#20316;&#20026;&#21253;&#35013;&#22120;&#65292;&#20197;&#26500;&#24314;&#22810;&#36895;&#29575;&#26799;&#24230;&#38376;&#25511;&#26426;&#21046;&#12290;&#25105;&#20204;&#20005;&#26684;&#35777;&#26126; G$^2$ &#32531;&#35299;&#20102;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#24182;&#20801;&#35768;&#35774;&#35745;&#28145;&#24230; GNNs&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23454;&#35777;&#32467;&#26524;&#65292;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#21508;&#31181;&#22270;&#23398;&#20064;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;&#24322;&#36136;&#22270;&#19978;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Gradient Gating (G$^2$), a novel framework for improving the performance of Graph Neural Networks (GNNs). Our framework is based on gating the output of GNN layers with a mechanism for multi-rate flow of message passing information across nodes of the underlying graph. Local gradients are harnessed to further modulate message passing updates. Our framework flexibly allows one to use any basic GNN layer as a wrapper around which the multi-rate gradient gating mechanism is built. We rigorously prove that G$^2$ alleviates the oversmoothing problem and allows the design of deep GNNs. Empirical results are presented to demonstrate that the proposed framework achieves state-of-the-art performance on a variety of graph learning tasks, including on large-scale heterophilic graphs.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#19981;&#21516;&#25277;&#35937;&#23618;&#27425;&#19978;&#30340;&#27867;&#21270;&#38382;&#39064;&#21450;&#20854;&#26041;&#27861;&#65292;&#20854;&#20013;&#26679;&#26412;&#27867;&#21270;&#24050;&#32463;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#26410;&#26469;&#38656;&#35201;&#37325;&#28857;&#20851;&#27880;&#20943;&#23569;&#36807;&#25311;&#21512;&#65307;&#20998;&#24067;&#27867;&#21270;&#19982;&#39046;&#22495;&#27867;&#21270;&#26377;&#30456;&#20284;&#20043;&#22788;&#65292;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#22256;&#38590;&#30340;&#26679;&#26412;&#25110;&#20998;&#24067;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2209.01610</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27867;&#21270;&#65306;&#32508;&#36848;&#19982;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Generalization in Neural Networks: A Broad Survey. (arXiv:2209.01610v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.01610
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#19981;&#21516;&#25277;&#35937;&#23618;&#27425;&#19978;&#30340;&#27867;&#21270;&#38382;&#39064;&#21450;&#20854;&#26041;&#27861;&#65292;&#20854;&#20013;&#26679;&#26412;&#27867;&#21270;&#24050;&#32463;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#26410;&#26469;&#38656;&#35201;&#37325;&#28857;&#20851;&#27880;&#20943;&#23569;&#36807;&#25311;&#21512;&#65307;&#20998;&#24067;&#27867;&#21270;&#19982;&#39046;&#22495;&#27867;&#21270;&#26377;&#30456;&#20284;&#20043;&#22788;&#65292;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#22256;&#38590;&#30340;&#26679;&#26412;&#25110;&#20998;&#24067;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#19981;&#21516;&#25277;&#35937;&#23618;&#27425;&#30340;&#27010;&#24565;&#12289;&#24314;&#27169;&#26041;&#27861;&#21644;&#26368;&#36817;&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#21253;&#25324;&#26679;&#26412;&#12289;&#20998;&#24067;&#12289;&#39046;&#22495;&#12289;&#20219;&#21153;&#12289;&#27169;&#24577;&#21644;&#33539;&#22260;&#19978;&#30340;&#27867;&#21270;&#12290;&#22312;&#26679;&#26412;&#27867;&#21270;&#26041;&#38754;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#20960;&#20046;&#25152;&#26377;&#30340;&#26368;&#26032;&#25913;&#36827;&#37117;&#20943;&#23567;&#20102;&#35757;&#32451;&#35823;&#24046;&#65292;&#32780;&#36807;&#25311;&#21512;&#20445;&#25345;&#19981;&#21464;&#65307;&#38543;&#30528;&#20960;&#20046;&#25152;&#26377;&#30340;&#35757;&#32451;&#35823;&#24046;&#34987;&#28040;&#38500;&#65292;&#26410;&#26469;&#30340;&#36827;&#23637;&#23558;&#38656;&#35201;&#38598;&#20013;&#20851;&#27880;&#20943;&#23569;&#36807;&#25311;&#21512;&#12290;&#20174;&#32479;&#35745;&#23398;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;(2)&#20998;&#24067;&#27867;&#21270;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#26679;&#26412;&#26435;&#37325;&#25110;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#30340;&#21464;&#21270;&#65307;&#22240;&#27492;&#65292;&#22312;&#39046;&#22495;&#27867;&#21270;&#25104;&#21151;&#30340;&#25216;&#26415;&#26377;&#21487;&#33021;&#24212;&#29992;&#20110;&#22256;&#38590;&#30340;&#26679;&#26412;&#25110;&#20998;&#24067;&#27867;&#21270;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;(3)&#39046;&#22495;&#27867;&#21270;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#26368;&#36817;&#30340;&#36827;&#23637;&#21644;&#20016;&#23500;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper reviews concepts, modeling approaches, and recent findings along a spectrum of different levels of abstraction of neural network models including generalization across (1) Samples, (2) Distributions, (3) Domains, (4) Tasks, (5) Modalities, and (6) Scopes. Results on (1) sample generalization show that, in the case of ImageNet, nearly all the recent improvements reduced training error while overfitting stayed flat; with nearly all the training error eliminated, future progress will require a focus on reducing overfitting. Perspectives from statistics highlight how (2) distribution generalization can be viewed alternately as a change in sample weights or a change in the input-output relationship; thus, techniques that have been successful in domain generalization have the potential to be applied to difficult forms of sample or distribution generalization. Transfer learning approaches to (3) domain generalization are summarized, as are recent advances and the wealth of domain a
&lt;/p&gt;</description></item><item><title>DreamBooth&#26159;&#19968;&#31181;&#38024;&#23545;&#20027;&#39064;&#39537;&#21160;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20010;&#24615;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#20351;&#29992;&#26679;&#26412;&#22270;&#20687;&#26469;&#23454;&#29616;&#29983;&#25104;&#36924;&#30495;&#22270;&#20687;&#30340;&#29420;&#29305;&#26631;&#35782;&#31526;&#32465;&#23450;&#65292;&#20351;&#20854;&#21487;&#20197;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#21512;&#25104;&#35813;&#20027;&#39064;&#30340;&#26032;&#29256;&#36924;&#30495;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2208.12242</link><description>&lt;p&gt;
DreamBooth&#65306;&#38024;&#23545;&#20027;&#39064;&#39537;&#21160;&#30340;&#29983;&#25104;&#36827;&#34892;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation. (arXiv:2208.12242v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12242
&lt;/p&gt;
&lt;p&gt;
DreamBooth&#26159;&#19968;&#31181;&#38024;&#23545;&#20027;&#39064;&#39537;&#21160;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20010;&#24615;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#20351;&#29992;&#26679;&#26412;&#22270;&#20687;&#26469;&#23454;&#29616;&#29983;&#25104;&#36924;&#30495;&#22270;&#20687;&#30340;&#29420;&#29305;&#26631;&#35782;&#31526;&#32465;&#23450;&#65292;&#20351;&#20854;&#21487;&#20197;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#21512;&#25104;&#35813;&#20027;&#39064;&#30340;&#26032;&#29256;&#36924;&#30495;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#21457;&#23637;&#35753;AI&#30340;&#28436;&#21464;&#36798;&#21040;&#20102;&#19968;&#20010;&#26174;&#33879;&#30340;&#39134;&#36291;&#65292;&#23454;&#29616;&#20102;&#20174;&#32473;&#23450;&#25991;&#26412;&#25552;&#31034;&#20013;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#22270;&#20687;&#21512;&#25104;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#32570;&#20047;&#27169;&#20223;&#32473;&#23450;&#21442;&#32771;&#38598;&#20013;&#20027;&#20307;&#20986;&#29616;&#30340;&#22806;&#35266;&#21644;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#21512;&#25104;&#23427;&#20204;&#30340;&#26032;&#29256;&#26412;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#8220;&#20010;&#24615;&#21270;&#8221;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#21482;&#38656;&#36755;&#20837;&#19968;&#20123;&#35813;&#20027;&#39064;&#30340;&#22270;&#20687;&#65292;&#25105;&#20204;&#23601;&#23545;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#32780;&#20351;&#20854;&#23398;&#20250;&#23558;&#21807;&#19968;&#26631;&#35782;&#31526;&#19982;&#35813;&#29305;&#23450;&#20027;&#39064;&#32465;&#23450;&#12290;&#19968;&#26086;&#35813;&#20027;&#39064;&#34987;&#23884;&#20837;&#27169;&#22411;&#30340;&#36755;&#20986;&#22495;&#20013;&#65292;&#35813;&#21807;&#19968;&#26631;&#35782;&#31526;&#23601;&#21487;&#20197;&#29992;&#20110;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#21512;&#25104;&#20027;&#39064;&#30340;&#26032;&#39062;&#36924;&#30495;&#22270;&#20687;&#12290;&#36890;&#36807;&#21033;&#29992;&#23884;&#20837;&#22312;&#27169;&#22411;&#20013;&#30340;&#35821;&#20041;&#20808;&#39564;&#21644;&#26032;&#30340;&#33258;&#27835;&#31867;&#29305;&#23450;&#20808;&#39564;&#20445;&#23384;&#25439;&#22833;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#20351;&#24471;&#22312;&#19981;&#21516;&#22330;&#26223;&#12289;&#23039;&#24577;&#12289;&#35270;&#35282;&#21644;&#20809;&#29031;&#26465;&#20214;&#19979;&#21512;&#25104;&#20027;&#39064;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large text-to-image models achieved a remarkable leap in the evolution of AI, enabling high-quality and diverse synthesis of images from a given text prompt. However, these models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts. In this work, we present a new approach for "personalization" of text-to-image diffusion models. Given as input just a few images of a subject, we fine-tune a pretrained text-to-image model such that it learns to bind a unique identifier with that specific subject. Once the subject is embedded in the output domain of the model, the unique identifier can be used to synthesize novel photorealistic images of the subject contextualized in different scenes. By leveraging the semantic prior embedded in the model with a new autogenous class-specific prior preservation loss, our technique enables synthesizing the subject in diverse scenes, poses, views and lighting conditions th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32852;&#21512;&#36974;&#34109;&#35270;&#35273;&#21644;&#35821;&#35328;&#24314;&#27169;&#65292;&#22312;&#36328;&#27169;&#24577;&#23545;&#40784;&#26041;&#38754;&#21462;&#24471;&#25104;&#26524;&#65292;&#24182;&#22312;&#30334;&#19975;&#32423;&#21035;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#33539;&#22260;&#20869;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.02131</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#36974;&#34109;&#35270;&#35273;&#21644;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Masked Vision and Language Modeling for Multi-modal Representation Learning. (arXiv:2208.02131v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.02131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32852;&#21512;&#36974;&#34109;&#35270;&#35273;&#21644;&#35821;&#35328;&#24314;&#27169;&#65292;&#22312;&#36328;&#27169;&#24577;&#23545;&#40784;&#26041;&#38754;&#21462;&#24471;&#25104;&#26524;&#65292;&#24182;&#22312;&#30334;&#19975;&#32423;&#21035;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#33539;&#22260;&#20869;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#65288;V + L&#65289;&#34920;&#31034;&#23398;&#20064;&#20013;&#20351;&#29992;&#36974;&#34109;&#20449;&#21495;&#24314;&#27169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24314;&#31435;&#32852;&#21512;&#36974;&#34109;&#35270;&#35273;&#21644;&#35821;&#35328;&#24314;&#27169;&#65292;&#20854;&#20013;&#19968;&#20010;&#27169;&#24577;&#30340;&#36974;&#34109;&#20449;&#21495;&#22312;&#21478;&#19968;&#20010;&#27169;&#24577;&#30340;&#24110;&#21161;&#19979;&#36827;&#34892;&#37325;&#24314;&#12290;&#36825;&#26159;&#30001;&#22270;&#20687;&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#30340;&#24615;&#36136;&#25152;&#39537;&#21160;&#30340;&#65292;&#22240;&#20026;&#22270;&#20687;&#21644;&#25991;&#26412;&#37117;&#20256;&#36798;&#20960;&#20046;&#30456;&#21516;&#30340;&#20449;&#24687;&#20294;&#20197;&#19981;&#21516;&#30340;&#26684;&#24335;&#21576;&#29616;&#12290;&#19968;&#20010;&#27169;&#24577;&#30340;&#36974;&#34109;&#20449;&#21495;&#37325;&#24314;&#20197;&#21478;&#19968;&#27169;&#24577;&#20026;&#26465;&#20214;&#20063;&#21487;&#20197;&#38544;&#24335;&#22320;&#23398;&#20064;&#35821;&#35328;&#26631;&#35760;&#21644;&#22270;&#20687;&#34917;&#19969;&#20043;&#38388;&#30340;&#36328;&#27169;&#24577;&#23545;&#40784;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;V + L&#20219;&#21153;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#36830;&#21516;&#24120;&#35265;&#30340;V + L&#23545;&#40784;&#25439;&#22833;&#65292;&#22312;&#30334;&#19975;&#32423;&#21035;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#33539;&#22260;&#20869;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#22330;&#26223;&#20013;&#65292;&#25105;&#20204;&#36229;&#36807;&#20102;&#20854;&#20182;&#31454;&#20105;&#23545;&#25163;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study how to use masked signal modeling in vision and language (V+L) representation learning. Instead of developing masked language modeling (MLM) and masked image modeling (MIM) independently, we propose to build joint masked vision and language modeling, where the masked signal of one modality is reconstructed with the help from another modality. This is motivated by the nature of image-text paired data that both of the image and the text convey almost the same information but in different formats. The masked signal reconstruction of one modality conditioned on another modality can also implicitly learn cross-modal alignment between language tokens and image patches. Our experiments on various V+L tasks show that the proposed method, along with common V+L alignment losses, achieves state-of-the-art performance in the regime of millions of pre-training data. Also, we outperforms the other competitors by a significant margin in limited data scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20132;&#20114;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#21453;&#39304;&#39057;&#29575;&#23545;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#23450;&#37327;&#21270;&#30740;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#27809;&#26377;&#21333;&#19968;&#30340;&#29702;&#24819;&#21453;&#39304;&#39057;&#29575;&#23384;&#22312;&#65292;&#24212;&#35813;&#26681;&#25454;&#20855;&#20307;&#30340;&#20219;&#21153;&#21644;&#26426;&#22120;&#20154;&#22797;&#26434;&#24615;&#36827;&#34892;&#35843;&#25972;&#12290;</title><link>http://arxiv.org/abs/2207.09845</link><description>&lt;p&gt;
&#20132;&#20114;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#21453;&#39304;&#39057;&#29575;&#23545;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#24433;&#21709;&#30340;&#23450;&#37327;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Quantifying the Effect of Feedback Frequency in Interactive Reinforcement Learning for Robotic Tasks. (arXiv:2207.09845v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.09845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20132;&#20114;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#21453;&#39304;&#39057;&#29575;&#23545;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#23450;&#37327;&#21270;&#30740;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#27809;&#26377;&#21333;&#19968;&#30340;&#29702;&#24819;&#21453;&#39304;&#39057;&#29575;&#23384;&#22312;&#65292;&#24212;&#35813;&#26681;&#25454;&#20855;&#20307;&#30340;&#20219;&#21153;&#21644;&#26426;&#22120;&#20154;&#22797;&#26434;&#24615;&#36827;&#34892;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#26426;&#22120;&#20154;&#25511;&#21046;&#20013;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#25104;&#21151;&#26696;&#20363;&#65292;&#20294;&#19968;&#20010;&#37325;&#35201;&#30340;&#25345;&#20037;&#24615;&#38382;&#39064;&#26159;&#25968;&#25454;&#25928;&#29575;&#38750;&#24120;&#20302;&#12290;&#20132;&#20114;&#21453;&#39304;&#26159;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#22823;&#22823;&#21152;&#36895;RL&#12290;&#22240;&#27492;&#65292;&#26377;&#22823;&#37327;&#19981;&#21516;&#30340;&#31574;&#30053;&#65292;&#28982;&#32780;&#36825;&#20123;&#31574;&#30053;&#20027;&#35201;&#26159;&#22312;&#31163;&#25955;&#30340;&#32593;&#26684;&#19990;&#30028;&#21644;&#23567;&#35268;&#27169;&#30340;&#26368;&#20248;&#25511;&#21046;&#22330;&#26223;&#20013;&#27979;&#35797;&#30340;&#12290;&#22312;&#25991;&#29486;&#20013;&#65292;&#23545;&#20110;&#21738;&#31181;&#21453;&#39304;&#39057;&#29575;&#26368;&#20248;&#25110;&#22312;&#20160;&#20040;&#26102;&#20505;&#21453;&#39304;&#26368;&#26377;&#30410;&#24182;&#27809;&#26377;&#20849;&#35782;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#24046;&#24322;&#65292;&#25105;&#20204;&#22312;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#20998;&#31163;&#24182;&#37327;&#21270;&#20102;&#21453;&#39304;&#39057;&#29575;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#28085;&#30422;&#20102;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#26426;&#26800;&#33218;&#30340;&#36870;&#36816;&#21160;&#23398;&#23398;&#20064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#34920;&#38754;&#19978;&#30683;&#30462;&#30340;&#29616;&#35937;&#22312;&#19981;&#21516;&#30340;&#22797;&#26434;&#24230;&#27700;&#24179;&#19978;&#20986;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#27809;&#26377;&#21333;&#19968;&#30340;&#29702;&#24819;&#21453;&#39304;&#39057;&#29575;&#23384;&#22312;&#12290;&#21453;&#39304;&#39057;&#29575;&#24212;&#35813;&#26681;&#25454;&#20855;&#20307;&#30340;&#20219;&#21153;&#21644;&#26426;&#22120;&#20154;&#22797;&#26434;&#24615;&#36827;&#34892;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has become widely adopted in robot control. Despite many successes, one major persisting problem can be very low data efficiency. One solution is interactive feedback, which has been shown to speed up RL considerably. As a result, there is an abundance of different strategies, which are, however, primarily tested on discrete grid-world and small scale optimal control scenarios. In the literature, there is no consensus about which feedback frequency is optimal or at which time the feedback is most beneficial. To resolve these discrepancies we isolate and quantify the effect of feedback frequency in robotic tasks with continuous state and action spaces. The experiments encompass inverse kinematics learning for robotic manipulator arms of different complexity. We show that seemingly contradictory reported phenomena occur at different complexity levels. Furthermore, our results suggest that no single ideal feedback frequency exists. Rather that feedback frequenc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;GAMI-Tree&#65292;&#20351;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#26641;&#20197;&#21450;&#26032;&#30340;&#20132;&#20114;&#36807;&#28388;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25311;&#21512;&#24213;&#23618;&#20132;&#20114;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#26356;&#39640;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2207.06950</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#26641;&#21644;&#25552;&#21319;&#26041;&#27861;&#25311;&#21512;&#20302;&#38454;&#20989;&#25968;ANOVA&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Using Model-Based Trees with Boosting to Fit Low-Order Functional ANOVA Models. (arXiv:2207.06950v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.06950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;GAMI-Tree&#65292;&#20351;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#26641;&#20197;&#21450;&#26032;&#30340;&#20132;&#20114;&#36807;&#28388;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25311;&#21512;&#24213;&#23618;&#20132;&#20114;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#26356;&#39640;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#38454;&#20989;&#25968;ANOVA&#27169;&#22411;&#24050;&#32463;&#34987;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#37325;&#26032;&#21457;&#29616;&#65292;&#24182;&#31216;&#20043;&#20026;&#20869;&#22312;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;GAMI-Tree&#65292;&#31867;&#20284;&#20110;EBM&#65292;&#20294;&#20855;&#26377;&#19968;&#20123;&#36235;&#21521;&#26356;&#22909;&#24615;&#33021;&#30340;&#29305;&#24615;&#12290;&#25105;&#20204;&#37319;&#29992;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#26641;&#65292;&#24182;&#34701;&#20837;&#19968;&#31181;&#26032;&#30340;&#20132;&#20114;&#36807;&#28388;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#23545;&#24213;&#23618;&#20132;&#20114;&#30340;&#25429;&#25417;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#36845;&#20195;&#35757;&#32451;&#26041;&#27861;&#25910;&#25947;&#20110;&#20855;&#26377;&#26356;&#22909;&#39044;&#27979;&#24615;&#33021;&#30340;&#27169;&#22411;&#65292;&#24182;&#30830;&#20445;&#30456;&#20114;&#20316;&#29992;&#22312;&#20998;&#23618;&#24847;&#20041;&#19978;&#27491;&#20132;&#20110;&#20027;&#25928;&#24212;&#12290;&#35813;&#31639;&#27861;&#19981;&#38656;&#35201;&#24191;&#27867;&#30340;&#35843;&#25972;&#65292;&#24182;&#19988;&#23454;&#29616;&#24555;&#36895;&#39640;&#25928;&#12290;&#25105;&#20204;&#20351;&#29992;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-order functional ANOVA (fANOVA) models have been rediscovered in the machine learning (ML) community under the guise of inherently interpretable machine learning. Explainable Boosting Machines or EBM (Lou et al. 2013) and GAMI-Net (Yang et al. 2021) are two recently proposed ML algorithms for fitting functional main effects and second-order interactions. We propose a new algorithm, called GAMI-Tree, that is similar to EBM, but has a number of features that lead to better performance. It uses model-based trees as base learners and incorporates a new interaction filtering method that is better at capturing the underlying interactions. In addition, our iterative training method converges to a model with better predictive performance, and the embedded purification ensures that interactions are hierarchically orthogonal to main effects. The algorithm does not need extensive tuning, and our implementation is fast and efficient. We use simulated and real datasets to compare the performanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Betty&#30340;&#33258;&#21160;&#24494;&#20998;&#24211;&#65292;&#21487;&#29992;&#20110;&#22823;&#35268;&#27169;&#30340;&#26799;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#65292;&#22312;&#24191;&#27867;&#30340;&#22810;&#23618;&#27425;&#20248;&#21270;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2207.02849</link><description>&lt;p&gt;
Betty: &#19968;&#20010;&#29992;&#20110;&#22810;&#23618;&#27425;&#20248;&#21270;&#30340;&#33258;&#21160;&#24494;&#20998;&#24211;
&lt;/p&gt;
&lt;p&gt;
Betty: An Automatic Differentiation Library for Multilevel Optimization. (arXiv:2207.02849v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.02849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Betty&#30340;&#33258;&#21160;&#24494;&#20998;&#24211;&#65292;&#21487;&#29992;&#20110;&#22823;&#35268;&#27169;&#30340;&#26799;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#65292;&#22312;&#24191;&#27867;&#30340;&#22810;&#23618;&#27425;&#20248;&#21270;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#30340;&#22810;&#23618;&#27425;&#20248;&#21270;(MLO)&#24050;&#25104;&#20026;&#30740;&#31350;&#20247;&#22810;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#36229;&#21442;&#25968;&#20248;&#21270;&#12289;&#20803;&#23398;&#20064;&#12289;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#12290;&#28982;&#32780;&#65292;MLO&#20013;&#30340;&#26799;&#24230;&#65292;&#26159;&#36890;&#36807;&#38142;&#24335;&#27861;&#21017;&#32452;&#25104;&#26368;&#20339;&#21709;&#24212;Jacobi&#30697;&#38453;&#32780;&#33719;&#24471;&#30340;&#65292;&#20855;&#26377;&#35745;&#31639;&#21644;&#20869;&#23384;&#23494;&#38598;&#31561;&#19981;&#21033;&#22240;&#32032;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38754;&#21521;&#22823;&#35268;&#27169;MLO&#30340;&#36719;&#20214;&#24211;Betty&#65292;&#20174;&#32780;&#21021;&#27493;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;&#22312;&#20854;&#26680;&#24515;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;MLO&#25968;&#25454;&#27969;&#22270;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;(1)&#20026;MLO&#24320;&#21457;&#39640;&#25928;&#30340;&#33258;&#21160;&#24494;&#20998;&#65292;&#23558;&#35745;&#31639;&#22797;&#26434;&#24230;&#20174;O(d^3)&#38477;&#33267;O(d^2)&#65292;(2)&#34701;&#20837;&#31995;&#32479;&#25903;&#25345;&#65292;&#20363;&#22914;&#28151;&#21512;&#31934;&#24230;&#21644;&#25968;&#25454;&#24182;&#34892;&#35757;&#32451;&#65292;&#20197;&#23454;&#29616;&#21487;&#20280;&#32553;&#24615;&#65292;(3)&#20415;&#20110;&#23454;&#29616;&#20219;&#24847;&#22797;&#26434;&#24230;&#30340;MLO&#31243;&#24207;&#65292;&#21516;&#26102;&#20801;&#35768;&#22810;&#26679;&#21270;&#30340;&#31639;&#27861;&#21644;&#31995;&#32479;&#35774;&#35745;&#36873;&#25321;&#30340;&#27169;&#22359;&#21270;&#25509;&#21475;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;Betty&#22312;&#24191;&#27867;&#30340;MLO&#20219;&#21153;&#20013;&#37117;&#23454;&#29616;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#36229;&#21442;&#25968;&#20248;&#21270;&#12289;&#20803;&#23398;&#20064;&#21644;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient-based multilevel optimization (MLO) has gained attention as a framework for studying numerous problems, ranging from hyperparameter optimization and meta-learning to neural architecture search and reinforcement learning. However, gradients in MLO, which are obtained by composing best-response Jacobians via the chain rule, are notoriously difficult to implement and memory/compute intensive. We take an initial step towards closing this gap by introducing Betty, a software library for large-scale MLO. At its core, we devise a novel dataflow graph for MLO, which allows us to (1) develop efficient automatic differentiation for MLO that reduces the computational complexity from O(d^3) to O(d^2), (2) incorporate systems support such as mixed-precision and data-parallel training for scalability, and (3) facilitate implementation of MLO programs of arbitrary complexity while allowing a modular interface for diverse algorithmic and systems design choices. We empirically demonstrate that
&lt;/p&gt;</description></item><item><title>NovelCraft&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#24320;&#25918;&#19990;&#30028;&#20013;&#26032;&#39062;&#24615;&#26816;&#27979;&#19982;&#21457;&#29616;&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;&#22312;&#22797;&#26434;&#30340;&#22330;&#26223;&#20013;&#25554;&#20837;&#26032;&#39062;&#29289;&#20307;&#30340;&#26816;&#27979;&#38656;&#35201;&#26356;&#22909;&#30340;&#22522;&#20934;&#65292;&#24182;&#21457;&#29616;&#20102;&#25511;&#21046;&#20551;&#38451;&#24615;&#26102;&#26356;&#31616;&#21333;&#30340;&#26041;&#27861;&#21487;&#33021;&#27604;&#22797;&#26434;&#30340;&#26041;&#27861;&#26356;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2206.11736</link><description>&lt;p&gt;
NovelCraft&#65306;&#24320;&#25918;&#19990;&#30028;&#20013;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#21644;&#21457;&#29616;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
NovelCraft: A Dataset for Novelty Detection and Discovery in Open Worlds. (arXiv:2206.11736v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.11736
&lt;/p&gt;
&lt;p&gt;
NovelCraft&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#24320;&#25918;&#19990;&#30028;&#20013;&#26032;&#39062;&#24615;&#26816;&#27979;&#19982;&#21457;&#29616;&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;&#22312;&#22797;&#26434;&#30340;&#22330;&#26223;&#20013;&#25554;&#20837;&#26032;&#39062;&#29289;&#20307;&#30340;&#26816;&#27979;&#38656;&#35201;&#26356;&#22909;&#30340;&#22522;&#20934;&#65292;&#24182;&#21457;&#29616;&#20102;&#25511;&#21046;&#20551;&#38451;&#24615;&#26102;&#26356;&#31616;&#21333;&#30340;&#26041;&#27861;&#21487;&#33021;&#27604;&#22797;&#26434;&#30340;&#26041;&#27861;&#26356;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35753;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#25104;&#21151;&#25191;&#34892;&#20219;&#21153;&#65292;&#24517;&#39035;&#33021;&#22815;&#26816;&#27979;&#21644;&#36866;&#24212;&#26032;&#39062;&#24615;&#12290;&#28982;&#32780;&#65292;&#35270;&#35273;&#26032;&#39062;&#24615;&#26816;&#27979;&#30740;&#31350;&#36890;&#24120;&#21482;&#35780;&#20272;&#26088;&#22312;&#36827;&#34892;&#23545;&#35937;&#20998;&#31867;&#30340;&#37325;&#22797;&#21033;&#29992;&#25968;&#25454;&#38598;&#65288;&#22914;CIFAR-10&#65289;&#65292;&#20854;&#20013;&#22270;&#20687;&#32858;&#28966;&#20110;&#19968;&#20010;&#26126;&#26174;&#12289;&#23621;&#20013;&#30340;&#23545;&#35937;&#12290;&#38656;&#35201;&#26032;&#30340;&#22522;&#20934;&#26469;&#20195;&#34920;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#23548;&#33322;&#22797;&#26434;&#22330;&#26223;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26032;NovelCraft&#25968;&#25454;&#38598;&#21253;&#21547;&#23436;&#25104;&#20462;&#25913;&#21518;&#30340;Minecraft&#29615;&#22659;&#20013;&#30340;&#36339;&#36339;&#29699;&#35013;&#37197;&#20219;&#21153;&#30340;&#20195;&#29702;&#25152;&#30475;&#21040;&#30340;&#22270;&#20687;&#21644;&#31526;&#21495;&#19990;&#30028;&#29366;&#24577;&#30340;&#22810;&#27169;&#24335;&#24773;&#33410;&#25968;&#25454;&#12290;&#22312;&#26576;&#20123;&#24773;&#33410;&#20013;&#65292;&#25105;&#20204;&#22312;&#22797;&#26434;&#30340;3D&#22330;&#26223;&#20013;&#25554;&#20837;&#26032;&#39062;&#29289;&#20307;&#65292;&#36825;&#20123;&#29289;&#20307;&#21487;&#33021;&#24433;&#21709;&#28216;&#25103;&#29609;&#27861;&#24182;&#20986;&#29616;&#22312;&#21508;&#31181;&#22823;&#23567;&#21644;&#20301;&#32622;&#20013;&#12290;&#25105;&#20204;&#30340;&#35270;&#35273;&#26032;&#39062;&#24615;&#26816;&#27979;&#22522;&#20934;&#21457;&#29616;&#65292;&#25511;&#21046;&#20551;&#38451;&#24615;&#26102;&#65292;&#26368;&#22909;&#30340;&#38754;&#31215;&#19979;&#26354;&#32447;&#24230;&#37327;&#30340;&#26041;&#27861;&#21487;&#33021;&#20250;&#34987;&#26356;&#31616;&#21333;&#30340;&#26367;&#20195;&#26041;&#27861;&#36229;&#36807;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order for artificial agents to successfully perform tasks in changing environments, they must be able to both detect and adapt to novelty. However, visual novelty detection research often only evaluates on repurposed datasets such as CIFAR-10 originally intended for object classification, where images focus on one distinct, well-centered object. New benchmarks are needed to represent the challenges of navigating the complex scenes of an open world. Our new NovelCraft dataset contains multimodal episodic data of the images and symbolic world-states seen by an agent completing a pogo stick assembly task within a modified Minecraft environment. In some episodes, we insert novel objects within the complex 3D scene that may impact gameplay and appear in a variety of sizes and positions. Our visual novelty detection benchmark finds that methods that rank best on popular area-under-the-curve metrics may be outperformed by simpler alternatives when controlling false positives matters most. 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#38454;&#27573;&#28176;&#36827;&#23398;&#20064;&#30340;&#26041;&#27861;&#21644;&#19968;&#31181;&#32806;&#21512;&#35843;&#33410;&#19981;&#24179;&#34913;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#19981;&#24179;&#34913;&#25968;&#25454;&#20998;&#31867;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#29305;&#21035;&#36866;&#29992;&#20110;&#20855;&#26377;&#22833;&#34913;&#25110;&#26679;&#26412;&#36739;&#23569;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2205.12117</link><description>&lt;p&gt;
&#20351;&#29992;&#32806;&#21512;&#35843;&#33410;&#19981;&#24179;&#34913;&#25439;&#22833;&#30340;&#20998;&#38454;&#27573;&#28176;&#36827;&#23398;&#20064;&#26469;&#35299;&#20915;&#19981;&#24179;&#34913;&#25968;&#25454;&#20998;&#31867;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Phased Progressive Learning with Coupling-Regulation-Imbalance Loss for Imbalanced Data Classification. (arXiv:2205.12117v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12117
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#38454;&#27573;&#28176;&#36827;&#23398;&#20064;&#30340;&#26041;&#27861;&#21644;&#19968;&#31181;&#32806;&#21512;&#35843;&#33410;&#19981;&#24179;&#34913;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#19981;&#24179;&#34913;&#25968;&#25454;&#20998;&#31867;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#29305;&#21035;&#36866;&#29992;&#20110;&#20855;&#26377;&#22833;&#34913;&#25110;&#26679;&#26412;&#36739;&#23569;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#38754;&#20020;&#25968;&#37327;&#19981;&#24179;&#34913;&#21644;&#20998;&#31867;&#22256;&#38590;&#30340;&#25968;&#25454;&#38598;&#26102;&#65292;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#34920;&#29616;&#19981;&#20339;&#12290;&#23613;&#31649;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#25968;&#25454;&#38598;&#20559;&#35265;&#25110;&#22495;&#20559;&#31227;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#38454;&#27573;&#36880;&#28176;&#23558;&#37325;&#28857;&#20174;&#34920;&#31034;&#23398;&#20064;&#36716;&#21521;&#35757;&#32451;&#19978;&#23618;&#20998;&#31867;&#22120;&#30340;&#23398;&#20064;&#35745;&#21010;&#12290;&#23545;&#20110;&#20855;&#26377;&#36739;&#22823;&#22833;&#34913;&#25110;&#26679;&#26412;&#36739;&#23569;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#31181;&#26041;&#27861;&#29305;&#21035;&#26377;&#30410;&#12290;&#21478;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32806;&#21512;&#35843;&#33410;&#19981;&#24179;&#34913;&#25439;&#22833;&#20989;&#25968;&#65292;&#23427;&#23558;&#19977;&#20010;&#37096;&#20998;&#32452;&#21512;&#22312;&#19968;&#36215;&#65306;&#26657;&#27491;&#39033;&#65292;Focal &#25439;&#22833;&#21644; LDAM &#25439;&#22833;&#12290;&#36825;&#31181;&#25439;&#22833;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#25968;&#37327;&#19981;&#24179;&#34913;&#21644;&#31163;&#32676;&#20540;&#38382;&#39064;&#65292;&#21516;&#26102;&#35843;&#33410;&#20851;&#27880;&#20855;&#26377;&#19981;&#21516;&#20998;&#31867;&#22256;&#38590;&#24230;&#30340;&#26679;&#26412;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324; Imbalanced CIFAR10&#12289;Imbalanced CIFAR100&#12289;ImageNet-LT &#21644; iNaturalist&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep convolutional neural networks often perform poorly when faced with datasets that suffer from quantity imbalances and classification difficulties. Despite advances in the field, existing two-stage approaches still exhibit dataset bias or domain shift. To counter this, a phased progressive learning schedule has been proposed that gradually shifts the emphasis from representation learning to training the upper classifier. This approach is particularly beneficial for datasets with larger imbalances or fewer samples. Another new method a coupling-regulation-imbalance loss function is proposed, which combines three parts: a correction term, Focal loss, and LDAM loss. This loss is effective in addressing quantity imbalances and outliers, while regulating the focus of attention on samples with varying classification difficulties. These approaches have yielded satisfactory results on several benchmark datasets, including Imbalanced CIFAR10, Imbalanced CIFAR100, ImageNet-LT, and iNaturalist
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20462;&#25913;&#36807;&#30340;&#39532;&#36420;&#34444;&#20808;&#39564;&#30340;&#22810;&#35270;&#35282;&#28508;&#21464;&#37327;&#27169;&#22411;MuVI&#65292;&#29992;&#20110;&#24314;&#27169;&#32467;&#26500;&#31232;&#30095;&#24615;&#12290;&#23427;&#33021;&#22815;&#32435;&#20837;&#26377;&#38480;&#19988;&#22122;&#22768;&#30340;&#39046;&#22495;&#30693;&#35782;&#65292;&#20197;&#20869;&#22312;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#20998;&#26512;&#22810;&#35270;&#35282;&#25968;&#25454;&#65292;&#20248;&#20110;&#29616;&#26377;&#32467;&#26500;&#31232;&#30095;&#24615;&#24314;&#27169;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2204.06242</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#28508;&#21464;&#37327;&#27169;&#22411;&#20013;&#30340;&#39046;&#22495;&#30693;&#35782;&#32534;&#30721;:&#24102;&#32467;&#26500;&#31232;&#30095;&#36125;&#21494;&#26031;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Encoding Domain Knowledge in Multi-view Latent Variable Models: A Bayesian Approach with Structured Sparsity. (arXiv:2204.06242v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.06242
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20462;&#25913;&#36807;&#30340;&#39532;&#36420;&#34444;&#20808;&#39564;&#30340;&#22810;&#35270;&#35282;&#28508;&#21464;&#37327;&#27169;&#22411;MuVI&#65292;&#29992;&#20110;&#24314;&#27169;&#32467;&#26500;&#31232;&#30095;&#24615;&#12290;&#23427;&#33021;&#22815;&#32435;&#20837;&#26377;&#38480;&#19988;&#22122;&#22768;&#30340;&#39046;&#22495;&#30693;&#35782;&#65292;&#20197;&#20869;&#22312;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#20998;&#26512;&#22810;&#35270;&#35282;&#25968;&#25454;&#65292;&#20248;&#20110;&#29616;&#26377;&#32467;&#26500;&#31232;&#30095;&#24615;&#24314;&#27169;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#31995;&#32479;&#19981;&#20165;&#26377;&#26469;&#33258;&#21333;&#20010;&#25968;&#25454;&#28304;&#30340;&#25968;&#25454;&#65292;&#36824;&#26377;&#26469;&#33258;&#22810;&#20010;&#25968;&#25454;&#35270;&#35282;&#30340;&#25968;&#25454;&#12290;&#20363;&#22914;&#65292;&#22312;&#22522;&#22240;&#32452;&#21307;&#23398;&#20013;&#65292;&#24739;&#32773;&#21487;&#20197;&#36890;&#36807;&#26469;&#33258;&#19981;&#21516;&#20998;&#23376;&#23618;&#38754;&#30340;&#25968;&#25454;&#36827;&#34892;&#25551;&#36848;&#12290;&#21033;&#29992;&#20855;&#26377;&#32467;&#26500;&#31232;&#30095;&#24615;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#26159;&#25581;&#31034;&#25968;&#25454;&#35270;&#35282;&#20869;&#21644;&#36328;&#35270;&#35282;&#21464;&#21270;&#30340;&#24120;&#29992;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#21487;&#35299;&#37322;&#24615;&#36739;&#24046;&#65292;&#38656;&#35201;&#19987;&#23478;&#30452;&#25509;&#26816;&#26597;&#21644;&#35299;&#37322;&#27599;&#20010;&#35201;&#32032;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MuVI&#65292;&#19968;&#31181;&#22522;&#20110;&#20462;&#25913;&#36807;&#30340;&#39532;&#36420;&#34444;&#20808;&#39564;&#30340;&#26032;&#22411;&#22810;&#35270;&#35282;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#29992;&#20110;&#24314;&#27169;&#32467;&#26500;&#31232;&#30095;&#24615;&#12290;&#36825;&#26377;&#21161;&#20110;&#23545;&#26377;&#38480;&#30340;&#21644;&#22122;&#22768;&#39046;&#22495;&#30693;&#35782;&#36827;&#34892;&#32435;&#20837;&#65292;&#20174;&#32780;&#20197;&#20869;&#22312;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#20998;&#26512;&#22810;&#35270;&#35282;&#25968;&#25454;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#37325;&#24314;&#35823;&#24046;&#21644;&#31934;&#30830;&#24230;/&#21484;&#22238;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#32467;&#26500;&#31232;&#30095;&#24615;&#24314;&#27169;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#31283;&#20581;&#22320;&#25972;&#21512;&#22122;&#22768;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world systems are described not only by data from a single source but via multiple data views. In genomic medicine, for instance, patients can be characterized by data from different molecular layers. Latent variable models with structured sparsity are a commonly used tool for disentangling variation within and across data views. However, their interpretability is cumbersome since it requires a direct inspection and interpretation of each factor from domain experts. Here, we propose MuVI, a novel multi-view latent variable model based on a modified horseshoe prior for modeling structured sparsity. This facilitates the incorporation of limited and noisy domain knowledge, thereby allowing for an analysis of multi-view data in an inherently explainable manner. We demonstrate that our model (i) outperforms state-of-the-art approaches for modeling structured sparsity in terms of the reconstruction error and the precision/recall, (ii) robustly integrates noisy domain expertise in t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#20855;&#26377;&#24377;&#24615;&#30340;&#26080;&#32447;&#30005;&#36164;&#28304;&#31649;&#29702;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#39640;&#32858;&#21512;&#36895;&#29575;&#24182;&#30830;&#20445;&#25152;&#26377;&#29992;&#25143;&#30340;&#20844;&#24179;&#24615;&#65292;&#24182;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#32622;&#25442;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26550;&#26500;&#22522;&#20110;&#30636;&#26102;&#20449;&#36947;&#26465;&#20214;&#25512;&#23548;&#20986;&#30340;&#22270;&#24418;&#25299;&#25169;&#26469;&#21442;&#25968;&#21270;RRM&#31574;&#30053;</title><link>http://arxiv.org/abs/2203.11012</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20855;&#26377;&#24377;&#24615;&#30340;&#26080;&#32447;&#30005;&#36164;&#28304;&#31649;&#29702;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning Resilient Radio Resource Management Policies with Graph Neural Networks. (arXiv:2203.11012v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.11012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#20855;&#26377;&#24377;&#24615;&#30340;&#26080;&#32447;&#30005;&#36164;&#28304;&#31649;&#29702;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#39640;&#32858;&#21512;&#36895;&#29575;&#24182;&#30830;&#20445;&#25152;&#26377;&#29992;&#25143;&#30340;&#20844;&#24179;&#24615;&#65292;&#24182;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#32622;&#25442;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26550;&#26500;&#22522;&#20110;&#30636;&#26102;&#20449;&#36947;&#26465;&#20214;&#25512;&#23548;&#20986;&#30340;&#22270;&#24418;&#25299;&#25169;&#26469;&#21442;&#25968;&#21270;RRM&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#21253;&#21547;&#22810;&#20010;&#25509;&#20837;&#28857;&#21644;&#19968;&#32452;&#29992;&#25143;&#35774;&#22791;&#30340;&#26080;&#32447;&#24178;&#25200;&#32593;&#32476;&#20013;&#65292;&#23454;&#29616;&#29992;&#25143;&#36873;&#25321;&#21644;&#21151;&#29575;&#25511;&#21046;&#65292;&#20197;&#23454;&#29616;&#39640;&#32858;&#21512;&#36895;&#29575;&#24182;&#30830;&#20445;&#25152;&#26377;&#29992;&#25143;&#30340;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#26494;&#24347;&#21464;&#37327;&#65292;&#23558;&#24377;&#24615;&#26080;&#32447;&#30005;&#36164;&#28304;&#31649;&#29702;&#65288;RRM&#65289;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#19982;&#36866;&#24212;&#24213;&#23618;&#32593;&#32476;&#26465;&#20214;&#30340;&#27599;&#20010;&#29992;&#25143;&#26368;&#20302;&#23481;&#37327;&#32422;&#26463;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#22312;Lagrangian&#21452;&#37325;&#22495;&#20013;&#37325;&#26032;&#23450;&#20041;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#26377;&#38480;&#30340;&#21442;&#25968;&#38598;&#26469;&#21442;&#25968;&#21270;RRM&#31574;&#30053;&#65292;&#36890;&#36807;&#19968;&#20010;&#32463;&#36807;&#35777;&#23454;&#30340;&#23567;&#20559;&#24046;&#30340;&#26080;&#30417;&#30563;&#21407;&#22987;-&#21452;&#37325;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#32622;&#25442;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26550;&#26500;&#26469;&#22522;&#20110;&#30636;&#26102;&#20449;&#36947;&#26465;&#20214;&#25512;&#23548;&#20986;&#30340;&#22270;&#24418;&#25299;&#25169;&#21442;&#25968;&#21270;RRM&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problems of user selection and power control in wireless interference networks, comprising multiple access points (APs) communicating with a group of user equipment devices (UEs) over a shared wireless medium. To achieve a high aggregate rate, while ensuring fairness across all users, we formulate a resilient radio resource management (RRM) policy optimization problem with per-user minimum-capacity constraints that adapt to the underlying network conditions via learnable slack variables. We reformulate the problem in the Lagrangian dual domain, and show that we can parameterize the RRM policies using a finite set of parameters, which can be trained alongside the slack and dual variables via an unsupervised primal-dual approach thanks to a provably small duality gap. We use a scalable and permutation-equivariant graph neural network (GNN) architecture to parameterize the RRM policies based on a graph topology derived from the instantaneous channel conditions. Through exp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#31163;&#25955;&#20313;&#24358;&#21464;&#25442;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#28857;&#31215;&#27880;&#24847;&#21147;&#35745;&#31639;&#33499;&#21051;&#30340;&#20869;&#23384;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#38480;&#21046;&#65292;&#20855;&#26377;&#26356;&#23567;&#30340;&#20869;&#23384;&#21344;&#29992;&#21644;&#26356;&#30701;&#30340;&#25512;&#29702;&#26102;&#38388;&#65292;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2203.01178</link><description>&lt;p&gt;
DCT-Former: &#37319;&#29992;&#31163;&#25955;&#20313;&#24358;&#21464;&#25442;&#30340;&#39640;&#25928;&#33258;&#27880;&#24847;&#21147;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DCT-Former: Efficient Self-Attention with Discrete Cosine Transform. (arXiv:2203.01178v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.01178
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#31163;&#25955;&#20313;&#24358;&#21464;&#25442;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#28857;&#31215;&#27880;&#24847;&#21147;&#35745;&#31639;&#33499;&#21051;&#30340;&#20869;&#23384;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#38480;&#21046;&#65292;&#20855;&#26377;&#26356;&#23567;&#30340;&#20869;&#23384;&#21344;&#29992;&#21644;&#26356;&#30701;&#30340;&#25512;&#29702;&#26102;&#38388;&#65292;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#27880;&#24847;&#21147;&#27169;&#22411;&#26159;&#30446;&#21069;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#21344;&#25454;&#32479;&#27835;&#22320;&#20301;&#30340;&#27169;&#22411;&#32467;&#26500;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28857;&#31215;&#27880;&#24847;&#21147;&#30340;&#35745;&#31639;&#38656;&#27714;&#25104;&#20493;&#22686;&#21152;&#65292;&#26102;&#38388;&#22797;&#26434;&#24230;&#39640;&#36798;O(n^2)&#65292;&#36825;&#22312;&#24314;&#27169;&#38271;&#24207;&#21015;&#26102;&#20855;&#26377;&#22825;&#28982;&#30340;&#38480;&#21046;&#12290;&#27492;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#20197;&#31163;&#25955;&#20313;&#24358;&#21464;&#25442;&#20026;&#22522;&#30784;&#30340;&#33258;&#27880;&#24847;&#21147;&#36817;&#20284;&#26041;&#27861;&#65292;&#21462;&#24471;&#26356;&#23567;&#30340;&#20869;&#23384;&#21344;&#29992;&#21644;&#26356;&#30701;&#30340;&#25512;&#29702;&#26102;&#38388;&#65292;&#23454;&#39564;&#39564;&#35777;&#34920;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#36739;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since their introduction the Trasformer architectures emerged as the dominating architectures for both natural language processing and, more recently, computer vision applications. An intrinsic limitation of this family of "fully-attentive" architectures arises from the computation of the dot-product attention, which grows both in memory consumption and number of operations as $O(n^2)$ where $n$ stands for the input sequence length, thus limiting the applications that require modeling very long sequences. Several approaches have been proposed so far in the literature to mitigate this issue, with varying degrees of success. Our idea takes inspiration from the world of lossy data compression (such as the JPEG algorithm) to derive an approximation of the attention module by leveraging the properties of the Discrete Cosine Transform. An extensive section of experiments shows that our method takes up less memory for the same performance, while also drastically reducing inference time. This 
&lt;/p&gt;</description></item><item><title>Weisfeiler-Leman&#31639;&#27861;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22788;&#29702;&#22270;&#21644;&#20851;&#31995;&#25968;&#25454;&#12290;&#26412;&#25991;&#20840;&#38754;&#20171;&#32461;&#20102;&#35813;&#31639;&#27861;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#29702;&#35770;&#32972;&#26223;&#12289;&#25193;&#23637;&#12289;&#19982;&#31561;&#21464;&#31070;&#32463;&#32593;&#26684;&#30340;&#32852;&#31995;&#12289;&#24182;&#21015;&#20986;&#20102;&#24403;&#21069;&#24212;&#29992;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2112.09992</link><description>&lt;p&gt;
Weisfeiler&#21644;Leman&#26469;&#20570;&#26426;&#22120;&#23398;&#20064;&#20102;&#65306;&#30446;&#21069;&#30340;&#30740;&#31350;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weisfeiler and Leman go Machine Learning: The Story so far. (arXiv:2112.09992v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.09992
&lt;/p&gt;
&lt;p&gt;
Weisfeiler-Leman&#31639;&#27861;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22788;&#29702;&#22270;&#21644;&#20851;&#31995;&#25968;&#25454;&#12290;&#26412;&#25991;&#20840;&#38754;&#20171;&#32461;&#20102;&#35813;&#31639;&#27861;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#29702;&#35770;&#32972;&#26223;&#12289;&#25193;&#23637;&#12289;&#19982;&#31561;&#21464;&#31070;&#32463;&#32593;&#26684;&#30340;&#32852;&#31995;&#12289;&#24182;&#21015;&#20986;&#20102;&#24403;&#21069;&#24212;&#29992;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;Weisfeiler-Leman&#31639;&#27861;&#30340;&#31639;&#27861;&#21644;&#31070;&#32463;&#26550;&#26500;&#24050;&#25104;&#20026;&#22788;&#29702;&#22270;&#21644;&#20851;&#31995;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#26412;&#25991;&#20840;&#38754;&#20171;&#32461;&#31639;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#20351;&#29992;&#24773;&#20917;&#65292;&#37325;&#28857;&#20851;&#27880;&#30417;&#30563;&#23398;&#20064;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#29702;&#35770;&#32972;&#26223;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20854;&#29992;&#20110;&#30417;&#30563;&#22270;&#24418;&#21644;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#65292;&#35752;&#35770;&#20102;&#26368;&#36817;&#30340;&#25193;&#23637;&#65292;&#24182;&#27010;&#36848;&#20102;&#31639;&#27861;&#19982;&#65288;&#32622;&#25442;&#65289;&#31561;&#21464;&#31070;&#32463;&#32593;&#26684;&#30340;&#32852;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#27010;&#36848;&#20102;&#24403;&#21069;&#30340;&#24212;&#29992;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#20197;&#21050;&#28608;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, algorithms and neural architectures based on the Weisfeiler-Leman algorithm, a well-known heuristic for the graph isomorphism problem, have emerged as a powerful tool for machine learning with graphs and relational data. Here, we give a comprehensive overview of the algorithm's use in a machine-learning setting, focusing on the supervised regime. We discuss the theoretical background, show how to use it for supervised graph and node representation learning, discuss recent extensions, and outline the algorithm's connection to (permutation-)equivariant neural architectures. Moreover, we give an overview of current applications and future directions to stimulate further research.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#32422;&#26463;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21033;&#26222;&#24076;&#33576;&#24120;&#25968;&#65292;&#24182;&#20351;&#29992;&#21333;&#35843;&#21097;&#20313;&#36830;&#25509;&#20351;&#27169;&#22411;&#30340;&#26576;&#20123;&#36755;&#20837;&#21333;&#35843;&#65292;&#36866;&#29992;&#20110;&#38656;&#35201;&#39046;&#22495;&#30693;&#35782;&#25351;&#23548;&#20381;&#36182;&#24615;&#30340;&#22330;&#26223;&#65292;&#22914;&#31639;&#27861;&#20844;&#24179;&#24615;&#35201;&#27714;&#21450;&#29289;&#29702;&#23398;&#20013;&#30340;&#27425;&#21407;&#23376;&#31890;&#23376;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2112.00038</link><description>&lt;p&gt;
&#24378;&#20581;&#19988;&#21487;&#35777;&#26126;&#21333;&#35843;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Robust and Provably Monotonic Networks. (arXiv:2112.00038v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.00038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#32422;&#26463;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21033;&#26222;&#24076;&#33576;&#24120;&#25968;&#65292;&#24182;&#20351;&#29992;&#21333;&#35843;&#21097;&#20313;&#36830;&#25509;&#20351;&#27169;&#22411;&#30340;&#26576;&#20123;&#36755;&#20837;&#21333;&#35843;&#65292;&#36866;&#29992;&#20110;&#38656;&#35201;&#39046;&#22495;&#30693;&#35782;&#25351;&#23548;&#20381;&#36182;&#24615;&#30340;&#22330;&#26223;&#65292;&#22914;&#31639;&#27861;&#20844;&#24179;&#24615;&#35201;&#27714;&#21450;&#29289;&#29702;&#23398;&#20013;&#30340;&#27425;&#21407;&#23376;&#31890;&#23376;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#34920;&#31034;&#36755;&#20837;&#21644;&#36755;&#20986;&#31354;&#38388;&#20043;&#38388;&#26144;&#23556;&#30340;&#21033;&#26222;&#24076;&#33576;&#24120;&#25968;&#26159;&#35780;&#20272;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#33258;&#28982;&#24230;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#32422;&#26463;&#23494;&#38598;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21033;&#26222;&#24076;&#33576;&#24120;&#25968;&#65292;&#35813;&#26041;&#27861;&#20063;&#21487;&#25512;&#24191;&#21040;&#20854;&#20182;&#26550;&#26500;&#12290;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#35757;&#32451;&#26399;&#38388;&#30340;&#31616;&#21333;&#26435;&#37325;&#24402;&#19968;&#21270;&#26041;&#26696;&#65292;&#20197;&#30830;&#20445;&#27599;&#20010;&#23618;&#30340;&#21033;&#26222;&#24076;&#33576;&#24120;&#25968;&#20302;&#20110;&#20998;&#26512;&#24072;&#25351;&#23450;&#30340;&#19978;&#38480;&#12290;&#28982;&#21518;&#21487;&#20197;&#20351;&#29992;&#31616;&#21333;&#30340;&#21333;&#35843;&#21097;&#20313;&#36830;&#25509;&#20351;&#27169;&#22411;&#22312;&#20854;&#20219;&#20309;&#23376;&#38598;&#30340;&#36755;&#20837;&#20013;&#21333;&#35843;&#65292;&#36825;&#22312;&#39046;&#22495;&#30693;&#35782;&#25351;&#23548;&#27492;&#31867;&#20381;&#36182;&#24615;&#30340;&#22330;&#26223;&#20013;&#38750;&#24120;&#26377;&#29992;&#65292;&#20363;&#22914;&#22312;&#31639;&#27861;&#20844;&#24179;&#24615;&#35201;&#27714;&#20013;&#65292;&#25110;&#32773;&#20687;&#22312;&#27492;&#22788;&#23637;&#31034;&#30340;&#37027;&#26679;&#65292;&#22312;&#23545;CERN&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#20135;&#29983;&#30340;&#27425;&#21407;&#23376;&#31890;&#23376;&#30340;&#34928;&#20943;&#36827;&#34892;&#20998;&#31867;&#26102;&#12290;&#25105;&#20204;&#30340;&#24402;&#19968;&#21270;&#26041;&#27861;&#23545;&#26550;&#26500;&#30340;&#32422;&#26463;&#26368;&#23567;&#65292;&#24182;&#20801;&#35768;&#20445;&#25345;&#26356;&#39640;&#30340;&#34920;&#29616;&#21147;&#65292;&#30456;&#27604;&#20854;&#20182;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Lipschitz constant of the map between the input and output space represented by a neural network is a natural metric for assessing the robustness of the model. We present a new method to constrain the Lipschitz constant of dense deep learning models that can also be generalized to other architectures. The method relies on a simple weight normalization scheme during training that ensures the Lipschitz constant of every layer is below an upper limit specified by the analyst. A simple monotonic residual connection can then be used to make the model monotonic in any subset of its inputs, which is useful in scenarios where domain knowledge dictates such dependence. Examples can be found in algorithmic fairness requirements or, as presented here, in the classification of the decays of subatomic particles produced at the CERN Large Hadron Collider. Our normalization is minimally constraining and allows the underlying architecture to maintain higher expressiveness compared to other techniq
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21457;&#29616;&#20102;&#23545;&#25239;&#35757;&#32451;&#20013;&#23384;&#22312;&#30340;&#26631;&#31614;&#22122;&#22768;&#65292;&#24182;&#35299;&#37322;&#20102;&#20854;&#23545;&#40065;&#26834;&#36807;&#24230;&#25311;&#21512;&#30340;&#26222;&#36941;&#23384;&#22312;&#20197;&#21450;&#25200;&#21160;&#21322;&#24452;&#21644;&#25968;&#25454;&#36136;&#37327;&#30340;&#20381;&#36182;&#24615;&#12290;&#36890;&#36807;&#35813;&#35770;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#26657;&#20934;&#26631;&#31614;&#20197;&#24212;&#23545;&#26631;&#31614;&#22122;&#22768;&#21644;&#40065;&#26834;&#36807;&#24230;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2110.03135</link><description>&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#26631;&#31614;&#22122;&#22768;&#65306;&#30740;&#31350;&#40065;&#26834;&#36807;&#24230;&#25311;&#21512;&#30340;&#26032;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Label Noise in Adversarial Training: A Novel Perspective to Study Robust Overfitting. (arXiv:2110.03135v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.03135
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21457;&#29616;&#20102;&#23545;&#25239;&#35757;&#32451;&#20013;&#23384;&#22312;&#30340;&#26631;&#31614;&#22122;&#22768;&#65292;&#24182;&#35299;&#37322;&#20102;&#20854;&#23545;&#40065;&#26834;&#36807;&#24230;&#25311;&#21512;&#30340;&#26222;&#36941;&#23384;&#22312;&#20197;&#21450;&#25200;&#21160;&#21322;&#24452;&#21644;&#25968;&#25454;&#36136;&#37327;&#30340;&#20381;&#36182;&#24615;&#12290;&#36890;&#36807;&#35813;&#35770;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#26657;&#20934;&#26631;&#31614;&#20197;&#24212;&#23545;&#26631;&#31614;&#22122;&#22768;&#21644;&#40065;&#26834;&#36807;&#24230;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#23384;&#22312;&#26631;&#31614;&#22122;&#22768;&#12290;&#36825;&#31181;&#26631;&#31614;&#22122;&#22768;&#26159;&#30001;&#20110;&#23545;&#25239;&#26679;&#26412;&#30340;&#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#19982;&#20174;&#24178;&#20928;&#26679;&#26412;&#32487;&#25215;&#30340;&#26631;&#31614;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#36896;&#25104;&#30340; - &#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#34987;&#23545;&#25239;&#25200;&#21160;&#25197;&#26354;&#65292;&#20294;&#20174;&#24178;&#20928;&#26679;&#26412;&#32487;&#25215;&#26631;&#31614;&#30340;&#24120;&#35265;&#20570;&#27861;&#21364;&#24573;&#30053;&#20102;&#36825;&#19968;&#28857;&#12290;&#35748;&#35782;&#21040;&#26631;&#31614;&#22122;&#22768;&#26377;&#21161;&#20110;&#27934;&#23519;&#23545;&#25239;&#35757;&#32451;&#20013;&#40065;&#26834;&#36807;&#24230;&#25311;&#21512;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#24182;&#35299;&#37322;&#20102;&#20854;&#23545;&#25200;&#21160;&#21322;&#24452;&#21644;&#25968;&#25454;&#36136;&#37327;&#30340;&#22855;&#29305;&#20381;&#36182;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26631;&#31614;&#22122;&#22768;&#35270;&#35282;&#19982;&#25105;&#20204;&#23545;&#23545;&#25239;&#35757;&#32451;&#20013;&#32426;&#20803;&#21452;&#19979;&#38477;&#29616;&#35937;&#30340;&#35266;&#23519;&#30456;&#21563;&#21512;&#12290;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#33258;&#21160;&#26657;&#20934;&#26631;&#31614;&#20197;&#24212;&#23545;&#26631;&#31614;&#22122;&#22768;&#21644;&#40065;&#26834;&#36807;&#24230;&#25311;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19981;&#24341;&#20837;&#26032;&#30340;&#36229;&#21442;&#25968;&#25110;&#39069;&#22806;&#30340;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that label noise exists in adversarial training. Such label noise is due to the mismatch between the true label distribution of adversarial examples and the label inherited from clean examples - the true label distribution is distorted by the adversarial perturbation, but is neglected by the common practice that inherits labels from clean examples. Recognizing label noise sheds insights on the prevalence of robust overfitting in adversarial training, and explains its intriguing dependence on perturbation radius and data quality. Also, our label noise perspective aligns well with our observations of the epoch-wise double descent in adversarial training. Guided by our analyses, we proposed a method to automatically calibrate the label to address the label noise and robust overfitting. Our method achieves consistent performance improvements across various models and datasets without introducing new hyper-parameters or additional tuning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#21449;&#27169;&#24577;&#28508;&#22312;&#34920;&#31034;&#30340;&#22810;&#35828;&#35805;&#20154;&#33080;&#21521;&#35821;&#38899;&#27169;&#22411;Facetron&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#35828;&#35805;&#20154;&#26465;&#20214;&#19979;&#28789;&#27963;&#22320;&#29983;&#25104;&#35821;&#38899;&#27874;&#24418;&#65292;&#24182;&#22312;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2107.12003</link><description>&lt;p&gt;
&#22522;&#20110;&#20132;&#21449;&#27169;&#24577;&#28508;&#22312;&#34920;&#31034;&#30340;&#22810;&#35828;&#35805;&#20154;&#33080;&#21521;&#35821;&#38899;&#27169;&#22411;Facetron
&lt;/p&gt;
&lt;p&gt;
Facetron: A Multi-speaker Face-to-Speech Model based on Cross-modal Latent Representations. (arXiv:2107.12003v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.12003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#21449;&#27169;&#24577;&#28508;&#22312;&#34920;&#31034;&#30340;&#22810;&#35828;&#35805;&#20154;&#33080;&#21521;&#35821;&#38899;&#27169;&#22411;Facetron&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#35828;&#35805;&#20154;&#26465;&#20214;&#19979;&#28789;&#27963;&#22320;&#29983;&#25104;&#35821;&#38899;&#27874;&#24418;&#65292;&#24182;&#22312;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35828;&#35805;&#20154;&#33080;&#21521;&#35821;&#38899;&#27874;&#24418;&#29983;&#25104;&#27169;&#22411;Facetron&#65292;&#36866;&#29992;&#20110;&#26410;&#30693;&#35828;&#35805;&#20154;&#26465;&#20214;&#19979;&#12290;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#35821;&#35328;&#21644;&#35828;&#35805;&#20154;&#29305;&#24449;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#20316;&#20026;&#36741;&#21161;&#26465;&#20214;&#65292;&#30452;&#25509;&#22312;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#26694;&#26550;&#19979;&#23558;&#38754;&#37096;&#22270;&#20687;&#36716;&#25442;&#20026;&#35821;&#38899;&#27874;&#24418;&#12290;&#35821;&#35328;&#29305;&#24449;&#26159;&#20351;&#29992;&#21767;&#35821;&#35782;&#21035;&#27169;&#22411;&#20174;&#21767;&#37096;&#36816;&#21160;&#20013;&#25552;&#21462;&#30340;&#65292;&#35828;&#35805;&#20154;&#29305;&#24449;&#21017;&#36890;&#36807;&#19982;&#39044;&#35757;&#32451;&#30340;&#22768;&#23398;&#27169;&#22411;&#30340;&#20132;&#21449;&#27169;&#24577;&#23398;&#20064;&#20174;&#38754;&#37096;&#22270;&#20687;&#20013;&#39044;&#27979;&#24471;&#20986;&#12290;&#30001;&#20110;&#36825;&#20004;&#20010;&#29305;&#24449;&#26159;&#19981;&#30456;&#20851;&#30340;&#19988;&#29420;&#31435;&#25511;&#21046;&#30340;&#65292;&#22240;&#27492;&#25105;&#20204;&#21487;&#20197;&#28789;&#27963;&#22320;&#21512;&#25104;&#35821;&#38899;&#27874;&#24418;&#65292;&#20854;&#35828;&#35805;&#20154;&#29305;&#24449;&#21462;&#20915;&#20110;&#36755;&#20837;&#30340;&#38754;&#37096;&#22270;&#20687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#32467;&#26524;&#26041;&#38754;&#27604;&#20256;&#32479;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a multi-speaker face-to-speech waveform generation model that also works for unseen speaker conditions. Using a generative adversarial network (GAN) with linguistic and speaker characteristic features as auxiliary conditions, our method directly converts face images into speech waveforms under an end-to-end training framework. The linguistic features are extracted from lip movements using a lip-reading model, and the speaker characteristic features are predicted from face images using cross-modal learning with a pre-trained acoustic model. Since these two features are uncorrelated and controlled independently, we can flexibly synthesize speech waveforms whose speaker characteristics vary depending on the input face images. We show the superiority of our proposed model over conventional methods in terms of objective and subjective evaluation results. Specifically, we evaluate the performances of linguistic features by measuring their accuracy on an automatic sp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#36125;&#21494;&#26031;&#31215;&#20998;&#26041;&#26696;&#65292;&#29992;&#20110;&#36793;&#32536;&#21270;&#39640;&#26031;&#36807;&#31243;&#26680;&#26063;&#65292;&#20197;&#33719;&#24471;&#20855;&#26377;&#33391;&#22909;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#28789;&#27963;&#27169;&#22411;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#25928;&#23454;&#29992;&#12290;</title><link>http://arxiv.org/abs/2106.07452</link><description>&lt;p&gt;
&#29992;&#36125;&#21494;&#26031;&#31215;&#20998;&#23545;&#24179;&#31283;&#26680;&#36827;&#34892;&#36793;&#32536;&#21270;
&lt;/p&gt;
&lt;p&gt;
Marginalising over Stationary Kernels with Bayesian Quadrature. (arXiv:2106.07452v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.07452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#36125;&#21494;&#26031;&#31215;&#20998;&#26041;&#26696;&#65292;&#29992;&#20110;&#36793;&#32536;&#21270;&#39640;&#26031;&#36807;&#31243;&#26680;&#26063;&#65292;&#20197;&#33719;&#24471;&#20855;&#26377;&#33391;&#22909;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#28789;&#27963;&#27169;&#22411;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#25928;&#23454;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#39640;&#26031;&#36807;&#31243;&#26680;&#26063;&#36827;&#34892;&#36793;&#32536;&#21270;&#21487;&#20197;&#20135;&#29983;&#20855;&#26377;&#33391;&#22909;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#28789;&#27963;&#27169;&#22411;&#12290;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#35780;&#20272;&#35768;&#22810;&#26680;&#30340;&#20284;&#28982;&#24615;&#65292;&#20351;&#23427;&#20204;&#23545;&#20110;&#36739;&#22823;&#30340;&#25968;&#25454;&#38598;&#26469;&#35828;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#31215;&#20998;&#26041;&#26696;&#65292;&#20351;&#24471;&#36825;&#31181;&#36793;&#32536;&#21270;&#26356;&#21152;&#39640;&#25928;&#65292;&#22240;&#27492;&#26356;&#21152;&#23454;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#20998;&#24067;&#20043;&#38388;&#30340;&#26368;&#22823;&#24179;&#22343;&#24046;&#24322;&#65292;&#25105;&#20204;&#23450;&#20041;&#19968;&#31181;&#25429;&#25417;&#35889;&#28151;&#21512;&#65288;SM&#65289;&#26680;&#20043;&#38388;&#19981;&#21464;&#24615;&#30340;&#26680;&#12290;&#36890;&#36807;&#25512;&#24191;&#38590;&#20197;&#23450;&#20041;&#30340;&#21464;&#24418;&#36125;&#21494;&#26031;&#31215;&#20998;&#21462;&#24471;&#26679;&#26412;&#26680;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#27604;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#23588;&#20854;&#26159;&#24403;&#32473;&#20986;&#26377;&#38480;&#30340;&#65288;&#22681;&#26102;&#38047;&#65289;&#26102;&#38388;&#39044;&#31639;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Marginalising over families of Gaussian Process kernels produces flexible model classes with well-calibrated uncertainty estimates. Existing approaches require likelihood evaluations of many kernels, rendering them prohibitively expensive for larger datasets. We propose a Bayesian Quadrature scheme to make this marginalisation more efficient and thereby more practical. Through use of the maximum mean discrepancies between distributions, we define a kernel over kernels that captures invariances between Spectral Mixture (SM) Kernels. Kernel samples are selected by generalising an information-theoretic acquisition function for warped Bayesian Quadrature. We show that our framework achieves more accurate predictions with better calibrated uncertainty than state-of-the-art baselines, especially when given limited (wall-clock) time budgets.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#31639;&#27861;ANTS&#65292;&#23427;&#23558;&#35268;&#21010;&#21644;&#23398;&#20064;&#32467;&#21512;&#22312;&#26368;&#22823;&#29109;&#33539;&#24335;&#20013;&#65292;&#24182;&#36890;&#36807;&#22312;Atari&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20854;&#26126;&#26174;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;AlphaZero&#31995;&#32479;&#30340;&#35268;&#21010;&#32452;&#20214;PUCT&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#31283;&#20581;&#24615;&#65292;&#21487;&#25512;&#21160;&#22522;&#20110;&#26641;&#30340;&#35268;&#21010;&#26041;&#27861;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2102.06808</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#29109;&#26641;&#25628;&#32034;&#30340;&#35268;&#21010;&#19982;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Planning and Learning Using Adaptive Entropy Tree Search. (arXiv:2102.06808v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.06808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#31639;&#27861;ANTS&#65292;&#23427;&#23558;&#35268;&#21010;&#21644;&#23398;&#20064;&#32467;&#21512;&#22312;&#26368;&#22823;&#29109;&#33539;&#24335;&#20013;&#65292;&#24182;&#36890;&#36807;&#22312;Atari&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20854;&#26126;&#26174;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;AlphaZero&#31995;&#32479;&#30340;&#35268;&#21010;&#32452;&#20214;PUCT&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#31283;&#20581;&#24615;&#65292;&#21487;&#25512;&#21160;&#22522;&#20110;&#26641;&#30340;&#35268;&#21010;&#26041;&#27861;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#20154;&#24037;&#26234;&#33021;&#31361;&#30772;&#34920;&#26126;&#65292;&#23558;&#22522;&#20110;&#26641;&#30340;&#35268;&#21010;&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20316;&#33258;&#36866;&#24212;&#29109;&#26641;&#25628;&#32034;&#65288;ANTS&#65289;&#30340;&#20840;&#26032;&#31639;&#27861;&#65292;&#23558;&#35268;&#21010;&#21644;&#23398;&#20064;&#32467;&#21512;&#22312;&#26368;&#22823;&#29109;&#33539;&#24335;&#20013;&#12290;&#36890;&#36807;&#22312;Atari&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;ANTS&#26126;&#26174;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;AlphaZero&#31995;&#32479;&#30340;&#35268;&#21010;&#32452;&#20214;PUCT&#12290;ANTS&#24314;&#31435;&#22312;&#26368;&#22823;&#29109;&#35268;&#21010;&#26041;&#27861;&#30340;&#22522;&#30784;&#20043;&#19978;&#65292;&#28982;&#32780;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#22312;&#19982;&#23398;&#20064;&#30456;&#32467;&#21512;&#26102;&#34920;&#29616;&#19981;&#20339;&#65292;ANTS&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#24182;&#36798;&#21040;&#20102;&#19982;&#30446;&#21069;&#26368;&#20808;&#36827;&#31639;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;ANTS&#22312;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#19979;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;ANTS&#30340;&#39640;&#24615;&#33021;&#21644;&#31283;&#20581;&#24615;&#23558;&#20351;&#22522;&#20110;&#26641;&#30340;&#35268;&#21010;&#26041;&#27861;&#26356;&#21152;&#36866;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent breakthroughs in Artificial Intelligence have shown that the combination of tree-based planning with deep learning can lead to superior performance. We present Adaptive Entropy Tree Search (ANTS) - a novel algorithm combining planning and learning in the maximum entropy paradigm. Through a comprehensive suite of experiments on the Atari benchmark we show that ANTS significantly outperforms PUCT, the planning component of the state-of-the-art AlphaZero system. ANTS builds upon recent work on maximum entropy planning methods - which however, as we show, fail in combination with learning. ANTS resolves this issue to reach state-of-the-art performance. We further find that ANTS exhibits superior robustness to different hyperparameter choices, compared to the previous algorithms. We believe that the high performance and robustness of ANTS can bring tree search planning one step closer to wide practical adoption.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ES-ENAS&#26041;&#27861;&#65292;&#23558;&#36827;&#21270;&#31574;&#30053;&#21644;&#32452;&#21512;&#20248;&#21270;&#22120;&#32467;&#21512;&#36215;&#26469;&#26469;&#20248;&#21270;&#28151;&#21512;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#22312;CIFAR-10&#19978;&#21462;&#24471;&#20102;&#36229;&#36234;&#26368;&#26032;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2101.07415</link><description>&lt;p&gt;
ES-ENAS: &#39640;&#25928;&#28436;&#21270;&#20248;&#21270;&#22823;&#22411;&#28151;&#21512;&#25628;&#32034;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
ES-ENAS: Efficient Evolutionary Optimization for Large Hybrid Search Spaces. (arXiv:2101.07415v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.07415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ES-ENAS&#26041;&#27861;&#65292;&#23558;&#36827;&#21270;&#31574;&#30053;&#21644;&#32452;&#21512;&#20248;&#21270;&#22120;&#32467;&#21512;&#36215;&#26469;&#26469;&#20248;&#21270;&#28151;&#21512;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#22312;CIFAR-10&#19978;&#21462;&#24471;&#20102;&#36229;&#36234;&#26368;&#26032;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#28151;&#21512;&#25628;&#32034;&#31354;&#38388;&#65288;&#30001;&#32452;&#21512;&#21644;&#36830;&#32493;&#21442;&#25968;&#32452;&#25104;&#65289;&#19978;&#20248;&#21270;&#40657;&#30418;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20808;&#21069;&#20381;&#36182;&#20110;&#21464;&#24322;&#31574;&#30053;&#30340;&#36827;&#21270;&#31639;&#27861;&#65292;&#22312;&#32452;&#21512;&#31354;&#38388;&#19978;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#20294;&#22312;&#39640;&#32500;&#36830;&#32493;&#31354;&#38388;&#19978;&#23384;&#22312;&#32500;&#24230;&#28798;&#38590;&#30340;&#38382;&#39064;&#65292;&#20174;&#29702;&#35770;&#21644;&#23454;&#39564;&#19978;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#28151;&#21512;&#25628;&#32034;&#31354;&#38388;&#19978;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;ES-ENAS&#65292;&#19968;&#31181;&#31616;&#21333;&#19988;&#27169;&#22359;&#21270;&#30340;&#32852;&#21512;&#20248;&#21270;&#36807;&#31243;&#65292;&#23558;&#25928;&#29575;&#20302;&#19979;&#30340;&#32452;&#21512;&#20248;&#21270;&#22120;&#19982;&#28436;&#21270;&#31574;&#30053;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#21551;&#21457;&#24335;&#30340;&#19968;&#27425;&#24615;&#25110;&#36229;&#32423;&#32593;&#32476;&#33539;&#24335;&#65292;&#23454;&#29616;&#39640;&#24230;&#21487;&#25193;&#23637;&#19988;&#30452;&#35266;&#30340;&#20248;&#21270;&#36807;&#31243;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#22312;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#32463;&#39564;&#39564;&#35777;&#65292;&#22312;&#21333;&#20010;GPU&#19978;&#25968;&#23567;&#26102;&#20869;&#23601;&#21487;&#20197;&#20248;&#21270;CIFAR-10&#30340;&#22797;&#26434;&#28151;&#21512;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we approach the problem of optimizing blackbox functions over large hybrid search spaces consisting of both combinatorial and continuous parameters. We demonstrate that previous evolutionary algorithms which rely on mutation-based approaches, while flexible over combinatorial spaces, suffer from a curse of dimensionality in high dimensional continuous spaces both theoretically and empirically, which thus limits their scope over hybrid search spaces as well. In order to combat this curse, we propose ES-ENAS, a simple and modular joint optimization procedure combining the class of sample-efficient smoothed gradient techniques, commonly known as Evolutionary Strategies (ES), with combinatorial optimizers in a highly scalable and intuitive way, inspired by the one-shot or supernet paradigm introduced in Efficient Neural Architecture Search (ENAS). By doing so, we achieve significantly more sample efficiency, which we empirically demonstrate over synthetic benchmarks, and are
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36127;&#36733;&#21387;&#32553;&#23884;&#20837;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#23398;&#20064; AC-OPF &#26102;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#25968;&#37327;&#32423;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2101.03973</link><description>&lt;p&gt;
&#23398;&#20064; AC-OPF &#30340;&#36127;&#36733;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Load Encoding for Learning AC-OPF. (arXiv:2101.03973v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.03973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36127;&#36733;&#21387;&#32553;&#23884;&#20837;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#23398;&#20064; AC-OPF &#26102;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#25968;&#37327;&#32423;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AC Optimal Power Flow (AC-OPF) &#38382;&#39064;&#26159;&#30005;&#21147;&#20256;&#36755;&#31995;&#32479;&#20013;&#30340;&#26680;&#24515;&#26500;&#24314;&#22359;&#12290;&#23427;&#23547;&#27714;&#26368;&#32463;&#27982;&#30340;&#26377;&#21151;&#21644;&#26080;&#21151;&#21457;&#30005;&#35843;&#24230;&#20197;&#28385;&#36275;&#38656;&#27714;&#65292;&#21516;&#26102;&#28385;&#36275;&#20256;&#36755;&#25805;&#20316;&#38480;&#21046;&#12290; &#36817;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#25552;&#20379; AC-OPF &#35299;&#20915;&#26041;&#26696;&#30340;&#31934;&#30830;&#36817;&#20284;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290; &#20294;&#26159;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#30005;&#21147;&#32593;&#32476;&#26102;&#24448;&#24448;&#20250;&#36935;&#21040;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290; &#26412;&#25991;&#38024;&#23545;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36127;&#36733;&#21387;&#32553;&#23884;&#20837;&#26041;&#26696;&#65292;&#20351;&#29992;&#19977;&#27493;&#26041;&#27861;&#20943;&#23567;&#35757;&#32451;&#27169;&#22411;&#30340;&#22823;&#23567;&#12290;&#35813;&#26041;&#27861;&#22312;&#26469;&#33258;PGLib&#30340;&#22823;&#35268;&#27169;&#27979;&#35797;&#29992;&#20363;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#24182;&#22312;&#35757;&#32451;&#25910;&#25947;&#24615;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#20135;&#29983;&#20102;&#25968;&#37327;&#32423;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The AC Optimal Power Flow (AC-OPF) problem is a core building block in electrical transmission system. It seeks the most economical active and reactive generation dispatch to meet demands while satisfying transmission operational limits. It is often solved repeatedly, especially in regions with large penetration of wind farms to avoid violating operational and physical limits. Recent work has shown that deep learning techniques have huge potential in providing accurate approximations of AC-OPF solutions. However, deep learning approaches often suffer from scalability issues, especially when applied to real life power grids. This paper focuses on the scalability limitation and proposes a load compression embedding scheme to reduce training model sizes using a 3-step approach. The approach is evaluated experimentally on large-scale test cases from the PGLib, and produces an order of magnitude improvements in training convergence and prediction accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#31354;&#38388;&#22495;&#20013;&#37325;&#29616;&#38519;&#27874;&#28388;&#27874;&#22120;&#30340;&#25928;&#26524;&#65292;&#20197;&#29983;&#25104;&#39640;&#24230;&#36924;&#30495;&#19988;&#8220;&#26816;&#27979;&#38590;&#20197;&#25417;&#25720;&#8221;&#30340;DeepFakes&#12290;</title><link>http://arxiv.org/abs/2009.09213</link><description>&lt;p&gt;
&#38544;&#24335;&#31354;&#38388;&#22495;&#38519;&#27874;&#28388;&#27874;&#22120;&#65306;&#36530;&#36991;DeepFake&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Dodging DeepFake Detection via Implicit Spatial-Domain Notch Filtering. (arXiv:2009.09213v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.09213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#31354;&#38388;&#22495;&#20013;&#37325;&#29616;&#38519;&#27874;&#28388;&#27874;&#22120;&#30340;&#25928;&#26524;&#65292;&#20197;&#29983;&#25104;&#39640;&#24230;&#36924;&#30495;&#19988;&#8220;&#26816;&#27979;&#38590;&#20197;&#25417;&#25720;&#8221;&#30340;DeepFakes&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#65292;DeepFake&#22270;&#20687;&#30340;&#39640;&#20445;&#30495;&#29983;&#25104;&#21644;&#39640;&#31934;&#24230;&#26816;&#27979;&#27491;&#22788;&#20110;&#19968;&#22330;&#20891;&#22791;&#31454;&#36187;&#20013;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#29983;&#25104;&#39640;&#24230;&#36924;&#30495;&#19988;&#8220;&#26816;&#27979;&#38590;&#20197;&#25417;&#25720;&#8221;&#30340;DeepFakes&#21487;&#20197;&#20026;&#26410;&#26469;&#30340;DeepFake&#26816;&#27979;&#33021;&#21147;&#30340;&#25552;&#39640;&#26381;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#27969;&#31243;&#65292;&#36890;&#36807;&#36827;&#34892;&#38544;&#24335;&#31354;&#38388;&#22495;&#38519;&#27874;&#28388;&#27874;&#26469;&#20943;&#23569;&#20266;&#36896;&#22270;&#20687;&#30340;&#20266;&#24433;&#32441;&#29702;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#22270;&#20687;&#36136;&#37327;&#12290;&#39318;&#20808;&#35777;&#26126;&#20102;&#22312;&#31354;&#38388;&#22495;&#36827;&#34892;&#21608;&#26399;&#24615;&#22122;&#22768;&#30340;&#39057;&#22495;&#38519;&#27874;&#34429;&#28982;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#38519;&#27874;&#22120;&#38656;&#35201;&#25163;&#21160;&#35774;&#35745;&#19981;&#36866;&#21512;&#20110;&#25105;&#20204;&#30340;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#31354;&#38388;&#22495;&#20013;&#37325;&#29616;&#38519;&#27874;&#28388;&#27874;&#22120;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#32467;&#21512;&#22686;&#21152;&#21387;&#20498;&#24615;&#31354;&#38388;&#22122;&#22768;&#26469;&#25171;&#30772;&#21608;&#26399;&#22122;&#22768;&#27169;&#24335;&#65292;&#20877;&#21033;&#29992;&#28145;&#24230;&#22270;&#20687;&#28388;&#27874;&#26469;&#37325;&#26500;&#26080;&#22122;&#22768;DeepFakes&#12290;
&lt;/p&gt;
&lt;p&gt;
The current high-fidelity generation and high-precision detection of DeepFake images are at an arms race. We believe that producing DeepFakes that are highly realistic and 'detection evasive' can serve the ultimate goal of improving future generation DeepFake detection capabilities. In this paper, we propose a simple yet powerful pipeline to reduce the artifact patterns of fake images without hurting image quality by performing implicit spatial-domain notch filtering. We first demonstrate that frequency-domain notch filtering, although famously shown to be effective in removing periodic noise in the spatial domain, is infeasible for our task at hand due to the manual designs required for the notch filters. We, therefore, resort to a learning-based approach to reproduce the notch filtering effects, but solely in the spatial domain. We adopt a combination of adding overwhelming spatial noise for breaking the periodic noise pattern and deep image filtering to reconstruct the noise-free fa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#24322;&#26500;VEC&#22330;&#26223;&#20013;&#30340;&#35745;&#31639;&#21368;&#36733;&#38382;&#39064;&#25552;&#20986;&#20102;&#22522;&#20110;&#22810;&#33218;&#32769;&#34382;&#26426;&#29702;&#35770;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#21644;Off-policy&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#21160;&#24577;&#36873;&#25321;&#26368;&#23567;&#21368;&#36733;&#26102;&#38388;&#20026;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#22312;&#19968;&#32676;&#36710;&#36742;&#21644;&#22522;&#31449;&#38388;&#26435;&#34913;&#35745;&#31639;&#21368;&#36733;&#20307;&#39564;&#21644;&#32593;&#32476;&#36127;&#36733;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2008.06302</link><description>&lt;p&gt;
&#24322;&#26500;&#36710;&#36733;&#36793;&#32536;&#32593;&#32476;&#20013;&#30340;&#35745;&#31639;&#21368;&#36733;&#65306;&#22312;&#32447;&#21644;Off-policy&#21305;&#37197;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Computation Offloading in Heterogeneous Vehicular Edge Networks: On-line and Off-policy Bandit Solutions. (arXiv:2008.06302v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2008.06302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#24322;&#26500;VEC&#22330;&#26223;&#20013;&#30340;&#35745;&#31639;&#21368;&#36733;&#38382;&#39064;&#25552;&#20986;&#20102;&#22522;&#20110;&#22810;&#33218;&#32769;&#34382;&#26426;&#29702;&#35770;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#21644;Off-policy&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#21160;&#24577;&#36873;&#25321;&#26368;&#23567;&#21368;&#36733;&#26102;&#38388;&#20026;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#22312;&#19968;&#32676;&#36710;&#36742;&#21644;&#22522;&#31449;&#38388;&#26435;&#34913;&#35745;&#31639;&#21368;&#36733;&#20307;&#39564;&#21644;&#32593;&#32476;&#36127;&#36733;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#21644;&#36710;&#36733;&#36890;&#20449;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#36710;&#36742;&#36793;&#32536;&#35745;&#31639;&#65288;VEC&#65289;&#27491;&#22312;&#25104;&#20026;&#25903;&#25345;&#20302;&#24310;&#36831;ITS&#24212;&#29992;&#21644;&#26381;&#21153;&#30340;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#24322;&#26500;VEC&#22330;&#26223;&#20013;&#65292;&#20174;&#31227;&#21160;&#36710;&#36742;/&#29992;&#25143;&#35745;&#31639;&#21368;&#36733;&#30340;&#38382;&#39064;&#65292;&#24182;&#20851;&#27880;&#36873;&#25321;&#19981;&#21516;&#32593;&#32476;&#30340;&#32593;&#32476;&#21644;&#22522;&#31449;&#36873;&#25321;&#38382;&#39064;&#12290;&#22312;&#24555;&#36895;&#21464;&#21270;&#30340;&#36710;&#36733;&#29615;&#22659;&#20013;&#65292;&#29992;&#25143;&#30340;&#35745;&#31639;&#21368;&#36733;&#20307;&#39564;&#21463;&#21040;&#19982;&#22522;&#31449;&#20849;&#23384;&#30340;&#36793;&#32536;&#35745;&#31639;&#26381;&#21153;&#22120;&#25317;&#22622;&#24341;&#36215;&#30340;&#24310;&#36831;&#30340;&#24378;&#28872;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#31181;&#29615;&#22659;&#30340;&#38750;&#24179;&#31283;&#29305;&#24615;&#21644;&#20449;&#24687;&#30701;&#32570;&#65292;&#39044;&#27979;&#36825;&#31181;&#25317;&#22622;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#22810;&#33218;&#32769;&#34382;&#26426;&#29702;&#35770;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#21644;Off-policy&#23398;&#20064;&#31639;&#27861;&#12290;&#20197;&#21160;&#24577;&#36873;&#25321;&#26368;&#23567;&#21368;&#36733;&#26102;&#38388;&#20026;&#30446;&#26631;&#65292;&#36890;&#36807;&#22312;&#19968;&#32676;&#36710;&#36742;&#21644;&#22522;&#31449;&#38388;&#26435;&#34913;&#35745;&#31639;&#21368;&#36733;&#20307;&#39564;&#21644;&#32593;&#32476;&#36127;&#36733;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid advancement of Intelligent Transportation Systems (ITS) and vehicular communications, Vehicular Edge Computing (VEC) is emerging as a promising technology to support low-latency ITS applications and services. In this paper, we consider the computation offloading problem from mobile vehicles/users in a heterogeneous VEC scenario, and focus on the network- and base station selection problems, where different networks have different traffic loads. In a fast-varying vehicular environment, computation offloading experience of users is strongly affected by the latency due to the congestion at the edge computing servers co-located with the base stations. However, as a result of the non-stationary property of such an environment and also information shortage, predicting this congestion is an involved task. To address this challenge, we propose an on-line learning algorithm and an off-policy learning algorithm based on multi-armed bandit theory. To dynamically select the least co
&lt;/p&gt;</description></item></channel></rss>