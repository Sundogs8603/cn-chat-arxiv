<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#22914;&#20309;&#36890;&#36807;&#21452;&#37325;&#19981;&#30830;&#23450;&#24615;&#30340;&#33258;&#35757;&#32451;&#26041;&#24335;&#26469;&#25552;&#39640;&#20998;&#21106;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.04441</link><description>&lt;p&gt;
&#21452;&#37325;&#19981;&#30830;&#23450;&#24615;&#33258;&#35757;&#32451;&#29992;&#20110;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Self-training with dual uncertainty for semi-supervised medical image segmentation. (arXiv:2304.04441v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04441
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#22914;&#20309;&#36890;&#36807;&#21452;&#37325;&#19981;&#30830;&#23450;&#24615;&#30340;&#33258;&#35757;&#32451;&#26041;&#24335;&#26469;&#25552;&#39640;&#20998;&#21106;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#39046;&#22495;&#20013;&#65292;&#26631;&#35760;&#25968;&#25454;&#30701;&#32570;&#26159;&#19968;&#20010;&#26681;&#26412;&#24615;&#38382;&#39064;&#12290;&#22914;&#20309;&#26377;&#25928;&#22320;&#20174;&#26410;&#26631;&#35760;&#30340;&#22270;&#20687;&#20013;&#23398;&#20064;&#22270;&#20687;&#29305;&#24449;&#20197;&#25552;&#39640;&#20998;&#21106;&#31934;&#24230;&#26159;&#36825;&#19968;&#39046;&#22495;&#30340;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#12290;&#20256;&#32479;&#30340;&#33258;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#20026;&#36845;&#20195;&#35757;&#32451;&#29983;&#25104;&#20266;&#26631;&#31614;&#37096;&#20998;&#35299;&#20915;&#26631;&#35760;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30001;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#20135;&#29983;&#30340;&#22122;&#22768;&#30452;&#25509;&#24433;&#21709;&#20102;&#20998;&#21106;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#33258;&#35757;&#32451;&#26694;&#26550;&#20013;&#22686;&#21152;&#20102;&#26679;&#26412;&#23618;&#38754;&#21644;&#20687;&#32032;&#23618;&#38754;&#30340;&#19981;&#30830;&#23450;&#24615;&#20197;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#20445;&#23384;&#20102;&#27169;&#22411;&#30340;&#20960;&#20010;&#26102;&#21051;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#22312;&#26410;&#26631;&#35760;&#26679;&#26412;&#19978;&#30340;&#39044;&#27979;&#20043;&#38388;&#30340;&#24046;&#24322;&#20316;&#20026;&#35813;&#26679;&#26412;&#30340;&#26679;&#26412;&#23618;&#38754;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36880;&#28176;&#28155;&#21152;&#20174;&#26131;&#21040;&#38590;&#30340;&#26410;&#26631;&#35760;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#28155;&#21152;&#20102;&#19968;&#20010;&#24102;&#26377;&#19981;&#21516;&#19978;&#37319;&#26679;&#26041;&#27861;&#30340;&#35299;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of semi-supervised medical image segmentation, the shortage of labeled data is the fundamental problem. How to effectively learn image features from unlabeled images to improve segmentation accuracy is the main research direction in this field. Traditional self-training methods can partially solve the problem of insufficient labeled data by generating pseudo labels for iterative training. However, noise generated due to the model's uncertainty during training directly affects the segmentation results. Therefore, we added sample-level and pixel-level uncertainty to stabilize the training process based on the self-training framework. Specifically, we saved several moments of the model during pre-training, and used the difference between their predictions on unlabeled samples as the sample-level uncertainty estimate for that sample. Then, we gradually add unlabeled samples from easy to hard during training. At the same time, we added a decoder with different upsampling method
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PCR&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#23545;&#27604;&#24335;&#22238;&#25918;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#20915;&#22312;&#32447;&#31867;&#22686;&#37327;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#21382;&#21490;&#30693;&#35782;&#36951;&#24536;&#38382;&#39064;&#12290;&#32463;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#20004;&#20010;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#65292;PCR&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.04408</link><description>&lt;p&gt;
PCR: &#22522;&#20110;&#20195;&#29702;&#30340;&#23545;&#27604;&#24335;&#22238;&#25918;&#22312;&#22312;&#32447;&#31867;&#22686;&#37327;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
PCR: Proxy-based Contrastive Replay for Online Class-Incremental Continual Learning. (arXiv:2304.04408v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PCR&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#23545;&#27604;&#24335;&#22238;&#25918;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#20915;&#22312;&#32447;&#31867;&#22686;&#37327;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#21382;&#21490;&#30693;&#35782;&#36951;&#24536;&#38382;&#39064;&#12290;&#32463;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#20004;&#20010;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#65292;PCR&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#31867;&#22686;&#37327;&#36830;&#32493;&#23398;&#20064;&#26159;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#19968;&#39033;&#29305;&#23450;&#20219;&#21153;&#12290;&#23427;&#26088;&#22312;&#20174;&#25968;&#25454;&#27969;&#20013;&#19981;&#26029;&#23398;&#20064;&#26032;&#30340;&#31867;&#21035;&#65292;&#20294;&#25968;&#25454;&#27969;&#20013;&#30340;&#26679;&#26412;&#20165;&#38656;&#35266;&#23519;&#19968;&#27425;&#65292;&#36825;&#23481;&#26131;&#23548;&#33268;&#21382;&#21490;&#31867;&#21035;&#30340;&#30693;&#35782;&#36951;&#24536;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#22238;&#25918;&#30340;&#26041;&#27861;&#36890;&#36807;&#20197;&#20195;&#29702;&#20026;&#22522;&#30784;&#25110;&#20197;&#23545;&#27604;&#20026;&#22522;&#30784;&#30340;&#22238;&#25918;&#26041;&#24335;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#36825;&#19968;&#38382;&#39064;&#12290;&#23613;&#31649;&#36825;&#20004;&#31181;&#22238;&#25918;&#26041;&#24335;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#21069;&#32773;&#20250;&#22240;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#32780;&#20542;&#21521;&#20110;&#26032;&#31867;&#65292;&#21518;&#32773;&#21017;&#30001;&#20110;&#26679;&#26412;&#25968;&#37327;&#26377;&#38480;&#32780;&#19981;&#31283;&#23450;&#19988;&#38590;&#20197;&#25910;&#25947;&#12290;&#26412;&#25991;&#23545;&#36825;&#20004;&#31181;&#22238;&#25918;&#26041;&#24335;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#20114;&#34917;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22238;&#25918;-based&#26041;&#27861;&#65292;&#31216;&#20026;&#22522;&#20110;&#20195;&#29702;&#30340;&#23545;&#27604;&#24335;&#22238;&#25918;&#65288;PCR&#65289;&#12290;&#20851;&#38190;&#25805;&#20316;&#26159;&#23558;&#38170;&#23450;&#28857;&#30340;&#23545;&#27604;&#26679;&#26412;&#26367;&#25442;&#20026;&#30456;&#24212;&#20195;&#29702;&#30340;&#23545;&#27604;&#26679;&#26412;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35774;&#35745;&#24182;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#20004;&#31181;&#20195;&#29702;&#65292;&#21363;&#26087;&#20195;&#29702;&#21644;&#26032;&#20195;&#29702;&#65292;&#20197;&#31283;&#23450;&#35757;&#32451;&#24182;&#32531;&#35299;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;PCR&#22312;&#20004;&#20010;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online class-incremental continual learning is a specific task of continual learning. It aims to continuously learn new classes from data stream and the samples of data stream are seen only once, which suffers from the catastrophic forgetting issue, i.e., forgetting historical knowledge of old classes. Existing replay-based methods effectively alleviate this issue by saving and replaying part of old data in a proxy-based or contrastive-based replay manner. Although these two replay manners are effective, the former would incline to new classes due to class imbalance issues, and the latter is unstable and hard to converge because of the limited number of samples. In this paper, we conduct a comprehensive analysis of these two replay manners and find that they can be complementary. Inspired by this finding, we propose a novel replay-based method called proxy-based contrastive replay (PCR). The key operation is to replace the contrastive samples of anchors with corresponding proxies in th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#33258;&#36866;&#24212;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;CAVL&#65292;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#36890;&#36807;&#23545;&#27604;&#25439;&#22833;&#23398;&#20064;&#25972;&#20010;&#21477;&#23376;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#24182;&#22312;&#24494;&#35843;&#38454;&#27573;&#24341;&#20837;&#36731;&#37327;&#32423;&#33258;&#36866;&#24212;&#32593;&#32476;&#65292;&#23454;&#29616;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#26368;&#20808;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.04399</link><description>&lt;p&gt;
CAVL&#65306;&#23398;&#20064;&#23545;&#27604;&#21644;&#33258;&#36866;&#24212;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
CAVL: Learning Contrastive and Adaptive Representations of Vision and Language. (arXiv:2304.04399v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#33258;&#36866;&#24212;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;CAVL&#65292;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#36890;&#36807;&#23545;&#27604;&#25439;&#22833;&#23398;&#20064;&#25972;&#20010;&#21477;&#23376;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#24182;&#22312;&#24494;&#35843;&#38454;&#27573;&#24341;&#20837;&#36731;&#37327;&#32423;&#33258;&#36866;&#24212;&#32593;&#32476;&#65292;&#23454;&#29616;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#26368;&#20808;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#39044;&#35757;&#32451;&#26088;&#22312;&#19968;&#36215;&#23398;&#20064;&#35270;&#35273;&#21644;&#35821;&#35328;&#34920;&#31034;&#65292;&#24182;&#21487;&#36716;&#31227;&#21040;&#35270;&#35273;&#35821;&#35328;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#35821;&#35328;&#21644;&#35270;&#35273;&#20043;&#38388;&#23384;&#22312;&#35821;&#20041;&#28151;&#28102;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#36716;&#31227;&#21040;&#19979;&#28216;&#20219;&#21153;&#26102;&#24448;&#24448;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#23545;&#27604;&#21644;&#33258;&#36866;&#24212;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#34920;&#31034;&#65292;&#21363;CAVL&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#19968;&#23545;&#19968;&#23545;&#30340;&#23545;&#27604;&#25439;&#22833;&#65292;&#20197;&#23398;&#20064;&#25972;&#20010;&#21477;&#23376;&#21644;&#21516;&#19968;&#25209;&#27425;&#20013;&#27599;&#20010;&#22270;&#20687;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#22312;&#24494;&#35843;&#38454;&#27573;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#36731;&#37327;&#32423;&#33258;&#36866;&#24212;&#32593;&#32476;&#65292;&#20197;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#24182;&#22686;&#21152;&#35757;&#32451;&#36895;&#24230;&#65292;&#20197;&#33410;&#30465;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#12289;&#35270;&#35273;&#36890;&#35782;&#25512;&#29702;&#65288;VCL&#65289;&#21644;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#31561;&#20845;&#20010;&#20027;&#35201;&#19979;&#28216;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;CAVL&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CAVL&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual and linguistic pre-training aims to learn vision and language representations together, which can be transferred to visual-linguistic downstream tasks. However, there exists semantic confusion between language and vision during the pre-training stage. Moreover, current pre-trained models tend to take lots of computation resources for fine-tuning when transferred to downstream tasks. In this work, we present a simple but effective approach for learning Contrastive and Adaptive representations of Vision and Language, namely CAVL. Specifically, we introduce a pair-wise contrastive loss to learn alignments between the whole sentence and each image in the same batch during the pre-training process. At the fine-tuning stage, we introduce two lightweight adaptation networks to reduce model parameters and increase training speed for saving computation resources. We evaluate our CAVL on six main downstream tasks, including Visual Question Answering (VQA), Visual Commonsense Reasoning (VC
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38543;&#26426;&#21644;&#30830;&#23450;&#24615;&#27880;&#24847;&#21147;&#31232;&#30095;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#36229;&#21442;&#25968;&#21270;&#29305;&#24449;&#32500;&#24230;&#65292;&#20854;&#30446;&#26631;&#26159;&#25214;&#21040;&#36866;&#24403;&#30340;&#30697;&#38453;Y&#65292;&#20351;&#24471;&#20854;&#28385;&#36275;&#29305;&#23450;&#26465;&#20214;&#65292;&#20174;&#32780;&#24110;&#21161;&#35299;&#20915;&#27880;&#24847;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.04397</link><description>&lt;p&gt;
&#38024;&#23545;&#36229;&#21442;&#25968;&#21270;&#29305;&#24449;&#32500;&#24230;&#30340;&#38543;&#26426;&#21644;&#30830;&#23450;&#24615;&#27880;&#24847;&#21147;&#31232;&#30095;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Randomized and Deterministic Attention Sparsification Algorithms for Over-parameterized Feature Dimension. (arXiv:2304.04397v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38543;&#26426;&#21644;&#30830;&#23450;&#24615;&#27880;&#24847;&#21147;&#31232;&#30095;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#36229;&#21442;&#25968;&#21270;&#29305;&#24449;&#32500;&#24230;&#65292;&#20854;&#30446;&#26631;&#26159;&#25214;&#21040;&#36866;&#24403;&#30340;&#30697;&#38453;Y&#65292;&#20351;&#24471;&#20854;&#28385;&#36275;&#29305;&#23450;&#26465;&#20214;&#65292;&#20174;&#32780;&#24110;&#21161;&#35299;&#20915;&#27880;&#24847;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#23454;&#21147;&#12290;&#20316;&#20026;LLMs&#30340;&#19968;&#20010;&#37325;&#35201;&#23376;&#20363;&#31243;&#65292;&#27880;&#24847;&#21147;&#35745;&#31639;&#20063;&#24341;&#36215;&#20102;&#29702;&#35770;&#19978;&#30340;&#20852;&#36259;&#12290;&#26368;&#36817;&#65292;[Alman and Song 2023]&#21644;[Brand&#65292;Song and Zhou 2023]&#20174;&#31639;&#27861;&#21644;&#22256;&#38590;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#27880;&#24847;&#30697;&#38453;&#30340;&#38745;&#24577;&#35745;&#31639;&#21644;&#21160;&#24577;&#32500;&#25252;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#27880;&#24847;&#38382;&#39064;&#30340;&#31232;&#30095;&#21270;&#12290;&#25105;&#20204;&#20570;&#20102;&#19968;&#20010;&#31616;&#21270;&#65292;&#21363;logit&#30697;&#38453;&#26159;&#23545;&#31216;&#30340;&#12290;&#20551;&#35774;$n$&#34920;&#31034;&#21477;&#23376;&#38271;&#24230;&#65292;$d$&#34920;&#31034;&#23884;&#20837;&#32500;&#24230;&#12290;&#32473;&#23450;&#19968;&#20010;&#30697;&#38453;$X \in \mathbb{R} ^{n \times d}$&#65292;&#20551;&#35774;$d \gg n$&#19988;$\| X X^\top \|_{\infty} &lt; r$&#65292;&#20854;&#20013;$r \in (0,0.1)$&#65292;&#21017;&#25105;&#20204;&#26088;&#22312;&#25214;&#21040;$Y \in \mathbb{R}^{n \times m}$&#65288;&#20854;&#20013;$m\ll d$&#65289;, &#20351;&#24471; $D(Y)^{-1} \exp(Y Y ^\top) D(X)^{-1}\exp(X X ^\top)$&#30340;$\| \_\|_{\infty}$&#33539;&#25968;$\leq O&#65288;r&#65289;$&#12290;&#25105;&#20204;&#20026;&#36825;&#20010;&#38382;&#39064;&#25552;&#20379;&#20102;&#20004;&#20010;&#32467;&#26524;&#12290;$\bullet$&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#32467;&#26524;&#26159;&#19968;&#20010;&#38543;&#26426;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown their power in different areas. Attention computation, as an important subroutine of LLMs, has also attracted interests in theory. Recently the static computation and dynamic maintenance of attention matrix has been studied by [Alman and Song 2023] and [Brand, Song and Zhou 2023] from both algorithmic perspective and hardness perspective. In this work, we consider the sparsification of the attention problem. We make one simplification which is the logit matrix is symmetric. Let $n$ denote the length of sentence, let $d$ denote the embedding dimension. Given a matrix $X \in \mathbb{R}^{n \times d}$, suppose $d \gg n$ and $\| X X^\top \|_{\infty} &lt; r$ with $r \in (0,0.1)$, then we aim for finding $Y \in \mathbb{R}^{n \times m}$ (where $m\ll d$) such that \begin{align*} \| D(Y)^{-1} \exp( Y Y^\top ) D(X)^{-1} \exp( X X^\top) \|_{\infty} \leq O(r) \end{align*} We provide two results for this problem.  $\bullet$ Our first result is a randomized algo
&lt;/p&gt;</description></item><item><title>CAFIN&#26159;&#19968;&#31181;&#22522;&#20110;&#33410;&#28857;&#20013;&#24515;&#24615;&#30340;&#20844;&#24179;&#24615;&#22686;&#24378;&#36827;&#31243;&#25216;&#26415;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CAFIN&#22312;&#25552;&#20379;&#26368;&#20248;&#20844;&#24179;&#32467;&#26524;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#31454;&#20105;&#21147;&#25110;&#26356;&#22909;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.04391</link><description>&lt;p&gt;
CAFIN: &#22522;&#20110;&#33410;&#28857;&#20013;&#24515;&#24615;&#30340;&#20844;&#24179;&#24615;&#22686;&#24378;&#36827;&#31243;&#30340;&#26080;&#30417;&#30563;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CAFIN: Centrality Aware Fairness inducing IN-processing for Unsupervised Representation Learning on Graphs. (arXiv:2304.04391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04391
&lt;/p&gt;
&lt;p&gt;
CAFIN&#26159;&#19968;&#31181;&#22522;&#20110;&#33410;&#28857;&#20013;&#24515;&#24615;&#30340;&#20844;&#24179;&#24615;&#22686;&#24378;&#36827;&#31243;&#25216;&#26415;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CAFIN&#22312;&#25552;&#20379;&#26368;&#20248;&#20844;&#24179;&#32467;&#26524;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#31454;&#20105;&#21147;&#25110;&#26356;&#22909;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25152;&#23398;&#23884;&#20837;&#30340;&#32039;&#20945;&#24615;&#21644;&#20016;&#23500;&#24615;&#20197;&#21450;&#26410;&#26631;&#35760;&#22270;&#25968;&#25454;&#30340;&#20016;&#23500;&#24615;&#65292;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#22270;&#34920;&#31034;&#22312;(&#22823;&#22411;)&#22270;&#19978;&#24050;&#32463;&#21463;&#21040;&#30740;&#31350;&#30028;&#30340;&#37325;&#35270;&#12290;&#24403;&#36825;&#20123;&#33410;&#28857;&#34920;&#31034;&#34987;&#37096;&#32626;&#26102;&#65292;&#24517;&#39035;&#20351;&#29992;&#36866;&#24403;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#26465;&#20214;&#29983;&#25104;&#20197;&#20943;&#23569;&#23427;&#20204;&#23545;&#19979;&#28216;&#20219;&#21153;&#36896;&#25104;&#30340;&#20559;&#24046;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#24050;&#32463;&#35843;&#26597;&#20102;&#22270;&#23398;&#20064;&#31639;&#27861;&#30340;&#32676;&#20307;&#21644;&#20010;&#20307;&#20844;&#24179;&#24615;&#27010;&#24565;&#12290;&#36825;&#20123;&#20844;&#24179;&#24615;&#27010;&#24565;&#30340;&#20027;&#35201;&#23616;&#38480;&#24615;&#26159;&#27809;&#26377;&#32771;&#34385;&#36830;&#25509;&#27169;&#24335;&#22312;&#22270;&#20013;&#23548;&#33268;&#30340;&#19981;&#21516;&#33410;&#28857;&#24433;&#21709;(&#25110;&#20013;&#24515;&#24615;&#33021;&#37327;)&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#24402;&#32435;&#22270;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#20013;&#24515;&#24615;&#30340;&#20844;&#24179;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CAFIN&#65288;Centrality Aware Fairness inducing IN-processing&#65289;&#65292;&#19968;&#31181;&#21033;&#29992;&#22270;&#32467;&#26500;&#25913;&#36827;GraphSAGE&#34920;&#31034;&#30340;&#36827;&#31243;&#25216;&#26415;&#8212;&#8212;&#26080;&#30417;&#30563;&#22270;&#23398;&#20064;&#25991;&#29486;&#20013;&#30340;&#19968;&#31181;&#27969;&#34892;&#26694;&#26550;&#12290;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;CAFIN&#22312;&#25552;&#20379;&#20855;&#26377;&#31454;&#20105;&#21147;&#25110;&#26356;&#22909;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20844;&#24179;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised representation learning on (large) graphs has received significant attention in the research community due to the compactness and richness of the learned embeddings and the abundance of unlabelled graph data. When deployed, these node representations must be generated with appropriate fairness constraints to minimize bias induced by them on downstream tasks. Consequently, group and individual fairness notions for graph learning algorithms have been investigated for specific downstream tasks. One major limitation of these fairness notions is that they do not consider the connectivity patterns in the graph leading to varied node influence (or centrality power). In this paper, we design a centrality-aware fairness framework for inductive graph representation learning algorithms. We propose CAFIN (Centrality Aware Fairness inducing IN-processing), an in-processing technique that leverages graph structure to improve GraphSAGE's representations - a popular framework in the unsup
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#22312;&#28508;&#31354;&#38388;&#20013;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#27880;&#20837;&#23545;&#25239;&#25200;&#21160;&#26469;&#29983;&#25104;&#23545;&#25239;&#25915;&#20987;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#29983;&#25104;&#28508;&#31354;&#38388;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#26102;&#19982;&#20687;&#32032;&#31354;&#38388;&#20013;&#22522;&#20110;&#23545;&#25239;&#25915;&#20987;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#21516;&#31561;&#30340;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#24230;&#30340;&#35270;&#35273;&#30495;&#23454;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.04386</link><description>&lt;p&gt;
&#22312;&#28508;&#31354;&#38388;&#20013;&#29983;&#25104;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Generating Adversarial Attacks in the Latent Space. (arXiv:2304.04386v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04386
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#22312;&#28508;&#31354;&#38388;&#20013;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#27880;&#20837;&#23545;&#25239;&#25200;&#21160;&#26469;&#29983;&#25104;&#23545;&#25239;&#25915;&#20987;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#29983;&#25104;&#28508;&#31354;&#38388;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#26102;&#19982;&#20687;&#32032;&#31354;&#38388;&#20013;&#22522;&#20110;&#23545;&#25239;&#25915;&#20987;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#21516;&#31561;&#30340;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#24230;&#30340;&#35270;&#35273;&#30495;&#23454;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#27880;&#20837;&#22122;&#38899;&#26469;&#29983;&#25104;&#23545;&#25239;&#25915;&#20987;&#36890;&#24120;&#38656;&#35201;$L_1$&#25110;$L_{\infty}$&#33539;&#25968;&#31561;&#22122;&#38899;&#30028;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;&#28508;&#31354;&#38388;&#20013;&#27880;&#20837;&#23545;&#25239;&#25200;&#21160;&#65292;&#36991;&#20813;&#20102;&#22522;&#20110;&#30028;&#38480;&#30340;&#20808;&#39564;&#12290;&#22312;MNIST&#12289;CIFAR10&#12289;Fashion-MNIST&#12289;CIFAR100&#21644;Stanford Dogs&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#29983;&#25104;&#28508;&#31354;&#38388;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#26102;&#20855;&#26377;&#24456;&#39640;&#30340;&#35270;&#35273;&#30495;&#23454;&#24230;&#21644;&#20687;&#32032;&#31354;&#38388;&#20013;&#22522;&#20110;&#23545;&#25239;&#25915;&#20987;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#21516;&#31561;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks in the input (pixel) space typically incorporate noise margins such as $L_1$ or $L_{\infty}$-norm to produce imperceptibly perturbed data that confound deep learning networks. Such noise margins confine the magnitude of permissible noise. In this work, we propose injecting adversarial perturbations in the latent (feature) space using a generative adversarial network, removing the need for margin-based priors. Experiments on MNIST, CIFAR10, Fashion-MNIST, CIFAR100 and Stanford Dogs datasets support the effectiveness of the proposed method in generating adversarial attacks in the latent space while ensuring a high degree of visual realism with respect to pixel-based adversarial attack methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#40065;&#26834;&#24615;&#26694;&#26550;&#65292;&#20998;&#26512;&#20102;&#22810;&#31181;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#24178;&#39044;&#25216;&#26415;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;1.5&#20493;&#33267;4&#20493;&#30340;&#40065;&#26834;&#24615;&#25552;&#39640;&#65292;&#21516;&#26102;&#22312;AudioSet 20K&#19978;&#23454;&#29616;&#20102;44.2 mAP&#30340;&#26377;&#31454;&#20105;&#21147;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.04385</link><description>&lt;p&gt;
&#20851;&#20110;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
On Robustness in Multimodal Learning. (arXiv:2304.04385v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#40065;&#26834;&#24615;&#26694;&#26550;&#65292;&#20998;&#26512;&#20102;&#22810;&#31181;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#24178;&#39044;&#25216;&#26415;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;1.5&#20493;&#33267;4&#20493;&#30340;&#40065;&#26834;&#24615;&#25552;&#39640;&#65292;&#21516;&#26102;&#22312;AudioSet 20K&#19978;&#23454;&#29616;&#20102;44.2 mAP&#30340;&#26377;&#31454;&#20105;&#21147;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#34987;&#23450;&#20041;&#20026;&#23545;&#22810;&#31181;&#24322;&#26500;&#36755;&#20837;&#27169;&#24577;&#65288;&#22914;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#25991;&#26412;&#31561;&#65289;&#36827;&#34892;&#23398;&#20064;&#12290;&#26412;&#25991;&#20851;&#27880;&#20102;&#35299;&#24403;&#35757;&#32451;&#21644;&#37096;&#32626;&#20043;&#38388;&#30340;&#27169;&#24577;&#31867;&#22411;&#19981;&#21516;&#26102;&#65292;&#27169;&#22411;&#22914;&#20309;&#34920;&#29616;&#65292;&#36825;&#22312;&#35768;&#22810;&#24212;&#29992;&#22810;&#27169;&#24577;&#23398;&#20064;&#24212;&#29992;&#20110;&#30828;&#20214;&#24179;&#21488;&#26102;&#20250;&#33258;&#28982;&#21457;&#29983;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#40065;&#26834;&#24615;&#26694;&#26550;&#26469;&#31995;&#32479;&#20998;&#26512;&#24120;&#35265;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#24178;&#39044;&#25216;&#26415;&#65292;&#22312;AudioSet&#12289;Kinetics-400&#21644;ImageNet-Captions&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;1.5&#20493;&#33267;4&#20493;&#30340;&#40065;&#26834;&#24615;&#25552;&#39640;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#24178;&#39044;&#25216;&#26415;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#39069;&#22806;&#30340;&#27169;&#24577;&#65292;&#22312;AudioSet 20K&#19978;&#23454;&#29616;&#20102;44.2 mAP&#30340;&#26377;&#31454;&#20105;&#21147;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal learning is defined as learning over multiple heterogeneous input modalities such as video, audio, and text. In this work, we are concerned with understanding how models behave as the type of modalities differ between training and deployment, a situation that naturally arises in many applications of multimodal learning to hardware platforms. We present a multimodal robustness framework to provide a systematic analysis of common multimodal representation learning methods. Further, we identify robustness short-comings of these approaches and propose two intervention techniques leading to $1.5\times$-$4\times$ robustness improvements on three datasets, AudioSet, Kinetics-400 and ImageNet-Captions. Finally, we demonstrate that these interventions better utilize additional modalities, if present, to achieve competitive results of $44.2$ mAP on AudioSet 20K.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;OpenAGI&#24179;&#21488;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#24418;&#24335;&#65292;&#23454;&#29616;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2304.04370</link><description>&lt;p&gt;
OpenAGI&#65306;&#24403;LLM&#36935;&#21040;&#39046;&#22495;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04370
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;OpenAGI&#24179;&#21488;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#24418;&#24335;&#65292;&#23454;&#29616;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20855;&#26377;&#23558;&#22522;&#26412;&#25216;&#33021;&#32452;&#21512;&#25104;&#22797;&#26434;&#25216;&#33021;&#20197;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#36825;&#31181;&#33021;&#21147;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#21516;&#26679;&#37325;&#35201;&#65292;&#22240;&#27492;&#65292;&#25105;&#20204;&#26029;&#35328;&#65292;&#38500;&#20102;&#24320;&#21457;&#22823;&#22411;&#32508;&#21512;&#26234;&#33021;&#27169;&#22411;&#22806;&#65292;&#23558;&#19981;&#21516;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#24212;&#29992;&#20110;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#33021;&#21147;&#21516;&#26679;&#20851;&#38190;&#65292;&#20197;&#22312;&#20154;&#24037;&#26234;&#33021;&#36890;&#29992;&#26234;&#33021;&#30340;&#36861;&#27714;&#20013;&#20351;&#20854;&#20855;&#22791;&#36825;&#31181;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#35777;&#26126;&#20854;&#20855;&#26377;&#20986;&#33394;&#30340;&#23398;&#20064;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#25104;&#20026;&#36873;&#25321;&#12289;&#32508;&#21512;&#21644;&#25191;&#34892;&#22806;&#37096;&#27169;&#22411;&#20197;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#25511;&#21046;&#22120;&#30340;&#26377;&#21069;&#36884;&#30340;&#36873;&#25321;&#12290;&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;OpenAGI&#30340;&#24320;&#28304;AGI&#30740;&#31350;&#24179;&#21488;&#65292;&#19987;&#38376;&#35774;&#35745;&#20026;&#25552;&#20379;&#22797;&#26434;&#30340;&#22810;&#27493;&#39588;&#20219;&#21153;&#65292;&#24182;&#37197;&#26377;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#21508;&#31181;&#21487;&#25193;&#23637;&#27169;&#22411;&#12290;OpenAGI&#23558;&#22797;&#26434;&#20219;&#21153;&#38416;&#37322;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#65292;&#26088;&#22312;&#20419;&#36827;&#39046;&#22495;&#19987;&#23478;&#21644;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human intelligence has the remarkable ability to assemble basic skills into complex ones so as to solve complex tasks. This ability is equally important for Artificial Intelligence (AI), and thus, we assert that in addition to the development of large, comprehensive intelligent models, it is equally crucial to equip such models with the capability to harness various domain-specific expert models for complex task-solving in the pursuit of Artificial General Intelligence (AGI). Recent developments in Large Language Models (LLMs) have demonstrated remarkable learning and reasoning abilities, making them promising as a controller to select, synthesize, and execute external models to solve complex tasks. In this project, we develop OpenAGI, an open-source AGI research platform, specifically designed to offer complex, multi-step tasks and accompanied by task-specific datasets, evaluation metrics, and a diverse range of extensible models. OpenAGI formulates complex tasks as natural language q
&lt;/p&gt;</description></item><item><title>Eagle &#26159;&#19968;&#20010;&#22522;&#20110;&#31471;&#21040;&#31471;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#36924;&#30495;&#30340;&#27169;&#25311;&#26694;&#26550;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#31574;&#30053;&#65292;&#30452;&#25509;&#20197;&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#25511;&#21046; PTZ &#25668;&#20687;&#22836;&#65292;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#25668;&#20687;&#22836;&#25511;&#21046;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.04356</link><description>&lt;p&gt;
Eagle: &#22522;&#20110;&#31471;&#21040;&#31471;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340; PTZ &#25668;&#20687;&#22836;&#33258;&#20027;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Eagle: End-to-end Deep Reinforcement Learning based Autonomous Control of PTZ Cameras. (arXiv:2304.04356v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04356
&lt;/p&gt;
&lt;p&gt;
Eagle &#26159;&#19968;&#20010;&#22522;&#20110;&#31471;&#21040;&#31471;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#36924;&#30495;&#30340;&#27169;&#25311;&#26694;&#26550;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#31574;&#30053;&#65292;&#30452;&#25509;&#20197;&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#25511;&#21046; PTZ &#25668;&#20687;&#22836;&#65292;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#25668;&#20687;&#22836;&#25511;&#21046;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#29992;&#20110; PTZ &#25668;&#20687;&#22836;&#33258;&#20027;&#25511;&#21046;&#30340;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#22810;&#20010;&#38454;&#27573;&#65292;&#20854;&#20013;&#30446;&#26631;&#26816;&#27979;&#21644;&#23450;&#20301;&#21333;&#29420;&#20110; PTZ &#26426;&#21046;&#30340;&#25511;&#21046;&#12290;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#25163;&#21160;&#26631;&#27880;&#65292;&#24182;&#30001;&#20110;&#20449;&#24687;&#27969;&#37327;&#30340;&#22810;&#38454;&#27573;&#20256;&#36882;&#23548;&#33268;&#24615;&#33021;&#29942;&#39048;&#12290;&#30446;&#26631;&#26816;&#27979;&#31070;&#32463;&#32593;&#32476;&#30340;&#22823;&#23610;&#23544;&#20063;&#20351;&#24471;&#20043;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#30340;&#23454;&#26102;&#37096;&#32626;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31471;&#21040;&#31471;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#35299;&#20915;&#26041;&#26696;&#21517;&#20026; Eagle&#65292;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#31574;&#30053;&#65292;&#30452;&#25509;&#20197;&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#65292;&#25511;&#21046; PTZ &#25668;&#20687;&#22836;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#35757;&#32451;&#22240;&#26631;&#27880;&#24037;&#20316;&#37327;&#12289;&#36816;&#34892;&#29615;&#22659;&#38543;&#26426;&#24615;&#21644;&#26131;&#25439;&#23454;&#39564;&#35774;&#32622;&#32780;&#21464;&#24471;&#22797;&#26434;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36924;&#30495;&#30340;&#27169;&#25311;&#26694;&#26550;&#65292;&#20197;&#35757;&#32451;&#21644;&#35780;&#20272; PTZ &#25668;&#20687;&#22836;&#25511;&#21046;&#31574;&#30053;&#12290;Eagle &#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#25668;&#20687;&#22836;&#25511;&#21046;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing approaches for autonomous control of pan-tilt-zoom (PTZ) cameras use multiple stages where object detection and localization are performed separately from the control of the PTZ mechanisms. These approaches require manual labels and suffer from performance bottlenecks due to error propagation across the multi-stage flow of information. The large size of object detection neural networks also makes prior solutions infeasible for real-time deployment in resource-constrained devices. We present an end-to-end deep reinforcement learning (RL) solution called Eagle to train a neural network policy that directly takes images as input to control the PTZ camera. Training reinforcement learning is cumbersome in the real world due to labeling effort, runtime environment stochasticity, and fragile experimental setups. We introduce a photo-realistic simulation framework for training and evaluation of PTZ camera control policies. Eagle achieves superior camera control performance by maintain
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#21327;&#35758;&#39044;&#27979;&#37327;&#23376;&#22810;&#20307;&#31995;&#32479;&#30340;&#22522;&#24577;&#21450;&#20854;&#24615;&#36136;&#65292;&#20854;&#31934;&#24230;&#20026; $\varepsilon$&#65292;&#24182;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#20445;&#38556;&#65307;&#20294;&#23545;&#20110;&#26222;&#36941;&#30340;&#33021;&#38553;&#21704;&#23494;&#39039;&#37327;&#65292;&#26679;&#26412;&#20010;&#25968; $N = m^{{\cal{O}} \left(\frac{1}{\varepsilon}\right)}$&#65292;&#21482;&#36866;&#29992;&#20110;&#21442;&#25968;&#31354;&#38388;&#32500;&#24230;&#36739;&#22823;&#65292;&#19988;&#31934;&#24230;&#19981;&#26159;&#32039;&#36843;&#22240;&#32032;&#65292;&#26080;&#27861;&#36827;&#20837;&#26356;&#31934;&#30830;&#30340;&#23398;&#20064;&#21644;&#39044;&#27979;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2304.04353</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#37327;&#23376;&#22810;&#20307;&#24577;&#30340;&#26426;&#22120;&#23398;&#20064;&#26174;&#33879;&#25552;&#39640;&#25928;&#29575;&#24182;&#20855;&#26377;&#21487;&#35777;&#26126;&#20445;&#38556;
&lt;/p&gt;
&lt;p&gt;
Exponentially Improved Efficient Machine Learning for Quantum Many-body States with Provable Guarantees. (arXiv:2304.04353v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04353
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#21327;&#35758;&#39044;&#27979;&#37327;&#23376;&#22810;&#20307;&#31995;&#32479;&#30340;&#22522;&#24577;&#21450;&#20854;&#24615;&#36136;&#65292;&#20854;&#31934;&#24230;&#20026; $\varepsilon$&#65292;&#24182;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#20445;&#38556;&#65307;&#20294;&#23545;&#20110;&#26222;&#36941;&#30340;&#33021;&#38553;&#21704;&#23494;&#39039;&#37327;&#65292;&#26679;&#26412;&#20010;&#25968; $N = m^{{\cal{O}} \left(\frac{1}{\varepsilon}\right)}$&#65292;&#21482;&#36866;&#29992;&#20110;&#21442;&#25968;&#31354;&#38388;&#32500;&#24230;&#36739;&#22823;&#65292;&#19988;&#31934;&#24230;&#19981;&#26159;&#32039;&#36843;&#22240;&#32032;&#65292;&#26080;&#27861;&#36827;&#20837;&#26356;&#31934;&#30830;&#30340;&#23398;&#20064;&#21644;&#39044;&#27979;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#32463;&#20856;&#31639;&#27861;&#32780;&#35328;&#65292;&#35299;&#20915;&#37327;&#23376;&#22810;&#20307;&#31995;&#32479;&#30340;&#22522;&#24577;&#21450;&#20854;&#24615;&#36136;&#36890;&#24120;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#12290;&#23545;&#20110;&#23450;&#20041;&#22312;&#29289;&#29702;&#21442;&#25968; $m$ &#32500;&#31354;&#38388;&#19978;&#30340;&#21704;&#23494;&#39039;&#37327;&#26063;&#65292;&#21482;&#35201;&#21487;&#20197;&#39640;&#25928;&#22320;&#20934;&#22791;&#21644;&#27979;&#37327;&#19968;&#32452; $N$ &#20010;&#24577;&#65292;&#23601;&#21487;&#20197;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#21327;&#35758;&#39044;&#27979;&#20854;&#22522;&#24577;&#21450;&#20854;&#22312;&#20219;&#24847;&#21442;&#25968;&#37197;&#32622;&#19979;&#30340;&#24615;&#36136;&#65292;&#31934;&#24230;&#20026; $\varepsilon$&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350; [Huang &#31561;&#20154;&#65292;Science 377&#65292;eabk3333&#65288;2022&#65289;] &#23545;&#36825;&#31181;&#19968;&#33324;&#21270;&#25552;&#20986;&#20102;&#20005;&#26684;&#30340;&#20445;&#38556;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#23545;&#20110;&#26222;&#36941;&#30340;&#33021;&#38553;&#21704;&#23494;&#39039;&#37327;&#65292;&#26222;&#36866;&#30340;&#25351;&#25968;&#32553;&#25918;&#20026; $N = m^{{\cal{O}} \left(\frac{1}{\varepsilon}\right)}$&#65292;&#36825;&#20010;&#32467;&#26524;&#20165;&#36866;&#29992;&#20110;&#21442;&#25968;&#31354;&#38388;&#30340;&#32500;&#24230;&#36739;&#22823;&#65292;&#32780;&#31934;&#24230;&#30340;&#32553;&#25918;&#21017;&#19981;&#26159;&#19968;&#20010;&#32039;&#36843;&#30340;&#22240;&#32032;&#65292;&#19981;&#33021;&#36827;&#20837;&#26356;&#31934;&#30830;&#30340;&#23398;&#20064;&#21644;&#39044;&#27979;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving the ground state and the ground-state properties of quantum many-body systems is generically a hard task for classical algorithms. For a family of Hamiltonians defined on an $m$-dimensional space of physical parameters, the ground state and its properties at an arbitrary parameter configuration can be predicted via a machine learning protocol up to a prescribed prediction error $\varepsilon$, provided that a sample set (of size $N$) of the states can be efficiently prepared and measured. In a recent work [Huang et al., Science 377, eabk3333 (2022)], a rigorous guarantee for such an generalization was proved. Unfortunately, an exponential scaling, $N = m^{ {\cal{O}} \left(\frac{1}{\varepsilon} \right) }$, was found to be universal for generic gapped Hamiltonians. This result applies to the situation where the dimension of the parameter space is large while the scaling with the accuracy is not an urgent factor, not entering the realm of more precise learning and prediction. In th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#22270;&#20687;&#32534;&#36753;&#31639;&#27861;&#65292;&#21487;&#26681;&#25454;&#25552;&#20379;&#30340;&#25991;&#26412;&#25551;&#36848;&#24555;&#36895;&#23398;&#20064;&#21644;&#24212;&#29992;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#25805;&#20316;&#65292;&#27604;&#26087;&#31639;&#27861;&#23398;&#20064;&#22270;&#20687;&#25805;&#20316;&#26356;&#24555;4.5-10&#20493;&#65292;&#24212;&#29992;&#26356;&#24555;&#36895;&#30340;8&#20493;&#12290;</title><link>http://arxiv.org/abs/2304.04344</link><description>&lt;p&gt;
&#22522;&#20110;&#26080;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#23454;&#26102;&#25991;&#26412;&#39537;&#21160;&#22270;&#20687;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Towards Real-time Text-driven Image Manipulation with Unconditional Diffusion Models. (arXiv:2304.04344v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#22270;&#20687;&#32534;&#36753;&#31639;&#27861;&#65292;&#21487;&#26681;&#25454;&#25552;&#20379;&#30340;&#25991;&#26412;&#25551;&#36848;&#24555;&#36895;&#23398;&#20064;&#21644;&#24212;&#29992;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#25805;&#20316;&#65292;&#27604;&#26087;&#31639;&#27861;&#23398;&#20064;&#22270;&#20687;&#25805;&#20316;&#26356;&#24555;4.5-10&#20493;&#65292;&#24212;&#29992;&#26356;&#24555;&#36895;&#30340;8&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#20026;&#22270;&#20687;&#32534;&#36753;&#25552;&#20379;&#20102;&#35768;&#22810;&#24378;&#22823;&#30340;&#24037;&#20855;&#12290;&#20854;&#20013;&#20043;&#19968;&#26159;&#25991;&#26412;&#39537;&#21160;&#30340;&#22270;&#20687;&#25805;&#20316;&#65306;&#26681;&#25454;&#25552;&#20379;&#30340;&#25991;&#26412;&#25551;&#36848;&#32534;&#36753;&#22270;&#20687;&#30340;&#35821;&#20041;&#23646;&#24615;&#12290;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#20026;&#24191;&#27867;&#30340;&#25991;&#26412;&#25552;&#31034;&#25552;&#20379;&#20102;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#25805;&#20316;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#21363;&#20351;&#20351;&#29992;&#39640;&#31471;GPU&#65292;&#36825;&#20123;&#26041;&#27861;&#20063;&#38656;&#35201;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#36825;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#25193;&#25955;&#27169;&#22411;&#22270;&#20687;&#32534;&#36753;&#22312;&#28508;&#22312;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#29992;&#25143;&#35774;&#22791;&#19978;&#36816;&#34892;&#26102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22522;&#20110;&#26080;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#36817;&#30340;&#25991;&#26412;&#39537;&#21160;&#32534;&#36753;&#26041;&#27861;&#30340;&#25928;&#29575;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#27604;&#26087;&#31639;&#27861;&#23398;&#20064;&#22270;&#20687;&#25805;&#20316;4.5-10&#20493;&#30340;&#26356;&#24555;&#65292;&#24182;&#19988;&#24212;&#29992;&#26356;&#24555;&#36895;&#30340;8&#20493;&#12290;&#25105;&#20204;&#20180;&#32454;&#35780;&#20272;&#20102;&#35270;&#35273;&#36136;&#37327;&#21644;&#34920;&#29616;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in diffusion models enable many powerful instruments for image editing. One of these instruments is text-driven image manipulations: editing semantic attributes of an image according to the provided text description. % Popular text-conditional diffusion models offer various high-quality image manipulation methods for a broad range of text prompts. Existing diffusion-based methods already achieve high-quality image manipulations for a broad range of text prompts. However, in practice, these methods require high computation costs even with a high-end GPU. This greatly limits potential real-world applications of diffusion-based image editing, especially when running on user devices.  In this paper, we address efficiency of the recent text-driven editing methods based on unconditional diffusion models and develop a novel algorithm that learns image manipulations 4.5-10 times faster and applies them 8 times faster. We carefully evaluate the visual quality and expressiveness 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21487;&#35777;&#40657;&#30418;&#25915;&#20987;&#65292;&#33021;&#22815;&#20445;&#35777;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#35774;&#35745;&#20102;&#22810;&#31181;&#26032;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2304.04343</link><description>&lt;p&gt;
&#21487;&#35777;&#40657;&#30418;&#25915;&#20987;&#65306;&#30830;&#20445;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;
&lt;/p&gt;
&lt;p&gt;
Certifiable Black-Box Attack: Ensuring Provably Successful Attack for Adversarial Examples. (arXiv:2304.04343v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21487;&#35777;&#40657;&#30418;&#25915;&#20987;&#65292;&#33021;&#22815;&#20445;&#35777;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#35774;&#35745;&#20102;&#22810;&#31181;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;&#20855;&#26377;&#30772;&#22351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24378;&#22823;&#28508;&#21147;&#12290;&#29616;&#26377;&#30340;&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;&#36890;&#36807;&#36845;&#20195;&#26597;&#35810;&#30446;&#26631;&#27169;&#22411;&#21644;/&#25110;&#21033;&#29992;&#26412;&#22320;&#20195;&#29702;&#27169;&#22411;&#30340;&#21487;&#36716;&#31227;&#24615;&#26469;&#21046;&#20316;&#23545;&#25239;&#26679;&#26412;&#12290;&#24403;&#23454;&#39564;&#35774;&#35745;&#25915;&#20987;&#26102;&#65292;&#25915;&#20987;&#26159;&#21542;&#25104;&#21151;&#23545;&#25915;&#20987;&#32773;&#26469;&#35828;&#20173;&#28982;&#26159;&#26410;&#30693;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#20462;&#25913;&#38543;&#26426;&#24179;&#28369;&#24615;&#29702;&#35770;&#65292;&#39318;&#27425;&#30740;&#31350;&#20102;&#21487;&#35777;&#40657;&#30418;&#25915;&#20987;&#30340;&#26032;&#33539;&#20363;&#65292;&#33021;&#22815;&#20445;&#35777;&#21046;&#20316;&#30340;&#23545;&#25239;&#26679;&#26412;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#20026;&#27492;&#35774;&#35745;&#20102;&#22810;&#31181;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Black-box adversarial attacks have shown strong potential to subvert machine learning models. Existing black-box adversarial attacks craft the adversarial examples by iteratively querying the target model and/or leveraging the transferability of a local surrogate model. Whether such attack can succeed remains unknown to the adversary when empirically designing the attack. In this paper, to our best knowledge, we take the first step to study a new paradigm of adversarial attacks -- certifiable black-box attack that can guarantee the attack success rate of the crafted adversarial examples. Specifically, we revise the randomized smoothing to establish novel theories for ensuring the attack success rate of the adversarial examples. To craft the adversarial examples with the certifiable attack success rate (CASR) guarantee, we design several novel techniques, including a randomized query method to query the target model, an initialization method with smoothed self-supervised perturbation to
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#20013;&#65292;&#22914;&#20309;&#22312;&#26399;&#26395;&#21644;&#23614;&#37096;&#39118;&#38505;&#20043;&#38388;&#20570;&#20986;&#26368;&#20248;&#26435;&#34913;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#33021;&#22815;&#23454;&#29616;&#26368;&#22351;&#21644;&#23454;&#20363;&#30456;&#20851;&#30340;&#20248;&#24322;&#34920;&#29616;&#65292;&#24182;&#19988;&#33021;&#22815;&#26368;&#23567;&#21270;&#36951;&#25022;&#23614;&#37096;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.04341</link><description>&lt;p&gt;
&#38543;&#26426;&#36172;&#21338;&#26426;&#20013;&#30340;&#36951;&#25022;&#20998;&#24067;&#65306;&#26399;&#26395;&#21644;&#23614;&#37096;&#39118;&#38505;&#20043;&#38388;&#30340;&#26368;&#20248;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Regret Distribution in Stochastic Bandits: Optimal Trade-off between Expectation and Tail Risk. (arXiv:2304.04341v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04341
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#20013;&#65292;&#22914;&#20309;&#22312;&#26399;&#26395;&#21644;&#23614;&#37096;&#39118;&#38505;&#20043;&#38388;&#20570;&#20986;&#26368;&#20248;&#26435;&#34913;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#33021;&#22815;&#23454;&#29616;&#26368;&#22351;&#21644;&#23454;&#20363;&#30456;&#20851;&#30340;&#20248;&#24322;&#34920;&#29616;&#65292;&#24182;&#19988;&#33021;&#22815;&#26368;&#23567;&#21270;&#36951;&#25022;&#23614;&#37096;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#20013;&#65292;&#36951;&#25022;&#20998;&#24067;&#30340;&#26399;&#26395;&#21644;&#23614;&#37096;&#39118;&#38505;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#25105;&#20204;&#23436;&#20840;&#21051;&#30011;&#20102;&#31574;&#30053;&#35774;&#35745;&#20013;&#19977;&#20010;&#26399;&#26395;&#24615;&#36136;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65306;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#26368;&#20248;&#24615;&#65292;&#23454;&#20363;&#30456;&#20851;&#30340;&#19968;&#33268;&#24615;&#21644;&#36731;&#23614;&#39118;&#38505;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26399;&#26395;&#36951;&#25022;&#30340;&#39034;&#24207;&#22914;&#20309;&#24433;&#21709;&#36951;&#25022;&#23614;&#37096;&#27010;&#29575;&#30340;&#34928;&#20943;&#29575;&#65292;&#21516;&#26102;&#21253;&#25324;&#20102;&#26368;&#22351;&#24773;&#20917;&#21644;&#23454;&#20363;&#30456;&#20851;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#20197;&#34920;&#24449;&#23545;&#20110;&#20219;&#20309;&#36951;&#25022;&#38408;&#20540;&#30340;&#26368;&#20248;&#36951;&#25022;&#23614;&#37096;&#27010;&#29575;&#12290;&#20855;&#20307;&#22320;&#65292;&#23545;&#20110;&#20219;&#20309;&#32473;&#23450;&#30340;$\alpha \in [1/2, 1)$&#21644;$\beta \in [0, \alpha]$&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#21487;&#20197;&#23454;&#29616;&#24179;&#22343;&#26399;&#26395;&#36951;&#25022;$\tilde O(T^\alpha)$&#30340;&#26368;&#22351;&#24773;&#20917;&#19979;$\alpha$-&#26368;&#20248;&#21644;&#26399;&#26395;&#36951;&#25022;$\tilde O(T^\beta)$&#30340;&#23454;&#20363;&#30456;&#20851;&#30340;$\beta$-&#19968;&#33268;&#24615;&#65292;&#24182;&#19988;&#20139;&#26377;&#19968;&#23450;&#30340;&#27010;&#29575;&#21487;&#20197;&#36991;&#20813;$\tilde O(T^\delta)$&#30340;&#36951;&#25022;($\delta \geq \alpha$&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#21644;$\delta \geq \beta$&#22312;&#23454;&#20363;&#30456;&#20851;&#30340;&#24773;&#20917;&#19979;)&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the trade-off between expectation and tail risk for regret distribution in the stochastic multi-armed bandit problem. We fully characterize the interplay among three desired properties for policy design: worst-case optimality, instance-dependent consistency, and light-tailed risk. We show how the order of expected regret exactly affects the decaying rate of the regret tail probability for both the worst-case and instance-dependent scenario. A novel policy is proposed to characterize the optimal regret tail probability for any regret threshold. Concretely, for any given $\alpha\in[1/2, 1)$ and $\beta\in[0, \alpha]$, our policy achieves a worst-case expected regret of $\tilde O(T^\alpha)$ (we call it $\alpha$-optimal) and an instance-dependent expected regret of $\tilde O(T^\beta)$ (we call it $\beta$-consistent), while enjoys a probability of incurring an $\tilde O(T^\delta)$ regret ($\delta\geq\alpha$ in the worst-case scenario and $\delta\geq\beta$ in the instance-dependent s
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#39044;&#35757;&#32451;&#23884;&#20837;&#22312;&#30005;&#23376;&#21830;&#21153;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#23384;&#22312;&#35768;&#22810;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#30495;&#23454;&#29983;&#20135;&#31995;&#32479;&#20013;&#12290;&#23613;&#31649;&#39044;&#35757;&#32451;&#23884;&#20837;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#23427;&#30340;&#25104;&#21151;&#19982;&#19979;&#28216;&#27169;&#22411;&#30340;&#35774;&#35745;&#20197;&#21450;&#22914;&#20309;&#36890;&#36807;&#23884;&#20837;&#21521;&#37327;&#26469;&#32534;&#30721;&#21644;&#35299;&#30721;&#20449;&#24687;&#26377;&#30528;&#28145;&#21051;&#30340;&#32852;&#31995;&#21644;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2304.04330</link><description>&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#26426;&#22120;&#23398;&#20064;&#20013;&#39044;&#35757;&#32451;&#23884;&#20837;&#30340;&#22833;&#36133;&#19982;&#21407;&#22240;
&lt;/p&gt;
&lt;p&gt;
Pretrained Embeddings for E-commerce Machine Learning: When it Fails and Why?. (arXiv:2304.04330v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04330
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#39044;&#35757;&#32451;&#23884;&#20837;&#22312;&#30005;&#23376;&#21830;&#21153;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#23384;&#22312;&#35768;&#22810;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#30495;&#23454;&#29983;&#20135;&#31995;&#32479;&#20013;&#12290;&#23613;&#31649;&#39044;&#35757;&#32451;&#23884;&#20837;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#23427;&#30340;&#25104;&#21151;&#19982;&#19979;&#28216;&#27169;&#22411;&#30340;&#35774;&#35745;&#20197;&#21450;&#22914;&#20309;&#36890;&#36807;&#23884;&#20837;&#21521;&#37327;&#26469;&#32534;&#30721;&#21644;&#35299;&#30721;&#20449;&#24687;&#26377;&#30528;&#28145;&#21051;&#30340;&#32852;&#31995;&#21644;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#23884;&#20837;&#22312;&#29616;&#20195;&#30005;&#23376;&#21830;&#21153;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#21464;&#24471;&#26222;&#36941;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#24403;&#22312;&#30495;&#23454;&#30340;&#29983;&#20135;&#31995;&#32479;&#20013;&#24212;&#29992;&#39044;&#35757;&#32451;&#23884;&#20837;&#26102;&#65292;&#25105;&#20204;&#36935;&#21040;&#20102;&#20960;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#20854;&#20013;&#35768;&#22810;&#38382;&#39064;&#26080;&#27861;&#23436;&#20840;&#29992;&#24403;&#21069;&#30340;&#30693;&#35782;&#35299;&#37322;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#32570;&#20047;&#23545;&#39044;&#35757;&#32451;&#23884;&#20837;&#30340;&#24037;&#20316;&#26041;&#24335;&#26377;&#20840;&#38754;&#30340;&#20102;&#35299;&#65292;&#29305;&#21035;&#26159;&#23427;&#20204;&#30340;&#20869;&#22312;&#23646;&#24615;&#21644;&#19982;&#19979;&#28216;&#20219;&#21153;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#38590;&#20197;&#22312;&#23454;&#36341;&#20013;&#20570;&#20986;&#20132;&#20114;&#21644;&#21487;&#25193;&#23637;&#30340;&#20915;&#31574;&#65292;&#20197;&#20851;&#20110;&#39044;&#35757;&#32451;&#23884;&#20837;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26377;&#20004;&#20010;&#37325;&#35201;&#30340;&#21457;&#29616;&#65292;&#20851;&#20110;&#22312;&#30005;&#23376;&#21830;&#21153;&#24212;&#29992;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;&#23884;&#20837;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#29616;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#27169;&#22411;&#30340;&#35774;&#35745;&#65292;&#29305;&#21035;&#26159;&#23427;&#20204;&#22914;&#20309;&#36890;&#36807;&#23884;&#20837;&#21521;&#37327;&#32534;&#30721;&#21644;&#35299;&#30721;&#20449;&#24687;&#65292;&#21487;&#20197;&#20135;&#29983;&#28145;&#36828;&#30340;&#24433;&#21709;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#35299;&#20915;&#23884;&#20837;&#24615;&#36136;&#38382;&#39064;&#24314;&#31435;&#20102;&#39044;&#35757;&#32451;&#23884;&#20837;&#30340;&#21407;&#29702;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of pretrained embeddings has become widespread in modern e-commerce machine learning (ML) systems. In practice, however, we have encountered several key issues when using pretrained embedding in a real-world production system, many of which cannot be fully explained by current knowledge. Unfortunately, we find that there is a lack of a thorough understanding of how pre-trained embeddings work, especially their intrinsic properties and interactions with downstream tasks. Consequently, it becomes challenging to make interactive and scalable decisions regarding the use of pre-trained embeddings in practice.  Our investigation leads to two significant discoveries about using pretrained embeddings in e-commerce applications. Firstly, we find that the design of the pretraining and downstream models, particularly how they encode and decode information via embedding vectors, can have a profound impact. Secondly, we establish a principled perspective of pre-trained embeddings via the le
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20998;&#24067;&#20013;&#30693;&#35782;&#33976;&#39311;&#65288;IDKD&#65289;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#33410;&#28857;&#38388;&#21516;&#36136;&#21270;&#25968;&#25454;&#20998;&#24067;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20844;&#20849;&#25968;&#25454;&#38598;&#26469;&#20174;&#27599;&#20010;&#33410;&#28857;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#30340;&#26631;&#31614;&#23558;&#20854;&#20256;&#36882;&#32473;&#20854;&#37051;&#23621;&#65292;&#20197;&#23454;&#29616;&#30456;&#21516;&#30340;&#30446;&#26631;&#65292;&#21516;&#26102;&#20445;&#25345;&#38544;&#31169;&#32422;&#26463;&#12290;&#22312;&#38750;i.i.d.&#25968;&#25454;&#38598;&#19978;&#65292;IDKD&#30340;&#24615;&#33021;&#26174;&#30528;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#21516;&#26102;&#20445;&#25345;&#38544;&#31169;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.04326</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#24067;&#20013;&#30693;&#35782;&#33976;&#39311;&#30340;&#38750;IID&#25968;&#25454;&#21516;&#36136;&#21270;&#26041;&#27861;&#26469;&#36827;&#34892;&#20998;&#25955;&#24335;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Homogenizing Non-IID datasets via In-Distribution Knowledge Distillation for Decentralized Learning. (arXiv:2304.04326v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20998;&#24067;&#20013;&#30693;&#35782;&#33976;&#39311;&#65288;IDKD&#65289;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#33410;&#28857;&#38388;&#21516;&#36136;&#21270;&#25968;&#25454;&#20998;&#24067;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20844;&#20849;&#25968;&#25454;&#38598;&#26469;&#20174;&#27599;&#20010;&#33410;&#28857;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#30340;&#26631;&#31614;&#23558;&#20854;&#20256;&#36882;&#32473;&#20854;&#37051;&#23621;&#65292;&#20197;&#23454;&#29616;&#30456;&#21516;&#30340;&#30446;&#26631;&#65292;&#21516;&#26102;&#20445;&#25345;&#38544;&#31169;&#32422;&#26463;&#12290;&#22312;&#38750;i.i.d.&#25968;&#25454;&#38598;&#19978;&#65292;IDKD&#30340;&#24615;&#33021;&#26174;&#30528;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#21516;&#26102;&#20445;&#25345;&#38544;&#31169;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;&#24335;&#23398;&#20064;&#20801;&#35768;&#22312;&#22810;&#20010;&#33410;&#28857;&#19978;&#20197;&#20998;&#25955;&#24335;&#26041;&#24335;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#25968;&#25454;&#28304;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#20998;&#25955;&#24335;&#23398;&#20064;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#33410;&#28857;&#38388;&#25968;&#25454;&#20998;&#24067;&#30340;&#24322;&#36136;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20998;&#24067;&#20013;&#30693;&#35782;&#33976;&#39311;&#65288;IDKD&#65289;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#27492;&#25361;&#25112;&#12290;IDKD&#30340;&#30446;&#26631;&#26159;&#21516;&#36136;&#21270;&#33410;&#28857;&#20043;&#38388;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#34429;&#28982;&#36825;&#31181;&#25968;&#25454;&#21516;&#36136;&#21270;&#21487;&#20197;&#36890;&#36807;&#22312;&#33410;&#28857;&#20043;&#38388;&#20132;&#25442;&#25968;&#25454;&#26469;&#23454;&#29616;&#65292;&#20294;&#36825;&#26679;&#20250;&#29306;&#29298;&#38544;&#31169;&#12290;IDKD&#20351;&#29992;&#20844;&#20849;&#25968;&#25454;&#38598;&#26469;&#20174;&#27599;&#20010;&#33410;&#28857;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#30340;&#26631;&#31614;&#23558;&#20854;&#20256;&#36882;&#32473;&#20854;&#37051;&#23621;&#65292;&#20197;&#23454;&#29616;&#30456;&#21516;&#30340;&#30446;&#26631;&#65292;&#21516;&#26102;&#20445;&#25345;&#38544;&#31169;&#32422;&#26463;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;IDKD&#22312;&#38750;i.i.d.&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#26174;&#31034;&#23427;&#22312;&#21516;&#36136;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#26174;&#30528;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#21516;&#26102;&#20445;&#25345;&#38544;&#31169;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decentralized learning enables serverless training of deep neural networks (DNNs) in a distributed manner on multiple nodes. This allows for the use of large datasets, as well as the ability to train with a wide variety of data sources. However, one of the key challenges with decentralized learning is heterogeneity in the data distribution across the nodes. In this paper, we propose In-Distribution Knowledge Distillation (IDKD) to address the challenge of heterogeneous data distribution. The goal of IDKD is to homogenize the data distribution across the nodes. While such data homogenization can be achieved by exchanging data among the nodes sacrificing privacy, IDKD achieves the same objective using a common public dataset across nodes without breaking the privacy constraint. This public dataset is different from the training dataset and is used to distill the knowledge from each node and communicate it to its neighbors through the generated labels. With traditional knowledge distillat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#29289;&#29702;&#30693;&#35782;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#36827;&#34892;&#30452;&#25509;&#24494;&#38663;&#25104;&#20687;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#32858;&#28966;&#30340;&#28304;&#22270;&#20687;&#65292;&#21363;&#20351;&#21482;&#26377;&#26497;&#23569;&#30340;&#35760;&#24405;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#21487;&#38752;&#19988;&#31934;&#30830;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.04315</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#30693;&#35782;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24494;&#38663;&#28304;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
Microseismic source imaging using physics-informed neural networks with hard constraints. (arXiv:2304.04315v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#29289;&#29702;&#30693;&#35782;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#36827;&#34892;&#30452;&#25509;&#24494;&#38663;&#25104;&#20687;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#32858;&#28966;&#30340;&#28304;&#22270;&#20687;&#65292;&#21363;&#20351;&#21482;&#26377;&#26497;&#23569;&#30340;&#35760;&#24405;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#21487;&#38752;&#19988;&#31934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#38663;&#28304;&#25104;&#20687;&#22312;&#34987;&#21160;&#22320;&#38663;&#30417;&#27979;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#30001;&#20110;&#31232;&#30095;&#30340;&#27979;&#37327;&#25968;&#25454;&#23481;&#26131;&#20986;&#29616;&#28151;&#21472;&#38382;&#39064;&#65292;&#23548;&#33268;&#20854;&#24120;&#24120;&#22833;&#36133;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#30340;&#30452;&#25509;&#24494;&#38663;&#25104;&#20687;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#29983;&#25104;&#32858;&#28966;&#30340;&#28304;&#22270;&#20687;&#65292;&#21363;&#20351;&#21482;&#26377;&#26497;&#23569;&#30340;&#35760;&#24405;&#12290;&#25105;&#20204;&#20351;&#29992;PINNs&#34920;&#31034;&#22810;&#39057;&#27874;&#22330;&#65292;&#28982;&#21518;&#24212;&#29992;&#36870;&#20613;&#37324;&#21494;&#21464;&#25442;&#26469;&#25552;&#21462;&#28304;&#22270;&#20687;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#30828;&#32422;&#26463;&#26469;&#20462;&#25913;&#39057;&#22495;&#27874;&#22330;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#20174;&#32780;&#26412;&#36136;&#19978;&#28385;&#36275;&#36793;&#30028;&#26465;&#20214;&#65288;&#34920;&#23618;&#19978;&#30340;&#27979;&#37327;&#25968;&#25454;&#65289;&#65292;&#36991;&#20813;&#20102;&#22312;PINNs&#20013;&#24179;&#34913;&#25968;&#25454;&#21644;PDE&#25439;&#22833;&#30340;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#28145;&#24230;&#30340;&#22240;&#26524;&#24615;&#25439;&#22833;&#23454;&#29616;&#65292;&#20197;&#25552;&#39640;PINNs&#30340;&#25910;&#25947;&#24615;&#12290;&#22312;Overthrust&#27169;&#22411;&#19978;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#21487;&#38752;&#19988;&#31934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Microseismic source imaging plays a significant role in passive seismic monitoring. However, such a process is prone to failure due to the aliasing problem when dealing with sparse measured data. Thus, we propose a direct microseismic imaging framework based on physics-informed neural networks (PINNs), which can generate focused source images, even with very sparse recordings. We use the PINNs to represent a multi-frequency wavefield and then apply the inverse Fourier transform to extract the source image. Specially, we modify the representation of the frequency-domain wavefield to inherently satisfy the boundary conditions (the measured data on the surface) by means of the hard constraint, which helps to avoid the difficulty in balancing the data and PDE losses in PINNs. Furthermore, we propose the causality loss implementation with respect to depth to enhance the convergence of PINNs. The numerical experiments on the Overthrust model show that the method can admit reliable and accura
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36807;&#25311;&#21512;&#20803;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#21457;&#29616;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#65288;MAML&#65289;&#30340;&#36807;&#25311;&#21512;&#26368;&#23567; $\ell_2$ &#33539;&#25968;&#35299;&#21487;&#20197;&#26159;&#26377;&#30410;&#30340;&#65292;&#31867;&#20284;&#20110;&#26368;&#36817;&#20851;&#20110;&#8220;&#33391;&#24615;&#36807;&#25311;&#21512;&#8221;&#21644;&#8220;&#21452;&#19979;&#38477;&#8221;&#29616;&#35937;&#30340;&#26174;&#30528;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.04312</link><description>&lt;p&gt;
&#36807;&#25311;&#21512;&#20803;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#33021;&#30340;&#29702;&#35770;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
Theoretical Characterization of the Generalization Performance of Overfitted Meta-Learning. (arXiv:2304.04312v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36807;&#25311;&#21512;&#20803;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#21457;&#29616;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#65288;MAML&#65289;&#30340;&#36807;&#25311;&#21512;&#26368;&#23567; $\ell_2$ &#33539;&#25968;&#35299;&#21487;&#20197;&#26159;&#26377;&#30410;&#30340;&#65292;&#31867;&#20284;&#20110;&#26368;&#36817;&#20851;&#20110;&#8220;&#33391;&#24615;&#36807;&#25311;&#21512;&#8221;&#21644;&#8220;&#21452;&#19979;&#38477;&#8221;&#29616;&#35937;&#30340;&#26174;&#30528;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#25104;&#21151;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35768;&#22810;&#30456;&#20284;&#30340;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#26469;&#25552;&#39640;&#35757;&#32451;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#20013;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#65288;&#20363;&#22914;DNNs&#65289;&#65292;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#21487;&#20197;&#22312;&#20803;&#23398;&#20064;&#20013;&#24456;&#22909;&#22320;&#36827;&#34892;&#27867;&#21270;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#20316;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#21021;&#27493;&#27493;&#39588;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#39640;&#26031;&#29305;&#24449;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#19979;&#36807;&#25311;&#21512;&#20803;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#19982;&#19968;&#20123;&#26368;&#36817;&#30340;&#30740;&#31350;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#20219;&#24847;&#22823;&#20110;&#22320;&#38754;&#23454;&#20917;&#20449;&#21495;&#20013;&#30340;&#29305;&#24449;&#25968;&#37327;&#65292;&#22240;&#27492;&#33258;&#28982;&#22320;&#25429;&#25417;&#28145;&#24230;&#20803;&#23398;&#20064;&#23454;&#36341;&#20013;&#30340;&#36807;&#24230;&#21442;&#25968;&#21270;&#21046;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#65288;MAML&#65289;&#30340;&#36807;&#25311;&#21512;&#26368;&#23567; $\ell_2$&#33539;&#25968;&#35299;&#21487;&#20197;&#26159;&#26377;&#30410;&#30340;&#65292;&#36825;&#31867;&#20284;&#20110;&#26368;&#36817;&#20851;&#20110;&#8220;&#33391;&#24615;&#36807;&#25311;&#21512;&#8221;&#21644;&#8220;&#21452;&#19979;&#38477;&#8221;&#29616;&#35937;&#30340;&#26174;&#30528;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-learning has arisen as a successful method for improving training performance by training over many similar tasks, especially with deep neural networks (DNNs). However, the theoretical understanding of when and why overparameterized models such as DNNs can generalize well in meta-learning is still limited. As an initial step towards addressing this challenge, this paper studies the generalization performance of overfitted meta-learning under a linear regression model with Gaussian features. In contrast to a few recent studies along the same line, our framework allows the number of model parameters to be arbitrarily larger than the number of features in the ground truth signal, and hence naturally captures the overparameterized regime in practical deep meta-learning. We show that the overfitted min $\ell_2$-norm solution of model-agnostic meta-learning (MAML) can be beneficial, which is similar to the recent remarkable findings on ``benign overfitting'' and ``double descent'' pheno
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#40065;&#26834;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#24378;&#40065;&#26834;&#24615;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#38598;&#25104;&#65292;&#21462;&#24471;&#20102;&#27604;&#26368;&#20339;&#38598;&#25104;&#25104;&#21592;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.04308</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#36866;&#24212;&#40065;&#26834;&#20248;&#21270;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38598;&#25104;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ensemble Modeling for Time Series Forecasting: an Adaptive Robust Optimization Approach. (arXiv:2304.04308v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#40065;&#26834;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#24378;&#40065;&#26834;&#24615;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#38598;&#25104;&#65292;&#21462;&#24471;&#20102;&#27604;&#26368;&#20339;&#38598;&#25104;&#25104;&#21592;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#23545;&#20110;&#28041;&#21450;&#26102;&#38388;&#25968;&#25454;&#30340;&#24191;&#27867;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#38598;&#25104;&#24314;&#27169;&#26159;&#19968;&#31181;&#21033;&#29992;&#22810;&#20010;&#39044;&#27979;&#27169;&#22411;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#25104;&#29087;&#25216;&#26415;&#65292;&#22240;&#20026;&#21333;&#20010;&#39044;&#27979;&#22120;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#22240;&#22522;&#30784;&#25968;&#25454;&#20998;&#24067;&#30340;&#36716;&#25442;&#32780;&#39640;&#24230;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;&#24378;&#40065;&#26834;&#24615;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#38598;&#25104;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#33258;&#36866;&#24212;&#40065;&#26834;&#20248;&#21270;&#65288;ARO&#65289;&#26500;&#24314;&#20102;&#19968;&#20010;&#32447;&#24615;&#22238;&#24402;&#38598;&#25104;&#27169;&#22411;&#65292;&#20854;&#20013;&#27169;&#22411;&#30340;&#26435;&#37325;&#21487;&#20197;&#38543;&#26102;&#38388;&#33258;&#36866;&#24212;&#35843;&#25972;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#21512;&#25104;&#23454;&#39564;&#21644;&#29616;&#23454;&#24212;&#29992;&#65292;&#21253;&#25324;&#31354;&#27668;&#27745;&#26579;&#31649;&#29702;&#12289;&#33021;&#28304;&#28040;&#32791;&#39044;&#27979;&#21644;&#28909;&#24102;&#27668;&#26059;&#24378;&#24230;&#39044;&#27979;&#31561;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#33258;&#36866;&#24212;&#38598;&#25104;&#27169;&#22411;&#22312;&#22343;&#26041;&#26681;&#35823;&#24046;&#21644;&#26465;&#20214;&#39118;&#38505;&#20215;&#20540;&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;&#26368;&#22909;&#30340;&#38598;&#25104;&#25104;&#21592;&#65292;&#20998;&#21035;&#25552;&#39640;&#20102;16-26%&#21644;14-28%&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate time series forecasting is critical for a wide range of problems with temporal data. Ensemble modeling is a well-established technique for leveraging multiple predictive models to increase accuracy and robustness, as the performance of a single predictor can be highly variable due to shifts in the underlying data distribution. This paper proposes a new methodology for building robust ensembles of time series forecasting models. Our approach utilizes Adaptive Robust Optimization (ARO) to construct a linear regression ensemble in which the models' weights can adapt over time. We demonstrate the effectiveness of our method through a series of synthetic experiments and real-world applications, including air pollution management, energy consumption forecasting, and tropical cyclone intensity forecasting. Our results show that our adaptive ensembles outperform the best ensemble member in hindsight by 16-26% in root mean square error and 14-28% in conditional value at risk and improv
&lt;/p&gt;</description></item><item><title>PriorCVAE &#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564; MCMC &#21442;&#25968;&#25512;&#26029;&#30340;&#36125;&#21494;&#26031;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#26032;&#26041;&#27861;&#65292;&#21487;&#36890;&#36807;&#23558; VAE &#24314;&#27169;&#26465;&#20214;&#21270;&#20110;&#38543;&#26426;&#36807;&#31243;&#36229;&#21442;&#25968;&#22788;&#29702;&#36229;&#21442;&#25968;&#25512;&#26029;&#19982;&#23398;&#20064;&#20808;&#39564;&#20043;&#38388;&#30340;&#20449;&#24687;&#27969;&#26029;&#35010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.04307</link><description>&lt;p&gt;
PriorCVAE: &#22522;&#20110;&#36125;&#21494;&#26031;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#30340;&#21487;&#25193;&#23637; MCMC &#21442;&#25968;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
PriorCVAE: scalable MCMC parameter inference with Bayesian deep generative modelling. (arXiv:2304.04307v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04307
&lt;/p&gt;
&lt;p&gt;
PriorCVAE &#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564; MCMC &#21442;&#25968;&#25512;&#26029;&#30340;&#36125;&#21494;&#26031;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#26032;&#26041;&#27861;&#65292;&#21487;&#36890;&#36807;&#23558; VAE &#24314;&#27169;&#26465;&#20214;&#21270;&#20110;&#38543;&#26426;&#36807;&#31243;&#36229;&#21442;&#25968;&#22788;&#29702;&#36229;&#21442;&#25968;&#25512;&#26029;&#19982;&#23398;&#20064;&#20808;&#39564;&#20043;&#38388;&#30340;&#20449;&#24687;&#27969;&#26029;&#35010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#25512;&#29702;&#36895;&#24230;&#21644;&#27169;&#22411;&#28789;&#27963;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#36125;&#21494;&#26031;&#25512;&#26029;&#22312;&#20855;&#26377;&#38543;&#26426;&#36807;&#31243;&#20808;&#39564;&#30340;&#27169;&#22411;&#20013;&#65288;&#22914;&#39640;&#26031;&#36807;&#31243;&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#31561;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#32534;&#30721;&#30001; GP &#20808;&#39564;&#25110;&#20854;&#26377;&#38480;&#23454;&#29616;&#24341;&#36215;&#30340;&#35745;&#31639;&#29942;&#39048;&#65292;&#24182;&#19988;&#25152;&#23398;&#29983;&#25104;&#22120;&#21487;&#20197;&#20195;&#26367; MCMC &#25512;&#26029;&#20013;&#30340;&#21407;&#22987;&#20808;&#39564;&#12290;&#34429;&#28982;&#27492;&#26041;&#27861;&#23454;&#29616;&#20102;&#24555;&#36895;&#32780;&#39640;&#25928;&#30340;&#25512;&#29702;&#65292;&#20294;&#23427;&#20002;&#22833;&#20102;&#20851;&#20110;&#38543;&#26426;&#36807;&#31243;&#36229;&#21442;&#25968;&#30340;&#20449;&#24687;&#65292;&#23548;&#33268;&#36229;&#21442;&#25968;&#25512;&#26029;&#19981;&#21487;&#33021;&#21644;&#23398;&#21040;&#30340;&#20808;&#39564;&#27169;&#31946;&#19981;&#28165;&#12290;&#25105;&#20204;&#24314;&#35758;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558; VAE &#24314;&#27169;&#26465;&#20214;&#21270;&#20110;&#38543;&#26426;&#36807;&#31243;&#36229;&#21442;&#25968;&#65292;&#20197;&#20415;&#36229;&#21442;&#25968;&#19982; GP &#23454;&#29616;&#19968;&#36215;&#36827;&#34892;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
In applied fields where the speed of inference and model flexibility are crucial, the use of Bayesian inference for models with a stochastic process as their prior, e.g. Gaussian processes (GPs) is ubiquitous. Recent literature has demonstrated that the computational bottleneck caused by GP priors or their finite realizations can be encoded using deep generative models such as variational autoencoders (VAEs), and the learned generators can then be used instead of the original priors during Markov chain Monte Carlo (MCMC) inference in a drop-in manner. While this approach enables fast and highly efficient inference, it loses information about the stochastic process hyperparameters, and, as a consequence, makes inference over hyperparameters impossible and the learned priors indistinct. We propose to resolve the aforementioned issue and disentangle the learned priors by conditioning the VAE on stochastic process hyperparameters. This way, the hyperparameters are encoded alongside GP real
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#29616;&#26377;CILG&#25216;&#26415;&#30340;&#26368;&#26032;&#29366;&#24577;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#29616;&#26377;&#24037;&#20316;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;CILG&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#22270;&#34920;&#31034;&#23398;&#20064;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#20811;&#26381;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.04300</link><description>&lt;p&gt;
&#22270;&#20013;&#31867;&#21035;&#19981;&#24179;&#34913;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Class-Imbalanced Learning on Graphs: A Survey. (arXiv:2304.04300v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#29616;&#26377;CILG&#25216;&#26415;&#30340;&#26368;&#26032;&#29366;&#24577;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#29616;&#26377;&#24037;&#20316;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;CILG&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#22270;&#34920;&#31034;&#23398;&#20064;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#20811;&#26381;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30740;&#31350;&#30340;&#24555;&#36895;&#21457;&#23637;&#22686;&#21152;&#20102;&#23545;&#26377;&#25928;&#30340;&#22270;&#25968;&#25454;&#20998;&#26512;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#25968;&#25454;&#36890;&#24120;&#21576;&#29616;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#23548;&#33268;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#19981;&#20339;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#31867;&#21035;&#19981;&#24179;&#34913;&#23398;&#20064;&#22312;&#22270;&#19978;&#65288;CILG&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#32467;&#21512;&#20102;&#22270;&#34920;&#31034;&#23398;&#20064;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#23398;&#20064;&#30340;&#20248;&#21183;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;CILG&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26412;&#32508;&#36848;&#26088;&#22312;&#20840;&#38754;&#20102;&#35299;CILG&#30340;&#29616;&#26377;&#26368;&#26032;&#25216;&#26415;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#35265;&#35299;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20221;&#25345;&#32493;&#26356;&#26032;&#30340;&#35770;&#25991;&#38405;&#35835;&#21015;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement in data-driven research has increased the demand for effective graph data analysis. However, real-world data often exhibits class imbalance, leading to poor performance of machine learning models. To overcome this challenge, class-imbalanced learning on graphs (CILG) has emerged as a promising solution that combines the strengths of graph representation learning and class-imbalanced learning. In recent years, significant progress has been made in CILG. Anticipating that such a trend will continue, this survey aims to offer a comprehensive understanding of the current state-of-the-art in CILG and provide insights for future research directions. Concerning the former, we introduce the first taxonomy of existing work and its connection to existing imbalanced learning literature. Concerning the latter, we critically analyze recent work in CILG and discuss urgent lines of inquiry within the topic. Moreover, we provide a continuously maintained reading list of papers an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;discGAN&#65289;&#29992;&#20110;&#29983;&#25104;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#65292;&#24182;&#19988;&#22312;&#24212;&#29992;&#20013;&#25104;&#21151;&#27169;&#25311;&#20986;&#38750;&#39640;&#26031;&#22810;&#27169;&#24335;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#30340;&#20998;&#24067;&#65292;&#20854;&#29983;&#25104;&#30340;&#25968;&#25454;&#20998;&#24067;&#19982;&#30495;&#23454;&#25968;&#25454;&#30456;&#20284;&#12290;</title><link>http://arxiv.org/abs/2304.04290</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;discGAN&#65289;&#29992;&#20110;&#21512;&#25104;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Distributed Conditional GAN (discGAN) For Synthetic Healthcare Data Generation. (arXiv:2304.04290v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;discGAN&#65289;&#29992;&#20110;&#29983;&#25104;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#65292;&#24182;&#19988;&#22312;&#24212;&#29992;&#20013;&#25104;&#21151;&#27169;&#25311;&#20986;&#38750;&#39640;&#26031;&#22810;&#27169;&#24335;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#30340;&#20998;&#24067;&#65292;&#20854;&#29983;&#25104;&#30340;&#25968;&#25454;&#20998;&#24067;&#19982;&#30495;&#23454;&#25968;&#25454;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;discGAN&#65289;&#26469;&#29983;&#25104;&#29305;&#23450;&#20110;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#12290;&#34429;&#28982;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26469;&#29983;&#25104;&#22270;&#20687;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#20960;&#20046;&#27809;&#26377;&#20154;&#20851;&#27880;&#29983;&#25104;&#34920;&#26684;&#25968;&#25454;&#12290;&#24314;&#27169;&#31163;&#25955;&#21644;&#36830;&#32493;&#34920;&#26684;&#25968;&#25454;&#30340;&#20998;&#24067;&#26159;&#19968;&#39033;&#38750;&#24120;&#26377;&#29992;&#30340;&#38750;&#24179;&#20961;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;discGAN&#24212;&#29992;&#20110;&#27169;&#25311;&#38750;&#39640;&#26031;&#22810;&#27169;&#24335;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#12290;&#25105;&#20204;&#20174;&#21407;&#22987;&#30340;2,027&#20010;eICU&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#20102;249,000&#20010;&#21512;&#25104;&#35760;&#24405;&#12290;&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21151;&#25928;&#12289;&#36830;&#32493;&#21464;&#37327;&#30340;Kolmogorov-Smirnov&#65288;KS&#65289;&#26816;&#39564;&#21644;&#31163;&#25955;&#21464;&#37327;&#30340;&#21345;&#26041;&#26816;&#39564;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;discGAN&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#19982;&#30495;&#23454;&#25968;&#25454;&#31867;&#20284;&#30340;&#20998;&#24067;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a distributed Generative Adversarial Networks (discGANs) to generate synthetic tabular data specific to the healthcare domain. While using GANs to generate images has been well studied, little to no attention has been given to generation of tabular data. Modeling distributions of discrete and continuous tabular data is a non-trivial task with high utility. We applied discGAN to model non-Gaussian multi-modal healthcare data. We generated 249,000 synthetic records from original 2,027 eICU dataset. We evaluated the performance of the model using machine learning efficacy, the Kolmogorov-Smirnov (KS) test for continuous variables and chi-squared test for discrete variables. Our results show that discGAN was able to generate data with distributions similar to the real data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ST-Impute&#30340;&#21322;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#26041;&#27861;&#65292;&#20351;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#19979;&#28216;&#20219;&#21153;&#30340;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#22522;&#20110;&#31232;&#30095;&#33258;&#25105;&#20851;&#27880;&#30340;&#26041;&#27861;&#27604;&#29616;&#26377;&#30340;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#26041;&#27861;&#26356;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2304.04275</link><description>&lt;p&gt;
&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#22635;&#34917;&#32570;&#22833;&#20540;&#65306;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;
&lt;/p&gt;
&lt;p&gt;
Filling out the missing gaps: Time Series Imputation with Semi-Supervised Learning. (arXiv:2304.04275v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ST-Impute&#30340;&#21322;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#26041;&#27861;&#65292;&#20351;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#19979;&#28216;&#20219;&#21153;&#30340;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#22522;&#20110;&#31232;&#30095;&#33258;&#25105;&#20851;&#27880;&#30340;&#26041;&#27861;&#27604;&#29616;&#26377;&#30340;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#26041;&#27861;&#26356;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#32570;&#22833;&#25968;&#25454;&#26159;&#24433;&#21709;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#30001;&#20110;&#25968;&#25454;&#20002;&#22833;&#25110;&#20256;&#24863;&#22120;&#25925;&#38556;&#31561;&#38382;&#39064;&#23548;&#33268;&#32570;&#22833;&#25968;&#25454;&#30340;&#21457;&#29983;&#12290;&#25554;&#34917;&#26041;&#27861;&#29992;&#20110;&#22635;&#34917;&#36825;&#20123;&#20540;&#65292;&#32780;&#25554;&#34917;&#30340;&#36136;&#37327;&#23545;&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#20998;&#31867;&#65289;&#30340;&#24433;&#21709;&#24456;&#22823;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#25554;&#34917;&#26041;&#27861;ST-Impute&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#20197;&#21450;&#19979;&#28216;&#20219;&#21153;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;ST-Impute&#22522;&#20110;&#31232;&#30095;&#33258;&#25105;&#20851;&#27880;&#65292;&#24182;&#35757;&#32451;&#31867;&#20284;&#20110;&#25554;&#34917;&#36807;&#31243;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25554;&#34917;&#36136;&#37327;&#20197;&#21450;&#25554;&#34917;&#26102;&#38388;&#24207;&#21015;&#30340;&#19979;&#28216;&#20219;&#21153;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Missing data in time series is a challenging issue affecting time series analysis. Missing data occurs due to problems like data drops or sensor malfunctioning. Imputation methods are used to fill in these values, with quality of imputation having a significant impact on downstream tasks like classification. In this work, we propose a semi-supervised imputation method, ST-Impute, that uses both unlabeled data along with downstream task's labeled data. ST-Impute is based on sparse self-attention and trains on tasks that mimic the imputation process. Our results indicate that the proposed method outperforms the existing supervised and unsupervised time series imputation methods measured on the imputation quality as well as on the downstream tasks ingesting imputed time series.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#22411;&#30340;&#39550;&#39542;&#21592;&#35748;&#30693;&#36127;&#33655;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#37319;&#29992;&#22810;&#27169;&#24577;&#29983;&#29702;&#20449;&#21495;&#19982;&#30524;&#21160;&#36861;&#36394;&#25968;&#25454;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#24615;&#33021;&#65292;&#20026;&#35813;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2304.04273</link><description>&lt;p&gt;
&#22810;&#27169;&#24335;&#33041;&#26426;&#25509;&#21475;&#22312;&#36710;&#20869;&#39550;&#39542;&#21592;&#35748;&#30693;&#36127;&#33655;&#27979;&#37327;&#20013;&#30340;&#24212;&#29992;&#65306;&#25968;&#25454;&#38598;&#19982;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
Multimodal Brain-Computer Interface for In-Vehicle Driver Cognitive Load Measurement: Dataset and Baselines. (arXiv:2304.04273v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#22411;&#30340;&#39550;&#39542;&#21592;&#35748;&#30693;&#36127;&#33655;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#37319;&#29992;&#22810;&#27169;&#24577;&#29983;&#29702;&#20449;&#21495;&#19982;&#30524;&#21160;&#36861;&#36394;&#25968;&#25454;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#24615;&#33021;&#65292;&#20026;&#35813;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#22411;&#30340;&#39550;&#39542;&#21592;&#35748;&#30693;&#36127;&#33655;&#35780;&#20272;&#25968;&#25454;&#38598;CL-Drive&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#39550;&#39542;&#26102;&#30340;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#12289;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#12289;&#30382;&#32932;&#30005;&#27963;&#21160;&#65288;EDA&#65289;&#20449;&#21495;&#20197;&#21450;&#30524;&#21160;&#36861;&#36394;&#25968;&#25454;&#12290;&#25968;&#25454;&#37319;&#38598;&#33258;21&#21517;&#34987;&#35797;&#22312;&#27785;&#28024;&#24335;&#36710;&#36742;&#27169;&#25311;&#22120;&#20013;&#39550;&#39542;&#65292;&#21253;&#25324;&#19981;&#21516;&#39550;&#39542;&#26465;&#20214;&#19979;&#30340;&#20219;&#21153;&#65292;&#20197;&#35825;&#21457;&#34987;&#35797;&#30340;&#19981;&#21516;&#35748;&#30693;&#36127;&#33655;&#27700;&#24179;&#65292;&#27599;&#20010;&#20219;&#21153;&#25345;&#32493;3&#20998;&#38047;&#65292;&#20849;9&#20010;&#22797;&#26434;&#24230;&#32423;&#21035;&#12290;&#27599;&#20010;&#39550;&#39542;&#21592;&#22312;&#23454;&#39564;&#36807;&#31243;&#20013;&#27599;10&#31186;&#25253;&#21578;&#19968;&#27425;&#20027;&#35266;&#35748;&#30693;&#36127;&#33655;&#12290;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#20027;&#35266;&#35748;&#30693;&#36127;&#33655;&#35760;&#24405;&#20316;&#20026;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20379;&#20102;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20108;&#20803;&#21644;&#19977;&#20803;&#26631;&#31614;&#20998;&#24067;&#30340;&#22522;&#20934;&#20998;&#31867;&#32467;&#26524;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#20004;&#31181;&#35780;&#20272;&#26631;&#20934;&#65292;&#21363;10&#20493;&#20132;&#21449;&#39564;&#35777;&#21644;&#30041;&#19968;&#27861;&#12290;&#25105;&#20204;&#23545;&#25163;&#24037;&#21046;&#20316;&#30340;&#29305;&#24449;&#21644;&#21407;&#22987;&#20449;&#21495;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#29983;&#29702;&#20449;&#21495;&#19982;&#30524;&#21160;&#36861;&#36394;&#25968;&#25454;&#30456;&#27604;&#20165;&#20351;&#29992;EEG&#25968;&#25454;&#20855;&#26377;&#26356;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21644;&#20998;&#31867;&#32467;&#26524;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#39550;&#39542;&#21592;&#35748;&#30693;&#36127;&#33655;&#35780;&#20272;&#25552;&#20379;&#20102;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Through this paper, we introduce a novel driver cognitive load assessment dataset, CL-Drive, which contains Electroencephalogram (EEG) signals along with other physiological signals such as Electrocardiography (ECG) and Electrodermal Activity (EDA) as well as eye tracking data. The data was collected from 21 subjects while driving in an immersive vehicle simulator, in various driving conditions, to induce different levels of cognitive load in the subjects. The tasks consisted of 9 complexity levels for 3 minutes each. Each driver reported their subjective cognitive load every 10 seconds throughout the experiment. The dataset contains the subjective cognitive load recorded as ground truth. In this paper, we also provide benchmark classification results for different machine learning and deep learning models for both binary and ternary label distributions. We followed 2 evaluation criteria namely 10-fold and leave-one-subject-out (LOSO). We have trained our models on both hand-crafted fe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MixUp&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21363;MixUp++&#21644;LatentMixUp++&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;1&#65285;- 15&#65285;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#65292;&#24182;&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#25193;&#23637;&#21040;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2304.04271</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#23604;&#23596;&#31616;&#21333;&#28151;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Embarrassingly Simple MixUp for Time-series. (arXiv:2304.04271v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MixUp&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21363;MixUp++&#21644;LatentMixUp++&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;1&#65285;- 15&#65285;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#65292;&#24182;&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#25193;&#23637;&#21040;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#21644;&#25968;&#25454;&#30340;&#21160;&#24577;&#24615;&#65292;&#26631;&#35760;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26159;&#19968;&#39033;&#26114;&#36149;&#30340;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#32463;&#24120;&#38656;&#35201;&#22788;&#29702;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#12290;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#35832;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#20043;&#31867;&#30340;&#39046;&#22495;&#65292;&#20197;&#21033;&#29992;&#29616;&#26377;&#26631;&#35760;&#25968;&#25454;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#23558;&#20854;&#20013;&#19968;&#31181;&#26368;&#24120;&#29992;&#30340;&#25216;&#26415;MixUp&#65292;&#24212;&#29992;&#20110;&#26102;&#24207;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;MixUp++&#21644;LatentMixUp++&#26041;&#27861;&#20998;&#21035;&#23545;&#21407;&#22987;&#26102;&#38388;&#24207;&#21015;&#21644;&#20998;&#31867;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#25554;&#20540;&#30340;&#31616;&#21333;&#20462;&#25913;&#12290;&#25105;&#20204;&#20063;&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#23558;&#36825;&#20123;&#26041;&#27861;&#25193;&#23637;&#21040;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23545;&#20110;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65292;&#26080;&#35770;&#26159;&#20302;&#26631;&#35760;&#25968;&#25454;&#36824;&#26159;&#39640;&#26631;&#35760;&#25968;&#25454;&#21046;&#24230;&#65292;&#36890;&#36807;LatentMixUp ++&#65292;&#37117;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;1&#65285;- 15&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Labeling time series data is an expensive task because of domain expertise and dynamic nature of the data. Hence, we often have to deal with limited labeled data settings. Data augmentation techniques have been successfully deployed in domains like computer vision to exploit the use of existing labeled data. We adapt one of the most commonly used technique called MixUp, in the time series domain. Our proposed, MixUp++ and LatentMixUp++, use simple modifications to perform interpolation in raw time series and classification model's latent space, respectively. We also extend these methods with semi-supervised learning to exploit unlabeled data. We observe significant improvements of 1\% - 15\% on time series classification on two public datasets, for both low labeled data as well as high labeled data regimes, with LatentMixUp++.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#30340;&#38271;&#35270;&#39057;&#23545;&#35937;&#20998;&#21106;&#25968;&#25454;&#38598;CLVOS23&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#23454;&#29616;&#20102;&#19968;&#20010;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#21322;&#30417;&#30563;&#35270;&#39057;&#23545;&#35937;&#20998;&#21106;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.04259</link><description>&lt;p&gt;
CLVOS23&#65306;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#30340;&#38271;&#35270;&#39057;&#23545;&#35937;&#20998;&#21106;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CLVOS23: A Long Video Object Segmentation Dataset for Continual Learning. (arXiv:2304.04259v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#30340;&#38271;&#35270;&#39057;&#23545;&#35937;&#20998;&#21106;&#25968;&#25454;&#38598;CLVOS23&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#23454;&#29616;&#20102;&#19968;&#20010;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#21322;&#30417;&#30563;&#35270;&#39057;&#23545;&#35937;&#20998;&#21106;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#22330;&#26223;&#19979;&#30340;&#25345;&#32493;&#23398;&#20064;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#36890;&#24120;&#30340;&#25345;&#32493;&#23398;&#20064;&#27169;&#22411;&#24212;&#20855;&#26377;&#24658;&#23450;&#30340;&#35760;&#24518;&#22823;&#23567;&#21644;&#26080;&#39044;&#23450;&#20041;&#30340;&#20219;&#21153;&#36793;&#30028;&#65292;&#32780;&#36825;&#31181;&#24773;&#20917;&#22312;&#21322;&#30417;&#30563;&#35270;&#39057;&#23545;&#35937;&#20998;&#21106;&#65288;VOS&#65289;&#20013;&#23588;&#20026;&#26126;&#26174;&#65292;&#20854;&#20013;&#25345;&#32493;&#23398;&#20064;&#30340;&#25361;&#25112;&#20027;&#35201;&#20307;&#29616;&#22312;&#22788;&#29702;&#38271;&#35270;&#39057;&#24207;&#21015;&#19978;&#12290;&#26412;&#25991;&#39318;&#20808;&#23558;&#21322;&#30417;&#30563;VOS&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#32447;VOS&#38382;&#39064;&#65292;&#24418;&#24335;&#21270;&#20026;&#25345;&#32493;&#23398;&#20064;&#38382;&#39064;&#65292;&#28982;&#21518;&#25552;&#20379;&#19968;&#20010;&#20844;&#20849;VOS&#25968;&#25454;&#38598;CLVOS23&#65292;&#37325;&#28857;&#20851;&#27880;&#25345;&#32493;&#23398;&#20064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#22312;&#29616;&#26377;&#30340;&#22312;&#32447;VOS&#22522;&#32447;LWL&#19978;&#65292;&#20197;&#23637;&#31034;&#25345;&#32493;&#23398;&#20064;&#22312;&#24212;&#29992;&#20110;&#22312;&#32447;VOS&#26102;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;CLVOS23&#22522;&#32447;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#22522;&#32447;&#24212;&#29992;&#20110;&#38271;&#35270;&#39057;&#25968;&#25454;&#38598;&#20197;&#21450;&#20004;&#20010;&#30701;&#35270;&#39057;VOS&#25968;&#25454;&#38598;DAVIS16&#21644;DAVIS17&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;VOS&#39318;&#27425;&#34987;&#23450;&#20041;&#20026;&#25345;&#32493;&#23398;&#20064;&#38382;&#39064;&#24182;&#26500;&#24314;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning in real-world scenarios is a major challenge. A general continual learning model should have a constant memory size and no predefined task boundaries, as is the case in semi-supervised Video Object Segmentation (VOS), where continual learning challenges particularly present themselves in working on long video sequences. In this article, we first formulate the problem of semi-supervised VOS, specifically online VOS, as a continual learning problem, and then secondly provide a public VOS dataset, CLVOS23, focusing on continual learning. Finally, we propose and implement a regularization-based continual learning approach on LWL, an existing online VOS baseline, to demonstrate the efficacy of continual learning when applied to online VOS and to establish a CLVOS23 baseline. We apply the proposed baseline to the Long Videos dataset as well as to two short video VOS datasets, DAVIS16 and DAVIS17. To the best of our knowledge, this is the first time that VOS has been define
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#33258;&#28982;&#21644;&#21487;&#35299;&#37322;&#30340;&#25928;&#29992;&#20989;&#25968;&#65292;&#26356;&#22909;&#22320;&#21453;&#26144;&#20102;KNN&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25552;&#20379;&#20102;&#30456;&#24212;&#35745;&#31639;&#36807;&#31243;&#65292;&#35813;&#26041;&#27861;&#34987;&#31216;&#20026;&#36719;&#26631;&#31614;KNN-SV&#65292;&#19982;&#21407;&#22987;&#26041;&#27861;&#20855;&#26377;&#30456;&#21516;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.04258</link><description>&lt;p&gt;
&#20851;&#20110;&#8220;&#26368;&#36817;&#37051;&#31639;&#27861;&#30340;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#26377;&#25928;&#24615;&#8221;&#30340;&#27880;&#35760;&#65288;arXiv&#65306;2304.04258v1 [stat.ML]&#65289;
&lt;/p&gt;
&lt;p&gt;
A Note on "Efficient Task-Specific Data Valuation for Nearest Neighbor Algorithms". (arXiv:2304.04258v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#33258;&#28982;&#21644;&#21487;&#35299;&#37322;&#30340;&#25928;&#29992;&#20989;&#25968;&#65292;&#26356;&#22909;&#22320;&#21453;&#26144;&#20102;KNN&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25552;&#20379;&#20102;&#30456;&#24212;&#35745;&#31639;&#36807;&#31243;&#65292;&#35813;&#26041;&#27861;&#34987;&#31216;&#20026;&#36719;&#26631;&#31614;KNN-SV&#65292;&#19982;&#21407;&#22987;&#26041;&#27861;&#20855;&#26377;&#30456;&#21516;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#26377;&#25928;&#24615;&#26159;&#19968;&#20010;&#30740;&#31350;&#21333;&#20010;&#25968;&#25454;&#28857;&#23545;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#24433;&#21709;&#30340;&#26085;&#30410;&#22686;&#38271;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#22522;&#20110;&#21512;&#20316;&#21338;&#24328;&#35770;&#21644;&#32463;&#27982;&#23398;&#65292;&#25968;&#25454; Shapley &#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#25968;&#25454;&#26377;&#25928;&#24615;&#35745;&#31639;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#37117;&#30693;&#36947; Shapley &#20540;&#65288;SV&#65289;&#30340;&#35745;&#31639;&#21487;&#33021;&#38750;&#24120;&#26114;&#36149;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;Jia &#31561;&#20154;&#65288;2019&#65289;&#34920;&#26126;&#65292;&#23545;&#20110; K &#26368;&#36817;&#37051;&#65288;KNN&#65289;&#27169;&#22411;&#65292;&#35745;&#31639; Data Shapley &#31455;&#28982;&#38750;&#24120;&#31616;&#21333;&#21644;&#39640;&#25928;&#12290;&#22312;&#26412;&#31508;&#35760;&#20013;&#65292;&#25105;&#20204;&#37325;&#23457;&#20102; Jia &#31561;&#20154;&#65288;2019&#65289;&#30340;&#24037;&#20316;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#33258;&#28982;&#21644;&#21487;&#35299;&#37322;&#30340;&#25928;&#29992;&#20989;&#25968;&#65292;&#26356;&#22909;&#22320;&#21453;&#26144;&#20102; KNN &#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#20855;&#26377;&#26032;&#25928;&#29992;&#20989;&#25968;&#30340; KNN &#20998;&#31867;&#22120;/&#22238;&#24402;&#22120;&#30340; Data Shapley &#30340;&#30456;&#24212;&#35745;&#31639;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#34987;&#31216;&#20026;&#36719;&#26631;&#31614; KNN-SV&#65292;&#19982;&#21407;&#22987;&#26041;&#27861;&#20855;&#26377;&#30456;&#21516;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#65288;LSH&#65289;&#30340;&#36719;&#26631;&#31614; KNN-SV &#30340;&#39640;&#25928;&#36817;&#20284;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data valuation is a growing research field that studies the influence of individual data points for machine learning (ML) models. Data Shapley, inspired by cooperative game theory and economics, is an effective method for data valuation. However, it is well-known that the Shapley value (SV) can be computationally expensive. Fortunately, Jia et al. (2019) showed that for K-Nearest Neighbors (KNN) models, the computation of Data Shapley is surprisingly simple and efficient.  In this note, we revisit the work of Jia et al. (2019) and propose a more natural and interpretable utility function that better reflects the performance of KNN models. We derive the corresponding calculation procedure for the Data Shapley of KNN classifiers/regressors with the new utility functions. Our new approach, dubbed soft-label KNN-SV, achieves the same time complexity as the original method. We further provide an efficient approximation algorithm for soft-label KNN-SV based on locality sensitive hashing (LSH
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#21306;&#22359;&#38142;&#25216;&#26415;&#30340;&#23433;&#20840;&#36335;&#30001;&#31639;&#27861;&#65288;SRABC&#65289;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#25552;&#20379;&#28304;&#33410;&#28857;&#21040;&#30446;&#30340;&#33410;&#28857;&#30340;&#23433;&#20840;&#12289;&#32463;&#36807;&#35748;&#35777;&#21644;&#38450;&#31713;&#25913;&#30340;&#36335;&#30001;&#26469;&#20445;&#25252;&#25511;&#21046;&#21644;&#25968;&#25454;&#27969;&#65292;&#39044;&#38450;MANET&#21463;&#21040;&#21508;&#31181;&#25915;&#20987;&#25163;&#27573;&#65292;&#24182;&#23545;&#33410;&#28857;&#36827;&#34892;&#35748;&#35777;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;SRABC&#31639;&#27861;&#20248;&#20110;&#24403;&#21069;&#26041;&#27861;&#65292;&#26159;&#30830;&#20445;MANET&#23433;&#20840;&#30340;&#21487;&#34892;&#36873;&#39033;&#12290;</title><link>http://arxiv.org/abs/2304.04254</link><description>&lt;p&gt;
&#21306;&#22359;&#38142;&#25216;&#26415;&#22312;MANET&#20013;&#30340;&#23433;&#20840;&#36335;&#30001;&#21327;&#35758;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Secure Routing Protocol To Mitigate Attacks By Using Blockchain Technology In Manet. (arXiv:2304.04254v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04254
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#21306;&#22359;&#38142;&#25216;&#26415;&#30340;&#23433;&#20840;&#36335;&#30001;&#31639;&#27861;&#65288;SRABC&#65289;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#25552;&#20379;&#28304;&#33410;&#28857;&#21040;&#30446;&#30340;&#33410;&#28857;&#30340;&#23433;&#20840;&#12289;&#32463;&#36807;&#35748;&#35777;&#21644;&#38450;&#31713;&#25913;&#30340;&#36335;&#30001;&#26469;&#20445;&#25252;&#25511;&#21046;&#21644;&#25968;&#25454;&#27969;&#65292;&#39044;&#38450;MANET&#21463;&#21040;&#21508;&#31181;&#25915;&#20987;&#25163;&#27573;&#65292;&#24182;&#23545;&#33410;&#28857;&#36827;&#34892;&#35748;&#35777;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;SRABC&#31639;&#27861;&#20248;&#20110;&#24403;&#21069;&#26041;&#27861;&#65292;&#26159;&#30830;&#20445;MANET&#23433;&#20840;&#30340;&#21487;&#34892;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MANET&#26159;&#19968;&#32452;&#36890;&#36807;&#26080;&#32447;&#32593;&#32476;&#36890;&#20449;&#30340;&#31227;&#21160;&#33410;&#28857;&#65292;&#23427;&#20204;&#20174;&#19968;&#20010;&#28857;&#31227;&#21160;&#21040;&#21478;&#19968;&#20010;&#28857;&#12290;&#30001;&#20110;MANET&#26159;&#19968;&#20010;&#27809;&#26377;&#22522;&#30784;&#35774;&#26045;&#19988;&#25299;&#25169;&#32467;&#26500;&#21487;&#21464;&#30340;&#32593;&#32476;&#65292;&#22240;&#27492;&#24456;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#24694;&#24847;&#32593;&#32476;&#33410;&#28857;&#26159;&#32593;&#32476;&#25915;&#20987;&#30340;&#28304;&#22836;&#12290;&#22312;MANET&#20013;&#65292;&#25915;&#20987;&#21487;&#20197;&#37319;&#21462;&#21508;&#31181;&#24418;&#24335;&#65292;&#24182;&#20197;&#20854;&#29420;&#29305;&#30340;&#26041;&#24335;&#25913;&#21464;&#32593;&#32476;&#30340;&#36816;&#34892;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#35768;&#22810;&#24418;&#24335;&#30340;&#25915;&#20987;&#12289;&#23427;&#20204;&#23545;MANET&#30340;&#24433;&#21709;&#20197;&#21450;&#30446;&#21069;&#23454;&#26045;&#30340;MANET&#38450;&#24481;&#25514;&#26045;&#12290;&#25152;&#25552;&#20986;&#30340;&#37319;&#29992;&#21306;&#22359;&#38142;&#25216;&#26415;&#30340;&#23433;&#20840;&#36335;&#30001;&#31639;&#27861;&#65288;SRABC&#65289;&#21487;&#20445;&#25252;MANET&#20813;&#21463;&#25915;&#20987;&#24182;&#23545;&#33410;&#28857;&#36827;&#34892;&#35748;&#35777;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#25552;&#20379;&#28304;&#33410;&#28857;&#21040;&#30446;&#30340;&#33410;&#28857;&#30340;&#23433;&#20840;&#12289;&#32463;&#36807;&#35748;&#35777;&#21644;&#38450;&#31713;&#25913;&#30340;&#36335;&#30001;&#26469;&#20445;&#25252;&#25511;&#21046;&#21644;&#25968;&#25454;&#27969;&#65292;&#38450;&#33539;&#23041;&#32961;&#12290;&#20351;&#29992;NS2&#27169;&#25311;&#22120;&#35780;&#20272;&#20102;SRABC&#31639;&#27861;&#30340;&#20851;&#38190;&#24615;&#33021;&#21442;&#25968;&#65292;&#22914;&#25968;&#25454;&#21253;&#20256;&#36882;&#29575;&#12289;&#31471;&#21040;&#31471;&#24310;&#36831;&#21644;&#21534;&#21520;&#37327;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;SRABC&#31639;&#27861;&#20248;&#20110;&#24403;&#21069;&#26041;&#27861;&#65292;&#26159;&#30830;&#20445;MANET&#23433;&#20840;&#30340;&#21487;&#34892;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
MANET is a collection of mobile nodes that communicate through wireless networks as they move from one point to another. MANET is an infrastructure-less network with a changeable topology; as a result, it is very susceptible to attacks. MANET attack prevention represents a serious difficulty. Malicious network nodes are the source of network-based attacks. In a MANET, attacks can take various forms, and each one alters the network's operation in its unique way. In general, attacks can be separated into two categories: those that target the data traffic on a network and those that target the control traffic. This article explains the many sorts of assaults, their impact on MANET, and the MANET-based defence measures that are currently in place. The suggested SRA that employs blockchain technology (SRABC) protects MANET from attacks and authenticates nodes. The secure routing algorithm (SRA) proposed by blockchain technology safeguards control and data flow against threats. This is achie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#24565;&#20540;&#29942;&#39048;&#27169;&#22411;LACE&#65292;&#29992;&#20110;&#21487;&#25511;&#25991;&#26412;&#25512;&#33616;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#29992;&#25143;&#25991;&#26723;&#23398;&#20064;&#20010;&#24615;&#21270;&#30340;&#27010;&#24565;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#22810;&#31181;&#20132;&#20114;&#26041;&#24335;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#25511;&#21046;&#25512;&#33616;&#30340;&#26426;&#21046;&#65292;&#39564;&#35777;&#20102;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#23454;&#39564;&#20013;&#35813;&#27169;&#22411;&#30340;&#25512;&#33616;&#36136;&#37327;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.04250</link><description>&lt;p&gt;
&#21487;&#32534;&#36753;&#29992;&#25143;&#26723;&#26696;&#30340;&#21487;&#25511;&#25991;&#26412;&#25512;&#33616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Editable User Profiles for Controllable Text Recommendation. (arXiv:2304.04250v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#24565;&#20540;&#29942;&#39048;&#27169;&#22411;LACE&#65292;&#29992;&#20110;&#21487;&#25511;&#25991;&#26412;&#25512;&#33616;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#29992;&#25143;&#25991;&#26723;&#23398;&#20064;&#20010;&#24615;&#21270;&#30340;&#27010;&#24565;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#22810;&#31181;&#20132;&#20114;&#26041;&#24335;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#25511;&#21046;&#25512;&#33616;&#30340;&#26426;&#21046;&#65292;&#39564;&#35777;&#20102;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#23454;&#39564;&#20013;&#35813;&#27169;&#22411;&#30340;&#25512;&#33616;&#36136;&#37327;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#39640;&#36136;&#37327;&#25512;&#33616;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#20174;&#20132;&#20114;&#25968;&#25454;&#20013;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#12290;&#28982;&#32780;&#36825;&#20123;&#26041;&#27861;&#27809;&#26377;&#25552;&#20379;&#32473;&#29992;&#25143;&#25511;&#21046;&#25152;&#25509;&#25910;&#30340;&#25512;&#33616;&#30340;&#26426;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;LACE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#24565;&#20540;&#29942;&#39048;&#27169;&#22411;&#65292;&#29992;&#20110;&#21487;&#25511;&#25991;&#26412;&#25512;&#33616;&#12290;LACE&#22522;&#20110;&#29992;&#25143;&#20132;&#20114;&#30340;&#25991;&#26723;&#26816;&#32034;&#65292;&#23558;&#27599;&#20010;&#29992;&#25143;&#34920;&#31034;&#20026;&#31616;&#27905;&#30340;&#21487;&#35835;&#30340;&#27010;&#24565;&#38598;&#65292;&#24182;&#22522;&#20110;&#29992;&#25143;&#25991;&#26723;&#23398;&#20064;&#27010;&#24565;&#30340;&#20010;&#24615;&#21270;&#34920;&#31034;&#12290;&#35813;&#22522;&#20110;&#27010;&#24565;&#30340;&#29992;&#25143;&#26723;&#26696;&#34987;&#21033;&#29992;&#26469;&#20570;&#20986;&#25512;&#33616;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#35774;&#35745;&#36890;&#36807;&#36879;&#26126;&#30340;&#29992;&#25143;&#26723;&#26696;&#65292;&#25552;&#20379;&#20102;&#25511;&#21046;&#25512;&#33616;&#30340;&#22810;&#31181;&#30452;&#35266;&#20132;&#20114;&#26041;&#24335;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#19977;&#20010;&#25512;&#33616;&#20219;&#21153;&#65288;&#28201;&#21551;&#21160;&#12289;&#20919;&#21551;&#21160;&#21644;&#38646;&#26679;&#26412;&#65289;&#30340;&#20845;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#31163;&#32447;&#35780;&#20272;&#65292;&#39564;&#35777;&#20102;&#20174;LACE&#33719;&#24471;&#30340;&#25512;&#33616;&#36136;&#37327;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#22312;&#22312;&#32447;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;LACE&#30340;&#26377;&#25928;&#24615;&#21644;&#29992;&#25143;&#25511;&#21046;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods for making high-quality recommendations often rely on learning latent representations from interaction data. These methods, while performant, do not provide ready mechanisms for users to control the recommendation they receive. Our work tackles this problem by proposing LACE, a novel concept value bottleneck model for controllable text recommendations. LACE represents each user with a succinct set of human-readable concepts through retrieval given user-interacted documents and learns personalized representations of the concepts based on user documents. This concept based user profile is then leveraged to make recommendations. The design of our model affords control over the recommendations through a number of intuitive interactions with a transparent user profile. We first establish the quality of recommendations obtained from LACE in an offline evaluation on three recommendation tasks spanning six datasets in warm-start, cold-start, and zero-shot setups. Next, we validate the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21152;&#24378;&#20102;&#19968;&#20123;&#20808;&#21069;&#24369;&#19968;&#20123;&#30340;&#38543;&#26426;&#26862;&#26519;&#21464;&#20307;&#30340;&#35777;&#26126;&#26041;&#27861;&#24182;&#25552;&#39640;&#20102;&#36825;&#20123;&#21464;&#20307;&#30340;&#25968;&#25454;&#21033;&#29992;&#29575;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#29702;&#35770;&#24615;&#33021;&#21644;&#23454;&#39564;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#22810;&#39033;&#24335;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#65292;&#20854;&#22797;&#26434;&#24230;&#20302;&#19988;&#28385;&#36275;&#24378;&#19968;&#33268;&#24615;&#65292;&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#38382;&#39064;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#36229;&#36807;&#20102;&#26631;&#20934;&#38543;&#26426;&#26862;&#26519;&#12290;</title><link>http://arxiv.org/abs/2304.04240</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#22810;&#39033;&#24335;&#38543;&#26426;&#26862;&#26519;
&lt;/p&gt;
&lt;p&gt;
Data-driven multinomial random forest. (arXiv:2304.04240v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21152;&#24378;&#20102;&#19968;&#20123;&#20808;&#21069;&#24369;&#19968;&#20123;&#30340;&#38543;&#26426;&#26862;&#26519;&#21464;&#20307;&#30340;&#35777;&#26126;&#26041;&#27861;&#24182;&#25552;&#39640;&#20102;&#36825;&#20123;&#21464;&#20307;&#30340;&#25968;&#25454;&#21033;&#29992;&#29575;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#29702;&#35770;&#24615;&#33021;&#21644;&#23454;&#39564;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#22810;&#39033;&#24335;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#65292;&#20854;&#22797;&#26434;&#24230;&#20302;&#19988;&#28385;&#36275;&#24378;&#19968;&#33268;&#24615;&#65292;&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#38382;&#39064;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#36229;&#36807;&#20102;&#26631;&#20934;&#38543;&#26426;&#26862;&#26519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21152;&#24378;&#20102;&#19968;&#20123;&#20808;&#21069;&#24369;&#19968;&#20123;&#30340;&#38543;&#26426;&#26862;&#26519;&#21464;&#20307;&#30340;&#35777;&#26126;&#26041;&#27861;&#65292;&#20351;&#20854;&#25104;&#20026;&#24378;&#19968;&#33268;&#24615;&#30340;&#35777;&#26126;&#26041;&#27861;&#65292;&#24182;&#25552;&#39640;&#20102;&#36825;&#20123;&#21464;&#20307;&#30340;&#25968;&#25454;&#21033;&#29992;&#29575;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#29702;&#35770;&#24615;&#33021;&#21644;&#23454;&#39564;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#22810;&#39033;&#24335;&#38543;&#26426;&#26862;&#26519;&#65288;MRF&#65289;&#21644;&#20271;&#21162;&#21033;&#38543;&#26426;&#26862;&#26519;&#65288;BRF&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#22810;&#39033;&#24335;&#38543;&#26426;&#26862;&#26519;&#65288;DMRF&#65289;&#31639;&#27861;&#65292;&#20854;&#27604;MRF&#30340;&#22797;&#26434;&#24230;&#26356;&#20302;&#65292;&#27604;BRF&#30340;&#22797;&#26434;&#24230;&#26356;&#39640;&#65292;&#21516;&#26102;&#28385;&#36275;&#24378;&#19968;&#33268;&#24615;&#12290;&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#38382;&#39064;&#19978;&#65292;&#23427;&#27604;&#20043;&#21069;&#21482;&#28385;&#36275;&#24369;&#19968;&#33268;&#24615;&#30340;RF&#21464;&#20307;&#34920;&#29616;&#26356;&#22909;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#26631;&#20934;&#38543;&#26426;&#26862;&#26519;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;DMRF&#26159;&#30446;&#21069;&#26368;&#20986;&#33394;&#30340;&#20302;&#31639;&#27861;&#22797;&#26434;&#24615;&#30340;&#24378;&#38543;&#26426;&#26862;&#26519;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article, we strengthen the proof methods of some previously weakly consistent variants of random forests into strongly consistent proof methods, and improve the data utilization of these variants, in order to obtain better theoretical properties and experimental performance. In addition, based on the multinomial random forest (MRF) and Bernoulli random forest (BRF), we propose a data-driven multinomial random forest (DMRF) algorithm, which has lower complexity than MRF and higher complexity than BRF while satisfying strong consistency. It has better performance in classification and regression problems than previous RF variants that only satisfy weak consistency, and in most cases even surpasses standard random forest. To the best of our knowledge, DMRF is currently the most excellent strongly consistent RF variant with low algorithm complexity
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21464;&#20998;&#31639;&#23376;&#23398;&#20064;&#65288;VOL&#65289;&#30340;&#33539;&#24335;&#65292;&#21516;&#26102;&#35757;&#32451;&#31070;&#32463;&#31639;&#23376;&#21644;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#12290;&#20351;&#29992;&#27491;&#21453;&#20256;&#36882;&#24490;&#29615;&#21644;&#33258;&#21160;&#24494;&#20998;&#23454;&#29616;&#20102;&#21464;&#20998;&#25805;&#20316;&#65292;&#36890;&#36807;&#26368;&#36895;&#19979;&#38477;&#27861;&#21644;&#20849;&#36717;&#26799;&#24230;&#27861;&#36827;&#34892;&#31070;&#32463;&#31639;&#23376;&#30340;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#38750;&#24120;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.04234</link><description>&lt;p&gt;
&#21464;&#20998;&#31639;&#23376;&#23398;&#20064;&#65306;&#19968;&#31181;&#35757;&#32451;&#31070;&#32463;&#31639;&#23376;&#21644;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#32479;&#19968;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Variational operator learning: A unified paradigm for training neural operators and solving partial differential equations. (arXiv:2304.04234v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21464;&#20998;&#31639;&#23376;&#23398;&#20064;&#65288;VOL&#65289;&#30340;&#33539;&#24335;&#65292;&#21516;&#26102;&#35757;&#32451;&#31070;&#32463;&#31639;&#23376;&#21644;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#12290;&#20351;&#29992;&#27491;&#21453;&#20256;&#36882;&#24490;&#29615;&#21644;&#33258;&#21160;&#24494;&#20998;&#23454;&#29616;&#20102;&#21464;&#20998;&#25805;&#20316;&#65292;&#36890;&#36807;&#26368;&#36895;&#19979;&#38477;&#27861;&#21644;&#20849;&#36717;&#26799;&#24230;&#27861;&#36827;&#34892;&#31070;&#32463;&#31639;&#23376;&#30340;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#38750;&#24120;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#26041;&#27861;&#30340;&#26032;&#33539;&#24335;&#65292;&#20026;&#35757;&#32451;&#31070;&#32463;&#31639;&#23376;&#21644;&#29992;&#21464;&#20998;&#24418;&#24335;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#21464;&#20998;&#31639;&#23376;&#23398;&#20064;&#65288;VOL&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;&#31070;&#32463;&#31639;&#23376;&#32473;&#20986;&#30340;&#33410;&#28857;&#35299;&#39044;&#27979;&#20013;&#25512;&#23548;&#20986;&#31995;&#32479;&#30340;&#20989;&#25968;&#36924;&#36817;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#24494;&#20998;&#36827;&#34892;&#21464;&#20998;&#25805;&#20316;&#65292;&#26500;&#24314;&#27491;&#21453;&#20256;&#36882;&#24490;&#29615;&#26469;&#25512;&#23548;&#32447;&#24615;&#31995;&#32479;&#30340;&#27531;&#24046;&#12290;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#26368;&#36895;&#19979;&#38477;&#27861;&#65288;SD&#65289;&#21644;&#20849;&#36717;&#26799;&#24230;&#27861;&#65288;CG&#65289;&#30340;&#19968;&#20010;&#25110;&#22810;&#20010;&#26356;&#26032;&#27493;&#39588;&#65292;&#20316;&#20026;&#35757;&#32451;&#31070;&#32463;&#31639;&#23376;&#30340;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26356;&#26032;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;VOL&#21487;&#20197;&#23398;&#20064;&#21040;&#22312;&#31283;&#23450;&#20256;&#28909;&#21644;&#21464;&#21018;&#24230;&#24377;&#24615;PDE&#20013;&#21508;&#31181;&#35299;&#31639;&#23376;&#65292;&#32467;&#26524;&#20196;&#20154;&#28385;&#24847;&#65292;&#35823;&#24046;&#36739;&#23567;&#12290;&#35813;&#26041;&#27861;&#20960;&#20046;&#23454;&#29616;&#26080;&#26631;&#31614;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Based on the variational method, we propose a novel paradigm that provides a unified framework of training neural operators and solving partial differential equations (PDEs) with the variational form, which we refer to as the variational operator learning (VOL). We first derive the functional approximation of the system from the node solution prediction given by neural operators, and then conduct the variational operation by automatic differentiation, constructing a forward-backward propagation loop to derive the residual of the linear system. One or several update steps of the steepest decent method (SD) and the conjugate gradient method (CG) are provided in every iteration as a cheap yet effective update for training the neural operators. Experimental results show the proposed VOL can learn a variety of solution operators in PDEs of the steady heat transfer and the variable stiffness elasticity with satisfactory results and small error. The proposed VOL achieves nearly label-free tra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;RISC&#30340;Python&#24320;&#28304;&#21253;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#23427;&#21487;&#20197;&#29983;&#25104;&#31867;&#20284;&#20110;&#39745;&#21271;&#20811;&#30465;&#27861;&#35268;&#20445;&#38505;&#26684;&#24335;&#30340;&#27773;&#36710;&#20445;&#38505;&#21512;&#21516;&#65292;&#21253;&#25324;&#27861;&#35821;&#21644;&#33521;&#35821;&#29256;&#26412;&#65292;&#22522;&#20110;&#27492;&#29983;&#25104;&#30340;RISCBAC&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;NLP&#30740;&#31350;&#20013;&#30340;&#26080;&#30417;&#30563;&#33258;&#21160;&#25688;&#35201;&#12289;&#38382;&#31572;&#12289;&#25991;&#26412;&#31616;&#21270;&#12289;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#65292;&#20197;&#21450;&#30417;&#30563;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.04212</link><description>&lt;p&gt;
RISC: &#29983;&#25104;&#30495;&#23454;&#30340;&#21452;&#35821;&#20445;&#38505;&#21512;&#21516;&#30340;&#21512;&#25104;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
RISC: Generating Realistic Synthetic Bilingual Insurance Contract. (arXiv:2304.04212v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;RISC&#30340;Python&#24320;&#28304;&#21253;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#23427;&#21487;&#20197;&#29983;&#25104;&#31867;&#20284;&#20110;&#39745;&#21271;&#20811;&#30465;&#27861;&#35268;&#20445;&#38505;&#26684;&#24335;&#30340;&#27773;&#36710;&#20445;&#38505;&#21512;&#21516;&#65292;&#21253;&#25324;&#27861;&#35821;&#21644;&#33521;&#35821;&#29256;&#26412;&#65292;&#22522;&#20110;&#27492;&#29983;&#25104;&#30340;RISCBAC&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;NLP&#30740;&#31350;&#20013;&#30340;&#26080;&#30417;&#30563;&#33258;&#21160;&#25688;&#35201;&#12289;&#38382;&#31572;&#12289;&#25991;&#26412;&#31616;&#21270;&#12289;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#65292;&#20197;&#21450;&#30417;&#30563;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;RISC&#30340;Python&#24320;&#28304;&#21253;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#23427;&#21487;&#20197;&#29983;&#25104;&#31867;&#20284;&#20110;&#39745;&#21271;&#20811;&#30465;&#27861;&#35268;&#20445;&#38505;&#26684;&#24335;&#30340;&#27773;&#36710;&#20445;&#38505;&#21512;&#21516;&#65292;&#21253;&#25324;&#27861;&#35821;&#21644;&#33521;&#35821;&#29256;&#26412;&#12290;&#20445;&#38505;&#21512;&#21516;&#36890;&#24120;&#38271;&#36798;90&#21040;100&#39029;&#65292;&#20351;&#29992;&#19987;&#19994;&#30340;&#27861;&#24459;&#21644;&#20445;&#38505;&#26415;&#35821;&#65292;&#22240;&#27492;&#27604;&#20256;&#32479;&#30340;NLP&#35821;&#26009;&#24211;&#20013;&#30340;&#25991;&#26723;&#31867;&#21035;&#26356;&#20026;&#22797;&#26434;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22522;&#20110;&#39745;&#21271;&#20811;&#30465;&#24378;&#21046;&#27773;&#36710;&#20445;&#38505;&#21512;&#21516;&#30340;&#36924;&#30495;&#20445;&#38505;&#21512;&#25104;&#21452;&#35821;&#27773;&#36710;&#21512;&#21516;&#25968;&#25454;&#38598;RISCBAC&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;10,000&#20221;&#26410;&#32463;&#26631;&#27880;&#30340;&#27861;&#35821;&#21644;&#33521;&#35821;&#20445;&#38505;&#21512;&#21516;&#12290;RISCBAC&#21487;&#29992;&#20110;NLP&#30740;&#31350;&#20013;&#30340;&#26080;&#30417;&#30563;&#33258;&#21160;&#25688;&#35201;&#12289;&#38382;&#31572;&#12289;&#25991;&#26412;&#31616;&#21270;&#12289;&#26426;&#22120;&#32763;&#35793;&#31561;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#21487;&#20197;&#36827;&#19968;&#27493;&#33258;&#21160;&#26631;&#27880;&#20026;&#30417;&#30563;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents RISC, an open-source Python package data generator (https://github.com/GRAAL-Research/risc). RISC generates look-alike automobile insurance contracts based on the Quebec regulatory insurance form in French and English. Insurance contracts are 90 to 100 pages long and use complex legal and insurance-specific vocabulary for a layperson. Hence, they are a much more complex class of documents than those in traditional NLP corpora. Therefore, we introduce RISCBAC, a Realistic Insurance Synthetic Bilingual Automobile Contract dataset based on the mandatory Quebec car insurance contract. The dataset comprises 10,000 French and English unannotated insurance contracts. RISCBAC enables NLP research for unsupervised automatic summarisation, question answering, text simplification, machine translation and more. Moreover, it can be further automatically annotated as a dataset for supervised tasks such as NER
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#23545;&#27604;&#30340;&#23545;&#25239;&#29983;&#25104;&#24335;&#24322;&#24120;&#26816;&#27979;&#27169;&#24335;AGAD&#65292;&#23427;&#25104;&#21151;&#35299;&#20915;&#20102;&#22240;&#32570;&#23569;&#24322;&#24120;&#25968;&#25454;&#32780;&#36896;&#25104;&#30340;&#31283;&#20581;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#20026;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#22330;&#26223;&#29983;&#25104;&#20266;&#24322;&#24120;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2304.04211</link><description>&lt;p&gt;
AGAD: &#23545;&#25239;&#29983;&#25104;&#24335;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
AGAD: Adversarial Generative Anomaly Detection. (arXiv:2304.04211v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04211
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#23545;&#27604;&#30340;&#23545;&#25239;&#29983;&#25104;&#24335;&#24322;&#24120;&#26816;&#27979;&#27169;&#24335;AGAD&#65292;&#23427;&#25104;&#21151;&#35299;&#20915;&#20102;&#22240;&#32570;&#23569;&#24322;&#24120;&#25968;&#25454;&#32780;&#36896;&#25104;&#30340;&#31283;&#20581;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#20026;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#22330;&#26223;&#29983;&#25104;&#20266;&#24322;&#24120;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#21463;&#21040;&#24322;&#24120;&#25968;&#25454;&#30340;&#32570;&#20047;&#21644;&#24322;&#24120;&#31867;&#22411;&#30340;&#22810;&#26679;&#24615;&#32780;&#22256;&#25200;&#12290;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#36890;&#24120;&#29992;&#20110;&#20165;&#21033;&#29992;&#27491;&#24120;&#25968;&#25454;&#26469;&#26816;&#27979;&#20559;&#31163;&#23398;&#20064;&#21040;&#30340;&#27491;&#24120;&#20998;&#24067;&#30340;&#24322;&#24120;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#37492;&#20110;&#23454;&#36341;&#20013;&#21482;&#33021;&#20197;&#36739;&#23567;&#30340;&#20195;&#20215;&#33719;&#24471;&#26377;&#38480;&#30340;&#24322;&#24120;&#25968;&#25454;&#65292;&#19968;&#20123;&#30740;&#31350;&#36824;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#24322;&#24120;&#25968;&#25454;&#30417;&#30563;&#22330;&#26223;&#19979;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#31283;&#20581;&#24322;&#24120;&#26816;&#27979;&#20013;&#32570;&#23569;&#24322;&#24120;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#25239;&#29983;&#25104;&#24335;&#24322;&#24120;&#26816;&#27979;&#65288;AGAD&#65289;&#65292;&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#23545;&#27604;&#30340;&#24322;&#24120;&#26816;&#27979;&#27169;&#24335;&#65292;&#23427;&#36890;&#36807;&#20174;&#22823;&#37327;&#27491;&#24120;&#26679;&#26412;&#20013;&#29983;&#25104;&#8220;&#19978;&#19979;&#25991;&#23545;&#25239;&#20449;&#24687;&#8221;&#26469;&#23398;&#20064;&#26816;&#27979;&#24322;&#24120;&#12290;&#26412;&#36136;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#22330;&#26223;&#29983;&#25104;&#20266;&#24322;&#24120;&#25968;&#25454;&#12290;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection suffered from the lack of anomalies due to the diversity of abnormalities and the difficulties of obtaining large-scale anomaly data. Semi-supervised anomaly detection methods are often used to solely leverage normal data to detect abnormalities that deviated from the learnt normality distributions. Meanwhile, given the fact that limited anomaly data can be obtained with a minor cost in practice, some researches also investigated anomaly detection methods under supervised scenarios with limited anomaly data. In order to address the lack of abnormal data for robust anomaly detection, we propose Adversarial Generative Anomaly Detection (AGAD), a self-contrast-based anomaly detection paradigm that learns to detect anomalies by generating \textit{contextual adversarial information} from the massive normal examples. Essentially, our method generates pseudo-anomaly data for both supervised and semi-supervised anomaly detection scenarios. Extensive experiments are carried ou
&lt;/p&gt;</description></item><item><title>OpenDriver&#26159;&#19968;&#20221;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#39550;&#39542;&#21592;&#29983;&#29702;&#25968;&#25454;&#38598;&#23384;&#22312;&#38382;&#39064;&#30340;&#24320;&#25918;&#36335;&#20917;&#39550;&#39542;&#21592;&#29366;&#24577;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20845;&#36724;&#24815;&#24615;&#20449;&#21495;&#21644;&#24515;&#30005;&#22270;&#20449;&#21495;&#20004;&#31181;&#27169;&#24577;&#30340;&#25968;&#25454;&#65292;&#21487;&#29992;&#20110;&#39550;&#39542;&#21592;&#21463;&#25439;&#26816;&#27979;&#21644;&#29983;&#29289;&#35782;&#21035;&#25968;&#25454;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2304.04203</link><description>&lt;p&gt;
OpenDriver: &#19968;&#20221;&#24320;&#25918;&#36335;&#20917;&#39550;&#39542;&#21592;&#29366;&#24577;&#26816;&#27979;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
OpenDriver: an open-road driver state detection dataset. (arXiv:2304.04203v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04203
&lt;/p&gt;
&lt;p&gt;
OpenDriver&#26159;&#19968;&#20221;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#39550;&#39542;&#21592;&#29983;&#29702;&#25968;&#25454;&#38598;&#23384;&#22312;&#38382;&#39064;&#30340;&#24320;&#25918;&#36335;&#20917;&#39550;&#39542;&#21592;&#29366;&#24577;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20845;&#36724;&#24815;&#24615;&#20449;&#21495;&#21644;&#24515;&#30005;&#22270;&#20449;&#21495;&#20004;&#31181;&#27169;&#24577;&#30340;&#25968;&#25454;&#65292;&#21487;&#29992;&#20110;&#39550;&#39542;&#21592;&#21463;&#25439;&#26816;&#27979;&#21644;&#29983;&#29289;&#35782;&#21035;&#25968;&#25454;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#31038;&#20250;&#20013;&#65292;&#36947;&#36335;&#23433;&#20840;&#20005;&#37325;&#20381;&#36182;&#20110;&#39550;&#39542;&#21592;&#30340;&#24515;&#29702;&#21644;&#29983;&#29702;&#29366;&#24577;&#12290;&#30130;&#21171;&#12289;&#26127;&#26127;&#27442;&#30561;&#21644;&#21387;&#21147;&#31561;&#36127;&#38754;&#22240;&#32032;&#20250;&#24433;&#21709;&#39550;&#39542;&#21592;&#30340;&#21453;&#24212;&#26102;&#38388;&#21644;&#20915;&#31574;&#33021;&#21147;&#65292;&#23548;&#33268;&#20132;&#36890;&#20107;&#25925;&#30340;&#21457;&#29983;&#29575;&#22686;&#21152;&#12290;&#22312;&#20247;&#22810;&#30340;&#39550;&#39542;&#21592;&#34892;&#20026;&#30417;&#27979;&#30740;&#31350;&#20013;&#65292;&#21487;&#31359;&#25140;&#29983;&#29702;&#27979;&#37327;&#26159;&#19968;&#31181;&#23454;&#26102;&#30417;&#27979;&#39550;&#39542;&#21592;&#29366;&#24577;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22312;&#24320;&#25918;&#36947;&#36335;&#22330;&#26223;&#19979;&#65292;&#32570;&#23569;&#39550;&#39542;&#21592;&#29983;&#29702;&#25968;&#25454;&#38598;&#65292;&#24050;&#26377;&#30340;&#25968;&#25454;&#38598;&#23384;&#22312;&#20449;&#21495;&#36136;&#37327;&#24046;&#12289;&#26679;&#26412;&#37327;&#23567;&#21644;&#25968;&#25454;&#25910;&#38598;&#26102;&#38388;&#30701;&#31561;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#35774;&#35745;&#24182;&#25551;&#36848;&#20102;&#19968;&#31181;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#39550;&#39542;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#39550;&#39542;&#21592;&#21463;&#25439;&#26816;&#27979;&#21644;&#29983;&#29289;&#35782;&#21035;&#25968;&#25454;&#35782;&#21035;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20004;&#31181;&#39550;&#39542;&#20449;&#21495;&#27169;&#24577;&#65306;&#20845;&#36724;&#24815;&#24615;&#20449;&#21495;&#21644;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#20449;&#21495;&#65292;&#36825;&#20123;&#20449;&#21495;&#26159;&#22312;100&#22810;&#21517;&#39550;&#39542;&#21592;&#36981;&#24490;&#30456;&#21516;&#36335;&#32447;&#34892;&#39542;&#26102;&#35760;&#24405;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In modern society, road safety relies heavily on the psychological and physiological state of drivers. Negative factors such as fatigue, drowsiness, and stress can impair drivers' reaction time and decision making abilities, leading to an increased incidence of traffic accidents. Among the numerous studies for impaired driving detection, wearable physiological measurement is a real-time approach to monitoring a driver's state. However, currently, there are few driver physiological datasets in open road scenarios and the existing datasets suffer from issues such as poor signal quality, small sample sizes, and short data collection periods. Therefore, in this paper, a large-scale multimodal driving dataset for driver impairment detection and biometric data recognition is designed and described. The dataset contains two modalities of driving signals: six-axis inertial signals and electrocardiogram (ECG) signals, which were recorded while over one hundred drivers were following the same ro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#27979;&#35797;&#21644;&#35843;&#35797;&#26694;&#26550;DICE&#65292;&#29992;&#20110;&#34913;&#37327;DNN&#20013;&#20844;&#24179;&#24615;&#32570;&#38519;&#30340;&#20005;&#37325;&#24615;&#21450;&#20854;&#26681;&#28304;&#65292;&#20197;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#36827;&#34892;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2304.04199</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#20844;&#24179;&#24615;&#32570;&#38519;&#30340;&#20449;&#24687;&#35770;&#27979;&#35797;&#21644;&#35843;&#35797;
&lt;/p&gt;
&lt;p&gt;
Information-Theoretic Testing and Debugging of Fairness Defects in Deep Neural Networks. (arXiv:2304.04199v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#27979;&#35797;&#21644;&#35843;&#35797;&#26694;&#26550;DICE&#65292;&#29992;&#20110;&#34913;&#37327;DNN&#20013;&#20844;&#24179;&#24615;&#32570;&#38519;&#30340;&#20005;&#37325;&#24615;&#21450;&#20854;&#26681;&#28304;&#65292;&#20197;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#36827;&#34892;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;(DNN)&#22312;&#21508;&#31181;&#20915;&#31574;&#25903;&#25345;&#36719;&#20214;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290; DNN&#22312;&#23427;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#33021;&#22815;&#23547;&#25214;&#21040;&#26368;&#23567;&#12289;&#36275;&#22815;&#30340;&#32479;&#35745;&#27169;&#24335;&#12290;&#22240;&#27492;&#65292;DNN&#21487;&#33021;&#23398;&#20064;&#21040;&#32534;&#30721;&#20915;&#31574;&#30340;&#33021;&#21147;&#8212;&#8212;&#25918;&#22823;&#29616;&#26377;&#30340;&#20559;&#35265;&#25110;&#24341;&#20837;&#26032;&#30340;&#20559;&#35265;&#8212;&#8212;&#36825;&#20123;&#21487;&#33021;&#20250;&#20351;&#21463;&#20445;&#25252;&#30340;&#20010;&#20307;/&#32676;&#20307;&#22788;&#20110;&#19981;&#21033;&#22320;&#20301;&#65292;&#24182;&#21487;&#33021;&#36829;&#21453;&#27861;&#24459;&#20445;&#25252;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#22522;&#20110;&#25628;&#32034;&#30340;&#36719;&#20214;&#27979;&#35797;&#26041;&#27861;&#22312;&#21457;&#29616;&#20844;&#27491;&#24615;&#32570;&#38519;&#26041;&#38754;&#25928;&#26524;&#24456;&#22909;&#65292;&#20294;&#23427;&#20204;&#27809;&#26377;&#25552;&#20379;&#19982;&#20005;&#37325;&#24615;&#21644;&#22240;&#26524;&#35299;&#37322;&#31561;&#35843;&#35797;&#36741;&#21161;&#24037;&#20855;&#65292;&#36825;&#23545;&#20110;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#36827;&#34892;&#22788;&#29702;&#24182;&#20915;&#23450;&#19979;&#19968;&#27493;&#34892;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#33021;&#21542;&#24230;&#37327;DNN&#20013;&#20844;&#24179;&#24615;&#32570;&#38519;&#30340;&#20005;&#37325;&#24615;&#65311;&#36825;&#20123;&#32570;&#38519;&#26159;&#30001;&#19981;&#24403;&#30340;&#35757;&#32451;&#24341;&#36215;&#30340;&#65292;&#36824;&#26159;&#20165;&#20165;&#21453;&#26144;&#20102;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DICE&#65306;&#19968;&#20010;&#20449;&#24687;&#35770;&#27979;&#35797;&#21644;&#35843;&#35797;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The deep feedforward neural networks (DNNs) are increasingly deployed in socioeconomic critical decision support software systems. DNNs are exceptionally good at finding minimal, sufficient statistical patterns within their training data. Consequently, DNNs may learn to encode decisions -amplifying existing biases or introducing new ones -- that may disadvantage protected individuals/groups and may stand to violate legal protections. While the existing search based software testing approaches have been effective in discovering fairness defects, they do not supplement these defects with debugging aids -- such as severity and causal explanations -- crucial to help developers triage and decide on the next course of action. Can we measure the severity of fairness defects in DNNs? Are these defects symptomatic of improper training or they merely reflect biases present in the training data? To answer such questions, we present DICE: an information-theoretic testing and debugging framework 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#36229;&#32593;&#32476;&#26550;&#26500;HyperINR&#65292;&#33021;&#22815;&#39640;&#25928;&#24555;&#36895;&#22320;&#26500;&#24314;&#20986;&#26368;&#20808;&#36827;&#25512;&#29702;&#24615;&#33021;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INR&#65289;&#65292;&#24182;&#19988;&#25903;&#25345;&#20132;&#20114;&#24335;&#30340;&#29031;&#29255;&#32423;&#20307;&#31215;&#21487;&#35270;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.04188</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#36229;&#32593;&#32476;HyperINR&#65306;&#39640;&#25928;&#24555;&#36895;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
HyperINR: A Fast and Predictive Hypernetwork for Implicit Neural Representations via Knowledge Distillation. (arXiv:2304.04188v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#36229;&#32593;&#32476;&#26550;&#26500;HyperINR&#65292;&#33021;&#22815;&#39640;&#25928;&#24555;&#36895;&#22320;&#26500;&#24314;&#20986;&#26368;&#20808;&#36827;&#25512;&#29702;&#24615;&#33021;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INR&#65289;&#65292;&#24182;&#19988;&#25903;&#25345;&#20132;&#20114;&#24335;&#30340;&#29031;&#29255;&#32423;&#20307;&#31215;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INRs&#65289;&#22312;&#31185;&#23398;&#21487;&#35270;&#21270;&#39046;&#22495;&#30340;&#25968;&#25454;&#29983;&#25104;&#21644;&#21487;&#35270;&#21270;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#34920;&#31034;&#36890;&#24120;&#30001;&#22823;&#22411;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLPs&#65289;&#32452;&#25104;&#65292;&#23545;&#20110;&#21333;&#27425;&#21069;&#20256;&#38656;&#35201;&#25968;&#30334;&#19975;&#25805;&#20316;&#65292;&#20174;&#32780;&#38459;&#30861;&#20132;&#20114;&#24335;&#35270;&#35273;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;HyperINR&#65292;&#19968;&#31181;&#26032;&#22411;&#36229;&#32593;&#32476;&#26550;&#26500;&#65292;&#33021;&#22815;&#30452;&#25509;&#39044;&#27979;&#32039;&#20945;INR&#30340;&#26435;&#37325;&#65292;&#36890;&#36807;&#21516;&#27493;&#21033;&#29992;&#22810;&#20998;&#36776;&#29575;&#21704;&#24076;&#32534;&#30721;&#21333;&#20803;&#30340;&#38598;&#21512;&#65292;&#20174;&#32780;&#20351;&#24471;&#32467;&#26524;INR&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#25512;&#29702;&#24615;&#33021;(&#26368;&#39640;&#21487;&#36798;100&#20493;&#30340;&#25512;&#29702;&#24102;&#23485;)&#65292;&#24182;&#19988;&#33021;&#22815;&#25903;&#25345;&#20132;&#20114;&#24335;&#30340;&#29031;&#29255;&#32423;&#20307;&#31215;&#21487;&#35270;&#21270;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#20351;&#24471;HyperINR&#21363;&#20415;&#22312;&#23384;&#22312;&#26410;&#35265;&#36807;&#30340;&#21442;&#25968;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#33391;&#22909;&#22320;&#36827;&#34892;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit Neural Representations (INRs) have recently exhibited immense potential in the field of scientific visualization for both data generation and visualization tasks. However, these representations often consist of large multi-layer perceptrons (MLPs), necessitating millions of operations for a single forward pass, consequently hindering interactive visual exploration. While reducing the size of the MLPs and employing efficient parametric encoding schemes can alleviate this issue, it compromises generalizability for unseen parameters, rendering it unsuitable for tasks such as temporal super-resolution. In this paper, we introduce HyperINR, a novel hypernetwork architecture capable of directly predicting the weights for a compact INR. By harnessing an ensemble of multiresolution hash encoding units in unison, the resulting INR attains state-of-the-art inference performance (up to 100x higher inference bandwidth) and can support interactive photo-realistic volume visualization. Addi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#25277;&#26679;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#26041;&#27861;&#65292;&#36890;&#36807;1-&#26368;&#36817;&#37051;&#26469;&#36817;&#20284;&#32534;&#30721;&#38646;&#20551;&#35774;&#30340;&#26465;&#20214;&#20998;&#24067;&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#29983;&#25104;&#26679;&#26412;&#30340;&#20998;&#24067;&#38750;&#24120;&#25509;&#36817;&#30495;&#23454;&#30340;&#26465;&#20214;&#20998;&#24067;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#37319;&#29992;&#20998;&#31867;&#22120;-based&#26465;&#20214;&#20114;&#20449;&#24687;&#20272;&#35745;&#22120;&#20316;&#20026;&#26816;&#39564;&#32479;&#35745;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.04183</link><description>&lt;p&gt;
&#22522;&#20110;&#26368;&#36817;&#37051;&#25277;&#26679;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Nearest-Neighbor Sampling Based Conditional Independence Testing. (arXiv:2304.04183v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#25277;&#26679;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#26041;&#27861;&#65292;&#36890;&#36807;1-&#26368;&#36817;&#37051;&#26469;&#36817;&#20284;&#32534;&#30721;&#38646;&#20551;&#35774;&#30340;&#26465;&#20214;&#20998;&#24067;&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#29983;&#25104;&#26679;&#26412;&#30340;&#20998;&#24067;&#38750;&#24120;&#25509;&#36817;&#30495;&#23454;&#30340;&#26465;&#20214;&#20998;&#24067;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#37319;&#29992;&#20998;&#31867;&#22120;-based&#26465;&#20214;&#20114;&#20449;&#24687;&#20272;&#35745;&#22120;&#20316;&#20026;&#26816;&#39564;&#32479;&#35745;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#38543;&#26426;&#21270;&#26816;&#39564;&#65288;CRT&#65289;&#26368;&#36817;&#34987;&#25552;&#20986;&#29992;&#20110;&#27979;&#35797;&#38543;&#26426;&#21464;&#37327;X&#21644;Y&#22312;&#32473;&#23450;&#38543;&#26426;&#21464;&#37327;Z&#30340;&#26465;&#20214;&#19979;&#26159;&#21542;&#30456;&#20114;&#29420;&#31435;&#12290;CRT&#20551;&#23450;&#22312;&#38646;&#20551;&#35774;&#19979;X&#32473;&#23450;Z&#30340;&#26465;&#20214;&#20998;&#24067;&#24050;&#30693;&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;&#21407;&#22987;&#25968;&#25454;&#30340;&#35266;&#23519;&#26679;&#26412;&#30340;&#20998;&#24067;&#30456;&#27604;&#36739;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#26368;&#36817;&#37051;&#25277;&#26679;&#26469;&#24320;&#21457;CRT&#30340;&#26032;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#32780;&#19981;&#38656;&#35201;&#20551;&#23450;X&#32473;&#23450;Z&#30340;&#20998;&#24067;&#24418;&#24335;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;1-&#26368;&#36817;&#37051;&#26469;&#36817;&#20284;&#32534;&#30721;&#38646;&#20551;&#35774;&#30340;&#26465;&#20214;&#20998;&#24067;&#12290;&#28982;&#21518;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#29983;&#25104;&#26679;&#26412;&#30340;&#20998;&#24067;&#22312;&#24635;&#21464;&#24046;&#36317;&#31163;&#26041;&#38754;&#38750;&#24120;&#25509;&#36817;&#30495;&#23454;&#30340;&#26465;&#20214;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#22522;&#20110;&#20998;&#31867;&#22120;&#30340;&#26465;&#20214;&#20114;&#20449;&#24687;&#20272;&#35745;&#22120;&#20316;&#20026;&#25105;&#20204;&#30340;&#26816;&#39564;&#32479;&#35745;&#37327;&#12290;&#26816;&#39564;&#32479;&#35745;&#37327;&#20316;&#20026;&#32463;&#39564;&#22522;&#30784;&#20449;&#24687;th&#12290;
&lt;/p&gt;
&lt;p&gt;
The conditional randomization test (CRT) was recently proposed to test whether two random variables X and Y are conditionally independent given random variables Z. The CRT assumes that the conditional distribution of X given Z is known under the null hypothesis and then it is compared to the distribution of the observed samples of the original data. The aim of this paper is to develop a novel alternative of CRT by using nearest-neighbor sampling without assuming the exact form of the distribution of X given Z. Specifically, we utilize the computationally efficient 1-nearest-neighbor to approximate the conditional distribution that encodes the null hypothesis. Then, theoretically, we show that the distribution of the generated samples is very close to the true conditional distribution in terms of total variation distance. Furthermore, we take the classifier-based conditional mutual information estimator as our test statistic. The test statistic as an empirical fundamental information th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#26368;&#36817;&#30340;&#20004;&#31181;&#19982;&#21160;&#37327;&#27010;&#24565;&#30456;&#20851;&#30340;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#31283;&#23450;&#30340;&#38543;&#26426;&#20248;&#21270;&#65292;&#23545;&#23398;&#20064;&#29575;&#36873;&#25321;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#22312;&#26080;&#22122;&#22768;&#21644;&#26377;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#36895;&#29575;&#22343;&#20026;&#26368;&#20248;&#12290;</title><link>http://arxiv.org/abs/2304.04172</link><description>&lt;p&gt;
$\mu^2$-SGD: &#36890;&#36807;&#21452;&#21160;&#37327;&#26426;&#21046;&#23454;&#29616;&#31283;&#23450;&#30340;&#38543;&#26426;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
$\mu^2$-SGD: Stable Stochastic Optimization via a Double Momentum Mechanism. (arXiv:2304.04172v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04172
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#26368;&#36817;&#30340;&#20004;&#31181;&#19982;&#21160;&#37327;&#27010;&#24565;&#30456;&#20851;&#30340;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#31283;&#23450;&#30340;&#38543;&#26426;&#20248;&#21270;&#65292;&#23545;&#23398;&#20064;&#29575;&#36873;&#25321;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#22312;&#26080;&#22122;&#22768;&#21644;&#26377;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#36895;&#29575;&#22343;&#20026;&#26368;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#30446;&#26631;&#20989;&#25968;&#20026;&#24179;&#28369;&#20989;&#25968;&#26399;&#26395;&#30340;&#38543;&#26426;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#38024;&#23545;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#24314;&#35758;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#26368;&#36817;&#30340;&#20004;&#31181;&#19982;&#21160;&#37327;&#27010;&#24565;&#30456;&#20851;&#30340;&#26426;&#21046;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;SGD&#26679;&#24335;&#30340;&#31639;&#27861;&#21644;&#19968;&#20010;&#21152;&#36895;&#29256;&#65292;&#21033;&#29992;&#36825;&#20010;&#26032;&#30340;&#20272;&#35745;&#22120;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#26032;&#26041;&#27861;&#23545;&#23398;&#20064;&#29575;&#30340;&#36873;&#25321;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#20351;&#29992;&#30456;&#21516;&#30340;&#22266;&#23450;&#23398;&#20064;&#29575;&#36873;&#25321;&#22312;&#26080;&#22122;&#22768;&#21644;&#26377;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#26377;&#22122;&#22768;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#22312;&#38750;&#24120;&#24191;&#27867;&#30340;&#23398;&#20064;&#29575;&#33539;&#22260;&#20869;&#23454;&#29616;&#20102;&#30456;&#21516;&#30340;&#26368;&#20248;&#35823;&#24046;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider stochastic convex optimization problems where the objective is an expectation over smooth functions. For this setting we suggest a novel gradient estimate that combines two recent mechanism that are related to notion of momentum. Then, we design an SGD-style algorithm as well as an accelerated version that make use of this new estimator, and demonstrate the robustness of these new approaches to the choice of the learning rate. Concretely, we show that these approaches obtain the optimal convergence rates for both noiseless and noisy case with the same choice of fixed learning rate. Moreover, for the noisy case we show that these approaches achieve the same optimal bound for a very wide range of learning rates.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24930;&#26597;&#35810;&#28857;&#25216;&#26415;&#25913;&#36827;Local-SGD&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23545;&#25239;&#24322;&#26500;&#24773;&#20917;&#19979;&#26412;&#22320;&#26356;&#26032;&#30340;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2304.04169</link><description>&lt;p&gt;
SLowcal-SGD&#65306;&#24930;&#26597;&#35810;&#28857;&#25552;&#39640;&#20102;&#38543;&#26426;&#20984;&#20248;&#21270;&#30340;Local-SGD&#31639;&#27861;&#65288;arXiv:2304.04169v1 [cs.LG]&#65289;
&lt;/p&gt;
&lt;p&gt;
SLowcal-SGD: Slow Query Points Improve Local-SGD for Stochastic Convex Optimization. (arXiv:2304.04169v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24930;&#26597;&#35810;&#28857;&#25216;&#26415;&#25913;&#36827;Local-SGD&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23545;&#25239;&#24322;&#26500;&#24773;&#20917;&#19979;&#26412;&#22320;&#26356;&#26032;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#22330;&#26223;&#20013;&#65292;M&#21488;&#35745;&#31639;&#26426;&#19982;&#21442;&#25968;&#26381;&#21153;&#22120;&#20132;&#20114;&#65292;&#36890;&#36807;&#22810;&#20010;&#36890;&#20449;&#36718;&#27425;&#26469;&#26368;&#23567;&#21270;&#32852;&#21512;&#30446;&#26631;&#20989;&#25968;&#12290;&#25105;&#20204;&#20851;&#27880;&#24322;&#26500;&#24773;&#20917;&#65292;&#21363;&#19981;&#21516;&#35745;&#31639;&#26426;&#21487;&#33021;&#20174;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#20013;&#25277;&#21462;&#26679;&#26412;&#65292;&#35774;&#35745;&#20102;&#31532;&#19968;&#31181;&#21487;&#20197;&#35777;&#26126;&#20248;&#20110;&#20004;&#20010;&#26368;&#31361;&#20986;&#30340;&#20998;&#24067;&#24335;&#22522;&#20934;&#32447;&#8212;&#8212;Minibatch-SGD&#21644;Local-SGD&#30340;&#26412;&#22320;&#26356;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#22312;&#20110;&#24930;&#26597;&#35810;&#25216;&#26415;&#65292;&#25105;&#20204;&#23558;&#20854;&#23450;&#21046;&#32473;&#20998;&#24067;&#24335;&#35774;&#32622;&#20351;&#29992;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#20943;&#36731;&#26412;&#22320;&#26356;&#26032;&#24102;&#26469;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider distributed learning scenarios where M machines interact with a parameter server along several communication rounds in order to minimize a joint objective function. Focusing on the heterogeneous case, where different machines may draw samples from different data-distributions, we design the first local update method that provably benefits over the two most prominent distributed baselines: namely Minibatch-SGD and Local-SGD. Key to our approach is a slow querying technique that we customize to the distributed setting, which in turn enables a better mitigation of the bias caused by local updates.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;G-RNA&#65292;&#19968;&#20010;&#38754;&#21521;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#40065;&#26834;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#36890;&#36807;&#28155;&#21152;&#22270;&#32467;&#26500;&#25513;&#30721;&#25805;&#20316;&#26469;&#35774;&#35745;&#20449;&#24687;&#20256;&#36882;&#26426;&#21046;&#30340;&#19968;&#20010;&#40065;&#26834;&#25628;&#32034;&#31354;&#38388;&#65292;&#20174;&#32780;&#20801;&#35768;&#25105;&#20204;&#25628;&#32034;&#38450;&#24481;&#24615;GNNs&#12290;</title><link>http://arxiv.org/abs/2304.04168</link><description>&lt;p&gt;
&#38754;&#21521;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#40065;&#26834;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Adversarially Robust Neural Architecture Search for Graph Neural Networks. (arXiv:2304.04168v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;G-RNA&#65292;&#19968;&#20010;&#38754;&#21521;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#40065;&#26834;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#36890;&#36807;&#28155;&#21152;&#22270;&#32467;&#26500;&#25513;&#30721;&#25805;&#20316;&#26469;&#35774;&#35745;&#20449;&#24687;&#20256;&#36882;&#26426;&#21046;&#30340;&#19968;&#20010;&#40065;&#26834;&#25628;&#32034;&#31354;&#38388;&#65292;&#20174;&#32780;&#20801;&#35768;&#25105;&#20204;&#25628;&#32034;&#38450;&#24481;&#24615;GNNs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#24314;&#27169;&#20851;&#31995;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#65292;&#36825;&#23545;&#23558;GNN&#24212;&#29992;&#20110;&#39118;&#38505;&#25935;&#24863;&#39046;&#22495;&#26500;&#25104;&#20102;&#24040;&#22823;&#23041;&#32961;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#26082;&#19981;&#33021;&#20445;&#35777;&#38754;&#23545;&#26032;&#25968;&#25454;/&#20219;&#21153;&#25110;&#23545;&#25239;&#25915;&#20987;&#26102;&#30340;&#24615;&#33021;&#65292;&#20063;&#19981;&#33021;&#20174;&#26550;&#26500;&#35282;&#24230;&#25552;&#20379;&#29702;&#35299;GNN&#40065;&#26834;&#24615;&#30340;&#35265;&#35299;&#12290;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#36890;&#36807;&#33258;&#21160;&#21270;GNN&#26550;&#26500;&#35774;&#35745;&#20855;&#26377;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22270;&#24418;NAS&#26041;&#27861;&#32570;&#20047;&#40065;&#26834;&#30340;&#35774;&#35745;&#65292;&#26131;&#21463;&#23545;&#25239;&#25915;&#20987;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;GNNs&#30340;&#26032;&#22411;Robust&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#26694;&#26550;&#65288;G-RNA&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#25628;&#32034;&#31354;&#38388;&#20013;&#28155;&#21152;&#22270;&#32467;&#26500;&#25513;&#30721;&#25805;&#20316;&#26469;&#35774;&#35745;&#20449;&#24687;&#20256;&#36882;&#26426;&#21046;&#30340;&#19968;&#20010;&#40065;&#26834;&#25628;&#32034;&#31354;&#38388;&#65292;&#20854;&#20013;&#21253;&#25324;&#21508;&#31181;&#38450;&#24481;&#25805;&#20316;&#20505;&#36873;&#20154;&#65292;&#24182;&#20801;&#35768;&#25105;&#20204;&#25628;&#32034;&#38450;&#24481;&#24615;GNNs&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#40065;&#26834;&#24615;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) obtain tremendous success in modeling relational data. Still, they are prone to adversarial attacks, which are massive threats to applying GNNs to risk-sensitive domains. Existing defensive methods neither guarantee performance facing new data/tasks or adversarial attacks nor provide insights to understand GNN robustness from an architectural perspective. Neural Architecture Search (NAS) has the potential to solve this problem by automating GNN architecture designs. Nevertheless, current graph NAS approaches lack robust design and are vulnerable to adversarial attacks. To tackle these challenges, we propose a novel Robust Neural Architecture search framework for GNNs (G-RNA). Specifically, we design a robust search space for the message-passing mechanism by adding graph structure mask operations into the search space, which comprises various defensive operation candidates and allows us to search for defensive GNNs. Furthermore, we define a robustness metric
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#32463;&#39564;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32463;&#39564;&#30340;&#20195;&#29702;&#36741;&#21161;&#36827;&#21270;&#31639;&#27861;(SAEA)&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#30456;&#20851;&#38382;&#39064;&#25152;&#33719;&#24471;&#30340;&#32463;&#39564;&#26469;&#26377;&#25928;&#22320;&#35299;&#20915;&#38590;&#20197;&#20248;&#21270;&#30340;&#26114;&#36149;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.04166</link><description>&lt;p&gt;
&#22522;&#20110;&#32463;&#39564;&#30340;&#36827;&#21270;&#31639;&#27861;&#29992;&#20110;&#26114;&#36149;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Experience-Based Evolutionary Algorithms for Expensive Optimization. (arXiv:2304.04166v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#32463;&#39564;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32463;&#39564;&#30340;&#20195;&#29702;&#36741;&#21161;&#36827;&#21270;&#31639;&#27861;(SAEA)&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#30456;&#20851;&#38382;&#39064;&#25152;&#33719;&#24471;&#30340;&#32463;&#39564;&#26469;&#26377;&#25928;&#22320;&#35299;&#20915;&#38590;&#20197;&#20248;&#21270;&#30340;&#26114;&#36149;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#31639;&#27861;&#19982;&#20154;&#31867;&#20248;&#21270;&#32773;&#38750;&#24120;&#19981;&#21516;&#12290;&#20154;&#31867;&#36890;&#36807;&#35299;&#20915;&#38382;&#39064;&#33719;&#24471;&#26356;&#22810;&#32463;&#39564;&#65292;&#36825;&#26377;&#21161;&#20110;&#35299;&#20915;&#26032;&#30340;&#26410;&#30693;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20248;&#21270;&#31639;&#27861;&#36890;&#36807;&#35299;&#20915;&#26356;&#22810;&#38382;&#39064;&#20174;&#19981;&#33719;&#24471;&#20219;&#20309;&#32463;&#39564;&#12290;&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#36171;&#20104;&#20248;&#21270;&#31639;&#27861;&#19968;&#20123;&#32463;&#39564;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#21363;&#22522;&#20110;&#32463;&#39564;&#30340;&#20248;&#21270;&#12290;&#26412;&#25991;&#35748;&#20026;&#21487;&#20197;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#30456;&#20851;&#38382;&#39064;&#25152;&#33719;&#24471;&#30340;&#32463;&#39564;&#26469;&#26377;&#25928;&#22320;&#35299;&#20915;&#38590;&#20197;&#20248;&#21270;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#26114;&#36149;&#20248;&#21270;&#30340;&#32972;&#26223;&#19979;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#24819;&#27861;&#65292;&#21363;&#36890;&#36807;&#23613;&#21487;&#33021;&#23569;&#30340;&#36866;&#24212;&#24230;&#35780;&#20272;&#26469;&#23547;&#25214;&#26114;&#36149;&#20248;&#21270;&#38382;&#39064;&#30340;&#36817;&#20284;&#26368;&#20248;&#35299;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32463;&#39564;&#30340;&#20195;&#29702;&#36741;&#21161;&#36827;&#21270;&#31639;&#27861;(SAEA)&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#26114;&#36149;&#38382;&#39064;&#30340;&#20248;&#21270;&#25928;&#29575;&#65292;&#20854;&#20013;&#32463;&#39564;&#26159;&#36328;&#30456;&#20851;&#38382;&#39064;&#33719;&#24471;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimization algorithms are very different from human optimizers. A human being would gain more experiences through problem-solving, which helps her/him in solving a new unseen problem. Yet an optimization algorithm never gains any experiences by solving more problems. In recent years, efforts have been made towards endowing optimization algorithms with some abilities of experience learning, which is regarded as experience-based optimization. In this paper, we argue that hard optimization problems could be tackled efficiently by making better use of experiences gained in related problems. We demonstrate our ideas in the context of expensive optimization, where we aim to find a near-optimal solution to an expensive optimization problem with as few fitness evaluations as possible. To achieve this, we propose an experience-based surrogate-assisted evolutionary algorithm (SAEA) framework to enhance the optimization efficiency of expensive problems, where experiences are gained across relat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#21738;&#20123;&#27169;&#22359;&#26356;&#23481;&#26131;&#36951;&#24536;&#65292;&#24182;&#21457;&#29616;&#21482;&#26377;&#23569;&#25968;&#27169;&#22359;&#26159;&#26356;&#21152;&#20219;&#21153;&#29305;&#23450;&#19988;&#22312;&#20219;&#21153;&#20043;&#38388;&#25935;&#24863;&#22320;&#25913;&#21464;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#36951;&#24536;&#20027;&#35201;&#24402;&#22240;&#20110;&#36825;&#20123;&#27169;&#22359;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#8220;&#36951;&#24536;&#20248;&#20808;&#24494;&#35843;&#8221;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.04158</link><description>&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#20013;&#26159;&#21542;&#25152;&#26377;&#21442;&#25968;&#37117;&#20250;&#21516;&#26679;&#36951;&#24536;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does Continual Learning Equally Forget All Parameters?. (arXiv:2304.04158v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#21738;&#20123;&#27169;&#22359;&#26356;&#23481;&#26131;&#36951;&#24536;&#65292;&#24182;&#21457;&#29616;&#21482;&#26377;&#23569;&#25968;&#27169;&#22359;&#26159;&#26356;&#21152;&#20219;&#21153;&#29305;&#23450;&#19988;&#22312;&#20219;&#21153;&#20043;&#38388;&#25935;&#24863;&#22320;&#25913;&#21464;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#36951;&#24536;&#20027;&#35201;&#24402;&#22240;&#20110;&#36825;&#20123;&#27169;&#22359;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#8220;&#36951;&#24536;&#20248;&#20808;&#24494;&#35843;&#8221;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#21464;&#21270;&#36890;&#24120;&#20250;&#23548;&#33268;&#31070;&#32463;&#32593;&#32476;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#23613;&#31649;&#36890;&#36807;&#21453;&#22797;&#37325;&#25918;&#32531;&#20914;&#25968;&#25454;&#21487;&#20197;&#20943;&#36731;&#36825;&#31181;&#24773;&#20917;&#65292;&#20294;&#27599;&#27425;&#37325;&#25918;&#38750;&#24120;&#32791;&#26102;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#21738;&#20123;&#27169;&#22359;&#26356;&#23481;&#26131;&#34987;&#36951;&#24536;&#65292;&#24182;&#36890;&#36807;&#30740;&#31350;&#20854;&#25345;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#35757;&#32451;&#21160;&#24577;&#26469;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#24230;&#37327;&#34920;&#26126;&#65292;&#21482;&#26377;&#23569;&#25968;&#27169;&#22359;&#26159;&#26356;&#21152;&#20219;&#21153;&#29305;&#23450;&#19988;&#22312;&#20219;&#21153;&#20043;&#38388;&#25935;&#24863;&#22320;&#25913;&#21464;&#30340;&#65292;&#32780;&#20854;&#20182;&#27169;&#22359;&#21487;&#20197;&#20316;&#20026;&#20849;&#21516;&#30693;&#35782;&#22312;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#36951;&#24536;&#20027;&#35201;&#24402;&#22240;&#20110;&#21069;&#32773;&#65292;&#24182;&#21457;&#29616;&#20165;&#22312;&#20219;&#20309;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#26411;&#23614;&#23545;&#23427;&#20204;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#24102;&#26469;&#38750;&#24179;&#20961;&#30340;&#25913;&#36827;&#12290;&#30001;&#20110;&#34987;&#24494;&#35843;&#30340;&#21442;&#25968;&#25968;&#37327;&#24456;&#23569;&#65292;&#22240;&#27492;&#36825;&#31181;&#8220;&#36951;&#24536;&#20248;&#20808;&#24494;&#35843;&#65288;FPF&#65289;&#8221;&#22312;&#35745;&#31639;&#19978;&#38750;&#24120;&#39640;&#25928;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#39640;&#25928;&#12289;&#26356;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#23427;&#23436;&#20840;&#21024;&#38500;&#20102;&#27599;&#27425;&#37325;&#25918;&#65292;&#24182;&#29992;&#20165; $k$ &#27425; FPF &#21608;&#26399;&#26469;&#26367;&#25442;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distribution shift (e.g., task or domain shift) in continual learning (CL) usually results in catastrophic forgetting of neural networks. Although it can be alleviated by repeatedly replaying buffered data, the every-step replay is time-consuming. In this paper, we study which modules in neural networks are more prone to forgetting by investigating their training dynamics during CL. Our proposed metrics show that only a few modules are more task-specific and sensitively alter between tasks, while others can be shared across tasks as common knowledge. Hence, we attribute forgetting mainly to the former and find that finetuning them only on a small buffer at the end of any CL method can bring non-trivial improvement. Due to the small number of finetuned parameters, such ``Forgetting Prioritized Finetuning (FPF)'' is efficient in computation. We further propose a more efficient and simpler method that entirely removes the every-step replay and replaces them by only $k$-times of FPF period
&lt;/p&gt;</description></item><item><title>RMIX&#26159;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#23376;&#20154;&#32676;&#20559;&#31227;&#30340;&#31616;&#21333;&#23454;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#8220;&#28151;&#21512;&#8221;&#26679;&#26412;&#36827;&#34892;&#37325;&#35201;&#24615;&#21152;&#26435;&#65292;&#25506;&#32034;&#37051;&#22495;&#31354;&#38388;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#31283;&#20581;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.04148</link><description>&lt;p&gt;
&#22788;&#29702;&#23376;&#20154;&#32676;&#20559;&#31227;&#30340;&#21152;&#26435;mixup&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reweighted Mixup for Subpopulation Shift. (arXiv:2304.04148v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04148
&lt;/p&gt;
&lt;p&gt;
RMIX&#26159;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#23376;&#20154;&#32676;&#20559;&#31227;&#30340;&#31616;&#21333;&#23454;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#8220;&#28151;&#21512;&#8221;&#26679;&#26412;&#36827;&#34892;&#37325;&#35201;&#24615;&#21152;&#26435;&#65292;&#25506;&#32034;&#37051;&#22495;&#31354;&#38388;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#31283;&#20581;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23376;&#20154;&#32676;&#20559;&#31227;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#25351;&#30340;&#26159;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#21253;&#21547;&#30456;&#21516;&#30340;&#23376;&#20154;&#32676;&#32452;&#65292;&#20294;&#20855;&#26377;&#19981;&#21516;&#30340;&#23376;&#20154;&#32676;&#27604;&#20363;&#12290;&#24573;&#30053;&#23376;&#20154;&#32676;&#20559;&#31227;&#21487;&#33021;&#23548;&#33268;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#37325;&#35201;&#24615;&#35780;&#20272;&#26435;&#37325;&#26159;&#22788;&#29702;&#23376;&#20154;&#32676;&#20559;&#31227;&#30340;&#32463;&#20856;&#21644;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;&#20855;&#26377;&#20219;&#24847;&#25311;&#21512;&#35757;&#32451;&#26679;&#26412;&#33021;&#21147;&#30340;&#36229;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#26080;&#27861;&#25913;&#21892;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#32780;&#23454;&#29992;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#21152;&#26435;mixup&#65288;RMIX&#65289;&#65292;&#36890;&#36807;&#23545;&#8220;&#28151;&#21512;&#8221;&#26679;&#26412;&#36827;&#34892;&#37325;&#35201;&#24615;&#21152;&#26435;&#26469;&#20943;&#36731;&#36229;&#21442;&#25968;&#21270;&#27169;&#22411;&#20013;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#30001;&#20110;&#21033;&#29992;mixup&#20013;&#30340;&#37325;&#21152;&#26435;&#65292;RMIX&#20801;&#35768;&#27169;&#22411;&#26356;&#22810;&#22320;&#25506;&#32034;&#23569;&#25968;&#26679;&#26412;&#30340;&#37051;&#22495;&#31354;&#38388;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#31283;&#20581;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Subpopulation shift exists widely in many real-world applications, which refers to the training and test distributions that contain the same subpopulation groups but with different subpopulation proportions. Ignoring subpopulation shifts may lead to significant performance degradation and fairness concerns. Importance reweighting is a classical and effective way to handle the subpopulation shift. However, recent studies have recognized that most of these approaches fail to improve the performance especially when applied to over-parameterized neural networks which are capable of fitting any training samples. In this work, we propose a simple yet practical framework, called reweighted mixup (RMIX), to mitigate the overfitting issue in over-parameterized models by conducting importance weighting on the ''mixed'' samples. Benefiting from leveraging reweighting in mixup, RMIX allows the model to explore the vicinal space of minority samples more, thereby obtaining more robust model against 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36827;&#21270;&#32858;&#31867;&#26041;&#27861;&#21644;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#30340;&#28151;&#21512;&#26469;&#23454;&#29616;&#19968;&#27425;&#32852;&#37030;&#23398;&#20064;&#20998;&#31867;&#65292;&#20197;&#20445;&#25252;&#38544;&#31169;&#24182;&#35299;&#20915;&#36890;&#20449;&#24320;&#38144;&#21644;&#26377;&#38480;&#36164;&#28304;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.04147</link><description>&lt;p&gt;
&#36827;&#21270;&#32858;&#31867;&#26041;&#27861;&#21644;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#30340;&#28151;&#21512;&#29992;&#20110;&#19968;&#27425;&#32852;&#37030;&#23398;&#20064;&#20998;&#31867;&#65306;FedPNN
&lt;/p&gt;
&lt;p&gt;
FedPNN: One-shot Federated Classification via Evolving Clustering Method and Probabilistic Neural Network hybrid. (arXiv:2304.04147v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36827;&#21270;&#32858;&#31867;&#26041;&#27861;&#21644;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#30340;&#28151;&#21512;&#26469;&#23454;&#29616;&#19968;&#27425;&#32852;&#37030;&#23398;&#20064;&#20998;&#31867;&#65292;&#20197;&#20445;&#25252;&#38544;&#31169;&#24182;&#35299;&#20915;&#36890;&#20449;&#24320;&#38144;&#21644;&#26377;&#38480;&#36164;&#28304;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37329;&#34701;&#12289;&#38134;&#34892;&#21644;&#21307;&#30103;&#31561;&#39046;&#22495;&#65292;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#33267;&#20851;&#37325;&#35201;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30001;&#20110;&#20854;&#20998;&#25955;&#12289;&#20998;&#24067;&#24335;&#30340;&#35757;&#32451;&#21644;&#21516;&#26102;&#33719;&#24471;&#20840;&#23616;&#20849;&#20139;&#27169;&#22411;&#30340;&#20445;&#25252;&#38544;&#31169;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;FL&#38754;&#20020;&#30528;&#36890;&#20449;&#24320;&#38144;&#21644;&#26377;&#38480;&#30340;&#36164;&#28304;&#33021;&#21147;&#31561;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#30340;&#30446;&#26631;&#65292;&#24182;&#36827;&#34892;&#20102;&#20197;&#19979;&#39318;&#27425;&#30740;&#31350;&#65306;&#65288;i&#65289;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#20998;&#24067;&#20316;&#20026;&#22122;&#22768;&#26469;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25913;&#36827;&#30340;&#26465;&#20214;&#34920;&#29983;&#25104;&#23545;&#25239;&#31070;&#32463;&#32593;&#32476;&#65288;CTGAN&#65289;&#65292;&#65288;ii&#65289;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#24320;&#21457;&#21644;&#37319;&#29992;&#32852;&#37030;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#65288;FedPNN&#65289;&#26469;&#26500;&#24314;&#20840;&#23616;&#20849;&#20139;&#20998;&#31867;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;&#21512;&#25104;&#25968;&#25454;&#38598;&#25351;&#26631;&#26469;&#26816;&#26597;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protecting data privacy is paramount in the fields such as finance, banking, and healthcare. Federated Learning (FL) has attracted widespread attention due to its decentralized, distributed training and the ability to protect the privacy while obtaining a global shared model. However, FL presents challenges such as communication overhead, and limited resource capability. This motivated us to propose a two-stage federated learning approach toward the objective of privacy protection, which is a first-of-its-kind study as follows: (i) During the first stage, the synthetic dataset is generated by employing two different distributions as noise to the vanilla conditional tabular generative adversarial neural network (CTGAN) resulting in modified CTGAN, and (ii) In the second stage, the Federated Probabilistic Neural Network (FedPNN) is developed and employed for building globally shared classification model. We also employed synthetic dataset metrics to check the quality of the generated syn
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#30721;&#29575;-&#22833;&#30495;&#29702;&#35770;&#65292;&#32467;&#21512;&#20998;&#24067;&#24335;&#28857;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#32423;&#20998;&#31867;&#20013;&#36873;&#25321;&#22810;&#26679;&#21270;&#30340;&#23398;&#20064;&#25968;&#25454;&#26679;&#26412;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.04137</link><description>&lt;p&gt;
RD-DPP: &#30721;&#29575;-&#22833;&#30495;&#29702;&#35770;&#19982;&#20998;&#24067;&#24335;&#28857;&#36807;&#31243;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#22810;&#26679;&#21270;&#23398;&#20064;&#25968;&#25454;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
RD-DPP: Rate-Distortion Theory Meets Determinantal Point Process to Diversify Learning Data Samples. (arXiv:2304.04137v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04137
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#30721;&#29575;-&#22833;&#30495;&#29702;&#35770;&#65292;&#32467;&#21512;&#20998;&#24067;&#24335;&#28857;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#32423;&#20998;&#31867;&#20013;&#36873;&#25321;&#22810;&#26679;&#21270;&#30340;&#23398;&#20064;&#25968;&#25454;&#26679;&#26412;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20123;&#23454;&#38469;&#30340;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#22914;&#20132;&#36890;&#35270;&#39057;&#20998;&#26512;&#65292;&#21487;&#29992;&#35757;&#32451;&#26679;&#26412;&#30340;&#25968;&#37327;&#21463;&#21040;&#19981;&#21516;&#22240;&#32032;&#30340;&#38480;&#21046;&#65292;&#22914;&#26377;&#38480;&#30340;&#36890;&#20449;&#24102;&#23485;&#21644;&#35745;&#31639;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#36873;&#25321;&#23545;&#23398;&#20064;&#31995;&#32479;&#36136;&#37327;&#20570;&#20986;&#26368;&#22823;&#36129;&#29486;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#26679;&#26412;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#36873;&#25321;&#22810;&#26679;&#21270;&#26679;&#26412;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#26159;&#20998;&#24067;&#24335;&#28857;&#36807;&#31243; (DPP)&#12290;&#28982;&#32780;&#65292;DPP&#23384;&#22312;&#19968;&#20123;&#24050;&#30693;&#30340;&#32570;&#28857;&#65292;&#20363;&#22914;&#23558;&#26679;&#26412;&#25968;&#37327;&#38480;&#21046;&#20026;&#30456;&#20284;&#24615;&#30697;&#38453;&#30340;&#31209;&#65292;&#24182;&#19988;&#19981;&#33021;&#20026;&#29305;&#23450;&#30340;&#23398;&#20064;&#20219;&#21153;&#65288;&#20363;&#22914;&#22810;&#32423;&#20998;&#31867;&#20219;&#21153;&#65289;&#23450;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30721;&#29575;-&#22833;&#30495; (RD) &#29702;&#35770;&#30340;&#34913;&#37327;&#20219;&#21153;&#23450;&#21521;&#22810;&#26679;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#32423;&#20998;&#31867;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;DPP&#21644;RD&#29702;&#35770;&#20043;&#38388;&#30340;&#22522;&#26412;&#20851;&#31995;&#65292;&#35774;&#35745;&#20102;RD-DPP&#65292;&#19968;&#31181;&#22522;&#20110;RD&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#29992;&#20110;&#35780;&#20272;&#25968;&#25454;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#22686;&#30410;&#12290;&#25105;&#20204;&#36824;&#24471;&#21040;&#20102;&#19968;&#20123;&#29305;&#23450;&#24773;&#20917;&#19979;RD-DPP&#30340;&#38381;&#21512;&#35299;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#34920;&#26126;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#26679;&#24615;&#20248;&#21270;&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In some practical learning tasks, such as traffic video analysis, the number of available training samples is restricted by different factors, such as limited communication bandwidth and computation power; therefore, it is imperative to select diverse data samples that contribute the most to the quality of the learning system. One popular approach to selecting diverse samples is Determinantal Point Process (DPP). However, it suffers from a few known drawbacks, such as restriction of the number of samples to the rank of the similarity matrix, and not being customizable for specific learning tasks (e.g., multi-level classification tasks). In this paper, we propose a new way of measuring task-oriented diversity based on the Rate-Distortion (RD) theory, appropriate for multi-level classification. To this end, we establish a fundamental relationship between DPP and RD theory, which led to designing RD-DPP, an RD-based value function to evaluate the diversity gain of data samples. We also ob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Sat-NeRF&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#23569;&#37327;&#30340;&#21355;&#26143;&#22270;&#20687;&#38598;&#21512;&#20013;&#21512;&#25104;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#20934;&#30830;&#22320;&#20272;&#35745;&#22330;&#26223;&#34920;&#38754;&#30340;&#39640;&#31243;&#12290;</title><link>http://arxiv.org/abs/2304.04133</link><description>&lt;p&gt;
&#22522;&#20110;NeRF&#25216;&#26415;&#30340;&#21355;&#26143;&#22270;&#20687;&#34920;&#38754;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
NeRF applied to satellite imagery for surface reconstruction. (arXiv:2304.04133v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Sat-NeRF&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#23569;&#37327;&#30340;&#21355;&#26143;&#22270;&#20687;&#38598;&#21512;&#20013;&#21512;&#25104;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#20934;&#30830;&#22320;&#20272;&#35745;&#22330;&#26223;&#34920;&#38754;&#30340;&#39640;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Sat-NeRF&#27169;&#22411;&#65292;&#26159;&#23545;&#26368;&#36817;&#24341;&#20837;&#30340;S-NeRF&#27169;&#22411;&#30340;&#20462;&#25913;&#23454;&#29616;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#20174;&#31232;&#30095;&#30340;&#21355;&#26143;&#22270;&#20687;&#38598;&#21512;&#20013;&#21512;&#25104;&#26032;&#30340;&#35270;&#35282;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#22270;&#29255;&#20013;&#30340;&#20809;&#29031;&#21464;&#21270;&#12290;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#36824;&#33021;&#22815;&#31934;&#30830;&#22320;&#20272;&#35745;&#22330;&#26223;&#34920;&#38754;&#30340;&#39640;&#31243;&#65292;&#36825;&#23545;&#21355;&#26143;&#35266;&#27979;&#24212;&#29992;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;S-NeRF&#26041;&#27861;&#25913;&#36827;&#20102;&#26631;&#20934;&#30340;NeRF&#26041;&#27861;&#65292;&#23558;&#36752;&#23556;&#24378;&#24230;&#32771;&#34385;&#20026;&#39640;&#21453;&#23556;&#29575;&#21644;&#20837;&#23556;&#36752;&#29031;&#24230;&#30340;&#20989;&#25968;&#12290;&#36825;&#20004;&#20010;&#37327;&#37117;&#26159;&#27169;&#22411;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#26525;&#26465;&#30340;&#36755;&#20986;&#65292;&#32780;&#21518;&#32773;&#21017;&#34987;&#35270;&#20026;&#26469;&#33258;&#22826;&#38451;&#30340;&#30452;&#25509;&#20809;&#32447;&#21644;&#26469;&#33258;&#22825;&#31354;&#30340;&#28459;&#21453;&#23556;&#39068;&#33394;&#20989;&#25968;&#12290;&#35813;&#23454;&#29616;&#22522;&#20110;&#29992;&#32553;&#25918;-&#35009;&#21098;&#25216;&#26415;&#22686;&#24378;&#30340;&#21355;&#26143;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#23545;NeRF&#36827;&#34892;&#20102;&#36229;&#21442;&#25968;&#30740;&#31350;&#65292;&#24471;&#20986;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Sat-NeRF, a modified implementation of the recently introduced Shadow Neural Radiance Field (S-NeRF) model. This method is able to synthesize novel views from a sparse set of satellite images of a scene, while accounting for the variation in lighting present in the pictures. The trained model can also be used to accurately estimate the surface elevation of the scene, which is often a desirable quantity for satellite observation applications. S-NeRF improves on the standard Neural Radiance Field (NeRF) method by considering the radiance as a function of the albedo and the irradiance. Both these quantities are output by fully connected neural network branches of the model, and the latter is considered as a function of the direct light from the sun and the diffuse color from the sky. The implementations were run on a dataset of satellite images, augmented using a zoom-and-crop technique. A hyperparameter study for NeRF was carried out, leading to intriguing observations on the 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#38024;&#23545;&#36817;&#20284;&#30828;&#20214;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#21152;&#36895;&#20102;&#35757;&#32451;&#36807;&#31243;&#65292;&#20026;&#22312;&#38656;&#35201;&#36827;&#34892;&#25512;&#29702;&#20219;&#21153;&#30340;&#30005;&#27744;&#25805;&#20316;&#35774;&#22791;&#19978;&#23454;&#29616;&#39640;&#25928;&#28145;&#24230;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2304.04125</link><description>&lt;p&gt;
&#22312;&#36817;&#20284;&#30828;&#20214;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Training Neural Networks for Execution on Approximate Hardware. (arXiv:2304.04125v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04125
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#38024;&#23545;&#36817;&#20284;&#30828;&#20214;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#21152;&#36895;&#20102;&#35757;&#32451;&#36807;&#31243;&#65292;&#20026;&#22312;&#38656;&#35201;&#36827;&#34892;&#25512;&#29702;&#20219;&#21153;&#30340;&#30005;&#27744;&#25805;&#20316;&#35774;&#22791;&#19978;&#23454;&#29616;&#39640;&#25928;&#28145;&#24230;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20284;&#35745;&#31639;&#26041;&#27861;&#22312;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#30001;&#20110;&#30828;&#20214;&#25104;&#26412;&#38477;&#20302;&#65292;&#36825;&#20123;&#26041;&#27861;&#29305;&#21035;&#36866;&#29992;&#20110;&#21463;&#21040;&#21151;&#29575;&#39044;&#31639;&#38480;&#21046;&#30340;&#38656;&#35201;&#36827;&#34892;&#25512;&#29702;&#20219;&#21153;&#30340;&#30005;&#27744;&#25805;&#20316;&#35774;&#22791;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#26377;&#20851;&#35757;&#32451;&#26041;&#27861;&#30340;&#30740;&#31350;&#65292;&#36817;&#20284;&#35745;&#31639;&#23578;&#26410;&#20805;&#20998;&#21457;&#25381;&#20854;&#28508;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36817;&#20284;&#30828;&#20214;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#20026;&#36817;&#20284;&#30828;&#20214;&#20248;&#21270;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#22810;&#36798;18&#20493;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Approximate computing methods have shown great potential for deep learning. Due to the reduced hardware costs, these methods are especially suitable for inference tasks on battery-operated devices that are constrained by their power budget. However, approximate computing hasn't reached its full potential due to the lack of work on training methods. In this work, we discuss training methods for approximate hardware. We demonstrate how training needs to be specialized for approximate hardware, and propose methods to speed up the training process by up to 18X.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24635;&#30456;&#20851;&#24615;&#30340;&#29983;&#25104;&#27169;&#22411;TC-VAE&#65292;&#21487;&#20197;&#25581;&#31034;&#25968;&#25454;&#29983;&#25104;&#22240;&#32032;&#20013;&#30340;&#26410;&#30693;&#20998;&#24067;&#25968;&#25454;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#19981;&#24179;&#34913;&#29983;&#25104;&#22240;&#32032;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2304.04103</link><description>&lt;p&gt;
TC-VAE&#65306;&#25581;&#31034;&#25968;&#25454;&#29983;&#25104;&#22240;&#32032;&#20013;&#30340;&#26410;&#30693;&#20998;&#24067;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
TC-VAE: Uncovering Out-of-Distribution Data Generative Factors. (arXiv:2304.04103v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24635;&#30456;&#20851;&#24615;&#30340;&#29983;&#25104;&#27169;&#22411;TC-VAE&#65292;&#21487;&#20197;&#25581;&#31034;&#25968;&#25454;&#29983;&#25104;&#22240;&#32032;&#20013;&#30340;&#26410;&#30693;&#20998;&#24067;&#25968;&#25454;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#19981;&#24179;&#34913;&#29983;&#25104;&#22240;&#32032;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25581;&#31034;&#25968;&#25454;&#29983;&#25104;&#22240;&#32032;&#26159;&#35299;&#20915;&#35299;&#32544;&#32467;&#23398;&#20064;&#30340;&#26368;&#32456;&#30446;&#26631;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;-TC-VAE&#65292;&#23427;&#21487;&#20197;&#22522;&#20110;&#25152;&#23398;&#30340;&#28508;&#22312;&#34920;&#24449;&#21644;&#36755;&#20837;&#25968;&#25454;&#20043;&#38388;&#30340;&#24635;&#30456;&#20851;&#24615;&#19979;&#30028;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#21457;&#29616;&#19981;&#22312;&#25968;&#25454;&#38598;&#20013;&#26174;&#24335;&#20986;&#29616;&#30340;&#21464;&#21270;&#22240;&#32032;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#20351;&#29992;&#20855;&#26377;&#19981;&#24179;&#34913;&#30340;&#29983;&#25104;&#22240;&#32032;&#25968;&#25454;&#38598;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#23450;&#37327;&#21644;&#23450;&#24615;&#23454;&#39564;&#20013;&#34920;&#26126;&#20102;TC-VAE&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncovering data generative factors is the ultimate goal of disentanglement learning. Although many works proposed disentangling generative models able to uncover the underlying generative factors of a dataset, so far no one was able to uncover OOD generative factors (i.e., factors of variations that are not explicitly shown on the dataset). Moreover, the datasets used to validate these models are synthetically generated using a balanced mixture of some predefined generative factors, implicitly assuming that generative factors are uniformly distributed across the datasets. However, real datasets do not present this property. In this work we analyse the effect of using datasets with unbalanced generative factors, providing qualitative and quantitative results for widely used generative models. Moreover, we propose TC-VAE, a generative model optimized using a lower bound of the joint total correlation between the learned latent representations and the input data. We show that the proposed
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#23884;&#20837;&#26041;&#27861;&#21644;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26080;&#30417;&#30563;&#22312;&#32447;&#25925;&#20107;&#21457;&#29616;&#26694;&#26550;USTORY&#65292;&#21487;&#20197;&#21160;&#24577;&#34920;&#31034;&#25991;&#31456;&#21644;&#25925;&#20107;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#20849;&#20139;&#30340;&#26102;&#38388;&#20027;&#39064;&#21644;&#26032;&#39062;&#24615;&#65292;&#20197;&#24110;&#21161;&#20154;&#20204;&#28040;&#21270;&#22823;&#37327;&#30340;&#26032;&#38395;&#27969;&#12290;</title><link>http://arxiv.org/abs/2304.04099</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#25193;&#23637;&#30340;&#20027;&#39064;&#23884;&#20837;&#20174;&#36830;&#32493;&#26032;&#38395;&#27969;&#20013;&#26080;&#30417;&#30563;&#22320;&#21457;&#29616;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Story Discovery from Continuous News Streams via Scalable Thematic Embedding. (arXiv:2304.04099v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#23884;&#20837;&#26041;&#27861;&#21644;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26080;&#30417;&#30563;&#22312;&#32447;&#25925;&#20107;&#21457;&#29616;&#26694;&#26550;USTORY&#65292;&#21487;&#20197;&#21160;&#24577;&#34920;&#31034;&#25991;&#31456;&#21644;&#25925;&#20107;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#20849;&#20139;&#30340;&#26102;&#38388;&#20027;&#39064;&#21644;&#26032;&#39062;&#24615;&#65292;&#20197;&#24110;&#21161;&#20154;&#20204;&#28040;&#21270;&#22823;&#37327;&#30340;&#26032;&#38395;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#22320;&#21457;&#29616;&#23454;&#26102;&#30456;&#20851;&#26032;&#38395;&#25991;&#31456;&#25925;&#20107;&#65292;&#26377;&#21161;&#20110;&#20154;&#20204;&#22312;&#19981;&#38656;&#35201;&#26114;&#36149;&#20154;&#24037;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#28040;&#21270;&#22823;&#37327;&#30340;&#26032;&#38395;&#27969;&#12290;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#22312;&#32447;&#25925;&#20107;&#21457;&#29616;&#30740;&#31350;&#30340;&#26222;&#36941;&#26041;&#27861;&#26159;&#29992;&#31526;&#21495;&#25110;&#22522;&#20110;&#22270;&#30340;&#23884;&#20837;&#26469;&#34920;&#31034;&#26032;&#38395;&#25991;&#31456;&#65292;&#24182;&#23558;&#23427;&#20204;&#36880;&#27493;&#32858;&#31867;&#25104;&#25925;&#20107;&#12290;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#26395;&#36827;&#19968;&#27493;&#25913;&#21892;&#23884;&#20837;&#65292;&#20294;&#26159;&#36890;&#36807;&#26080;&#24046;&#21035;&#22320;&#32534;&#30721;&#25991;&#31456;&#20013;&#30340;&#25152;&#26377;&#20449;&#24687;&#26469;&#30452;&#25509;&#37319;&#29992;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#26377;&#25928;&#22788;&#29702;&#23500;&#21547;&#25991;&#26412;&#19988;&#19981;&#26029;&#21457;&#23637;&#30340;&#26032;&#38395;&#27969;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#23884;&#20837;&#26041;&#27861;&#65292;&#20351;&#29992;&#29616;&#25104;&#30340;&#39044;&#35757;&#32451;&#21477;&#23376;&#32534;&#30721;&#22120;&#26469;&#21160;&#24577;&#34920;&#31034;&#25991;&#31456;&#21644;&#25925;&#20107;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#20849;&#20139;&#30340;&#26102;&#38388;&#20027;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#26080;&#30417;&#30563;&#22312;&#32447;&#25925;&#20107;&#21457;&#29616;&#30340;&#24819;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#26694;&#26550;USTORY&#65292;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#25216;&#26415;&#65292;&#21363;&#20027;&#39064;&#21644;&#26102;&#38388;&#24863;&#30693;&#30340;&#21160;&#24577;&#23884;&#20837;&#21644;&#26032;&#39062;&#24615;&#24863;&#30693;&#30340;&#33258;&#36866;&#24212;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised discovery of stories with correlated news articles in real-time helps people digest massive news streams without expensive human annotations. A common approach of the existing studies for unsupervised online story discovery is to represent news articles with symbolic- or graph-based embedding and incrementally cluster them into stories. Recent large language models are expected to improve the embedding further, but a straightforward adoption of the models by indiscriminately encoding all information in articles is ineffective to deal with text-rich and evolving news streams. In this work, we propose a novel thematic embedding with an off-the-shelf pretrained sentence encoder to dynamically represent articles and stories by considering their shared temporal themes. To realize the idea for unsupervised online story discovery, a scalable framework USTORY is introduced with two main techniques, theme- and time-aware dynamic embedding and novelty-aware adaptive clustering, fuel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#65292;&#22312;&#20809;&#28369;&#21644;&#31561;&#21608;&#26465;&#20214;&#19979;&#65292;MALA&#30340;&#28151;&#21512;&#26102;&#38388;&#20165;&#19982;Hessian&#30697;&#38453;&#30340;trace&#26377;&#20851;&#65292;&#32780;&#19982;&#20854;&#31639;&#23376;&#33539;&#25968;&#21644;log-concave&#27809;&#26377;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2304.04095</link><description>&lt;p&gt;
&#20803;&#21345;&#27931;&#35843;&#25972;&#26391;&#20043;&#19975;(MALA)&#22312;&#20809;&#28369;&#19988;&#31561;&#21608;&#26465;&#20214;&#19979;&#30340;&#28151;&#21512;&#31616;&#21333;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
A Simple Proof of the Mixing of Metropolis-Adjusted Langevin Algorithm under Smoothness and Isoperimetry. (arXiv:2304.04095v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#65292;&#22312;&#20809;&#28369;&#21644;&#31561;&#21608;&#26465;&#20214;&#19979;&#65292;MALA&#30340;&#28151;&#21512;&#26102;&#38388;&#20165;&#19982;Hessian&#30697;&#38453;&#30340;trace&#26377;&#20851;&#65292;&#32780;&#19982;&#20854;&#31639;&#23376;&#33539;&#25968;&#21644;log-concave&#27809;&#26377;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;$\mathbb{R}^d$&#19978;&#26679;&#26412;&#30446;&#26631;&#23494;&#24230;&#30340;&#20803;&#21345;&#27931;&#35843;&#25972;&#26391;&#20043;&#19975;&#65288;MALA&#65289;&#30340;&#28151;&#21512;&#26102;&#38388;&#12290;&#25105;&#20204;&#20551;&#35774;&#30446;&#26631;&#23494;&#24230;&#28385;&#36275;$\psi_\mu$-&#31561;&#21608;&#21644;&#23427;&#30340;&#40657;&#22622;&#30697;&#38453;&#30340;trace&#21644;&#31639;&#23376;&#33539;&#25968;&#20998;&#21035;&#21463;$L$&#21644;$\Upsilon$&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#35770;&#26159;&#65292;&#20174;&#28909;&#21551;&#21160;&#24320;&#22987;&#65292;&#20026;&#20102;&#36798;&#21040;$\epsilon$&#24635;&#21464;&#24046;&#36317;&#31163;&#65292;MALA&#22312;$O\left(\frac{(L\Upsilon)^{\frac12}}{\psi_\mu^2} \log\left(\frac{1}{\epsilon}\right)\right)$&#27425;&#36845;&#20195;&#20013;&#28151;&#21512;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#35813;&#32467;&#26524;&#19981;&#20165;&#36866;&#29992;&#20110;&#23545;&#25968;&#20985;&#37319;&#26679;&#35774;&#32622;&#65292;&#32780;&#19988;&#28151;&#21512;&#26102;&#38388;&#20165;&#21462;&#20915;&#20110;$\Upsilon$&#65292;&#32780;&#19981;&#26159;&#20854;&#19978;&#30028;$Ld$&#12290;&#22312;$m$-&#24378;&#23545;&#25968;&#20985;&#21644;$L$-&#20809;&#28369;&#37319;&#26679;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#24674;&#22797;&#20102;&#20197;&#21069;&#30340;MALA&#30340;&#26368;&#23567;&#20540;&#28151;&#21512;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the mixing time of Metropolis-Adjusted Langevin algorithm (MALA) for sampling a target density on $\mathbb{R}^d$. We assume that the target density satisfies $\psi_\mu$-isoperimetry and that the operator norm and trace of its Hessian are bounded by $L$ and $\Upsilon$ respectively. Our main result establishes that, from a warm start, to achieve $\epsilon$-total variation distance to the target density, MALA mixes in $O\left(\frac{(L\Upsilon)^{\frac12}}{\psi_\mu^2} \log\left(\frac{1}{\epsilon}\right)\right)$ iterations. Notably, this result holds beyond the log-concave sampling setting and the mixing time depends on only $\Upsilon$ rather than its upper bound $L d$. In the $m$-strongly logconcave and $L$-log-smooth sampling setting, our bound recovers the previous minimax mixing bound of MALA~\cite{wu2021minimax}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#19968;&#20010;&#26377;&#20851;&#26368;&#20248;&#33218;&#35782;&#21035;&#30340;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#35201;&#27714;&#25152;&#36873;&#33218;&#24517;&#39035;&#23545;&#25152;&#26377;&#20122;&#32676;&#37117;&#20844;&#24179;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#24182;&#35777;&#26126;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.04091</link><description>&lt;p&gt;
&#20855;&#26377;&#20122;&#32676;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#26368;&#20248;&#33218;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Best Arm Identification with Fairness Constraints on Subpopulations. (arXiv:2304.04091v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#19968;&#20010;&#26377;&#20851;&#26368;&#20248;&#33218;&#35782;&#21035;&#30340;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#35201;&#27714;&#25152;&#36873;&#33218;&#24517;&#39035;&#23545;&#25152;&#26377;&#20122;&#32676;&#37117;&#20844;&#24179;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#24182;&#35777;&#26126;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#12289;&#20998;&#26512;&#21644;&#35299;&#20915;&#20102;&#19968;&#20010;&#20855;&#26377;&#20122;&#32676;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#26368;&#20248;&#33218;&#35782;&#21035;&#38382;&#39064;&#65288;BAICS&#65289;&#12290;&#26631;&#20934;&#30340;&#26368;&#20248;&#33218;&#35782;&#21035;&#38382;&#39064;&#26088;&#22312;&#36873;&#25321;&#19968;&#20010;&#26399;&#26395;&#22870;&#21169;&#26368;&#22823;&#30340;&#33218;&#65292;&#20854;&#20013;&#26399;&#26395;&#26159;&#38024;&#23545;&#25972;&#20010;&#20154;&#32676;&#35745;&#31639;&#30340;&#12290;BAICS&#38382;&#39064;&#35201;&#27714;&#25152;&#36873;&#33218;&#24517;&#39035;&#23545;&#25152;&#26377;&#20122;&#32676;&#65288;&#20363;&#22914;&#65292;&#19981;&#21516;&#30340;&#26063;&#32676;&#12289;&#24180;&#40836;&#32452;&#25110;&#23458;&#25143;&#31867;&#22411;&#65289;&#37117;&#20844;&#24179;&#65292;&#36890;&#36807;&#28385;&#36275;&#27599;&#20010;&#20122;&#32676;&#26465;&#20214;&#19979;&#30340;&#26399;&#26395;&#22870;&#21169;&#22823;&#20110;&#19968;&#20123;&#38408;&#20540;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;BAICS&#38382;&#39064;&#26088;&#22312;&#20197;&#39640;&#32622;&#20449;&#24230;&#27491;&#30830;&#37492;&#23450;&#20986;&#25152;&#26377;&#31526;&#21512;&#20122;&#32676;&#32422;&#26463;&#26465;&#20214;&#30340;&#33218;&#20013;&#26399;&#26395;&#22870;&#21169;&#26368;&#22823;&#30340;&#33218;&#12290;&#25105;&#20204;&#36890;&#36807;&#35777;&#26126;&#26368;&#23567;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#26368;&#20339;&#23454;&#29616;&#19979;&#30028;&#30340;&#38381;&#24335;&#34920;&#31034;&#26469;&#20998;&#26512;BAICS&#38382;&#39064;&#30340;&#22797;&#26434;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#35813;&#31639;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#19979;&#30028;&#30340;&#38454;&#25968;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
We formulate, analyze and solve the problem of best arm identification with fairness constraints on subpopulations (BAICS). Standard best arm identification problems aim at selecting an arm that has the largest expected reward where the expectation is taken over the entire population. The BAICS problem requires that an selected arm must be fair to all subpopulations (e.g., different ethnic groups, age groups, or customer types) by satisfying constraints that the expected reward conditional on every subpopulation needs to be larger than some thresholds. The BAICS problem aims at correctly identify, with high confidence, the arm with the largest expected reward from all arms that satisfy subpopulation constraints. We analyze the complexity of the BAICS problem by proving a best achievable lower bound on the sample complexity with closed-form representation. We then design an algorithm and prove that the algorithm's sample complexity matches with the lower bound in terms of order. A brief
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#31867;&#23391;&#21152;&#25289;&#35821;&#30340;&#26377;&#23475;&#35780;&#35770;&#12290;&#20351;&#29992;LSTM&#21644;BERT&#23884;&#20837;&#23454;&#29616;&#20102;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#21516;&#26102;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#21452;&#21521;LSTM&#19982;&#27880;&#24847;&#26426;&#21046;&#32452;&#21512;&#23454;&#29616;&#20102;&#22810;&#26631;&#31614;&#20998;&#31867;&#65292;&#20934;&#30830;&#29575;&#21644;&#21152;&#26435;F1-score&#22343;&#36739;&#39640;&#12290;</title><link>http://arxiv.org/abs/2304.04087</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21487;&#35299;&#37322;&#30340;&#22810;&#26631;&#31614;&#23391;&#21152;&#25289;&#26377;&#23475;&#35780;&#35770;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Interpretable Multi Labeled Bengali Toxic Comments Classification using Deep Learning. (arXiv:2304.04087v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#31867;&#23391;&#21152;&#25289;&#35821;&#30340;&#26377;&#23475;&#35780;&#35770;&#12290;&#20351;&#29992;LSTM&#21644;BERT&#23884;&#20837;&#23454;&#29616;&#20102;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#21516;&#26102;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#21452;&#21521;LSTM&#19982;&#27880;&#24847;&#26426;&#21046;&#32452;&#21512;&#23454;&#29616;&#20102;&#22810;&#26631;&#31614;&#20998;&#31867;&#65292;&#20934;&#30830;&#29575;&#21644;&#21152;&#26435;F1-score&#22343;&#36739;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#26696;&#26469;&#20998;&#31867;&#23391;&#21152;&#25289;&#35821;&#30340;&#26377;&#23475;&#35780;&#35770;&#65292;&#39318;&#20808;&#20351;&#29992;&#20108;&#20803;&#20998;&#31867;&#27169;&#22411;&#30830;&#23450;&#35780;&#35770;&#26159;&#21542;&#26377;&#23475;&#65292;&#28982;&#21518;&#20351;&#29992;&#22810;&#26631;&#31614;&#20998;&#31867;&#22120;&#30830;&#23450;&#35813;&#35780;&#35770;&#23646;&#20110;&#21738;&#31181;&#27602;&#24615;&#31867;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20934;&#22791;&#20102;&#19968;&#20010;&#25163;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;16,073&#20010;&#23454;&#20363;&#65292;&#20854;&#20013;8,488&#20010;&#26159;&#26377;&#23475;&#30340;&#65292;&#24182;&#19988;&#20219;&#20309;&#26377;&#23475;&#30340;&#35780;&#35770;&#21487;&#33021;&#21516;&#26102;&#23646;&#20110;&#20845;&#31181;&#26377;&#23475;&#31867;&#22411;-&#20302;&#20439;&#65292;&#20167;&#24680;&#65292;&#23447;&#25945;&#65292;&#23041;&#32961;&#65292;&#24694;&#24847;&#21644;&#20398;&#36785;&#12290;&#22312;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#20351;&#29992;LSTM&#21644;BERT&#23884;&#20837;&#23454;&#29616;&#20102;89.42&#65285;&#30340;&#20934;&#30830;&#29575;&#65307;&#22312;&#22810;&#26631;&#31614;&#20998;&#31867;&#22120;&#26041;&#38754;&#65292;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#21452;&#21521;LSTM&#65288;CNN-BiLSTM&#65289;&#19982;&#27880;&#24847;&#26426;&#21046;&#32452;&#21512;&#65292;&#33719;&#24471;&#20102;78.92&#65285;&#30340;&#20934;&#30830;&#29575;&#21644;0.86&#30340;&#21152;&#26435;F1-score&#12290;&#20026;&#20102;&#35299;&#37322;&#39044;&#27979;&#32467;&#26524;&#24182;&#35299;&#37322;&#20998;&#31867;&#26399;&#38388;&#30340;&#21333;&#35789;&#29305;&#24449;&#37325;&#35201;&#24615;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;LIME&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a deep learning-based pipeline for categorizing Bengali toxic comments, in which at first a binary classification model is used to determine whether a comment is toxic or not, and then a multi-label classifier is employed to determine which toxicity type the comment belongs to. For this purpose, we have prepared a manually labeled dataset consisting of 16,073 instances among which 8,488 are Toxic and any toxic comment may correspond to one or more of the six toxic categories - vulgar, hate, religious, threat, troll, and insult simultaneously. Long Short Term Memory (LSTM) with BERT Embedding achieved 89.42% accuracy for the binary classification task while as a multi-label classifier, a combination of Convolutional Neural Network and Bi-directional Long Short Term Memory (CNN-BiLSTM) with attention mechanism achieved 78.92% accuracy and 0.86 as weighted F1-score. To explain the predictions and interpret the word feature importance during classification by the propos
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23631;&#38556;-&#26446;&#20122;&#26222;&#35834;&#22827;Actor-Critic&#65288;BLAC&#65289;&#26694;&#26550;&#65292;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#26102;&#30340;&#23433;&#20840;&#31283;&#23450;&#25511;&#21046;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#20854;&#20013;&#65292;&#22522;&#20110;&#37325;&#25918;&#32531;&#20914;&#21306;&#37319;&#26679;&#30340;&#25968;&#25454;&#26500;&#24314;&#20102;CBF&#23433;&#20840;&#32422;&#26463;&#21644;CLF&#31283;&#23450;&#32422;&#26463;&#65292;&#24182;&#20351;&#29992;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#26469;&#26356;&#26032;&#22522;&#20110;RL&#30340;&#25511;&#21046;&#22120;&#30340;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2304.04066</link><description>&lt;p&gt;
&#22522;&#20110;&#23631;&#38556;-&#26446;&#20122;&#26222;&#35834;&#22827;Actor-Critic&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#23433;&#20840;&#31283;&#23450;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
A Barrier-Lyapunov Actor-Critic Reinforcement Learning Approach for Safe and Stable Control. (arXiv:2304.04066v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23631;&#38556;-&#26446;&#20122;&#26222;&#35834;&#22827;Actor-Critic&#65288;BLAC&#65289;&#26694;&#26550;&#65292;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#26102;&#30340;&#23433;&#20840;&#31283;&#23450;&#25511;&#21046;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#20854;&#20013;&#65292;&#22522;&#20110;&#37325;&#25918;&#32531;&#20914;&#21306;&#37319;&#26679;&#30340;&#25968;&#25454;&#26500;&#24314;&#20102;CBF&#23433;&#20840;&#32422;&#26463;&#21644;CLF&#31283;&#23450;&#32422;&#26463;&#65292;&#24182;&#20351;&#29992;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#26469;&#26356;&#26032;&#22522;&#20110;RL&#30340;&#25511;&#21046;&#22120;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#35270;&#39057;&#28216;&#25103;&#21644;&#26426;&#22120;&#20154;&#31561;&#39046;&#22495;&#23637;&#29616;&#20986;&#24778;&#20154;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#26102;&#65292;&#30830;&#20445;&#23433;&#20840;&#21644;&#31283;&#23450;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20026;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#25552;&#20379;&#23433;&#20840;&#21644;&#31283;&#23450;&#24615;&#30340;&#23450;&#20041;&#65292;&#28982;&#21518;&#23558;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#65288;CBF&#65289;&#21644;&#25511;&#21046;&#26446;&#20122;&#26222;&#35834;&#22827;&#20989;&#25968;&#65288;CLF&#65289;&#26041;&#27861;&#19982;Actor-Critic&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23631;&#38556;-&#26446;&#20122;&#26222;&#35834;&#22827;Actor-Critic&#65288;BLAC&#65289;&#26694;&#26550;&#65292;&#26377;&#21161;&#20110;&#20445;&#25345;&#31995;&#32479;&#30340;&#23433;&#20840;&#21644;&#31283;&#23450;&#24615;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#22522;&#20110;&#26469;&#33258;&#37325;&#25918;&#32531;&#20914;&#21306;&#37319;&#26679;&#30340;&#25968;&#25454;&#26500;&#24314;&#20102;CBF&#23433;&#20840;&#32422;&#26463;&#21644;CLF&#31283;&#23450;&#32422;&#26463;&#65292;&#24182;&#20351;&#29992;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#26469;&#26356;&#26032;&#22522;&#20110;RL&#30340;&#25511;&#21046;&#22120;&#30340;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#22791;&#29992;&#25511;&#21046;&#22120;&#65292;&#20197;&#38450;RL&#25511;&#21046;&#22120;&#26080;&#27861;&#25552;&#20379;&#31283;&#23450;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has demonstrated impressive performance in various areas such as video games and robotics. However, ensuring safety and stability, which are two critical properties from a control perspective, remains a significant challenge when using RL to control real-world systems. In this paper, we first provide definitions of safety and stability for the RL system, and then combine the control barrier function (CBF) and control Lyapunov function (CLF) methods with the actor-critic method in RL to propose a Barrier-Lyapunov Actor-Critic (BLAC) framework which helps maintain the aforementioned safety and stability for the system. In this framework, CBF constraints for safety and CLF constraint for stability are constructed based on the data sampled from the replay buffer, and the augmented Lagrangian method is used to update the parameters of the RL-based controller. Furthermore, an additional backup controller is introduced in case the RL-based controller cannot provide
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;CFEs&#65289;&#30830;&#23450;&#31070;&#32463;&#32593;&#32476;&#40657;&#30418;&#29983;&#25104;&#30340;&#21709;&#24212;&#26354;&#32447;&#19978;&#26368;&#20855;&#30456;&#20851;&#24615;&#30340;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#24037;&#19994;&#36164;&#20135;&#39044;&#27979;&#32500;&#25252;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#21487;&#20197;&#35299;&#37322;&#27169;&#22411;&#30340;&#34892;&#20026;&#24182;&#26816;&#27979;&#24322;&#24120;&#34892;&#20026;&#30340;&#28508;&#22312;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2304.04063</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#21709;&#24212;&#26354;&#32447;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explanations of Neural Network-Generated Response Curves. (arXiv:2304.04063v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;CFEs&#65289;&#30830;&#23450;&#31070;&#32463;&#32593;&#32476;&#40657;&#30418;&#29983;&#25104;&#30340;&#21709;&#24212;&#26354;&#32447;&#19978;&#26368;&#20855;&#30456;&#20851;&#24615;&#30340;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#24037;&#19994;&#36164;&#20135;&#39044;&#27979;&#32500;&#25252;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#21487;&#20197;&#35299;&#37322;&#27169;&#22411;&#30340;&#34892;&#20026;&#24182;&#26816;&#27979;&#24322;&#24120;&#34892;&#20026;&#30340;&#28508;&#22312;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21709;&#24212;&#26354;&#32447;&#23637;&#31034;&#20102;&#19968;&#20010;&#25935;&#24863;&#31995;&#32479;&#23545;&#19981;&#21516;&#21050;&#28608;&#30340;&#21709;&#24212;&#31243;&#24230;&#12290;&#28982;&#32780;&#65292;&#36825;&#31867;&#31995;&#32479;&#30340;&#21453;&#24212;&#21487;&#33021;&#23545;&#22810;&#20010;&#19981;&#19968;&#23450;&#29420;&#31435;&#30340;&#21050;&#28608;&#65288;&#21363;&#36755;&#20837;&#29305;&#24449;&#65289;&#25935;&#24863;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#19968;&#20010;&#36873;&#23450;&#30340;&#36755;&#20837;&#29305;&#24449;&#65288;&#31216;&#20026;&#8220;&#27963;&#21160;&#29305;&#24449;&#8221;&#65289;&#25152;&#29983;&#25104;&#30340;&#21709;&#24212;&#26354;&#32447;&#30340;&#24418;&#29366;&#21487;&#33021;&#20381;&#36182;&#20110;&#20854;&#20182;&#36755;&#20837;&#29305;&#24449;&#65288;&#31216;&#20026;&#8220;&#34987;&#21160;&#29305;&#24449;&#8221;&#65289;&#30340;&#20540;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20351;&#29992;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#21709;&#24212;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;CFEs&#65289;&#26469;&#30830;&#23450;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#40657;&#30418;&#29983;&#25104;&#30340;&#21709;&#24212;&#26354;&#32447;&#19978;&#26368;&#20855;&#30456;&#20851;&#24615;&#30340;&#29305;&#24449;&#12290; CFE&#26159;&#36890;&#36807;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#26041;&#27861;&#29983;&#25104;&#30340;&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#23545;&#20110;&#20026;&#20027;&#21160;&#29305;&#24449;&#29983;&#25104;&#30340;&#21709;&#24212;&#26354;&#32447;&#65292;CFE&#25214;&#21040;&#38656;&#35201;&#20462;&#25913;&#30340;&#34987;&#21160;&#29305;&#24449;&#30340;&#26368;&#23567;&#32452;&#21512;&#65292;&#20197;&#25913;&#21464;&#26354;&#32447;&#30340;&#24418;&#29366;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#20004;&#20010;&#24037;&#19994;&#36164;&#20135;&#39044;&#27979;&#32500;&#25252;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#20854;&#20013;&#40657;&#30418;&#22238;&#24402;&#27169;&#22411;&#29983;&#25104;&#30340;&#21709;&#24212;&#26354;&#32447;&#29992;&#20110;&#26816;&#27979;&#24322;&#24120;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;CFEs&#26469;&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#24182;&#26816;&#27979;&#24322;&#24120;&#34892;&#20026;&#30340;&#28508;&#22312;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Response curves exhibit the magnitude of the response of a sensitive system to a varying stimulus. However, response of such systems may be sensitive to multiple stimuli (i.e., input features) that are not necessarily independent. As a consequence, the shape of response curves generated for a selected input feature (referred to as "active feature") might depend on the values of the other input features (referred to as "passive features"). In this work we consider the case of systems whose response is approximated using regression neural networks. We propose to use counterfactual explanations (CFEs) for the identification of the features with the highest relevance on the shape of response curves generated by neural network black boxes. CFEs are generated by a genetic algorithm-based approach that solves a multi-objective optimization problem. In particular, given a response curve generated for an active feature, a CFE finds the minimum combination of passive features that need to be mod
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#24739;&#32773;&#30340;&#22810;&#27169;&#24577;EHR&#25968;&#25454;&#39044;&#27979;&#22810;&#21457;&#24615;&#30828;&#21270;&#30151;&#30142;&#30149;&#20005;&#37325;&#31243;&#24230;&#65292;&#20197;&#20415;&#23454;&#29616;&#26089;&#26399;&#24178;&#39044;&#21644;&#27835;&#30103;&#12290;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#27169;&#22411;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.04062</link><description>&lt;p&gt;
&#21033;&#29992;&#22810;&#27169;&#24577;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#22810;&#21457;&#24615;&#30828;&#21270;&#30151;&#30142;&#30149;&#20005;&#37325;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;
Predicting multiple sclerosis disease severity with multimodal deep neural networks. (arXiv:2304.04062v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#24739;&#32773;&#30340;&#22810;&#27169;&#24577;EHR&#25968;&#25454;&#39044;&#27979;&#22810;&#21457;&#24615;&#30828;&#21270;&#30151;&#30142;&#30149;&#20005;&#37325;&#31243;&#24230;&#65292;&#20197;&#20415;&#23454;&#29616;&#26089;&#26399;&#24178;&#39044;&#21644;&#27835;&#30103;&#12290;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#27169;&#22411;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21457;&#24615;&#30828;&#21270;&#30151;&#65288;MS&#65289;&#26159;&#19968;&#31181;&#21457;&#23637;&#22312;&#20154;&#31867;&#22823;&#33041;&#21644;&#33034;&#39635;&#20013;&#30340;&#24930;&#24615;&#30142;&#30149;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#31070;&#32463;&#27704;&#20037;&#24615;&#25439;&#20260;&#25110;&#24694;&#21270;&#12290;MS&#30149;&#24773;&#30340;&#20005;&#37325;&#31243;&#24230;&#26159;&#36890;&#36807;&#25193;&#23637;&#27531;&#30142;&#29366;&#24577;&#35780;&#20998;&#65288;EDSS&#65289;&#26469;&#30417;&#27979;&#30340;&#65292;&#35813;&#35780;&#20998;&#30001;&#20960;&#20010;&#21151;&#33021;&#23376;&#20998;&#25968;&#32452;&#25104;&#12290;&#26089;&#26399;&#21644;&#20934;&#30830;&#30340;MS&#30142;&#30149;&#20005;&#37325;&#31243;&#24230;&#20998;&#31867;&#23545;&#20110;&#36890;&#36807;&#24212;&#29992;&#26089;&#26399;&#27835;&#30103;&#24178;&#39044;&#31574;&#30053;&#26469;&#20943;&#32531;&#25110;&#39044;&#38450;&#30142;&#30149;&#36827;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290; &#36817;&#24180;&#26469;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#23637;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#30340;&#24191;&#27867;&#24212;&#29992;&#20026;&#24212;&#29992;&#25968;&#25454;&#39537;&#21160;&#21644;&#39044;&#27979;&#24314;&#27169;&#24037;&#20855;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#20197;&#24448;&#19987;&#27880;&#20110;&#21033;&#29992;&#21333;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#30740;&#31350;&#30001;&#20110;&#25968;&#25454;&#19981;&#36275;&#25110;&#27169;&#22411;&#31616;&#21333;&#32780;&#38480;&#21046;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#24739;&#32773;&#30340;&#22810;&#27169;&#24577;&#32437;&#21521;&#21644;&#27178;&#21521;EHR&#25968;&#25454;&#39044;&#27979;&#21307;&#38498;&#35775;&#38382;&#26102;&#30340;&#22810;&#21457;&#24615;&#30828;&#21270;&#30151;&#30142;&#30149;&#20005;&#37325;&#31243;&#24230;&#30340;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiple Sclerosis (MS) is a chronic disease developed in human brain and spinal cord, which can cause permanent damage or deterioration of the nerves. The severity of MS disease is monitored by the Expanded Disability Status Scale (EDSS), composed of several functional sub-scores. Early and accurate classification of MS disease severity is critical for slowing down or preventing disease progression via applying early therapeutic intervention strategies. Recent advances in deep learning and the wide use of Electronic Health Records (EHR) creates opportunities to apply data-driven and predictive modeling tools for this goal. Previous studies focusing on using single-modal machine learning and deep learning algorithms were limited in terms of prediction accuracy due to the data insufficiency or model simplicity. In this paper, we proposed an idea of using patients' multimodal longitudinal and longitudinal EHR data to predict multiple sclerosis disease severity at the hospital visit. This
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#33021;&#37327;&#30340;&#29983;&#25104;&#37327;&#23376;&#22810;&#20307;&#24577;&#34920;&#31034;&#26041;&#27861;&#65292;&#33021;&#22815;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#26500;&#24314;&#33021;&#22815;&#25429;&#33719;&#37327;&#23376;&#22810;&#20307;&#24577;&#22797;&#26434;&#30456;&#20851;&#24615;&#21644;&#23545;&#31216;&#24615;&#30340;&#34920;&#31034;&#65292;&#35813;&#26041;&#27861;&#39640;&#25928;&#22320;&#23398;&#20064;1D&#21644;2D&#30340;&#37327;&#23376;&#22810;&#20307;&#24577;&#65292;&#24182;&#21487;&#33258;&#28982;&#22320;&#32435;&#20837;&#29289;&#29702;&#32422;&#26463;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2304.04058</link><description>&lt;p&gt;
&#23398;&#20064;&#37327;&#23376;&#22810;&#20307;&#24577;&#30340;&#22522;&#20110;&#33021;&#37327;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Energy-Based Representations of Quantum Many-Body States. (arXiv:2304.04058v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04058
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#33021;&#37327;&#30340;&#29983;&#25104;&#37327;&#23376;&#22810;&#20307;&#24577;&#34920;&#31034;&#26041;&#27861;&#65292;&#33021;&#22815;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#26500;&#24314;&#33021;&#22815;&#25429;&#33719;&#37327;&#23376;&#22810;&#20307;&#24577;&#22797;&#26434;&#30456;&#20851;&#24615;&#21644;&#23545;&#31216;&#24615;&#30340;&#34920;&#31034;&#65292;&#35813;&#26041;&#27861;&#39640;&#25928;&#22320;&#23398;&#20064;1D&#21644;2D&#30340;&#37327;&#23376;&#22810;&#20307;&#24577;&#65292;&#24182;&#21487;&#33258;&#28982;&#22320;&#32435;&#20837;&#29289;&#29702;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#22320;&#34920;&#31034;&#37327;&#23376;&#22810;&#20307;&#24577;&#22312;&#32463;&#20856;&#35745;&#31639;&#26426;&#19978;&#26159;&#19968;&#20010;&#26497;&#20855;&#23454;&#29992;&#24847;&#20041;&#30340;&#38382;&#39064;&#12290;&#29702;&#24819;&#30340;&#37327;&#23376;&#24577;&#34920;&#31034;&#32467;&#21512;&#20102;&#31995;&#32479;&#32467;&#26500;&#21644;&#23545;&#31216;&#24615;&#30340;&#31616;&#27905;&#25551;&#36848;&#20197;&#21450;&#39044;&#27979;&#24863;&#20852;&#36259;&#30340;&#29289;&#29702;&#21487;&#35266;&#27979;&#37327;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#37319;&#29992;&#20102;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#26500;&#24314;&#36825;&#26679;&#30340;&#32463;&#20856;&#34920;&#31034;&#65292;&#20351;&#20854;&#33021;&#22815;&#39044;&#27979;&#21487;&#35266;&#27979;&#37327;&#24182;&#32771;&#34385;&#29289;&#29702;&#23545;&#31216;&#24615;&#12290;&#28982;&#32780;&#65292;&#38500;&#38750;&#37319;&#29992;&#22522;&#20110;&#31995;&#32479;&#20808;&#39564;&#30693;&#35782;&#30340;&#19987;&#19994;&#20808;&#39564;&#24418;&#24335;&#65292;&#21542;&#21017;&#37327;&#23376;&#24577;&#30340;&#32467;&#26500;&#36890;&#24120;&#20250;&#20002;&#22833;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#36825;&#26679;&#30340;&#26041;&#27861;&#27809;&#26377;&#25552;&#20379;&#20851;&#20110;&#21738;&#20123;&#29366;&#24577;&#27604;&#20854;&#20182;&#29366;&#24577;&#26356;&#23481;&#26131;&#23398;&#20064;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#33021;&#37327;&#30340;&#29983;&#25104;&#37327;&#23376;&#22810;&#20307;&#24577;&#34920;&#31034;&#65292;&#35813;&#34920;&#31034;&#28304;&#33258;&#20110;&#29992;&#20110;&#27169;&#25311;&#32463;&#20856;&#33258;&#26059;&#31995;&#32479;&#30340;&#28909;&#24577;&#30340;Gibbs&#20998;&#24067;&#12290;&#22522;&#20110;&#20808;&#39564;&#20449;&#24687;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#26500;&#24314;&#34920;&#31034;&#65292;&#25429;&#33719;&#29366;&#24577;&#30340;&#22797;&#26434;&#30456;&#20851;&#24615;&#21644;&#23545;&#31216;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22914;&#20309;&#39640;&#25928;&#22320;&#23398;&#20064;1D&#21644;2D&#30340;&#37327;&#23376;&#22810;&#20307;&#24577;&#65292;&#26080;&#35770;&#26159;&#25509;&#36817;&#20020;&#30028;&#28857;&#36824;&#26159;&#36828;&#31163;&#20020;&#30028;&#28857;&#65292;&#20197;&#21450;&#22914;&#20309;&#33258;&#28982;&#22320;&#32435;&#20837;&#29289;&#29702;&#32422;&#26463;&#26465;&#20214;&#65292;&#22914;&#23432;&#24658;&#23450;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient representation of quantum many-body states on classical computers is a problem of enormous practical interest. An ideal representation of a quantum state combines a succinct characterization informed by the system's structure and symmetries, along with the ability to predict the physical observables of interest. A number of machine learning approaches have been recently used to construct such classical representations [1-6] which enable predictions of observables [7] and account for physical symmetries [8]. However, the structure of a quantum state gets typically lost unless a specialized ansatz is employed based on prior knowledge of the system [9-12]. Moreover, most such approaches give no information about what states are easier to learn in comparison to others. Here, we propose a new generative energy-based representation of quantum many-body states derived from Gibbs distributions used for modeling the thermal states of classical spin systems. Based on the prior informat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#20110;SemEval-2023&#30340;&#20219;&#21153;9&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;transformer&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#20102;&#38598;&#25104;&#23398;&#20064;&#65292;&#22312;&#22810;&#35821;&#35328;&#25512;&#29305;&#20146;&#23494;&#24230;&#26816;&#27979;&#20013;&#25490;&#21517;&#31532;4&#65292;&#36798;&#21040;&#20102;0.5688&#30340;&#23439;&#24179;&#22343;F1&#20998;&#25968;&#12290;&#20026;&#20102;&#25552;&#39640;&#23545;&#26410;&#35265;&#35821;&#35328;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#27599;&#20010;&#25512;&#29305;&#37117;&#36827;&#34892;&#20102;&#33521;&#25991;&#32763;&#35793;&#30340;&#34917;&#20805;&#12290;</title><link>http://arxiv.org/abs/2304.04054</link><description>&lt;p&gt;
tmn&#22312;SemEval-2023&#20219;&#21153;9&#20013;&#30340;&#24212;&#29992;&#65306;&#20351;&#29992;XLM-T&#12289;Google&#32763;&#35793;&#21644;&#38598;&#25104;&#23398;&#20064;&#36827;&#34892;&#22810;&#35821;&#35328;&#25512;&#29305;&#20146;&#23494;&#24230;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
tmn at SemEval-2023 Task 9: Multilingual Tweet Intimacy Detection using XLM-T, Google Translate, and Ensemble Learning. (arXiv:2304.04054v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#20110;SemEval-2023&#30340;&#20219;&#21153;9&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;transformer&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#20102;&#38598;&#25104;&#23398;&#20064;&#65292;&#22312;&#22810;&#35821;&#35328;&#25512;&#29305;&#20146;&#23494;&#24230;&#26816;&#27979;&#20013;&#25490;&#21517;&#31532;4&#65292;&#36798;&#21040;&#20102;0.5688&#30340;&#23439;&#24179;&#22343;F1&#20998;&#25968;&#12290;&#20026;&#20102;&#25552;&#39640;&#23545;&#26410;&#35265;&#35821;&#35328;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#27599;&#20010;&#25512;&#29305;&#37117;&#36827;&#34892;&#20102;&#33521;&#25991;&#32763;&#35793;&#30340;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;transformer&#30340;&#31995;&#32479;&#65292;&#38024;&#23545;SemEval-2023&#20219;&#21153;9&#65306;&#22810;&#35821;&#35328;&#25512;&#29305;&#20146;&#23494;&#24230;&#20998;&#26512;&#36827;&#34892;&#35774;&#35745;&#12290;&#20219;&#21153;&#30340;&#30446;&#30340;&#26159;&#39044;&#27979;&#19968;&#31995;&#21015;&#25512;&#29305;&#30340;&#20146;&#23494;&#24230;&#65292;&#33539;&#22260;&#20174;1&#65288;&#23436;&#20840;&#19981;&#20146;&#23494;&#65289;&#21040;5&#65288;&#38750;&#24120;&#20146;&#23494;&#65289;&#12290;&#27604;&#36187;&#30340;&#23448;&#26041;&#35757;&#32451;&#38598;&#21253;&#21547;&#20845;&#31181;&#35821;&#35328;&#30340;&#25512;&#29305;&#65288;&#33521;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#12289;&#24847;&#22823;&#21033;&#35821;&#12289;&#33889;&#33796;&#29273;&#35821;&#12289;&#27861;&#35821;&#21644;&#20013;&#25991;&#65289;&#12290;&#27979;&#35797;&#38598;&#21253;&#25324;&#20845;&#31181;&#32473;&#23450;&#30340;&#35821;&#35328;&#20197;&#21450;&#22806;&#37096;&#25968;&#25454;&#65292;&#20854;&#20013;&#21253;&#25324;&#35757;&#32451;&#38598;&#20013;&#26410;&#20986;&#29616;&#30340;&#22235;&#31181;&#35821;&#35328;&#65288;&#21360;&#22320;&#35821;&#12289;&#38463;&#25289;&#20271;&#35821;&#12289;&#33655;&#20848;&#35821;&#21644;&#38889;&#35821;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;XLM-T&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#36866;&#29992;&#20110;Twitter&#39046;&#22495;&#30340;&#22810;&#35821;&#31181;RoBERTa&#27169;&#22411;&#30340;&#38598;&#25104;&#12290;&#20026;&#20102;&#25552;&#39640;&#23545;&#26410;&#35265;&#35821;&#35328;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#25105;&#20204;&#23545;&#27599;&#26465;&#25512;&#29305;&#36827;&#34892;&#20102;&#33521;&#25991;&#32763;&#35793;&#30340;&#34917;&#20805;&#12290;&#25105;&#20204;&#25506;&#31350;&#20102;&#23558;&#32763;&#35793;&#25968;&#25454;&#24212;&#29992;&#20110;&#24494;&#35843;&#20013;&#30475;&#21040;&#30340;&#35821;&#35328;&#19982;&#26410;&#30475;&#21040;&#30340;&#35821;&#35328;&#30340;transformer&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20272;&#35745;&#20351;&#29992;&#32763;&#35793;&#25968;&#25454;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;50&#20010;&#22242;&#38431;&#20013;&#25490;&#21517;&#31532;4&#65292;&#24182;&#23454;&#29616;&#20102;0.5688&#30340;&#23439;&#24179;&#22343;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper describes a transformer-based system designed for SemEval-2023 Task 9: Multilingual Tweet Intimacy Analysis. The purpose of the task was to predict the intimacy of tweets in a range from 1 (not intimate at all) to 5 (very intimate). The official training set for the competition consisted of tweets in six languages (English, Spanish, Italian, Portuguese, French, and Chinese). The test set included the given six languages as well as external data with four languages not presented in the training set (Hindi, Arabic, Dutch, and Korean). We presented a solution based on an ensemble of XLM-T, a multilingual RoBERTa model adapted to the Twitter domain. To improve the performance of unseen languages, each tweet was supplemented by its English translation. We explored the effectiveness of translated data for the languages seen in fine-tuning compared to unseen languages and estimated strategies for using translated data in transformer-based models. Our solution ranked 4th on the leade
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#27491;&#21017;&#21270;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#36827;&#34892;&#27604;&#36739;&#65292;&#20998;&#26512;&#20102;&#20165;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.04052</link><description>&lt;p&gt;
&#20165;&#35299;&#30721;&#22120;&#25110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65311;&#23558;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#20026;&#27491;&#21017;&#21270;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Decoder-Only or Encoder-Decoder? Interpreting Language Model as a Regularized Encoder-Decoder. (arXiv:2304.04052v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04052
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#27491;&#21017;&#21270;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#36827;&#34892;&#27604;&#36739;&#65292;&#20998;&#26512;&#20102;&#20165;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#21040;&#24207;&#21015;&#65288;seq2seq&#65289;&#20219;&#21153;&#26088;&#22312;&#22522;&#20110;&#32473;&#23450;&#30340;&#36755;&#20837;&#28304;&#24207;&#21015;&#29983;&#25104;&#30446;&#26631;&#24207;&#21015;&#12290; &#20256;&#32479;&#19978;&#65292;&#22823;&#22810;&#25968;seq2seq&#20219;&#21153;&#37117;&#26159;&#36890;&#36807;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#35299;&#20915;&#30340;&#65292;&#35813;&#26694;&#26550;&#38656;&#35201;&#32534;&#30721;&#22120;&#26469;&#32534;&#30721;&#28304;&#24207;&#21015;&#65292;&#24182;&#19988;&#38656;&#35201;&#35299;&#30721;&#22120;&#26469;&#29983;&#25104;&#30446;&#26631;&#25991;&#26412;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#26032;&#26041;&#27861;&#65292;&#23558;&#20165;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#24212;&#29992;&#20110;seq2seq&#20219;&#21153;&#12290;&#23613;&#31649;&#22312;&#23558;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;seq2seq&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#20173;&#28982;&#32570;&#20047;&#23545;&#20165;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#26377;&#25928;&#24615;&#30340;&#24443;&#24213;&#20998;&#26512;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23545;&#27491;&#21017;&#21270;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#36827;&#34892;&#20998;&#26512;&#26469;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#12290;&#35813;&#32467;&#26500;&#26088;&#22312;&#22797;&#21046;&#32463;&#20856;&#20165;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25152;&#26377;&#34892;&#20026;&#65292;&#20294;&#20855;&#26377;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#65292;&#20174;&#32780;&#26356;&#23481;&#26131;&#36827;&#34892;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The sequence-to-sequence (seq2seq) task aims at generating the target sequence based on the given input source sequence. Traditionally, most of the seq2seq task is resolved by the Encoder-Decoder framework which requires an encoder to encode the source sequence and a decoder to generate the target text. Recently, a bunch of new approaches have emerged that apply decoder-only language models directly to the seq2seq task. Despite the significant advancements in applying language models to the seq2seq task, there is still a lack of thorough analysis on the effectiveness of the decoder-only language model architecture. This paper aims to address this gap by conducting a detailed comparison between the encoder-decoder architecture and the decoder-only language model framework through the analysis of a regularized encoder-decoder structure. This structure is designed to replicate all behaviors in the classical decoder-only language model but has an encoder and a decoder making it easier to b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230; Q-Learning &#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104; ReLCol &#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#22270;&#30528;&#33394;&#38382;&#39064;&#65292;&#19988;&#30456;&#36739;&#20110;&#29616;&#26377;&#31639;&#27861;&#20855;&#26377;&#31454;&#20105;&#24615;&#21644;&#20248;&#36234;&#24615;&#33021;&#12290;&#24378;&#21270;&#23398;&#20064;&#26159;&#25506;&#31350;&#22270;&#30528;&#33394;&#38382;&#39064;&#26356;&#26377;&#21069;&#36884;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.04051</link><description>&lt;p&gt;
&#21033;&#29992;&#28145;&#24230; Q-Learning &#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#22270;&#30528;&#33394;&#21551;&#21457;&#24335;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Generating a Graph Colouring Heuristic with Deep Q-Learning and Graph Neural Networks. (arXiv:2304.04051v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230; Q-Learning &#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104; ReLCol &#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#22270;&#30528;&#33394;&#38382;&#39064;&#65292;&#19988;&#30456;&#36739;&#20110;&#29616;&#26377;&#31639;&#27861;&#20855;&#26377;&#31454;&#20105;&#24615;&#21644;&#20248;&#36234;&#24615;&#33021;&#12290;&#24378;&#21270;&#23398;&#20064;&#26159;&#25506;&#31350;&#22270;&#30528;&#33394;&#38382;&#39064;&#26356;&#26377;&#21069;&#36884;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#30528;&#33394;&#38382;&#39064;&#26159;&#23558;&#26631;&#31614;&#25110;&#39068;&#33394;&#20998;&#37197;&#32473;&#22270;&#30340;&#39030;&#28857;&#65292;&#20197;&#20351;&#24471;&#30456;&#37051;&#30340;&#20004;&#20010;&#39030;&#28857;&#19981;&#20250;&#20849;&#20139;&#30456;&#21516;&#30340;&#39068;&#33394;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26159;&#21542;&#21487;&#29992;&#20110;&#21457;&#29616;&#19968;&#20010;&#31454;&#20105;&#24615;&#30340;&#22270;&#30528;&#33394;&#26500;&#36896;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;ReLCol&#65292;&#20351;&#29992;&#28145;&#24230; Q-Learning &#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#24182;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#21442;&#25968;&#21270;&#26041;&#27861;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#20351;&#29992;&#20855;&#26377;&#19981;&#21516;&#25299;&#25169;&#32467;&#26500;&#30340;&#26631;&#20934;&#22522;&#20934;&#22270;&#65292;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#35780;&#20272;&#20102; ReLCol &#23398;&#20064;&#21040;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#19982;&#29616;&#26377;&#26500;&#36896;&#31639;&#27861;&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#35777;&#26126;&#20102;&#24378;&#21270;&#23398;&#20064;&#26159;&#36827;&#19968;&#27493;&#30740;&#31350;&#22270;&#30528;&#33394;&#38382;&#39064;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The graph colouring problem consists of assigning labels, or colours, to the vertices of a graph such that no two adjacent vertices share the same colour. In this work we investigate whether deep reinforcement learning can be used to discover a competitive construction heuristic for graph colouring. Our proposed approach, ReLCol, uses deep Q-learning together with a graph neural network for feature extraction, and employs a novel way of parameterising the graph that results in improved performance. Using standard benchmark graphs with varied topologies, we empirically evaluate the benefits and limitations of the heuristic learned by ReLCol relative to existing construction algorithms, and demonstrate that reinforcement learning is a promising direction for further research on the graph colouring problem.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21517;&#20026;BSDE-Gen&#65292;&#23427;&#23558;&#21453;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#28789;&#27963;&#24615;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#22823;&#33021;&#21147;&#32467;&#21512;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#32500;&#22797;&#26434;&#30446;&#26631;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#22312;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#12290;BSDE-Gen&#30340;&#29983;&#25104;&#24314;&#27169;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#38543;&#26426;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#26159;&#19968;&#31181;&#26377;&#25928;&#32780;&#33258;&#28982;&#30340;&#29983;&#25104;&#39640;&#32500;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.04049</link><description>&lt;p&gt;
&#24102;&#21453;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Deep Generative Modeling with Backward Stochastic Differential Equations. (arXiv:2304.04049v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04049
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21517;&#20026;BSDE-Gen&#65292;&#23427;&#23558;&#21453;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#28789;&#27963;&#24615;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#22823;&#33021;&#21147;&#32467;&#21512;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#32500;&#22797;&#26434;&#30446;&#26631;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#22312;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#12290;BSDE-Gen&#30340;&#29983;&#25104;&#24314;&#27169;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#38543;&#26426;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#26159;&#19968;&#31181;&#26377;&#25928;&#32780;&#33258;&#28982;&#30340;&#29983;&#25104;&#39640;&#32500;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21517;&#20026;BSDE-Gen&#65292;&#23427;&#23558;&#21453;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;BSDEs&#65289;&#30340;&#28789;&#27963;&#24615;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#22823;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#20197;&#29983;&#25104;&#39640;&#32500;&#22797;&#26434;&#30446;&#26631;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#22312;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#12290;&#22312;&#29983;&#25104;&#24314;&#27169;&#36807;&#31243;&#20013;&#24341;&#20837;&#38543;&#26426;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#20351;BSDE-Gen&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#32780;&#33258;&#28982;&#30340;&#29983;&#25104;&#39640;&#32500;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;BSDE-Gen&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#25551;&#36848;&#20102;&#20854;&#27169;&#22411;&#26550;&#26500;&#65292;&#32473;&#20986;&#20102;&#29992;&#20110;&#35757;&#32451;&#30340;&#26368;&#22823;&#24179;&#22343;&#20559;&#24046;&#65288;MMD&#65289;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#25253;&#21578;&#20102;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel deep generative model, called BSDE-Gen, which combines the flexibility of backward stochastic differential equations (BSDEs) with the power of deep neural networks for generating high-dimensional complex target data, particularly in the field of image generation. The incorporation of stochasticity and uncertainty in the generative modeling process makes BSDE-Gen an effective and natural approach for generating high-dimensional data. The paper provides a theoretical framework for BSDE-Gen, describes its model architecture, presents the maximum mean discrepancy (MMD) loss function used for training, and reports experimental results.
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#21487;&#20197;&#30452;&#25509;&#36827;&#34892;&#24418;&#29366;&#25512;&#26029;&#65292;&#24182;&#19988;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#22522;&#20110;&#30690;&#37327;&#30340;&#24037;&#20316;&#27969;&#65292;&#35813;&#27169;&#22411;&#22312;&#22320;&#29702;&#31354;&#38388;&#35268;&#21010;&#20013;&#30340;&#30690;&#37327;&#21270;&#34920;&#29616;&#20013;&#39046;&#20808;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.04048</link><description>&lt;p&gt;
&#22810;&#36793;&#24418;&#21270;&#22120;&#65306;&#19968;&#31181;&#33258;&#22238;&#24402;&#24314;&#31569;&#36718;&#24275;&#32447;&#25552;&#21462;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Polygonizer: An auto-regressive building delineator. (arXiv:2304.04048v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04048
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#21487;&#20197;&#30452;&#25509;&#36827;&#34892;&#24418;&#29366;&#25512;&#26029;&#65292;&#24182;&#19988;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#22522;&#20110;&#30690;&#37327;&#30340;&#24037;&#20316;&#27969;&#65292;&#35813;&#27169;&#22411;&#22312;&#22320;&#29702;&#31354;&#38388;&#35268;&#21010;&#20013;&#30340;&#30690;&#37327;&#21270;&#34920;&#29616;&#20013;&#39046;&#20808;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22320;&#29702;&#31354;&#38388;&#35268;&#21010;&#20013;&#65292;&#20197;&#30690;&#37327;&#26684;&#24335;&#34920;&#31034;&#23545;&#35937;&#36890;&#24120;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#22240;&#20026;&#36825;&#31181;&#26684;&#24335;&#21487;&#20197;&#36731;&#26494;&#36716;&#25442;&#20026;&#19979;&#28216;&#20219;&#21153;&#65292;&#22914;Web&#24320;&#21457;&#12289;&#22270;&#24418;&#25110;&#35774;&#35745;&#12290;&#34429;&#28982;&#36825;&#20123;&#38382;&#39064;&#36890;&#24120;&#20351;&#29992;&#35821;&#20041;&#20998;&#21106;&#26469;&#35299;&#20915;&#65292;&#20294;&#38656;&#35201;&#39069;&#22806;&#30340;&#21518;&#22788;&#29702;&#25165;&#33021;&#23545;&#23545;&#35937;&#36827;&#34892;&#38750;&#24179;&#20961;&#22320;&#30690;&#37327;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#20687;&#21040;&#24207;&#21015;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#30452;&#25509;&#36827;&#34892;&#24418;&#29366;&#25512;&#26029;&#65292;&#24182;&#19988;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#22522;&#20110;&#30690;&#37327;&#30340;&#24037;&#20316;&#27969;&#12290;&#25105;&#20204;&#20197;&#21508;&#31181;&#26041;&#24335;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#23545;&#19982;&#36965;&#24863;&#24212;&#29992;&#20013;&#24120;&#35265;&#30340;&#21464;&#21270;&#25110;&#20266;&#24433;&#23545;&#24212;&#30340;&#22270;&#20687;&#36755;&#20837;&#30340;&#25200;&#21160;&#12290;&#24403;&#20351;&#29992;&#22320;&#38754;&#30495;&#20540;&#36793;&#30028;&#26694;&#65288;&#27599;&#20010;&#22270;&#20687;&#19968;&#20010;&#23545;&#35937;&#65289;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35823;&#24046;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#20316;&#21697;&#65292;&#23454;&#29616;&#20102;&#26368;&#20302;&#30340;&#26368;&#22823;&#20999;&#32447;&#35282;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In geospatial planning, it is often essential to represent objects in a vectorized format, as this format easily translates to downstream tasks such as web development, graphics, or design. While these problems are frequently addressed using semantic segmentation, which requires additional post-processing to vectorize objects in a non-trivial way, we present an Image-to-Sequence model that allows for direct shape inference and is ready for vector-based workflows out of the box. We demonstrate the model's performance in various ways, including perturbations to the image input that correspond to variations or artifacts commonly encountered in remote sensing applications. Our model outperforms prior works when using ground truth bounding boxes (one object per image), achieving the lowest maximum tangent angle error.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21453;&#27491;&#21017;&#21270;&#30340;&#28145;&#24230;&#38598;&#25104;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#39640;&#32500;&#22238;&#24402;&#21644;&#20998;&#31867;&#20013;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20869;&#37096;&#21644;&#22806;&#37096;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.04042</link><description>&lt;p&gt;
&#28145;&#24230;&#21453;&#27491;&#21017;&#21270;&#38598;&#25104;&#25552;&#20379;&#21487;&#38752;&#30340;&#22495;&#22806;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Deep Anti-Regularized Ensembles provide reliable out-of-distribution uncertainty quantification. (arXiv:2304.04042v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21453;&#27491;&#21017;&#21270;&#30340;&#28145;&#24230;&#38598;&#25104;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#39640;&#32500;&#22238;&#24402;&#21644;&#20998;&#31867;&#20013;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20869;&#37096;&#21644;&#22806;&#37096;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#39640;&#32500;&#22238;&#24402;&#21644;&#20998;&#31867;&#20013;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#38382;&#39064;&#65292;&#28145;&#24230;&#38598;&#25104;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35757;&#32451;&#33539;&#22260;&#20043;&#22806;&#65292;&#28145;&#24230;&#38598;&#25104;&#24448;&#24448;&#36820;&#22238;&#36807;&#24230;&#33258;&#20449;&#30340;&#20272;&#35745;&#20540;&#65292;&#36825;&#26159;&#19968;&#31181;&#20027;&#35201;&#30340;&#38480;&#21046;&#65292;&#22240;&#20026;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#32463;&#24120;&#36935;&#21040;&#20559;&#31227;&#20998;&#24067;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#35299;&#20915;&#22686;&#21152;&#38598;&#25104;&#36755;&#20986;&#30340;&#22810;&#26679;&#24615;&#21644;&#36827;&#34892;&#20934;&#30830;&#30340;&#20869;&#37096;&#39044;&#27979;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#20855;&#26377;&#22823;&#26435;&#37325;&#30340;&#32593;&#32476;&#30340;&#38598;&#25104;&#24456;&#21487;&#33021;&#23454;&#29616;&#36825;&#20004;&#20010;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#23454;&#29992;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#36825;&#26679;&#30340;&#38598;&#25104;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#24809;&#32602;&#23567;&#26435;&#37325;&#30340;&#21407;&#22987;&#21453;&#27491;&#21017;&#21270;&#26415;&#35821;&#21644;&#25511;&#21046;&#26435;&#37325;&#22686;&#21152;&#30340;&#36807;&#31243;&#65292;&#20197;&#20445;&#25345;&#20869;&#37096;&#20998;&#24067;&#25439;&#22833;&#22312;&#21487;&#25509;&#21463;&#30340;&#38408;&#20540;&#33539;&#22260;&#20869;&#12290;&#25152;&#24320;&#21457;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#22806;&#37096;&#25968;&#25454;&#25110;&#22797;&#26434;&#30340;&#35757;&#32451;&#25216;&#26415;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#38598;&#25104;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20869;&#37096;&#21644;&#22806;&#37096;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of uncertainty quantification in high dimensional regression and classification for which deep ensemble have proven to be promising methods. Recent observations have shown that deep ensemble often return overconfident estimates outside the training domain, which is a major limitation because shifted distributions are often encountered in real-life scenarios. The principal challenge for this problem is to solve the trade-off between increasing the diversity of the ensemble outputs and making accurate in-distribution predictions. In this work, we show that an ensemble of networks with large weights fitting the training data are likely to meet these two objectives. We derive a simple and practical approach to produce such ensembles, based on an original anti-regularization term penalizing small weights and a control process of the weight increase which maintains the in-distribution loss under an acceptable threshold. The developed approach does not require any out-
&lt;/p&gt;</description></item><item><title>RescueSNN&#26159;&#19968;&#31181;&#29992;&#20110;&#20943;&#36731;SNN&#33455;&#29255;&#35745;&#31639;&#24341;&#25806;&#20013;&#27704;&#20037;&#25925;&#38556;&#30340;&#26041;&#27861;&#65292;&#21487;&#32500;&#25345;&#24615;&#33021;&#21644;&#36136;&#37327;&#24182;&#20943;&#23569;&#37325;&#26032;&#35757;&#32451;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2304.04041</link><description>&lt;p&gt;
RescueSNN: &#22312;&#27704;&#20037;&#25925;&#38556;&#19979;&#25552;&#39640;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;&#30340;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
RescueSNN: Enabling Reliable Executions on Spiking Neural Network Accelerators under Permanent Faults. (arXiv:2304.04041v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04041
&lt;/p&gt;
&lt;p&gt;
RescueSNN&#26159;&#19968;&#31181;&#29992;&#20110;&#20943;&#36731;SNN&#33455;&#29255;&#35745;&#31639;&#24341;&#25806;&#20013;&#27704;&#20037;&#25925;&#38556;&#30340;&#26041;&#27861;&#65292;&#21487;&#32500;&#25345;&#24615;&#33021;&#21644;&#36136;&#37327;&#24182;&#20943;&#23569;&#37325;&#26032;&#35757;&#32451;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#23884;&#20837;&#24335;&#31995;&#32479;&#19978;&#26368;&#22823;&#21270;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#22788;&#29702;&#30340;&#24615;&#33021;&#21644;&#33021;&#25928;&#65292;&#37319;&#29992;&#20102;&#19987;&#38376;&#30340;&#30828;&#20214;&#21152;&#36895;&#22120;/&#33455;&#29255;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;SNN&#33455;&#29255;&#21487;&#33021;&#20250;&#21463;&#21040;&#27704;&#20037;&#25925;&#38556;&#30340;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#37325;&#22823;&#30340;&#31934;&#24230;&#38477;&#20302;&#21644;&#31995;&#32479;&#25925;&#38556;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RescueSNN&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20943;&#36731;SNN&#33455;&#29255;&#35745;&#31639;&#24341;&#25806;&#20013;&#30340;&#27704;&#20037;&#25925;&#38556;&#65292;&#20174;&#32780;&#26174;&#30528;&#38477;&#20302;&#35774;&#35745;&#26102;&#38388;&#21644;&#37325;&#26032;&#35757;&#32451;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#21534;&#21520;&#37327;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
To maximize the performance and energy efficiency of Spiking Neural Network (SNN) processing on resource-constrained embedded systems, specialized hardware accelerators/chips are employed. However, these SNN chips may suffer from permanent faults which can affect the functionality of weight memory and neuron behavior, thereby causing potentially significant accuracy degradation and system malfunctioning. Such permanent faults may come from manufacturing defects during the fabrication process, and/or from device/transistor damages (e.g., due to wear out) during the run-time operation. However, the impact of permanent faults in SNN chips and the respective mitigation techniques have not been thoroughly investigated yet. Toward this, we propose RescueSNN, a novel methodology to mitigate permanent faults in the compute engine of SNN chips without requiring additional retraining, thereby significantly cutting down the design time and retraining costs, while maintaining the throughput and qu
&lt;/p&gt;</description></item><item><title>EnforceSNN &#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35774;&#35745;&#26694;&#26550;&#65292;&#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#32771;&#34385;&#36817;&#20284;DRAM&#65292;&#20351;&#29992;&#37327;&#21270;&#26435;&#37325;&#38477;&#20302;DRAM&#30340;&#35775;&#38382;&#33021;&#37327;&#65292;&#23454;&#29616;&#20102;&#24377;&#24615;&#21644;&#33410;&#33021;&#30340;SNN&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2304.04039</link><description>&lt;p&gt;
EnforceSNN: &#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#32771;&#34385;&#36817;&#20284;DRAM&#65292;&#23454;&#29616;&#24377;&#24615;&#21644;&#33410;&#33021;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
EnforceSNN: Enabling Resilient and Energy-Efficient Spiking Neural Network Inference considering Approximate DRAMs for Embedded Systems. (arXiv:2304.04039v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04039
&lt;/p&gt;
&lt;p&gt;
EnforceSNN &#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35774;&#35745;&#26694;&#26550;&#65292;&#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#32771;&#34385;&#36817;&#20284;DRAM&#65292;&#20351;&#29992;&#37327;&#21270;&#26435;&#37325;&#38477;&#20302;DRAM&#30340;&#35775;&#38382;&#33021;&#37327;&#65292;&#23454;&#29616;&#20102;&#24377;&#24615;&#21644;&#33410;&#33021;&#30340;SNN&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#30001;&#20110;&#20854;&#29983;&#29289;&#21487;&#34892;&#35745;&#31639;&#33021;&#21147;&#65292;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#20855;&#26377;&#39640;&#31934;&#24230;&#21644;&#20302;&#25805;&#20316;&#21151;&#29575;/&#33021;&#37327;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;DRAM&#30340; off-chip &#20869;&#23384;&#35775;&#38382;&#21344;&#25454;&#20102;SNN&#22788;&#29702;&#30340;&#33021;&#37327;&#28040;&#32791;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#24182;&#26410;&#20248;&#21270;DRAM&#30340;&#27599;&#27425;&#35775;&#38382;&#30340;&#33021;&#37327;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#22522;&#20110;SNN&#30340;&#31995;&#32479;&#23454;&#29616;&#36827;&#19968;&#27493;&#30340;&#33410;&#33021;&#25928;&#30410;&#12290;&#20026;&#20102;&#22823;&#24133;&#24230;&#38477;&#20302;DRAM&#30340;&#27599;&#27425;&#35775;&#38382;&#30340;&#33021;&#37327;&#65292;&#19968;&#20010;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#38477;&#20302;DRAM&#20379;&#30005;&#30005;&#21387;&#65292;&#20294;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;DRAM&#21333;&#20803;&#30340;&#38169;&#35823;&#65288;&#21363;&#25152;&#35859;&#30340;&#36817;&#20284;DRAM&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EnforceSNN&#65292;&#19968;&#31181;&#26032;&#30340;&#35774;&#35745;&#26694;&#26550;&#65292;&#20351;&#29992;&#38477;&#21387;DRAM&#23454;&#29616;&#24377;&#24615;&#21644;&#33410;&#33021;&#30340; SNN &#25512;&#29702;&#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#12290;&#25105;&#20204; EnforceSNN &#30340;&#20851;&#38190;&#26426;&#21046;&#26159;:(1)&#37319;&#29992;&#37327;&#21270;&#26435;&#37325;&#38477;&#20302;DRAM&#30340;&#35775;&#38382;&#33021;&#37327;&#65307;(2)&#35774;&#35745;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;DRAM&#26144;&#23556;p
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs) have shown capabilities of achieving high accuracy under unsupervised settings and low operational power/energy due to their bio-plausible computations. Previous studies identified that DRAM-based off-chip memory accesses dominate the energy consumption of SNN processing. However, state-of-the-art works do not optimize the DRAM energy-per-access, thereby hindering the SNN-based systems from achieving further energy efficiency gains. To substantially reduce the DRAM energy-per-access, an effective solution is to decrease the DRAM supply voltage, but it may lead to errors in DRAM cells (i.e., so-called approximate DRAM). Towards this, we propose \textit{EnforceSNN}, a novel design framework that provides a solution for resilient and energy-efficient SNN inference using reduced-voltage DRAM for embedded systems. The key mechanisms of our EnforceSNN are: (1) employing quantized weights to reduce the DRAM access energy; (2) devising an efficient DRAM mapping p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#40065;&#26834;&#24615;&#21028;&#21035;&#20998;&#31867;&#22120;&#19982;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#21457;&#29616;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#65292;&#38750;&#23450;&#21521;&#23545;&#25239;&#28857;&#38750;&#24120;&#21487;&#33021;&#22312;&#37492;&#21035;&#24615;&#27169;&#22411;&#20013;&#38544;&#21547;&#30340;&#29983;&#25104;&#27169;&#22411;&#20013;&#25317;&#26377;&#20302;&#33021;&#37327;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39640;&#33021;&#37327;PGD&#30340;&#26032;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2304.04033</link><description>&lt;p&gt;
&#25506;&#31350;&#40065;&#26834;&#24615;&#27169;&#22411;&#19982;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
Exploring the Connection between Robust and Generative Models. (arXiv:2304.04033v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#40065;&#26834;&#24615;&#21028;&#21035;&#20998;&#31867;&#22120;&#19982;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#21457;&#29616;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#65292;&#38750;&#23450;&#21521;&#23545;&#25239;&#28857;&#38750;&#24120;&#21487;&#33021;&#22312;&#37492;&#21035;&#24615;&#27169;&#22411;&#20013;&#38544;&#21547;&#30340;&#29983;&#25104;&#27169;&#22411;&#20013;&#25317;&#26377;&#20302;&#33021;&#37327;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39640;&#33021;&#37327;PGD&#30340;&#26032;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#36890;&#36807;&#20998;&#35299;&#40065;&#26834;&#24615;&#21028;&#21035;&#20998;&#31867;&#22120;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25506;&#31350;&#40065;&#26834;&#24615;&#21028;&#21035;&#20998;&#31867;&#22120;&#19982;&#33021;&#37327;&#22522;&#27169;&#22411;(EBM)&#24418;&#24335;&#30340;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#24120;&#35265;&#30340;&#20551;&#35774;&#26159;&#23545;&#25239;&#28857;&#31163;&#24320;&#20102;&#36755;&#20837;&#25968;&#25454;&#30340;&#27969;&#24418;&#65292;&#20294;&#26159;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#65292;&#38750;&#23450;&#21521;&#23545;&#25239;&#28857;&#38750;&#24120;&#21487;&#33021;&#22312;&#37492;&#21035;&#24615;&#27169;&#22411;&#20013;&#38544;&#21547;&#30340;&#29983;&#25104;&#27169;&#22411;&#20013;&#25317;&#26377;&#20302;&#33021;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#35777;&#25454;:&#38750;&#23450;&#21521;&#25915;&#20987;&#30340;&#27010;&#29575;&#29978;&#33267;&#27604;&#33258;&#28982;&#25968;&#25454;&#36824;&#35201;&#39640;&#65292;&#24182;&#19988;&#38543;&#30528;&#25915;&#20987;&#24378;&#24230;&#30340;&#22686;&#21152;&#65292;&#20854;&#27010;&#29575;&#20063;&#20250;&#22686;&#21152;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#36731;&#26494;&#22320;&#26816;&#27979;&#23427;&#20204;&#24182;&#35774;&#35745;&#19968;&#31181;&#21517;&#20026;&#39640;&#33021;&#37327;PGD&#30340;&#26032;&#25915;&#20987;&#65292;&#33021;&#22815;&#27450;&#39575;&#20998;&#31867;&#22120;&#20294;&#20855;&#26377;&#19982;&#25968;&#25454;&#38598;&#30456;&#20284;&#30340;&#33021;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We offer a study that connects robust discriminative classifiers trained with adversarial training (AT) with generative modeling in the form of Energy-based Models (EBM). We do so by decomposing the loss of a discriminative classifier and showing that the discriminative model is also aware of the input data density. Though a common assumption is that adversarial points leave the manifold of the input data, our study finds out that, surprisingly, untargeted adversarial points in the input space are very likely under the generative model hidden inside the discriminative classifier -- have low energy in the EBM. We present two evidence: untargeted attacks are even more likely than the natural data and their likelihood increases as the attack strength increases. This allows us to easily detect them and craft a novel attack called High-Energy PGD that fools the classifier yet has energy similar to the data set.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65306;NeBLa&#65292;&#21487;&#20197;&#20174;&#20840;&#26223;&#25918;&#23556;&#32447;&#22270;&#20013;&#36890;&#36807;&#31070;&#32463;&#21860;&#37202;-&#20848;&#20271;&#29305;&#27861;&#37325;&#24314;&#31934;&#30830;&#30340;3D&#21475;&#33108;&#32467;&#26500;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.04027</link><description>&lt;p&gt;
NeBLa: &#20351;&#29992;&#31070;&#32463;&#21860;&#37202;-&#20848;&#20271;&#29305;&#27861;&#20174;&#20840;&#26223;&#25918;&#23556;&#32447;&#22270;&#20013;&#37325;&#24314;&#21475;&#33108;&#32467;&#26500;&#30340;&#19977;&#32500;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
NeBLa: Neural Beer-Lambert for 3D Reconstruction of Oral Structures from Panoramic Radiographs. (arXiv:2304.04027v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04027
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65306;NeBLa&#65292;&#21487;&#20197;&#20174;&#20840;&#26223;&#25918;&#23556;&#32447;&#22270;&#20013;&#36890;&#36807;&#31070;&#32463;&#21860;&#37202;-&#20848;&#20271;&#29305;&#27861;&#37325;&#24314;&#31934;&#30830;&#30340;3D&#21475;&#33108;&#32467;&#26500;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#26223;X&#32447;&#29255;&#65288;&#20840;&#26223;&#25918;&#23556;&#32447;&#22270;&#65292;PX&#65289;&#26159;&#24120;&#29992;&#20110;&#29273;&#31185;&#26816;&#26597;&#30340;&#25104;&#20687;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#19982;3D&#38181;&#24418;&#26463;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CBCT&#65289;&#30456;&#27604;&#65292;PX&#30340;&#36866;&#29992;&#24615;&#26377;&#38480;&#65292;&#22240;&#20026;PX&#21482;&#25552;&#20379;&#21475;&#33108;&#32467;&#26500;&#30340;&#20108;&#32500;&#25153;&#24179;&#22270;&#20687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#30495;&#23454;&#30340;PX&#22270;&#20687;&#20272;&#35745;3D&#21475;&#33108;&#32467;&#26500;&#12290;&#30001;&#20110;PX&#21644;CBCT&#25968;&#25454;&#30340;&#21305;&#37197;&#19981;&#22810;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#26102;&#20351;&#29992;&#20102;&#20174;CBCT&#27169;&#25311;&#30340;PX&#65292;&#20294;&#22312;&#25512;&#29702;&#26102;&#20351;&#29992;&#20102;&#30495;&#23454;&#30340;&#20840;&#26223;&#25918;&#23556;&#32447;&#29255;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20809;&#32447;&#37319;&#26679;&#26041;&#27861;&#65292;&#21463;&#21040;&#20840;&#26223;&#25918;&#23556;&#32447;&#25104;&#20687;&#21407;&#29702;&#30340;&#21551;&#21457;&#65292;&#21033;&#29992;&#21860;&#37202;-&#20848;&#20271;&#29305;&#23450;&#24459;&#23548;&#20986;&#28210;&#26579;&#20989;&#25968;&#29983;&#25104;&#27169;&#25311;&#20840;&#26223;&#25918;&#23556;&#32447;&#22270;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#30001;&#19977;&#20010;&#37096;&#20998;&#32452;&#25104;&#65306;&#36716;&#25442;&#27169;&#22359;&#65292;&#29983;&#25104;&#27169;&#22359;&#21644;&#31934;&#28860;&#27169;&#22359;&#12290;&#36716;&#25442;&#27169;&#22359;&#23558;&#30495;&#23454;&#30340;&#20840;&#26223;&#25918;&#23556;&#32447;&#22270;&#36716;&#25442;&#20026;&#27169;&#25311;&#30340;&#35757;&#32451;&#22270;&#20687;&#39118;&#26684;&#12290;&#29983;&#25104;&#27169;&#22359;&#21033;&#29992;&#23556;&#32447;&#37319;&#26679;&#26041;&#27861;&#24471;&#21040;&#30340;&#27169;&#25311;&#20840;&#26223;&#25918;&#23556;&#32447;&#22270;&#32422;&#26463;&#19979;&#30340;&#36755;&#20837;&#22270;&#20687;&#29983;&#25104;3D&#32467;&#26500;&#12290;&#31934;&#28860;&#27169;&#22359;&#25913;&#21892;&#20102;3D&#32467;&#26500;&#30340;&#24179;&#28369;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#20174;&#20840;&#26223;&#25918;&#23556;&#32447;&#29255;&#25552;&#20379;&#30340;&#26377;&#38480;&#20449;&#24687;&#20013;&#29983;&#25104;&#31934;&#30830;&#30340;3D&#29273;&#31185;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Panoramic radiography (panoramic X-ray, PX) is a widely used imaging modality for dental examination. However, its applicability is limited as compared to 3D Cone-beam computed tomography (CBCT), because PX only provides 2D flattened images of the oral structure. In this paper, we propose a new framework which estimates 3D oral structure from real-world PX images. Since there are not many matching PX and CBCT data, we used simulated PX from CBCT for training, however, we used real-world panoramic radiographs at the inference time. We propose a new ray-sampling method to make simulated panoramic radiographs inspired by the principle of panoramic radiography along with the rendering function derived from the Beer-Lambert law. Our model consists of three parts: translation module, generation module, and refinement module. The translation module changes the real-world panoramic radiograph to the simulated training image style. The generation module makes the 3D structure from the input ima
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#36951;&#20256;&#35268;&#21010;&#31639;&#27861;&#26469;&#35299;&#20915;&#32771;&#34385;&#20154;-&#24037;&#21305;&#37197;&#30340;&#22242;&#38431;&#32452;&#24314;&#38382;&#39064;&#65292;&#37319;&#29992;&#38598;&#21512;&#31181;&#32676;&#31574;&#30053;&#21644;&#20195;&#29702;&#27169;&#22411;&#21152;&#24555;&#31639;&#27861;&#23398;&#20064;&#36807;&#31243;&#65292;&#23454;&#29616;&#21208;&#25506;&#21644;&#21033;&#29992;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.04022</link><description>&lt;p&gt;
&#19968;&#31181;&#32771;&#34385;&#20154;-&#24037;&#21305;&#37197;&#30340;&#22242;&#38431;&#32452;&#24314;&#38382;&#39064;&#30340;&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#36951;&#20256;&#35268;&#21010;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Reinforcement Learning-assisted Genetic Programming Algorithm for Team Formation Problem Considering Person-Job Matching. (arXiv:2304.04022v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04022
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#36951;&#20256;&#35268;&#21010;&#31639;&#27861;&#26469;&#35299;&#20915;&#32771;&#34385;&#20154;-&#24037;&#21305;&#37197;&#30340;&#22242;&#38431;&#32452;&#24314;&#38382;&#39064;&#65292;&#37319;&#29992;&#38598;&#21512;&#31181;&#32676;&#31574;&#30053;&#21644;&#20195;&#29702;&#27169;&#22411;&#21152;&#24555;&#31639;&#27861;&#23398;&#20064;&#36807;&#31243;&#65292;&#23454;&#29616;&#21208;&#25506;&#21644;&#21033;&#29992;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#30340;&#22242;&#38431;&#23545;&#20110;&#20844;&#21496;&#25104;&#21151;&#23436;&#25104;&#26032;&#39033;&#30446;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#35299;&#20915;&#32771;&#34385;&#20154;-&#24037;&#21305;&#37197;&#30340;&#22242;&#38431;&#32452;&#24314;&#38382;&#39064;&#65288;TFP-PJM&#65289;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;0-1&#25972;&#25968;&#35268;&#21010;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#20154;-&#24037;&#21305;&#37197;&#21644;&#22242;&#38431;&#25104;&#21592;&#36890;&#20449;&#24847;&#24895;&#23545;&#22242;&#38431;&#25928;&#29575;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#30452;&#35273;&#27169;&#31946;&#25968;&#35745;&#31639;&#20154;-&#24037;&#21305;&#37197;&#24471;&#20998;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#36951;&#20256;&#35268;&#21010;&#31639;&#27861;&#65288;RL-GP&#65289;&#20197;&#25552;&#39640;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#12290;RL-GP&#37319;&#29992;&#38598;&#21512;&#31181;&#32676;&#31574;&#30053;&#12290;&#22312;&#27599;&#19968;&#20195;&#31181;&#32676;&#36827;&#21270;&#20043;&#21069;&#65292;&#20195;&#29702;&#26681;&#25454;&#33719;&#24471;&#30340;&#20449;&#24687;&#20174;&#22235;&#31181;&#31181;&#32676;&#25628;&#32034;&#27169;&#24335;&#20013;&#36873;&#25321;&#19968;&#31181;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21208;&#25506;&#21644;&#21033;&#29992;&#30340;&#33391;&#22909;&#24179;&#34913;&#12290;&#27492;&#22806;&#65292;&#31639;&#27861;&#20351;&#29992;&#20195;&#29702;&#27169;&#22411;&#35780;&#20272;&#20010;&#20307;&#29983;&#25104;&#30340;&#32452;&#24314;&#26041;&#26696;&#65292;&#21152;&#24555;&#31639;&#27861;&#23398;&#20064;&#36807;&#31243;&#12290;&#28982;&#21518;&#65292;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23545;&#27604;&#23454;&#39564;&#20197;&#39564;&#35777;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
An efficient team is essential for the company to successfully complete new projects. To solve the team formation problem considering person-job matching (TFP-PJM), a 0-1 integer programming model is constructed, which considers both person-job matching and team members' willingness to communicate on team efficiency, with the person-job matching score calculated using intuitionistic fuzzy numbers. Then, a reinforcement learning-assisted genetic programming algorithm (RL-GP) is proposed to enhance the quality of solutions. The RL-GP adopts the ensemble population strategies. Before the population evolution at each generation, the agent selects one from four population search modes according to the information obtained, thus realizing a sound balance of exploration and exploitation. In addition, surrogate models are used in the algorithm to evaluate the formation plans generated by individuals, which speeds up the algorithm learning process. Afterward, a series of comparison experiments 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#31639;&#26415;&#24378;&#24230;&#24179;&#34913;&#21367;&#31215;&#65288;ABConv&#65289;&#21487;&#20197;&#22312;&#19981;&#29306;&#29298;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#26174;&#33879;&#38477;&#20302;&#24310;&#36831;&#65292;&#24182;&#22686;&#24378;&#30828;&#20214;&#24615;&#33021;&#65292;&#35813;&#26041;&#27861;&#24050;&#22312;Arm Ethos-U65 NPU&#19978;&#24471;&#21040;&#39564;&#35777;&#65292;&#21487;&#29992;&#20110;&#26367;&#25442;&#19968;&#20123;MobileNetV1&#21644;ResNet50&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.04016</link><description>&lt;p&gt;
&#30828;&#20214;&#24863;&#30693;&#26377;&#25928;&#22359;&#35774;&#35745;&#20013;&#30340;&#31639;&#26415;&#24378;&#24230;&#24179;&#34913;&#21367;&#31215;
&lt;/p&gt;
&lt;p&gt;
Arithmetic Intensity Balancing Convolution for Hardware-aware Efficient Block Design. (arXiv:2304.04016v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#31639;&#26415;&#24378;&#24230;&#24179;&#34913;&#21367;&#31215;&#65288;ABConv&#65289;&#21487;&#20197;&#22312;&#19981;&#29306;&#29298;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#26174;&#33879;&#38477;&#20302;&#24310;&#36831;&#65292;&#24182;&#22686;&#24378;&#30828;&#20214;&#24615;&#33021;&#65292;&#35813;&#26041;&#27861;&#24050;&#22312;Arm Ethos-U65 NPU&#19978;&#24471;&#21040;&#39564;&#35777;&#65292;&#21487;&#29992;&#20110;&#26367;&#25442;&#19968;&#20123;MobileNetV1&#21644;ResNet50&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#36793;&#32536;&#35774;&#22791;&#21644;&#36731;&#37327;&#32423;&#31070;&#32463;&#32593;&#32476;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#12290;&#20026;&#20102;&#20943;&#23569;AI&#21152;&#36895;&#22120;&#30340;&#24310;&#36831;&#65292;&#19981;&#20165;&#38656;&#35201;&#38477;&#20302;FLOPs&#65292;&#36824;&#38656;&#35201;&#22686;&#24378;&#30828;&#20214;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31639;&#26415;&#24378;&#24230;&#24179;&#34913;&#21367;&#31215;&#65288;ABConv&#65289;&#65292;&#20197;&#35299;&#20915;&#21367;&#31215;&#23567;&#31354;&#38388;&#22823;&#23567;&#30340;&#23567;&#26435;&#37325;&#31639;&#26415;&#24378;&#24230;&#38480;&#21046;&#25972;&#20307;&#24378;&#24230;&#30340;&#38382;&#39064;&#12290;ABConv&#21487;&#20197;&#22686;&#21152;&#25972;&#20307;&#31639;&#26415;&#24378;&#24230;&#30340;&#26368;&#22823;&#30028;&#38480;&#65292;&#24182;&#26174;&#33879;&#38477;&#20302;&#24310;&#36831;&#65292;&#32780;&#19981;&#29306;&#29298;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;Arm Ethos-U65 NPU&#19978;&#20197;&#22810;&#31181;&#37197;&#32622;&#27979;&#35797;&#20102;ABConv&#30340;&#24310;&#36831;&#21644;&#30828;&#20214;&#24615;&#33021;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;CIFAR100&#22270;&#20687;&#20998;&#31867;&#30340;MobileNetV1&#21644;ResNet50&#20013;&#30340;&#19968;&#20123;&#26367;&#25442;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep learning advances, edge devices and lightweight neural networks are becoming more important. To reduce latency in the AI accelerator, it's essential to not only reduce FLOPs but also enhance hardware performance. We proposed an arithmetic intensity balancing convolution (ABConv) to address the issue of the overall intensity being limited by the small weight arithmetic intensity for convolution with a small spatial size. ABConv increased the maximum bound of overall arithmetic intensity and significantly reduced latency, without sacrificing accuracy. We tested the latency and hardware performance of ABConv on the Arm Ethos-U65 NPU in various configurations and used it to replace some of MobileNetV1 and ResNet50 in image classification for CIFAR100.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23884;&#20837;&#24335;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#26102;&#26816;&#27979;&#30452;&#27969;&#20282;&#26381;&#30005;&#26426;&#30340;&#36807;&#36733;&#25925;&#38556;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#21462;&#23613;&#21487;&#33021;&#22810;&#30340;&#29305;&#24449;&#24182;&#22312;&#20302;&#20869;&#23384;&#24494;&#25511;&#21046;&#22120;&#20013;&#23454;&#29616;&#26377;&#25928;&#30340;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.04005</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#23454;&#26102;&#20282;&#26381;&#30005;&#26426;&#36807;&#36733;&#25925;&#38556;&#26816;&#27979;&#30340;&#23884;&#20837;&#24335;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30340;&#26032;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
A new transformation for embedded convolutional neural network approach toward real-time servo motor overload fault-detection. (arXiv:2304.04005v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04005
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23884;&#20837;&#24335;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#26102;&#26816;&#27979;&#30452;&#27969;&#20282;&#26381;&#30005;&#26426;&#30340;&#36807;&#36733;&#25925;&#38556;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#21462;&#23613;&#21487;&#33021;&#22810;&#30340;&#29305;&#24449;&#24182;&#22312;&#20302;&#20869;&#23384;&#24494;&#25511;&#21046;&#22120;&#20013;&#23454;&#29616;&#26377;&#25928;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#27969;&#20282;&#26381;&#30005;&#26426;&#30340;&#36229;&#36733;&#26159;&#24037;&#19994;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#65292;&#35768;&#22810;&#20844;&#21496;&#38754;&#20020;&#30528;&#25214;&#21040;&#19987;&#19994;&#25805;&#20316;&#21592;&#30340;&#38382;&#39064;&#65292;&#32780;&#20154;&#24037;&#30417;&#27979;&#21487;&#33021;&#19981;&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26032;&#30340;&#36716;&#25442;&#30340;&#23884;&#20837;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#20174;&#23454;&#26102;&#36755;&#20837;&#20449;&#21495;&#20013;&#25552;&#21462;&#25925;&#38556;&#65292;&#32780;&#26080;&#38656;&#20154;&#20026;&#24178;&#39044;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#30340;&#26159;&#20174;&#36755;&#20837;&#20449;&#21495;&#20013;&#25552;&#21462;&#23613;&#21487;&#33021;&#22810;&#30340;&#29305;&#24449;&#65292;&#20197;&#23454;&#29616;&#19968;&#20010;&#25918;&#26494;&#30340;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#26377;&#25928;&#20294;&#32039;&#20945;&#30340;&#32593;&#32476;&#65292;&#25552;&#20379;&#23454;&#26102;&#25925;&#38556;&#26816;&#27979;&#65292;&#29978;&#33267;&#22312;&#20302;&#20869;&#23384;&#24494;&#25511;&#21046;&#22120;&#20013;&#20063;&#21487;&#20197;&#23454;&#29616;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#25925;&#38556;&#26816;&#27979;&#26041;&#27861;&#65292;&#21363;&#21516;&#27493;&#21452;&#30005;&#26426;&#31995;&#32479;&#65292;&#20197;&#22312;&#25925;&#38556;&#20107;&#20214;&#20013;&#37319;&#21462;&#34892;&#21160;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#30340;&#65292;&#23545;&#27599;&#20010;&#30452;&#27969;&#20282;&#26381;&#30005;&#26426;&#30340;&#36755;&#20986;&#30005;&#27969;&#36827;&#34892;&#19968;&#32500;&#36755;&#20837;&#20449;&#21495;&#30340;&#30417;&#27979;&#21644;&#36716;&#25442;&#25104;&#20026;&#19968;&#20010;3D&#25968;&#25454;&#22534;&#26632;&#65292;&#28982;&#21518;&#23558;CNN&#23454;&#26045;&#21040;&#22788;&#29702;&#22120;&#20013;&#20197;&#26816;&#27979;&#19982;&#36755;&#20837;&#20449;&#21495;&#30456;&#23545;&#24212;&#30340;&#20219;&#20309;&#25925;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;
Overloading in DC servo motors is a major concern in industries, as many companies face the problem of finding expert operators, and also human monitoring may not be an effective solution. Therefore, this paper proposed an embedded Artificial intelligence (AI) approach using a Convolutional Neural Network (CNN) using a new transformation to extract faults from real-time input signals without human interference. Our main purpose is to extract as many as possible features from the input signal to achieve a relaxed dataset that results in an effective but compact network to provide real-time fault detection even in a low-memory microcontroller. Besides, fault detection method a synchronous dual-motor system is also proposed to take action in faulty events. To fulfill this intention, a one-dimensional input signal from the output current of each DC servo motor is monitored and transformed into a 3d stack of data and then the CNN is implemented into the processor to detect any fault corresp
&lt;/p&gt;</description></item><item><title>SimbaML&#26159;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#65292;&#23427;&#36890;&#36807;&#26426;&#26800;&#27169;&#22411;&#34917;&#20805;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#20174;&#21512;&#25104;&#21040;&#30495;&#23454;&#25968;&#25454;&#30340;&#36716;&#31227;&#23398;&#20064;&#12289;&#25968;&#25454;&#22686;&#24378;&#12289;&#29289;&#29702;&#21551;&#21457;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#31561;&#22810;&#39033;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.04000</link><description>&lt;p&gt;
SimbaML&#65306;&#20351;&#29992;&#22686;&#24378;&#25968;&#25454;&#36830;&#25509;&#26426;&#26800;&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
SimbaML: Connecting Mechanistic Models and Machine Learning with Augmented Data. (arXiv:2304.04000v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04000
&lt;/p&gt;
&lt;p&gt;
SimbaML&#26159;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#65292;&#23427;&#36890;&#36807;&#26426;&#26800;&#27169;&#22411;&#34917;&#20805;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#20174;&#21512;&#25104;&#21040;&#30495;&#23454;&#25968;&#25454;&#30340;&#36716;&#31227;&#23398;&#20064;&#12289;&#25968;&#25454;&#22686;&#24378;&#12289;&#29289;&#29702;&#21551;&#21457;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#31561;&#22810;&#39033;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#38598;&#65292;&#20294;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#26469;&#35828;&#65292;&#25910;&#38598;&#36825;&#20123;&#25968;&#25454;&#38598;&#24448;&#24448;&#26159;&#22256;&#38590;&#25110;&#26114;&#36149;&#30340;&#12290;&#22914;&#26524;&#26377;&#31995;&#32479;&#21160;&#24577;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#21017;&#21487;&#20197;&#20351;&#29992;&#26426;&#26800;&#27169;&#22411;&#26469;&#34917;&#20805;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SimbaML&#65288;&#22522;&#20110;&#20223;&#30495;&#30340;&#26426;&#22120;&#23398;&#20064;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#24037;&#20855;&#65292;&#23427;&#36890;&#36807;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#29983;&#25104;&#36924;&#30495;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24182;&#30452;&#25509;&#23558;&#20854;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#30340;&#20998;&#26512;&#21644;&#21253;&#21547;&#12290; SimbaML&#26041;&#20415;&#22320;&#21551;&#29992;&#20102;&#20174;&#21512;&#25104;&#21040;&#30495;&#23454;&#25968;&#25454;&#30340;&#36716;&#31227;&#23398;&#20064;&#65292;&#25968;&#25454;&#22686;&#24378;&#65292;&#30830;&#23450;&#23545;&#25968;&#25454;&#25910;&#38598;&#30340;&#38656;&#27714;&#20197;&#21450;&#22522;&#20934;&#27979;&#35797;&#29289;&#29702;&#21551;&#21457;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;SimbaML&#21487;&#22312;https://pypi.org/project/simba-ml/&#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training sophisticated machine learning (ML) models requires large datasets that are difficult or expensive to collect for many applications. If prior knowledge about system dynamics is available, mechanistic representations can be used to supplement real-world data. We present SimbaML (Simulation-Based ML), an open-source tool that unifies realistic synthetic dataset generation from ordinary differential equation-based models and the direct analysis and inclusion in ML pipelines. SimbaML conveniently enables investigating transfer learning from synthetic to real-world data, data augmentation, identifying needs for data collection, and benchmarking physics-informed ML approaches. SimbaML is available from https://pypi.org/project/simba-ml/.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#30340;&#26234;&#33021;&#30005;&#32593;&#21487;&#20877;&#29983;&#33021;&#28304;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;REDf&#65292;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#30340;&#33021;&#37327;&#38656;&#27714;&#39044;&#27979;&#65292;&#25913;&#21892;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#38598;&#25104;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20934;&#30830;&#24230;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.03997</link><description>&lt;p&gt;
REDf&#65306;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#30340;&#26234;&#33021;&#30005;&#32593;&#21487;&#20877;&#29983;&#33021;&#28304;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
REDf: A Renewable Energy Demand Forecasting Model for Smart Grids using Long Short Term Memory Network. (arXiv:2304.03997v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#30340;&#26234;&#33021;&#30005;&#32593;&#21487;&#20877;&#29983;&#33021;&#28304;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;REDf&#65292;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#30340;&#33021;&#37327;&#38656;&#27714;&#39044;&#27979;&#65292;&#25913;&#21892;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#38598;&#25104;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20934;&#30830;&#24230;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#19990;&#30028;&#21521;&#26356;&#21487;&#25345;&#32493;&#30340;&#33021;&#28304;&#26410;&#26469;&#21457;&#23637;&#65292;&#23558;&#21487;&#20877;&#29983;&#33021;&#28304;&#28304;&#32435;&#20837;&#30005;&#32593;&#30340;&#38598;&#25104;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#38388;&#27463;&#24615;&#20351;&#30005;&#32593;&#31649;&#29702;&#21644;&#30830;&#20445;&#31283;&#23450;&#30340;&#30005;&#21147;&#20379;&#24212;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#26234;&#33021;&#30005;&#32593;&#20013;&#30340;&#33021;&#37327;&#38656;&#27714;&#65292;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#20934;&#30830;&#30340;&#33021;&#37327;&#38656;&#27714;&#39044;&#27979;&#26469;&#25913;&#21892;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#38598;&#25104;&#12290;&#25105;&#20204;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#26469;&#25429;&#25417;&#33021;&#47071;&#38656;&#27714;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#27169;&#24335;&#21644;&#20381;&#36182;&#20851;&#31995;&#65292;&#36825;&#20123;&#32593;&#32476;&#29305;&#21035;&#36866;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#20102;&#22235;&#20010;&#21382;&#21490;&#33021;&#37327;&#38656;&#27714;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26469;&#33258;&#19981;&#21516;&#30340;&#33021;&#28304;&#20998;&#37197;&#20844;&#21496;&#65292;&#21253;&#25324;&#32654;&#22269;&#30005;&#21147;&#12289;Commonwealth Edison&#12289;Dayton Power and Light&#20197;&#21450;&#23486;&#22805;&#27861;&#23612;&#20122;-&#26032;&#27901;&#35199;-&#39532;&#37324;&#20848;&#20114;&#32852;&#32593;&#12290;&#35813;&#26041;&#27861;&#36824;&#23558;REDf&#27169;&#22411;&#19982;&#20854;&#20182;&#20004;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#22522;&#20934;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;REDf&#27169;&#22411;&#22312;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#12289;&#22343;&#26041;&#26681;&#35823;&#24046;&#21644;&#20915;&#23450;&#31995;&#25968;&#31561;&#20934;&#30830;&#24230;&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;REDf&#21487;&#20197;&#20316;&#20026;&#21487;&#20877;&#29983;&#33021;&#28304;&#38656;&#27714;&#39044;&#27979;&#30340;&#21487;&#38752;&#24037;&#20855;&#65292;&#24182;&#25552;&#39640;&#21487;&#20877;&#29983;&#33021;&#28304;&#32435;&#20837;&#26234;&#33021;&#30005;&#32593;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of renewable energy sources into the power grid is becoming increasingly important as the world moves towards a more sustainable energy future. However, the intermittent nature of renewable energy sources can make it challenging to manage the power grid and ensure a stable supply of electricity. In this paper, we propose a deep learning-based approach for predicting energy demand in a smart power grid, which can improve the integration of renewable energy sources by providing accurate predictions of energy demand. We use long short-term memory networks, which are well-suited for time series data, to capture complex patterns and dependencies in energy demand data. The proposed approach is evaluated using four datasets of historical energy demand data from different energy distribution companies including American Electric Power, Commonwealth Edison, Dayton Power and Light, and Pennsylvania-New Jersey-Maryland Interconnection. The proposed model is also compared with two 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#22270;&#35770;&#30340;&#35821;&#35328;&#21051;&#30011;&#20102;&#24046;&#20998;&#38544;&#31169;&#30340;&#20004;&#31181;&#24773;&#24418;&#19979;&#65292;&#32431;&#31929;&#21644;&#36817;&#20284;&#30340;&#23398;&#20064;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23450;&#20041;&#30683;&#30462;&#22270;$G$&#26469;&#25429;&#25417; $\mathcal{H}$ &#30340;&#32452;&#21512;&#32467;&#26500;&#65292;&#21457;&#29616;&#20998;&#25968;&#22242;&#25968;&#21644;&#22242;&#25968;&#26159;&#25551;&#36848;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#24615;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#31639;&#27861;&#23545;&#20854;&#36827;&#34892;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2304.03996</link><description>&lt;p&gt;
&#29992;&#22270;&#35770;&#32479;&#19968;&#21051;&#30011;&#24046;&#20998;&#38544;&#31169;&#21487;&#23398;&#20064;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Unified Characterization of Private Learnability via Graph Theory. (arXiv:2304.03996v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#22270;&#35770;&#30340;&#35821;&#35328;&#21051;&#30011;&#20102;&#24046;&#20998;&#38544;&#31169;&#30340;&#20004;&#31181;&#24773;&#24418;&#19979;&#65292;&#32431;&#31929;&#21644;&#36817;&#20284;&#30340;&#23398;&#20064;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23450;&#20041;&#30683;&#30462;&#22270;$G$&#26469;&#25429;&#25417; $\mathcal{H}$ &#30340;&#32452;&#21512;&#32467;&#26500;&#65292;&#21457;&#29616;&#20998;&#25968;&#22242;&#25968;&#21644;&#22242;&#25968;&#26159;&#25551;&#36848;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#24615;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#31639;&#27861;&#23545;&#20854;&#36827;&#34892;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#21051;&#30011;&#32431;&#31929;&#30340;&#21644;&#36817;&#20284;&#30340;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#24615;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#20102;&#22270;&#35770;&#30340;&#35821;&#35328;:&#23545;&#20110;&#19968;&#20010;&#27010;&#24565;&#31867; $\mathcal{H}$,&#25105;&#20204;&#23450;&#20041;&#20102; $\mathcal{H}$ &#30340;&#30683;&#30462;&#22270; $G$&#12290;&#23427;&#30340;&#39030;&#28857;&#26159;&#21487;&#23454;&#29616;&#30340;&#25968;&#25454;&#38598;&#65292;&#22914;&#26524;&#20004;&#20010;&#25968;&#25454;&#38598; $S$&#65292;$S'$ &#30456;&#20114;&#30683;&#30462;(&#21363;&#65292;&#22312; $S$ &#21644; $S'$ &#20013;&#26377;&#19968;&#20010;&#28857; $x$ &#20855;&#26377;&#19981;&#21516;&#30340;&#26631;&#35760;)&#65292;&#21017;&#23427;&#20204;&#20043;&#38388;&#26377;&#19968;&#26465;&#36793;&#36830;&#25509;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65292;$G$ &#30340;&#32452;&#21512;&#32467;&#26500;&#19982;&#22312;&#24046;&#20998;&#38544;&#31169;&#19979;&#23398;&#20064; $\mathcal{H}$ &#23494;&#20999;&#30456;&#20851;&#12290;&#22312;&#32431;&#31929;&#30340;&#24046;&#20998;&#38544;&#31169;&#19979;&#23398;&#20064; $\mathcal{H}$ &#30340;&#25429;&#33719;&#20026; $G$ &#30340;&#20998;&#25968;&#22242;&#25968;&#12290;&#22312;&#36817;&#20284;&#24046;&#20998;&#38544;&#31169;&#19979;&#23398;&#20064; $\mathcal{H}$ &#30340;&#25429;&#33719;&#20026; $G$ &#30340;&#22242;&#25968;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#25551;&#36848;&#24046;&#20998;&#38544;&#31169;&#21487;&#23398;&#20064;&#24615;&#30340;&#22270;&#35770;&#32500;&#24230;&#65306;&#22242;&#32500;&#21644;&#20998;&#25968;&#22242;&#32500;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#30683;&#30462;&#22270;&#30340;&#19968;&#20123;&#24615;&#36136;&#65292;&#36825;&#20123;&#24615;&#36136;&#21487;&#33021;&#26159;&#29420;&#31435;&#24863;&#20852;&#36259;&#30340;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20960;&#31181;&#31639;&#27861;&#26469;&#20272;&#35745; $G$ &#30340;&#36825;&#20123;&#24230;&#37327;&#65292;&#36890;&#36807;&#36825;&#20123;&#31639;&#27861;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20960;&#31181;&#27010;&#24565;&#31867;&#30340;&#23454;&#39564;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a unified framework for characterizing pure and approximate differentially private (DP) learnabiliity. The framework uses the language of graph theory: for a concept class $\mathcal{H}$, we define the contradiction graph $G$ of $\mathcal{H}$. It vertices are realizable datasets, and two datasets $S,S'$ are connected by an edge if they contradict each other (i.e., there is a point $x$ that is labeled differently in $S$ and $S'$). Our main finding is that the combinatorial structure of $G$ is deeply related to learning $\mathcal{H}$ under DP. Learning $\mathcal{H}$ under pure DP is captured by the fractional clique number of $G$. Learning $\mathcal{H}$ under approximate DP is captured by the clique number of $G$. Consequently, we identify graph-theoretic dimensions that characterize DP learnability: the clique dimension and fractional clique dimension. Along the way, we reveal properties of the contradiction graph which may be of independent interest. We also suggest several o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#40657;&#30418;&#20248;&#21270;&#21457;&#29616;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#36951;&#20256;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;&#22810;&#20010;&#20248;&#21270;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#36890;&#24120;&#36873;&#25321;&#23569;&#37327;&#39640;&#24230;&#36866;&#24212;&#30340;&#21464;&#20307;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#30340;&#27880;&#24847;&#26426;&#21046;&#26469;&#36866;&#24212;&#20854;&#25628;&#32034;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2304.03995</link><description>&lt;p&gt;
&#36890;&#36807;&#20803;&#40657;&#30418;&#20248;&#21270;&#21457;&#29616;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#36951;&#20256;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Discovering Attention-Based Genetic Algorithms via Meta-Black-Box Optimization. (arXiv:2304.03995v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#40657;&#30418;&#20248;&#21270;&#21457;&#29616;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#36951;&#20256;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;&#22810;&#20010;&#20248;&#21270;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#36890;&#24120;&#36873;&#25321;&#23569;&#37327;&#39640;&#24230;&#36866;&#24212;&#30340;&#21464;&#20307;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#30340;&#27880;&#24847;&#26426;&#21046;&#26469;&#36866;&#24212;&#20854;&#25628;&#32034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36951;&#20256;&#31639;&#27861;&#26159;&#19968;&#31867;&#40657;&#30418;&#20248;&#21270;&#31639;&#27861;&#65292;&#21463;&#21040;&#29983;&#29289;&#36827;&#21270;&#21407;&#29702;&#30340;&#21551;&#21457;&#32780;&#26469;&#12290;&#34429;&#28982;&#23427;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#20248;&#21270;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#30340;&#29305;&#23450;&#23454;&#20363;&#21487;&#33021;&#26159;&#21551;&#21457;&#24335;&#30340;&#65292;&#24182;&#21463;&#21040;&#26494;&#25955;&#30340;&#29983;&#29289;&#30452;&#35273;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#26681;&#26412;&#19981;&#21516;&#30340;&#26041;&#27861;&#65306;&#22312;&#32473;&#23450;&#36275;&#22815;&#28789;&#27963;&#30340;&#36951;&#20256;&#31639;&#23376;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#21457;&#29616;&#20840;&#26032;&#30340;&#36951;&#20256;&#31639;&#27861;&#12290;&#26356;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23558;&#36873;&#25321;&#21644;&#21464;&#24322;&#29575;&#33258;&#36866;&#24212;&#21442;&#25968;&#21270;&#20026;&#20132;&#21449;&#21644;&#33258;&#27880;&#24847;&#27169;&#22359;&#65292;&#24182;&#20351;&#29992;&#20803;&#40657;&#30418;&#20248;&#21270;&#26469;&#28436;&#21270;&#23427;&#20204;&#22312;&#19968;&#32452;&#19981;&#21516;&#30340;&#20248;&#21270;&#20219;&#21153;&#19978;&#30340;&#21442;&#25968;&#12290;&#25152;&#24471;&#21040;&#30340;&#23398;&#20064;&#36951;&#20256;&#31639;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#33258;&#36866;&#24212;&#22522;&#32447;&#36951;&#20256;&#31639;&#27861;&#65292;&#24182;&#19988;&#36828;&#36828;&#36229;&#20986;&#20102;&#20803;&#35757;&#32451;&#35774;&#32622;&#12290;&#23398;&#20064;&#21040;&#30340;&#31639;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#20808;&#21069;&#26410;&#35265;&#36807;&#30340;&#20248;&#21270;&#38382;&#39064;&#12289;&#25628;&#32034;&#32500;&#24230;&#21644;&#35780;&#20272;&#39044;&#31639;&#12290;&#25105;&#20204;&#27880;&#37322;&#20854;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#23637;&#31034;&#23427;&#36890;&#24120;&#36873;&#25321;&#23569;&#37327;&#39640;&#24230;&#36866;&#24212;&#30340;&#21464;&#20307;&#65292;&#24182;&#36890;&#36807;&#20854;&#23398;&#20064;&#30340;&#27880;&#24847;&#26426;&#21046;&#26469;&#36866;&#24212;&#20854;&#25628;&#32034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Genetic algorithms constitute a family of black-box optimization algorithms, which take inspiration from the principles of biological evolution. While they provide a general-purpose tool for optimization, their particular instantiations can be heuristic and motivated by loose biological intuition. In this work we explore a fundamentally different approach: Given a sufficiently flexible parametrization of the genetic operators, we discover entirely new genetic algorithms in a data-driven fashion. More specifically, we parametrize selection and mutation rate adaptation as cross- and self-attention modules and use Meta-Black-Box-Optimization to evolve their parameters on a set of diverse optimization tasks. The resulting Learned Genetic Algorithm outperforms state-of-the-art adaptive baseline genetic algorithms and generalizes far beyond its meta-training settings. The learned algorithm can be applied to previously unseen optimization problems, search dimensions &amp; evaluation budgets. We c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22359;&#27491;&#21017;&#21270;5&#215;2&#20132;&#21449;&#39564;&#35777;McNemar&#26816;&#39564;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35268;&#33539;&#21270;&#35757;&#32451;&#38598;&#20043;&#38388;&#30340;&#37325;&#21472;&#35760;&#24405;&#26469;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#35823;&#24046;&#29575;&#20272;&#35745;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#30041;&#32622;&#26041;&#27861;&#26377;&#26356;&#39640;&#30340;&#21151;&#29575;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03990</link><description>&lt;p&gt;
&#27604;&#36739;&#20004;&#31181;&#20998;&#31867;&#31639;&#27861;&#30340;&#22359;&#27491;&#21017;&#21270;5&#215;2&#20132;&#21449;&#39564;&#35777;McNemar&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Block-regularized 5$\times$2 Cross-validated McNemar's Test for Comparing Two Classification Algorithms. (arXiv:2304.03990v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03990
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22359;&#27491;&#21017;&#21270;5&#215;2&#20132;&#21449;&#39564;&#35777;McNemar&#26816;&#39564;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35268;&#33539;&#21270;&#35757;&#32451;&#38598;&#20043;&#38388;&#30340;&#37325;&#21472;&#35760;&#24405;&#26469;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#35823;&#24046;&#29575;&#20272;&#35745;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#30041;&#32622;&#26041;&#27861;&#26377;&#26356;&#39640;&#30340;&#21151;&#29575;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27604;&#36739;&#20004;&#31181;&#20998;&#31867;&#31639;&#27861;&#30340;&#20219;&#21153;&#20013;&#65292;&#24191;&#27867;&#20351;&#29992;&#30340;McNemar&#26816;&#39564;&#26088;&#22312;&#25512;&#26029;&#20986;&#20004;&#31181;&#20998;&#31867;&#31639;&#27861;&#30340;&#38169;&#35823;&#29575;&#20043;&#38388;&#23384;&#22312;&#37325;&#22823;&#24046;&#24322;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;McNemar&#26816;&#39564;&#30340;&#21151;&#29575;&#36890;&#24120;&#19981;&#22826;&#29702;&#24819;&#65292;&#22240;&#20026;&#27979;&#35797;&#20013;&#30340;&#30041;&#32622;&#65288;HO&#65289;&#26041;&#27861;&#20165;&#20351;&#29992;&#19968;&#27425;&#35757;&#32451;&#39564;&#35777;&#25286;&#20998;&#65292;&#36825;&#36890;&#24120;&#20250;&#20135;&#29983;&#39640;&#24230;&#21464;&#21270;&#30340;&#38169;&#35823;&#29575;&#20272;&#35745;&#12290;&#30456;&#21453;&#65292;&#20132;&#21449;&#39564;&#35777;&#65288;CV&#65289;&#26041;&#27861;&#22312;&#22810;&#27425;&#37325;&#22797;HO&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#20135;&#29983;&#31283;&#23450;&#30340;&#20272;&#35745;&#65292;&#22240;&#27492;CV&#26041;&#27861;&#22312;&#25552;&#39640;McNemar&#26816;&#39564;&#21151;&#29575;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#20248;&#21183;&#12290;&#22312;&#25152;&#26377;&#31867;&#22411;&#30340;CV&#26041;&#27861;&#20013;&#65292;&#22359;&#27491;&#21017;&#21270;5&#215;2 CV&#65288;BCV&#65289;&#22312;&#35768;&#22810;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#24050;&#32463;&#26174;&#31034;&#20986;&#27604;&#20854;&#20182;CV&#26041;&#27861;&#22312;&#31639;&#27861;&#27604;&#36739;&#20219;&#21153;&#20013;&#26356;&#20026;&#20248;&#36234;&#65292;&#22240;&#20026;5&#215;2 BCV&#21487;&#20197;&#36890;&#36807;&#20351;&#25152;&#26377;&#35757;&#32451;&#38598;&#20043;&#38388;&#30340;&#37325;&#21472;&#35760;&#24405;&#25968;&#35268;&#33539;&#21270;&#26469;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#35823;&#24046;&#29575;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the task of comparing two classification algorithms, the widely-used McNemar's test aims to infer the presence of a significant difference between the error rates of the two classification algorithms. However, the power of the conventional McNemar's test is usually unpromising because the hold-out (HO) method in the test merely uses a single train-validation split that usually produces a highly varied estimation of the error rates. In contrast, a cross-validation (CV) method repeats the HO method in multiple times and produces a stable estimation. Therefore, a CV method has a great advantage to improve the power of McNemar's test. Among all types of CV methods, a block-regularized 5$\times$2 CV (BCV) has been shown in many previous studies to be superior to the other CV methods in the comparison task of algorithms because the 5$\times$2 BCV can produce a high-quality estimator of the error rate by regularizing the numbers of overlapping records between all training sets. In this stu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#24320;&#25918;&#38598;(UIOS)&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#35757;&#32451;&#20013;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#26679;&#26412;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#35745;&#31639;&#19981;&#30830;&#23450;&#24615;&#24471;&#20998;&#26469;&#34920;&#36798;&#20854;&#32622;&#20449;&#24230;&#65292;&#24182;&#22312;&#22810;&#20010;&#27979;&#35797;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#20026;&#30495;&#23454;&#19990;&#30028;&#30340;&#35270;&#32593;&#33180;&#24322;&#24120;&#31579;&#26597;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.03981</link><description>&lt;p&gt;
&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#24320;&#25918;&#38598;&#23398;&#20064;&#29992;&#20110;&#35270;&#32593;&#33180;&#24322;&#24120;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-inspired Open Set Learning for Retinal Anomaly Identification. (arXiv:2304.03981v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03981
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#24320;&#25918;&#38598;(UIOS)&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#35757;&#32451;&#20013;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#26679;&#26412;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#35745;&#31639;&#19981;&#30830;&#23450;&#24615;&#24471;&#20998;&#26469;&#34920;&#36798;&#20854;&#32622;&#20449;&#24230;&#65292;&#24182;&#22312;&#22810;&#20010;&#27979;&#35797;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#20026;&#30495;&#23454;&#19990;&#30028;&#30340;&#35270;&#32593;&#33180;&#24322;&#24120;&#31579;&#26597;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#23454;&#38469;&#35270;&#32593;&#33180;&#24322;&#24120;&#20998;&#31867;&#24212;&#29992;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#26159;&#26080;&#27861;&#35782;&#21035;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#26679;&#26412;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#24320;&#25918;&#38598;(UIOS)&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#20102;9&#20010;&#24120;&#35265;&#35270;&#32593;&#33180;&#30149;&#21464;&#30340;&#30524;&#24213;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#12290;&#38500;&#20102;&#27599;&#20010;&#31867;&#21035;&#30340;&#27010;&#29575;&#65292;UIOS&#36824;&#35745;&#31639;&#20102;&#19981;&#30830;&#23450;&#24615;&#24471;&#20998;&#26469;&#34920;&#36798;&#20854;&#32622;&#20449;&#24230;&#12290;&#25105;&#20204;&#30340;UIOS&#27169;&#22411;&#36890;&#36807;&#35774;&#32622;&#38408;&#20540;&#31574;&#30053;&#65292;&#22312;&#20869;&#37096;&#27979;&#35797;&#25968;&#25454;&#38598;&#12289;&#22806;&#37096;&#27979;&#35797;&#25968;&#25454;&#38598;&#21644;&#38750;&#20856;&#22411;&#27979;&#35797;&#25968;&#25454;&#38598;&#20013;&#30340;F1&#20998;&#21035;&#36798;&#21040;&#20102;99.55&#65285;&#12289;97.01&#65285;&#21644;91.91&#65285;&#65292;&#30456;&#27604;&#26631;&#20934;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;F1&#20998;&#21035;&#20026;92.20&#65285;&#12289;80.69&#65285;&#21644;64.74&#65285;&#12290;&#27492;&#22806;&#65292;UIOS&#27491;&#30830;&#39044;&#27979;&#20102;&#32597;&#35265;&#35270;&#32593;&#33180;&#30142;&#30149;&#12289;&#20302;&#36136;&#37327;&#30524;&#24213;&#22270;&#20687;&#21644;&#38750;&#30524;&#24213;&#22270;&#20687;&#31561;&#25968;&#25454;&#38598;&#20013;&#30340;&#39640;&#19981;&#30830;&#23450;&#24615;&#20998;&#25968;&#65292;&#25552;&#31034;&#38656;&#35201;&#25163;&#21160;&#26816;&#26597;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#30495;&#23454;&#19990;&#30028;&#30340;&#35270;&#32593;&#33180;&#24322;&#24120;&#31579;&#26597;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Failure to recognize samples from the classes unseen during training is a major limit of artificial intelligence (AI) in real-world implementation of retinal anomaly classification. To resolve this obstacle, we propose an uncertainty-inspired open-set (UIOS) model which was trained with fundus images of 9 common retinal conditions. Besides the probability of each category, UIOS also calculates an uncertainty score to express its confidence. Our UIOS model with thresholding strategy achieved an F1 score of 99.55%, 97.01% and 91.91% for the internal testing set, external testing set and non-typical testing set, respectively, compared to the F1 score of 92.20%, 80.69% and 64.74% by the standard AI model. Furthermore, UIOS correctly predicted high uncertainty scores, which prompted the need for a manual check, in the datasets of rare retinal diseases, low-quality fundus images, and non-fundus images. This work provides a robust method for real-world screening of retinal anomalies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;CapsNet&#22312;&#20223;&#23556;&#21464;&#25442;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;MNIST&#65292;GTSRB&#21644;CIFAR10&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#20256;&#32479;CNN&#30456;&#27604;&#65292;CapsNet&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#23545;&#25239;&#31034;&#20363;&#21644;&#20223;&#23556;&#21464;&#25442;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03973</link><description>&lt;p&gt;
RobCaps: &#35780;&#20272;&#33014;&#22218;&#32593;&#32476;&#22312;&#20223;&#23556;&#21464;&#25442;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#20013;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
RobCaps: Evaluating the Robustness of Capsule Networks against Affine Transformations and Adversarial Attacks. (arXiv:2304.03973v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;CapsNet&#22312;&#20223;&#23556;&#21464;&#25442;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;MNIST&#65292;GTSRB&#21644;CIFAR10&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#20256;&#32479;CNN&#30456;&#27604;&#65292;CapsNet&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#23545;&#25239;&#31034;&#20363;&#21644;&#20223;&#23556;&#21464;&#25442;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33014;&#22218;&#32593;&#32476;(CapsNet)&#33021;&#22815;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#20998;&#23618;&#20445;&#25345;&#22810;&#20010;&#23545;&#35937;&#20043;&#38388;&#30340;&#23039;&#24577;&#20851;&#31995;&#12290;&#38500;&#20102;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#22806;&#65292;&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#24212;&#29992;&#20013;&#37096;&#32626;CapsNet&#30340;&#21478;&#19968;&#20010;&#30456;&#20851;&#22240;&#32032;&#26159;&#20854;&#23545;&#36755;&#20837;&#21464;&#25442;&#21644;&#24694;&#24847;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#20998;&#26512;&#21644;&#35780;&#20272;&#20102;&#24433;&#21709;CapsNet&#40065;&#26834;&#24615;&#30340;&#19981;&#21516;&#22240;&#32032;&#65292;&#19982;&#20256;&#32479;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20026;&#20102;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#65292;&#25105;&#20204;&#22312;MNIST&#65292;GTSRB&#21644;CIFAR10&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#20004;&#20010;CapsNet&#27169;&#22411;&#21644;&#20004;&#20010;CNN&#27169;&#22411;&#65292;&#20197;&#21450;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#20223;&#23556;&#21464;&#25442;&#29256;&#26412;&#12290;&#36890;&#36807;&#28145;&#20837;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#26550;&#26500;&#30340;&#21738;&#20123;&#29305;&#24615;&#26356;&#26377;&#21161;&#20110;&#22686;&#21152;&#20854;&#40065;&#26834;&#24615;&#20197;&#21450;&#20854;&#23616;&#38480;&#24615;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#19982;&#31867;&#20284;&#25968;&#37327;&#30340;&#20256;&#32479;CNN&#30456;&#27604;&#65292;CapsNet&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#23545;&#25239;&#31034;&#20363;&#21644;&#20223;&#23556;&#21464;&#25442;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Capsule Networks (CapsNets) are able to hierarchically preserve the pose relationships between multiple objects for image classification tasks. Other than achieving high accuracy, another relevant factor in deploying CapsNets in safety-critical applications is the robustness against input transformations and malicious adversarial attacks.  In this paper, we systematically analyze and evaluate different factors affecting the robustness of CapsNets, compared to traditional Convolutional Neural Networks (CNNs). Towards a comprehensive comparison, we test two CapsNet models and two CNN models on the MNIST, GTSRB, and CIFAR10 datasets, as well as on the affine-transformed versions of such datasets. With a thorough analysis, we show which properties of these architectures better contribute to increasing the robustness and their limitations. Overall, CapsNets achieve better robustness against adversarial examples and affine transformations, compared to a traditional CNN with a similar number 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;TabNet&#27169;&#22411;&#65292;&#32467;&#21512;&#22522;&#20110;&#26641;&#30340;&#31639;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#65292;&#39044;&#27979;&#20102;&#22374;&#26705;&#23612;&#20122;&#27700;&#27893;&#30340;&#32500;&#20462;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#34920;&#26126;TabNet&#22312;&#35757;&#32451;&#19981;&#24179;&#34913;&#25968;&#25454;&#26102;&#24615;&#33021;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.03969</link><description>&lt;p&gt;
&#29992;&#27880;&#24847;&#21147;&#34920;&#26684;&#23398;&#20064;&#39044;&#27979;&#27700;&#27893;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
Pump It Up: Predict Water Pump Status using Attentive Tabular Learning. (arXiv:2304.03969v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;TabNet&#27169;&#22411;&#65292;&#32467;&#21512;&#22522;&#20110;&#26641;&#30340;&#31639;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#65292;&#39044;&#27979;&#20102;&#22374;&#26705;&#23612;&#20122;&#27700;&#27893;&#30340;&#32500;&#20462;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#34920;&#26126;TabNet&#22312;&#35757;&#32451;&#19981;&#24179;&#34913;&#25968;&#25454;&#26102;&#24615;&#33021;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#21361;&#26426;&#26159;&#20840;&#29699;&#20851;&#27880;&#30340;&#28966;&#28857;&#38382;&#39064;&#12290;&#22312;&#24178;&#26097;&#22269;&#23478;&#65292;&#21450;&#26102;&#32500;&#25252;&#27700;&#27893;&#23545;&#20110;&#20381;&#38752;&#20117;&#27700;&#30340;&#31038;&#21306;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#21033;&#29992;&#24207;&#21015;&#21270;&#30340;&#27880;&#24847;&#21147;&#28145;&#24230;&#31070;&#32463;&#32467;&#26500;TabNet&#65292;&#20998;&#26512;&#39044;&#27979;&#22374;&#26705;&#23612;&#20122;&#27700;&#27893;&#32500;&#20462;&#29366;&#24577;&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#22522;&#20110;&#26641;&#30340;&#31639;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#28857;&#65292;&#23454;&#29616;&#20102;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#65292;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#65292;&#31232;&#30095;&#29305;&#24449;&#36873;&#25321;&#20197;&#21450;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#39640;&#25928;&#23398;&#20064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;TabNet&#30340;&#34920;&#29616;&#19982;&#20687;XGBoost&#12289;LightGBM&#12289;CatBoost&#31561;&#27969;&#34892;&#30340;&#26799;&#24230;&#26641;&#25552;&#21319;&#31639;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#35757;&#32451;&#19981;&#24179;&#34913;&#25968;&#25454;&#26102;&#65292;&#36890;&#36807;&#36873;&#25321;&#32858;&#28966;&#25439;&#22833;&#65288;focal loss&#65289;&#20316;&#20026;&#30446;&#26631;&#20989;&#25968;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Water crisis is a crucial concern around the globe. Appropriate and timely maintenance of water pumps in drought-hit countries is vital for communities relying on the well. In this paper, we analyze and apply a sequential attentive deep neural architecture, TabNet, for predicting water pump repair status in Tanzania. The model combines the valuable benefits of tree-based algorithms and neural networks, enabling end-to-end training, model interpretability, sparse feature selection, and efficient learning on tabular data. Finally, we compare the performance of TabNet with popular gradient tree-boosting algorithms like XGBoost, LightGBM,CatBoost, and demonstrate how we can further uplift the performance by choosing focal loss as the objective function while training on imbalanced data.
&lt;/p&gt;</description></item><item><title>&#37327;&#21270;&#27169;&#22411;&#22312;&#21463;&#21040;&#21508;&#31181;&#22122;&#22768;&#30340;&#24433;&#21709;&#26102;&#34920;&#29616;&#20986;&#33030;&#24369;&#24615;&#65292;&#36739;&#20302;&#20301;&#30340;&#37327;&#21270;&#23545;&#25239;&#25915;&#20987;&#26356;&#20855;&#24377;&#24615;&#65292;&#20294;&#26356;&#23481;&#26131;&#21463;&#21040;&#33258;&#28982;&#25200;&#21160;&#21644;&#31995;&#32479;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.03968</link><description>&lt;p&gt;
&#37327;&#21270;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking the Robustness of Quantized Models. (arXiv:2304.03968v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03968
&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#27169;&#22411;&#22312;&#21463;&#21040;&#21508;&#31181;&#22122;&#22768;&#30340;&#24433;&#21709;&#26102;&#34920;&#29616;&#20986;&#33030;&#24369;&#24615;&#65292;&#36739;&#20302;&#20301;&#30340;&#37327;&#21270;&#23545;&#25239;&#25915;&#20987;&#26356;&#20855;&#24377;&#24615;&#65292;&#20294;&#26356;&#23481;&#26131;&#21463;&#21040;&#33258;&#28982;&#25200;&#21160;&#21644;&#31995;&#32479;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#24050;&#32463;&#25104;&#20026;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#35774;&#22791;&#19978;&#37096;&#32626;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#65292;&#37327;&#21270;&#27169;&#22411;&#22312;&#21463;&#21040;&#21508;&#31181;&#22122;&#22768;&#30340;&#24433;&#21709;&#26102;&#34920;&#29616;&#20986;&#33030;&#24369;&#24615;&#12290;&#23613;&#31649;&#35780;&#20272;&#37327;&#21270;&#23545;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#24456;&#37325;&#35201;&#65292;&#20294;&#26159;&#20851;&#20110;&#36825;&#20010;&#20027;&#39064;&#30340;&#29616;&#26377;&#30740;&#31350;&#26377;&#38480;&#19988;&#24120;&#24120;&#24573;&#30053;&#20102;&#24050;&#32463;&#24314;&#31435;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#21407;&#21017;&#65292;&#23548;&#33268;&#20102;&#19981;&#23436;&#25972;&#21644;&#26080;&#27861;&#19979;&#32467;&#35770;&#30340;&#30740;&#31350;&#32467;&#26524;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#22312;ImageNet&#19978;&#20805;&#20998;&#35780;&#20272;&#20102;&#37327;&#21270;&#27169;&#22411;&#23545;&#21508;&#31181;&#22122;&#22768;(&#23545;&#25239;&#25915;&#20987;&#12289;&#33258;&#28982;&#25200;&#21160;&#21644;&#31995;&#32479;&#22122;&#22768;)&#30340;&#40065;&#26834;&#24615;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36739;&#20302;&#20301;&#30340;&#37327;&#21270;&#23545;&#25239;&#25915;&#20987;&#26356;&#20855;&#24377;&#24615;&#65292;&#20294;&#26356;&#23481;&#26131;&#21463;&#21040;&#33258;&#28982;&#25200;&#21160;&#21644;&#31995;&#32479;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#33033;&#20914;&#22122;&#22768;(&#22312;&#33258;&#28982;&#25200;&#21160;&#20013;)&#21644;&#26368;&#36817;&#37051;&#25554;&#20540;(&#22312;&#31995;&#32479;&#22122;&#22768;&#20013;)&#23545;&#37327;&#21270;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#24433;&#21709;&#26368;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization has emerged as an essential technique for deploying deep neural networks (DNNs) on devices with limited resources. However, quantized models exhibit vulnerabilities when exposed to various noises in real-world applications. Despite the importance of evaluating the impact of quantization on robustness, existing research on this topic is limited and often disregards established principles of robustness evaluation, resulting in incomplete and inconclusive findings. To address this gap, we thoroughly evaluated the robustness of quantized models against various noises (adversarial attacks, natural corruptions, and systematic noises) on ImageNet. Extensive experiments demonstrate that lower-bit quantization is more resilient to adversarial attacks but is more susceptible to natural corruptions and systematic noises. Notably, our investigation reveals that impulse noise (in natural corruptions) and the nearest neighbor interpolation (in systematic noises) have the most significan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FlexMoE&#30340;DNN&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#21160;&#24577;&#25968;&#25454;&#27969;&#31995;&#32479;&#24615;&#12289;&#36879;&#26126;&#22320;&#35299;&#20915;&#21160;&#24577;&#25968;&#25454;&#27969;&#24102;&#26469;&#30340;&#20302;&#25928;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31232;&#30095;&#38376;&#25511;&#19987;&#23478;&#28151;&#21512;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.03946</link><description>&lt;p&gt;
FlexMoE&#65306;&#36890;&#36807;&#21160;&#24577;&#35774;&#22791;&#37197;&#32622;&#25193;&#23637;&#22823;&#35268;&#27169;&#31232;&#30095;&#39044;&#35757;&#32451;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
FlexMoE: Scaling Large-scale Sparse Pre-trained Model Training via Dynamic Device Placement. (arXiv:2304.03946v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FlexMoE&#30340;DNN&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#21160;&#24577;&#25968;&#25454;&#27969;&#31995;&#32479;&#24615;&#12289;&#36879;&#26126;&#22320;&#35299;&#20915;&#21160;&#24577;&#25968;&#25454;&#27969;&#24102;&#26469;&#30340;&#20302;&#25928;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31232;&#30095;&#38376;&#25511;&#19987;&#23478;&#28151;&#21512;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#37327;&#30340;&#22686;&#21152;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#23558;&#30693;&#35782;&#23384;&#20648;&#21040;&#22823;&#37327;&#27169;&#22411;&#21442;&#25968;&#20013;&#25104;&#20026;&#19968;&#31181;&#36235;&#21183;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#35757;&#32451;&#30001;&#22823;&#37327;&#30340;&#23494;&#38598;&#20195;&#25968;&#25805;&#20316;&#32452;&#25104;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#30828;&#20214;&#36164;&#28304;&#12290;&#26368;&#36817;&#65292;&#31232;&#30095;&#38376;&#25511;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#24182;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#21487;&#35266;&#30340;&#39044;&#35757;&#32451;&#21487;&#25193;&#23637;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36335;&#30001;&#19981;&#24179;&#34913;&#21644;&#27874;&#21160;&#38382;&#39064;&#65292;&#36825;&#31181;&#31232;&#30095;&#26465;&#20214;&#35745;&#31639;&#22312;&#23454;&#38469;&#31995;&#32479;&#20013;&#21487;&#33021;&#24182;&#19981;&#22914;&#39044;&#26399;&#37027;&#20040;&#26377;&#25928;&#12290;&#36890;&#24120;&#65292;MoE&#27491;&#22312;&#25104;&#20026;&#25968;&#25454;&#29983;&#21629;&#21608;&#26399;&#20013;&#30340;&#26032;&#30340;&#25968;&#25454;&#20998;&#26512;&#33539;&#20363;&#65292;&#24182;&#38754;&#20020;&#30528;&#20197;&#21069;&#20174;&#26410;&#26377;&#36807;&#30340;&#35268;&#27169;&#12289;&#22797;&#26434;&#24615;&#21644;&#39063;&#31890;&#24230;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing data volume, there is a trend of using large-scale pre-trained models to store the knowledge into an enormous number of model parameters. The training of these models is composed of lots of dense algebras, requiring a huge amount of hardware resources. Recently, sparsely-gated Mixture-of-Experts (MoEs) are becoming more popular and have demonstrated impressive pretraining scalability in various downstream tasks. However, such a sparse conditional computation may not be effective as expected in practical systems due to the routing imbalance and fluctuation problems. Generally, MoEs are becoming a new data analytics paradigm in the data life cycle and suffering from unique challenges at scales, complexities, and granularities never before possible.  In this paper, we propose a novel DNN training framework, FlexMoE, which systematically and transparently address the inefficiency caused by dynamic dataflow. We first present an empirical analysis on the problems and oppo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;NGFKT&#65292;&#36890;&#36807;&#26657;&#20934;&#25216;&#33021;&#20851;&#31995;&#30697;&#38453;&#21644;Q&#30697;&#38453;&#38477;&#20302;&#20027;&#35266;&#26631;&#35760;&#30340;&#24433;&#21709;&#65292;&#24182;&#24341;&#20837;&#30693;&#35782;&#20851;&#31995;&#25490;&#24207;&#26426;&#21046;&#26469;&#27169;&#25311;&#19981;&#21516;&#20851;&#31995;&#30340;&#37325;&#35201;&#24615;&#65292;&#24212;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#24314;&#27169;&#19981;&#21516;&#31867;&#22411;&#33410;&#28857;&#20043;&#38388;&#30340;&#24322;&#26500;&#20132;&#20114;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#36827;&#34892;&#22797;&#26434;&#12289;&#21160;&#24577;&#30340;&#30693;&#35782;&#36861;&#36394;&#12290;</title><link>http://arxiv.org/abs/2304.03945</link><description>&lt;p&gt;
&#30693;&#35782;&#20851;&#31995;&#25490;&#24207;&#22686;&#24378;&#24322;&#26500;&#23398;&#20064;&#20132;&#20114;&#27169;&#22411;&#29992;&#20110;&#31070;&#32463;&#22270;&#36951;&#24536;&#30693;&#35782;&#36861;&#36394;
&lt;/p&gt;
&lt;p&gt;
Knowledge Relation Rank Enhanced Heterogeneous Learning Interaction Modeling for Neural Graph Forgetting Knowledge Tracing. (arXiv:2304.03945v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03945
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;NGFKT&#65292;&#36890;&#36807;&#26657;&#20934;&#25216;&#33021;&#20851;&#31995;&#30697;&#38453;&#21644;Q&#30697;&#38453;&#38477;&#20302;&#20027;&#35266;&#26631;&#35760;&#30340;&#24433;&#21709;&#65292;&#24182;&#24341;&#20837;&#30693;&#35782;&#20851;&#31995;&#25490;&#24207;&#26426;&#21046;&#26469;&#27169;&#25311;&#19981;&#21516;&#20851;&#31995;&#30340;&#37325;&#35201;&#24615;&#65292;&#24212;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#24314;&#27169;&#19981;&#21516;&#31867;&#22411;&#33410;&#28857;&#20043;&#38388;&#30340;&#24322;&#26500;&#20132;&#20114;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#36827;&#34892;&#22797;&#26434;&#12289;&#21160;&#24577;&#30340;&#30693;&#35782;&#36861;&#36394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#24050;&#32463;&#24212;&#29992;&#20110;&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;&#20013;&#65292;&#20363;&#22914;&#33258;&#27880;&#24847;&#21147;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#65288;SAKT&#65289;&#65292;&#23427;&#24314;&#27169;&#20102;&#32451;&#20064;&#21644;&#30693;&#35782;&#27010;&#24565;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#20013;&#30340;&#20851;&#31995;&#24314;&#27169;&#21482;&#32771;&#34385;&#38745;&#24577;&#30340;&#38382;&#39064;-&#30693;&#35782;&#20851;&#31995;&#21644;&#30693;&#35782;-&#30693;&#35782;&#20851;&#31995;&#65292;&#24182;&#23558;&#36825;&#20123;&#20851;&#31995;&#21516;&#31561;&#37325;&#35270;&#12290;&#36825;&#31181;&#20851;&#31995;&#24314;&#27169;&#38590;&#20197;&#36991;&#20813;&#20027;&#35266;&#26631;&#35760;&#30340;&#24433;&#21709;&#65292;&#24182;&#21333;&#29420;&#32771;&#34385;&#32451;&#20064;&#21644;KCs&#20043;&#38388;&#65292;&#25110;KCs&#21644;KCs&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#65292;&#21629;&#21517;&#20026;&#30693;&#35782;&#20851;&#31995;&#25490;&#24207;&#22686;&#24378;&#24322;&#26500;&#23398;&#20064;&#20132;&#20114;&#27169;&#22411;&#29992;&#20110;&#31070;&#32463;&#22270;&#36951;&#24536;&#30693;&#35782;&#36861;&#36394;&#65288;NGFKT&#65289;&#65292;&#36890;&#36807;&#26657;&#20934;&#25216;&#33021;&#20851;&#31995;&#30697;&#38453;&#21644;Q&#30697;&#38453;&#38477;&#20302;&#20027;&#35266;&#26631;&#35760;&#30340;&#24433;&#21709;&#65292;&#24182;&#24212;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#26469;&#24314;&#27169;&#19981;&#21516;&#31867;&#22411;&#33410;&#28857;&#20043;&#38388;&#30340;&#24322;&#26500;&#20132;&#20114;&#65292;&#21363;&#32451;&#20064;&#65292;Kcs&#21644;&#23398;&#29983;&#12290;NGFKT&#27169;&#22411;&#24341;&#20837;&#20102;&#30693;&#35782;&#20851;&#31995;&#25490;&#24207;&#26426;&#21046;&#26469;&#27169;&#25311;&#19981;&#21516;&#20851;&#31995;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#22797;&#26434;&#12289;&#21160;&#24577;&#30340;&#30693;&#35782;&#36861;&#36394;&#36807;&#31243;&#30340;&#24314;&#27169;&#12290;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;NGFKT&#27169;&#22411;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;AUC&#26041;&#38754;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, knowledge tracing models have been applied in educational data mining such as the Self-attention knowledge tracing model(SAKT), which models the relationship between exercises and Knowledge concepts(Kcs). However, relation modeling in traditional Knowledge tracing models only considers the static question-knowledge relationship and knowledge-knowledge relationship and treats these relationships with equal importance. This kind of relation modeling is difficult to avoid the influence of subjective labeling and considers the relationship between exercises and KCs, or KCs and KCs separately. In this work, a novel knowledge tracing model, named Knowledge Relation Rank Enhanced Heterogeneous Learning Interaction Modeling for Neural Graph Forgetting Knowledge Tracing(NGFKT), is proposed to reduce the impact of the subjective labeling by calibrating the skill relation matrix and the Q-matrix and apply the Graph Convolutional Network(GCN) to model the heterogeneous interactions betwe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;GAN&#21512;&#25104;&#32974;&#20799;&#33041;&#37096;&#36229;&#22768;&#22270;&#20687;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;GAN-based&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#31283;&#23450;&#12289;&#36924;&#30495;&#30340;&#36229;&#22768;&#22270;&#20687;&#65292;&#20026;&#21307;&#23398;AI&#21644;ML&#26041;&#27861;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.03941</link><description>&lt;p&gt;
&#23454;&#29616;&#36924;&#30495;&#30340;&#32974;&#20799;&#33041;&#37096;&#36229;&#22768;&#25104;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Towards Realistic Ultrasound Fetal Brain Imaging Synthesis. (arXiv:2304.03941v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;GAN&#21512;&#25104;&#32974;&#20799;&#33041;&#37096;&#36229;&#22768;&#22270;&#20687;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;GAN-based&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#31283;&#23450;&#12289;&#36924;&#30495;&#30340;&#36229;&#22768;&#22270;&#20687;&#65292;&#20026;&#21307;&#23398;AI&#21644;ML&#26041;&#27861;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20135;&#21069;&#36229;&#22768;&#25104;&#20687;&#26159;&#35780;&#20272;&#32974;&#20799;&#20581;&#24247;&#30340;&#39318;&#36873;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20020;&#24202;&#25968;&#25454;&#37327;&#19981;&#36275;&#12289;&#24739;&#32773;&#38544;&#31169;&#12289;&#30149;&#21464;&#32597;&#35265;&#21644;&#25968;&#25454;&#25910;&#38598;&#21644;&#39564;&#35777;&#30340;&#19987;&#23478;&#26377;&#38480;&#31561;&#21407;&#22240;&#65292;&#20844;&#20849;&#30340;&#36229;&#22768;&#32974;&#20799;&#25104;&#20687;&#25968;&#25454;&#38598;&#24456;&#23569;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#25968;&#25454;&#31232;&#32570;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#25193;&#25955;&#36229;&#20998;&#36776;&#29575;GAN&#21644;&#22522;&#20110;&#21464;&#24418;&#22120;&#30340;GAN&#65292;&#29992;&#26469;&#21512;&#25104;&#26469;&#33258;&#19968;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#32974;&#20799;&#36229;&#22768;&#33041;&#24179;&#38754;&#22270;&#20687;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;GAN-based&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;256x256&#20687;&#32032;&#22823;&#23567;&#30340;&#32974;&#20799;&#36229;&#22768;&#32463;&#23567;&#33041;&#33041;&#22270;&#20687;&#24179;&#38754;&#65292;&#24182;&#19988;&#35757;&#32451;&#31283;&#23450;&#25439;&#22833;&#65292;&#20854;&#20013;&#65292;&#25193;&#25955;&#36229;&#20998;&#36776;&#29575;GAN&#65288;&#24179;&#22343;7.04&#65292;FID&#20540;&#20302;&#20110;5.09 at epoch 10&#65289;&#30340;FID&#20540;&#20302;&#20110;&#21464;&#24418;&#22120;GAN&#65288;&#24179;&#22343;36.0&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prenatal ultrasound imaging is the first-choice modality to assess fetal health. Medical image datasets for AI and ML methods must be diverse (i.e. diagnoses, diseases, pathologies, scanners, demographics, etc), however there are few public ultrasound fetal imaging datasets due to insufficient amounts of clinical data, patient privacy, rare occurrence of abnormalities in general practice, and limited experts for data collection and validation. To address such data scarcity, we proposed generative adversarial networks (GAN)-based models, diffusion-super-resolution-GAN and transformer-based-GAN, to synthesise images of fetal ultrasound brain planes from one public dataset. We reported that GAN-based methods can generate 256x256 pixel size of fetal ultrasound trans-cerebellum brain image plane with stable training losses, resulting in lower FID values for diffusion-super-resolution-GAN (average 7.04 and lower FID 5.09 at epoch 10) than the FID values of transformer-based-GAN (average 36.0
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#27744;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#30690;&#37327;&#37327;&#21270;&#21387;&#32553;&#22768;&#23398;&#19978;&#30456;&#20284;&#30340;&#34920;&#31034;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#21644;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2304.03940</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#27744;&#21270;&#30340;&#30690;&#37327;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Speech Representation Pooling Using Vector Quantization. (arXiv:2304.03940v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03940
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#27744;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#30690;&#37327;&#37327;&#21270;&#21387;&#32553;&#22768;&#23398;&#19978;&#30456;&#20284;&#30340;&#34920;&#31034;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#27169;&#22411;&#29983;&#25104;&#36890;&#29992;&#35821;&#38899;&#34920;&#31034;&#65292;&#23558;&#19968;&#20010;&#27169;&#22411;&#24212;&#29992;&#21040;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#24050;&#25104;&#20026;&#19968;&#31181;&#20107;&#23454;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#27744;&#21270;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65307;&#35821;&#38899;&#34920;&#31034;&#30340;&#38271;&#24230;&#22266;&#26377;&#22320;&#26159;&#21487;&#21464;&#30340;&#12290;&#23613;&#31649;&#24573;&#30053;&#20102;&#35821;&#38899;&#30340;&#29305;&#24615;&#65292;&#20363;&#22914;&#19981;&#21516;&#38271;&#24230;&#30340;&#38899;&#32032;&#65292;&#20294;&#36890;&#24120;&#20351;&#29992;&#31616;&#21333;&#30340;&#24179;&#22343;&#27744;&#21270;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27744;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#30690;&#37327;&#37327;&#21270;&#26469;&#21387;&#32553;&#22768;&#23398;&#19978;&#30456;&#20284;&#30340;&#34920;&#31034;&#65292;&#19982;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27744;&#21270;&#26041;&#27861;&#19981;&#21516;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#26080;&#30417;&#30563;&#27744;&#21270;&#26041;&#27861;&#22312;&#21508;&#31181;&#33258;&#30417;&#30563;&#27169;&#22411;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#25955;&#33853;&#22312;&#35821;&#38899;&#21644;&#25991;&#26412;&#39046;&#22495;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#65306;&#20851;&#38190;&#23383;&#35782;&#21035;&#12289;&#35828;&#35805;&#20154;&#35782;&#21035;&#12289;&#24847;&#22270;&#20998;&#31867;&#21644;&#24773;&#24863;&#35782;&#21035;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#65292;&#23558;&#20854;&#19982;&#26377;&#30417;&#30563;&#30340;&#27744;&#21270;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of general-purpose speech representations from large-scale self-supervised models, applying a single model to multiple downstream tasks is becoming a de-facto approach. However, the pooling problem remains; the length of speech representations is inherently variable. The naive average pooling is often used, even though it ignores the characteristics of speech, such as differently lengthed phonemes. Hence, we design a novel pooling method to squash acoustically similar representations via vector quantization, which does not require additional training, unlike attention-based pooling. Further, we evaluate various unsupervised pooling methods on various self-supervised models. We gather diverse methods scattered around speech and text to evaluate on various tasks: keyword spotting, speaker identification, intent classification, and emotion recognition. Finally, we quantitatively and qualitatively analyze our method, comparing it with supervised pooling methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20844;&#24179;&#24615;&#24494;&#35843;&#31070;&#32463;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#24050;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#19981;&#20851;&#27880;&#20844;&#24179;&#24615;&#30340;&#25439;&#22833;&#24494;&#35843;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#21518;&#19968;&#23618;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#39640;&#27700;&#24179;&#30340;&#20844;&#24179;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#26631;&#20934;&#24615;&#33021;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2304.03935</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#21518;&#19968;&#23618;&#20844;&#24179;&#24494;&#35843;&#31616;&#21333;&#26377;&#25928;
&lt;/p&gt;
&lt;p&gt;
Last-Layer Fairness Fine-tuning is Simple and Effective for Neural Networks. (arXiv:2304.03935v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20844;&#24179;&#24615;&#24494;&#35843;&#31070;&#32463;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#24050;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#19981;&#20851;&#27880;&#20844;&#24179;&#24615;&#30340;&#25439;&#22833;&#24494;&#35843;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#21518;&#19968;&#23618;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#39640;&#27700;&#24179;&#30340;&#20844;&#24179;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#26631;&#20934;&#24615;&#33021;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#22312;&#29616;&#20195;&#25968;&#25454;&#31185;&#23398;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#31639;&#27861;&#20844;&#24179;&#24615;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#20851;&#27880;&#28857;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#22810;&#31181;&#20844;&#24179;&#24615;&#26631;&#20934;&#12290;&#20854;&#20013;&#65292;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#26045;&#21152;&#20844;&#24179;&#24615;&#32422;&#26463;&#65292;&#21363;&#36827;&#34892;&#22788;&#29702;&#20844;&#24179;&#24615;&#35757;&#32451;&#65292;&#24050;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#22240;&#20026;&#19982;&#21518;&#22788;&#29702;&#26041;&#27861;&#19981;&#21516;&#65292;&#23427;&#20204;&#19981;&#38656;&#35201;&#22312;&#27979;&#35797;&#26399;&#38388;&#35775;&#38382;&#25935;&#24863;&#23646;&#24615;&#12290;&#34429;&#28982;&#22312;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#23545;&#26045;&#21152;&#20844;&#24179;&#24615;&#32422;&#26463;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#36825;&#20123;&#25216;&#26415;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24433;&#21709;&#20173;&#19981;&#28165;&#26970;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#30446;&#26631;&#20989;&#25968;&#20013;&#28155;&#21152;&#20844;&#24179;&#24615;&#32422;&#26463;&#20250;&#23548;&#33268;&#22823;&#22411;&#27169;&#22411;&#36807;&#24230;&#25311;&#21512;&#20844;&#24179;&#24615;&#26631;&#20934;&#65292;&#22914;&#20309;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;&#26234;&#24935;&#21644;&#33021;&#21147;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26032;&#39062;&#30340;&#26694;&#26550;&#26469;&#35757;&#32451;&#20844;&#24179;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#22312;&#28385;&#36275;&#24863;&#20852;&#36259;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#19978;&#65292;&#20351;&#29992;&#19981;&#20851;&#27880;&#20844;&#24179;&#24615;&#30340;&#25439;&#22833;&#24494;&#35843;&#24050;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#39640;&#27700;&#24179;&#30340;&#20844;&#24179;&#24615;&#65292;&#32780;&#19981;&#29306;&#29298;&#26631;&#20934;&#24615;&#33021;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#36275;&#22815;&#28789;&#27963;&#65292;&#21487;&#20197;&#32435;&#20837;&#20219;&#20309;&#24863;&#20852;&#36259;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#65292;&#24182;&#19988;&#23545;&#20110;&#39640;&#32500;&#25968;&#25454;&#21487;&#20197;&#36827;&#34892;&#20960;&#20046;&#27809;&#26377;&#39069;&#22806;&#35745;&#31639;&#30340;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
As machine learning has been deployed ubiquitously across applications in modern data science, algorithmic fairness has become a great concern and varieties of fairness criteria have been proposed. Among them, imposing fairness constraints during learning, i.e. in-processing fair training, has been a popular type of training method because they don't require accessing sensitive attributes during test time in contrast to post-processing methods. Although imposing fairness constraints have been studied extensively for classical machine learning models, the effect these techniques have on deep neural networks is still unclear. Recent research has shown that adding fairness constraints to the objective function leads to severe over-fitting to fairness criteria in large models, and how to solve this challenge is an important open question. To address this challenge, we leverage the wisdom and power of pre-training and fine-tuning and develop a simple but novel framework to train fair neural
&lt;/p&gt;</description></item><item><title>&#37319;&#26679;&#39640;&#32500;&#20998;&#24067;&#26159;&#19968;&#20010;&#22522;&#26412;&#30340;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#36755;&#36816;&#30340;&#37319;&#26679;&#26041;&#27861;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;TemperFlow&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#28201;&#24230;&#20998;&#24067;&#65292;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.03933</link><description>&lt;p&gt;
&#36890;&#36807;&#28201;&#24230;&#20998;&#24067;&#27969;&#23454;&#29616;&#39640;&#25928;&#22810;&#27169;&#24577;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Efficient Multimodal Sampling via Tempered Distribution Flow. (arXiv:2304.03933v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03933
&lt;/p&gt;
&lt;p&gt;
&#37319;&#26679;&#39640;&#32500;&#20998;&#24067;&#26159;&#19968;&#20010;&#22522;&#26412;&#30340;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#36755;&#36816;&#30340;&#37319;&#26679;&#26041;&#27861;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;TemperFlow&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#28201;&#24230;&#20998;&#24067;&#65292;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#39640;&#32500;&#20998;&#24067;&#20013;&#37319;&#26679;&#26159;&#32479;&#35745;&#30740;&#31350;&#21644;&#23454;&#36341;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#24403;&#30446;&#26631;&#23494;&#24230;&#20989;&#25968;&#19981;&#35268;&#33539;&#21270;&#19988;&#21253;&#21547;&#23396;&#31435;&#27169;&#24335;&#26102;&#65292;&#23601;&#20250;&#20986;&#29616;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#25311;&#21512;&#19968;&#20010;&#34987;&#31216;&#20026;&#36755;&#36816;&#26144;&#23556;&#30340;&#21487;&#36870;&#21464;&#25442;&#26144;&#23556;&#65288;&#22312;&#21442;&#32771;&#27010;&#29575;&#27979;&#37327;&#21644;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#65289;&#65292;&#20351;&#24471;&#36890;&#36807;&#36755;&#36816;&#26144;&#23556;&#23558;&#21442;&#32771;&#26679;&#26412;&#25512;&#21521;&#21069;&#27839;&#21363;&#21487;&#23454;&#29616;&#20174;&#30446;&#26631;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;Wasserstein&#26799;&#24230;&#27969;&#29702;&#35770;&#29702;&#35770;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#22522;&#20110;&#36755;&#36816;&#30340;&#37319;&#26679;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TemperFlow&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#22810;&#27169;&#24577;&#38382;&#39064;&#12290;TemperFlow&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#19968;&#31995;&#21015;&#28201;&#24230;&#20998;&#24067;&#65292;&#20197;&#36880;&#27493;&#25509;&#36817;&#30446;&#26631;&#20998;&#24067;&#65292;&#24182;&#35777;&#26126;&#23427;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#21508;&#31181;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26032;&#22411;&#37319;&#26679;&#22120;&#30456;&#23545;&#20110;&#29616;&#26377;&#37319;&#26679;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sampling from high-dimensional distributions is a fundamental problem in statistical research and practice. However, great challenges emerge when the target density function is unnormalized and contains isolated modes. We tackle this difficulty by fitting an invertible transformation mapping, called a transport map, between a reference probability measure and the target distribution, so that sampling from the target distribution can be achieved by pushing forward a reference sample through the transport map. We theoretically analyze the limitations of existing transport-based sampling methods using the Wasserstein gradient flow theory, and propose a new method called TemperFlow that addresses the multimodality issue. TemperFlow adaptively learns a sequence of tempered distributions to progressively approach the target distribution, and we prove that it overcomes the limitations of existing methods. Various experiments demonstrate the superior performance of this novel sampler compared 
&lt;/p&gt;</description></item><item><title>3D GAN&#26159;&#29983;&#25104;&#19977;&#32500;&#37325;&#24314;&#12289;&#28857;&#20113;&#37325;&#24314;&#21644;3D&#35821;&#20041;&#22330;&#26223;&#23436;&#25104;&#30340;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#12290;&#36873;&#25321;&#22122;&#22768;&#20998;&#24067;&#23545;&#24212;&#30528;&#28508;&#31354;&#38388;&#65292;&#29702;&#35299;&#20854;&#32467;&#26500;&#26377;&#21161;&#20110;&#24494;&#35843;&#29983;&#25104;&#26679;&#26412;&#12290;&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;3D GAN&#21450;&#20854;&#35757;&#32451;&#26041;&#27861;&#65292;&#38024;&#23545;&#26410;&#26469;&#30740;&#31350;&#25552;&#20986;&#20102;&#28508;&#22312;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.03932</link><description>&lt;p&gt;
3D GAN&#19982;&#28508;&#31354;&#38388;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
3D GANs and Latent Space: A comprehensive survey. (arXiv:2304.03932v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03932
&lt;/p&gt;
&lt;p&gt;
3D GAN&#26159;&#29983;&#25104;&#19977;&#32500;&#37325;&#24314;&#12289;&#28857;&#20113;&#37325;&#24314;&#21644;3D&#35821;&#20041;&#22330;&#26223;&#23436;&#25104;&#30340;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#12290;&#36873;&#25321;&#22122;&#22768;&#20998;&#24067;&#23545;&#24212;&#30528;&#28508;&#31354;&#38388;&#65292;&#29702;&#35299;&#20854;&#32467;&#26500;&#26377;&#21161;&#20110;&#24494;&#35843;&#29983;&#25104;&#26679;&#26412;&#12290;&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;3D GAN&#21450;&#20854;&#35757;&#32451;&#26041;&#27861;&#65292;&#38024;&#23545;&#26410;&#26469;&#30740;&#31350;&#25552;&#20986;&#20102;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#36890;&#36807;&#23558;&#20302;&#32500;&#38543;&#26426;&#22122;&#22768;&#26144;&#23556;&#21040;&#39640;&#32500;&#31354;&#38388;&#20013;&#65292;&#22312;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#20013;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#36825;&#20123;&#32593;&#32476;&#24050;&#34987;&#29992;&#20110;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21644;&#19977;&#32500;&#29289;&#20307;&#12290;&#22312;&#28216;&#25103;&#25110;&#27169;&#25311;&#31561;3D&#22270;&#24418;&#29615;&#22659;&#30340;&#24320;&#21457;&#36807;&#31243;&#20013;&#65292;&#39640;&#25928;&#24314;&#27169;3D&#23545;&#35937;&#21644;&#20154;&#33080;&#33267;&#20851;&#37325;&#35201;&#12290;3D GAN&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;3D&#37325;&#24314;&#12289;&#28857;&#20113;&#37325;&#24314;&#21644;3D&#35821;&#20041;&#22330;&#26223;&#23436;&#25104;&#12290;&#22122;&#22768;&#20998;&#24067;&#30340;&#36873;&#25321;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#34920;&#31034;&#20102;&#28508;&#31354;&#38388;&#12290;&#20026;&#20102;&#24494;&#35843;&#29983;&#25104;&#30340;&#26679;&#26412;&#65292;&#29702;&#35299;GAN&#30340;&#28508;&#31354;&#38388;&#26159;&#24517;&#35201;&#30340;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#23545;&#22270;&#20687;&#30340;&#35821;&#20041;&#26377;&#24847;&#20041;&#30340;&#37096;&#20998;&#36827;&#34892;&#24418;&#24577;&#21464;&#25442;&#26469;&#35777;&#26126;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#28508;&#31354;&#38388;&#21644;3D GAN&#65292;&#30740;&#31350;&#20102;&#20960;&#31181;GAN&#21464;&#20307;&#21644;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#27934;&#23519;&#25552;&#39640;3D GAN&#35757;&#32451;&#30340;&#35265;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GANs) have emerged as a significant player in generative modeling by mapping lower-dimensional random noise to higher-dimensional spaces. These networks have been used to generate high-resolution images and 3D objects. The efficient modeling of 3D objects and human faces is crucial in the development process of 3D graphical environments such as games or simulations. 3D GANs are a new type of generative model used for 3D reconstruction, point cloud reconstruction, and 3D semantic scene completion. The choice of distribution for noise is critical as it represents the latent space. Understanding a GAN's latent space is essential for fine-tuning the generated samples, as demonstrated by the morphing of semantically meaningful parts of images. In this work, we explore the latent space and 3D GANs, examine several GAN variants and training methods to gain insights into improving 3D GAN training, and suggest potential future directions for further research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24494;&#35843;&#26399;&#38388;&#26816;&#27979;&#21644;&#26126;&#30830;&#21306;&#20998;&#21463;&#24433;&#21709;&#31867;&#21035;&#30340;&#38169;&#35823;&#23646;&#24615;&#65292;&#32531;&#35299;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;&#65292;&#21516;&#26102;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#21644;&#25351;&#21521;&#30446;&#26631;&#39046;&#22495;&#30340;&#26377;&#24847;&#20041;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2304.03916</link><description>&lt;p&gt;
&#22312;&#24494;&#35843;&#26102;&#32531;&#35299;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Mitigating Spurious Correlations in Multi-modal Models during Fine-tuning. (arXiv:2304.03916v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24494;&#35843;&#26399;&#38388;&#26816;&#27979;&#21644;&#26126;&#30830;&#21306;&#20998;&#21463;&#24433;&#21709;&#31867;&#21035;&#30340;&#38169;&#35823;&#23646;&#24615;&#65292;&#32531;&#35299;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;&#65292;&#21516;&#26102;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#21644;&#25351;&#21521;&#30446;&#26631;&#39046;&#22495;&#30340;&#26377;&#24847;&#20041;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25439;&#23475;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#25110;&#23548;&#33268;&#27169;&#22411;&#22522;&#20110;&#38169;&#35823;&#21407;&#22240;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;&#26159;&#23454;&#38469;&#37096;&#32626;&#38754;&#20020;&#30340;&#20027;&#35201;&#40065;&#26834;&#24615;&#38382;&#39064;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#22312;&#39044;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#26399;&#38388;&#32531;&#35299;&#36825;&#20123;&#30456;&#20851;&#24615;&#21487;&#33021;&#25104;&#26412;&#39640;&#26114;&#19988;&#19981;&#20999;&#23454;&#38469;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#27809;&#26377;&#39640;&#24615;&#33021;&#35745;&#31639;&#36164;&#28304;&#30340;&#20154;&#26469;&#35828;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29305;&#23450;&#39046;&#22495;&#30340;&#24494;&#35843;&#26399;&#38388;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;&#12290;&#38024;&#23545;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#20363;&#22914;CLIP&#65289;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#19981;&#21516;&#27169;&#24577;&#26469;&#26816;&#27979;&#24182;&#26126;&#30830;&#21306;&#20998;&#21463;&#24433;&#21709;&#31867;&#21035;&#30340;&#38169;&#35823;&#23646;&#24615;&#65292;&#36890;&#36807;&#34920;&#36798;&#35821;&#35328;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#22312;CLIP&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#21644;&#28145;&#20837;&#30340;&#21487;&#35270;&#21270;&#26174;&#31034;&#65292;&#36825;&#31181;&#20171;&#20837;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#65292;&#32780;&#19981;&#23384;&#22312;&#38169;&#35823;&#23646;&#24615;&#65292;&#24182;&#23558;&#27169;&#22411;&#25351;&#21521;&#30446;&#26631;&#39046;&#22495;&#30340;&#26377;&#24847;&#20041;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spurious correlations that degrade model generalization or lead the model to be right for the wrong reasons are one of the main robustness concerns for real-world deployments. However, mitigating these correlations during pre-training for large-scale models can be costly and impractical, particularly for those without access to high-performance computing resources. This paper proposes a novel approach to address spurious correlations during fine-tuning for a given domain of interest. With a focus on multi-modal models (e.g., CLIP), the proposed method leverages different modalities in these models to detect and explicitly set apart spurious attributes from the affected class, achieved through a multi-modal contrastive loss function that expresses spurious relationships through language. Our experimental results and in-depth visualizations on CLIP show that such an intervention can effectively i) improve the model's accuracy when spurious attributes are not present, and ii) directs the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#38480;&#32500;&#29305;&#24449;&#36924;&#36817;&#30340;&#38750;&#32447;&#24615;&#21160;&#24577;&#35889;&#23884;&#20837;&#25511;&#21046;&#31639;&#27861;&#65288;SDEC&#65289;&#29992;&#20110;&#35299;&#20915;&#38543;&#26426;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2304.03907</link><description>&lt;p&gt;
&#22522;&#20110;&#26377;&#38480;&#32500;&#35889;&#21160;&#24577;&#23884;&#20837;&#30340;&#38543;&#26426;&#38750;&#32447;&#24615;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Stochastic Nonlinear Control via Finite-dimensional Spectral Dynamic Embedding. (arXiv:2304.03907v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#38480;&#32500;&#29305;&#24449;&#36924;&#36817;&#30340;&#38750;&#32447;&#24615;&#21160;&#24577;&#35889;&#23884;&#20837;&#25511;&#21046;&#31639;&#27861;&#65288;SDEC&#65289;&#29992;&#20110;&#35299;&#20915;&#38543;&#26426;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#26368;&#20248;&#25511;&#21046;&#19968;&#30452;&#26159;&#19968;&#20010;&#26840;&#25163;&#30340;&#38382;&#39064;&#12290;Ren&#31561;&#20154;&#24341;&#20837;&#20102;&#35889;&#21160;&#24577;&#23884;&#20837;&#26469;&#24320;&#21457;&#25511;&#21046;&#26410;&#30693;&#31995;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#23427;&#20351;&#29992;&#26080;&#31351;&#32500;&#29305;&#24449;&#26469;&#32447;&#24615;&#34920;&#31034;&#29366;&#24577;&#20540;&#20989;&#25968;&#65292;&#24182;&#21033;&#29992;&#26377;&#38480;&#32500;&#30340;&#25130;&#26029;&#36924;&#36817;&#36827;&#34892;&#23454;&#38469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#22312;&#24050;&#30693;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#25511;&#21046;&#20013;&#30340;&#26377;&#38480;&#32500;&#36924;&#36817;&#24615;&#36136;&#23578;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#38543;&#26426;&#38750;&#32447;&#24615;&#25511;&#21046;&#31639;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#26377;&#38480;&#32500;&#29305;&#24449;&#36924;&#36817;&#30340;&#38750;&#32447;&#24615;&#21160;&#24577;&#35889;&#23884;&#20837;&#25511;&#21046;&#65288;SDEC&#65289;&#65292;&#24182;&#36827;&#34892;&#28145;&#20837;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#20197;&#34920;&#24449;&#30001;&#26377;&#38480;&#32500;&#25130;&#26029;&#24341;&#36215;&#30340;&#36924;&#36817;&#35823;&#24046;&#21644;&#30001;&#26377;&#38480;&#26679;&#26412;&#36924;&#36817;&#24341;&#36215;&#30340;&#32479;&#35745;&#35823;&#24046;&#65292;&#21516;&#26102;&#36827;&#34892;&#25919;&#31574;&#35780;&#20272;&#21644;&#25919;&#31574;&#20248;&#21270;&#30340;&#23454;&#39564;&#27979;&#35797;&#21644;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal control is notoriously difficult for stochastic nonlinear systems. Ren et al. introduced Spectral Dynamics Embedding for developing reinforcement learning methods for controlling an unknown system. It uses an infinite-dimensional feature to linearly represent the state-value function and exploits finite-dimensional truncation approximation for practical implementation. However, the finite-dimensional approximation properties in control have not been investigated even when the model is known. In this paper, we provide a tractable stochastic nonlinear control algorithm that exploits the nonlinear dynamics upon the finite-dimensional feature approximation, Spectral Dynamics Embedding Control (SDEC), with an in-depth theoretical analysis to characterize the approximation error induced by the finite-dimension truncation and statistical error induced by finite-sample approximation in both policy evaluation and policy optimization. We also empirically test the algorithm and compare th
&lt;/p&gt;</description></item><item><title>InstructBio&#26159;&#19968;&#31181;&#38024;&#23545;&#29983;&#29289;&#21270;&#23398;&#38382;&#39064;&#30340;&#22823;&#35268;&#27169;&#21322;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65292;&#24341;&#20837;&#25945;&#32451;&#27169;&#22411;&#25552;&#20379;&#26377;&#25928;&#30340;&#32622;&#20449;&#24230;&#27604;&#29575;&#26469;&#25351;&#23548;&#30446;&#26631;&#27169;&#22411;&#23545;&#19981;&#21516;&#25968;&#25454;&#28857;&#32473;&#20104;&#26126;&#26174;&#20851;&#27880;&#65292;&#36991;&#20813;&#20381;&#36182;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#21644;&#19981;&#27491;&#30830;&#30340;&#20266;&#27880;&#37322;&#65292;&#25552;&#39640;&#20102;&#20998;&#23376;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.03906</link><description>&lt;p&gt;
InstructBio&#65306;&#19968;&#31181;&#38024;&#23545;&#29983;&#29289;&#21270;&#23398;&#38382;&#39064;&#30340;&#22823;&#35268;&#27169;&#21322;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
InstructBio: A Large-scale Semi-supervised Learning Paradigm for Biochemical Problems. (arXiv:2304.03906v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03906
&lt;/p&gt;
&lt;p&gt;
InstructBio&#26159;&#19968;&#31181;&#38024;&#23545;&#29983;&#29289;&#21270;&#23398;&#38382;&#39064;&#30340;&#22823;&#35268;&#27169;&#21322;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65292;&#24341;&#20837;&#25945;&#32451;&#27169;&#22411;&#25552;&#20379;&#26377;&#25928;&#30340;&#32622;&#20449;&#24230;&#27604;&#29575;&#26469;&#25351;&#23548;&#30446;&#26631;&#27169;&#22411;&#23545;&#19981;&#21516;&#25968;&#25454;&#28857;&#32473;&#20104;&#26126;&#26174;&#20851;&#27880;&#65292;&#36991;&#20813;&#20381;&#36182;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#21644;&#19981;&#27491;&#30830;&#30340;&#20266;&#27880;&#37322;&#65292;&#25552;&#39640;&#20102;&#20998;&#23376;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#38754;&#23545;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#20013;&#30340;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#22987;&#32456;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#26159;&#22312;&#22823;&#22411;&#26410;&#26631;&#35760;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#24378;&#21147;&#30340;&#20219;&#21153;&#26080;&#20851;&#27169;&#22411;&#65292;&#20294;&#22312;&#21521;&#19979;&#28216;&#20219;&#21153;&#36716;&#31227;&#30693;&#35782;&#26041;&#38754;&#21487;&#33021;&#23384;&#22312;&#22256;&#38590;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InstructBio&#65292;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65292;&#26356;&#22909;&#22320;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#26679;&#20363;&#12290;&#23427;&#24341;&#20837;&#25945;&#32451;&#27169;&#22411;&#26469;&#25552;&#20379;&#20266;&#26631;&#31614;&#21487;&#38752;&#24615;&#30340;&#32622;&#20449;&#24230;&#27604;&#29575;&#12290;&#36825;&#20123;&#32622;&#20449;&#24230;&#20998;&#25968;&#28982;&#21518;&#25351;&#23548;&#30446;&#26631;&#27169;&#22411;&#23545;&#19981;&#21516;&#30340;&#25968;&#25454;&#28857;&#32473;&#20104;&#26126;&#26174;&#30340;&#20851;&#27880;&#65292;&#36991;&#20813;&#23545;&#26631;&#35760;&#25968;&#25454;&#30340;&#36807;&#24230;&#20381;&#36182;&#20197;&#21450;&#19981;&#27491;&#30830;&#30340;&#20266;&#27880;&#37322;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;InstructBio&#26174;&#33879;&#25552;&#39640;&#20102;&#20998;&#23376;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#19981;&#20165;&#22312;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#26041;&#38754;&#65292;&#22312;&#27963;&#24615;&#24748;&#23830;&#20272;&#35745;&#26041;&#38754;&#20063;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of artificial intelligence for science, it is consistently an essential challenge to face a limited amount of labeled data for real-world problems. The prevailing approach is to pretrain a powerful task-agnostic model on a large unlabeled corpus but may struggle to transfer knowledge to downstream tasks. In this study, we propose InstructMol, a semi-supervised learning algorithm, to take better advantage of unlabeled examples. It introduces an instructor model to provide the confidence ratios as the measurement of pseudo-labels' reliability. These confidence scores then guide the target model to pay distinct attention to different data points, avoiding the over-reliance on labeled data and the negative influence of incorrect pseudo-annotations. Comprehensive experiments show that InstructBio substantially improves the generalization ability of molecular models, in not only molecular property predictions but also activity cliff estimations, demonstrating the superiority of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22478;&#24066;&#35268;&#21010;&#19982;&#20154;&#24037;&#26234;&#33021;&#30340;&#20132;&#21449;&#24212;&#29992;&#65292;&#37325;&#28857;&#26159;&#33258;&#21160;&#21270;&#29992;&#22320;&#37197;&#32622;&#65292;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#12289;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#12289;&#28145;&#24230;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#12289;&#23545;&#35805;&#24335; AI &#21644;&#22320;&#29702;&#31354;&#38388;&#21644;&#26102;&#38388;&#26426;&#22120;&#23398;&#20064;&#31561;&#25216;&#26415;&#65292;AI &#21487;&#20197;&#20026;&#29616;&#20195;&#22478;&#24066;&#35268;&#21010;&#24102;&#26469;&#19981;&#23569;&#21019;&#26032;&#19982;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2304.03892</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#22478;&#24066;&#35268;&#21010;&#65306;&#29983;&#25104;&#24335;&#21644;&#32842;&#22825;&#24335; AI &#30456;&#32467;&#21512;&#30340;&#22478;&#24066;&#35268;&#21010;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Towards Automated Urban Planning: When Generative and ChatGPT-like AI Meets Urban Planning. (arXiv:2304.03892v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22478;&#24066;&#35268;&#21010;&#19982;&#20154;&#24037;&#26234;&#33021;&#30340;&#20132;&#21449;&#24212;&#29992;&#65292;&#37325;&#28857;&#26159;&#33258;&#21160;&#21270;&#29992;&#22320;&#37197;&#32622;&#65292;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#12289;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#12289;&#28145;&#24230;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#12289;&#23545;&#35805;&#24335; AI &#21644;&#22320;&#29702;&#31354;&#38388;&#21644;&#26102;&#38388;&#26426;&#22120;&#23398;&#20064;&#31561;&#25216;&#26415;&#65292;AI &#21487;&#20197;&#20026;&#29616;&#20195;&#22478;&#24066;&#35268;&#21010;&#24102;&#26469;&#19981;&#23569;&#21019;&#26032;&#19982;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#35268;&#21010;&#39046;&#22495;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#26366;&#32463;&#26159;&#29420;&#31435;&#21457;&#23637;&#30340;&#65292;&#20294;&#29616;&#22312;&#20004;&#20010;&#39046;&#22495;&#24320;&#22987;&#20132;&#21449;&#27719;&#21512;&#65292;&#20114;&#30456;&#20511;&#37492;&#21644;&#21463;&#30410;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22478;&#24066;&#35268;&#21010;&#20174;&#21487;&#25345;&#32493;&#24615;&#12289;&#29983;&#27963;&#12289;&#32463;&#27982;&#12289;&#28798;&#23475;&#21644;&#29615;&#22659;&#31561;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#65292;&#22238;&#39038;&#20102;&#22478;&#24066;&#35268;&#21010;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#24182;&#23558;&#36825;&#20123;&#27010;&#24565;&#19982;&#26426;&#22120;&#23398;&#20064;&#30340;&#20851;&#38190;&#24320;&#25918;&#38382;&#39064;&#32852;&#31995;&#36215;&#26469;&#65292;&#21253;&#25324;&#23545;&#25239;&#23398;&#20064;&#12289;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#12289;&#28145;&#24230;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#12289;&#23545;&#35805;&#24335; AI &#20197;&#21450;&#22320;&#29702;&#31354;&#38388;&#21644;&#26102;&#38388;&#26426;&#22120;&#23398;&#20064;&#31561;&#65292;&#35780;&#20272;&#20102; AI &#22914;&#20309;&#20026;&#29616;&#20195;&#22478;&#24066;&#35268;&#21010;&#20570;&#20986;&#36129;&#29486;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#33258;&#21160;&#21270;&#29992;&#22320;&#37197;&#32622;&#65292;&#21363;&#20174;&#21608;&#22260;&#30340;&#22320;&#29702;&#31354;&#38388;&#12289;&#20154;&#31867;&#31227;&#21160;&#12289;&#31038;&#20132;&#23186;&#20307;&#12289;&#29615;&#22659;&#21644;&#32463;&#27982;&#27963;&#21160;&#20013;&#20026;&#30446;&#26631;&#21306;&#22495;&#29983;&#25104;&#22303;&#22320;&#29992;&#36884;&#21644;&#24314;&#31569;&#37197;&#32622;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#21246;&#30011;&#20102;&#38598;&#25104; AI &#21644;&#22478;&#24066;&#35268;&#21010;&#38754;&#20020;&#30340;&#19968;&#20123;&#25361;&#25112;&#21644;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The two fields of urban planning and artificial intelligence (AI) arose and developed separately. However, there is now cross-pollination and increasing interest in both fields to benefit from the advances of the other. In the present paper, we introduce the importance of urban planning from the sustainability, living, economic, disaster, and environmental perspectives. We review the fundamental concepts of urban planning and relate these concepts to crucial open problems of machine learning, including adversarial learning, generative neural networks, deep encoder-decoder networks, conversational AI, and geospatial and temporal machine learning, thereby assaying how AI can contribute to modern urban planning. Thus, a central problem is automated land-use configuration, which is formulated as the generation of land uses and building configuration for a target area from surrounding geospatial, human mobility, social media, environment, and economic activities. Finally, we delineate some 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;DiffDock-PP&#29992;&#20110;&#21018;&#24615;&#34507;&#30333;&#36136;-&#34507;&#30333;&#36136;&#23545;&#25509;&#65292;&#20854;&#22312;DIPS&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#27604;&#25152;&#26377;&#22522;&#20110;&#25628;&#32034;&#30340;&#26041;&#27861;&#26356;&#24555;&#65292;&#20026;&#20854;&#39044;&#27979;&#29983;&#25104;&#20102;&#21487;&#38752;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2304.03889</link><description>&lt;p&gt;
DiffDock-PP&#65306;&#25193;&#25955;&#27169;&#22411;&#19979;&#30340;&#21018;&#24615;&#34507;&#30333;&#36136;-&#34507;&#30333;&#36136;&#23545;&#25509;
&lt;/p&gt;
&lt;p&gt;
DiffDock-PP: Rigid Protein-Protein Docking with Diffusion Models. (arXiv:2304.03889v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;DiffDock-PP&#29992;&#20110;&#21018;&#24615;&#34507;&#30333;&#36136;-&#34507;&#30333;&#36136;&#23545;&#25509;&#65292;&#20854;&#22312;DIPS&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#27604;&#25152;&#26377;&#22522;&#20110;&#25628;&#32034;&#30340;&#26041;&#27861;&#26356;&#24555;&#65292;&#20026;&#20854;&#39044;&#27979;&#29983;&#25104;&#20102;&#21487;&#38752;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#34507;&#30333;&#36136;&#32467;&#26500;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#23545;&#20110;&#29616;&#20195;&#29983;&#29289;&#23398;&#33267;&#20851;&#37325;&#35201;&#65292;&#22312;&#33647;&#29289;&#21457;&#29616;&#21644;&#34507;&#30333;&#36136;&#35774;&#35745;&#20013;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31867;&#20284;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21018;&#24615;&#34507;&#30333;&#36136;-&#34507;&#30333;&#36136;&#23545;&#25509;&#65306;DiffDock-PP&#26159;&#19968;&#31181;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#23398;&#20064;&#23558;&#19981;&#32467;&#21512;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#36716;&#21270;&#21644;&#26059;&#36716;&#20026;&#23427;&#20204;&#32467;&#21512;&#30340;&#26500;&#35937;&#12290;&#25105;&#20204;&#22312;DIPS&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20013;&#20301;&#25968;C-RMSD&#20026;4.85&#65292;&#20248;&#20110;&#25152;&#26377;&#32771;&#34385;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;DiffDock-PP&#27604;&#25152;&#26377;&#22522;&#20110;&#25628;&#32034;&#30340;&#26041;&#27861;&#37117;&#35201;&#24555;&#65292;&#24182;&#20026;&#20854;&#39044;&#27979;&#29983;&#25104;&#21487;&#38752;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312; $\texttt{https://github.com/ketatam/DiffDock-PP}$ &#19978;&#20844;&#24320;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding how proteins structurally interact is crucial to modern biology, with applications in drug discovery and protein design. Recent machine learning methods have formulated protein-small molecule docking as a generative problem with significant performance boosts over both traditional and deep learning baselines. In this work, we propose a similar approach for rigid protein-protein docking: DiffDock-PP is a diffusion generative model that learns to translate and rotate unbound protein structures into their bound conformations. We achieve state-of-the-art performance on DIPS with a median C-RMSD of 4.85, outperforming all considered baselines. Additionally, DiffDock-PP is faster than all search-based methods and generates reliable confidence estimates for its predictions. Our code is publicly available at $\texttt{https://github.com/ketatam/DiffDock-PP}$
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;OFTER&#65292;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#20013;&#31561;&#35268;&#27169;&#22810;&#31181;&#26102;&#38388;&#24207;&#21015;&#30340;&#22312;&#32447;&#39044;&#27979;&#31649;&#32447;&#65292;&#33021;&#22815;&#32988;&#36807;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;OFTER&#26159;&#37329;&#34701;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#38382;&#39064;&#30340;&#29702;&#24819;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.03877</link><description>&lt;p&gt;
OFTER&#65306;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#20013;&#31561;&#35268;&#27169;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30340;&#22312;&#32447;&#39044;&#27979;&#31649;&#32447;
&lt;/p&gt;
&lt;p&gt;
OFTER: An Online Pipeline for Time Series Forecasting. (arXiv:2304.03877v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;OFTER&#65292;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#20013;&#31561;&#35268;&#27169;&#22810;&#31181;&#26102;&#38388;&#24207;&#21015;&#30340;&#22312;&#32447;&#39044;&#27979;&#31649;&#32447;&#65292;&#33021;&#22815;&#32988;&#36807;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;OFTER&#26159;&#37329;&#34701;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#38382;&#39064;&#30340;&#29702;&#24819;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;OFTER&#65292;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#20013;&#31561;&#35268;&#27169;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30340;&#39044;&#27979;&#31649;&#32447;&#12290;OFTER&#21033;&#29992;kNN&#21644;&#24191;&#20041;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#21442;&#25968;&#27169;&#22411;&#65292;&#24182;&#19982;&#38477;&#32500;&#32452;&#20214;&#38598;&#25104;&#22312;&#19968;&#36215;&#12290;&#20026;&#20102;&#36991;&#20813;&#39640;&#32500;&#24230;&#30340;&#22256;&#22659;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#20462;&#25913;&#21518;&#30340;&#26368;&#22823;&#30456;&#20851;&#31995;&#25968;&#30340;&#21152;&#26435;&#33539;&#25968;&#12290;&#25105;&#20204;&#20171;&#32461;&#30340;&#31649;&#32447;&#19987;&#38376;&#38024;&#23545;&#22312;&#32447;&#20219;&#21153;&#36827;&#34892;&#35774;&#35745;&#65292;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#33021;&#22815;&#32988;&#36807;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;&#31639;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;&#12289;&#22312;&#32447;&#24615;&#36136;&#20197;&#21450;&#22312;&#20302;&#20449;&#22122;&#27604;&#29615;&#22659;&#19979;&#36816;&#34892;&#30340;&#33021;&#21147;&#20351;OFTER&#25104;&#20026;&#37329;&#34701;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#38382;&#39064;&#30340;&#29702;&#24819;&#35299;&#20915;&#26041;&#26696;&#65292;&#20363;&#22914;&#27599;&#26085;&#32929;&#31080;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce OFTER, a time series forecasting pipeline tailored for mid-sized multivariate time series. OFTER utilizes the non-parametric models of k-nearest neighbors and Generalized Regression Neural Networks, integrated with a dimensionality reduction component. To circumvent the curse of dimensionality, we employ a weighted norm based on a modified version of the maximal correlation coefficient. The pipeline we introduce is specifically designed for online tasks, has an interpretable output, and is able to outperform several state-of-the art baselines. The computational efficacy of the algorithm, its online nature, and its ability to operate in low signal-to-noise regimes, render OFTER an ideal approach for financial multivariate time series problems, such as daily equity forecasting. Our work demonstrates that while deep learning models hold significant promise for time series forecasting, traditional methods carefully integrating mainstream tools remain very competitive alternati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#8212;&#8212;&#20027;&#21160;&#36873;&#25321;&#24615;&#39044;&#27979;&#65288;ASPEST&#65289;&#65292;&#23427;&#21487;&#20197;&#22312;&#36716;&#31227;&#30446;&#26631;&#39046;&#22495;&#20013;&#23398;&#20064;&#26597;&#35810;&#26356;&#22810;&#26377;&#20449;&#24687;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#23454;&#29616;&#20943;&#23569;&#20154;&#24037;&#26631;&#27880;&#24037;&#20316;&#30340;&#21516;&#26102;&#22686;&#21152;&#20934;&#30830;&#24615;&#21644;&#35206;&#30422;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.03870</link><description>&lt;p&gt;
ASPEST&#65306;&#20027;&#21160;&#23398;&#20064;&#21644;&#36873;&#25321;&#39044;&#27979;&#20043;&#38388;&#30340;&#24357;&#21512;
&lt;/p&gt;
&lt;p&gt;
ASPEST: Bridging the Gap Between Active Learning and Selective Prediction. (arXiv:2304.03870v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#8212;&#8212;&#20027;&#21160;&#36873;&#25321;&#24615;&#39044;&#27979;&#65288;ASPEST&#65289;&#65292;&#23427;&#21487;&#20197;&#22312;&#36716;&#31227;&#30446;&#26631;&#39046;&#22495;&#20013;&#23398;&#20064;&#26597;&#35810;&#26356;&#22810;&#26377;&#20449;&#24687;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#23454;&#29616;&#20943;&#23569;&#20154;&#24037;&#26631;&#27880;&#24037;&#20316;&#30340;&#21516;&#26102;&#22686;&#21152;&#20934;&#30830;&#24615;&#21644;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#24615;&#39044;&#27979;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#21487;&#38752;&#30340;&#27169;&#22411;&#65292;&#24403;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#24456;&#39640;&#26102;&#65292;&#21487;&#20197;&#36991;&#20813;&#36827;&#34892;&#39044;&#27979;&#12290;&#38543;&#21518;&#65292;&#21487;&#20197;&#23558;&#36825;&#20123;&#39044;&#27979;&#25512;&#36831;&#32473;&#20154;&#31867;&#19987;&#23478;&#36827;&#34892;&#36827;&#19968;&#27493;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#27979;&#35797;&#25968;&#25454;&#30340;&#20998;&#24067;&#19982;&#35757;&#32451;&#25968;&#25454;&#19981;&#21516;&#12290;&#36825;&#23548;&#33268;&#26356;&#19981;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#38656;&#35201;&#22686;&#21152;&#20154;&#24037;&#26631;&#27880;&#65292;&#36825;&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#37117;&#26159;&#22256;&#38590;&#21644;&#26114;&#36149;&#30340;&#12290;&#20027;&#21160;&#23398;&#20064;&#36890;&#36807;&#20165;&#26597;&#35810;&#26368;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#31034;&#20363;&#26469;&#36991;&#20813;&#36825;&#31181;&#22256;&#38590;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#26696;&#20363;&#20013;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#38477;&#20302;&#24635;&#20307;&#30340;&#26631;&#27880;&#24037;&#20316;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24357;&#21512;&#20102;&#36873;&#25321;&#24615;&#39044;&#27979;&#21644;&#20027;&#21160;&#23398;&#20064;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#31216;&#20026;&#20027;&#21160;&#36873;&#25321;&#24615;&#39044;&#27979;&#65288;active selective prediction&#65289;&#65292;&#23427;&#21487;&#20197;&#22312;&#22686;&#21152;&#20934;&#30830;&#24615;&#21644;&#35206;&#30422;&#29575;&#30340;&#21516;&#26102;&#22312;&#36716;&#31227;&#30446;&#26631;&#39046;&#22495;&#20013;&#23398;&#20064;&#26597;&#35810;&#26356;&#22810;&#26377;&#20449;&#24687;&#30340;&#26679;&#26412;&#12290;&#23545;&#20110;&#36825;&#20010;&#26032;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;ASPEST&#65292;&#23427;&#35757;&#32451;&#27169;&#22411;&#24555;&#29031;&#30340;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Selective prediction aims to learn a reliable model that abstains from making predictions when the model uncertainty is high. These predictions can then be deferred to a human expert for further evaluation. In many real-world scenarios, however, the distribution of test data is different from the training data. This results in more inaccurate predictions, necessitating increased human labeling, which is difficult and expensive in many scenarios. Active learning circumvents this difficulty by only querying the most informative examples and, in several cases, has been shown to lower the overall labeling effort. In this work, we bridge the gap between selective prediction and active learning, proposing a new learning paradigm called active selective prediction which learns to query more informative samples from the shifted target domain while increasing accuracy and coverage. For this new problem, we propose a simple but effective solution, ASPEST, that trains ensembles of model snapshots
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#31163;&#32447;&#22522;&#20110;&#27169;&#22411;&#30340;&#20248;&#21270;&#20013;&#65292;&#20445;&#23432;&#30340;&#23458;&#35266;&#27169;&#22411;&#65288;COMs&#65289;&#26159;&#19968;&#31181;&#29305;&#27530;&#30340;&#22522;&#20110;&#23545;&#27604;&#25955;&#24230;&#33021;&#37327;&#27169;&#22411;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#29992;Langevin MCMC&#37319;&#26679;&#22120;&#26367;&#25442;&#26799;&#24230;&#19978;&#21319;&#37319;&#26679;&#22120;&#65292;&#20197;&#25552;&#39640;&#26679;&#26412;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.03866</link><description>&lt;p&gt;
&#20445;&#23432;&#30340;&#23458;&#35266;&#27169;&#22411;&#26159;&#19968;&#31181;&#29305;&#27530;&#30340;&#22522;&#20110;&#23545;&#27604;&#25955;&#24230;&#33021;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Conservative objective models are a special kind of contrastive divergence-based energy model. (arXiv:2304.03866v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#31163;&#32447;&#22522;&#20110;&#27169;&#22411;&#30340;&#20248;&#21270;&#20013;&#65292;&#20445;&#23432;&#30340;&#23458;&#35266;&#27169;&#22411;&#65288;COMs&#65289;&#26159;&#19968;&#31181;&#29305;&#27530;&#30340;&#22522;&#20110;&#23545;&#27604;&#25955;&#24230;&#33021;&#37327;&#27169;&#22411;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#29992;Langevin MCMC&#37319;&#26679;&#22120;&#26367;&#25442;&#26799;&#24230;&#19978;&#21319;&#37319;&#26679;&#22120;&#65292;&#20197;&#25552;&#39640;&#26679;&#26412;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20445;&#23432;&#30340;&#23458;&#35266;&#27169;&#22411;&#65288;COMs&#65289;&#29992;&#20110;&#31163;&#32447;&#22522;&#20110;&#27169;&#22411;&#30340;&#20248;&#21270;&#65288;MBO&#65289;&#26159;&#19968;&#31181;&#29305;&#27530;&#30340;&#22522;&#20110;&#23545;&#27604;&#25955;&#24230;&#33021;&#37327;&#27169;&#22411;&#65292;&#20854;&#20013;&#33021;&#37327;&#20989;&#25968;&#26082;&#34920;&#31034;&#36755;&#20837;&#30340;&#26080;&#26465;&#20214;&#27010;&#29575;&#65292;&#20063;&#34920;&#31034;&#22870;&#21169;&#21464;&#37327;&#30340;&#26465;&#20214;&#27010;&#29575;&#12290;&#34429;&#28982;&#26368;&#21021;&#30340;&#20844;&#24335;&#21482;&#20174;&#20854;&#23398;&#20064;&#20998;&#24067;&#20013;&#25277;&#26679;&#27169;&#22411;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20462;&#22797;&#26041;&#27861;&#65292;&#29992;Langevin MCMC&#37319;&#26679;&#22120;&#26367;&#25442;&#26799;&#24230;&#19978;&#21319;&#37319;&#26679;&#22120;&#12290;&#36825;&#20135;&#29983;&#20102;&#19968;&#31181;&#29305;&#27530;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#20854;&#20013;&#37319;&#26679;&#36755;&#20837;&#30340;&#27010;&#29575;&#19982;&#20854;&#39044;&#27979;&#30340;&#22870;&#21169;&#25104;&#27604;&#20363;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#22914;&#26524;&#23558;&#27169;&#22411;&#20998;&#35299;&#65292;&#20351;&#26080;&#26465;&#20214;&#27010;&#29575;&#21644;&#26465;&#20214;&#27010;&#29575;&#20998;&#21035;&#24314;&#27169;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we theoretically show that conservative objective models (COMs) for offline model-based optimisation (MBO) are a special kind of contrastive divergence-based energy model, one where the energy function represents both the unconditional probability of the input and the conditional probability of the reward variable. While the initial formulation only samples modes from its learned distribution, we propose a simple fix that replaces its gradient ascent sampler with a Langevin MCMC sampler. This gives rise to a special probabilistic model where the probability of sampling an input is proportional to its predicted reward. Lastly, we show that better samples can be obtained if the model is decoupled so that the unconditional and conditional probabilities are modelled separately.
&lt;/p&gt;</description></item><item><title>SGDP&#26159;&#19968;&#31181;&#26032;&#22411;&#22522;&#20110;&#27969;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39044;&#21462;&#22120;&#65292;&#36890;&#36807;&#24314;&#27169;LBA&#22686;&#37327;&#27969;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#21462;&#28151;&#21512;&#29305;&#24449;&#26469;&#36827;&#34892;&#25968;&#25454;&#39044;&#21462;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23427;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.03864</link><description>&lt;p&gt;
SGDP: &#19968;&#31181;&#22522;&#20110;&#27969;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39044;&#21462;&#22120;
&lt;/p&gt;
&lt;p&gt;
SGDP: A Stream-Graph Neural Network Based Data Prefetcher. (arXiv:2304.03864v1 [cs.OS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03864
&lt;/p&gt;
&lt;p&gt;
SGDP&#26159;&#19968;&#31181;&#26032;&#22411;&#22522;&#20110;&#27969;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39044;&#21462;&#22120;&#65292;&#36890;&#36807;&#24314;&#27169;LBA&#22686;&#37327;&#27969;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#21462;&#28151;&#21512;&#29305;&#24449;&#26469;&#36827;&#34892;&#25968;&#25454;&#39044;&#21462;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23427;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39044;&#21462;&#23545;&#20110;&#23384;&#20648;&#31995;&#32479;&#20248;&#21270;&#21644;&#35775;&#38382;&#24615;&#33021;&#25552;&#21319;&#38750;&#24120;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#39044;&#21462;&#22120;&#36866;&#29992;&#20110;&#25366;&#25496;&#39034;&#24207;&#36923;&#36753;&#22359;&#22320;&#22336;&#65288;LBA&#65289;&#30340;&#35775;&#38382;&#27169;&#24335;&#65292;&#20294;&#26159;&#26080;&#27861;&#22788;&#29702;&#29616;&#23454;&#19990;&#30028;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#22797;&#26434;&#38750;&#39034;&#24207;&#27169;&#24335;&#12290;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#39044;&#21462;&#22120;&#35206;&#30422;&#20102;&#26356;&#22810;&#30340;LBA&#35775;&#38382;&#65292;&#20294;&#26159;&#23427;&#20204;&#19981;&#36275;&#20197;&#20805;&#20998;&#32771;&#34385;LBA&#22686;&#37327;&#20043;&#38388;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#23548;&#33268;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#27969;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39044;&#21462;&#22120;&#65288;SGDP&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;SGDP&#20351;&#29992;&#21152;&#26435;&#26377;&#21521;&#22270;&#32467;&#26500;&#26469;&#24314;&#27169;LBA&#22686;&#37327;&#27969;&#20197;&#34920;&#31034;LBA&#22686;&#37327;&#20043;&#38388;&#30340;&#20132;&#20114;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#19968;&#27493;&#25552;&#21462;&#28151;&#21512;&#29305;&#24449;&#36827;&#34892;&#25968;&#25454;&#39044;&#21462;&#12290;&#25105;&#20204;&#23545;&#20843;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#23454;&#35777;&#32467;&#26524;&#39564;&#35777;&#20102;SGDP&#22312;&#21629;&#20013;&#29575;&#26041;&#38754;&#20248;&#20110;SOTA&#26041;&#27861;6.21&#65285;&#65292;&#25928;&#26524;&#20063;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data prefetching is important for storage system optimization and access performance improvement. Traditional prefetchers work well for mining access patterns of sequential logical block address (LBA) but cannot handle complex non-sequential patterns that commonly exist in real-world applications. The state-of-the-art (SOTA) learning-based prefetchers cover more LBA accesses. However, they do not adequately consider the spatial interdependencies between LBA deltas, which leads to limited performance and robustness. This paper proposes a novel Stream-Graph neural network-based Data Prefetcher (SGDP). Specifically, SGDP models LBA delta streams using a weighted directed graph structure to represent interactive relations among LBA deltas and further extracts hybrid features by graph neural networks for data prefetching. We conduct extensive experiments on eight real-world datasets. Empirical results verify that SGDP outperforms the SOTA methods in terms of the hit ratio by 6.21%, the effe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#39044;&#27979;&#21453;&#32534;&#35793;&#20195;&#30721;&#20013;&#21464;&#37327;&#21517;&#31216;&#21644;&#31867;&#22411;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#20854;&#20013;&#25552;&#21040;&#22522;&#20110;Transformer&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;DIRTY&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#21517;&#31216;&#21644;&#31867;&#22411;&#25552;&#21462;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03854</link><description>&lt;p&gt;
&#37325;&#26032;&#25506;&#35752;&#28145;&#24230;&#23398;&#20064;&#22312;&#21464;&#37327;&#31867;&#22411;&#24674;&#22797;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Revisiting Deep Learning for Variable Type Recovery. (arXiv:2304.03854v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03854
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#39044;&#27979;&#21453;&#32534;&#35793;&#20195;&#30721;&#20013;&#21464;&#37327;&#21517;&#31216;&#21644;&#31867;&#22411;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#20854;&#20013;&#25552;&#21040;&#22522;&#20110;Transformer&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;DIRTY&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#21517;&#31216;&#21644;&#31867;&#22411;&#25552;&#21462;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36870;&#21521;&#24037;&#31243;&#12289;&#24694;&#24847;&#36719;&#20214;&#20998;&#26512;&#21644;&#36719;&#20214;&#31995;&#32479;&#32500;&#25252;&#20013;&#65292;&#24050;&#32534;&#35793;&#30340;&#20108;&#36827;&#21046;&#21487;&#25191;&#34892;&#25991;&#20214;&#24448;&#24448;&#26159;&#21807;&#19968;&#21487;&#29992;&#30340;&#24037;&#20214;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#20687;&#21464;&#37327;&#31867;&#22411;&#36825;&#26679;&#30340;&#35821;&#20041;&#20449;&#24687;&#20351;&#24471;&#29702;&#35299;&#20108;&#36827;&#21046;&#25991;&#20214;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#25913;&#36827;&#20108;&#36827;&#21046;&#25991;&#20214;&#30340;&#21487;&#29702;&#35299;&#24615;&#65292;&#36817;&#26399;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#39044;&#27979;&#21407;&#22987;&#28304;&#20195;&#30721;&#20013;&#21253;&#21547;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;Chen&#31561;&#20154;&#23454;&#29616;&#20102;DIRTY&#65292;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#33021;&#22815;&#21033;&#29992;&#21453;&#32534;&#35793;&#22120;&#36755;&#20986;&#26631;&#35760;&#21644;&#21464;&#37327;&#22823;&#23567;&#20449;&#24687;&#65292;&#22686;&#24378;&#21453;&#32534;&#35793;&#20195;&#30721;&#30340;&#21464;&#37327;&#21517;&#31216;&#21644;&#31867;&#22411;&#12290;&#30456;&#27604;&#29616;&#26377;&#30340;&#38745;&#24577;&#20998;&#26512;&#21644;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#25216;&#26415;&#65292;Chen&#31561;&#20154;&#33021;&#22815;&#35777;&#26126;DIRTY&#22312;Hex-Rays&#21453;&#32534;&#35793;&#22120;&#36755;&#20986;&#30340;&#21517;&#31216;&#21644;&#31867;&#22411;&#25552;&#21462;&#20934;&#30830;&#24615;&#19978;&#26377;&#22823;&#24133;&#25552;&#21319;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#30001;&#24320;&#28304;Ghidra&#21453;&#32534;&#35793;&#22120;&#20135;&#29983;&#30340;&#25968;&#25454;&#38598;&#19978;&#37325;&#26032;&#35757;&#32451;DIRTY&#27169;&#22411;&#26469;&#25193;&#23637;&#21407;&#22987;DIRTY&#32467;&#26524;&#12290;&#23613;&#31649;Chen&#31561;&#20154;&#24471;&#20986;&#32467;&#35770;&#65292;
&lt;/p&gt;
&lt;p&gt;
Compiled binary executables are often the only available artifact in reverse engineering, malware analysis, and software systems maintenance. Unfortunately, the lack of semantic information like variable types makes comprehending binaries difficult. In efforts to improve the comprehensibility of binaries, researchers have recently used machine learning techniques to predict semantic information contained in the original source code. Chen et al. implemented DIRTY, a Transformer-based Encoder-Decoder architecture capable of augmenting decompiled code with variable names and types by leveraging decompiler output tokens and variable size information. Chen et al. were able to demonstrate a substantial increase in name and type extraction accuracy on Hex-Rays decompiler outputs compared to existing static analysis and AI-based techniques. We extend the original DIRTY results by re-training the DIRTY model on a dataset produced by the open-source Ghidra decompiler. Although Chen et al. conclu
&lt;/p&gt;</description></item><item><title>StepMix&#26159;&#19968;&#20010;&#29992;&#20110;&#22806;&#37096;&#21464;&#37327;&#24191;&#20041;&#28151;&#21512;&#27169;&#22411;&#30340;&#20266;&#20284;&#28982;&#20272;&#35745;&#30340;Python&#21253;&#65292;&#25552;&#20379;&#20102;&#21333;&#27493;&#21644;&#36880;&#27493;&#20272;&#35745;&#26041;&#27861;&#65292;&#24110;&#21161;&#20174;&#19994;&#20154;&#21592;&#36827;&#34892;&#27169;&#22411;&#20272;&#35745;&#12289;&#36873;&#25321;&#21644;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2304.03853</link><description>&lt;p&gt;
StepMix: &#19968;&#20010;&#29992;&#20110;&#22806;&#37096;&#21464;&#37327;&#24191;&#20041;&#28151;&#21512;&#27169;&#22411;&#30340;&#20266;&#20284;&#28982;&#20272;&#35745;&#30340;Python&#21253;
&lt;/p&gt;
&lt;p&gt;
StepMix: A Python Package for Pseudo-Likelihood Estimation of Generalized Mixture Models with External Variables. (arXiv:2304.03853v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03853
&lt;/p&gt;
&lt;p&gt;
StepMix&#26159;&#19968;&#20010;&#29992;&#20110;&#22806;&#37096;&#21464;&#37327;&#24191;&#20041;&#28151;&#21512;&#27169;&#22411;&#30340;&#20266;&#20284;&#28982;&#20272;&#35745;&#30340;Python&#21253;&#65292;&#25552;&#20379;&#20102;&#21333;&#27493;&#21644;&#36880;&#27493;&#20272;&#35745;&#26041;&#27861;&#65292;&#24110;&#21161;&#20174;&#19994;&#20154;&#21592;&#36827;&#34892;&#27169;&#22411;&#20272;&#35745;&#12289;&#36873;&#25321;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
StepMix&#26159;&#19968;&#20010;&#29992;&#20110;&#24191;&#20041;&#26377;&#38480;&#28151;&#21512;&#27169;&#22411;(&#28508;&#22312;&#21078;&#38754;&#21644;&#28508;&#22312;&#31867;&#20998;&#26512;)&#19982;&#22806;&#37096;&#21464;&#37327;(&#21327;&#21464;&#37327;&#21644;&#36828;&#31243;&#32467;&#26524;)&#30340;&#20266;&#20284;&#28982;&#20272;&#35745;(&#21333;&#27493;&#12289;&#20004;&#27493;&#21644;&#19977;&#27493;&#26041;&#27861;)&#30340;&#24320;&#28304;&#36719;&#20214;&#21253;&#12290;&#22312;&#35768;&#22810;&#31038;&#20250;&#31185;&#23398;&#30340;&#24212;&#29992;&#20013;&#65292;&#20027;&#35201;&#30446;&#26631;&#19981;&#20165;&#26159;&#23558;&#20010;&#20307;&#32858;&#31867;&#25104;&#28508;&#22312;&#31867;&#21035;&#65292;&#36824;&#21253;&#25324;&#20351;&#29992;&#36825;&#20123;&#31867;&#21035;&#26469;&#24320;&#21457;&#26356;&#22797;&#26434;&#30340;&#32479;&#35745;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20998;&#20026;&#19968;&#20010;&#23558;&#28508;&#22312;&#31867;&#21035;&#19982;&#35266;&#23519;&#25351;&#26631;&#30456;&#20851;&#32852;&#30340;&#27979;&#37327;&#27169;&#22411;&#21644;&#19968;&#20010;&#23558;&#21327;&#21464;&#37327;&#21644;&#32467;&#26524;&#21464;&#37327;&#19982;&#28508;&#22312;&#31867;&#21035;&#30456;&#20851;&#32852;&#30340;&#32467;&#26500;&#27169;&#22411;&#12290;&#27979;&#37327;&#21644;&#32467;&#26500;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#25152;&#35859;&#30340;&#19968;&#27493;&#27861;&#20849;&#21516;&#20272;&#35745;&#65292;&#20063;&#21487;&#20197;&#20351;&#29992;&#36880;&#27493;&#26041;&#27861;&#36880;&#27493;&#20272;&#35745;&#65292;&#23545;&#20110;&#20174;&#19994;&#20154;&#21592;&#26469;&#35828;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#20272;&#35745;&#28508;&#22312;&#31867;&#21035;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;&#38500;&#20102;&#19968;&#27493;&#27861;&#65292;StepMix&#36824;&#23454;&#29616;&#20102;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#26368;&#37325;&#35201;&#30340;&#36880;&#27493;&#20272;&#35745;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#65292;&#26041;&#20415;&#27169;&#22411;&#30340;&#20272;&#35745;&#12289;&#36873;&#25321;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
StepMix is an open-source software package for the pseudo-likelihood estimation (one-, two- and three-step approaches) of generalized finite mixture models (latent profile and latent class analysis) with external variables (covariates and distal outcomes). In many applications in social sciences, the main objective is not only to cluster individuals into latent classes, but also to use these classes to develop more complex statistical models. These models generally divide into a measurement model that relates the latent classes to observed indicators, and a structural model that relates covariates and outcome variables to the latent classes. The measurement and structural models can be estimated jointly using the so-called one-step approach or sequentially using stepwise methods, which present significant advantages for practitioners regarding the interpretability of the estimated latent classes. In addition to the one-step approach, StepMix implements the most important stepwise estim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#25512;&#29702;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#27979;&#35797;&#25512;&#29702;&#22312;&#35757;&#32451;&#25968;&#25454;&#30001;&#30456;&#20114;&#24433;&#21709;&#24378;&#28872;&#30340;&#23616;&#37096;&#21464;&#37327;&#38598;&#32676;&#32452;&#25104;&#26102;&#26159;&#21542;&#26377;&#25928;&#12290;&#36890;&#36807;&#19968;&#27493;&#27493;&#30340;&#25512;&#29702;&#65292;&#33021;&#22815;&#23558;&#20934;&#30830;&#30340;&#23616;&#37096;&#25512;&#29702;&#38142;&#25509;&#22312;&#19968;&#36215;&#65292;&#20197;&#20272;&#31639;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#21516;&#26102;&#35266;&#23519;&#21040;&#30340;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2304.03843</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#35201;&#36880;&#27493;&#24605;&#32771;&#65311;&#25512;&#29702;&#28304;&#20110;&#32463;&#39564;&#30340;&#23616;&#37096;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Why think step-by-step? Reasoning emerges from the locality of experience. (arXiv:2304.03843v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#25512;&#29702;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#27979;&#35797;&#25512;&#29702;&#22312;&#35757;&#32451;&#25968;&#25454;&#30001;&#30456;&#20114;&#24433;&#21709;&#24378;&#28872;&#30340;&#23616;&#37096;&#21464;&#37327;&#38598;&#32676;&#32452;&#25104;&#26102;&#26159;&#21542;&#26377;&#25928;&#12290;&#36890;&#36807;&#19968;&#27493;&#27493;&#30340;&#25512;&#29702;&#65292;&#33021;&#22815;&#23558;&#20934;&#30830;&#30340;&#23616;&#37096;&#25512;&#29702;&#38142;&#25509;&#22312;&#19968;&#36215;&#65292;&#20197;&#20272;&#31639;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#21516;&#26102;&#35266;&#23519;&#21040;&#30340;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26377;&#30528;&#24378;&#22823;&#32780;&#31070;&#31192;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#32431;&#31929;&#30340;&#24605;&#32500;&#27493;&#39588;&#65292;&#25105;&#20204;&#21487;&#20197;&#25512;&#29702;&#20986;&#25105;&#20204;&#26080;&#27861;&#30452;&#25509;&#24471;&#20986;&#30340;&#25512;&#35770; - &#23613;&#31649;&#25105;&#20204;&#20174;&#19990;&#30028;&#19978;&#27809;&#26377;&#24471;&#21040;&#20219;&#20309;&#39069;&#22806;&#25968;&#25454;&#12290;&#21516;&#26679;&#22320;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#19968;&#27493;&#27493;&#30340;&#25512;&#29702;&#65292;&#22312;&#22238;&#31572;&#38382;&#39064;&#20043;&#21069;&#29983;&#25104;&#20013;&#38388;&#27493;&#39588;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#23436;&#25104;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#25512;&#29702;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#27979;&#35797;&#25512;&#29702;&#22312;&#35757;&#32451;&#25968;&#25454;&#30001;&#30456;&#20114;&#24433;&#21709;&#24378;&#28872;&#30340;&#23616;&#37096;&#21464;&#37327;&#38598;&#32676;&#32452;&#25104;&#26102;&#26159;&#21542;&#26377;&#25928;&#12290;&#36825;&#20123;&#35757;&#32451;&#26465;&#20214;&#33021;&#22815;&#23558;&#20934;&#30830;&#30340;&#23616;&#37096;&#25512;&#29702;&#38142;&#25509;&#22312;&#19968;&#36215;&#65292;&#20197;&#20272;&#31639;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#21516;&#26102;&#35266;&#23519;&#21040;&#30340;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#20351;&#29992;&#36125;&#21494;&#26031;&#32593;&#32476;&#23450;&#20041;&#30340;&#32852;&#21512;&#20998;&#24067;&#30340;&#26679;&#21697;&#23545;&#33258;&#22238;&#24402;&#21464;&#21387;&#22120;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#27599;&#20010;&#26679;&#21697;&#21482;&#21253;&#25324;&#20854;&#20013;&#30340;&#19968;&#37096;&#20998;&#21464;&#37327;&#12290;&#25105;&#20204;&#27604;&#36739;&#20351;&#29992;&#25512;&#29702;&#29983;&#25104;&#30340;&#21464;&#37327;&#23376;&#38598;&#19982;&#20351;&#29992;&#23436;&#25972;&#38598;&#21512;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans have a powerful and mysterious capacity to reason. By working through a series of purely mental steps, we can make inferences we would not be capable of making directly -- despite that fact that we get no additional data from the world. Similarly, large language models can perform better at complex tasks through chain-of-thought reasoning, where they generate intermediate steps before answering a question. We use language models to investigate the questions of when and why reasoning is helpful, testing the hypothesis that reasoning is effective when training data consisting of local clusters of variables that influence each other strongly. These training conditions enable the chaining of accurate local inferences in order to estimate relationships between variables that were not seen together in training. We train an autoregressive transformer on samples from joint distributions defined by Bayes nets, but only include a subset of all the variables in each sample. We compare lang
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#27809;&#26377;&#36523;&#20221;&#27880;&#37322;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#20154;&#33080;&#35782;&#21035;&#23884;&#20837;&#21521;&#37327;&#20316;&#20026;&#36523;&#20221;&#26631;&#35782;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20154;&#33080;&#27169;&#22411;&#30340;&#36523;&#20221;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03838</link><description>&lt;p&gt;
&#25552;&#39640;&#20154;&#33080;&#27169;&#22411;&#30340;&#36523;&#20221;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Identity-Robustness for Face Models. (arXiv:2304.03838v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03838
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#27809;&#26377;&#36523;&#20221;&#27880;&#37322;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#20154;&#33080;&#35782;&#21035;&#23884;&#20837;&#21521;&#37327;&#20316;&#20026;&#36523;&#20221;&#26631;&#35782;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20154;&#33080;&#27169;&#22411;&#30340;&#36523;&#20221;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20154;&#20204;&#20173;&#28982;&#25285;&#24515;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#23398;&#20064;&#21040;&#24555;&#25463;&#26041;&#24335;&#65292;&#24182;&#19988;&#32570;&#20047;&#23545;&#26080;&#20851;&#28151;&#28102;&#22240;&#32032;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#30452;&#25509;&#35757;&#32451;&#20110;&#20154;&#33080;&#19978;&#30340;&#27169;&#22411;&#20013;&#65292;&#19968;&#20010;&#25935;&#24863;&#30340;&#28151;&#28102;&#22240;&#32032;&#26159;&#20154;&#30340;&#36523;&#20221;&#12290;&#35768;&#22810;&#19982;&#20154;&#33080;&#30456;&#20851;&#30340;&#20219;&#21153;&#29702;&#24819;&#24773;&#20917;&#19979;&#24212;&#35813;&#26159;&#19982;&#36523;&#20221;&#26080;&#20851;&#30340;&#65292;&#24182;&#22312;&#19981;&#21516;&#20010;&#20307;&#20043;&#38388;&#34920;&#29616;&#19968;&#33268;&#65288;&#21363;&#20844;&#24179;&#65289;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#24378;&#21046;&#25191;&#34892;&#36825;&#31181;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#22343;&#21248;&#24615;&#26159;&#24230;&#37327;&#21644;&#23454;&#26045;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#20551;&#35774;&#21487;&#20197;&#22312;&#35268;&#27169;&#19978;&#33719;&#21462;&#19982;&#36523;&#20221;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#20197;&#21450;&#25910;&#38598;&#27492;&#31867;&#20449;&#24687;&#30340;&#25104;&#26412;&#65292;&#36825;&#36890;&#24120;&#19981;&#26159;&#24773;&#20917;&#65292;&#22823;&#22810;&#25968;&#20154;&#33080;&#25968;&#25454;&#38598;&#21482;&#21253;&#21547;&#36755;&#20837;&#22270;&#20687;&#21450;&#20854;&#30456;&#24212;&#30340;&#20219;&#21153;&#26631;&#31614;&#12290;&#22240;&#27492;&#65292;&#26080;&#38656;&#27492;&#31867;&#27880;&#37322;&#21363;&#21487;&#25552;&#39640;&#36523;&#20221;&#30456;&#20851;&#40065;&#26834;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#35752;&#20351;&#29992;&#20154;&#33080;&#35782;&#21035;&#23884;&#20837;&#21521;&#37327;&#20316;&#20026;&#36523;&#20221;&#26631;&#35782;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20197;&#25191;&#34892;&#36825;&#31181;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of deep-learning models in many tasks, there have been concerns about such models learning shortcuts, and their lack of robustness to irrelevant confounders. When it comes to models directly trained on human faces, a sensitive confounder is that of human identities. Many face-related tasks should ideally be identity-independent, and perform uniformly across different individuals (i.e. be fair). One way to measure and enforce such robustness and performance uniformity is through enforcing it during training, assuming identity-related information is available at scale. However, due to privacy concerns and also the cost of collecting such information, this is often not the case, and most face datasets simply contain input images and their corresponding task-related labels. Thus, improving identity-related robustness without the need for such annotations is of great importance. Here, we explore using face-recognition embedding vectors, as proxies for identities, to enfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#23398;&#20064;&#28436;&#31034;&#20013;&#34892;&#20026;&#31354;&#38388;&#19981;&#21305;&#37197;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#20854;&#20182;&#24418;&#24577;&#26377;&#26174;&#30528;&#19981;&#21516;&#30340;&#20195;&#29702;&#30340;&#28436;&#31034;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#27425;&#20248;&#28436;&#31034;&#20013;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2304.03833</link><description>&lt;p&gt;
&#23398;&#20064;&#28436;&#31034;&#20013;&#30340;&#34892;&#20026;&#31354;&#38388;&#21305;&#37197;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Bridging Action Space Mismatch in Learning from Demonstrations. (arXiv:2304.03833v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#23398;&#20064;&#28436;&#31034;&#20013;&#34892;&#20026;&#31354;&#38388;&#19981;&#21305;&#37197;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#20854;&#20182;&#24418;&#24577;&#26377;&#26174;&#30528;&#19981;&#21516;&#30340;&#20195;&#29702;&#30340;&#28436;&#31034;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#27425;&#20248;&#28436;&#31034;&#20013;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#28436;&#31034;&#30340;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#25945;&#24072;&#28436;&#31034;&#36798;&#21040;&#29305;&#23450;&#30340;&#30446;&#30340;&#65292;&#20294;&#26159;&#24403;&#25945;&#24072;&#29992;&#20110;&#28436;&#31034;&#30340;&#34892;&#20026;&#31354;&#38388;&#19982;&#23398;&#29983;&#19981;&#21516;&#26102;&#65292;&#23601;&#20250;&#20986;&#29616;&#34892;&#20026;&#31354;&#38388;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#21363;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#24418;&#24577;&#36866;&#24212;&#65288;MAIL&#65289;&#65292;&#26088;&#22312;&#24110;&#21161;&#23398;&#29983;&#20195;&#29702;&#26681;&#25454;&#20854;&#20182;&#24418;&#24577;&#26174;&#30528;&#19981;&#21516;&#30340;&#20195;&#29702;&#30340;&#28436;&#31034;&#36827;&#34892;&#35757;&#32451;&#12290; MAIL&#21487;&#20197;&#20174;&#27425;&#20248;&#28436;&#31034;&#20013;&#23398;&#20064;&#65292;&#21482;&#35201;&#36825;&#20123;&#28436;&#31034;&#25552;&#20379;&#20102;&#19968;&#20123;&#25351;&#24341;&#65292;&#20197;&#36798;&#21040;&#39044;&#26399;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20316;&#32773;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23478;&#24237;&#24067;&#26009;&#25805;&#20316;&#20219;&#21153;&#19978;&#28436;&#31034;&#20102;MAIL&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;DRY CLOTH&#20219;&#21153; - &#22312;&#19977;&#32500;&#31354;&#38388;&#20013;&#24102;&#26377;&#38556;&#30861;&#29289;&#30340;&#24067;&#26009;&#25805;&#20316;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from demonstrations (LfD) methods guide learning agents to a desired solution using demonstrations from a teacher. While some LfD methods can handle small mismatches in the action spaces of the teacher and student, here we address the case where the teacher demonstrates the task in an action space that can be substantially different from that of the student -- thereby inducing a large action space mismatch. We bridge this gap with a framework, Morphological Adaptation in Imitation Learning (MAIL), that allows training an agent from demonstrations by other agents with significantly different morphologies (from the student or each other). MAIL is able to learn from suboptimal demonstrations, so long as they provide some guidance towards a desired solution. We demonstrate MAIL on challenging household cloth manipulation tasks and introduce a new DRY CLOTH task -- cloth manipulation in 3D task with obstacles. In these tasks, we train a visual control policy for a robot with one en
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; NL2Fix &#38382;&#39064;&#65292;&#21363;&#23558;&#20195;&#30721;&#26356;&#25913;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#32763;&#35793;&#25104;&#27491;&#30830;&#30340;&#20195;&#30721;&#20462;&#22797;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; Defects4J-NL2Fix&#65292;&#36825;&#26159;&#19968;&#20010;&#30001; 283 &#20010; Java &#31243;&#24207;&#32452;&#25104;&#30340;&#27969;&#34892; Defects4J &#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.03816</link><description>&lt;p&gt;
&#20174;&#33258;&#28982;&#35821;&#35328;&#38169;&#35823;&#25551;&#36848;&#20013;&#29983;&#25104;&#21151;&#33021;&#19978;&#27491;&#30830;&#30340;&#20195;&#30721;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Towards Generating Functionally Correct Code Edits from Natural Language Issue Descriptions. (arXiv:2304.03816v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; NL2Fix &#38382;&#39064;&#65292;&#21363;&#23558;&#20195;&#30721;&#26356;&#25913;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#32763;&#35793;&#25104;&#27491;&#30830;&#30340;&#20195;&#30721;&#20462;&#22797;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; Defects4J-NL2Fix&#65292;&#36825;&#26159;&#19968;&#20010;&#30001; 283 &#20010; Java &#31243;&#24207;&#32452;&#25104;&#30340;&#27969;&#34892; Defects4J &#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914; OpenAI &#30340; Codex&#65292;&#22312;&#21508;&#31181;&#32534;&#31243;&#20219;&#21153;&#20013;&#24050;&#32463;&#23637;&#31034;&#20102;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#29983;&#25104;&#20195;&#30721;&#30340;&#28508;&#21147;&#12290;&#26368;&#36817;&#20986;&#29616;&#20102;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272; LLM &#29983;&#25104;&#19982;&#19968;&#32452;&#38544;&#34255;&#27979;&#35797;&#29992;&#20363;&#30456;&#23545;&#24212;&#30340;&#33258;&#28982;&#35821;&#35328;&#24847;&#22270;&#30340;&#21151;&#33021;&#19978;&#27491;&#30830;&#30340;&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;&#36825;&#20351;&#24471;&#30740;&#31350;&#31038;&#21306;&#33021;&#22815;&#30830;&#23450; LLM &#33021;&#21147;&#30340;&#26174;&#33879;&#21644;&#21487;&#37325;&#22797;&#30340;&#36827;&#23637;&#12290;&#20294;&#26159;&#65292;&#30446;&#21069;&#32570;&#20047;&#22522;&#20934;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272; LLM &#26681;&#25454;&#39044;&#26399;&#26356;&#25913;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#29983;&#25104;&#21151;&#33021;&#24615;&#27491;&#30830;&#30340;&#20195;&#30721;&#32534;&#36753;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#38382;&#39064; NL2Fix&#65292;&#21363;&#23558;&#20195;&#30721;&#26356;&#25913;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65288;&#21363;&#24211;&#20013;&#38382;&#39064;&#25253;&#21578;&#20013;&#25551;&#36848;&#30340;&#38169;&#35823;&#20462;&#22797;&#65289;&#32763;&#35793;&#25104;&#27491;&#30830;&#30340;&#20195;&#30721;&#20462;&#22797;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; Defects4J-NL2Fix&#65292;&#36825;&#26159;&#19968;&#20010;&#30001; 283 &#20010; Java &#31243;&#24207;&#32452;&#25104;&#30340;&#27969;&#34892; Defects4J &#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as OpenAI's Codex, have demonstrated their potential to generate code from natural language descriptions across a wide range of programming tasks. Several benchmarks have recently emerged to evaluate the ability of LLMs to generate functionally correct code from natural language intent with respect to a set of hidden test cases. This has enabled the research community to identify significant and reproducible advancements in LLM capabilities. However, there is currently a lack of benchmark datasets for assessing the ability of LLMs to generate functionally correct code edits based on natural language descriptions of intended changes. This paper aims to address this gap by motivating the problem NL2Fix of translating natural language descriptions of code changes (namely bug fixes described in Issue reports in repositories) into correct code fixes. To this end, we introduce Defects4J-NL2Fix, a dataset of 283 Java programs from the popular Defects4J datas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#23454;&#29616;&#21516;&#24577;&#21152;&#23494;&#25216;&#26415;&#19979;&#38544;&#31169;&#20445;&#25252;&#30340;CNN&#35757;&#32451;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#36716;&#25442;&#24605;&#24819;&#21644;&#26356;&#24555;&#30340;&#26799;&#24230;&#21464;&#20307;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.03807</link><description>&lt;p&gt;
&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#30340;CNN&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving CNN Training with Transfer Learning. (arXiv:2304.03807v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#23454;&#29616;&#21516;&#24577;&#21152;&#23494;&#25216;&#26415;&#19979;&#38544;&#31169;&#20445;&#25252;&#30340;CNN&#35757;&#32451;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#36716;&#25442;&#24605;&#24819;&#21644;&#26356;&#24555;&#30340;&#26799;&#24230;&#21464;&#20307;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#30340;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#24050;&#32463;&#24471;&#21040;&#24456;&#22909;&#30340;&#30740;&#31350;&#65292;&#21516;&#26102;&#20445;&#25345;&#21516;&#24577;CNN&#35757;&#32451;&#20173;&#28982;&#26159;&#19968;&#39033;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#23454;&#29616;&#22522;&#20110;&#21516;&#24577;&#21152;&#23494;&#25216;&#26415;&#30340;&#38544;&#31169;&#20445;&#25252;CNN&#35757;&#32451;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#27425;&#25104;&#21151;&#31361;&#30772;&#36825;&#20010;&#38590;&#39064;&#65292;&#20197;&#21069;&#27809;&#26377;&#20219;&#20309;&#24037;&#20316;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#12290;&#37319;&#29992;&#20102;&#20960;&#31181;&#25216;&#26415;&#65306;&#65288;1&#65289;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#65292;&#21487;&#20197;&#23558;&#38544;&#31169;&#20445;&#25252;&#30340;CNN&#35757;&#32451;&#31616;&#21270;&#20026;&#21516;&#24577;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#29978;&#33267;&#26159;&#22810;&#31867;&#36923;&#36753;&#22238;&#24402;&#65288;MLR&#65289;&#35757;&#32451;&#65307;&#65288;2&#65289;&#36890;&#36807;&#26356;&#24555;&#30340;&#26799;&#24230;&#21464;&#20307;$\texttt{Quadratic Gradient}$&#65292;&#24212;&#29992;&#20110;MLR&#30340;&#22686;&#24378;&#26799;&#24230;&#26041;&#27861;&#65292;&#22312;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65307;&#65288;3&#65289;&#25105;&#20204;&#37319;&#29992;&#25968;&#23398;&#20013;&#30340;&#21464;&#25442;&#24605;&#24819;&#65292;&#23558;&#21152;&#23494;&#22495;&#20013;&#30340;&#36817;&#20284;Softmax&#20989;&#25968;&#36716;&#25442;&#25104;&#24050;&#32463;&#30740;&#31350;&#36807;&#30340;&#36924;&#36817;&#26041;&#27861;&#65292;&#20174;&#32780;&#24471;&#21040;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy-preserving nerual network inference has been well studied while homomorphic CNN training still remains an open challenging task. In this paper, we present a practical solution to implement privacy-preserving CNN training based on mere Homomorphic Encryption (HE) technique. To our best knowledge, this is the first attempt successfully to crack this nut and no work ever before has achieved this goal. Several techniques combine to make it done: (1) with transfer learning, privacy-preserving CNN training can be reduced to homomorphic neural network training, or even multiclass logistic regression (MLR) training; (2) via a faster gradient variant called $\texttt{Quadratic Gradient}$, an enhanced gradient method for MLR with a state-of-the-art performance in converge speed is applied in this work to achieve high performance; (3) we employ the thought of transformation in mathematics to transform approximating Softmax function in encryption domain to the well-studied approximation of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;ABC-GAN&#33539;&#24335;&#65292;&#36890;&#36807;ABC&#21512;&#24182;&#20219;&#20309;&#21487;&#29992;&#30340;&#20027;&#35266;&#30693;&#35782;&#26469;&#21327;&#21161;GAN&#65292;&#28040;&#38500;&#20102;&#27169;&#22411;&#20013;&#20284;&#28982;&#24615;&#38169;&#35823;&#35268;&#33539;&#30340;&#38656;&#27714;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#20010;&#22312;&#20302;&#25968;&#25454;&#37327;&#24773;&#20917;&#19979;&#36816;&#34892;&#33391;&#22909;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.03805</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#32416;&#27491;&#27169;&#22411;&#38169;&#35823;&#35268;&#33539;
&lt;/p&gt;
&lt;p&gt;
Correcting Model Misspecification via Generative Adversarial Networks. (arXiv:2304.03805v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;ABC-GAN&#33539;&#24335;&#65292;&#36890;&#36807;ABC&#21512;&#24182;&#20219;&#20309;&#21487;&#29992;&#30340;&#20027;&#35266;&#30693;&#35782;&#26469;&#21327;&#21161;GAN&#65292;&#28040;&#38500;&#20102;&#27169;&#22411;&#20013;&#20284;&#28982;&#24615;&#38169;&#35823;&#35268;&#33539;&#30340;&#38656;&#27714;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#20010;&#22312;&#20302;&#25968;&#25454;&#37327;&#24773;&#20917;&#19979;&#36816;&#34892;&#33391;&#22909;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24120;&#24120;&#22312;&#20284;&#28982;&#24615;&#19978;&#23384;&#22312;&#38169;&#35823;&#35268;&#33539;&#65292;&#36825;&#23548;&#33268;&#39044;&#27979;&#30340;&#40065;&#26834;&#24615;&#19981;&#36275;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#29992;&#20110;&#32416;&#27491;&#22810;&#20010;&#33539;&#24335;&#20013;&#30340;&#20284;&#28982;&#24615;&#38169;&#35823;&#35268;&#33539;&#65292;&#24182;&#27979;&#35797;&#27169;&#22411;&#28040;&#38500;&#38169;&#35823;&#35268;&#33539;&#30340;&#33021;&#21147;&#12290;&#8220;ABC-GAN&#8221;&#26694;&#26550;&#26159;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#24314;&#27169;&#33539;&#24335;&#65292;&#23427;&#32467;&#21512;&#20102;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21644;&#36817;&#20284;&#36125;&#21494;&#26031;&#35745;&#31639;&#65288;ABC&#65289;&#12290;&#36825;&#31181;&#26032;&#30340;&#33539;&#24335;&#36890;&#36807;ABC&#21512;&#24182;&#20219;&#20309;&#21487;&#29992;&#20110;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#20027;&#35266;&#30693;&#35782;&#65292;&#20316;&#20026;&#19968;&#20010;&#35268;&#33539;&#21270;&#39033;&#26469;&#21327;&#21161;&#29616;&#26377;&#30340;GAN&#65292;&#20174;&#32780;&#20135;&#29983;&#19968;&#20010;&#22312;&#20302;&#25968;&#25454;&#37327;&#24773;&#20917;&#19979;&#36816;&#34892;&#33391;&#22909;&#12289;&#37096;&#20998;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;&#19982;&#20219;&#20309;&#36125;&#21494;&#26031;&#20998;&#26512;&#19981;&#21516;&#65292;&#29983;&#25104;&#22120;&#21487;&#20197;&#21464;&#24471;&#20219;&#24847;&#22797;&#26434;&#65292;&#22240;&#27492;ABC-GAN&#28040;&#38500;&#20102;&#23545;&#25688;&#35201;&#32479;&#35745;&#21644;&#36317;&#31163;&#24230;&#37327;&#30340;&#38656;&#27714;&#65292;&#22240;&#20026;&#37492;&#21035;&#22120;&#38544;&#24615;&#22320;&#23398;&#20064;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models are often misspecified in the likelihood, which leads to a lack of robustness in the predictions. In this paper, we introduce a framework for correcting likelihood misspecifications in several paradigm agnostic noisy prior models and test the model's ability to remove the misspecification. The "ABC-GAN" framework introduced is a novel generative modeling paradigm, which combines Generative Adversarial Networks (GANs) and Approximate Bayesian Computation (ABC). This new paradigm assists the existing GANs by incorporating any subjective knowledge available about the modeling process via ABC, as a regularizer, resulting in a partially interpretable model that operates well under low data regimes. At the same time, unlike any Bayesian analysis, the explicit knowledge need not be perfect, since the generator in the GAN can be made arbitrarily complex. ABC-GAN eliminates the need for summary statistics and distance metrics as the discriminator implicitly learns them a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ChiroDiff"&#30340;&#27169;&#22411;&#31867;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;Denoising Diffusion Probabilistic Models&#65288;DDPMs&#65289;&#35299;&#20915;&#20102;&#25163;&#20889;&#25968;&#25454;&#24314;&#27169;&#36807;&#31243;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#23398;&#20064;&#25429;&#25417;&#25972;&#20307;&#27010;&#24565;&#65292;&#22312;&#26356;&#39640;&#30340;&#26102;&#38388;&#37319;&#26679;&#29575;&#19979;&#20445;&#25345;&#24377;&#24615;&#65292;&#24182;&#20855;&#26377;&#35768;&#22810;&#19979;&#28216;&#23454;&#29992;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2304.03785</link><description>&lt;p&gt;
ChiroDiff: &#22522;&#20110;&#24357;&#25955;&#27169;&#22411;&#30340;&#25163;&#20889;&#25968;&#25454;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
ChiroDiff: Modelling chirographic data with Diffusion Models. (arXiv:2304.03785v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ChiroDiff"&#30340;&#27169;&#22411;&#31867;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;Denoising Diffusion Probabilistic Models&#65288;DDPMs&#65289;&#35299;&#20915;&#20102;&#25163;&#20889;&#25968;&#25454;&#24314;&#27169;&#36807;&#31243;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#23398;&#20064;&#25429;&#25417;&#25972;&#20307;&#27010;&#24565;&#65292;&#22312;&#26356;&#39640;&#30340;&#26102;&#38388;&#37319;&#26679;&#29575;&#19979;&#20445;&#25345;&#24377;&#24615;&#65292;&#24182;&#20855;&#26377;&#35768;&#22810;&#19979;&#28216;&#23454;&#29992;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#22238;&#24402;&#20998;&#24067;&#65292;&#24050;&#32463;&#23454;&#29616;&#20102;&#23545;&#20110;&#36830;&#32493;&#26102;&#38388;&#20960;&#20309;&#32467;&#26500;&#65288;&#22914;&#25163;&#20889;&#12289;&#33609;&#22270;&#12289;&#22270;&#30011;&#31561;&#65289;&#30340;&#29983;&#25104;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20005;&#26684;&#26377;&#24207;&#30340;&#31163;&#25955;&#20998;&#35299;&#26080;&#27861;&#25429;&#25417;&#25163;&#20889;&#25968;&#25454;&#30340;&#20851;&#38190;&#23646;&#24615;&#8212;&#8212;&#30001;&#20110;&#21333;&#21521;&#21487;&#35265;&#24615;&#65288;&#22240;&#26524;&#24615;&#65289;&#65292;&#23427;&#26080;&#27861;&#24314;&#31435;&#25972;&#20307;&#30340;&#26102;&#38388;&#27010;&#24565;&#29702;&#35299;&#65292;&#22240;&#27492;&#22312;&#24314;&#27169;&#26102;&#23558;&#26102;&#38388;&#25968;&#25454;&#24314;&#27169;&#20026;&#22266;&#23450;&#37319;&#26679;&#29575;&#30340;&#31163;&#25955;&#26631;&#35760;&#24207;&#21015;&#65292;&#32780;&#26410;&#33021;&#25429;&#33719;&#30495;&#27491;&#30340;&#22522;&#30784;&#27010;&#24565;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#21435;&#22122;&#24357;&#25955;&#27010;&#29575;&#27169;&#22411;&#8221;&#65288;DDPMs&#65289;&#30340;&#24378;&#22823;&#27169;&#22411;&#31867;&#65292;&#19987;&#38376;&#20026;&#25163;&#20889;&#25968;&#25454;&#35299;&#20915;&#36825;&#20123;&#32570;&#38519;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21517;&#20026;&#8220;ChiroDiff&#8221;&#65292;&#26159;&#38750;&#33258;&#22238;&#24402;&#30340;&#65292;&#23398;&#20064;&#25429;&#25417;&#25972;&#20307;&#27010;&#24565;&#65292;&#22240;&#27492;&#22312;&#26356;&#39640;&#30340;&#26102;&#38388;&#37319;&#26679;&#29575;&#19979;&#20445;&#25345;&#24377;&#24615;&#33267;&#23569;&#26377;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35768;&#22810;&#37325;&#35201;&#30340;&#19979;&#28216;&#23454;&#29992;&#31243;&#24207;&#65288;&#22914;&#26465;&#20214;&#37319;&#26679;&#12289;&#21019;&#24847;&#28151;&#21512;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative modelling over continuous-time geometric constructs, a.k.a such as handwriting, sketches, drawings etc., have been accomplished through autoregressive distributions. Such strictly-ordered discrete factorization however falls short of capturing key properties of chirographic data -- it fails to build holistic understanding of the temporal concept due to one-way visibility (causality). Consequently, temporal data has been modelled as discrete token sequences of fixed sampling rate instead of capturing the true underlying concept. In this paper, we introduce a powerful model-class namely "Denoising Diffusion Probabilistic Models" or DDPMs for chirographic data that specifically addresses these flaws. Our model named "ChiroDiff", being non-autoregressive, learns to capture holistic concepts and therefore remains resilient to higher temporal sampling rate up to a good extent. Moreover, we show that many important downstream utilities (e.g. conditional sampling, creative mixing) c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#29983;&#25104;AI&#21512;&#25104;&#35270;&#39057;&#26469;&#21019;&#24314;&#22312;&#32447;&#25945;&#32946;&#20869;&#23481;&#30340;&#25928;&#29992;&#65292;&#20351;&#29992;&#28151;&#21512;&#26041;&#27861;&#21644;&#25104;&#24180;&#23398;&#20064;&#32773;&#36827;&#34892;&#23454;&#39564;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#21512;&#25104;&#23398;&#20064;&#35270;&#39057;&#23545;&#23398;&#20064;&#20869;&#23481;&#33719;&#21462;&#21644;&#23398;&#20064;&#20307;&#39564;&#26377;&#30528;&#31215;&#26497;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.03784</link><description>&lt;p&gt;
&#29983;&#25104;AI&#29992;&#20110;&#23398;&#20064;&#65306;&#30740;&#31350;&#21512;&#25104;&#23398;&#20064;&#35270;&#39057;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI for learning: Investigating the potential of synthetic learning videos. (arXiv:2304.03784v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#29983;&#25104;AI&#21512;&#25104;&#35270;&#39057;&#26469;&#21019;&#24314;&#22312;&#32447;&#25945;&#32946;&#20869;&#23481;&#30340;&#25928;&#29992;&#65292;&#20351;&#29992;&#28151;&#21512;&#26041;&#27861;&#21644;&#25104;&#24180;&#23398;&#20064;&#32773;&#36827;&#34892;&#23454;&#39564;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#21512;&#25104;&#23398;&#20064;&#35270;&#39057;&#23545;&#23398;&#20064;&#20869;&#23481;&#33719;&#21462;&#21644;&#23398;&#20064;&#20307;&#39564;&#26377;&#30528;&#31215;&#26497;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#24341;&#36215;&#20102;&#20840;&#29699;&#30340;&#20851;&#27880;&#12290;&#20687;Dalle-2&#21644;ChatGPT&#36825;&#26679;&#30340;&#24037;&#20855;&#34920;&#26126;&#65292;&#20197;&#21069;&#34987;&#35748;&#20026;&#36229;&#20986;&#20102;AI&#33021;&#21147;&#30340;&#20219;&#21153;&#29616;&#22312;&#21487;&#20197;&#20197;&#21508;&#31181;&#26032;&#26041;&#24335;&#22686;&#21152;&#21019;&#24847;&#23186;&#20307;&#30340;&#29983;&#20135;&#21147;&#65292;&#21253;&#25324;&#29983;&#25104;&#21512;&#25104;&#35270;&#39057;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#29983;&#25104;AI&#21512;&#25104;&#35270;&#39057;&#26469;&#21019;&#24314;&#22312;&#22312;&#32447;&#25945;&#32946;&#29615;&#22659;&#19979;&#21487;&#34892;&#30340;&#25945;&#32946;&#20869;&#23481;&#30340;&#25928;&#29992;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36824;&#27809;&#26377;&#30740;&#31350;&#35843;&#26597;AI&#29983;&#25104;&#30340;&#21512;&#25104;&#23186;&#20307;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#25945;&#32946;&#20215;&#20540;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#37319;&#29992;&#28151;&#21512;&#26041;&#27861;&#38543;&#26426;&#23558;&#25104;&#24180;&#23398;&#20064;&#32773;&#65288;n = 83&#65289;&#20998;&#37197;&#21040;&#20004;&#31181;&#24494;&#22411;&#23398;&#20064;&#26465;&#20214;&#20043;&#19968;&#65292;&#25910;&#38598;&#21069;&#21518;&#23398;&#20064;&#35780;&#20272;&#65292;&#24182;&#35843;&#26597;&#21442;&#19982;&#32773;&#30340;&#23398;&#20064;&#20307;&#39564;&#12290;&#25511;&#21046;&#32452;
&lt;/p&gt;
&lt;p&gt;
Recent advances in generative artificial intelligence (AI) have captured worldwide attention. Tools such as Dalle-2 and ChatGPT suggest that tasks previously thought to be beyond the capabilities of AI may now augment the productivity of creative media in various new ways, including through the generation of synthetic video. This research paper explores the utility of using AI-generated synthetic video to create viable educational content for online educational settings. To date, there is limited research investigating the real-world educational value of AI-generated synthetic media. To address this gap, we examined the impact of using AI-generated synthetic video in an online learning platform on both learners content acquisition and learning experience. We took a mixed-method approach, randomly assigning adult learners (n=83) into one of two micro-learning conditions, collecting pre- and post-learning assessments, and surveying participants on their learning experience. The control c
&lt;/p&gt;</description></item><item><title>AutoQNN&#26159;&#19968;&#31181;&#21487;&#33258;&#21160;&#37327;&#21270;DNN&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#21033;&#29992;&#19977;&#31181;&#25216;&#26415;&#23454;&#29616;&#20102;&#23545;&#36866;&#21512;&#19981;&#21516;&#23618;&#27425;&#30340;&#28151;&#21512;&#31934;&#24230;&#31574;&#30053;&#30340;&#25628;&#32034;&#65292;&#24182;&#21487;&#26377;&#25928;&#25552;&#39640;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03782</link><description>&lt;p&gt;
AutoQNN: &#19968;&#31181;&#33258;&#21160;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AutoQNN: An End-to-End Framework for Automatically Quantizing Neural Networks. (arXiv:2304.03782v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03782
&lt;/p&gt;
&lt;p&gt;
AutoQNN&#26159;&#19968;&#31181;&#21487;&#33258;&#21160;&#37327;&#21270;DNN&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#21033;&#29992;&#19977;&#31181;&#25216;&#26415;&#23454;&#29616;&#20102;&#23545;&#36866;&#21512;&#19981;&#21516;&#23618;&#27425;&#30340;&#28151;&#21512;&#31934;&#24230;&#31574;&#30053;&#30340;&#25628;&#32034;&#65292;&#24182;&#21487;&#26377;&#25928;&#25552;&#39640;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#36866;&#21512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21387;&#32553;&#30340;&#37327;&#21270;&#26041;&#26696;&#19982;&#28151;&#21512;&#31934;&#24230;&#31574;&#30053;&#26159;&#25552;&#39640;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#33258;&#21160;&#21387;&#32553;&#26041;&#27861;&#30340;&#24040;&#22823;&#25628;&#32034;&#31354;&#38388;&#23548;&#33268;&#20102;&#22823;&#37327;&#30340;&#35745;&#31639;&#39044;&#31639;&#65292;&#36825;&#20351;&#24471;&#33258;&#21160;&#36807;&#31243;&#38590;&#20197;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoQNN&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#37327;&#21270;&#19981;&#21516;&#23618;&#27425;&#30340;DNN&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#20154;&#24037;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploring the expected quantizing scheme with suitable mixed-precision policy is the key point to compress deep neural networks (DNNs) in high efficiency and accuracy. This exploration implies heavy workloads for domain experts, and an automatic compression method is needed. However, the huge search space of the automatic method introduces plenty of computing budgets that make the automatic process challenging to be applied in real scenarios. In this paper, we propose an end-to-end framework named AutoQNN, for automatically quantizing different layers utilizing different schemes and bitwidths without any human labor. AutoQNN can seek desirable quantizing schemes and mixed-precision policies for mainstream DNN models efficiently by involving three techniques: quantizing scheme search (QSS), quantizing precision learning (QPL), and quantized architecture generation (QAG). QSS introduces five quantizing schemes and defines three new schemes as a candidate set for scheme search, and then u
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#21307;&#30103;&#20445;&#20581;&#20013;&#39044;&#27979;&#27169;&#22411;&#30340;&#39564;&#35777;&#38382;&#39064;&#65292;&#24314;&#35758;&#20351;&#29992;&#26469;&#33258;&#30446;&#26631;&#20154;&#32676;&#30340;&#26032;&#25968;&#25454;&#36827;&#34892;&#22806;&#37096;&#39564;&#35777;&#65292;&#30830;&#20445;&#39564;&#35777;&#24615;&#33021;&#23545;&#27169;&#22411;&#21487;&#38752;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#22312;&#27169;&#22411;&#24320;&#21457;&#26399;&#38388;&#35748;&#30495;&#30740;&#31350;&#27169;&#22411;&#22312;&#26356;&#24191;&#27867;&#29615;&#22659;&#20013;&#30340;&#25299;&#23637;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20221;&#36335;&#32447;&#22270;&#65292;&#20197;&#20415;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#24320;&#21457;&#21644;&#24212;&#29992;&#21487;&#38752;&#12289;&#20844;&#24179;&#12289;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.03779</link><description>&lt;p&gt;
&#21307;&#30103;&#20445;&#20581;&#20013;&#20844;&#24179;&#21644;&#21487;&#20449;&#39044;&#27979;&#27169;&#22411;&#39564;&#35777;&#30340;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
A roadmap to fair and trustworthy prediction model validation in healthcare. (arXiv:2304.03779v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03779
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#21307;&#30103;&#20445;&#20581;&#20013;&#39044;&#27979;&#27169;&#22411;&#30340;&#39564;&#35777;&#38382;&#39064;&#65292;&#24314;&#35758;&#20351;&#29992;&#26469;&#33258;&#30446;&#26631;&#20154;&#32676;&#30340;&#26032;&#25968;&#25454;&#36827;&#34892;&#22806;&#37096;&#39564;&#35777;&#65292;&#30830;&#20445;&#39564;&#35777;&#24615;&#33021;&#23545;&#27169;&#22411;&#21487;&#38752;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#22312;&#27169;&#22411;&#24320;&#21457;&#26399;&#38388;&#35748;&#30495;&#30740;&#31350;&#27169;&#22411;&#22312;&#26356;&#24191;&#27867;&#29615;&#22659;&#20013;&#30340;&#25299;&#23637;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20221;&#36335;&#32447;&#22270;&#65292;&#20197;&#20415;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#24320;&#21457;&#21644;&#24212;&#29992;&#21487;&#38752;&#12289;&#20844;&#24179;&#12289;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#19968;&#20010;&#39044;&#27979;&#27169;&#22411;&#33021;&#22815;&#25512;&#24191;&#21040;&#24320;&#21457;&#25968;&#25454;&#20197;&#22806;&#30340;&#25968;&#25454;&#65292;&#24182;&#36827;&#34892;&#22806;&#37096;&#39564;&#35777;&#65292;&#37027;&#20040;&#23427;&#23601;&#26159;&#26368;&#26377;&#29992;&#30340;&#12290;&#20294;&#26159;&#65292;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#23427;&#21487;&#20197;&#25512;&#24191;&#20173;&#19981;&#28165;&#26970;&#12290;&#23454;&#38469;&#19978;&#65292;&#39044;&#27979;&#27169;&#22411;&#20351;&#29992;&#26469;&#33258;&#20854;&#20182;&#21307;&#30103;&#31995;&#32479;&#25110;&#22269;&#23478;&#30340;&#20154;&#21475;&#31561;&#38750;&#24120;&#19981;&#21516;&#30340;&#25968;&#25454;&#36827;&#34892;&#22806;&#37096;&#39564;&#35777;&#65292;&#39044;&#27979;&#32467;&#26524;&#36890;&#24120;&#24456;&#24046;&#12290;&#36825;&#21487;&#33021;&#19981;&#26159;&#23545;&#29305;&#23450;&#30446;&#26631;&#20154;&#32676;&#25110;&#29615;&#22659;&#35774;&#35745;&#30340;&#27169;&#22411;&#34920;&#29616;&#30340;&#20844;&#27491;&#21453;&#26144;&#65292;&#24182;&#19988;&#21487;&#33021;&#20250;&#25289;&#20280;&#39044;&#26399;&#30340;&#27169;&#22411;&#25512;&#24191;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#26469;&#33258;&#30446;&#26631;&#20154;&#32676;&#30340;&#26032;&#25968;&#25454;&#26469;&#22806;&#37096;&#39564;&#35777;&#27169;&#22411;&#65292;&#20197;&#30830;&#20445;&#39564;&#35777;&#24615;&#33021;&#23545;&#27169;&#22411;&#21487;&#38752;&#24615;&#30340;&#28165;&#26224;&#24433;&#21709;&#65292;&#32780;&#27169;&#22411;&#25512;&#24191;&#21040;&#26356;&#24191;&#27867;&#30340;&#29615;&#22659;&#24212;&#22312;&#27169;&#22411;&#24320;&#21457;&#26399;&#38388;&#36827;&#34892;&#20180;&#32454;&#35843;&#26597;&#65292;&#32780;&#19981;&#26159;&#20107;&#21518;&#25506;&#35752;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#36335;&#32447;&#22270;&#65292;&#20197;&#20415;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#24320;&#21457;&#21644;&#24212;&#29992;&#21487;&#38752;&#12289;&#20844;&#24179;&#12289;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20221;&#36335;&#32447;&#22270;&#24212;&#35813;&#28165;&#26224;&#23450;&#20041;&#20844;&#24179;&#24615;&#21644;&#21487;&#20449;&#24615;&#65292;&#32771;&#34385;&#27169;&#22411;&#20351;&#29992;&#30340;&#20262;&#29702;&#24433;&#21709;&#65292;&#36827;&#34892;&#20122;&#32452;&#20998;&#26512;&#65292;&#24182;&#37319;&#29992;&#20005;&#26684;&#30340;&#39564;&#35777;&#21327;&#35758;&#65292;&#20854;&#20013;&#21253;&#25324;&#22312;&#30446;&#26631;&#20154;&#32676;&#20013;&#36827;&#34892;&#22806;&#37096;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
A prediction model is most useful if it generalizes beyond the development data with external validations, but to what extent should it generalize remains unclear. In practice, prediction models are externally validated using data from very different settings, including populations from other health systems or countries, with predictably poor results. This may not be a fair reflection of the performance of the model which was designed for a specific target population or setting, and may be stretching the expected model generalizability. To address this, we suggest to externally validate a model using new data from the target population to ensure clear implications of validation performance on model reliability, whereas model generalizability to broader settings should be carefully investigated during model development instead of explored post-hoc. Based on this perspective, we propose a roadmap that facilitates the development and application of reliable, fair, and trustworthy artifici
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#31526;&#21512;&#24615;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#21160;&#21147;&#21644;&#36895;&#24230;&#26469;&#20026;&#33258;&#34892;&#36710;&#27604;&#36187;&#20013;&#30340;&#27599;&#20010;&#39569;&#25163;&#25552;&#20379;&#21345;&#36335;&#37324;&#38656;&#27714;&#30340;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2304.03778</link><description>&lt;p&gt;
&#38024;&#23545;Jumbo-Visma&#38431;&#21345;&#36335;&#37324;&#39044;&#27979;&#30340;&#31526;&#21512;&#24615;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Conformal Regression in Calorie Prediction for Team Jumbo-Visma. (arXiv:2304.03778v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#31526;&#21512;&#24615;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#21160;&#21147;&#21644;&#36895;&#24230;&#26469;&#20026;&#33258;&#34892;&#36710;&#27604;&#36187;&#20013;&#30340;&#27599;&#20010;&#39569;&#25163;&#25552;&#20379;&#21345;&#36335;&#37324;&#38656;&#27714;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
UCI WorldTour&#36187;&#20107;&#26159;&#30007;&#23376;&#31934;&#33521;&#20844;&#36335;&#33258;&#34892;&#36710;&#27604;&#36187;&#30340;&#39030;&#32423;&#36187;&#20107;&#65292;&#23545;&#39569;&#34892;&#32773;&#30340;&#20307;&#33021;&#21644;&#32784;&#21147;&#36827;&#34892;&#32771;&#39564;&#12290;Jumbo-Visma&#38431;&#30340;&#25945;&#32451;&#20204;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#36127;&#36131;&#39044;&#27979;&#27599;&#20010;&#27604;&#36187;&#26085;&#21382;&#20013;&#33655;&#20848;&#38431;&#30340;&#27599;&#20301;&#39569;&#25163;&#30340;&#33021;&#37327;&#38656;&#27714;&#65292;&#20197;&#30830;&#20445;&#39569;&#25163;&#22312;&#27604;&#36187;&#36807;&#31243;&#20013;&#26377;&#36275;&#22815;&#30340;&#33021;&#37327;&#21644;&#36164;&#28304;&#26469;&#20445;&#25345;&#39640;&#27700;&#24179;&#34920;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26356;&#26377;&#25928;&#30340;&#39044;&#27979;&#39569;&#34892;&#27604;&#36187;&#33021;&#37327;&#38656;&#27714;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22238;&#24402;&#27169;&#22411;&#39044;&#27979;&#36895;&#24230;&#21644;&#21160;&#21147;&#65292;&#20026;&#27599;&#20010;&#38454;&#27573;&#30340;&#27599;&#20010;&#20010;&#20307;&#39569;&#25163;&#25552;&#20379;&#21345;&#36335;&#37324;&#38656;&#27714;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
UCI WorldTour races, the premier men's elite road cycling tour, are grueling events that put riders' physical fitness and endurance to the test. The coaches of Team Jumbo-Visma have long been responsible for predicting the energy needs of each rider of the Dutch team for every race on the calendar. Those must be estimated to ensure riders have the energy and resources necessary to maintain a high level of performance throughout a race. This task, however, is both time-consuming and challenging, as it requires precise estimates of race speed and power output. Traditionally, the approach to predicting energy needs has relied on coaches' judgement and experience, but this method has its limitations and often leads to inaccurate predictions. In this paper, we propose a new, more effective approach to predicting energy needs for cycling races. By predicting the speed and power with regression models, we provide the coaches with calorie needs estimate for each individual rider per stage inst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#29289;&#24207;&#21015;&#26680;&#30340;&#25968;&#23398;&#25361;&#25112;&#65292;&#25506;&#35752;&#20102;&#29983;&#29289;&#24207;&#21015;&#31354;&#38388;&#32467;&#26500;&#21644;&#29983;&#29289;&#24207;&#21015;&#30456;&#20284;&#24615;&#30340;&#29305;&#27530;&#24615;&#23545;&#20110;&#26680;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.03775</link><description>&lt;p&gt;
&#20855;&#26377;&#20445;&#35777;&#28789;&#27963;&#24615;&#30340;&#29983;&#29289;&#24207;&#21015;&#26680;
&lt;/p&gt;
&lt;p&gt;
Biological Sequence Kernels with Guaranteed Flexibility. (arXiv:2304.03775v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#29289;&#24207;&#21015;&#26680;&#30340;&#25968;&#23398;&#25361;&#25112;&#65292;&#25506;&#35752;&#20102;&#29983;&#29289;&#24207;&#21015;&#31354;&#38388;&#32467;&#26500;&#21644;&#29983;&#29289;&#24207;&#21015;&#30456;&#20284;&#24615;&#30340;&#29305;&#27530;&#24615;&#23545;&#20110;&#26680;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#29983;&#29289;&#24207;&#21015;&#65288;DNA&#65292;RNA&#21644;&#34507;&#30333;&#36136;&#65289;&#20855;&#26377;&#25512;&#21160;&#20154;&#31867;&#20581;&#24247;&#12289;&#29615;&#22659;&#21487;&#25345;&#32493;&#24615;&#21644;&#22522;&#30784;&#29983;&#29289;&#23398;&#29702;&#35299;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#20010;&#38382;&#39064;&#22495;&#20013;&#26159;&#26080;&#25928;&#25110;&#19981;&#21487;&#38752;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#26680;&#30340;&#35270;&#35282;&#20174;&#29702;&#35770;&#19978;&#30740;&#31350;&#36825;&#20123;&#25361;&#25112;&#12290;&#23613;&#31649;&#27809;&#26377;&#26174;&#24335;&#20351;&#29992;&#26680;&#30340;&#26041;&#27861;&#20173;&#28982;&#22312;&#38544;&#21547;&#22320;&#20381;&#36182;&#23427;&#20204;&#65292;&#21253;&#25324;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#21644;&#22522;&#20110;&#29289;&#29702;&#30340;&#25216;&#26415;&#12290;&#34429;&#28982;&#20854;&#20182;&#31867;&#22411;&#25968;&#25454;&#30340;&#26680;&#24050;&#34987;&#29702;&#35770;&#19978;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#29983;&#29289;&#24207;&#21015;&#31354;&#38388;&#30340;&#32467;&#26500;&#65288;&#31163;&#25955;&#12289;&#21487;&#21464;&#38271;&#24230;&#24207;&#21015;&#65289;&#20197;&#21450;&#29983;&#29289;&#24207;&#21015;&#30456;&#20284;&#24615;&#30340;&#29305;&#27530;&#25968;&#23398;&#25361;&#25112;&#29420;&#19968;&#26080;&#20108;&#12290;&#25105;&#20204;&#27491;&#24335;&#20998;&#26512;&#29983;&#29289;&#24207;&#21015;&#26680;&#33021;&#22815;&#22810;&#22909;&#22320;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
Applying machine learning to biological sequences - DNA, RNA and protein has enormous potential to advance human health, environmental sustainability, and fundamental biological understanding. However, many existing machine learning methods are ineffective or unreliable in this problem domain. We study these challenges theoretically, through the lens of kernels. Methods based on kernels are ubiquitous: they are used to predict molecular phenotypes, design novel proteins, compare sequence distributions, and more. Many methods that do not use kernels explicitly still rely on them implicitly, including a wide variety of both deep learning and physics-based techniques. While kernels for other types of data are well-studied theoretically, the structure of biological sequence space (discrete, variable length sequences), as well as biological notions of sequence similarity, present unique mathematical challenges. We formally analyze how well kernels for biological sequences can approximate 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#24815;&#24615;&#21160;&#20316;&#25429;&#25417;&#25216;&#26415;&#35760;&#24405;&#30340;&#24037;&#19994;&#25805;&#20316;&#21592;&#21644;&#25163;&#33402;&#20154;&#21592;&#30495;&#23454;&#24037;&#20316;&#20013;&#30340;&#19987;&#19994;&#21160;&#20316;&#30340;&#19971;&#20010;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#24314;&#27169;&#12289;&#20998;&#26512;&#21644;&#29983;&#25104;&#30340;&#30740;&#31350;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;Gesture Operational Model&#20316;&#20026;&#26041;&#27861;&#30340;&#21021;&#27493;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2304.03771</link><description>&lt;p&gt;
&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#20998;&#26512;&#30340;&#30495;&#23454;&#24037;&#19994;&#20219;&#21153;&#21644;&#20256;&#32479;&#25163;&#24037;&#33402;&#30340;&#21160;&#20316;&#25429;&#25417;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Motion Capture Benchmark of Real Industrial Tasks and Traditional Crafts for Human Movement Analysis. (arXiv:2304.03771v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#24815;&#24615;&#21160;&#20316;&#25429;&#25417;&#25216;&#26415;&#35760;&#24405;&#30340;&#24037;&#19994;&#25805;&#20316;&#21592;&#21644;&#25163;&#33402;&#20154;&#21592;&#30495;&#23454;&#24037;&#20316;&#20013;&#30340;&#19987;&#19994;&#21160;&#20316;&#30340;&#19971;&#20010;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#24314;&#27169;&#12289;&#20998;&#26512;&#21644;&#29983;&#25104;&#30340;&#30740;&#31350;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;Gesture Operational Model&#20316;&#20026;&#26041;&#27861;&#30340;&#21021;&#27493;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#36816;&#21160;&#20998;&#26512;&#26159;&#26426;&#22120;&#20154;&#25216;&#26415;&#12289;&#29983;&#29289;&#21147;&#23398;&#21644;&#25968;&#25454;&#31185;&#23398;&#30740;&#31350;&#39046;&#22495;&#30340;&#19968;&#20010;&#20851;&#38190;&#39046;&#22495;&#65292;&#21253;&#25324;&#36319;&#36394;&#12289;&#23039;&#24577;&#20272;&#35745;&#21644;&#36816;&#21160;&#21512;&#25104;&#12290;&#34429;&#28982;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#26041;&#27861;&#65292;&#20294;&#20173;&#38656;&#35201;&#20351;&#29992;&#21487;&#39564;&#35777;&#30340;&#19977;&#32500;&#20154;&#20307;&#36816;&#21160;&#22522;&#20934;&#25968;&#25454;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#31995;&#32479;&#21644;&#23450;&#37327;&#30340;&#35780;&#20272;&#65292;&#20197;&#23450;&#20041;&#24403;&#21069;&#25216;&#26415;&#27700;&#24179;&#30340;&#29616;&#29366;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22522;&#20110;&#24815;&#24615;&#30340;&#21160;&#20316;&#25429;&#25417;&#25216;&#26415;&#35760;&#24405;&#30340;&#19971;&#20010;&#25968;&#25454;&#38598;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;&#22312;&#23454;&#38469;&#26465;&#20214;&#19979;&#30001;&#24037;&#19994;&#25805;&#20316;&#21592;&#21644;&#29087;&#32451;&#30340;&#25163;&#33402;&#20154;&#21592;&#25191;&#34892;&#30340;&#19987;&#19994;&#21160;&#20316;&#12290;&#25968;&#25454;&#38598;&#26088;&#22312;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#24314;&#27169;&#12289;&#20998;&#26512;&#21644;&#29983;&#25104;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;&#35814;&#32454;&#25551;&#36848;&#20102;&#25968;&#25454;&#37319;&#38598;&#21327;&#35758;&#65292;&#24182;&#25552;&#20379;&#20102;&#25910;&#38598;&#25968;&#25454;&#30340;&#21021;&#27493;&#20998;&#26512;&#20316;&#20026;&#22522;&#20934;&#27979;&#35797;&#12290;Gesture Operational Model&#26159;&#19968;&#31181;&#22522;&#20110;&#36816;&#21160;&#25551;&#36848;&#31526;&#30340;&#28151;&#21512;&#38543;&#26426;-&#29983;&#29289;&#21147;&#23398;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human movement analysis is a key area of research in robotics, biomechanics, and data science. It encompasses tracking, posture estimation, and movement synthesis. While numerous methodologies have evolved over time, a systematic and quantitative evaluation of these approaches using verifiable ground truth data of three-dimensional human movement is still required to define the current state of the art. This paper presents seven datasets recorded using inertial-based motion capture. The datasets contain professional gestures carried out by industrial operators and skilled craftsmen performed in real conditions in-situ. The datasets were created with the intention of being used for research in human motion modeling, analysis, and generation. The protocols for data collection are described in detail, and a preliminary analysis of the collected data is provided as a benchmark. The Gesture Operational Model, a hybrid stochastic-biomechanical approach based on kinematic descriptors, is util
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#26426;&#22120;ID&#30340;&#23545;&#27604;&#23398;&#20064;&#39044;&#35757;&#32451;&#30340;&#38899;&#39057;&#34920;&#31034;&#26469;&#36827;&#34892;&#24322;&#24120;&#22768;&#38899;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#22312;DCASE 2020 Challenge Task2 &#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#25110;&#33258;&#30417;&#30563;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.03588</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#26426;&#22120;ID&#30340;&#23545;&#27604;&#23398;&#20064;&#39044;&#35757;&#32451;&#30340;&#38899;&#39057;&#34920;&#31034;&#36827;&#34892;&#24322;&#24120;&#22768;&#38899;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Anomalous Sound Detection using Audio Representation with Machine ID based Contrastive Learning Pretraining. (arXiv:2304.03588v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#26426;&#22120;ID&#30340;&#23545;&#27604;&#23398;&#20064;&#39044;&#35757;&#32451;&#30340;&#38899;&#39057;&#34920;&#31034;&#26469;&#36827;&#34892;&#24322;&#24120;&#22768;&#38899;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#22312;DCASE 2020 Challenge Task2 &#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#25110;&#33258;&#30417;&#30563;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#29992;&#20110;&#24322;&#24120;&#22768;&#38899;&#26816;&#27979;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#26679;&#26412;&#22686;&#24378;&#30340;&#23545;&#27604;&#26469;&#31934;&#28860;&#27599;&#20010;&#38899;&#39057;&#26679;&#26412;&#30340;&#38899;&#39057;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#26426;&#22120;&#22768;&#38899;&#30340;&#29289;&#29702;&#29305;&#24615;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#21463;&#21040;&#22686;&#24378;&#25968;&#25454;&#30340;&#20559;&#24046;&#65292;&#20174;&#32780;&#38480;&#21046;&#26816;&#27979;&#24615;&#33021;&#12290;&#26412;&#25991;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#31934;&#28860;&#27599;&#20010;&#26426;&#22120;ID&#30340;&#38899;&#39057;&#34920;&#31034;&#65292;&#32780;&#19981;&#26159;&#27599;&#20010;&#38899;&#39057;&#26679;&#26412;&#12290;&#25552;&#20986;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#39044;&#35757;&#32451;&#38899;&#39057;&#34920;&#31034;&#27169;&#22411;&#65292;&#36890;&#36807;&#34701;&#20837;&#26426;&#22120;ID&#21644;&#33258;&#30417;&#30563;ID&#20998;&#31867;&#22120;&#26469;&#23545;&#23398;&#20064;&#30340;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#21516;&#26102;&#22686;&#24378;&#26469;&#33258;&#30456;&#21516;ID&#30340;&#38899;&#39057;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;DCASE 2020&#25361;&#25112;&#20219;&#21153;2&#25968;&#25454;&#38598;&#19978;&#30340;&#25972;&#20307;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#20248;&#20110;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#25110;&#33258;&#30417;&#30563;&#20998;&#31867;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing contrastive learning methods for anomalous sound detection refine the audio representation of each audio sample by using the contrast between the samples' augmentations (e.g., with time or frequency masking). However, they might be biased by the augmented data, due to the lack of physical properties of machine sound, thereby limiting the detection performance. This paper uses contrastive learning to refine audio representations for each machine ID, rather than for each audio sample. The proposed two-stage method uses contrastive learning to pretrain the audio representation model by incorporating machine ID and a self-supervised ID classifier to fine-tune the learnt model, while enhancing the relation between audio features from the same ID. Experiments show that our method outperforms the state-of-the-art methods using contrastive learning or self-supervised classification in overall anomaly detection performance and stability on DCASE 2020 Challenge Task2 dataset.
&lt;/p&gt;</description></item><item><title>&#25991;&#31456;&#37325;&#26032;&#32771;&#34385;&#20102;&#22522;&#20110;GNN&#30340;&#24322;&#26500;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#23545;&#40784;&#12290;&#20026;&#25506;&#31350;EA&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#65292;&#25552;&#20986;&#20102;&#26356;&#25509;&#36817;&#29616;&#23454;&#30340;&#39640;&#24230;&#24322;&#26500;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.03468</link><description>&lt;p&gt;
&#37325;&#26032;&#32771;&#34385;&#22522;&#20110;GNN&#30340;&#24322;&#26500;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#23545;&#40784;&#65306;&#26032;&#25968;&#25454;&#38598;&#21644;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rethinking GNN-based Entity Alignment on Heterogeneous Knowledge Graphs: New Datasets and A New Method. (arXiv:2304.03468v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03468
&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#37325;&#26032;&#32771;&#34385;&#20102;&#22522;&#20110;GNN&#30340;&#24322;&#26500;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#23545;&#40784;&#12290;&#20026;&#25506;&#31350;EA&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#65292;&#25552;&#20986;&#20102;&#26356;&#25509;&#36817;&#29616;&#23454;&#30340;&#39640;&#24230;&#24322;&#26500;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#24212;&#29992;&#30340;&#21457;&#23637;&#23548;&#33268;&#20102;&#38656;&#35201;&#20174;&#21508;&#31181;&#26469;&#28304;&#25552;&#21462;&#30340;&#24322;&#26500;KG&#20043;&#38388;&#30340;&#23454;&#20307;&#23545;&#40784;&#65288;EA&#65289;&#30340;&#19981;&#26029;&#22686;&#38271;&#38656;&#27714;&#12290;&#36817;&#26469;&#65292;&#30001;&#20110;GNN&#30340;&#20986;&#33394;&#32467;&#26500;&#20449;&#24687;&#25429;&#25417;&#33021;&#21147;&#65292;&#22312;EA&#20219;&#21153;&#20013;&#24191;&#27867;&#37319;&#29992;GNN&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#29616;&#26377;&#24120;&#35265;EA&#25968;&#25454;&#38598;&#30340;&#36807;&#20110;&#31616;&#21333;&#21270;&#30340;&#35774;&#32622;&#19982;&#29616;&#23454;&#22330;&#26223;&#30456;&#36317;&#29978;&#36828;&#65292;&#36825;&#22952;&#30861;&#20102;&#23545;&#26368;&#36817;&#26041;&#27861;&#25152;&#21462;&#24471;&#36827;&#23637;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#36825;&#31181;&#29616;&#35937;&#20351;&#25105;&#20204;&#28145;&#24605;&#65306;&#29616;&#26377;&#22522;&#20110;GNN&#30340;EA&#26041;&#27861;&#26159;&#21542;&#30495;&#30340;&#21462;&#24471;&#20102;&#20255;&#22823;&#36827;&#23637;&#65311;&#20026;&#20102;&#30740;&#31350;EA&#26041;&#27861;&#22312;&#29616;&#23454;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#26412;&#25991;&#32858;&#28966;&#20110;&#39640;&#24230;&#24322;&#26500;&#30340;KG&#65288;HHKG&#65289;&#65288;&#20363;&#22914;&#65292;&#20107;&#20214;KG&#21644;&#36890;&#29992;KG&#65289;&#30340;&#23545;&#40784;&#65292;&#36825;&#20123;KG&#22312;&#35268;&#27169;&#21644;&#32467;&#26500;&#19978;&#19981;&#21516;&#65292;&#24182;&#20849;&#20139;&#26356;&#23569;&#30340;&#37325;&#21472;&#23454;&#20307;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#28165;&#29702;&#20102;&#19981;&#21512;&#29702;&#30340;&#35774;&#32622;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;HHKG&#25968;&#25454;&#38598;&#65292;&#20854;&#23494;&#20999;&#22320;&#27169;&#25311;&#20102;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of knowledge graph (KG) applications has led to a rising need for entity alignment (EA) between heterogeneous KGs that are extracted from various sources. Recently, graph neural networks (GNNs) have been widely adopted in EA tasks due to GNNs' impressive ability to capture structure information. However, we have observed that the oversimplified settings of the existing common EA datasets are distant from real-world scenarios, which obstructs a full understanding of the advancements achieved by recent methods. This phenomenon makes us ponder: Do existing GNN-based EA methods really make great progress?  In this paper, to study the performance of EA methods in realistic settings, we focus on the alignment of highly heterogeneous KGs (HHKGs) (e.g., event KGs and general KGs) which are different with regard to the scale and structure, and share fewer overlapping entities. First, we sweep the unreasonable settings, and propose two new HHKG datasets that closely mimic real-wo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;NARS&#21644;&#24378;&#21270;&#23398;&#20064;&#22312;&#35299;&#20915;&#24207;&#21015;&#20219;&#21153;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;NARS&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#37117;&#26377;&#36739;&#22909;&#30340;&#34920;&#29616;&#65292;&#23588;&#20854;&#26159;&#22312;&#38750;&#30830;&#23450;&#24615;&#29615;&#22659;&#20013;&#12290;</title><link>http://arxiv.org/abs/2304.03291</link><description>&lt;p&gt;
&#27604;&#36739;NARS&#21644;&#24378;&#21270;&#23398;&#20064;&#65306;&#23545;ONA&#21644;$Q$-Learning&#31639;&#27861;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Comparing NARS and Reinforcement Learning: An Analysis of ONA and $Q$-Learning Algorithms. (arXiv:2304.03291v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;NARS&#21644;&#24378;&#21270;&#23398;&#20064;&#22312;&#35299;&#20915;&#24207;&#21015;&#20219;&#21153;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;NARS&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#37117;&#26377;&#36739;&#22909;&#30340;&#34920;&#29616;&#65292;&#23588;&#20854;&#26159;&#22312;&#38750;&#30830;&#23450;&#24615;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#25104;&#20026;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20013;&#22522;&#20110;&#24207;&#21015;&#20219;&#21153;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23547;&#25214;RL&#30340;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#20173;&#28982;&#26159;&#19968;&#20010;&#20196;&#20154;&#20852;&#22859;&#21644;&#21019;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#20854;&#20013;&#19968;&#20010;&#22791;&#21463;&#20851;&#27880;&#30340;&#26367;&#20195;&#26041;&#26696;&#26159;&#38750;&#20844;&#29702;&#25512;&#29702;&#31995;&#32479;&#65288;NARS&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#35748;&#30693;&#25512;&#29702;&#26694;&#26550;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;NARS&#20316;&#20026;RL&#26367;&#20195;&#26041;&#26696;&#22312;&#35299;&#20915;&#22522;&#20110;&#24207;&#21015;&#20219;&#21153;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#22312;&#20351;&#29992;Open AI gym&#21019;&#24314;&#30340;&#21508;&#31181;&#29615;&#22659;&#20013;&#65292;&#23545;ONA&#20316;&#20026;NARS&#23454;&#29616;&#21644;$Q$-Learning&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;&#36825;&#20123;&#29615;&#22659;&#20855;&#26377;&#19981;&#21516;&#30340;&#38590;&#24230;&#32423;&#21035;&#65292;&#20174;&#31616;&#21333;&#21040;&#22797;&#26434;&#19981;&#31561;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#65292;&#23588;&#20854;&#26159;&#22312;&#38750;&#30830;&#23450;&#24615;&#29615;&#22659;&#20013;&#65292;NARS&#26159;&#19968;&#20010;&#26377;&#31454;&#20105;&#21147;&#30340;RL&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, reinforcement learning (RL) has emerged as a popular approach for solving sequence-based tasks in machine learning. However, finding suitable alternatives to RL remains an exciting and innovative research area. One such alternative that has garnered attention is the Non-Axiomatic Reasoning System (NARS), which is a general-purpose cognitive reasoning framework. In this paper, we delve into the potential of NARS as a substitute for RL in solving sequence-based tasks. To investigate this, we conduct a comparative analysis of the performance of ONA as an implementation of NARS and $Q$-Learning in various environments that were created using the Open AI gym. The environments have different difficulty levels, ranging from simple to complex. Our results demonstrate that NARS is a promising alternative to RL, with competitive performance in diverse environments, particularly in non-deterministic ones.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RNAS&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#29983;&#25104;&#39640;&#36136;&#37327;&#26550;&#26500;&#65292;&#20351;&#29992;&#22122;&#22768;&#26679;&#26412;&#20943;&#23569;&#25628;&#32034;&#25104;&#26412;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#23545;&#25239;&#25915;&#20987;&#20013;&#22343;&#36798;&#21040;&#26368;&#20808;&#36827;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2304.02845</link><description>&lt;p&gt;
&#22362;&#38887;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Robust Neural Architecture Search. (arXiv:2304.02845v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02845
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RNAS&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#29983;&#25104;&#39640;&#36136;&#37327;&#26550;&#26500;&#65292;&#20351;&#29992;&#22122;&#22768;&#26679;&#26412;&#20943;&#23569;&#25628;&#32034;&#25104;&#26412;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#23545;&#25239;&#25915;&#20987;&#20013;&#22343;&#36798;&#21040;&#26368;&#20808;&#36827;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;NAS&#29983;&#25104;&#30340;&#27169;&#22411;&#24448;&#24448;&#26356;&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#24694;&#24847;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#35768;&#22810;&#24378;&#20581;&#30340;NAS&#26041;&#27861;&#21033;&#29992;&#23545;&#25239;&#35757;&#32451;&#26469;&#22686;&#24378;NAS&#29983;&#25104;&#30340;&#27169;&#22411;&#30340;&#24378;&#20581;&#24615;&#65292;&#20294;&#26159;&#23427;&#20204;&#24573;&#30053;&#20102;NAS&#29983;&#25104;&#30340;&#27169;&#22411;&#30340;&#26412;&#36136;&#20934;&#30830;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;NAS&#26041;&#27861;&#65292;&#21517;&#20026;Robust Neural Architecture Search&#65288;RNAS&#65289;&#12290;&#20026;&#20102;&#35774;&#35745;&#20986;&#19968;&#20010;&#27491;&#21017;&#21270;&#39033;&#26469;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;RNAS&#29983;&#25104;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#21644;&#33391;&#22909;&#40065;&#26834;&#24615;&#30340;&#26550;&#26500;&#12290;&#20026;&#20102;&#20943;&#23569;&#25628;&#32034;&#25104;&#26412;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22122;&#22768;&#26679;&#26412;&#32780;&#19981;&#26159;&#23545;&#25239;&#24615;&#26679;&#26412;&#20316;&#20026;&#25628;&#32034;&#26550;&#26500;&#30340;&#36755;&#20837;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;RNAS&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#23545;&#25239;&#25915;&#20987;&#26041;&#38754;&#22343;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#65288;SOTA&#65289;&#30340;&#24615;&#33021;&#65292;&#36825;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;RNAS&#22312;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Architectures Search (NAS) becomes more and more popular over these years. However, NAS-generated models tends to suffer greater vulnerability to various malicious attacks. Lots of robust NAS methods leverage adversarial training to enhance the robustness of NAS-generated models, however, they neglected the nature accuracy of NAS-generated models. In our paper, we propose a novel NAS method, Robust Neural Architecture Search (RNAS). To design a regularization term to balance accuracy and robustness, RNAS generates architectures with both high accuracy and good robustness. To reduce search cost, we further propose to use noise examples instead adversarial examples as input to search architectures. Extensive experiments show that RNAS achieves state-of-the-art (SOTA) performance on both image classification and adversarial attacks, which illustrates the proposed RNAS achieves a good tradeoff between robustness and accuracy.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25351;&#20986;&#24494;&#23567;&#20294;&#26377;&#25928;&#30340;&#22270;&#20687;&#25200;&#21160;&#26041;&#27861;&#24182;&#19981;&#33021;&#25269;&#24481;JPEG&#21387;&#32553;&#65292;&#22240;&#27492;&#19981;&#33021;&#26377;&#25928;&#20445;&#25252;&#22270;&#20687;&#20813;&#21463;&#24694;&#24847;&#32534;&#36753;&#21644;&#28145;&#24230;&#20266;&#36896;&#25915;&#20987;&#65292;&#24314;&#35758;&#37319;&#29992;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2304.02234</link><description>&lt;p&gt;
JPEG&#21387;&#32553;&#22270;&#20687;&#21487;&#20197;&#32469;&#36807;&#23545;&#25239;AI&#32534;&#36753;&#30340;&#20445;&#25252;&#25514;&#26045;
&lt;/p&gt;
&lt;p&gt;
JPEG Compressed Images Can Bypass Protections Against AI Editing. (arXiv:2304.02234v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02234
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25351;&#20986;&#24494;&#23567;&#20294;&#26377;&#25928;&#30340;&#22270;&#20687;&#25200;&#21160;&#26041;&#27861;&#24182;&#19981;&#33021;&#25269;&#24481;JPEG&#21387;&#32553;&#65292;&#22240;&#27492;&#19981;&#33021;&#26377;&#25928;&#20445;&#25252;&#22270;&#20687;&#20813;&#21463;&#24694;&#24847;&#32534;&#36753;&#21644;&#28145;&#24230;&#20266;&#36896;&#25915;&#20987;&#65292;&#24314;&#35758;&#37319;&#29992;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#20351;&#24471;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#32534;&#36753;&#25110;&#29983;&#25104;&#21464;&#24471;&#26356;&#21152;&#23481;&#26131;&#12290;&#28982;&#32780;&#65292;&#20854;&#26131;&#29992;&#24615;&#24341;&#21457;&#20102;&#23545;&#24694;&#24847;&#32534;&#36753;&#25110;&#28145;&#24230;&#20266;&#36896;&#30340;&#25285;&#24551;&#12290;&#20026;&#38450;&#27490;&#20256;&#25773;&#27169;&#22411;&#29983;&#25104;&#30495;&#23454;&#22270;&#20687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24494;&#19981;&#21487;&#35265;&#30340;&#25200;&#21160;&#20316;&#20026;&#20445;&#25252;&#22270;&#20687;&#20813;&#21463;&#24694;&#24847;&#25915;&#20987;&#30340;&#25163;&#27573;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#25200;&#21160;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#19981;&#33021;&#32463;&#21463;JPEG&#21387;&#32553;&#30340;&#32771;&#39564;&#65292;&#36825;&#26159;&#19968;&#20010;&#20027;&#35201;&#30340;&#24369;&#28857;&#65292;&#22240;&#20026;JPEG&#20855;&#26377;&#26222;&#36941;&#30340;&#20351;&#29992;&#21644;&#21487;&#33719;&#21462;&#24615;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#21152;&#24615;&#24494;&#19981;&#21487;&#35265;&#25200;&#21160;&#40065;&#26834;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#40723;&#21169;&#37319;&#29992;&#20854;&#20182;&#26041;&#27861;&#26469;&#20445;&#25252;&#22270;&#20687;&#20813;&#21463;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently developed text-to-image diffusion models make it easy to edit or create high-quality images. Their ease of use has raised concerns about the potential for malicious editing or deepfake creation. Imperceptible perturbations have been proposed as a means of protecting images from malicious editing by preventing diffusion models from generating realistic images. However, we find that the aforementioned perturbations are not robust to JPEG compression, which poses a major weakness because of the common usage and availability of JPEG. We discuss the importance of robustness for additive imperceptible perturbations and encourage alternative approaches to protect images against editing.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#24452;&#21521;&#22522;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#31867;&#21035;&#65292;&#35777;&#26126;&#36825;&#20123;&#32593;&#32476;&#33021;&#22815;&#36924;&#36817;&#20219;&#20309;&#36830;&#32493;&#22810;&#20803;&#20989;&#25968;&#65292;&#36824;&#35752;&#35770;&#20102;&#26377;&#38480;&#20010;&#22266;&#23450;&#20013;&#24515;&#30340;RBF&#32593;&#32476;&#30340;&#36924;&#36817;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2304.02220</link><description>&lt;p&gt;
&#20851;&#20110;&#24452;&#21521;&#22522;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#26222;&#36866;&#36924;&#36817;&#24615;&#36136;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the universal approximation property of radial basis function neural networks. (arXiv:2304.02220v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#24452;&#21521;&#22522;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#31867;&#21035;&#65292;&#35777;&#26126;&#36825;&#20123;&#32593;&#32476;&#33021;&#22815;&#36924;&#36817;&#20219;&#20309;&#36830;&#32493;&#22810;&#20803;&#20989;&#25968;&#65292;&#36824;&#35752;&#35770;&#20102;&#26377;&#38480;&#20010;&#22266;&#23450;&#20013;&#24515;&#30340;RBF&#32593;&#32476;&#30340;&#36924;&#36817;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#24452;&#21521;&#22522;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#31867;&#21035;&#65292;&#20854;&#20013;&#24179;&#28369;&#22240;&#23376;&#34987;&#26367;&#25442;&#20026;&#20301;&#31227;&#12290;&#25105;&#20204;&#22312;&#28608;&#27963;&#20989;&#25968;&#30340;&#19968;&#23450;&#26465;&#20214;&#19979;&#35777;&#26126;&#20102;&#36825;&#20123;&#32593;&#32476;&#33021;&#22815;&#36924;&#36817;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;d&#32500;&#32039;&#33268;&#23376;&#38598;&#19978;&#30340;&#20219;&#20309;&#36830;&#32493;&#22810;&#20803;&#20989;&#25968;&#12290;&#23545;&#20110;&#26377;&#38480;&#20010;&#22266;&#23450;&#20013;&#24515;&#30340;RBF&#32593;&#32476;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20445;&#35777;&#20219;&#24847;&#31934;&#24230;&#36924;&#36817;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we consider a new class of RBF (Radial Basis Function) neural networks, in which smoothing factors are replaced with shifts. We prove under certain conditions on the activation function that these networks are capable of approximating any continuous multivariate function on any compact subset of the $d$-dimensional Euclidean space. For RBF networks with finitely many fixed centroids we describe conditions guaranteeing approximation with arbitrary precision.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;X&#23556;&#32447;CT&#22270;&#20687;&#25581;&#31034;&#36203;&#24211;&#20848;&#23612;&#22982;&#32440;&#33609;&#21367;&#38544;&#34255;&#25991;&#26412;&#30340;&#36719;&#20214;&#31649;&#36947;&#21644;&#25968;&#25454;&#38598;&#12290;&#20182;&#20204;&#36816;&#29992;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#20960;&#20309;&#26694;&#26550;&#30340;&#26041;&#27861;&#26816;&#27979;&#8220;&#19981;&#21487;&#35265;&#8221;&#30340;&#30899;&#22696;&#65292;&#36798;&#21040;&#20102;&#20154;&#31867;&#19987;&#23478;&#26631;&#35760;&#32773;&#38590;&#20197;&#36798;&#21040;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.02084</link><description>&lt;p&gt;
&#12298;EduceLab-Scrolls&#65306;&#21033;&#29992;X&#23556;&#32447;CT&#20174;&#36203;&#24211;&#20848;&#23612;&#22982;&#32440;&#33609;&#21367;&#20013;&#21487;&#39564;&#35777;&#22320;&#24674;&#22797;&#25991;&#26412;&#12299;
&lt;/p&gt;
&lt;p&gt;
EduceLab-Scrolls: Verifiable Recovery of Text from Herculaneum Papyri using X-ray CT. (arXiv:2304.02084v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02084
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;X&#23556;&#32447;CT&#22270;&#20687;&#25581;&#31034;&#36203;&#24211;&#20848;&#23612;&#22982;&#32440;&#33609;&#21367;&#38544;&#34255;&#25991;&#26412;&#30340;&#36719;&#20214;&#31649;&#36947;&#21644;&#25968;&#25454;&#38598;&#12290;&#20182;&#20204;&#36816;&#29992;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#20960;&#20309;&#26694;&#26550;&#30340;&#26041;&#27861;&#26816;&#27979;&#8220;&#19981;&#21487;&#35265;&#8221;&#30340;&#30899;&#22696;&#65292;&#36798;&#21040;&#20102;&#20154;&#31867;&#19987;&#23478;&#26631;&#35760;&#32773;&#38590;&#20197;&#36798;&#21040;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#25581;&#31034;&#36203;&#24211;&#20848;&#23612;&#22982;&#32440;&#33609;&#21367;&#38544;&#34255;&#25991;&#26412;&#30340;&#23436;&#25972;&#36719;&#20214;&#31649;&#36947;&#12290;&#36825;&#20010;&#22686;&#24378;&#30340;&#34394;&#25311;&#23637;&#24320;&#27969;&#27700;&#32447;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#19968;&#31181;&#26032;&#39062;&#30340;&#20960;&#20309;&#26694;&#26550;&#30456;&#32467;&#21512;&#65292;&#23558;&#19977;&#32500;&#21644;&#20108;&#32500;&#22270;&#20687;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;EduceLab-Scrolls&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#20195;&#34920;&#20102;&#20108;&#21313;&#24180;&#26469;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#30740;&#31350;&#21162;&#21147;&#12290;EduceLab-Scrolls&#21253;&#21547;&#20102;&#19968;&#32452;&#23567;&#30862;&#29255;&#21644;&#23436;&#25972;&#21367;&#36724;&#30340;&#20307;&#31215;X&#23556;&#32447;CT&#22270;&#20687;&#12290;&#35813;&#25968;&#25454;&#38598;&#36824;&#21253;&#21547;&#29992;&#20110;&#30417;&#30563;&#35757;&#32451;&#27833;&#22696;&#26816;&#27979;&#27169;&#22411;&#30340;&#20108;&#32500;&#22270;&#20687;&#26631;&#31614;&#12290;&#36890;&#36807;&#23558;&#21367;&#36724;&#30862;&#29255;&#30340;&#39057;&#35889;&#29031;&#29255;&#19982;&#30456;&#21516;&#30862;&#29255;&#30340;X&#23556;&#32447;CT&#22270;&#20687;&#23545;&#20934;&#65292;&#20174;&#32780;&#21019;&#24314;&#20102;&#19968;&#20010;&#21487;&#26426;&#22120;&#23398;&#20064;&#30340;&#22270;&#20687;&#31354;&#38388;&#21644;&#27169;&#24577;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#36825;&#31181;&#23545;&#20934;&#20801;&#35768;&#26377;&#30417;&#30563;&#22320;&#23398;&#20064;&#26816;&#27979;X&#23556;&#32447;CT&#20013;&#8220;&#38544;&#24418;&#8221;&#30899;&#22696;&#30340;&#20219;&#21153;&#65292;&#21363;&#20351;&#23545;&#20110;&#20154;&#31867;&#19987;&#23478;&#26631;&#35760;&#32773;&#26469;&#35828;&#20063;&#26159;&#8220;&#19981;&#21487;&#33021;&#8221;&#30340;&#20219;&#21153;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23545;&#40784;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a complete software pipeline for revealing the hidden texts of the Herculaneum papyri using X-ray CT images. This enhanced virtual unwrapping pipeline combines machine learning with a novel geometric framework linking 3D and 2D images. We also present EduceLab-Scrolls, a comprehensive open dataset representing two decades of research effort on this problem. EduceLab-Scrolls contains a set of volumetric X-ray CT images of both small fragments and intact, rolled scrolls. The dataset also contains 2D image labels that are used in the supervised training of an ink detection model. Labeling is enabled by aligning spectral photography of scroll fragments with X-ray CT images of the same fragments, thus creating a machine-learnable mapping between image spaces and modalities. This alignment permits supervised learning for the detection of "invisible" carbon ink in X-ray CT, a task that is "impossible" even for human expert labelers. To our knowledge, this is the first aligned datas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26377;&#36873;&#25321;&#30340;&#32852;&#37030;&#33976;&#39311;&#26426;&#21046;Selective-FD&#65292;&#21487;&#20197;&#31934;&#30830;&#22320;&#35782;&#21035;&#26469;&#33258;&#26412;&#22320;&#21644;&#38598;&#21512;&#39044;&#27979;&#30340;&#30693;&#35782;&#65292;&#20197;&#35299;&#20915;&#23616;&#37096;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#24322;&#21644;&#32570;&#20047;&#22909;&#30340;&#25945;&#24072;&#27169;&#22411;&#32780;&#23548;&#33268;&#30340;&#35823;&#23548;&#21644;&#27169;&#31946;&#30340;&#30693;&#35782;&#20849;&#20139;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#39640;&#30340;&#27169;&#22411;&#24615;&#33021;&#21644;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.01731</link><description>&lt;p&gt;
&#38754;&#21521;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#33976;&#39311;&#30340;&#26377;&#36873;&#25321;&#30693;&#35782;&#20849;&#20139;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Selective Knowledge Sharing for Privacy-Preserving Federated Distillation without A Good Teacher. (arXiv:2304.01731v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26377;&#36873;&#25321;&#30340;&#32852;&#37030;&#33976;&#39311;&#26426;&#21046;Selective-FD&#65292;&#21487;&#20197;&#31934;&#30830;&#22320;&#35782;&#21035;&#26469;&#33258;&#26412;&#22320;&#21644;&#38598;&#21512;&#39044;&#27979;&#30340;&#30693;&#35782;&#65292;&#20197;&#35299;&#20915;&#23616;&#37096;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#24322;&#21644;&#32570;&#20047;&#22909;&#30340;&#25945;&#24072;&#27169;&#22411;&#32780;&#23548;&#33268;&#30340;&#35823;&#23548;&#21644;&#27169;&#31946;&#30340;&#30693;&#35782;&#20849;&#20139;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#39640;&#30340;&#27169;&#22411;&#24615;&#33021;&#21644;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#21327;&#20316;&#23398;&#20064;&#26041;&#27861;&#65292;&#20294;&#26159;&#23481;&#26131;&#21463;&#21040;&#30333;&#30418;&#25915;&#20987;&#65292;&#24182;&#19988;&#38590;&#20197;&#36866;&#24212;&#24322;&#26500;&#23458;&#25143;&#31471;&#12290;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#32852;&#37030;&#33976;&#39311;&#26159;&#19968;&#31181;&#25552;&#20379;&#22686;&#24378;&#38544;&#31169;&#20445;&#35777;&#24182;&#35299;&#20915;&#27169;&#22411;&#24322;&#26500;&#24615;&#30340;&#26367;&#20195;&#33539;&#20363;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#36873;&#25321;&#30340;&#32852;&#37030;&#33976;&#39311;&#26426;&#21046;Selective-FD&#26469;&#24212;&#23545;&#23616;&#37096;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#24322;&#21644;&#32570;&#20047;&#22909;&#30340;&#25945;&#24072;&#27169;&#22411;&#32780;&#23548;&#33268;&#30340;&#35823;&#23548;&#21644;&#27169;&#31946;&#30340;&#30693;&#35782;&#20849;&#20139;&#38382;&#39064;&#12290;&#23427;&#21253;&#25324;&#23458;&#25143;&#31471;&#36873;&#25321;&#22120;&#21644;&#26381;&#21153;&#22120;&#36873;&#25321;&#22120;&#65292;&#20197;&#31934;&#30830;&#22320;&#35782;&#21035;&#26469;&#33258;&#26412;&#22320;&#21644;&#38598;&#21512;&#39044;&#27979;&#30340;&#30693;&#35782;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;Selective-FD&#21487;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
While federated learning is promising for privacy-preserving collaborative learning without revealing local data, it remains vulnerable to white-box attacks and struggles to adapt to heterogeneous clients. Federated distillation (FD), built upon knowledge distillation--an effective technique for transferring knowledge from a teacher model to student models--emerges as an alternative paradigm, which provides enhanced privacy guarantees and addresses model heterogeneity. Nevertheless, challenges arise due to variations in local data distributions and the absence of a well-trained teacher model, which leads to misleading and ambiguous knowledge sharing that significantly degrades model performance. To address these issues, this paper proposes a selective knowledge sharing mechanism for FD, termed Selective-FD. It includes client-side selectors and a server-side selector to accurately and precisely identify knowledge from local and ensemble predictions, respectively. Empirical studies, bac
&lt;/p&gt;</description></item><item><title>EPVT&#26159;&#19968;&#31181;&#22522;&#20110;&#29615;&#22659;&#24863;&#30693;&#30340;&#25552;&#31034;&#35270;&#35273;Transformer&#65292;&#29992;&#20110;&#35299;&#20915;&#30382;&#32932;&#30149;&#21464;&#35782;&#21035;&#20013;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#36807;&#24230;&#20381;&#36182;&#30142;&#30149;&#19981;&#30456;&#20851;&#22270;&#20687;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23884;&#20837;&#19968;&#32452;&#39046;&#22495;&#25552;&#31034;&#21644;&#19968;&#20010;&#20849;&#20139;&#25552;&#31034;&#26469;&#36827;&#34892;&#39046;&#22495;&#19968;&#33324;&#21270;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#39046;&#22495;&#25552;&#31034;&#29983;&#25104;&#22120;&#20419;&#36827;&#30693;&#35782;&#20849;&#20139;&#12290;</title><link>http://arxiv.org/abs/2304.01508</link><description>&lt;p&gt;
EPVT: &#22522;&#20110;&#29615;&#22659;&#24863;&#30693;&#30340;&#25552;&#31034;&#35270;&#35273;Transformer&#22312;&#30382;&#32932;&#30149;&#21464;&#35782;&#21035;&#39046;&#22495;&#19968;&#33324;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
EPVT: Environment-aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition. (arXiv:2304.01508v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01508
&lt;/p&gt;
&lt;p&gt;
EPVT&#26159;&#19968;&#31181;&#22522;&#20110;&#29615;&#22659;&#24863;&#30693;&#30340;&#25552;&#31034;&#35270;&#35273;Transformer&#65292;&#29992;&#20110;&#35299;&#20915;&#30382;&#32932;&#30149;&#21464;&#35782;&#21035;&#20013;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#36807;&#24230;&#20381;&#36182;&#30142;&#30149;&#19981;&#30456;&#20851;&#22270;&#20687;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23884;&#20837;&#19968;&#32452;&#39046;&#22495;&#25552;&#31034;&#21644;&#19968;&#20010;&#20849;&#20139;&#25552;&#31034;&#26469;&#36827;&#34892;&#39046;&#22495;&#19968;&#33324;&#21270;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#39046;&#22495;&#25552;&#31034;&#29983;&#25104;&#22120;&#20419;&#36827;&#30693;&#35782;&#20849;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#30382;&#32932;&#30149;&#21464;&#35782;&#21035;&#24050;&#21462;&#24471;&#37325;&#22823;&#36827;&#23637;&#65292;&#32780;&#22312;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#37096;&#32626;&#36825;&#20123;&#31995;&#32479;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29992;&#20110;&#30382;&#32932;&#30149;&#21464;&#35782;&#21035;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#36807;&#24230;&#20381;&#36182;&#20110;&#19982;&#30142;&#30149;&#19981;&#30456;&#20851;&#30340;&#22270;&#20687;&#29305;&#24449;&#65288;&#22914;&#26263;&#35282;&#12289;&#27987;&#23494;&#27611;&#21457;&#65289;&#65292;&#23548;&#33268;&#22312;&#30475;&#19981;&#35265;&#30340;&#29615;&#22659;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39046;&#22495;&#19968;&#33324;&#21270;&#26041;&#27861;&#8212;&#8212;EPVT&#65292;&#23427;&#23558;&#25552;&#31034;&#23884;&#20837;&#21040;Vision Transformer&#20013;&#65292;&#20197;&#21327;&#21516;&#23398;&#20064;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#30693;&#35782;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;EPVT&#21033;&#29992;&#19968;&#32452;&#39046;&#22495;&#25552;&#31034;&#65292;&#27599;&#20010;&#39046;&#22495;&#25552;&#31034;&#37117;&#25198;&#28436;&#39046;&#22495;&#19987;&#23478;&#30340;&#35282;&#33394;&#65292;&#20197;&#25429;&#33719;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782;&#65307;&#20197;&#21450;&#19968;&#20010;&#20849;&#20139;&#25552;&#31034;&#26469;&#33719;&#24471;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#36890;&#29992;&#30693;&#35782;&#12290;&#20026;&#20102;&#20419;&#36827;&#30693;&#35782;&#20849;&#20139;&#21644;&#19981;&#21516;&#25552;&#31034;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#39046;&#22495;&#25552;&#31034;&#29983;&#25104;&#22120;&#65292;&#23427;&#20351;&#24471;&#39046;&#22495;&#25552;&#31034;&#19982;&#20849;&#20139;&#25552;&#31034;&#20043;&#38388;&#21487;&#20197;&#36827;&#34892;&#20302;&#31209;&#20056;&#24615;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Skin lesion recognition using deep learning has made remarkable progress, and there is an increasing need for deploying these systems in real-world scenarios. However, recent research has revealed that deep neural networks for skin lesion recognition may overly depend on disease-irrelevant image artifacts (i.e. dark corners, dense hairs), leading to poor generalization in unseen environments. To address this issue, we propose a novel domain generalization method called EPVT, which involves embedding prompts into the vision transformer to collaboratively learn knowledge from diverse domains. Concretely, EPVT leverages a set of domain prompts, each of which plays as a domain expert, to capture domain-specific knowledge; and a shared prompt for general knowledge over the entire dataset. To facilitate knowledge sharing and the interaction of different prompts, we introduce a domain prompt generator that enables low-rank multiplicative updates between domain prompts and the shared prompt. A
&lt;/p&gt;</description></item><item><title>TPU v4&#26159;&#19968;&#27454;&#25903;&#25345;&#23884;&#20837;&#24335;&#30828;&#20214;&#30340;&#21487;&#37325;&#26500;&#20809;&#23398;&#36229;&#32423;&#35745;&#31639;&#26426;&#65292;&#37319;&#29992;&#20809;&#23398;&#30005;&#36335;&#20132;&#25442;&#26426;&#37325;&#26032;&#37197;&#32622;&#20114;&#36830;&#25299;&#25169;&#65292;&#25552;&#39640;&#35268;&#27169;&#12289;&#21487;&#29992;&#24615;&#12289;&#21033;&#29992;&#29575;&#12289;&#27169;&#22359;&#21270;&#12289;&#37096;&#32626;&#12289;&#23433;&#20840;&#12289;&#21151;&#29575;&#21644;&#24615;&#33021;&#65292;&#23427;&#36890;&#36807;SparseCores&#21152;&#36895;&#23884;&#20837;&#24335;&#27169;&#22411;&#65292;&#24615;&#33021;&#20248;&#36234;&#65292;&#21151;&#32791;&#20302;&#12290;</title><link>http://arxiv.org/abs/2304.01433</link><description>&lt;p&gt;
TPU v4&#65306;&#19968;&#27454;&#25903;&#25345;&#23884;&#20837;&#24335;&#30828;&#20214;&#30340;&#21487;&#37325;&#26500;&#20809;&#23398;&#36229;&#32423;&#35745;&#31639;&#26426;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings. (arXiv:2304.01433v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01433
&lt;/p&gt;
&lt;p&gt;
TPU v4&#26159;&#19968;&#27454;&#25903;&#25345;&#23884;&#20837;&#24335;&#30828;&#20214;&#30340;&#21487;&#37325;&#26500;&#20809;&#23398;&#36229;&#32423;&#35745;&#31639;&#26426;&#65292;&#37319;&#29992;&#20809;&#23398;&#30005;&#36335;&#20132;&#25442;&#26426;&#37325;&#26032;&#37197;&#32622;&#20114;&#36830;&#25299;&#25169;&#65292;&#25552;&#39640;&#35268;&#27169;&#12289;&#21487;&#29992;&#24615;&#12289;&#21033;&#29992;&#29575;&#12289;&#27169;&#22359;&#21270;&#12289;&#37096;&#32626;&#12289;&#23433;&#20840;&#12289;&#21151;&#29575;&#21644;&#24615;&#33021;&#65292;&#23427;&#36890;&#36807;SparseCores&#21152;&#36895;&#23884;&#20837;&#24335;&#27169;&#22411;&#65292;&#24615;&#33021;&#20248;&#36234;&#65292;&#21151;&#32791;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21019;&#26032;&#65292;&#29983;&#20135;&#24037;&#20316;&#36127;&#36733;&#21457;&#29983;&#20102;&#26681;&#26412;&#24615;&#21644;&#36805;&#36895;&#30340;&#21464;&#21270;&#12290;TPU v4&#26159;&#35895;&#27468;&#30340;&#31532;&#20116;&#20195;&#38754;&#21521;&#29305;&#23450;&#39046;&#22495;&#26550;&#26500;&#65288;DSA&#65289;&#65292;&#26159;&#20854;&#31532;&#19977;&#20010;&#29992;&#20110;&#22788;&#29702;&#27492;&#31867;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36229;&#32423;&#35745;&#31639;&#26426;&#12290;&#20809;&#23398;&#30005;&#36335;&#20132;&#25442;&#26426;&#65288;OCS&#65289;&#21160;&#24577;&#37325;&#26032;&#37197;&#32622;&#20854;&#20114;&#36830;&#25299;&#25169;&#65292;&#20197;&#25552;&#39640;&#35268;&#27169;&#12289;&#21487;&#29992;&#24615;&#12289;&#21033;&#29992;&#29575;&#12289;&#27169;&#22359;&#21270;&#12289;&#37096;&#32626;&#12289;&#23433;&#20840;&#12289;&#21151;&#29575;&#21644;&#24615;&#33021;&#12290;&#37096;&#32626;&#33258;2020&#24180;&#20197;&#26469;&#65292;TPU v4&#36229;&#32423;&#35745;&#31639;&#26426;&#30340;&#34920;&#29616;&#20248;&#20110;TPU v3&#65292;&#21516;&#26102;&#24615;&#33021;/Watt&#25552;&#39640;&#20102;2.7&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
In response to innovations in machine learning (ML) models, production workloads changed radically and rapidly. TPU v4 is the fifth Google domain specific architecture (DSA) and its third supercomputer for such ML models. Optical circuit switches (OCSes) dynamically reconfigure its interconnect topology to improve scale, availability, utilization, modularity, deployment, security, power, and performance; users can pick a twisted 3D torus topology if desired. Much cheaper, lower power, and faster than Infiniband, OCSes and underlying optical components are &lt;5% of system cost and &lt;3% of system power. Each TPU v4 includes SparseCores, dataflow processors that accelerate models that rely on embeddings by 5x-7x yet use only 5% of die area and power. Deployed since 2020, TPU v4 outperforms TPU v3 by 2.1x and improves performance/Watt by 2.7x. The TPU v4 supercomputer is 4x larger at 4096 chips and thus ~10x faster overall, which along with OCS flexibility helps large language models. For sim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340; DeepAccident &#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#39550;&#39542;&#20013;&#21457;&#29983;&#30340;&#20107;&#25925;&#22330;&#26223;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#36816;&#21160;&#21644;&#20107;&#25925;&#39044;&#27979;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#21487;&#29992;&#20110;&#30452;&#25509;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#31639;&#27861;&#30340;&#20107;&#25925;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.01168</link><description>&lt;p&gt;
DeepAccident&#65306;V2X&#33258;&#21160;&#39550;&#39542;&#36816;&#21160;&#21644;&#20107;&#25925;&#39044;&#27979;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DeepAccident: A Motion and Accident Prediction Benchmark for V2X Autonomous Driving. (arXiv:2304.01168v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340; DeepAccident &#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#39550;&#39542;&#20013;&#21457;&#29983;&#30340;&#20107;&#25925;&#22330;&#26223;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#36816;&#21160;&#21644;&#20107;&#25925;&#39044;&#27979;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#21487;&#29992;&#20110;&#30452;&#25509;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#31639;&#27861;&#30340;&#20107;&#25925;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#26159;&#33258;&#21160;&#39550;&#39542;&#30340;&#39318;&#35201;&#20219;&#21153;&#12290;&#20294;&#26159;&#65292;&#30446;&#21069;&#27809;&#26377;&#24050;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#25903;&#25345;&#33258;&#21160;&#39550;&#39542;&#30340;&#30452;&#25509;&#21644;&#21487;&#35299;&#37322;&#30340;&#23433;&#20840;&#35780;&#20272;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; DeepAccident&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#29616;&#23454;&#27169;&#25311;&#22120;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#32463;&#24120;&#22312;&#29616;&#23454;&#39550;&#39542;&#20013;&#21457;&#29983;&#30340;&#21508;&#31181;&#20107;&#25925;&#22330;&#26223;&#12290;DeepAccident &#25968;&#25454;&#38598;&#21253;&#21547; 57k &#20010;&#24102;&#27880;&#37322;&#24103;&#21644; 285k &#20010;&#24102;&#27880;&#37322;&#30340;&#26679;&#26412;&#65292;&#36825;&#20960;&#20046;&#26159;&#22823;&#35268;&#27169; nuScenes &#25968;&#25454;&#38598;&#30340; 7 &#20493;&#65292;&#20854;&#26679;&#26412;&#25968;&#20026; 40k&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;&#25152;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#31471;&#21040;&#31471;&#30340;&#36816;&#21160;&#21644;&#20107;&#25925;&#39044;&#27979;&#65292;&#21487;&#29992;&#20110;&#30452;&#25509;&#35780;&#20272;&#19981;&#21516;&#33258;&#21160;&#39550;&#39542;&#31639;&#27861;&#30340;&#20107;&#25925;&#39044;&#27979;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#27599;&#31181;&#22330;&#26223;&#65292;&#25105;&#20204;&#35774;&#32622;&#20102;&#22235;&#36742;&#36710;&#21644;&#19968;&#20010;&#22522;&#30784;&#35774;&#26045;&#26469;&#35760;&#24405;&#25968;&#25454;&#65292;&#20174;&#32780;&#20026;&#20107;&#25925;&#22330;&#26223;&#25552;&#20379;&#20102;&#22810;&#31181;&#35270;&#35282;&#65292;&#24182;&#20351; V2X&#65288;&#36710;&#36742;&#23545;&#19968;&#20999;&#65289;&#24863;&#30693;&#21644;&#39044;&#27979;&#30740;&#31350;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safety is the primary priority of autonomous driving. Nevertheless, no published dataset currently supports the direct and explainable safety evaluation for autonomous driving. In this work, we propose DeepAccident, a large-scale dataset generated via a realistic simulator containing diverse accident scenarios that frequently occur in real-world driving. The proposed DeepAccident dataset contains 57K annotated frames and 285K annotated samples, approximately 7 times more than the large-scale nuScenes dataset with 40k annotated samples. In addition, we propose a new task, end-to-end motion and accident prediction, based on the proposed dataset, which can be used to directly evaluate the accident prediction ability for different autonomous driving algorithms. Furthermore, for each scenario, we set four vehicles along with one infrastructure to record data, thus providing diverse viewpoints for accident scenarios and enabling V2X (vehicle-to-everything) research on perception and predicti
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#31181;&#26032;&#38544;&#31169;&#25285;&#24551;&#65306;&#20598;&#28982;&#29983;&#25104;&#65292;&#21487;&#33021;&#35823;&#23548;&#27861;&#24459;&#21644;&#30417;&#31649;&#26426;&#26500;&#65292;&#26292;&#38706;&#20010;&#20154;&#32918;&#20687;&#30340;&#27861;&#24459;&#21644;&#30417;&#31649;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2304.01108</link><description>&lt;p&gt;
&#20598;&#28982;&#29983;&#25104;&#8212;&#8212;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#38544;&#31169;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Coincidental Generation. (arXiv:2304.01108v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01108
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#31181;&#26032;&#38544;&#31169;&#25285;&#24551;&#65306;&#20598;&#28982;&#29983;&#25104;&#65292;&#21487;&#33021;&#35823;&#23548;&#27861;&#24459;&#21644;&#30417;&#31649;&#26426;&#26500;&#65292;&#26292;&#38706;&#20010;&#20154;&#32918;&#20687;&#30340;&#27861;&#24459;&#21644;&#30417;&#31649;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#36328;&#21508;&#34892;&#19994;&#30340;&#22810;&#21151;&#33021;&#24037;&#20855;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#38544;&#31169;&#25968;&#25454;&#20849;&#20139;&#12289;&#35745;&#31639;&#33402;&#26415;&#12289;&#20135;&#21697;&#21644;&#26381;&#21153;&#30340;&#20010;&#24615;&#21270;&#20197;&#21450;&#27785;&#28024;&#24335;&#23089;&#20048;&#31561;&#39046;&#22495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#37319;&#29992;&#21644;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#20013;&#20986;&#29616;&#30340;&#19968;&#31181;&#26032;&#30340;&#38544;&#31169;&#25285;&#24551;&#8212;&#8212;&#20598;&#28982;&#29983;&#25104;&#12290;&#22312;&#20598;&#28982;&#29983;&#25104;&#20013;&#65292;&#29983;&#25104;&#27169;&#22411;&#30340;&#36755;&#20986;&#19982;&#29616;&#26377;&#23454;&#20307;&#30456;&#20284;&#65292;&#36229;&#20986;&#20102;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#25152;&#20195;&#34920;&#30340;&#33539;&#22260;&#65292;&#29978;&#33267;&#34987;&#35823;&#35748;&#20026;&#26159;&#26576;&#20010;&#23454;&#20307;&#12290;&#20030;&#20010;&#20363;&#23376;&#65292;&#34394;&#25311;&#27169;&#29305;&#26426;&#26500;&#21644;&#34394;&#25311;&#32929;&#31080;&#29031;&#29255;&#31561;&#21830;&#19994;&#24212;&#29992;&#20013;&#24120;&#24120;&#20351;&#29992;&#21512;&#25104;&#32918;&#20687;&#22270;&#29983;&#25104;&#22120;&#12290;&#30001;&#20110;&#20154;&#33080;&#24863;&#30693;&#30340;&#20302;&#20869;&#22312;&#32500;&#24230;&#65292;&#27599;&#20010;&#21512;&#25104;&#29983;&#25104;&#30340;&#33080;&#37117;&#20250;&#20598;&#28982;&#22320;&#25110;&#22810;&#25110;&#23569;&#22320;&#31867;&#20284;&#20110;&#30495;&#23454;&#20154;&#29289;&#12290;&#36825;&#26679;&#30340;&#20598;&#28982;&#29983;&#25104;&#20363;&#23376;&#20960;&#20046;&#21487;&#20197;&#20445;&#35777;&#35270;&#35273;&#30456;&#20284;&#24615;&#35823;&#23548;&#27861;&#24459;&#21644;&#30417;&#31649;&#26426;&#26500;&#65292;&#26292;&#38706;&#20102;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#32452;&#32455;&#38754;&#20020;&#30340;&#27861;&#24459;&#21644;&#30417;&#31649;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative A.I. models have emerged as versatile tools across diverse industries, with applications in privacy-preserving data sharing, computational art, personalization of products and services, and immersive entertainment. Here, we introduce a new privacy concern in the adoption and use of generative A.I. models: that of coincidental generation, where a generative model's output is similar enough to an existing entity, beyond those represented in the dataset used to train the model, to be mistaken for it. Consider, for example, synthetic portrait generators, which are today deployed in commercial applications such as virtual modeling agencies and synthetic stock photography. Due to the low intrinsic dimensionality of human face perception, every synthetically generated face will coincidentally resemble an actual person. Such examples of coincidental generation all but guarantee the misappropriation of likeness and expose organizations that use generative A.I. to legal and regulatory
&lt;/p&gt;</description></item><item><title>RePAST&#26159;&#19968;&#31181;&#30456;&#23545;&#20301;&#23039;&#27880;&#24847;&#21147;&#22330;&#26223;&#34920;&#31034;&#21464;&#25442;&#22120;&#65292;&#20854;&#23558;&#25104;&#23545;&#30340;&#30456;&#23545;&#30456;&#26426;&#23039;&#24577;&#20449;&#24687;&#30452;&#25509;&#27880;&#20837;&#36716;&#25442;&#22120;&#30340;&#27880;&#24847;&#26426;&#21046;&#20013;&#65292;&#19981;&#38656;&#35201;&#22266;&#23450;&#21442;&#32771;&#24103;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#21407;&#22987;&#26041;&#27861;&#30340;&#20840;&#37096;&#21151;&#33021;&#65292;&#21152;&#20837;&#36825;&#31181;&#19981;&#21464;&#24615;&#24182;&#19981;&#20250;&#23548;&#33268;&#36136;&#37327;&#19979;&#38477;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2304.00947</link><description>&lt;p&gt;
RePAST&#65306;&#30456;&#23545;&#20301;&#23039;&#27880;&#24847;&#21147;&#22330;&#26223;&#34920;&#31034;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
RePAST: Relative Pose Attention Scene Representation Transformer. (arXiv:2304.00947v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00947
&lt;/p&gt;
&lt;p&gt;
RePAST&#26159;&#19968;&#31181;&#30456;&#23545;&#20301;&#23039;&#27880;&#24847;&#21147;&#22330;&#26223;&#34920;&#31034;&#21464;&#25442;&#22120;&#65292;&#20854;&#23558;&#25104;&#23545;&#30340;&#30456;&#23545;&#30456;&#26426;&#23039;&#24577;&#20449;&#24687;&#30452;&#25509;&#27880;&#20837;&#36716;&#25442;&#22120;&#30340;&#27880;&#24847;&#26426;&#21046;&#20013;&#65292;&#19981;&#38656;&#35201;&#22266;&#23450;&#21442;&#32771;&#24103;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#21407;&#22987;&#26041;&#27861;&#30340;&#20840;&#37096;&#21151;&#33021;&#65292;&#21152;&#20837;&#36825;&#31181;&#19981;&#21464;&#24615;&#24182;&#19981;&#20250;&#23548;&#33268;&#36136;&#37327;&#19979;&#38477;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22330;&#26223;&#34920;&#31034;&#21464;&#25442;&#22120;&#65288;SRT&#65289;&#26159;&#19968;&#31181;&#26368;&#36817;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#20132;&#20114;&#36895;&#29575;&#28210;&#26579;&#26032;&#35270;&#22270;&#12290;&#30001;&#20110;SRT&#20351;&#29992;&#30456;&#23545;&#20110;&#20219;&#24847;&#36873;&#25321;&#30340;&#21442;&#32771;&#25668;&#20687;&#26426;&#30340;&#30456;&#26426;&#23039;&#24577;&#65292;&#22240;&#27492;&#23427;&#23545;&#36755;&#20837;&#35270;&#22270;&#30340;&#39034;&#24207;&#19981;&#21464;&#12290;&#22240;&#27492;&#65292;SRT&#19981;&#30452;&#25509;&#36866;&#29992;&#20110;&#38656;&#35201;&#23450;&#26399;&#26356;&#25913;&#21442;&#32771;&#24103;&#30340;&#22823;&#35268;&#27169;&#22330;&#26223;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#23545;&#23039;&#24577;&#27880;&#24847;&#21147;SRT&#65288;RePAST&#65289;&#65306;&#25105;&#20204;&#23558;&#25104;&#23545;&#30340;&#30456;&#23545;&#30456;&#26426;&#23039;&#24577;&#20449;&#24687;&#30452;&#25509;&#27880;&#20837;&#36716;&#25442;&#22120;&#30340;&#27880;&#24847;&#26426;&#21046;&#20013;&#65292;&#32780;&#19981;&#26159;&#22312;&#36755;&#20837;&#26102;&#22266;&#23450;&#19968;&#20010;&#21442;&#32771;&#24103;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#20854;&#23450;&#20041;&#19981;&#21464;&#20110;&#20219;&#20309;&#20840;&#23616;&#21442;&#32771;&#24103;&#30340;&#36873;&#25321;&#65292;&#21516;&#26102;&#20173;&#20445;&#30041;&#21407;&#22987;&#26041;&#27861;&#30340;&#20840;&#37096;&#21151;&#33021;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#36825;&#31181;&#19981;&#21464;&#24615;&#28155;&#21152;&#21040;&#27169;&#22411;&#20013;&#24182;&#19981;&#20250;&#23548;&#33268;&#36136;&#37327;&#19979;&#38477;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#21521;&#24212;&#29992;&#23436;&#20840;&#28508;&#22312;&#30340;&#22522;&#20110;Transformer&#30340;&#28210;&#26579;&#26041;&#27861;&#20110;&#22823;&#35268;&#27169;&#22330;&#26223;&#36808;&#20986;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Scene Representation Transformer (SRT) is a recent method to render novel views at interactive rates. Since SRT uses camera poses with respect to an arbitrarily chosen reference camera, it is not invariant to the order of the input views. As a result, SRT is not directly applicable to large-scale scenes where the reference frame would need to be changed regularly. In this work, we propose Relative Pose Attention SRT (RePAST): Instead of fixing a reference frame at the input, we inject pairwise relative camera pose information directly into the attention mechanism of the Transformers. This leads to a model that is by definition invariant to the choice of any global reference frame, while still retaining the full capabilities of the original method. Empirical results show that adding this invariance to the model does not lead to a loss in quality. We believe that this is a step towards applying fully latent transformer-based rendering methods to large-scale scenes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#37327;&#21270;&#19981;&#21516;&#21306;&#22495;&#23545;&#30446;&#26631;&#35782;&#21035;&#30340;&#36129;&#29486;&#21644;&#35299;&#37322;&#25968;&#25454;&#20559;&#24046;&#21644;&#27169;&#22411;&#20559;&#24046;&#23545;&#38750;&#22240;&#26524;&#24615;&#30340;&#24433;&#21709;&#65292;&#25552;&#20379;&#20102;&#25913;&#21892;&#28145;&#24230;&#23398;&#20064;&#22312;SAR ATR&#20013;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.00668</link><description>&lt;p&gt;
&#22312;SAR ATR&#20013;&#21457;&#29616;&#21644;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#30340;&#38750;&#22240;&#26524;&#24615;
&lt;/p&gt;
&lt;p&gt;
Discovering and Explaining the Non-Causality of Deep Learning in SAR ATR. (arXiv:2304.00668v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#37327;&#21270;&#19981;&#21516;&#21306;&#22495;&#23545;&#30446;&#26631;&#35782;&#21035;&#30340;&#36129;&#29486;&#21644;&#35299;&#37322;&#25968;&#25454;&#20559;&#24046;&#21644;&#27169;&#22411;&#20559;&#24046;&#23545;&#38750;&#22240;&#26524;&#24615;&#30340;&#24433;&#21709;&#65292;&#25552;&#20379;&#20102;&#25913;&#21892;&#28145;&#24230;&#23398;&#20064;&#22312;SAR ATR&#20013;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;SAR ATR&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#24182;&#22312;MSTAR&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21463;&#38480;&#30340;&#25104;&#20687;&#26465;&#20214;&#65292;MSTAR&#23384;&#22312;&#32972;&#26223;&#30456;&#20851;&#31561;&#25968;&#25454;&#20559;&#35265;&#65292;&#21363;&#32972;&#26223;&#26434;&#27874;&#29305;&#24615;&#19982;&#30446;&#26631;&#31867;&#21035;&#23384;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#28145;&#24230;&#23398;&#20064;&#21487;&#20197;&#36807;&#24230;&#25311;&#21512;&#26434;&#27874;&#20197;&#20943;&#23569;&#35757;&#32451;&#35823;&#24046;&#12290;&#22240;&#27492;&#65292;&#26434;&#27874;&#30340;&#36807;&#24230;&#25311;&#21512;&#31243;&#24230;&#21453;&#26144;&#20102;SAR ATR&#20013;&#28145;&#24230;&#23398;&#20064;&#30340;&#38750;&#22240;&#26524;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#20165;&#23450;&#24615;&#20998;&#26512;&#27492;&#29616;&#35937;&#12290;&#26412;&#25991;&#22522;&#20110;Shapley&#20540;&#37327;&#21270;&#19981;&#21516;&#21306;&#22495;&#23545;&#30446;&#26631;&#35782;&#21035;&#30340;&#36129;&#29486;&#12290;&#26434;&#27874;&#30340;Shapley&#20540;&#21487;&#20197;&#34913;&#37327;&#20854;&#36807;&#24230;&#25311;&#21512;&#31243;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#25968;&#25454;&#20559;&#24046;&#21644;&#27169;&#22411;&#20559;&#24046;&#23545;&#38750;&#22240;&#26524;&#24615;&#30340;&#24433;&#21709;&#12290;&#31616;&#35328;&#20043;&#65292;&#25968;&#25454;&#20559;&#24046;&#23548;&#33268;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20013;&#30340;&#20449;&#26434;&#27604;&#21644;&#26434;&#27874;&#32441;&#29702;&#21487;&#27604;&#65292;&#32780;&#21508;&#31181;&#27169;&#22411;&#32467;&#26500;&#23545;&#36825;&#20123;&#20559;&#24046;&#30340;&#36807;&#25311;&#21512;&#31243;&#24230;&#19981;&#21516;&#12290;&#38750;&#22240;&#26524;&#24615;&#30340;&#35299;&#37322;&#20026;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#22312;SAR ATR&#20013;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep learning has been widely used in SAR ATR and achieved excellent performance on the MSTAR dataset. However, due to constrained imaging conditions, MSTAR has data biases such as background correlation, i.e., background clutter properties have a spurious correlation with target classes. Deep learning can overfit clutter to reduce training errors. Therefore, the degree of overfitting for clutter reflects the non-causality of deep learning in SAR ATR. Existing methods only qualitatively analyze this phenomenon. In this paper, we quantify the contributions of different regions to target recognition based on the Shapley value. The Shapley value of clutter measures the degree of overfitting. Moreover, we explain how data bias and model bias contribute to non-causality. Concisely, data bias leads to comparable signal-to-clutter ratios and clutter textures in training and test sets. And various model structures have different degrees of overfitting for these biases. The exp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24314;&#26500;&#24615;&#21516;&#21270;&#26041;&#27861;&#65292;&#32467;&#21512;&#29983;&#25104;&#35270;&#22270;&#21644;&#19987;&#23478;&#36716;&#25442;&#65292;&#25552;&#39640;&#20102;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.00601</link><description>&lt;p&gt;
&#24314;&#26500;&#24615;&#21516;&#21270;&#65306;&#36890;&#36807;&#35270;&#22270;&#29983;&#25104;&#31574;&#30053;&#25552;&#21319;&#23545;&#27604;&#23398;&#20064;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Constructive Assimilation: Boosting Contrastive Learning Performance through View Generation Strategies. (arXiv:2304.00601v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24314;&#26500;&#24615;&#21516;&#21270;&#26041;&#27861;&#65292;&#32467;&#21512;&#29983;&#25104;&#35270;&#22270;&#21644;&#19987;&#23478;&#36716;&#25442;&#65292;&#25552;&#39640;&#20102;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65288;&#19987;&#23478;&#36716;&#25442;&#65289;&#30340;&#21464;&#25442;&#65292;&#20363;&#22914;&#38543;&#26426;&#35009;&#21098;&#21644;&#39068;&#33394;&#25200;&#21160;&#65292;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#65288;&#20363;&#22914;SimCLR&#65289;&#25104;&#21151;&#30340;&#20851;&#38190;&#12290;&#26368;&#36817;&#65292;&#26377;&#20960;&#27425;&#23581;&#35797;&#29992;&#23398;&#20064;&#30340;&#29983;&#25104;&#35270;&#22270;&#20195;&#26367;&#36825;&#26679;&#30340;&#39046;&#22495;&#29305;&#23450;&#30340;&#12289;&#20154;&#24037;&#35774;&#35745;&#30340;&#36716;&#25442;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#38024;&#23545;&#22270;&#20687;&#25968;&#25454;&#65292;&#36825;&#20123;&#35270;&#22270;&#29983;&#25104;&#26041;&#27861;&#37117;&#26080;&#27861;&#32988;&#20219;&#19987;&#23478;&#36716;&#25442;&#30340;&#24037;&#20316;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19981;&#21516;&#30340;&#38382;&#39064;&#65306;&#25105;&#20204;&#33021;&#21542;&#23558;&#29983;&#25104;&#30340;&#35270;&#22270;&#19982;&#19987;&#23478;&#36716;&#25442;&#24314;&#26500;&#24615;&#22320;&#21516;&#21270;&#65292;&#32780;&#19981;&#26159;&#23558;&#20854;&#26367;&#25442;&#20026;&#19987;&#23478;&#36716;&#25442;&#65311;&#25105;&#20204;&#32943;&#23450;&#22320;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35270;&#22270;&#29983;&#25104;&#26041;&#27861;&#21644;&#19968;&#20010;&#31616;&#21333;&#26377;&#25928;&#30340;&#21516;&#21270;&#26041;&#27861;&#65292;&#36825;&#20004;&#32773;&#19968;&#36215;&#25552;&#39640;&#20102;&#19977;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#27700;&#24179;&#65292;&#26368;&#39640;&#36798;&#21040;&#20102; ~3.6%&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#31995;&#32479;&#22320;&#20998;&#26512;&#20102;&#19968;&#31995;&#21015;&#30340;&#35270;&#22270;&#29983;&#25104;&#21644;&#21516;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformations based on domain expertise (expert transformations), such as random-resized-crop and color-jitter, have proven critical to the success of contrastive learning techniques such as SimCLR. Recently, several attempts have been made to replace such domain-specific, human-designed transformations with generated views that are learned. However for imagery data, so far none of these view-generation methods has been able to outperform expert transformations. In this work, we tackle a different question: instead of replacing expert transformations with generated views, can we constructively assimilate generated views with expert transformations? We answer this question in the affirmative and propose a view generation method and a simple, effective assimilation method that together improve the state-of-the-art by up to ~3.6% on three different datasets. Importantly, we conduct a detailed empirical study that systematically analyzes a range of view generation and assimilation method
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24674;&#22797;&#35302;&#21457;&#29366;&#24577;(RTS)&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;RL&#20195;&#29702;&#20813;&#21463;&#21453;&#21521;&#25915;&#20987;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#26500;&#24314;&#26367;&#20195;&#32593;&#32476;&#26469;&#36817;&#20284;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#23558;&#35302;&#21457;&#29366;&#24577;&#24674;&#22797;&#20026;&#24178;&#20928;&#29366;&#24577;&#26469;&#38450;&#27490;&#25915;&#20987;&#32773;&#36890;&#36807;&#35302;&#21457;&#22120;&#28608;&#27963;&#38544;&#34255;&#22312;&#20195;&#29702;&#20013;&#30340;&#21518;&#38376;&#12290;</title><link>http://arxiv.org/abs/2304.00252</link><description>&lt;p&gt;
RL&#20013;&#30340;&#21453;&#21521;&#25915;&#20987;&#20445;&#25252;&#65306;&#24674;&#22797;&#35302;&#21457;&#29366;&#24577;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Recover Triggered States: Protect Model Against Backdoor Attack in Reinforcement Learning. (arXiv:2304.00252v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24674;&#22797;&#35302;&#21457;&#29366;&#24577;(RTS)&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;RL&#20195;&#29702;&#20813;&#21463;&#21453;&#21521;&#25915;&#20987;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#26500;&#24314;&#26367;&#20195;&#32593;&#32476;&#26469;&#36817;&#20284;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#23558;&#35302;&#21457;&#29366;&#24577;&#24674;&#22797;&#20026;&#24178;&#20928;&#29366;&#24577;&#26469;&#38450;&#27490;&#25915;&#20987;&#32773;&#36890;&#36807;&#35302;&#21457;&#22120;&#28608;&#27963;&#38544;&#34255;&#22312;&#20195;&#29702;&#20013;&#30340;&#21518;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#25915;&#20987;&#21487;&#20197;&#20351;&#24694;&#24847;&#29992;&#25143;&#25805;&#32437;&#29615;&#22659;&#25110;&#30772;&#22351;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#23558;&#19968;&#20010;&#38544;&#34255;&#30340;&#21518;&#38376;&#25554;&#20837;&#21040;&#35757;&#32451;&#20195;&#29702;&#31243;&#24207;&#20013;&#12290;&#36825;&#31181;&#25915;&#20987;&#21361;&#21450;RL&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#65292;&#22312;&#21508;&#20010;&#20851;&#38190;&#39046;&#22495;&#21487;&#33021;&#20250;&#36896;&#25104;&#28798;&#38590;&#24615;&#30340;&#24433;&#21709;&#12290;&#19982;&#27492;&#30456;&#27604;&#65292;&#23545;&#20110;RL&#20013;&#30340;&#21453;&#21521;&#25915;&#20987;&#26377;&#25928;&#30340;&#38450;&#24481;&#25514;&#26045;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#8212;&#8212;&#24674;&#22797;&#35302;&#21457;&#29366;&#24577;(RTS)&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20445;&#25252;&#21463;&#23475;&#20195;&#29702;&#20813;&#21463;&#21453;&#21521;&#25915;&#20987;&#12290; RTS&#38656;&#35201;&#26500;&#24314;&#19968;&#20010;&#26367;&#20195;&#32593;&#32476;&#26469;&#36817;&#20284;&#21160;&#24577;&#27169;&#22411;&#12290;&#24320;&#21457;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#23558;&#35302;&#21457;&#29366;&#24577;&#24674;&#22797;&#20026;&#24178;&#20928;&#29366;&#24577;&#26469;&#38450;&#27490;&#25915;&#20987;&#32773;&#36890;&#36807;&#35302;&#21457;&#22120;&#28608;&#27963;&#20195;&#29702;&#20013;&#38544;&#34255;&#30340;&#21518;&#38376;&#12290;&#22312;&#35757;&#32451;&#26367;&#20195;&#32593;&#32476;&#26469;&#39044;&#27979;&#29366;&#24577;&#26102;&#65292;&#25105;&#20204;&#23558;&#20195;&#29702;&#21160;&#20316;&#20449;&#24687;&#24182;&#20837;&#65292;&#20943;&#23569;&#20195;&#29702;&#22312;&#39044;&#27979;&#29366;&#24577;&#19978;&#37319;&#21462;&#30340;&#21160;&#20316;&#21644;&#23454;&#38469;&#29366;&#24577;&#19978;&#37319;&#21462;&#30340;&#21160;&#20316;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
A backdoor attack allows a malicious user to manipulate the environment or corrupt the training data, thus inserting a backdoor into the trained agent. Such attacks compromise the RL system's reliability, leading to potentially catastrophic results in various key fields. In contrast, relatively limited research has investigated effective defenses against backdoor attacks in RL. This paper proposes the Recovery Triggered States (RTS) method, a novel approach that effectively protects the victim agents from backdoor attacks. RTS involves building a surrogate network to approximate the dynamics model. Developers can then recover the environment from the triggered state to a clean state, thereby preventing attackers from activating backdoors hidden in the agent by presenting the trigger. When training the surrogate to predict states, we incorporate agent action information to reduce the discrepancy between the actions taken by the agent on predicted states and the actions taken on real sta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20351;&#29992;&#38544;&#24335;&#25968;&#20540;&#21021;&#20540;&#38382;&#39064;&#27714;&#35299;&#22120;&#27169;&#26495;&#30340;ODE-nets&#12290;&#20351;&#29992;&#23637;&#24320;&#30340;&#38544;&#24335;&#26041;&#26696;&#23545;ODE-nets&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#36820;&#22238;&#19968;&#20010;&#25509;&#36817;&#20110;&#21453;&#36716;&#20462;&#27491;&#24494;&#20998;&#26041;&#31243;(IMDE)&#30340;&#36817;&#20284;&#20540;&#65292;&#24182;&#19988;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#33258;&#36866;&#24212;&#31639;&#27861;&#21152;&#36895;&#35757;&#32451;&#24182;&#20445;&#25345;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.17824</link><description>&lt;p&gt;
&#38544;&#24335;&#27169;&#26495;&#21270;ODE-nets&#30340;&#23454;&#29616;&#21644;(&#21453;&#36716;&#20462;&#27491;)&#35823;&#24046;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Implementation and (Inverse Modified) Error Analysis for implicitly-templated ODE-nets. (arXiv:2303.17824v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20351;&#29992;&#38544;&#24335;&#25968;&#20540;&#21021;&#20540;&#38382;&#39064;&#27714;&#35299;&#22120;&#27169;&#26495;&#30340;ODE-nets&#12290;&#20351;&#29992;&#23637;&#24320;&#30340;&#38544;&#24335;&#26041;&#26696;&#23545;ODE-nets&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#36820;&#22238;&#19968;&#20010;&#25509;&#36817;&#20110;&#21453;&#36716;&#20462;&#27491;&#24494;&#20998;&#26041;&#31243;(IMDE)&#30340;&#36817;&#20284;&#20540;&#65292;&#24182;&#19988;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#33258;&#36866;&#24212;&#31639;&#27861;&#21152;&#36895;&#35757;&#32451;&#24182;&#20445;&#25345;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30528;&#37325;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#38544;&#24335;&#25968;&#20540;&#21021;&#20540;&#38382;&#39064;&#27714;&#35299;&#22120;&#27169;&#26495;&#30340;ODE-nets&#26469;&#23398;&#20064;&#25968;&#25454;&#20013;&#30340;&#38544;&#21547;&#21160;&#21147;&#23398;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#23637;&#24320;&#30340;&#38544;&#24335;&#26041;&#26696;&#23545;ODE-nets&#36827;&#34892;&#21453;&#36716;&#20462;&#27491;&#35823;&#24046;&#20998;&#26512;&#20197;&#26041;&#20415;&#35299;&#37322;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#23637;&#24320;&#30340;&#38544;&#24335;&#26041;&#26696;&#23545;ODE-nets&#36827;&#34892;&#35757;&#32451;&#36820;&#22238;&#20102;&#19968;&#20010;&#25509;&#36817;&#20110;&#21453;&#36716;&#20462;&#27491;&#24494;&#20998;&#26041;&#31243;(IMDE)&#30340;&#36817;&#20284;&#20540;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#38024;&#23545;&#35757;&#32451;&#27492;&#31867;ODE-nets&#36827;&#34892;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#32780;&#24403;&#21069;&#30340;&#31574;&#30053;&#36890;&#24120;&#23558;ODE-nets&#30340;&#25968;&#20540;&#31215;&#20998;&#35270;&#20026;&#40657;&#21283;&#23376;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30417;&#27979;&#35823;&#24046;&#32423;&#21035;&#24182;&#35843;&#25972;(&#23637;&#24320;&#30340;)&#38544;&#24335;&#35299;&#27861;&#30340;&#36845;&#20195;&#27425;&#25968;&#65292;&#20197;&#20351;&#23637;&#24320;&#30340;&#36817;&#20284;&#35823;&#24046;&#23567;&#20110;&#24403;&#21069;&#30340;&#23398;&#20064;&#25439;&#22833;&#12290;&#36825;&#26377;&#21161;&#20110;&#21152;&#36895;&#35757;&#32451;&#65292;&#21516;&#26102;&#20445;&#25345;&#31934;&#24230;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#20010;&#25968;&#20540;&#23454;&#39564;&#20197;&#23637;&#31034;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We focus on learning hidden dynamics from data using ODE-nets templated on implicit numerical initial value problem solvers. First, we perform Inverse Modified error analysis of the ODE-nets using unrolled implicit schemes for ease of interpretation. It is shown that training an ODE-net using an unrolled implicit scheme returns a close approximation of an Inverse Modified Differential Equation (IMDE). In addition, we establish a theoretical basis for hyper-parameter selection when training such ODE-nets, whereas current strategies usually treat numerical integration of ODE-nets as a black box. We thus formulate an adaptive algorithm which monitors the level of error and adapts the number of (unrolled) implicit solution iterations during the training process, so that the error of the unrolled approximation is less than the current learning loss. This helps accelerate training, while maintaining accuracy. Several numerical experiments are performed to demonstrate the advantages of the pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;FPGA&#35774;&#22791;&#30340;&#22522;&#20110;&#24310;&#36831;&#30340;3D-CNN&#21152;&#36895;&#22120;&#24037;&#20855;&#38142;HARFLOW3D&#65292;&#23427;&#20197;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;FPGA&#30340;&#29305;&#24615;&#25551;&#36848;&#20026;&#36755;&#20837;&#65292;&#29983;&#25104;&#26368;&#23567;&#21270;&#35745;&#31639;&#24310;&#36831;&#30340;&#35774;&#35745;&#12290;&#23454;&#39564;&#35777;&#26126;HARFLOW3D&#30456;&#27604;&#20854;&#20182;&#26041;&#26696;&#33021;&#22815;&#23454;&#29616;&#26356;&#20302;&#30340;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2303.17218</link><description>&lt;p&gt;
HARFLOW3D&#65306;&#19968;&#31181;&#38754;&#21521;FPGA&#35774;&#22791;&#30340;&#22522;&#20110;&#24310;&#36831;&#30340;3D-CNN&#21152;&#36895;&#22120;&#24037;&#20855;&#38142;
&lt;/p&gt;
&lt;p&gt;
HARFLOW3D: A Latency-Oriented 3D-CNN Accelerator Toolflow for HAR on FPGA Devices. (arXiv:2303.17218v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;FPGA&#35774;&#22791;&#30340;&#22522;&#20110;&#24310;&#36831;&#30340;3D-CNN&#21152;&#36895;&#22120;&#24037;&#20855;&#38142;HARFLOW3D&#65292;&#23427;&#20197;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;FPGA&#30340;&#29305;&#24615;&#25551;&#36848;&#20026;&#36755;&#20837;&#65292;&#29983;&#25104;&#26368;&#23567;&#21270;&#35745;&#31639;&#24310;&#36831;&#30340;&#35774;&#35745;&#12290;&#23454;&#39564;&#35777;&#26126;HARFLOW3D&#30456;&#27604;&#20854;&#20182;&#26041;&#26696;&#33021;&#22815;&#23454;&#29616;&#26356;&#20302;&#30340;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#35777;&#26126;&#22312;&#20154;&#20307;&#21160;&#20316;&#35782;&#21035;&#20219;&#21153;&#20013;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27969;&#24335;&#26550;&#26500;&#30340;&#24037;&#20855;&#38142;&#65292;&#23558;&#27492;&#31867;&#27169;&#22411;&#26144;&#23556;&#21040;FPGA&#19978;&#65292;&#32771;&#34385;&#27169;&#22411;&#22266;&#26377;&#29305;&#24615;&#21644;&#30446;&#26631;FPGA&#35774;&#22791;&#30340;&#29305;&#24449;&#12290;HARFLOW3D&#24037;&#20855;&#38142;&#20197;ONNX&#26684;&#24335;&#30340;3D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;FPGA&#29305;&#24615;&#25551;&#36848;&#20026;&#36755;&#20837;&#65292;&#29983;&#25104;&#26368;&#23567;&#21270;&#35745;&#31639;&#24310;&#36831;&#30340;&#35774;&#35745;&#12290;&#35813;&#24037;&#20855;&#38142;&#30001;&#22810;&#20010;&#37096;&#20998;&#32452;&#25104;&#65292;&#21253;&#25324;i) 3D CNN&#35299;&#26512;&#22120;&#65292;ii) &#24615;&#33021;&#21644;&#36164;&#28304;&#27169;&#22411;&#65292;iii) &#29992;&#20110;&#22312;&#29983;&#25104;&#30340;&#30828;&#20214;&#19978;&#25191;&#34892;3D&#27169;&#22411;&#30340;&#35843;&#24230;&#31639;&#27861;&#65292;iv) &#38024;&#23545;3D&#27169;&#22411;&#37327;&#36523;&#23450;&#21046;&#30340;&#36164;&#28304;&#24863;&#30693;&#20248;&#21270;&#24341;&#25806;&#65292;v) &#33258;&#21160;&#26144;&#23556;&#21040;&#21487;&#21512;&#25104;&#30340;FPGA&#20195;&#30721;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;3D CNN&#21644;FPGA&#31995;&#32479;&#37197;&#23545;&#36827;&#34892;&#22810;&#20010;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#24037;&#20855;&#38142;&#25903;&#25345;&#24191;&#27867;&#27169;&#22411;&#21644;&#35774;&#22791;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;3D CNN&#21152;&#36895;&#22120;&#35774;&#35745;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#24037;&#20855;&#38142;&#23454;&#29616;&#20102;&#26356;&#20302;&#30340;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;
For Human Action Recognition tasks (HAR), 3D Convolutional Neural Networks have proven to be highly effective, achieving state-of-the-art results. This study introduces a novel streaming architecture based toolflow for mapping such models onto FPGAs considering the model's inherent characteristics and the features of the targeted FPGA device. The HARFLOW3D toolflow takes as input a 3D CNN in ONNX format and a description of the FPGA characteristics, generating a design that minimizes the latency of the computation. The toolflow is comprised of a number of parts, including i) a 3D CNN parser, ii) a performance and resource model, iii) a scheduling algorithm for executing 3D models on the generated hardware, iv) a resource-aware optimization engine tailored for 3D models, v) an automated mapping to synthesizable code for FPGAs. The ability of the toolflow to support a broad range of models and devices is shown through a number of experiments on various 3D CNN and FPGA system pairs. Furth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#26356;&#20016;&#23500;&#30340;&#35821;&#35328;&#21453;&#39304;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#36890;&#36807;&#19977;&#20010;&#36845;&#20195;&#27493;&#39588;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#20197;&#29983;&#25104;&#26356;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2303.16755</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#35268;&#27169;&#21270;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training Language Models with Language Feedback at Scale. (arXiv:2303.16755v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#26356;&#20016;&#23500;&#30340;&#35821;&#35328;&#21453;&#39304;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#36890;&#36807;&#19977;&#20010;&#36845;&#20195;&#27493;&#39588;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#20197;&#29983;&#25104;&#26356;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#32463;&#24120;&#29983;&#25104;&#19981;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#30340;&#36755;&#20986;&#65292;&#20363;&#22914;&#26377;&#23475;&#30340;&#25991;&#26412;&#25110;&#20107;&#23454;&#19981;&#27491;&#30830;&#30340;&#25688;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#23398;&#20064;&#19968;&#31181;&#31616;&#21333;&#30340;&#20154;&#31867;&#21453;&#39304;&#24418;&#24335;&#65288;&#21363;&#27169;&#22411;&#29983;&#25104;&#36755;&#20986;&#20043;&#38388;&#30340;&#27604;&#36739;&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#27604;&#36739;&#21453;&#39304;&#21482;&#33021;&#20256;&#36798;&#26377;&#38480;&#30340;&#20851;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8212;&#8212;&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65288;ILF&#65289;&#65292;&#23427;&#21033;&#29992;&#20102;&#26356;&#20016;&#23500;&#30340;&#35821;&#35328;&#21453;&#39304;&#12290;ILF&#30001;&#19977;&#20010;&#36845;&#20195;&#27493;&#39588;&#32452;&#25104;&#65306;&#31532;&#19968;&#27493;&#65292;&#26681;&#25454;&#36755;&#20837;&#65292;&#21021;&#22987;LM&#36755;&#20986;&#21644;&#21453;&#39304;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35843;&#33410;&#20197;&#29983;&#25104;&#25913;&#36827;&#12290;&#31532;&#20108;&#27493;&#65292;&#36873;&#25321;&#26368;&#22810;&#21453;&#39304;&#30340;&#25913;&#36827;&#12290;&#31532;&#19977;&#27493;&#65292;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#26368;&#22823;&#21270;&#22312;&#32473;&#23450;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#36873;&#25321;&#30340;&#25913;&#36827;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;ILF&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#31867;&#20284;&#20110;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;ILF&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models often generate outputs that are not in line with human preferences, such as harmful text or factually incorrect summaries. Recent work approaches the above issues by learning from a simple form of human feedback: comparisons between pairs of model-generated outputs. However, comparison feedback only conveys limited information about human preferences. In this paper, we introduce Imitation learning from Language Feedback (ILF), a new approach that utilizes more informative language feedback. ILF consists of three steps that are applied iteratively: first, conditioning the language model on the input, an initial LM output, and feedback to generate refinements. Second, selecting the refinement incorporating the most feedback. Third, finetuning the language model to maximize the likelihood of the chosen refinement given the input. We show theoretically that ILF can be viewed as Bayesian Inference, similar to Reinforcement Learning from human feedback. We evaluate
&lt;/p&gt;</description></item><item><title>FedLEO&#26159;&#19968;&#31181;&#22312;&#36229;&#23494;&#38598;LEO&#21355;&#26143;&#26143;&#24231;&#19978;&#36827;&#34892;&#32852;&#21512;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20854;&#22312;LEO&#19978;&#36827;&#34892;&#35270;&#39057;/&#25968;&#25454;&#20256;&#36755;&#21644;&#32858;&#31867;&#65292;&#32780;&#20302;&#26102;&#24310;&#30340;&#22320;&#38754;&#32593;&#20851;&#26381;&#21153;&#22120; (GS) &#20165;&#36127;&#36131;&#21021;&#22987;&#20449;&#21495;&#25511;&#21046;&#12290;&#38543;&#30528;LEO&#26381;&#21153;&#22120;&#30340;&#21464;&#21270;&#21644;&#37325;&#26032;&#32858;&#31867;&#65292;FedLEO&#33021;&#22815;&#36866;&#24212;&#21160;&#24577;LEO&#26143;&#24231;&#24182;&#22312;&#19981;&#21516;&#30340;LEO&#26143;&#24231;&#35774;&#32622;&#19979;&#23454;&#29616;&#39640;&#25928;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.16071</link><description>&lt;p&gt;
&#20809;&#23398;&#26143;&#38388;&#38142;&#36335;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#36793;&#32536;&#36873;&#25321;&#21644;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Edge Selection and Clustering for Federated Learning in Optical Inter-LEO Satellite Constellation. (arXiv:2303.16071v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16071
&lt;/p&gt;
&lt;p&gt;
FedLEO&#26159;&#19968;&#31181;&#22312;&#36229;&#23494;&#38598;LEO&#21355;&#26143;&#26143;&#24231;&#19978;&#36827;&#34892;&#32852;&#21512;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20854;&#22312;LEO&#19978;&#36827;&#34892;&#35270;&#39057;/&#25968;&#25454;&#20256;&#36755;&#21644;&#32858;&#31867;&#65292;&#32780;&#20302;&#26102;&#24310;&#30340;&#22320;&#38754;&#32593;&#20851;&#26381;&#21153;&#22120; (GS) &#20165;&#36127;&#36131;&#21021;&#22987;&#20449;&#21495;&#25511;&#21046;&#12290;&#38543;&#30528;LEO&#26381;&#21153;&#22120;&#30340;&#21464;&#21270;&#21644;&#37325;&#26032;&#32858;&#31867;&#65292;FedLEO&#33021;&#22815;&#36866;&#24212;&#21160;&#24577;LEO&#26143;&#24231;&#24182;&#22312;&#19981;&#21516;&#30340;LEO&#26143;&#24231;&#35774;&#32622;&#19979;&#23454;&#29616;&#39640;&#25928;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20302;&#22320;&#29699;&#36712;&#36947; (LEO) &#21355;&#26143;&#21487;&#20197;&#25910;&#38598;&#22823;&#37327;&#30340;&#24433;&#20687;&#25110;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#22240;&#27492;&#24050;&#32463;&#34987;&#24191;&#27867;&#29992;&#20110;&#21508;&#31181;&#22320;&#29699;&#35266;&#27979;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#19978;&#65292;&#25968;&#25454;&#35757;&#32451;&#36807;&#31243;&#26159;&#22312;&#22320;&#38754;&#20113;&#26381;&#21153;&#22120;&#19978;&#36827;&#34892;&#30340;&#65292;&#36825;&#23548;&#33268;&#20102;&#24456;&#39640;&#30340;&#20256;&#36755;&#24320;&#38144;&#12290;&#26368;&#36817; LEO &#30340;&#21457;&#23637;&#26356;&#36843;&#20999;&#22320;&#38656;&#35201;&#25552;&#20379;&#20855;&#22791;&#22686;&#24378;&#30340;&#26426;&#36733;&#35745;&#31639;&#33021;&#21147;&#30340;&#36229;&#23494;&#38598; LEO &#21355;&#26143;&#26143;&#24231;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312; LEO &#21355;&#26143;&#26143;&#24231;&#19978;&#36827;&#34892;&#21327;&#20316;&#32852;&#21512;&#23398;&#20064;&#30340;&#26041;&#27861; (FedLEO)&#12290;&#25105;&#20204;&#23558;&#25972;&#20010;&#36807;&#31243;&#37117;&#20998;&#37197;&#22312;&#20855;&#22791;&#20302;&#36733;&#33655;&#26143;&#38388;&#20256;&#36755;&#30340; LEO &#19978;&#65292;&#32780;&#20302;&#26102;&#24310;&#30340;&#22320;&#38754;&#32593;&#20851;&#26381;&#21153;&#22120; (GS) &#20165;&#36127;&#36131;&#21021;&#22987;&#20449;&#21495;&#25511;&#21046;&#12290;GS &#39318;&#20808;&#36873;&#25321;&#19968;&#20010; LEO &#26381;&#21153;&#22120;&#65292;&#32780;&#20854; LEO &#23458;&#25143;&#31471;&#37117;&#36890;&#36807;&#32858;&#31867;&#26426;&#21046;&#21644;&#20809;&#23398;&#26143;&#38388;&#38142;&#36335;(ISLs)&#30340;&#36890;&#20449;&#33021;&#21147;&#20915;&#23450;&#12290;&#26356;&#25442; LEO &#26381;&#21153;&#22120;&#26102;&#30340;&#37325;&#26032;&#32858;&#31867;&#23558;&#35302;&#21457;&#26032;&#19968;&#36718;&#30340; LEO &#23458;&#25143;&#31471;&#32858;&#31867;&#65292;&#20174;&#32780;&#20351; FedLEO &#33021;&#22815;&#36866;&#24212;&#21160;&#24577; LEO &#26143;&#24231;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;FedLEO &#21487;&#20197;&#22312;&#19981;&#21516;&#30340; LEO &#26143;&#24231;&#35774;&#32622;&#19979;&#23454;&#29616;&#39640;&#25928;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-Earth orbit (LEO) satellites have been prosperously deployed for various Earth observation missions due to its capability of collecting a large amount of image or sensor data. However, traditionally, the data training process is performed in the terrestrial cloud server, which leads to a high transmission overhead. With the recent development of LEO, it is more imperative to provide ultra-dense LEO constellation with enhanced on-board computation capability. Benefited from it, we have proposed a collaborative federated learning over LEO satellite constellation (FedLEO). We allocate the entire process on LEOs with low payload inter-satellite transmissions, whilst the low-delay terrestrial gateway server (GS) only takes care for initial signal controlling. The GS initially selects an LEO server, whereas its LEO clients are all determined by clustering mechanism and communication capability through the optical inter-satellite links (ISLs). The re-clustering of changing LEO server will
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#39044;&#35757;&#32451;&#30340;Transformer-based&#34920;&#26684;&#27169;&#22411;&#65306;TabRet&#65292;&#33021;&#22815;&#25903;&#25345;&#26410;&#30693;&#21015;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#31168;&#12290;&#37325;&#26032;&#26631;&#35760;&#21270;&#21644;&#38543;&#26426;&#27927;&#29260;&#22686;&#24378;&#23545;&#24615;&#33021;&#25552;&#21319;&#26377;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2303.15747</link><description>&lt;p&gt;
TabRet: &#39044;&#35757;&#32451;Transformer-based&#34920;&#26684;&#27169;&#22411;&#65292;&#25903;&#25345;&#26410;&#30693;&#21015;
&lt;/p&gt;
&lt;p&gt;
TabRet: Pre-training Transformer-based Tabular Models for Unseen Columns. (arXiv:2303.15747v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15747
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#39044;&#35757;&#32451;&#30340;Transformer-based&#34920;&#26684;&#27169;&#22411;&#65306;TabRet&#65292;&#33021;&#22815;&#25903;&#25345;&#26410;&#30693;&#21015;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#31168;&#12290;&#37325;&#26032;&#26631;&#35760;&#21270;&#21644;&#38543;&#26426;&#27927;&#29260;&#22686;&#24378;&#23545;&#24615;&#33021;&#25552;&#21319;&#26377;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TabRet&#30340;&#21487;&#39044;&#35757;&#32451;Transformer-based&#34920;&#26684;&#27169;&#22411;&#12290;TabRet&#26088;&#22312;&#20026;&#21253;&#21547;&#26410;&#22312;&#39044;&#35757;&#32451;&#20013;&#35265;&#36807;&#30340;&#21015;&#30340;&#19979;&#28216;&#20219;&#21153;&#25552;&#20379;&#25903;&#25345;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;TabRet&#22312;&#24494;&#35843;&#20043;&#21069;&#26377;&#19968;&#20010;&#39069;&#22806;&#30340;&#23398;&#20064;&#27493;&#39588;&#65292;&#31216;&#20026;&#37325;&#26032;&#26631;&#35760;&#21270;&#65292;&#23427;&#22522;&#20110;&#36974;&#34109;&#33258;&#21160;&#32534;&#30721;&#25439;&#22833;&#26469;&#26657;&#20934;&#29305;&#24449;&#23884;&#20837;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#37327;&#30340;&#20844;&#20849;&#20581;&#24247;&#35843;&#26597;&#25968;&#25454;&#23545;TabRet&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20339;AUC&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#36827;&#34892;&#37325;&#26032;&#26631;&#35760;&#21270;&#21644;&#38543;&#26426;&#27927;&#29260;&#22686;&#24378;&#23545;&#24615;&#33021;&#25552;&#21319;&#26377;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present \emph{TabRet}, a pre-trainable Transformer-based model for tabular data. TabRet is designed to work on a downstream task that contains columns not seen in pre-training. Unlike other methods, TabRet has an extra learning step before fine-tuning called \emph{retokenizing}, which calibrates feature embeddings based on the masked autoencoding loss. In experiments, we pre-trained TabRet with a large collection of public health surveys and fine-tuned it on classification tasks in healthcare, and TabRet achieved the best AUC performance on four datasets. In addition, an ablation study shows retokenizing and random shuffle augmentation of columns during pre-training contributed to performance gains.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#25968;&#25454;&#28304;&#21644;&#22312;&#35757;&#32451;&#20013;&#26126;&#30830;&#20248;&#21270;&#27867;&#21270;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26032;&#21307;&#38498;&#20013;&#39044;&#27979;ICU&#24739;&#32773;&#19981;&#33391;&#20107;&#20214;&#30340;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.15354</link><description>&lt;p&gt;
&#20174;&#21333;&#20010;&#21307;&#38498;&#21040;&#22810;&#20010;&#20013;&#24515;&#24212;&#29992;&#65306;&#22686;&#24378;ICU&#20013;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
From Single-Hospital to Multi-Centre Applications: Enhancing the Generalisability of Deep Learning Models for Adverse Event Prediction in the ICU. (arXiv:2303.15354v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#25968;&#25454;&#28304;&#21644;&#22312;&#35757;&#32451;&#20013;&#26126;&#30830;&#20248;&#21270;&#27867;&#21270;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26032;&#21307;&#38498;&#20013;&#39044;&#27979;ICU&#24739;&#32773;&#19981;&#33391;&#20107;&#20214;&#30340;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#24110;&#21161;&#21307;&#29983;&#21450;&#26089;&#26816;&#27979;&#24739;&#32773;&#24694;&#21270;&#29366;&#24577;&#65292;&#20026;&#20182;&#20204;&#25552;&#20379;&#21453;&#24212;&#26102;&#38388;&#24182;&#38450;&#27490;&#19981;&#33391;&#32467;&#26524;&#12290;&#34429;&#28982;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26089;&#26399;&#39044;&#35686;&#27169;&#22411;&#36890;&#24120;&#22312;&#23427;&#20204;&#21463;&#36807;&#35757;&#32451;&#30340;&#21307;&#38498;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#26032;&#21307;&#38498;&#24212;&#29992;&#26102;&#23427;&#20204;&#24448;&#24448;&#19981;&#22826;&#21487;&#38752;&#12290;&#36825;&#20351;&#24471;&#22312;&#35268;&#27169;&#19978;&#37096;&#32626;&#23427;&#20204;&#21464;&#24471;&#22256;&#38590;&#12290;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#27431;&#27954;&#21644;&#32654;&#22269;&#22235;&#20010;&#25968;&#25454;&#28304;&#30340;&#31934;&#24515;&#21327;&#35843;&#30340;&#37325;&#30151;&#30417;&#25252;&#25968;&#25454;&#65288;&#24635;&#35745;334,812&#20010;&#20572;&#30041;&#26102;&#38388;&#65289;&#65292;&#31995;&#32479;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#19977;&#31181;&#24120;&#35265;&#19981;&#33391;&#20107;&#20214;&#30340;&#21487;&#38752;&#24615;&#65306;&#27515;&#20129;&#12289;&#24613;&#24615;&#32958;&#25439;&#20260;&#65288;AKI&#65289;&#21644;&#33043;&#27602;&#30151;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#20351;&#29992;&#22810;&#20010;&#25968;&#25454;&#28304;&#21644;/&#25110;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26126;&#30830;&#20248;&#21270;&#27867;&#21270;&#33021;&#21147;&#26159;&#21542;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#26032;&#21307;&#38498;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27169;&#22411;&#22312;&#35757;&#32451;&#21307;&#38498;&#23545;&#20110;&#27515;&#20129;&#29575;&#65288;0.838-0.869&#65289;&#12289;AKI&#65288;0.823-0.866&#65289;&#21644;&#33043;&#27602;&#30151;&#65288;0.749-0.824&#65289;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;AUROC&#12290;&#39044;&#26399;&#22320;&#65292;&#24615;&#33021;&#22312;&#26032;&#21307;&#38498;&#19979;&#38477;&#65292;&#26377;&#26102;&#19979;&#38477;&#20102;-0.200&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#36229;&#36807;&#19968;&#20010;&#25968;&#25454;&#28304;&#24182;&#26126;&#30830;&#20248;&#21270;&#27867;&#21270;&#24615;&#33021;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#26032;&#21307;&#38498;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#37096;&#32626;&#20581;&#22766;&#19988;&#26222;&#36866;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;ICU&#20013;&#30340;&#19981;&#33391;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) can aid doctors in detecting worsening patient states early, affording them time to react and prevent bad outcomes. While DL-based early warning models usually work well in the hospitals they were trained for, they tend to be less reliable when applied at new hospitals. This makes it difficult to deploy them at scale. Using carefully harmonised intensive care data from four data sources across Europe and the US (totalling 334,812 stays), we systematically assessed the reliability of DL models for three common adverse events: death, acute kidney injury (AKI), and sepsis. We tested whether using more than one data source and/or explicitly optimising for generalisability during training improves model performance at new hospitals. We found that models achieved high AUROC for mortality (0.838-0.869), AKI (0.823-0.866), and sepsis (0.749-0.824) at the training hospital. As expected, performance dropped at new hospitals, sometimes by as much as -0.200. Using more than one 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32972;&#38376;&#25968;&#23383;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#20844;&#20849;&#25968;&#25454;&#30340;&#23433;&#20840;&#12290;&#36890;&#36807;&#22312;&#25968;&#25454;&#38598;&#20013;&#25554;&#20837;&#26497;&#23569;&#37327;&#30340;&#25968;&#23383;&#27700;&#21360;&#26679;&#26412;&#65292;&#38544;&#24335;&#23398;&#20064;&#19968;&#20010;&#38544;&#34255;&#30340;&#20989;&#25968;&#20316;&#20026;&#25968;&#23383;&#27700;&#21360;&#65292;&#20197;&#36319;&#36394;&#38750;&#27861;&#20351;&#29992;&#27492;&#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#12290;&#20351;&#29992;&#8220;&#28165;&#27905;&#26631;&#31614;&#32972;&#38376;&#8221;&#26041;&#27861;&#23454;&#29616;&#20102;&#25968;&#23383;&#27700;&#21360;&#65292;&#19981;&#20250;&#30772;&#22351;&#21407;&#22987;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#26816;&#27979;&#21040;&#38750;&#27861;&#21033;&#29992;&#25968;&#25454;&#38598;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2303.11470</link><description>&lt;p&gt;
&#20320;&#26377;&#22312;&#20351;&#29992;&#25105;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#21527;&#65311;&#20351;&#29992;&#28165;&#27905;&#26631;&#31614;&#32972;&#38376;&#25968;&#23383;&#27700;&#21360;&#23454;&#29616;&#20844;&#20849;&#25968;&#25454;&#38598;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Did You Train on My Dataset? Towards Public Dataset Protection with Clean-Label Backdoor Watermarking. (arXiv:2303.11470v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11470
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32972;&#38376;&#25968;&#23383;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#20844;&#20849;&#25968;&#25454;&#30340;&#23433;&#20840;&#12290;&#36890;&#36807;&#22312;&#25968;&#25454;&#38598;&#20013;&#25554;&#20837;&#26497;&#23569;&#37327;&#30340;&#25968;&#23383;&#27700;&#21360;&#26679;&#26412;&#65292;&#38544;&#24335;&#23398;&#20064;&#19968;&#20010;&#38544;&#34255;&#30340;&#20989;&#25968;&#20316;&#20026;&#25968;&#23383;&#27700;&#21360;&#65292;&#20197;&#36319;&#36394;&#38750;&#27861;&#20351;&#29992;&#27492;&#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#12290;&#20351;&#29992;&#8220;&#28165;&#27905;&#26631;&#31614;&#32972;&#38376;&#8221;&#26041;&#27861;&#23454;&#29616;&#20102;&#25968;&#23383;&#27700;&#21360;&#65292;&#19981;&#20250;&#30772;&#22351;&#21407;&#22987;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#26816;&#27979;&#21040;&#38750;&#27861;&#21033;&#29992;&#25968;&#25454;&#38598;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#19978;&#28304;&#28304;&#19981;&#26029;&#30340;&#25903;&#25345;&#35757;&#32451;&#25968;&#25454;&#26159;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22823;&#37327;&#30340;&#20844;&#20849;&#25968;&#25454;&#20063;&#24341;&#36215;&#20102;&#23545;&#25968;&#25454;&#38598;&#34987;&#26410;&#32463;&#25480;&#26435;&#30340;&#29992;&#20110;&#21830;&#19994;&#30446;&#30340;&#30340;&#25285;&#24551;&#65292;&#36825;&#26159;&#25968;&#25454;&#38598;&#35768;&#21487;&#35777;&#25152;&#31105;&#27490;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32972;&#38376;&#25968;&#23383;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#20316;&#20026;&#20445;&#25252;&#20844;&#20849;&#25968;&#25454;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#36890;&#36807;&#21521;&#25968;&#25454;&#38598;&#20013;&#25554;&#20837;&#23569;&#37327;&#30340;&#25968;&#23383;&#27700;&#21360;&#26679;&#26412;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#38544;&#24335;&#23398;&#20064;&#30001;&#38450;&#24481;&#32773;&#35774;&#32622;&#30340;&#31192;&#23494;&#20989;&#25968;&#12290;&#36825;&#20010;&#38544;&#34255;&#30340;&#20989;&#25968;&#21487;&#20197;&#20316;&#20026;&#25968;&#23383;&#27700;&#21360;&#65292;&#29992;&#20110;&#36319;&#36394;&#38750;&#27861;&#20351;&#29992;&#25968;&#25454;&#38598;&#30340;&#31532;&#19977;&#26041;&#27169;&#22411;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#32972;&#38376;&#25554;&#20837;&#26041;&#27861;&#24448;&#24448;&#28041;&#21450;&#21521;&#35757;&#32451;&#38598;&#20013;&#28155;&#21152;&#20219;&#24847;&#30340;&#12289;&#38169;&#35823;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#23548;&#33268;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#24182;&#23481;&#26131;&#34987;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#26816;&#27979;&#21040;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#28165;&#27905;&#26631;&#35760;&#32972;&#38376;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#25968;&#23383;&#27700;&#21360;&#32780;&#19981;&#30772;&#22351;&#21407;&#22987;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#26816;&#27979;&#38750;&#27861;&#25968;&#25454;&#38598;&#20351;&#29992;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The huge supporting training data on the Internet has been a key factor in the success of deep learning models. However, this abundance of public-available data also raises concerns about the unauthorized exploitation of datasets for commercial purposes, which is forbidden by dataset licenses. In this paper, we propose a backdoor-based watermarking approach that serves as a general framework for safeguarding public-available data. By inserting a small number of watermarking samples into the dataset, our approach enables the learning model to implicitly learn a secret function set by defenders. This hidden function can then be used as a watermark to track down third-party models that use the dataset illegally. Unfortunately, existing backdoor insertion methods often entail adding arbitrary and mislabeled data to the training set, leading to a significant drop in performance and easy detection by anomaly detection algorithms. To overcome this challenge, we introduce a clean-label backdoo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20809;&#21051;&#27169;&#22411;&#33539;&#24335;&#65292;&#36890;&#36807;&#20248;&#21270;&#22797;&#20540;&#31070;&#32463;&#22330;&#25191;&#34892;&#20809;&#23398;&#26680;&#22238;&#24402;&#24182;&#23558;&#20809;&#21051;&#31995;&#32479;&#25286;&#35299;&#20026;&#38750;&#21442;&#25968;&#25513;&#27169;&#25805;&#20316;&#21644;&#21253;&#21547;&#34892;&#21015;&#24335;&#28304;&#12289;&#30643;&#23380;&#21644;&#20809;&#21051;&#20449;&#24687;&#30340;&#23398;&#20064;&#20809;&#23398;&#26680;&#65292;&#20351;&#29992;&#23567;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.08435</link><description>&lt;p&gt;
&#20351;&#29992;&#22797;&#20540;&#31070;&#32463;&#22330;&#30340;&#29289;&#29702;&#20449;&#24687;&#20809;&#23398;&#26680;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Optical Kernel Regression Using Complex-valued Neural Fields. (arXiv:2303.08435v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20809;&#21051;&#27169;&#22411;&#33539;&#24335;&#65292;&#36890;&#36807;&#20248;&#21270;&#22797;&#20540;&#31070;&#32463;&#22330;&#25191;&#34892;&#20809;&#23398;&#26680;&#22238;&#24402;&#24182;&#23558;&#20809;&#21051;&#31995;&#32479;&#25286;&#35299;&#20026;&#38750;&#21442;&#25968;&#25513;&#27169;&#25805;&#20316;&#21644;&#21253;&#21547;&#34892;&#21015;&#24335;&#28304;&#12289;&#30643;&#23380;&#21644;&#20809;&#21051;&#20449;&#24687;&#30340;&#23398;&#20064;&#20809;&#23398;&#26680;&#65292;&#20351;&#29992;&#23567;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#21051;&#26159;&#38598;&#25104;&#30005;&#36335;&#21046;&#36896;&#30340;&#22522;&#30784;&#65292;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#12290;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20809;&#21051;&#27169;&#22411;&#30340;&#21457;&#23637;&#32531;&#35299;&#20102;&#21046;&#36896;&#36807;&#31243;&#24320;&#38144;&#21644;&#33021;&#21147;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#20197;&#21069;&#30340;&#26041;&#27861;&#37117;&#23558;&#20809;&#21051;&#31995;&#32479;&#35270;&#20026;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#40657;&#30418;&#26144;&#23556;&#65292;&#21033;&#29992;&#32593;&#32476;&#21442;&#25968;&#36890;&#36807;&#27515;&#35760;&#30828;&#32972;&#26144;&#23556;&#22823;&#37327;&#30340;&#25513;&#27169;&#21040;&#31354;&#20013;&#25110;&#25513;&#27169;&#21040;&#30005;&#38459;&#22270;&#20687;&#23545;&#65292;&#23548;&#33268;&#25512;&#24191;&#33021;&#21147;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33539;&#24335;&#65292;&#23558;&#20005;&#26684;&#30340;&#20809;&#21051;&#27169;&#22411;&#25286;&#35299;&#20026;&#38750;&#21442;&#25968;&#25513;&#27169;&#25805;&#20316;&#21644;&#21253;&#21547;&#34892;&#21015;&#24335;&#28304;&#12289;&#30643;&#23380;&#21644;&#20809;&#21051;&#20449;&#24687;&#30340;&#23398;&#20064;&#20809;&#23398;&#26680;&#12290;&#36890;&#36807;&#20248;&#21270;&#22797;&#20540;&#31070;&#32463;&#22330;&#20197;&#25191;&#34892;&#20809;&#23398;&#26680;&#22238;&#24402;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#22320;&#24674;&#22797;&#20809;&#21051;&#31995;&#32479;&#65292;&#21516;&#26102;&#20351;&#29992;&#36739;&#23569;&#30340;&#21442;&#25968;&#36827;&#34892;&#23567;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lithography is fundamental to integrated circuit fabrication, necessitating large computation overhead. The advancement of machine learning (ML)-based lithography models alleviates the trade-offs between manufacturing process expense and capability. However, all previous methods regard the lithography system as an image-to-image black box mapping, utilizing network parameters to learn by rote mappings from massive mask-to-aerial or mask-to-resist image pairs, resulting in poor generalization capability. In this paper, we propose a new ML-based paradigm disassembling the rigorous lithographic model into non-parametric mask operations and learned optical kernels containing determinant source, pupil, and lithography information. By optimizing complex-valued neural fields to perform optical kernel regression from coordinates, our method can accurately restore lithography system using a small-scale training dataset with fewer parameters, demonstrating superior generalization capability as w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#22810;&#27169;&#24577;Transformer&#65292;&#20174;&#26230;&#20307;&#32467;&#26500;&#21644;&#33021;&#37327;&#20837;&#25163;&#39044;&#27979;DOS&#65292;&#21487;&#24212;&#29992;&#20110;&#22768;&#23376;&#21644;&#30005;&#23376;DOS&#12290;</title><link>http://arxiv.org/abs/2303.07000</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;Transformer&#39044;&#27979;&#24577;&#23494;&#24230;
&lt;/p&gt;
&lt;p&gt;
Predicting Density of States via Multi-modal Transformer. (arXiv:2303.07000v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07000
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#22810;&#27169;&#24577;Transformer&#65292;&#20174;&#26230;&#20307;&#32467;&#26500;&#21644;&#33021;&#37327;&#20837;&#25163;&#39044;&#27979;DOS&#65292;&#21487;&#24212;&#29992;&#20110;&#22768;&#23376;&#21644;&#30005;&#23376;DOS&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29366;&#24577;&#23494;&#24230;&#65288;DOS&#65289;&#26159;&#26448;&#26009;&#30340;&#35889;&#23398;&#29305;&#24615;&#65292;&#23427;&#25552;&#20379;&#20102;&#20851;&#20110;&#26448;&#26009;&#21508;&#31181;&#29305;&#24615;&#30340;&#22522;&#26412;&#35265;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#26469;&#39044;&#27979;DOS&#65292;&#36890;&#36807;&#21453;&#26144;DOS&#30340;&#24615;&#36136;&#65306;DOS&#30830;&#23450;&#20316;&#20026;&#33021;&#37327;&#20989;&#25968;&#30340;&#29366;&#24577;&#30340;&#19968;&#33324;&#20998;&#24067;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#22810;&#27169;&#24577;Transformer&#38598;&#25104;&#26230;&#20307;&#32467;&#26500;&#21644;&#33021;&#37327;&#33719;&#24471;&#30340;&#24322;&#26500;&#20449;&#24687;&#65292;&#20174;&#32780;&#27169;&#25311;&#26230;&#20307;&#32467;&#26500;&#20013;&#21407;&#23376;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#21644;&#21508;&#31181;&#33021;&#32423;&#12290;&#36890;&#36807;&#21508;&#31181;&#30495;&#23454;&#24773;&#20917;&#19979;&#30340;&#20004;&#31181;DOS&#65288;&#21363;&#22768;&#23376;DOS&#21644;&#30005;&#23376;DOS&#65289;&#30340;&#24191;&#27867;&#23454;&#39564;&#26174;&#31034;&#20102;DOSTransformer&#30340;&#20248;&#36234;&#24615;&#12290;DOSTransformer&#30340;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/HeewoongNoh/DOSTransformer&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
The density of states (DOS) is a spectral property of materials, which provides fundamental insights on various characteristics of materials. In this paper, we propose a model to predict the DOS by reflecting the nature of DOS: DOS determines the general distribution of states as a function of energy. Specifically, we integrate the heterogeneous information obtained from the crystal structure and the energies via multi-modal transformer, thereby modeling the complex relationships between the atoms in the crystal structure, and various energy levels. Extensive experiments on two types of DOS, i.e., Phonon DOS and Electron DOS, with various real-world scenarios demonstrate the superiority of DOSTransformer. The source code for DOSTransformer is available at https://github.com/HeewoongNoh/DOSTransformer.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#20957;&#32858;&#30340;&#30456;&#22270;&#65292;&#26088;&#22312;&#25552;&#20379;&#23545;&#31070;&#32463;&#32593;&#32476;&#21160;&#21147;&#23398;&#21306;&#22495;&#21450;&#20854;&#19982;&#21021;&#22987;&#21270;&#30456;&#20851;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#32508;&#21512;&#29702;&#35299;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35814;&#32454;&#35299;&#37322;&#20102;&#23567;&#21021;&#22987;&#21270;&#23548;&#33268;&#22312;&#21021;&#22987;&#35757;&#32451;&#38454;&#27573;&#20986;&#29616;&#20957;&#32858;&#30340;&#22522;&#26412;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.06561</link><description>&lt;p&gt;
&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#20957;&#32858;&#30340;&#30456;&#22270;
&lt;/p&gt;
&lt;p&gt;
Phase Diagram of Initial Condensation for Two-layer Neural Networks. (arXiv:2303.06561v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#20957;&#32858;&#30340;&#30456;&#22270;&#65292;&#26088;&#22312;&#25552;&#20379;&#23545;&#31070;&#32463;&#32593;&#32476;&#21160;&#21147;&#23398;&#21306;&#22495;&#21450;&#20854;&#19982;&#21021;&#22987;&#21270;&#30456;&#20851;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#32508;&#21512;&#29702;&#35299;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35814;&#32454;&#35299;&#37322;&#20102;&#23567;&#21021;&#22987;&#21270;&#23548;&#33268;&#22312;&#21021;&#22987;&#35757;&#32451;&#38454;&#27573;&#20986;&#29616;&#20957;&#32858;&#30340;&#22522;&#26412;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#21516;&#21021;&#22987;&#21270;&#27604;&#20363;&#19979;&#21576;&#29616;&#20986;&#19981;&#21516;&#34892;&#20026;&#30340;&#29616;&#35937;&#19968;&#30452;&#26159;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#38590;&#39064;&#12290;&#26412;&#25991;&#22522;&#20110;Luo&#31561;&#20154;&#26089;&#26399;&#30340;&#24037;&#20316;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#20957;&#32858;&#30340;&#30456;&#22270;&#12290;&#20957;&#32858;&#26159;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26435;&#37325;&#21521;&#37327;&#38598;&#20013;&#20110;&#29420;&#31435;&#26041;&#21521;&#30340;&#29616;&#35937;&#65292;&#22312;&#38750;&#32447;&#24615;&#23398;&#20064;&#36807;&#31243;&#20013;&#26159;&#19968;&#31181;&#29305;&#24615;&#65292;&#20351;&#31070;&#32463;&#32593;&#32476;&#25317;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30456;&#22270;&#26088;&#22312;&#25552;&#20379;&#23545;&#31070;&#32463;&#32593;&#32476;&#21160;&#21147;&#23398;&#21306;&#22495;&#21450;&#20854;&#19982;&#21021;&#22987;&#21270;&#30456;&#20851;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#32508;&#21512;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35814;&#32454;&#23637;&#31034;&#20102;&#23567;&#21021;&#22987;&#21270;&#23548;&#33268;&#22312;&#21021;&#22987;&#35757;&#32451;&#38454;&#27573;&#20986;&#29616;&#20957;&#32858;&#30340;&#22522;&#26412;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The phenomenon of distinct behaviors exhibited by neural networks under varying scales of initialization remains an enigma in deep learning research. In this paper, based on the earlier work by Luo et al.~\cite{luo2021phase}, we present a phase diagram of initial condensation for two-layer neural networks. Condensation is a phenomenon wherein the weight vectors of neural networks concentrate on isolated orientations during the training process, and it is a feature in non-linear learning process that enables neural networks to possess better generalization abilities. Our phase diagram serves to provide a comprehensive understanding of the dynamical regimes of neural networks and their dependence on the choice of hyperparameters related to initialization. Furthermore, we demonstrate in detail the underlying mechanisms by which small initialization leads to condensation at the initial training stage.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#22270;&#24418;&#26159;&#21542;&#38656;&#35201;&#30828;&#20214;&#25903;&#25345;&#65292;&#21457;&#29616;&#24403;&#21069;GPU&#24615;&#33021;&#26080;&#27861;&#28385;&#36275;&#23545;4K&#20998;&#36776;&#29575;60FPS&#28210;&#26579;&#30340;&#38656;&#27714;&#65292;&#19988;&#22312;&#22686;&#24378;&#29616;&#23454;/&#34394;&#25311;&#29616;&#23454;&#24212;&#29992;&#20013;&#24615;&#33021;&#32570;&#21475;&#26356;&#22823;&#12290;&#20316;&#32773;&#30830;&#23450;&#36755;&#20837;&#32534;&#30721;&#21644;MLP&#20869;&#26680;&#26159;&#24615;&#33021;&#29942;&#39048;&#12290;</title><link>http://arxiv.org/abs/2303.05735</link><description>&lt;p&gt;
&#31070;&#32463;&#22270;&#24418;&#30340;&#30828;&#20214;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
Hardware Acceleration of Neural Graphics. (arXiv:2303.05735v2 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#22270;&#24418;&#26159;&#21542;&#38656;&#35201;&#30828;&#20214;&#25903;&#25345;&#65292;&#21457;&#29616;&#24403;&#21069;GPU&#24615;&#33021;&#26080;&#27861;&#28385;&#36275;&#23545;4K&#20998;&#36776;&#29575;60FPS&#28210;&#26579;&#30340;&#38656;&#27714;&#65292;&#19988;&#22312;&#22686;&#24378;&#29616;&#23454;/&#34394;&#25311;&#29616;&#23454;&#24212;&#29992;&#20013;&#24615;&#33021;&#32570;&#21475;&#26356;&#22823;&#12290;&#20316;&#32773;&#30830;&#23450;&#36755;&#20837;&#32534;&#30721;&#21644;MLP&#20869;&#26680;&#26159;&#24615;&#33021;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#28210;&#26579;&#21644;&#21453;&#28210;&#26579;&#31639;&#27861;&#24050;&#34987;&#31070;&#32463;&#34920;&#31034;&#65288;NR&#65289;&#25152;&#21462;&#20195;&#12290;NR&#26368;&#36817;&#34987;&#29992;&#20110;&#23398;&#20064;&#22330;&#26223;&#30340;&#20960;&#20309;&#21644;&#26448;&#36136;&#23646;&#24615;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#20449;&#24687;&#21512;&#25104;&#30495;&#23454;&#30340;&#22270;&#20687;&#65292;&#22240;&#27492;&#25215;&#35834;&#29992;&#21487;&#20280;&#32553;&#30340;&#36136;&#37327;&#21644;&#21487;&#39044;&#27979;&#30340;&#24615;&#33021;&#26367;&#25442;&#20256;&#32479;&#30340;&#28210;&#26579;&#31639;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#38382;&#39064;&#65306;&#31070;&#32463;&#22270;&#24418;&#65288;NG&#65289;&#26159;&#21542;&#38656;&#35201;&#30828;&#20214;&#25903;&#25345;&#65311;&#25105;&#20204;&#30740;&#31350;&#20102;&#20195;&#34920;&#24615;&#30340;NG&#24212;&#29992;&#31243;&#24207;&#65292;&#21457;&#29616;&#22914;&#26524;&#25105;&#20204;&#35201;&#22312;&#24403;&#21069;&#30340;GPU&#19978;&#20197;60FPS&#28210;&#26579;4K&#20998;&#36776;&#29575;&#65292;&#21017;&#25152;&#38656;&#24615;&#33021;&#19982;&#24403;&#21069;GPU&#30340;&#23454;&#38469;&#24615;&#33021;&#23384;&#22312;1.5&#20493;&#33267;55&#20493;&#30340;&#24046;&#36317;&#12290;&#23545;&#20110;&#22686;&#24378;&#29616;&#23454;/&#34394;&#25311;&#29616;&#23454;&#24212;&#29992;&#31243;&#24207;&#65292;&#25152;&#38656;&#24615;&#33021;&#19982;&#25152;&#38656;&#31995;&#32479;&#21151;&#29575;&#20043;&#38388;&#23384;&#22312;&#26356;&#22823;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#30830;&#23450;&#36755;&#20837;&#32534;&#30721;&#21644;MLP&#20869;&#26680;&#26159;&#24615;&#33021;&#29942;&#39048;&#65292;&#23545;&#20110;&#22810;&#20998;&#36776;&#29575;&#21704;&#24076;&#32593;&#26684;&#12289;&#22810;&#20998;&#36776;&#29575;&#23494;&#38598;&#32593;&#26684;&#21644;&#20302;&#20998;&#36776;&#29575;&#23494;&#38598;&#32593;&#26684;&#65292;&#23427;&#20204;&#21344;&#24212;&#29992;&#31243;&#24207;&#26102;&#38388;&#30340;72&#65285;&#12289;60&#65285;&#21644;59&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rendering and inverse-rendering algorithms that drive conventional computer graphics have recently been superseded by neural representations (NR). NRs have recently been used to learn the geometric and the material properties of the scenes and use the information to synthesize photorealistic imagery, thereby promising a replacement for traditional rendering algorithms with scalable quality and predictable performance. In this work we ask the question: Does neural graphics (NG) need hardware support? We studied representative NG applications showing that, if we want to render 4k res. at 60FPS there is a gap of 1.5X-55X in the desired performance on current GPUs. For AR/VR applications, there is an even larger gap of 2-4 OOM between the desired performance and the required system power. We identify that the input encoding and the MLP kernels are the performance bottlenecks, consuming 72%,60% and 59% of application time for multi res. hashgrid, multi res. densegrid and low res. densegrid 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;softmax&#36924;&#36817;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#20272;&#35745;&#26368;&#20248;&#30340;&#21160;&#24577;&#27835;&#30103;&#26041;&#26696;&#24182;&#23545;&#20854;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2303.04416</link><description>&lt;p&gt;
&#36890;&#36807;Softmax&#36924;&#36817;&#23454;&#29616;&#21160;&#24577;&#26368;&#20248;&#31574;&#30053;&#30340;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Inference on Optimal Dynamic Policies via Softmax Approximation. (arXiv:2303.04416v2 [econ.EM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;softmax&#36924;&#36817;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#20272;&#35745;&#26368;&#20248;&#30340;&#21160;&#24577;&#27835;&#30103;&#26041;&#26696;&#24182;&#23545;&#20854;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#20272;&#35745;&#26368;&#20248;&#30340;&#21160;&#24577;&#31574;&#30053;&#26159;&#21160;&#24577;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#22312;&#22240;&#26524;&#25512;&#26029;&#30340;&#32972;&#26223;&#19979;&#65292;&#35813;&#38382;&#39064;&#34987;&#31216;&#20026;&#20272;&#35745;&#26368;&#20248;&#30340;&#21160;&#24577;&#27835;&#30103;&#26041;&#26696;&#12290;&#21363;&#20351;&#23384;&#22312;&#22823;&#37327;&#30340;&#20272;&#35745;&#26041;&#27861;&#65292;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#26469;&#20272;&#35745;&#26368;&#20248;&#31574;&#30053;&#30340;&#20215;&#20540;&#21450;&#20854;&#30456;&#20851;&#30340;&#32467;&#26500;&#21442;&#25968;&#26412;&#36136;&#19978;&#26356;&#21152;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#21040;&#26410;&#30693;&#37327;&#30340;&#38750;&#32447;&#24615;&#21644;&#38750;&#21487;&#24494;&#20989;&#25968;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#37319;&#29992;&#20102;&#20122;&#26679;&#26412;&#26041;&#27861;&#65292;&#20294;&#21487;&#33021;&#20250;&#38477;&#20302;&#20272;&#35745;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#23545;&#20110;&#19968;&#20010;&#36866;&#24403;&#22686;&#38271;&#30340;&#28201;&#24230;&#21442;&#25968;&#65292;&#26368;&#20248;&#27835;&#30103;&#26041;&#26696;&#30340;&#31616;&#21333;softmax&#36924;&#36817;&#21487;&#20197;&#23454;&#29616;&#23545;&#30495;&#27491;&#26368;&#20248;&#26041;&#26696;&#30340;&#26377;&#25928;&#25512;&#26029;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#29992;&#20110;&#20004;&#26399;&#30340;&#26368;&#20248;&#21160;&#24577;&#26041;&#26696;&#65292;&#24182;&#23558;&#35813;&#26041;&#27861;&#30452;&#25509;&#25512;&#24191;&#21040;&#26377;&#38480;&#30340;&#26102;&#38388;&#27573;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#32467;&#21512;&#20102;&#21322;&#21442;&#25968;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating optimal dynamic policies from offline data is a fundamental problem in dynamic decision making. In the context of causal inference, the problem is known as estimating the optimal dynamic treatment regime. Even though there exists a plethora of methods for estimation, constructing confidence intervals for the value of the optimal regime and structural parameters associated with it is inherently harder, as it involves non-linear and non-differentiable functionals of un-known quantities that need to be estimated. Prior work resorted to sub-sample approaches that can deteriorate the quality of the estimate. We show that a simple soft-max approximation to the optimal treatment regime, for an appropriately fast growing temperature parameter, can achieve valid inference on the truly optimal regime. We illustrate our result for a two-period optimal dynamic regime, though our approach should directly extend to the finite horizon case. Our work combines techniques from semi-parametric
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MMKGL&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#22810;&#27169;&#24577;&#38598;&#25104;&#20013;&#21508;&#27169;&#24577;&#20043;&#38388;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#20174;&#22810;&#20010;&#22270;&#20013;&#25552;&#21462;&#24322;&#36136;&#20449;&#24687;&#65292;&#20197;&#36827;&#34892;&#33258;&#38381;&#30151;&#30340;&#39044;&#27979;&#21644;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.03388</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#24577;&#22810;&#26680;&#22270;&#23398;&#20064;&#30340;&#33258;&#38381;&#30151;&#39044;&#27979;&#19982;&#29983;&#29289;&#26631;&#24535;&#29289;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Multi-kernel Graph Learning for Autism Prediction and Biomarker Discovery. (arXiv:2303.03388v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MMKGL&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#22810;&#27169;&#24577;&#38598;&#25104;&#20013;&#21508;&#27169;&#24577;&#20043;&#38388;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#20174;&#22810;&#20010;&#22270;&#20013;&#25552;&#21462;&#24322;&#36136;&#20449;&#24687;&#65292;&#20197;&#36827;&#34892;&#33258;&#38381;&#30151;&#30340;&#39044;&#27979;&#21644;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#38598;&#25104;&#21644;&#20998;&#31867;&#26159;&#30142;&#30149;&#39044;&#27979;&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#38556;&#30861;&#20043;&#19968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MMKGL&#30340;&#26032;&#26041;&#27861;&#26469;&#26377;&#25928;&#25269;&#28040;&#22810;&#27169;&#24577;&#38598;&#25104;&#36807;&#31243;&#20013;&#21508;&#27169;&#24577;&#20043;&#38388;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#20174;&#22270;&#20013;&#25552;&#21462;&#24322;&#36136;&#20449;&#24687;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#22270;&#23884;&#20837;&#27169;&#22359;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#29983;&#25104;&#22810;&#20010;&#22270;&#65292;&#28982;&#21518;&#25552;&#20986;&#22810;&#26680;&#22270;&#23398;&#20064;&#27169;&#22359;&#65292;&#20174;&#22810;&#27169;&#24577;&#22270;&#20013;&#25552;&#21462;&#24322;&#36136;&#20449;&#24687;&#12290;&#22312;&#19981;&#21516;&#23618;&#27425;&#19978;&#32858;&#21512;&#22810;&#27169;&#24577;&#22270;&#20013;&#30340;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#23545;&#33258;&#38381;&#30151;&#30340;&#39044;&#27979;&#21644;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to its complexity, graph learning-based multi-modal integration and classification is one of the most challenging obstacles for disease prediction. To effectively offset the negative impact between modalities in the process of multi-modal integration and extract heterogeneous information from graphs, we propose a novel method called MMKGL (Multi-modal Multi-Kernel Graph Learning). For the problem of negative impact between modalities, we propose a multi-modal graph embedding module to construct a multi-modal graph. Different from conventional methods that manually construct static graphs for all modalities, each modality generates a separate graph by adaptive learning, where a function graph and a supervision graph are introduced for optimization during the multi-graph fusion embedding process. We then propose a multi-kernel graph learning module to extract heterogeneous information from the multi-modal graph. The information in the multi-modal graph at different levels is aggregat
&lt;/p&gt;</description></item><item><title>&#36866;&#29992;&#20110;&#24314;&#31435;&#39044;&#27979;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#21307;&#30103;&#39046;&#22495;&#21644;&#20854;&#20182;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#27169;&#22411;&#30340;&#32500;&#25252;&#21644;&#30417;&#25511;&#24456;&#20851;&#38190;&#65292;&#22240;&#20026;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#25968;&#25454;&#30340;&#21464;&#21270;&#21644;&#20256;&#36755;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2303.01513</link><description>&lt;p&gt;
&#23398;&#20064;&#26426;&#22120;&#22312;&#21307;&#30103;&#21450;&#20854;&#20182;&#26041;&#38754;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning machines for health and beyond. (arXiv:2303.01513v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01513
&lt;/p&gt;
&lt;p&gt;
&#36866;&#29992;&#20110;&#24314;&#31435;&#39044;&#27979;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#21307;&#30103;&#39046;&#22495;&#21644;&#20854;&#20182;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#27169;&#22411;&#30340;&#32500;&#25252;&#21644;&#30417;&#25511;&#24456;&#20851;&#38190;&#65292;&#22240;&#20026;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#25968;&#25454;&#30340;&#21464;&#21270;&#21644;&#20256;&#36755;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#26500;&#24314;&#39044;&#27979;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#25928;&#26524;&#65292;&#22240;&#20026;&#23427;&#20204;&#25797;&#38271;&#35782;&#21035;&#22823;&#22411;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22797;&#26434;&#30340;&#29616;&#23454;&#38382;&#39064;&#65292;&#27169;&#22411;&#30340;&#24320;&#21457;&#24448;&#24448;&#20572;&#30041;&#22312;&#21457;&#34920;&#35770;&#25991;&#12289;&#27010;&#24565;&#39564;&#35777;&#25110;&#36890;&#36807;&#26576;&#31181;&#37096;&#32626;&#27169;&#24335;&#30340;&#21487;&#35775;&#38382;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#30103;&#39046;&#22495;&#37324;&#65292;&#27169;&#22411;&#30340;&#24739;&#32773;&#20154;&#21475;&#20250;&#21457;&#29983;&#21464;&#21270;&#65292;&#22240;&#27492;&#27169;&#22411;&#30340;&#32500;&#25252;&#21644;&#30417;&#25511;&#26159;&#30830;&#20445;&#20854;&#38271;&#26399;&#23433;&#20840;&#26377;&#25928;&#20351;&#29992;&#30340;&#20851;&#38190;&#12290;&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26159;&#26377;&#25928;&#22320;&#35757;&#32451;&#20197;&#22312;&#21487;&#29992;&#25968;&#25454;&#38598;&#20013;&#23547;&#25214;&#27169;&#24335;&#30340;&#65292;&#22240;&#27492;&#65292;&#23545;&#20110;&#22797;&#26434;&#30340;&#29616;&#23454;&#38382;&#39064;&#65292;&#27169;&#22411;&#30340;&#24615;&#33021;&#19981;&#20250;&#22312;&#21457;&#34920;&#25110;&#37096;&#32626;&#26102;&#36798;&#21040;&#23792;&#20540;&#21518;&#22266;&#23450;&#19981;&#21464;&#12290;&#30456;&#21453;&#65292;&#25968;&#25454;&#20250;&#38543;&#30528;&#26102;&#38388;&#30340;&#21464;&#21270;&#32780;&#20135;&#29983;&#21464;&#21270;&#65292;&#32780;&#24403;&#27169;&#22411;&#34987;&#36816;&#24448;&#26032;&#30340;&#22320;&#26041;&#20379;&#26032;&#30340;&#20154;&#32676;&#20351;&#29992;&#26102;&#65292;&#23427;&#20204;&#20063;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning techniques are effective for building predictive models because they are good at identifying patterns in large datasets. Development of a model for complex real life problems often stops at the point of publication, proof of concept or when made accessible through some mode of deployment. However, a model in the medical domain risks becoming obsolete as soon as patient demographic changes. The maintenance and monitoring of predictive models post-publication is crucial to guarantee their safe and effective long term use. As machine learning techniques are effectively trained to look for patterns in available datasets, the performance of a model for complex real life problems will not peak and remain fixed at the point of publication or even point of deployment. Rather, data changes over time, and they also changed when models are transported to new places to be used by new demography.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20219;&#20309;&#25968;&#20540;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#30340;&#24369;&#24418;&#24335;&#20272;&#35745;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#65288;WENDy&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#38750;&#32447;&#24615;ODE&#31995;&#32479;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#35813;&#26041;&#27861;&#31934;&#24230;&#39640;&#12289;&#40065;&#26834;&#24615;&#24378;&#12289;&#36895;&#24230;&#24555;&#12290;</title><link>http://arxiv.org/abs/2302.13271</link><description>&lt;p&gt;
&#20351;&#29992;WENDy&#30452;&#25509;&#20272;&#35745;ODE&#27169;&#22411;&#21442;&#25968;&#65306;&#24369;&#24418;&#24335;&#20272;&#35745;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Direct Estimation of Parameters in ODE Models Using WENDy: Weak-form Estimation of Nonlinear Dynamics. (arXiv:2302.13271v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13271
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20219;&#20309;&#25968;&#20540;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#30340;&#24369;&#24418;&#24335;&#20272;&#35745;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#65288;WENDy&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#38750;&#32447;&#24615;ODE&#31995;&#32479;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#35813;&#26041;&#27861;&#31934;&#24230;&#39640;&#12289;&#40065;&#26834;&#24615;&#24378;&#12289;&#36895;&#24230;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#24369;&#24418;&#24335;&#20272;&#35745;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#65288;WENDy&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#38750;&#32447;&#24615;ODE&#31995;&#32479;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;WENDy&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#25968;&#20540;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#65292;&#21487;&#20197;&#35745;&#31639;&#20934;&#30830;&#30340;&#20272;&#35745;&#20540;&#65292;&#24182;&#19988;&#23545;&#20110;&#22823;&#37327;&#65288;&#29983;&#29289;&#23398;&#19978;&#37325;&#35201;&#30340;&#65289;&#27979;&#37327;&#22122;&#22768;&#26159;&#40065;&#26834;&#30340;&#12290;&#23545;&#20110;&#20302;&#32500;&#31995;&#32479;&#21644;&#36866;&#24230;&#25968;&#37327;&#30340;&#25968;&#25454;&#65292;WENDy&#22312;&#36895;&#24230;&#21644;&#31934;&#24230;&#19978;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#21069;&#21521;&#27714;&#35299;&#22120;&#30340;&#38750;&#32447;&#24615;&#26368;&#23567;&#20108;&#20056;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;&#23545;&#20110;&#39640;&#32500;&#31995;&#32479;&#21644;&#24378;&#31995;&#32479;&#65292;WENDy&#36890;&#24120;&#27604;&#22522;&#20110;&#21069;&#21521;&#27714;&#35299;&#22120;&#30340;&#26041;&#27861;&#26356;&#24555;&#65288;&#36890;&#24120;&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#65289;&#21644;&#26356;&#31934;&#30830;&#12290;&#20854;&#26680;&#24515;&#25968;&#23398;&#24605;&#24819;&#28041;&#21450;&#23558;&#27169;&#22411;&#30340;&#24378;&#24418;&#24335;&#34920;&#31034;&#26377;&#25928;&#22320;&#36716;&#25442;&#20026;&#20854;&#24369;&#24418;&#24335;&#65292;&#28982;&#21518;&#35299;&#20915;&#22238;&#24402;&#38382;&#39064;&#20197;&#36827;&#34892;&#21442;&#25968;&#25512;&#26029;&#12290;&#20854;&#26680;&#24515;&#32479;&#35745;&#24605;&#24819;&#22522;&#20110;&#35823;&#24046;&#23545;&#21464;&#37327;&#26694;&#26550;&#65292;&#36825;&#38656;&#35201;&#20351;&#29992;&#36845;&#20195;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the Weak-form Estimation of Nonlinear Dynamics (WENDy) method for estimating model parameters for non-linear systems of ODEs. Without relying on any numerical differential equation solvers, WENDy computes accurate estimates and is robust to large (biologically relevant) levels of measurement noise. For low dimensional systems with modest amounts of data, WENDy is competitive with conventional forward solver-based nonlinear least squares methods in terms of speed and accuracy. For both higher dimensional systems and stiff systems, WENDy is typically both faster (often by orders of magnitude) and more accurate than forward solver-based approaches.  The core mathematical idea involves an efficient conversion of the strong form representation of a model to its weak form, and then solving a regression problem to perform parameter inference. The core statistical idea rests on the Errors-In-Variables framework, which necessitates the use of the iteratively reweighted least square
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#37319;&#29992;ADMM-PINNs&#31639;&#27861;&#26694;&#26550;&#35299;&#20915;&#38750;&#20809;&#28369;PDE&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#22312;&#36845;&#20195;&#20013;&#37319;&#29992;ADMM&#23558;PDE&#32422;&#26463;&#21644;&#38750;&#20809;&#28369;&#27491;&#21017;&#21270;&#39033;&#20998;&#31163;&#65292;&#20351;&#24471;&#21487;&#20197;&#39640;&#25928;&#22320;&#35299;&#20915; PDE-constrained optimization problems&#12290;</title><link>http://arxiv.org/abs/2302.08309</link><description>&lt;p&gt;
&#38024;&#23545;&#38750;&#20809;&#28369;&#20559;&#24494;&#20998;&#26041;&#31243;&#32422;&#26463;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#20351;&#29992;ADMM-PINNs&#31639;&#27861;&#26694;&#26550;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
The ADMM-PINNs Algorithmic Framework for Nonsmooth PDE-Constrained Optimization: A Deep Learning Approach. (arXiv:2302.08309v1 [math.OC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#37319;&#29992;ADMM-PINNs&#31639;&#27861;&#26694;&#26550;&#35299;&#20915;&#38750;&#20809;&#28369;PDE&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#22312;&#36845;&#20195;&#20013;&#37319;&#29992;ADMM&#23558;PDE&#32422;&#26463;&#21644;&#38750;&#20809;&#28369;&#27491;&#21017;&#21270;&#39033;&#20998;&#31163;&#65292;&#20351;&#24471;&#21487;&#20197;&#39640;&#25928;&#22320;&#35299;&#20915; PDE-constrained optimization problems&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22810;&#37325;&#20056;&#25968;&#20132;&#26367;&#26041;&#21521;&#27861;(ADMM)&#21644;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#30340;&#32452;&#21512;&#26469;&#35299;&#20915;&#19968;&#33324;&#31867;&#21035;&#30340;&#38750;&#20809;&#28369;&#20559;&#24494;&#20998;&#26041;&#31243;(PDE)&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#12290;&#24403;&#38656;&#35201;&#23545;&#25511;&#21046;&#25110;&#35774;&#35745;&#21464;&#37327;&#36827;&#34892;&#38480;&#21046;&#26102;&#65292;&#21487;&#20197;&#37319;&#29992;&#38468;&#21152;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;&#25152;&#24471;&#21040;&#30340;ADMM-PINNs&#31639;&#27861;&#26694;&#26550;&#26497;&#22823;&#22320;&#25193;&#23637;&#20102;PINNs&#30340;&#36866;&#29992;&#33539;&#22260;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#38750;&#20809;&#28369;&#30340;PDE&#32422;&#26463;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#37319;&#29992;ADMM&#21487;&#20197;&#23558;PDE&#32422;&#26463;&#21644;&#38750;&#20809;&#28369;&#27491;&#21017;&#21270;&#39033;&#20998;&#31163;&#20986;&#26469;&#65292;&#20351;&#36845;&#20195;&#26356;&#21152;&#39640;&#25928;&#12290;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#20854;&#20013;&#30340;&#19968;&#20010;&#23376;&#38382;&#39064;&#26159;&#24179;&#28369;&#30340;PDE&#21463;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#65292;&#21487;&#20197;&#36890;&#36807;PINNs&#26377;&#25928;&#22320;&#35299;&#20915;&#65292;&#32780;&#21478;&#19968;&#20010;&#26159;&#31616;&#21333;&#30340;&#38750;&#20809;&#28369;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#24120;&#26377;&#23553;&#38381;&#24418;&#24335;&#35299;&#25110;&#21487;&#20197;&#36890;&#36807;&#22810;&#31181;&#26631;&#20934;&#20248;&#21270;&#31639;&#27861;&#25110;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#26377;&#25928;&#22320;&#35299;&#20915;&#12290;&#36890;&#36807;&#20004;&#20010;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;ADMM-PINNs&#31639;&#27861;&#26694;&#26550;&#22312;&#35299;&#20915;PDE&#32422;&#26463;&#30340;&#38750;&#20809;&#28369;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the combination of the alternating direction method of multipliers (ADMM) with physics-informed neural networks (PINNs) for a general class of nonsmooth partial differential equation (PDE)-constrained optimization problems, where additional regularization can be employed for constraints on the control or design variables. The resulting ADMM-PINNs algorithmic framework substantially enlarges the applicable range of PINNs to nonsmooth cases of PDE-constrained optimization problems. The application of the ADMM makes it possible to untie the PDE constraints and the nonsmooth regularization terms for iterations. Accordingly, at each iteration, one of the resulting subproblems is a smooth PDE-constrained optimization which can be efficiently solved by PINNs, and the other is a simple nonsmooth optimization problem which usually has a closed-form solution or can be efficiently solved by various standard optimization algorithms or pre-trained neural networks. The ADMM-PINNs algorithmi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#29305;&#23450;&#39046;&#22495;&#35821;&#35328;&#65292;&#29992;&#20110;&#23558;&#20998;&#25955;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#26696;&#26144;&#23556;&#21040;FastFlow&#24182;&#34892;&#32534;&#31243;&#24211;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#22788;&#29702;&#22120;&#24179;&#21488;&#19978;&#29983;&#25104;&#19981;&#21516;&#30340;DML&#26041;&#26696;&#65292;&#30740;&#31350;&#32773;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#26041;&#26696;&#21644;&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#33021;&#28304;&#25928;&#29575;&#65292;&#24182;&#25104;&#21151;&#31227;&#26893;&#20102;PyTorch&#26694;&#26550;&#21040;RISC-V&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2302.07946</link><description>&lt;p&gt;
&#29992;&#20110;&#20998;&#25955;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#20852;RISC-V&#31995;&#32479;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Experimenting with Emerging RISC-V Systems for Decentralised Machine Learning. (arXiv:2302.07946v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07946
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#29305;&#23450;&#39046;&#22495;&#35821;&#35328;&#65292;&#29992;&#20110;&#23558;&#20998;&#25955;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#26696;&#26144;&#23556;&#21040;FastFlow&#24182;&#34892;&#32534;&#31243;&#24211;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#22788;&#29702;&#22120;&#24179;&#21488;&#19978;&#29983;&#25104;&#19981;&#21516;&#30340;DML&#26041;&#26696;&#65292;&#30740;&#31350;&#32773;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#26041;&#26696;&#21644;&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#33021;&#28304;&#25928;&#29575;&#65292;&#24182;&#25104;&#21151;&#31227;&#26893;&#20102;PyTorch&#26694;&#26550;&#21040;RISC-V&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;DML&#65289;&#20351;&#21512;&#20316;&#26426;&#22120;&#23398;&#20064;&#25670;&#33073;&#20102;&#38598;&#20013;&#24335;&#36755;&#20837;&#25968;&#25454;&#12290;&#32852;&#21512;&#23398;&#20064;&#65288;FL&#65289;&#21644;&#36793;&#32536;&#25512;&#26029;&#26159;DML&#30340;&#31034;&#20363;&#12290;&#34429;&#28982;DML&#24037;&#20855;&#65288;&#29305;&#21035;&#26159;FL&#65289;&#24320;&#22987;&#34028;&#21187;&#21457;&#23637;&#65292;&#20294;&#35768;&#22810;&#24037;&#20855;&#19981;&#22815;&#28789;&#27963;&#21644;&#20415;&#25658;&#65292;&#26080;&#27861;&#29992;&#20110;&#23454;&#39564;&#26032;&#22411;&#22788;&#29702;&#22120;&#65288;&#20363;&#22914;RISC-V&#65289;&#65292;&#38750;&#20840;&#36830;&#25509;&#32593;&#32476;&#25299;&#25169;&#21644;&#24322;&#27493;&#21327;&#20316;&#26041;&#26696;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#29305;&#23450;&#39046;&#22495;&#30340;&#35821;&#35328;&#20811;&#26381;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#23558;DML&#26041;&#26696;&#26144;&#23556;&#21040;&#22522;&#30784;&#20013;&#38388;&#20214;&#65288;&#21363;FastFlow&#24182;&#34892;&#32534;&#31243;&#24211;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;x86-64&#21644;ARM&#24179;&#21488;&#20197;&#21450;&#26032;&#20852;&#30340;RISC-V&#24179;&#21488;&#19978;&#29983;&#25104;&#19981;&#21516;&#30340;DML&#26041;&#26696;&#26469;&#36827;&#34892;&#23454;&#39564;&#12290;&#25105;&#20204;&#34920;&#24449;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#21644;&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#33021;&#28304;&#25928;&#29575;&#12290;&#20316;&#20026;&#38468;&#24102;&#20135;&#21697;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PyTorch&#26694;&#26550;&#30340;RISC-V&#31227;&#26893;&#65292;&#36825;&#26159;&#25105;&#20204;&#25152;&#30693;&#36947;&#30340;&#31532;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#31227;&#26893;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decentralised Machine Learning (DML) enables collaborative machine learning without centralised input data. Federated Learning (FL) and Edge Inference are examples of DML. While tools for DML (especially FL) are starting to flourish, many are not flexible and portable enough to experiment with novel processors (e.g., RISC-V), non-fully connected network topologies, and asynchronous collaboration schemes. We overcome these limitations via a domain-specific language allowing us to map DML schemes to an underlying middleware, i.e. the FastFlow parallel programming library. We experiment with it by generating different working DML schemes on x86-64 and ARM platforms and an emerging RISC-V one. We characterise the performance and energy efficiency of the presented schemes and systems. As a byproduct, we introduce a RISC-V porting of the PyTorch framework, the first publicly available to our knowledge.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22312;DTI&#39046;&#22495;&#20013;&#37319;&#29992;&#32852;&#37030;&#23398;&#20064;&#26469;&#27719;&#38598;&#21046;&#33647;&#25968;&#25454;&#65292;&#30456;&#23545;&#20110;&#26368;&#20339;&#38750;&#38544;&#31169;&#20445;&#25252;&#26367;&#20195;&#26041;&#27861;&#21487;&#33719;&#24471;&#39640;&#36798;15%&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#19988;&#38750;IID&#25968;&#25454;&#20998;&#24067;&#19981;&#20250;&#38477;&#20302;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.07684</link><description>&lt;p&gt;
&#38754;&#21521;&#33647;&#29289;&#38774;&#21521;&#30456;&#20114;&#20316;&#29992;&#30340;&#32852;&#37030;&#23398;&#20064;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
A Federated Learning Benchmark for Drug-Target Interaction. (arXiv:2302.07684v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22312;DTI&#39046;&#22495;&#20013;&#37319;&#29992;&#32852;&#37030;&#23398;&#20064;&#26469;&#27719;&#38598;&#21046;&#33647;&#25968;&#25454;&#65292;&#30456;&#23545;&#20110;&#26368;&#20339;&#38750;&#38544;&#31169;&#20445;&#25252;&#26367;&#20195;&#26041;&#27861;&#21487;&#33719;&#24471;&#39640;&#36798;15%&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#19988;&#38750;IID&#25968;&#25454;&#20998;&#24067;&#19981;&#20250;&#38477;&#20302;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33647;&#29289;&#38774;&#21521;&#30456;&#20114;&#20316;&#29992;(DTI)&#39046;&#22495;&#20013;&#27719;&#38598;&#21046;&#33647;&#25968;&#25454;&#20855;&#26377;&#25552;&#20379;&#25405;&#25937;&#29983;&#21629;&#30340;&#31361;&#30772;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30417;&#31649;&#38480;&#21046;&#21644;&#21830;&#19994;&#21033;&#30410;&#65292;&#36825;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#24212;&#29992;&#65292;&#35748;&#20026;&#36825;&#19982;&#34892;&#19994;&#30340;&#38480;&#21046;&#26159;&#21644;&#35299;&#30340;&#65292;&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#20849;&#20139;&#20219;&#20309;&#20449;&#24687;&#65292;&#36825;&#20123;&#20449;&#24687;&#21487;&#20197;&#25581;&#31034;&#23454;&#20307;&#30340;&#25968;&#25454;&#25110;&#20219;&#20309;&#20854;&#20182;&#30340;&#39640;&#27700;&#24179;&#24635;&#32467;&#12290;&#24403;&#36816;&#29992;&#20110;&#20195;&#34920;&#24615;&#30340;GraphDTA&#27169;&#22411;&#21644;KIBA&#25968;&#25454;&#38598;&#26102;&#65292;&#30456;&#23545;&#20110;&#26368;&#20339;&#30340;&#38750;&#38544;&#31169;&#20445;&#25252;&#26367;&#20195;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;15%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#19968;&#31995;&#21015;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;DTI&#25968;&#25454;&#38598;&#20013;&#65292;&#19982;&#20854;&#20182;&#39046;&#22495;&#19981;&#21516;&#30340;&#26159;&#65292;&#38750;IID&#25968;&#25454;&#20998;&#24067;&#19981;&#20250;&#38477;&#20302;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#28155;&#21152;&#26032;&#25968;&#25454;&#30340;&#30410;&#22788;&#19982;&#28155;&#21152;&#26356;&#22810;&#23458;&#25143;&#30340;&#25104;&#26412;&#20043;&#38388;&#30340;&#23454;&#36136;&#24615;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aggregating pharmaceutical data in the drug-target interaction (DTI) domain has the potential to deliver life-saving breakthroughs. It is, however, notoriously difficult due to regulatory constraints and commercial interests. This work proposes the application of federated learning, which we argue to be reconcilable with the industry's constraints, as it does not require sharing of any information that would reveal the entities' data or any other high-level summary of it. When used on a representative GraphDTA model and the KIBA dataset it achieves up to 15% improved performance relative to the best available non-privacy preserving alternative. Our extensive battery of experiments shows that, unlike in other domains, the non-IID data distribution in the DTI datasets does not deteriorate FL performance. Additionally, we identify a material trade-off between the benefits of adding new data, and the cost of adding more clients.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#22522;&#20110;&#34507;&#30333;&#36136;&#20849;&#36827;&#21270;&#30340;Transformer&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#30693;&#35782;&#21487;&#20197;&#36716;&#31227;&#21040;RNA&#25509;&#35302;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#25968;&#25454;&#31232;&#32570;&#30340;RNA&#25509;&#35302;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.06120</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35268;&#27169;&#34507;&#30333;&#36136;&#25509;&#35302;&#39044;&#27979;&#27169;&#22411;&#30340;&#30693;&#35782;&#21487;&#20197;&#36716;&#21270;&#21040;&#25968;&#25454;&#31232;&#32570;&#30340;RNA&#25509;&#35302;&#39044;&#27979;&#20219;&#21153;&#20013;
&lt;/p&gt;
&lt;p&gt;
Knowledge from Large-Scale Protein Contact Prediction Models Can Be Transferred to the Data-Scarce RNA Contact Prediction Task. (arXiv:2302.06120v2 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06120
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#22522;&#20110;&#34507;&#30333;&#36136;&#20849;&#36827;&#21270;&#30340;Transformer&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#30693;&#35782;&#21487;&#20197;&#36716;&#31227;&#21040;RNA&#25509;&#35302;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#25968;&#25454;&#31232;&#32570;&#30340;RNA&#25509;&#35302;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
RNA&#36890;&#36807;&#20854;&#32467;&#26500;&#22312;&#35768;&#22810;&#29983;&#29289;&#27963;&#21160;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20854;&#21151;&#33021;&#24448;&#24448;&#21463;&#20854;&#32467;&#26500;&#30340;&#24433;&#21709;&#12290;&#39044;&#27979;RNA&#24207;&#21015;&#20013;&#27599;&#20010;&#26680;&#33527;&#37240;&#20043;&#38388;&#30340;&#32467;&#26500;&#25509;&#36817;&#24615;&#21487;&#20197;&#34920;&#24449;RNA&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#20256;&#32479;&#19978;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20351;&#29992;&#19987;&#23478;&#35774;&#35745;&#30340;&#29305;&#24449;&#24182;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22522;&#20110;&#34507;&#30333;&#36136;&#20849;&#36827;&#21270;&#30340;Transformer&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#30693;&#35782;&#21487;&#20197;&#36716;&#31227;&#21040;RNA&#25509;&#35302;&#39044;&#27979;&#20219;&#21153;&#20013;&#12290;&#30001;&#20110;&#34507;&#30333;&#36136;&#25968;&#25454;&#38598;&#27604;RNA&#25509;&#35302;&#39044;&#27979;&#30340;&#25968;&#25454;&#38598;&#22823;&#20960;&#20010;&#25968;&#37327;&#32423;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#21457;&#29616;&#21644;&#38543;&#21518;&#30340;&#26694;&#26550;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#25968;&#25454;&#31232;&#32570;&#30340;&#29942;&#39048;&#12290;&#23454;&#39564;&#35777;&#23454;&#65292;&#20351;&#29992;&#20844;&#24320;&#30340;&#34507;&#30333;&#36136;&#27169;&#22411;&#36827;&#34892;&#36716;&#31227;&#23398;&#20064;&#30340;RNA&#25509;&#35302;&#39044;&#27979;&#25928;&#26524;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#34507;&#30333;&#36136;&#23398;&#20064;&#30340;&#32467;&#26500;&#27169;&#24335;&#21487;&#20197;&#36716;&#31227;&#21040;RNA&#19978;&#65292;&#20026;RNA&#32467;&#26500;&#39044;&#27979;&#24320;&#36767;&#20102;&#28508;&#22312;&#30340;&#26032;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
RNA, whose functionality is largely determined by its structure, plays an important role in many biological activities. The prediction of pairwise structural proximity between each nucleotide of an RNA sequence can characterize the structural information of the RNA. Historically, this problem has been tackled by machine learning models using expert-engineered features and trained on scarce labeled datasets. Here, we find that the knowledge learned by a protein-coevolution Transformer-based deep neural network can be transferred to the RNA contact prediction task. As protein datasets are orders of magnitude larger than those for RNA contact prediction, our findings and the subsequent framework greatly reduce the data scarcity bottleneck. Experiments confirm that RNA contact prediction through transfer learning using a publicly available protein model is greatly improved. Our findings indicate that the learned structural patterns of proteins can be transferred to RNAs, opening up potenti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#30340;KG&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.01313</link><description>&lt;p&gt;
&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Double Permutation Equivariance for Knowledge Graph Completion. (arXiv:2302.01313v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#30340;KG&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#30693;&#35782;&#22270;&#35889;(KGs)&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#65292;&#24182;&#31216;&#20043;&#20026;&#21452;&#20132;&#25442;&#23646;&#24615;&#22270;&#65292;&#20854;&#20013;&#33410;&#28857;&#21644;&#20108;&#20803;&#65288;&#20004;&#20010;&#33410;&#28857;&#20043;&#38388;&#30340;&#65289;&#34920;&#31034;&#24517;&#39035;&#23545;&#33410;&#28857;&#21495;&#21644;&#36793;&#65288;&#21450;&#33410;&#28857;&#65289;&#23646;&#24615;&#65288;&#20851;&#31995;&#21644;&#33410;&#28857;&#29305;&#24449;&#65289;&#30340;&#25490;&#21015;&#31561;&#21464;&#12290;&#21452;&#37325;&#25490;&#21015;&#31561;&#21464;&#30340;KG&#34920;&#31034;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#31561;&#21464;&#24615;&#23545;&#20851;&#31995;&#30340;&#32467;&#26500;&#34920;&#31034;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#31561;&#21464;&#34920;&#31034;&#34013;&#22270;&#65292;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;GNN&#30340;&#21452;&#25490;&#21015;&#31561;&#21464;&#31070;&#32463;&#32467;&#26500;&#65292;&#22312;WN18RR&#12289;FB237&#21644;NELL995&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#24182;&#33021;&#22815;&#20934;&#30830;&#25191;&#34892;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25191;&#34892;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work provides a formalization of Knowledge Graphs (KGs) as a new class of graphs that we denote doubly exchangeable attributed graphs, where node and pairwise (joint 2-node) representations must be equivariant to permutations of both node ids and edge (&amp; node) attributes (relations &amp; node features). Double-permutation equivariant KG representations open a new research direction in KGs. We show that this equivariance imposes a structural representation of relations that allows neural networks to perform complex logical reasoning tasks in KGs. Finally, we introduce a general blueprint for such equivariant representations and test a simple GNN-based double-permutation equivariant neural architecture that achieve state-of-the-art Hits@10 test accuracy in the WN18RR, FB237 and NELL995 inductive KG completion tasks, and can accurately perform logical reasoning tasks that no existing methods can perform, to the best of our knowledge.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#21487;&#36801;&#31227;&#27169;&#22359;&#21644;&#36866;&#24212;&#24615;&#23398;&#20064;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#26377;&#22768;&#33410;&#28857;&#30340;&#30693;&#35782;&#36716;&#31227;&#32473;&#26080;&#22768;&#33410;&#28857;&#65292;&#23454;&#29616;&#20102;&#38024;&#23545;&#21547;&#26377;&#8220;&#27785;&#40664;&#22823;&#22810;&#25968;&#8221;&#30340;&#22270;&#34920;&#30340;&#25239;&#24046;&#24314;&#27169;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#37117;&#20855;&#26377;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.00873</link><description>&lt;p&gt;
&#38024;&#23545;&#21547;&#26377;&#8220;&#27785;&#40664;&#22823;&#22810;&#25968;&#8221;&#30340;&#22270;&#34920;&#36827;&#34892;&#25239;&#24046;&#24314;&#27169;&#30340;&#30693;&#35782;&#21487;&#36801;&#31227;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Predicting the Silent Majority on Graphs: Knowledge Transferable Graph Neural Network. (arXiv:2302.00873v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#21487;&#36801;&#31227;&#27169;&#22359;&#21644;&#36866;&#24212;&#24615;&#23398;&#20064;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#26377;&#22768;&#33410;&#28857;&#30340;&#30693;&#35782;&#36716;&#31227;&#32473;&#26080;&#22768;&#33410;&#28857;&#65292;&#23454;&#29616;&#20102;&#38024;&#23545;&#21547;&#26377;&#8220;&#27785;&#40664;&#22823;&#22810;&#25968;&#8221;&#30340;&#22270;&#34920;&#30340;&#25239;&#24046;&#24314;&#27169;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#37117;&#20855;&#26377;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#23384;&#22312;&#30001;&#26377;&#22768;&#33410;&#28857;&#65288;&#8220;&#26377;&#22768;&#23569;&#25968;&#8221;&#65289;&#21644;&#26080;&#22768;&#33410;&#28857;&#65288;&#8220;&#27785;&#40664;&#22823;&#22810;&#25968;&#8221;&#65289;&#32452;&#25104;&#30340;&#22270;&#34920;&#65292;&#21363; VS-Graph&#65292;&#20854;&#20013;&#26377;&#22768;&#33410;&#28857;&#24448;&#24448;&#20855;&#26377;&#20016;&#23500;&#30340;&#29305;&#24449;&#21644;&#26631;&#31614;&#65292;&#32780;&#26080;&#22768;&#33410;&#28857;&#20165;&#20855;&#26377;&#19981;&#23436;&#25972;&#30340;&#29305;&#24449;&#21644;&#31232;&#32570;&#30340;&#26631;&#31614;&#12290; &#39044;&#27979;&#27785;&#40664;&#22823;&#22810;&#25968;&#20173;&#28982;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#30693;&#35782;&#21487;&#36801;&#31227;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;KT-GNN&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#26377;&#22768;&#33410;&#28857;&#30340;&#30693;&#35782;&#36716;&#31227;&#32473;&#26080;&#22768;&#33410;&#28857;&#26469;&#23454;&#29616;&#28040;&#24687;&#20256;&#36882;&#21644;&#34920;&#24449;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#20998;&#24067;&#28418;&#31227;&#24314;&#27169;&#12290; &#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KT-GNN &#22312;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#37117;&#20855;&#26377;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphs consisting of vocal nodes ("the vocal minority") and silent nodes ("the silent majority"), namely VS-Graph, are ubiquitous in the real world. The vocal nodes tend to have abundant features and labels. In contrast, silent nodes only have incomplete features and rare labels, e.g., the description and political tendency of politicians (vocal) are abundant while not for ordinary people (silent) on the twitter's social network. Predicting the silent majority remains a crucial yet challenging problem. However, most existing message-passing based GNNs assume that all nodes belong to the same domain, without considering the missing features and distribution-shift between domains, leading to poor ability to deal with VS-Graph. To combat the above challenges, we propose Knowledge Transferable Graph Neural Network (KT-GNN), which models distribution shifts during message passing and representation learning by transferring knowledge from vocal nodes to silent nodes. Specifically, we design 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#38544;&#21464;&#37327;&#35889;&#27169;&#22411;&#30340;&#39640;&#32500;PDEs&#39640;&#25928;&#31934;&#30830;&#27714;&#35299;&#22120;&#65292;&#23427;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20998;&#23618;&#25237;&#24433;&#32593;&#32476;&#23558;&#39640;&#32500;&#25968;&#25454;&#22312;&#32447;&#24615;&#26102;&#38388;&#20869;&#32553;&#23567;&#21040;&#19968;&#20010;&#32039;&#20945;&#30340;&#28508;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#35889;&#26041;&#27861;&#22312;&#28508;&#31354;&#38388;&#20013;&#23398;&#20064;&#31639;&#23376;&#35299;&#20915;&#32500;&#24230;&#35781;&#21650;&#12290;&#25928;&#26524;&#21644;&#21487;&#25193;&#23637;&#24615;&#22312;Navier-Stokes&#21644;Schr&#246;dinger&#26041;&#31243;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2301.12664</link><description>&lt;p&gt;
&#22522;&#20110;&#38544;&#21464;&#37327;&#35889;&#27169;&#22411;&#27714;&#35299;&#39640;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Solving High-Dimensional PDEs with Latent Spectral Models. (arXiv:2301.12664v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#38544;&#21464;&#37327;&#35889;&#27169;&#22411;&#30340;&#39640;&#32500;PDEs&#39640;&#25928;&#31934;&#30830;&#27714;&#35299;&#22120;&#65292;&#23427;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20998;&#23618;&#25237;&#24433;&#32593;&#32476;&#23558;&#39640;&#32500;&#25968;&#25454;&#22312;&#32447;&#24615;&#26102;&#38388;&#20869;&#32553;&#23567;&#21040;&#19968;&#20010;&#32039;&#20945;&#30340;&#28508;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#35889;&#26041;&#27861;&#22312;&#28508;&#31354;&#38388;&#20013;&#23398;&#20064;&#31639;&#23376;&#35299;&#20915;&#32500;&#24230;&#35781;&#21650;&#12290;&#25928;&#26524;&#21644;&#21487;&#25193;&#23637;&#24615;&#22312;Navier-Stokes&#21644;Schr&#246;dinger&#26041;&#31243;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#27169;&#22411;&#22312;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#12290;&#19968;&#31181;&#26032;&#20852;&#30340;&#33539;&#20363;&#26159;&#23398;&#20064;&#31070;&#32463;&#31639;&#23376;&#26469;&#36817;&#20284;PDEs&#30340;&#36755;&#20837;&#36755;&#20986;&#26144;&#23556;&#12290;&#34429;&#28982;&#20197;&#21069;&#30340;&#28145;&#24230;&#27169;&#22411;&#24050;&#32463;&#25506;&#32034;&#20102;&#22810;&#23610;&#24230;&#30340;&#20307;&#31995;&#32467;&#26500;&#21644;&#21508;&#31181;&#31639;&#23376;&#35774;&#35745;&#65292;&#20294;&#23427;&#20204;&#20165;&#38480;&#20110;&#22312;&#22352;&#26631;&#31354;&#38388;&#20013;&#25972;&#20307;&#23398;&#20064;&#31639;&#23376;&#12290;&#22312;&#23454;&#38469;&#29289;&#29702;&#31185;&#23398;&#38382;&#39064;&#20013;&#65292;PDEs&#26159;&#20855;&#26377;&#22797;&#26434;&#32806;&#21512;&#26041;&#31243;&#30340;&#65292;&#25968;&#20540;&#27714;&#35299;&#22120;&#20381;&#36182;&#20110;&#39640;&#32500;&#22352;&#26631;&#31354;&#38388;&#30340;&#31163;&#25955;&#21270;&#65292;&#36825;&#19981;&#33021;&#34987;&#21333;&#20010;&#31639;&#23376;&#20934;&#30830;&#22320;&#36817;&#20284;&#65292;&#20063;&#19981;&#33021;&#30001;&#20110;&#32500;&#24230;&#35781;&#21650;&#32780;&#26377;&#25928;&#22320;&#23398;&#20064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38544;&#21464;&#37327;&#35889;&#27169;&#22411;&#65288;LSM&#65289;&#30340;&#39640;&#32500;PDEs&#39640;&#25928;&#31934;&#30830;&#27714;&#35299;&#22120;&#12290;LSM&#19981;&#20165;&#36229;&#20986;&#20102;&#22352;&#26631;&#31354;&#38388;&#65292;&#32780;&#19988;&#36824;&#21033;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20998;&#23618;&#25237;&#24433;&#32593;&#32476;&#23558;&#39640;&#32500;&#25968;&#25454;&#22312;&#32447;&#24615;&#26102;&#38388;&#20869;&#32553;&#23567;&#21040;&#19968;&#20010;&#32039;&#20945;&#30340;&#28508;&#31354;&#38388;&#12290;&#21463;&#25968;&#20540;&#20998;&#26512;&#20013;&#32463;&#20856;&#30340;&#35889;&#26041;&#27861;&#30340;&#21551;&#31034;&#65292;LSM&#21033;&#29992;&#35889;&#26041;&#27861;&#22312;&#28508;&#31354;&#38388;&#20013;&#23398;&#20064;&#31639;&#23376;&#65292;&#21487;&#20197;&#25429;&#25417;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#20132;&#20114;&#24182;&#26377;&#25928;&#22320;&#35299;&#20915;&#32500;&#24230;&#35781;&#21650;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#39640;&#32500;PDEs&#19978;&#23637;&#31034;&#20102;LSM&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#22914;Navier-Stokes&#26041;&#31243;&#21644;Schr&#246;dinger&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep models have achieved impressive progress in solving partial differential equations (PDEs). A burgeoning paradigm is learning neural operators to approximate the input-output mappings of PDEs. While previous deep models have explored the multiscale architectures and various operator designs, they are limited to learning the operators as a whole in the coordinate space. In real physical science problems, PDEs are complex coupled equations with numerical solvers relying on discretization into high-dimensional coordinate space, which cannot be precisely approximated by a single operator nor efficiently learned due to the curse of dimensionality. We present Latent Spectral Models (LSM) toward an efficient and precise solver for high-dimensional PDEs. Going beyond the coordinate space, LSM enables an attention-based hierarchical projection network to reduce the high-dimensional data into a compact latent space in linear time. Inspired by classical spectral methods in numerical analysis,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FedHKD&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#23454;&#29616;&#26080;&#25968;&#25454;&#36229;&#30693;&#35782;&#33976;&#39311;&#65292;&#35299;&#20915;&#20102;&#24322;&#26500;&#25968;&#25454;&#23548;&#33268;&#30340;&#20840;&#23616;&#21644;&#20010;&#24615;&#21270;&#27169;&#22411;&#20934;&#30830;&#24615;&#38382;&#39064;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#34920;&#26126;&#20248;&#20110;&#29616;&#26377;FL&#26041;&#27861;&#21644;&#20013;&#24515;&#21270;&#35757;&#32451;&#26041;&#24335;&#19979;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2301.08968</link><description>&lt;p&gt;
&#26368;&#20339;&#32467;&#21512;&#65306;&#36890;&#36807;&#26080;&#25968;&#25454;&#36229;&#30693;&#35782;&#33976;&#39311;&#23454;&#29616;&#20934;&#30830;&#30340;&#20840;&#23616;&#21644;&#20010;&#24615;&#21270;&#27169;&#22411;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
The Best of Both Worlds: Accurate Global and Personalized Models through Federated Learning with Data-Free Hyper-Knowledge Distillation. (arXiv:2301.08968v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08968
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FedHKD&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#23454;&#29616;&#26080;&#25968;&#25454;&#36229;&#30693;&#35782;&#33976;&#39311;&#65292;&#35299;&#20915;&#20102;&#24322;&#26500;&#25968;&#25454;&#23548;&#33268;&#30340;&#20840;&#23616;&#21644;&#20010;&#24615;&#21270;&#27169;&#22411;&#20934;&#30830;&#24615;&#38382;&#39064;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#34920;&#26126;&#20248;&#20110;&#29616;&#26377;FL&#26041;&#27861;&#21644;&#20013;&#24515;&#21270;&#35757;&#32451;&#26041;&#24335;&#19979;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#30340;&#24322;&#26500;&#24615;&#38480;&#21046;&#20102;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#35757;&#32451;&#30340;&#20840;&#23616;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#20855;&#26377;&#23616;&#37096;&#25968;&#25454;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#12290;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;PFL&#65289;&#20316;&#20026;&#35299;&#20915;&#24322;&#26500;&#25968;&#25454;&#25361;&#25112;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#24050;&#32463;&#20986;&#29616;&#12290;&#20294;&#26159;&#29616;&#26377;&#30340;PFL&#26041;&#27861;&#36890;&#24120;&#20197;&#29306;&#29298;&#20840;&#23616;&#27169;&#22411;&#20934;&#30830;&#24615;&#20026;&#20195;&#20215;&#26469;&#25552;&#39640;&#26412;&#22320;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FedHKD&#65288;&#32852;&#37030;&#36229;&#30693;&#35782;&#33976;&#39311;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;FL&#31639;&#27861;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#20381;&#38752;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#26469;&#35757;&#32451;&#26412;&#22320;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FedHKD&#21487;&#20197;&#22312;&#20840;&#23616;&#21644;&#20010;&#24615;&#21270;&#20934;&#30830;&#24615;&#26041;&#38754;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#36229;&#36234;&#29616;&#26377;&#30340;FL&#26041;&#27861;&#65292;&#29978;&#33267;&#36229;&#36807;&#20197;&#38598;&#20013;&#26041;&#24335;&#35757;&#32451;&#30340;&#25152;&#26377;&#25968;&#25454;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneity of data distributed across clients limits the performance of global models trained through federated learning, especially in the settings with highly imbalanced class distributions of local datasets. In recent years, personalized federated learning (pFL) has emerged as a potential solution to the challenges presented by heterogeneous data. However, existing pFL methods typically enhance performance of local models at the expense of the global model's accuracy. We propose FedHKD (Federated Hyper-Knowledge Distillation), a novel FL algorithm in which clients rely on knowledge distillation (KD) to train local models. In particular, each client extracts and sends to the server the means of local data representations and the corresponding soft predictions -- information that we refer to as ``hyper-knowledge". The server aggregates this information and broadcasts it to the clients in support of local training. Notably, unlike other KD-based pFL methods, FedHKD does not rely on 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27010;&#24565;&#21457;&#29616;&#30340;&#27169;&#22411;&#19981;&#21487;&#30693;&#20803;&#23398;&#20064;&#26041;&#27861;&#65288;COMAML&#65289;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#25968;&#25454;&#29305;&#24449;&#20043;&#38388;&#30340;&#32467;&#26500;&#65292;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2301.07850</link><description>&lt;p&gt;
&#24555;&#36895;&#36866;&#24212;&#30340;&#27010;&#24565;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Concept Discovery for Fast Adapatation. (arXiv:2301.07850v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07850
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27010;&#24565;&#21457;&#29616;&#30340;&#27169;&#22411;&#19981;&#21487;&#30693;&#20803;&#23398;&#20064;&#26041;&#27861;&#65288;COMAML&#65289;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#25968;&#25454;&#29305;&#24449;&#20043;&#38388;&#30340;&#32467;&#26500;&#65292;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#27493;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#33021;&#36229;&#36234;&#20154;&#31867;&#34920;&#29616;&#65292;&#20294;&#23545;&#20110;&#19968;&#20010;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#24555;&#36895;&#36866;&#24212;&#21040;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#20803;&#23398;&#20064;&#65288;&#20063;&#31216;&#20026;&#23398;&#20064;&#22914;&#20309;&#23398;&#20064;&#65289;&#26159;&#19968;&#20010;&#24456;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#65292;&#22312;few-shot&#23398;&#20064;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#22312;&#25552;&#21462;&#21487;&#32467;&#26500;&#21270;&#21644;&#21487;&#36716;&#31227;&#30340;&#30693;&#35782;&#26041;&#38754;&#20173;&#19982;&#20154;&#31867;&#23398;&#20064;&#36807;&#31243;&#26377;&#24456;&#22823;&#19981;&#21516;&#12290;&#36825;&#19968;&#32570;&#28857;&#20351;&#24471;&#24403;&#21069;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#38590;&#20197;&#35299;&#37322;&#65292;&#20063;&#38590;&#20197;&#25193;&#23637;&#21040;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#27010;&#24565;&#21457;&#29616;&#24341;&#20837;&#21040;few-shot&#23398;&#20064;&#38382;&#39064;&#20013;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#25968;&#25454;&#29305;&#24449;&#20043;&#38388;&#30340;&#32467;&#26500;&#65292;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#36866;&#24212;&#65292;&#20174;&#32780;&#23548;&#20986;&#25968;&#25454;&#30340;&#22797;&#21512;&#34920;&#31034;&#24418;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#27010;&#24565;&#30340;&#27169;&#22411;&#19981;&#21487;&#30693;&#20803;&#23398;&#20064;&#26041;&#27861;&#65288;COMAML&#65289;&#24050;&#32463;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#19968;&#33268;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advances in deep learning have enabled machine learning methods to outperform human beings in various areas, but it remains a great challenge for a well-trained model to quickly adapt to a new task. One promising solution to realize this goal is through meta-learning, also known as learning to learn, which has achieved promising results in few-shot learning. However, current approaches are still enormously different from human beings' learning process, especially in the ability to extract structural and transferable knowledge. This drawback makes current meta-learning frameworks non-interpretable and hard to extend to more complex tasks. We tackle this problem by introducing concept discovery to the few-shot learning problem, where we achieve more effective adaptation by meta-learning the structure among the data features, leading to a composite representation of the data. Our proposed method Concept-Based Model-Agnostic Meta-Learning (COMAML) has been shown to achieve consistent i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#20849;&#35821;&#25163;&#21183;&#30340;&#29983;&#25104;&#30740;&#31350;&#65292;&#37325;&#28857;&#22312;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#19978;&#12290;&#20849;&#35821;&#25163;&#21183;&#26159;&#33258;&#28982;&#36890;&#20449;&#30340;&#19968;&#37096;&#20998;&#65292;&#23545;&#20110;&#30005;&#24433;&#21644;&#28216;&#25103;&#31561;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#38543;&#30528;&#36234;&#26469;&#36234;&#22823;&#30340;&#25163;&#21183;&#25968;&#25454;&#38598;&#21644;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#30340;&#36827;&#27493;&#65292;&#35813;&#39046;&#22495;&#26377;&#30528;&#24191;&#38420;&#30340;&#30740;&#31350;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2301.05339</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20849;&#35821;&#25163;&#21183;&#29983;&#25104;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Review of Data-Driven Co-Speech Gesture Generation. (arXiv:2301.05339v4 [cs.GR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05339
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#20849;&#35821;&#25163;&#21183;&#30340;&#29983;&#25104;&#30740;&#31350;&#65292;&#37325;&#28857;&#22312;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#19978;&#12290;&#20849;&#35821;&#25163;&#21183;&#26159;&#33258;&#28982;&#36890;&#20449;&#30340;&#19968;&#37096;&#20998;&#65292;&#23545;&#20110;&#30005;&#24433;&#21644;&#28216;&#25103;&#31561;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#38543;&#30528;&#36234;&#26469;&#36234;&#22823;&#30340;&#25163;&#21183;&#25968;&#25454;&#38598;&#21644;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#30340;&#36827;&#27493;&#65292;&#35813;&#39046;&#22495;&#26377;&#30528;&#24191;&#38420;&#30340;&#30740;&#31350;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20276;&#38543;&#35821;&#35328;&#30340;&#25163;&#21183;&#26159;&#33258;&#28982;&#32780;&#26377;&#25928;&#30340;&#20154;&#31867;&#20132;&#27969;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#12290;&#33258;&#21160;&#29983;&#25104;&#36825;&#31181;&#20849;&#35821;&#25163;&#21183;&#26159;&#35745;&#31639;&#26426;&#21160;&#30011;&#20013;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#34987;&#35748;&#20026;&#26159;&#30005;&#24433;&#12289;&#28216;&#25103;&#12289;&#34394;&#25311;&#31038;&#20132;&#31354;&#38388;&#20197;&#21450;&#19982;&#31038;&#20132;&#26426;&#22120;&#20154;&#20132;&#20114;&#30340;&#19968;&#31181;&#20351;&#33021;&#25216;&#26415;&#12290;&#30001;&#20110;&#20154;&#31867;&#20849;&#35821;&#25163;&#21183;&#36816;&#21160;&#30340;&#29420;&#29305;&#24615;&#21644;&#38750;&#21608;&#26399;&#24615;&#65292;&#20197;&#21450;&#25163;&#21183;&#25152;&#21253;&#25324;&#30340;&#20132;&#38469;&#21151;&#33021;&#30340;&#22810;&#26679;&#24615;&#65292;&#20351;&#24471;&#35813;&#38382;&#39064;&#38590;&#20197;&#35299;&#20915;&#12290;&#26368;&#36817;&#65292;&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#31867;&#25163;&#21183;&#25968;&#25454;&#38598;&#30340;&#20986;&#29616;&#65292;&#21152;&#19978;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#36827;&#27493;&#65292;&#25163;&#21183;&#29983;&#25104;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gestures that accompany speech are an essential part of natural and efficient embodied human communication. The automatic generation of such co-speech gestures is a long-standing problem in computer animation and is considered an enabling technology in film, games, virtual social spaces, and for interaction with social robots. The problem is made challenging by the idiosyncratic and non-periodic nature of human co-speech gesture motion, and by the great diversity of communicative functions that gestures encompass. Gesture generation has seen surging interest recently, owing to the emergence of more and larger datasets of human gesture motion, combined with strides in deep-learning-based generative models, that benefit from the growing availability of data. This review article summarizes co-speech gesture generation research, with a particular focus on deep generative models. First, we articulate the theory describing human gesticulation and how it complements speech. Next, we briefly d
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pix2Map&#30340;&#36328;&#27169;&#24577;&#26816;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#33258;&#25105;&#35270;&#35282;&#22270;&#20687;&#20013;&#25512;&#26029;&#22478;&#24066;&#34903;&#36947;&#22320;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#26356;&#26032;&#25110;&#25193;&#23637;&#29616;&#26377;&#22320;&#22270;&#12290;&#20351;&#29992;Argoverse&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.04224</link><description>&lt;p&gt;
Pix2Map: &#20174;&#22270;&#20687;&#20013;&#25512;&#26029;&#34903;&#36947;&#22320;&#22270;&#30340;&#36328;&#27169;&#24577;&#26816;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Pix2Map: Cross-modal Retrieval for Inferring Street Maps from Images. (arXiv:2301.04224v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pix2Map&#30340;&#36328;&#27169;&#24577;&#26816;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#33258;&#25105;&#35270;&#35282;&#22270;&#20687;&#20013;&#25512;&#26029;&#22478;&#24066;&#34903;&#36947;&#22320;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#26356;&#26032;&#25110;&#25193;&#23637;&#29616;&#26377;&#22320;&#22270;&#12290;&#20351;&#29992;Argoverse&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20381;&#36182;&#22478;&#24066;&#34903;&#36947;&#22320;&#22270;&#36827;&#34892;&#33258;&#20027;&#23548;&#33322;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Pix2Map&#65292;&#19968;&#31181;&#20174;&#33258;&#25105;&#35270;&#35282;&#22270;&#20687;&#20013;&#25512;&#26029;&#22478;&#24066;&#34903;&#36947;&#22320;&#22270;&#25299;&#25169;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#20197;&#19981;&#26029;&#26356;&#26032;&#21644;&#25193;&#23637;&#29616;&#26377;&#22320;&#22270;&#12290;&#36825;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#25105;&#20204;&#38656;&#35201;&#30452;&#25509;&#20174;&#21407;&#22987;&#22270;&#20687;&#25968;&#25454;&#25512;&#26029;&#22797;&#26434;&#30340;&#22478;&#24066;&#36947;&#36335;&#25299;&#25169;&#32467;&#26500;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#24605;&#36335;&#26159;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#22270;&#20687;&#21644;&#29616;&#26377;&#22320;&#22270;&#30340;&#32852;&#21512;&#12289;&#36328;&#27169;&#24577;&#23884;&#20837;&#31354;&#38388;&#65292;&#23558;&#36825;&#20010;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#36328;&#27169;&#24577;&#26816;&#32034;&#12290;&#25105;&#20204;&#20351;&#29992;Argoverse&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#20165;&#20174;&#22270;&#20687;&#25968;&#25454;&#20013;&#20934;&#30830;&#26816;&#32034;&#23545;&#24212;&#24050;&#30693;&#21644;&#26410;&#30693;&#36947;&#36335;&#30340;&#34903;&#36947;&#22320;&#22270;&#30340;&#21487;&#34892;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#26816;&#32034;&#21040;&#30340;&#22320;&#22270;&#21487;&#20197;&#29992;&#20110;&#26356;&#26032;&#25110;&#25193;&#23637;&#29616;&#26377;&#22320;&#22270;&#65292;&#24182;&#23637;&#31034;&#20102;&#35270;&#35273;&#23450;&#20301;&#21644;&#22270;&#20687;&#26816;&#32034;&#30340;&#27010;&#24565;&#35777;&#26126;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-driving vehicles rely on urban street maps for autonomous navigation. In this paper, we introduce Pix2Map, a method for inferring urban street map topology directly from ego-view images, as needed to continually update and expand existing maps. This is a challenging task, as we need to infer a complex urban road topology directly from raw image data. The main insight of this paper is that this problem can be posed as cross-modal retrieval by learning a joint, cross-modal embedding space for images and existing maps, represented as discrete graphs that encode the topological layout of the visual surroundings. We conduct our experimental evaluation using the Argoverse dataset and show that it is indeed possible to accurately retrieve street maps corresponding to both seen and unseen roads solely from image data. Moreover, we show that our retrieved maps can be used to update or expand existing maps and even show proof-of-concept results for visual localization and image retrieval fr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#20013;&#30340;&#25361;&#25112;&#19982;&#26426;&#20250;&#65292;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#28304;&#29305;&#23450;&#21644;&#38745;&#24577;&#35266;&#27979;&#25968;&#25454;&#19978;&#65292;&#23454;&#38469;&#38382;&#39064;&#22312;&#20110;&#35266;&#27979;&#25968;&#25454;&#21482;&#33021;&#36880;&#27493;&#33719;&#24471;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.01026</link><description>&lt;p&gt;
&#36830;&#32493;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#65306;&#25361;&#25112;&#19982;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
Continual Causal Effect Estimation: Challenges and Opportunities. (arXiv:2301.01026v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#20013;&#30340;&#25361;&#25112;&#19982;&#26426;&#20250;&#65292;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#28304;&#29305;&#23450;&#21644;&#38745;&#24577;&#35266;&#27979;&#25968;&#25454;&#19978;&#65292;&#23454;&#38469;&#38382;&#39064;&#22312;&#20110;&#35266;&#27979;&#25968;&#25454;&#21482;&#33021;&#36880;&#27493;&#33719;&#24471;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39046;&#22495;&#65292;&#22914;&#32463;&#27982;&#23398;&#12289;&#21307;&#30103;&#20445;&#20581;&#12289;&#20844;&#20849;&#25919;&#31574;&#12289;&#32593;&#32476;&#25366;&#25496;&#12289;&#22312;&#32447;&#24191;&#21578;&#21644;&#33829;&#38144;&#27963;&#21160;&#20013;&#65292;&#29702;&#35299;&#35266;&#23519;&#25968;&#25454;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#38750;&#24120;&#20851;&#38190;&#12290;&#23613;&#31649;&#22312;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#22914;&#22788;&#29702;&#32570;&#22833;&#30340;&#23545;&#29031;&#32467;&#26524;&#21644;&#27835;&#30103;&#19982;&#25511;&#21046;&#32452;&#20043;&#38388;&#30340;&#36873;&#25321;&#20559;&#24046;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#28304;&#29305;&#23450;&#21644;&#38745;&#24577;&#35266;&#27979;&#25968;&#25454;&#19978;&#12290;&#36825;&#20123;&#23398;&#20064;&#31574;&#30053;&#20551;&#35774;&#25152;&#26377;&#35266;&#27979;&#25968;&#25454;&#24050;&#32463;&#22312;&#35757;&#32451;&#38454;&#27573;&#21487;&#29992;&#19988;&#20165;&#26469;&#33258;&#20110;&#19968;&#20010;&#26469;&#28304;&#12290;&#36825;&#31181;&#21487;&#35775;&#38382;&#24615;&#30340;&#23454;&#38469;&#38382;&#39064;&#22312;&#21508;&#31181;&#23398;&#26415;&#21644;&#24037;&#19994;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#65292;&#25105;&#20204;&#38754;&#20020;&#30528;&#21033;&#29992;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#30340;&#26032;&#25361;&#25112;&#65292;&#21363;&#38024;&#23545;&#36880;&#27493;&#21487;&#29992;&#30340;&#35266;&#27979;&#25968;&#25454;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#39069;&#22806;&#39046;&#22495;&#36866;&#24212;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
A further understanding of cause and effect within observational data is critical across many domains, such as economics, health care, public policy, web mining, online advertising, and marketing campaigns. Although significant advances have been made to overcome the challenges in causal effect estimation with observational data, such as missing counterfactual outcomes and selection bias between treatment and control groups, the existing methods mainly focus on source-specific and stationary observational data. Such learning strategies assume that all observational data are already available during the training phase and from only one source. This practical concern of accessibility is ubiquitous in various academic and industrial applications. That's what it boiled down to: in the era of big data, we face new challenges in causal inference with observational data, i.e., the extensibility for incrementally available observational data, the adaptability for extra domain adaptation proble
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25552;&#20986;&#20102;&#36873;&#25321;&#20381;&#36182;&#20998;&#31867;&#65288;SDC&#65289;&#21464;&#20307;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#28436;&#31034;&#20102;&#27880;&#24847;&#21147;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#26080;&#35823;&#20294;&#19981;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#30340;&#22810;&#31181;&#38169;&#35823;&#27169;&#24335;&#12290;&#35813;&#30740;&#31350;&#20026;&#35780;&#20272;SDC&#27169;&#22411;&#21450;&#20854;&#35299;&#37322;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#26550;&#26500;&#30340;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.14776</link><description>&lt;p&gt;
&#20851;&#20110;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Interpretability of Attention Networks. (arXiv:2212.14776v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25552;&#20986;&#20102;&#36873;&#25321;&#20381;&#36182;&#20998;&#31867;&#65288;SDC&#65289;&#21464;&#20307;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#28436;&#31034;&#20102;&#27880;&#24847;&#21147;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#26080;&#35823;&#20294;&#19981;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#30340;&#22810;&#31181;&#38169;&#35823;&#27169;&#24335;&#12290;&#35813;&#30740;&#31350;&#20026;&#35780;&#20272;SDC&#27169;&#22411;&#21450;&#20854;&#35299;&#37322;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#26550;&#26500;&#30340;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#26159;&#22810;&#20010;&#25104;&#21151;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#26680;&#24515;&#32452;&#20214;&#65292;&#22522;&#20110;&#19968;&#20010;&#20851;&#38190;&#24819;&#27861;&#65306;&#8220;&#36755;&#20986;&#20165;&#21462;&#20915;&#20110;&#36755;&#20837;&#30340;&#19968;&#20010;&#23567;&#65288;&#20294;&#26410;&#30693;&#65289;&#37096;&#20998;&#8221;&#12290;&#22312;&#22270;&#20687;&#23383;&#24149;&#21644;&#35821;&#35328;&#32763;&#35793;&#31561;&#22810;&#20010;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#36825;&#36890;&#24120;&#26159;&#27491;&#30830;&#30340;&#12290;&#22312;&#20855;&#26377;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35757;&#32451;&#27169;&#22411;&#20013;&#65292;&#32534;&#30721;&#19982;&#36755;&#20986;&#30456;&#20851;&#30340;&#36755;&#20837;&#27573;&#30340;&#20013;&#38388;&#27169;&#22359;&#30340;&#36755;&#20986;&#36890;&#24120;&#34987;&#29992;&#20316;&#31397;&#35270;&#32593;&#32476;&#8220;&#25512;&#29702;&#8221;&#30340;&#19968;&#31181;&#26041;&#24335;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#27880;&#24847;&#21147;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#35299;&#20915;&#19968;&#20010;&#21464;&#20307;&#20998;&#31867;&#38382;&#39064;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#36873;&#25321;&#20381;&#36182;&#20998;&#31867;&#65288;SDC&#65289;&#65292;&#20174;&#32780;&#26356;&#21152;&#28165;&#26224;&#22320;&#38416;&#36848;&#20102;&#36825;&#31181;&#27010;&#24565;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22810;&#31181;&#38169;&#35823;&#27169;&#24335;&#65292;&#20854;&#20013;&#27880;&#24847;&#21147;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#26080;&#35823;&#20294;&#19981;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#34920;&#26126;&#36825;&#31181;&#27169;&#22411;&#22240;&#35757;&#32451;&#32780;&#20986;&#29616;&#12290;&#25105;&#20204;&#36824;&#38416;&#36848;&#20102;&#21487;&#20197;&#21152;&#24378;&#21644;&#20943;&#36731;&#27492;&#34892;&#20026;&#30340;&#22810;&#31181;&#24773;&#20917;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#30446;&#26631;&#23450;&#20041;&#20102;&#19968;&#31181;&#23545;&#20110;SDC&#27169;&#22411;&#21450;&#20854;&#35299;&#37322;&#24615;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#26550;&#26500;&#30340;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention mechanisms form a core component of several successful deep learning architectures, and are based on one key idea: ''The output depends only on a small (but unknown) segment of the input.'' In several practical applications like image captioning and language translation, this is mostly true. In trained models with an attention mechanism, the outputs of an intermediate module that encodes the segment of input responsible for the output is often used as a way to peek into the `reasoning` of the network. We make such a notion more precise for a variant of the classification problem that we term selective dependence classification (SDC) when used with attention model architectures. Under such a setting, we demonstrate various error modes where an attention model can be accurate but fail to be interpretable, and show that such models do occur as a result of training. We illustrate various situations that can accentuate and mitigate this behaviour. Finally, we use our objective def
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21322;&#30417;&#30563;&#33258;&#32534;&#30721;&#22120;&#20197;&#21450;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#23613;&#21487;&#33021;&#23569;&#30340;&#26631;&#35760;&#26679;&#26412;&#26469;&#24320;&#21457;&#36719;&#27979;&#37327;&#20256;&#24863;&#22120;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#20102;&#25104;&#26412;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#20316;&#32773;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#21462;&#24471;&#22909;&#30340;&#39044;&#27979;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.13067</link><description>&lt;p&gt;
&#20351;&#29992;&#21322;&#30417;&#30563;&#33258;&#32534;&#30721;&#22120;&#30340;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#36827;&#34892;&#36719;&#27979;&#37327;&#24320;&#21457;
&lt;/p&gt;
&lt;p&gt;
Online Active Learning for Soft Sensor Development using Semi-Supervised Autoencoders. (arXiv:2212.13067v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21322;&#30417;&#30563;&#33258;&#32534;&#30721;&#22120;&#20197;&#21450;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#23613;&#21487;&#33021;&#23569;&#30340;&#26631;&#35760;&#26679;&#26412;&#26469;&#24320;&#21457;&#36719;&#27979;&#37327;&#20256;&#24863;&#22120;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#20102;&#25104;&#26412;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#20316;&#32773;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#21462;&#24471;&#22909;&#30340;&#39044;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#36719;&#27979;&#37327;&#22312;&#24037;&#19994;&#21644;&#21270;&#23398;&#36807;&#31243;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20197;&#39044;&#27979;&#38590;&#20197;&#27979;&#37327;&#30340;&#36807;&#31243;&#21464;&#37327;&#12290;&#36825;&#20123;&#20256;&#24863;&#22120;&#20351;&#29992;&#30340;&#22238;&#24402;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#30340;&#26679;&#26412;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#36136;&#37327;&#26816;&#26597;&#38656;&#35201;&#39640;&#26114;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#65292;&#33719;&#21462;&#26631;&#31614;&#20449;&#24687;&#21487;&#33021;&#38750;&#24120;&#26114;&#36149;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#21487;&#33021;&#38750;&#24120;&#26377;&#30410;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#24314;&#35758;&#26597;&#35810;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#20026;&#22238;&#24402;&#25552;&#20986;&#30340;&#22823;&#22810;&#25968;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#37117;&#38598;&#20013;&#22312;&#31163;&#32447;&#22330;&#26223;&#12290;&#26412;&#25991;&#23558;&#20854;&#20013;&#19968;&#20123;&#26041;&#27861;&#36866;&#24212;&#20110;&#27969;&#24335;&#22330;&#26223;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#22522;&#20110;&#27491;&#20132;&#33258;&#32534;&#30721;&#22120;&#30340;&#21322;&#30417;&#30563;&#26550;&#26500;&#23398;&#20064;&#20302;&#32500;&#31354;&#38388;&#20013;&#30340;&#26174;&#33879;&#29305;&#24449;&#12290;&#25105;&#20204;&#20063;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#30000;&#32435;&#35199;&#19996;&#26364;&#36807;&#31243;&#27604;&#36739;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven soft sensors are extensively used in industrial and chemical processes to predict hard-to-measure process variables whose real value is difficult to track during routine operations. The regression models used by these sensors often require a large number of labeled examples, yet obtaining the label information can be very expensive given the high time and cost required by quality inspections. In this context, active learning methods can be highly beneficial as they can suggest the most informative labels to query. However, most of the active learning strategies proposed for regression focus on the offline setting. In this work, we adapt some of these approaches to the stream-based scenario and show how they can be used to select the most informative data points. We also demonstrate how to use a semi-supervised architecture based on orthogonal autoencoders to learn salient features in a lower dimensional space. The Tennessee Eastman Process is used to compare the predictive 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;PE&#65292;&#21363;&#22522;&#20110;&#21152;&#26435;&#22270;&#36941;&#21382;&#33258;&#21160;&#26426;&#30340;&#22270;&#33258;&#21160;&#26426;PE&#65288;GAPE&#65289;&#12290;&#19982;&#20854;&#20182;PE&#26041;&#26696;&#30456;&#27604;&#65292;GAPE&#24050;&#35777;&#26126;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;PE&#65292;&#24182;&#22312;&#26426;&#22120;&#32763;&#35793;&#21644;&#22270;&#24418;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#23545;&#35768;&#22810;&#26368;&#36817;&#22312;&#22270;&#24418;Transformer&#20013;&#30340;PE&#36827;&#34892;&#20102;&#29702;&#35770;&#21644;&#25511;&#21046;&#23454;&#39564;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2212.06898</link><description>&lt;p&gt;
&#21033;&#29992;&#21152;&#26435;&#22270;&#36941;&#21382;&#33258;&#21160;&#26426;&#20026;Transformer&#24314;&#31435;&#22270;&#24418;&#20301;&#32622;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Bridging Graph Position Encodings for Transformers with Weighted Graph-Walking Automata. (arXiv:2212.06898v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;PE&#65292;&#21363;&#22522;&#20110;&#21152;&#26435;&#22270;&#36941;&#21382;&#33258;&#21160;&#26426;&#30340;&#22270;&#33258;&#21160;&#26426;PE&#65288;GAPE&#65289;&#12290;&#19982;&#20854;&#20182;PE&#26041;&#26696;&#30456;&#27604;&#65292;GAPE&#24050;&#35777;&#26126;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;PE&#65292;&#24182;&#22312;&#26426;&#22120;&#32763;&#35793;&#21644;&#22270;&#24418;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#23545;&#35768;&#22810;&#26368;&#36817;&#22312;&#22270;&#24418;Transformer&#20013;&#30340;PE&#36827;&#34892;&#20102;&#29702;&#35770;&#21644;&#25511;&#21046;&#23454;&#39564;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#39046;&#22495;&#30340;&#19968;&#20010;&#30446;&#26631;&#26159;&#35753;Transformer&#33021;&#22815;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#65292;&#23613;&#31649;&#20182;&#20204;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#30001;&#20110;Transformer&#26368;&#21021;&#30340;&#27491;&#24358;&#20301;&#32622;&#32534;&#30721;&#65288;PE&#65289;&#19981;&#36866;&#29992;&#20110;&#22270;&#24418;&#65292;&#22240;&#27492;&#26368;&#36817;&#30340;&#24037;&#20316;&#38598;&#20013;&#20110;&#24320;&#21457;&#22522;&#20110;&#35889;&#22270;&#29702;&#35770;&#25110;&#22270;&#24418;&#21508;&#31181;&#31354;&#38388;&#29305;&#24449;&#30340;&#22270;PE&#12290; &#26412;&#25991;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#22270;PE&#8212;&#8212;&#22522;&#20110;&#21152;&#26435;&#22270;&#36941;&#21382;&#33258;&#21160;&#26426;&#30340;&#22270;&#33258;&#21160;&#26426;PE&#65288;GAPE&#65289;&#65292;&#24182;&#22312;&#26426;&#22120;&#32763;&#35793;&#21644;&#22270;&#24418;&#20219;&#21153;&#20013;&#23558;&#20854;&#19982;&#20854;&#20182;PE&#26041;&#26696;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#34920;&#26126;&#23427;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;PE&#12290;&#35813;&#30740;&#31350;&#30340;&#21478;&#19968;&#20010;&#36129;&#29486;&#26159;&#22312;&#29420;&#31435;&#20110;&#36793;&#32536;&#29305;&#24449;&#20351;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#35768;&#22810;&#26368;&#36817;&#22312;&#22270;&#24418;Transformer&#20013;&#30340;PE&#36827;&#34892;&#29702;&#35770;&#21644;&#25511;&#21046;&#23454;&#39564;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
A current goal in the graph neural network literature is to enable transformers to operate on graph-structured data, given their success on language and vision tasks. Since the transformer's original sinusoidal positional encodings (PEs) are not applicable to graphs, recent work has focused on developing graph PEs, rooted in spectral graph theory or various spatial features of a graph. In this work, we introduce a new graph PE, Graph Automaton PE (GAPE), based on weighted graph-walking automata (a novel extension of graph-walking automata). We compare the performance of GAPE with other PE schemes on both machine translation and graph-structured tasks, and we show that it generalizes several other PEs. An additional contribution of this study is a theoretical and controlled experimental comparison of many recent PEs in graph transformers, independent of the use of edge features.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MobileTL&#65292;&#19968;&#31181;&#22522;&#20110;&#20869;&#37096;&#26631;&#20934;&#21270;&#23618;&#20855;&#26377;&#20869;&#23384;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;IRBs&#27169;&#22411;&#12290;MobileTL&#36890;&#36807;&#35757;&#32451;&#20869;&#37096;&#35268;&#33539;&#21270;&#23618;&#30340;&#31227;&#20301;&#26469;&#36991;&#20813;&#23384;&#20648;&#21521;&#21518;&#20256;&#36882;&#30340;&#28608;&#27963;&#22270;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#20869;&#23384;&#20351;&#29992;&#29575;&#65292;&#24182;&#22312;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#20013;&#20445;&#25345;&#20102;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.03246</link><description>&lt;p&gt;
MobileTL: &#22522;&#20110;Inverted Residual Blocks&#30340;&#35774;&#22791;&#26412;&#22320;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MobileTL: On-device Transfer Learning with Inverted Residual Blocks. (arXiv:2212.03246v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MobileTL&#65292;&#19968;&#31181;&#22522;&#20110;&#20869;&#37096;&#26631;&#20934;&#21270;&#23618;&#20855;&#26377;&#20869;&#23384;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;IRBs&#27169;&#22411;&#12290;MobileTL&#36890;&#36807;&#35757;&#32451;&#20869;&#37096;&#35268;&#33539;&#21270;&#23618;&#30340;&#31227;&#20301;&#26469;&#36991;&#20813;&#23384;&#20648;&#21521;&#21518;&#20256;&#36882;&#30340;&#28608;&#27963;&#22270;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#20869;&#23384;&#20351;&#29992;&#29575;&#65292;&#24182;&#22312;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#20013;&#20445;&#25345;&#20102;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#22791;&#26412;&#22320;&#36801;&#31227;&#23398;&#20064;&#38754;&#20020;&#26377;&#38480;&#30340;&#35774;&#22791;&#36164;&#28304;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#21442;&#25968;&#30340;&#23376;&#38598;&#25110;&#21152;&#20837;&#27169;&#22411;&#34917;&#19969;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20026;&#20102;&#26356;&#39640;&#25928;&#30340;&#25512;&#29702;&#65292;Inverted Residual Blocks&#65288;IRBs&#65289;&#23558;&#21367;&#31215;&#23618;&#20998;&#20026;&#36880;&#23618;&#28145;&#24230;&#21644;&#36880;&#28857;&#21367;&#31215;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22810;&#21367;&#31215;&#12289;&#26631;&#20934;&#21270;&#21644;&#28608;&#27963;&#23618;&#30340;&#22534;&#21472;&#12290;&#34429;&#28982;&#23427;&#20204;&#23545;&#20110;&#25512;&#29702;&#26159;&#39640;&#25928;&#30340;&#65292;&#20294;IRBs&#38656;&#35201;&#22312;&#20869;&#23384;&#20013;&#23384;&#20648;&#39069;&#22806;&#30340;&#28608;&#27963;&#26144;&#23556;&#26469;&#35757;&#32451;&#21367;&#31215;&#23618;&#30340;&#26435;&#37325;&#21644;&#26631;&#20934;&#21270;&#23618;&#30340;&#35268;&#27169;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#30340;&#39640;&#20869;&#23384;&#25104;&#26412;&#38459;&#30861;&#20102;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#35757;&#32451;IRBs&#65292;&#20351;&#20854;&#22312;&#36801;&#31227;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#19981;&#36866;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MobileTL&#65292;&#19968;&#31181;&#22522;&#20110;&#20869;&#37096;&#26631;&#20934;&#21270;&#23618;&#30340;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;IRBs&#27169;&#22411;&#12290;MobileTL&#35757;&#32451;&#20869;&#37096;&#35268;&#33539;&#21270;&#23618;&#30340;&#31227;&#20301;&#65292;&#20197;&#36991;&#20813;&#23384;&#20648;&#21521;&#21518;&#20256;&#36882;&#30340;&#28608;&#27963;&#22270;&#12290;&#22312;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#20013;&#65292;MobileTL&#26174;&#33879;&#38477;&#20302;&#20102;&#20869;&#23384;&#20351;&#29992;&#29575;&#65292;&#24182;&#20445;&#25345;&#20102;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20010;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#25104;&#21151;&#30340;&#36801;&#31227;&#23398;&#20064;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning on edge is challenging due to on-device limited resources. Existing work addresses this issue by training a subset of parameters or adding model patches. Developed with inference in mind, Inverted Residual Blocks (IRBs) split a convolutional layer into depthwise and pointwise convolutions, leading to more stacking layers, e.g., convolution, normalization, and activation layers. Though they are efficient for inference, IRBs require that additional activation maps are stored in memory for training weights for convolution layers and scales for normalization layers. As a result, their high memory cost prohibits training IRBs on resource-limited edge devices, and making them unsuitable in the context of transfer learning. To address this issue, we present MobileTL, a memory and computationally efficient on-device transfer learning method for models built with IRBs. MobileTL trains the shifts for internal normalization layers to avoid storing activation maps for the backwar
&lt;/p&gt;</description></item><item><title>&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#31181;&#29305;&#27530;&#26550;&#26500;&#65292;&#23427;&#21487;&#22312;&#19981;&#27844;&#38706;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#32452;&#21512;&#26412;&#22320;&#27169;&#22411;&#35757;&#32451;&#30340;&#32467;&#26524;&#26469;&#24314;&#31435;&#23436;&#25972;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#26412;&#25991;&#26159;&#23545;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#29616;&#26377;&#30740;&#31350;&#36827;&#34892;&#20102;&#32467;&#26500;&#21270;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;&#20854;&#30740;&#31350;&#29616;&#29366;&#12289;&#24212;&#29992;&#12289;&#38480;&#21046;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2212.00622</link><description>&lt;p&gt;
&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65306;&#19968;&#39033;&#32467;&#26500;&#21270;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Vertical Federated Learning: A Structured Literature Review. (arXiv:2212.00622v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00622
&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#31181;&#29305;&#27530;&#26550;&#26500;&#65292;&#23427;&#21487;&#22312;&#19981;&#27844;&#38706;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#32452;&#21512;&#26412;&#22320;&#27169;&#22411;&#35757;&#32451;&#30340;&#32467;&#26524;&#26469;&#24314;&#31435;&#23436;&#25972;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#26412;&#25991;&#26159;&#23545;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#29616;&#26377;&#30740;&#31350;&#36827;&#34892;&#20102;&#32467;&#26500;&#21270;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;&#20854;&#30740;&#31350;&#29616;&#29366;&#12289;&#24212;&#29992;&#12289;&#38480;&#21046;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#20855;&#26377;&#25968;&#25454;&#38544;&#31169;&#20248;&#21183;&#30340;&#26377;&#21069;&#36884;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#33539;&#24335;&#12290;&#38543;&#30528;&#23545;&#25968;&#25454;&#25152;&#26377;&#32773;&#20043;&#38388;&#21512;&#20316;&#30340;&#20852;&#36259;&#22686;&#21152;&#65292;FL&#24050;&#24341;&#36215;&#32452;&#32455;&#30340;&#37325;&#35270;&#12290;FL&#30340;&#24819;&#27861;&#26159;&#20351;&#21512;&#20316;&#21442;&#19982;&#32773;&#22312;&#19981;&#27844;&#38706;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#20998;&#25955;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#12290;&#31616;&#21333;&#22320;&#35828;&#65292;&#32852;&#37030;&#23398;&#20064;&#26159;&#8220;&#23558;&#27169;&#22411;&#24102;&#21040;&#25968;&#25454;&#65292;&#32780;&#19981;&#26159;&#23558;&#25968;&#25454;&#24102;&#21040;&#27169;&#22411;&#8221;&#30340;&#26041;&#27861;&#12290;&#24403;&#32852;&#37030;&#23398;&#20064;&#24212;&#29992;&#20110;&#22402;&#30452;&#20998;&#21306;&#30340;&#25968;&#25454;&#26102;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#32452;&#21512;&#20351;&#29992;&#21508;&#22320;&#28857;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#30340;&#26412;&#22320;&#27169;&#22411;&#35757;&#32451;&#30340;&#27169;&#22411;&#26469;&#24314;&#31435;&#23436;&#25972;&#30340;ML&#27169;&#22411;&#12290;&#36825;&#31181;FL&#30340;&#26550;&#26500;&#34987;&#31216;&#20026;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65288;VFL&#65289;&#65292;&#23427;&#19981;&#21516;&#20110;&#27700;&#24179;&#20998;&#21306;&#30340;&#20256;&#32479;FL&#12290;&#30001;&#20110;VFL&#19982;&#20256;&#32479;FL&#19981;&#21516;&#65292;&#22240;&#27492;&#23427;&#20855;&#26377;&#33258;&#36523;&#30340;&#38382;&#39064;&#21644;&#25361;&#25112;&#12290;&#26412;&#25991;&#23545;&#29616;&#26377;&#30740;&#31350;&#20013;&#20851;&#20110;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#30340;&#32467;&#26500;&#21270;&#25991;&#29486;&#32508;&#36848;&#65292;&#20998;&#26512;&#20102;VFL&#30340;&#24403;&#21069;&#30740;&#31350;&#29616;&#29366;&#12289;&#24212;&#29992;&#12289;&#38480;&#21046;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has emerged as a promising distributed learning paradigm with an added advantage of data privacy. With the growing interest in having collaboration among data owners, FL has gained significant attention of organizations. The idea of FL is to enable collaborating participants train machine learning (ML) models on decentralized data without breaching privacy. In simpler words, federated learning is the approach of ``bringing the model to the data, instead of bringing the data to the mode''. Federated learning, when applied to data which is partitioned vertically across participants, is able to build a complete ML model by combining local models trained only using the data with distinct features at the local sites. This architecture of FL is referred to as vertical federated learning (VFL), which differs from the conventional FL on horizontally partitioned data. As VFL is different from conventional FL, it comes with its own issues and challenges. In this paper, we
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;&#26032;&#22411;&#19979;&#30028;&#65292;&#26500;&#24314;&#21487;&#35777;&#26126;&#30340;&#26368;&#20248;&#31232;&#30095;&#22238;&#24402;&#26641;&#65292;&#24182;&#22312;&#20960;&#31186;&#38047;&#20869;&#25214;&#21040;&#26368;&#20248;&#30340;&#31232;&#30095;&#26641;&#12290;</title><link>http://arxiv.org/abs/2211.14980</link><description>&lt;p&gt;
&#26368;&#20248;&#31232;&#30095;&#22238;&#24402;&#26641;
&lt;/p&gt;
&lt;p&gt;
Optimal Sparse Regression Trees. (arXiv:2211.14980v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;&#26032;&#22411;&#19979;&#30028;&#65292;&#26500;&#24314;&#21487;&#35777;&#26126;&#30340;&#26368;&#20248;&#31232;&#30095;&#22238;&#24402;&#26641;&#65292;&#24182;&#22312;&#20960;&#31186;&#38047;&#20869;&#25214;&#21040;&#26368;&#20248;&#30340;&#31232;&#30095;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#24402;&#26641;&#26159;&#26368;&#21476;&#32769;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#20043;&#19968;&#65292;&#20854;&#39044;&#27979;&#21487;&#20197;&#22312;&#27809;&#26377;&#35745;&#31639;&#22120;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#29305;&#21035;&#26159;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#24191;&#27867;&#20351;&#29992;&#12290;&#22312;&#22238;&#24402;&#26641;&#30340;&#22823;&#37327;&#25991;&#29486;&#20013;&#65292;&#23545;&#20110;&#23436;&#20840;&#21487;&#35777;&#26126;&#30340;&#20248;&#21270;&#24037;&#20316;&#24456;&#23569;&#65292;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#38382;&#39064;&#30340;&#35745;&#31639;&#38590;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#36793;&#30028;&#30340;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#26469;&#26500;&#24314;&#21487;&#35777;&#26126;&#30340;&#26368;&#20248;&#31232;&#30095;&#22238;&#24402;&#26641;&#12290;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;1&#32500;k&#22343;&#20540;&#32858;&#31867;&#31639;&#27861;&#30340;&#26368;&#20248;&#35299;&#30340;&#26032;&#22411;&#19979;&#30028;&#26469;&#36827;&#34892;&#20248;&#21270;&#12290;&#21363;&#20351;&#26159;&#23545;&#20110;&#28041;&#21450;&#22823;&#37327;&#26679;&#26412;&#21644;&#39640;&#24230;&#30456;&#20851;&#29305;&#24449;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36890;&#24120;&#20063;&#33021;&#22312;&#20960;&#31186;&#38047;&#20869;&#25214;&#21040;&#26368;&#20248;&#30340;&#31232;&#30095;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regression trees are one of the oldest forms of AI models, and their predictions can be made without a calculator, which makes them broadly useful, particularly for high-stakes applications. Within the large literature on regression trees, there has been little effort towards full provable optimization, mainly due to the computational hardness of the problem. This work proposes a dynamic-programming-with-bounds approach to the construction of provably-optimal sparse regression trees. We leverage a novel lower bound based on an optimal solution to the k-Means clustering algorithm in 1-dimension over the set of labels. We are often able to find optimal sparse trees in seconds, even for challenging datasets that involve large numbers of samples and highly-correlated features.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#24402;&#32435;&#20559;&#24046;&#30340;&#24433;&#21709;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#36873;&#25321;&#23545;&#23398;&#20064;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2211.14699</link><description>&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#24402;&#32435;&#20559;&#24046;&#30340;&#29702;&#35770;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Theoretical Study of Inductive Biases in Contrastive Learning. (arXiv:2211.14699v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#24402;&#32435;&#20559;&#24046;&#30340;&#24433;&#21709;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#36873;&#25321;&#23545;&#23398;&#20064;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#33258;&#30417;&#30563;&#23398;&#20064;&#26159;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20043;&#21069;&#30340;&#29702;&#35770;&#30740;&#31350;&#23545;&#39044;&#35757;&#32451;&#25439;&#22833;&#30340;&#20316;&#29992;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#35270;&#20026;&#26222;&#36890;&#30340;&#40657;&#30418;&#23376;&#12290;&#28982;&#32780;&#65292;Saunshi&#31561;&#20154;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#27169;&#22411;&#32467;&#26500;&#8212;&#8212;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#24456;&#23569;&#20851;&#27880;&#30340;&#19968;&#20010;&#32452;&#25104;&#37096;&#20998;&#8212;&#8212;&#23545;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#19979;&#28216;&#24615;&#33021;&#20063;&#26377;&#26174;&#33879;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#23558;&#28304;&#20110;&#27169;&#22411;&#31867;&#30340;&#24402;&#32435;&#20559;&#24046;&#30340;&#24433;&#21709;&#32435;&#20837;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20851;&#27880;&#23545;&#20110;&#35270;&#35273;&#39046;&#22495;&#26222;&#36941;&#20351;&#29992;&#30340;&#19968;&#31181;&#27969;&#34892;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#23545;&#27604;&#23398;&#20064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#27169;&#22411;&#20855;&#26377;&#26377;&#38480;&#30340;&#23481;&#37327;&#26102;&#65292;&#23545;&#27604;&#34920;&#31034;&#20250;&#24674;&#22797;&#19982;&#27169;&#22411;&#32467;&#26500;&#20860;&#23481;&#30340;&#26576;&#20123;&#29305;&#27530;&#32858;&#31867;&#32467;&#26500;&#65292;&#20294;&#20250;&#24573;&#30053;&#25968;&#25454;&#20998;&#24067;&#20013;&#30340;&#35768;&#22810;&#20854;&#20182;&#32858;&#31867;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#21487;&#20197;&#25429;&#25417;&#23545;&#27604;&#23398;&#20064;&#26356;&#20026;&#22797;&#26434;&#30340;&#34892;&#20026;&#65292;&#25552;&#20379;&#20102;&#26377;&#20851;&#27169;&#22411;&#32467;&#26500;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#23398;&#20064;&#36807;&#31243;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding self-supervised learning is important but challenging. Previous theoretical works study the role of pretraining losses, and view neural networks as general black boxes. However, the recent work of Saunshi et al. argues that the model architecture -- a component largely ignored by previous works -- also has significant influences on the downstream performance of self-supervised learning. In this work, we provide the first theoretical analysis of self-supervised learning that incorporates the effect of inductive biases originating from the model class. In particular, we focus on contrastive learning -- a popular self-supervised learning method that is widely used in the vision domain. We show that when the model has limited capacity, contrastive representations would recover certain special clustering structures that are compatible with the model architecture, but ignore many other clustering structures in the data distribution. As a result, our theory can capture the more 
&lt;/p&gt;</description></item><item><title>PatchGT&#26159;&#19968;&#31181;&#22522;&#20110;&#38750;&#21487;&#35757;&#32451;&#32858;&#31867;&#30340;Transformer&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#23558;&#22270;&#20998;&#21106;&#20026;patch&#23398;&#20064;&#22270;&#34920;&#31034;&#65292;&#20860;&#20855;GNN&#21644;Transformer&#30340;&#20248;&#28857;&#65292;&#21487;&#22312;&#22810;&#31181;&#22270;&#23398;&#20064;&#20219;&#21153;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.14425</link><description>&lt;p&gt;
PatchGT&#65306;&#22522;&#20110;&#38750;&#21487;&#35757;&#32451;&#32858;&#31867;&#30340;Transformer&#29992;&#20110;&#23398;&#20064;&#22270;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
PatchGT: Transformer over Non-trainable Clusters for Learning Graph Representations. (arXiv:2211.14425v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14425
&lt;/p&gt;
&lt;p&gt;
PatchGT&#26159;&#19968;&#31181;&#22522;&#20110;&#38750;&#21487;&#35757;&#32451;&#32858;&#31867;&#30340;Transformer&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#23558;&#22270;&#20998;&#21106;&#20026;patch&#23398;&#20064;&#22270;&#34920;&#31034;&#65292;&#20860;&#20855;GNN&#21644;Transformer&#30340;&#20248;&#28857;&#65292;&#21487;&#22312;&#22810;&#31181;&#22270;&#23398;&#20064;&#20219;&#21153;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;Transformer&#32467;&#26500;&#22312;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;Transformer&#27169;&#22411;&#30452;&#25509;&#22788;&#29702;&#22270;&#33410;&#28857;&#65292;&#21487;&#33021;&#38590;&#20197;&#23398;&#20064;&#39640;&#23618;&#27425;&#30340;&#20449;&#24687;&#12290;&#21463;&#22270;&#20687;patch&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;Transformer&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;Patch Graph Transformer&#65288;PatchGT&#65289;&#12290;&#19982;&#20808;&#21069;&#29992;&#20110;&#23398;&#20064;&#22270;&#34920;&#31034;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#19981;&#21516;&#65292;PatchGT&#20174;&#38750;&#21487;&#35757;&#32451;&#22270;patch&#20013;&#23398;&#20064;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#20174;&#33410;&#28857;&#23398;&#20064;&#12290;&#23427;&#21487;&#20197;&#24110;&#21161;&#33410;&#30465;&#35745;&#31639;&#37327;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#20351;&#29992;&#35889;&#32858;&#31867;&#23558;&#22270;&#20998;&#21106;&#20026;patch&#65292;&#24182;&#20351;&#29992;GNN&#23618;&#39318;&#20808;&#23398;&#20064;patch&#32423;&#21035;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#20351;&#29992;Transformer&#33719;&#21462;&#22270;&#32423;&#21035;&#30340;&#34920;&#31034;&#12290;&#35813;&#26550;&#26500;&#21033;&#29992;&#20102;&#22270;&#30340;&#35889;&#20449;&#24687;&#65292;&#32467;&#21512;&#20102;GNN&#21644;Transformer&#30340;&#20248;&#28857;&#12290;&#36827;&#19968;&#27493;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20808;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#21253;&#25324;&#33410;&#28857;&#20998;&#31867;&#12289;&#38142;&#36335;&#39044;&#27979;&#21644;&#22270;&#20998;&#31867;&#22312;&#20869;&#30340;&#21508;&#31181;&#22522;&#20110;&#22270;&#30340;&#20219;&#21153;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PatchGT&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently the Transformer structure has shown good performances in graph learning tasks. However, these Transformer models directly work on graph nodes and may have difficulties learning high-level information. Inspired by the vision transformer, which applies to image patches, we propose a new Transformer-based graph neural network: Patch Graph Transformer (PatchGT). Unlike previous transformer-based models for learning graph representations, PatchGT learns from non-trainable graph patches, not from nodes directly. It can help save computation and improve the model performance. The key idea is to segment a graph into patches based on spectral clustering without any trainable parameters, with which the model can first use GNN layers to learn patch-level representations and then use Transformer to obtain graph-level representations. The architecture leverages the spectral information of graphs and combines the strengths of GNNs and Transformers. Further, we show the limitations of previo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26435;&#37325;&#38598;&#25104;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24320;&#21457;&#20801;&#35768;&#25968;&#25454;&#30456;&#20851;&#30340;&#21152;&#26435;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#39640;&#26368;&#36817;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#30340;&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#25913;&#21464;&#21407;&#26377;&#30340;&#26550;&#26500;&#65292;&#20854;&#22312; ImageNet-1K &#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340; DINO &#21644; MSN &#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#23567;&#26679;&#26412;&#35774;&#32622;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2211.09981</link><description>&lt;p&gt;
&#24102;&#26435;&#37325;&#38598;&#25104;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Weighted Ensemble Self-Supervised Learning. (arXiv:2211.09981v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26435;&#37325;&#38598;&#25104;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24320;&#21457;&#20801;&#35768;&#25968;&#25454;&#30456;&#20851;&#30340;&#21152;&#26435;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#39640;&#26368;&#36817;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#30340;&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#25913;&#21464;&#21407;&#26377;&#30340;&#26550;&#26500;&#65292;&#20854;&#22312; ImageNet-1K &#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340; DINO &#21644; MSN &#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#23567;&#26679;&#26412;&#35774;&#32622;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#20581;&#22766;&#24615;&#30340;&#26377;&#25928;&#25216;&#26415;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#36827;&#23637;&#20351;&#24471;&#21033;&#29992;&#22823;&#35268;&#27169;&#26410;&#26631;&#35760;&#35821;&#26009;&#24211;&#36827;&#34892;&#26368;&#20808;&#36827;&#30340;&#23567;&#26679;&#26412;&#21644;&#30417;&#30563;&#23398;&#20064;&#25104;&#20026;&#21487;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#20801;&#35768;&#25968;&#25454;&#30456;&#20851;&#30340;&#21152;&#26435;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#26694;&#26550;&#26469;&#25913;&#36827;&#26368;&#36817;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#12290;&#25105;&#20204;&#36991;&#20813;&#23545;&#34920;&#31034;&#39592;&#24178;&#36827;&#34892;&#38598;&#25104;&#65307;&#36825;&#20010;&#36873;&#25321;&#20135;&#29983;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#23427;&#30340;&#35757;&#32451;&#25104;&#26412;&#24456;&#23567;&#65292;&#23545;&#19979;&#28216;&#35780;&#20272;&#19981;&#38656;&#35201;&#36827;&#34892;&#26550;&#26500;&#25913;&#21464;&#25110;&#35745;&#31639;&#24320;&#38144;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312; ImageNet-1K &#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#20102;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861; DINO (Caron &#31561;&#20154;&#65292;2021) &#21644; MSN (Assran &#31561;&#20154;&#65292;2022)&#65292;&#22312;&#22810;&#20010;&#35780;&#20272;&#25351;&#26631;&#19978;&#22343;&#20248;&#20110;&#23427;&#20204;&#65292;&#23588;&#20854;&#22312;&#23567;&#26679;&#26412;&#35774;&#32622;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#20960;&#31181;&#21152;&#26435;&#26041;&#26696;&#65292;&#24182;&#21457;&#29616;&#8230;&#65288;&#26410;&#23436;&#25104;&#65289;
&lt;/p&gt;
&lt;p&gt;
Ensembling has proven to be a powerful technique for boosting model performance, uncertainty estimation, and robustness in supervised learning. Advances in self-supervised learning (SSL) enable leveraging large unlabeled corpora for state-of-the-art few-shot and supervised learning performance. In this paper, we explore how ensemble methods can improve recent SSL techniques by developing a framework that permits data-dependent weighted cross-entropy losses. We refrain from ensembling the representation backbone; this choice yields an efficient ensemble method that incurs a small training cost and requires no architectural changes or computational overhead to downstream evaluation. The effectiveness of our method is demonstrated with two state-of-the-art SSL methods, DINO (Caron et al., 2021) and MSN (Assran et al., 2022). Our method outperforms both in multiple evaluation metrics on ImageNet-1K, particularly in the few-shot setting. We explore several weighting schemes and find that th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;QSL&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20132;&#21449;&#36890;&#36947;&#27744;&#21270;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#21152;&#36895;&#25910;&#25947;&#12289;&#20943;&#23569;&#36890;&#35759;&#25104;&#26412;&#21644;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2211.06524</link><description>&lt;p&gt;
&#22522;&#20110;&#20132;&#21449;&#36890;&#36947;&#27744;&#21270;&#30340;&#37327;&#23376;&#20998;&#35010;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Quantum Split Neural Network Learning using Cross-Channel Pooling. (arXiv:2211.06524v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;QSL&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20132;&#21449;&#36890;&#36947;&#27744;&#21270;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#21152;&#36895;&#25910;&#25947;&#12289;&#20943;&#23569;&#36890;&#35759;&#25104;&#26412;&#21644;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#37327;&#23376;&#31185;&#23398;&#39046;&#22495;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#12289;&#37327;&#23376;&#36890;&#20449;&#21644;&#37327;&#23376;&#35745;&#31639;&#31561;&#22810;&#20010;&#23398;&#31185;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#22312;&#36825;&#20123;&#26032;&#20852;&#39046;&#22495;&#20013;&#65292;&#30001;&#20110;&#23558;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;(QNNs)&#19982;&#20256;&#32479;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;(FL)&#30456;&#32467;&#21512;&#65292;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;(QFL)&#24341;&#36215;&#20102;&#29305;&#21035;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#37327;&#23376;&#20998;&#35010;&#23398;&#20064;(QSL)&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#26159;&#32463;&#20856;&#20998;&#35010;&#23398;&#20064;&#30340;&#20808;&#36827;&#24310;&#20280;&#12290;&#22312;&#20256;&#32479;&#35745;&#31639;&#26426;&#39046;&#22495;&#20013;&#65292;&#20998;&#35010;&#23398;&#20064;&#24050;&#32463;&#35777;&#26126;&#20102;&#35768;&#22810;&#20248;&#28857;&#65292;&#20363;&#22914;&#21152;&#36895;&#25910;&#25947;&#65292;&#20943;&#23569;&#36890;&#35759;&#25104;&#26412;&#21644;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#12290;&#20026;&#20102;&#26368;&#22823;&#21270;QSL&#30340;&#28508;&#21147;&#65292;&#24341;&#20837;&#20102;&#20132;&#21449;&#36890;&#36947;&#27744;&#21270;&#25216;&#26415;&#65292;&#36825;&#31181;&#25216;&#26415;&#21033;&#29992;&#20102;QNNs&#25152;&#23454;&#29616;&#30340;&#37327;&#23376;&#24577;&#37325;&#26500;&#30340;&#29420;&#29305;&#24615;&#36136;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#25968;&#20540;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;QSL&#19981;&#20165;&#23454;&#29616;&#20102;&#20248;&#24322;&#30340;&#23398;&#20064;&#24615;&#33021;&#65292;&#32780;&#19988;&#22312;&#22312;&#20445;&#25252;&#38544;&#31169;&#26041;&#38754;&#20063;&#26356;&#20026;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the field of quantum science has attracted significant interest across various disciplines, including quantum machine learning, quantum communication, and quantum computing. Among these emerging areas, quantum federated learning (QFL) has gained particular attention due to the integration of quantum neural networks (QNNs) with traditional federated learning (FL) techniques. In this study, a novel approach entitled quantum split learning (QSL) is presented, which represents an advanced extension of classical split learning. Previous research in classical computing has demonstrated numerous advantages of split learning, such as accelerated convergence, reduced communication costs, and enhanced privacy protection. To maximize the potential of QSL, cross-channel pooling is introduced, a technique that capitalizes on the distinctive properties of quantum state tomography facilitated by QNNs. Through rigorous numerical analysis, evidence is provided that QSL not only achieve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SUPRA&#30340;&#36229;&#20687;&#32032;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;SLICLoss&#19982;&#20108;&#20803;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20869;&#31397;&#38236;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.04658</link><description>&lt;p&gt;
SUPRA:&#36229;&#20687;&#32032;&#24341;&#23548;&#25439;&#22833;&#29992;&#20110;&#25913;&#36827;&#20869;&#31397;&#38236;&#22810;&#27169;&#24577;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
SUPRA: Superpixel Guided Loss for Improved Multi-modal Segmentation in Endoscopy. (arXiv:2211.04658v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SUPRA&#30340;&#36229;&#20687;&#32032;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;SLICLoss&#19982;&#20108;&#20803;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20869;&#31397;&#38236;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#36716;&#31227;&#26159;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#30340;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#38382;&#39064;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#20869;&#31397;&#38236;&#22270;&#20687;&#20998;&#26512;&#65292;&#20854;&#20013;&#25968;&#25454;&#21487;&#20197;&#20855;&#26377;&#19981;&#21516;&#30340;&#27169;&#24577;&#65292;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26041;&#27861;&#30340;&#24615;&#33021;&#21463;&#21040;&#19981;&#21033;&#24433;&#21709;&#12290;&#22312;&#30495;&#23454;&#30340;&#20020;&#24202;&#29615;&#22659;&#20013;&#65292;&#20869;&#38236;&#21307;&#29983;&#20250;&#22312;&#27169;&#24577;&#20043;&#38388;&#20999;&#25442;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#40655;&#33180;&#21487;&#35270;&#21270;&#25928;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#39046;&#22495;&#27010;&#25324;&#25216;&#26415;&#65292;&#20351;DL&#26041;&#27861;&#21487;&#20197;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;Simple Linear Iterative Clustering (SLIC) &#20135;&#29983;&#30340;&#36229;&#20687;&#32032;&#65292;&#31216;&#20026; "SUPRA"&#65292;&#29992;&#20110;&#36229;&#20687;&#32032;&#22686;&#24378;&#26041;&#27861;&#12290;SUPRA&#39318;&#20808;&#29983;&#25104;&#39044;&#23450;&#20998;&#21106;&#25513;&#27169;&#65292;&#21033;&#29992;&#25105;&#20204;&#30340;&#26032;&#25439;&#22833;"SLICLoss"&#40723;&#21169;&#20934;&#30830;&#21644;&#39068;&#33394;&#19968;&#33268;&#30340;&#20998;&#21106;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#23558;SLICLoss&#19982;&#20108;&#20803;&#20132;&#21449;&#29109;&#25439;&#22833;&#65288;BCE&#65289;&#30456;&#32467;&#21512;&#26102;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#27010;&#25324;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain shift is a well-known problem in the medical imaging community. In particular, for endoscopic image analysis where the data can have different modalities the performance of deep learning (DL) methods gets adversely affected. In other words, methods developed on one modality cannot be used for a different modality. However, in real clinical settings, endoscopists switch between modalities for better mucosal visualisation. In this paper, we explore the domain generalisation technique to enable DL methods to be used in such scenarios. To this extend, we propose to use super pixels generated with Simple Linear Iterative Clustering (SLIC) which we refer to as "SUPRA" for SUPeRpixel Augmented method. SUPRA first generates a preliminary segmentation mask making use of our new loss "SLICLoss" that encourages both an accurate and color-consistent segmentation. We demonstrate that SLICLoss when combined with Binary Cross Entropy loss (BCE) can improve the model's generalisability with dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#23569;&#26679;&#26412;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#26032;&#20808;&#21069;&#20250;&#35805;&#31867;&#21407;&#22411;&#20197;&#34920;&#31034;&#30495;&#27491;&#30340;&#31867;&#24179;&#22343;&#20540;&#65292;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#20445;&#30041;&#20808;&#21069;&#24050;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#21363;&#20351;&#35757;&#32451;&#26032;&#31867;&#26102;&#20063;&#26159;&#22914;&#27492;&#12290;</title><link>http://arxiv.org/abs/2211.02947</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#21407;&#22411;&#22235;&#20803;&#32452;
&lt;/p&gt;
&lt;p&gt;
Prototypical quadruplet for few-shot class incremental learning. (arXiv:2211.02947v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#23569;&#26679;&#26412;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#26032;&#20808;&#21069;&#20250;&#35805;&#31867;&#21407;&#22411;&#20197;&#34920;&#31034;&#30495;&#27491;&#30340;&#31867;&#24179;&#22343;&#20540;&#65292;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#20445;&#30041;&#20808;&#21069;&#24050;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#21363;&#20351;&#35757;&#32451;&#26032;&#31867;&#26102;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#29616;&#20195;&#35745;&#31639;&#26426;&#35270;&#35273;&#31639;&#27861;&#20013;&#65292;&#25968;&#25454;&#31232;&#32570;&#21644;&#26032;&#20219;&#21153;&#30340;&#22686;&#37327;&#23398;&#20064;&#26500;&#25104;&#20102;&#20004;&#20010;&#20027;&#35201;&#29942;&#39048;&#12290;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#65292;&#21363;&#27169;&#22411;&#22312;&#20351;&#29992;&#26032;&#25209;&#27425;&#25968;&#25454;&#35757;&#32451;&#21518;&#26080;&#27861;&#23545;&#20808;&#21069;&#23398;&#20064;&#30340;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20256;&#32479;&#26041;&#27861;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#21516;&#26102;&#20250;&#29306;&#29298;&#24403;&#21069;&#20250;&#35805;&#30340;&#35757;&#32451;&#12290;&#29983;&#25104;&#22797;&#30424;&#26041;&#27861;&#65292;&#22914;&#29983;&#25104;&#25932;&#23545;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#24050;&#34987;&#25552;&#20986;&#20197;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#20294;&#20351;&#29992;&#23569;&#37327;&#26679;&#26412;&#35757;&#32451;GAN&#21487;&#33021;&#23548;&#33268;&#19981;&#31283;&#23450;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25913;&#36827;&#30340;&#23545;&#27604;&#25439;&#22833;&#26469;&#35782;&#21035;&#26356;&#22909;&#30340;&#23884;&#20837;&#31354;&#38388;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#31867;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26356;&#26032;&#20808;&#21069;&#20250;&#35805;&#31867;&#21407;&#22411;&#20197;&#34920;&#31034;&#30495;&#27491;&#30340;&#31867;&#24179;&#22343;&#20540;&#65292;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#20445;&#30041;&#20808;&#21069;&#24050;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#21363;&#20351;&#35757;&#32451;&#26032;&#31867;&#26102;&#20063;&#26159;&#22914;&#27492;&#65292;&#36825;&#23545;&#20110;&#25105;&#20204;&#30340;&#26368;&#36817;&#31867;&#21035;&#20998;&#31867;&#22120;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scarcity of data and incremental learning of new tasks pose two major bottlenecks for many modern computer vision algorithms. The phenomenon of catastrophic forgetting, i.e., the model's inability to classify previously learned data after training with new batches of data, is a major challenge. Conventional methods address catastrophic forgetting while compromising the current session's training. Generative replay-based approaches, such as generative adversarial networks (GANs), have been proposed to mitigate catastrophic forgetting, but training GANs with few samples may lead to instability. To address these challenges, we propose a novel method that improves classification robustness by identifying a better embedding space using an improved contrasting loss. Our approach retains previously acquired knowledge in the embedding space, even when trained with new classes, by updating previous session class prototypes to represent the true class mean, which is crucial for our nearest class
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;GFlowNets&#21644;&#21464;&#20998;&#36125;&#21494;&#26031;&#32852;&#21512;&#23398;&#20064;&#22240;&#26524;&#27169;&#22411;&#30340;&#32467;&#26500;&#21644;&#26426;&#21046;&#65292;&#19981;&#20165;&#33021;&#22815;&#22788;&#29702;&#38750;&#32447;&#24615;&#21644;&#38750;&#39640;&#26031;&#25968;&#25454;&#65292;&#22312;&#27169;&#25311;&#25968;&#25454;&#19978;&#20063;&#33021;&#19982;&#20960;&#20010;&#22522;&#32447;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;</title><link>http://arxiv.org/abs/2211.02763</link><description>&lt;p&gt;
GFlowNets&#19982;&#21464;&#20998;&#36125;&#21494;&#26031;&#30340;&#22240;&#26524;&#32467;&#26500;&#21644;&#26426;&#21046;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bayesian learning of Causal Structure and Mechanisms with GFlowNets and Variational Bayes. (arXiv:2211.02763v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;GFlowNets&#21644;&#21464;&#20998;&#36125;&#21494;&#26031;&#32852;&#21512;&#23398;&#20064;&#22240;&#26524;&#27169;&#22411;&#30340;&#32467;&#26500;&#21644;&#26426;&#21046;&#65292;&#19981;&#20165;&#33021;&#22815;&#22788;&#29702;&#38750;&#32447;&#24615;&#21644;&#38750;&#39640;&#26031;&#25968;&#25454;&#65292;&#22312;&#27169;&#25311;&#25968;&#25454;&#19978;&#20063;&#33021;&#19982;&#20960;&#20010;&#22522;&#32447;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#26088;&#22312;&#23398;&#20064;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#19978;&#30340;&#21518;&#39564;&#20998;&#24067;&#21644;&#23450;&#20041;&#29238;&#21464;&#37327;&#21644;&#23376;&#21464;&#37327;&#20043;&#38388;&#20851;&#31995;&#30340;&#26426;&#21046;&#12290;&#26412;&#25991;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21464;&#20998;&#36125;&#21494;&#26031;&#32852;&#21512;&#23398;&#20064;&#22240;&#26524;&#27169;&#22411;&#30340;&#32467;&#26500;&#21644;&#26426;&#21046;&#65292;&#31216;&#20026;&#21464;&#20998;&#36125;&#21494;&#26031;DAG-GFlowNet&#65288;VBG&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;GFlowNets&#25193;&#23637;&#20102;&#36125;&#21494;&#26031;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#19981;&#20165;&#23398;&#20064;&#32467;&#26500;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#36824;&#23398;&#20064;&#32447;&#24615;&#39640;&#26031;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#25968;&#25454;&#19978;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;VBG&#22312;&#24314;&#27169;DAG&#21644;&#26426;&#21046;&#30340;&#21518;&#39564;&#20998;&#24067;&#26102;&#65292;&#19981;&#20165;&#33021;&#22815;&#22788;&#29702;&#38750;&#32447;&#24615;&#21644;&#38750;&#39640;&#26031;&#25968;&#25454;&#65292;&#32780;&#19988;&#36824;&#33021;&#19982;&#20960;&#20010;&#22522;&#32447;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian causal structure learning aims to learn a posterior distribution over directed acyclic graphs (DAGs), and the mechanisms that define the relationship between parent and child variables. By taking a Bayesian approach, it is possible to reason about the uncertainty of the causal model. The notion of modelling the uncertainty over models is particularly crucial for causal structure learning since the model could be unidentifiable when given only a finite amount of observational data. In this paper, we introduce a novel method to jointly learn the structure and mechanisms of the causal model using Variational Bayes, which we call Variational Bayes-DAG-GFlowNet (VBG). We extend the method of Bayesian causal structure learning using GFlowNets to learn not only the posterior distribution over the structure, but also the parameters of a linear-Gaussian model. Our results on simulated data suggest that VBG is competitive against several baselines in modelling the posterior over DAGs an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#26367;&#20195;&#21697;&#25512;&#33616;&#36866;&#24212;&#21040;&#35821;&#35328;&#21305;&#37197;&#38382;&#39064;&#20013;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#36716;&#25442;&#26041;&#27861;&#21435;&#38500;&#20449;&#21495;&#22122;&#38899;&#65292;&#24182;&#32771;&#34385;&#20102;&#22810;&#35821;&#35328;&#25903;&#25345;&#12290;&#35813;&#27169;&#22411;&#24050;&#25104;&#21151;&#22312;&#19968;&#20010;&#22823;&#22411;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#19978;&#30340;11&#20010;&#24066;&#22330;&#21644;6&#31181;&#35821;&#35328;&#20013;&#37096;&#32626;&#65292;&#25552;&#39640;&#20102;&#39038;&#23458;&#24544;&#35802;&#24230;&#21644;&#36141;&#20080;&#29575;&#12290;</title><link>http://arxiv.org/abs/2211.02533</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26367;&#20195;&#21697;&#25512;&#33616;&#27169;&#22411;&#65292;&#34701;&#21512;&#20102;&#24369;&#30417;&#30563;&#30340;&#39038;&#23458;&#34892;&#20026;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
A Transformer-Based Substitute Recommendation Model Incorporating Weakly Supervised Customer Behavior Data. (arXiv:2211.02533v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#26367;&#20195;&#21697;&#25512;&#33616;&#36866;&#24212;&#21040;&#35821;&#35328;&#21305;&#37197;&#38382;&#39064;&#20013;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#36716;&#25442;&#26041;&#27861;&#21435;&#38500;&#20449;&#21495;&#22122;&#38899;&#65292;&#24182;&#32771;&#34385;&#20102;&#22810;&#35821;&#35328;&#25903;&#25345;&#12290;&#35813;&#27169;&#22411;&#24050;&#25104;&#21151;&#22312;&#19968;&#20010;&#22823;&#22411;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#19978;&#30340;11&#20010;&#24066;&#22330;&#21644;6&#31181;&#35821;&#35328;&#20013;&#37096;&#32626;&#65292;&#25552;&#39640;&#20102;&#39038;&#23458;&#24544;&#35802;&#24230;&#21644;&#36141;&#20080;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26367;&#20195;&#21697;&#30340;&#25512;&#33616;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#26367;&#20195;&#21697;&#32473;&#39038;&#23458;&#12290;&#20294;&#26159;&#29616;&#26377;&#30740;&#31350;&#36890;&#24120;&#20351;&#29992;&#39038;&#23458;&#30340;&#34892;&#20026;&#20449;&#21495;&#65288;&#22914;&#20849;&#21516;&#27983;&#35272;&#21644;&#27983;&#35272;&#20294;&#36141;&#20080;&#21478;&#19968;&#20010;&#20135;&#21697;&#65289;&#26469;&#25429;&#25417;&#26367;&#20195;&#20851;&#31995;&#12290;&#23613;&#31649;&#36825;&#20010;&#26041;&#27861;&#21548;&#36215;&#26469;&#24456;&#30452;&#35266;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#20570;&#27861;&#21487;&#33021;&#20250;&#24573;&#30053;&#20135;&#21697;&#30340;&#21151;&#33021;&#21644;&#29305;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20197;&#20135;&#21697;&#26631;&#39064;&#25551;&#36848;&#20316;&#20026;&#27169;&#22411;&#36755;&#20837;&#65292;&#24182;&#32771;&#34385;&#20135;&#21697;&#21151;&#33021;&#65292;&#23558;&#26367;&#20195;&#21697;&#25512;&#33616;&#36866;&#24212;&#21040;&#35821;&#35328;&#21305;&#37197;&#38382;&#39064;&#20013;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#36716;&#25442;&#26041;&#27861;&#26469;&#21435;&#38500;&#20174;&#29983;&#20135;&#25968;&#25454;&#20013;&#24471;&#20986;&#30340;&#20449;&#21495;&#22122;&#22768;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#24037;&#31243;&#35282;&#24230;&#32771;&#34385;&#22810;&#35821;&#35328;&#25903;&#25345;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31471;&#21040;&#31471;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#23454;&#39564;&#20013;&#22343;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#24050;&#37096;&#32626;&#22312;&#19968;&#20010;&#22823;&#22411;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#19978;&#30340;11&#20010;&#24066;&#22330;&#21644;6&#31181;&#35821;&#35328;&#20013;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#39038;&#23458;&#24544;&#35802;&#24230;&#21644;&#36141;&#20080;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The substitute-based recommendation is widely used in E-commerce to provide better alternatives to customers. However, existing research typically uses the customer behavior signals like co-view and view-but-purchase-another to capture the substitute relationship. Despite its intuitive soundness, we find that such an approach might ignore the functionality and characteristics of products. In this paper, we adapt substitute recommendation into language matching problem by taking product title description as model input to consider product functionality. We design a new transformation method to de-noise the signals derived from production data. In addition, we consider multilingual support from the engineering point of view. Our proposed end-to-end transformer-based model achieves both successes from offline and online experiments. The proposed model has been deployed in a large-scale E-commerce website for 11 marketplaces in 6 languages. Our proposed model is demonstrated to increase re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2210.14891</link><description>&lt;p&gt;
&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Broken Neural Scaling Laws. (arXiv:2210.14891v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a smoothly broken power law functional form (referred to as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks for various architectures and a large and diverse set of tasks, including vision, language, audio, video, generative modeling, contrastive learning, robotics, uncertainty estimation/calibration, adversarial robustness, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforcement learning.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#65288;BNSL&#65289;&#65289;&#65292;&#23427;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65288;&#21363;&#24863;&#20852;&#36259;&#30340;&#35780;&#20272;&#25351;&#26631;&#38543;&#29992;&#20110;&#35757;&#32451;&#30340;&#35745;&#31639;&#37327;&#12289;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#12289;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#25110;&#19978;&#28216;&#24615;&#33021;&#21464;&#21270;&#32780;&#21464;&#21270;&#65289;&#23545;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#27599;&#20010;&#20219;&#21153;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#25193;&#25955;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#22810;&#27169;&#24577;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;AI&#23545;&#40784;&#12289;&#26426;&#22120;&#20154;&#12289;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#12289;&#25345;&#32493;&#23398;&#20064;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#33976;&#39311;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a smoothly broken power law functional form (referred to by us as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for various architectures and for each of various tasks within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, robotics, out-of-distribution (OOD) generalization, continual learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, distillation, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#24403;&#28145;&#24230;&#20026;2&#30340;&#31070;&#32463;&#32593;&#32476;&#37319;&#29992;&#36275;&#22815;&#24179;&#28369;&#20984;&#30340;&#28608;&#27963;&#20989;&#25968;&#26102;&#65292;SGD&#21487;&#20197;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#23567;&#20540;&#65292;&#35777;&#26126;&#36807;&#31243;&#20013;&#24341;&#20837;Frobenius&#33539;&#25968;&#27491;&#21017;&#21270;&#19982;&#24688;&#24403;&#20998;&#24067;&#30340;&#21442;&#25968;&#21021;&#22987;&#21270;&#65292;&#21516;&#26102;&#25299;&#23637;&#20102;&#36830;&#32493;&#26102;&#38388;&#30340;&#25910;&#25947;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.11452</link><description>&lt;p&gt;
&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#19978;SGD&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Global Convergence of SGD On Two Layer Neural Nets. (arXiv:2210.11452v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11452
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#24403;&#28145;&#24230;&#20026;2&#30340;&#31070;&#32463;&#32593;&#32476;&#37319;&#29992;&#36275;&#22815;&#24179;&#28369;&#20984;&#30340;&#28608;&#27963;&#20989;&#25968;&#26102;&#65292;SGD&#21487;&#20197;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#23567;&#20540;&#65292;&#35777;&#26126;&#36807;&#31243;&#20013;&#24341;&#20837;Frobenius&#33539;&#25968;&#27491;&#21017;&#21270;&#19982;&#24688;&#24403;&#20998;&#24067;&#30340;&#21442;&#25968;&#21021;&#22987;&#21270;&#65292;&#21516;&#26102;&#25299;&#23637;&#20102;&#36830;&#32493;&#26102;&#38388;&#30340;&#25910;&#25947;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#28145;&#24230;&#20026;2&#30340;&#32593;&#32476;&#37319;&#29992;&#36275;&#22815;&#24179;&#28369;&#19988;&#26377;&#36793;&#30028;&#30340;&#28608;&#27963;&#20989;&#25968;&#65288;&#27604;&#22914;sigmoid&#21644;tanh&#65289;&#26102;&#65292;SGD&#21487;&#20197;&#35777;&#26126;&#24615;&#22320;&#25910;&#25947;&#21040;&#36866;&#24403;&#27491;&#21017;&#21270;&#30340;$\ell_2-$&#32463;&#39564;&#39118;&#38505;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;--&#23545;&#20110;&#20219;&#24847;&#25968;&#25454;&#21644;&#20219;&#24847;&#25968;&#37327;&#30340;&#38376;&#12290;&#25105;&#20204;&#22312;[1]&#30340;&#30740;&#31350;&#25104;&#26524;&#19978;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#24182;&#22312;&#26435;&#37325;&#19978;&#28155;&#21152;&#20102;&#24658;&#23450;&#37327;&#30340;Frobenius&#33539;&#25968;&#27491;&#21017;&#21270;&#65292;&#21516;&#26102;&#36873;&#21462;&#20102;&#24688;&#24403;&#30340;&#20998;&#24067;&#23545;&#21021;&#22987;&#26435;&#37325;&#36827;&#34892;&#37319;&#26679;&#12290;&#25105;&#20204;&#36824;&#32473;&#20986;&#20102;&#19968;&#20010;&#36830;&#32493;&#26102;&#38388;&#30340;SGD&#25910;&#25947;&#32467;&#26524;&#65292;&#21516;&#26679;&#36866;&#29992;&#20110;&#22914;SoftPlus&#36825;&#26679;&#30340;&#24179;&#28369;&#26080;&#36793;&#30028;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24819;&#27861;&#26159;&#23637;&#31034;&#20102;&#23384;&#22312;&#20110;&#22266;&#23450;&#22823;&#23567;&#30340;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23427;&#20204;&#26159;&#8220;Villani&#20989;&#25968;&#8221;[1] Bin Shi, Weijie J. Su, and Michael I. Jordan. On learning rates and schr\"odinger operators, 2020. arXiv:2004.06977
&lt;/p&gt;
&lt;p&gt;
In this note we demonstrate provable convergence of SGD to the global minima of appropriately regularized $\ell_2-$empirical risk of depth $2$ nets -- for arbitrary data and with any number of gates, if they are using adequately smooth and bounded activations like sigmoid and tanh. We build on the results in [1] and leverage a constant amount of Frobenius norm regularization on the weights, along with sampling of the initial weights from an appropriate distribution. We also give a continuous time SGD convergence result that also applies to smooth unbounded activations like SoftPlus. Our key idea is to show the existence loss functions on constant sized neural nets which are "Villani Functions". [1] Bin Shi, Weijie J. Su, and Michael I. Jordan. On learning rates and schr\"odinger operators, 2020. arXiv:2004.06977
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#35757;&#32451;&#24320;&#22987;&#23601;&#36827;&#34892;&#20266;&#26631;&#35760;&#30340;ASR&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#25511;&#21046;&#20266;&#26631;&#31614;&#29983;&#25104;&#26469;&#20943;&#23569;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#30340;&#36807;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2210.08711</link><description>&lt;p&gt;
&#20174;&#19968;&#24320;&#22987;&#23601;&#36827;&#34892;&#36830;&#32493;&#20266;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;
Continuous Pseudo-Labeling from the Start. (arXiv:2210.08711v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#35757;&#32451;&#24320;&#22987;&#23601;&#36827;&#34892;&#20266;&#26631;&#35760;&#30340;ASR&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#25511;&#21046;&#20266;&#26631;&#31614;&#29983;&#25104;&#26469;&#20943;&#23569;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#30340;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#35757;&#32451;&#25110;&#20266;&#26631;&#35760;&#30001;&#20110;&#20854;&#22312;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#26041;&#38754;&#30340;&#25104;&#21151;&#32780;&#24341;&#36215;&#20102;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31038;&#21306;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;ASR&#20013;&#36890;&#36807;&#21160;&#24577;&#25511;&#21046;&#20266;&#26631;&#31614;&#29983;&#25104;&#26469;&#23454;&#29616;&#20174;&#35757;&#32451;&#24320;&#22987;&#23601;&#36827;&#34892;&#20266;&#26631;&#35760;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#30340;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-training (ST), or pseudo-labeling has sparked significant interest in the automatic speech recognition (ASR) community recently because of its success in harnessing unlabeled data. Unlike prior semi-supervised learning approaches that relied on iteratively regenerating pseudo-labels (PLs) from a trained model and using them to train a new model, recent state-of-the-art methods perform `continuous training' where PLs are generated using a very recent version of the model being trained. Nevertheless, these approaches still rely on bootstrapping the ST using an initial supervised learning phase where the model is trained on labeled data alone. We believe this has the potential for over-fitting to the labeled dataset in low resource settings and that ST from the start of training should reduce over-fitting. In this paper we show how we can do this by dynamically controlling the evolution of PLs during the training process in ASR. To the best of our knowledge, this is the first study t
&lt;/p&gt;</description></item><item><title>&#22312;&#21160;&#24577;&#29615;&#22659;&#19979;&#65292;&#35774;&#35745;&#19968;&#20010;&#38271;&#26399;&#20844;&#24179;&#30340;&#20998;&#31867;&#22120;&#24456;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#24179;&#24615;&#27010;&#24565;EI&#65292;&#23427;&#30830;&#20445;&#22312;&#25298;&#32477;&#30340;&#26679;&#26412;&#20184;&#20986;&#19968;&#23450;&#30340;&#21162;&#21147;&#36827;&#34892;&#29305;&#24449;&#25913;&#36827;&#21518;&#65292;&#19981;&#21516;&#32676;&#20307;&#30340;&#26679;&#26412;&#29305;&#24449;&#20998;&#24067;&#24471;&#21040;&#24179;&#34913;&#65292;&#36825;&#27604;&#29616;&#26377;&#30340;&#20844;&#24179;&#24615;&#27010;&#24565;&#23454;&#29616;long-term fairness&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2210.06732</link><description>&lt;p&gt;
&#24179;&#31561;&#25913;&#21892;: &#32771;&#34385;&#38271;&#26399;&#24433;&#21709;&#30340;&#26032;&#20844;&#24179;&#24615;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Equal Improvability: A New Fairness Notion Considering the Long-term Impact. (arXiv:2210.06732v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06732
&lt;/p&gt;
&lt;p&gt;
&#22312;&#21160;&#24577;&#29615;&#22659;&#19979;&#65292;&#35774;&#35745;&#19968;&#20010;&#38271;&#26399;&#20844;&#24179;&#30340;&#20998;&#31867;&#22120;&#24456;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#24179;&#24615;&#27010;&#24565;EI&#65292;&#23427;&#30830;&#20445;&#22312;&#25298;&#32477;&#30340;&#26679;&#26412;&#20184;&#20986;&#19968;&#23450;&#30340;&#21162;&#21147;&#36827;&#34892;&#29305;&#24449;&#25913;&#36827;&#21518;&#65292;&#19981;&#21516;&#32676;&#20307;&#30340;&#26679;&#26412;&#29305;&#24449;&#20998;&#24067;&#24471;&#21040;&#24179;&#34913;&#65292;&#36825;&#27604;&#29616;&#26377;&#30340;&#20844;&#24179;&#24615;&#27010;&#24565;&#23454;&#29616;long-term fairness&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#35774;&#35745;&#19968;&#20010;&#20844;&#24179;&#30340;&#20998;&#31867;&#22120;&#20197;&#36991;&#20813;&#27495;&#35270;&#19981;&#21516;&#32676;&#20307;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#23450;&#20041;&#32676;&#20307;&#20844;&#24179;&#30340;&#26041;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#21482;&#20851;&#27880;&#20102;&#31435;&#21363;&#20844;&#24179;&#24615;&#65292;&#24573;&#30053;&#20102;&#20855;&#26377;&#21160;&#24577;&#22330;&#26223;&#30340;&#20844;&#24179;&#20998;&#31867;&#22120;&#30340;&#38271;&#26399;&#24433;&#21709;&#65292;&#20854;&#20013;&#27599;&#20010;&#20010;&#20307;&#37117;&#21487;&#20197;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#25913;&#21892;&#20854;&#29305;&#24449;&#12290;&#22312;&#36825;&#31181;&#21160;&#24577;&#35774;&#32622;&#20013;&#65292;&#38271;&#26399;&#20844;&#24179;&#24212;&#35813;&#22312;&#25298;&#32477;&#30340;&#26679;&#26412;&#33457;&#36153;&#19968;&#23450;&#30340;&#21162;&#21147;&#20197;&#25913;&#36827;&#21518;&#65292;&#24179;&#34913;&#19981;&#21516;&#32676;&#20307;&#26679;&#26412;&#30340;&#29305;&#24449;&#20998;&#24067;&#12290;&#20026;&#20102;&#25552;&#39640;&#38271;&#26399;&#20844;&#24179;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24179;&#24615;&#27010;&#24565;&#65292;&#31216;&#20026;Equal Improvability (EI)&#65292;&#20854;&#31561;&#21270;&#19981;&#21516;&#32676;&#20307;&#34987;&#25298;&#32477;&#26679;&#26412;&#22312;&#20551;&#23450;&#25152;&#26377;&#26679;&#26412;&#37117;&#23558;&#33457;&#36153;&#26377;&#38480;&#30340;&#21162;&#21147;&#26469;&#25913;&#36827;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#30340;&#28508;&#22312;&#25509;&#21463;&#29575;&#12290;&#25105;&#20204;&#35777;&#26126;EI&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#32479;&#35745;&#24179;&#31561;&#20844;&#24179;&#27010;&#24565;&#30456;&#20860;&#23481;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#20998;&#31867;&#22120;&#20197;&#23454;&#29616;EI&#30340;&#26041;&#27861;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#38271;&#26399;&#20844;&#24179;&#26041;&#38754;&#65292;&#35757;&#32451;&#22312;EI&#19979;&#30340;&#20998;&#31867;&#22120;&#26126;&#26174;&#20248;&#20110;&#22522;&#32447;&#20844;&#24179;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Devising a fair classifier that does not discriminate against different groups is an important problem in machine learning. Although researchers have proposed various ways of defining group fairness, most of them only focused on the immediate fairness, ignoring the long-term impact of a fair classifier under the dynamic scenario where each individual can improve its feature over time. Such dynamic scenarios happen in real world, e.g., college admission and credit loaning, where each rejected sample makes effort to change its features to get accepted afterwards. In this dynamic setting, the long-term fairness should equalize the samples' feature distribution across different groups after the rejected samples make some effort to improve. In order to promote long-term fairness, we propose a new fairness notion called Equal Improvability (EI), which equalizes the potential acceptance rate of the rejected samples across different groups assuming a bounded level of effort will be spent by ea
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#29289;&#23398;&#21407;&#29702;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#21033;&#29992;&#26377;&#20851;&#28508;&#22312;&#28304;&#39046;&#22495;&#30340;&#20449;&#24687;&#26469;&#25552;&#21462;&#30456;&#20851;&#32852;&#30340;&#28508;&#22312;&#28304;&#65292;&#20174;&#32780;&#26497;&#22823;&#22320;&#25913;&#21892;&#20102;&#23545;&#30456;&#20851;&#28304;&#20998;&#31163;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.04222</link><description>&lt;p&gt;
&#22522;&#20110;&#30456;&#20851;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#30456;&#20851;&#28304;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Correlative Information Maximization Based Biologically Plausible Neural Networks for Correlated Source Separation. (arXiv:2210.04222v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04222
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#29289;&#23398;&#21407;&#29702;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#21033;&#29992;&#26377;&#20851;&#28508;&#22312;&#28304;&#39046;&#22495;&#30340;&#20449;&#24687;&#26469;&#25552;&#21462;&#30456;&#20851;&#32852;&#30340;&#28508;&#22312;&#28304;&#65292;&#20174;&#32780;&#26497;&#22823;&#22320;&#25913;&#21892;&#20102;&#23545;&#30456;&#20851;&#28304;&#20998;&#31163;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#21487;&#20197;&#36731;&#26494;&#22320;&#25552;&#21462;&#21050;&#28608;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#20294;&#23427;&#22914;&#20309;&#22312;&#32593;&#32476;&#23618;&#38754;&#19978;&#20570;&#21040;&#36825;&#19968;&#28857;&#20173;&#28982;&#26410;&#30693;&#12290;&#20808;&#21069;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#22823;&#22810;&#25968;&#23581;&#35797;&#37117;&#25552;&#20986;&#20102;&#23454;&#29616;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#30340;&#38480;&#21046;&#22312;&#20110;&#28508;&#22312;&#21407;&#22240;&#26159;&#30456;&#20114;&#29420;&#31435;&#30340;&#12290;&#22312;&#27492;&#25105;&#20204;&#19981;&#20877;&#36981;&#24490;&#27492;&#38480;&#21046;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#29983;&#29289;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#21033;&#29992;&#26377;&#20851;&#20854;&#39046;&#22495;&#30340;&#20449;&#24687;&#26469;&#25552;&#21462;&#30456;&#20851;&#30340;&#28508;&#22312;&#28304;&#12290;&#25105;&#20204;&#36873;&#21462;&#20174;&#36755;&#20837;&#21040;&#36755;&#20986;&#30340;&#26368;&#22823;&#30456;&#20851;&#20449;&#24687;&#20256;&#36755;&#20316;&#20026;&#20998;&#31163;&#30446;&#26631;&#65292;&#24182;&#32422;&#26463;&#36755;&#20986;&#37327;&#38480;&#21046;&#22312;&#20854;&#39044;&#23450;&#38598;&#21512;&#20869;&#65292;&#20197;&#23548;&#20986;&#27492;&#32593;&#32476;&#12290;&#22312;&#32447;&#24418;&#24335;&#21270;&#27492;&#20248;&#21270;&#38382;&#39064;&#33258;&#28982;&#22320;&#23548;&#33268;&#20855;&#26377;&#26412;&#22320;&#23398;&#20064;&#35268;&#21017;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#26080;&#38480;&#22810;&#31181;&#28304;&#22495;&#36873;&#25321;&#65292;&#24182;&#21487;&#20197;&#28789;&#27963;&#22320;&#23545;&#22797;&#26434;&#30340;&#28508;&#22312;&#32467;&#26500;&#36827;&#34892;&#24314;&#27169;&#12290;&#36873;&#25321;&#21333;&#32431;&#24418;&#25110;&#22810;&#38754;&#20307;&#28304;&#22495;&#30340;&#32467;&#26524;&#26159;&#20855;&#26377;&#20998;&#27573;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#32593;&#32476;&#65292;&#20174;&#32780;&#26497;&#22823;&#22320;&#25913;&#21892;&#20102;&#23545;&#30456;&#20851;&#28304;&#20998;&#31163;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The brain effortlessly extracts latent causes of stimuli, but how it does this at the network level remains unknown. Most prior attempts at this problem proposed neural networks that implement independent component analysis which works under the limitation that latent causes are mutually independent. Here, we relax this limitation and propose a biologically plausible neural network that extracts correlated latent sources by exploiting information about their domains. To derive this network, we choose maximum correlative information transfer from inputs to outputs as the separation objective under the constraint that the outputs are restricted to their presumed sets. The online formulation of this optimization problem naturally leads to neural networks with local learning rules. Our framework incorporates infinitely many source domain choices and flexibly models complex latent structures. Choices of simplex or polytopic source domains result in networks with piecewise-linear activation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24369;&#30417;&#30563;&#20449;&#24687;&#30340;&#26631;&#31614;&#20256;&#25773;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#19978;&#30340;&#27010;&#29575;&#20551;&#35774;&#26631;&#31614;&#65292;&#32467;&#21512;&#23616;&#37096;&#20960;&#20309;&#29305;&#24615;&#21644;&#20808;&#39564;&#20449;&#24687;&#30340;&#36136;&#37327;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#35823;&#24046;&#30028;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#21512;&#24182;&#22810;&#20010;&#22122;&#22768;&#20449;&#24687;&#28304;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#24369;&#30417;&#30563;&#20998;&#31867;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#26041;&#27861;&#30340;&#33021;&#21147;&#65292;&#26174;&#31034;&#20986;&#23545;&#29616;&#26377;&#21322;&#30417;&#30563;&#21644;&#24369;&#30417;&#30563;&#26041;&#27861;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2210.03594</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#19979;&#30340;&#26631;&#31614;&#20256;&#25773;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Label Propagation with Weak Supervision. (arXiv:2210.03594v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24369;&#30417;&#30563;&#20449;&#24687;&#30340;&#26631;&#31614;&#20256;&#25773;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#19978;&#30340;&#27010;&#29575;&#20551;&#35774;&#26631;&#31614;&#65292;&#32467;&#21512;&#23616;&#37096;&#20960;&#20309;&#29305;&#24615;&#21644;&#20808;&#39564;&#20449;&#24687;&#30340;&#36136;&#37327;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#35823;&#24046;&#30028;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#21512;&#24182;&#22810;&#20010;&#22122;&#22768;&#20449;&#24687;&#28304;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#24369;&#30417;&#30563;&#20998;&#31867;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#26041;&#27861;&#30340;&#33021;&#21147;&#65292;&#26174;&#31034;&#20986;&#23545;&#29616;&#26377;&#21322;&#30417;&#30563;&#21644;&#24369;&#30417;&#30563;&#26041;&#27861;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a label propagation algorithm that utilizes weak supervision information, specifically probabilistic hypothesized labels on the unlabeled data, and provides an error bound that exploits both the local geometric properties of the underlying graph and the quality of the prior information. The approach is demonstrated on multiple benchmark weakly supervised classification tasks, showing improvements upon existing semi-supervised and weakly supervised methods.
&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#24369;&#30417;&#30563;&#23398;&#20064;&#26159;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#26088;&#22312;&#20943;&#23569;&#26631;&#35760;&#25968;&#25454;&#38656;&#27714;&#30340;&#37325;&#35201;&#33539;&#24335;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#32463;&#20856;&#26631;&#31614;&#20256;&#25773;&#31639;&#27861;&#65288;LPA&#65289;&#65288;Zhu&#65286;Ghahramani&#65292;2002&#65289;&#30340;&#20998;&#26512;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20102;&#26377;&#29992;&#30340;&#20808;&#39564;&#20449;&#24687;&#65292;&#29305;&#21035;&#26159;&#26410;&#26631;&#35760;&#25968;&#25454;&#19978;&#30340;&#27010;&#29575;&#20551;&#35774;&#26631;&#31614;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#35823;&#24046;&#30028;&#65292;&#21033;&#29992;&#20102;&#24213;&#23618;&#22270;&#24418;&#30340;&#23616;&#37096;&#20960;&#20309;&#29305;&#24615;&#21644;&#20808;&#39564;&#20449;&#24687;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#21512;&#24182;&#22810;&#20010;&#22122;&#22768;&#20449;&#24687;&#28304;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#24369;&#30417;&#30563;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#25105;&#20204;&#30340;&#20449;&#24687;&#26469;&#28304;&#26159;&#24369;&#26631;&#35760;&#32773;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#24369;&#30417;&#30563;&#20998;&#31867;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#33021;&#21147;&#65292;&#26174;&#31034;&#20986;&#23545;&#29616;&#26377;&#21322;&#30417;&#30563;&#21644;&#24369;&#30417;&#30563;&#26041;&#27861;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning and weakly supervised learning are important paradigms that aim to reduce the growing demand for labeled data in current machine learning applications. In this paper, we introduce a novel analysis of the classical label propagation algorithm (LPA) (Zhu &amp; Ghahramani, 2002) that moreover takes advantage of useful prior information, specifically probabilistic hypothesized labels on the unlabeled data. We provide an error bound that exploits both the local geometric properties of the underlying graph and the quality of the prior information. We also propose a framework to incorporate multiple sources of noisy information. In particular, we consider the setting of weak supervision, where our sources of information are weak labelers. We demonstrate the ability of our approach on multiple benchmark weakly supervised classification tasks, showing improvements upon existing semi-supervised and weakly supervised methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#29305;&#24449;&#31526;&#21512;&#39044;&#27979;&#30340;&#39044;&#27979;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#34920;&#31034;&#23398;&#20064;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#25193;&#23637;&#20102;&#31526;&#21512;&#39044;&#27979;&#21040;&#35821;&#20041;&#29305;&#24449;&#31354;&#38388;&#12290;&#20174;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#26469;&#30475;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#24120;&#35268;&#31526;&#21512;&#39044;&#27979;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#20219;&#21153;&#19978;&#23637;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.00173</link><description>&lt;p&gt;
&#22522;&#20110;&#29305;&#24449;&#31526;&#21512;&#39044;&#27979;&#30340;&#39044;&#27979;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Predictive Inference with Feature Conformal Prediction. (arXiv:2210.00173v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#29305;&#24449;&#31526;&#21512;&#39044;&#27979;&#30340;&#39044;&#27979;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#34920;&#31034;&#23398;&#20064;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#25193;&#23637;&#20102;&#31526;&#21512;&#39044;&#27979;&#21040;&#35821;&#20041;&#29305;&#24449;&#31354;&#38388;&#12290;&#20174;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#26469;&#30475;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#24120;&#35268;&#31526;&#21512;&#39044;&#27979;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#20219;&#21153;&#19978;&#23637;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21512;&#39044;&#27979;&#26159;&#19968;&#31181;&#26080;&#20998;&#24067;&#25216;&#26415;&#65292;&#29992;&#20110;&#24314;&#31435;&#26377;&#25928;&#30340;&#39044;&#27979;&#38388;&#38548;&#12290;&#34429;&#28982;&#20256;&#32479;&#19978;&#20154;&#20204;&#22312;&#36755;&#20986;&#31354;&#38388;&#20013;&#36827;&#34892;&#31526;&#21512;&#39044;&#27979;&#65292;&#20294;&#36825;&#24182;&#19981;&#26159;&#21807;&#19968;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#29305;&#24449;&#30340;&#31526;&#21512;&#39044;&#27979;&#65292;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#34920;&#31034;&#23398;&#20064;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#25193;&#23637;&#20102;&#31526;&#21512;&#39044;&#27979;&#23545;&#35821;&#20041;&#29305;&#24449;&#31354;&#38388;&#30340;&#33539;&#22260;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;&#29305;&#24449;&#30340;&#31526;&#21512;&#39044;&#27979;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#21487;&#20197;&#35777;&#26126;&#20248;&#20110;&#24120;&#35268;&#31526;&#21512;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#19982;&#26222;&#36890;&#31526;&#21512;&#39044;&#27979;&#32467;&#21512;&#20351;&#29992;&#65292;&#32780;&#19988;&#21487;&#20197;&#19982;&#20854;&#20182;&#33258;&#36866;&#24212;&#31526;&#21512;&#39044;&#27979;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#12290;&#38500;&#20102;&#29616;&#26377;&#39044;&#27979;&#25512;&#26029;&#22522;&#20934;&#27979;&#35797;&#30340;&#23454;&#39564;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#20219;&#21153;&#65288;&#22914;ImageNet&#20998;&#31867;&#21644;Cityscapes&#22270;&#20687;&#20998;&#21106;&#65289;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conformal prediction is a distribution-free technique for establishing valid prediction intervals. Although conventionally people conduct conformal prediction in the output space, this is not the only possibility. In this paper, we propose feature conformal prediction, which extends the scope of conformal prediction to semantic feature spaces by leveraging the inductive bias of deep representation learning. From a theoretical perspective, we demonstrate that feature conformal prediction provably outperforms regular conformal prediction under mild assumptions. Our approach could be combined with not only vanilla conformal prediction, but also other adaptive conformal prediction methods. Apart from experiments on existing predictive inference benchmarks, we also demonstrate the state-of-the-art performance of the proposed methods on large-scale tasks such as ImageNet classification and Cityscapes image segmentation.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;GNN&#21152;&#36895;&#35757;&#32451;&#26041;&#27861; - MLPInit, &#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#31561;&#25928;MLP&#65288;PeerMLP&#65289;, &#24182;&#20351;&#29992;&#23427;&#30340;&#26435;&#37325;&#26469;&#21021;&#22987;&#21270;&#30446;&#26631;GNN&#65292;&#22312;&#19981;&#25439;&#22833;&#31934;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;GNN&#30340;&#25910;&#25947;&#25928;&#26524;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.00102</link><description>&lt;p&gt;
MLPInit: &#20351;&#29992;MLP&#21021;&#22987;&#21270;&#38750;&#24120;&#31616;&#21333;&#30340;GNN&#35757;&#32451;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
MLPInit: Embarrassingly Simple GNN Training Acceleration with MLP Initialization. (arXiv:2210.00102v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00102
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;GNN&#21152;&#36895;&#35757;&#32451;&#26041;&#27861; - MLPInit, &#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#31561;&#25928;MLP&#65288;PeerMLP&#65289;, &#24182;&#20351;&#29992;&#23427;&#30340;&#26435;&#37325;&#26469;&#21021;&#22987;&#21270;&#30446;&#26631;GNN&#65292;&#22312;&#19981;&#25439;&#22833;&#31934;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;GNN&#30340;&#25910;&#25947;&#25928;&#26524;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#22270;&#19978;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#38750;&#24120;&#22797;&#26434;&#19988;&#26497;&#20854;&#32791;&#26102;&#65292;&#36825;&#26159;&#30001;&#20110;&#31232;&#30095;&#30697;&#38453;&#20056;&#27861;&#24341;&#36215;&#30340;&#24320;&#38144;&#12290;&#24403;&#20165;&#20351;&#29992;&#33410;&#28857;&#29305;&#24449;&#26469;&#35757;&#32451;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#26102;&#65292;&#21487;&#20197;&#36991;&#20813;&#36825;&#20123;&#24320;&#38144;&#12290;MLP&#36890;&#36807;&#24573;&#30053;&#22270;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#23545;&#20110;&#22270;&#24418;&#25968;&#25454;&#32780;&#35328;&#26356;&#21152;&#31616;&#21333;&#21644;&#24555;&#36895;&#65292;&#20294;&#36890;&#24120;&#20250;&#29306;&#29298;&#39044;&#27979;&#31934;&#24230;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#22270;&#24418;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23545;&#20110;&#22823;&#22810;&#25968;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#30340;GNN&#65292;&#25105;&#20204;&#21487;&#20197;&#36731;&#26494;&#22320;&#25512;&#23548;&#20986;&#19968;&#20010;&#31561;&#25928;&#30340;&#27169;&#25311;MLP&#65288;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;PeerMLP&#65289;&#24182;&#35774;&#32622;&#21487;&#35757;&#32451;&#30340;&#21442;&#25968;&#20855;&#26377;&#30456;&#21516;&#30340;&#24418;&#29366;&#65292;&#36825;&#20351;&#25105;&#20204;&#22909;&#22855;&#8220;&#20351;&#29992;&#20174;&#23436;&#20840;&#35757;&#32451;&#30340;PeerMLP&#23548;&#20986;&#30340;&#26435;&#37325;&#30340;GNN&#30340;&#34920;&#29616;&#22914;&#20309;&#65311;&#8221;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#36825;&#20123;&#26435;&#37325;&#21021;&#22987;&#21270;&#30340;GNN&#26126;&#26174;&#20248;&#20110;&#23427;&#20204;&#30340;PeerMLP&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#20351;&#29992;PeerMLP&#35757;&#32451;&#20316;&#20026;GNN&#35757;&#32451;&#30340;&#21069;&#23548;&#21021;&#22987;&#21270;&#27493;&#39588;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#24120;&#31616;&#21333;&#20294;&#38750;&#24120;&#26377;&#25928;&#30340;MLP&#21021;&#22987;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;MLPInit&#12290; MLPInit&#21253;&#25324;&#22312;&#19982;&#30446;&#26631;GNN&#30456;&#21516;&#25968;&#25454;&#19978;&#35757;&#32451;PeerMLP&#65292;&#28982;&#21518;&#20351;&#29992;&#20854;&#26435;&#37325;&#21021;&#22987;&#21270;&#30446;&#26631;GNN&#12290;&#20174;&#32463;&#39564;&#19978;&#30475;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;MLPInit&#26377;&#25928;&#22320;&#23558;&#26377;&#29992;&#20449;&#24687;&#20174;PeerMLP&#20256;&#36755;&#21040;&#30446;&#26631;GNN&#65292;&#26174;&#30528;&#25552;&#39640;&#20102;GNN&#30340;&#25910;&#25947;&#25928;&#26524;&#21644;&#39044;&#27979;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;MLP&#30340;&#31616;&#21333;&#21644;&#36895;&#24230;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training graph neural networks (GNNs) on large graphs is complex and extremely time consuming. This is attributed to overheads caused by sparse matrix multiplication, which are sidestepped when training multi-layer perceptrons (MLPs) with only node features. MLPs, by ignoring graph context, are simple and faster for graph data, however they usually sacrifice prediction accuracy, limiting their applications for graph data. We observe that for most message passing-based GNNs, we can trivially derive an analog MLP (we call this a PeerMLP) with an equivalent weight space, by setting the trainable parameters with the same shapes, making us curious about \textbf{\emph{how do GNNs using weights from a fully trained PeerMLP perform?}} Surprisingly, we find that GNNs initialized with such weights significantly outperform their PeerMLPs, motivating us to use PeerMLP training as a precursor, initialization step to GNN training. To this end, we propose an embarrassingly simple, yet hugely effectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#22312;&#25554;&#20540;&#35757;&#32451;&#25968;&#25454;&#26102;&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#21457;&#29616;&#20854;&#25439;&#22833;&#38160;&#24230;&#36981;&#24490;&#38750;&#20809;&#28369;&#30340;&#20108;&#27425;&#26354;&#32447;&#65292;&#24403;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#24230;&#36880;&#28176;&#22686;&#21152;&#26102;&#65292;&#27979;&#35797;&#35823;&#24046;&#20250;&#20808;&#38477;&#21518;&#21319;&#65288;&#21363;&#8220;&#21452;&#19979;&#38477;&#8221;&#29616;&#35937;&#65289;&#12290;</title><link>http://arxiv.org/abs/2209.10080</link><description>&lt;p&gt;
&#22522;&#20110;&#24179;&#28369;&#25554;&#20540;&#30340;&#28145;&#24230;&#21452;&#37325;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
Deep Double Descent via Smooth Interpolation. (arXiv:2209.10080v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#22312;&#25554;&#20540;&#35757;&#32451;&#25968;&#25454;&#26102;&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#21457;&#29616;&#20854;&#25439;&#22833;&#38160;&#24230;&#36981;&#24490;&#38750;&#20809;&#28369;&#30340;&#20108;&#27425;&#26354;&#32447;&#65292;&#24403;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#24230;&#36880;&#28176;&#22686;&#21152;&#26102;&#65292;&#27979;&#35797;&#35823;&#24046;&#20250;&#20808;&#38477;&#21518;&#21319;&#65288;&#21363;&#8220;&#21452;&#19979;&#38477;&#8221;&#29616;&#35937;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#34920;&#26126;&#65292;&#36229;&#21442;&#25968;&#21270;&#28145;&#24230;&#32593;&#32476;&#20855;&#26377;&#25554;&#20540;&#22122;&#22768;&#25968;&#25454;&#21644;&#34920;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#30340;&#33021;&#21147;&#65292;&#36825;&#31181;&#29616;&#35937;&#36890;&#36807;&#27979;&#35797;&#35823;&#24046;&#30340;&#21452;&#37325;&#19979;&#38477;&#26354;&#32447;&#24471;&#21040;&#20102;&#34920;&#24449;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#28145;&#24230;&#32593;&#32476;&#25554;&#20540;&#21644;&#27867;&#21270;&#20043;&#38388;&#30340;&#31934;&#30830;&#20851;&#31995;&#36824;&#27809;&#26377;&#24471;&#21040;&#26126;&#30830;&#30340;&#23450;&#37327;&#25551;&#36848;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;&#25554;&#20540;&#35757;&#32451;&#25968;&#25454;&#26102;&#19982;&#27599;&#20010;&#35757;&#32451;&#28857;&#21608;&#22260;&#30340;&#36755;&#20837;&#21464;&#37327;&#30456;&#20851;&#32852;&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#23450;&#37327;&#34913;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#25311;&#21512;&#38160;&#24230;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#25439;&#22833;&#38160;&#24230;&#36981;&#24490;&#19968;&#20010;&#38750;&#20809;&#28369;&#30340;&#20108;&#27425;&#26354;&#32447;&#65292;&#36825;&#19982;&#20256;&#32479;&#30340;&#22810;&#39033;&#24335;&#22238;&#24402;&#30340;&#20998;&#26512;&#32467;&#35770;&#26377;&#19968;&#23450;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#24403;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#24230;&#36880;&#28176;&#22686;&#21152;&#26102;&#65292;&#27979;&#35797;&#35823;&#24046;&#20250;&#20808;&#38477;&#21518;&#21319;&#65288;&#21363;&#8220;&#21452;&#19979;&#38477;&#8221;&#29616;&#35937;&#65289;&#65292;&#36825;&#19982;&#20043;&#21069;&#30740;&#31350;&#30340;&#32467;&#35770;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability of overparameterized deep networks to interpolate noisy data, while at the same time showing good generalization performance, has been recently characterized in terms of the double descent curve for the test error. Common intuition from polynomial regression suggests that overparameterized networks are able to sharply interpolate noisy data, without considerably deviating from the ground-truth signal, thus preserving generalization ability. At present, a precise characterization of the relationship between interpolation and generalization for deep networks is missing. In this work, we quantify sharpness of fit of the training data interpolated by neural network functions, by studying the loss landscape w.r.t. to the input variable locally to each training point, over volumes around cleanly- and noisily-labelled training samples, as we systematically increase the number of model parameters and training epochs. Our findings show that loss sharpness in the input space follows 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TSFool&#30340;&#40657;&#30418;&#26041;&#27861;, &#21487;&#20197;&#26377;&#25928;&#22320;&#29983;&#25104;&#38024;&#23545;RNN&#20998;&#31867;&#22120;&#30340;&#39640;&#24230;&#38590;&#20197;&#23519;&#35273;&#30340;&#23545;&#25239;&#24615;&#26102;&#38388;&#24207;&#21015;&#65292;&#22312;&#32771;&#34385;&#23545;&#25239;&#26679;&#26412;&#38590;&#20197;&#23519;&#35273;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#23545;&#25239;&#24615;&#25915;&#20987;&#25913;&#36827;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#26469;&#22686;&#24378;&#25200;&#21160;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2209.06388</link><description>&lt;p&gt;
TSFool: &#36890;&#36807;&#22810;&#30446;&#26631;&#40657;&#30418;&#25915;&#20987;&#26041;&#27861;&#29983;&#25104;&#39640;&#24230;&#38590;&#20197;&#23519;&#35273;&#30340;&#23545;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#24615;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
TSFool: Crafting Highly-imperceptible Adversarial Time Series through Multi-objective Black-box Attack to Fool RNN Classifiers. (arXiv:2209.06388v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TSFool&#30340;&#40657;&#30418;&#26041;&#27861;, &#21487;&#20197;&#26377;&#25928;&#22320;&#29983;&#25104;&#38024;&#23545;RNN&#20998;&#31867;&#22120;&#30340;&#39640;&#24230;&#38590;&#20197;&#23519;&#35273;&#30340;&#23545;&#25239;&#24615;&#26102;&#38388;&#24207;&#21015;&#65292;&#22312;&#32771;&#34385;&#23545;&#25239;&#26679;&#26412;&#38590;&#20197;&#23519;&#35273;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#23545;&#25239;&#24615;&#25915;&#20987;&#25913;&#36827;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#26469;&#22686;&#24378;&#25200;&#21160;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#24456;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#29616;&#26377;&#30340;&#26799;&#24230;&#25915;&#20987;&#26041;&#27861;&#22312;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21644;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#19979;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#26159;&#22240;&#20026;RNN&#30340;&#24490;&#29615;&#32467;&#26500;&#38459;&#27490;&#20102;&#30452;&#25509;&#30340;&#27169;&#22411;&#24046;&#20998;&#65292;&#32780;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#23545;&#25200;&#21160;&#30340;&#35270;&#35273;&#25935;&#24863;&#24615;&#25361;&#25112;&#20102;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#20256;&#32479;&#23616;&#37096;&#20248;&#21270;&#30446;&#26631;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TSFool&#30340;&#40657;&#30418;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#29983;&#25104;&#38024;&#23545;RNN&#20998;&#31867;&#22120;&#30340;&#39640;&#24230;&#38590;&#20197;&#23519;&#35273;&#30340;&#23545;&#25239;&#24615;&#26102;&#38388;&#24207;&#21015;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20840;&#23616;&#20248;&#21270;&#30446;&#26631;&#65292;&#31216;&#20026;Camouflage Coefficient&#65292;&#20174;&#31867;&#20998;&#24067;&#30340;&#35282;&#24230;&#32771;&#34385;&#23545;&#25239;&#26679;&#26412;&#30340;&#38590;&#20197;&#23519;&#35273;&#24615;&#65292;&#24182;&#30456;&#24212;&#22320;&#23558;&#23545;&#25239;&#24615;&#25915;&#20987;&#25913;&#36827;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#20197;&#22686;&#24378;&#25200;&#21160;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#25670;&#33073;&#19981;&#21516;&#27169;&#22411;&#38388;&#30340;&#36716;&#31227;&#24615;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#29305;&#23450;&#20110;&#27169;&#22411;&#30340;&#22238;&#36991;&#35268;&#21017;&#12290;&#22312;&#20154;&#36896;&#25968;&#25454;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TSFool&#21487;&#20197;&#29983;&#25104;&#39640;&#38590;&#24230;&#25915;&#20987;&#21516;&#26102;&#20445;&#25345;&#23545;&#25239;&#26679;&#26412;&#30340;&#19981;&#26131;&#34987;&#26816;&#27979;&#24615;&#65292;&#24182;&#26377;&#24456;&#39640;&#30340;&#36716;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network (NN) classifiers are vulnerable to adversarial attacks. Although the existing gradient-based attacks achieve state-of-the-art performance in feed-forward NNs and image recognition tasks, they do not perform as well on time series classification with recurrent neural network (RNN) models. This is because the cyclical structure of RNN prevents direct model differentiation and the visual sensitivity of time series data to perturbations challenges the traditional local optimization objective of the adversarial attack. In this paper, a black-box method called TSFool is proposed to efficiently craft highly-imperceptible adversarial time series for RNN classifiers. We propose a novel global optimization objective named Camouflage Coefficient to consider the imperceptibility of adversarial samples from the perspective of class distribution, and accordingly refine the adversarial attack as a multi-objective optimization problem to enhance the perturbation quality. To get rid of t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65288;Reg-DGM&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#38750;&#21487;&#36716;&#31227;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#38477;&#20302;&#26377;&#38480;&#25968;&#25454;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#24046;&#65292;&#22312;&#26497;&#38480;&#24773;&#20917;&#19979;&#30830;&#20445;&#20840;&#23616;&#26368;&#23567;&#28857;&#30340;&#23384;&#22312;&#21644;&#21807;&#19968;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.14133</link><description>&lt;p&gt;
&#26377;&#38480;&#25968;&#25454;&#19979;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#19982;&#38750;&#21487;&#36716;&#31227;&#39044;&#35757;&#32451;&#27169;&#22411;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Deep Generative Modeling on Limited Data with Regularization by Nontransferable Pre-trained Models. (arXiv:2208.14133v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.14133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65288;Reg-DGM&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#38750;&#21487;&#36716;&#31227;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#38477;&#20302;&#26377;&#38480;&#25968;&#25454;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#24046;&#65292;&#22312;&#26497;&#38480;&#24773;&#20917;&#19979;&#30830;&#20445;&#20840;&#23616;&#26368;&#23567;&#28857;&#30340;&#23384;&#22312;&#21644;&#21807;&#19968;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26377;&#38480;&#25968;&#25454;&#19978;&#23398;&#20064;&#22797;&#26434;&#27169;&#22411;&#24456;&#23481;&#26131;&#20986;&#29616;&#22823;&#26041;&#24046;&#21644;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#22240;&#27492;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#25165;&#33021;&#23454;&#29616;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65288;Reg-DGM&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#38750;&#21487;&#36716;&#31227;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#38477;&#20302;&#26377;&#38480;&#25968;&#25454;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#24046;&#12290;Reg-DGM &#20248;&#21270;&#19968;&#23450;&#30340;&#24046;&#24322;&#21644;&#33021;&#37327;&#20989;&#25968;&#26399;&#26395;&#30340;&#21152;&#26435;&#21644;&#65288;&#33021;&#37327;&#20989;&#25968;&#26159;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#23450;&#20041;&#30340;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#39640;&#26031;&#25311;&#21512;&#26469;&#20998;&#26512;&#21152;&#26435;&#36229;&#21442;&#25968;&#22914;&#20309;&#26435;&#34913;&#20559;&#24046;&#21644;&#26041;&#24046;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#22312;&#38750;&#21442;&#25968;&#35774;&#32622;&#19979;&#34920;&#24449;&#20102; Reg-DGM &#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#23384;&#22312;&#24615;&#21644;&#21807;&#19968;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#19982;&#26799;&#24230;&#22522;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models (DGMs) are data-eager because learning a complex model on limited data suffers from a large variance and easily overfits. Inspired by the classical perspective of the bias-variance tradeoff, we propose regularized deep generative model (Reg-DGM), which leverages a nontransferable pre-trained model to reduce the variance of generative modeling with limited data. Formally, Reg-DGM optimizes a weighted sum of a certain divergence and the expectation of an energy function, where the divergence is between the data and the model distributions, and the energy function is defined by the pre-trained model w.r.t. the model distribution. We analyze a simple yet representative Gaussian-fitting case to demonstrate how the weighting hyperparameter trades off the bias and the variance. Theoretically, we characterize the existence and the uniqueness of the global minimum of Reg-DGM in a non-parametric setting and prove its convergence with neural networks trained by gradient-bas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21629;&#21517;&#20026;&#8220;&#35821;&#20041;&#22686;&#24378;&#30340;&#22270;&#20687;&#32858;&#31867;(SIC)&#8221;&#30340;&#26032;&#22411;&#22270;&#20687;&#32858;&#31867;&#26041;&#27861;&#65292;&#22522;&#20110;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;CLIP&#25351;&#23548;&#65292;&#33021;&#22815;&#21306;&#20998;&#35270;&#35273;&#19978;&#30456;&#20284;&#20294;&#22312;&#35821;&#20041;&#19978;&#26377;&#25152;&#19981;&#21516;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2208.09849</link><description>&lt;p&gt;
&#35821;&#20041;&#22686;&#24378;&#30340;&#22270;&#20687;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Semantic-Enhanced Image Clustering. (arXiv:2208.09849v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21629;&#21517;&#20026;&#8220;&#35821;&#20041;&#22686;&#24378;&#30340;&#22270;&#20687;&#32858;&#31867;(SIC)&#8221;&#30340;&#26032;&#22411;&#22270;&#20687;&#32858;&#31867;&#26041;&#27861;&#65292;&#22522;&#20110;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;CLIP&#25351;&#23548;&#65292;&#33021;&#22815;&#21306;&#20998;&#35270;&#35273;&#19978;&#30456;&#20284;&#20294;&#22312;&#35821;&#20041;&#19978;&#26377;&#25152;&#19981;&#21516;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#32858;&#31867;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#25991;&#23383;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22270;&#20687;&#32858;&#31867;&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#21306;&#20998;&#35270;&#35273;&#19978;&#30456;&#20284;&#20294;&#22312;&#35821;&#20041;&#19978;&#26377;&#25152;&#19981;&#21516;&#30340;&#22270;&#20687;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21482;&#30693;&#36947;&#32858;&#31867;&#30340;&#25968;&#37327;&#65292;&#22240;&#27492;&#22914;&#20309;&#23558;&#22270;&#20687;&#26144;&#23556;&#21040;&#36866;&#24403;&#30340;&#35821;&#20041;&#31354;&#38388;&#65292;&#20197;&#21450;&#22914;&#20309;&#20174;&#22270;&#20687;&#21644;&#35821;&#20041;&#31354;&#38388;&#32858;&#31867;&#22270;&#20687;&#26159;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30001;&#35270;&#35273; - &#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;CLIP&#25351;&#23548;&#30340;&#26032;&#22411;&#22270;&#20687;&#32858;&#31867;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;&#8220;&#35821;&#20041;&#22686;&#24378;&#30340;&#22270;&#20687;&#32858;&#31867;(SIC)&#8221;&#12290;&#22312;&#36825;&#31181;&#26032;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#32473;&#23450;&#30340;&#22270;&#20687;&#26144;&#23556;&#21040;&#36866;&#24403;&#30340;&#35821;&#20041;&#31354;&#38388;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image clustering is an important and open-challenging task in computer vision. Although many methods have been proposed to solve the image clustering task, they only explore images and uncover clusters according to the image features, thus being unable to distinguish visually similar but semantically different images. In this paper, we propose to investigate the task of image clustering with the help of a visual-language pre-training model. Different from the zero-shot setting, in which the class names are known, we only know the number of clusters in this setting. Therefore, how to map images to a proper semantic space and how to cluster images from both image and semantic spaces are two key problems. To solve the above problems, we propose a novel image clustering method guided by the visual-language pre-training model CLIP, named \textbf{Semantic-Enhanced Image Clustering (SIC)}. In this new method, we propose a method to map the given images to a proper semantic space first and eff
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#30340;&#22522;&#20110;&#28145;&#24230;&#29305;&#24449;&#30340;&#30149;&#29702;&#22270;&#20687;&#21487;&#21464;&#24418;&#37197;&#20934;&#26694;&#26550;&#65292;&#20351;&#29992;&#25506;&#27979;&#22120;&#21644;&#26080;&#25506;&#27979;&#22120;&#30340;&#28145;&#24230;&#23398;&#20064;&#29305;&#24449;&#32593;&#32476;&#25552;&#21462;&#23494;&#38598;&#30340;&#29305;&#24449;&#28857;&#65292;&#24182;&#36890;&#36807;&#23396;&#31435;&#26862;&#26519;&#32479;&#35745;&#27169;&#22411;&#21644;&#23616;&#37096;&#20223;&#23556;&#26657;&#27491;&#27169;&#22411;&#32467;&#21512;&#30340;&#31163;&#32676;&#20540;&#26816;&#27979;&#26041;&#27861;&#36827;&#19968;&#27493;&#20943;&#23569;&#38169;&#35823;&#21305;&#37197;&#12290;&#22312;ANHIR&#25361;&#25112;&#25968;&#25454;&#38598;&#19978;&#65292;&#26412;&#25991;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#27604;&#20256;&#32479;&#26041;&#27861;&#25552;&#39640;&#20102;17&#65285;&#20197;&#19978;&#30340;A&#24230;&#37327;&#20540;&#12290;</title><link>http://arxiv.org/abs/2208.07655</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#29305;&#24449;&#30340;&#30149;&#29702;&#22270;&#20687;&#21487;&#21464;&#24418;&#37197;&#20934;&#30340;&#28151;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Hybrid Deep Feature-Based Deformable Image Registration Method for Pathology Images. (arXiv:2208.07655v4 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#30340;&#22522;&#20110;&#28145;&#24230;&#29305;&#24449;&#30340;&#30149;&#29702;&#22270;&#20687;&#21487;&#21464;&#24418;&#37197;&#20934;&#26694;&#26550;&#65292;&#20351;&#29992;&#25506;&#27979;&#22120;&#21644;&#26080;&#25506;&#27979;&#22120;&#30340;&#28145;&#24230;&#23398;&#20064;&#29305;&#24449;&#32593;&#32476;&#25552;&#21462;&#23494;&#38598;&#30340;&#29305;&#24449;&#28857;&#65292;&#24182;&#36890;&#36807;&#23396;&#31435;&#26862;&#26519;&#32479;&#35745;&#27169;&#22411;&#21644;&#23616;&#37096;&#20223;&#23556;&#26657;&#27491;&#27169;&#22411;&#32467;&#21512;&#30340;&#31163;&#32676;&#20540;&#26816;&#27979;&#26041;&#27861;&#36827;&#19968;&#27493;&#20943;&#23569;&#38169;&#35823;&#21305;&#37197;&#12290;&#22312;ANHIR&#25361;&#25112;&#25968;&#25454;&#38598;&#19978;&#65292;&#26412;&#25991;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#27604;&#20256;&#32479;&#26041;&#27861;&#25552;&#39640;&#20102;17&#65285;&#20197;&#19978;&#30340;A&#24230;&#37327;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30149;&#29702;&#23398;&#23478;&#38656;&#35201;&#32467;&#21512;&#26469;&#33258;&#19981;&#21516;&#26579;&#33394;&#30340;&#30149;&#29702;&#20999;&#29255;&#30340;&#20449;&#24687;&#36827;&#34892;&#20934;&#30830;&#30340;&#35786;&#26029;&#12290;&#21487;&#21464;&#24418;&#22270;&#20687;&#37197;&#20934;&#26159;&#34701;&#21512;&#22810;&#27169;&#24577;&#30149;&#29702;&#20999;&#29255;&#30340;&#24517;&#35201;&#25216;&#26415;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#30340;&#22522;&#20110;&#28145;&#24230;&#29305;&#24449;&#30340;&#21487;&#21464;&#24418;&#22270;&#20687;&#37197;&#20934;&#26694;&#26550;&#65292;&#29992;&#20110;&#26579;&#33394;&#30149;&#29702;&#26679;&#26412;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#22522;&#20110;&#25506;&#27979;&#22120;&#21644;&#26080;&#25506;&#27979;&#22120;&#30340;&#28145;&#24230;&#23398;&#20064;&#29305;&#24449;&#32593;&#32476;&#25552;&#21462;&#23494;&#38598;&#29305;&#24449;&#28857;&#65292;&#24182;&#36827;&#34892;&#28857;&#21305;&#37197;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#36827;&#19968;&#27493;&#20943;&#23569;&#38169;&#35823;&#21305;&#37197;&#65292;&#25552;&#20986;&#19968;&#31181;&#31163;&#32676;&#20540;&#26816;&#27979;&#26041;&#27861;&#65292;&#23558;&#23396;&#31435;&#26862;&#26519;&#32479;&#35745;&#27169;&#22411;&#21644;&#23616;&#37096;&#20223;&#23556;&#26657;&#27491;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;&#26368;&#21518;&#65292;&#25554;&#20540;&#26041;&#27861;&#22522;&#20110;&#19978;&#36848;&#21305;&#37197;&#28857;&#29983;&#25104;&#30149;&#29702;&#22270;&#20687;&#27880;&#20876;&#30340;&#21487;&#21464;&#24418;&#21521;&#37327;&#22330;&#12290;&#25105;&#20204;&#22312;&#19982;IEEE ISBI 2019&#20250;&#35758;&#32852;&#21512;&#32452;&#32455;&#30340;Non-rigid Histology Image Registration&#65288;ANHIR&#65289;&#25361;&#25112;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#27604;&#20256;&#32479;&#26041;&#27861;&#25552;&#39640;&#20102;17&#65285;&#20197;&#19978;&#30340;A&#24230;&#37327;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pathologists need to combine information from differently stained pathology slices for accurate diagnosis. Deformable image registration is a necessary technique for fusing multi-modal pathology slices. This paper proposes a hybrid deep feature-based deformable image registration framework for stained pathology samples. We first extract dense feature points via the detector-based and detector-free deep learning feature networks and perform points matching. Then, to further reduce false matches, an outlier detection method combining the isolation forest statistical model and the local affine correction model is proposed. Finally, the interpolation method generates the deformable vector field for pathology image registration based on the above matching points. We evaluate our method on the dataset of the Non-rigid Histology Image Registration (ANHIR) challenge, which is co-organized with the IEEE ISBI 2019 conference. Our technique outperforms the traditional approaches by 17% with the A
&lt;/p&gt;</description></item><item><title>&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#22312;&#38899;&#39057;&#39118;&#26684;&#36716;&#25442;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;WORLD&#21512;&#25104;&#22120;&#65292;&#24182;&#36890;&#36807;&#22768;&#23398;&#29305;&#24449;&#21442;&#25968;&#26469;&#23454;&#29616;&#38899;&#39640;&#21644;&#38899;&#33394;&#20449;&#24687;&#30340;&#20998;&#31163;&#12290;</title><link>http://arxiv.org/abs/2208.07282</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#24494;&#20998;WORLD&#21512;&#25104;&#22120;&#30340;&#31070;&#32463;&#22768;&#30721;&#22120;&#21450;&#20854;&#22312;&#31471;&#21040;&#31471;&#38899;&#39057;&#39118;&#26684;&#36716;&#25442;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Differentiable WORLD Synthesizer-based Neural Vocoder With Application To End-To-End Audio Style Transfer. (arXiv:2208.07282v4 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07282
&lt;/p&gt;
&lt;p&gt;
&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#22312;&#38899;&#39057;&#39118;&#26684;&#36716;&#25442;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;WORLD&#21512;&#25104;&#22120;&#65292;&#24182;&#36890;&#36807;&#22768;&#23398;&#29305;&#24449;&#21442;&#25968;&#26469;&#23454;&#29616;&#38899;&#39640;&#21644;&#38899;&#33394;&#20449;&#24687;&#30340;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;WORLD&#21512;&#25104;&#22120;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#31471;&#21040;&#31471;&#38899;&#39057;&#39118;&#26684;&#36716;&#25442;&#20219;&#21153;&#65288;&#22914;&#65288;&#21809;&#65289;&#22768;&#38899;&#36716;&#25442;&#21644;DDSP&#38899;&#33394;&#36716;&#25442;&#20219;&#21153;&#65289;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#22522;&#32447;&#21487;&#24494;&#20998;&#21512;&#25104;&#22120;&#27809;&#26377;&#27169;&#22411;&#21442;&#25968;&#65292;&#20294;&#23427;&#20135;&#29983;&#20102;&#36275;&#22815;&#30340;&#21512;&#25104;&#36136;&#37327;&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#38468;&#21152;&#36731;&#37327;&#32423;&#30340;&#40657;&#31665;&#21518;&#32593;&#32476;&#26469;&#25193;&#23637;&#22522;&#32447;&#21512;&#25104;&#22120;&#65292;&#20197;&#36827;&#19968;&#27493;&#22788;&#29702;&#22522;&#32447;&#36755;&#20986;&#65292;&#20174;&#32780;&#25552;&#39640;&#20445;&#30495;&#24230;&#12290;&#21478;&#19968;&#31181;&#21487;&#24494;&#20998;&#26041;&#27861;&#26159;&#30452;&#25509;&#25552;&#21462;&#28304;&#28608;&#21457;&#35889;&#65292;&#36825;&#21487;&#20197;&#25913;&#21892;&#33258;&#28982;&#24230;&#65292;&#20294;&#20165;&#36866;&#29992;&#20110;&#36739;&#31364;&#30340;&#39118;&#26684;&#36716;&#25442;&#24212;&#29992;&#31867;&#21035;&#12290;&#25105;&#20204;&#26041;&#27861;&#20351;&#29992;&#30340;&#22768;&#23398;&#29305;&#24449;&#21442;&#25968;&#21270;&#20855;&#26377;&#38468;&#21152;&#30340;&#22909;&#22788;&#65292;&#23427;&#33258;&#28982;&#22320;&#23558;&#38899;&#39640;&#21644;&#38899;&#33394;&#20449;&#24687;&#20998;&#31163;&#24320;&#26469;&#65292;&#20197;&#20415;&#23427;&#20204;&#21487;&#20197;&#20998;&#21035;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#26377;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#27861;&#21487;&#20197;&#20174;&#21333;&#22768;&#36947;&#38899;&#39057;&#28304;&#20272;&#35745;&#36825;&#20123;&#22768;&#23398;&#29305;&#24449;&#65292;&#23427;&#20801;&#35768;&#39118;&#26684;&#36716;&#25442;&#20219;&#21153;&#20013;&#30340;&#21442;&#25968;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a differentiable WORLD synthesizer and demonstrate its use in end-to-end audio style transfer tasks such as (singing) voice conversion and the DDSP timbre transfer task. Accordingly, our baseline differentiable synthesizer has no model parameters, yet it yields adequate synthesis quality. We can extend the baseline synthesizer by appending lightweight black-box postnets which apply further processing to the baseline output in order to improve fidelity. An alternative differentiable approach considers extraction of the source excitation spectrum directly, which can improve naturalness albeit for a narrower class of style transfer applications. The acoustic feature parameterization used by our approaches has the added benefit that it naturally disentangles pitch and timbral information so that they can be modeled separately. Moreover, as there exists a robust means of estimating these acoustic features from monophonic audio sources, it allows for parameter loss 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CPD&#30340;&#39640;&#25928;&#30340;&#20223;&#30495;&#21040;&#30495;&#23454;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38543;&#26426;&#21442;&#25968;&#33539;&#22260;&#20998;&#20026;&#20960;&#20010;&#23567;&#30340;&#23376;&#22495;&#65292;&#24182;&#20026;&#27599;&#20010;&#23376;&#22495;&#20998;&#37197;&#19968;&#20010;&#26412;&#22320;&#31574;&#30053;&#65292;&#21152;&#24555;&#20102;&#23398;&#20064;&#36895;&#24230;&#24182;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2207.14561</link><description>&lt;p&gt;
&#24490;&#29615;&#31574;&#30053;&#33976;&#39311;&#65306;&#24102;&#26377;&#22495;&#38543;&#26426;&#21270;&#30340;&#39640;&#25928;&#29575;&#30340;&#20223;&#30495;&#21040;&#30495;&#23454;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cyclic Policy Distillation: Sample-Efficient Sim-to-Real Reinforcement Learning with Domain Randomization. (arXiv:2207.14561v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.14561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CPD&#30340;&#39640;&#25928;&#30340;&#20223;&#30495;&#21040;&#30495;&#23454;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38543;&#26426;&#21442;&#25968;&#33539;&#22260;&#20998;&#20026;&#20960;&#20010;&#23567;&#30340;&#23376;&#22495;&#65292;&#24182;&#20026;&#27599;&#20010;&#23376;&#22495;&#20998;&#37197;&#19968;&#20010;&#26412;&#22320;&#31574;&#30053;&#65292;&#21152;&#24555;&#20102;&#23398;&#20064;&#36895;&#24230;&#24182;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24490;&#29615;&#31574;&#30053;&#33976;&#39311;&#65288;CPD&#65289;&#30340;&#39640;&#25928;&#29575;&#20223;&#30495;&#21040;&#30495;&#23454;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;CPD&#23558;&#38543;&#26426;&#21442;&#25968;&#33539;&#22260;&#20998;&#20026;&#20960;&#20010;&#23567;&#30340;&#23376;&#22495;&#24182;&#20026;&#27599;&#20010;&#23376;&#22495;&#20998;&#37197;&#19968;&#20010;&#26412;&#22320;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#22312;&#24490;&#29615;&#36807;&#28193;&#21040;&#23376;&#22495;&#26102;&#23398;&#20064;&#26412;&#22320;&#31574;&#30053;&#12290;CPD&#36890;&#36807;&#22522;&#20110;&#39044;&#26399;&#24615;&#33021;&#25552;&#39640;&#30340;&#30693;&#35782;&#20256;&#36882;&#21152;&#36895;&#23398;&#20064;&#12290;&#26368;&#21518;&#65292;&#25152;&#26377;&#23398;&#20064;&#21040;&#30340;&#26412;&#22320;&#31574;&#30053;&#37117;&#34987;&#25552;&#28860;&#20026;&#19968;&#20010;&#29992;&#20110;&#20223;&#30495;&#21040;&#30495;&#23454;&#36716;&#31227;&#30340;&#20840;&#23616;&#31574;&#30053;&#12290;&#36890;&#36807;&#22312;OpenAI Gym&#20013;&#36827;&#34892;Pendulum&#12289;Reacher&#12289;HalfCheetah&#21644;Ant&#22235;&#20010;&#20219;&#21153;&#30340;&#20223;&#30495;&#23454;&#39564;&#20197;&#21450;&#23558;&#20854;&#36716;&#31227;&#21040;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#20013;&#65292;&#35777;&#26126;&#20102;CPD&#30340;&#26377;&#25928;&#24615;&#21644;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning with domain randomization learns a control policy in various simulations with randomized physical and sensor model parameters to become transferable to the real world in a zero-shot setting. However, a huge number of samples are often required to learn an effective policy when the range of randomized parameters is extensive due to the instability of policy updates. To alleviate this problem, we propose a sample-efficient method named cyclic policy distillation (CPD). CPD divides the range of randomized parameters into several small sub-domains and assigns a local policy to each one. Then local policies are learned while cyclically transitioning to sub-domains. CPD accelerates learning through knowledge transfer based on expected performance improvements. Finally, all of the learned local policies are distilled into a global policy for sim-to-real transfers. CPD's effectiveness and sample efficiency are demonstrated through simulations with four tasks (Pendul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#24352;&#22270;&#20687;&#30340;&#25345;&#20037;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;&#26041;&#27861;&#65292;&#20351;&#29992;&#26465;&#20214;&#31070;&#32463;&#22320;&#38754;&#35745;&#21010;&#26469;&#34920;&#31034;&#22330;&#26223;&#65292;&#33021;&#22815;&#36827;&#34892;&#35270;&#35282;&#21512;&#25104;&#12289;&#22330;&#26223;&#35299;&#32806;&#34920;&#31034;&#21644;&#21487;&#31227;&#21160;&#32452;&#20214;&#30340;&#20998;&#31163;&#65292;&#37325;&#26500;&#21487;&#31227;&#21160;&#29289;&#20307;&#33021;&#22815;&#36827;&#34892;&#22810;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2207.11232</link><description>&lt;p&gt;
&#31070;&#32463;&#22320;&#38754;&#35745;&#21010;&#65306;&#22522;&#20110;&#21333;&#24352;&#22270;&#29255;&#30340;&#25345;&#20037;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Neural Groundplans: Persistent Neural Scene Representations from a Single Image. (arXiv:2207.11232v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#24352;&#22270;&#20687;&#30340;&#25345;&#20037;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;&#26041;&#27861;&#65292;&#20351;&#29992;&#26465;&#20214;&#31070;&#32463;&#22320;&#38754;&#35745;&#21010;&#26469;&#34920;&#31034;&#22330;&#26223;&#65292;&#33021;&#22815;&#36827;&#34892;&#35270;&#35282;&#21512;&#25104;&#12289;&#22330;&#26223;&#35299;&#32806;&#34920;&#31034;&#21644;&#21487;&#31227;&#21160;&#32452;&#20214;&#30340;&#20998;&#31163;&#65292;&#37325;&#26500;&#21487;&#31227;&#21160;&#29289;&#20307;&#33021;&#22815;&#36827;&#34892;&#22810;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#22330;&#26223;&#30340;2D&#22270;&#20687;&#35266;&#27979;&#26144;&#23556;&#21040;&#25345;&#20037;&#30340;3D&#22330;&#26223;&#34920;&#31034;&#20013;&#65292;&#23454;&#29616;&#20102;&#26032;&#39062;&#30340;&#35270;&#35282;&#21512;&#25104;&#21644;&#22330;&#26223;&#21487;&#31227;&#21160;&#21644;&#19981;&#21487;&#31227;&#21160;&#32452;&#20214;&#30340;&#35299;&#32806;&#34920;&#31034;&#12290;&#21463;&#35270;&#35273;&#21644;&#26426;&#22120;&#20154;&#24120;&#29992;&#30340;&#40479;&#30640;&#22270;&#65288;BEV&#65289;&#34920;&#31034;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26465;&#20214;&#31070;&#32463;&#22320;&#38754;&#35745;&#21010;&#65292;&#21363;&#22320;&#38754;&#23545;&#40784;&#30340;2D&#29305;&#24449;&#32593;&#26684;&#65292;&#20316;&#20026;&#25345;&#20037;&#19988;&#21344;&#29992;&#20869;&#23384;&#23569;&#30340;&#22330;&#26223;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#26080;&#26631;&#31614;&#30340;&#22810;&#35270;&#35282;&#35266;&#27979;&#30340;&#33258;&#25105;&#30417;&#30563;&#35757;&#32451;&#65292;&#20351;&#29992;&#21487;&#24494;&#20998;&#28210;&#26579;&#23398;&#20064;&#23436;&#25104;&#36974;&#25377;&#21306;&#22495;&#30340;&#20960;&#20309;&#21644;&#22806;&#35266;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#21487;&#20197;&#21033;&#29992;&#22810;&#35270;&#35282;&#35270;&#39057;&#22312;&#35757;&#32451;&#26102;&#26469;&#23398;&#20064;&#20998;&#21035;&#20174;&#21333;&#24352;&#22270;&#20687;&#20013;&#37325;&#26500;&#22330;&#26223;&#30340;&#38745;&#24577;&#21644;&#21487;&#31227;&#21160;&#32452;&#20214;&#12290;&#20998;&#21035;&#37325;&#26500;&#21487;&#31227;&#21160;&#29289;&#20307;&#30340;&#33021;&#21147;&#65292;&#20351;&#29992;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20351;&#20854;&#21487;&#20197;&#36827;&#34892;&#35832;&#22810;&#19979;&#28216;&#20219;&#21153;&#65292;&#22914;&#25552;&#21462;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;3D&#34920;&#31034;&#12289;&#26032;&#39062;&#30340;&#35270;&#35282;&#21512;&#25104;&#21644;&#29289;&#20307;&#25805;&#20316;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a method to map 2D image observations of a scene to a persistent 3D scene representation, enabling novel view synthesis and disentangled representation of the movable and immovable components of the scene. Motivated by the bird's-eye-view (BEV) representation commonly used in vision and robotics, we propose conditional neural groundplans, ground-aligned 2D feature grids, as persistent and memory-efficient scene representations. Our method is trained self-supervised from unlabeled multi-view observations using differentiable rendering, and learns to complete geometry and appearance of occluded regions. In addition, we show that we can leverage multi-view videos at training time to learn to separately reconstruct static and movable components of the scene from a single image at test time. The ability to separately reconstruct movable objects enables a variety of downstream tasks using simple heuristics, such as extraction of object-centric 3D representations, novel view synthe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#25968;&#25454;&#38598;&#30340;&#20840;&#23616;&#23646;&#24615;&#26426;&#23494;&#24615;&#20445;&#25252;&#38382;&#39064;&#65292;&#24182;&#24212;&#29992;&#20998;&#24067;&#38544;&#31169;&#26694;&#26550;&#25104;&#21151;&#38477;&#20302;&#20102;&#23454;&#38469;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#39640;&#25928;&#29992;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2207.08367</link><description>&lt;p&gt;
&#29992;&#20998;&#24067;&#38544;&#31169;&#26426;&#21046;&#20445;&#25252;&#25968;&#25454;&#38598;&#30340;&#20840;&#23616;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Protecting Global Properties of Datasets with Distribution Privacy Mechanisms. (arXiv:2207.08367v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.08367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#25968;&#25454;&#38598;&#30340;&#20840;&#23616;&#23646;&#24615;&#26426;&#23494;&#24615;&#20445;&#25252;&#38382;&#39064;&#65292;&#24182;&#24212;&#29992;&#20998;&#24067;&#38544;&#31169;&#26694;&#26550;&#25104;&#21151;&#38477;&#20302;&#20102;&#23454;&#38469;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#39640;&#25928;&#29992;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20445;&#25252;&#25968;&#25454;&#38598;&#30340;&#20840;&#23616;&#23646;&#24615;&#30340;&#26426;&#23494;&#24615;&#12290;&#36825;&#20123;&#23646;&#24615;&#21487;&#20197;&#32534;&#30721;&#25935;&#24863;&#20449;&#24687;&#65292;&#22914;&#21830;&#19994;&#26426;&#23494;&#25110;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#65292;&#32780;&#28041;&#21450;&#21040;&#19982;&#25991;&#29486;&#20013;&#36890;&#24120;&#35752;&#35770;&#30340;&#20010;&#20307;&#35760;&#24405;&#38544;&#31169;&#19981;&#21516;&#30340;&#25968;&#25454;&#20445;&#25252;&#27010;&#24565;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#24212;&#29992;&#20998;&#24067;&#38544;&#31169;&#26694;&#26550;&#26469;&#24418;&#24335;&#21270;&#36825;&#31181;&#25968;&#25454;&#26426;&#23494;&#24615;&#12290;&#25105;&#20204;&#23558; Pufferfish &#38544;&#31169;&#30340; Wasserstein &#26426;&#21046;&#21644;&#23646;&#24615;&#38544;&#31169;&#30340;&#39640;&#26031;&#26426;&#21046;&#25193;&#23637;&#21040;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#28982;&#21518;&#20998;&#26512;&#23427;&#20204;&#30340;&#22522;&#26412;&#25968;&#25454;&#20551;&#35774;&#20197;&#21450;&#22914;&#20309;&#25918;&#23485;&#36825;&#20123;&#20551;&#35774;&#12290;&#25105;&#20204;&#25509;&#30528;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#26426;&#21046;&#30340;&#38544;&#31169;&#25928;&#29992;&#26435;&#34913;&#65292;&#24182;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#38024;&#23545;&#25968;&#25454;&#38598;&#20840;&#23616;&#23646;&#24615;&#30340;&#23454;&#38469;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26426;&#21046;&#30830;&#23454;&#21487;&#20197;&#38477;&#20302;&#25915;&#20987;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#25552;&#20379;&#27604;&#31895;&#31961;&#30340;&#22522;&#20110;&#20998;&#24067;&#38544;&#31169;&#26426;&#21046;&#35201;&#39640;&#24471;&#22810;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of ensuring confidentiality of dataset properties aggregated over many records of a dataset. Such properties can encode sensitive information, such as trade secrets or demographic data, while involving a notion of data protection different to the privacy of individual records typically discussed in the literature. In this work, we demonstrate how a distribution privacy framework can be applied to formalize such data confidentiality. We extend the Wasserstein Mechanism from Pufferfish privacy and the Gaussian Mechanism from attribute privacy to this framework, then analyze their underlying data assumptions and how they can be relaxed. We then empirically evaluate the privacy-utility tradeoffs of these mechanisms and apply them against a practical property inference attack which targets global properties of datasets. The results show that our mechanisms can indeed reduce the effectiveness of the attack while providing utility substantially greater than a crude gro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;HOOD&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#22686;&#24378;&#20869;&#23481;&#21644;&#39118;&#26684;&#26469;&#35782;&#21035;&#33391;&#24615;&#21644;&#24694;&#24615;OOD&#25968;&#25454;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#21508;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2207.03162</link><description>&lt;p&gt;
&#36890;&#36807;&#22686;&#24378;&#20869;&#23481;&#21644;&#39118;&#26684;&#26469;&#21033;&#29992;OOD&#20363;&#23376;
&lt;/p&gt;
&lt;p&gt;
Harnessing Out-Of-Distribution Examples via Augmenting Content and Style. (arXiv:2207.03162v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.03162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;HOOD&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#22686;&#24378;&#20869;&#23481;&#21644;&#39118;&#26684;&#26469;&#35782;&#21035;&#33391;&#24615;&#21644;&#24694;&#24615;OOD&#25968;&#25454;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#21508;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;OOD&#20363;&#23376;&#30340;&#24433;&#21709;&#65292;&#36825;&#24050;&#32463;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#32570;&#20047;&#23545;&#19981;&#21516;&#31867;&#22411;OOD&#25968;&#25454;&#23436;&#25972;&#30340;&#29702;&#35299;&#65306;&#26377;&#19968;&#20123;&#33391;&#24615;&#30340;OOD&#25968;&#25454;&#21487;&#20197;&#36866;&#24403;&#22320;&#36827;&#34892;&#25913;&#36827;&#20197;&#22686;&#24378;&#23398;&#20064;&#24615;&#33021;&#65292;&#20294;&#20854;&#20182;&#24694;&#21155;&#30340;OOD&#25968;&#25454;&#21017;&#20250;&#20005;&#37325;&#38477;&#20302;&#20998;&#31867;&#32467;&#26524;&#12290;&#20026;&#20102;&#21033;&#29992;OOD&#25968;&#25454;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;HOOD&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;&#27599;&#20010;&#22270;&#20687;&#23454;&#20363;&#30340;&#20869;&#23481;&#21644;&#39118;&#26684;&#26469;&#35782;&#21035;&#33391;&#24615;&#21644;&#24694;&#24615;OOD&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21464;&#20998;&#25512;&#26029;&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#26469;&#22240;&#26524;&#22320;&#20998;&#31163;&#20869;&#23481;&#21644;&#26679;&#24335;&#29305;&#24449;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#24178;&#39044;&#36807;&#31243;&#22686;&#24378;&#20869;&#23481;&#21644;&#39118;&#26684;&#65292;&#20998;&#21035;&#20135;&#29983;&#24694;&#24615;&#21644;&#33391;&#24615;OOD&#25968;&#25454;&#12290;&#33391;&#24615;OOD&#25968;&#25454;&#21253;&#21547;&#26032;&#30340;&#39118;&#26684;&#65292;&#20294;&#20445;&#30041;&#20102;&#25105;&#20204;&#24863;&#20852;&#36259;&#30340;&#20869;&#23481;&#65292;&#21487;&#20197;&#24110;&#21161;&#35757;&#32451;&#19968;&#20010;&#20855;&#26377;&#26679;&#24335;&#19981;&#21464;&#24615;&#30340;&#27169;&#22411;&#12290;&#30456;&#21453;&#65292;&#24694;&#24615;OOD&#25968;&#25454;&#20855;&#26377;&#19981;&#21463;&#27426;&#36814;&#30340;&#20869;&#23481;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#19968;&#20010;&#26356;&#21152;&#31283;&#20581;&#30340;&#25269;&#24481;OOD&#20363;&#23376;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#21508;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models are vulnerable to Out-Of-Distribution (OOD) examples, and such a problem has drawn much attention. However, current methods lack a full understanding of different types of OOD data: there are benign OOD data that can be properly adapted to enhance the learning performance, while other malign OOD data would severely degenerate the classification result. To Harness OOD data, this paper proposes a HOOD method that can leverage the content and style from each image instance to identify benign and malign OOD data. Particularly, we design a variational inference framework to causally disentangle content and style features by constructing a structural causal model. Subsequently, we augment the content and style through an intervention process to produce malign and benign OOD data, respectively. The benign OOD data contain novel styles but hold our interested contents, and they can be leveraged to help train a style-invariant model. In contrast, the malign OOD data inhe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#39640;&#32500;&#25968;&#25454;&#20013;&#30340;&#24046;&#20998;&#38544;&#31169;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#36138;&#24515;&#22352;&#26631;&#19979;&#38477;&#31639;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#30340;&#32467;&#26500;&#24615;&#36136;&#65292;&#22312;&#24191;&#27867;&#30340;&#38382;&#39064;&#33539;&#22260;&#20869;&#23454;&#29616;&#23545;&#32500;&#24230;&#30340;&#23545;&#25968;&#20381;&#36182;&#24615;&#12290;</title><link>http://arxiv.org/abs/2207.01560</link><description>&lt;p&gt;
&#36138;&#24515;&#22352;&#26631;&#19979;&#38477;&#23454;&#29616;&#39640;&#32500;&#31169;&#26377;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
High-Dimensional Private Empirical Risk Minimization by Greedy Coordinate Descent. (arXiv:2207.01560v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.01560
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#39640;&#32500;&#25968;&#25454;&#20013;&#30340;&#24046;&#20998;&#38544;&#31169;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#36138;&#24515;&#22352;&#26631;&#19979;&#38477;&#31639;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#30340;&#32467;&#26500;&#24615;&#36136;&#65292;&#22312;&#24191;&#27867;&#30340;&#38382;&#39064;&#33539;&#22260;&#20869;&#23454;&#29616;&#23545;&#32500;&#24230;&#30340;&#23545;&#25968;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;DP-ERM&#65289;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#38543;&#30528;&#32500;&#24230;&#30340;&#22686;&#21152;&#65292;DP-ERM&#30340;&#26368;&#22351;&#24773;&#20917;&#25928;&#29992;&#20250;&#22810;&#39033;&#24335;&#38477;&#20302;&#12290;&#36825;&#26159;&#31169;&#26377;&#23398;&#20064;&#22823;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#22312;&#39640;&#32500;&#20013;&#65292;&#19968;&#20123;&#27169;&#22411;&#21442;&#25968;&#25658;&#24102;&#30340;&#20449;&#24687;&#27604;&#20854;&#20182;&#21442;&#25968;&#26356;&#22810;&#26159;&#24456;&#24120;&#35265;&#30340;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#36138;&#24515;&#22352;&#26631;&#19979;&#38477;&#65288;DP-GCD&#65289;&#31639;&#27861;&#12290;&#22312;&#27599;&#20010;&#36845;&#20195;&#27493;&#39588;&#20013;&#65292;DP-GCD&#27839;&#30528;&#26799;&#24230;(&#22823;&#33268;&#22320;)&#26368;&#22823;&#30340;&#26465;&#30446;&#36827;&#34892;&#22352;&#26631;&#26799;&#24230;&#27493;&#39588;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;DP-GCD&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#22320;&#21033;&#29992;&#20854;&#32467;&#26500;&#24615;&#36136;&#65288;&#20363;&#22914;&#25311;&#31232;&#30095;&#35299;&#65289;&#22312;&#24191;&#27867;&#30340;&#38382;&#39064;&#33539;&#22260;&#20869;&#23454;&#29616;&#23545;&#32500;&#24230;&#30340;&#23545;&#25968;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#35745;&#31639;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#26469;&#35828;&#26126;&#36825;&#31181;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study differentially private empirical risk minimization (DP-ERM). It has been shown that the worst-case utility of DP-ERM reduces polynomially as the dimension increases. This is a major obstacle to privately learning large machine learning models. In high dimension, it is common for some model's parameters to carry more information than others. To exploit this, we propose a differentially private greedy coordinate descent (DP-GCD) algorithm. At each iteration, DP-GCD privately performs a coordinate-wise gradient step along the gradients' (approximately) greatest entry. We show theoretically that DP-GCD can achieve a logarithmic dependence on the dimension for a wide range of problems by naturally exploiting their structural properties (such as quasi-sparse solutions). We illustrate this behavior numerically, both on synthetic and real datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#37325;&#26500;&#26041;&#27861;&#65292;&#38598;&#25104;&#20110;&#20219;&#20309;&#31867;&#22411;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#20197;&#22686;&#21152;&#21516;&#36136;&#24615;&#12290;&#26041;&#27861;&#21253;&#25324;&#33258;&#36866;&#24212;&#35889;&#32858;&#31867;&#12289;&#23494;&#24230;&#24863;&#30693;&#21516;&#36136;&#24615;&#24230;&#37327;&#26041;&#27861;&#21644;&#22522;&#20110;&#32858;&#31867;&#32467;&#26524;&#30340;&#37051;&#25509;&#30697;&#38453;&#37325;&#26500;&#12290;</title><link>http://arxiv.org/abs/2206.02386</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#36866;&#24212;&#35889;&#32858;&#31867;&#30340;&#22270;&#37325;&#26500;&#25552;&#39640;&#21516;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
Restructuring Graph for Higher Homophily via Adaptive Spectral Clustering. (arXiv:2206.02386v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#37325;&#26500;&#26041;&#27861;&#65292;&#38598;&#25104;&#20110;&#20219;&#20309;&#31867;&#22411;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#20197;&#22686;&#21152;&#21516;&#36136;&#24615;&#12290;&#26041;&#27861;&#21253;&#25324;&#33258;&#36866;&#24212;&#35889;&#32858;&#31867;&#12289;&#23494;&#24230;&#24863;&#30693;&#21516;&#36136;&#24615;&#24230;&#37327;&#26041;&#27861;&#21644;&#22522;&#20110;&#32858;&#31867;&#32467;&#26524;&#30340;&#37051;&#25509;&#30697;&#38453;&#37325;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36234;&#26469;&#36234;&#22810;&#30340;&#25991;&#29486;&#30740;&#31350;&#20102;&#36866;&#29992;&#20110;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#22270;&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20294;&#22312;&#23558;&#20256;&#32479;&#22270;&#31070;&#32463;&#32593;&#32476;&#36866;&#24212;&#20110; less-homophilic &#22270;&#26041;&#38754;&#20570;&#24471;&#24456;&#23569;&#12290;&#34429;&#28982;&#20256;&#32479;&#22270;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702; less-homophilic &#22270;&#30340;&#33021;&#21147;&#21463;&#21040;&#38480;&#21046;&#65292;&#20294;&#20173;&#20855;&#26377;&#25928;&#29575;&#39640;&#12289;&#31616;&#21333;&#12289;&#21487;&#35299;&#37322;&#24615;&#22909;&#31561;&#22810;&#20010;&#20248;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#37325;&#26500;&#26041;&#27861;&#65292;&#21487;&#38598;&#25104;&#21040;&#20219;&#20309;&#31867;&#22411;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#21253;&#25324;&#20256;&#32479;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#21457;&#25381;&#29616;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#28857;&#21516;&#26102;&#20943;&#36731;&#20854;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;: a) &#23398;&#20064;&#25311;&#21512;&#33410;&#28857;&#26631;&#31614;&#30340;&#33258;&#36866;&#24212;&#35889;&#32858;&#31867;&#30340; pseudo-eigenvector &#26435;&#37325;&#65292;b) &#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#23494;&#24230;&#24863;&#30693;&#30340;&#21516;&#36136;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#26631;&#31614;&#19981;&#24179;&#34913;&#24615;&#40065;&#26834;&#24615;&#65292;c) &#22522;&#20110;&#33258;&#36866;&#24212;&#35889;&#32858;&#31867;&#30340;&#32467;&#26524;&#37325;&#26500;&#37051;&#25509;&#30697;&#38453;&#65292;&#20197;&#26368;&#22823;&#21270;&#21516;&#36136;&#24615;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
While a growing body of literature has been studying new Graph Neural Networks (GNNs) that work on both homophilic and heterophilic graphs, little has been done on adapting classical GNNs to less-homophilic graphs. Although the ability to handle less-homophilic graphs is restricted, classical GNNs still stand out in several nice properties such as efficiency, simplicity, and explainability. In this work, we propose a novel graph restructuring method that can be integrated into any type of GNNs, including classical GNNs, to leverage the benefits of existing GNNs while alleviating their limitations. Our contribution is threefold: a) learning the weight of pseudo-eigenvectors for an adaptive spectral clustering that aligns well with known node labels, b) proposing a new density-aware homophilic metric that is robust to label imbalance, and c) reconstructing the adjacency matrix based on the result of adaptive spectral clustering to maximize the homophilic scores. The experimental results 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#23384;&#22312;&#20559;&#35265;&#26102;&#29616;&#26377;&#20844;&#24179;&#26631;&#20934;&#30340;&#40065;&#26834;&#24615;&#65292;&#25506;&#31350;&#20102;&#26631;&#35760;&#21644;&#27979;&#37327;&#35823;&#24046;&#23545;&#20854;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19968;&#20123;&#32422;&#26463;&#21487;&#20197;&#22312;&#38754;&#23545;&#26576;&#20123;&#32479;&#35745;&#20559;&#24046;&#26102;&#20445;&#25345;&#31283;&#20581;&#65292;&#32780;&#21478;&#19968;&#20123;&#21017;&#20250;&#22312;&#35757;&#32451;&#20559;&#35265;&#25968;&#25454;&#38598;&#26102;&#34987;&#26174;&#33879;&#36829;&#21453;&#12290;</title><link>http://arxiv.org/abs/2206.00137</link><description>&lt;p&gt;
&#31038;&#20250;&#20559;&#35265;&#36935;&#21040;&#25968;&#25454;&#20559;&#35265;: &#26631;&#27880;&#21644;&#27979;&#37327;&#35823;&#24046;&#23545;&#20844;&#24179;&#26631;&#20934;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Social Bias Meets Data Bias: The Impacts of Labeling and Measurement Errors on Fairness Criteria. (arXiv:2206.00137v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#23384;&#22312;&#20559;&#35265;&#26102;&#29616;&#26377;&#20844;&#24179;&#26631;&#20934;&#30340;&#40065;&#26834;&#24615;&#65292;&#25506;&#31350;&#20102;&#26631;&#35760;&#21644;&#27979;&#37327;&#35823;&#24046;&#23545;&#20854;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19968;&#20123;&#32422;&#26463;&#21487;&#20197;&#22312;&#38754;&#23545;&#26576;&#20123;&#32479;&#35745;&#20559;&#24046;&#26102;&#20445;&#25345;&#31283;&#20581;&#65292;&#32780;&#21478;&#19968;&#20123;&#21017;&#20250;&#22312;&#35757;&#32451;&#20559;&#35265;&#25968;&#25454;&#38598;&#26102;&#34987;&#26174;&#33879;&#36829;&#21453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#20844;&#24179;&#26631;&#20934;&#26469;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#19981;&#20250;&#34920;&#29616;&#20986;&#25110;&#25918;&#22823;&#25105;&#20204;&#29616;&#26377;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#20294;&#36825;&#20123;&#31639;&#27861;&#26159;&#22312;&#26412;&#36523;&#21487;&#33021;&#23384;&#22312;&#32479;&#35745;&#20559;&#24046;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#31639;&#27861;&#35757;&#32451;&#22312;&#20559;&#35265;&#25968;&#25454;&#38598;&#19978;&#26102;&#19968;&#20123;&#29616;&#26377;(&#20154;&#21475;)&#20844;&#24179;&#26631;&#20934;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#24418;&#24335;&#30340;&#25968;&#25454;&#38598;&#20559;&#24046;&#65306;&#26631;&#35760;&#36807;&#31243;&#20013;&#30340;&#20808;&#21069;&#20915;&#31574;&#21046;&#23450;&#32773;&#30340;&#38169;&#35823;&#21644;&#23545;&#21155;&#21183;&#20010;&#20307;&#29305;&#24449;&#30340;&#27979;&#37327;&#35823;&#24046;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#19968;&#20123;&#32422;&#26463;(&#20363;&#22914;&#20154;&#21475;&#22343;&#31561;&#24615;)&#22312;&#38754;&#23545;&#26576;&#20123;&#32479;&#35745;&#20559;&#24046;&#26102;&#21487;&#20197;&#20445;&#25345;&#31283;&#20581;&#65292;&#32780;&#21478;&#19968;&#20123;(&#20363;&#22914;&#24179;&#31561;&#26426;&#20250;)&#21017;&#20250;&#22312;&#35757;&#32451;&#20559;&#35265;&#25968;&#25454;&#38598;&#26102;&#34987;&#26174;&#33879;&#36829;&#21453;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#36825;&#20123;&#26631;&#20934;&#21644;&#20915;&#31574;&#21046;&#23450;&#32773;&#25928;&#29992;&#23545;&#20559;&#35265;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#22522;&#20110;&#19977;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;(FICO&#12289;&#25104;&#20154;&#21644;&#24503;&#22269;&#20449;&#29992;&#35780;&#20998;&#25968;&#25454;&#38598;)&#25552;&#20379;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#25903;&#25345;&#25105;&#20204;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although many fairness criteria have been proposed to ensure that machine learning algorithms do not exhibit or amplify our existing social biases, these algorithms are trained on datasets that can themselves be statistically biased. In this paper, we investigate the robustness of a number of existing (demographic) fairness criteria when the algorithm is trained on biased data. We consider two forms of dataset bias: errors by prior decision makers in the labeling process, and errors in measurement of the features of disadvantaged individuals. We analytically show that some constraints (such as Demographic Parity) can remain robust when facing certain statistical biases, while others (such as Equalized Odds) are significantly violated if trained on biased data. We also analyze the sensitivity of these criteria and the decision maker's utility to biases. We provide numerical experiments based on three real-world datasets (the FICO, Adult, and German credit score datasets) supporting our 
&lt;/p&gt;</description></item><item><title>AVIDA&#26159;&#19968;&#20010;&#21487;&#35270;&#21270;&#21644;&#25972;&#21512;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#25191;&#34892;&#25968;&#25454;&#23545;&#40784;&#21644;&#38477;&#32500;&#65292;&#24182;&#25104;&#21151;&#22320;&#23545;&#40784;&#20102;&#39640;&#32500;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#20445;&#30041;&#20102;&#27599;&#20010;&#25968;&#25454;&#38598;&#30340;&#32467;&#26500;&#65292;&#29305;&#21035;&#26159;&#32852;&#21512;&#20302;&#32500;&#21487;&#35270;&#21270;&#20013;&#30340;&#19981;&#21516;&#23616;&#37096;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2206.00135</link><description>&lt;p&gt;
AVIDA: &#21487;&#35270;&#21270;&#21644;&#25972;&#21512;&#25968;&#25454;&#30340;&#20132;&#26367;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AVIDA: Alternating method for Visualizing and Integrating Data. (arXiv:2206.00135v2 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00135
&lt;/p&gt;
&lt;p&gt;
AVIDA&#26159;&#19968;&#20010;&#21487;&#35270;&#21270;&#21644;&#25972;&#21512;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#25191;&#34892;&#25968;&#25454;&#23545;&#40784;&#21644;&#38477;&#32500;&#65292;&#24182;&#25104;&#21151;&#22320;&#23545;&#40784;&#20102;&#39640;&#32500;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#20445;&#30041;&#20102;&#27599;&#20010;&#25968;&#25454;&#38598;&#30340;&#32467;&#26500;&#65292;&#29305;&#21035;&#26159;&#32852;&#21512;&#20302;&#32500;&#21487;&#35270;&#21270;&#20013;&#30340;&#19981;&#21516;&#23616;&#37096;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#22810;&#27169;&#24577;&#25968;&#25454;&#22312;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#20013;&#20986;&#29616;&#12290;&#24403;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#26679;&#26412;&#21644;&#29305;&#24449;&#20043;&#38388;&#27809;&#26377;&#24050;&#30693;&#30340;&#23545;&#24212;&#20851;&#31995;&#26102;&#65292;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#25972;&#21512;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AVIDA&#65292;&#36825;&#26159;&#19968;&#20010;&#21516;&#26102;&#25191;&#34892;&#25968;&#25454;&#23545;&#40784;&#21644;&#38477;&#32500;&#30340;&#26694;&#26550;&#12290;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;Gromov-Wasserstein&#26368;&#20248;&#36816;&#36755;&#21644;t&#20998;&#24067;&#38543;&#26426;&#37051;&#23621;&#23884;&#20837;&#20316;&#20026;&#23545;&#40784;&#21644;&#38477;&#32500;&#27169;&#22359;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;AVIDA&#27491;&#30830;&#22320;&#23545;&#40784;&#20102;&#20855;&#26377;&#22235;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#30495;&#23454;&#22810;&#27169;&#24577;&#21333;&#32454;&#32990;&#25968;&#25454;&#38598;&#30340;&#39640;&#32500;&#25968;&#25454;&#38598;&#12290;&#19982;&#20960;&#31181;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;AVIDA&#26356;&#22909;&#22320;&#20445;&#30041;&#20102;&#27599;&#20010;&#25968;&#25454;&#38598;&#30340;&#32467;&#26500;&#65292;&#29305;&#21035;&#26159;&#32852;&#21512;&#20302;&#32500;&#21487;&#35270;&#21270;&#20013;&#30340;&#19981;&#21516;&#23616;&#37096;&#32467;&#26500;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#30456;&#24403;&#30340;&#23545;&#40784;&#24615;&#33021;&#12290;&#36825;&#31181;&#29305;&#24615;&#22312;&#22810;&#27169;&#24577;&#21333;&#32454;&#32990;&#30740;&#31350;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-dimensional multimodal data arises in many scientific fields. The integration of multimodal data becomes challenging when there is no known correspondence between the samples and the features of different datasets. To tackle this challenge, we introduce AVIDA, a framework for simultaneously performing data alignment and dimension reduction. In the numerical experiments, Gromov-Wasserstein optimal transport and t-distributed stochastic neighbor embedding are used as the alignment and dimension reduction modules respectively. We show that AVIDA correctly aligns high-dimensional datasets without common features with four synthesized datasets and two real multimodal single-cell datasets. Compared to several existing methods, we demonstrate that AVIDA better preserves structures of individual datasets, especially distinct local structures in the joint low-dimensional visualization, while achieving comparable alignment performance. Such a property is important in multimodal single-cell 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616; SHapley Additive exPlanations (SHAP) &#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#37322;&#21463;&#21040;&#32972;&#26223;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#24433;&#21709;&#65292;&#32780;&#36873;&#25321;&#21512;&#36866;&#30340;&#32972;&#26223;&#25968;&#25454;&#38598;&#33021;&#22815;&#30830;&#20445; SHAP &#35299;&#37322;&#30340;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.11351</link><description>&lt;p&gt;
&#32972;&#26223;&#25968;&#25454;&#35268;&#27169;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411; SHAP &#35299;&#37322;&#31283;&#23450;&#24615;&#30340;&#24433;&#21709;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An empirical study of the effect of background data size on the stability of SHapley Additive exPlanations (SHAP) for deep learning models. (arXiv:2204.11351v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.11351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616; SHapley Additive exPlanations (SHAP) &#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#37322;&#21463;&#21040;&#32972;&#26223;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#24433;&#21709;&#65292;&#32780;&#36873;&#25321;&#21512;&#36866;&#30340;&#32972;&#26223;&#25968;&#25454;&#38598;&#33021;&#22815;&#30830;&#20445; SHAP &#35299;&#37322;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25512;&#26029;&#32467;&#26524;&#20934;&#30830;&#24615;&#21516;&#26679;&#37325;&#35201;&#30340;&#26159;&#20854;&#21487;&#35299;&#37322;&#24615;&#12290;&#26576;&#20123;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22914;&#20915;&#31574;&#26641;&#25317;&#26377;&#22825;&#28982;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#32780;&#20854;&#20182;&#27169;&#22411;&#22914;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21017;&#38656;&#35201;&#22806;&#37096;&#26041;&#27861;&#25581;&#31034;&#20854;&#25512;&#26029;&#26426;&#21046;&#12290;SHapley Additive exPlanations (SHAP)&#23601;&#26159;&#19968;&#31181;&#22806;&#37096;&#35299;&#37322;&#26041;&#24335;&#65292;&#23427;&#38656;&#35201;&#19968;&#20010;&#32972;&#26223;&#25968;&#25454;&#38598;&#23545;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35299;&#37322;&#12290;&#36890;&#24120;&#65292;&#32972;&#26223;&#25968;&#25454;&#38598;&#30001;&#20174;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#38543;&#26426;&#25277;&#26679;&#30340;&#23454;&#20363;&#32452;&#25104;&#12290;&#28982;&#32780;&#65292;&#32972;&#26223;&#25968;&#25454;&#38598;&#30340;&#25277;&#26679;&#35268;&#27169;&#21450;&#20854;&#23545; SHAP &#30340;&#24433;&#21709;&#20173;&#26410;&#34987;&#25506;&#35752;&#12290;&#22312;&#25105;&#20204;&#23545; MIMIC-III &#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#19981;&#21516;&#30340;&#38543;&#26426;&#25277;&#26679;&#24471;&#21040;&#30340;&#32972;&#26223;&#25968;&#25454;&#38598;&#20250;&#23548;&#33268;&#26680;&#24515;&#35299;&#37322;&#8212;&#8212; SHAP &#20540;&#21644;&#21464;&#37327;&#25490;&#24207;&#20540;&#30340;&#27874;&#21160;&#65292;&#36825;&#34920;&#26126;&#29992;&#25143;&#19981;&#33021;&#36731;&#20449; SHAP &#25552;&#20379;&#30340;&#19968;&#27425;&#24615;&#35299;&#37322;&#32467;&#26524;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#36825;&#26679;&#30340;&#27874;&#21160;&#24182;&#19981;&#24847;&#21619;&#30528; SHAP &#22833;&#36133;&#65292;&#32780;&#26159;&#34920;&#26126;&#36873;&#25321;&#21512;&#36866;&#30340;&#32972;&#26223;&#25968;&#25454;&#38598;&#23545;&#20110;&#30830;&#20445; SHAP &#35299;&#37322;&#30340;&#31283;&#23450;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, the interpretation of why a machine learning (ML) model makes certain inferences is as crucial as the accuracy of such inferences. Some ML models like the decision tree possess inherent interpretability that can be directly comprehended by humans. Others like artificial neural networks (ANN), however, rely on external methods to uncover the deduction mechanism. SHapley Additive exPlanations (SHAP) is one of such external methods, which requires a background dataset when interpreting ANNs. Generally, a background dataset consists of instances randomly sampled from the training dataset. However, the sampling size and its effect on SHAP remain to be unexplored. In our empirical study on the MIMIC-III dataset, we show that the two core explanations - SHAP values and variable rankings fluctuate when using different background datasets acquired from random sampling, indicating that users cannot unquestioningly trust the one-shot interpretation from SHAP. Luckily, such fluctuation d
&lt;/p&gt;</description></item><item><title>FAIR4Cov&#26159;&#19968;&#31181;&#38024;&#23545;COVID-19&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#23427;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#36523;&#20307;&#22768;&#38899;&#30340;&#27874;&#24418;&#21644;&#35889;&#22270;&#34920;&#31034;&#30340;&#20851;&#33410;&#29305;&#24449;&#21521;&#37327;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;COVID-19&#24739;&#32773;&#65292;&#32988;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2204.10581</link><description>&lt;p&gt;
FAIR4Cov&#65306;&#29992;&#20110; COVID-19 &#26816;&#27979;&#30340;&#34701;&#21512;&#38899;&#39057;&#23454;&#20363;&#21644;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
FAIR4Cov: Fused Audio Instance and Representation for COVID-19 Detection. (arXiv:2204.10581v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.10581
&lt;/p&gt;
&lt;p&gt;
FAIR4Cov&#26159;&#19968;&#31181;&#38024;&#23545;COVID-19&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#23427;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#36523;&#20307;&#22768;&#38899;&#30340;&#27874;&#24418;&#21644;&#35889;&#22270;&#34920;&#31034;&#30340;&#20851;&#33410;&#29305;&#24449;&#21521;&#37327;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;COVID-19&#24739;&#32773;&#65292;&#32988;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36523;&#20307;&#22768;&#38899;&#30340;&#20998;&#31867;&#25216;&#26415;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#34987;&#30740;&#31350;&#29992;&#20110;&#25903;&#25345;&#35786;&#26029;&#20915;&#31574;&#65292;&#29305;&#21035;&#26159;&#22312;&#32954;&#37096;&#30142;&#30149;&#26041;&#38754;&#12290;&#38024;&#23545; COVID-19 &#30123;&#24773;&#30340;&#32039;&#36843;&#24615;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#27169;&#22411;&#34987;&#24320;&#21457;&#26469;&#22522;&#20110;&#22768;&#23398;&#36755;&#20837;&#35782;&#21035; COVID-19 &#24739;&#32773;&#12290;&#22823;&#22810;&#25968;&#27169;&#22411;&#20391;&#37325;&#20110;&#21683;&#22013;&#65292;&#22240;&#20026;&#24178;&#21683;&#26159; COVID-19 &#26368;&#20026;&#20154;&#25152;&#30693;&#30340;&#30151;&#29366;&#12290;&#28982;&#32780;&#65292;&#21628;&#21560;&#21644;&#35328;&#35821;&#31561;&#20854;&#20182;&#36523;&#20307;&#22768;&#38899;&#20063;&#34987;&#21457;&#29616;&#19982; COVID-19 &#30456;&#20851;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; FAIR4Cov&#65292;&#23427;&#19981;&#20381;&#36182;&#20110;&#29305;&#23450;&#30340;&#36523;&#20307;&#22768;&#38899;&#65292;&#32780;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#36523;&#20307;&#22768;&#38899;&#30340;&#27874;&#24418;&#21644;&#35889;&#22270;&#34920;&#31034;&#30340;&#20851;&#33410;&#29305;&#24449;&#21521;&#37327;&#12290;FAIR4Cov &#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#19968;&#20010;&#33258;&#27880;&#24847;&#34701;&#21512;&#21333;&#20803;&#65292;&#23427;&#30340;&#35757;&#32451;&#30446;&#30340;&#26159;&#24314;&#31435;&#22810;&#20010;&#36523;&#20307;&#22768;&#38899;&#21644;&#38899;&#39057;&#34920;&#31034;&#30340;&#20851;&#31995;&#24182;&#23558;&#20854;&#38598;&#25104;&#21040;&#19968;&#20010;&#32039;&#20945;&#30340;&#29305;&#24449;&#21521;&#37327;&#20013;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#35774;&#32622;&#20102;&#23454;&#39564;&#65292;&#24182;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#25552;&#35758;&#26041;&#27861;&#65292;&#21253;&#25324;&#36328;&#25968;&#25454;&#38598;&#35780;&#20272;&#21644;&#26089;&#26399;&#26816;&#27979;&#35774;&#32622;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FAIR4Cov &#32988;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#21033;&#29992;&#21508;&#31181;&#36523;&#20307;&#22768;&#38899;&#26816;&#27979; COVID-19 &#24739;&#32773;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio-based classification techniques on body sounds have long been studied to support diagnostic decisions, particularly in pulmonary diseases. In response to the urgency of the COVID-19 pandemic, a growing number of models are developed to identify COVID-19 patients based on acoustic input. Most models focus on cough because the dry cough is the best-known symptom of COVID-19. However, other body sounds, such as breath and speech, have also been revealed to correlate with COVID-19 as well. In this work, rather than relying on a specific body sound, we propose Fused Audio Instance and Representation for COVID-19 Detection (FAIR4Cov). It relies on constructing a joint feature vector obtained from a plurality of body sounds in waveform and spectrogram representation. The core component of FAIR4Cov is a self-attention fusion unit that is trained to establish the relation of multiple body sounds and audio representations and integrate it into a compact feature vector. We set up our experi
&lt;/p&gt;</description></item><item><title>InCoder&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#36827;&#34892;&#31243;&#24207;&#21512;&#25104;&#21644;&#21452;&#21521;&#19978;&#19979;&#25991;&#30340;&#20195;&#30721;&#22635;&#20805;&#65292;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#30452;&#25509;&#36827;&#34892;&#38646;&#26679;&#26412;&#20195;&#30721;&#22635;&#20805;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2204.05999</link><description>&lt;p&gt;
InCoder&#65306;&#19968;&#31181;&#20195;&#30721;&#22635;&#20805;&#21644;&#21512;&#25104;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
InCoder: A Generative Model for Code Infilling and Synthesis. (arXiv:2204.05999v3 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.05999
&lt;/p&gt;
&lt;p&gt;
InCoder&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#36827;&#34892;&#31243;&#24207;&#21512;&#25104;&#21644;&#21452;&#21521;&#19978;&#19979;&#25991;&#30340;&#20195;&#30721;&#22635;&#20805;&#65292;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#30452;&#25509;&#36827;&#34892;&#38646;&#26679;&#26412;&#20195;&#30721;&#22635;&#20805;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#24448;&#24448;&#19981;&#26159;&#19968;&#27425;&#20174;&#24038;&#21040;&#21491;&#30340;&#20889;&#20316;&#36807;&#31243;&#65292;&#32780;&#26159;&#21453;&#22797;&#32534;&#36753;&#21644;&#25913;&#36827;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;InCoder&#65292;&#19968;&#31181;&#32479;&#19968;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#20174;&#24038;&#21040;&#21491;&#30340;&#29983;&#25104;&#36827;&#34892;&#31243;&#24207;&#21512;&#25104;&#65292;&#20063;&#21487;&#20197;&#36827;&#34892;&#32534;&#36753;&#65288;&#36890;&#36807;&#22635;&#20805;&#65289;&#12290;InCoder&#36890;&#36807;&#20174;&#19968;&#20010;&#22823;&#22411;&#24320;&#28304;&#20195;&#30721;&#24211;&#20013;&#38543;&#26426;&#23631;&#34109;&#20195;&#30721;&#22359;&#24182;&#23558;&#20854;&#31227;&#21160;&#21040;&#27599;&#20010;&#25991;&#20214;&#26411;&#23614;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#20854;&#21487;&#20197;&#36827;&#34892;&#21452;&#21521;&#19978;&#19979;&#25991;&#30340;&#20195;&#30721;&#22635;&#20805;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#30452;&#25509;&#36827;&#34892;&#38646;&#26679;&#26412;&#20195;&#30721;&#22635;&#20805;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#22312;&#31867;&#22411;&#25512;&#26029;&#12289;&#27880;&#37322;&#29983;&#25104;&#21644;&#21464;&#37327;&#37325;&#21629;&#21517;&#31561;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#20855;&#26377;&#21452;&#21521;&#19978;&#19979;&#25991;&#30340;&#26465;&#20214;&#19979;&#65292;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#25191;&#34892;&#36825;&#20123;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#32780;&#22312;&#26631;&#20934;&#31243;&#24207;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#19982;&#30456;&#20284;&#35268;&#27169;&#30340;&#20165;&#20174;&#24038;&#21040;&#21491;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#24615;&#33021;&#30456;&#24403;&#12290;InCoder&#27169;&#22411;&#21644;&#20195;&#30721;&#24050;&#32463;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code is seldom written in a single left-to-right pass and is instead repeatedly edited and refined. We introduce InCoder, a unified generative model that can perform program synthesis (via left-to-right generation) as well as editing (via infilling). InCoder is trained to generate code files from a large corpus of permissively licensed code, where regions of code have been randomly masked and moved to the end of each file, allowing code infilling with bidirectional context. Our model is the first generative model that is able to directly perform zero-shot code infilling, which we evaluate on challenging tasks such as type inference, comment generation, and variable re-naming. We find that the ability to condition on bidirectional context substantially improves performance on these tasks, while still performing comparably on standard program synthesis benchmarks in comparison to left-to-right only models pretrained at similar scale. The InCoder models and code are publicly released. htt
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;&#34507;&#30333;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20445;&#30041;&#37325;&#35201;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#32467;&#21512;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;&#22312;&#20004;&#20010;&#34507;&#30333;&#36136;&#20998;&#31867;&#27979;&#35797;&#20013;&#22343;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2204.04213</link><description>&lt;p&gt;
&#32467;&#26500;&#24863;&#30693;&#30340;&#34507;&#30333;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Structure-aware Protein Self-supervised Learning. (arXiv:2204.04213v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.04213
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;&#34507;&#30333;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20445;&#30041;&#37325;&#35201;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#32467;&#21512;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;&#22312;&#20004;&#20010;&#34507;&#30333;&#36136;&#20998;&#31867;&#27979;&#35797;&#20013;&#22343;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#65292;&#23588;&#20854;&#26159;&#34507;&#30333;&#36136;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#23637;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#36824;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20102;&#34507;&#30333;&#36136;&#26631;&#31614;&#25968;&#37327;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#22312;&#34507;&#30333;&#36136;&#24207;&#21015;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#32780;&#24573;&#30053;&#20102;&#37325;&#35201;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32467;&#26500;&#24863;&#30693;&#30340;&#34507;&#30333;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#26469;&#26377;&#25928;&#22320;&#25429;&#33719;&#34507;&#30333;&#36136;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20248;&#31168;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#27531;&#22522;&#38388;&#36317;&#21644;&#20108;&#38754;&#35282;&#30340;&#33258;&#30417;&#30563;&#20219;&#21153;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#26469;&#20445;&#30041;&#34507;&#30333;&#36136;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#21033;&#29992;&#24050;&#26377;&#30340;&#22312;&#34507;&#30333;&#36136;&#24207;&#21015;&#19978;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26469;&#22686;&#24378;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550; GP-SSL&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#22522;&#20110;GNN&#30340;&#32467;&#26500;&#27169;&#22411;&#21644;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#24207;&#21015;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;fine-tuning&#23558;&#20174;GNN&#27169;&#22411;&#23398;&#21040;&#30340;&#26377;&#29992;&#30340;&#32467;&#26500;&#24863;&#30693;&#34920;&#31034;&#36716;&#31227;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#34507;&#30333;&#36136;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;GP-SSL&#22312;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#21644;&#32467;&#26500;&#20449;&#24687;&#20445;&#30041;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;BERT&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/microsoft/GP-SSL&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protein representation learning methods have shown great potential to yield useful representation for many downstream tasks, especially on protein classification. Moreover, a few recent studies have shown great promise in addressing insufficient labels of proteins with self-supervised learning methods. However, existing protein language models are usually pretrained on protein sequences without considering the important protein structural information. To this end, we propose a novel structure-aware protein self-supervised learning method to effectively capture structural information of proteins. In particular, a well-designed graph neural network (GNN) model is pretrained to preserve the protein structural information with self-supervised tasks from a pairwise residue distance perspective and a dihedral angle perspective, respectively. Furthermore, we propose to leverage the available protein language model pretrained on protein sequences to enhance the self-supervised learning. Specif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#38543;&#26426;&#38160;&#24230;&#24863;&#30693;&#35757;&#32451;&#65288;RST&#65289;&#30340;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;SGD&#21644;SAM&#20043;&#38388;&#38543;&#26426;&#36873;&#25321;&#26469;&#20943;&#23569;&#35745;&#31639;&#37327;&#65292;&#21516;&#26102;&#20445;&#35777;&#27169;&#22411;&#25910;&#25947;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;&#35843;&#24230;&#20989;&#25968;&#30340;&#25928;&#26524;&#21644;&#35745;&#31639;&#25104;&#26412;&#36827;&#34892;&#20102;&#23454;&#39564;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2203.09962</link><description>&lt;p&gt;
&#38754;&#21521;&#28145;&#24230;&#23398;&#20064;&#35745;&#31639;&#25928;&#29575;&#25552;&#21319;&#30340;&#38543;&#26426;&#38160;&#24230;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Randomized Sharpness-Aware Training for Boosting Computational Efficiency in Deep Learning. (arXiv:2203.09962v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.09962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#38543;&#26426;&#38160;&#24230;&#24863;&#30693;&#35757;&#32451;&#65288;RST&#65289;&#30340;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;SGD&#21644;SAM&#20043;&#38388;&#38543;&#26426;&#36873;&#25321;&#26469;&#20943;&#23569;&#35745;&#31639;&#37327;&#65292;&#21516;&#26102;&#20445;&#35777;&#27169;&#22411;&#25910;&#25947;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;&#35843;&#24230;&#20989;&#25968;&#30340;&#25928;&#26524;&#21644;&#35745;&#31639;&#25104;&#26412;&#36827;&#34892;&#20102;&#23454;&#39564;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#27169;&#22411;&#25910;&#25947;&#20110;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#65292;SAM&#31561;&#38160;&#24230;&#24863;&#30693;&#30340;&#23398;&#20064;&#31639;&#27861;&#24050;&#26174;&#31034;&#20986;&#23454;&#29616;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31639;&#27861;&#36890;&#24120;&#20250;&#22312;&#27599;&#27425;&#35757;&#32451;&#36845;&#20195;&#20013;&#22810;&#36827;&#34892;&#19968;&#27425;&#21069;&#21521;-&#21453;&#21521;&#20256;&#25773;&#65292;&#20174;&#32780;&#22823;&#22823;&#22686;&#21152;&#20102;&#35745;&#31639;&#37327;&#65292;&#29305;&#21035;&#26159;&#22312;&#21487;&#25193;&#23637;&#27169;&#22411;&#20013;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#38543;&#26426;&#38160;&#24230;&#24863;&#30693;&#35757;&#32451;(RST)&#30340;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#35757;&#32451;&#26041;&#26696;&#12290;RST&#20013;&#30340;&#20248;&#21270;&#22120;&#27599;&#27425;&#36845;&#20195;&#37117;&#20250;&#36827;&#34892;&#20271;&#21162;&#21033;&#23454;&#39564;&#65292;&#20197;&#30001;&#39044;&#23450;&#20041;&#30340;&#35843;&#24230;&#20989;&#25968;&#23433;&#25490;&#30340;&#27010;&#29575;&#38543;&#26426;&#36873;&#25321;&#22522;&#26412;&#31639;&#27861;&#65288;SGD&#65289;&#21644;&#38160;&#24230;&#24863;&#30693;&#31639;&#27861;&#65288;SAM&#65289;&#20043;&#19968;&#12290;&#30001;&#20110;&#22522;&#26412;&#31639;&#27861;&#30340;&#28151;&#21512;&#65292;&#20256;&#25773;&#23545;&#30340;&#24635;&#25968;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;RST&#30340;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#20102;&#21508;&#31181;&#35843;&#24230;&#20989;&#25968;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#25928;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#35774;&#32622;&#36866;&#24403;&#35843;&#24230;&#20989;&#25968;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
By driving models to converge to flat minima, sharpness-aware learning algorithms (such as SAM) have shown the power to achieve state-of-the-art performances. However, these algorithms will generally incur one extra forward-backward propagation at each training iteration, which largely burdens the computation especially for scalable models. To this end, we propose a simple yet efficient training scheme, called Randomized Sharpness-Aware Training (RST). Optimizers in RST would perform a Bernoulli trial at each iteration to choose randomly from base algorithms (SGD) and sharpness-aware algorithms (SAM) with a probability arranged by a predefined scheduling function. Due to the mixture of base algorithms, the overall count of propagation pairs could be largely reduced. Also, we give theoretical analysis on the convergence of RST. Then, we empirically study the computation cost and effect of various types of scheduling functions, and give directions on setting appropriate scheduling functi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#31934;&#24230;&#31163;&#25955;&#21270;&#30340;&#39640;&#25928;&#37327;&#21270;&#32593;&#32476;&#21644;&#20108;&#36827;&#21046;&#25551;&#36848;&#31526;&#8212;&#8212;ZippyPoint&#65292;&#21487;&#23454;&#29616;&#22312;&#35745;&#31639;&#21463;&#38480;&#30340;&#24179;&#21488;&#19978;&#24555;&#36895;&#30340;&#20852;&#36259;&#28857;&#26816;&#27979;&#12289;&#25551;&#36848;&#21644;&#21305;&#37197;&#65292;&#21516;&#26102;&#22312;&#20960;&#20010;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2203.03610</link><description>&lt;p&gt;
ZippyPoint: &#22522;&#20110;&#28151;&#21512;&#31934;&#24230;&#31163;&#25955;&#21270;&#30340;&#24555;&#36895;&#20852;&#36259;&#28857;&#26816;&#27979;&#12289;&#25551;&#36848;&#21644;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
ZippyPoint: Fast Interest Point Detection, Description, and Matching through Mixed Precision Discretization. (arXiv:2203.03610v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.03610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#31934;&#24230;&#31163;&#25955;&#21270;&#30340;&#39640;&#25928;&#37327;&#21270;&#32593;&#32476;&#21644;&#20108;&#36827;&#21046;&#25551;&#36848;&#31526;&#8212;&#8212;ZippyPoint&#65292;&#21487;&#23454;&#29616;&#22312;&#35745;&#31639;&#21463;&#38480;&#30340;&#24179;&#21488;&#19978;&#24555;&#36895;&#30340;&#20852;&#36259;&#28857;&#26816;&#27979;&#12289;&#25551;&#36848;&#21644;&#21305;&#37197;&#65292;&#21516;&#26102;&#22312;&#20960;&#20010;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#35273;&#31995;&#32479;&#30340;&#23450;&#20301;&#21644;&#24314;&#22270;&#20013;&#65292;&#23545;&#22270;&#20687;&#20013;&#20960;&#20309;&#21306;&#22495;&#30340;&#39640;&#25928;&#26816;&#27979;&#21644;&#25551;&#36848;&#26159;&#20808;&#20915;&#26465;&#20214;&#12290;&#36825;&#20123;&#31995;&#32479;&#20173;&#28982;&#20381;&#36182;&#20110;&#20256;&#32479;&#30340;&#25163;&#24037;&#21046;&#20316;&#26041;&#27861;&#26469;&#29983;&#25104;&#36731;&#37327;&#32423;&#25551;&#36848;&#31526;&#65292;&#32780;&#26356;&#24378;&#22823;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20855;&#26377;&#39640;&#35745;&#31639;&#21644;&#29305;&#23450;&#30828;&#20214;&#35201;&#27714;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#26816;&#27979;&#21644;&#25551;&#36848;&#31070;&#32463;&#32593;&#32476;&#25152;&#38656;&#30340;&#33258;&#36866;&#24212;&#25514;&#26045;&#65292;&#20197;&#20351;&#23427;&#20204;&#21487;&#20197;&#22312;&#35745;&#31639;&#21463;&#38480;&#30340;&#24179;&#21488;&#19978;&#20351;&#29992;&#65292;&#22914;&#26426;&#22120;&#20154;&#12289;&#31227;&#21160;&#35774;&#22791;&#21644;&#22686;&#24378;&#29616;&#23454;&#35774;&#22791;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#21644;&#36866;&#24212;&#20102;&#32593;&#32476;&#37327;&#21270;&#25216;&#26415;&#26469;&#21152;&#36895;&#25512;&#29702;&#24182;&#20351;&#20854;&#22312;&#35745;&#31639;&#21463;&#38480;&#30340;&#24179;&#21488;&#19978;&#20351;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#25551;&#36848;&#31526;&#37327;&#21270;&#30340;&#24120;&#35265;&#20570;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#20108;&#36827;&#21046;&#25551;&#36848;&#31526;&#24402;&#19968;&#21270;&#23618;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#29983;&#25104;&#20855;&#26377;&#19968;&#23450;&#25968;&#37327;&#8220;1&#8221;&#30340;&#29420;&#29305;&#20108;&#36827;&#21046;&#25551;&#36848;&#31526;&#12290;ZippyPoint&#26159;&#25105;&#20204;&#30340;&#39640;&#25928;&#37327;&#21270;&#32593;&#32476;&#21644;&#20108;&#36827;&#21046;&#25551;&#36848;&#31526;&#65292;&#23454;&#29616;&#20102;&#22312;&#20960;&#20010;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#21516;&#26102;&#26174;&#31034;&#20986;&#19982;&#31454;&#20105;&#26041;&#27861;&#30456;&#27604;&#25968;&#20493;&#30340;&#26356;&#24555;&#25512;&#29702;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient detection and description of geometric regions in images is a prerequisite in visual systems for localization and mapping. Such systems still rely on traditional hand-crafted methods for efficient generation of lightweight descriptors, a common limitation of the more powerful neural network models that come with high compute and specific hardware requirements. In this paper, we focus on the adaptations required by detection and description neural networks to enable their use in computationally limited platforms such as robots, mobile, and augmented reality devices. To that end, we investigate and adapt network quantization techniques to accelerate inference and enable its use on compute limited platforms. In addition, we revisit common practices in descriptor quantization and propose the use of a binary descriptor normalization layer, enabling the generation of distinctive binary descriptors with a constant number of ones. ZippyPoint, our efficient quantized network with bina
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#26657;&#20934;P&#20540;&#20197;&#21450;&#22914;&#20309;&#36890;&#36807;&#26465;&#20214;&#20998;&#26512;&#25511;&#21046;&#21327;&#21464;&#37327;&#25110;&#20998;&#25968;&#26469;&#35780;&#20272;&#23376;&#24635;&#20307;&#19982;&#20840;&#24635;&#20307;&#20043;&#38388;&#30340;&#21709;&#24212;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2202.00100</link><description>&lt;p&gt;
P&#20540;&#30340;&#26657;&#20934;&#21450;&#23376;&#24635;&#20307;&#19982;&#20840;&#24635;&#20307;&#20559;&#24046;&#30340;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Calibration of P-values for calibration and for deviation of a subpopulation from the full population. (arXiv:2202.00100v7 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.00100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#26657;&#20934;P&#20540;&#20197;&#21450;&#22914;&#20309;&#36890;&#36807;&#26465;&#20214;&#20998;&#26512;&#25511;&#21046;&#21327;&#21464;&#37327;&#25110;&#20998;&#25968;&#26469;&#35780;&#20272;&#23376;&#24635;&#20307;&#19982;&#20840;&#24635;&#20307;&#20043;&#38388;&#30340;&#21709;&#24212;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#24182;&#32508;&#21512;&#20102;&#22810;&#24180;&#30340;&#30740;&#31350;&#65292;&#38416;&#36848;&#20102;&#22914;&#20309;&#26657;&#20934;P&#20540;&#65292;&#20197;&#21450;&#22914;&#20309;&#36890;&#36807;&#26465;&#20214;&#20998;&#26512;&#25511;&#21046;&#21327;&#21464;&#37327;&#25110;&#20998;&#25968;&#26469;&#35780;&#20272;&#23376;&#24635;&#20307;&#19982;&#20840;&#24635;&#20307;&#20043;&#38388;&#30340;&#21709;&#24212;&#24046;&#24322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#35745;&#31639;&#32463;&#36807;&#26657;&#20934;&#30340;P&#20540;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#23545;&#27491;&#24335;&#30340;&#26174;&#33879;&#24615;&#26816;&#39564;&#36827;&#34892;&#24191;&#27867;&#30340;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
The author's recent research papers, "Cumulative deviation of a subpopulation from the full population" and "A graphical method of cumulative differences between two subpopulations" (both published in volume 8 of Springer's open-access "Journal of Big Data" during 2021), propose graphical methods and summary statistics, without extensively calibrating formal significance tests. The summary metrics and methods can measure the calibration of probabilistic predictions and can assess differences in responses between a subpopulation and the full population while controlling for a covariate or score via conditioning on it. These recently published papers construct significance tests based on the scalar summary statistics, but only sketch how to calibrate the attained significance levels (also known as "P-values") for the tests. The present article reviews and synthesizes work spanning many decades in order to detail how to calibrate the P-values. The present paper presents computationally ef
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#23610;&#24230;&#33258;&#36866;&#24212;&#22270;&#31070;&#32463;&#32593;&#32476; (MAGNN) &#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#20013;&#21482;&#33021;&#23398;&#20064;&#21333;&#19968;&#20381;&#36182;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#23610;&#24230;&#37329;&#23383;&#22612;&#32593;&#32476;&#21644;&#33258;&#36866;&#24212;&#22270;&#23398;&#20064;&#27169;&#22359;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#26102;&#38388;&#23610;&#24230;&#19979;&#25512;&#23548;&#20986;&#23610;&#24230;&#29305;&#23450;&#30340;&#20132;&#20114;&#21464;&#37327;&#20381;&#36182;&#24615;&#65292;&#21516;&#26102;&#65292;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#26377;&#25928;&#25429;&#33719;&#26368;&#20855;&#20449;&#24687;&#30340;&#39044;&#27979;&#27169;&#24335;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MAGNN &#23545;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#29616;&#23454;&#25968;&#25454;&#38598;&#30340;&#39044;&#27979;&#25928;&#26524;&#20248;&#20110;&#29616;&#26377;&#26368;&#20248;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2201.04828</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#33258;&#36866;&#24212;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-Scale Adaptive Graph Neural Network for Multivariate Time Series Forecasting. (arXiv:2201.04828v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.04828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#23610;&#24230;&#33258;&#36866;&#24212;&#22270;&#31070;&#32463;&#32593;&#32476; (MAGNN) &#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#20013;&#21482;&#33021;&#23398;&#20064;&#21333;&#19968;&#20381;&#36182;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#23610;&#24230;&#37329;&#23383;&#22612;&#32593;&#32476;&#21644;&#33258;&#36866;&#24212;&#22270;&#23398;&#20064;&#27169;&#22359;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#26102;&#38388;&#23610;&#24230;&#19979;&#25512;&#23548;&#20986;&#23610;&#24230;&#29305;&#23450;&#30340;&#20132;&#20114;&#21464;&#37327;&#20381;&#36182;&#24615;&#65292;&#21516;&#26102;&#65292;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#26377;&#25928;&#25429;&#33719;&#26368;&#20855;&#20449;&#24687;&#30340;&#39044;&#27979;&#27169;&#24335;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MAGNN &#23545;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#29616;&#23454;&#25968;&#25454;&#38598;&#30340;&#39044;&#27979;&#25928;&#26524;&#20248;&#20110;&#29616;&#26377;&#26368;&#20248;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;(MTS)&#39044;&#27979;&#22312;&#26234;&#33021;&#24212;&#29992;&#30340;&#33258;&#21160;&#21270;&#21644;&#20248;&#21270;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#20294;&#20854;&#38656;&#35201;&#32771;&#34385;&#21040;&#22797;&#26434;&#30340; intra-variable &#21644; inter-variable &#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20165;&#36890;&#36807;&#21333;&#19968;&#30340; inter-variable &#30340;&#20381;&#36182;&#20851;&#31995;&#26469;&#23398;&#20064;&#26102;&#38388;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#30340;MTS&#25968;&#25454;&#20013;&#65292;&#23384;&#22312;&#22810;&#23610;&#24230;&#30340;&#26102;&#38388;&#27169;&#24335;&#12290;&#21333;&#20010;&#20132;&#20114;&#21464;&#37327;&#20381;&#36182;&#20851;&#31995;&#20351;&#24471;&#27169;&#22411;&#26356;&#20542;&#21521;&#20110;&#23398;&#20064;&#19968;&#31181;&#31361;&#20986;&#21644;&#20849;&#20139;&#30340;&#26102;&#38388;&#27169;&#24335;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#33258;&#36866;&#24212;&#22270;&#31070;&#32463;&#32593;&#32476;(MAGNN)&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;MAGNN&#21033;&#29992;&#22810;&#23610;&#24230;&#37329;&#23383;&#22612;&#32593;&#32476;&#26469;&#20445;&#30041;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#19979;&#30340;&#22522;&#26412;&#26102;&#38388;&#20381;&#36182;&#24615;&#12290;&#30001;&#20110;&#22312;&#19981;&#21516;&#30340;&#26102;&#38388;&#23610;&#24230;&#19979;&#65292;&#20132;&#20114;&#21464;&#37327;&#20381;&#36182;&#20851;&#31995;&#21487;&#33021;&#19981;&#21516;&#65292;&#22240;&#27492;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#22270;&#23398;&#20064;&#27169;&#22359;&#26469;&#25512;&#23548;&#23610;&#24230;&#29305;&#23450;&#30340;&#20132;&#20114;&#21464;&#37327;&#20381;&#36182;&#24615;&#65292;&#32780;&#19981;&#38656;&#35201;&#39044;&#23450;&#20041;&#20808;&#39564;&#30693;&#35782;&#12290;&#37492;&#20110;&#22810;&#23610;&#24230;&#34920;&#31034;&#65292;MAGNN&#36827;&#19968;&#27493;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#26377;&#25928;&#22320;&#25429;&#33719;&#26368;&#20855;&#20449;&#24687;&#30340;&#39044;&#27979;&#27169;&#24335;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26368;&#20248;&#26041;&#27861;&#30456;&#27604;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;MAGNN&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#29616;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#39044;&#27979;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate time series (MTS) forecasting plays an important role in the automation and optimization of intelligent applications. It is a challenging task, as we need to consider both complex intra-variable dependencies and inter-variable dependencies. Existing works only learn temporal patterns with the help of single inter-variable dependencies. However, there are multi-scale temporal patterns in many real-world MTS. Single inter-variable dependencies make the model prefer to learn one type of prominent and shared temporal patterns. In this paper, we propose a multi-scale adaptive graph neural network (MAGNN) to address the above issue. MAGNN exploits a multi-scale pyramid network to preserve the underlying temporal dependencies at different time scales. Since the inter-variable dependencies may be different under distinct time scales, an adaptive graph learning module is designed to infer the scale-specific inter-variable dependencies without pre-defined priors. Given the multi-sca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31232;&#32570;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#31896;&#25509;&#36807;&#31243;&#35774;&#35745;&#21442;&#25968;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#12290;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#21644;&#22522;&#20110;&#20195;&#29702;&#30340;&#32422;&#26463;&#22788;&#29702;&#25216;&#26415;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#26174;&#24335;&#35780;&#20272;&#32422;&#26463;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#25506;&#32034;&#21442;&#25968;&#31354;&#38388;&#21644;&#24179;&#34913;&#24320;&#21457;&#20043;&#38388;&#25214;&#21040;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2112.08760</link><description>&lt;p&gt;
&#38480;&#21046;&#26465;&#20214;&#19979;&#31232;&#26377;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#36807;&#31243;&#35774;&#35745;&#21442;&#25968;&#22810;&#30446;&#26631;&#20248;&#21270;&#65306;&#19968;&#31181;&#29992;&#20110;&#31896;&#25509;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Constrained multi-objective optimization of process design parameters in settings with scarce data: an application to adhesive bonding. (arXiv:2112.08760v3 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.08760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31232;&#32570;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#31896;&#25509;&#36807;&#31243;&#35774;&#35745;&#21442;&#25968;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#12290;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#21644;&#22522;&#20110;&#20195;&#29702;&#30340;&#32422;&#26463;&#22788;&#29702;&#25216;&#26415;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#26174;&#24335;&#35780;&#20272;&#32422;&#26463;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#25506;&#32034;&#21442;&#25968;&#31354;&#38388;&#21644;&#24179;&#34913;&#24320;&#21457;&#20043;&#38388;&#25214;&#21040;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31896;&#25509;&#26159;&#24037;&#19994;&#20013;&#36234;&#26469;&#36234;&#24120;&#29992;&#30340;&#19968;&#31181;&#25216;&#26415;&#65292;&#22240;&#20854;&#39640;&#24378;&#24230;&#37325;&#37327;&#27604;&#12289;&#35774;&#35745;&#28789;&#27963;&#24615;&#12289;&#24212;&#21147;&#38598;&#20013;&#26377;&#38480;&#12289;&#24179;&#38754;&#21147;&#20256;&#36882;&#12289;&#33391;&#22909;&#30340;&#32784;&#25439;&#24615;&#21644;&#30130;&#21171;&#25239;&#24615;&#31561;&#20248;&#28857;&#32780;&#22791;&#21463;&#38738;&#30544;&#12290;&#23547;&#25214;&#31896;&#25509;&#24037;&#33402;&#30340;&#26368;&#20339;&#21442;&#25968;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65306;&#20248;&#21270;&#26412;&#36136;&#19978;&#26159;&#22810;&#30446;&#26631;&#30340;&#65288;&#26088;&#22312;&#26368;&#22823;&#21270;&#26029;&#35010;&#24378;&#24230;&#24182;&#26368;&#23567;&#21270;&#25104;&#26412;&#65289;&#65292;&#21463;&#38480;&#21046;&#30340;&#65288;&#24037;&#33402;&#19981;&#24212;&#23548;&#33268;&#20219;&#20309;&#26448;&#26009;&#35270;&#35273;&#25439;&#20260;&#65292;&#24212;&#21147;&#27979;&#35797;&#19981;&#24212;&#23548;&#33268;&#40655;&#38468;&#30456;&#20851;&#30340;&#25925;&#38556;&#65289;&#21644;&#19981;&#30830;&#23450;&#30340;&#65288;&#22810;&#27425;&#27979;&#35797;&#21516;&#19968;&#24037;&#33402;&#21442;&#25968;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#30340;&#26029;&#35010;&#24378;&#24230;&#65289;&#12290;&#23454;&#39564;&#23460;&#20013;&#30340;&#30495;&#23454;&#29289;&#29702;&#23454;&#39564;&#24456;&#26114;&#36149;&#12290;&#20256;&#32479;&#30340;&#36827;&#21270;&#26041;&#27861;&#65288;&#20363;&#22914;&#36951;&#20256;&#31639;&#27861;&#65289;&#21017;&#19981;&#36866;&#21512;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22240;&#20026;&#35780;&#20272;&#25152;&#38656;&#30340;&#23454;&#39564;&#25968;&#37327;&#26159;&#24040;&#22823;&#30340;&#12290;&#34429;&#28982;&#36125;&#21494;&#26031;&#20248;&#21270;&#21487;&#20197;&#24110;&#21161;&#20943;&#23569;&#25152;&#38656;&#30340;&#23454;&#39564;&#25968;&#37327;&#65292;&#20294;&#22312;&#31232;&#26377;&#25968;&#25454;&#24773;&#20917;&#19979;&#20351;&#29992;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31896;&#25509;&#30340;&#38480;&#21046;&#26465;&#20214;&#19979;&#31232;&#26377;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#22312;&#25506;&#32034;&#21644;&#24320;&#21457;&#20043;&#38388;&#24179;&#34913;&#25506;&#32034;&#21442;&#25968;&#31354;&#38388;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#30340;&#32422;&#26463;&#22788;&#29702;&#25216;&#26415;&#65292;&#20197;&#22312;&#19981;&#26174;&#24335;&#35780;&#20272;&#32422;&#26463;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#23558;&#20854;&#21253;&#21547;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#12290;&#35813;&#26041;&#27861;&#22312;&#30495;&#23454;&#30340;&#24037;&#19994;&#26696;&#20363;&#30740;&#31350;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#22312;&#20943;&#23569;&#25152;&#38656;&#23454;&#39564;&#30340;&#21516;&#26102;&#21487;&#20197;&#26377;&#25928;&#22320;&#25214;&#21040;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adhesive joints are increasingly used in industry for a wide variety of applications because of their favorable characteristics such as high strength-to-weight ratio, design flexibility, limited stress concentrations, planar force transfer, good damage tolerance, and fatigue resistance. Finding the optimal process parameters for an adhesive bonding process is challenging: the optimization is inherently multi-objective (aiming to maximize break strength while minimizing cost), constrained (the process should not result in any visual damage to the materials, and stress tests should not result in failures that are adhesion-related), and uncertain (testing the same process parameters several times may lead to different break strengths). Real-life physical experiments in the lab are expensive to perform. Traditional evolutionary approaches (such as genetic algorithms) are then ill-suited to solve the problem, due to the prohibitive amount of experiments required for evaluation. Although Bay
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#24179;&#26041;&#26681;&#36895;&#24230;&#20989;&#25968;&#30340;&#26032;&#34920;&#31034;&#26041;&#27861;&#21644;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#21644;&#27604;&#36739;&#26641;&#29366;&#19977;&#32500;&#29289;&#20307;&#65292;&#20174;&#32780;&#25552;&#39640;&#29289;&#20307;&#24418;&#29366;&#24046;&#24322;&#35745;&#31639;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2110.08693</link><description>&lt;p&gt;
&#23398;&#20064;&#26641;&#29366;&#19977;&#32500;&#29289;&#20307;&#30340;&#20960;&#20309;&#21644;&#25299;&#25169;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Generative Models of the Geometry and Topology of Tree-like 3D Objects. (arXiv:2110.08693v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.08693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#24179;&#26041;&#26681;&#36895;&#24230;&#20989;&#25968;&#30340;&#26032;&#34920;&#31034;&#26041;&#27861;&#21644;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#21644;&#27604;&#36739;&#26641;&#29366;&#19977;&#32500;&#29289;&#20307;&#65292;&#20174;&#32780;&#25552;&#39640;&#29289;&#20307;&#24418;&#29366;&#24046;&#24322;&#35745;&#31639;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#20998;&#26512;&#23637;&#29616;&#20986;&#22797;&#26434;&#20960;&#20309;&#21644;&#25299;&#25169;&#21464;&#21270;&#30340;&#35814;&#32454;&#19977;&#32500;&#29983;&#29289;&#29289;&#20307;&#65292;&#20363;&#22914;&#31070;&#32463;&#20803;&#21644;&#26893;&#29289;&#26641;&#65311;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#29992;&#20110;&#34920;&#31034;&#12289;&#27604;&#36739;&#21644;&#35745;&#31639;&#36825;&#20123;&#26641;&#29366;&#19977;&#32500;&#23545;&#35937;&#30340;&#24418;&#29366;&#24046;&#24322;&#65292;&#24182;&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#26469;&#37327;&#21270;&#23558;&#19968;&#20010;&#26641;&#29366;&#29289;&#20307;&#21464;&#24418;&#20026;&#21478;&#19968;&#20010;&#29289;&#20307;&#25152;&#38656;&#30340;&#24367;&#26354;&#12289;&#25289;&#20280;&#21644;&#20998;&#25903;&#28369;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can one analyze detailed 3D biological objects, such as neurons and botanical trees, that exhibit complex geometrical and topological variation? In this paper, we develop a novel mathematical framework for representing, comparing, and computing geodesic deformations between the shapes of such tree-like 3D objects. A hierarchical organization of subtrees characterizes these objects -- each subtree has the main branch with some side branches attached -- and one needs to match these structures across objects for meaningful comparisons. We propose a novel representation that extends the Square-Root Velocity Function (SRVF), initially developed for Euclidean curves, to tree-shaped 3D objects. We then define a new metric that quantifies the bending, stretching, and branch sliding needed to deform one tree-shaped object into the other. Compared to the current metrics, such as the Quotient Euclidean Distance (QED) and the Tree Edit Distance (TED), the proposed representation and metric cap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#33258;&#36866;&#24212;&#26597;&#35810;&#26469;&#26816;&#27979;&#31181;&#26893;&#23376;&#22270;&#23384;&#22312;&#65292;&#30830;&#23450;&#20102;&#24517;&#35201;&#21644;&#20805;&#20998;&#30340;&#26597;&#35810;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2110.00744</link><description>&lt;p&gt;
&#20351;&#29992;&#26597;&#35810;&#26469;&#26816;&#27979;&#38543;&#26426;&#23376;&#22270;
&lt;/p&gt;
&lt;p&gt;
Random Subgraph Detection Using Queries. (arXiv:2110.00744v4 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.00744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#33258;&#36866;&#24212;&#26597;&#35810;&#26469;&#26816;&#27979;&#31181;&#26893;&#23376;&#22270;&#23384;&#22312;&#65292;&#30830;&#23450;&#20102;&#24517;&#35201;&#21644;&#20805;&#20998;&#30340;&#26597;&#35810;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31181;&#26893;&#30340;&#26368;&#23494;&#23376;&#22270;&#26816;&#27979;&#38382;&#39064;&#26159;&#25351;&#27979;&#35797;&#22312;&#32473;&#23450;&#30340;&#65288;&#38543;&#26426;&#65289;&#22270;&#20013;&#26159;&#21542;&#23384;&#22312;&#19968;&#20010;&#24322;&#24120;&#23494;&#38598;&#30340;&#23376;&#22270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#21464;&#20307;&#65292;&#21363;&#21482;&#33021;&#20351;&#29992;&#33258;&#36866;&#24212;&#36793;&#26597;&#35810;&#26469;&#35266;&#23519;&#22270;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#22312;&#36825;&#20010;&#27169;&#22411;&#19979;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26816;&#27979;&#31181;&#26893;&#23376;&#22270;&#23384;&#22312;&#25152;&#38656;&#30340;&#26597;&#35810;&#25968;&#37327;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20219;&#20309;&#65288;&#21487;&#33021;&#26159;&#38543;&#26426;&#21270;&#30340;&#65289;&#31639;&#27861;&#24517;&#39035;&#36827;&#34892; $\mathsf{Q} = \Omega(\frac{n^2}{k^2\chi^4(p||q)}\log^2n)$ &#20010;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
The planted densest subgraph detection problem refers to the task of testing whether in a given (random) graph there is a subgraph that is unusually dense. Specifically, we observe an undirected and unweighted graph on $n$ nodes. Under the null hypothesis, the graph is a realization of an Erd\H{o}s-R\'{e}nyi graph with edge probability (or, density) $q$. Under the alternative, there is a subgraph on $k$ vertices with edge probability $p&gt;q$. The statistical as well as the computational barriers of this problem are well-understood for a wide range of the edge parameters $p$ and $q$. In this paper, we consider a natural variant of the above problem, where one can only observe a small part of the graph using adaptive edge queries.  For this model, we determine the number of queries necessary and sufficient for detecting the presence of the planted subgraph. Specifically, we show that any (possibly randomized) algorithm must make $\mathsf{Q} = \Omega(\frac{n^2}{k^2\chi^4(p||q)}\log^2n)$ ada
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#23478;&#26063;&#35889;&#30340;&#31181;&#32676;&#35757;&#32451;&#65288;GPBT&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#12290;&#19981;&#21516;&#20110;&#20854;&#20182;&#31639;&#27861;&#65292;GPBT&#21033;&#29992;&#30456;&#20851;&#27169;&#22411;&#20043;&#38388;&#30340;&#20849;&#20139;&#21382;&#21490;&#20197;&#39640;&#25928;&#22320;&#21033;&#29992;HP&#21644;&#27169;&#22411;&#20043;&#38388;&#30340;&#32806;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#25552;&#39640;&#31934;&#24230;&#24182;&#20943;&#23567;&#32467;&#26524;&#30340;&#24046;&#24322;&#65292;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/2109.14925</link><description>&lt;p&gt;
&#22522;&#20110;&#23478;&#26063;&#35889;&#30340;&#31181;&#32676;&#35757;&#32451;&#36827;&#34892;&#36229;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Genealogical Population-Based Training for Hyperparameter Optimization. (arXiv:2109.14925v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.14925
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23478;&#26063;&#35889;&#30340;&#31181;&#32676;&#35757;&#32451;&#65288;GPBT&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#12290;&#19981;&#21516;&#20110;&#20854;&#20182;&#31639;&#27861;&#65292;GPBT&#21033;&#29992;&#30456;&#20851;&#27169;&#22411;&#20043;&#38388;&#30340;&#20849;&#20139;&#21382;&#21490;&#20197;&#39640;&#25928;&#22320;&#21033;&#29992;HP&#21644;&#27169;&#22411;&#20043;&#38388;&#30340;&#32806;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#25552;&#39640;&#31934;&#24230;&#24182;&#20943;&#23567;&#32467;&#26524;&#30340;&#24046;&#24322;&#65292;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#65288;HPO&#65289;&#26088;&#22312;&#20197;&#26368;&#24555;&#12289;&#26368;&#39640;&#25928;&#30340;&#26041;&#24335;&#23547;&#25214;&#23398;&#20064;&#27169;&#22411;&#65288;&#22914;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#26368;&#20339;&#36229;&#21442;&#25968;&#65288;HP&#65289;&#12290;&#22823;&#22810;&#25968;&#26368;&#26032;&#30340;HPO&#31639;&#27861;&#23581;&#35797;&#20248;&#21270;HP&#32780;&#19981;&#32771;&#34385;&#33719;&#24471;HP&#30340;&#27169;&#22411;&#65292;&#20551;&#23450;&#23545;&#20110;&#19981;&#21516;&#30340;&#27169;&#22411;&#65292;&#30456;&#21516;&#30340;HP&#23558;&#20135;&#29983;&#38750;&#24120;&#30456;&#20284;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#25171;&#30772;&#20102;&#36825;&#31181;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24335;&#65292;&#31216;&#20026;&#22522;&#22240;&#23478;&#35889;&#31181;&#32676;&#35757;&#32451;&#65288;GPBT&#65289;&#65292;&#36890;&#36807;&#8220;&#35889;&#31995;&#8221;&#30456;&#20851;&#27169;&#22411;&#30340;&#20849;&#20139;&#21382;&#21490;&#65292;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#21033;&#29992;HP&#21644;&#27169;&#22411;&#20043;&#38388;&#30340;&#32806;&#21512;&#12290;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#24403;&#21069;&#31639;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#35745;&#31639;&#25104;&#26412;&#38477;&#20302;&#20102;2&#21040;3&#20493;&#65292;&#36890;&#24120;&#20801;&#35768;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#30340;1%&#31934;&#24230;&#25552;&#39640;&#65292;&#24182;&#23558;&#32467;&#26524;&#30340;&#24046;&#24322;&#38477;&#20302;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25628;&#32034;&#31639;&#27861;&#26080;&#20851;&#65292;&#22240;&#27492;&#20869;&#37096;&#25628;&#32034;&#31243;&#24207;&#21487;&#20197;&#26159;&#20219;&#20309;&#25628;&#32034;&#31639;&#27861;&#65292;&#22914;TPE&#12290;
&lt;/p&gt;
&lt;p&gt;
HyperParameter Optimization (HPO) aims at finding the best HyperParameters (HPs) of learning models, such as neural networks, in the fastest and most efficient way possible. Most recent HPO algorithms try to optimize HPs regardless of the model that obtained them, assuming that for different models, same HPs will produce very similar results. We break free from this paradigm and propose a new take on preexisting methods that we called Genealogical Population Based Training (GPBT). GPBT, via the shared histories of "genealogically"-related models, exploit the coupling of HPs and models in an efficient way. We experimentally demonstrate that our method cuts down by 2 to 3 times the computational cost required, generally allows a 1% accuracy improvement on computer vision tasks, and reduces the variance of the results by an order of magnitude, compared to the current algorithms. Our method is search-algorithm agnostic so that the inner search routine can be any search algorithm like TPE, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23556;&#39057;&#22320;&#22270;&#36827;&#34892;&#23454;&#26102;&#23460;&#22806;&#23450;&#20301;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#31934;&#24230;&#23450;&#20301;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#23454;&#26102;&#24212;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#31181;&#26032;&#30340;&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2106.12556</link><description>&lt;p&gt;
&#22522;&#20110;&#23556;&#39057;&#22320;&#22270;&#30340;&#23454;&#26102;&#23460;&#22806;&#23450;&#20301;&#65306;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Real-time Outdoor Localization Using Radio Maps: A Deep Learning Approach. (arXiv:2106.12556v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.12556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23556;&#39057;&#22320;&#22270;&#36827;&#34892;&#23454;&#26102;&#23460;&#22806;&#23450;&#20301;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#31934;&#24230;&#23450;&#20301;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#23454;&#26102;&#24212;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#31181;&#26032;&#30340;&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#23548;&#33322;&#21355;&#26143;&#31995;&#32479;&#22312;&#22478;&#24066;&#29615;&#22659;&#19979;&#34920;&#29616;&#36890;&#24120;&#36739;&#24046;&#65292;&#22240;&#27492;&#38656;&#35201;&#20351;&#29992;&#20854;&#20182;&#23450;&#20301;&#26041;&#27861;&#26469;&#23454;&#29616;&#36739;&#39640;&#30340;&#23450;&#20301;&#31934;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;LocUNet&#65306;&#19968;&#31181;&#21367;&#31215;&#12289;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#23450;&#20301;&#20219;&#21153;&#65292;&#33021;&#22815;&#20174;&#23569;&#37327;&#22522;&#31449;&#30340;&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;(RSS)&#20272;&#35745;&#29992;&#25143;&#30340;&#20301;&#32622;&#12290;&#20351;&#29992;&#22522;&#31449;&#30340;&#36335;&#24452;&#25439;&#32791;&#23556;&#39057;&#22320;&#22270;&#30340;&#20272;&#35745;&#20540;&#21644;&#24453;&#23450;&#20301;&#29992;&#25143;&#30340;RSS&#27979;&#37327;&#20540;&#65292;LocUNet&#21487;&#20197;&#20197;&#26368;&#20808;&#36827;&#30340;&#31934;&#24230;&#23450;&#20301;&#29992;&#25143;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#36866;&#24212;&#23556;&#39057;&#22320;&#22270;&#20272;&#35745;&#19981;&#20934;&#30830;&#30340;&#24773;&#20917;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#29983;&#25104;&#27599;&#20010;&#29305;&#23450;&#21306;&#22495;&#30340;RSS&#25351;&#32441;&#65292;&#24182;&#36866;&#29992;&#20110;&#23454;&#26102;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#21019;&#24314;&#24182;&#20844;&#24320;&#20102;&#20004;&#31181;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#22312;&#29616;&#23454;&#23460;&#22806;&#22330;&#26223;&#20013;&#30340;RSS&#21644;ToA&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Global Navigation Satellite Systems typically perform poorly in urban environments, where the likelihood of line-of-sight conditions between devices and satellites is low. Therefore, alternative location methods are required to achieve good accuracy. We present LocUNet: A convolutional, end-to-end trained neural network (NN) for the localization task, which is able to estimate the position of a user from the received signal strength (RSS) of a small number of Base Stations (BS). Using estimations of pathloss radio maps of the BSs and the RSS measurements of the users to be localized, LocUNet can localize users with state-of-the-art accuracy and enjoys high robustness to inaccuracies in the estimations of radio maps. The proposed method does not require generating RSS fingerprints of each specific area where the localization task is performed and is suitable for real-time applications. Moreover, two novel datasets that allow for numerical evaluations of RSS and ToA methods in realistic 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23558;&#36125;&#21494;&#26031;&#25512;&#26029;&#20013;&#32447;&#24615;&#21306;&#22495;&#30340;&#33258;&#36866;&#24212;&#25554;&#20540;&#26041;&#27861;&#25512;&#24191;&#21040;&#27425;&#32447;&#24615;&#21306;&#22495;&#65292;&#24314;&#31435;&#20102;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#30340;&#24402;&#19968;&#21270;&#20114;&#20449;&#24687;&#21644;&#26368;&#23567;&#22343;&#26041;&#35823;&#24046;&#30340;&#31934;&#30830;&#28176;&#36817;&#34920;&#36798;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#36817;&#20284;&#20449;&#24687;&#20256;&#36882;&#31639;&#27861;&#20197;&#25509;&#36817;&#26368;&#23567;&#22343;&#26041;&#35823;&#24046;&#22522;&#26412;&#26497;&#38480;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2101.11156</link><description>&lt;p&gt;
&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#20013;&#30340;&#22522;&#26412;&#26497;&#38480;&#21644;&#31639;&#27861;&#19982;&#27425;&#32447;&#24615;&#31232;&#30095;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fundamental limits and algorithms for sparse linear regression with sublinear sparsity. (arXiv:2101.11156v6 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.11156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#36125;&#21494;&#26031;&#25512;&#26029;&#20013;&#32447;&#24615;&#21306;&#22495;&#30340;&#33258;&#36866;&#24212;&#25554;&#20540;&#26041;&#27861;&#25512;&#24191;&#21040;&#27425;&#32447;&#24615;&#21306;&#22495;&#65292;&#24314;&#31435;&#20102;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#30340;&#24402;&#19968;&#21270;&#20114;&#20449;&#24687;&#21644;&#26368;&#23567;&#22343;&#26041;&#35823;&#24046;&#30340;&#31934;&#30830;&#28176;&#36817;&#34920;&#36798;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#36817;&#20284;&#20449;&#24687;&#20256;&#36882;&#31639;&#27861;&#20197;&#25509;&#36817;&#26368;&#23567;&#22343;&#26041;&#35823;&#24046;&#22522;&#26412;&#26497;&#38480;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#27425;&#32447;&#24615;&#31232;&#30095;&#24615;&#21306;&#38388;&#24314;&#31435;&#20102;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#30340;&#24402;&#19968;&#21270;&#20114;&#20449;&#24687;&#21644;&#26368;&#23567;&#22343;&#26041;&#35823;&#24046;&#65288;MMSE&#65289;&#30340;&#31934;&#30830;&#28176;&#36817;&#34920;&#36798;&#24335;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#36125;&#21494;&#26031;&#25512;&#26029;&#20013;&#32447;&#24615;&#21306;&#22495;&#30340;&#33258;&#36866;&#24212;&#25554;&#20540;&#26041;&#27861;&#25512;&#24191;&#21040;&#27425;&#32447;&#24615;&#21306;&#22495;&#26469;&#23454;&#29616;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#30528;&#21517;&#30340;&#36817;&#20284;&#20449;&#24687;&#20256;&#36882;&#31639;&#27861;&#20197;&#25509;&#36817;MMSE&#22522;&#26412;&#26497;&#38480;&#30340;&#26041;&#27861;&#65292;&#24182;&#23545;&#20854;&#29366;&#24577;&#28436;&#21270;&#36827;&#34892;&#20102;&#20005;&#26684;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#31232;&#30095;&#20449;&#21495;&#65292;&#22797;&#21046;&#21644;&#33258;&#36866;&#24212;&#25554;&#20540;&#26041;&#27861;&#20013;&#20449;&#21495;&#32500;&#25968;&#21644;&#35266;&#27979;&#20010;&#25968;&#20043;&#38388;&#30340;&#20256;&#32479;&#32447;&#24615;&#20551;&#35774;&#26159;&#19981;&#24517;&#35201;&#30340;&#12290;&#23427;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#29616;&#26377;&#30340;&#30528;&#21517;&#30340;&#32447;&#24615;&#21306;&#22495;&#30340;AMP&#31639;&#27861;&#20462;&#25913;&#20026;&#27425;&#32447;&#24615;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
We establish exact asymptotic expressions for the normalized mutual information and minimum mean-square-error (MMSE) of sparse linear regression in the sub-linear sparsity regime. Our result is achieved by a generalization of the adaptive interpolation method in Bayesian inference for linear regimes to sub-linear ones. A modification of the well-known approximate message passing algorithm to approach the MMSE fundamental limit is also proposed, and its state evolution is rigorously analyzed. Our results show that the traditional linear assumption between the signal dimension and number of observations in the replica and adaptive interpolation methods is not necessary for sparse signals. They also show how to modify the existing well-known AMP algorithms for linear regimes to sub-linear ones.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#37327;&#23376;&#26230;&#26684;&#27169;&#22411;&#30340;&#33258;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#26126;&#30830;&#36981;&#23432;&#35268;&#33539;&#23545;&#31216;&#24615;&#25110;&#20219;&#24847;&#23376;&#32422;&#26463;&#65292;&#33021;&#22815;&#25552;&#20379;&#31934;&#30830;&#34920;&#31034;&#37327;&#23376;&#26230;&#26684;&#27169;&#22411;&#22522;&#24577;&#21644;&#28608;&#21457;&#24577;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2101.07243</link><description>&lt;p&gt;
&#37327;&#23376;&#26230;&#26684;&#27169;&#22411;&#30340;&#35268;&#33539;&#19981;&#21464;&#24615;&#21644;&#20219;&#24847;&#23376;&#23545;&#31216;&#33258;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Gauge Invariant and Anyonic Symmetric Autoregressive Neural Networks for Quantum Lattice Models. (arXiv:2101.07243v3 [cond-mat.str-el] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.07243
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#37327;&#23376;&#26230;&#26684;&#27169;&#22411;&#30340;&#33258;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#26126;&#30830;&#36981;&#23432;&#35268;&#33539;&#23545;&#31216;&#24615;&#25110;&#20219;&#24847;&#23376;&#32422;&#26463;&#65292;&#33021;&#22815;&#25552;&#20379;&#31934;&#30830;&#34920;&#31034;&#37327;&#23376;&#26230;&#26684;&#27169;&#22411;&#22522;&#24577;&#21644;&#28608;&#21457;&#24577;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#31216;&#24615;&#65292;&#22914;&#35268;&#33539;&#19981;&#21464;&#24615;&#21644;&#20219;&#24847;&#23376;&#23545;&#31216;&#24615;&#65292;&#22312;&#37327;&#23376;&#22810;&#20307;&#29289;&#29702;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#26469;&#26500;&#24314;&#38024;&#23545;&#37327;&#23376;&#26230;&#26684;&#27169;&#22411;&#30340;&#35268;&#33539;&#19981;&#21464;&#24615;&#25110;&#20219;&#24847;&#23376;&#23545;&#31216;&#24615;&#33258;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#21253;&#25324; Transformer &#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#31561;&#21508;&#31181;&#26550;&#26500;&#12290;&#36825;&#20123;&#32593;&#32476;&#21487;&#20197;&#39640;&#25928;&#22320;&#37319;&#26679;&#65292;&#24182;&#26126;&#30830;&#22320;&#36981;&#23432;&#35268;&#33539;&#23545;&#31216;&#24615;&#25110;&#20219;&#24847;&#23376;&#32422;&#26463;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#31934;&#30830;&#34920;&#31034;2D&#21644;3D&#25197;&#26354;&#30721;&#20197;&#21450;X-&#31435;&#26041;&#20307;&#20998;&#24418;&#27169;&#22411;&#30340;&#22522;&#24577;&#21644;&#28608;&#21457;&#24577;&#12290;&#25105;&#20204;&#21464;&#20998;&#20248;&#21270;&#20102;&#25105;&#20204;&#30340;&#23545;&#31216;&#24615;&#21512;&#24182;&#30340;&#33258;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#21508;&#31181;&#27169;&#22411;&#30340;&#22522;&#24577;&#20197;&#21450;&#23454;&#26102;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#27169;&#25311;&#20102; $\text{U(1)}$ &#32593;&#26684;&#35268;&#33539;&#29702;&#35770;&#30340;&#37327;&#23376;&#38142;&#25509;&#27169;&#22411;&#30340;&#21160;&#21147;&#23398;&#21644;&#22522;&#24577;&#65292;&#24471;&#21040;&#20102; 2D $\mathbb{Z}_2$ &#35268;&#33539;&#29702;&#35770;&#30340;&#30456;&#22270;&#65292;&#30830;&#23450;&#20102; $\text{SU(2)}$ &#20020;&#30028;&#38142;&#30340;&#30456;&#21464;&#21644;&#20013;&#24515;&#33655;&#65292;&#20197;&#21450;&#30740;&#31350;&#20102; 2D &#19977;&#35282;&#26230;&#26684;&#30340;&#33258;&#26059;&#27169;&#22411;&#30340;&#28909;&#21270;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symmetries such as gauge invariance and anyonic symmetry play a crucial role in quantum many-body physics. We develop a general approach to constructing gauge invariant or anyonic symmetric autoregressive neural networks, including a wide range of architectures such as Transformer and recurrent neural network, for quantum lattice models. These networks can be efficiently sampled and explicitly obey gauge symmetries or anyonic constraint. We prove that our methods can provide exact representation for the ground and excited states of the 2D and 3D toric codes, and the X-cube fracton model. We variationally optimize our symmetry incorporated autoregressive neural networks for ground states as well as real-time dynamics for a variety of models. We simulate the dynamics and the ground states of the quantum link model of $\text{U(1)}$ lattice gauge theory, obtain the phase diagram for the 2D $\mathbb{Z}_2$ gauge theory, determine the phase transition and the central charge of the $\text{SU(2
&lt;/p&gt;</description></item></channel></rss>