<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#25351;&#20986;&#20256;&#32479;&#30340;&#22270;&#29255;&#20998;&#31867;&#22120;&#23398;&#20064;&#36807;&#31243;&#24573;&#35270;&#27880;&#37322;&#36807;&#31243;&#20013;&#30340;&#36741;&#21161;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#27880;&#37322;&#21103;&#20135;&#21697;&#26469;&#35757;&#32451;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#34394;&#20551;&#30456;&#20851;&#24615;&#24182;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.17595</link><description>&lt;p&gt;
&#34987;&#24573;&#35270;&#30340;&#20813;&#36153;&#21320;&#39184;&#8212;&#8212;&#20351;&#29992;&#27880;&#37322;&#21103;&#20135;&#21697;&#23398;&#20064;&#22270;&#20687;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Neglected Free Lunch -- Learning Image Classifiers Using Annotation Byproducts. (arXiv:2303.17595v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25351;&#20986;&#20256;&#32479;&#30340;&#22270;&#29255;&#20998;&#31867;&#22120;&#23398;&#20064;&#36807;&#31243;&#24573;&#35270;&#27880;&#37322;&#36807;&#31243;&#20013;&#30340;&#36741;&#21161;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#27880;&#37322;&#21103;&#20135;&#21697;&#26469;&#35757;&#32451;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#34394;&#20551;&#30456;&#20851;&#24615;&#24182;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#30417;&#30563;&#23398;&#20064;&#23558;&#20154;&#31867;&#30693;&#35782;&#36890;&#36807;&#22270;&#20687;&#21644;&#30456;&#24212;&#26631;&#31614;&#65288;X&#65292;Y&#65289;&#30340;&#23545;&#24212;&#20851;&#31995;&#36716;&#21270;&#20026;&#21442;&#25968;&#27169;&#22411;&#12290;&#26412;&#25991;&#35748;&#20026;&#36825;&#31181;&#31616;&#21333;&#19988;&#24191;&#27867;&#20351;&#29992;&#30340;&#20154;&#31867;&#30693;&#35782;&#34920;&#31034;&#24573;&#35270;&#20102;&#27880;&#37322;&#36807;&#31243;&#20013;&#20016;&#23500;&#30340;&#36741;&#21161;&#20449;&#24687;&#65292;&#20363;&#22914;&#22312;&#22270;&#20687;&#36873;&#25321;&#21518;&#30041;&#19979;&#30340;&#40736;&#26631;&#36712;&#36857;&#21644;&#28857;&#20987;&#30340;&#26102;&#38388;&#24207;&#21015;&#31561;&#12290;&#25105;&#20204;&#30340;&#27934;&#35265;&#26159;&#65292;&#36825;&#20123;&#27880;&#37322;&#21103;&#20135;&#21697;Z&#25552;&#20379;&#20102;&#36817;&#20284;&#30340;&#20154;&#31867;&#20851;&#27880;&#20449;&#24687;&#65292;&#24369;&#21270;&#20102;&#27169;&#22411;&#23545;&#21069;&#26223;&#32447;&#32034;&#30340;&#20851;&#27880;&#65292;&#20943;&#23569;&#20102;&#34394;&#20551;&#30456;&#20851;&#24615;&#24182;&#38450;&#27490;&#20102;&#25463;&#24452;&#23398;&#20064;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;ImageNet-AB&#21644;COCO-AB&#12290;&#23427;&#20204;&#26159;&#36890;&#36807;&#22797;&#21046;&#30456;&#24212;&#30340;&#21407;&#22987;&#27880;&#37322;&#20219;&#21153;&#26469;&#33719;&#24471;&#30340;ImageNet&#21644;COCO&#35757;&#32451;&#38598;&#65292;&#22686;&#21152;&#20102;&#26679;&#26412;&#32423;&#21035;&#30340;&#27880;&#37322;&#21103;&#20135;&#21697;&#12290;&#25105;&#20204;&#31216;&#20351;&#29992;&#27880;&#37322;&#21103;&#20135;&#21697;&#26469;&#35757;&#32451;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#20026;&#23398;&#20064;&#27880;&#37322;&#21103;&#20135;&#21697;&#65288;LUAB&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#20219;&#21153;&#25439;&#22833;&#65292;&#29992;&#20110;&#21516;&#26102;&#22238;&#24402;Z&#21644;Y&#24050;&#32463;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised learning of image classifiers distills human knowledge into a parametric model through pairs of images and corresponding labels (X,Y). We argue that this simple and widely used representation of human knowledge neglects rich auxiliary information from the annotation procedure, such as the time-series of mouse traces and clicks left after image selection. Our insight is that such annotation byproducts Z provide approximate human attention that weakly guides the model to focus on the foreground cues, reducing spurious correlations and discouraging shortcut learning. To verify this, we create ImageNet-AB and COCO-AB. They are ImageNet and COCO training sets enriched with sample-wise annotation byproducts, collected by replicating the respective original annotation tasks. We refer to the new paradigm of training models with annotation byproducts as learning using annotation byproducts (LUAB). We show that a simple multitask loss for regressing Z together with Y already improves 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#20113;&#30340;&#23398;&#20064;&#26041;&#27861;&#26469;&#25511;&#21046;&#20154;&#19982;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#36882;&#36865;&#36807;&#31243;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#25945;&#24072;-&#23398;&#29983;&#26694;&#26550;&#36827;&#34892;&#20154;&#26426;&#20132;&#20114;&#35757;&#32451;&#65292;&#22312;&#27169;&#25311;&#22522;&#20934;&#12289;&#27169;&#25311;&#21040;&#27169;&#25311;&#36716;&#31227;&#21644;&#27169;&#25311;&#21040;&#30495;&#23454;&#36716;&#31227;&#20013;&#22343;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2303.17592</link><description>&lt;p&gt;
&#22522;&#20110;&#28857;&#20113;&#30340;&#20154;&#26426;&#20132;&#20114;&#26426;&#22120;&#20154;&#36882;&#36865;&#25511;&#21046;&#26041;&#27861;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning Human-to-Robot Handovers from Point Clouds. (arXiv:2303.17592v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17592
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#20113;&#30340;&#23398;&#20064;&#26041;&#27861;&#26469;&#25511;&#21046;&#20154;&#19982;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#36882;&#36865;&#36807;&#31243;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#25945;&#24072;-&#23398;&#29983;&#26694;&#26550;&#36827;&#34892;&#20154;&#26426;&#20132;&#20114;&#35757;&#32451;&#65292;&#22312;&#27169;&#25311;&#22522;&#20934;&#12289;&#27169;&#25311;&#21040;&#27169;&#25311;&#36716;&#31227;&#21644;&#27169;&#25311;&#21040;&#30495;&#23454;&#36716;&#31227;&#20013;&#22343;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#23398;&#20064;&#22522;&#20110;&#35270;&#35273;&#30340;&#20154;&#26426;&#20132;&#20114;&#26426;&#22120;&#20154;&#36882;&#36865;&#25511;&#21046;&#26041;&#27861;&#30340;&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#22312;&#20855;&#36523;&#21270;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#65292;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#35757;&#32451;&#26426;&#22120;&#20154;&#20195;&#29702;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#27169;&#25311;&#20154;&#31867;&#30340;&#22256;&#38590;&#32780;&#19982;&#20154;&#20132;&#20114;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#24320;&#21457;&#20986;&#36924;&#30495;&#30340;&#20154;&#26426;&#20132;&#20114;&#26426;&#22120;&#20154;&#36882;&#36865;&#30340;&#27169;&#25311;&#29615;&#22659;&#12290;&#21033;&#29992;&#36825;&#19968;&#32467;&#26524;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#25945;&#24072;-&#23398;&#29983;&#26694;&#26550;&#36827;&#34892;&#20154;&#26426;&#20132;&#20114;&#35757;&#32451;&#65292;&#20351;&#29992;&#36816;&#21160;&#21644;&#25235;&#25569;&#35268;&#21010;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#33258;&#30417;&#30563;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#22522;&#20934;&#12289;&#27169;&#25311;&#21040;&#27169;&#25311;&#36716;&#31227;&#21644;&#27169;&#25311;&#21040;&#30495;&#23454;&#36716;&#31227;&#20013;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the first framework to learn control policies for vision-based human-to-robot handovers, a critical task for human-robot interaction. While research in Embodied AI has made significant progress in training robot agents in simulated environments, interacting with humans remains challenging due to the difficulties of simulating humans. Fortunately, recent research has developed realistic simulated environments for human-to-robot handovers. Leveraging this result, we introduce a method that is trained with a human-in-the-loop via a two-stage teacher-student framework that uses motion and grasp planning, reinforcement learning, and self-supervision. We show significant performance gains over baselines on a simulation benchmark, sim-to-sim transfer and sim-to-real transfer.
&lt;/p&gt;</description></item><item><title>&#19981;&#24536;&#25105;&#26159;&#19968;&#31181;&#39640;&#25928;&#12289;&#20302;&#25104;&#26412;&#30340;&#35299;&#20915;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#21024;&#38500;&#25351;&#23450;&#30340;&#36523;&#20221;&#12289;&#23545;&#35937;&#25110;&#26679;&#24335;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#35760;&#24518;&#20998;&#25968; (M-Score) &#21644;&#27010;&#24565;&#22522;&#20934; (ConceptBench) &#26469;&#34913;&#37327;&#27169;&#22411;&#29983;&#25104;&#36890;&#29992;&#27010;&#24565;&#30340;&#33021;&#21147;&#12290;&#22312;&#19977;&#20010;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#26174;&#31034;&#20986;&#20102;&#19981;&#24536;&#25105;&#30340;&#26377;&#21069;&#36884;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.17591</link><description>&lt;p&gt;
&#8220;&#19981;&#24536;&#25105;&#8221;&#65306;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36951;&#24536;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Forget-Me-Not: Learning to Forget in Text-to-Image Diffusion Models. (arXiv:2303.17591v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17591
&lt;/p&gt;
&lt;p&gt;
&#19981;&#24536;&#25105;&#26159;&#19968;&#31181;&#39640;&#25928;&#12289;&#20302;&#25104;&#26412;&#30340;&#35299;&#20915;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#21024;&#38500;&#25351;&#23450;&#30340;&#36523;&#20221;&#12289;&#23545;&#35937;&#25110;&#26679;&#24335;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#35760;&#24518;&#20998;&#25968; (M-Score) &#21644;&#27010;&#24565;&#22522;&#20934; (ConceptBench) &#26469;&#34913;&#37327;&#27169;&#22411;&#29983;&#25104;&#36890;&#29992;&#27010;&#24565;&#30340;&#33021;&#21147;&#12290;&#22312;&#19977;&#20010;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#26174;&#31034;&#20986;&#20102;&#19981;&#24536;&#25105;&#30340;&#26377;&#21069;&#36884;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36951;&#24536;&#38382;&#39064;&#26366;&#19968;&#24230;&#26159;&#23398;&#26415;&#30028;&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#65292;&#20294;&#22914;&#20170;&#24050;&#25104;&#20026;&#20135;&#19994;&#30028;&#30340;&#26222;&#36941;&#38382;&#39064;&#12290;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#25216;&#26415;&#30340;&#37325;&#22823;&#36827;&#23637;&#24341;&#21457;&#20102;&#20840;&#29699;&#23545;&#38544;&#31169;&#12289;&#29256;&#26435;&#21644;&#23433;&#20840;&#30340;&#35752;&#35770;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#23398;&#20064;&#20102;&#22823;&#37327;&#26410;&#25480;&#26435;&#30340;&#20010;&#20154;&#36523;&#20221;&#12289;&#20869;&#23481;&#12289;&#33402;&#26415;&#21019;&#20316;&#21644;&#28508;&#22312;&#30340;&#26377;&#23475;&#29289;&#36136;&#65292;&#38543;&#21518;&#29992;&#20110;&#29983;&#25104;&#21644;&#20998;&#21457;&#26080;&#25511;&#21046;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#19981;&#24536;&#25105;&#8221;&#65292;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#12289;&#20302;&#25104;&#26412;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26088;&#22312;&#23433;&#20840;&#22320;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#21024;&#38500;&#25351;&#23450;&#30340;&#36523;&#20221;&#12289;&#23545;&#35937;&#25110;&#26679;&#24335;&#65292;&#21482;&#38656;&#19981;&#21040;30&#31186;&#30340;&#26102;&#38388;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#20854;&#29983;&#25104;&#20854;&#20182;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20043;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#35760;&#24518;&#20998;&#25968; (M-Score)&#8221;&#21644;&#8220;&#27010;&#24565;&#22522;&#20934; (ConceptBench)&#8221;&#26469;&#34913;&#37327;&#27169;&#22411;&#29983;&#25104;&#36890;&#29992;&#27010;&#24565;&#30340;&#33021;&#21147;&#65292;&#20998;&#20026;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#65306;ID&#12289;&#23545;&#35937;&#21644;&#26679;&#24335;&#12290;&#20351;&#29992;M-Score&#21644;ConceptBench&#65292;&#25105;&#20204;&#23545;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#24536;&#35760;&#25105;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35760;&#24518;&#20445;&#30041;&#29575;&#12289;&#22270;&#20687;&#36136;&#37327;&#21644;&#25512;&#29702;&#36895;&#24230;&#31561;&#26041;&#38754;&#30340;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The unlearning problem of deep learning models, once primarily an academic concern, has become a prevalent issue in the industry. The significant advances in text-to-image generation techniques have prompted global discussions on privacy, copyright, and safety, as numerous unauthorized personal IDs, content, artistic creations, and potentially harmful materials have been learned by these models and later utilized to generate and distribute uncontrolled content. To address this challenge, we propose \textbf{Forget-Me-Not}, an efficient and low-cost solution designed to safely remove specified IDs, objects, or styles from a well-configured text-to-image model in as little as 30 seconds, without impairing its ability to generate other content. Alongside our method, we introduce the \textbf{Memorization Score (M-Score)} and \textbf{ConceptBench} to measure the models' capacity to generate general concepts, grouped into three primary categories: ID, object, and style. Using M-Score and Conc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#26435;&#37325;&#26497;&#24615;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#20010;&#24605;&#36335;&#65306;&#21457;&#32946;&#36807;&#31243;&#20250;&#21021;&#22987;&#21270;&#26377;&#20248;&#21183;&#26497;&#24615;&#37197;&#32622;&#30340;&#33258;&#28982;&#26234;&#33021;&#65292;&#24403;&#33258;&#28982;&#26234;&#33021;&#25104;&#38271;&#21644;&#23398;&#20064;&#26102;&#65292;&#31361;&#35302;&#30340;&#22823;&#23567;&#21457;&#29983;&#21464;&#21270;&#65292;&#20294;&#26497;&#24615;&#22522;&#26412;&#20445;&#25345;&#19981;&#21464;&#65292;&#22914;&#26524;&#26435;&#37325;&#26497;&#24615;&#34987;&#36866;&#24403;&#22320;&#35774;&#32622;&#22312;&#20808;&#65292;&#37027;&#20040;&#32593;&#32476;&#23398;&#20064;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#25968;&#25454;&#23558;&#20250;&#20943;&#23569;&#65292;&#20174;&#32780;&#22686;&#21152;&#23398;&#20064;&#21644;&#36716;&#31227;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.17589</link><description>&lt;p&gt;
&#26497;&#24615;&#26159;&#24744;&#23398;&#20064;&#21644;&#24555;&#36895;&#20256;&#36882;&#25152;&#38656;&#30340;&#20840;&#37096;
&lt;/p&gt;
&lt;p&gt;
Polarity is all you need to learn and transfer faster. (arXiv:2303.17589v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#26435;&#37325;&#26497;&#24615;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#20010;&#24605;&#36335;&#65306;&#21457;&#32946;&#36807;&#31243;&#20250;&#21021;&#22987;&#21270;&#26377;&#20248;&#21183;&#26497;&#24615;&#37197;&#32622;&#30340;&#33258;&#28982;&#26234;&#33021;&#65292;&#24403;&#33258;&#28982;&#26234;&#33021;&#25104;&#38271;&#21644;&#23398;&#20064;&#26102;&#65292;&#31361;&#35302;&#30340;&#22823;&#23567;&#21457;&#29983;&#21464;&#21270;&#65292;&#20294;&#26497;&#24615;&#22522;&#26412;&#20445;&#25345;&#19981;&#21464;&#65292;&#22914;&#26524;&#26435;&#37325;&#26497;&#24615;&#34987;&#36866;&#24403;&#22320;&#35774;&#32622;&#22312;&#20808;&#65292;&#37027;&#20040;&#32593;&#32476;&#23398;&#20064;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#25968;&#25454;&#23558;&#20250;&#20943;&#23569;&#65292;&#20174;&#32780;&#22686;&#21152;&#23398;&#20064;&#21644;&#36716;&#31227;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#26234;&#33021;&#22312;&#21160;&#24577;&#19990;&#30028;&#20013;&#33537;&#22766;&#25104;&#38271;&#8212;&#8212;&#23427;&#20204;&#21487;&#20197;&#24456;&#24555;&#22320;&#23398;&#20064;&#65292;&#26377;&#26102;&#20165;&#38656;&#35201;&#23569;&#37327;&#26679;&#26412;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#24037;&#26234;&#33021;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#21644;&#35745;&#31639;&#33021;&#21147;&#25165;&#33021;&#23398;&#20064;&#12290;&#20160;&#20040;&#35774;&#35745;&#21407;&#21017;&#20351;&#24471;&#33258;&#28982;&#26234;&#33021;&#21644;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#23384;&#22312;&#22914;&#27492;&#26126;&#26174;&#30340;&#24046;&#24322;&#65311;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20174;&#26435;&#37325;&#26497;&#24615;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#20010;&#24605;&#36335;&#65306;&#21457;&#32946;&#36807;&#31243;&#20250;&#21021;&#22987;&#21270;&#26377;&#20248;&#21183;&#26497;&#24615;&#37197;&#32622;&#30340;&#33258;&#28982;&#26234;&#33021;&#65292;&#24403;&#33258;&#28982;&#26234;&#33021;&#25104;&#38271;&#21644;&#23398;&#20064;&#26102;&#65292;&#31361;&#35302;&#30340;&#22823;&#23567;&#21457;&#29983;&#21464;&#21270;&#65292;&#20294;&#26497;&#24615;&#22522;&#26412;&#20445;&#25345;&#19981;&#21464;&#12290;&#36890;&#36807;&#27169;&#25311;&#21644;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22914;&#26524;&#26435;&#37325;&#26497;&#24615;&#34987;&#36866;&#24403;&#22320;&#35774;&#32622;&#22312;&#20808;&#65292;&#37027;&#20040;&#32593;&#32476;&#23398;&#20064;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#25968;&#25454;&#23558;&#20250;&#20943;&#23569;&#12290;&#25105;&#20204;&#36824;&#26126;&#30830;&#35828;&#26126;&#20102;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#20808;&#39564;&#35774;&#32622;&#26435;&#37325;&#26497;&#24615;&#20250;&#23545;&#32593;&#32476;&#20135;&#29983;&#19981;&#21033;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20174;&#23398;&#20064;&#30340;&#32479;&#35745;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#35282;&#24230;&#38416;&#36848;&#20102;&#26435;&#37325;&#26497;&#24615;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural intelligences (NIs) thrive in a dynamic world - they learn quickly, sometimes with only a few samples. In contrast, Artificial intelligences (AIs) typically learn with prohibitive amount of training samples and computational power. What design principle difference between NI and AI could contribute to such a discrepancy? Here, we propose an angle from weight polarity: development processes initialize NIs with advantageous polarity configurations; as NIs grow and learn, synapse magnitudes update yet polarities are largely kept unchanged. We demonstrate with simulation and image classification tasks that if weight polarities are adequately set $\textit{a priori}$, then networks learn with less time and data. We also explicitly illustrate situations in which $\textit{a priori}$ setting the weight polarities is disadvantageous for networks. Our work illustrates the value of weight polarities from the perspective of statistical and computational efficiency during learning.
&lt;/p&gt;</description></item><item><title>&#29992;ChatGPT&#20316;&#20026;&#20219;&#21153;&#35268;&#21010;&#24037;&#20855;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#25511;&#21046;&#22120;&#26469;&#25972;&#21512;&#29616;&#26377;&#30340;AI&#27169;&#22411;&#65292;&#35299;&#20915;&#22797;&#26434;&#30340;AI&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.17580</link><description>&lt;p&gt;
HuggingGPT: &#22312;HugingFace&#20013;&#20351;&#29992;ChatGPT&#21450;&#20854;&#20249;&#20276;&#35299;&#20915;AI&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace. (arXiv:2303.17580v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17580
&lt;/p&gt;
&lt;p&gt;
&#29992;ChatGPT&#20316;&#20026;&#20219;&#21153;&#35268;&#21010;&#24037;&#20855;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#25511;&#21046;&#22120;&#26469;&#25972;&#21512;&#29616;&#26377;&#30340;AI&#27169;&#22411;&#65292;&#35299;&#20915;&#22797;&#26434;&#30340;AI&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#21644;&#27169;&#24577;&#30340;&#22797;&#26434;AI&#20219;&#21153;&#26159;&#36890;&#21521;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#25511;&#21046;&#22120;&#26469;&#31649;&#29702;&#29616;&#26377;&#30340;AI&#27169;&#22411;&#20197;&#35299;&#20915;AI&#20219;&#21153;&#65292;&#35821;&#35328;&#25104;&#20026;&#36890;&#29992;&#25509;&#21475;&#26469;&#36171;&#33021;&#23427;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;ChatGPT&#20316;&#20026;&#20219;&#21153;&#35268;&#21010;&#24037;&#20855;&#65292;&#26681;&#25454;HuggingFace&#20013;&#21487;&#29992;&#30340;&#27169;&#22411;&#21151;&#33021;&#25551;&#36848;&#26469;&#36873;&#25321;&#27169;&#22411;&#65292;&#22312;&#36873;&#23450;AI&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#27599;&#20010;&#23376;&#20219;&#21153;&#65292;&#24182;&#24635;&#32467;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence (AGI). While there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks. Considering large language models (LLMs) have exhibited exceptional ability in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this. Based on this philosophy, we present HuggingGPT, a system that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., HuggingFace) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in HuggingFace, execute each subtask with the selected AI model, and summarize the response acco
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#35777;&#26126;&#21487;&#20197;&#26500;&#36896;&#19968;&#20010;&#37096;&#20998;&#27010;&#24565;&#31867;&#21035;&#65292;&#20854;&#26159;&#22312;&#32447;&#21487;&#23398;&#20064;&#30340;&#65292;&#20294;&#20854;&#20219;&#20309;&#25193;&#23637;&#25104;&#20026;&#24635;&#27010;&#24565;&#31867;&#21035;&#21364;&#24182;&#19981;&#22312;&#32447;&#21487;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2303.17578</link><description>&lt;p&gt;
&#22312;&#32447;&#23398;&#20064;&#19982;&#37096;&#20998;&#27010;&#24565;&#31867;&#21035;&#30340;&#28040;&#23696;
&lt;/p&gt;
&lt;p&gt;
Online Learning and Disambiguations of Partial Concept Classes. (arXiv:2303.17578v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17578
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#35777;&#26126;&#21487;&#20197;&#26500;&#36896;&#19968;&#20010;&#37096;&#20998;&#27010;&#24565;&#31867;&#21035;&#65292;&#20854;&#26159;&#22312;&#32447;&#21487;&#23398;&#20064;&#30340;&#65292;&#20294;&#20854;&#20219;&#20309;&#25193;&#23637;&#25104;&#20026;&#24635;&#27010;&#24565;&#31867;&#21035;&#21364;&#24182;&#19981;&#22312;&#32447;&#21487;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;Alon&#12289;Hanneke&#12289;Holzman&#21644;Moran&#65288;FOCS '21&#65289;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#30740;&#31350;&#37096;&#20998;&#27010;&#24565;&#31867;&#21035;&#30340;&#21487;&#23398;&#20064;&#24615;&#12290;&#20182;&#20204;&#30740;&#31350;&#30340;&#20854;&#20013;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#65292;&#37096;&#20998;&#27010;&#24565;&#31867;&#21035;&#30340;&#21487;&#23398;&#20064;&#24615;&#26159;&#21542;&#24635;&#26159;&#26469;&#33258;&#20110;&#23558;&#20854;&#8220;&#25193;&#23637;&#8221;&#21040;&#24635;&#27010;&#24565;&#31867;&#21035;&#30340;&#21487;&#23398;&#20064;&#24615;&#12290;&#20182;&#20204;&#23637;&#31034;&#20102;&#36825;&#19968;&#28857;&#23545;&#20110;PAC&#23398;&#20064;&#19981;&#25104;&#31435;&#65292;&#20294;&#23545;&#20110;&#22312;&#32447;&#23398;&#20064;&#30340;&#26356;&#24378;&#27010;&#24565;&#20182;&#20204;&#23558;&#38382;&#39064;&#30041;&#32473;&#20102;&#35835;&#32773;&#12290;&#26412;&#25991;&#36890;&#36807;&#26500;&#36896;&#19968;&#20010;&#37096;&#20998;&#27010;&#24565;&#31867;&#21035;&#65292;&#35777;&#26126;&#20854;&#26159;&#22312;&#32447;&#21487;&#23398;&#20064;&#30340;&#65292;&#20294;&#20854;&#20219;&#20309;&#25193;&#23637;&#25104;&#20026;&#24635;&#27010;&#24565;&#31867;&#21035;&#21364;&#24182;&#19981;&#22312;&#32447;&#21487;&#23398;&#20064;&#65288;&#21363;&#20351;&#26159;PAC&#21487;&#23398;&#20064;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a recent article, Alon, Hanneke, Holzman, and Moran (FOCS '21) introduced a unifying framework to study the learnability of classes of partial concepts. One of the central questions studied in their work is whether the learnability of a partial concept class is always inherited from the learnability of some ``extension'' of it to a total concept class.  They showed this is not the case for PAC learning but left the problem open for the stronger notion of online learnability.  We resolve this problem by constructing a class of partial concepts that is online learnable, but no extension of it to a class of total concepts is online learnable (or even PAC learnable).
&lt;/p&gt;</description></item><item><title>EWR&#26041;&#27861;&#36890;&#36807;&#36153;&#33293;&#23572;&#20449;&#24687;&#30697;&#38453;&#26435;&#34913;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#20013;&#20010;&#20307;&#21442;&#25968;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#39640;&#23545;&#35805;&#22238;&#22797;&#30340;&#24544;&#23454;&#24615;&#65292;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.17574</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#24544;&#23454;&#21644;&#25277;&#35937;&#21270;&#23545;&#35805;&#29983;&#25104;&#30340;&#24377;&#24615;&#26435;&#37325;&#21435;&#38500;
&lt;/p&gt;
&lt;p&gt;
Elastic Weight Removal for Faithful and Abstractive Dialogue Generation. (arXiv:2303.17574v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17574
&lt;/p&gt;
&lt;p&gt;
EWR&#26041;&#27861;&#36890;&#36807;&#36153;&#33293;&#23572;&#20449;&#24687;&#30697;&#38453;&#26435;&#34913;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#20013;&#20010;&#20307;&#21442;&#25968;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#39640;&#23545;&#35805;&#22238;&#22797;&#30340;&#24544;&#23454;&#24615;&#65292;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#23545;&#35805;&#31995;&#32479;&#24212;&#35813;&#29983;&#25104;&#24544;&#23454;&#20110;&#30456;&#20851;&#25991;&#26723;&#20013;&#21253;&#21547;&#30340;&#30693;&#35782;&#30340;&#22238;&#22797;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#27169;&#22411;&#29983;&#25104;&#20102;&#24187;&#24819;&#30340;&#21709;&#24212;&#65292;&#20854;&#20013;&#21253;&#21547;&#19982;&#20854;&#30456;&#30683;&#30462;&#30340;&#20449;&#24687;&#25110;&#19981;&#21487;&#39564;&#35777;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#19981;&#33391;&#34892;&#20026;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#22312;&#36127;&#38754;&#31034;&#20363;&#19978;&#24494;&#35843;&#8220;&#36127;&#38754;&#19987;&#23478;&#8221;&#65292;&#24182;&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#20013;&#20943;&#21435;&#23427;&#30340;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#30452;&#35273;&#19978;&#65292;&#36825;&#24182;&#27809;&#26377;&#32771;&#34385;&#21040;&#26576;&#20123;&#21442;&#25968;&#27604;&#20854;&#20182;&#21442;&#25968;&#26356;&#36127;&#36131;&#23548;&#33268;&#24187;&#35273;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#65288;&#36817;&#20284;&#65289;&#36153;&#33293;&#23572;&#20449;&#24687;&#30697;&#38453;&#26469;&#26435;&#34913;&#23427;&#20204;&#30340;&#20010;&#20307;&#37325;&#35201;&#24615;&#65292;&#35813;&#30697;&#38453;&#34913;&#37327;&#20854;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#23558;&#27492;&#26041;&#27861;&#31216;&#20026;&#24377;&#24615;&#26435;&#37325;&#21435;&#38500;&#65288;EWR&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;Flan-T5&#19981;&#21516;&#21464;&#20307;&#20316;&#20026;&#39592;&#24178;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#20449;&#24687;&#23547;&#27714;&#23545;&#35805;&#29983;&#25104;&#25968;&#25454;&#38598;&#19978;&#27604;&#36739;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#24544;&#23454;&#24615;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ideally, dialogue systems should generate responses that are faithful to the knowledge contained in relevant documents. However, many models generate hallucinated responses instead that contradict it or contain unverifiable information. To mitigate such undesirable behaviour, it has been proposed to fine-tune a `negative expert' on negative examples and subtract its parameters from those of a pre-trained model. However, intuitively, this does not take into account that some parameters are more responsible than others in causing hallucinations. Thus, we propose to weigh their individual importance via (an approximation of) the Fisher Information matrix, which measures the uncertainty of their estimate. We call this method Elastic Weight Removal (EWR). We evaluate our method -- using different variants of Flan-T5 as a backbone language model -- on multiple datasets for information-seeking dialogue generation and compare our method with state-of-the-art techniques for faithfulness, such a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36828;&#31243;&#35780;&#20272;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#36816;&#21160;&#34920;&#29616;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#37325;&#22797;&#29992;&#20110;&#31867;&#20284;&#30340;&#36816;&#21160;&#20219;&#21153;&#65292;&#25317;&#26377;&#36739;&#39640;&#30340;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17573</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#22312;&#23478;&#20013;&#27979;&#37327;&#24085;&#37329;&#26862;&#30149;&#30340;&#20005;&#37325;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;
Using AI to Measure Parkinson's Disease Severity at Home. (arXiv:2303.17573v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17573
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36828;&#31243;&#35780;&#20272;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#36816;&#21160;&#34920;&#29616;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#37325;&#22797;&#29992;&#20110;&#31867;&#20284;&#30340;&#36816;&#21160;&#20219;&#21153;&#65292;&#25317;&#26377;&#36739;&#39640;&#30340;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36828;&#31243;&#35780;&#20272;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#36816;&#21160;&#34920;&#29616;&#30340;&#26041;&#27861;&#12290;&#21442;&#19982;&#32773;&#22312;&#32593;&#32476;&#25668;&#20687;&#22836;&#21069;&#23436;&#25104;&#20102;&#36816;&#21160;&#20219;&#21153;&#65288;&#21363;&#28857;&#20987;&#25163;&#25351;&#65289;&#65292;250&#21517;&#20840;&#29699;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#25353;&#29031;&#36816;&#21160;&#38556;&#30861;&#21327;&#20250;&#32479;&#19968;&#24085;&#37329;&#26862;&#30149;&#35780;&#20998;&#37327;&#34920; (MDS-UPDRS) &#30340;&#26631;&#20934;&#30001;&#19977;&#21517;&#19987;&#23478;&#31070;&#32463;&#23398;&#23478;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#31070;&#32463;&#23398;&#23478;&#30340;&#35780;&#20272;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#38752;&#24615;&#65292;&#20869;&#37096;&#19968;&#33268;&#24615;&#31995;&#25968;&#65288;ICC&#65289;&#20026;0.88&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#35745;&#31639;&#26426;&#31639;&#27861;&#26469;&#33719;&#24471;&#19982;MDS-UPDRS&#25351;&#21335;&#19968;&#33268;&#19988;&#19982;&#31070;&#32463;&#23398;&#23478;&#30340;&#35780;&#20272;&#39640;&#24230;&#30456;&#20851;&#30340;&#23458;&#35266;&#27979;&#37327;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#36825;&#20123;&#25351;&#26631;&#30340;&#35757;&#32451;&#19979;&#34920;&#29616;&#20248;&#20110;&#19968;&#20010;MDS-UPDRS&#35748;&#35777;&#30340;&#35780;&#20998;&#32773;&#65292;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#20026;0.59&#65292;&#32780;&#35780;&#20998;&#32773;&#30340;MAE&#20026;0.79&#12290;&#28982;&#32780;&#65292;&#35813;&#27169;&#22411;&#30340;&#34920;&#29616;&#30053;&#36874;&#20110;&#19987;&#23478;&#31070;&#32463;&#23398;&#23478;&#65288;0.53 MAE&#65289;&#12290;&#35813;&#26041;&#27861;&#21487;&#37325;&#22797;&#29992;&#20110;&#31867;&#20284;&#30340;&#36816;&#21160;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an artificial intelligence system to remotely assess the motor performance of individuals with Parkinson's disease (PD). Participants performed a motor task (i.e., tapping fingers) in front of a webcam, and data from 250 global participants were rated by three expert neurologists following the Movement Disorder Society Unified Parkinson's Disease Rating Scale (MDS-UPDRS). The neurologists' ratings were highly reliable, with an intra-class correlation coefficient (ICC) of 0.88. We developed computer algorithms to obtain objective measurements that align with the MDS-UPDRS guideline and are strongly correlated with the neurologists' ratings. Our machine learning model trained on these measures outperformed an MDS-UPDRS certified rater, with a mean absolute error (MAE) of 0.59 compared to the rater's MAE of 0.79. However, the model performed slightly worse than the expert neurologists (0.53 MAE). The methodology can be replicated for similar motor tasks, providing the possibili
&lt;/p&gt;</description></item><item><title>CodeGeeX&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;130&#20159;&#21442;&#25968;&#65292;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#12290;&#32463;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;CodeGeeX&#22312;HumanEval-X&#19978;&#30340;&#20195;&#30721;&#29983;&#25104;&#21644;&#32763;&#35793;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;&#27492;&#22806;&#65292;CodeGeeX&#21487;&#20197;&#23558;&#31243;&#24207;&#21592;&#30340;&#29983;&#20135;&#21147;&#25552;&#39640;22%&#12290;</title><link>http://arxiv.org/abs/2303.17568</link><description>&lt;p&gt;
CodeGeeX&#65306;&#22810;&#35821;&#35328;&#35780;&#20272;&#19979;&#30340;&#39044;&#35757;&#32451;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X. (arXiv:2303.17568v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17568
&lt;/p&gt;
&lt;p&gt;
CodeGeeX&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;130&#20159;&#21442;&#25968;&#65292;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#12290;&#32463;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;CodeGeeX&#22312;HumanEval-X&#19978;&#30340;&#20195;&#30721;&#29983;&#25104;&#21644;&#32763;&#35793;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;&#27492;&#22806;&#65292;CodeGeeX&#21487;&#20197;&#23558;&#31243;&#24207;&#21592;&#30340;&#29983;&#20135;&#21147;&#25552;&#39640;22%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;OpenAI Codex&#65289;&#21487;&#20197;&#29983;&#25104;&#27491;&#30830;&#35821;&#27861;&#21644;&#21151;&#33021;&#30340;&#20195;&#30721;&#65292;&#20351;&#31243;&#24207;&#21592;&#30340;&#32534;&#30721;&#26356;&#21152;&#39640;&#25928;&#65292;&#20351;&#25105;&#20204;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#36861;&#27714;&#26356;&#21152;&#36148;&#36817;&#29616;&#23454;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;CodeGeeX&#65292;&#19968;&#20010;&#20855;&#26377;130&#20159;&#21442;&#25968;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#12290;CodeGeeX&#22312;2022&#24180;6&#26376;&#26102;&#22522;&#20110;23&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;8500&#20159;&#20196;&#29260;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;CodeGeeX&#22312;HumanEval-X&#19978;&#30340;&#20195;&#30721;&#29983;&#25104;&#21644;&#32763;&#35793;&#20219;&#21153;&#20013;&#22343;&#20248;&#20110;&#35268;&#27169;&#30456;&#20284;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#27169;&#22411;&#12290;&#22312;HumanEval&#65288;&#20165;&#38480;Python&#65289;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;HumanEval-X&#22522;&#20934;&#27979;&#35797;&#65292;&#36890;&#36807;&#25163;&#20889;C ++&#12289;Java&#12289;JavaScript&#21644;Go&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#35780;&#20272;&#22810;&#35821;&#35328;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;Visual Studio Code&#12289;JetBrains&#21644;Cloud Studio&#19978;&#26500;&#24314;&#20102;&#22522;&#20110;CodeGeeX&#30340;&#25193;&#23637;&#65292;&#27599;&#21608;&#20026;&#25968;&#20197;&#19975;&#35745;&#30340;&#27963;&#36291;&#29992;&#25143;&#29983;&#25104;47&#20159;&#20196;&#29260;&#12290;&#25105;&#20204;&#30340;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;CodeGeeX&#21487;&#20197;&#23558;&#31243;&#24207;&#21592;&#30340;&#29983;&#20135;&#21147;&#25552;&#39640;22%&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained code generation models, such as OpenAI Codex, can generate syntax- and function-correct code, making the coding of programmers more productive and our pursuit of artificial general intelligence closer. In this paper, we introduce CodeGeeX, a multilingual model with 13 billion parameters for code generation. CodeGeeX is pre-trained on 850 billion tokens of 23 programming languages as of June 2022. Our extensive experiments suggest that CodeGeeX outperforms multilingual code models of similar scale for both the tasks of code generation and translation on HumanEval-X. Building upon HumanEval (Python only), we develop the HumanEval-X benchmark for evaluating multilingual models by hand-writing the solutions in C++, Java, JavaScript, and Go. In addition, we build CodeGeeX-based extensions on Visual Studio Code, JetBrains, and Cloud Studio, generating 4.7 billion tokens for tens of thousands of active users per week. Our user study demonstrates that CodeGeeX can help to inc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;BloombergGPT&#65292;&#19968;&#20010;500&#20159;&#21442;&#25968;&#30340;&#37329;&#34701;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#22522;&#20110;Bloomberg&#30340;&#24191;&#27867;&#25968;&#25454;&#26469;&#28304;&#21644;&#36890;&#29992;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#28151;&#21512;&#25968;&#25454;&#38598;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#22312;&#37329;&#34701;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#19981;&#20250;&#29306;&#29298;&#22312;&#26222;&#36890;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.17564</link><description>&lt;p&gt;
BloombergGPT&#65306;&#37329;&#34701;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BloombergGPT: A Large Language Model for Finance. (arXiv:2303.17564v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;BloombergGPT&#65292;&#19968;&#20010;500&#20159;&#21442;&#25968;&#30340;&#37329;&#34701;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#22522;&#20110;Bloomberg&#30340;&#24191;&#27867;&#25968;&#25454;&#26469;&#28304;&#21644;&#36890;&#29992;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#28151;&#21512;&#25968;&#25454;&#38598;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#22312;&#37329;&#34701;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#19981;&#20250;&#29306;&#29298;&#22312;&#26222;&#36890;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#37329;&#34701;&#25216;&#26415;&#39046;&#22495;&#26377;&#30528;&#24191;&#27867;&#32780;&#22797;&#26434;&#30340;&#24212;&#29992;&#65292;&#20174;&#24773;&#24863;&#20998;&#26512;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21040;&#38382;&#31572;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#38750;&#24120;&#26377;&#25928;&#65307;&#28982;&#32780;&#65292;&#19987;&#20026;&#37329;&#34701;&#39046;&#22495;&#35774;&#35745;&#30340;LLM&#23578;&#26410;&#22312;&#25991;&#29486;&#20013;&#25253;&#21578;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BloombergGPT&#65292;&#19968;&#20010;&#25317;&#26377;500&#20159;&#20010;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#26159;&#22522;&#20110;&#24191;&#27867;&#30340;&#37329;&#34701;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;3630&#20159;&#20010;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#22522;&#20110;&#24429;&#21338;&#31038;&#30340;&#24191;&#27867;&#25968;&#25454;&#26469;&#28304;&#65292;&#21487;&#33021;&#26159;&#36804;&#20170;&#26368;&#22823;&#30340;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#21448;&#22686;&#21152;&#20102;&#26469;&#33258;&#36890;&#29992;&#25968;&#25454;&#38598;&#30340;3450&#20159;&#20010;&#26631;&#35760;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;LLM&#22522;&#20934;&#12289;&#24320;&#25918;&#24335;&#37329;&#34701;&#22522;&#20934;&#21644;&#19968;&#22871;&#26368;&#33021;&#20934;&#30830;&#21453;&#26144;&#25105;&#20204;&#39044;&#26399;&#29992;&#36884;&#30340;&#20869;&#37096;&#22522;&#20934;&#19978;&#39564;&#35777;&#20102;BloombergGPT&#12290;&#25105;&#20204;&#30340;&#28151;&#21512;&#25968;&#25454;&#38598;&#35757;&#32451;&#20135;&#29983;&#20102;&#19968;&#20010;&#22312;&#37329;&#34701;&#20219;&#21153;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#19981;&#20250;&#29306;&#29298;&#26222;&#36890;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#27431;&#30431;&#30340;AI&#27861;&#26696;&#26159;&#21542;&#36275;&#20197;&#34920;&#26126;&#22312;&#20854;&#27861;&#24459;&#26694;&#26550;&#20013;&#23384;&#22312;&#25216;&#26415;&#21487;&#35299;&#37322;&#24615;&#26435;&#21033;&#65292;&#24182;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#26159;&#21542;&#38656;&#35201;&#23558;&#20854;&#32435;&#20837;&#29616;&#34892;&#31435;&#27861;&#20013;&#12290;</title><link>http://arxiv.org/abs/2303.17558</link><description>&lt;p&gt;
&#12298;AI&#27861;&#26696;&#25552;&#26696;&#65306;&#19968;&#39033;&#26032;&#30340;&#25216;&#26415;&#21487;&#35299;&#37322;&#24615;&#26435;&#21033;&#65311;&#12299;
&lt;/p&gt;
&lt;p&gt;
The AI Act proposal: a new right to technical interpretability?. (arXiv:2303.17558v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#27431;&#30431;&#30340;AI&#27861;&#26696;&#26159;&#21542;&#36275;&#20197;&#34920;&#26126;&#22312;&#20854;&#27861;&#24459;&#26694;&#26550;&#20013;&#23384;&#22312;&#25216;&#26415;&#21487;&#35299;&#37322;&#24615;&#26435;&#21033;&#65292;&#24182;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#26159;&#21542;&#38656;&#35201;&#23558;&#20854;&#32435;&#20837;&#29616;&#34892;&#31435;&#27861;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#30340;&#35299;&#37322;&#26435;&#38382;&#39064;&#28041;&#21450;&#22823;&#37327;&#25991;&#29486;&#30340;&#35752;&#35770;&#12290;&#22312;&#27861;&#24459;&#23398;&#32773;&#20013;&#65292;&#38598;&#20013;&#20110;&#12298;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#12299;&#20013;&#30340;&#31532;22&#26465;&#65307;&#22312;&#25216;&#26415;&#23398;&#32773;&#20013;&#65292;&#38598;&#20013;&#20110;&#33021;&#22815;&#24110;&#21161;&#35299;&#37322;&#26576;&#20010;&#27169;&#22411;&#36755;&#20986;&#30340;&#25216;&#26415;&#65288;XAI&#65289;&#19978;&#12290;&#26412;&#25991;&#26088;&#22312;&#35843;&#26597;&#12298;AI&#27861;&#26696;&#12299;&#20013;&#24341;&#20837;&#30340;&#26032;&#35268;&#23450;&#19982;&#12298;108&#20844;&#32422;&#12299;&#21644;&#12298;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#12299;&#30456;&#32467;&#21512;&#26159;&#21542;&#36275;&#20197;&#34920;&#26126;&#22312;&#27431;&#30431;&#30340;&#27861;&#24459;&#26694;&#26550;&#20013;&#23384;&#22312;&#25216;&#26415;&#21487;&#35299;&#37322;&#24615;&#26435;&#21033;&#65292;&#22914;&#26524;&#19981;&#26159;&#65292;&#21017;&#27431;&#30431;&#26159;&#21542;&#24212;&#23558;&#20854;&#21253;&#21547;&#22312;&#20854;&#29616;&#34892;&#31435;&#27861;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The debate about the concept of the so called right to explanation in AI is the subject of a wealth of literature. It has focused, in the legal scholarship, on art. 22 GDPR and, in the technical scholarship, on techniques that help explain the output of a certain model (XAI). The purpose of this work is to investigate if the new provisions introduced by the proposal for a Regulation laying down harmonised rules on artificial intelligence (AI Act), in combination with Convention 108 plus and GDPR, are enough to indicate the existence of a right to technical explainability in the EU legal framework and, if not, whether the EU should include it in its current legislation. This is a preliminary work submitted to the online event organised by the Information Society Law Center and it will be later developed into a full paper.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#35760;&#24518;&#21644;&#27867;&#21270;&#20165;&#22312;&#35757;&#32451;&#20013;&#23569;&#27425;&#35266;&#23519;&#21040;&#30340;&#20363;&#23376;&#12290;</title><link>http://arxiv.org/abs/2303.17557</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#35760;&#24518;&#30340;&#35782;&#21035;&#12289;&#22238;&#24518;&#21644;&#20445;&#25345;
&lt;/p&gt;
&lt;p&gt;
Recognition, recall, and retention of few-shot memories in large language models. (arXiv:2303.17557v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#35760;&#24518;&#21644;&#27867;&#21270;&#20165;&#22312;&#35757;&#32451;&#20013;&#23569;&#27425;&#35266;&#23519;&#21040;&#30340;&#20363;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#22312;&#19968;&#20010;&#22823;&#22810;&#25968;&#35757;&#32451;&#26679;&#26412;&#20165;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#30475;&#21040;&#20960;&#27425;&#30340;&#27169;&#24335;&#19979;&#36827;&#34892;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31616;&#21333;&#30340;&#35782;&#21035;&#12289;&#22238;&#24518;&#21644;&#20445;&#25345;&#23454;&#39564;&#65292;&#25506;&#31350;&#27169;&#22411;&#23545;&#20165;&#22312;&#35757;&#32451;&#26399;&#38388;&#23569;&#27425;&#35266;&#23519;&#21040;&#30340;&#26679;&#26412;&#30340;&#35760;&#24518;&#21450;&#20854;&#22312;&#32487;&#32493;&#35757;&#32451;&#26102;&#25345;&#32493;&#30340;&#26102;&#38388;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20165;&#19968;&#20010;&#25509;&#35302;&#36890;&#24120;&#36275;&#20197;&#35753;&#27169;&#22411;&#22312;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35782;&#21035;&#23454;&#39564;&#20013;&#36798;&#21040;&#36817;&#20046;&#23436;&#32654;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20272;&#35745;&#65292;&#22312;&#36830;&#32493;&#35757;&#32451;&#26032;&#26679;&#26412;&#30340;&#20960;&#20010;&#26102;&#26399;&#20869;&#65292;&#27169;&#22411;&#20445;&#25345;&#20102;&#24050;&#35265;&#26679;&#26412;&#30340;&#21487;&#35782;&#21035;&#29305;&#24449;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#35760;&#24518;&#21644;&#27867;&#21270;&#23569;&#26679;&#26412;&#20363;&#23376;&#30340;&#33021;&#21147;&#65292;&#36825;&#19982;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#21331;&#36234;&#34920;&#29616;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
The training of modern large language models (LLMs) takes place in a regime where most training examples are seen only a few times by the model during the course of training. What does a model remember about such examples seen only a few times during training and how long does that memory persist in the face of continuous training with new examples? Here, we investigate these questions through simple recognition, recall, and retention experiments with LLMs. In recognition experiments, we ask if the model can distinguish the seen example from a novel example; in recall experiments, we ask if the model can correctly recall the seen example when cued by a part of it; and in retention experiments, we periodically probe the model's memory for the original examples as the model is trained continuously with new examples. We find that a single exposure is generally sufficient for a model to achieve near perfect accuracy even in very challenging recognition experiments. We estimate that the rec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25209;&#21028;&#24615;&#22238;&#39038;AI&#20844;&#24179;&#24615;&#25991;&#29486;&#20013;30&#31687;&#20132;&#32455;&#24615;&#35752;&#35770;&#65292;&#25581;&#31034;&#30740;&#31350;&#20154;&#21592;&#26222;&#36941;&#32570;&#20047;&#23545;&#20132;&#32455;&#24615;&#30340;&#25972;&#20307;&#29702;&#35299;&#65292;&#20854;&#19968;&#26041;&#38754;&#23558;&#20854;&#32553;&#23567;&#20026;&#22312;&#32676;&#20307;&#23376;&#32452;&#19978;&#36827;&#34892;&#20844;&#24179;&#24230;&#37327;&#30340;&#20248;&#21270;&#65292;&#21478;&#19968;&#26041;&#38754;&#21017;&#22312;&#31038;&#20250;&#32972;&#26223;&#21644;&#26435;&#21147;&#32467;&#26500;&#30340;&#35752;&#35770;&#26041;&#38754;&#23384;&#22312;&#27424;&#32570;&#12290;</title><link>http://arxiv.org/abs/2303.17555</link><description>&lt;p&gt;
&#23545;&#21387;&#36843;&#30697;&#38453;&#30340;&#20998;&#35299;:&#25581;&#31034;&#20132;&#32455;&#24615;&#22312;AI&#20844;&#24179;&#24615;&#20013;&#30340;&#20316;&#29992;&#30340;&#25209;&#21028;&#24615;&#22238;&#39038;&#19982;&#20877;&#24819;&#35937;
&lt;/p&gt;
&lt;p&gt;
Factoring the Matrix of Domination: A Critical Review and Reimagination of Intersectionality in AI Fairness. (arXiv:2303.17555v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25209;&#21028;&#24615;&#22238;&#39038;AI&#20844;&#24179;&#24615;&#25991;&#29486;&#20013;30&#31687;&#20132;&#32455;&#24615;&#35752;&#35770;&#65292;&#25581;&#31034;&#30740;&#31350;&#20154;&#21592;&#26222;&#36941;&#32570;&#20047;&#23545;&#20132;&#32455;&#24615;&#30340;&#25972;&#20307;&#29702;&#35299;&#65292;&#20854;&#19968;&#26041;&#38754;&#23558;&#20854;&#32553;&#23567;&#20026;&#22312;&#32676;&#20307;&#23376;&#32452;&#19978;&#36827;&#34892;&#20844;&#24179;&#24230;&#37327;&#30340;&#20248;&#21270;&#65292;&#21478;&#19968;&#26041;&#38754;&#21017;&#22312;&#31038;&#20250;&#32972;&#26223;&#21644;&#26435;&#21147;&#32467;&#26500;&#30340;&#35752;&#35770;&#26041;&#38754;&#23384;&#22312;&#27424;&#32570;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#32455;&#24615;&#26159;&#19968;&#20010;&#20851;&#38190;&#26694;&#26550;&#65292;&#36890;&#36807;&#35843;&#26597;&#21644;&#23454;&#36341;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#26816;&#26597;&#31038;&#20250;&#19981;&#24179;&#31561;&#22914;&#20309;&#36890;&#36807;&#32467;&#26500;&#21644;&#32426;&#24459;&#39046;&#22495;&#25345;&#32493;&#23384;&#22312;&#12290;&#22312;AI&#20844;&#24179;&#30340;&#29702;&#24565;&#20013;&#65292;&#8220;&#20844;&#24179;&#24615;&#8221;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#25105;&#20204;&#35748;&#20026;&#37319;&#29992;&#20132;&#32455;&#24615;&#20316;&#20026;&#20998;&#26512;&#26694;&#26550;&#23545;&#20110;&#26377;&#25928;&#22320;&#23454;&#29616;&#20844;&#24179;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#23545;AI&#20844;&#24179;&#25991;&#29486;&#20013;30&#31687;&#20851;&#20110;&#20132;&#32455;&#24615;&#30340;&#35752;&#35770;&#36827;&#34892;&#25209;&#21028;&#24615;&#22238;&#39038;&#65292;&#25105;&#20204;&#24402;&#32435;&#21644;&#28436;&#32462;&#20986;:1)&#20132;&#32455;&#24615;&#25351;&#23548;&#22914;&#20309;&#22312;AI&#20844;&#24179;&#33539;&#20363;&#20013;&#25805;&#20316;&#65292;2)&#25581;&#31034;&#20132;&#32455;&#24615;&#30340;&#27010;&#24565;&#21270;&#21644;&#23454;&#29616;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30740;&#31350;&#20154;&#21592;&#26222;&#36941;&#23558;&#20132;&#32455;&#24615;&#32553;&#20943;&#20026;&#38024;&#23545;&#20154;&#21475;&#20122;&#32452;&#30340;&#20844;&#24179;&#25351;&#26631;&#36827;&#34892;&#20248;&#21270;&#12290;&#20182;&#20204;&#20063;&#26410;&#33021;&#35752;&#35770;&#23427;&#20204;&#30340;&#31038;&#20250;&#32972;&#26223;&#65292;&#24403;&#25552;&#21040;&#26435;&#21147;&#26102;&#65292;&#20182;&#20204;&#20027;&#35201;&#23558;&#20854;&#32622;&#20110;AI&#27969;&#31243;&#20013;&#12290;&#25105;&#20204;&#23558;&#36827;&#19968;&#27493;&#38416;&#36848;&#24182;&#35780;&#20272;&#36825;&#20123;&#24046;&#36317;&#23545;&#20110;&#20020;&#24202;&#30740;&#31350;&#21644;&#23454;&#36341;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intersectionality is a critical framework that, through inquiry and praxis, allows us to examine how social inequalities persist through domains of structure and discipline. Given AI fairness' raison d'\^etre of ``fairness,'' we argue that adopting intersectionality as an analytical framework is pivotal to effectively operationalizing fairness. Through a critical review of how intersectionality is discussed in 30 papers from the AI fairness literature, we deductively and inductively: 1) map how intersectionality tenets operate within the AI fairness paradigm and 2) uncover gaps between the conceptualization and operationalization of intersectionality. We find that researchers overwhelmingly reduce intersectionality to optimizing for fairness metrics over demographic subgroups. They also fail to discuss their social context and when mentioning power, they mostly situate it only within the AI pipeline. We: 3) outline and assess the implications of these gaps for critical inquiry and prax
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35843;&#26597;&#39640;&#36136;&#37327;&#30340;&#20844;&#20849;&#27665;&#24847;&#35843;&#26597;&#26469;&#21019;&#24314;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;OpinionsQA&#65292;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#21453;&#26144;&#30340;&#35266;&#28857;&#19982;60&#20010;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#32452;&#30340;&#35266;&#28857;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#21457;&#29616;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#21453;&#26144;&#30340;&#35266;&#28857;&#19982;&#32654;&#22269;&#20154;&#32676;&#32452;&#30340;&#35266;&#28857;&#23384;&#22312;&#24040;&#22823;&#24046;&#24322;&#65292;&#29978;&#33267;&#36890;&#36807;&#26126;&#30830;&#35843;&#25972;LM&#21453;&#26144;&#20986;&#30340;&#35266;&#28857;&#65292;&#20173;&#28982;&#26080;&#27861;&#28040;&#38500;&#12290;</title><link>http://arxiv.org/abs/2303.17548</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21453;&#26144;&#20102;&#35841;&#30340;&#35266;&#28857;&#65311;
&lt;/p&gt;
&lt;p&gt;
Whose Opinions Do Language Models Reflect?. (arXiv:2303.17548v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35843;&#26597;&#39640;&#36136;&#37327;&#30340;&#20844;&#20849;&#27665;&#24847;&#35843;&#26597;&#26469;&#21019;&#24314;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;OpinionsQA&#65292;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#21453;&#26144;&#30340;&#35266;&#28857;&#19982;60&#20010;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#32452;&#30340;&#35266;&#28857;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#21457;&#29616;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#21453;&#26144;&#30340;&#35266;&#28857;&#19982;&#32654;&#22269;&#20154;&#32676;&#32452;&#30340;&#35266;&#28857;&#23384;&#22312;&#24040;&#22823;&#24046;&#24322;&#65292;&#29978;&#33267;&#36890;&#36807;&#26126;&#30830;&#35843;&#25972;LM&#21453;&#26144;&#20986;&#30340;&#35266;&#28857;&#65292;&#20173;&#28982;&#26080;&#27861;&#28040;&#38500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#24320;&#25918;&#29615;&#22659;&#20013;&#34987;&#20351;&#29992;&#65292;&#22312;&#38024;&#23545;&#20027;&#35266;&#26597;&#35810;&#30340;&#21709;&#24212;&#20013;&#21453;&#26144;&#30340;&#35266;&#28857;&#21487;&#33021;&#20250;&#23545;&#29992;&#25143;&#28385;&#24847;&#24230;&#20135;&#29983;&#28145;&#36828;&#24433;&#21709;&#65292;&#21516;&#26102;&#20063;&#21487;&#33021;&#22609;&#36896;&#25972;&#20010;&#31038;&#20250;&#30340;&#35266;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23450;&#37327;&#26694;&#26550;&#65292;&#20197;&#35843;&#26597;LM&#21453;&#26144;&#30340;&#35266;&#28857;&#12290;&#25105;&#20204;&#21033;&#29992;&#39640;&#36136;&#37327;&#30340;&#20844;&#20849;&#27665;&#24847;&#35843;&#26597;&#21644;&#30456;&#20851;&#30340;&#20154;&#31867;&#21453;&#24212;&#26469;&#21019;&#24314;OpinionsQA&#65292;&#24182;&#23545;60&#20010;&#32654;&#22269;&#20154;&#21475;&#32479;&#35745;&#32452;&#30340;&#24847;&#35265;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#28041;&#21450;&#20174;&#22549;&#32974;&#21040;&#33258;&#21160;&#21270;&#30340;&#21508;&#31181;&#35805;&#39064;&#12290;&#22312;&#21508;&#20010;&#35805;&#39064;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;LM&#21453;&#26144;&#30340;&#35266;&#28857;&#19982;&#32654;&#22269;&#20154;&#32676;&#32452;&#20043;&#38388;&#23384;&#22312;&#37325;&#22823;&#24046;&#24322;&#65292;&#36825;&#19982;&#27665;&#20027;&#20826;&#21644;&#20849;&#21644;&#20826;&#22312;&#27668;&#20505;&#21464;&#21270;&#38382;&#39064;&#19978;&#30340;&#20998;&#27495;&#24046;&#19981;&#22810;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21363;&#20351;&#26126;&#30830;&#23558;LM&#23450;&#21521;&#20110;&#29305;&#23450;&#30340;&#20154;&#21475;&#32479;&#35745;&#32452;&#65292;&#36825;&#31181;&#24046;&#24322;&#20173;&#28982;&#23384;&#22312;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#19981;&#20165;&#30830;&#35748;&#20102;&#20808;&#21069;&#23545;&#24038;&#20542;&#20542;&#21521;&#30340;&#35266;&#23519;&#32467;&#26524;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#36825;&#31181;&#24046;&#24322;&#30340;&#19968;&#20010;&#20840;&#26032;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) are increasingly being used in open-ended contexts, where the opinions reflected by LMs in response to subjective queries can have a profound impact, both on user satisfaction, as well as shaping the views of society at large. In this work, we put forth a quantitative framework to investigate the opinions reflected by LMs -- by leveraging high-quality public opinion polls and their associated human responses. Using this framework, we create OpinionsQA, a new dataset for evaluating the alignment of LM opinions with those of 60 US demographic groups over topics ranging from abortion to automation. Across topics, we find substantial misalignment between the views reflected by current LMs and those of US demographic groups: on par with the Democrat-Republican divide on climate change. Notably, this misalignment persists even after explicitly steering the LMs towards particular demographic groups. Our analysis not only confirms prior observations about the left-leaning
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#32467;&#26500;&#21644;&#22806;&#35266;&#37197;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#23545;&#35937;&#32423;&#22270;&#20687;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#31934;&#32454;&#25511;&#21046;&#22270;&#20687;&#20013;&#30340;&#19981;&#21516;&#23545;&#35937;&#23646;&#24615;&#65292;&#21516;&#26102;&#33258;&#21160;&#20256;&#25773;&#27880;&#20837;&#30340;&#22806;&#35266;&#21040;&#20855;&#26377;&#30456;&#20284;&#32467;&#26500;&#30340;&#23545;&#35937;&#12290;</title><link>http://arxiv.org/abs/2303.17546</link><description>&lt;p&gt;
PAIR-Diffusion: &#37319;&#29992;&#32467;&#26500;&#21644;&#22806;&#35266;&#37197;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#23545;&#35937;&#32423;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
PAIR-Diffusion: Object-Level Image Editing with Structure-and-Appearance Paired Diffusion Models. (arXiv:2303.17546v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#32467;&#26500;&#21644;&#22806;&#35266;&#37197;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#23545;&#35937;&#32423;&#22270;&#20687;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#31934;&#32454;&#25511;&#21046;&#22270;&#20687;&#20013;&#30340;&#19981;&#21516;&#23545;&#35937;&#23646;&#24615;&#65292;&#21516;&#26102;&#33258;&#21160;&#20256;&#25773;&#27880;&#20837;&#30340;&#22806;&#35266;&#21040;&#20855;&#26377;&#30456;&#20284;&#32467;&#26500;&#30340;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#22270;&#20687;&#32534;&#36753;&#21457;&#23637;&#36805;&#36895;&#12290;&#20197;&#21069;&#30340;&#20316;&#21697;&#21487;&#20197;&#36890;&#36807;&#21508;&#31181;&#26041;&#24335;&#36827;&#34892;&#25511;&#21046;&#21644;&#32534;&#36753;&#22270;&#20687;&#65292;&#26576;&#20123;&#20316;&#21697;&#20351;&#29992;&#39640;&#32423;&#26465;&#20214;&#65288;&#20363;&#22914;&#25991;&#26412;&#65289;&#65292;&#32780;&#20854;&#20182;&#20316;&#21697;&#20351;&#29992;&#20302;&#32423;&#26465;&#20214;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20316;&#21697;&#32570;&#20047;&#23545;&#22270;&#20687;&#20013;&#19981;&#21516;&#23545;&#35937;&#30340;&#23646;&#24615;&#36827;&#34892;&#31934;&#32454;&#21270;&#25511;&#21046;&#65292;&#21363;&#23545;&#35937;&#32423;&#22270;&#20687;&#32534;&#36753;&#12290;&#26412;&#25991;&#23558;&#22270;&#20687;&#35270;&#20026;&#30001;&#22810;&#20010;&#23545;&#35937;&#32452;&#25104;&#65292;&#27599;&#20010;&#23545;&#35937;&#30001;&#19981;&#21516;&#23646;&#24615;&#23450;&#20041;&#12290;&#25105;&#20204;&#21457;&#29616;&#32467;&#26500;&#21644;&#22806;&#35266;&#26159;&#26368;&#30452;&#35266;&#19988;&#26368;&#26377;&#29992;&#20110;&#32534;&#36753;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#32467;&#26500;&#21644;&#22806;&#35266;&#37197;&#23545;&#25193;&#25955;&#27169;&#22411;&#65288;PAIR-Diffusion&#65289;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#20174;&#22270;&#20687;&#20013;&#26126;&#30830;&#25552;&#21462;&#30340;&#32467;&#26500;&#21644;&#22806;&#35266;&#20449;&#24687;&#36827;&#34892;&#35757;&#32451;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20351;&#29992;&#25143;&#33021;&#22815;&#22312;&#23545;&#35937;&#21644;&#20840;&#23616;&#32423;&#21035;&#23558;&#21442;&#32771;&#22270;&#20687;&#30340;&#22806;&#35266;&#27880;&#20837;&#36755;&#20837;&#22270;&#20687;&#20013;&#12290;&#27492;&#22806;&#65292;PAIR-Diffusion&#33258;&#21160;&#23558;&#27880;&#20837;&#30340;&#22806;&#35266;&#20256;&#25773;&#21040;&#36755;&#20837;&#22270;&#20687;&#20013;&#20855;&#26377;&#31867;&#20284;&#32467;&#26500;&#30340;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image editing using diffusion models has witnessed extremely fast-paced growth recently. There are various ways in which previous works enable controlling and editing images. Some works use high-level conditioning such as text, while others use low-level conditioning. Nevertheless, most of them lack fine-grained control over the properties of the different objects present in the image, i.e. object-level image editing. In this work, we consider an image as a composition of multiple objects, each defined by various properties. Out of these properties, we identify structure and appearance as the most intuitive to understand and useful for editing purposes. We propose Structure-and-Appearance Paired Diffusion model (PAIR-Diffusion), which is trained using structure and appearance information explicitly extracted from the images. The proposed model enables users to inject a reference image's appearance into the input image at both the object and global levels. Additionally, PAIR-Diffusion a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#35299;&#20915;&#37327;&#23376;&#35745;&#31639;&#20013;&#30340;&#20445;&#30495;&#24230;&#38382;&#39064;&#65292;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#39044;&#27979;&#37327;&#23376;&#30005;&#36335;&#30340;&#20445;&#30495;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.17523</link><description>&lt;p&gt;
&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#25552;&#39640;&#37327;&#23376;&#30005;&#36335;&#20445;&#30495;&#24230;
&lt;/p&gt;
&lt;p&gt;
Quantum Circuit Fidelity Improvement with Long Short-Term Memory Networks. (arXiv:2303.17523v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#35299;&#20915;&#37327;&#23376;&#35745;&#31639;&#20013;&#30340;&#20445;&#30495;&#24230;&#38382;&#39064;&#65292;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#39044;&#27979;&#37327;&#23376;&#30005;&#36335;&#30340;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#24050;&#36827;&#20837;&#22122;&#22768;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#65288;NISQ&#65289;&#26102;&#20195;&#65292;&#30446;&#21069;&#25105;&#20204;&#25317;&#26377;&#30340;&#37327;&#23376;&#22788;&#29702;&#22120;&#23545;&#36752;&#23556;&#21644;&#28201;&#24230;&#31561;&#29615;&#22659;&#21464;&#37327;&#25935;&#24863;&#65292;&#22240;&#27492;&#20250;&#20135;&#29983;&#22024;&#26434;&#30340;&#36755;&#20986;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#35768;&#22810;&#31639;&#27861;&#21644;&#24212;&#29992;&#31243;&#24207;&#29992;&#20110;NISQ&#22788;&#29702;&#22120;&#65292;&#20294;&#25105;&#20204;&#20173;&#38754;&#20020;&#30528;&#35299;&#37322;&#20854;&#22024;&#26434;&#32467;&#26524;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23545;&#25152;&#36873;&#25321;&#30340;&#37327;&#23376;&#24577;&#26377;&#22810;&#23569;&#20449;&#24515;&#65311;&#36825;&#31181;&#20449;&#24515;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;NISQ&#35745;&#31639;&#26426;&#23558;&#36755;&#20986;&#20854;&#37327;&#23376;&#20301;&#27979;&#37327;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#26377;&#26102;&#24456;&#38590;&#21306;&#20998;&#20998;&#24067;&#26159;&#21542;&#34920;&#31034;&#26377;&#24847;&#20041;&#30340;&#35745;&#31639;&#25110;&#21482;&#26159;&#38543;&#26426;&#22122;&#22768;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23558;&#37327;&#23376;&#30005;&#36335;&#20445;&#30495;&#24230;&#39044;&#27979;&#26694;&#26550;&#20026;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#65292;&#22240;&#27492;&#21487;&#20197;&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#19968;&#20010;&#23436;&#25972;&#30340;&#24037;&#20316;&#27969;&#31243;&#26469;&#26500;&#24314;&#35757;&#32451;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Quantum computing has entered the Noisy Intermediate-Scale Quantum (NISQ) era. Currently, the quantum processors we have are sensitive to environmental variables like radiation and temperature, thus producing noisy outputs. Although many proposed algorithms and applications exist for NISQ processors, we still face uncertainties when interpreting their noisy results. Specifically, how much confidence do we have in the quantum states we are picking as the output? This confidence is important since a NISQ computer will output a probability distribution of its qubit measurements, and it is sometimes hard to distinguish whether the distribution represents meaningful computation or just random noise. This paper presents a novel approach to attack this problem by framing quantum circuit fidelity prediction as a Time Series Forecasting problem, therefore making it possible to utilize the power of Long Short-Term Memory (LSTM) neural networks. A complete workflow to build the training circuit d
&lt;/p&gt;</description></item><item><title>Pgx&#26159;&#19968;&#20010;&#29992;JAX&#32534;&#20889;&#30340;&#28216;&#25103;&#27169;&#25311;&#22120;&#38598;&#21512;&#65292;&#20855;&#26377;&#24378;&#21270;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#33021;&#21147;&#65292;&#25903;&#25345;&#24182;&#34892;&#25191;&#34892;&#65292;&#36895;&#24230;&#27604;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#24211;&#24555;10&#20493;&#12290; &#23427;&#23454;&#29616;&#20102;Backgammon&#65292;Shogi&#21644;Go&#31561;&#22522;&#20934;&#27979;&#35797;&#28216;&#25103;&#12290;</title><link>http://arxiv.org/abs/2303.17503</link><description>&lt;p&gt;
Pgx:&#24378;&#21270;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#30340;&#24182;&#34892;&#28216;&#25103;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
Pgx: Hardware-accelerated parallel game simulation for reinforcement learning. (arXiv:2303.17503v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17503
&lt;/p&gt;
&lt;p&gt;
Pgx&#26159;&#19968;&#20010;&#29992;JAX&#32534;&#20889;&#30340;&#28216;&#25103;&#27169;&#25311;&#22120;&#38598;&#21512;&#65292;&#20855;&#26377;&#24378;&#21270;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#33021;&#21147;&#65292;&#25903;&#25345;&#24182;&#34892;&#25191;&#34892;&#65292;&#36895;&#24230;&#27604;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#24211;&#24555;10&#20493;&#12290; &#23427;&#23454;&#29616;&#20102;Backgammon&#65292;Shogi&#21644;Go&#31561;&#22522;&#20934;&#27979;&#35797;&#28216;&#25103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Pgx&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;JAX&#32534;&#20889;&#30340;&#26827;&#30424;&#28216;&#25103;&#27169;&#25311;&#22120;&#38598;&#21512;&#12290;&#30001;&#20110;JAX&#30340;&#33258;&#21160;&#21521;&#37327;&#21270;&#21644;&#21363;&#26102;&#32534;&#35793;&#21151;&#33021;&#65292;Pgx&#26131;&#20110;&#22312;GPU/TPU&#21152;&#36895;&#22120;&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#24182;&#34892;&#25191;&#34892;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#21333;&#20010;A100 GPU&#19978;&#30340;Pgx&#27169;&#25311;&#27604;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#24211;&#24555;10&#20493;&#12290;Pgx&#23454;&#29616;&#20102;&#34987;&#35748;&#20026;&#26159;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#28216;&#25103;&#65292;&#22914;Backgammon&#65292;Shogi&#21644;Go&#12290; Pgx&#21487;&#22312;https://github.com/sotetsuk/pgx&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Pgx, a collection of board game simulators written in JAX. Thanks to auto-vectorization and Just-In-Time compilation of JAX, Pgx scales easily to thousands of parallel execution on GPU/TPU accelerators. We found that the simulation of Pgx on a single A100 GPU is 10x faster than that of existing reinforcement learning libraries. Pgx implements games considered vital benchmarks in artificial intelligence research, such as Backgammon, Shogi, and Go. Pgx is available at https://github.com/sotetsuk/pgx.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#27668;&#20505;&#27169;&#22411;&#22810;&#23610;&#24230;&#23376;&#32593;&#26684;&#21442;&#25968;&#21270;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#23376;&#32593;&#26684;&#24378;&#36843;&#20540;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17496</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#27668;&#20505;&#27169;&#22411;&#23376;&#32593;&#26684;&#21442;&#25968;&#21270;&#22810;&#23610;&#24230;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Data-driven multiscale modeling of subgrid parameterizations in climate models. (arXiv:2303.17496v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17496
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#27668;&#20505;&#27169;&#22411;&#22810;&#23610;&#24230;&#23376;&#32593;&#26684;&#21442;&#25968;&#21270;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#23376;&#32593;&#26684;&#24378;&#36843;&#20540;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23376;&#32593;&#26684;&#21442;&#25968;&#21270;&#26159;&#30446;&#21069;&#27668;&#20505;&#27169;&#22411;&#20013;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#29992;&#20110;&#34920;&#31034;&#20302;&#20110;&#27169;&#22411;&#20998;&#36776;&#29575;&#19979;&#21457;&#29983;&#30340;&#29289;&#29702;&#36807;&#31243;&#65292;&#20174;&#32780;&#20135;&#29983;&#20934;&#30830;&#30340;&#38271;&#26399;&#27668;&#20505;&#39044;&#27979;&#12290;&#24050;&#32463;&#23581;&#35797;&#20102;&#22810;&#31181;&#26041;&#27861;&#26469;&#35774;&#35745;&#36825;&#20123;&#32452;&#20214;&#65292;&#21253;&#25324;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#26041;&#27861;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#39044;&#27979;&#38382;&#39064;&#30340;&#27010;&#24565;&#39564;&#35777;&#12290;&#25105;&#20204;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#27979;&#35797;&#27169;&#22411;&#19978;&#30340;&#23376;&#32593;&#26684;&#24378;&#36843;&#20540;&#65292;&#24182;&#30740;&#31350;&#22312;&#31934;&#32454;-&#31895;&#31961;&#21644;&#31895;&#31961;-&#31934;&#32454;&#26041;&#21521;&#19978;&#20351;&#29992;&#39069;&#22806;&#20449;&#24687;&#21487;&#20197;&#33719;&#24471;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Subgrid parameterizations, which represent physical processes occurring below the resolution of current climate models, are an important component in producing accurate, long-term predictions for the climate. A variety of approaches have been tested to design these components, including deep learning methods. In this work, we evaluate a proof of concept illustrating a multiscale approach to this prediction problem. We train neural networks to predict subgrid forcing values on a testbed model and examine improvements in prediction accuracy that can be obtained by using additional information in both fine-to-coarse and coarse-to-fine directions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#26041;&#26696;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25191;&#34892;&#35745;&#31639;&#26426;&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#24182;&#22312;MiniWoB++&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.17491</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#35299;&#20915;&#35745;&#31639;&#26426;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Language Models can Solve Computer Tasks. (arXiv:2303.17491v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#26041;&#26696;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25191;&#34892;&#35745;&#31639;&#26426;&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#24182;&#22312;MiniWoB++&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#22312;&#35745;&#31639;&#26426;&#19978;&#25191;&#34892;&#36890;&#29992;&#20219;&#21153;&#30340;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#33258;&#21160;&#21270;&#37325;&#22797;&#20219;&#21153;&#21644;&#21327;&#21161;&#22797;&#26434;&#38382;&#39064;&#30340;&#35299;&#20915;&#26469;&#25552;&#39640;&#25928;&#29575;&#21644;&#29983;&#20135;&#21147;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#20195;&#29702;&#24212;&#35813;&#33021;&#22815;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#35299;&#20915;&#26032;&#30340;&#35745;&#31639;&#26426;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#19987;&#23478;&#31034;&#33539;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#36825;&#20004;&#32773;&#23545;&#20110;&#26032;&#20219;&#21153;&#26469;&#35828;&#37117;&#19981;&#20999;&#23454;&#38469;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#21487;&#20197;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#26041;&#26696;&#65288;RCI&#65289;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#23548;&#25191;&#34892;&#35745;&#31639;&#26426;&#20219;&#21153;&#65292;&#24182;&#22312;&#25209;&#35780;&#21644;&#25913;&#36827;&#36755;&#20986;&#30340;&#36807;&#31243;&#20013;&#21462;&#24471;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;RCI&#26041;&#27861;&#22312;&#33258;&#21160;&#21270;&#35745;&#31639;&#26426;&#20219;&#21153;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;LLM&#26041;&#27861;&#65292;&#24182;&#22312;MiniWoB++&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;&#30417;&#30563;&#23398;&#20064;&#65288;SL&#65289;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#12290;RCI&#26041;&#27861;&#20351;&#29992;&#27599;&#20010;&#20219;&#21153;&#20165;&#26377;&#30340;&#23569;&#25968;&#31034;&#33539;&#65292;&#19982;&#26368;&#26032;&#30340;SL+RL&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agents capable of carrying out general tasks on a computer can improve efficiency and productivity by automating repetitive tasks and assisting in complex problem-solving. Ideally, such agents should be able to solve new computer tasks presented to them through natural language commands. However, previous approaches to this problem require large amounts of expert demonstrations and task-specific reward functions, both of which are impractical for new tasks. In this work, we show that a pre-trained large language model (LLM) agent can execute computer tasks guided by natural language using a simple prompting scheme where the agent recursively criticizes and improves its output (RCI). The RCI approach significantly outperforms existing LLM methods for automating computer tasks and surpasses supervised learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++ benchmark. RCI is competitive with the state-of-the-art SL+RL method, using only a handful of demonstrations per ta
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20195;&#20215;&#25935;&#24863;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;CSGNN&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#31227;&#21160;&#31038;&#20132;&#32593;&#32476;&#27450;&#35784;&#26816;&#27979;&#20013;&#30340;&#22270;&#20687;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#20010;&#24320;&#28304;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2303.17486</link><description>&lt;p&gt;
&#22522;&#20110;&#20195;&#20215;&#25935;&#24863;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31227;&#21160;&#31038;&#20132;&#32593;&#32476;&#27450;&#35784;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Cost Sensitive GNN-based Imbalanced Learning for Mobile Social Network Fraud Detection. (arXiv:2303.17486v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20195;&#20215;&#25935;&#24863;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;CSGNN&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#31227;&#21160;&#31038;&#20132;&#32593;&#32476;&#27450;&#35784;&#26816;&#27979;&#20013;&#30340;&#22270;&#20687;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#20010;&#24320;&#28304;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31227;&#21160;&#32593;&#32476;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20154;&#20204;&#30340;&#31038;&#20132;&#32852;&#31995;&#24471;&#21040;&#20102;&#26497;&#22823;&#30340;&#20415;&#21033;&#12290;&#28982;&#32780;&#65292;&#31227;&#21160;&#31038;&#20132;&#32593;&#32476;&#27450;&#35784;&#30340;&#20852;&#36215;&#32473;&#20154;&#20204;&#24102;&#26469;&#20102;&#24040;&#22823;&#22256;&#25200;&#65292;&#21487;&#33021;&#36896;&#25104;&#20010;&#20154;&#21644;&#31038;&#20250;&#36130;&#23500;&#30340;&#25439;&#22833;&#65292;&#24182;&#28508;&#22312;&#22320;&#23545;&#32463;&#27982;&#36896;&#25104;&#37325;&#22823;&#25439;&#23475;&#12290;&#20026;&#20102;&#26816;&#27979;&#27450;&#35784;&#29992;&#25143;&#65292;&#24191;&#27867;&#20351;&#29992;&#21453;&#26144;&#29992;&#25143;&#31227;&#21160;&#32593;&#32476;&#20013;&#31038;&#20132;&#34892;&#20026;&#30340;&#36890;&#35805;&#35814;&#21333;&#35760;&#24405;&#65288;CDR&#65289;&#25968;&#25454;&#12290;&#20294;&#26159;&#65292;&#19978;&#36848;&#25968;&#25454;&#20013;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#21487;&#33021;&#20005;&#37325;&#38459;&#30861;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#27450;&#35784;&#26816;&#27979;&#22120;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#20960;&#20046;&#27809;&#26377;&#24471;&#21040;&#35299;&#20915;&#12290;&#26412;&#25991;&#23558;&#21019;&#36896;&#24615;&#22320;&#32467;&#21512;&#20195;&#20215;&#25935;&#24863;&#23398;&#20064;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20195;&#20215;&#25935;&#24863;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;CSGNN&#65289;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#24320;&#28304;&#23454;&#38469;&#31227;&#21160;&#32593;&#32476;&#27450;&#35784;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;CSGNN&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#31227;&#21160;&#31038;&#20132;&#32593;&#32476;&#27450;&#35784;&#26816;&#27979;&#20013;&#30340;&#22270;&#20687;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#22312;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#27169;&#22411;&#19978;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of mobile networks, the people's social contacts have been considerably facilitated. However, the rise of mobile social network fraud upon those networks, has caused a great deal of distress, in case of depleting personal and social wealth, then potentially doing significant economic harm. To detect fraudulent users, call detail record (CDR) data, which portrays the social behavior of users in mobile networks, has been widely utilized. But the imbalance problem in the aforementioned data, which could severely hinder the effectiveness of fraud detectors based on graph neural networks(GNN), has hardly been addressed in previous work. In this paper, we are going to present a novel Cost-Sensitive Graph Neural Network (CSGNN) by creatively combining cost-sensitive learning and graph neural networks. We conduct extensive experiments on two open-source realworld mobile network fraud datasets. The results show that CSGNN can effectively solve the graph imbalance prob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#19977;&#21521;&#22240;&#26524;&#23646;&#24615;&#20559;&#24207;&#32467;&#26500;&#65288;3WCAPOS&#65289;&#30340;&#26041;&#27861;&#65292;&#23558;&#20559;&#24207;&#27491;&#24335;&#32467;&#26500;&#20998;&#26512;&#65288;POFSA&#65289;&#28436;&#21464;&#20026;&#22240;&#26524;&#35206;&#30422;&#65292;&#20174;&#32780;&#22686;&#24378;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20998;&#31867;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#22240;&#26524;&#22240;&#23376;&#65288;CF&#65289;&#27010;&#24565;&#35780;&#20272;&#23646;&#24615;&#21644;&#20915;&#31574;&#23646;&#24615;&#20043;&#38388;&#30340;&#22240;&#26524;&#30456;&#20851;&#24615;&#65292;&#24182;&#32467;&#21512;&#19977;&#21521;&#20915;&#31574;&#26500;&#24314;3WCAPOS&#65292;&#20351;&#24471;&#32467;&#26500;&#20013;&#33410;&#28857;&#30340;&#32431;&#24230;&#26356;&#28165;&#26224;&#65292;&#32423;&#21035;&#20043;&#38388;&#30340;&#21464;&#21270;&#26356;&#26126;&#26174;&#12290;</title><link>http://arxiv.org/abs/2303.17482</link><description>&lt;p&gt;
&#19977;&#21521;&#22240;&#26524;&#23646;&#24615;&#20559;&#24207;&#32467;&#26500;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Three-way causal attribute partial order structure analysis. (arXiv:2303.17482v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#19977;&#21521;&#22240;&#26524;&#23646;&#24615;&#20559;&#24207;&#32467;&#26500;&#65288;3WCAPOS&#65289;&#30340;&#26041;&#27861;&#65292;&#23558;&#20559;&#24207;&#27491;&#24335;&#32467;&#26500;&#20998;&#26512;&#65288;POFSA&#65289;&#28436;&#21464;&#20026;&#22240;&#26524;&#35206;&#30422;&#65292;&#20174;&#32780;&#22686;&#24378;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20998;&#31867;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#22240;&#26524;&#22240;&#23376;&#65288;CF&#65289;&#27010;&#24565;&#35780;&#20272;&#23646;&#24615;&#21644;&#20915;&#31574;&#23646;&#24615;&#20043;&#38388;&#30340;&#22240;&#26524;&#30456;&#20851;&#24615;&#65292;&#24182;&#32467;&#21512;&#19977;&#21521;&#20915;&#31574;&#26500;&#24314;3WCAPOS&#65292;&#20351;&#24471;&#32467;&#26500;&#20013;&#33410;&#28857;&#30340;&#32431;&#24230;&#26356;&#28165;&#26224;&#65292;&#32423;&#21035;&#20043;&#38388;&#30340;&#21464;&#21270;&#26356;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#24207;&#27491;&#24335;&#32467;&#26500;&#20998;&#26512;&#65288;POFSA&#65289;&#20316;&#20026;&#19968;&#31181;&#26032;&#20852;&#30340;&#35748;&#30693;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#30693;&#35782;&#22788;&#29702;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#19977;&#21521;&#22240;&#26524;&#23646;&#24615;&#20559;&#24207;&#32467;&#26500;&#65288;3WCAPOS&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#20174;&#38598;&#21512;&#35206;&#30422;&#21521;&#22240;&#26524;&#35206;&#30422;&#28436;&#21464;&#65292;&#20197;&#22686;&#21152;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20998;&#31867;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25552;&#20986;&#20102;&#22240;&#26524;&#22240;&#23376;&#65288;CF&#65289;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#35780;&#20272;&#24418;&#24335;&#20915;&#31574;&#29615;&#22659;&#20013;&#23646;&#24615;&#21644;&#20915;&#31574;&#23646;&#24615;&#20043;&#38388;&#30340;&#22240;&#26524;&#30456;&#20851;&#24615;&#12290;&#28982;&#21518;&#65292;&#23558;CF&#19982;&#23646;&#24615;&#20559;&#24207;&#32467;&#26500;&#30456;&#32467;&#21512;&#65292;&#23450;&#20041;&#20102;&#22240;&#26524;&#23646;&#24615;&#20559;&#24207;&#32467;&#26500;&#30340;&#27010;&#24565;&#65292;&#20351;&#24471;&#38598;&#21512;&#35206;&#30422;&#28436;&#21464;&#25104;&#22240;&#26524;&#35206;&#30422;&#12290;&#26368;&#21518;&#65292;&#32467;&#21512;&#19977;&#21521;&#20915;&#31574;&#30340;&#24605;&#24819;&#65292;&#24418;&#25104;&#20102;3WCAPOS&#65292;&#20351;&#32467;&#26500;&#20013;&#33410;&#28857;&#30340;&#32431;&#24230;&#26356;&#28165;&#26224;&#65292;&#32423;&#21035;&#20043;&#38388;&#30340;&#21464;&#21270;&#26356;&#26126;&#26174;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#20174;&#20998;&#31867;&#33021;&#21147;&#20986;&#21457;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
As an emerging concept cognitive learning model, partial order formal structure analysis (POFSA) has been widely used in the field of knowledge processing. In this paper, we propose the method named three-way causal attribute partial order structure (3WCAPOS) to evolve the POFSA from set coverage to causal coverage in order to increase the interpretability and classification performance of the model. First, the concept of causal factor (CF) is proposed to evaluate the causal correlation between attributes and decision attributes in the formal decision context. Then, combining CF with attribute partial order structure, the concept of causal attribute partial order structure is defined and makes set coverage evolve into causal coverage. Finally, combined with the idea of three-way decision, 3WCAPOS is formed, which makes the purity of nodes in the structure clearer and the changes between levels more obviously. In addition, the experiments are carried out from the classification ability 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#36895;&#29575;&#31639;&#27861;&#20013;&#35745;&#31639;&#24310;&#36831;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20943;&#23569;&#35745;&#31639;&#24310;&#36831;&#24182;&#25552;&#39640;&#31639;&#27861;&#21709;&#24212;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.17477</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#36895;&#29575;&#31639;&#27861;&#20013;&#35745;&#31639;&#24310;&#36831;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
On the Analysis of Computational Delays in Reinforcement Learning-based Rate Adaptation Algorithms. (arXiv:2303.17477v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#36895;&#29575;&#31639;&#27861;&#20013;&#35745;&#31639;&#24310;&#36831;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20943;&#23569;&#35745;&#31639;&#24310;&#36831;&#24182;&#25552;&#39640;&#31639;&#27861;&#21709;&#24212;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20010;&#30740;&#31350;&#24037;&#20316;&#24050;&#32463;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#35299;&#20915;Wi-Fi&#32593;&#32476;&#20013;&#30340;&#36895;&#29575;&#33258;&#36866;&#24212;&#38382;&#39064;&#12290;&#26080;&#32447;&#30005;&#38142;&#36335;&#30340;&#21160;&#24577;&#29305;&#24615;&#35201;&#27714;&#31639;&#27861;&#23545;&#38142;&#25509;&#36136;&#37327;&#30340;&#21464;&#21270;&#20570;&#20986;&#21453;&#24212;&#12290;&#31639;&#27861;&#25191;&#34892;&#20013;&#30340;&#24310;&#36831;&#21487;&#33021;&#20250;&#23545;&#20854;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#65292;&#24182;&#36827;&#19968;&#27493;&#38477;&#20302;&#32593;&#32476;&#24615;&#33021;&#12290;&#36825;&#19968;&#26041;&#38754;&#22312;&#29616;&#26377;&#30740;&#31350;&#20013;&#34987;&#24573;&#35270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24120;&#35265;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#35745;&#31639;&#24310;&#36831;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#24212;&#29992;&#20110;&#20943;&#23569;&#36825;&#20123;&#35745;&#31639;&#24310;&#36831;&#24182;&#22686;&#21152;&#27492;&#31867;&#31639;&#27861;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#29616;&#26377;&#36895;&#29575;&#33258;&#36866;&#24212;&#31639;&#27861;&#12290;&#25152;&#24471;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#31639;&#27861;&#25191;&#34892;&#26102;&#38388;&#20943;&#23569;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#25552;&#39640;&#20102;&#20854;&#23545;&#38142;&#25509;&#36136;&#37327;&#21464;&#21270;&#30340;&#21709;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several research works have applied Reinforcement Learning (RL) algorithms to solve the Rate Adaptation (RA) problem in Wi-Fi networks. The dynamic nature of the radio link requires the algorithms to be responsive to changes in link quality. Delays in the execution of the algorithm may be detrimental to its performance, which in turn may decrease network performance. This aspect has been overlooked in the state of the art. In this paper, we present an analysis of common computational delays in RL-based RA algorithms, and propose a methodology that may be applied to reduce these computational delays and increase the efficiency of this type of algorithms. We apply the proposed methodology to an existing RL-based RA algorithm. The obtained experimental results indicate a reduction of one order of magnitude in the execution time of the algorithm, improving its responsiveness to link quality changes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#34920;&#31034;&#65288;&#23884;&#20837;&#65289;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32447;&#24615;&#26102;&#38388;&#20272;&#35745;softmax&#24402;&#19968;&#21270;&#24120;&#25968;&#26469;&#23454;&#29616;&#23398;&#20064;&#36807;&#31243;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#36127;&#37319;&#26679;&#26041;&#27861;&#24182;&#22312;&#22810;&#39033;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17475</link><description>&lt;p&gt;
&#36229;&#36234;&#36127;&#37319;&#26679;&#30340;&#39640;&#25928;&#20998;&#24067;&#24335;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient distributed representations beyond negative sampling. (arXiv:2303.17475v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#34920;&#31034;&#65288;&#23884;&#20837;&#65289;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32447;&#24615;&#26102;&#38388;&#20272;&#35745;softmax&#24402;&#19968;&#21270;&#24120;&#25968;&#26469;&#23454;&#29616;&#23398;&#20064;&#36807;&#31243;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#36127;&#37319;&#26679;&#26041;&#27861;&#24182;&#22312;&#22810;&#39033;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#23398;&#20064;&#20998;&#24067;&#24335;&#34920;&#31034;&#65288;&#20063;&#31216;&#20026;&#23884;&#20837;&#65289;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#19968;&#20010;&#31867;&#20284;&#20110;Word2Vec&#31639;&#27861;&#20013;&#24341;&#20837;&#24182;&#22312;&#22810;&#20010;&#24037;&#20316;&#20013;&#37319;&#29992;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#23454;&#29616;&#12290;&#20248;&#21270;&#35745;&#31639;&#30340;&#29942;&#39048;&#26159;softmax&#24402;&#19968;&#21270;&#24120;&#25968;&#30340;&#35745;&#31639;&#65292;&#36825;&#38656;&#35201;&#19982;&#26679;&#26412;&#22823;&#23567;&#21576;&#20108;&#27425;&#27604;&#20363;&#30340;&#25805;&#20316;&#25968;&#12290;&#36825;&#31181;&#22797;&#26434;&#24230;&#19981;&#36866;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#25152;&#20197;&#36127;&#37319;&#26679;&#26159;&#19968;&#20010;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19982;&#26679;&#26412;&#22823;&#23567;&#32447;&#24615;&#30456;&#20851;&#30340;&#26102;&#38388;&#20869;&#33719;&#24471;&#20998;&#24067;&#24335;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36127;&#37319;&#26679;&#20250;&#25913;&#21464;&#25439;&#22833;&#20989;&#25968;&#65292;&#22240;&#27492;&#35299;&#20915;&#30340;&#26159;&#19982;&#26368;&#21021;&#25552;&#20986;&#30340;&#19981;&#21516;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#22312;&#20110;&#23637;&#31034;&#22914;&#20309;&#36890;&#36807;&#32447;&#24615;&#26102;&#38388;&#20272;&#35745;softmax&#24402;&#19968;&#21270;&#24120;&#25968;&#65292;&#20174;&#32780;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20248;&#21270;&#31574;&#30053;&#26469;&#23398;&#20064;&#20998;&#24067;&#24335;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23884;&#20837;&#36136;&#37327;&#21644;&#35757;&#32451;&#26102;&#38388;&#26041;&#38754;&#20248;&#20110;&#36127;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article describes an efficient method to learn distributed representations, also known as embeddings. This is accomplished minimizing an objective function similar to the one introduced in the Word2Vec algorithm and later adopted in several works. The optimization computational bottleneck is the calculation of the softmax normalization constants for which a number of operations scaling quadratically with the sample size is required. This complexity is unsuited for large datasets and negative sampling is a popular workaround, allowing one to obtain distributed representations in linear time with respect to the sample size. Negative sampling consists, however, in a change of the loss function and hence solves a different optimization problem from the one originally proposed. Our contribution is to show that the sotfmax normalization constants can be estimated in linear time, allowing us to design an efficient optimization strategy to learn distributed representations. We test our ap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26367;&#20195;&#27169;&#22411;&#26469;&#20248;&#21270;&#39640;&#20445;&#30495;&#20223;&#30495;&#30340;&#26032;&#26041;&#27861;&#65292;&#38024;&#23545;&#35745;&#31639;&#23494;&#38598;&#22411;&#21644;&#38750;&#32447;&#24615;&#30340;&#36712;&#36857;&#35268;&#21010;&#20223;&#30495;&#65292;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#25214;&#21040;&#26368;&#20339;&#21442;&#32771;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2303.17468</link><description>&lt;p&gt;
&#29992;&#26367;&#20195;&#31070;&#32463;&#32593;&#32476;&#26469;&#36827;&#34892;&#39640;&#25928;&#30340;&#22522;&#20110;&#20223;&#30495;&#36712;&#36857;&#35268;&#21010;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Surrogate Neural Networks for Efficient Simulation-based Trajectory Planning Optimization. (arXiv:2303.17468v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26367;&#20195;&#27169;&#22411;&#26469;&#20248;&#21270;&#39640;&#20445;&#30495;&#20223;&#30495;&#30340;&#26032;&#26041;&#27861;&#65292;&#38024;&#23545;&#35745;&#31639;&#23494;&#38598;&#22411;&#21644;&#38750;&#32447;&#24615;&#30340;&#36712;&#36857;&#35268;&#21010;&#20223;&#30495;&#65292;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#25214;&#21040;&#26368;&#20339;&#21442;&#32771;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#26367;&#20195;&#27169;&#22411;&#26469;&#20943;&#23569;&#21442;&#32771;&#36335;&#24452;&#30340;&#20223;&#30495;&#20248;&#21270;&#30340;&#35745;&#31639;&#26102;&#38388;&#12290;&#22312;&#27809;&#26377;&#31995;&#32479;&#30340;&#35299;&#26512;&#24418;&#24335;&#65292;&#21482;&#26377;&#36755;&#20837;&#36755;&#20986;&#25968;&#25454;&#21487;&#20197;&#29992;&#20110;&#21019;&#24314;&#20223;&#30495;&#26367;&#20195;&#27169;&#22411;&#26102;&#65292;&#38656;&#35201;&#36827;&#34892;&#22522;&#20110;&#20223;&#30495;&#30340;&#20248;&#21270;&#12290;&#20687;&#35768;&#22810;&#39640;&#20445;&#30495;&#20223;&#30495;&#19968;&#26679;&#65292;&#36825;&#20010;&#36712;&#36857;&#35268;&#21010;&#20223;&#30495;&#26159;&#38750;&#24120;&#38750;&#32447;&#24615;&#21644;&#35745;&#31639;&#23494;&#38598;&#22411;&#65292;&#20351;&#24471;&#36845;&#20195;&#20248;&#21270;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25214;&#21040;&#20102;&#38477;&#33853;&#39640;&#36229;&#22768;&#36895;&#39134;&#34892;&#22120;&#30340;&#26368;&#20339;&#21442;&#32771;&#36335;&#24452;&#12290;&#19982;&#20043;&#21069;&#25991;&#29486;&#20013;&#29992;&#20110;&#21019;&#24314;&#26367;&#20195;&#27169;&#22411;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19987;&#38376;&#35774;&#35745;&#20026;&#26368;&#23567;&#21270;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#22120;&#25152;&#38656;&#30340;&#20223;&#30495;&#25191;&#34892;&#27425;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#27604;&#25163;&#21160;&#35843;&#25972;&#36755;&#20837;&#21442;&#25968;&#30340;&#26631;&#20934;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel methodology that uses surrogate models in the form of neural networks to reduce the computation time of simulation-based optimization of a reference trajectory. Simulation-based optimization is necessary when there is no analytical form of the system accessible, only input-output data that can be used to create a surrogate model of the simulation. Like many high-fidelity simulations, this trajectory planning simulation is very nonlinear and computationally expensive, making it challenging to optimize iteratively. Through gradient descent optimization, our approach finds the optimal reference trajectory for landing a hypersonic vehicle. In contrast to the large datasets used to create the surrogate models in prior literature, our methodology is specifically designed to minimize the number of simulation executions required by the gradient descent optimizer. We demonstrated this methodology to be more efficient than the standard practice of hand-tuning the inpu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#24230;&#37327;&#26041;&#27861;&#29992;&#20110;&#35780;&#20272;&#19994;&#21153;&#36807;&#31243;&#20223;&#30495;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#20197;&#35780;&#20272;&#20854;&#22797;&#21046;&#36807;&#31243;&#35266;&#23519;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.17463</link><description>&lt;p&gt;
&#25105;&#33021;&#30456;&#20449;&#25105;&#30340;&#20223;&#30495;&#27169;&#22411;&#21527;&#65311;&#27979;&#37327;&#19994;&#21153;&#36807;&#31243;&#20223;&#30495;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can I Trust My Simulation Model? Measuring the Quality of Business Process Simulation Models. (arXiv:2303.17463v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#24230;&#37327;&#26041;&#27861;&#29992;&#20110;&#35780;&#20272;&#19994;&#21153;&#36807;&#31243;&#20223;&#30495;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#20197;&#35780;&#20272;&#20854;&#22797;&#21046;&#36807;&#31243;&#35266;&#23519;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19994;&#21153;&#36807;&#31243;&#20223;&#30495;(BPS)&#26159;&#19968;&#31181;&#20998;&#26512;&#19981;&#21516;&#22330;&#26223;&#19979;&#19994;&#21153;&#27969;&#31243;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290; BPS&#20801;&#35768;&#25105;&#20204;&#20272;&#35745;&#22914;&#26524;&#19968;&#20010;&#25110;&#22810;&#20010;&#36164;&#28304;&#19981;&#21487;&#29992;&#65292;&#36807;&#31243;&#30340;&#21608;&#26399;&#26102;&#38388;&#20250;&#26159;&#22810;&#23569;&#12290; BPS&#30340;&#36215;&#28857;&#26159;&#19968;&#20010;&#27880;&#37322;&#26377;&#20223;&#30495;&#21442;&#25968;&#30340;&#36807;&#31243;&#27169;&#22411;&#65288;BPS&#27169;&#22411;&#65289;&#12290; BPS&#27169;&#22411;&#21487;&#20197;&#25163;&#21160;&#35774;&#35745;&#65292;&#22522;&#20110;&#20174;&#21033;&#30410;&#30456;&#20851;&#32773;&#21644;&#32463;&#39564;&#35266;&#23519;&#25910;&#38598;&#30340;&#20449;&#24687;&#65292;&#25110;&#32773;&#26159;&#26681;&#25454;&#25191;&#34892;&#25968;&#25454;&#33258;&#21160;&#21457;&#29616;&#30340;&#12290;&#22312;&#20351;&#29992;BPS&#27169;&#22411;&#26102;&#65292;&#26080;&#35770;&#20854;&#26469;&#28304;&#22914;&#20309;&#65292;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#22914;&#20309;&#35780;&#20272;&#20854;&#36136;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#35780;&#20272;BPS&#27169;&#22411;&#36136;&#37327;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;&#20854;&#22797;&#21046;&#36807;&#31243;&#35266;&#23519;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20513;&#23548;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#19981;&#21516;&#24230;&#37327;&#26041;&#27861;&#38024;&#23545;&#19981;&#21516;&#30340;&#27969;&#31243;&#35282;&#24230;&#12290;&#25105;&#20204;&#35780;&#20272;&#25552;&#20986;&#30340;&#24230;&#37327;&#26041;&#27861;&#35782;&#21035;&#20462;&#25913;BPS&#27169;&#22411;&#30340;&#24433;&#21709;&#30340;&#33021;&#21147;&#20197;&#21450;&#25581;&#31034;&#20854;&#30456;&#23545;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Business Process Simulation (BPS) is an approach to analyze the performance of business processes under different scenarios. For example, BPS allows us to estimate what would be the cycle time of a process if one or more resources became unavailable. The starting point of BPS is a process model annotated with simulation parameters (a BPS model). BPS models may be manually designed, based on information collected from stakeholders and empirical observations, or automatically discovered from execution data. Regardless of its origin, a key question when using a BPS model is how to assess its quality. In this paper, we propose a collection of measures to evaluate the quality of a BPS model w.r.t. its ability to replicate the observed behavior of the process. We advocate an approach whereby different measures tackle different process perspectives. We evaluate the ability of the proposed measures to discern the impact of modifications to a BPS model, and their ability to uncover the relative
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#24040;&#22411;&#20851;&#31995;&#20107;&#20214;&#32593;&#32476;&#30340;&#22522;&#20110;&#21487;&#33021;&#24615;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#24555;&#36895;&#25512;&#29702;&#20986;&#28508;&#22312;&#31354;&#38388;&#21160;&#24577;&#65292;&#24182;&#23454;&#29616;&#20998;&#23618;&#25512;&#26029;&#32593;&#32476;&#31038;&#21306;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2303.17460</link><description>&lt;p&gt;
&#24040;&#22411;&#20851;&#31995;&#20107;&#20214;&#32593;&#32476;&#20013;&#28508;&#22312;&#31354;&#38388;&#21160;&#24577;&#30340;&#24555;&#36895;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Fast inference of latent space dynamics in huge relational event networks. (arXiv:2303.17460v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#24040;&#22411;&#20851;&#31995;&#20107;&#20214;&#32593;&#32476;&#30340;&#22522;&#20110;&#21487;&#33021;&#24615;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#24555;&#36895;&#25512;&#29702;&#20986;&#28508;&#22312;&#31354;&#38388;&#21160;&#24577;&#65292;&#24182;&#23454;&#29616;&#20998;&#23618;&#25512;&#26029;&#32593;&#32476;&#31038;&#21306;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#20107;&#20214;&#26159;&#31038;&#20132;&#20114;&#21160;&#30340;&#19968;&#31181;&#31867;&#22411;&#65292;&#26377;&#26102;&#34987;&#31216;&#20026;&#21160;&#24577;&#32593;&#32476;&#12290;&#23427;&#30340;&#21160;&#24577;&#36890;&#24120;&#21462;&#20915;&#20110;&#26032;&#20852;&#30340;&#27169;&#24335;&#65292;&#21363;&#20869;&#29983;&#21464;&#37327;&#65292;&#25110;&#22806;&#37096;&#21147;&#37327;&#65292;&#21363;&#22806;&#29983;&#21464;&#37327;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#32593;&#32476;&#20013;&#30340;&#34892;&#20026;&#32773;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22823;&#22411;&#32593;&#32476;&#65292;&#20840;&#38754;&#30340;&#20449;&#24687;&#26159;&#32597;&#35265;&#30340;&#12290;&#32593;&#32476;&#20998;&#26512;&#20013;&#30340;&#28508;&#22312;&#31354;&#38388;&#26041;&#27861;&#26159;&#35299;&#37322;&#39537;&#21160;&#32593;&#32476;&#37197;&#32622;&#30340;&#26410;&#27979;&#37327;&#21327;&#21464;&#37327;&#30340;&#27969;&#34892;&#26041;&#24335;&#12290;&#36125;&#21494;&#26031;&#21644;EM&#31867;&#22411;&#30340;&#31639;&#27861;&#24050;&#34987;&#25552;&#20986;&#29992;&#20110;&#25512;&#26029;&#28508;&#22312;&#31354;&#38388;&#65292;&#20294;&#26159;&#35768;&#22810;&#31038;&#20132;&#32593;&#32476;&#24212;&#29992;&#31243;&#24207;&#30340;&#35268;&#27169;&#20197;&#21450;&#36807;&#31243;&#65288;&#22240;&#27492;&#26159;&#28508;&#22312;&#31354;&#38388;&#65289;&#30340;&#21160;&#24577;&#29305;&#24615;&#20351;&#24471;&#35745;&#31639;&#21464;&#24471;&#26497;&#20854;&#26114;&#36149;&#12290;&#22312;&#26412;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#33021;&#24615;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#24040;&#22411;&#20851;&#31995;&#20107;&#20214;&#32593;&#32476;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#20837;&#21040;&#21487;&#35299;&#37322;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#25512;&#26029;&#32593;&#32476;&#31038;&#21306;&#21160;&#24577;&#30340;&#20998;&#23618;&#31574;&#30053;&#12290;&#33410;&#28857;&#21160;&#24577;&#26159;&#36890;&#36807;&#21521;&#21518;&#27010;&#29575;&#25512;&#26029;&#30340;&#36125;&#21494;&#26031;&#32479;&#35745;&#27169;&#22411;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relational events are a type of social interactions, that sometimes are referred to as dynamic networks. Its dynamics typically depends on emerging patterns, so-called endogenous variables, or external forces, referred to as exogenous variables. Comprehensive information on the actors in the network, especially for huge networks, is rare, however. A latent space approach in network analysis has been a popular way to account for unmeasured covariates that are driving network configurations. Bayesian and EM-type algorithms have been proposed for inferring the latent space, but both the sheer size many social network applications as well as the dynamic nature of the process, and therefore the latent space, make computations prohibitively expensive. In this work we propose a likelihood-based algorithm that can deal with huge relational event networks. We propose a hierarchical strategy for inferring network community dynamics embedded into an interpretable latent space. Node dynamics are d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#32467;&#21512;Copula&#29702;&#35770;&#26469;&#35299;&#20915;&#24322;&#26500;&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#21464;&#21270;&#26816;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.17448</link><description>&lt;p&gt;
NN-Copula-CD&#65306;&#19968;&#31181;&#22522;&#20110;Copula&#30340;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#24322;&#26500;&#36965;&#24863;&#22270;&#20687;&#21464;&#21270;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
NN-Copula-CD: A Copula-Guided Interpretable Neural Network for Change Detection in Heterogeneous Remote Sensing Images. (arXiv:2303.17448v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17448
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#32467;&#21512;Copula&#29702;&#35770;&#26469;&#35299;&#20915;&#24322;&#26500;&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#21464;&#21270;&#26816;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#21464;&#21270;&#26816;&#27979;&#26159;&#19968;&#20010;&#23454;&#38469;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#36807;&#21435;&#21313;&#24180;&#26469;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#21457;&#23637;&#35753;&#24322;&#26500;&#21464;&#21270;&#26816;&#27979;&#38382;&#39064;&#21463;&#30410;&#21290;&#27973;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;DNN&#22987;&#32456;&#20687;&#40657;&#21283;&#23376;&#19968;&#26679;&#65292;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#36825;&#38480;&#21046;&#20102;DNN&#22312;&#22823;&#22810;&#25968;&#23454;&#38469;&#21464;&#21270;&#26816;&#27979;&#24212;&#29992;&#20013;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#25511;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Copula&#30340;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#24322;&#26500;&#21464;&#21270;&#26816;&#27979;&#26041;&#27861;(NN-Copula-CD)&#12290;&#22312;NN-Copula-CD&#20013;&#65292;Copula&#30340;&#25968;&#23398;&#29305;&#24449;&#34987;&#35774;&#35745;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#30417;&#30563;&#19968;&#20010;&#31616;&#21333;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Change detection (CD) in heterogeneous remote sensing images is a practical and challenging issue for real-life emergencies. In the past decade, the heterogeneous CD problem has significantly benefited from the development of deep neural networks (DNN). However, the data-driven DNNs always perform like a black box where the lack of interpretability limits the trustworthiness and controllability of DNNs in most practical CD applications. As a strong knowledge-driven tool to measure correlation between random variables, Copula theory has been introduced into CD, yet it suffers from non-robust CD performance without manual prior selection for Copula functions. To address the above issues, we propose a knowledge-data-driven heterogeneous CD method (NN-Copula-CD) based on the Copula-guided interpretable neural network. In our NN-Copula-CD, the mathematical characteristics of Copula are designed as the losses to supervise a simple fully connected neural network to learn the correlation betwe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;-&#20840;&#23616;&#21305;&#37197;&#21644;&#38754;&#31215;&#24179;&#34913;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#21487;&#20197;&#31227;&#38500;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#30417;&#30563;&#65292;&#24182;&#22312;&#24369;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26041;&#38754;&#36798;&#21040;&#26368;&#26032;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2303.17410</link><description>&lt;p&gt;
&#36890;&#36807;&#23616;&#37096;-&#20840;&#23616;&#21305;&#37197;&#21644;&#38754;&#31215;&#24179;&#34913;&#31227;&#38500;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#30417;&#30563;
&lt;/p&gt;
&lt;p&gt;
Removing supervision in semantic segmentation with local-global matching and area balancing. (arXiv:2303.17410v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;-&#20840;&#23616;&#21305;&#37197;&#21644;&#38754;&#31215;&#24179;&#34913;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#21487;&#20197;&#31227;&#38500;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#30417;&#30563;&#65292;&#24182;&#22312;&#24369;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26041;&#38754;&#36798;&#21040;&#26368;&#26032;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#38500;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#30417;&#30563;&#20173;&#28982;&#24456;&#26840;&#25163;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#24120;&#35265;&#30340;&#20998;&#31867;&#27169;&#24335;&#65292;&#20294;&#38656;&#35201;&#22810;&#38454;&#27573;&#26550;&#26500;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#21033;&#29992;&#23616;&#37096;-&#20840;&#23616;&#34917;&#19969;&#21305;&#37197;&#39044;&#27979;&#35821;&#20041;&#20998;&#21106;&#30340;&#31867;&#21035;&#12289;&#33391;&#22909;&#30340;&#23450;&#20301;&#12289;&#23545;&#35937;&#30340;&#21306;&#22495;&#21644;&#24418;&#29366;&#12290;&#23616;&#37096;-&#20840;&#23616;&#21305;&#37197;&#21463;&#21040;&#28385;&#36275;&#36817;&#20284;&#24418;&#29366;&#39044;&#27979;&#30340;&#38754;&#31215;&#32422;&#26463;&#30340;&#26368;&#20248;&#20256;&#36755;&#35745;&#21010;&#30340;&#39537;&#21160;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21482;&#26377;&#22270;&#20687;&#32423;&#21035;&#26631;&#31614;&#30340;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#26032;&#27700;&#24179;&#65292;PascalVOC2012&#39564;&#35777;&#38598;&#19978;&#30340;mIoU&#20026;75&#65285;&#65292;&#22312;MS-COCO2014&#39564;&#35777;&#38598;&#19978;&#20026;46&#65285;&#12290;&#25918;&#24323;&#22270;&#20687;&#32423;&#21035;&#26631;&#31614;&#24182;&#23545;&#32858;&#31867;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#29305;&#24449;&#36827;&#34892;&#20998;&#32452;&#20197;&#20135;&#29983;&#20266;&#22810;&#32423;&#26631;&#31614;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#31181;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;&#30340;&#26080;&#30417;&#30563;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#22312;&#26080;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#26032;&#27700;&#24179;&#65292;PascalVOC2012&#39564;&#35777;&#38598;&#19978;&#30340;mIoU&#20026;43.6&#65285;&#65292;&#22312;MS-COCO2014&#39564;&#35777;&#38598;&#19978;&#20026;19.4&#65285;&#12290;&#20195;&#30721;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Removing supervision in semantic segmentation is still tricky. Current approaches can deal with common categorical patterns yet resort to multi-stage architectures. We design a novel end-to-end model leveraging local-global patch matching to predict categories, good localization, area and shape of objects for semantic segmentation. The local-global matching is, in turn, compelled by optimal transport plans fulfilling area constraints nearing a solution for exact shape prediction. Our model attains state-of-the-art in Weakly Supervised Semantic Segmentation, only image-level labels, with 75% mIoU on PascalVOC2012 val set and 46% on MS-COCO2014 val set. Dropping the image-level labels and clustering self-supervised learned features to yield pseudo-multi-level labels, we obtain an unsupervised model for semantic segmentation. We also attain state-of-the-art on Unsupervised Semantic Segmentation with 43.6% mIoU on PascalVOC2012 val set and 19.4% on MS-COCO2014 val set. The code is availabl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#31163;&#32447;RL&#20195;&#29702;&#30340;&#22312;&#32447;&#24494;&#35843;&#38382;&#39064;&#12290;&#36890;&#36807;&#30740;&#31350;&#25968;&#25454;&#22810;&#26679;&#24615;&#12289;&#31639;&#27861;&#36873;&#25321;&#21644;&#22312;&#32447;&#22238;&#25918;&#20998;&#24067;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20445;&#23432;&#30340;&#31574;&#30053;&#20248;&#21270;&#36807;&#31243;&#65292;&#21487;&#20197;&#20174;&#31163;&#32447;&#39044;&#35757;&#32451;&#20013;&#23454;&#29616;&#31283;&#23450;&#19988;&#26679;&#26412;&#26377;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2303.17396</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24494;&#35843;&#65306;&#25361;&#25112;&#12289;&#24179;&#34913;&#21644;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Finetuning from Offline Reinforcement Learning: Challenges, Trade-offs and Practical Solutions. (arXiv:2303.17396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#31163;&#32447;RL&#20195;&#29702;&#30340;&#22312;&#32447;&#24494;&#35843;&#38382;&#39064;&#12290;&#36890;&#36807;&#30740;&#31350;&#25968;&#25454;&#22810;&#26679;&#24615;&#12289;&#31639;&#27861;&#36873;&#25321;&#21644;&#22312;&#32447;&#22238;&#25918;&#20998;&#24067;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20445;&#23432;&#30340;&#31574;&#30053;&#20248;&#21270;&#36807;&#31243;&#65292;&#21487;&#20197;&#20174;&#31163;&#32447;&#39044;&#35757;&#32451;&#20013;&#23454;&#29616;&#31283;&#23450;&#19988;&#26679;&#26412;&#26377;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20801;&#35768;&#22312;&#27809;&#26377;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#24773;&#20917;&#19979;&#36890;&#36807;&#31163;&#32447;&#25968;&#25454;&#38598;&#35757;&#32451;&#26377;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#12290;&#36825;&#20123;&#31163;&#32447;&#27169;&#22411;&#30340;&#22312;&#32447;&#24494;&#35843;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#22914;&#20309;&#29702;&#24819;&#22320;&#24494;&#35843;&#20174;&#31163;&#32447;RL&#35757;&#32451;&#20013;&#33719;&#24471;&#30340;&#20195;&#29702;&#65311;&#34429;&#28982;&#31163;&#32447;RL&#31639;&#27861;&#21407;&#21017;&#19978;&#21487;&#20197;&#29992;&#20110;&#24494;&#35843;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#23427;&#20204;&#30340;&#22312;&#32447;&#24615;&#33021;&#25552;&#39640;&#32531;&#24930;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#21487;&#20197;&#20351;&#29992;&#26631;&#20934;&#30340;&#22312;&#32447;&#31163;&#31574;&#30053;&#31639;&#27861;&#36827;&#34892;&#26356;&#24555;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20250;&#36973;&#21463;&#31574;&#30053;&#23849;&#28291;&#65292;&#21363;&#22312;&#21021;&#22987;&#22312;&#32447;&#23398;&#20064;&#36807;&#31243;&#20013;&#31574;&#30053;&#20250;&#20005;&#37325;&#36864;&#21270;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#31574;&#30053;&#23849;&#28291;&#30340;&#38382;&#39064;&#20197;&#21450;&#23427;&#19982;&#25968;&#25454;&#22810;&#26679;&#24615;&#12289;&#31639;&#27861;&#36873;&#25321;&#21644;&#22312;&#32447;&#22238;&#25918;&#20998;&#24067;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22522;&#20110;&#36825;&#20123;&#27934;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20445;&#23432;&#30340;&#31574;&#30053;&#20248;&#21270;&#36807;&#31243;&#65292;&#21487;&#20197;&#20174;&#31163;&#32447;&#39044;&#35757;&#32451;&#20013;&#23454;&#29616;&#31283;&#23450;&#19988;&#26679;&#26412;&#26377;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) allows for the training of competent agents from offline datasets without any interaction with the environment. Online finetuning of such offline models can further improve performance. But how should we ideally finetune agents obtained from offline RL training? While offline RL algorithms can in principle be used for finetuning, in practice, their online performance improves slowly. In contrast, we show that it is possible to use standard online off-policy algorithms for faster improvement. However, we find this approach may suffer from policy collapse, where the policy undergoes severe performance deterioration during initial online learning. We investigate the issue of policy collapse and how it relates to data diversity, algorithm choices and online replay distribution. Based on these insights, we propose a conservative policy optimization procedure that can achieve stable and sample-efficient online learning from offline pretraining.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#31454;&#20105;&#23398;&#20064;&#31639;&#27861;&#30340;&#30333;&#30418;&#21487;&#35299;&#37322;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65292;&#19982;&#40657;&#21283;&#23376;&#26041;&#27861;&#30456;&#27604;&#65292;&#33021;&#22815;&#25552;&#20379;&#26356;&#20026;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#19988;&#36164;&#28304;&#28040;&#32791;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2303.17387</link><description>&lt;p&gt;
&#20351;&#29992;&#31454;&#20105;&#23398;&#20064;&#25216;&#26415;&#30340;&#21487;&#35299;&#37322;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Explainable Intrusion Detection Systems Using Competitive Learning Techniques. (arXiv:2303.17387v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#31454;&#20105;&#23398;&#20064;&#31639;&#27861;&#30340;&#30333;&#30418;&#21487;&#35299;&#37322;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65292;&#19982;&#40657;&#21283;&#23376;&#26041;&#27861;&#30456;&#27604;&#65292;&#33021;&#22815;&#25552;&#20379;&#26356;&#20026;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#19988;&#36164;&#28304;&#28040;&#32791;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21551;&#29992;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#37319;&#29992;&#21508;&#31181;&#40657;&#21283;&#23376;&#26041;&#27861;&#12290;&#36825;&#20123;&#40657;&#21283;&#23376;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#22522;&#20110;&#38169;&#35823;&#23398;&#20064;&#65288;EBL&#65289;&#25216;&#26415;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19987;&#27880;&#20110;&#21019;&#24314;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#39640;&#24615;&#33021;&#25104;&#26412;&#19988;&#19981;&#26131;&#35299;&#37322;&#12290;&#22522;&#20110;&#31454;&#20105;&#23398;&#20064;&#65288;CL&#65289;&#30340;&#30333;&#30418;&#21487;&#35299;&#37322;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;X-IDS&#65289;&#20026;&#36825;&#20123;&#38382;&#39064;&#25552;&#20379;&#20102;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;CL&#27169;&#22411;&#21033;&#29992;&#19982;EBL&#26041;&#27861;&#23436;&#20840;&#19981;&#21516;&#30340;&#23398;&#20064;&#33539;&#24335;&#12290;&#36825;&#31181;&#19981;&#21516;&#30340;&#23398;&#20064;&#36807;&#31243;&#20351;&#24471;CL&#31639;&#27861;&#26063;&#22266;&#26377;&#22320;&#21487;&#35299;&#37322;&#19988;&#36164;&#28304;&#28040;&#32791;&#36739;&#23567;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;DARPA&#23545;&#21487;&#35299;&#37322;&#31995;&#32479;&#30340;&#24314;&#35758;&#30340;X-IDS&#26550;&#26500;&#12290;&#22312;&#25105;&#20204;&#30340;&#26550;&#26500;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20687;&#33258;&#32452;&#32455;&#26144;&#23556;&#65288;SOM&#65289;&#12289;&#22686;&#38271;&#24335;&#33258;&#32452;&#32455;&#26144;&#23556;&#65288;GSOM&#65289;&#21644;&#22686;&#38271;&#24335;&#20998;&#23618;&#33258;&#32452;&#32455;&#26144;&#23556;&#65288;GHSOM&#65289;&#31561;CL&#31639;&#27861;&#12290;&#29983;&#25104;&#30340;&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#25968;&#25454;&#25366;&#25496;&#20197;&#21019;&#24314;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current state of the art systems in Artificial Intelligence (AI) enabled intrusion detection use a variety of black box methods. These black box methods are generally trained using Error Based Learning (EBL) techniques with a focus on creating accurate models. These models have high performative costs and are not easily explainable. A white box Competitive Learning (CL) based eXplainable Intrusion Detection System (X-IDS) offers a potential solution to these problem. CL models utilize an entirely different learning paradigm than EBL approaches. This different learning process makes the CL family of algorithms innately explainable and less resource intensive. In this paper, we create an X-IDS architecture that is based on DARPA's recommendation for explainable systems. In our architecture we leverage CL algorithms like, Self Organizing Maps (SOM), Growing Self Organizing Maps (GSOM), and Growing Hierarchical Self Organizing Map (GHSOM). The resulting models can be data-mined to crea
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30528;&#37325;&#30740;&#31350;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#22810;&#20219;&#21153;&#33258;&#22238;&#24402;&#35299;&#30721;&#22120;&#65292;&#36890;&#36807;&#24191;&#27867;&#30340;&#31995;&#32479;&#35797;&#39564;&#20998;&#26512;&#20102;&#20219;&#21153;&#21644;&#25968;&#25454;&#28151;&#21512;&#12289;&#35757;&#32451;&#21644;&#27491;&#21017;&#21270;&#36229;&#21442;&#25968;&#12289;&#26465;&#20214;&#31867;&#22411;&#21644;&#29305;&#24322;&#24615;&#12289;&#27169;&#24577;&#32452;&#21512;&#31561;&#22240;&#32032;&#65292;&#21457;&#29616;&#36890;&#36807;&#20923;&#32467;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#65292;&#37319;&#29992;&#23567;&#22411;&#35299;&#30721;&#22120;&#21487;&#25509;&#36817;&#20110;&#21333;&#20219;&#21153;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2303.17376</link><description>&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#22810;&#20219;&#21153;&#33258;&#22238;&#24402;&#35299;&#30721;&#22120;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study of Autoregressive Decoders for Multi-Tasking in Computer Vision. (arXiv:2303.17376v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30528;&#37325;&#30740;&#31350;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#22810;&#20219;&#21153;&#33258;&#22238;&#24402;&#35299;&#30721;&#22120;&#65292;&#36890;&#36807;&#24191;&#27867;&#30340;&#31995;&#32479;&#35797;&#39564;&#20998;&#26512;&#20102;&#20219;&#21153;&#21644;&#25968;&#25454;&#28151;&#21512;&#12289;&#35757;&#32451;&#21644;&#27491;&#21017;&#21270;&#36229;&#21442;&#25968;&#12289;&#26465;&#20214;&#31867;&#22411;&#21644;&#29305;&#24322;&#24615;&#12289;&#27169;&#24577;&#32452;&#21512;&#31561;&#22240;&#32032;&#65292;&#21457;&#29616;&#36890;&#36807;&#20923;&#32467;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#65292;&#37319;&#29992;&#23567;&#22411;&#35299;&#30721;&#22120;&#21487;&#25509;&#36817;&#20110;&#21333;&#20219;&#21153;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20986;&#29616;&#20102;&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#65292;&#33021;&#22815;&#25191;&#34892;&#22810;&#39033;&#20219;&#21153;&#65292;&#30001;&#22270;&#20687;&#32534;&#30721;&#22120;&#65288;&#36890;&#24120;&#26159; ViT&#65289;&#21644;&#33258;&#22238;&#24402;&#35299;&#30721;&#22120;&#65288;&#36890;&#24120;&#26159; Transformer&#65289;&#32452;&#25104;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#24037;&#20316;&#20165;&#21576;&#29616;&#19968;&#20010;&#31995;&#32479;&#21450;&#20854;&#32467;&#26524;&#65292;&#23545;&#36825;&#20123;&#31995;&#32479;&#30340;&#35774;&#35745;&#20915;&#31574;&#21644;&#26435;&#34913;&#26041;&#38754;&#30041;&#19979;&#20102;&#35768;&#22810;&#30097;&#38382;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#36825;&#20123;&#31572;&#26696;&#12290;&#25105;&#20204;&#20180;&#32454;&#30740;&#31350;&#20102;&#33258;&#22238;&#24402;&#35299;&#30721;&#22120;&#22312;&#22810;&#27169;&#24577;&#35745;&#31639;&#26426;&#35270;&#35273;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#23383;&#24149;&#12289;&#35270;&#35273;&#38382;&#31572;&#21644;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#31561;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#31995;&#32479;&#35797;&#39564;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20219;&#21153;&#21644;&#25968;&#25454;&#28151;&#21512;&#12289;&#35757;&#32451;&#21644;&#27491;&#21017;&#21270;&#36229;&#21442;&#25968;&#12289;&#26465;&#20214;&#31867;&#22411;&#21644;&#29305;&#24322;&#24615;&#12289;&#27169;&#24577;&#32452;&#21512;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#32467;&#26524;&#19982;&#32463;&#36807;&#20805;&#20998;&#35843;&#35797;&#30340;&#21333;&#20219;&#21153;&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#20197;&#20984;&#26174;&#22810;&#20219;&#21153;&#23398;&#20064;&#25152;&#24102;&#26469;&#30340;&#25104;&#26412;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#21457;&#29616;&#26159;&#65292;&#23545;&#20110;&#20923;&#32467;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#19978;&#23398;&#20064;&#30340;&#23567;&#22411;&#35299;&#30721;&#22120;&#65292;&#24615;&#33021;&#25509;&#36817;&#20110;&#21333;&#20219;&#21153;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been a recent explosion of computer vision models which perform many tasks and are composed of an image encoder (usually a ViT) and an autoregressive decoder (usually a Transformer). However, most of this work simply presents one system and its results, leaving many questions regarding design decisions and trade-offs of such systems unanswered. In this work, we aim to provide such answers. We take a close look at autoregressive decoders for multi-task learning in multimodal computer vision, including classification, captioning, visual question answering, and optical character recognition. Through extensive systematic experiments, we study the effects of task and data mixture, training and regularization hyperparameters, conditioning type and specificity, modality combination, and more. Importantly, we compare these to well-tuned single-task baselines to highlight the cost incurred by multi-tasking. A key finding is that a small decoder learned on top of a frozen pretrained en
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#23545;&#31216;&#22635;&#20805;&#30340;&#21487;&#36870;&#21367;&#31215;&#26041;&#27861;&#65292;&#21487;&#36890;&#36807;DFT&#35299;&#26512;&#21453;&#28436;&#65292;&#21487;&#24212;&#29992;&#20110;&#22810;&#31181;&#22635;&#20805;&#27169;&#24335;&#65307;&#30456;&#20851;&#20195;&#30721;&#21487;&#22312;GitHub&#33719;&#21462;&#12290;</title><link>http://arxiv.org/abs/2303.17361</link><description>&lt;p&gt;
&#23545;&#31216;&#22635;&#20805;&#30340;&#21487;&#36870;&#21367;&#31215;
&lt;/p&gt;
&lt;p&gt;
Invertible Convolution with Symmetric Paddings. (arXiv:2303.17361v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#23545;&#31216;&#22635;&#20805;&#30340;&#21487;&#36870;&#21367;&#31215;&#26041;&#27861;&#65292;&#21487;&#36890;&#36807;DFT&#35299;&#26512;&#21453;&#28436;&#65292;&#21487;&#24212;&#29992;&#20110;&#22810;&#31181;&#22635;&#20805;&#27169;&#24335;&#65307;&#30456;&#20851;&#20195;&#30721;&#21487;&#22312;GitHub&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#23545;&#31216;&#22635;&#20805;&#30340;&#21367;&#31215;&#21487;&#20197;&#36890;&#36807;DFT&#24471;&#21040;&#35299;&#26512;&#21453;&#28436;&#12290;&#25105;&#20204;&#20840;&#38754;&#20998;&#26512;&#20102;&#20960;&#31181;&#19981;&#21516;&#30340;&#23545;&#31216;&#21644;&#21453;&#23545;&#31216;&#22635;&#20805;&#27169;&#24335;&#65292;&#24182;&#23637;&#31034;&#20102;&#22810;&#31181;&#21487;&#23454;&#29616;&#21453;&#28436;&#30340;&#24773;&#20917;&#12290;&#27492;&#23454;&#29616;&#21487;&#22312;\url{https://github.com/prclibo/iconv_dft}&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that symmetrically padded convolution can be analytically inverted via DFT. We comprehensively analyze several different symmetric and anti-symmetric padding modes and show that multiple cases exist where the inversion can be achieved. The implementation is available at \url{https://github.com/prclibo/iconv_dft}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FL-DP$^3$S&#30340;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#20102;&#25968;&#25454;&#20998;&#26512;&#21644;DPP&#37319;&#26679;&#25216;&#26415;&#65292;&#26377;&#25928;&#22320;&#22810;&#26679;&#21270;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#20445;&#25252;&#20182;&#20204;&#30340;&#25968;&#25454;&#38544;&#31169;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#29942;&#39048;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#22522;&#20934;&#26041;&#26696;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.17358</link><description>&lt;p&gt;
&#22522;&#20110;DPP&#30340;&#38750;IID&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;&#23458;&#25143;&#31471;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DPP-based Client Selection for Federated Learning with Non-IID Data. (arXiv:2303.17358v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FL-DP$^3$S&#30340;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#20102;&#25968;&#25454;&#20998;&#26512;&#21644;DPP&#37319;&#26679;&#25216;&#26415;&#65292;&#26377;&#25928;&#22320;&#22810;&#26679;&#21270;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#20445;&#25252;&#20182;&#20204;&#30340;&#25968;&#25454;&#38544;&#31169;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#29942;&#39048;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#22522;&#20934;&#26041;&#26696;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23458;&#25143;&#31471;&#36873;&#25321;&#65288;CS&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#30340;&#36890;&#20449;&#29942;&#39048;&#38382;&#39064;&#65292;&#21516;&#26102;&#20063;&#35299;&#20915;FL&#20013;&#25968;&#25454;&#24322;&#26500;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#21033;&#29992;&#25968;&#25454;&#20998;&#26512;&#21644;&#30830;&#23450;&#24615;&#28857;&#36807;&#31243;&#65288;DPP&#65289;&#37319;&#26679;&#25216;&#26415;&#24320;&#21457;&#20102;&#19968;&#31181;&#31639;&#27861;FL-DP$^3$S&#65292;&#23427;&#22312;&#27599;&#36718;&#35757;&#32451;&#20013;&#26377;&#25928;&#22320;&#22810;&#26679;&#21270;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#20445;&#25252;&#20182;&#20204;&#30340;&#25968;&#25454;&#38544;&#31169;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#26469;&#26816;&#39564;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#26696;&#20855;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20197;&#21450;&#27604;&#20960;&#20010;&#22522;&#20934;&#26041;&#26696;&#26356;&#23567;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a client selection (CS) method to tackle the communication bottleneck of federated learning (FL) while concurrently coping with FL's data heterogeneity issue. Specifically, we first analyze the effect of CS in FL and show that FL training can be accelerated by adequately choosing participants to diversify the training dataset in each round of training. Based on this, we leverage data profiling and determinantal point process (DPP) sampling techniques to develop an algorithm termed Federated Learning with DPP-based Participant Selection (FL-DP$^3$S). This algorithm effectively diversifies the participants' datasets in each round of training while preserving their data privacy. We conduct extensive experiments to examine the efficacy of our proposed method. The results show that our scheme attains a faster convergence rate, as well as a smaller communication overhead than several baselines.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;Transformer&#39592;&#24178;&#32593;&#32476;&#30340;&#28176;&#36827;&#24335;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#21644;&#23450;&#20301;&#65292;&#20854;&#20013;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;MAE&#27169;&#22411;&#36827;&#34892;&#27491;&#24120;&#22270;&#20687;&#30340;&#35757;&#32451;&#65292;&#31532;&#20108;&#38454;&#27573;&#20351;&#29992;&#20687;&#32032;&#32423;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#29983;&#25104;&#25439;&#22351;&#30340;&#27491;&#24120;&#22270;&#20687;&#65292;&#26368;&#32456;&#36890;&#36807;&#20687;&#32032;&#37325;&#24314;&#35823;&#24046;&#30697;&#38453;&#21644;&#20687;&#32032;&#24322;&#24120;&#27010;&#29575;&#30697;&#38453;&#32508;&#21512;&#24471;&#21040;&#19968;&#20010;&#24322;&#24120;&#24471;&#20998;&#30697;&#38453;&#12290;</title><link>http://arxiv.org/abs/2303.17354</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#28176;&#36827;&#24335;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#24322;&#24120;&#26816;&#27979;&#21644;&#23450;&#20301;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Incremental Self-Supervised Learning Based on Transformer for Anomaly Detection and Localization. (arXiv:2303.17354v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17354
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;Transformer&#39592;&#24178;&#32593;&#32476;&#30340;&#28176;&#36827;&#24335;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#21644;&#23450;&#20301;&#65292;&#20854;&#20013;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;MAE&#27169;&#22411;&#36827;&#34892;&#27491;&#24120;&#22270;&#20687;&#30340;&#35757;&#32451;&#65292;&#31532;&#20108;&#38454;&#27573;&#20351;&#29992;&#20687;&#32032;&#32423;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#29983;&#25104;&#25439;&#22351;&#30340;&#27491;&#24120;&#22270;&#20687;&#65292;&#26368;&#32456;&#36890;&#36807;&#20687;&#32032;&#37325;&#24314;&#35823;&#24046;&#30697;&#38453;&#21644;&#20687;&#32032;&#24322;&#24120;&#27010;&#29575;&#30697;&#38453;&#32508;&#21512;&#24471;&#21040;&#19968;&#20010;&#24322;&#24120;&#24471;&#20998;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#23545;&#20110;&#22270;&#20687;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#21644;&#23450;&#20301;&#30340;&#30740;&#31350;&#65292;&#23588;&#20854;&#26159;&#22312;&#24037;&#19994;&#32570;&#38519;&#26816;&#27979;&#31561;&#23454;&#38469;&#24212;&#29992;&#20013;&#24341;&#36215;&#20102;&#37325;&#35270;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#20316;&#20026;&#39592;&#24178;&#32593;&#32476;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#39592;&#24178;&#32593;&#32476;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#20004;&#38454;&#27573;&#30340;&#28176;&#36827;&#24335;&#23398;&#20064;&#31574;&#30053;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#20165;&#20351;&#29992;&#27491;&#24120;&#22270;&#20687;&#23545;Masked Autoencoder &#65288;MAE&#65289;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20687;&#32032;&#32423;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#20197;&#29983;&#25104;&#24050;&#25439;&#22351;&#30340;&#27491;&#24120;&#22270;&#20687;&#21450;&#20854;&#30456;&#24212;&#30340;&#20687;&#32032;&#26631;&#31614;&#12290;&#36825;&#20010;&#36807;&#31243;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#22914;&#20309;&#20462;&#22797;&#25439;&#22351;&#30340;&#21306;&#22495;&#21644;&#20998;&#31867;&#27599;&#20010;&#20687;&#32032;&#30340;&#29366;&#24577;&#12290;&#26368;&#32456;&#65292;&#35813;&#27169;&#22411;&#20135;&#29983;&#19968;&#20010;&#20687;&#32032;&#37325;&#24314;&#35823;&#24046;&#30697;&#38453;&#21644;&#19968;&#20010;&#20687;&#32032;&#24322;&#24120;&#27010;&#29575;&#30697;&#38453;&#65292;&#36825;&#20004;&#20010;&#30697;&#38453;&#32508;&#21512;&#25104;&#19968;&#20010;&#24322;&#24120;&#24471;&#20998;&#30697;&#38453;&#65292;&#26377;&#25928;&#22320;&#29992;&#20110;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the machine learning domain, research on anomaly detection and localization within image data has garnered significant attention, particularly in practical applications such as industrial defect detection. While existing approaches predominantly rely on Convolutional Neural Networks (CNN) as their backbone network, we propose an innovative method based on the Transformer backbone network. Our approach employs a two-stage incremental learning strategy. In the first stage, we train a Masked Autoencoder (MAE) model exclusively on normal images. Subsequently, in the second stage, we implement pixel-level data augmentation techniques to generate corrupted normal images and their corresponding pixel labels. This process enables the model to learn how to repair corrupted regions and classify the state of each pixel. Ultimately, the model produces a pixel reconstruction error matrix and a pixel anomaly probability matrix, which are combined to create an anomaly scoring matrix that effective
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#30005;&#20449;&#27450;&#35784;&#26816;&#27979;&#32780;&#35774;&#35745;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21517;&#20026;GAT-COBO&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#22522;&#20110;GAT&#30340;&#22522;&#26412;&#20998;&#31867;&#22120;&#21644;&#25104;&#26412;&#25935;&#24863;&#23398;&#20064;&#22120;&#26469;&#35299;&#20915;&#22270;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#31454;&#20105;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.17334</link><description>&lt;p&gt;
GAT-COBO&#65306;&#19968;&#31181;&#38024;&#23545;&#30005;&#20449;&#27450;&#35784;&#26816;&#27979;&#30340;&#25104;&#26412;&#25935;&#24863;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
GAT-COBO: Cost-Sensitive Graph Neural Network for Telecom Fraud Detection. (arXiv:2303.17334v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#30005;&#20449;&#27450;&#35784;&#26816;&#27979;&#32780;&#35774;&#35745;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21517;&#20026;GAT-COBO&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#22522;&#20110;GAT&#30340;&#22522;&#26412;&#20998;&#31867;&#22120;&#21644;&#25104;&#26412;&#25935;&#24863;&#23398;&#20064;&#22120;&#26469;&#35299;&#20915;&#22270;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#31454;&#20105;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31227;&#21160;&#36890;&#20449;&#25216;&#26415;&#65288;&#22914;5G&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#30005;&#20449;&#27450;&#35784;&#24471;&#21040;&#20102;&#26174;&#33879;&#22686;&#21152;&#65292;&#24433;&#21709;&#20102;&#20010;&#20154;&#36130;&#23500;&#21644;&#31038;&#20250;&#36130;&#23500;&#30340;&#25439;&#22833;&#12290;&#26368;&#36817;&#65292;&#22270;&#25366;&#25496;&#25216;&#26415;&#36880;&#28176;&#25104;&#20026;&#26816;&#27979;&#30005;&#20449;&#27450;&#35784;&#30340;&#20027;&#27969;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24085;&#32047;&#25176;&#21407;&#22240;&#24341;&#36215;&#30340;&#22270;&#19981;&#24179;&#34913;&#38382;&#39064;&#32473;&#22270;&#25968;&#25454;&#25366;&#25496;&#24102;&#26469;&#20102;&#20005;&#37325;&#30340;&#25361;&#25112;&#12290;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#20294;&#20043;&#21069;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22270;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;&#25104;&#26412;&#25935;&#24863;&#22686;&#24378;&#65288;GAT-COBO&#65289;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;GAT&#30340;&#22522;&#26412;&#20998;&#31867;&#22120;&#26469;&#23398;&#20064;&#22270;&#20013;&#25152;&#26377;&#33410;&#28857;&#30340;&#23884;&#20837;&#12290;&#28982;&#21518;&#65292;&#23558;&#23884;&#20837;&#39304;&#36865;&#21040;&#32463;&#36807;&#33391;&#22909;&#35774;&#35745;&#30340;&#25104;&#26412;&#25935;&#24863;&#23398;&#20064;&#22120;&#20013;&#36827;&#34892;&#19981;&#24179;&#34913;&#23398;&#20064;&#12290;&#25509;&#19979;&#26469;&#65292;&#26681;&#25454;&#20998;&#31867;&#38169;&#35823;&#25104;&#26412;&#26356;&#26032;&#26435;&#37325;&#65292;&#20351;&#27169;&#22411;&#26356;&#22810;&#22320;&#20851;&#27880;&#23569;&#25968;&#31867;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#23558;&#22522;&#20110;&#20998;&#31867;&#22120;&#21644;&#25104;&#26412;&#25935;&#24863;&#23398;&#20064;&#22120;&#33719;&#24471;&#30340;&#33410;&#28857;&#23884;&#20837;&#30456;&#21152;&#65292;&#24471;&#20986;&#26368;&#32456;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;GAT-COBO&#26041;&#27861;&#22312;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#32447;&#30456;&#27604;&#26102;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Along with the rapid evolution of mobile communication technologies, such as 5G, there has been a drastically increase in telecom fraud, which significantly dissipates individual fortune and social wealth. In recent years, graph mining techniques are gradually becoming a mainstream solution for detecting telecom fraud. However, the graph imbalance problem, caused by the Pareto principle, brings severe challenges to graph data mining. This is a new and challenging problem, but little previous work has been noticed. In this paper, we propose a Graph ATtention network with COst-sensitive BOosting (GAT-COBO) for the graph imbalance problem. First, we design a GAT-based base classifier to learn the embeddings of all nodes in the graph. Then, we feed the embeddings into a well-designed cost-sensitive learner for imbalanced learning. Next, we update the weights according to the misclassification cost to make the model focus more on the minority class. Finally, we sum the node embeddings obtai
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;Sasaki&#24230;&#37327;&#23454;&#29616;&#27604;&#36739;&#31163;&#25955;&#36712;&#36857;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#22312;&#27969;&#24418;&#20540;&#27979;&#37327;&#20013;&#24314;&#31435;&#20102;&#19968;&#20010;&#35745;&#31639;&#39640;&#25928;&#12289;&#22266;&#26377;&#30340;&#40654;&#26364;&#20998;&#23618;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#36824;&#21487;&#29992;&#20110;&#32676;&#20307;&#36235;&#21183;&#30340;&#24179;&#22343;&#36712;&#36857;&#20272;&#35745;&#65292;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#22312;&#39123;&#39118;&#36712;&#36857;&#24378;&#24230;&#20998;&#31867;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17299</link><description>&lt;p&gt;
&#26679;&#26465;&#27169;&#22411;&#20013;&#27979;&#37327;&#27969;&#24418;&#20540;&#36712;&#36857;&#30340;Sasaki&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Sasaki Metric for Spline Models of Manifold-Valued Trajectories. (arXiv:2303.17299v1 [math.DG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17299
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;Sasaki&#24230;&#37327;&#23454;&#29616;&#27604;&#36739;&#31163;&#25955;&#36712;&#36857;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#22312;&#27969;&#24418;&#20540;&#27979;&#37327;&#20013;&#24314;&#31435;&#20102;&#19968;&#20010;&#35745;&#31639;&#39640;&#25928;&#12289;&#22266;&#26377;&#30340;&#40654;&#26364;&#20998;&#23618;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#36824;&#21487;&#29992;&#20110;&#32676;&#20307;&#36235;&#21183;&#30340;&#24179;&#22343;&#36712;&#36857;&#20272;&#35745;&#65292;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#22312;&#39123;&#39118;&#36712;&#36857;&#24378;&#24230;&#20998;&#31867;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26102;&#31354;&#26694;&#26550;&#26469;&#20998;&#26512;&#27969;&#24418;&#20540;&#27979;&#37327;&#65292;&#23427;&#20801;&#35768;&#20351;&#29992;&#22266;&#26377;&#19988;&#35745;&#31639;&#39640;&#25928;&#30340;&#40654;&#26364;&#20998;&#23618;&#27169;&#22411;&#12290;&#29305;&#21035;&#22320;&#65292;&#21033;&#29992;&#22238;&#24402;&#65292;&#25105;&#20204;&#36890;&#36807;&#22797;&#21512;B\' ezier&#26679;&#26465;&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#34920;&#36798;&#31163;&#25955;&#36712;&#36857;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#30001;Sasaki&#24230;&#37327;&#35825;&#23548;&#30340;&#33258;&#28982;&#24230;&#37327;&#26469;&#27604;&#36739;&#36712;&#36857;&#65292;&#24182;&#23558;&#24179;&#22343;&#36712;&#36857;&#20272;&#35745;&#20026;&#32676;&#20307;&#36235;&#21183;&#12290;&#25105;&#20204;&#22312;&#39123;&#39118;&#36712;&#36857;&#19978;&#36827;&#34892;&#20102;&#23450;&#24615;&#21644;&#23450;&#37327;&#23454;&#39564;&#65292;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#26679;&#26465;&#30340;&#26041;&#27861;&#22312;&#36712;&#36857;&#24378;&#24230;&#20998;&#31867;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a generic spatiotemporal framework to analyze manifold-valued measurements, which allows for employing an intrinsic and computationally efficient Riemannian hierarchical model. Particularly, utilizing regression, we represent discrete trajectories in a Riemannian manifold by composite B\' ezier splines, propose a natural metric induced by the Sasaki metric to compare the trajectories, and estimate average trajectories as group-wise trends. We evaluate our framework in comparison to state-of-the-art methods within qualitative and quantitative experiments on hurricane tracks. Notably, our results demonstrate the superiority of spline-based approaches for an intensity classification of the tracks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#12289;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#22312;&#20154;&#31867;&#24605;&#32500;&#27169;&#24335;&#20013;&#30340;&#34920;&#29616;, &#36816;&#29992;&#35748;&#35782;&#35770;&#29702;&#35770;&#25552;&#20379;&#20102;&#31526;&#21495;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#30340;&#20154;&#31867;&#21028;&#26029;&#25968;&#25454;&#28857;&#20197;&#21450;ETR&#39044;&#27979;&#25968;&#25454;&#28857;&#30340;&#25968;&#37327;&#32423;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#26816;&#39564;&#12290;</title><link>http://arxiv.org/abs/2303.17276</link><description>&lt;p&gt;
&#22312;GPT&#25104;&#21151;&#21644;&#22833;&#36133;&#30340;&#24773;&#20917;&#19979;&#65292;&#20154;&#31867;&#36755;&#20837;&#20154;&#31867;&#36755;&#20986;&#65306;&#35770;GPT&#26397;&#21521;&#24120;&#35782;&#30340;&#36235;&#21516;&#24615;
&lt;/p&gt;
&lt;p&gt;
Humans in Humans Out: On GPT Converging Toward Common Sense in both Success and Failure. (arXiv:2303.17276v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#12289;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#22312;&#20154;&#31867;&#24605;&#32500;&#27169;&#24335;&#20013;&#30340;&#34920;&#29616;, &#36816;&#29992;&#35748;&#35782;&#35770;&#29702;&#35770;&#25552;&#20379;&#20102;&#31526;&#21495;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#30340;&#20154;&#31867;&#21028;&#26029;&#25968;&#25454;&#28857;&#20197;&#21450;ETR&#39044;&#27979;&#25968;&#25454;&#28857;&#30340;&#25968;&#37327;&#32423;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#26816;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#35268;&#27169;&#21644;&#24494;&#35843;&#30340;&#22686;&#21152;&#20351;&#24471;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20363;&#22914;GPT&#30340;&#36755;&#20986;&#36136;&#37327;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;&#37492;&#20110;GPT-3&#21644;GPT-4&#37117;&#26159;&#20351;&#29992;&#22823;&#37327;&#30001;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#25105;&#20204;&#21487;&#20197;&#38382;&#20182;&#20204;&#30340;&#36755;&#20986;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#21453;&#26144;&#20102;&#20154;&#31867;&#24605;&#32500;&#30340;&#27169;&#24335;&#65292;&#26080;&#35770;&#26159;&#27491;&#30830;&#36824;&#26159;&#38169;&#35823;&#30340;&#24773;&#20917;&#12290;&#35748;&#35782;&#35770;&#29702;&#35770;&#25552;&#20379;&#20102;&#20851;&#20110;&#20154;&#31867;&#25104;&#21151;&#21644;&#22833;&#36133;&#30340;&#31526;&#21495;&#29983;&#25104;&#27169;&#22411;&#65292;&#21253;&#25324;&#21629;&#39064;&#12289;&#37327;&#21270;&#12289;&#27010;&#29575;&#25512;&#29702;&#21644;&#20915;&#31574;&#12290;&#26412;&#25991;&#23558;ETR&#30340;&#26368;&#36817;&#19968;&#20010;&#20070;&#26412;&#30340;61&#20010;&#26680;&#24515;&#25512;&#29702;&#21644;&#21028;&#26029;&#38382;&#39064;&#25552;&#20379;&#32473;&#20102;GPT-3&#12289;GPT-3.5&#21644;GPT-4&#65292;&#36825;&#20123;&#38382;&#39064;&#21253;&#25324;&#32463;&#36807;&#23454;&#39564;&#35777;&#23454;&#30340;&#20154;&#31867;&#21028;&#26029;&#25968;&#25454;&#28857;&#21644;ETR&#39044;&#27979;&#30340;&#25968;&#25454;&#28857;&#65292;&#21516;&#26102;&#21253;&#21547;&#27491;&#30830;&#30340;&#25512;&#29702;&#27169;&#24335;&#21644;&#35884;&#35823;&#21644;&#26694;&#26550;&#25928;&#24212;&#65288;ETR61&#22522;&#20934;&#27979;&#35797;&#65289;&#12290; ETR61&#21253;&#25324;&#20102;Wason&#30340;&#21345;&#29260;&#20219;&#21153;&#12289;&#38169;&#35273;&#25512;&#29702;&#12289;&#35825;&#39285;&#25928;&#24212;&#31561;&#32463;&#20856;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Increase in computational scale and fine-tuning has seen a dramatic improvement in the quality of outputs of large language models (LLMs) like GPT. Given that both GPT-3 and GPT-4 were trained on large quantities of human-generated text, we might ask to what extent their outputs reflect patterns of human thinking, both for correct and incorrect cases. The Erotetic Theory of Reason (ETR) provides a symbolic generative model of both human success and failure in thinking, across propositional, quantified, and probabilistic reasoning, as well as decision-making. We presented GPT-3, GPT-3.5, and GPT-4 with 61 central inference and judgment problems from a recent book-length presentation of ETR, consisting of experimentally verified data-points on human judgment and extrapolated data-points predicted by ETR, with correct inference patterns as well as fallacies and framing effects (the ETR61 benchmark). ETR61 includes classics like Wason's card task, illusory inferences, the decoy effect, and
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#31616;&#21333;&#26131;&#29992;&#30340;&#28145;&#24230;&#27169;&#22411;&#65292;&#21033;&#29992;&#32467;&#26500;&#21270; Koopman &#30697;&#38453;&#21644;&#35299;&#20851;&#32852;&#23454;&#29616;&#22810;&#22240;&#32032;&#21435;&#20851;&#32852;&#30340;&#20219;&#21153;&#65292;&#21487;&#20197;&#36890;&#36807;&#35889;&#25439;&#22833;&#39033;&#24471;&#21040;&#21435;&#20851;&#32852;&#34920;&#31034;&#65292;&#22312;&#21160;&#24577;&#32441;&#29702;&#21644;&#21160;&#20316;&#25429;&#25417;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#26032;&#30340;&#21435;&#20851;&#32852;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.17264</link><description>&lt;p&gt;
&#36890;&#36807;&#32467;&#26500;&#21270; Koopman &#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#22810;&#22240;&#32032;&#39034;&#24207;&#21435;&#20851;&#32852;
&lt;/p&gt;
&lt;p&gt;
Multifactor Sequential Disentanglement via Structured Koopman Autoencoders. (arXiv:2303.17264v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17264
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#31616;&#21333;&#26131;&#29992;&#30340;&#28145;&#24230;&#27169;&#22411;&#65292;&#21033;&#29992;&#32467;&#26500;&#21270; Koopman &#30697;&#38453;&#21644;&#35299;&#20851;&#32852;&#23454;&#29616;&#22810;&#22240;&#32032;&#21435;&#20851;&#32852;&#30340;&#20219;&#21153;&#65292;&#21487;&#20197;&#36890;&#36807;&#35889;&#25439;&#22833;&#39033;&#24471;&#21040;&#21435;&#20851;&#32852;&#34920;&#31034;&#65292;&#22312;&#21160;&#24577;&#32441;&#29702;&#21644;&#21160;&#20316;&#25429;&#25417;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#26032;&#30340;&#21435;&#20851;&#32852;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22797;&#26434;&#25968;&#25454;&#21435;&#20851;&#32852;&#25104;&#20026;&#20854;&#28508;&#22312;&#21464;&#21270;&#22240;&#32032;&#26159;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#20851;&#20110;&#39034;&#24207;&#21435;&#20851;&#32852;&#30340;&#24037;&#20316;&#22823;&#22810;&#25552;&#20379;&#20004;&#20010;&#22240;&#32032;&#34920;&#31034;&#65292;&#21363;&#23558;&#25968;&#25454;&#20998;&#31163;&#20026;&#26102;&#21464;&#21644;&#26102;&#19981;&#21464;&#22240;&#32032;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#32771;&#34385;&#22810;&#22240;&#32032;&#21435;&#20851;&#32852;&#65292;&#20854;&#20013;&#29983;&#25104;&#22810;&#20010;&#65288;&#36229;&#20986;&#20004;&#20010;&#65289;&#35821;&#20041;&#21435;&#20851;&#32852;&#32452;&#20214;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#24378;&#22823;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#21363;&#25105;&#20204;&#20551;&#35774;&#28508;&#22312;&#21160;&#24577;&#21487;&#20197;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#32447;&#24615;&#34920;&#36798;&#12290;&#22312;&#36825;&#31181;&#20551;&#35774;&#19979;&#65292;&#21033;&#29992;&#26368;&#36817;&#24341;&#20837;&#30340; Koopman &#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#23601;&#21464;&#24471;&#33258;&#28982;&#12290;&#20294;&#26159;&#65292;&#22312; Koopman &#26041;&#27861;&#20013;&#24182;&#19981;&#33021;&#20445;&#35777;&#24471;&#21040;&#21435;&#20851;&#32852;&#34920;&#31034;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35889;&#25439;&#22833;&#39033;&#65292;&#21487;&#20197;&#24471;&#21040;&#32467;&#26500;&#21270; Koopman &#30697;&#38453;&#21644;&#35299;&#20851;&#32852;&#12290;&#24635;&#20307;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#26131;&#29992;&#30340;&#26032;&#28145;&#24230;&#27169;&#22411;&#65292;&#23436;&#20840;&#26080;&#30417;&#30563;&#65292;&#25903;&#25345;&#22810;&#22240;&#32032;&#21435;&#20851;&#32852;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#26032;&#21435;&#20851;&#32852;&#33021;&#21147;&#65292;&#21253;&#25324;&#21160;&#24577;&#32441;&#29702;&#21644;&#21160;&#20316;&#25429;&#25417;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disentangling complex data to its latent factors of variation is a fundamental task in representation learning. Existing work on sequential disentanglement mostly provides two factor representations, i.e., it separates the data to time-varying and time-invariant factors. In contrast, we consider multifactor disentanglement in which multiple (more than two) semantic disentangled components are generated. Key to our approach is a strong inductive bias where we assume that the underlying dynamics can be represented linearly in the latent space. Under this assumption, it becomes natural to exploit the recently introduced Koopman autoencoder models. However, disentangled representations are not guaranteed in Koopman approaches, and thus we propose a novel spectral loss term which leads to structured Koopman matrices and disentanglement. Overall, we propose a simple and easy to code new deep model that is fully unsupervised and it supports multifactor disentanglement. We showcase new disenta
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#25991;&#31456;&#25581;&#31034;&#20102;&#20851;&#20110;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#30340;&#26222;&#36941;&#35823;&#35299;&#65292;&#24378;&#35843;&#38656;&#35201;&#20197;&#20005;&#35880;&#12289;&#20844;&#27491;&#21644;&#36127;&#36131;&#20219;&#30340;&#26041;&#24335;&#35752;&#35770;&#34394;&#20551;&#20449;&#24687;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2303.17251</link><description>&lt;p&gt;
&#25581;&#24320;&#23545;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#30340;&#35823;&#35299;
&lt;/p&gt;
&lt;p&gt;
Demystifying Misconceptions in Social Bots Research. (arXiv:2303.17251v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17251
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#25581;&#31034;&#20102;&#20851;&#20110;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#30340;&#26222;&#36941;&#35823;&#35299;&#65292;&#24378;&#35843;&#38656;&#35201;&#20197;&#20005;&#35880;&#12289;&#20844;&#27491;&#21644;&#36127;&#36131;&#20219;&#30340;&#26041;&#24335;&#35752;&#35770;&#34394;&#20551;&#20449;&#24687;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#26426;&#22120;&#20154;&#31185;&#23398;&#23547;&#27714;&#35299;&#20915;&#32593;&#32476;&#34394;&#20551;&#20449;&#24687;&#26368;&#21463;&#20105;&#35758;&#30340;&#24418;&#24335;&#20043;&#19968;&#30340;&#30693;&#35782;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#21463;&#21040;&#26222;&#36941;&#30340;&#20559;&#35265;&#12289;&#22840;&#22823;&#30340;&#32467;&#26524;&#21644;&#35823;&#35299;&#30340;&#22256;&#25200;&#65292;&#36825;&#20123;&#37117;&#20026;&#27495;&#20041;&#12289;&#19981;&#20999;&#23454;&#38469;&#30340;&#26399;&#26395;&#21644;&#30475;&#20284;&#26080;&#27861;&#35843;&#21644;&#30340;&#21457;&#29616;&#25171;&#19979;&#20102;&#22522;&#30784;&#12290;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#23545;&#20110;&#30830;&#20445;&#21487;&#38752;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#37325;&#30003;&#31185;&#23398;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#20462;&#35746;&#20102;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#20013;&#30340;&#19968;&#20123;&#26368;&#26032;&#32467;&#26524;&#65292;&#24378;&#35843;&#21644;&#32416;&#27491;&#20102;&#20107;&#23454;&#38169;&#35823;&#20197;&#21450;&#26041;&#27861;&#35770;&#21644;&#27010;&#24565;&#38382;&#39064;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#25581;&#24320;&#20102;&#26222;&#36941;&#30340;&#35823;&#35299;&#65292;&#35299;&#20915;&#20102;&#26377;&#20851;&#22914;&#20309;&#35752;&#35770;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#30340;&#22522;&#26412;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#20197;&#20005;&#35880;&#12289;&#20844;&#27491;&#21644;&#36127;&#36131;&#20219;&#30340;&#26041;&#24335;&#35752;&#35770;&#34394;&#20551;&#20449;&#24687;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#30830;&#23450;&#24182;&#39539;&#26021;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#30340;&#25903;&#25345;&#32773;&#21644;&#21453;&#23545;&#32773;&#24120;&#29992;&#30340;&#35884;&#35823;&#35770;&#35777;&#65292;&#25903;&#25345;&#36825;&#31181;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The science of social bots seeks knowledge and solutions to one of the most debated forms of online misinformation. Yet, social bots research is plagued by widespread biases, hyped results, and misconceptions that set the stage for ambiguities, unrealistic expectations, and seemingly irreconcilable findings. Overcoming such issues is instrumental towards ensuring reliable solutions and reaffirming the validity of the scientific method. In this contribution we revise some recent results in social bots research, highlighting and correcting factual errors as well as methodological and conceptual issues. More importantly, we demystify common misconceptions, addressing fundamental points on how social bots research is discussed. Our analysis surfaces the need to discuss misinformation research in a rigorous, unbiased, and responsible way. This article bolsters such effort by identifying and refuting common fallacious arguments used by both proponents and opponents of social bots research as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#19978;&#22522;&#30784;&#30340;&#28145;&#24230;MvC&#26041;&#27861;&#65288;MvCAN&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#23454;&#38469;&#22330;&#26223;&#20013;&#22024;&#26434;&#35270;&#22270;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#29616;&#22810;&#35270;&#22270;&#19968;&#33268;&#24615;&#12289;&#20114;&#34917;&#24615;&#21644;&#22122;&#22768;&#40065;&#26834;&#24615;&#26469;&#20943;&#23569;&#22024;&#26434;&#35270;&#22270;&#30340;&#21103;&#20316;&#29992;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;MvC&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.17245</link><description>&lt;p&gt;
&#30740;&#31350;&#21644;&#20943;&#36731;&#22810;&#35270;&#35282;&#32858;&#31867;&#20013;&#23454;&#38469;&#22330;&#26223;&#20013;&#22024;&#26434;&#35270;&#22270;&#30340;&#21103;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Investigating and Mitigating the Side Effects of Noisy Views in Multi-view Clustering in Practical Scenarios. (arXiv:2303.17245v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#19978;&#22522;&#30784;&#30340;&#28145;&#24230;MvC&#26041;&#27861;&#65288;MvCAN&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#23454;&#38469;&#22330;&#26223;&#20013;&#22024;&#26434;&#35270;&#22270;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#29616;&#22810;&#35270;&#22270;&#19968;&#33268;&#24615;&#12289;&#20114;&#34917;&#24615;&#21644;&#22122;&#22768;&#40065;&#26834;&#24615;&#26469;&#20943;&#23569;&#22024;&#26434;&#35270;&#22270;&#30340;&#21103;&#20316;&#29992;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;MvC&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#32858;&#31867;&#65288;MvC&#65289;&#26088;&#22312;&#25506;&#32034;&#22810;&#35270;&#22270;&#25968;&#25454;&#30340;&#31867;&#21035;&#32467;&#26500;&#65292;&#32780;&#26080;&#38656;&#26631;&#31614;&#30417;&#30563;&#12290;&#22810;&#35270;&#22270;&#27604;&#21333;&#35270;&#22270;&#25552;&#20379;&#26356;&#22810;&#20449;&#24687;&#65292;&#22240;&#27492;&#29616;&#26377;&#30340;MvC&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#22914;&#26524;&#35270;&#22270;&#22024;&#26434;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#20005;&#37325;&#36864;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#27491;&#24335;&#30740;&#31350;&#20102;&#22024;&#26434;&#35270;&#22270;&#30340;&#32570;&#28857;&#65292;&#38543;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#22522;&#30784;&#30340;&#28145;&#24230;MvC&#26041;&#27861;&#65288;&#31216;&#20026;MvCAN&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;MvC&#30446;&#26631;&#65292;&#20351;&#24471;&#19981;&#20849;&#20139;&#21442;&#25968;&#21644;&#19981;&#19968;&#33268;&#30340;&#32858;&#31867;&#39044;&#27979;&#21487;&#20197;&#36328;&#36234;&#22810;&#20010;&#35270;&#22270;&#65292;&#20197;&#20943;&#23569;&#22024;&#26434;&#35270;&#22270;&#30340;&#21103;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#36845;&#20195;&#36807;&#31243;&#65292;&#20197;&#29983;&#25104;&#19968;&#20010;&#31283;&#20581;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#20197;&#25366;&#25496;&#22810;&#20010;&#35270;&#22270;&#30340;&#26377;&#29992;&#20449;&#24687;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;MvCAN&#30340;&#24037;&#20316;&#26159;&#36890;&#36807;&#23454;&#29616;&#22810;&#35270;&#22270;&#19968;&#33268;&#24615;&#65292;&#20114;&#34917;&#24615;&#21644;&#22122;&#22768;&#40065;&#26834;&#24615;&#26469;&#23454;&#29616;&#30340;&#12290;&#26368;&#21518;&#65292;&#23545;&#20844;&#24320;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#26032;&#25910;&#38598;&#30340;&#23454;&#38469;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;MvCAN&#22312;&#22788;&#29702;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#22024;&#26434;&#35270;&#22270;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;MvC&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-view clustering (MvC) aims at exploring the category structure among multi-view data without label supervision. Multiple views provide more information than single views and thus existing MvC methods can achieve satisfactory performance. However, their performance might seriously degenerate when the views are noisy in practical scenarios. In this paper, we first formally investigate the drawback of noisy views and then propose a theoretically grounded deep MvC method (namely MvCAN) to address this issue. Specifically, we propose a novel MvC objective that enables un-shared parameters and inconsistent clustering predictions across multiple views to reduce the side effects of noisy views. Furthermore, a non-parametric iterative process is designed to generate a robust learning target for mining multiple views' useful information. Theoretical analysis reveals that MvCAN works by achieving the multi-view consistency, complementarity, and noise robustness. Finally, experiments on publ
&lt;/p&gt;</description></item><item><title>Shapley Chains &#23558; Shapley &#20540;&#25193;&#23637;&#21040;&#20998;&#31867;&#22120;&#38142;&#19978;&#65292;&#36890;&#36807;&#32771;&#34385;&#26631;&#31614;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#25552;&#20379;&#20102;&#26356;&#23436;&#25972;&#30340;&#29305;&#24449;&#36129;&#29486;&#24402;&#23646;&#35299;&#37322;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#22810;&#36755;&#20986;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.17243</link><description>&lt;p&gt;
Shapley Chains: &#23558; Shapley &#20540;&#25193;&#23637;&#21040;&#20998;&#31867;&#22120;&#38142;&#19978;
&lt;/p&gt;
&lt;p&gt;
Shapley Chains: Extending Shapley Values to Classifier Chains. (arXiv:2303.17243v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17243
&lt;/p&gt;
&lt;p&gt;
Shapley Chains &#23558; Shapley &#20540;&#25193;&#23637;&#21040;&#20998;&#31867;&#22120;&#38142;&#19978;&#65292;&#36890;&#36807;&#32771;&#34385;&#26631;&#31614;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#25552;&#20379;&#20102;&#26356;&#23436;&#25972;&#30340;&#29305;&#24449;&#36129;&#29486;&#24402;&#23646;&#35299;&#37322;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#22810;&#36755;&#20986;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20294;&#35299;&#37322;&#22810;&#20010;&#36755;&#20986;&#39044;&#27979;&#30340;&#26041;&#27861;&#36824;&#27809;&#26377;&#24471;&#21040;&#24191;&#27867;&#35299;&#20915;&#12290;&#20351;&#29992; Shapley &#20540;&#23558;&#29305;&#24449;&#36129;&#29486;&#24402;&#22240;&#20110;&#20915;&#31574;&#36807;&#31243;&#26159;&#35299;&#37322;&#23616;&#37096;&#20010;&#20307;&#21644;&#20840;&#23616;&#39044;&#27979;&#30340;&#26368;&#27969;&#34892;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#36755;&#20986;&#20219;&#21153;&#20013;&#65292;&#36825;&#20123;&#26041;&#27861;&#30001;&#20110;&#21333;&#29420;&#32771;&#34385;&#27599;&#20010;&#36755;&#20986;&#32780;&#26080;&#27861;&#25552;&#20379;&#23436;&#25972;&#30340;&#29305;&#24449;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; Shapley Chains&#65292;&#36890;&#36807;&#22312;&#35299;&#37322;&#35774;&#35745;&#36807;&#31243;&#20013;&#21253;&#21547;&#26631;&#31614;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#20351;&#29992;&#20998;&#31867;&#22120;&#38142;&#23558; Shapley &#20540;&#20998;&#37197;&#20026;&#22810;&#36755;&#20986;&#20998;&#31867;&#20013;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#25968;&#65292;&#20174;&#32780;&#21306;&#20998;&#36825;&#20123;&#29305;&#24449;&#20998;&#25968;&#30340;&#30452;&#25509;&#21644;&#38388;&#25509;&#24433;&#21709;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26356;&#23436;&#25972;&#22320;&#23558;&#29305;&#24449;&#36129;&#29486;&#24402;&#23646;&#20110;&#22810;&#36755;&#20986;&#20998;&#31867;&#20219;&#21153;&#30340;&#39044;&#27979;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In spite of increased attention on explainable machine learning models, explaining multi-output predictions has not yet been extensively addressed. Methods that use Shapley values to attribute feature contributions to the decision making are one of the most popular approaches to explain local individual and global predictions. By considering each output separately in multi-output tasks, these methods fail to provide complete feature explanations. We propose Shapley Chains to overcome this issue by including label interdependencies in the explanation design process. Shapley Chains assign Shapley values as feature importance scores in multi-output classification using classifier chains, by separating the direct and indirect influence of these feature scores. Compared to existing methods, this approach allows to attribute a more complete feature contribution to the predictions of multi-output classification tasks. We provide a mechanism to distribute the hidden contributions of the output
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#33258;&#30417;&#30563;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#29992;&#30340;&#26631;&#31614;&#26469;&#27867;&#21270;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#36830;&#32493;&#24494;&#35843;&#26469;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2303.17235</link><description>&lt;p&gt;
&#23454;&#29992;&#30340;&#33258;&#30417;&#30563;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65306;&#36830;&#32493;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Practical self-supervised continual learning with continual fine-tuning. (arXiv:2303.17235v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#33258;&#30417;&#30563;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#29992;&#30340;&#26631;&#31614;&#26469;&#27867;&#21270;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#36830;&#32493;&#24494;&#35843;&#26469;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#32780;&#22312;&#36830;&#32493;&#23398;&#20064;&#24773;&#26223;&#20013;&#65292;&#27169;&#22411;&#20173;&#28982;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20351;&#29992;&#20219;&#20309;&#27493;&#39588;&#20013;&#30340;&#21487;&#29992;&#26631;&#31614;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#27867;&#21270;&#33258;&#30417;&#30563;&#36830;&#32493;&#23398;&#20064;&#65292;&#24182;&#19988;&#33021;&#22815;&#25552;&#20379;&#26356;&#22810;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has shown remarkable performance in computer vision tasks when trained offline. However, in a Continual Learning (CL) scenario where new data is introduced progressively, models still suffer from catastrophic forgetting. Retraining a model from scratch to adapt to newly generated data is time-consuming and inefficient. Previous approaches suggested re-purposing self-supervised objectives with knowledge distillation to mitigate forgetting across tasks, assuming that labels from all tasks are available during fine-tuning. In this paper, we generalize self-supervised continual learning in a practical setting where available labels can be leveraged in any step of the SSL process. With an increasing number of continual tasks, this offers more flexibility in the pre-training and fine-tuning phases. With Kaizen, we introduce a training architecture that is able to mitigate catastrophic forgetting for both the feature extractor and classifier with a carefully des
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#28508;&#22312;&#20301;&#32622;&#27169;&#22411;&#19978;&#30340;&#22270;&#24418;Nadaraya-Watson&#20272;&#35745;&#22120;&#30340;&#24615;&#36136;&#65292;&#23545;&#20110;&#26356;&#22797;&#26434;&#30340;&#26041;&#27861;&#26377;&#29702;&#35770;&#25351;&#23548;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2303.17229</link><description>&lt;p&gt;
&#28508;&#22312;&#20301;&#32622;&#27169;&#22411;&#19978;&#30340;&#22270;&#24418;Nadaraya-Watson&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
The Graphical Nadaraya-Watson Estimator on Latent Position Models. (arXiv:2303.17229v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17229
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#28508;&#22312;&#20301;&#32622;&#27169;&#22411;&#19978;&#30340;&#22270;&#24418;Nadaraya-Watson&#20272;&#35745;&#22120;&#30340;&#24615;&#36136;&#65292;&#23545;&#20110;&#26356;&#22797;&#26434;&#30340;&#26041;&#27861;&#26377;&#29702;&#35770;&#25351;&#23548;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#26377;&#26631;&#35760;&#33410;&#28857;&#30340;&#22270;&#24418;&#65292;&#25105;&#20204;&#23545;&#20272;&#35745;&#22120;&#30340;&#36136;&#37327;&#24863;&#20852;&#36259;&#65292;&#35813;&#20272;&#35745;&#22120;&#38024;&#23545;&#26410;&#26631;&#35760;&#33410;&#28857;&#39044;&#27979;&#20854;&#26631;&#35760;&#37051;&#23621;&#30340;&#35266;&#27979;&#20540;&#30340;&#24179;&#22343;&#20540;&#12290;&#25105;&#20204;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#20005;&#26684;&#30740;&#31350;&#20102;&#27987;&#24230;&#23646;&#24615;&#12289;&#26041;&#24046;&#30028;&#21644;&#39118;&#38505;&#30028;&#12290;&#34429;&#28982;&#20272;&#35745;&#22120;&#26412;&#36523;&#38750;&#24120;&#31616;&#21333;&#65292;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#36807;&#20110;&#29702;&#24819;&#65292;&#20294;&#25105;&#20204;&#30456;&#20449;&#25105;&#20204;&#30340;&#23567;&#27493;&#39588;&#23558;&#26377;&#21161;&#20110;&#26356;&#22797;&#26434;&#26041;&#27861;&#65288;&#22914;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a graph with a subset of labeled nodes, we are interested in the quality of the averaging estimator which for an unlabeled node predicts the average of the observations of its labeled neighbours. We rigorously study concentration properties, variance bounds and risk bounds in this context. While the estimator itself is very simple and the data generating process is too idealistic for practical applications, we believe that our small steps will contribute towards the theoretical understanding of more sophisticated methods such as Graph Neural Networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;FPGA&#35774;&#22791;&#30340;&#22522;&#20110;&#24310;&#36831;&#30340;3D-CNN&#21152;&#36895;&#22120;&#24037;&#20855;&#38142;HARFLOW3D&#65292;&#23427;&#20197;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;FPGA&#30340;&#29305;&#24615;&#25551;&#36848;&#20026;&#36755;&#20837;&#65292;&#29983;&#25104;&#26368;&#23567;&#21270;&#35745;&#31639;&#24310;&#36831;&#30340;&#35774;&#35745;&#12290;&#23454;&#39564;&#35777;&#26126;HARFLOW3D&#30456;&#27604;&#20854;&#20182;&#26041;&#26696;&#33021;&#22815;&#23454;&#29616;&#26356;&#20302;&#30340;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2303.17218</link><description>&lt;p&gt;
HARFLOW3D&#65306;&#19968;&#31181;&#38754;&#21521;FPGA&#35774;&#22791;&#30340;&#22522;&#20110;&#24310;&#36831;&#30340;3D-CNN&#21152;&#36895;&#22120;&#24037;&#20855;&#38142;
&lt;/p&gt;
&lt;p&gt;
HARFLOW3D: A Latency-Oriented 3D-CNN Accelerator Toolflow for HAR on FPGA Devices. (arXiv:2303.17218v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;FPGA&#35774;&#22791;&#30340;&#22522;&#20110;&#24310;&#36831;&#30340;3D-CNN&#21152;&#36895;&#22120;&#24037;&#20855;&#38142;HARFLOW3D&#65292;&#23427;&#20197;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;FPGA&#30340;&#29305;&#24615;&#25551;&#36848;&#20026;&#36755;&#20837;&#65292;&#29983;&#25104;&#26368;&#23567;&#21270;&#35745;&#31639;&#24310;&#36831;&#30340;&#35774;&#35745;&#12290;&#23454;&#39564;&#35777;&#26126;HARFLOW3D&#30456;&#27604;&#20854;&#20182;&#26041;&#26696;&#33021;&#22815;&#23454;&#29616;&#26356;&#20302;&#30340;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#35777;&#26126;&#22312;&#20154;&#20307;&#21160;&#20316;&#35782;&#21035;&#20219;&#21153;&#20013;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27969;&#24335;&#26550;&#26500;&#30340;&#24037;&#20855;&#38142;&#65292;&#23558;&#27492;&#31867;&#27169;&#22411;&#26144;&#23556;&#21040;FPGA&#19978;&#65292;&#32771;&#34385;&#27169;&#22411;&#22266;&#26377;&#29305;&#24615;&#21644;&#30446;&#26631;FPGA&#35774;&#22791;&#30340;&#29305;&#24449;&#12290;HARFLOW3D&#24037;&#20855;&#38142;&#20197;ONNX&#26684;&#24335;&#30340;3D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;FPGA&#29305;&#24615;&#25551;&#36848;&#20026;&#36755;&#20837;&#65292;&#29983;&#25104;&#26368;&#23567;&#21270;&#35745;&#31639;&#24310;&#36831;&#30340;&#35774;&#35745;&#12290;&#35813;&#24037;&#20855;&#38142;&#30001;&#22810;&#20010;&#37096;&#20998;&#32452;&#25104;&#65292;&#21253;&#25324;i) 3D CNN&#35299;&#26512;&#22120;&#65292;ii) &#24615;&#33021;&#21644;&#36164;&#28304;&#27169;&#22411;&#65292;iii) &#29992;&#20110;&#22312;&#29983;&#25104;&#30340;&#30828;&#20214;&#19978;&#25191;&#34892;3D&#27169;&#22411;&#30340;&#35843;&#24230;&#31639;&#27861;&#65292;iv) &#38024;&#23545;3D&#27169;&#22411;&#37327;&#36523;&#23450;&#21046;&#30340;&#36164;&#28304;&#24863;&#30693;&#20248;&#21270;&#24341;&#25806;&#65292;v) &#33258;&#21160;&#26144;&#23556;&#21040;&#21487;&#21512;&#25104;&#30340;FPGA&#20195;&#30721;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;3D CNN&#21644;FPGA&#31995;&#32479;&#37197;&#23545;&#36827;&#34892;&#22810;&#20010;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#24037;&#20855;&#38142;&#25903;&#25345;&#24191;&#27867;&#27169;&#22411;&#21644;&#35774;&#22791;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;3D CNN&#21152;&#36895;&#22120;&#35774;&#35745;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#24037;&#20855;&#38142;&#23454;&#29616;&#20102;&#26356;&#20302;&#30340;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;
For Human Action Recognition tasks (HAR), 3D Convolutional Neural Networks have proven to be highly effective, achieving state-of-the-art results. This study introduces a novel streaming architecture based toolflow for mapping such models onto FPGAs considering the model's inherent characteristics and the features of the targeted FPGA device. The HARFLOW3D toolflow takes as input a 3D CNN in ONNX format and a description of the FPGA characteristics, generating a design that minimizes the latency of the computation. The toolflow is comprised of a number of parts, including i) a 3D CNN parser, ii) a performance and resource model, iii) a scheduling algorithm for executing 3D models on the generated hardware, iv) a resource-aware optimization engine tailored for 3D models, v) an automated mapping to synthesizable code for FPGAs. The ability of the toolflow to support a broad range of models and devices is shown through a number of experiments on various 3D CNN and FPGA system pairs. Furth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MAHALO&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#32479;&#19968;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#22522;&#20110;&#35266;&#27979;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#24110;&#21161;&#22788;&#29702;&#25968;&#25454;&#38598;&#36136;&#37327;&#19981;&#20339;&#30340;&#24773;&#20917;&#19979;&#30340;&#39034;&#24207;&#20915;&#31574;&#21046;&#23450;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17156</link><description>&lt;p&gt;
MAHALO: &#32479;&#19968;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#22522;&#20110;&#35266;&#27979;&#30340;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MAHALO: Unifying Offline Reinforcement Learning and Imitation Learning from Observations. (arXiv:2303.17156v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MAHALO&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#32479;&#19968;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#22522;&#20110;&#35266;&#27979;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#24110;&#21161;&#22788;&#29702;&#25968;&#25454;&#38598;&#36136;&#37327;&#19981;&#20339;&#30340;&#24773;&#20917;&#19979;&#30340;&#39034;&#24207;&#20915;&#31574;&#21046;&#23450;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21517;&#20026;&#31163;&#32447;&#35266;&#27979;&#23398;&#20064;&#65288;PLfO&#65289;&#30340;&#26032;&#30340;&#39034;&#24207;&#20915;&#31574;&#21046;&#23450;&#33539;&#20363;&#12290;&#31163;&#32447;PLfO&#26088;&#22312;&#20351;&#29992;&#36136;&#37327;&#19981;&#20339;&#30340;&#25968;&#25454;&#38598;&#26469;&#23398;&#20064;&#31574;&#30053;&#65306;1&#65289;&#20165;&#26377;&#19968;&#37096;&#20998;&#36712;&#36857;&#34987;&#26631;&#35760;&#20026;&#22870;&#21169;&#65292;2&#65289;&#26631;&#35760;&#36712;&#36857;&#21487;&#33021;&#19981;&#21253;&#21547;&#21160;&#20316;&#65292;3&#65289;&#26631;&#35760;&#36712;&#36857;&#21487;&#33021;&#36136;&#37327;&#19981;&#39640;&#65292;4&#65289;&#24635;&#20307;&#25968;&#25454;&#21487;&#33021;&#19981;&#20855;&#22791;&#20840;&#38754;&#24615;&#12290;&#36825;&#20123;&#32570;&#38519;&#22312;&#30495;&#23454;&#30340;&#23398;&#20064;&#22330;&#26223;&#20013;&#24456;&#24120;&#35265;&#65292;&#22240;&#27492;&#31163;&#32447;PLfO&#21253;&#25324;&#35768;&#22810;&#29616;&#26377;&#30340;&#31163;&#32447;&#23398;&#20064;&#35774;&#32622;&#65292;&#21253;&#25324;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#65292;ILfO&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27169;&#24577;&#19981;&#21487;&#30693;&#23545;&#25239;&#20551;&#35774;&#36866;&#24212;&#23398;&#20064;&#30340;&#31163;&#32447;PLfO&#30340;&#36890;&#29992;&#26041;&#27861;&#65288;MAHALO&#65289;&#12290; MAHALO&#22522;&#20110;&#31163;&#32447;RL&#20013;&#30340;&#24754;&#35266;&#20027;&#20041;&#27010;&#24565;&#65292;&#20351;&#29992;&#32771;&#34385;&#30001;&#20110;&#25968;&#25454;&#38598;&#19981;&#36275;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#24615;&#33021;&#19979;&#30028;&#26469;&#20248;&#21270;&#31574;&#30053;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#26377;&#20851;&#35266;&#23519;&#21644;&#21160;&#20316;&#27169;&#24577;&#30340;&#26377;&#38480;&#20551;&#35774;&#36827;&#34892;&#23545;&#25239;&#21464;&#25442;&#26469;&#23454;&#29616;&#36825;&#19968;&#24819;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MAHALO&#22312;&#21508;&#31181;&#31163;&#32447;PLfO&#20219;&#21153;&#19978;&#37117;&#24456;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a new paradigm for sequential decision making, called offline Policy Learning from Observation (PLfO). Offline PLfO aims to learn policies using datasets with substandard qualities: 1) only a subset of trajectories is labeled with rewards, 2) labeled trajectories may not contain actions, 3) labeled trajectories may not be of high quality, and 4) the overall data may not have full coverage. Such imperfection is common in real-world learning scenarios, so offline PLfO encompasses many existing offline learning setups, including offline imitation learning (IL), ILfO, and reinforcement learning (RL). In this work, we present a generic approach, called Modality-agnostic Adversarial Hypothesis Adaptation for Learning from Observations (MAHALO), for offline PLfO. Built upon the pessimism concept in offline RL, MAHALO optimizes the policy using a performance lower bound that accounts for uncertainty due to the dataset's insufficient converge. We implement this idea by adversarially tr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#28151;&#21512;&#33258;&#32534;&#30721;&#22120;&#65288;MixedAE&#65289;&#29992;&#20110;&#33258;&#30417;&#30563;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#65292;&#22312;MAE&#26500;&#26550;&#19979;&#36890;&#36807;&#21516;&#28304;&#35782;&#21035;&#31561;&#36741;&#21161;&#39044;&#25991;&#26412;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#22686;&#24378;&#19979;&#30456;&#20114;&#20449;&#24687;&#22686;&#21152;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#22686;&#24378;&#20013;&#26368;&#20808;&#36827;&#30340;&#36716;&#31227;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.17152</link><description>&lt;p&gt;
&#28151;&#21512;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#33258;&#30417;&#30563;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Mixed Autoencoder for Self-supervised Visual Representation Learning. (arXiv:2303.17152v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#28151;&#21512;&#33258;&#32534;&#30721;&#22120;&#65288;MixedAE&#65289;&#29992;&#20110;&#33258;&#30417;&#30563;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#65292;&#22312;MAE&#26500;&#26550;&#19979;&#36890;&#36807;&#21516;&#28304;&#35782;&#21035;&#31561;&#36741;&#21161;&#39044;&#25991;&#26412;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#22686;&#24378;&#19979;&#30456;&#20114;&#20449;&#24687;&#22686;&#21152;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#22686;&#24378;&#20013;&#26368;&#20808;&#36827;&#30340;&#36716;&#31227;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#36890;&#36807;&#38543;&#26426;&#36974;&#30422;&#22270;&#20687;&#34917;&#19969;&#21644;&#37325;&#24314;&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;MAE&#30340;&#26377;&#25928;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#20173;&#28982;&#26159;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#19981;&#21516;&#20110;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;MAE&#30340;&#26222;&#36941;&#28151;&#21512;&#22686;&#24378;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#26420;&#32032;&#28151;&#21512;&#23558;&#30001;&#20110;&#30456;&#20114;&#20449;&#24687;&#30340;&#22686;&#21152;&#32780;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21516;&#28304;&#35782;&#21035;&#26041;&#27861;&#65292;&#19968;&#31181;&#36741;&#21161;&#30340;&#39044;&#25991;&#26412;&#20219;&#21153;&#65292;&#19981;&#20165;&#36890;&#36807;&#26126;&#30830;&#35201;&#27714;&#27599;&#20010;&#34917;&#19969;&#35782;&#21035;&#21516;&#28304;&#34917;&#19969;&#26469;&#32531;&#35299;&#30456;&#20114;&#20449;&#24687;&#30340;&#22686;&#21152;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#25191;&#34892;&#38754;&#21521;&#23545;&#35937;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#19979;&#28216;&#23494;&#38598;&#24863;&#30693;&#24615;&#33021;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#28151;&#21512;&#33258;&#32534;&#30721;&#22120;&#65288;MixedAE&#65289;&#22312;&#19981;&#21516;&#30340;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#22686;&#24378;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#36716;&#31227;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked Autoencoder (MAE) has demonstrated superior performance on various vision tasks via randomly masking image patches and reconstruction. However, effective data augmentation strategies for MAE still remain open questions, different from those in contrastive learning that serve as the most important part. This paper studies the prevailing mixing augmentation for MAE. We first demonstrate that naive mixing will in contrast degenerate model performance due to the increase of mutual information (MI). To address, we propose homologous recognition, an auxiliary pretext task, not only to alleviate the MI increasement by explicitly requiring each patch to recognize homologous patches, but also to perform object-aware self-supervised pre-training for better downstream dense perception performance. With extensive experiments, we demonstrate that our proposed Mixed Autoencoder (MixedAE) achieves the state-of-the-art transfer results among masked image modeling (MIM) augmentations on differen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#25552;&#39640;&#26080;&#32447;&#32593;&#32476;&#31649;&#29702;&#25928;&#29575;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DGMs&#30340;&#31649;&#29702;&#26694;&#26550;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#39033;DGM&#27169;&#22411;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2303.17114</link><description>&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21450;&#20854;&#22312;&#39640;&#25928;&#26080;&#32447;&#32593;&#32476;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;: &#25945;&#31243;&#21644;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Deep Generative Model and Its Applications in Efficient Wireless Network Management: A Tutorial and Case Study. (arXiv:2303.17114v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#25552;&#39640;&#26080;&#32447;&#32593;&#32476;&#31649;&#29702;&#25928;&#29575;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DGMs&#30340;&#31649;&#29702;&#26694;&#26550;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#39033;DGM&#27169;&#22411;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25193;&#25955;&#27169;&#22411;&#21644;ChatGPT&#30340;&#24778;&#20154;&#25104;&#21151;&#65292;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;(DGMs)&#27491;&#22312;&#32463;&#21382;2022&#24180;&#30340;&#29190;&#28856;&#24335;&#22686;&#38271;&#12290;&#19981;&#38480;&#20110;&#20869;&#23481;&#29983;&#25104;&#65292;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#22797;&#26434;&#27169;&#24335;&#34920;&#31034;&#33021;&#21147;&#21644;&#29983;&#25104;&#20986;&#21487;&#20449;&#26679;&#26412;&#30340;&#33021;&#21147;&#65292;DGMs&#20063;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#29289;&#32852;&#32593;&#12289;&#20803;&#23431;&#23449;&#21644;&#25968;&#23383;&#23402;&#29983;&#31561;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;DGMs&#22312;&#37325;&#35201;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#21363;&#25552;&#39640;&#26080;&#32447;&#32593;&#32476;&#31649;&#29702;&#30340;&#25928;&#29575;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;&#29983;&#25104;AI&#65292;&#20197;&#21450;&#19977;&#31181;&#20195;&#34920;&#24615;&#30340;DGMs&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DGMs&#22686;&#24378;&#30340;&#26080;&#32447;&#32593;&#32476;&#31649;&#29702;&#26694;&#26550;&#65292;&#22312;&#26694;&#26550;&#20013;&#35814;&#32454;&#35828;&#26126;&#20102;&#20256;&#32479;&#32593;&#32476;&#31649;&#29702;&#26041;&#27861;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#20026;&#20160;&#20040;DGMs&#33021;&#22815;&#39640;&#25928;&#22320;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#38416;&#36848;&#20102;&#22312;&#31649;&#29702;&#26080;&#32447;&#32593;&#32476;&#20013;&#24212;&#29992;DGMs&#30340;&#36880;&#27493;&#24037;&#20316;&#27969;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;DGM&#27169;&#22411;&#8212;&#8212;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#32593;&#32476;&#32463;&#27982;&#23398;&#19978;&#36827;&#34892;&#20102;&#19968;&#39033;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#29983;&#25104;&#26377;&#25928;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the phenomenal success of diffusion models and ChatGPT, deep generation models (DGMs) have been experiencing explosive growth from 2022. Not limited to content generation, DGMs are also widely adopted in Internet of Things, Metaverse, and digital twin, due to their outstanding ability to represent complex patterns and generate plausible samples. In this article, we explore the applications of DGMs in a crucial task, i.e., improving the efficiency of wireless network management. Specifically, we firstly overview the generative AI, as well as three representative DGMs. Then, a DGM-empowered framework for wireless network management is proposed, in which we elaborate the issues of the conventional network management approaches, why DGMs can address them efficiently, and the step-by-step workflow for applying DGMs in managing wireless networks. Moreover, we conduct a case study on network economics, using the state-of-the-art DGM model, i.e., diffusion model, to generate effective con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#27010;&#29575;&#35302;&#21457;&#33218;&#30340;&#24773;&#22659;&#32452;&#21512;&#36172;&#21338;&#26426;&#65292;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#35774;&#35745;&#20102;C$^2$-UCB-T&#31639;&#27861;&#21644;VAC$^2$-UCB&#31639;&#27861;&#65292;&#24182;&#20998;&#21035;&#23548;&#20986;&#20102;&#23545;&#24212;&#30340;&#36951;&#25022;&#20540;&#19978;&#38480;&#65292;&#20026;&#30456;&#20851;&#24212;&#29992;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2303.17110</link><description>&lt;p&gt;
&#24102;&#26377;&#27010;&#29575;&#35302;&#21457;&#33218;&#30340;&#24773;&#22659;&#32452;&#21512;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Contextual Combinatorial Bandits with Probabilistically Triggered Arms. (arXiv:2303.17110v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#27010;&#29575;&#35302;&#21457;&#33218;&#30340;&#24773;&#22659;&#32452;&#21512;&#36172;&#21338;&#26426;&#65292;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#35774;&#35745;&#20102;C$^2$-UCB-T&#31639;&#27861;&#21644;VAC$^2$-UCB&#31639;&#27861;&#65292;&#24182;&#20998;&#21035;&#23548;&#20986;&#20102;&#23545;&#24212;&#30340;&#36951;&#25022;&#20540;&#19978;&#38480;&#65292;&#20026;&#30456;&#20851;&#24212;&#29992;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#25429;&#25417;&#24191;&#27867;&#24212;&#29992;&#33539;&#22260;&#30340;&#19968;&#31995;&#21015;&#24179;&#28369;&#26465;&#20214;&#19979;&#30340;&#24102;&#26377;&#27010;&#29575;&#35302;&#21457;&#33218;&#30340;&#24773;&#22659;&#32452;&#21512;&#36172;&#21338;&#26426;(C$^2$MAB-T)&#65292;&#20363;&#22914;&#24773;&#22659;&#32423;&#32852;&#36172;&#21338;&#26426;&#21644;&#24773;&#22659;&#26368;&#22823;&#21270;&#36172;&#21338;&#26426;&#12290;&#22312;&#27169;&#25311;&#35302;&#21457;&#27010;&#29575;(TPM)&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;C$^2$-UCB-T&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;$\tilde{O}(d\sqrt{KT})$&#30340;&#36951;&#25022;&#20540;&#19978;&#38480;&#65292;&#28040;&#38500;&#20102;&#19968;&#20010;&#21487;&#33021;&#25351;&#25968;&#32423;&#22686;&#38271;&#30340;&#22240;&#23376;$O(1/p_{\min})$&#65292;&#20854;&#20013;$d$&#26159;&#24773;&#22659;&#30340;&#32500;&#25968;&#65292;$p_{\min}$&#26159;&#33021;&#34987;&#35302;&#21457;&#30340;&#20219;&#20309;&#33218;&#30340;&#26368;&#23567;&#27491;&#27010;&#29575;&#65292;&#25209;&#22823;&#23567;$K$&#26159;&#27599;&#36718;&#33021;&#34987;&#35302;&#21457;&#30340;&#33218;&#30340;&#26368;&#22823;&#25968;&#37327;&#12290;&#22312;&#26041;&#24046;&#35843;&#21046;(VM)&#25110;&#35302;&#21457;&#27010;&#29575;&#21644;&#26041;&#24046;&#35843;&#21046;(TPVM)&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24046;&#33258;&#36866;&#24212;&#31639;&#27861;VAC$^2$-UCB&#65292;&#24182;&#23548;&#20986;&#20102;&#19968;&#20010;$\tilde{O}(d\sqrt{T})$&#30340;&#36951;&#25022;&#20540;&#19978;&#38480;&#65292;&#35813;&#19978;&#38480;&#19982;&#25209;&#22823;&#23567;$K$&#26080;&#20851;&#12290;&#20316;&#20026;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
We study contextual combinatorial bandits with probabilistically triggered arms (C$^2$MAB-T) under a variety of smoothness conditions that capture a wide range of applications, such as contextual cascading bandits and contextual influence maximization bandits. Under the triggering probability modulated (TPM) condition, we devise the C$^2$-UCB-T algorithm and propose a novel analysis that achieves an $\tilde{O}(d\sqrt{KT})$ regret bound, removing a potentially exponentially large factor $O(1/p_{\min})$, where $d$ is the dimension of contexts, $p_{\min}$ is the minimum positive probability that any arm can be triggered, and batch-size $K$ is the maximum number of arms that can be triggered per round. Under the variance modulated (VM) or triggering probability and variance modulated (TPVM) conditions, we propose a new variance-adaptive algorithm VAC$^2$-UCB and derive a regret bound $\tilde{O}(d\sqrt{T})$, which is independent of the batch-size $K$. As a valuable by-product, we find our a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#27491;&#21322;&#23450;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20013;&#39640;&#25928;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#27491;&#21322;&#23450;-PSD&#27169;&#22411;&#22312;&#31934;&#24230;$\varepsilon$&#19979;&#29983;&#25104;iid&#26679;&#26412;&#12290;&#31639;&#27861;&#22797;&#26434;&#24230;&#20026;$O(T d \log(1/\varepsilon) m^2 + d m^{\beta+1} \log(T)/\varepsilon^2)$&#65292;&#20854;&#20013;$T$&#26159;&#26102;&#38388;&#27493;&#25968;&#65292;$\beta$&#26159;Fokker-Planck&#35299;&#30340;&#27491;&#21017;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17109</link><description>&lt;p&gt;
&#27491;&#21322;&#23450;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#39640;&#25928;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Efficient Sampling of Stochastic Differential Equations with Positive Semi-Definite Models. (arXiv:2303.17109v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#27491;&#21322;&#23450;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20013;&#39640;&#25928;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#27491;&#21322;&#23450;-PSD&#27169;&#22411;&#22312;&#31934;&#24230;$\varepsilon$&#19979;&#29983;&#25104;iid&#26679;&#26412;&#12290;&#31639;&#27861;&#22797;&#26434;&#24230;&#20026;$O(T d \log(1/\varepsilon) m^2 + d m^{\beta+1} \log(T)/\varepsilon^2)$&#65292;&#20854;&#20013;$T$&#26159;&#26102;&#38388;&#27493;&#25968;&#65292;$\beta$&#26159;Fokker-Planck&#35299;&#30340;&#27491;&#21017;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#24050;&#30693;&#28418;&#31227;&#20989;&#25968;&#21644;&#25193;&#25955;&#30697;&#38453;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20013;&#39640;&#25928;&#37319;&#26679;&#30340;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#19968;&#20010;&#26368;&#36817;&#30340;&#27010;&#29575;&#27169;&#22411;&#65288;&#27491;&#21322;&#23450;-PSD&#27169;&#22411;&#65289;\citep{rudi2021psd}&#65292;&#20174;&#20013;&#21487;&#20197;&#33719;&#24471;&#31934;&#24230;&#20026;$\varepsilon$&#30340;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;iid&#65289;&#26679;&#26412;&#65292;&#20854;&#25104;&#26412;&#20026;$m^2 d \log(1/\varepsilon)$&#65292;&#20854;&#20013;$m$&#26159;&#27169;&#22411;&#30340;&#32500;&#24230;&#65292;$d$&#26159;&#31354;&#38388;&#30340;&#32500;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21253;&#25324;&#65306;&#39318;&#20808;&#35745;&#31639;&#28385;&#36275;&#19982;SDE&#30456;&#20851;&#32852;&#30340;Fokker-Planck&#26041;&#31243;&#65288;&#25110;&#20854;&#20998;&#25968;&#21464;&#20307;&#65289;&#30340;PSD&#27169;&#22411;&#65292;&#35823;&#24046;&#20026;$\varepsilon$&#65292;&#28982;&#21518;&#20174;&#29983;&#25104;&#30340;PSD&#27169;&#22411;&#20013;&#37319;&#26679;&#12290;&#20551;&#35774;Fokker-Planck&#35299;&#20855;&#26377;&#19968;&#23450;&#30340;&#27491;&#21017;&#24615;&#65288;&#21363;$\beta$&#38454;&#21487;&#24494;&#24615;&#20197;&#21450;&#20854;&#38646;&#28857;&#30340;&#19968;&#20123;&#20960;&#20309;&#26465;&#20214;&#65289;&#65292;&#25105;&#20204;&#24471;&#21040;&#19968;&#20010;&#31639;&#27861;&#65306;&#65288;a&#65289;&#22312;&#20934;&#22791;&#38454;&#27573;&#65292;&#33719;&#24471;&#20855;&#26377;L2&#36317;&#31163;$\varepsilon$&#30340;PSD&#27169;&#22411;&#20316;&#20026;&#30495;&#23454;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#30340;&#20272;&#35745;&#65307;&#65288;b&#65289;&#22312;&#37319;&#26679;&#38454;&#27573;&#65292;&#20197;&#31934;&#24230;$\varepsilon$&#29983;&#25104;SDE&#35299;&#30340;iid&#26679;&#26412;&#12290;&#25152;&#24471;&#21040;&#30340;&#22797;&#26434;&#24230;&#20026;$O(T d \log(1/\varepsilon) m^2 + d m^{\beta+1} \log(T)/\varepsilon^2)$&#65292;&#20854;&#20013;$T$&#26159;SDE&#30340;&#26102;&#38388;&#27493;&#25968;&#65292;$\beta$&#26159;Fokker-Planck&#35299;&#30340;&#27491;&#21017;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper deals with the problem of efficient sampling from a stochastic differential equation, given the drift function and the diffusion matrix. The proposed approach leverages a recent model for probabilities \citep{rudi2021psd} (the positive semi-definite -- PSD model) from which it is possible to obtain independent and identically distributed (i.i.d.) samples at precision $\varepsilon$ with a cost that is $m^2 d \log(1/\varepsilon)$ where $m$ is the dimension of the model, $d$ the dimension of the space. The proposed approach consists in: first, computing the PSD model that satisfies the Fokker-Planck equation (or its fractional variant) associated with the SDE, up to error $\varepsilon$, and then sampling from the resulting PSD model. Assuming some regularity of the Fokker-Planck solution (i.e. $\beta$-times differentiability plus some geometric condition on its zeros) We obtain an algorithm that: (a) in the preparatory phase obtains a PSD model with L2 distance $\varepsilon$ fr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;OpenMix&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#25298;&#32477;&#24322;&#24120;&#26679;&#26412;&#29983;&#25104;&#30340;&#20266;&#26679;&#26412;&#26469;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#20998;&#31867;&#22120;&#30340;&#21487;&#38752;&#24615;&#65292;&#20174;&#32780;&#26816;&#27979;&#24050;&#30693;&#31867;&#21035;&#30340;&#20998;&#31867;&#38169;&#35823;&#21644;&#26410;&#30693;&#31867;&#21035;&#30340;OOD&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2303.17093</link><description>&lt;p&gt;
OpenMix: &#25506;&#32034;&#24322;&#24120;&#26679;&#26412;&#20197;&#26816;&#27979;&#20998;&#31867;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
OpenMix: Exploring Outlier Samples for Misclassification Detection. (arXiv:2303.17093v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17093
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;OpenMix&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#25298;&#32477;&#24322;&#24120;&#26679;&#26412;&#29983;&#25104;&#30340;&#20266;&#26679;&#26412;&#26469;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#20998;&#31867;&#22120;&#30340;&#21487;&#38752;&#24615;&#65292;&#20174;&#32780;&#26816;&#27979;&#24050;&#30693;&#31867;&#21035;&#30340;&#20998;&#31867;&#38169;&#35823;&#21644;&#26410;&#30693;&#31867;&#21035;&#30340;OOD&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#65292;&#21487;&#38752;&#30340;&#28145;&#24230;&#31070;&#32463;&#20998;&#31867;&#22120;&#32622;&#20449;&#24230;&#20272;&#35745;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#22522;&#26412;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#23545;&#20854;&#38169;&#35823;&#39044;&#27979;&#36807;&#20110;&#33258;&#20449;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26131;&#20110;&#33719;&#21462;&#30340;&#24322;&#24120;&#26679;&#26412;&#65292;&#21363;&#26469;&#33258;&#38750;&#30446;&#26631;&#31867;&#30340;&#26410;&#26631;&#35760;&#26679;&#26412;&#65292;&#24110;&#21161;&#26816;&#27979;&#20998;&#31867;&#38169;&#35823;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21457;&#29616;&#20986;&#21517;&#30340;Outlier Exposure&#22312;&#26816;&#27979;&#26410;&#30693;&#31867;&#21035;&#30340;&#26679;&#26412;&#20013;&#38750;&#24120;&#24378;&#22823;&#65292;&#20294;&#22312;&#35782;&#21035;&#20998;&#31867;&#38169;&#35823;&#26041;&#38754;&#24182;&#27809;&#26377;&#25552;&#20379;&#20219;&#20309;&#24110;&#21161;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OpenMix&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#25298;&#32477;&#36890;&#36807;&#24322;&#24120;&#36716;&#25442;&#29983;&#25104;&#30340;&#19981;&#30830;&#23450;&#20266;&#26679;&#26412;&#26469;&#34701;&#21512;&#24320;&#25918;&#19990;&#30028;&#30340;&#30693;&#35782;&#12290;OpenMix&#22312;&#21508;&#31181;&#24773;&#22659;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#21487;&#38752;&#24615;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#24378;&#22823;&#32780;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#24050;&#30693;&#31867;&#21035;&#30340;&#20998;&#31867;&#38169;&#35823;&#21644;&#26410;&#30693;&#31867;&#21035;&#30340;OOD&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliable confidence estimation for deep neural classifiers is a challenging yet fundamental requirement in high-stakes applications. Unfortunately, modern deep neural networks are often overconfident for their erroneous predictions. In this work, we exploit the easily available outlier samples, i.e., unlabeled samples coming from non-target classes, for helping detect misclassification errors. Particularly, we find that the well-known Outlier Exposure, which is powerful in detecting out-of-distribution (OOD) samples from unknown classes, does not provide any gain in identifying misclassification errors. Based on these observations, we propose a novel method called OpenMix, which incorporates open-world knowledge by learning to reject uncertain pseudo-samples generated via outlier transformation. OpenMix significantly improves confidence reliability under various scenarios, establishing a strong and unified framework for detecting both misclassified samples from known classes and OOD sa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#27602;&#30244;&#25307;&#21215;&#8221;&#30340;&#25968;&#25454;&#25915;&#20987;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#25209;&#37327;&#37319;&#26679;&#26469;&#28151;&#28102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21363;&#36890;&#36807;&#20248;&#21270;&#32467;&#26500;&#21270;&#35757;&#32451;&#25209;&#27425;&#30340;&#27602;&#30244;&#25968;&#37327;&#26469;&#38477;&#20302;&#30446;&#26631;&#31867;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.17080</link><description>&lt;p&gt;
&#8220;&#36873;&#25321;&#24615;&#25209;&#37327;&#37319;&#26679;&#20013;&#30340;&#27602;&#30244;&#25307;&#21215;&#65306;&#19968;&#31181;&#28151;&#28102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25968;&#25454;&#25915;&#20987;&#8221;
&lt;/p&gt;
&lt;p&gt;
Mole Recruitment: Poisoning of Image Classifiers via Selective Batch Sampling. (arXiv:2303.17080v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#27602;&#30244;&#25307;&#21215;&#8221;&#30340;&#25968;&#25454;&#25915;&#20987;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#25209;&#37327;&#37319;&#26679;&#26469;&#28151;&#28102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21363;&#36890;&#36807;&#20248;&#21270;&#32467;&#26500;&#21270;&#35757;&#32451;&#25209;&#27425;&#30340;&#27602;&#30244;&#25968;&#37327;&#26469;&#38477;&#20302;&#30446;&#26631;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#27602;&#30244;&#25915;&#20987;&#65292;&#35813;&#25915;&#20987;&#19981;&#38656;&#35201;&#23545;&#22270;&#20687;&#25110;&#26631;&#31614;&#36827;&#34892;&#20219;&#20309;&#25805;&#20316;&#23601;&#21487;&#20197;&#28151;&#28102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36825;&#26159;&#36890;&#36807;&#20165;&#20165;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#20013;&#26368;&#28151;&#28102;&#30340;&#33258;&#28982;&#26679;&#26412;&#26469;&#23454;&#29616;&#30340;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#26377;&#38024;&#23545;&#24615;&#30340;&#25915;&#20987;&#24418;&#24335;&#65292;&#31216;&#20026;&#8220;&#27602;&#30244;&#25307;&#21215;&#8221;&#12290;&#25105;&#20204;&#23558;&#27602;&#30244;&#23450;&#20041;&#20026;&#19968;&#20010;&#31867;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#20854;&#19982;&#21478;&#19968;&#20010;&#31867;&#30340;&#26679;&#26412;&#26368;&#30456;&#20284;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#20248;&#21270;&#27602;&#30244;&#25968;&#37327;&#32467;&#26500;&#21270;&#35757;&#32451;&#25209;&#27425;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#30446;&#26631;&#31867;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#31163;&#32447;&#35774;&#32622;&#19979;&#23637;&#31034;&#20102;&#36825;&#31181;&#26032;&#22411;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#36830;&#32493;&#23398;&#20064; (CL) &#35774;&#32622;&#19979;&#23637;&#31034;&#20102;&#36825;&#31181;&#25915;&#20987;&#30340;&#23454;&#38469;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#8220;&#27602;&#30244;&#25307;&#21215;&#8221;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#26292;&#38706;&#20102;&#22270;&#20687;&#20998;&#31867;&#22120;&#20197;&#21069;&#26410;&#34987;&#21457;&#29616;&#30340;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present a data poisoning attack that confounds machine learning models without any manipulation of the image or label. This is achieved by simply leveraging the most confounding natural samples found within the training data itself, in a new form of a targeted attack coined "Mole Recruitment." We define moles as the training samples of a class that appear most similar to samples of another class, and show that simply restructuring training batches with an optimal number of moles can lead to significant degradation in the performance of the targeted class. We show the efficacy of this novel attack in an offline setting across several standard image classification datasets, and demonstrate the real-world viability of this attack in a continual learning (CL) setting. Our analysis reveals that state-of-the-art models are susceptible to Mole Recruitment, thereby exposing a previously undetected vulnerability of image classifiers.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#20559;&#24494;&#20998;&#26041;&#31243;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#65292;&#20027;&#35201;&#21253;&#25324;&#21457;&#29616;&#26032;&#30340;PDE&#12289;&#23398;&#20064;&#22352;&#26631;&#31995;&#32479;&#21644;&#34920;&#31034;&#35299;&#31639;&#23376;&#65292;&#26088;&#22312;&#20351;PDE&#26356;&#26131;&#20110;&#20998;&#26512;&#21644;&#25913;&#36827;&#20256;&#32479;&#30340;&#25968;&#20540;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.17078</link><description>&lt;p&gt;
&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290; &#65288;arXiv&#65306;2303.17078v1 [cs.LG]&#65289;
&lt;/p&gt;
&lt;p&gt;
Machine Learning for Partial Differential Equations. (arXiv:2303.17078v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#20559;&#24494;&#20998;&#26041;&#31243;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#65292;&#20027;&#35201;&#21253;&#25324;&#21457;&#29616;&#26032;&#30340;PDE&#12289;&#23398;&#20064;&#22352;&#26631;&#31995;&#32479;&#21644;&#34920;&#31034;&#35299;&#31639;&#23376;&#65292;&#26088;&#22312;&#20351;PDE&#26356;&#26131;&#20110;&#20998;&#26512;&#21644;&#25913;&#36827;&#20256;&#32479;&#30340;&#25968;&#20540;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#26159;&#33258;&#28982;&#29289;&#29702;&#23450;&#24459;&#20013;&#26368;&#26222;&#36941;&#21644;&#31616;&#27905;&#30340;&#25551;&#36848;&#20043;&#19968;&#65292;&#21487;&#20197;&#25429;&#25417;&#21040;&#22823;&#37327;&#19981;&#21516;&#23610;&#24230;&#29289;&#29702;&#29616;&#35937;&#30340;&#20016;&#23500;&#22810;&#26679;&#24615;&#24182;&#36827;&#34892;&#32039;&#20945;&#30340;&#31526;&#21495;&#34920;&#31034;&#12290;&#26412;&#32508;&#36848;&#23558;&#26816;&#26597;&#20960;&#20010;&#26426;&#22120;&#23398;&#20064;&#27491;&#22312;&#25512;&#36827;&#30340;&#26377;&#21069;&#36884;&#30340;PDE&#30740;&#31350;&#39046;&#22495;&#65292;&#21253;&#25324;&#65306;1&#65289;&#21457;&#29616;&#26032;&#30340;&#29992;&#20110;&#22797;&#26434;&#33258;&#28982;&#21644;&#24037;&#31243;&#31995;&#32479;&#30340;&#20027;&#23548;PDE&#21644;&#31895;&#31890;&#21270;&#36817;&#20284;&#65292;2&#65289;&#23398;&#20064;&#26377;&#25928;&#30340;&#22352;&#26631;&#31995;&#32479;&#21644;&#38477;&#38454;&#27169;&#22411;&#65292;&#20197;&#20351;PDE&#26356;&#26131;&#20998;&#26512;&#65292;&#20197;&#21450;3&#65289;&#34920;&#31034;&#35299;&#31639;&#23376;&#21644;&#25913;&#36827;&#20256;&#32479;&#30340;&#25968;&#20540;&#31639;&#27861;&#12290;&#22312;&#27599;&#20010;&#39046;&#22495;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#20851;&#38190;&#36827;&#23637;&#12289;&#25345;&#32493;&#25361;&#25112;&#21644;&#36827;&#19968;&#27493;&#21457;&#23637;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partial differential equations (PDEs) are among the most universal and parsimonious descriptions of natural physical laws, capturing a rich variety of phenomenology and multi-scale physics in a compact and symbolic representation. This review will examine several promising avenues of PDE research that are being advanced by machine learning, including: 1) the discovery of new governing PDEs and coarse-grained approximations for complex natural and engineered systems, 2) learning effective coordinate systems and reduced-order models to make PDEs more amenable to analysis, and 3) representing solution operators and improving traditional numerical algorithms. In each of these fields, we summarize key advances, ongoing challenges, and opportunities for further development.
&lt;/p&gt;</description></item><item><title>DiffCollage&#26159;&#19968;&#31181;&#32452;&#21512;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#24182;&#34892;&#29983;&#25104;&#20219;&#24847;&#23610;&#23544;&#21644;&#24418;&#29366;&#30340;&#20869;&#23481;&#65292;&#24212;&#29992;&#20110;&#26080;&#38480;&#22270;&#20687;&#29983;&#25104;&#65292;&#20840;&#26223;&#22270;&#20687;&#29983;&#25104;&#21644;&#38271;&#26102;&#38388;&#25991;&#26412;&#24341;&#23548;&#36816;&#21160;&#29983;&#25104;&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.17076</link><description>&lt;p&gt;
DiffCollage: &#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#22823;&#23610;&#23544;&#20869;&#23481;&#24182;&#34892;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DiffCollage: Parallel Generation of Large Content with Diffusion Models. (arXiv:2303.17076v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17076
&lt;/p&gt;
&lt;p&gt;
DiffCollage&#26159;&#19968;&#31181;&#32452;&#21512;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#24182;&#34892;&#29983;&#25104;&#20219;&#24847;&#23610;&#23544;&#21644;&#24418;&#29366;&#30340;&#20869;&#23481;&#65292;&#24212;&#29992;&#20110;&#26080;&#38480;&#22270;&#20687;&#29983;&#25104;&#65292;&#20840;&#26223;&#22270;&#20687;&#29983;&#25104;&#21644;&#38271;&#26102;&#38388;&#25991;&#26412;&#24341;&#23548;&#36816;&#21160;&#29983;&#25104;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;DiffCollage&#65292;&#19968;&#31181;&#32452;&#21512;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#21033;&#29992;&#24050;&#35757;&#32451;&#22312;&#29983;&#25104;&#22823;&#20869;&#23481;&#29255;&#27573;&#19978;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#22823;&#23610;&#23544;&#20869;&#23481;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#22240;&#23376;&#22270;&#34920;&#31034;&#65292;&#20854;&#20013;&#27599;&#20010;&#22240;&#23376;&#33410;&#28857;&#34920;&#31034;&#20869;&#23481;&#30340;&#19968;&#37096;&#20998;&#65292;&#21464;&#37327;&#33410;&#28857;&#34920;&#31034;&#23427;&#20204;&#30340;&#37325;&#21472;&#12290;&#35813;&#34920;&#31034;&#20801;&#35768;&#25105;&#20204;&#32858;&#21512;&#22312;&#21508;&#20010;&#33410;&#28857;&#19978;&#23450;&#20041;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#20013;&#38388;&#36755;&#20986;&#65292;&#20197;&#24182;&#34892;&#29983;&#25104;&#20219;&#24847;&#23610;&#23544;&#21644;&#24418;&#29366;&#30340;&#20869;&#23481;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#33258;&#22238;&#24402;&#29983;&#25104;&#36807;&#31243;&#12290;&#25105;&#20204;&#23558;DiffCollage&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#26080;&#38480;&#22270;&#20687;&#29983;&#25104;&#65292;&#20840;&#26223;&#22270;&#20687;&#29983;&#25104;&#21644;&#38271;&#26102;&#38388;&#25991;&#26412;&#24341;&#23548;&#36816;&#21160;&#29983;&#25104;&#12290;&#19982;&#24378;&#33258;&#22238;&#24402;&#22522;&#32447;&#30340;&#27604;&#36739;&#30340;&#24191;&#27867;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present DiffCollage, a compositional diffusion model that can generate large content by leveraging diffusion models trained on generating pieces of the large content. Our approach is based on a factor graph representation where each factor node represents a portion of the content and a variable node represents their overlap. This representation allows us to aggregate intermediate outputs from diffusion models defined on individual nodes to generate content of arbitrary size and shape in parallel without resorting to an autoregressive generation procedure. We apply DiffCollage to various tasks, including infinite image generation, panorama image generation, and long-duration text-guided motion generation. Extensive experimental results with a comparison to strong autoregressive baselines verify the effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DERA&#30340;&#23545;&#35805;&#22411;&#35299;&#20915;&#20195;&#29702;&#65292;&#35813;&#20195;&#29702;&#21033;&#29992;LLM&#30340;&#23545;&#35805;&#33021;&#21147;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#34917;&#20840;&#33021;&#21147;&#12290;DERA&#26694;&#26550;&#21270;&#20026;&#20004;&#20010;&#20195;&#29702;&#31867;&#22411;&#20043;&#38388;&#30340;&#35752;&#35770;&#65292;&#21487;&#20197;&#22312;&#21307;&#30103;&#23545;&#35805;&#25688;&#35201;&#21644;&#25252;&#29702;&#35745;&#21010;&#29983;&#25104;&#26041;&#38754;&#23454;&#29616;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2303.17071</link><description>&lt;p&gt;
DERA: &#20351;&#29992;&#23545;&#35805;&#22411;&#35299;&#20915;&#20195;&#29702;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34917;&#20840;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
DERA: Enhancing Large Language Model Completions with Dialog-Enabled Resolving Agents. (arXiv:2303.17071v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DERA&#30340;&#23545;&#35805;&#22411;&#35299;&#20915;&#20195;&#29702;&#65292;&#35813;&#20195;&#29702;&#21033;&#29992;LLM&#30340;&#23545;&#35805;&#33021;&#21147;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#34917;&#20840;&#33021;&#21147;&#12290;DERA&#26694;&#26550;&#21270;&#20026;&#20004;&#20010;&#20195;&#29702;&#31867;&#22411;&#20043;&#38388;&#30340;&#35752;&#35770;&#65292;&#21487;&#20197;&#22312;&#21307;&#30103;&#23545;&#35805;&#25688;&#35201;&#21644;&#25252;&#29702;&#35745;&#21010;&#29983;&#25104;&#26041;&#38754;&#23454;&#29616;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#25104;&#20026;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#12290;&#22312;&#21307;&#30103;&#31561;&#20851;&#20046;&#23433;&#20840;&#24615;&#30340;&#24212;&#29992;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#21462;&#20915;&#20110;&#23427;&#20204;&#29983;&#25104;&#30340;&#36755;&#20986;&#26159;&#21542;&#20855;&#22791;&#20107;&#23454;&#20934;&#30830;&#21644;&#23436;&#25972;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#35805;&#22411;&#35299;&#20915;&#20195;&#29702;&#65288;DERA&#65289;&#12290;DERA&#26159;&#19968;&#31181;&#30001;LLM&#65288;&#29305;&#21035;&#26159;GPT-4&#65289;&#30340;&#23545;&#35805;&#33021;&#21147;&#25552;&#20379;&#25903;&#25345;&#30340;&#27169;&#24335;&#65292;&#23427;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#19988;&#26131;&#20110;&#29702;&#35299;&#30340;&#35770;&#22363;&#65292;&#29992;&#20110;&#27807;&#36890;&#21453;&#39304;&#24182;&#36845;&#20195;&#25913;&#36827;&#36755;&#20986;&#12290;&#25105;&#20204;&#23558;&#23545;&#35805;&#26694;&#26550;&#21270;&#20026;&#20004;&#20010;&#20195;&#29702;&#31867;&#22411;&#20043;&#38388;&#30340;&#35752;&#35770;&#65306;&#19968;&#20010;&#30740;&#31350;&#20154;&#21592;&#65292;&#36127;&#36131;&#22788;&#29702;&#20449;&#24687;&#24182;&#35782;&#21035;&#20851;&#38190;&#38382;&#39064;&#32452;&#20214;&#65307;&#20197;&#21450;&#19968;&#20010;&#20915;&#31574;&#32773;&#65292;&#20855;&#26377;&#23558;&#30740;&#31350;&#20154;&#21592;&#30340;&#20449;&#24687;&#38598;&#25104;&#24182;&#23545;&#26368;&#32456;&#36755;&#20986;&#20570;&#20986;&#21028;&#23450;&#30340;&#33258;&#20027;&#26435;&#12290;&#25105;&#20204;&#23558;DERA&#29992;&#20110;&#19977;&#20010;&#20020;&#24202;&#30456;&#20851;&#20219;&#21153;&#30340;&#27979;&#35797;&#12290;&#22312;&#21307;&#30103;&#23545;&#35805;&#25688;&#35201;&#21644;&#25252;&#29702;&#35745;&#21010;&#29983;&#25104;&#26041;&#38754;&#65292;DERA&#26174;&#31034;&#20986;&#27604;&#22522;&#30784;GPT-4&#24615;&#33021;&#26174;&#33879;&#25552;&#39640;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have emerged as valuable tools for many natural language understanding tasks. In safety-critical applications such as healthcare, the utility of these models is governed by their ability to generate outputs that are factually accurate and complete. In this work, we present dialog-enabled resolving agents (DERA). DERA is a paradigm made possible by the increased conversational abilities of LLMs, namely GPT-4. It provides a simple, interpretable forum for models to communicate feedback and iteratively improve output. We frame our dialog as a discussion between two agent types - a Researcher, who processes information and identifies crucial problem components, and a Decider, who has the autonomy to integrate the Researcher's information and makes judgments on the final output.  We test DERA against three clinically-focused tasks. For medical conversation summarization and care plan generation, DERA shows significant improvement over the base GPT-4 performance 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#21160;&#37197;&#32622;&#36755;&#20986;&#31354;&#38388;&#20197;&#26368;&#23567;&#21270;&#19982;&#20915;&#31574;&#30456;&#20851;&#20449;&#24687;&#30340;&#25439;&#22833;&#26469;&#21046;&#23450;&#31616;&#21270;&#25277;&#35937;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.17062</link><description>&lt;p&gt;
&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#20013;&#30340;&#29702;&#24819;&#25277;&#35937;
&lt;/p&gt;
&lt;p&gt;
Ideal Abstractions for Decision-Focused Learning. (arXiv:2303.17062v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#21160;&#37197;&#32622;&#36755;&#20986;&#31354;&#38388;&#20197;&#26368;&#23567;&#21270;&#19982;&#20915;&#31574;&#30456;&#20851;&#20449;&#24687;&#30340;&#25439;&#22833;&#26469;&#21046;&#23450;&#31616;&#21270;&#25277;&#35937;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35782;&#21035;&#21644;&#21033;&#29992;&#20915;&#31574;&#30340;&#25928;&#29992;&#32467;&#26500;&#26469;&#21046;&#23450;&#31616;&#21270;&#25277;&#35937;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#26041;&#27861;&#12290;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#36890;&#24120;&#28041;&#21450;&#39640;&#32500;&#36755;&#20986;&#31354;&#38388;&#65288;&#20363;&#22914;&#22270;&#20687;&#20013;&#27599;&#20010;&#20687;&#32032;&#25110;&#22270;&#20013;&#33410;&#28857;&#30340;&#39044;&#27979;&#65289;&#65292;&#23613;&#31649;&#23545;&#20110;&#19979;&#28216;&#30340;&#20915;&#31574;&#21046;&#23450;&#26469;&#35828;&#65292;&#31895;&#30053;&#30340;&#36755;&#20986;&#31354;&#38388;&#36890;&#24120;&#24050;&#32463;&#36275;&#22815;&#20102;&#65288;&#20363;&#22914;&#22270;&#20687;&#20013;&#30340;&#21306;&#22495;&#32780;&#19981;&#26159;&#20687;&#32032;&#65289;&#12290;&#24320;&#21457;&#32773;&#24120;&#24120;&#25163;&#24037;&#21046;&#23450;&#36755;&#20986;&#31354;&#38388;&#30340;&#25277;&#35937;&#65292;&#20294;&#23384;&#22312;&#30528;&#20247;&#22810;&#30340;&#25277;&#35937;&#24418;&#24335;&#65292;&#32780;&#19988;&#36873;&#25321;&#27169;&#22411;&#36755;&#20986;&#31354;&#38388;&#30340;&#24433;&#21709;&#23545;&#20854;&#22312;&#19979;&#28216;&#20915;&#31574;&#21046;&#23450;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#33258;&#21160;&#37197;&#32622;&#36755;&#20986;&#31354;&#38388;&#20197;&#26368;&#23567;&#21270;&#19982;&#20915;&#31574;&#30456;&#20851;&#20449;&#24687;&#30340;&#25439;&#22833;&#12290;&#37319;&#29992;&#20960;&#20309;&#35282;&#24230;&#65292;&#25105;&#20204;&#23558;&#31639;&#27861;&#30340;&#19968;&#27493;&#20316;&#20026;&#27010;&#29575;&#21333;&#32431;&#24418;&#30340;&#25237;&#24433;&#65292;&#31216;&#20043;&#20026;fold&#65292;&#20197;&#26368;&#23567;&#21270;&#20915;&#31574;&#30456;&#20851;&#20449;&#24687;&#22312;H-&#29109;&#24847;&#20041;&#19979;&#30340;&#24635;&#25439;&#22833;&#12290;&#20851;&#38190;&#26159;&#65292;L&#8230;
&lt;/p&gt;
&lt;p&gt;
We present a methodology for formulating simplifying abstractions in machine learning systems by identifying and harnessing the utility structure of decisions. Machine learning tasks commonly involve high-dimensional output spaces (e.g., predictions for every pixel in an image or node in a graph), even though a coarser output would often suffice for downstream decision-making (e.g., regions of an image instead of pixels). Developers often hand-engineer abstractions of the output space, but numerous abstractions are possible and it is unclear how the choice of output space for a model impacts its usefulness in downstream decision-making. We propose a method that configures the output space automatically in order to minimize the loss of decision-relevant information. Taking a geometric perspective, we formulate a step of the algorithm as a projection of the probability simplex, termed fold, that minimizes the total loss of decision-related information in the H-entropy sense. Crucially, l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340; AVGN &#32593;&#32476;&#21033;&#29992;&#38899;&#39057;-&#35270;&#35273;&#26679;&#26412;&#21644;&#20998;&#32452;&#27880;&#24847;&#26426;&#21046;&#65292;&#21457;&#29616;&#31867;&#21035;&#24863;&#30693;&#30340;&#38899;&#39057;&#21644;&#35270;&#35273;&#29305;&#24449;&#24182;&#23545;&#40784;&#65292;&#20197;&#23454;&#29616;&#21516;&#26102;&#23450;&#20301;&#22810;&#20010;&#22768;&#28304;&#30340;&#30446;&#26631;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#23450;&#20301;&#31934;&#24230;&#21644;&#23545;&#22797;&#26434;&#29615;&#22659;&#30340;&#40065;&#26834;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.17056</link><description>&lt;p&gt;
&#38899;&#35270;&#39057;&#20998;&#32452;&#32593;&#32476;&#29992;&#20110;&#28151;&#21512;&#22768;&#38899;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Audio-Visual Grouping Network for Sound Localization from Mixtures. (arXiv:2303.17056v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340; AVGN &#32593;&#32476;&#21033;&#29992;&#38899;&#39057;-&#35270;&#35273;&#26679;&#26412;&#21644;&#20998;&#32452;&#27880;&#24847;&#26426;&#21046;&#65292;&#21457;&#29616;&#31867;&#21035;&#24863;&#30693;&#30340;&#38899;&#39057;&#21644;&#35270;&#35273;&#29305;&#24449;&#24182;&#23545;&#40784;&#65292;&#20197;&#23454;&#29616;&#21516;&#26102;&#23450;&#20301;&#22810;&#20010;&#22768;&#28304;&#30340;&#30446;&#26631;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#23450;&#20301;&#31934;&#24230;&#21644;&#23545;&#22797;&#26434;&#29615;&#22659;&#30340;&#40065;&#26834;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22768;&#28304;&#23450;&#20301;&#26159;&#19968;&#39033;&#20856;&#22411;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#21487;&#39044;&#27979;&#35270;&#39057;&#20013;&#22768;&#28304;&#30340;&#20301;&#32622;&#12290;&#20197;&#24448;&#30340;&#21333;&#22768;&#28304;&#26041;&#27861;&#20027;&#35201;&#21033;&#29992;&#38899;&#35270;&#39057;&#20851;&#32852;&#20316;&#20026;&#32447;&#32034;&#65292;&#20197;&#23450;&#20301;&#27599;&#24352;&#22270;&#29255;&#20013;&#30340;&#22768;&#28304;&#23545;&#35937;&#12290;&#30001;&#20110;&#21407;&#31354;&#38388;&#20013;&#26377;&#22810;&#20010;&#22768;&#28304;&#30340;&#28151;&#21512;&#23646;&#24615;&#65292;&#38500;&#20102;&#19968;&#39033;&#36817;&#26399;&#20351;&#29992;&#23545;&#27604;&#38543;&#26426;&#28216;&#36208;&#22312;&#22270;&#20687;&#21644;&#20998;&#31163;&#22768;&#38899;&#20316;&#20026;&#33410;&#28857;&#30340;&#22270;&#20013;&#22788;&#29702;&#22810;&#20010;&#22768;&#28304;&#30340;&#26041;&#27861;&#20043;&#22806;&#65292;&#24456;&#23569;&#26377;&#22810;&#22768;&#28304;&#26041;&#27861;&#21516;&#26102;&#36827;&#34892;&#26412;&#22320;&#21270;&#12290;&#23613;&#31649;&#23427;&#20204;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#21482;&#33021;&#22788;&#29702;&#22266;&#23450;&#25968;&#37327;&#30340;&#22768;&#28304;&#65292;&#24182;&#19988;&#19981;&#33021;&#20026;&#21508;&#20010;&#22768;&#28304;&#23398;&#20064;&#32039;&#20945;&#30340;&#31867;&#30693;&#35782;&#34920;&#31034;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#32570;&#28857;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38899;&#35270;&#39057;&#20998;&#32452;&#32593;&#32476;&#65292;&#21363; AVGN&#65292;&#23427;&#21487;&#20197;&#30452;&#25509;&#20174;&#36755;&#20837;&#30340;&#38899;&#39057;&#28151;&#21512;&#21644;&#22270;&#20687;&#20013;&#23398;&#20064;&#27599;&#20010;&#22768;&#28304;&#30340;&#31867;&#21035;&#35821;&#20041;&#29305;&#24449;&#65292;&#20197;&#22810;&#22768;&#28304;&#21516;&#26102;&#23450;&#20301;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340; AVGN &#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#38899;&#35270;&#39057;&#26679;&#26412;&#21644;&#20998;&#32452;&#27880;&#24847;&#26426;&#21046;&#65292;&#21457;&#29616;&#31867;&#21035;&#24863;&#30693;&#38899;&#39057;&#21644;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#36827;&#19968;&#27493;&#23545;&#20854;&#36827;&#34892;&#23545;&#40784;&#20197;&#23454;&#29616;&#35270;&#39057;&#20013;&#30340;&#22810;&#22768;&#28304;&#23450;&#20301;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23450;&#20301;&#31934;&#24230;&#21644;&#23545;&#22797;&#26434;&#29615;&#22659;&#30340;&#40065;&#26834;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sound source localization is a typical and challenging task that predicts the location of sound sources in a video. Previous single-source methods mainly used the audio-visual association as clues to localize sounding objects in each image. Due to the mixed property of multiple sound sources in the original space, there exist rare multi-source approaches to localizing multiple sources simultaneously, except for one recent work using a contrastive random walk in the graph with images and separated sound as nodes. Despite their promising performance, they can only handle a fixed number of sources, and they cannot learn compact class-aware representations for individual sources. To alleviate this shortcoming, in this paper, we propose a novel audio-visual grouping network, namely AVGN, that can directly learn category-wise semantic features for each source from the input audio mixture and image to localize multiple sources simultaneously. Specifically, our AVGN leverages learnable audio-v
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#35757;&#32451;&#20855;&#26377;ReLU&#21644;&#32447;&#24615;&#38408;&#20540;&#28608;&#27963;&#20989;&#25968;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#22266;&#23450;&#32500;&#24230;&#19979;&#30340;NP&#38590;&#24230;&#12290; &#22238;&#31572;&#20102;&#20004;&#20010;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#36825;&#20004;&#20010;&#38382;&#39064;&#22312;&#20108;&#32500;&#24773;&#20917;&#19979;&#26159;NP&#38590;&#30340;&#65292;&#27492;&#22806;&#22312;ReLU&#26696;&#20363;&#20013;&#35777;&#26126;&#20102;&#22266;&#23450;&#21442;&#25968;&#38382;&#39064;&#30340;&#21442;&#25968;&#21270;&#22266;&#23450;&#22797;&#26434;&#24230;&#32500;&#25968;&#21644;ReLU&#25968;&#37327;&#30340;&#32452;&#21512;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.17045</link><description>&lt;p&gt;
&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#22312;&#22266;&#23450;&#32500;&#24230;&#19978;&#26159;NP&#38590;&#30340;
&lt;/p&gt;
&lt;p&gt;
Training Neural Networks is NP-Hard in Fixed Dimension. (arXiv:2303.17045v1 [cs.CC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17045
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#35757;&#32451;&#20855;&#26377;ReLU&#21644;&#32447;&#24615;&#38408;&#20540;&#28608;&#27963;&#20989;&#25968;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#22266;&#23450;&#32500;&#24230;&#19979;&#30340;NP&#38590;&#24230;&#12290; &#22238;&#31572;&#20102;&#20004;&#20010;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#36825;&#20004;&#20010;&#38382;&#39064;&#22312;&#20108;&#32500;&#24773;&#20917;&#19979;&#26159;NP&#38590;&#30340;&#65292;&#27492;&#22806;&#22312;ReLU&#26696;&#20363;&#20013;&#35777;&#26126;&#20102;&#22266;&#23450;&#21442;&#25968;&#38382;&#39064;&#30340;&#21442;&#25968;&#21270;&#22266;&#23450;&#22797;&#26434;&#24230;&#32500;&#25968;&#21644;ReLU&#25968;&#37327;&#30340;&#32452;&#21512;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#36755;&#20837;&#25968;&#25454;&#32500;&#24230;&#21644;&#38544;&#34255;&#31070;&#32463;&#20803;&#25968;&#37327;&#26041;&#38754;&#23545;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21442;&#25968;&#21270;&#22797;&#26434;&#24615;&#30340;&#30740;&#31350;&#65292;&#32771;&#34385;ReLU&#21644;&#32447;&#24615;&#38408;&#20540;&#28608;&#27963;&#20989;&#25968;&#12290;&#23613;&#31649;&#36825;&#20123;&#38382;&#39064;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#36817;&#24180;&#26469;&#24050;&#32463;&#34987;&#22810;&#27425;&#30740;&#31350;&#65292;&#20294;&#20173;&#26377;&#20960;&#20010;&#38382;&#39064;&#23578;&#26410;&#35299;&#20915;&#12290;&#25105;&#20204;&#22238;&#31572;&#20102;Arora et al. [ICLR '18]&#21644;Khalife&#21644;Basu [IPCO '22]&#30340;&#38382;&#39064;&#65292;&#26174;&#31034;&#20004;&#20010;&#38382;&#39064;&#22312;&#20108;&#32500;&#24773;&#20917;&#19979;&#37117;&#26159;NP&#38590;&#30340;&#65292;&#36825;&#25490;&#38500;&#20102;&#20219;&#20309;&#24120;&#25968;&#32500;&#24230;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#22238;&#31572;&#20102;Froese&#31561;&#20154;[JAIR '22]&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#20855;&#26377;&#38646;&#22521;&#35757;&#35823;&#24046;&#30340;&#22235;&#20010;ReLU(&#25110;&#20004;&#20010;&#32447;&#24615;&#38408;&#20540;&#31070;&#32463;&#20803;)&#30340;W [1]-hardness&#12290;&#26368;&#21518;&#65292;&#22312;ReLU&#26696;&#20363;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21442;&#25968;&#21270;&#22266;&#23450;&#22797;&#26434;&#24230;&#32500;&#25968;&#21644;ReLU&#25968;&#37327;&#30340;&#32452;&#21512;&#21442;&#25968;&#65292;&#22914;&#26524;&#32593;&#32476;&#34987;&#20551;&#23450;&#20026;&#35745;&#31639;&#20984;&#26144;&#23556;&#65292;&#21017;&#21487;&#29992;&#20110;&#22266;&#23450;&#21442;&#25968;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20960;&#20046;&#23436;&#20840;&#35299;&#20915;&#20102;&#36825;&#20123;&#21442;&#25968;&#30340;&#22797;&#26434;&#24615;&#29366;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the parameterized complexity of training two-layer neural networks with respect to the dimension of the input data and the number of hidden neurons, considering ReLU and linear threshold activation functions. Albeit the computational complexity of these problems has been studied numerous times in recent years, several questions are still open. We answer questions by Arora et al. [ICLR '18] and Khalife and Basu [IPCO '22] showing that both problems are NP-hard for two dimensions, which excludes any polynomial-time algorithm for constant dimension. We also answer a question by Froese et al. [JAIR '22] proving W[1]-hardness for four ReLUs (or two linear threshold neurons) with zero training error. Finally, in the ReLU case, we show fixed-parameter tractability for the combined parameter number of dimensions and number of ReLUs if the network is assumed to compute a convex map. Our results settle the complexity status regarding these parameters almost completely.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#38543;&#26426;&#22810;&#33218;&#19978;&#19979;&#25991;&#36172;&#21338;&#31639;&#27861;&#20197;&#26368;&#22823;&#21270;&#32047;&#31215;&#22870;&#21169;&#65292;&#38024;&#23545;&#26410;&#30693;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#36890;&#36807;&#25191;&#34892;&#29305;&#24449;&#21521;&#37327;&#36716;&#25442;&#35299;&#20915;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.17043</link><description>&lt;p&gt;
&#26080;&#35266;&#27979;&#19978;&#19979;&#25991;&#30340;&#32852;&#37030;&#38543;&#26426;&#36172;&#21338;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Stochastic Bandit Learning with Unobserved Context. (arXiv:2303.17043v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#38543;&#26426;&#22810;&#33218;&#19978;&#19979;&#25991;&#36172;&#21338;&#31639;&#27861;&#20197;&#26368;&#22823;&#21270;&#32047;&#31215;&#22870;&#21169;&#65292;&#38024;&#23545;&#26410;&#30693;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#36890;&#36807;&#25191;&#34892;&#29305;&#24449;&#21521;&#37327;&#36716;&#25442;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#26410;&#30693;&#19978;&#19979;&#25991;&#30340;&#32852;&#37030;&#38543;&#26426;&#22810;&#33218;&#19978;&#19979;&#25991;&#36172;&#21338;&#38382;&#39064;&#65292;&#20854;&#20013;M&#20010;&#20195;&#29702;&#38754;&#20020;&#19981;&#21516;&#30340;&#36172;&#21338;&#26426;&#24182;&#21327;&#20316;&#23398;&#20064;&#12290;&#36890;&#20449;&#27169;&#22411;&#30001;&#20013;&#22830;&#26381;&#21153;&#22120;&#32452;&#25104;&#65292;&#24182;&#19988;&#20195;&#29702;&#20250;&#23450;&#26399;&#19982;&#20013;&#22830;&#26381;&#21153;&#22120;&#20849;&#20139;&#20854;&#20272;&#35745;&#32467;&#26524;&#65292;&#20197;&#20415;&#36873;&#25321;&#26368;&#20248;&#21160;&#20316;&#20197;&#26368;&#23567;&#21270;&#24635;&#21518;&#24724;&#12290;&#25105;&#20204;&#20551;&#35774;&#31934;&#30830;&#30340;&#19978;&#19979;&#25991;&#19981;&#21487;&#35266;&#23519;&#65292;&#20195;&#29702;&#20165;&#35266;&#27979;&#19978;&#19979;&#25991;&#30340;&#20998;&#24067;&#12290;&#20363;&#22914;&#65292;&#24403;&#19978;&#19979;&#25991;&#26412;&#36523;&#26159;&#22122;&#22768;&#27979;&#37327;&#25110;&#22522;&#20110;&#39044;&#27979;&#26426;&#21046;&#26102;&#65292;&#23601;&#20250;&#20986;&#29616;&#36825;&#31181;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#31181;&#20998;&#24067;&#24335;&#32852;&#37030;&#31639;&#27861;&#65292;&#20419;&#36827;&#20195;&#29702;&#20043;&#38388;&#30340;&#21327;&#20316;&#23398;&#20064;&#65292;&#36873;&#25321;&#19968;&#31995;&#21015;&#26368;&#20248;&#21160;&#20316;&#20197;&#26368;&#22823;&#21270;&#32047;&#31215;&#22870;&#21169;&#12290;&#36890;&#36807;&#25191;&#34892;&#29305;&#24449;&#21521;&#37327;&#36716;&#25442;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28040;&#38500;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#32447;&#24615;&#21442;&#25968;&#21270;&#22870;&#21169;&#20989;&#25968;&#30340;&#21518;&#24724;&#30028;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of federated stochastic multi-arm contextual bandits with unknown contexts, in which M agents are faced with different bandits and collaborate to learn. The communication model consists of a central server and the agents share their estimates with the central server periodically to learn to choose optimal actions in order to minimize the total regret. We assume that the exact contexts are not observable and the agents observe only a distribution of the contexts. Such a situation arises, for instance, when the context itself is a noisy measurement or based on a prediction mechanism. Our goal is to develop a distributed and federated algorithm that facilitates collaborative learning among the agents to select a sequence of optimal actions so as to maximize the cumulative reward. By performing a feature vector transformation, we propose an elimination-based algorithm and prove the regret bound for linearly parametrized reward functions. Finally, we validated the perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#26500;&#25104;&#30005;&#24433;&#27785;&#28024;&#24863;&#30340;&#20855;&#20307;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#21592;&#39537;&#21160;&#30340;&#25668;&#20687;&#26426;&#31227;&#21160;&#29983;&#25104;&#31995;&#32479;&#65292;&#20197;&#23454;&#29616;&#24773;&#24863;&#21644;&#31354;&#38388;&#30340;&#27785;&#28024;&#24863;&#12290;</title><link>http://arxiv.org/abs/2303.17041</link><description>&lt;p&gt;
&#27785;&#28024;&#24863;&#31192;&#35776;&#65306;&#22522;&#20110;&#28436;&#21592;&#30340;&#33258;&#21160;&#29983;&#25104;&#30005;&#24433;&#25668;&#24433;&#26426;&#31227;&#21160;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
The secret of immersion: actor driven camera movement generation for auto-cinematography. (arXiv:2303.17041v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26500;&#25104;&#30005;&#24433;&#27785;&#28024;&#24863;&#30340;&#20855;&#20307;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#21592;&#39537;&#21160;&#30340;&#25668;&#20687;&#26426;&#31227;&#21160;&#29983;&#25104;&#31995;&#32479;&#65292;&#20197;&#23454;&#29616;&#24773;&#24863;&#21644;&#31354;&#38388;&#30340;&#27785;&#28024;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27785;&#28024;&#24863;&#22312;&#35774;&#35745;&#30005;&#24433;&#26102;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#28982;&#32780;&#65292;&#27785;&#28024;&#24335;&#25293;&#25668;&#30340;&#22256;&#38590;&#38459;&#30861;&#20102;&#35774;&#35745;&#24072;&#21019;&#36896;&#20986;&#20196;&#20154;&#28385;&#24847;&#30340;&#25104;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#26500;&#25104;&#30005;&#24433;&#27785;&#28024;&#24863;&#30340;&#20855;&#20307;&#32452;&#25104;&#37096;&#20998;&#65292;&#32771;&#34385;&#20102;&#31354;&#38388;&#12289;&#24773;&#24863;&#21644;&#32654;&#23398;&#31561;&#26041;&#38754;&#65292;&#21516;&#26102;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#34987;&#32467;&#21512;&#21040;&#20102;&#19968;&#20010;&#39640;&#32423;&#35780;&#20272;&#26426;&#21046;&#20013;&#12290;&#22312;&#36825;&#26679;&#30340;&#27785;&#28024;&#26426;&#21046;&#30340;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#25668;&#20687;&#26426;&#25511;&#21046;&#31995;&#32479;&#65292;&#33021;&#22815;&#22312;3D&#34394;&#25311;&#29615;&#22659;&#20013;&#29983;&#25104;&#22522;&#20110;&#28436;&#21592;&#39537;&#21160;&#30340;&#25668;&#20687;&#26426;&#31227;&#21160;&#65292;&#20197;&#33719;&#24471;&#27785;&#28024;&#24335;&#30005;&#24433;&#24207;&#21015;&#12290;&#29983;&#25104;&#36807;&#31243;&#20013;&#25552;&#20986;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#23558;&#28436;&#21592;&#36816;&#21160;&#36716;&#25442;&#20026;&#20197;&#24773;&#24863;&#22240;&#32032;&#20026;&#26465;&#20214;&#30340;&#25668;&#20687;&#26426;&#36712;&#36857;&#65292;&#30830;&#20445;&#20102;&#28436;&#21592;&#19982;&#25668;&#20687;&#26426;&#30340;&#29289;&#29702;&#21644;&#24515;&#29702;&#21516;&#27493;&#20197;&#23454;&#29616;&#31354;&#38388;&#21644;&#24773;&#24863;&#30340;&#27785;&#28024;&#24863;&#12290;&#36890;&#36807;&#21152;&#20837;&#25511;&#21046;&#25668;&#20687;&#26426;&#25238;&#21160;&#20197;&#34920;&#36798;&#19981;&#21516;&#24515;&#29702;&#29366;&#24577;&#30340;&#27491;&#21017;&#21270;&#65292;&#24773;&#24863;&#27785;&#28024;&#24863;&#24471;&#21040;&#20102;&#36827;&#19968;&#27493;&#21152;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
Immersion plays a vital role when designing cinematic creations, yet the difficulty in immersive shooting prevents designers to create satisfactory outputs. In this work, we analyze the specific components that contribute to cinematographic immersion considering spatial, emotional, and aesthetic level, while these components are then combined into a high-level evaluation mechanism. Guided by such a immersion mechanism, we propose a GAN-based camera control system that is able to generate actor-driven camera movements in the 3D virtual environment to obtain immersive film sequences. The proposed encoder-decoder architecture in the generation flow transfers character motion into camera trajectory conditioned on an emotion factor. This ensures spatial and emotional immersion by performing actor-camera synchronization physically and psychologically. The emotional immersion is further strengthened by incorporating regularization that controls camera shakiness for expressing different mental
&lt;/p&gt;</description></item><item><title>EPG-MGCN&#26159;&#19968;&#31181;&#21033;&#29992;&#33258;&#25105;&#35268;&#21010;&#24341;&#23548;&#30340;&#22810;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#24314;&#31435;&#31038;&#20132;&#20132;&#20114;&#27169;&#22411;&#65292;&#21516;&#26102;&#20351;&#29992;&#21382;&#21490;&#36712;&#36857;&#20449;&#24687;&#21644;&#33258;&#20027;&#36710;&#36742;&#30340;&#26410;&#26469;&#35268;&#21010;&#26469;&#39044;&#27979;&#24322;&#26500;&#26234;&#33021;&#20307;&#30340;&#36712;&#36857;&#65292;&#20197;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#23433;&#20840;&#39550;&#39542;</title><link>http://arxiv.org/abs/2303.17027</link><description>&lt;p&gt;
EPG-MGCN: &#22522;&#20110;&#33258;&#25105;&#35268;&#21010;&#24341;&#23548;&#30340;&#22810;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#24322;&#26500;&#26234;&#33021;&#20307;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
EPG-MGCN: Ego-Planning Guided Multi-Graph Convolutional Network for Heterogeneous Agent Trajectory Prediction. (arXiv:2303.17027v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17027
&lt;/p&gt;
&lt;p&gt;
EPG-MGCN&#26159;&#19968;&#31181;&#21033;&#29992;&#33258;&#25105;&#35268;&#21010;&#24341;&#23548;&#30340;&#22810;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#24314;&#31435;&#31038;&#20132;&#20132;&#20114;&#27169;&#22411;&#65292;&#21516;&#26102;&#20351;&#29992;&#21382;&#21490;&#36712;&#36857;&#20449;&#24687;&#21644;&#33258;&#20027;&#36710;&#36742;&#30340;&#26410;&#26469;&#35268;&#21010;&#26469;&#39044;&#27979;&#24322;&#26500;&#26234;&#33021;&#20307;&#30340;&#36712;&#36857;&#65292;&#20197;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#23433;&#20840;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#38656;&#35201;&#20934;&#30830;&#39044;&#27979;&#21608;&#22260;&#24322;&#26500;&#20132;&#36890;&#26234;&#33021;&#20307;&#65288;&#22914;&#36710;&#36742;&#12289;&#34892;&#20154;&#12289;&#33258;&#34892;&#36710;&#31561;&#65289;&#30340;&#26410;&#26469;&#36712;&#36857;&#65292;&#20197;&#30830;&#20445;&#23433;&#20840;&#39550;&#39542;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#35268;&#21010;&#24341;&#23548;&#30340;&#22810;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;EPG-MGCN&#65289;&#65292;&#21033;&#29992;&#21382;&#21490;&#36712;&#36857;&#20449;&#24687;&#21644;&#33258;&#20027;&#36710;&#36742;&#30340;&#26410;&#26469;&#35268;&#21010;&#20449;&#24687;&#26469;&#39044;&#27979;&#24322;&#26500;&#26234;&#33021;&#20307;&#30340;&#36712;&#36857;&#12290;EPG-MGCN&#39318;&#20808;&#36890;&#36807;&#37319;&#29992;&#22235;&#31181;&#22270;&#24418;&#25299;&#25169;&#32467;&#26500;&#65288;&#36317;&#31163;&#22270;&#12289;&#33021;&#35265;&#24230;&#22270;&#12289;&#35268;&#21010;&#22270;&#21644;&#31867;&#21035;&#22270;&#65289;&#26469;&#24314;&#31435;&#31038;&#20132;&#20132;&#20114;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#33258;&#20027;&#36710;&#36742;&#30340;&#35268;&#21010;&#20449;&#24687;&#36890;&#36807;&#35268;&#21010;&#22270;&#21644;&#38543;&#21518;&#30340;&#35268;&#21010;&#24341;&#23548;&#39044;&#27979;&#27169;&#22359;&#36827;&#34892;&#32534;&#30721;&#65292;&#20197;&#20943;&#23569;&#36712;&#36857;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To drive safely in complex traffic environments, autonomous vehicles need to make an accurate prediction of the future trajectories of nearby heterogeneous traffic agents (i.e., vehicles, pedestrians, bicyclists, etc). Due to the interactive nature, human drivers are accustomed to infer what the future situations will become if they are going to execute different maneuvers. To fully exploit the impacts of interactions, this paper proposes a ego-planning guided multi-graph convolutional network (EPG-MGCN) to predict the trajectories of heterogeneous agents using both historical trajectory information and ego vehicle's future planning information. The EPG-MGCN first models the social interactions by employing four graph topologies, i.e., distance graphs, visibility graphs, planning graphs and category graphs. Then, the planning information of the ego vehicle is encoded by both the planning graph and the subsequent planning-guided prediction module to reduce uncertainty in the trajectory 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HyperDiffusion&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#26465;&#20214;&#29983;&#25104;&#24314;&#27169;&#30340;&#38544;&#24335;&#31070;&#32463;&#22330;&#12290;&#35813;&#26041;&#27861;&#22312;MLP&#26435;&#37325;&#19978;&#25805;&#20316;&#65292;&#29983;&#25104;&#20102;&#26032;&#30340;&#31070;&#32463;&#38544;&#24335;&#22330;&#65292;&#20854;&#20013;&#21253;&#21547;&#30001;&#21512;&#25104;&#30340;MLP&#21442;&#25968;&#32534;&#30721;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2303.17015</link><description>&lt;p&gt;
&#36229;&#25193;&#25955;&#65306;&#29992;&#26435;&#37325;&#31354;&#38388;&#25193;&#25955;&#29983;&#25104;&#38544;&#24335;&#31070;&#32463;&#22330;
&lt;/p&gt;
&lt;p&gt;
HyperDiffusion: Generating Implicit Neural Fields with Weight-Space Diffusion. (arXiv:2303.17015v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HyperDiffusion&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#26465;&#20214;&#29983;&#25104;&#24314;&#27169;&#30340;&#38544;&#24335;&#31070;&#32463;&#22330;&#12290;&#35813;&#26041;&#27861;&#22312;MLP&#26435;&#37325;&#19978;&#25805;&#20316;&#65292;&#29983;&#25104;&#20102;&#26032;&#30340;&#31070;&#32463;&#38544;&#24335;&#22330;&#65292;&#20854;&#20013;&#21253;&#21547;&#30001;&#21512;&#25104;&#30340;MLP&#21442;&#25968;&#32534;&#30721;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#31070;&#32463;&#22330;&#36890;&#24120;&#30001;&#22810;&#23618;&#24863;&#30693;&#22120;(MLP)&#32534;&#30721;&#65292;&#23558;&#22352;&#26631;(&#20363;&#22914;xyz)&#26144;&#23556;&#21040;&#20449;&#21495;(&#20363;&#22914;&#31526;&#21495;&#36317;&#31163;)&#65292;&#24050;&#26174;&#31034;&#20986;&#26497;&#39640;&#30340;&#31934;&#24230;&#21644;&#32039;&#20945;&#24615;&#12290;&#20294;&#26159;&#65292;&#32570;&#23569;&#35268;&#21017;&#21644;&#26126;&#30830;&#30340;&#32593;&#26684;&#32467;&#26500;&#20063;&#20351;&#24471;&#30452;&#25509;&#22312;&#38544;&#24335;&#31070;&#32463;&#22330;&#19978;&#24212;&#29992;&#29983;&#25104;&#24314;&#27169;&#20197;&#21512;&#25104;&#26032;&#25968;&#25454;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36229;&#25193;&#25955;(HyperDiffusion)&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#38544;&#24335;&#31070;&#32463;&#22330;&#26080;&#26465;&#20214;&#29983;&#25104;&#24314;&#27169;&#30340;&#26032;&#26041;&#27861;&#12290;HyperDiffusion&#30452;&#25509;&#22312;MLP&#26435;&#37325;&#19978;&#25805;&#20316;&#65292;&#24182;&#36890;&#36807;&#21512;&#25104;MLP&#21442;&#25968;&#29983;&#25104;&#26032;&#30340;&#31070;&#32463;&#38544;&#24335;&#22330;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#39318;&#20808;&#20248;&#21270;&#19968;&#31995;&#21015;MLP&#20197;&#24544;&#23454;&#22320;&#34920;&#31034;&#21508;&#20010;&#25968;&#25454;&#26679;&#26412;&#12290;&#38543;&#21518;&#65292;&#22312;MLP&#26435;&#37325;&#31354;&#38388;&#20013;&#35757;&#32451;&#25193;&#25955;&#36807;&#31243;&#20197;&#23545;&#31070;&#32463;&#38544;&#24335;&#22330;&#30340;&#22522;&#30784;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#12290;HyperDiffusion&#20351;&#24471;&#25193;&#25955;&#24314;&#27169;&#22312;&#38544;&#24335;&#65292;&#32039;&#20945;&#19988;&#39640;&#20445;&#30495;&#30340;&#34920;&#31034;&#19978;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit neural fields, typically encoded by a multilayer perceptron (MLP) that maps from coordinates (e.g., xyz) to signals (e.g., signed distances), have shown remarkable promise as a high-fidelity and compact representation. However, the lack of a regular and explicit grid structure also makes it challenging to apply generative modeling directly on implicit neural fields in order to synthesize new data. To this end, we propose HyperDiffusion, a novel approach for unconditional generative modeling of implicit neural fields. HyperDiffusion operates directly on MLP weights and generates new neural implicit fields encoded by synthesized MLP parameters. Specifically, a collection of MLPs is first optimized to faithfully represent individual data samples. Subsequently, a diffusion process is trained in this MLP weight space to model the underlying distribution of neural implicit fields. HyperDiffusion enables diffusion modeling over a implicit, compact, and yet high-fidelity representatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#35268;&#33539;&#24341;&#23548;&#37319;&#26679;&#25216;&#26415;&#26469;&#32858;&#21512;&#19987;&#19994;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20197;&#35821;&#20041;&#24863;&#30693;&#30340;&#26041;&#24335;&#25913;&#36827;&#27169;&#20223;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#23454;&#29616;&#20102;&#26174;&#30528;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2303.17010</link><description>&lt;p&gt;
&#35821;&#20041;&#24863;&#30693;&#27169;&#20223;&#23398;&#20064;&#30340;&#35268;&#33539;&#24341;&#23548;&#25968;&#25454;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Specification-Guided Data Aggregation for Semantically Aware Imitation Learning. (arXiv:2303.17010v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#35268;&#33539;&#24341;&#23548;&#37319;&#26679;&#25216;&#26415;&#26469;&#32858;&#21512;&#19987;&#19994;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20197;&#35821;&#20041;&#24863;&#30693;&#30340;&#26041;&#24335;&#25913;&#36827;&#27169;&#20223;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#23454;&#29616;&#20102;&#26174;&#30528;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20223;&#30495;&#21644;&#27491;&#24335;&#26041;&#27861;&#24341;&#23548;&#30340;&#29615;&#22659;&#37319;&#26679;&#30340;&#21457;&#23637;&#20351;&#24471;&#22312;&#35768;&#22810;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#20013;&#65292;&#22914;&#33258;&#21160;&#39550;&#39542;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24471;&#21040;&#20102;&#20005;&#26684;&#30340;&#35780;&#20272;&#12290;&#36825;&#20123;&#29615;&#22659;&#37319;&#26679;&#25216;&#26415;&#30340;&#24212;&#29992;&#65292;&#20197;&#25913;&#36827;&#23398;&#20064;&#30340;&#27169;&#22411;&#26412;&#36523;&#20026;&#30446;&#30340;&#65292;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30340;&#21033;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#35268;&#33539;&#24341;&#23548;&#37319;&#26679;&#25216;&#26415;&#20316;&#20026;&#32858;&#21512;&#26032;&#29615;&#22659;&#20013;&#19987;&#19994;&#25968;&#25454;&#30340;&#25163;&#27573;&#26469;&#20197;&#35821;&#20041;&#24863;&#30693;&#30340;&#26041;&#24335;&#25913;&#36827;&#27169;&#20223;&#23398;&#20064;&#30340;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21019;&#24314;&#19968;&#32452;&#27491;&#24335;&#35268;&#33539;&#65292;&#20316;&#20026;&#23558;&#21487;&#33021;&#29615;&#22659;&#31354;&#38388;&#21010;&#20998;&#20026;&#35821;&#20041;&#30456;&#20284;&#21306;&#22495;&#30340;&#25163;&#27573;&#65292;&#24182;&#30830;&#23450;&#35813;&#21010;&#20998;&#20013;&#25105;&#20204;&#25152;&#23398;&#20064;&#30340;&#27169;&#20223;&#34892;&#20026;&#19982;&#19987;&#23478;&#20043;&#38388;&#26368;&#19981;&#21516;&#30340;&#20803;&#32032;&#12290;&#28982;&#21518;&#22312;&#36825;&#20123;&#30830;&#23450;&#30340;&#21306;&#22495;&#20869;&#32858;&#21512;&#19987;&#23478;&#25968;&#25454;&#65292;&#20174;&#32780;&#26356;&#20934;&#30830;&#22320;&#27169;&#20223;&#19987;&#23478;&#30340;&#34892;&#20026;&#35821;&#20041;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#20363;&#21270;&#21040;&#33258;&#21160;&#39550;&#39542;&#30340;&#32972;&#26223;&#19979;&#65292;&#26174;&#31034;&#20986;&#27604;&#20256;&#32479;&#30340;&#27169;&#20223;&#23398;&#20064;&#25216;&#26415;&#26377;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in simulation and formal methods-guided environment sampling have enabled the rigorous evaluation of machine learning models in a number of safety-critical scenarios, such as autonomous driving. Application of these environment sampling techniques towards improving the learned models themselves has yet to be fully exploited. In this work, we introduce a novel method for improving imitation-learned models in a semantically aware fashion by leveraging specification-guided sampling techniques as a means of aggregating expert data in new environments. Specifically, we create a set of formal specifications as a means of partitioning the space of possible environments into semantically similar regions, and identify elements of this partition where our learned imitation behaves most differently from the expert. We then aggregate expert data on environments in these identified regions, leading to more accurate imitation of the expert's behavior semantics. We instantiate our approa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;12&#31181;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#26579;&#33394;&#36716;&#31227;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#35777;&#26126;&#20102;&#19968;&#20123;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#21487;&#29992;&#20110;&#22823;&#35268;&#27169;&#26579;&#33394;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#30340;&#21512;&#25104;&#12290;</title><link>http://arxiv.org/abs/2303.17009</link><description>&lt;p&gt;
&#32452;&#32455;&#30149;&#29702;&#23398;&#26579;&#33394;&#36716;&#31227;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;&#30340;&#27604;&#36739;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A comparative evaluation of image-to-image translation methods for stain transfer in histopathology. (arXiv:2303.17009v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;12&#31181;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#26579;&#33394;&#36716;&#31227;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#35777;&#26126;&#20102;&#19968;&#20123;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#21487;&#29992;&#20110;&#22823;&#35268;&#27169;&#26579;&#33394;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#30340;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#65288;I2I&#65289;&#26041;&#27861;&#20801;&#35768;&#29983;&#25104;&#20855;&#26377;&#19981;&#21516;&#39118;&#26684;&#20294;&#19982;&#21407;&#22987;&#22270;&#20687;&#20849;&#20139;&#20869;&#23481;&#30340;&#20154;&#24037;&#22270;&#20687;&#12290;&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#26041;&#27861;&#30340;&#36827;&#23637;&#65292;I2I&#26041;&#27861;&#23454;&#29616;&#20102;&#29983;&#25104;&#19982;&#33258;&#28982;&#22270;&#20687;&#26080;&#27861;&#21306;&#20998;&#30340;&#20154;&#24037;&#22270;&#20687;&#12290;&#26368;&#36817;&#65292;&#38024;&#23545;&#32452;&#32455;&#30149;&#29702;&#23398;&#20013;&#19981;&#21516;&#31867;&#22411;&#30340;&#26579;&#33394;&#65292;I2I&#26041;&#27861;&#20063;&#34987;&#29992;&#20110;&#29983;&#25104;&#27169;&#25311;&#30340;&#26579;&#33394;&#32452;&#32455;&#30340;&#20154;&#24037;&#22270;&#20687;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#36807;&#31243;&#31216;&#20026;&#26579;&#33394;&#36716;&#31227;&#12290;&#30001;&#20110;I2I&#21464;&#31181;&#30340;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#65292;&#36825;&#20351;&#24471;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;I2I&#26041;&#27861;&#36827;&#34892;&#26579;&#33394;&#36716;&#31227;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;12&#31181;&#26579;&#33394;&#36716;&#31227;&#26041;&#27861;&#65292;&#20854;&#20013;3&#31181;&#22522;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;9&#31181;&#22522;&#20110;GAN&#22270;&#20687;&#22788;&#29702;&#26041;&#27861;&#12290;&#20998;&#26512;&#20381;&#36182;&#20110;&#23450;&#37327;&#35780;&#20272;&#22270;&#20687;&#32763;&#35793;&#36136;&#37327;&#30340;&#34917;&#20805;&#25514;&#26045;&#65292;&#20197;&#21450;&#23545;&#28145;&#24230;&#23398;&#20064;&#32452;&#32455;&#25104;&#20687;&#21644;&#21307;&#23398;&#35786;&#26029;&#30340;&#36866;&#29992;&#24615;&#30340;&#35780;&#20272;&#65292;&#20197;&#21450;&#32463;&#39564;&#20016;&#23500;&#30340;&#30149;&#29702;&#23398;&#23478;&#30340;&#23450;&#24615;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#26377;&#20960;&#31181;I2I&#26041;&#27861;&#20248;&#20110;&#26579;&#33394;&#36716;&#31227;&#39046;&#22495;&#30340;&#29616;&#26377;&#25216;&#26415;&#65292;&#24182;&#21487;&#29992;&#20110;&#22823;&#35268;&#27169;&#26579;&#33394;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#30340;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-to-image translation (I2I) methods allow the generation of artificial images that share the content of the original image but have a different style. With the advances in Generative Adversarial Networks (GANs)-based methods, I2I methods enabled the generation of artificial images that are indistinguishable from natural images. Recently, I2I methods were also employed in histopathology for generating artificial images of in silico stained tissues from a different type of staining. We refer to this process as stain transfer. The number of I2I variants is constantly increasing, which makes a well justified choice of the most suitable I2I methods for stain transfer challenging. In our work, we compare twelve stain transfer approaches, three of which are based on traditional and nine on GAN-based image processing methods. The analysis relies on complementary quantitative measures for the quality of image translation, the assessment of the suitability for deep learning-based tissue gra
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#24052;&#35199;&#22823;&#23398;&#20837;&#23398;&#32771;&#35797;&#20013;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#65292;&#20998;&#26512;&#20102;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#65292;&#26368;&#32456;&#21457;&#29616;GPT-4&#19982;Chain-of-Thought&#25552;&#31034;&#32467;&#21512;&#34920;&#29616;&#26368;&#22909;&#65292;&#22312;2022&#24180;&#32771;&#35797;&#20013;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;87&#65285;&#12290;</title><link>http://arxiv.org/abs/2303.17003</link><description>&lt;p&gt;
&#22312;&#24052;&#35199;&#22823;&#23398;&#20837;&#23398;&#32771;&#35797;&#20013;&#35780;&#20272;GPT-3.5&#21644;GPT-4&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating GPT-3.5 and GPT-4 Models on Brazilian University Admission Exams. (arXiv:2303.17003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#24052;&#35199;&#22823;&#23398;&#20837;&#23398;&#32771;&#35797;&#20013;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#65292;&#20998;&#26512;&#20102;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#65292;&#26368;&#32456;&#21457;&#29616;GPT-4&#19982;Chain-of-Thought&#25552;&#31034;&#32467;&#21512;&#34920;&#29616;&#26368;&#22909;&#65292;&#22312;2022&#24180;&#32771;&#35797;&#20013;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;87&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#24212;&#23545;&#39640;&#39118;&#38505;&#30340;&#22810;&#39033;&#36873;&#25321;&#27979;&#35797;&#20013;&#30340;&#33021;&#21147;&#65292;&#36825;&#37324;&#20197;&#24052;&#35199;&#22823;&#23398;&#24191;&#27867;&#37319;&#29992;&#30340;&#22810;&#23398;&#31185;&#20837;&#23398;&#32771;&#35797;Exame Nacional do Ensino M&#233;dio&#65288;ENEM&#65289;&#20026;&#20363;&#12290;&#35813;&#32771;&#35797;&#23545;LMs&#25552;&#20986;&#20102;&#25361;&#25112;&#65292;&#22240;&#20026;&#20854;&#38382;&#39064;&#21487;&#33021;&#28041;&#21450;&#22810;&#20010;&#30693;&#35782;&#39046;&#22495;&#65292;&#38656;&#35201;&#29702;&#35299;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#20449;&#24687;&#12290;&#20363;&#22914;&#65292;&#19968;&#20010;&#38382;&#39064;&#21487;&#33021;&#38656;&#35201;&#29702;&#35299;&#32479;&#35745;&#23398;&#21644;&#29983;&#29289;&#23398;&#25165;&#33021;&#35299;&#20915;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#23545;2009&#24180;&#33267;2017&#24180;&#32771;&#35797;&#20197;&#21450;2022&#24180;&#20844;&#24320;&#30340;&#32771;&#35797;&#38382;&#39064;&#30340;&#21709;&#24212;&#12290;&#27492;&#22806;&#65292;&#36824;&#27979;&#35797;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#21253;&#25324;&#20351;&#29992;Chain-of-Thought&#65288;CoT&#65289;&#25552;&#31034;&#29983;&#25104;&#31572;&#26696;&#30340;&#35299;&#37322;&#12290;&#22312;2022&#24180;&#30340;&#32771;&#35797;&#20013;&#65292;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#26159;GPT-4&#24182;&#20351;&#29992;&#20102;CoT&#65292;&#22312;&#20934;&#30830;&#29575;&#26041;&#38754;&#36798;&#21040;&#20102;87&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
The present study aims to explore the capabilities of Language Models (LMs) in tackling high-stakes multiple-choice tests, represented here by the Exame Nacional do Ensino M\'edio (ENEM), a multidisciplinary entrance examination widely adopted by Brazilian universities. This exam poses challenging tasks for LMs, since its questions may span into multiple fields of knowledge, requiring understanding of information from diverse domains. For instance, a question may require comprehension of both statistics and biology to be solved. This work analyzed responses generated by GPT-3.5 and GPT-4 models for questions presented in the 2009-2017 exams, as well as for questions of the 2022 exam, which were made public after the training of the models was completed. Furthermore, different prompt strategies were tested, including the use of Chain-of-Thought (CoT) prompts to generate explanations for answers. On the 2022 edition, the best-performing model, GPT-4 with CoT, achieved an accuracy of 87%,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; G&#19981;&#21464;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376; &#29992;&#20110;&#22788;&#29702;&#25968;&#25454;&#38598;&#19981;&#20165;&#22312;&#27969;&#24418;&#19978;&#65292;&#32780;&#19988;&#22312;&#19968;&#20010;&#36830;&#32493;&#32676;&#30340;&#20316;&#29992;&#19979;&#20063;&#26159;&#23553;&#38381;&#30340;&#24773;&#24418;&#65292;&#30456;&#36739;&#20110;&#26631;&#20934;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2303.17001</link><description>&lt;p&gt;
G&#19981;&#21464;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
The G-invariant graph Laplacian. (arXiv:2303.17001v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; G&#19981;&#21464;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376; &#29992;&#20110;&#22788;&#29702;&#25968;&#25454;&#38598;&#19981;&#20165;&#22312;&#27969;&#24418;&#19978;&#65292;&#32780;&#19988;&#22312;&#19968;&#20010;&#36830;&#32493;&#32676;&#30340;&#20316;&#29992;&#19979;&#20063;&#26159;&#23553;&#38381;&#30340;&#24773;&#24418;&#65292;&#30456;&#36739;&#20110;&#26631;&#20934;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#31639;&#27861;&#24050;&#32463;&#34987;&#35777;&#26126;&#22312;&#38477;&#32500;&#12289;&#32858;&#31867;&#21644;&#21435;&#22122;&#31561;&#39046;&#22495;&#23545;&#27969;&#24418;&#25968;&#25454;&#38750;&#24120;&#26377;&#25928;&#12290;&#26412;&#25991;&#32771;&#34385;&#30340;&#25968;&#25454;&#38598;&#19981;&#20165;&#22312;&#27969;&#24418;&#19978;&#65292;&#32780;&#19988;&#22312;&#19968;&#20010;&#36830;&#32493;&#32676;&#30340;&#20316;&#29992;&#19979;&#20063;&#26159;&#23553;&#38381;&#30340;&#12290;&#36825;&#31867;&#25968;&#25454;&#38598;&#30340;&#19968;&#20010;&#20363;&#23376;&#26159;&#27839;&#30528;&#20302;&#32500;&#27969;&#24418;&#20256;&#25773;&#30340;&#20307;&#31215;&#65292;&#20854;&#20013;&#27599;&#20010;&#20307;&#31215;&#21487;&#20197;&#22312;&#19977;&#32500;&#31354;&#38388;&#20013;&#26059;&#36716;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;G&#19981;&#21464;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#65292;&#36890;&#36807;&#32771;&#34385;&#25968;&#25454;&#38598;&#19978;&#30340;&#32676;&#30340;&#20316;&#29992;&#26469;&#24191;&#20041;&#21270;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#12290;&#25105;&#20204;&#26174;&#31034;&#20102;&#19982;&#26631;&#20934;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#31867;&#20284;&#65292;G&#19981;&#21464;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#25910;&#25947;&#20110;&#25968;&#25454;&#27969;&#24418;&#19978;&#30340;Laplace-Beltrami&#31639;&#23376;&#65292;&#20294;&#25910;&#25947;&#36895;&#24230;&#26174;&#33879;&#25552;&#39640;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;G&#19981;&#21464;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#29305;&#24449;&#20989;&#25968;&#20855;&#26377;&#32676;&#20803;&#32032;&#21644;&#26576;&#20123;&#30697;&#38453;&#30340;&#29305;&#24449;&#21521;&#37327;&#30340;&#24352;&#37327;&#31215;&#24418;&#24335;&#65292;&#21487;&#20197;&#20351;&#29992;F&#39640;&#25928;&#22320;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Laplacian based algorithms for data lying on a manifold have been proven effective for tasks such as dimensionality reduction, clustering, and denoising. In this work, we consider data sets whose data point not only lie on a manifold, but are also closed under the action of a continuous group. An example of such data set is volumes that line on a low dimensional manifold, where each volume may be rotated in three-dimensional space. We introduce the G-invariant graph Laplacian that generalizes the graph Laplacian by accounting for the action of the group on the data set. We show that like the standard graph Laplacian, the G-invariant graph Laplacian converges to the Laplace-Beltrami operator on the data manifold, but with a significantly improved convergence rate. Furthermore, we show that the eigenfunctions of the G-invariant graph Laplacian admit the form of tensor products between the group elements and eigenvectors of certain matrices, which can be computed efficiently using F
&lt;/p&gt;</description></item><item><title>&#23558;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#25104;&#26412;&#38477;&#20302;&#21040;&#21487;&#25509;&#21463;&#30340;&#20219;&#21153;&#24615;&#33021;&#21644;&#21152;&#36895;&#25913;&#36827;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;IPU&#29420;&#29305;&#30340;&#30828;&#20214;&#29305;&#24615;&#21644;&#25968;&#25454;&#20013;&#23450;&#20041;&#30340;&#20219;&#20309;&#22359;&#32467;&#26500;&#23454;&#29616;&#22312;Graphcore IPUs&#19978;&#24555;&#36895;&#31232;&#30095;&#25805;&#20316;&#30340;&#24211;&#8212;&#8212;PopSparse&#12290;</title><link>http://arxiv.org/abs/2303.16999</link><description>&lt;p&gt;
PopSparse&#65306;&#22312;IPU&#19978;&#21152;&#36895;&#30340;&#22359;&#31232;&#30095;&#30697;&#38453;&#20056;&#27861;
&lt;/p&gt;
&lt;p&gt;
PopSparse: Accelerated block sparse matrix multiplication on IPU. (arXiv:2303.16999v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16999
&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#25104;&#26412;&#38477;&#20302;&#21040;&#21487;&#25509;&#21463;&#30340;&#20219;&#21153;&#24615;&#33021;&#21644;&#21152;&#36895;&#25913;&#36827;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;IPU&#29420;&#29305;&#30340;&#30828;&#20214;&#29305;&#24615;&#21644;&#25968;&#25454;&#20013;&#23450;&#20041;&#30340;&#20219;&#20309;&#22359;&#32467;&#26500;&#23454;&#29616;&#22312;Graphcore IPUs&#19978;&#24555;&#36895;&#31232;&#30095;&#25805;&#20316;&#30340;&#24211;&#8212;&#8212;PopSparse&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#31038;&#21306;&#20013;&#65292;&#20351;&#29992;&#31232;&#30095;&#24615;&#38477;&#20302;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#25104;&#26412;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#34429;&#28982;&#22312;&#32500;&#25345;&#21487;&#25509;&#21463;&#30340;&#20219;&#21153;&#24615;&#33021;&#30340;&#21516;&#26102;&#20943;&#23569;FLOP&#21644;&#21442;&#25968;&#25968;&#37327;&#21462;&#24471;&#20102;&#35768;&#22810;&#25104;&#21151;&#65292;&#20294;&#23454;&#38469;&#19978;&#33719;&#24471;&#21152;&#36895;&#25913;&#36827;&#36890;&#24120;&#26356;&#21152;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#36890;&#29992;&#21152;&#36895;&#22120;&#65288;GPA&#65289;&#19978;&#65292;&#22914;&#20351;&#29992;&#20302;&#31934;&#24230;&#25968;&#23383;&#26684;&#24335;&#30340;NVIDIA GPU&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PopSparse&#65292;&#19968;&#31181;&#21033;&#29992;IPU&#30340;&#29420;&#29305;&#30828;&#20214;&#29305;&#24615;&#20197;&#21450;&#25968;&#25454;&#20013;&#23450;&#20041;&#30340;&#20219;&#20309;&#22359;&#32467;&#26500;&#23454;&#29616;&#22312;Graphcore IPUs&#19978;&#24555;&#36895;&#31232;&#30095;&#25805;&#20316;&#30340;&#24211;&#12290;&#25105;&#20204;&#38024;&#23545;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#31232;&#30095;&#24615;&#65306;&#38745;&#24577;&#31232;&#30095;&#24615;&#65292;&#20854;&#20013;&#31232;&#30095;&#27169;&#24335;&#22312;&#32534;&#35793;&#26102;&#22266;&#23450;&#65307;&#21160;&#24577;&#31232;&#30095;&#24615;&#65292;&#20854;&#20013;&#27599;&#27425;&#36816;&#34892;&#27169;&#22411;&#26102;&#37117;&#21487;&#20197;&#25913;&#21464;&#12290;&#25105;&#20204;&#38024;&#23545;&#21508;&#31181;&#22359;&#22823;&#23567;&#12289;&#30697;&#38453;&#22823;&#23567;&#21644;&#23494;&#24230;&#22312;IPU&#19978;&#36827;&#34892;&#30697;&#38453;&#20056;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#32467;&#26524;&#34920;&#26126;&#65306;
&lt;/p&gt;
&lt;p&gt;
Reducing the computational cost of running large scale neural networks using sparsity has attracted great attention in the deep learning community. While much success has been achieved in reducing FLOP and parameter counts while maintaining acceptable task performance, achieving actual speed improvements has typically been much more difficult, particularly on general purpose accelerators (GPAs) such as NVIDIA GPUs using low precision number formats. In this work we introduce PopSparse, a library that enables fast sparse operations on Graphcore IPUs by leveraging both the unique hardware characteristics of IPUs as well as any block structure defined in the data. We target two different types of sparsity: static, where the sparsity pattern is fixed at compile-time; and dynamic, where it can change each time the model is run. We present benchmark results for matrix multiplication for both of these modes on IPU with a range of block sizes, matrix sizes and densities. Results indicate that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#24615;&#22312;&#35299;&#20915;&#19981;&#27491;&#30830;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#30340;&#20316;&#29992;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#26597;&#35810;$ O(\varepsilon^{-s}d^s) $&#20010;&#25805;&#20316;&#26469;&#33719;&#24471;$O(\varepsilon)$-&#26368;&#20248;&#34892;&#21160;&#65292;&#20854;&#20013;$s$&#26159;&#31232;&#30095;&#24615;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.16998</link><description>&lt;p&gt;
&#31232;&#30095;&#24615;&#26159;&#21542;&#26377;&#21161;&#20110;&#23398;&#20064;&#19981;&#27491;&#30830;&#30340;&#32447;&#24615;&#36172;&#21338;&#26426;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does Sparsity Help in Learning Misspecified Linear Bandits?. (arXiv:2303.16998v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#24615;&#22312;&#35299;&#20915;&#19981;&#27491;&#30830;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#30340;&#20316;&#29992;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#26597;&#35810;$ O(\varepsilon^{-s}d^s) $&#20010;&#25805;&#20316;&#26469;&#33719;&#24471;$O(\varepsilon)$-&#26368;&#20248;&#34892;&#21160;&#65292;&#20854;&#20013;$s$&#26159;&#31232;&#30095;&#24615;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23398;&#20064;&#32447;&#24615;&#19981;&#27491;&#30830;&#36172;&#21338;&#26426;&#24050;&#32463;&#20135;&#29983;&#20102;&#23545;&#23398;&#20064;&#36172;&#21338;&#26426;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#38590;&#24230;&#30340;&#26377;&#36259;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;Du&#31561;&#20154;&#65288;2020&#65289;&#34920;&#26126;&#65292;&#21363;&#20351;&#23398;&#20064;&#32773;&#34987;&#36171;&#20104;&#22312;$ \mathbb{R}^d$ &#20013;&#36817;&#20284;&#36172;&#21338;&#26426;&#25110;RL&#22870;&#21169;&#30340;&#32447;&#24615;&#29305;&#24449;&#65292;&#19988;&#35823;&#24046;&#22312;$\varepsilon$&#30340;&#33539;&#22260;&#20869;&#65292;&#23547;&#25214;&#19968;&#20010;$ O&#65288;\varepsilon&#65289;$ -&#26368;&#20248;&#34892;&#21160;&#38656;&#35201;&#33267;&#23569;&#25289;&#20986;$ \Omega(\exp(d)) $&#30340;&#26597;&#35810;&#12290;&#27492;&#22806;&#65292;Lattimore&#31561;&#20154;&#65288;2020&#65289;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;$\operatorname{poly}(d/\varepsilon)$&#30340;&#26597;&#35810;&#20013;&#23398;&#20064;&#24471;&#21040;&#36864;&#21270;&#30340;$O(\varepsilon\sqrt{d})$ -&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#23454;&#38469;&#21442;&#25968;&#30340;&#32467;&#26500;&#20551;&#35774;&#65292;&#22914;&#31232;&#30095;&#24615;&#65292;&#26159;&#21542;&#33021;&#25171;&#30772;$\varepsilon\sqrt{d}$&#30340;&#38556;&#30861;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;&#23637;&#31034;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#26597;&#35810;$ O(\varepsilon^{-s}d^s) $&#20010;&#25805;&#20316;&#26469;&#33719;&#24471;$O(\varepsilon)$-&#26368;&#20248;&#34892;&#21160;&#65292;&#20854;&#20013;$s$&#26159;&#31232;&#30095;&#24615;&#21442;&#25968;&#65292;&#20197;&#28040;&#38500;$ \exp(d)$-&#20381;&#36182;&#24615;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the study of linear misspecified bandits has generated intriguing implications of the hardness of learning in bandits and reinforcement learning (RL). In particular, Du et al. (2020) show that even if a learner is given linear features in $\mathbb{R}^d$ that approximate the rewards in a bandit or RL with a uniform error of $\varepsilon$, searching for an $O(\varepsilon)$-optimal action requires pulling at least $\Omega(\exp(d))$ queries. Furthermore, Lattimore et al. (2020) show that a degraded $O(\varepsilon\sqrt{d})$-optimal solution can be learned within $\operatorname{poly}(d/\varepsilon)$ queries. Yet it is unknown whether a structural assumption on the ground-truth parameter, such as sparsity, could break the $\varepsilon\sqrt{d}$ barrier. In this paper, we address this question by showing that algorithms can obtain $O(\varepsilon)$-optimal actions by querying $O(\varepsilon^{-s}d^s)$ actions, where $s$ is the sparsity parameter, removing the $\exp(d)$-dependence. We th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#32852;&#21512;&#20559;&#31227;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#25972;&#20307;&#25968;&#25454;&#38598;&#20559;&#31227;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#20256;&#36882;SJS&#12289;&#20462;&#27491;&#31867;&#21518;&#39564;&#27010;&#29575;&#12289;SJS&#30340;&#21487;&#36776;&#35748;&#24615;&#12289;SJS&#19982;&#21327;&#21464;&#37327;&#36716;&#31227;&#20851;&#31995;&#31561;&#26032;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.16971</link><description>&lt;p&gt;
&#22810;&#39033;&#24335;&#20998;&#31867;&#20013;&#30340;&#31232;&#30095;&#32852;&#21512;&#20559;&#31227;
&lt;/p&gt;
&lt;p&gt;
Sparse joint shift in multinomial classification. (arXiv:2303.16971v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16971
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#32852;&#21512;&#20559;&#31227;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#25972;&#20307;&#25968;&#25454;&#38598;&#20559;&#31227;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#20256;&#36882;SJS&#12289;&#20462;&#27491;&#31867;&#21518;&#39564;&#27010;&#29575;&#12289;SJS&#30340;&#21487;&#36776;&#35748;&#24615;&#12289;SJS&#19982;&#21327;&#21464;&#37327;&#36716;&#31227;&#20851;&#31995;&#31561;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#32852;&#21512;&#20559;&#31227;&#65288;SJS&#65289;&#26159;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#38598;&#25972;&#20307;&#20559;&#31227;&#30340;&#21487;&#22788;&#29702;&#27169;&#22411;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#29305;&#24449;&#21644;&#26631;&#31614;&#30340;&#36793;&#38469;&#20998;&#24067;&#20197;&#21450;&#21518;&#39564;&#27010;&#29575;&#21644;&#31867;&#26465;&#20214;&#29305;&#24449;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;&#22312;&#27809;&#26377;&#26631;&#31614;&#35266;&#27979;&#30340;&#24773;&#20917;&#19979;&#65292;&#20026;&#30446;&#26631;&#25968;&#25454;&#38598;&#25311;&#21512;SJS&#21487;&#33021;&#20250;&#20135;&#29983;&#26631;&#31614;&#30340;&#26377;&#25928;&#39044;&#27979;&#21644;&#31867;&#20808;&#39564;&#27010;&#29575;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#22312;&#29305;&#24449;&#38598;&#20043;&#38388;&#20256;&#36882;SJS&#26041;&#38754;&#25552;&#20379;&#20102;&#26032;&#30340;&#32467;&#26524;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#30446;&#26631;&#20998;&#24067;&#30340;&#31867;&#21518;&#39564;&#27010;&#29575;&#30340;&#26465;&#20214;&#20462;&#27491;&#20844;&#24335;&#65292;&#30830;&#23450;&#24615;SJS&#30340;&#21487;&#36776;&#35748;&#24615;&#20197;&#21450;SJS&#21644;&#21327;&#21464;&#37327;&#36716;&#31227;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;&#29992;&#20110;&#20272;&#35745;SJS&#29305;&#24449;&#30340;&#31639;&#27861;&#20013;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#20250;&#22952;&#30861;&#23547;&#25214;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse joint shift (SJS) was recently proposed as a tractable model for general dataset shift which may cause changes to the marginal distributions of features and labels as well as the posterior probabilities and the class-conditional feature distributions. Fitting SJS for a target dataset without label observations may produce valid predictions of labels and estimates of class prior probabilities. We present new results on the transmission of SJS from sets of features to larger sets of features, a conditional correction formula for the class posterior probabilities under the target distribution, identifiability of SJS, and the relationship between SJS and covariate shift. In addition, we point out inconsistencies in the algorithms which were proposed for estimating the characteristics of SJS, as they could hamper the search for optimal solutions.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FADO&#30340;&#25968;&#25454;&#35780;&#20272;&#26694;&#26550;&#65292;&#26088;&#22312;&#21516;&#26102;&#26368;&#22823;&#21270;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#65292;&#21487;&#20197;&#29992;&#20110;&#19968;&#31995;&#21015;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#30340;&#22522;&#20110;&#29109;&#30340;&#25968;&#25454;&#35780;&#20272;&#25351;&#26631;&#27604;&#29616;&#26377;&#25351;&#26631;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#65292;&#24182;&#21487;&#29992;&#20110;&#19981;&#20844;&#24179;&#24615;&#32531;&#35299;&#39044;&#22788;&#29702;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2303.16963</link><description>&lt;p&gt;
&#35757;&#32451;&#25968;&#25454;&#30340;&#20844;&#24179;&#20215;&#20540;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Fairness-Aware Data Valuation for Supervised Learning. (arXiv:2303.16963v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FADO&#30340;&#25968;&#25454;&#35780;&#20272;&#26694;&#26550;&#65292;&#26088;&#22312;&#21516;&#26102;&#26368;&#22823;&#21270;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#65292;&#21487;&#20197;&#29992;&#20110;&#19968;&#31995;&#21015;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#30340;&#22522;&#20110;&#29109;&#30340;&#25968;&#25454;&#35780;&#20272;&#25351;&#26631;&#27604;&#29616;&#26377;&#25351;&#26631;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#65292;&#24182;&#21487;&#29992;&#20110;&#19981;&#20844;&#24179;&#24615;&#32531;&#35299;&#39044;&#22788;&#29702;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#35780;&#20272;&#26159;&#30740;&#31350;&#35757;&#32451;&#23454;&#20363;&#23545;&#20110;&#32473;&#23450;&#39044;&#27979;&#20219;&#21153;&#20215;&#20540;&#30340;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#12290;&#34429;&#28982;&#25968;&#25454;&#20559;&#35265;&#26159;&#19979;&#28216;&#27169;&#22411;&#19981;&#20844;&#24179;&#30340;&#20027;&#35201;&#26469;&#28304;&#20043;&#19968;&#65292;&#20294;&#20197;&#24448;&#30340;&#25968;&#25454;&#35780;&#20272;&#24037;&#20316;&#24182;&#26410;&#32771;&#34385;&#35757;&#32451;&#23454;&#20363;&#22914;&#20309;&#24433;&#21709;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#24863;&#30693;&#25968;&#25454;&#35780;&#20272;&#65288;FADO&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#29992;&#20110;&#23558;&#20844;&#24179;&#24615;&#32771;&#34385;&#32435;&#20837;&#19968;&#31995;&#21015;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#65288;&#20363;&#22914;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512;&#12289;&#20027;&#21160;&#23398;&#20064;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#29109;&#30340;&#25968;&#25454;&#35780;&#20272;&#25351;&#26631;&#65292;&#36866;&#29992;&#20110;&#35299;&#20915;&#25105;&#20204;&#30340;&#20004;&#20010;&#30446;&#26631;&#65306;&#26368;&#22823;&#21270;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#12290;&#35813;&#25351;&#26631;&#27604;&#29616;&#26377;&#25351;&#26631;&#26356;&#20855;&#35745;&#31639;&#25928;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;FADO&#24212;&#29992;&#20026;&#19981;&#20844;&#24179;&#24615;&#32531;&#35299;&#39044;&#22788;&#29702;&#25216;&#26415;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#8212;&#8212;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;&#22312;&#24615;&#33021;&#25439;&#22833;&#23567;&#20110;1 p.p.&#30340;&#24773;&#20917;&#19979;&#65292;&#20844;&#24179;&#24615;&#25552;&#39640;&#20102;&#22810;&#36798;40 p.p.&#12290;
&lt;/p&gt;
&lt;p&gt;
Data valuation is a ML field that studies the value of training instances towards a given predictive task. Although data bias is one of the main sources of downstream model unfairness, previous work in data valuation does not consider how training instances may influence both performance and fairness of ML models. Thus, we propose Fairness-Aware Data vauatiOn (FADO), a data valuation framework that can be used to incorporate fairness concerns into a series of ML-related tasks (e.g., data pre-processing, exploratory data analysis, active learning). We propose an entropy-based data valuation metric suited to address our two-pronged goal of maximizing both performance and fairness, which is more computationally efficient than existing metrics. We then show how FADO can be applied as the basis for unfairness mitigation pre-processing techniques. Our methods achieve promising results -- up to a 40 p.p. improvement in fairness at a less than 1 p.p. loss in performance compared to a baseline 
&lt;/p&gt;</description></item><item><title>FeDiSa&#26159;&#19968;&#31181;&#21322;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#30005;&#21147;&#31995;&#32479;&#25925;&#38556;&#21644;&#32593;&#32476;&#25915;&#20987;&#21028;&#21035;&#65292;&#37319;&#29992;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#26469;&#23454;&#29616;&#21327;&#21516;&#35757;&#32451;&#65292;&#33021;&#22815;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#36827;&#34892;&#25915;&#20987;&#26816;&#27979;&#27169;&#22411;&#30340;&#21327;&#21516;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2303.16956</link><description>&lt;p&gt;
FeDiSa: &#19968;&#31181;&#21322;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#30005;&#21147;&#31995;&#32479;&#25925;&#38556;&#21644;&#32593;&#32476;&#25915;&#20987;&#21028;&#21035;
&lt;/p&gt;
&lt;p&gt;
FeDiSa: A Semi-asynchronous Federated Learning Framework for Power System Fault and Cyberattack Discrimination. (arXiv:2303.16956v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16956
&lt;/p&gt;
&lt;p&gt;
FeDiSa&#26159;&#19968;&#31181;&#21322;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#30005;&#21147;&#31995;&#32479;&#25925;&#38556;&#21644;&#32593;&#32476;&#25915;&#20987;&#21028;&#21035;&#65292;&#37319;&#29992;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#26469;&#23454;&#29616;&#21327;&#21516;&#35757;&#32451;&#65292;&#33021;&#22815;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#36827;&#34892;&#25915;&#20987;&#26816;&#27979;&#27169;&#22411;&#30340;&#21327;&#21516;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26234;&#33021;&#30005;&#32593;&#39046;&#22495;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#20851;&#20999;&#26085;&#30410;&#22686;&#21152;&#65292;&#36817;&#24180;&#26469;&#38024;&#23545;&#20851;&#38190;&#33021;&#28304;&#22522;&#30784;&#35774;&#26045;&#30340;&#20837;&#20405;&#26816;&#27979;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#38544;&#31169;&#20445;&#25252;&#21644;&#20998;&#25955;&#30340;&#30005;&#21147;&#21306;&#22495;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#21487;&#34892;&#30340;&#38544;&#31169;&#20445;&#25252;&#26367;&#20195;&#26041;&#26696;&#65292;&#23427;&#20351;&#24471;&#25915;&#20987;&#26816;&#27979;&#27169;&#22411;&#30340;&#21327;&#21516;&#35757;&#32451;&#25104;&#20026;&#21487;&#33021;&#65292;&#32780;&#26080;&#38656;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#20256;&#32479;&#21516;&#27493;FL&#25152;&#38754;&#20020;&#30340;&#25216;&#26415;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;FeDiSa&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#30005;&#21147;&#31995;&#32479;&#25925;&#38556;&#21644;&#32593;&#32476;&#25915;&#20987;&#21028;&#21035;&#30340;&#21322;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#32771;&#34385;&#21040;&#36890;&#20449;&#24310;&#36831;&#21644;&#20302;&#25928;&#30340;&#33410;&#28857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30001;&#30417;&#25511;&#25511;&#21046;&#21644;&#25968;&#25454;&#37319;&#38598;&#23376;&#31995;&#32479;&#21327;&#21516;&#35757;&#32451;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#65292;&#23427;&#20204;&#23558;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#19978;&#20256;&#21040;&#25511;&#21046;&#20013;&#24515;&#65292;&#28982;&#21518;&#25191;&#34892;&#21322;&#24322;&#27493;&#27169;&#22411;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
With growing security and privacy concerns in the Smart Grid domain, intrusion detection on critical energy infrastructure has become a high priority in recent years. To remedy the challenges of privacy preservation and decentralized power zones with strategic data owners, Federated Learning (FL) has contemporarily surfaced as a viable privacy-preserving alternative which enables collaborative training of attack detection models without requiring the sharing of raw data. To address some of the technical challenges associated with conventional synchronous FL, this paper proposes FeDiSa, a novel Semi-asynchronous Federated learning framework for power system faults and cyberattack Discrimination which takes into account communication latency and stragglers. Specifically, we propose a collaborative training of deep auto-encoder by Supervisory Control and Data Acquisition sub-systems which upload their local model updates to a control centre, which then perform a semi-asynchronous model ag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37327;&#23376;&#38376;&#29983;&#25104;&#26032;&#26679;&#26412;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#24182;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16955</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;&#37327;&#23376;&#38376;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Generative Modeling Approach Using Quantum Gates. (arXiv:2303.16955v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37327;&#23376;&#38376;&#29983;&#25104;&#26032;&#26679;&#26412;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#24182;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20123;&#24180;&#26469;&#65292;&#37327;&#23376;&#35745;&#31639;&#20316;&#20026;&#35299;&#20915;&#22797;&#26434;&#35745;&#31639;&#38382;&#39064;&#30340;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#65292;&#24320;&#22987;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#29983;&#25104;&#24314;&#27169;&#26159;&#19968;&#31181;&#25216;&#26415;&#65292;&#21487;&#20197;&#35753;&#25105;&#20204;&#23398;&#20064;&#24182;&#29983;&#25104;&#31867;&#20284;&#20110;&#21407;&#25968;&#25454;&#38598;&#30340;&#26032;&#25968;&#25454;&#26679;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37327;&#23376;&#38376;&#29983;&#25104;&#26032;&#26679;&#26412;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#12290;&#25105;&#20204;&#20174;&#31616;&#35201;&#20171;&#32461;&#37327;&#23376;&#35745;&#31639;&#21644;&#29983;&#25104;&#24314;&#27169;&#24320;&#22987;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#28041;&#21450;&#21040;&#23558;&#25968;&#25454;&#38598;&#32534;&#30721;&#25104;&#37327;&#23376;&#24577;&#65292;&#24182;&#20351;&#29992;&#37327;&#23376;&#38376;&#26469;&#25805;&#20316;&#36825;&#20123;&#29366;&#24577;&#20197;&#29983;&#25104;&#26032;&#26679;&#26412;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#25968;&#23398;&#32454;&#33410;&#65292;&#24182;&#36890;&#36807;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26469;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, quantum computing has emerged as a promising technology for solving complex computational problems. Generative modeling is a technique that allows us to learn and generate new data samples similar to the original dataset. In this paper, we propose a generative modeling approach using quantum gates to generate new samples from a given dataset. We start with a brief introduction to quantum computing and generative modeling. Then, we describe our proposed approach, which involves encoding the dataset into quantum states and using quantum gates to manipulate these states to generate new samples. We also provide mathematical details of our approach and demonstrate its effectiveness through experimental results on various datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#36125;&#21494;&#26031;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#22810;&#20010;&#27979;&#37327;&#21521;&#37327;&#20013;&#25512;&#26029;&#32852;&#21512;&#31232;&#30095;&#30340;&#21442;&#25968;&#21521;&#37327;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20849;&#21516;&#30340;&#20285;&#39532;&#20998;&#24067;&#36229;&#21442;&#25968;&#26469;&#24378;&#21046;&#32852;&#21512;&#31232;&#30095;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2303.16954</link><description>&lt;p&gt;
&#21033;&#29992;&#32852;&#21512;&#31232;&#30095;&#24615;&#30340;&#20998;&#23618;&#36125;&#21494;&#26031;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Leveraging joint sparsity in hierarchical Bayesian learning. (arXiv:2303.16954v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#36125;&#21494;&#26031;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#22810;&#20010;&#27979;&#37327;&#21521;&#37327;&#20013;&#25512;&#26029;&#32852;&#21512;&#31232;&#30095;&#30340;&#21442;&#25968;&#21521;&#37327;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20849;&#21516;&#30340;&#20285;&#39532;&#20998;&#24067;&#36229;&#21442;&#25968;&#26469;&#24378;&#21046;&#32852;&#21512;&#31232;&#30095;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#36125;&#21494;&#26031;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#22810;&#20010;&#27979;&#37327;&#21521;&#37327;&#20013;&#25512;&#26029;&#32852;&#21512;&#31232;&#30095;&#30340;&#21442;&#25968;&#21521;&#37327;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20026;&#27599;&#20010;&#21442;&#25968;&#21521;&#37327;&#20351;&#29992;&#21333;&#29420;&#30340;&#26465;&#20214;&#39640;&#26031;&#20808;&#39564;&#65292;&#24182;&#20351;&#29992;&#20849;&#21516;&#30340;&#20285;&#39532;&#20998;&#24067;&#36229;&#21442;&#25968;&#26469;&#24378;&#21046;&#32852;&#21512;&#31232;&#30095;&#24615;&#12290;&#24471;&#21040;&#30340;&#32852;&#21512;&#31232;&#30095;&#24615;&#20808;&#39564;&#19982;&#29616;&#26377;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#24418;&#25104;&#20102;&#19968;&#31995;&#21015;&#26032;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#21253;&#25324;&#22810;&#32447;&#22280;&#30913;&#20849;&#25391;&#25104;&#20687;&#24212;&#29992;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#24120;&#29992;&#30340;&#20998;&#23618;&#36125;&#21494;&#26031;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a hierarchical Bayesian learning approach to infer jointly sparse parameter vectors from multiple measurement vectors. Our model uses separate conditionally Gaussian priors for each parameter vector and common gamma-distributed hyper-parameters to enforce joint sparsity. The resulting joint-sparsity-promoting priors are combined with existing Bayesian inference methods to generate a new family of algorithms. Our numerical experiments, which include a multi-coil magnetic resonance imaging application, demonstrate that our new approach consistently outperforms commonly used hierarchical Bayesian methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#21487;&#24494;&#20984;&#20248;&#21270;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#29616;&#26377;&#30340;&#19968;&#38454;&#26356;&#26032;&#35268;&#21017;&#25512;&#24191;&#21040;&#26356;&#24191;&#30340;&#23478;&#26063;&#65292;&#35777;&#26126;&#22312;&#20803;&#23398;&#20064;&#32773;&#26377;&#36275;&#22815;&#31867;&#20284;&#20219;&#21153;&#30340;&#32463;&#39564;&#19979;&#65292;&#21487;&#20197;&#19968;&#27493;&#20248;&#21270;&#19968;&#31995;&#21015;&#32447;&#24615;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.16952</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#24494;&#20984;&#20248;&#21270;&#20803;&#23398;&#20064;&#21442;&#25968;&#21270;&#30340;&#19968;&#38454;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Meta-Learning Parameterized First-Order Optimizers using Differentiable Convex Optimization. (arXiv:2303.16952v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16952
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#21487;&#24494;&#20984;&#20248;&#21270;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#29616;&#26377;&#30340;&#19968;&#38454;&#26356;&#26032;&#35268;&#21017;&#25512;&#24191;&#21040;&#26356;&#24191;&#30340;&#23478;&#26063;&#65292;&#35777;&#26126;&#22312;&#20803;&#23398;&#20064;&#32773;&#26377;&#36275;&#22815;&#31867;&#20284;&#20219;&#21153;&#30340;&#32463;&#39564;&#19979;&#65292;&#21487;&#20197;&#19968;&#27493;&#20248;&#21270;&#19968;&#31995;&#21015;&#32447;&#24615;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#25511;&#21046;&#20013;&#30340;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#19968;&#38454;&#26356;&#26032;&#35268;&#21017;&#12290;&#38024;&#23545;&#26576;&#20010;&#20219;&#21153;&#36873;&#25321;&#21512;&#36866;&#26041;&#27861;&#21644;&#36229;&#21442;&#25968;&#24120;&#24120;&#38656;&#35201;&#35797;&#38169;&#25110;&#20174;&#19994;&#32773;&#30452;&#35273;&#65292;&#36825;&#20419;&#36827;&#20102;&#20803;&#23398;&#20064;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#20869;&#24490;&#29615;&#20248;&#21270;&#27493;&#39588;&#28041;&#21450;&#21487;&#24494;&#20984;&#20248;&#21270;(DCO)&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#25512;&#24191;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#29616;&#26377;&#26356;&#26032;&#35268;&#21017;&#23478;&#26063;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#27492;&#26041;&#27861;&#30340;&#29702;&#35770;&#21560;&#24341;&#21147;&#65292;&#35777;&#26126;&#20102;&#22312;&#20803;&#23398;&#20064;&#32773;&#26377;&#36275;&#22815;&#30340;&#31867;&#20284;&#20219;&#21153;&#32463;&#39564;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#21487;&#20197;&#19968;&#27493;&#20248;&#21270;&#19968;&#31995;&#21015;&#32447;&#24615;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#12290;&#22312;&#19968;&#31995;&#21015;&#35828;&#26126;&#24615;&#23454;&#39564;&#35774;&#32622;&#20013;&#65292;&#23558;DCO&#26356;&#26032;&#35268;&#21017;&#30340;&#21508;&#31181;&#23454;&#20363;&#19982;&#20256;&#32479;&#20248;&#21270;&#22120;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional optimization methods in machine learning and controls rely heavily on first-order update rules. Selecting the right method and hyperparameters for a particular task often involves trial-and-error or practitioner intuition, motivating the field of meta-learning. We generalize a broad family of preexisting update rules by proposing a meta-learning framework in which the inner loop optimization step involves solving a differentiable convex optimization (DCO). We illustrate the theoretical appeal of this approach by showing that it enables one-step optimization of a family of linear least squares problems, given that the meta-learner has sufficient exposure to similar tasks. Various instantiations of the DCO update rule are compared to conventional optimizers on a range of illustrative experimental settings.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#23494;&#38598;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#32806;&#21512;&#21644;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#21644;&#35299;&#32806;&#19982;&#21435;&#23450;&#20301;&#27169;&#22359;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#30446;&#26631;&#20998;&#31867;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#30446;&#26631;&#26816;&#27979;&#31561;&#20219;&#21153;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.16947</link><description>&lt;p&gt;
&#35299;&#32806;&#21644;&#21435;&#23450;&#20301;&#23494;&#38598;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
De-coupling and De-positioning Dense Self-supervised Learning. (arXiv:2303.16947v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16947
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#23494;&#38598;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#32806;&#21512;&#21644;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#21644;&#35299;&#32806;&#19982;&#21435;&#23450;&#20301;&#27169;&#22359;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#30446;&#26631;&#20998;&#31867;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#30446;&#26631;&#26816;&#27979;&#31561;&#20219;&#21153;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20102;&#22788;&#29702;&#20855;&#26377;&#22810;&#20010;&#29289;&#20307;&#30340;&#22270;&#20687;&#26102;&#20351;&#29992;&#22270;&#20687;&#32423;&#29305;&#24449;&#34920;&#31034;&#30340;&#38480;&#21046;&#12290;&#34429;&#28982;&#21033;&#29992;&#20998;&#21106;&#26144;&#23556;&#21644;&#36793;&#30028;&#26694;&#25552;&#21462;&#30340;&#23494;&#38598;&#29305;&#24449;&#20801;&#35768;&#32593;&#32476;&#23545;&#27599;&#20010;&#29289;&#20307;&#25191;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#23427;&#20204;&#20250;&#21463;&#21040;&#32806;&#21512;&#21644;&#20301;&#32622;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;&#36825;&#26159;&#30001;&#20110;&#24863;&#21463;&#37326;&#38543;&#30528;&#23618;&#28145;&#24230;&#21644;&#38646;&#22635;&#20805;&#32780;&#22686;&#21152;&#25152;&#23548;&#33268;&#30340;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#31181;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#24182;&#22312;&#35299;&#32806;&#27169;&#22359;&#21644;&#21435;&#23450;&#20301;&#27169;&#22359;&#20013;&#21033;&#29992;&#23427;&#20204;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;COCO&#21644;&#19968;&#20010;&#26032;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;OpenImage-MINI&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#29992;&#20110;&#30446;&#26631;&#20998;&#31867;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#30446;&#26631;&#26816;&#27979;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#27604;SOT&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dense Self-Supervised Learning (SSL) methods address the limitations of using image-level feature representations when handling images with multiple objects. Although the dense features extracted by employing segmentation maps and bounding boxes allow networks to perform SSL for each object, we show that they suffer from coupling and positional bias, which arise from the receptive field increasing with layer depth and zero-padding. We address this by introducing three data augmentation strategies, and leveraging them in (i) a decoupling module that aims to robustify the network to variations in the object's surroundings, and (ii) a de-positioning module that encourages the network to discard positional object information. We demonstrate the benefits of our method on COCO and on a new challenging benchmark, OpenImage-MINI, for object classification, semantic segmentation, and object detection. Our extensive experiments evidence the better generalization of our method compared to the SOT
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;&#24403;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;NAS&#22522;&#20934;&#27979;&#35797;&#36827;&#34892;&#20102;&#32463;&#39564;&#30740;&#31350;&#65292;&#21457;&#29616;&#21482;&#38656;&#19968;&#23567;&#37096;&#20998;&#30340;&#25805;&#20316;&#21363;&#21487;&#29983;&#25104;&#25509;&#36817;&#26368;&#39640;&#24615;&#33021;&#30340;&#26550;&#26500;&#65292;&#21516;&#26102;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#23384;&#22312;&#32570;&#28857;&#21487;&#33021;&#24433;&#21709;&#20844;&#24179;&#27604;&#36739;&#24182;&#25552;&#20379;&#19981;&#21487;&#38752;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.16938</link><description>&lt;p&gt;
&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#22522;&#20934;&#27979;&#35797;&#26159;&#21542;&#35774;&#35745;&#33391;&#22909;&#65311;&#23545;&#25805;&#20316;&#37325;&#35201;&#24615;&#30340;&#28145;&#20837;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Are Neural Architecture Search Benchmarks Well Designed? A Deeper Look Into Operation Importance. (arXiv:2303.16938v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;&#24403;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;NAS&#22522;&#20934;&#27979;&#35797;&#36827;&#34892;&#20102;&#32463;&#39564;&#30740;&#31350;&#65292;&#21457;&#29616;&#21482;&#38656;&#19968;&#23567;&#37096;&#20998;&#30340;&#25805;&#20316;&#21363;&#21487;&#29983;&#25104;&#25509;&#36817;&#26368;&#39640;&#24615;&#33021;&#30340;&#26550;&#26500;&#65292;&#21516;&#26102;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#23384;&#22312;&#32570;&#28857;&#21487;&#33021;&#24433;&#21709;&#20844;&#24179;&#27604;&#36739;&#24182;&#25552;&#20379;&#19981;&#21487;&#38752;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#22522;&#20934;&#27979;&#35797;&#26174;&#33879;&#25552;&#39640;&#20102;&#24320;&#21457;&#21644;&#27604;&#36739;NAS&#26041;&#27861;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#36890;&#36807;&#25552;&#20379;&#20851;&#20110;&#25968;&#21315;&#20010;&#35757;&#32451;&#36807;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#20803;&#20449;&#24687;&#65292;&#22823;&#24133;&#20943;&#23569;&#20102;&#35745;&#31639;&#24320;&#38144;&#12290;&#28982;&#32780;&#65292;&#34920;&#26684;&#22522;&#20934;&#27979;&#35797;&#20855;&#26377;&#20960;&#20010;&#32570;&#28857;&#65292;&#21487;&#33021;&#20250;&#38459;&#30861;&#20844;&#24179;&#27604;&#36739;&#24182;&#25552;&#20379;&#19981;&#21487;&#38752;&#30340;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#24191;&#27867;&#20351;&#29992;&#30340;NAS-Bench-101&#12289;NAS-Bench-201&#21644;TransNAS-Bench-101&#22522;&#20934;&#27979;&#35797;&#36827;&#34892;&#20102;&#32463;&#39564;&#24615;&#20998;&#26512;&#65292;&#37325;&#28857;&#20851;&#27880;&#23427;&#20204;&#30340;&#36890;&#29992;&#24615;&#20197;&#21450;&#19981;&#21516;&#25805;&#20316;&#22914;&#20309;&#24433;&#21709;&#25152;&#29983;&#25104;&#26550;&#26500;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#38656;&#35201;&#25805;&#20316;&#27744;&#30340;&#19968;&#37096;&#20998;&#21363;&#21487;&#29983;&#25104;&#25509;&#36817;&#26368;&#39640;&#24615;&#33021;&#33539;&#22260;&#30340;&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#24615;&#33021;&#20998;&#24067;&#20855;&#26377;&#36127;&#20559;&#26012;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Architecture Search (NAS) benchmarks significantly improved the capability of developing and comparing NAS methods while at the same time drastically reduced the computational overhead by providing meta-information about thousands of trained neural networks. However, tabular benchmarks have several drawbacks that can hinder fair comparisons and provide unreliable results. These usually focus on providing a small pool of operations in heavily constrained search spaces -- usually cell-based neural networks with pre-defined outer-skeletons. In this work, we conducted an empirical analysis of the widely used NAS-Bench-101, NAS-Bench-201 and TransNAS-Bench-101 benchmarks in terms of their generability and how different operations influence the performance of the generated architectures. We found that only a subset of the operation pool is required to generate architectures close to the upper-bound of the performance range. Also, the performance distribution is negatively skewed, havi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#26032;&#31639;&#27861;&#26469;&#36873;&#25321;&#22522;&#22240;&#32452;&#25968;&#25454;&#20013;&#26368;&#26377;&#20449;&#24687;&#30340;&#29305;&#24449;&#65292;&#20026;&#35786;&#26029;&#12289;&#39044;&#21518;&#21644;&#31934;&#20934;&#21307;&#23398;&#25552;&#20379;&#25903;&#25345;&#65292;&#24182;&#22312;&#24930;&#24615;&#28107;&#24052;&#32454;&#32990;&#30333;&#34880;&#30149;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#25928;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.16914</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;XAI&#30340;&#22522;&#22240;&#32452;&#23398;&#29305;&#24449;&#36873;&#25321;&#26032;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A New Deep Learning and XAI-Based Algorithm for Features Selection in Genomics. (arXiv:2303.16914v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#26032;&#31639;&#27861;&#26469;&#36873;&#25321;&#22522;&#22240;&#32452;&#25968;&#25454;&#20013;&#26368;&#26377;&#20449;&#24687;&#30340;&#29305;&#24449;&#65292;&#20026;&#35786;&#26029;&#12289;&#39044;&#21518;&#21644;&#31934;&#20934;&#21307;&#23398;&#25552;&#20379;&#25903;&#25345;&#65292;&#24182;&#22312;&#24930;&#24615;&#28107;&#24052;&#32454;&#32990;&#30333;&#34880;&#30149;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#25928;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21151;&#33021;&#22522;&#22240;&#32452;&#23398;&#39046;&#22495;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#23545;&#22522;&#22240;&#34920;&#36798;&#35889;&#30340;&#20998;&#26512;&#36234;&#26469;&#36234;&#33021;&#22815;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#27934;&#23519;&#21147;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22810;&#31181;&#30142;&#30149;&#30340;&#20998;&#26512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#33258;&#32534;&#30721;&#22120;&#30340;&#37325;&#26500;&#33021;&#21147;&#21644;&#19968;&#31181;&#19987;&#38376;&#23450;&#20041;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#22522;&#30784;&#20998;&#25968;&#65292;&#20197;&#36873;&#25321;&#20316;&#20026;&#35786;&#26029;&#12289;&#39044;&#21518;&#21644;&#31934;&#20934;&#21307;&#23398;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#22522;&#22240;&#32452;&#25968;&#25454;&#29305;&#24449;&#12290;&#22312;&#24930;&#24615;&#28107;&#24052;&#32454;&#32990;&#30333;&#34880;&#30149;&#25968;&#25454;&#38598;&#19978;&#30340;&#24212;&#29992;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#30830;&#23450;&#24182;&#25552;&#20379;&#19968;&#32452;&#20855;&#26377;&#24847;&#20041;&#30340;&#22522;&#22240;&#32452;&#25968;&#25454;&#29305;&#24449;&#65292;&#26377;&#21161;&#20110;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#21307;&#23398;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of functional genomics, the analysis of gene expression profiles through Machine and Deep Learning is increasingly providing meaningful insight into a number of diseases. The paper proposes a novel algorithm to perform Feature Selection on genomic-scale data, which exploits the reconstruction capabilities of autoencoders and an ad-hoc defined Explainable Artificial Intelligence-based score in order to select the most informative genes for diagnosis, prognosis, and precision medicine. Results of the application on a Chronic Lymphocytic Leukemia dataset evidence the effectiveness of the algorithm, by identifying and suggesting a set of meaningful genes for further medical investigation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;Population-based Bayesian hyper-heuristic&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#30340;&#19981;&#21516;&#38454;&#27573;&#33258;&#21160;&#23547;&#25214;&#26368;&#20339;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#26469;&#35757;&#32451;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2303.16912</link><description>&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#36229;&#21551;&#21457;&#24335;&#31639;&#27861;&#35757;&#32451;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Training Feedforward Neural Networks with Bayesian Hyper-Heuristics. (arXiv:2303.16912v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;Population-based Bayesian hyper-heuristic&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#30340;&#19981;&#21516;&#38454;&#27573;&#33258;&#21160;&#23547;&#25214;&#26368;&#20339;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#26469;&#35757;&#32451;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;(FFNNs)&#30340;&#36807;&#31243;&#21487;&#20197;&#20174;&#33258;&#21160;&#21270;&#36807;&#31243;&#20013;&#21463;&#30410;&#65292;&#20854;&#20013;&#26368;&#20339;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#36890;&#36807;&#39640;&#32423;&#22522;&#20110;&#27010;&#29575;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#33258;&#21160;&#23547;&#25214;&#26469;&#35757;&#32451;&#32593;&#32476;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;Population-based Bayesian hyper-heuristic (BHH)&#65292;&#29992;&#20110;&#35757;&#32451;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;(FFNNs)&#12290;BHH&#30340;&#24615;&#33021;&#19982;&#21313;&#31181;&#27969;&#34892;&#30340;&#20302;&#32423;&#21551;&#21457;&#24335;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#27599;&#31181;&#26041;&#27861;&#37117;&#20855;&#26377;&#19981;&#21516;&#30340;&#26597;&#25214;&#34892;&#20026;&#12290;&#25152;&#36873;&#30340;&#21551;&#21457;&#24335;&#27744;&#21253;&#25324;&#32463;&#20856;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#20197;&#21450;&#20803;&#21551;&#21457;&#24335;( MHs)&#12290;&#23454;&#35777;&#36807;&#31243;&#23545;&#21313;&#22235;&#20010;&#20855;&#26377;&#19981;&#21516;&#24615;&#36136;&#30340;&#20998;&#31867;&#21644;&#22238;&#24402;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#25191;&#34892;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;BHH&#33021;&#22815;&#24456;&#22909;&#22320;&#25191;&#34892;FFNNs&#30340;&#35757;&#32451;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#26469;&#22312;&#35757;&#32451;&#36807;&#31243;&#30340;&#21508;&#20010;&#38454;&#27573;&#20013;&#23547;&#25214;&#26368;&#20339;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The process of training feedforward neural networks (FFNNs) can benefit from an automated process where the best heuristic to train the network is sought out automatically by means of a high-level probabilistic-based heuristic. This research introduces a novel population-based Bayesian hyper-heuristic (BHH) that is used to train feedforward neural networks (FFNNs). The performance of the BHH is compared to that of ten popular low-level heuristics, each with different search behaviours. The chosen heuristic pool consists of classic gradient-based heuristics as well as meta-heuristics (MHs). The empirical process is executed on fourteen datasets consisting of classification and regression problems with varying characteristics. The BHH is shown to be able to train FFNNs well and provide an automated method for finding the best heuristic to train the FFNNs at various stages of the training process.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25253;&#36947;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26816;&#27979;&#30913;&#24615;&#33258;&#26059;&#32467;&#26500;&#30340;&#30740;&#31350;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#21487;&#38024;&#23545;&#20998;&#21106;&#38382;&#39064;&#26816;&#27979;&#22825;&#32447;&#30340;&#20301;&#32622;&#21644;&#24418;&#29366;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#33258;&#21160;&#21270;&#25928;&#29575;&#65292;&#25454;&#27492;&#26377;&#26395;&#20026;&#20869;&#23384;&#21644;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#26041;&#26696;&#25552;&#20379;&#21487;&#34892;&#24615;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2303.16905</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33258;&#26059;&#32467;&#26500;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Machine learning-based spin structure detection. (arXiv:2303.16905v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16905
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25253;&#36947;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26816;&#27979;&#30913;&#24615;&#33258;&#26059;&#32467;&#26500;&#30340;&#30740;&#31350;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#21487;&#38024;&#23545;&#20998;&#21106;&#38382;&#39064;&#26816;&#27979;&#22825;&#32447;&#30340;&#20301;&#32622;&#21644;&#24418;&#29366;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#33258;&#21160;&#21270;&#25928;&#29575;&#65292;&#25454;&#27492;&#26377;&#26395;&#20026;&#20869;&#23384;&#21644;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#26041;&#26696;&#25552;&#20379;&#21487;&#34892;&#24615;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#37325;&#35201;&#30340;&#30913;&#24615;&#33258;&#26059;&#32467;&#26500;&#20043;&#19968;&#26159;&#25299;&#25169;&#31283;&#23450;&#30340;&#22825;&#32447;&#29366;&#20934;&#31890;&#23376;&#12290;&#20854;&#26377;&#36259;&#30340;&#29289;&#29702;&#23646;&#24615;&#20351;&#20854;&#25104;&#20026;&#20869;&#23384;&#21644;&#39640;&#25928;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#26041;&#26696;&#30340;&#20505;&#36873;&#12290;&#23545;&#20110;&#22120;&#20214;&#25805;&#20316;&#65292;&#38656;&#35201;&#26816;&#27979;&#22825;&#32447;&#30340;&#20301;&#32622;&#12289;&#24418;&#29366;&#21644;&#23610;&#23544;&#65292;&#36890;&#24120;&#37319;&#29992;&#30913;&#24615;&#25104;&#20687;&#12290;&#24120;&#29992;&#25216;&#26415;&#26159;&#30913;&#20809;Kerr&#26174;&#24494;&#38236;&#65292;&#35813;&#25216;&#26415;&#26681;&#25454;&#26679;&#21697;&#30340;&#26448;&#26009;&#32452;&#25104;&#12289;&#28201;&#24230;&#12289;&#26448;&#26009;&#29983;&#38271;&#36807;&#31243;&#31561;&#65292;&#27979;&#37327;&#20250;&#21463;&#21040;&#22122;&#22768;&#12289;&#20302;&#23545;&#27604;&#24230;&#12289;&#24378;&#24230;&#26799;&#24230;&#25110;&#20854;&#20182;&#20809;&#23398;&#20266;&#20687;&#30340;&#24433;&#21709;&#12290;&#20256;&#32479;&#30340;&#22270;&#20687;&#20998;&#26512;&#36719;&#20214;&#38656;&#35201;&#25163;&#21160;&#22788;&#29702;&#65292;&#38656;&#35201;&#26356;&#33258;&#21160;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25253;&#36947;&#20102;&#19968;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#19987;&#38376;&#29992;&#20110;&#20998;&#21106;&#38382;&#39064;&#65292;&#20197;&#26816;&#27979;&#25105;&#20204;&#27979;&#37327;&#20013;&#30340;&#22825;&#32447;&#20301;&#32622;&#21644;&#24418;&#29366;&#12290;&#21033;&#29992;&#36873;&#25321;&#30340;&#25216;&#26415;&#35843;&#25972;&#32593;&#32476;&#20197;&#20248;&#21270;&#39044;&#27979;&#65292;&#29305;&#21035;&#26159;&#26816;&#27979;&#21040;&#30340;&#22242;&#31751;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most important magnetic spin structure is the topologically stabilised skyrmion quasi-particle. Its interesting physical properties make them candidates for memory and efficient neuromorphic computation schemes. For the device operation, detection of the position, shape, and size of skyrmions is required and magnetic imaging is typically employed. A frequently used technique is magneto-optical Kerr microscopy where depending on the samples material composition, temperature, material growing procedures, etc., the measurements suffer from noise, low-contrast, intensity gradients, or other optical artifacts. Conventional image analysis packages require manual treatment, and a more automatic solution is required. We report a convolutional neural network specifically designed for segmentation problems to detect the position and shape of skyrmions in our measurements. The network is tuned using selected techniques to optimize predictions and in particular the number of detected cl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#20108;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#23545;&#32954;&#37096;CT&#25195;&#25551;&#36827;&#34892;&#20005;&#37325;&#31243;&#24230;&#20998;&#31867;&#65292;&#20026;&#35786;&#26029;&#32954;&#37096;&#30142;&#30149;&#22914;COVID-19&#21644;&#32954;&#28814;&#25552;&#20379;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.16904</link><description>&lt;p&gt;
&#36890;&#36807;&#20108;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#32954;&#37096;CT&#25195;&#25551;&#23545;&#22320;&#29627;&#29827;&#24433;&#36827;&#34892;&#20005;&#37325;&#31243;&#24230;&#20998;&#31867;&#65306;&#19968;&#20010;&#19977;&#22825;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Severity classification of ground-glass opacity via 2-D convolutional neural network and lung CT scans: a 3-day exploration. (arXiv:2303.16904v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#20108;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#23545;&#32954;&#37096;CT&#25195;&#25551;&#36827;&#34892;&#20005;&#37325;&#31243;&#24230;&#20998;&#31867;&#65292;&#20026;&#35786;&#26029;&#32954;&#37096;&#30142;&#30149;&#22914;COVID-19&#21644;&#32954;&#28814;&#25552;&#20379;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29627;&#29827;&#24433;&#26159;&#35768;&#22810;&#32954;&#37096;&#30142;&#30149;&#30340;&#26631;&#24535;&#65292;&#21253;&#25324;COVID19&#21644;&#32954;&#28814;&#24739;&#32773;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#26032;&#24314;&#30340;&#34394;&#25311;&#29615;&#22659;&#65292;&#22312;2023&#24180;3&#26376;17&#26085;&#21019;&#24314;&#65292;&#36890;&#36807;&#23454;&#39564;&#25506;&#35752;&#20102;&#21508;&#31181;&#39044;&#35757;&#32451;&#30340;&#20108;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#22914;&#31264;&#23494;&#31070;&#32463;&#32593;&#32476;&#12289;ResNet&#21644;&#35270;&#35273;&#21464;&#25442;&#22120;&#65292;&#20197;&#21450;&#24494;&#35843;&#30340;&#31243;&#24230;&#12290;&#26681;&#25454;&#32463;&#39564;&#23454;&#39564;&#65292;&#25105;&#20204;&#36873;&#25321;&#20351;&#29992;ADAM&#20248;&#21270;&#31639;&#27861;&#23545;&#25152;&#26377;CNN&#26550;&#26500;&#36827;&#34892;&#26631;&#20934;&#23398;&#20064;&#29575;0.001&#30340;&#24494;&#35843;&#65292;&#24182;&#22312;&#39564;&#35777;&#25439;&#22833;&#36798;&#21040;&#24179;&#21407;&#26102;&#36827;&#34892;&#26089;&#20572;&#12290;&#20026;&#27599;&#20010;&#35757;&#32451;&#30340;CNN&#21046;&#23450;&#20005;&#37325;&#31243;&#24230;&#20998;&#31867;&#35268;&#21017;&#65292;&#24182;&#22312;&#19977;&#22825;&#20869;&#36827;&#34892;&#20102;&#23454;&#39564;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ground-glass opacity is a hallmark of numerous lung diseases, including patients with COVID19 and pneumonia. This brief note presents experimental results of a proof-of-concept framework that got implemented and tested over three days as driven by the third challenge entitled "COVID-19 Competition", hosted at the AI-Enabled Medical Image Analysis Workshop of the 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2023). Using a newly built virtual environment (created on March 17, 2023), we investigated various pre-trained two-dimensional convolutional neural networks (CNN) such as Dense Neural Network, Residual Neural Networks (ResNet), and Vision Transformers, as well as the extent of fine-tuning. Based on empirical experiments, we opted to fine-tune them using ADAM's optimization algorithm with a standard learning rate of 0.001 for all CNN architectures and apply early-stopping whenever the validation loss reached a plateau. For each trained CNN, th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MaMMUT&#30340;&#31616;&#21333;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#20004;&#27493;&#26041;&#27861;&#23481;&#32435;&#23545;&#27604;&#21644;&#29983;&#25104;&#23398;&#20064;&#65292;&#24182;&#22312;&#32852;&#21512;&#35757;&#32451;&#19981;&#21516;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#26102;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#25928;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.16839</link><description>&lt;p&gt;
MaMMUT: &#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#20219;&#21153;&#32852;&#21512;&#23398;&#20064;&#30340;&#31616;&#21333;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks. (arXiv:2303.16839v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16839
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MaMMUT&#30340;&#31616;&#21333;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#20004;&#27493;&#26041;&#27861;&#23481;&#32435;&#23545;&#27604;&#21644;&#29983;&#25104;&#23398;&#20064;&#65292;&#24182;&#22312;&#32852;&#21512;&#35757;&#32451;&#19981;&#21516;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#26102;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#25928;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#24050;&#20174;&#32534;&#30721;-&#35299;&#30721;&#36716;&#21521;&#20165;&#35299;&#30721;&#30340;&#35774;&#35745;&#12290;&#27492;&#22806;&#65292;&#26222;&#36941;&#35748;&#20026;&#65292;&#26368;&#27969;&#34892;&#30340;&#20004;&#31181;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#29983;&#25104;&#20219;&#21153;&#21644;&#23545;&#27604;&#20219;&#21153;&#65292;&#24448;&#24448;&#20114;&#30456;&#20914;&#31361;&#65292;&#38590;&#20197;&#22312;&#19968;&#20010;&#26550;&#26500;&#20013;&#23481;&#32435;&#65292;&#24182;&#36827;&#19968;&#27493;&#38656;&#35201;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#22797;&#26434;&#35843;&#25972;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22521;&#35757;&#33539;&#24335;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#20165;&#35299;&#30721;&#27169;&#22411;&#65292;&#36825;&#22312;&#32852;&#21512;&#23398;&#20064;&#36825;&#20123;&#19981;&#21516;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#36825;&#26159;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#27169;&#22411;MaMMUT&#23454;&#29616;&#30340;&#12290;&#23427;&#30001;&#21333;&#19968;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#25991;&#26412;&#35299;&#30721;&#22120;&#32452;&#25104;&#65292;&#24182;&#33021;&#22815;&#36890;&#36807;&#25991;&#26412;&#35299;&#30721;&#22120;&#19978;&#30340;&#26032;&#30340;&#20004;&#27493;&#26041;&#27861;&#23481;&#32435;&#23545;&#27604;&#21644;&#29983;&#25104;&#23398;&#20064;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20123;&#19981;&#21516;&#30446;&#26631;&#20219;&#21153;&#30340;&#32852;&#21512;&#35757;&#32451;&#26159;&#31616;&#21333;&#30340;&#65292;&#26377;&#25928;&#30340;&#65292;&#24182;&#26368;&#22823;&#21270;&#20102;&#27169;&#22411;&#30340;&#26435;&#37325;&#20849;&#20139;&#12290;&#27492;&#22806;&#65292;&#30456;&#21516;&#30340;&#26550;&#26500;&#20351;&#24471;&#23545;&#24320;&#25918;&#35789;&#27719;&#23545;&#35937;&#26816;&#27979;&#30340;&#31616;&#21333;&#25193;&#23637;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of language models have moved from encoder-decoder to decoder-only designs. In addition, the common knowledge has it that the two most popular multimodal tasks, the generative and contrastive tasks, tend to conflict with one another, are hard to accommodate in one architecture, and further need complex adaptations for downstream tasks. We propose a novel paradigm of training with a decoder-only model for multimodal tasks, which is surprisingly effective in jointly learning of these disparate vision-language tasks. This is done with a simple model, called MaMMUT. It consists of a single vision encoder and a text decoder, and is able to accommodate contrastive and generative learning by a novel two-pass approach on the text decoder. We demonstrate that joint training of these diverse-objective tasks is simple, effective, and maximizes the weight-sharing of the model. Furthermore, the same architecture enables straightforward extensions to open-vocabulary object detection 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#24046;&#20998;&#38544;&#31169;&#21644;&#24230;&#37327;&#38544;&#31169;&#23398;&#20064;&#22120;&#22312;&#23545;&#25239;&#32773;&#37325;&#26500;&#38169;&#35823;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#65292;&#24471;&#20986;&#20102;&#38750;&#28176;&#36827;&#24615;&#19979;&#30028;&#65292;&#35206;&#30422;&#20102;&#39640;&#32500;&#24773;&#20917;&#65292;&#19988;&#25193;&#23637;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#38544;&#31169;&#20998;&#26512;</title><link>http://arxiv.org/abs/2303.16372</link><description>&lt;p&gt;
&#35757;&#32451;&#25968;&#25454;&#37325;&#26500;&#30340;&#38750;&#28176;&#36827;&#24615;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
Non-Asymptotic Lower Bounds For Training Data Reconstruction. (arXiv:2303.16372v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#24046;&#20998;&#38544;&#31169;&#21644;&#24230;&#37327;&#38544;&#31169;&#23398;&#20064;&#22120;&#22312;&#23545;&#25239;&#32773;&#37325;&#26500;&#38169;&#35823;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#65292;&#24471;&#20986;&#20102;&#38750;&#28176;&#36827;&#24615;&#19979;&#30028;&#65292;&#35206;&#30422;&#20102;&#39640;&#32500;&#24773;&#20917;&#65292;&#19988;&#25193;&#23637;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#38544;&#31169;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19987;&#19994;&#23545;&#25163;&#36827;&#34892;&#35757;&#32451;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#26102;&#31169;&#26377;&#23398;&#20064;&#31639;&#27861;&#30340;&#35821;&#20041;&#20445;&#35777;&#24378;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#23548;&#20986;&#38750;&#28176;&#36827;&#37327;&#32423;&#19979;&#30028;&#26469;&#30740;&#31350;&#20102;&#28385;&#36275;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#21644;&#24230;&#37327;&#38544;&#31169;&#65288;mDP&#65289;&#30340;&#23398;&#20064;&#22120;&#23545;&#25239;&#32773;&#37325;&#26500;&#38169;&#35823;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#23545;mDP&#30340;&#20998;&#26512;&#35206;&#30422;&#20102;&#39640;&#32500;&#24773;&#20917;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#23545;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#22914;DP-SGD&#21644;Projected Noisy SGD&#36827;&#34892;&#20102;&#24230;&#37327;&#24046;&#20998;&#38544;&#31169;&#30340;&#25193;&#23637;&#38544;&#31169;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate semantic guarantees of private learning algorithms for their resilience to training Data Reconstruction Attacks (DRAs) by informed adversaries. To this end, we derive non-asymptotic minimax lower bounds on the adversary's reconstruction error against learners that satisfy differential privacy (DP) and metric differential privacy (mDP). Furthermore, we demonstrate that our lower bound analysis for the latter also covers the high dimensional regime, wherein, the input data dimensionality may be larger than the adversary's query budget. Motivated by the theoretical improvements conferred by metric DP, we extend the privacy analysis of popular deep learning algorithms such as DP-SGD and Projected Noisy SGD to cover the broader notion of metric differential privacy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#23637;&#20102;PCA-Net&#30340;&#36817;&#20284;&#29702;&#35770;&#65292;&#24471;&#20986;&#20102;&#36890;&#29992;&#36924;&#36817;&#32467;&#26524;&#65292;&#24182;&#35782;&#21035;&#20986;&#20102;&#20351;&#29992;PCA-Net&#36827;&#34892;&#39640;&#25928;&#25805;&#20316;&#23398;&#20064;&#30340;&#28508;&#22312;&#38556;&#30861;&#65306;&#36755;&#20986;&#20998;&#24067;&#30340;&#22797;&#26434;&#24615;&#21644;&#31639;&#23376;&#31354;&#38388;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16317</link><description>&lt;p&gt;
PCA-Net&#65306;&#25805;&#20316;&#23398;&#20064;&#30340;&#22797;&#26434;&#24615;&#19978;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
Operator learning with PCA-Net: upper and lower complexity bounds. (arXiv:2303.16317v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#23637;&#20102;PCA-Net&#30340;&#36817;&#20284;&#29702;&#35770;&#65292;&#24471;&#20986;&#20102;&#36890;&#29992;&#36924;&#36817;&#32467;&#26524;&#65292;&#24182;&#35782;&#21035;&#20986;&#20102;&#20351;&#29992;PCA-Net&#36827;&#34892;&#39640;&#25928;&#25805;&#20316;&#23398;&#20064;&#30340;&#28508;&#22312;&#38556;&#30861;&#65306;&#36755;&#20986;&#20998;&#24067;&#30340;&#22797;&#26434;&#24615;&#21644;&#31639;&#23376;&#31354;&#38388;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31639;&#23376;&#22312;&#35745;&#31639;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#22791;&#21463;&#20851;&#27880;&#12290;PCA-Net&#26159;&#19968;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#31070;&#32463;&#31639;&#23376;&#26550;&#26500;&#65292;&#23427;&#23558;&#20027;&#25104;&#20998;&#20998;&#26512;(PCA)&#19982;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#20197;&#36924;&#36817;&#28508;&#22312;&#30340;&#31639;&#23376;&#12290;&#26412;&#25991;&#23545;&#36825;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#36817;&#20284;&#29702;&#35770;&#30340;&#21457;&#23637;&#65292;&#25913;&#36827;&#24182;&#26174;&#30528;&#25193;&#23637;&#20102;&#27492;&#26041;&#21521;&#30340;&#20197;&#21069;&#30340;&#24037;&#20316;&#12290;&#22312;&#23450;&#24615;&#30028;&#38480;&#26041;&#38754;&#65292;&#26412;&#25991;&#24471;&#20986;&#20102;&#26032;&#39062;&#30340;&#36890;&#29992;&#36924;&#36817;&#32467;&#26524;&#65292;&#22312;&#23545;&#28508;&#22312;&#31639;&#23376;&#21644;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#30340;&#26368;&#23567;&#20551;&#35774;&#30340;&#21069;&#25552;&#19979;&#12290;&#22312;&#23450;&#37327;&#38480;&#21046;&#26041;&#38754;&#65292;&#26412;&#25991;&#35782;&#21035;&#20102;&#20351;&#29992;PCA-Net&#36827;&#34892;&#39640;&#25928;&#25805;&#20316;&#23398;&#20064;&#30340;&#20004;&#20010;&#28508;&#22312;&#38556;&#30861;&#65292;&#36890;&#36807;&#23548;&#20986;&#19979;&#30028;&#36827;&#34892;&#20102;&#20005;&#26684;&#35777;&#26126;&#65292;&#31532;&#19968;&#20010;&#38556;&#30861;&#19982;&#36755;&#20986;&#20998;&#24067;&#30340;&#22797;&#26434;&#24615;&#26377;&#20851;&#65292;&#30001;PCA&#29305;&#24449;&#20540;&#30340;&#32531;&#24930;&#34928;&#20943;&#26469;&#34913;&#37327;&#65307;&#21478;&#19968;&#20010;&#38556;&#30861;&#28041;&#21450;&#26080;&#38480;&#32500;&#36755;&#20837;&#21644;&#36755;&#20986;&#31354;&#38388;&#20043;&#38388;&#30340;&#31639;&#23376;&#31354;&#38388;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural operators are gaining attention in computational science and engineering. PCA-Net is a recently proposed neural operator architecture which combines principal component analysis (PCA) with neural networks to approximate an underlying operator. The present work develops approximation theory for this approach, improving and significantly extending previous work in this direction. In terms of qualitative bounds, this paper derives a novel universal approximation result, under minimal assumptions on the underlying operator and the data-generating distribution. In terms of quantitative bounds, two potential obstacles to efficient operator learning with PCA-Net are identified, and made rigorous through the derivation of lower complexity bounds; the first relates to the complexity of the output distribution, measured by a slow decay of the PCA eigenvalues. The other obstacle relates the inherent complexity of the space of operators between infinite-dimensional input and output spaces, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#21483;one-shot VFL&#30340;&#23454;&#29992;VFL&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#35299;&#20915;&#36890;&#20449;&#29942;&#39048;&#21644;&#26377;&#38480;&#37325;&#21472;&#26679;&#26412;&#30340;&#38382;&#39064;&#12290;few-shot VFL&#21017;&#21487;&#20197;&#22312;&#36827;&#34892;&#19968;&#27425;&#25110;&#20165;&#23569;&#37327;&#36890;&#20449;&#30340;&#24773;&#20917;&#19979;&#36827;&#19968;&#27493;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16270</link><description>&lt;p&gt;
&#22312;&#26377;&#38480;&#37325;&#21472;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#36890;&#20449;&#39640;&#25928;&#30340;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Communication-Efficient Vertical Federated Learning with Limited Overlapping Samples. (arXiv:2303.16270v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16270
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#21483;one-shot VFL&#30340;&#23454;&#29992;VFL&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#35299;&#20915;&#36890;&#20449;&#29942;&#39048;&#21644;&#26377;&#38480;&#37325;&#21472;&#26679;&#26412;&#30340;&#38382;&#39064;&#12290;few-shot VFL&#21017;&#21487;&#20197;&#22312;&#36827;&#34892;&#19968;&#27425;&#25110;&#20165;&#23569;&#37327;&#36890;&#20449;&#30340;&#24773;&#20917;&#19979;&#36827;&#19968;&#27493;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;(VFL)&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#21327;&#20316;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#23458;&#25143;&#31471;&#33021;&#22815;&#22312;&#19981;&#20849;&#20139;&#26412;&#22320;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#12290;VFL&#22788;&#29702;&#30340;&#26159;&#23458;&#25143;&#31471;&#25968;&#25454;&#20855;&#26377;&#19981;&#21516;&#30340;&#29305;&#24449;&#31354;&#38388;&#20294;&#20849;&#20139;&#19968;&#20123;&#37325;&#21472;&#26679;&#26412;&#30340;&#24773;&#20917;&#12290;&#29616;&#26377;&#30340;VFL&#26041;&#27861;&#23384;&#22312;&#36890;&#20449;&#25104;&#26412;&#39640;&#21644;&#26080;&#27861;&#26377;&#25928;&#22788;&#29702;&#29616;&#23454;&#19990;&#30028;&#20013;&#24120;&#35265;&#30340;&#26377;&#38480;&#37325;&#21472;&#26679;&#26412;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#21483; \textbf{one-shot VFL} &#30340;&#23454;&#29992;VFL&#26694;&#26550;&#65292;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#21487;&#20197;&#21516;&#26102;&#35299;&#20915;&#36890;&#20449;&#29942;&#39048;&#21644;&#26377;&#38480;&#37325;&#21472;&#26679;&#26412;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102; \textbf{few-shot VFL}&#65292;&#22312;&#21482;&#36827;&#34892;&#19968;&#27425;&#25110;&#20165;&#23569;&#37327;&#36890;&#20449;&#30340;&#24773;&#20917;&#19979;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#20013;&#65292;&#23458;&#25143;&#31471;&#21482;&#38656;&#35201;&#19982;&#26381;&#21153;&#22120;&#36827;&#34892;&#19968;&#27425;&#25110;&#20165;&#23569;&#37327;&#36890;&#20449;&#12290;&#25105;&#20204;&#22312;&#22270;&#20687;&#21644;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;VFL&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is a popular collaborative learning approach that enables clients to train a global model without sharing their local data. Vertical federated learning (VFL) deals with scenarios in which the data on clients have different feature spaces but share some overlapping samples. Existing VFL approaches suffer from high communication costs and cannot deal efficiently with limited overlapping samples commonly seen in the real world. We propose a practical vertical federated learning (VFL) framework called \textbf{one-shot VFL} that can solve the communication bottleneck and the problem of limited overlapping samples simultaneously based on semi-supervised learning. We also propose \textbf{few-shot VFL} to improve the accuracy further with just one more communication round between the server and the clients. In our proposed framework, the clients only need to communicate with the server once or only a few times. We evaluate the proposed VFL framework on both image and tabular
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#20219;&#20309;&#22312;&#22343;&#21248;&#20998;&#24067;&#19979;&#26377;&#25928;&#30340;PAC&#23398;&#20064;&#31639;&#27861;&#36716;&#25442;&#25104;&#19968;&#20010;&#22312;&#20219;&#24847;&#26410;&#30693;&#20998;&#24067;&#19979;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#32780;&#19988;&#23545;&#20110;&#21333;&#35843;&#20998;&#24067;&#65292;&#21482;&#38656;&#35201;&#29992;$\mathcal{D}$&#20013;&#30340;&#26679;&#26412;&#12290;&#31639;&#27861;&#30340;&#26680;&#24515;&#26159;&#36890;&#36807;&#19968;&#20010;&#31639;&#27861;&#23558;$\mathcal{D}$&#36924;&#36817;&#25104;&#30001;&#23376;&#31435;&#26041;&#20307;&#28151;&#21512;&#32780;&#25104;&#30340;&#28151;&#21512;&#22343;&#21248;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2303.16208</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#24067;&#20998;&#35299;&#25552;&#39640;&#22343;&#21248;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Lifting uniform learners via distributional decomposition. (arXiv:2303.16208v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#20219;&#20309;&#22312;&#22343;&#21248;&#20998;&#24067;&#19979;&#26377;&#25928;&#30340;PAC&#23398;&#20064;&#31639;&#27861;&#36716;&#25442;&#25104;&#19968;&#20010;&#22312;&#20219;&#24847;&#26410;&#30693;&#20998;&#24067;&#19979;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#32780;&#19988;&#23545;&#20110;&#21333;&#35843;&#20998;&#24067;&#65292;&#21482;&#38656;&#35201;&#29992;$\mathcal{D}$&#20013;&#30340;&#26679;&#26412;&#12290;&#31639;&#27861;&#30340;&#26680;&#24515;&#26159;&#36890;&#36807;&#19968;&#20010;&#31639;&#27861;&#23558;$\mathcal{D}$&#36924;&#36817;&#25104;&#30001;&#23376;&#31435;&#26041;&#20307;&#28151;&#21512;&#32780;&#25104;&#30340;&#28151;&#21512;&#22343;&#21248;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20219;&#20309;&#22312;&#22343;&#21248;&#20998;&#24067;&#19979;&#26377;&#25928;&#30340;PAC&#23398;&#20064;&#31639;&#27861;&#36716;&#25442;&#25104;&#19968;&#20010;&#22312;&#20219;&#24847;&#26410;&#30693;&#20998;&#24067;$\mathcal{D}$&#19979;&#26377;&#25928;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#36716;&#25442;&#25928;&#29575;&#38543;$\mathcal{D}$&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#32780;&#21464;&#21270;&#65292;&#23545;&#20110;&#22312;$\{\pm 1\}^n$&#19978;&#30340;&#20998;&#24067;&#65292;&#20854;pmf&#30001;&#28145;&#24230;&#20026;$d$&#30340;&#20915;&#31574;&#26641;&#35745;&#31639;&#65292;&#21017;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;$\mathrm{poly}(n, (md)^d)$&#65292;&#20854;&#20013;$m$&#26159;&#21407;&#22987;&#31639;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#23545;&#20110;&#21333;&#35843;&#20998;&#24067;&#65292;&#25105;&#20204;&#30340;&#36716;&#25442;&#20165;&#20351;&#29992;$\mathcal{D}$&#20013;&#30340;&#26679;&#26412;&#65292;&#32780;&#23545;&#20110;&#19968;&#33324;&#20998;&#24067;&#65292;&#25105;&#20204;&#20351;&#29992;&#23376;&#31435;&#26041;&#20307;&#26465;&#20214;&#26679;&#26412;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#25216;&#26415;&#26159;&#19968;&#20010;&#31639;&#27861;&#65292;&#23427;&#22312;&#32473;&#20986;$\mathcal{D}$&#30340;&#35775;&#38382;&#26435;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20135;&#29983;&#20102;&#19968;&#20010;&#26368;&#20248;&#20915;&#31574;&#26641;&#20998;&#35299;$\mathcal{D}$&#65306;&#19968;&#20010;&#36924;&#36817;&#20102;$\mathcal{D}$&#30340;&#28151;&#21512;&#22343;&#21248;&#20998;&#24067;&#30340;&#20998;&#31163;&#23376;&#31435;&#26041;&#20307;&#12290;&#36890;&#36807;&#36825;&#20010;&#20998;&#35299;&#65292;&#25105;&#20204;&#22312;&#27599;&#20010;&#23376;&#31435;&#26041;&#20307;&#19978;&#36816;&#34892;&#22343;&#21248;&#20998;&#24067;&#23398;&#20064;&#22120;&#65292;&#24182;&#23558;&#32467;&#26524;&#21512;&#24182;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show how any PAC learning algorithm that works under the uniform distribution can be transformed, in a blackbox fashion, into one that works under an arbitrary and unknown distribution $\mathcal{D}$. The efficiency of our transformation scales with the inherent complexity of $\mathcal{D}$, running in $\mathrm{poly}(n, (md)^d)$ time for distributions over $\{\pm 1\}^n$ whose pmfs are computed by depth-$d$ decision trees, where $m$ is the sample complexity of the original algorithm. For monotone distributions our transformation uses only samples from $\mathcal{D}$, and for general ones it uses subcube conditioning samples.  A key technical ingredient is an algorithm which, given the aforementioned access to $\mathcal{D}$, produces an optimal decision tree decomposition of $\mathcal{D}$: an approximation of $\mathcal{D}$ as a mixture of uniform distributions over disjoint subcubes. With this decomposition in hand, we run the uniform-distribution learner on each subcube and combine the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21322;&#30417;&#30563;&#22238;&#24402;&#27169;&#22411;&#65292;&#21487;&#30452;&#25509;&#20174;&#22270;&#29255;&#20013;&#35745;&#31639;&#31616;&#32504;&#32433;&#32447;&#30340;&#32447;&#23494;&#24230;&#65292;&#20197;&#24212;&#29992;&#20110;&#21476;&#32769;&#27833;&#30011;&#65292;&#20026;&#33402;&#26415;&#21697;&#20445;&#25252;&#21644;&#20462;&#22797;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#32447;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.15999</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#22238;&#24402;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#31616;&#32504;&#32433;&#32447;&#25968;&#30446;&#35745;&#31639;&#26041;&#27861;&#20197;&#24212;&#29992;&#20110;&#21476;&#32769;&#27833;&#30011;
&lt;/p&gt;
&lt;p&gt;
Thread Counting in Plain Weave for Old Paintings Using Semi-Supervised Regression Deep Learning Models. (arXiv:2303.15999v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21322;&#30417;&#30563;&#22238;&#24402;&#27169;&#22411;&#65292;&#21487;&#30452;&#25509;&#20174;&#22270;&#29255;&#20013;&#35745;&#31639;&#31616;&#32504;&#32433;&#32447;&#30340;&#32447;&#23494;&#24230;&#65292;&#20197;&#24212;&#29992;&#20110;&#21476;&#32769;&#27833;&#30011;&#65292;&#20026;&#33402;&#26415;&#21697;&#20445;&#25252;&#21644;&#20462;&#22797;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#32447;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20316;&#32773;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22238;&#24402;&#26041;&#27861;&#65292;&#20197;&#25191;&#34892;&#23545;&#31616;&#32504;&#32433;&#32447;&#23494;&#24230;&#30340;&#20272;&#35745;&#65292;&#20197;&#36827;&#34892;&#30011;&#24067;&#20998;&#26512;&#12290;&#35813;&#26032;&#26041;&#27861;&#21487;&#20197;&#30452;&#25509;&#20174;&#22270;&#20687;&#20013;&#35745;&#31639;&#32447;&#23494;&#24230;&#65292;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#20102;&#19987;&#23478;&#30693;&#35782;&#12290;&#35813;&#26041;&#27861;&#22312;&#33402;&#26415;&#20445;&#23384;&#21644;&#20462;&#22797;&#39046;&#22495;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work the authors develop regression approaches based on deep learning to perform thread density estimation for plain weave canvas analysis. Previous approaches were based on Fourier analysis, that are quite robust for some scenarios but fail in some other, in machine learning tools, that involve pre-labeling of the painting at hand, or the segmentation of thread crossing points, that provides good estimations in all scenarios with no need of pre-labeling. The segmentation approach is time-consuming as estimation of the densities is performed after locating the crossing points. In this novel proposal, we avoid this step by computing the density of threads directly from the image with a regression deep learning model. We also incorporate some improvements in the initial preprocessing of the input image with an impact on the final error. Several models are proposed and analyzed to retain the best one. Furthermore, we further reduce the density estimation error by introducing a sem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#30340;&#26080;&#32447;&#36793;&#32536;&#32593;&#32476;&#30340;&#39640;&#25928;&#24182;&#34892;&#20998;&#35010;&#23398;&#20064;&#65288;EPSL&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#21152;&#36895;&#27169;&#22411;&#35757;&#32451;&#12290;EPSL&#24182;&#34892;&#21270;&#23458;&#25143;&#31471;&#27169;&#22411;&#35757;&#32451;&#65292;&#36890;&#36807;&#32858;&#21512;&#26799;&#24230;&#38477;&#20302;&#20102;&#21453;&#21521;&#20256;&#25773;&#30340;&#23616;&#37096;&#26799;&#24230;&#32500;&#24230;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#26381;&#21153;&#22120;&#31471;&#30340;&#35757;&#32451;&#21644;&#36890;&#20449;&#24310;&#36831;&#12290;&#21516;&#26102;&#65292;EPSL&#36824;&#35774;&#35745;&#20102;&#36164;&#28304;&#20998;&#37197;&#31639;&#27861;&#20197;&#20248;&#21270;&#35745;&#31639;&#21644;&#36890;&#20449;&#36164;&#28304;&#20998;&#37197;&#12290;</title><link>http://arxiv.org/abs/2303.15991</link><description>&lt;p&gt;
&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#30340;&#26080;&#32447;&#36793;&#32536;&#32593;&#32476;&#30340;&#39640;&#25928;&#24182;&#34892;&#20998;&#35010;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Parallel Split Learning over Resource-constrained Wireless Edge Networks. (arXiv:2303.15991v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#30340;&#26080;&#32447;&#36793;&#32536;&#32593;&#32476;&#30340;&#39640;&#25928;&#24182;&#34892;&#20998;&#35010;&#23398;&#20064;&#65288;EPSL&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#21152;&#36895;&#27169;&#22411;&#35757;&#32451;&#12290;EPSL&#24182;&#34892;&#21270;&#23458;&#25143;&#31471;&#27169;&#22411;&#35757;&#32451;&#65292;&#36890;&#36807;&#32858;&#21512;&#26799;&#24230;&#38477;&#20302;&#20102;&#21453;&#21521;&#20256;&#25773;&#30340;&#23616;&#37096;&#26799;&#24230;&#32500;&#24230;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#26381;&#21153;&#22120;&#31471;&#30340;&#35757;&#32451;&#21644;&#36890;&#20449;&#24310;&#36831;&#12290;&#21516;&#26102;&#65292;EPSL&#36824;&#35774;&#35745;&#20102;&#36164;&#28304;&#20998;&#37197;&#31639;&#27861;&#20197;&#20248;&#21270;&#35745;&#31639;&#21644;&#36890;&#20449;&#36164;&#28304;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31070;&#32463;&#32593;&#32476;&#36234;&#26469;&#36234;&#28145;&#65292;&#36825;&#38459;&#30861;&#20102;&#32852;&#21512;&#23398;&#20064;&#31561;&#38544;&#31169;&#22686;&#24378;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#24335;&#65288;&#22914;&#32852;&#37030;&#23398;&#20064;&#65289;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#19978;&#30340;&#27665;&#20027;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#20513;&#23548;&#23558;&#36793;&#32536;&#35745;&#31639;&#33539;&#24335;&#21644;&#24182;&#34892;&#20998;&#35010;&#23398;&#20064;&#65288;PSL&#65289;&#30456;&#32467;&#21512;&#65292;&#20801;&#35768;&#22810;&#20010;&#23458;&#25143;&#31471;&#35774;&#22791;&#36890;&#36807;&#36880;&#23618;&#27169;&#22411;&#20998;&#35010;&#23558;&#22823;&#37327;&#30340;&#35757;&#32451;&#24037;&#20316;&#36127;&#36733;&#21368;&#36733;&#21040;&#36793;&#32536;&#26381;&#21153;&#22120;&#19978;&#12290;&#36890;&#36807;&#35266;&#23519;&#21040;&#29616;&#26377;&#30340;PSL&#26041;&#26696;&#20250;&#20135;&#29983;&#36807;&#22810;&#30340;&#35757;&#32451;&#24310;&#36831;&#21644;&#22823;&#37327;&#30340;&#25968;&#25454;&#20256;&#36755;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;PSL&#26694;&#26550;&#8212;&#8212;&#39640;&#25928;&#24182;&#34892;&#20998;&#35010;&#23398;&#20064;&#65288;EPSL&#65289;&#65292;&#20197;&#21152;&#36895;&#27169;&#22411;&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;EPSL&#23558;&#23458;&#25143;&#31471;&#27169;&#22411;&#35757;&#32451;&#24182;&#34892;&#21270;&#65292;&#24182;&#36890;&#36807;&#26368;&#21518;&#19968;&#23618;&#26799;&#24230;&#32858;&#21512;&#38477;&#20302;&#20102;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#30340;&#23616;&#37096;&#26799;&#24230;&#32500;&#24230;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#26381;&#21153;&#22120;&#31471;&#30340;&#35757;&#32451;&#21644;&#36890;&#20449;&#24310;&#36831;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#32771;&#34385;&#36793;&#32536;&#35774;&#22791;&#30340;&#24322;&#26500;&#36890;&#36947;&#26465;&#20214;&#21644;&#35745;&#31639;&#33021;&#21147;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#36164;&#28304;&#20998;&#37197;&#31639;&#27861;&#20197;&#20248;&#21270;&#35745;&#31639;&#21644;&#36890;&#20449;&#36164;&#28304;&#20998;&#37197;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EPSL&#36890;&#36807;&#23558;&#36890;&#20449;&#25104;&#26412;&#21644;&#35757;&#32451;&#26102;&#38388;&#20998;&#21035;&#38477;&#20302;76&#65285;&#21644;63&#65285;&#65292;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;PSL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasingly deeper neural networks hinder the democratization of privacy-enhancing distributed learning, such as federated learning (FL), to resource-constrained devices. To overcome this challenge, in this paper, we advocate the integration of edge computing paradigm and parallel split learning (PSL), allowing multiple client devices to offload substantial training workloads to an edge server via layer-wise model split. By observing that existing PSL schemes incur excessive training latency and large volume of data transmissions, we propose an innovative PSL framework, namely, efficient parallel split learning (EPSL), to accelerate model training. To be specific, EPSL parallelizes client-side model training and reduces the dimension of local gradients for back propagation (BP) via last-layer gradient aggregation, leading to a significant reduction in server-side training and communication latency. Moreover, by considering the heterogeneous channel conditions and computing capabil
&lt;/p&gt;</description></item><item><title>TraffNet&#26159;&#19968;&#20010;&#23398;&#20064;&#20132;&#36890;&#37327;&#29983;&#25104;&#21407;&#22240;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#34920;&#31034;&#20026;&#24322;&#26500;&#22270;&#65292;&#21033;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#23454;&#29616;&#20102;&#23545;&#20132;&#36890;&#29983;&#25104;&#21407;&#22240;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2303.15954</link><description>&lt;p&gt;
TraffNet&#65306;&#23398;&#20064;&#36947;&#36335;&#32593;&#32476;&#25968;&#23383;&#23402;&#29983;&#20132;&#36890;&#29983;&#25104;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
TraffNet: Learning Causality of Traffic Generation for Road Network Digital Twins. (arXiv:2303.15954v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15954
&lt;/p&gt;
&lt;p&gt;
TraffNet&#26159;&#19968;&#20010;&#23398;&#20064;&#20132;&#36890;&#37327;&#29983;&#25104;&#21407;&#22240;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#34920;&#31034;&#20026;&#24322;&#26500;&#22270;&#65292;&#21033;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#23454;&#29616;&#20102;&#23545;&#20132;&#36890;&#29983;&#25104;&#21407;&#22240;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36947;&#36335;&#32593;&#32476;&#25968;&#23383;&#23402;&#29983;&#65288;RNDT&#65289;&#22312;&#24320;&#21457;&#19979;&#19968;&#20195;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#20132;&#36890;&#35268;&#21010;&#21644;&#25511;&#21046;&#12290;&#20026;&#20102;&#25903;&#25345;&#23454;&#26102;&#20915;&#31574;&#65292;RNDT&#38656;&#35201;&#19968;&#20010;&#27169;&#22411;&#65292;&#20174;&#22312;&#32447;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#21160;&#24577;&#23398;&#20064;&#20132;&#36890;&#27169;&#24335;&#24182;&#29983;&#25104;&#39640;&#20445;&#30495;&#27169;&#25311;&#32467;&#26524;&#12290;&#23613;&#31649;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24403;&#21069;&#20132;&#36890;&#39044;&#27979;&#25216;&#26415;&#24050;&#32463;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#36825;&#20123;&#25216;&#26415;&#20165;&#36890;&#36807;&#25366;&#25496;&#21382;&#21490;&#20132;&#36890;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#24615;&#26469;&#39044;&#27979;&#26410;&#26469;&#20132;&#36890;&#65292;&#32780;&#24573;&#30053;&#20102;&#20132;&#36890;&#29983;&#25104;&#30340;&#21407;&#22240;&#65292;&#20363;&#22914;&#20132;&#36890;&#38656;&#27714;&#21644;&#36335;&#24452;&#36873;&#25321;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#23545;&#20110;&#23454;&#26102;&#20915;&#31574;&#26159;&#19981;&#21487;&#38752;&#30340;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#31216;&#20026; TraffNet&#65292;&#35813;&#26694;&#26550;&#20174;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#20013;&#23398;&#20064;&#20132;&#36890;&#37327;&#30340;&#22240;&#26524;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#24322;&#26500;&#22270;&#26469;&#34920;&#31034;&#36947;&#36335;&#32593;&#32476;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#24182;&#20837;&#39044;&#27979;&#25152;&#38656;&#30340;&#20854;&#20182;&#25968;&#25454;&#65292;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#20174;&#32780;&#33021;&#22815;&#39044;&#27979;&#20132;&#36890;&#37327;&#30340;&#22240;&#26524;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Road network digital twins (RNDTs) play a critical role in the development of next-generation intelligent transportation systems, enabling more precise traffic planning and control. To support just-in-time (JIT) decision making, RNDTs require a model that dynamically learns the traffic patterns from online sensor data and generates high-fidelity simulation results. Although current traffic prediction techniques based on graph neural networks have achieved state-of-the-art performance, these techniques only predict future traffic by mining correlations in historical traffic data, disregarding the causes of traffic generation, such as traffic demands and route selection. Therefore, their performance is unreliable for JIT decision making. To fill this gap, we introduce a novel deep learning framework called TraffNet that learns the causality of traffic volume from vehicle trajectory data. First, we use a heterogeneous graph to represent the road network, allowing the model to incorporate 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#31639;&#27861;&#65292;&#37319;&#29992;&#20102;&#37096;&#20998;&#26679;&#26412;&#30340;&#20132;&#21449;&#35270;&#22270;&#32858;&#21512;&#26426;&#21046;&#21644;&#20004;&#20010;&#21407;&#22411;&#23545;&#40784;&#31574;&#30053;&#65292;&#20197;&#35299;&#20915;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.15689</link><description>&lt;p&gt;
&#24102;&#26377;&#20132;&#21449;&#35270;&#22270;&#37096;&#20998;&#26679;&#26412;&#21644;&#21407;&#22411;&#23545;&#40784;&#30340;&#28145;&#24230;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Deep Incomplete Multi-view Clustering with Cross-view Partial Sample and Prototype Alignment. (arXiv:2303.15689v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15689
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#31639;&#27861;&#65292;&#37319;&#29992;&#20102;&#37096;&#20998;&#26679;&#26412;&#30340;&#20132;&#21449;&#35270;&#22270;&#32858;&#21512;&#26426;&#21046;&#21644;&#20004;&#20010;&#21407;&#22411;&#23545;&#40784;&#31574;&#30053;&#65292;&#20197;&#35299;&#20915;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22810;&#35270;&#22270;&#32858;&#31867;&#26041;&#27861;&#25104;&#21151;&#30340;&#21069;&#25552;&#26159;&#26679;&#26412;&#22312;&#22810;&#35270;&#22270;&#20013;&#20445;&#25345;&#23436;&#25972;&#24615;&#30340;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#22810;&#35270;&#22270;&#26679;&#26412;&#30001;&#20110;&#25968;&#25454;&#25439;&#22351;&#25110;&#20256;&#24863;&#22120;&#25925;&#38556;&#32780;&#37096;&#20998;&#22320;&#21487;&#29992;&#65292;&#23548;&#33268;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#30740;&#31350;&#38519;&#20837;&#22256;&#22659;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#19968;&#20123;&#23581;&#35797;&#26469;&#35299;&#20915;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#38382;&#39064;&#65292;&#20294;&#23384;&#22312;&#20197;&#19979;&#32570;&#28857;&#65306;&#65288;i&#65289;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#37319;&#29992;&#20132;&#21449;&#35270;&#22270;&#23545;&#27604;&#23398;&#20064;&#65292;&#24378;&#21046;&#27599;&#20010;&#26679;&#26412;&#22312;&#22810;&#20010;&#35270;&#22270;&#20013;&#30340;&#34920;&#31034;&#26159;&#23436;&#20840;&#30456;&#21516;&#30340;&#65292;&#36825;&#21487;&#33021;&#20250;&#24573;&#30053;&#35270;&#22270;&#24046;&#24322;&#21644;&#34920;&#31034;&#30340;&#28789;&#27963;&#24615;&#65307;&#65288;ii&#65289;&#30001;&#20110;&#22312;&#22810;&#20010;&#35270;&#22270;&#20013;&#19981;&#23384;&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#26679;&#26412;&#65292;&#25152;&#24471;&#21040;&#30340;&#32858;&#31867;&#21407;&#22411;&#21487;&#33021;&#19981;&#23545;&#40784;&#21644;&#20559;&#26012;&#65292;&#23548;&#33268;&#32858;&#21512;&#19981;&#27491;&#30830;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35270;&#22270;&#37096;&#20998;&#26679;&#26412;&#21644;&#21407;&#22411;&#23545;&#40784;&#32593;&#32476;(CPSPAN)&#65292;&#29992;&#20110;&#28145;&#24230;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#12290;&#39318;&#20808;&#65292;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#23545;&#27604;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#37319;&#29992;&#37096;&#20998;&#26679;&#26412;&#30340;&#20132;&#21449;&#35270;&#22270;&#32858;&#21512;&#26426;&#21046;&#26469;&#25429;&#25417;&#35270;&#22270;&#20043;&#38388;&#30340;&#24046;&#24322;&#29305;&#24449;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#37319;&#29992;&#20004;&#20010;&#21407;&#22411;&#23545;&#40784;&#31574;&#30053;&#26469;&#35299;&#20915;&#21407;&#22411;&#19981;&#23545;&#40784;&#21644;&#32858;&#21512;&#19981;&#27491;&#30830;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of existing multi-view clustering relies on the assumption of sample integrity across multiple views. However, in real-world scenarios, samples of multi-view are partially available due to data corruption or sensor failure, which leads to incomplete multi-view clustering study (IMVC). Although several attempts have been proposed to address IMVC, they suffer from the following drawbacks: i) Existing methods mainly adopt cross-view contrastive learning forcing the representations of each sample across views to be exactly the same, which might ignore view discrepancy and flexibility in representations; ii) Due to the absence of non-observed samples across multiple views, the obtained prototypes of clusters might be unaligned and biased, leading to incorrect fusion. To address the above issues, we propose a Cross-view Partial Sample and Prototype Alignment Network (CPSPAN) for Deep Incomplete Multi-view Clustering. Firstly, unlike existing contrastive-based methods, we adopt pa
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#20102;&#20845;&#39033;&#25233;&#37057;&#30151;&#33647;&#29289;&#27835;&#30103;&#30740;&#31350;&#30340;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#20102;&#32467;&#26524;&#23548;&#21521;&#30340;&#24739;&#32773;&#20122;&#32452;&#65292;&#20026;&#24739;&#32773;&#30340;&#20010;&#24615;&#21270;&#27835;&#30103;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2303.15202</link><description>&lt;p&gt;
&#26088;&#22312;&#23454;&#29616;&#32467;&#26524;&#23548;&#21521;&#30340;&#24739;&#32773;&#20122;&#32452;&#65306;&#20845;&#39033;&#25233;&#37057;&#30151;&#27835;&#30103;&#30740;&#31350;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Towards Outcome-Driven Patient Subgroups: A Machine Learning Analysis Across Six Depression Treatment Studies. (arXiv:2303.15202v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15202
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#20102;&#20845;&#39033;&#25233;&#37057;&#30151;&#33647;&#29289;&#27835;&#30103;&#30740;&#31350;&#30340;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#20102;&#32467;&#26524;&#23548;&#21521;&#30340;&#24739;&#32773;&#20122;&#32452;&#65292;&#20026;&#24739;&#32773;&#30340;&#20010;&#24615;&#21270;&#27835;&#30103;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#24230;&#25233;&#37057;&#38556;&#30861;(MDD) &#26159;&#19968;&#31181;&#22810;&#26679;&#24615;&#30142;&#30149;&#65292;&#22823;&#37327;&#30340;&#31070;&#32463;&#29983;&#29289;&#23398;&#22522;&#30784;&#21487;&#33021;&#19982;&#27835;&#30103;&#21453;&#24212;&#30340;&#21464;&#24322;&#24615;&#26377;&#20851;&#12290;&#29702;&#35299;&#36825;&#31181;&#21464;&#24322;&#24615;&#30340;&#26681;&#28304;&#24182;&#39044;&#27979;&#32467;&#26524;&#19968;&#30452;&#26159;&#38590;&#20197;&#23454;&#29616;&#30340;&#12290;&#26426;&#22120;&#23398;&#20064;&#24050;&#32463;&#22312;&#39044;&#27979;MDD&#30340;&#27835;&#30103;&#21453;&#24212;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#20854;&#20013;&#19968;&#39033;&#38480;&#21046;&#26159;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20020;&#24202;&#21487;&#35299;&#37322;&#24615;&#19981;&#36275;&#12290;&#25105;&#20204;&#20351;&#29992;&#24046;&#24322;&#21407;&#22411;&#31070;&#32463;&#32593;&#32476;(DPNN)&#20998;&#26512;&#20102;&#20845;&#20010;&#33647;&#29289;&#27835;&#30103;&#25233;&#37057;&#30151;&#20020;&#24202;&#35797;&#39564;&#30340;&#25968;&#25454;(&#24635;&#25968;n = 5438)&#12290;DPNN&#21487;&#20197;&#27966;&#29983;&#20986;&#21487;&#29992;&#20110;&#29983;&#25104;&#24739;&#32773;&#20122;&#32452;&#30340;&#30149;&#20154;&#21407;&#22411;&#65292;&#21516;&#26102;&#23398;&#20064;&#29983;&#25104;&#24046;&#24322;&#24615;&#27835;&#30103;&#21453;&#24212;&#30340;&#27010;&#29575;&#12290;&#20351;&#29992;&#20020;&#24202;&#21644;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#35757;&#32451;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#31867;&#32531;&#35299;&#24182;&#36755;&#20986;&#20116;&#31181;&#19968;&#32447;&#21333;&#33647;&#30103;&#27861;&#21644;&#19977;&#31181;&#32852;&#21512;&#27835;&#30103;&#30340;&#20010;&#20307;&#32531;&#35299;&#27010;&#29575;&#12290;&#27169;&#22411;&#20351;&#29992;&#30041;&#19968;&#30740;&#31350;&#27861;&#20132;&#21449;&#39564;&#35777;&#12289;&#32622;&#25442;&#27979;&#35797;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#26512;&#36827;&#34892;&#39564;&#35777;&#21644;&#20020;&#24202;&#35299;&#37322;&#24615;&#35780;&#20272;&#12290;DPNN&#27169;&#22411;&#20934;&#30830;&#39044;&#27979;&#20102;&#25152;&#26377;&#20845;&#20010;&#30740;&#31350;&#30340;&#32531;&#35299;&#29366;&#20917;&#65292;&#36890;&#36807;&#37492;&#23450;&#19982;&#29305;&#23450;&#27835;&#30103;&#21453;&#24212;&#26356;&#22909;&#30456;&#20851;&#30340;&#24739;&#32773;&#20122;&#32452;&#26469;&#23637;&#31034;&#39640;&#30340;&#20020;&#24202;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#26377;&#21487;&#33021;&#20026;MDD&#24739;&#32773;&#30340;&#20010;&#24615;&#21270;&#27835;&#30103;&#24314;&#35758;&#25552;&#20379;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Major depressive disorder (MDD) is a heterogeneous condition; multiple underlying neurobiological substrates could be associated with treatment response variability. Understanding the sources of this variability and predicting outcomes has been elusive. Machine learning has shown promise in predicting treatment response in MDD, but one limitation has been the lack of clinical interpretability of machine learning models. We analyzed data from six clinical trials of pharmacological treatment for depression (total n = 5438) using the Differential Prototypes Neural Network (DPNN), a neural network model that derives patient prototypes which can be used to derive treatment-relevant patient clusters while learning to generate probabilities for differential treatment response. A model classifying remission and outputting individual remission probabilities for five first-line monotherapies and three combination treatments was trained using clinical and demographic data. Model validity and clin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;LONGNN&#65292;&#23427;&#37319;&#29992;&#21487;&#23398;&#20064;&#27491;&#20132;&#26631;&#20934;&#22522;&#65292;&#24182;&#35299;&#20915;&#20102;&#22266;&#23450;&#22810;&#39033;&#24335;&#22522;&#21644;&#38750;&#24402;&#19968;&#21270;&#22522;&#30784;&#25152;&#24102;&#26469;&#30340;&#32570;&#38519;&#65292;&#32463;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#21508;&#31181;&#22270;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.13750</link><description>&lt;p&gt;
LONGNN: &#20855;&#26377;&#21487;&#23398;&#20064;&#27491;&#20132;&#26631;&#20934;&#22522;&#30340;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
LONGNN: Spectral GNNs with Learnable Orthonormal Basis. (arXiv:2303.13750v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;LONGNN&#65292;&#23427;&#37319;&#29992;&#21487;&#23398;&#20064;&#27491;&#20132;&#26631;&#20934;&#22522;&#65292;&#24182;&#35299;&#20915;&#20102;&#22266;&#23450;&#22810;&#39033;&#24335;&#22522;&#21644;&#38750;&#24402;&#19968;&#21270;&#22522;&#30784;&#25152;&#24102;&#26469;&#30340;&#32570;&#38519;&#65292;&#32463;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#21508;&#31181;&#22270;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#37327;&#30340;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26041;&#27861;&#21033;&#29992;&#21487;&#23398;&#20064;&#31995;&#25968;&#30340;&#22810;&#39033;&#24335;&#22522;&#22312;&#35768;&#22810;&#33410;&#28857;&#32423;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#39030;&#32423;&#24615;&#33021;&#12290;&#34429;&#28982;&#24050;&#32463;&#25506;&#32034;&#20102;&#21508;&#31181;&#22810;&#39033;&#24335;&#22522;&#65292;&#20294;&#26159;&#27599;&#31181;&#26041;&#27861;&#37117;&#37319;&#29992;&#20102;&#22266;&#23450;&#30340;&#22810;&#39033;&#24335;&#22522;&#65292;&#21487;&#33021;&#19981;&#26159;&#32473;&#23450;&#22270;&#24418;&#30340;&#26368;&#20339;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#20123;&#26041;&#27861;&#25152;&#35859;&#30340;&#36234;&#30028;&#38382;&#39064;&#65292;&#24182;&#34920;&#26126;&#36825;&#22312;&#23427;&#20204;&#19981;&#22826;&#31995;&#32479;&#21270;&#30340;&#27491;&#21017;&#21270;&#31574;&#30053;&#21644;&#38750;&#24402;&#19968;&#21270;&#22522;&#30784;&#19978;&#26377;&#25152;&#26681;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#21033;&#29992;&#38597;&#21508;&#27604;&#22810;&#39033;&#24335;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#20855;&#26377;&#21487;&#23398;&#20064;&#27491;&#20132;&#26631;&#20934;&#22522;&#30340;&#35889;GNN&#65292;LON-GNN&#65292;&#24182;&#35777;&#26126;&#20102;&#27491;&#21017;&#21270;&#31995;&#25968;&#29616;&#22312;&#31561;&#25928;&#20110;&#27491;&#21017;&#21270;&#25152;&#23398;&#28388;&#27874;&#20989;&#25968;&#30340;&#33539;&#25968;&#12290;&#25105;&#20204;&#22312;&#22810;&#26679;&#30340;&#22270;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;LON-GNN&#30340;&#25311;&#21512;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, a plethora of spectral graph neural networks (GNN) methods have utilized polynomial basis with learnable coefficients to achieve top-tier performances on many node-level tasks. Although various kinds of polynomial bases have been explored, each such method adopts a fixed polynomial basis which might not be the optimal choice for the given graph. Besides, we identify the so-called over-passing issue of these methods and show that it is somewhat rooted in their less-principled regularization strategy and unnormalized basis. In this paper, we make the first attempts to address these two issues. Leveraging Jacobi polynomials, we design a novel spectral GNN, LON-GNN, with Learnable OrthoNormal bases and prove that regularizing coefficients becomes equivalent to regularizing the norm of learned filter function now. We conduct extensive experiments on diverse graph datasets to evaluate the fitting and generalization capability of LON-GNN, where the results imply its superiori
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#32858;&#31867;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30830;&#23450;&#24182;&#35782;&#21035;&#23545;&#20110;TBI&#31561;&#24613;&#24615;&#30142;&#30149;&#27835;&#30103;&#38750;&#24120;&#37325;&#35201;&#30340;&#29983;&#29702;&#29366;&#24577;&#12290;&#30740;&#31350;&#36824;&#21033;&#29992;&#20020;&#24202;&#25968;&#25454;&#39564;&#35777;&#24182;&#35299;&#37322;&#25152;&#35782;&#21035;&#30340;&#29983;&#29702;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2303.13024</link><description>&lt;p&gt;
&#29992;&#20110;&#35782;&#21035;TBI&#29983;&#29702;&#29366;&#24577;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#33258;&#30417;&#30563;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Clustering of Multivariate Time-Series Data for Identifying TBI Physiological States. (arXiv:2303.13024v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13024
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#32858;&#31867;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30830;&#23450;&#24182;&#35782;&#21035;&#23545;&#20110;TBI&#31561;&#24613;&#24615;&#30142;&#30149;&#27835;&#30103;&#38750;&#24120;&#37325;&#35201;&#30340;&#29983;&#29702;&#29366;&#24577;&#12290;&#30740;&#31350;&#36824;&#21033;&#29992;&#20020;&#24202;&#25968;&#25454;&#39564;&#35777;&#24182;&#35299;&#37322;&#25152;&#35782;&#21035;&#30340;&#29983;&#29702;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30830;&#23450;&#20020;&#24202;&#30456;&#20851;&#30340;&#29983;&#29702;&#29366;&#24577;&#38750;&#24120;&#37325;&#35201;&#65292;&#36825;&#23545;&#20110;&#25552;&#20379;&#24613;&#24615;&#30142;&#30149;&#65288;&#22914;&#39045;&#33041;&#25439;&#20260;&#12289;&#21628;&#21560;&#34928;&#31469;&#21644;&#24515;&#21147;&#34928;&#31469;&#65289;&#30340;&#36866;&#24403;&#27835;&#30103;&#33267;&#20851;&#37325;&#35201;&#12290;&#21033;&#29992;&#38750;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#25110;&#25968;&#25454;&#25554;&#20540;&#21644;&#32858;&#21512;&#25216;&#26415;&#21487;&#33021;&#23548;&#33268;&#26377;&#20215;&#20540;&#20449;&#24687;&#30340;&#20002;&#22833;&#21644;&#20559;&#35265;&#20998;&#26512;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#22522;&#20110;&#33258;&#30417;&#30563;&#30340;SLAC-Time&#31639;&#27861;&#65292;&#36991;&#20813;&#20102;&#25554;&#20540;&#25110;&#32858;&#21512;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#34920;&#31034;&#24613;&#24615;&#24739;&#32773;&#29366;&#24577;&#12290;&#36890;&#36807;&#20351;&#29992;SLAC-Time&#26469;&#32858;&#31867;&#22823;&#22411;&#30740;&#31350;&#25968;&#25454;&#38598;&#20013;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;TBI&#29983;&#29702;&#29366;&#24577;&#21450;&#20854;&#20855;&#20307;&#29305;&#24449;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#21508;&#31181;&#32858;&#31867;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#32467;&#21512;&#20020;&#24202;&#39046;&#22495;&#19987;&#23478;&#30340;&#24847;&#35265;&#26469;&#39564;&#35777;&#21644;&#35299;&#37322;&#25152;&#35782;&#21035;&#30340;&#29983;&#29702;&#29366;&#24577;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#29305;&#23450;&#20020;&#24202;&#20107;&#20214;&#21644;&#29983;&#29702;&#29366;&#24577;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Determining clinically relevant physiological states from multivariate time series data with missing values is essential for providing appropriate treatment for acute conditions such as Traumatic Brain Injury (TBI), respiratory failure, and heart failure. Utilizing non-temporal clustering or data imputation and aggregation techniques may lead to loss of valuable information and biased analyses. In our study, we apply the SLAC-Time algorithm, an innovative self-supervision-based approach that maintains data integrity by avoiding imputation or aggregation, offering a more useful representation of acute patient states. By using SLAC-Time to cluster data in a large research dataset, we identified three distinct TBI physiological states and their specific feature profiles. We employed various clustering evaluation metrics and incorporated input from a clinical domain expert to validate and interpret the identified physiological states. Further, we discovered how specific clinical events and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36741;&#21161;&#32593;&#32476;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65288;ANCL&#65289;&#65292;&#36890;&#36807;&#23545;&#27969;&#20449;&#24687;&#30340;&#25511;&#21046;&#65292;&#33258;&#28982;&#25554;&#20540;&#21487;&#22609;&#24615;&#21644;&#31283;&#23450;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#26377;&#21161;&#20110;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#26356;&#22909;&#30340;&#31283;&#23450;&#24615;-&#21487;&#22609;&#24615;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2303.09483</link><description>&lt;p&gt;
&#36741;&#21161;&#32593;&#32476;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#23454;&#29616;&#26356;&#22909;&#30340;&#31283;&#23450;&#24615;-&#21487;&#22609;&#24615;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Achieving a Better Stability-Plasticity Trade-off via Auxiliary Networks in Continual Learning. (arXiv:2303.09483v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36741;&#21161;&#32593;&#32476;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65288;ANCL&#65289;&#65292;&#36890;&#36807;&#23545;&#27969;&#20449;&#24687;&#30340;&#25511;&#21046;&#65292;&#33258;&#28982;&#25554;&#20540;&#21487;&#22609;&#24615;&#21644;&#31283;&#23450;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#26377;&#21161;&#20110;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#26356;&#22909;&#30340;&#31283;&#23450;&#24615;-&#21487;&#22609;&#24615;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20154;&#31867;&#39034;&#24207;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#33258;&#28982;&#33021;&#21147;&#30456;&#27604;&#65292;&#31070;&#32463;&#32593;&#32476;&#34987;&#35748;&#20026;&#23481;&#26131;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21363;&#27169;&#22411;&#22312;&#34987;&#20248;&#21270;&#20026;&#26032;&#20219;&#21153;&#21518;&#65292;&#22312;&#26087;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#24613;&#21095;&#19979;&#38477;&#12290;&#20026;&#27492;&#65292;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#31038;&#21306;&#25552;&#20986;&#20102;&#20960;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#26088;&#22312;&#20351;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#23398;&#20064;&#24403;&#21069;&#20219;&#21153;&#65288;&#21487;&#22609;&#24615;&#65289;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#22312;&#20197;&#21069;&#30340;&#20219;&#21153;&#19978;&#23454;&#29616;&#39640;&#31934;&#24230;&#65288;&#31283;&#23450;&#24615;&#65289;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#36827;&#23637;&#65292;&#20294;&#31283;&#23450;&#24615;-&#21487;&#22609;&#24615;&#24179;&#34913;&#36824;&#36828;&#26410;&#24471;&#21040;&#35299;&#20915;&#65292;&#20854;&#22522;&#26412;&#26426;&#21046;&#23578;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#36741;&#21161;&#32593;&#32476;&#25345;&#32493;&#23398;&#20064;&#65288;ANCL&#65289;&#65292;&#23427;&#23558;&#19968;&#20010;&#39069;&#22806;&#30340;&#36741;&#21161;&#32593;&#32476;&#24212;&#29992;&#20110;&#20027;&#35201;&#20851;&#27880;&#31283;&#23450;&#24615;&#30340;&#25345;&#32493;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#20419;&#36827;&#27169;&#22411;&#30340;&#21487;&#22609;&#24615;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#36890;&#36807;&#25511;&#21046;&#20027;&#35201;&#32593;&#32476;&#21644;&#36741;&#21161;&#32593;&#32476;&#20043;&#38388;&#20449;&#24687;&#30340;&#27969;&#21160;&#26469;&#33258;&#28982;&#22320;&#25554;&#20540;&#21487;&#22609;&#24615;&#21644;&#31283;&#23450;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ANCL&#22312;&#21487;&#22609;&#24615;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
In contrast to the natural capabilities of humans to learn new tasks in a sequential fashion, neural networks are known to suffer from catastrophic forgetting, where the model's performances on old tasks drop dramatically after being optimized for a new task. Since then, the continual learning (CL) community has proposed several solutions aiming to equip the neural network with the ability to learn the current task (plasticity) while still achieving high accuracy on the previous tasks (stability). Despite remarkable improvements, the plasticity-stability trade-off is still far from being solved and its underlying mechanism is poorly understood. In this work, we propose Auxiliary Network Continual Learning (ANCL), a novel method that applies an additional auxiliary network which promotes plasticity to the continually learned model which mainly focuses on stability. More concretely, the proposed framework materializes in a regularizer that naturally interpolates between plasticity and st
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Semantic Token ViT (STViT)&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20840;&#23616;&#21644;&#26412;&#22320;&#35270;&#35273;Transformer&#30340;&#39640;&#25928;&#24615;&#33021;&#65292;&#21516;&#26102;&#21487;&#29992;&#20316;&#19979;&#28216;&#20219;&#21153;&#30340;&#20027;&#24178;&#39592;&#24178;&#12290;&#20854;&#36890;&#36807;&#32858;&#31867;&#20013;&#24515;&#30340;&#35821;&#20041;&#20196;&#29260;&#20195;&#34920;&#26469;&#20195;&#26367;&#22270;&#20687;&#20196;&#29260;&#65292;&#23454;&#29616;&#36739;&#23569;&#30340;&#35821;&#20041;&#20196;&#29260;&#21363;&#21487;&#36798;&#21040;&#21516;&#26679;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.08685</link><description>&lt;p&gt;
&#22522;&#20110;&#20196;&#29260;&#31232;&#30095;&#21270;&#35270;&#35282;&#30340;&#35270;&#35273;Transformer&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Making Vision Transformers Efficient from A Token Sparsification View. (arXiv:2303.08685v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Semantic Token ViT (STViT)&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20840;&#23616;&#21644;&#26412;&#22320;&#35270;&#35273;Transformer&#30340;&#39640;&#25928;&#24615;&#33021;&#65292;&#21516;&#26102;&#21487;&#29992;&#20316;&#19979;&#28216;&#20219;&#21153;&#30340;&#20027;&#24178;&#39592;&#24178;&#12290;&#20854;&#36890;&#36807;&#32858;&#31867;&#20013;&#24515;&#30340;&#35821;&#20041;&#20196;&#29260;&#20195;&#34920;&#26469;&#20195;&#26367;&#22270;&#20687;&#20196;&#29260;&#65292;&#23454;&#29616;&#36739;&#23569;&#30340;&#35821;&#20041;&#20196;&#29260;&#21363;&#21487;&#36798;&#21040;&#21516;&#26679;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer (ViTs)&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#38543;&#30528;&#20196;&#29260;&#25968;&#37327;&#21576;&#20108;&#27425;&#22686;&#38271;&#65292;&#38480;&#21046;&#20102;&#20854;&#23454;&#38469;&#24212;&#29992;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#25928;&#30340;ViT&#65292;&#24050;&#26377;&#22810;&#31181;&#26041;&#27861;&#36890;&#36807;&#20462;&#21098;&#20887;&#20313;&#20196;&#29260;&#26469;&#36798;&#21040;&#30446;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#23384;&#22312;&#20197;&#19979;&#38382;&#39064;&#65306;(i) &#26174;&#33879;&#30340;&#31934;&#24230;&#19979;&#38477;&#65292;(ii) &#26080;&#27861;&#24212;&#29992;&#20110;&#26412;&#22320;&#35270;&#35273;Transformer&#20013;&#65292;&#20197;&#21450; (iii) &#26080;&#27861;&#36890;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#32593;&#32476;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#20041;&#20196;&#29260;ViT&#65288;STViT&#65289;&#65292;&#29992;&#20110;&#23454;&#29616;&#20840;&#23616;&#21644;&#26412;&#22320;&#35270;&#35273;Transformer&#30340;&#39640;&#25928;&#24615;&#33021;&#65292;&#24182;&#21487;&#20316;&#20026;&#19979;&#28216;&#20219;&#21153;&#30340;&#20027;&#24178;&#39592;&#24178;&#36827;&#34892;&#20462;&#35746;&#12290;&#35821;&#20041;&#20196;&#29260;&#20195;&#34920;&#32858;&#31867;&#20013;&#24515;&#65292;&#20854;&#36890;&#36807;&#31354;&#38388;&#20869;&#30340;&#22270;&#20687;&#20196;&#29260;&#27719;&#38598;&#26469;&#21021;&#22987;&#21270;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#32452;&#20214;&#36827;&#34892;&#24674;&#22797;&#65292;&#33258;&#36866;&#24212;&#22320;&#34920;&#31034;&#20840;&#23616;&#25110;&#26412;&#22320;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#30001;&#20110;&#20854;&#32858;&#31867;&#24615;&#36136;&#65292;&#23569;&#37327;&#30340;&#35821;&#20041;&#20196;&#29260;&#21363;&#21487;&#23454;&#29616;&#19982;&#20247;&#22810;&#22270;&#20687;&#20196;&#29260;&#30456;&#21516;&#30340;&#25928;&#26524;&#65292;&#36866;&#29992;&#20110;&#20840;&#23616;&#21644;&#26412;&#22320;&#35270;&#35273;Transformer&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;DeiT-(Tiny, Small, Base)&#65292;&#20165;&#38656;16&#20010;&#35821;&#20041;&#20196;&#29260;&#21363;&#21487;&#36798;&#21040;&#30456;&#21516;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quadratic computational complexity to the number of tokens limits the practical applications of Vision Transformers (ViTs). Several works propose to prune redundant tokens to achieve efficient ViTs. However, these methods generally suffer from (i) dramatic accuracy drops, (ii) application difficulty in the local vision transformer, and (iii) non-general-purpose networks for downstream tasks. In this work, we propose a novel Semantic Token ViT (STViT), for efficient global and local vision transformers, which can also be revised to serve as backbone for downstream tasks. The semantic tokens represent cluster centers, and they are initialized by pooling image tokens in space and recovered by attention, which can adaptively represent global or local semantic information. Due to the cluster properties, a few semantic tokens can attain the same effect as vast image tokens, for both global and local vision transformers. For instance, only 16 semantic tokens on DeiT-(Tiny,Small,Base) can 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25935;&#24863;&#21306;&#22495;&#30340;&#20803;&#27979;&#35797;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#36716;&#25442;&#36825;&#20123;&#21306;&#22495;&#26469;&#26377;&#25928;&#22320;&#26816;&#27979;&#26131;&#20986;&#29616;&#38169;&#35823;&#20998;&#31867;&#30340;&#22270;&#20687;&#65307;&#25935;&#24863;&#21306;&#22495;&#21487;&#20197;&#30001;&#21487;&#35299;&#37322;AI&#25351;&#23450;&#12290;</title><link>http://arxiv.org/abs/2303.07580</link><description>&lt;p&gt;
&#22522;&#20110;&#25935;&#24863;&#21306;&#22495;&#30340;&#21487;&#35299;&#37322;AI&#21464;&#24577;&#27979;&#35797;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Sensitive Region-based Metamorphic Testing Framework using Explainable AI. (arXiv:2303.07580v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07580
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25935;&#24863;&#21306;&#22495;&#30340;&#20803;&#27979;&#35797;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#36716;&#25442;&#36825;&#20123;&#21306;&#22495;&#26469;&#26377;&#25928;&#22320;&#26816;&#27979;&#26131;&#20986;&#29616;&#38169;&#35823;&#20998;&#31867;&#30340;&#22270;&#20687;&#65307;&#25935;&#24863;&#21306;&#22495;&#21487;&#20197;&#30001;&#21487;&#35299;&#37322;AI&#25351;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#21463;&#27426;&#36814;&#30340;&#30740;&#31350;&#35838;&#39064;&#20043;&#19968;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#35782;&#21035;&#31995;&#32479;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#21464;&#24577;&#27979;&#35797;&#26469;&#26816;&#27979;&#38169;&#35823;&#20998;&#31867;&#30340;&#22270;&#20687;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#35752;&#35770;&#21464;&#24577;&#20851;&#31995;(MR)&#65292;&#20294;&#24456;&#23569;&#35752;&#35770;&#24212;&#35813;&#36716;&#25442;&#21738;&#20123;&#21306;&#22495;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#23384;&#22312;&#25935;&#24863;&#21306;&#22495;&#65292;&#21363;&#20351;&#36827;&#34892;&#23567;&#30340;&#36716;&#25442;&#20063;&#20250;&#23481;&#26131;&#25913;&#21464;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#21464;&#24577;&#27979;&#35797;&#26694;&#26550;&#65292;&#36890;&#36807;&#36716;&#25442;&#25935;&#24863;&#21306;&#22495;&#26377;&#25928;&#22320;&#26816;&#27979;&#26131;&#20986;&#29616;&#38169;&#35823;&#20998;&#31867;&#30340;&#21306;&#22495;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25935;&#24863;&#21306;&#22495;&#21487;&#20197;&#30001;&#21487;&#35299;&#37322;AI(XAI)&#25351;&#23450;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26377;&#25928;&#22320;&#26816;&#27979;&#25925;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning (DL) is one of the most popular research topics in machine learning and DL-driven image recognition systems have developed rapidly. Recent research has used metamorphic testing (MT) to detect misclassified images. Most of them discuss metamorphic relations (MR), with little discussion on which regions should be transformed. We focus on the fact that there are sensitive regions where even a small transformation can easily change the prediction results and propose an MT framework that efficiently tests for regions prone to misclassification by transforming the sensitive regions. Our evaluation showed that the sensitive regions can be specified by Explainable AI (XAI) and our framework effectively detects faults.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#21435;&#22122;&#26041;&#27861;&#22312;&#20020;&#24202;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#35780;&#20272;&#65292;&#21457;&#29616;&#20351;&#29992;&#20445;&#30495;&#24230;(FoMs)&#30340;&#35780;&#20272;&#19981;&#19968;&#23450;&#19982;&#20219;&#21153;&#20026;&#22522;&#30784;&#30340;&#35780;&#20272;&#19968;&#33268;&#65292;&#32780;&#22522;&#20110;&#20449;&#21495;&#26816;&#27979;&#29702;&#35770;(SDT)&#30340;&#35780;&#20272;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#23458;&#35266;&#12289;&#26377;&#24847;&#20041;&#30340;&#21435;&#22122;&#25928;&#26524;&#35780;&#20272;&#26041;&#24335;&#65292;&#24182;&#35777;&#26126;&#34394;&#25311;&#20020;&#24202;&#35797;&#39564;&#65288;VCTs&#65289;&#26159;&#35780;&#20272;DL&#26041;&#27861;&#30340;&#23454;&#29992;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2303.02110</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#21435;&#22122;&#26041;&#27861;&#30340;&#23458;&#35266;&#20219;&#21153;&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#65306;&#20197;&#24515;&#32908;&#28748;&#27880;SPECT&#20026;&#32972;&#26223;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Need for Objective Task-based Evaluation of Deep Learning-Based Denoising Methods: A Study in the Context of Myocardial Perfusion SPECT. (arXiv:2303.02110v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#21435;&#22122;&#26041;&#27861;&#22312;&#20020;&#24202;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#35780;&#20272;&#65292;&#21457;&#29616;&#20351;&#29992;&#20445;&#30495;&#24230;(FoMs)&#30340;&#35780;&#20272;&#19981;&#19968;&#23450;&#19982;&#20219;&#21153;&#20026;&#22522;&#30784;&#30340;&#35780;&#20272;&#19968;&#33268;&#65292;&#32780;&#22522;&#20110;&#20449;&#21495;&#26816;&#27979;&#29702;&#35770;(SDT)&#30340;&#35780;&#20272;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#23458;&#35266;&#12289;&#26377;&#24847;&#20041;&#30340;&#21435;&#22122;&#25928;&#26524;&#35780;&#20272;&#26041;&#24335;&#65292;&#24182;&#35777;&#26126;&#34394;&#25311;&#20020;&#24202;&#35797;&#39564;&#65288;VCTs&#65289;&#26159;&#35780;&#20272;DL&#26041;&#27861;&#30340;&#23454;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#22312;&#26680;&#21307;&#23398;&#20013;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20852;&#36259;&#65292;&#20854;&#20013;&#29305;&#21035;&#20851;&#27880;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#26041;&#27861;&#21435;&#22122;&#20302;&#21058;&#37327;&#12289;&#30701;&#37319;&#38598;&#26102;&#38388;&#25110;&#20004;&#32773;&#21516;&#26102;&#33719;&#21462;&#30340;&#22270;&#20687;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#23458;&#35266;&#35780;&#20272;&#23545;&#20110;&#20020;&#24202;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;DL&#21435;&#22122;&#26680;&#21307;&#23398;&#22270;&#20687;&#36890;&#24120;&#20351;&#29992;&#31867;&#20284;RMSE&#21644;SSIM&#36825;&#26679;&#30340;&#20445;&#30495;&#24230;&#65288;FoMs&#65289;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22270;&#20687;&#26159;&#20026;&#20020;&#24202;&#20219;&#21153;&#32780;&#37319;&#38598;&#30340;&#65292;&#22240;&#27492;&#24212;&#35813;&#26681;&#25454;&#23427;&#20204;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#26469;&#35780;&#20272;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;(1)&#35843;&#26597;&#20351;&#29992;&#36825;&#20123;FoMs&#30340;&#35780;&#20272;&#26159;&#21542;&#19982;&#23458;&#35266;&#30340;&#20020;&#24202;&#20219;&#21153;&#35780;&#20272;&#19968;&#33268;; (2)&#25552;&#20379;&#29992;&#20110;&#30830;&#23450;&#21435;&#22122;&#23545;&#20449;&#21495;&#26816;&#27979;&#20219;&#21153;&#24433;&#21709;&#30340;&#29702;&#35770;&#20998;&#26512;; (3)&#23637;&#31034;&#34394;&#25311;&#20020;&#24202;&#35797;&#39564;&#65288;VCTs&#65289;&#29992;&#20110;&#35780;&#20272;DL&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;&#20351;&#29992;&#36924;&#30495;&#30340;&#27169;&#25311;&#22120;&#36827;&#34892;&#20102;&#19968;&#20010;VCT&#26469;&#35780;&#20272;DL&#21435;&#22122;&#24515;&#32908;&#28748;&#27880;SPECT&#22270;&#20687;&#26041;&#27861;&#12290;&#37319;&#29992;&#23458;&#35266;&#30340;&#24378;&#21046;&#36873;&#25321;&#23454;&#39564;&#65292;&#20351;&#29992;&#20449;&#21495;&#26816;&#27979;&#29702;&#35770;&#65288;SDT&#65289;&#30340;&#22522;&#20110;&#20219;&#21153;&#30340;&#25351;&#26631;&#21644;FoMs&#35780;&#20272;&#20102;&#21435;&#22122;&#25928;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;FoMs&#35780;&#20272;&#21435;&#22122;&#25928;&#26524;&#19981;&#19968;&#23450;&#19982;&#22522;&#20110;&#20219;&#21153;&#30340;&#35780;&#20272;&#30456;&#20851;&#12290;SDT&#25351;&#26631;&#25552;&#20379;&#20102;&#26356;&#23458;&#35266;&#21644;&#26377;&#24847;&#20041;&#30340;&#21435;&#22122;&#25928;&#26524;&#35780;&#20272;&#26041;&#24335;&#12290;VCTs&#21487;&#20026;&#26680;&#21307;&#23398;&#20013;&#22522;&#20110;DL&#30340;&#21435;&#22122;&#26041;&#27861;&#30340;&#35780;&#20272;&#25552;&#20379;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence-based methods have generated substantial interest in nuclear medicine. An area of significant interest has been using deep-learning (DL)-based approaches for denoising images acquired with lower doses, shorter acquisition times, or both. Objective evaluation of these approaches is essential for clinical application. DL-based approaches for denoising nuclear-medicine images have typically been evaluated using fidelity-based figures of merit (FoMs) such as RMSE and SSIM. However, these images are acquired for clinical tasks and thus should be evaluated based on their performance in these tasks. Our objectives were to (1) investigate whether evaluation with these FoMs is consistent with objective clinical-task-based evaluation; (2) provide a theoretical analysis for determining the impact of denoising on signal-detection tasks; (3) demonstrate the utility of virtual clinical trials (VCTs) to evaluate DL-based methods. A VCT to evaluate a DL-based method for denoisi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26102;&#31354;&#22240;&#26524;&#20851;&#31995;&#30340;&#26032;&#22411;&#27700;&#20301;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#22240;&#26524;&#32467;&#26500;&#24418;&#24335;&#21270;&#20026;&#22810;&#23618;&#32593;&#32476;&#21644;&#20351;&#29992;&#33945;&#29256;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#20854;&#21487;&#35299;&#37322;&#24615;&#65292;&#36816;&#29992;&#20110;&#27721;&#27743;&#25968;&#25454;&#38598;&#30340;&#23454;&#38469;&#20998;&#26512;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2303.00515</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#31354;&#22240;&#26524;&#20851;&#31995;&#30340;&#21487;&#35299;&#37322;&#27700;&#20301;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Interpretable Water Level Forecaster with Spatiotemporal Causal Attention Mechanisms. (arXiv:2303.00515v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26102;&#31354;&#22240;&#26524;&#20851;&#31995;&#30340;&#26032;&#22411;&#27700;&#20301;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#22240;&#26524;&#32467;&#26500;&#24418;&#24335;&#21270;&#20026;&#22810;&#23618;&#32593;&#32476;&#21644;&#20351;&#29992;&#33945;&#29256;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#20854;&#21487;&#35299;&#37322;&#24615;&#65292;&#36816;&#29992;&#20110;&#27721;&#27743;&#25968;&#25454;&#38598;&#30340;&#23454;&#38469;&#20998;&#26512;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#27721;&#27743;&#27700;&#20301;&#23545;&#20110;&#20132;&#36890;&#25511;&#21046;&#21644;&#36991;&#20813;&#33258;&#28982;&#28798;&#23475;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#28041;&#21450;&#22810;&#31181;&#21464;&#37327;&#24182;&#30456;&#20114;&#22797;&#26434;&#22320;&#32852;&#31995;&#30528;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#30340;&#36716;&#25442;&#22120;&#65292;&#21033;&#29992;&#21464;&#37327;&#20808;&#21069;&#30693;&#35782;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#65292;&#39044;&#27979;&#27721;&#27743;&#27982;&#24030;&#26725;&#30340;&#27700;&#20301;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#32771;&#34385;&#21040;&#31354;&#38388;&#21644;&#26102;&#38388;&#22240;&#26524;&#20851;&#31995;&#65292;&#23558;&#22240;&#26524;&#32467;&#26500;&#24418;&#24335;&#21270;&#20026;&#22810;&#23618;&#32593;&#32476;&#24182;&#20351;&#29992;&#33945;&#29256;&#26041;&#27861;&#12290;&#20973;&#20511;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#26681;&#25454;&#20808;&#21069;&#30340;&#30693;&#35782;&#33719;&#24471;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#23454;&#38469;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;2016&#24180;&#33267;2021&#24180;&#30340;&#27721;&#27743;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting the water level of the Han river is important to control traffic and avoid natural disasters. There are many variables related to the Han river and they are intricately connected. In this work, we propose a novel transformer that exploits the causal relationship based on the prior knowledge among the variables and forecasts the water level at the Jamsu bridge in the Han river. Our proposed model considers both spatial and temporal causation by formalizing the causal structure as a multilayer network and using masking methods. Due to this approach, we can have interpretability that consistent with prior knowledge. In real data analysis, we use the Han river dataset from 2016 to 2021 and compare the proposed model with deep learning models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26680;&#24515;&#38598;&#36827;&#34892;&#36873;&#25321;&#24615;&#32463;&#39564;&#22238;&#25918;&#21387;&#32553;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#25552;&#21319;&#32456;&#36523;&#23398;&#20064;&#30340;&#25928;&#29575;&#65292;&#24212;&#29992;&#20110;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2302.11510</link><description>&lt;p&gt;
&#20351;&#29992;&#26680;&#24515;&#38598;&#30340;&#36873;&#25321;&#24615;&#32463;&#39564;&#22238;&#25918;&#21387;&#32553;&#29992;&#20110;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#20013;&#30340;&#32456;&#36523;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Selective experience replay compression using coresets for lifelong deep reinforcement learning in medical imaging. (arXiv:2302.11510v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26680;&#24515;&#38598;&#36827;&#34892;&#36873;&#25321;&#24615;&#32463;&#39564;&#22238;&#25918;&#21387;&#32553;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#25552;&#21319;&#32456;&#36523;&#23398;&#20064;&#30340;&#25928;&#29575;&#65292;&#24212;&#29992;&#20110;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#24615;&#32463;&#39564;&#22238;&#25918;&#26159;&#23558;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#19982;&#32456;&#36523;&#23398;&#20064;&#32467;&#21512;&#30340;&#19968;&#31181;&#27969;&#34892;&#31574;&#30053;&#12290;&#36873;&#25321;&#24615;&#32463;&#39564;&#22238;&#25918;&#26088;&#22312;&#37325;&#36848;&#20197;&#21069;&#20219;&#21153;&#20013;&#30340;&#36873;&#25321;&#24615;&#32463;&#39564;&#65292;&#20197;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#36873;&#25321;&#24615;&#32463;&#39564;&#22238;&#25918;&#30340;&#25216;&#26415;&#26159;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#65292;&#24182;&#20801;&#35768;&#22312;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#20849;&#20139;&#32463;&#39564;&#12290;&#28982;&#32780;&#65292;&#23384;&#20648;&#25152;&#26377;&#20197;&#21069;&#20219;&#21153;&#30340;&#32463;&#39564;&#20250;&#20351;&#24471;&#20351;&#29992;&#36873;&#25321;&#24615;&#32463;&#39564;&#22238;&#25918;&#30340;&#32456;&#36523;&#23398;&#20064;&#21464;&#24471;&#35745;&#31639;&#19978;&#38750;&#24120;&#26114;&#36149;&#21644;&#19981;&#20999;&#23454;&#38469;&#65292;&#29305;&#21035;&#26159;&#38543;&#30528;&#20219;&#21153;&#25968;&#37327;&#30340;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Selective experience replay is a popular strategy for integrating lifelong learning with deep reinforcement learning. Selective experience replay aims to recount selected experiences from previous tasks to avoid catastrophic forgetting. Furthermore, selective experience replay based techniques are model agnostic and allow experiences to be shared across different models. However, storing experiences from all previous tasks make lifelong learning using selective experience replay computationally very expensive and impractical as the number of tasks increase. To that end, we propose a reward distribution-preserving coreset compression technique for compressing experience replay buffers stored for selective experience replay.  We evaluated the coreset compression technique on the brain tumor segmentation (BRATS) dataset for the task of ventricle localization and on the whole-body MRI for localization of left knee cap, left kidney, right trochanter, left lung, and spleen. The coreset lifel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#21644;&#21457;&#23637;&#21382;&#31243;&#65292;&#21253;&#25324;&#23427;&#20204;&#30340;&#26550;&#26500;&#12289;&#22521;&#35757;&#30446;&#26631;&#12289;&#39044;&#22521;&#35757;&#20219;&#21153;&#12289;&#24494;&#35843;&#31574;&#30053;&#21644;&#35780;&#20272;&#12290;&#21516;&#26102;&#65292;&#35752;&#35770;&#20102;&#20854;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.09419</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#32508;&#36848;&#65306;&#20174;BERT&#21040;ChatGPT&#30340;&#21382;&#31243;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT. (arXiv:2302.09419v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#21644;&#21457;&#23637;&#21382;&#31243;&#65292;&#21253;&#25324;&#23427;&#20204;&#30340;&#26550;&#26500;&#12289;&#22521;&#35757;&#30446;&#26631;&#12289;&#39044;&#22521;&#35757;&#20219;&#21153;&#12289;&#24494;&#35843;&#31574;&#30053;&#21644;&#35780;&#20272;&#12290;&#21516;&#26102;&#65292;&#35752;&#35770;&#20102;&#20854;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;(PFMs)&#34987;&#35748;&#20026;&#26159;&#21508;&#31181;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#19979;&#28216;&#20219;&#21153;&#30340;&#22522;&#30784;&#12290;PFM(&#20363;&#22914;BERT&#12289;ChatGPT&#21644;GPT-4)&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20026;&#21508;&#31181;&#19979;&#28216;&#24212;&#29992;&#25552;&#20379;&#20102;&#21512;&#29702;&#30340;&#21442;&#25968;&#21021;&#22987;&#21270;&#12290;BERT&#20174;&#36716;&#25442;&#22120;&#20013;&#23398;&#20064;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#20316;&#20026;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#31867;&#20284;&#22320;&#65292;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;(GPT)&#26041;&#27861;&#37319;&#29992;&#36716;&#25442;&#22120;&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#24182;&#37319;&#29992;&#33258;&#22238;&#24402;&#33539;&#24335;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#26368;&#36817;&#65292;ChatGPT&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#23637;&#29616;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#25104;&#21151;&#65292;&#23427;&#37319;&#29992;&#33258;&#22238;&#24402;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#36827;&#34892;&#38646;&#23556;&#20987;&#25110;&#23569;&#23556;&#20987;&#25552;&#31034;&#12290;PFM&#30340;&#21331;&#36234;&#25104;&#23601;&#20026;&#21508;&#31181;AI&#39046;&#22495;&#24102;&#26469;&#20102;&#37325;&#22823;&#31361;&#30772;&#12290;&#35768;&#22810;&#30740;&#31350;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#23545;&#26356;&#26032;&#35843;&#26597;&#30340;&#38656;&#27714;&#12290;&#26412;&#30740;&#31350;&#20840;&#38754;&#22238;&#39038;&#20102;PFMs&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#23427;&#20204;&#30340;&#26550;&#26500;&#12289;&#22521;&#35757;&#30446;&#26631;&#12289;&#39044;&#22521;&#35757;&#20219;&#21153;&#12289;&#24494;&#35843;&#31574;&#30053;&#21644;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;PFMs&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained Foundation Models (PFMs) are regarded as the foundation for various downstream tasks with different data modalities. A PFM (e.g., BERT, ChatGPT, and GPT-4) is trained on large-scale data which provides a reasonable parameter initialization for a wide range of downstream applications. BERT learns bidirectional encoder representations from Transformers, which are trained on large datasets as contextual language models. Similarly, the generative pretrained transformer (GPT) method employs Transformers as the feature extractor and is trained using an autoregressive paradigm on large datasets. Recently, ChatGPT shows promising success on large language models, which applies an autoregressive language model with zero shot or few shot prompting. The remarkable achievements of PFM have brought significant breakthroughs to various fields of AI. Numerous studies have proposed different methods, raising the demand for an updated survey. This study provides a comprehensive review of rec
&lt;/p&gt;</description></item><item><title>AutoFed &#26159;&#19968;&#31181;&#25903;&#25345;&#24322;&#26500;&#24863;&#30693;&#32852;&#37030;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#19978;&#30340;&#22810;&#27169;&#24577;&#20256;&#24863;&#25968;&#25454;&#65292;&#24182;&#20197;&#27492;&#23454;&#29616;&#31283;&#20581;&#30340;&#33258;&#21160;&#39550;&#39542;&#12290;&#23427;&#36890;&#36807;&#20266;&#26631;&#31614;&#21644;&#33258;&#32534;&#30721;&#22120;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#22312;&#35299;&#20915;&#20998;&#24067;&#24335;AVs&#19978;&#20855;&#26377;&#24322;&#26500;&#25968;&#25454;&#30340;&#25361;&#25112;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2302.08646</link><description>&lt;p&gt;
AutoFed&#65306;&#29992;&#20110;&#31283;&#20581;&#33258;&#21160;&#39550;&#39542;&#30340;&#24322;&#26500;&#24863;&#30693;&#32852;&#37030;&#22810;&#27169;&#24577;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AutoFed: Heterogeneity-Aware Federated Multimodal Learning for Robust Autonomous Driving. (arXiv:2302.08646v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08646
&lt;/p&gt;
&lt;p&gt;
AutoFed &#26159;&#19968;&#31181;&#25903;&#25345;&#24322;&#26500;&#24863;&#30693;&#32852;&#37030;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#19978;&#30340;&#22810;&#27169;&#24577;&#20256;&#24863;&#25968;&#25454;&#65292;&#24182;&#20197;&#27492;&#23454;&#29616;&#31283;&#20581;&#30340;&#33258;&#21160;&#39550;&#39542;&#12290;&#23427;&#36890;&#36807;&#20266;&#26631;&#31614;&#21644;&#33258;&#32534;&#30721;&#22120;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#22312;&#35299;&#20915;&#20998;&#24067;&#24335;AVs&#19978;&#20855;&#26377;&#24322;&#26500;&#25968;&#25454;&#30340;&#25361;&#25112;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#20013;&#65292;&#22522;&#20110;&#36710;&#36733;&#20256;&#24863;&#22120;&#65288;&#22914;&#28608;&#20809;&#38647;&#36798;&#12289;&#38647;&#36798;&#21644;&#25668;&#20687;&#22836;&#65289;&#30340;&#30446;&#26631;&#26816;&#27979;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#32780;&#36825;&#20123;&#20256;&#24863;&#22120;&#22312;&#27169;&#24577;&#19978;&#20114;&#20026;&#34917;&#20805;&#12290;&#23613;&#31649;&#20247;&#24863;&#30693;&#25216;&#26415;&#21487;&#33021;&#28508;&#22312;&#22320;&#21033;&#29992;&#36825;&#20123;&#20256;&#24863;&#22120;&#65288;&#25968;&#37327;&#24040;&#22823;&#65289;&#26469;&#24471;&#20986;&#26356;&#20840;&#38754;&#30340;&#30693;&#35782;&#65292;&#20294;&#26159;&#65292;\textit{&#32852;&#37030;&#23398;&#20064;}&#65288;FL&#65289;&#20284;&#20046;&#26159;&#36798;&#21040;&#36825;&#20010;&#28508;&#21147;&#30340;&#24517;&#35201;&#24037;&#20855;&#65306;&#23427;&#20351;&#24471;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#33021;&#22815;&#22312;&#19981;&#26174;&#24335;&#20849;&#20139;&#21407;&#22987;&#20256;&#24863;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#20256;&#24863;&#22120;&#24341;&#20837;&#20102;&#20998;&#24067;&#24335;AVs&#65288;&#22914;&#26631;&#31614;&#25968;&#37327;&#20559;&#24046;&#21644;&#19981;&#21516;&#24418;&#24335;&#65289;&#30340;&#21508;&#31181;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#32473;&#26377;&#25928;FL&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoFed&#20316;&#20026;&#19968;&#31181;&#24322;&#26500;&#24863;&#30693;FL&#26694;&#26550;&#65292;&#20805;&#20998;&#21033;&#29992;AVs&#19978;&#30340;&#22810;&#27169;&#24577;&#20256;&#24863;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#31283;&#20581;&#30340;&#33258;&#21160;&#39550;&#39542;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#20266;&#26631;&#31614;&#26469;&#36991;&#20813;&#38169;&#35823;&#22320;&#23558;&#26410;&#26631;&#35760;&#30340;&#23545;&#35937;&#35270;&#20026;&#32972;&#26223;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#36890;&#29992;&#29305;&#24449;&#34920;&#31034;&#12290;&#20511;&#21161;&#36825;&#20123;&#25216;&#26415;&#65292;AutoFed&#21487;&#20197;&#25104;&#21151;&#22320;&#32858;&#21512;&#26469;&#33258;&#20855;&#26377;&#21508;&#31181;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#20998;&#24067;&#24335;AVs&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#24182;&#27604;&#20256;&#32479;FL&#21644;&#38750;FL&#26041;&#27861;&#23454;&#29616;&#26356;&#22909;&#30340;&#29289;&#20307;&#26816;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Object detection with on-board sensors (e.g., lidar, radar, and camera) play a crucial role in autonomous driving (AD), and these sensors complement each other in modalities. While crowdsensing may potentially exploit these sensors (of huge quantity) to derive more comprehensive knowledge, \textit{federated learning} (FL) appears to be the necessary tool to reach this potential: it enables autonomous vehicles (AVs) to train machine learning models without explicitly sharing raw sensory data. However, the multimodal sensors introduce various data heterogeneity across distributed AVs (e.g., label quantity skews and varied modalities), posing critical challenges to effective FL. To this end, we present AutoFed as a heterogeneity-aware FL framework to fully exploit multimodal sensory data on AVs and thus enable robust AD. Specifically, we first propose a novel model leveraging pseudo-labeling to avoid mistakenly treating unlabeled objects as the background. We also propose an autoencoder-b
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#25237;&#24433;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;(PVDM)&#65292;&#22312;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#20013;&#23398;&#20064;&#35270;&#39057;&#20998;&#24067;&#65292;&#20174;&#32780;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#36164;&#28304;&#19979;&#39640;&#25928;&#22320;&#35757;&#32451;&#39640;&#20998;&#36776;&#29575;&#35270;&#39057;&#12290;</title><link>http://arxiv.org/abs/2302.07685</link><description>&lt;p&gt;
&#25237;&#24433;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#35270;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Video Probabilistic Diffusion Models in Projected Latent Space. (arXiv:2302.07685v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07685
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#25237;&#24433;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;(PVDM)&#65292;&#22312;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#20013;&#23398;&#20064;&#35270;&#39057;&#20998;&#24067;&#65292;&#20174;&#32780;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#36164;&#28304;&#19979;&#39640;&#25928;&#22320;&#35757;&#32451;&#39640;&#20998;&#36776;&#29575;&#35270;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#23618;&#29983;&#25104;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#35270;&#39057;&#30340;&#39640;&#32500;&#24615;&#12289;&#22797;&#26434;&#30340;&#26102;&#38388;&#21160;&#24577;&#21644;&#22823;&#30340;&#31354;&#38388;&#21464;&#21270;&#65292;&#21512;&#25104;&#39640;&#20998;&#36776;&#29575;&#21644;&#26102;&#38388;&#36830;&#36143;&#30340;&#35270;&#39057;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#26041;&#38754;&#23637;&#29616;&#20102;&#20854;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25928;&#29575;&#21364;&#21463;&#21040;&#20005;&#37325;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#65292;&#31216;&#20026;&#25237;&#24433;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;(PVDM)&#65292;&#23427;&#26159;&#19968;&#31181;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#65292;&#23427;&#22312;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#20013;&#23398;&#20064;&#35270;&#39057;&#20998;&#24067;&#65292;&#22240;&#27492;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#36164;&#28304;&#19979;&#39640;&#25928;&#22320;&#35757;&#32451;&#39640;&#20998;&#36776;&#29575;&#35270;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the remarkable progress in deep generative models, synthesizing high-resolution and temporally coherent videos still remains a challenge due to their high-dimensionality and complex temporal dynamics along with large spatial variations. Recent works on diffusion models have shown their potential to solve this challenge, yet they suffer from severe computation- and memory-inefficiency that limit the scalability. To handle this issue, we propose a novel generative model for videos, coined projected latent video diffusion models (PVDM), a probabilistic diffusion model which learns a video distribution in a low-dimensional latent space and thus can be efficiently trained with high-resolution videos under limited resources. Specifically, PVDM is composed of two components: (a) an autoencoder that projects a given video as 2D-shaped latent vectors that factorize the complex cubic structure of video pixels and (b) a diffusion model architecture specialized for our new factorized laten
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#29366;&#24577;&#31354;&#38388;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#22826;&#38451;&#33021;&#20809;&#20239;&#33021;&#28304;&#29983;&#20135;&#30701;&#26399;&#39044;&#27979;&#21644;&#28388;&#27874;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#29616;&#20195;&#21464;&#20998;&#25512;&#29702;&#25216;&#26415;&#65292;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#21644;&#38750;&#39640;&#26031;&#20998;&#24067;&#25968;&#25454;&#26102;&#20855;&#26377;&#36739;&#39640;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#26377;&#26395;&#20026;&#30005;&#31449;&#36816;&#33829;&#31649;&#29702;&#25552;&#20379;&#26356;&#21152;&#20934;&#30830;&#30340;&#20915;&#31574;&#20381;&#25454;&#12290;</title><link>http://arxiv.org/abs/2302.00388</link><description>&lt;p&gt;
&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#39640;&#26031;&#36807;&#31243;&#30340;&#22826;&#38451;&#33021;&#21457;&#30005;&#30701;&#26399;&#39044;&#27979;&#19982;&#28388;&#27874;
&lt;/p&gt;
&lt;p&gt;
Short-term Prediction and Filtering of Solar Power Using State-Space Gaussian Processes. (arXiv:2302.00388v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#29366;&#24577;&#31354;&#38388;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#22826;&#38451;&#33021;&#20809;&#20239;&#33021;&#28304;&#29983;&#20135;&#30701;&#26399;&#39044;&#27979;&#21644;&#28388;&#27874;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#29616;&#20195;&#21464;&#20998;&#25512;&#29702;&#25216;&#26415;&#65292;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#21644;&#38750;&#39640;&#26031;&#20998;&#24067;&#25968;&#25454;&#26102;&#20855;&#26377;&#36739;&#39640;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#26377;&#26395;&#20026;&#30005;&#31449;&#36816;&#33829;&#31649;&#29702;&#25552;&#20379;&#26356;&#21152;&#20934;&#30830;&#30340;&#20915;&#31574;&#20381;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#30005;&#31449;&#36816;&#33829;&#31649;&#29702;&#32780;&#35328;&#65292;&#22826;&#38451;&#33021;&#20809;&#20239;&#33021;&#28304;&#65288;PV&#65289;&#20135;&#37327;&#30340;&#30701;&#26399;&#39044;&#27979;&#38750;&#24120;&#37325;&#35201;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#39044;&#27979;&#24212;&#35813;&#24102;&#26377;&#35823;&#24046;&#33539;&#22260;&#65292;&#20351;&#24471;&#19979;&#28216;&#20915;&#31574;&#21487;&#20197;&#32771;&#34385;&#21040;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20135;&#29983;&#24102;&#26377;&#35823;&#24046;&#33539;&#22260;&#30340;&#39044;&#27979;&#65292;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243; (GPs) &#26469;&#23545;&#33521;&#22269;&#22826;&#38451;&#33021;&#20809;&#20239;&#33021;&#28304;&#29983;&#20135;&#36827;&#34892;&#24314;&#27169;&#21644;&#39044;&#27979;&#12290;&#30001;&#20110; PV &#30340;&#25968;&#25454;&#37327;&#22823;&#19988; PV &#35835;&#25968;&#19981;&#26381;&#20174;&#39640;&#26031;&#20998;&#24067;&#65292;&#25152;&#20197;&#26631;&#20934; GP &#22238;&#24402;&#27169;&#22411;&#38590;&#20197;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#20511;&#21161;&#26368;&#36817;&#39640;&#26031;&#36807;&#31243;&#22823;&#35268;&#27169;&#25512;&#29702;&#30340;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#20351;&#29992; GPs &#30340;&#29366;&#24577;&#31354;&#38388;&#24418;&#24335;&#65292;&#20197;&#21450;&#29616;&#20195;&#21464;&#20998;&#25512;&#29702;&#25216;&#26415;&#65292;&#21487;&#20197;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#22240;&#27492;&#65292;&#24471;&#21040;&#30340;&#27169;&#22411;&#19981;&#20165;&#36866;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#36890;&#36807;&#21345;&#23572;&#26364;&#28388;&#27874;&#22788;&#29702;&#36830;&#32493;&#25968;&#25454;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;
Short-term forecasting of solar photovoltaic energy (PV) production is important for powerplant management. Ideally these forecasts are equipped with error bars, so that downstream decisions can account for uncertainty. To produce predictions with error bars in this setting, we consider Gaussian processes (GPs) for modelling and predicting solar photovoltaic energy production in the UK. A standard application of GP regression on the PV timeseries data is infeasible due to the large data size and non-Gaussianity of PV readings. However, this is made possible by leveraging recent advances in scalable GP inference, in particular, by using the state-space form of GPs, combined with modern variational inference techniques. The resulting model is not only scalable to large datasets but can also handle continuous data streams via Kalman filtering.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25193;&#23637;&#20102;&#20449;&#24687;&#22330;&#29702;&#35770;(IFT)&#21040;&#29289;&#29702;&#20449;&#24687;&#22330;&#29702;&#35770;(PIFT)&#65292;&#23558;&#25551;&#36848;&#22330;&#30340;&#29289;&#29702;&#23450;&#24459;&#30340;&#20449;&#24687;&#32534;&#30721;&#20026;&#20989;&#25968;&#20808;&#39564;&#12290;&#20174;&#36825;&#20010;PIFT&#24471;&#20986;&#30340;&#21518;&#39564;&#19982;&#20219;&#20309;&#25968;&#20540;&#26041;&#26696;&#26080;&#20851;&#65292;&#24182;&#19988;&#21487;&#20197;&#25429;&#25417;&#22810;&#31181;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2301.07609</link><description>&lt;p&gt;
&#29289;&#29702;&#23398;&#30693;&#35782;&#20316;&#20026;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#27169;&#22411;&#30340;&#20449;&#24687;&#22330;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Physics-informed Information Field Theory for Modeling Physical Systems with Uncertainty Quantification. (arXiv:2301.07609v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07609
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25193;&#23637;&#20102;&#20449;&#24687;&#22330;&#29702;&#35770;(IFT)&#21040;&#29289;&#29702;&#20449;&#24687;&#22330;&#29702;&#35770;(PIFT)&#65292;&#23558;&#25551;&#36848;&#22330;&#30340;&#29289;&#29702;&#23450;&#24459;&#30340;&#20449;&#24687;&#32534;&#30721;&#20026;&#20989;&#25968;&#20808;&#39564;&#12290;&#20174;&#36825;&#20010;PIFT&#24471;&#20986;&#30340;&#21518;&#39564;&#19982;&#20219;&#20309;&#25968;&#20540;&#26041;&#26696;&#26080;&#20851;&#65292;&#24182;&#19988;&#21487;&#20197;&#25429;&#25417;&#22810;&#31181;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#32467;&#21512;&#29289;&#29702;&#23398;&#30693;&#35782;&#26159;&#24314;&#27169;&#31995;&#32479;&#30340;&#24378;&#26377;&#21147;&#25216;&#26415;&#12290;&#27492;&#31867;&#27169;&#22411;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#23558;&#27979;&#37327;&#32467;&#26524;&#19982;&#24050;&#30693;&#29289;&#29702;&#23450;&#24459;&#30456;&#32467;&#21512;&#65292;&#39640;&#25928;&#22320;&#27714;&#35299;&#22522;&#26412;&#22330;&#12290;&#30001;&#20110;&#35768;&#22810;&#31995;&#32479;&#21253;&#21547;&#26410;&#30693;&#20803;&#32032;&#65292;&#22914;&#32570;&#22833;&#21442;&#25968;&#12289;&#22024;&#26434;&#25968;&#25454;&#25110;&#19981;&#23436;&#25972;&#30340;&#29289;&#29702;&#23450;&#24459;&#65292;&#22240;&#27492;&#36825;&#36890;&#24120;&#34987;&#35270;&#20026;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38382;&#39064;&#12290;&#22788;&#29702;&#25152;&#26377;&#21464;&#37327;&#30340;&#24120;&#35265;&#25216;&#26415;&#36890;&#24120;&#21462;&#20915;&#20110;&#29992;&#20110;&#36817;&#20284;&#21518;&#39564;&#30340;&#25968;&#20540;&#26041;&#26696;&#65292;&#24182;&#19988;&#24076;&#26395;&#26377;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#31163;&#25955;&#21270;&#30340;&#26041;&#27861;&#12290;&#20449;&#24687;&#22330;&#29702;&#35770;&#65288;IFT&#65289;&#25552;&#20379;&#20102;&#23545;&#19981;&#19968;&#23450;&#26159;&#39640;&#26031;&#22330;&#30340;&#22330;&#36827;&#34892;&#32479;&#35745;&#23398;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#25551;&#36848;&#22330;&#30340;&#29289;&#29702;&#23450;&#24459;&#30340;&#20449;&#24687;&#32534;&#30721;&#20026;&#20989;&#25968;&#20808;&#39564;&#26469;&#25193;&#23637;IFT&#21040;&#29289;&#29702;&#20449;&#24687;&#22330;&#29702;&#35770;&#65288;PIFT&#65289;&#12290;&#20174;&#36825;&#20010;PIFT&#24471;&#20986;&#30340;&#21518;&#39564;&#19982;&#20219;&#20309;&#25968;&#20540;&#26041;&#26696;&#26080;&#20851;&#65292;&#24182;&#19988;&#21487;&#20197;&#25429;&#25417;&#22810;&#31181;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven approaches coupled with physical knowledge are powerful techniques to model systems. The goal of such models is to efficiently solve for the underlying field by combining measurements with known physical laws. As many systems contain unknown elements, such as missing parameters, noisy data, or incomplete physical laws, this is widely approached as an uncertainty quantification problem. The common techniques to handle all the variables typically depend on the numerical scheme used to approximate the posterior, and it is desirable to have a method which is independent of any such discretization. Information field theory (IFT) provides the tools necessary to perform statistics over fields that are not necessarily Gaussian. We extend IFT to physics-informed IFT (PIFT) by encoding the functional priors with information about the physical laws which describe the field. The posteriors derived from this PIFT remain independent of any numerical scheme and can capture multiple modes,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;Frank-Wolfe&#20248;&#21270;&#30340;&#39640;&#25928;&#20869;&#23384;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20197;&#21382;&#21490;&#20915;&#31574;&#20026;&#22522;&#30784;&#65292;&#36866;&#24212;&#23454;&#26102;&#26102;&#21464;&#29615;&#22659;&#12290;&#35813;&#31639;&#27861;&#24191;&#27867;&#24212;&#29992;&#20110;&#21160;&#24577;&#31995;&#32479;&#30340;&#22312;&#32447;&#25511;&#21046;&#65292;&#32479;&#35745;&#22871;&#21033;&#21644;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#31561;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2301.00497</link><description>&lt;p&gt;
&#22522;&#20110;Frank-Wolfe&#20248;&#21270;&#30340;&#39640;&#25928;&#20869;&#23384;&#22312;&#32447;&#23398;&#20064;&#65306;&#20855;&#26377;&#26377;&#30028;&#21160;&#24577;&#36951;&#25022;&#30340;&#31639;&#27861;&#21644;&#25511;&#21046;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Efficient Online Learning with Memory via Frank-Wolfe Optimization: Algorithms with Bounded Dynamic Regret and Applications to Control. (arXiv:2301.00497v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;Frank-Wolfe&#20248;&#21270;&#30340;&#39640;&#25928;&#20869;&#23384;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20197;&#21382;&#21490;&#20915;&#31574;&#20026;&#22522;&#30784;&#65292;&#36866;&#24212;&#23454;&#26102;&#26102;&#21464;&#29615;&#22659;&#12290;&#35813;&#31639;&#27861;&#24191;&#27867;&#24212;&#29992;&#20110;&#21160;&#24577;&#31995;&#32479;&#30340;&#22312;&#32447;&#25511;&#21046;&#65292;&#32479;&#35745;&#22871;&#21033;&#21644;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#31561;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25237;&#24433;&#25805;&#20316;&#26159;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#20856;&#22411;&#35745;&#31639;&#29942;&#39048;&#12290;&#26412;&#25991;&#22312;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#35760;&#24518;&#26694;&#26550;&#20013;&#23454;&#29616;&#20102;&#26080;&#25237;&#24433;&#30340;&#22312;&#32447;&#23398;&#20064;&#12290;&#20855;&#20307;&#22320;&#65292;OCO-M&#21453;&#26144;&#20102;&#20915;&#31574;&#21382;&#21490;&#22914;&#20309;&#24433;&#21709;&#24403;&#21069;&#32467;&#26524;&#65292;&#24182;&#20801;&#35768;&#22312;&#32447;&#23398;&#20064;&#25439;&#22833;&#20989;&#25968;&#20381;&#36182;&#20110;&#24403;&#21069;&#21644;&#36807;&#21435;&#30340;&#20915;&#31574;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#20869;&#23384;&#30340;&#26080;&#25237;&#24433;&#22522;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#21160;&#24577;&#36951;&#25022;&#26368;&#23567;&#21270;&#65292;&#21363;&#26368;&#23567;&#21270;&#19982;&#20219;&#20309;&#26102;&#21464;&#20915;&#31574;&#24207;&#21015;&#30340;&#27425;&#20248;&#24615;&#12290;&#26412;&#31639;&#27861;&#20197;&#22312;&#32447;Frank-Wolfe&#65288;OFW&#65289;&#21644;Hedge&#31639;&#27861;&#20026;&#22522;&#30784;&#65292;&#26088;&#22312;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#38656;&#35201;&#22312;&#23454;&#26102;&#20013;&#36866;&#24212;&#26102;&#21464;&#29615;&#22659;&#65292;&#24182;&#32771;&#34385;&#36807;&#21435;&#20915;&#31574;&#23545;&#29616;&#22312;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#24212;&#29992;&#21253;&#25324;&#65306;&#21160;&#24577;&#31995;&#32479;&#30340;&#22312;&#32447;&#25511;&#21046;&#65292;&#32479;&#35745;&#22871;&#21033;&#21644;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Projection operations are a typical computation bottleneck in online learning. In this paper, we enable projection-free online learning within the framework of Online Convex Optimization with Memory (OCO-M) -- OCO-M captures how the history of decisions affects the current outcome by allowing the online learning loss functions to depend on both current and past decisions. Particularly, we introduce the first projection-free meta-base learning algorithm with memory that minimizes dynamic regret, i.e., that minimizes the suboptimality against any sequence of time-varying decisions. We are motivated by artificial intelligence applications where autonomous agents need to adapt to time-varying environments in real-time, accounting for how past decisions affect the present. Examples of such applications are: online control of dynamical systems; statistical arbitrage; and time series prediction. The algorithm builds on the Online Frank-Wolfe (OFW) and Hedge algorithms. We demonstrate how our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#29616;&#23454;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#65292;&#37319;&#29992;&#26465;&#20214;&#29983;&#25104;&#22120;&#25511;&#21046;&#22833;&#30495;&#21644;&#30495;&#23454;&#24863;&#30340;&#26435;&#34913;&#65292;&#20174;&#32780;&#21487;&#20197;&#29983;&#25104;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#32531;&#35299;&#20102;&#20197;&#21069;&#26041;&#27861;&#21512;&#25104;&#32454;&#33410;&#19981;&#21487;&#25511;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#22833;&#30495;-&#30495;&#23454;&#24863;&#26041;&#38754;&#21462;&#24471;&#20102;&#19994;&#30028;&#39046;&#20808;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.13824</link><description>&lt;p&gt;
&#20351;&#29992;&#26377;&#26465;&#20214;&#29983;&#25104;&#22120;&#30340;&#22810;&#29616;&#23454;&#22270;&#20687;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Multi-Realism Image Compression with a Conditional Generator. (arXiv:2212.13824v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#29616;&#23454;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#65292;&#37319;&#29992;&#26465;&#20214;&#29983;&#25104;&#22120;&#25511;&#21046;&#22833;&#30495;&#21644;&#30495;&#23454;&#24863;&#30340;&#26435;&#34913;&#65292;&#20174;&#32780;&#21487;&#20197;&#29983;&#25104;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#32531;&#35299;&#20102;&#20197;&#21069;&#26041;&#27861;&#21512;&#25104;&#32454;&#33410;&#19981;&#21487;&#25511;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#22833;&#30495;-&#30495;&#23454;&#24863;&#26041;&#38754;&#21462;&#24471;&#20102;&#19994;&#30028;&#39046;&#20808;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20248;&#21270;&#36895;&#29575;-&#22833;&#30495;-&#30495;&#23454;&#24863;&#30340;&#26435;&#34913;&#65292;&#29983;&#25104;&#24335;&#21387;&#32553;&#26041;&#27861;&#21487;&#20197;&#22312;&#20302;&#27604;&#29305;&#29575;&#19979;&#29983;&#25104;&#35814;&#32454;&#12289;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#32780;&#19981;&#26159;&#36895;&#29575;-&#22833;&#30495;&#20248;&#21270;&#27169;&#22411;&#29983;&#25104;&#30340;&#27169;&#31946;&#37325;&#24314;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#26041;&#27861;&#27809;&#26377;&#26126;&#30830;&#25511;&#21046;&#21512;&#25104;&#22810;&#23569;&#32454;&#33410;&#65292;&#36825;&#23548;&#33268;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#20010;&#26222;&#36941;&#25209;&#35780;&#65306;&#29992;&#25143;&#21487;&#33021;&#25285;&#24515;&#20250;&#29983;&#25104;&#19968;&#20010;&#36828;&#31163;&#36755;&#20837;&#22270;&#20687;&#30340;&#35823;&#23548;&#24615;&#37325;&#24314;&#22270;&#20687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#35299;&#30721;&#22120;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#35813;&#35299;&#30721;&#22120;&#21487;&#20197;&#36328;&#36234;&#20004;&#20010;&#29366;&#24577;&#24182;&#23548;&#33322;&#22833;&#30495;-&#30495;&#23454;&#24863;&#30340;&#26435;&#34913;&#12290;&#20174;&#21333;&#20010;&#21387;&#32553;&#34920;&#31034;&#20013;&#65292;&#25509;&#25910;&#22120;&#21487;&#20197;&#20915;&#23450;&#37325;&#26500;&#19968;&#20010;&#20302;&#22343;&#26041;&#35823;&#24046;&#25509;&#36817;&#36755;&#20837;&#30340;&#37325;&#26500;&#65292;&#20855;&#26377;&#39640;&#24863;&#30693;&#36136;&#37327;&#30340;&#36924;&#30495;&#37325;&#26500;&#65292;&#25110;&#20219;&#20309;&#20171;&#20110;&#20004;&#32773;&#20043;&#38388;&#30340;&#19996;&#35199;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#22833;&#30495;-&#30495;&#23454;&#24863;&#26041;&#38754;&#35774;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#19994;&#30028;&#26631;&#26438;&#65292;&#25512;&#21160;&#20102;&#21487;&#23454;&#29616;&#30340;&#22833;&#30495;-&#30495;&#23454;&#24863;&#37197;&#23545;&#30340;&#21069;&#27839;&#65292;&#21363;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;b
&lt;/p&gt;
&lt;p&gt;
By optimizing the rate-distortion-realism trade-off, generative compression approaches produce detailed, realistic images, even at low bit rates, instead of the blurry reconstructions produced by rate-distortion optimized models. However, previous methods do not explicitly control how much detail is synthesized, which results in a common criticism of these methods: users might be worried that a misleading reconstruction far from the input image is generated. In this work, we alleviate these concerns by training a decoder that can bridge the two regimes and navigate the distortion-realism trade-off. From a single compressed representation, the receiver can decide to either reconstruct a low mean squared error reconstruction that is close to the input, a realistic reconstruction with high perceptual quality, or anything in between. With our method, we set a new state-of-the-art in distortion-realism, pushing the frontier of achievable distortion-realism pairs, i.e., our method achieves b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19968;&#32500;&#38750;&#36127;&#27979;&#24230;&#20043;&#38388;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;&#38382;&#39064;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#20999;&#29255;&#30340;&#26041;&#24335;&#23450;&#20041;&#20102;&#20999;&#29255;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;&#36317;&#31163;&#12290;</title><link>http://arxiv.org/abs/2212.08049</link><description>&lt;p&gt;
&#20999;&#29255;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;
&lt;/p&gt;
&lt;p&gt;
Sliced Optimal Partial Transport. (arXiv:2212.08049v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19968;&#32500;&#38750;&#36127;&#27979;&#24230;&#20043;&#38388;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;&#38382;&#39064;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#20999;&#29255;&#30340;&#26041;&#24335;&#23450;&#20041;&#20102;&#20999;&#29255;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#20256;&#36755;&#65288;OT&#65289;&#24050;&#32463;&#22312;&#26426;&#22120;&#23398;&#20064;&#12289;&#25968;&#25454;&#31185;&#23398;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#21464;&#24471;&#26497;&#20854;&#27969;&#34892;&#12290;OT&#38382;&#39064;&#30340;&#26680;&#24515;&#20551;&#35774;&#26159;&#28304;&#21644;&#30446;&#26631;&#27979;&#24230;&#30340;&#24635;&#36136;&#37327;&#30456;&#31561;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#30340;&#24212;&#29992;&#12290;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;&#65288;OPT&#65289;&#26159;&#26368;&#36817;&#25552;&#20986;&#30340;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#30340;&#26041;&#27861;&#12290;&#19982;OT&#38382;&#39064;&#31867;&#20284;&#65292;OPT&#30340;&#35745;&#31639;&#20381;&#36182;&#20110;&#35299;&#20915;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#65288;&#36890;&#24120;&#22312;&#39640;&#32500;&#24230;&#20013;&#65289;&#65292;&#36825;&#21487;&#33021;&#20250;&#21464;&#24471;&#35745;&#31639;&#19978;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#19968;&#32500;&#38750;&#36127;&#27979;&#24230;&#20043;&#38388;OPT&#38382;&#39064;&#30340;&#26377;&#25928;&#31639;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#36981;&#24490;&#20999;&#29255;OT&#36317;&#31163;&#30340;&#24605;&#24819;&#65292;&#25105;&#20204;&#21033;&#29992;&#20999;&#29255;&#23450;&#20041;&#20102;&#20999;&#29255;OPT&#36317;&#31163;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20999;&#29255;OPT-based&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#20540;&#23454;&#39564;&#20013;&#30340;&#35745;&#31639;&#21644;&#31934;&#24230;&#20248;&#21183;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;Sliced-OPT&#22312;&#22122;&#22768;&#28857;&#20113;&#37197;&#20934;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal transport (OT) has become exceedingly popular in machine learning, data science, and computer vision. The core assumption in the OT problem is the equal total amount of mass in source and target measures, which limits its application. Optimal Partial Transport (OPT) is a recently proposed solution to this limitation. Similar to the OT problem, the computation of OPT relies on solving a linear programming problem (often in high dimensions), which can become computationally prohibitive. In this paper, we propose an efficient algorithm for calculating the OPT problem between two non-negative measures in one dimension. Next, following the idea of sliced OT distances, we utilize slicing to define the sliced OPT distance. Finally, we demonstrate the computational and accuracy benefits of the sliced OPT-based method in various numerical experiments. In particular, we show an application of our proposed Sliced-OPT in noisy point cloud registration.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;LLM-Planner&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#23454;&#20307;&#20195;&#29702;&#36827;&#34892;&#23569;&#26679;&#26412;&#35268;&#21010;&#65292;&#20197;&#23454;&#20307;&#20195;&#29702;&#30446;&#21069;&#25152;&#22312;&#30340;&#29615;&#22659;&#20026;&#22522;&#30784;&#65292;&#22686;&#24378;LLMs&#29983;&#25104;&#21644;&#26356;&#26032;&#35745;&#21010;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#22810;&#20219;&#21153;&#21644;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#36890;&#29992;&#20195;&#29702;&#30340;&#24320;&#21457;&#20013;&#20855;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2212.04088</link><description>&lt;p&gt;
LLM-Planner: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23569;&#26679;&#26412;&#23454;&#20307;&#20195;&#29702;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models. (arXiv:2212.04088v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;LLM-Planner&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#23454;&#20307;&#20195;&#29702;&#36827;&#34892;&#23569;&#26679;&#26412;&#35268;&#21010;&#65292;&#20197;&#23454;&#20307;&#20195;&#29702;&#30446;&#21069;&#25152;&#22312;&#30340;&#29615;&#22659;&#20026;&#22522;&#30784;&#65292;&#22686;&#24378;LLMs&#29983;&#25104;&#21644;&#26356;&#26032;&#35745;&#21010;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#22810;&#20219;&#21153;&#21644;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#36890;&#29992;&#20195;&#29702;&#30340;&#24320;&#21457;&#20013;&#20855;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#35268;&#21010;&#22120;&#65292;&#35753;&#23454;&#20307;&#20195;&#29702;&#21487;&#20197;&#25353;&#29031;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#23436;&#25104;&#22312;&#35270;&#35273;&#24863;&#30693;&#29615;&#22659;&#20013;&#30340;&#22797;&#26434;&#20219;&#21153;&#12290;&#29616;&#26377;&#26041;&#27861;&#30340;&#39640;&#25968;&#25454;&#25104;&#26412;&#21644;&#20302;&#26679;&#26412;&#25928;&#29575;&#38459;&#30861;&#20102;&#22810;&#20219;&#21153;&#21644;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#36890;&#29992;&#20195;&#29702;&#30340;&#24320;&#21457;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;LLM-Planner&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#23454;&#20307;&#20195;&#29702;&#36827;&#34892;&#23569;&#26679;&#26412;&#35268;&#21010;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#20307;&#20195;&#29702;&#30446;&#21069;&#25152;&#22312;&#30340;&#29615;&#22659;&#20026;&#22522;&#30784;&#65292;&#22686;&#24378;LLMs&#29983;&#25104;&#21644;&#26356;&#26032;&#35745;&#21010;&#12290;&#22312;ALFRED&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#38750;&#24120;&#26377;&#31454;&#20105;&#21147;&#30340;&#23569;&#26679;&#26412;&#24615;&#33021;&#65306;&#23613;&#31649;&#20351;&#29992;&#30340;&#37197;&#23545;&#35757;&#32451;&#25968;&#25454;&#19981;&#21040;0.5&#65285;&#65292;LLM-Planner&#30340;&#34920;&#29616;&#19982;&#20351;&#29992;&#23436;&#25972;&#35757;&#32451;&#25968;&#25454;&#35757;&#32451;&#30340;&#26368;&#26032;&#22522;&#32447;&#30456;&#24403;&#12290;&#29616;&#26377;&#26041;&#27861;&#20960;&#20046;&#26080;&#27861;&#23436;&#25104;&#20219;&#20309;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study focuses on using large language models (LLMs) as a planner for embodied agents that can follow natural language instructions to complete complex tasks in a visually-perceived environment. The high data cost and poor sample efficiency of existing methods hinders the development of versatile agents that are capable of many tasks and can learn new tasks quickly. In this work, we propose a novel method, LLM-Planner, that harnesses the power of large language models to do few-shot planning for embodied agents. We further propose a simple but effective way to enhance LLMs with physical grounding to generate and update plans that are grounded in the current environment. Experiments on the ALFRED dataset show that our method can achieve very competitive few-shot performance: Despite using less than 0.5% of paired training data, LLM-Planner achieves competitive performance with recent baselines that are trained using the full training data. Existing methods can barely complete any ta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MHCCL&#30340;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#23398;&#20064;&#35821;&#20041;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#23618;&#27425;&#32858;&#31867;&#32467;&#26500;&#20013;&#30340;&#22810;&#31890;&#24230;&#20449;&#24687;&#26469;&#28388;&#38500;&#34394;&#20551;&#36127;&#26679;&#26412;&#21644;&#34917;&#20805;&#27491;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2212.01141</link><description>&lt;p&gt;
MHCCL&#65306;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30340;&#23618;&#27425;&#25513;&#34109;&#32858;&#31867;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MHCCL: Masked Hierarchical Cluster-Wise Contrastive Learning for Multivariate Time Series. (arXiv:2212.01141v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MHCCL&#30340;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#23398;&#20064;&#35821;&#20041;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#23618;&#27425;&#32858;&#31867;&#32467;&#26500;&#20013;&#30340;&#22810;&#31890;&#24230;&#20449;&#24687;&#26469;&#28388;&#38500;&#34394;&#20551;&#36127;&#26679;&#26412;&#21644;&#34917;&#20805;&#27491;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#21407;&#22987;&#26080;&#26631;&#31614;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#23398;&#20064;&#35821;&#20041;&#20016;&#23500;&#30340;&#34920;&#31034;&#23545;&#20110;&#20998;&#31867;&#21644;&#39044;&#27979;&#31561;&#19979;&#28216;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#27604;&#23398;&#20064;&#26368;&#36817;&#23637;&#31034;&#20102;&#22312;&#32570;&#20047;&#19987;&#23478;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#29420;&#31435;&#22788;&#29702;&#27599;&#20010;&#23454;&#20363;&#65292;&#23548;&#33268;&#20849;&#20139;&#30456;&#21516;&#35821;&#20041;&#30340;&#20551;&#36127;&#26679;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MHCCL&#65292;&#19968;&#31181;&#23618;&#27425;&#25513;&#34109;&#32858;&#31867;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#30001;&#22810;&#20010;&#28508;&#22312;&#20998;&#21306;&#32452;&#25104;&#30340;&#23618;&#27425;&#32467;&#26500;&#33719;&#24471;&#30340;&#35821;&#20041;&#20449;&#24687;&#26469;&#20026;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#12290;&#21463;&#21040;&#32454;&#31890;&#24230;&#32858;&#31867;&#20445;&#30041;&#26356;&#39640;&#32431;&#24230;&#65292;&#32780;&#31895;&#31890;&#24230;&#32858;&#31867;&#21453;&#26144;&#26356;&#39640;&#32423;&#21035;&#35821;&#20041;&#30340;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21521;&#19979;&#25513;&#34109;&#31574;&#30053;&#65292;&#36890;&#36807;&#32467;&#21512;&#32858;&#31867;&#23618;&#27425;&#32467;&#26500;&#20013;&#30340;&#22810;&#31890;&#24230;&#20449;&#24687;&#65292;&#36807;&#28388;&#25481;&#34394;&#20551;&#36127;&#38754;&#23454;&#20363;&#24182;&#34917;&#20805;&#27491;&#38754;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning semantic-rich representations from raw unlabeled time series data is critical for downstream tasks such as classification and forecasting. Contrastive learning has recently shown its promising representation learning capability in the absence of expert annotations. However, existing contrastive approaches generally treat each instance independently, which leads to false negative pairs that share the same semantics. To tackle this problem, we propose MHCCL, a Masked Hierarchical Cluster-wise Contrastive Learning model, which exploits semantic information obtained from the hierarchical structure consisting of multiple latent partitions for multivariate time series. Motivated by the observation that fine-grained clustering preserves higher purity while coarse-grained one reflects higher-level semantics, we propose a novel downward masking strategy to filter out fake negatives and supplement positives by incorporating the multi-granularity information from the clustering hierarchy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;PFL&#26694;&#26550;PHN-HVI&#65292;&#21033;&#29992;&#36229;&#32593;&#32476;&#29983;&#25104;&#19968;&#32452;&#22810;&#26679;&#30340;&#35299;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#36825;&#20123;&#35299;&#23450;&#20041;&#30340;&#36229;&#20307;&#31215;&#25351;&#26631;&#26469;&#25552;&#39640;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2212.01130</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26679;&#26412;&#36229;&#32593;&#32476;&#25552;&#39640;&#24085;&#32047;&#25176;&#21069;&#27839;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improving Pareto Front Learning via Multi-Sample Hypernetworks. (arXiv:2212.01130v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;PFL&#26694;&#26550;PHN-HVI&#65292;&#21033;&#29992;&#36229;&#32593;&#32476;&#29983;&#25104;&#19968;&#32452;&#22810;&#26679;&#30340;&#35299;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#36825;&#20123;&#35299;&#23450;&#20041;&#30340;&#36229;&#20307;&#31215;&#25351;&#26631;&#26469;&#25552;&#39640;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24085;&#32047;&#25176;&#21069;&#27839;&#23398;&#20064;(PFL)&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33719;&#24471;&#20174;&#32473;&#23450;&#26435;&#34913;&#21521;&#37327;&#21040;&#24085;&#32047;&#25176;&#21069;&#27839;&#35299;&#30340;&#26144;&#23556;&#20989;&#25968;&#65292;&#20174;&#32780;&#35299;&#20915;&#22810;&#30446;&#26631;&#20248;&#21270;(MOO)&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PFL&#26041;&#27861;&#24573;&#30053;&#20102;&#20248;&#21270;&#36807;&#31243;&#20013;&#35299;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#33719;&#24471;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;PFL&#26694;&#26550;&#65292;&#21363;PHN-HVI&#65292;&#23427;&#20351;&#29992;&#36229;&#32593;&#32476;&#20174;&#22810;&#26679;&#30340;&#26435;&#34913;&#20559;&#22909;&#38598;&#29983;&#25104;&#22810;&#20010;&#35299;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#36825;&#20123;&#35299;&#23450;&#20041;&#30340;&#36229;&#20307;&#31215;&#25351;&#26631;&#26469;&#25552;&#39640;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#36136;&#37327;&#12290;&#22810;&#20010;MOO&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;PFL&#26041;&#27861;&#65292;PHN-HVI&#22312;&#24085;&#32047;&#25176;&#21069;&#27839;&#36817;&#20284;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pareto Front Learning (PFL) was recently introduced as an effective approach to obtain a mapping function from a given trade-off vector to a solution on the Pareto front, which solves the multi-objective optimization (MOO) problem. Due to the inherent trade-off between conflicting objectives, PFL offers a flexible approach in many scenarios in which the decision makers can not specify the preference of one Pareto solution over another, and must switch between them depending on the situation. However, existing PFL methods ignore the relationship between the solutions during the optimization process, which hinders the quality of the obtained front. To overcome this issue, we propose a novel PFL framework namely PHN-HVI, which employs a hypernetwork to generate multiple solutions from a set of diverse trade-off preferences and enhance the quality of the Pareto front by maximizing the Hypervolume indicator defined by these solutions. The experimental results on several MOO machine learning
&lt;/p&gt;</description></item><item><title>CODA-Prompt&#26159;&#19968;&#31181;&#22522;&#20110;&#20998;&#35299;&#27880;&#24847;&#21147;&#25552;&#31034;&#65292;&#26080;&#38656;&#37325;&#22797;&#35757;&#32451;&#21363;&#21487;&#36830;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#25552;&#31034;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2211.13218</link><description>&lt;p&gt;
CODA-Prompt&#65306;&#22522;&#20110;&#20998;&#35299;&#27880;&#24847;&#21147;&#25552;&#31034;&#30340;&#26080;&#37325;&#35757;&#32451;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CODA-Prompt: COntinual Decomposed Attention-based Prompting for Rehearsal-Free Continual Learning. (arXiv:2211.13218v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13218
&lt;/p&gt;
&lt;p&gt;
CODA-Prompt&#26159;&#19968;&#31181;&#22522;&#20110;&#20998;&#35299;&#27880;&#24847;&#21147;&#25552;&#31034;&#65292;&#26080;&#38656;&#37325;&#22797;&#35757;&#32451;&#21363;&#21487;&#36830;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#25552;&#31034;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#22312;&#23398;&#20064;&#19981;&#26029;&#21464;&#21270;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#26032;&#27010;&#24565;&#26102;&#23481;&#26131;&#20135;&#29983;&#25152;&#35859;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#12290;&#35299;&#20915;&#36825;&#20010;&#36830;&#32493;&#23398;&#20064;&#38382;&#39064;&#30340;&#20856;&#22411;&#26041;&#27861;&#38656;&#35201;&#23545;&#20808;&#21069;&#24050;&#32463;&#35265;&#36807;&#30340;&#25968;&#25454;&#36827;&#34892;&#22823;&#37327;&#30340;&#37325;&#22797;&#35757;&#32451;&#65292;&#36825;&#22686;&#21152;&#20102;&#20869;&#23384;&#25104;&#26412;&#24182;&#21487;&#33021;&#36829;&#21453;&#25968;&#25454;&#38544;&#31169;&#12290;&#26368;&#36817;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35270;&#35273;&#21464;&#25442;&#22120;&#27169;&#22411;&#30340;&#20986;&#29616;&#20351;&#25552;&#31034;&#26041;&#27861;&#25104;&#20026;&#19968;&#31181;&#26367;&#20195;&#25968;&#25454;&#37325;&#22797;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#20381;&#38752;&#20851;&#38190;&#26597;&#35810;&#26426;&#21046;&#29983;&#25104;&#25552;&#31034;&#65292;&#24182;&#24050;&#34987;&#21457;&#29616;&#22312;&#24050;&#32463;&#24314;&#31435;&#30340;&#26080;&#37325;&#35757;&#32451;&#36830;&#32493;&#23398;&#20064;&#35774;&#32622;&#20013;&#39640;&#24230;&#25269;&#25239;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#20851;&#38190;&#26426;&#21046;&#27809;&#26377;&#19982;&#20219;&#21153;&#24207;&#21015;&#19968;&#36215;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20250;&#23548;&#33268;&#23427;&#20204;&#30340;&#21487;&#22609;&#24615;&#38477;&#20302;&#65292;&#20174;&#32780;&#29306;&#29298;&#26032;&#20219;&#21153;&#20934;&#30830;&#24615;&#65292;&#24182;&#26080;&#27861;&#20174;&#25193;&#23637;&#30340;&#21442;&#25968;&#23481;&#37327;&#20013;&#21463;&#30410;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;CODA-Prompt&#65292;&#23427;&#20351;&#29992;&#20998;&#35299;&#27880;&#24847;&#21147;&#25552;&#31034;&#26426;&#21046;&#32467;&#21512;&#33976;&#39311;&#25439;&#22833;&#26469;&#35757;&#32451;&#25552;&#31034;&#32452;&#20214;&#65292;&#20174;&#32780;&#23454;&#29616;&#19982;&#20219;&#21153;&#24207;&#21015;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CODA-Prompt&#20248;&#20110;&#26368;&#36817;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#32780;&#19981;&#38656;&#35201;&#37325;&#22797;&#35757;&#32451;&#25110;&#39069;&#22806;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computer vision models suffer from a phenomenon known as catastrophic forgetting when learning novel concepts from continuously shifting training data. Typical solutions for this continual learning problem require extensive rehearsal of previously seen data, which increases memory costs and may violate data privacy. Recently, the emergence of large-scale pre-trained vision transformer models has enabled prompting approaches as an alternative to data-rehearsal. These approaches rely on a key-query mechanism to generate prompts and have been found to be highly resistant to catastrophic forgetting in the well-established rehearsal-free continual learning setting. However, the key mechanism of these methods is not trained end-to-end with the task sequence. Our experiments show that this leads to a reduction in their plasticity, hence sacrificing new task accuracy, and inability to benefit from expanded parameter capacity. We instead propose to learn a set of prompt components which are ass
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#25345;&#32493;&#30340;&#26080;&#25968;&#25454;&#32467;&#26500;&#21270;VL&#27010;&#24565;&#23398;&#20064;&#65288;ConStruct-VL&#65289;&#22522;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;VL&#27169;&#22411;&#22312;&#32467;&#26500;&#21270;VL&#27010;&#24565;&#25512;&#29702;&#26041;&#38754;&#30340;&#29942;&#39048;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;-free&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.09790</link><description>&lt;p&gt;
ConStruct-VL: &#26080;&#38656;&#25968;&#25454;&#30340;&#25345;&#32493;&#32467;&#26500;&#21270;&#35270;&#35273;&#35821;&#35328;&#27010;&#24565;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ConStruct-VL: Data-Free Continual Structured VL Concepts Learning. (arXiv:2211.09790v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09790
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#25345;&#32493;&#30340;&#26080;&#25968;&#25454;&#32467;&#26500;&#21270;VL&#27010;&#24565;&#23398;&#20064;&#65288;ConStruct-VL&#65289;&#22522;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;VL&#27169;&#22411;&#22312;&#32467;&#26500;&#21270;VL&#27010;&#24565;&#25512;&#29702;&#26041;&#38754;&#30340;&#29942;&#39048;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;-free&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#35268;&#27169;&#39044;&#20808;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#22312;&#35768;&#22810;&#38646;&#26679;&#26412;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#36890;&#36807;&#20165;&#21253;&#21547;&#30701;&#25991;&#26412;&#25552;&#31034;&#30340;&#23450;&#20041;&#26469;&#35782;&#21035;&#29289;&#20307;&#65292;&#24182;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#20063;&#24050;&#32463;&#34920;&#26126;&#65292;VL&#27169;&#22411;&#22312;&#32467;&#26500;&#21270;VL&#27010;&#24565;&#25512;&#29702;&#26041;&#38754;&#20173;&#28982;&#24456;&#33030;&#24369;&#65292;&#20363;&#22914;&#35782;&#21035;&#29289;&#20307;&#23646;&#24615;&#12289;&#29366;&#24577;&#21644;&#29289;&#20307;&#38388;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#36825;&#23548;&#33268;&#25512;&#29702;&#38169;&#35823;&#65292;&#38656;&#35201;&#36890;&#36807;&#25945;&#25480;VL&#27169;&#22411;&#32570;&#22833;&#30340;SVLC&#25216;&#33021;&#26469;&#36827;&#34892;&#26356;&#27491;&#65307;&#36890;&#24120;&#24517;&#39035;&#20351;&#29992;&#21457;&#29616;&#38382;&#39064;&#30340;&#31169;&#26377;&#25968;&#25454;&#26469;&#23436;&#25104;&#36825;&#19968;&#28857;&#65292;&#36825;&#33258;&#28982;&#32780;&#28982;&#22320;&#23548;&#33268;&#20102;&#19968;&#20010;&#26080;&#38656;&#20219;&#21153;ID&#30340;&#26080;&#25968;&#25454;&#25345;&#32493;VL&#23398;&#20064;&#35774;&#32622;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#25345;&#32493;&#30340;&#26080;&#25968;&#25454;&#32467;&#26500;&#21270;VL&#27010;&#24565;&#23398;&#20064;&#65288;ConStruct-VL&#65289;&#22522;&#20934;&#65292;&#24182;&#34920;&#26126;&#23427;&#23545;&#35768;&#22810;&#29616;&#26377;&#30340;&#26080;&#25968;&#25454;CL&#31574;&#30053;&#37117;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large-scale pre-trained Vision-and-Language (VL) foundation models have demonstrated remarkable capabilities in many zero-shot downstream tasks, achieving competitive results for recognizing objects defined by as little as short text prompts. However, it has also been shown that VL models are still brittle in Structured VL Concept (SVLC) reasoning, such as the ability to recognize object attributes, states, and inter-object relations. This leads to reasoning mistakes, which need to be corrected as they occur by teaching VL models the missing SVLC skills; often this must be done using private data where the issue was found, which naturally leads to a data-free continual (no task-id) VL learning setting. In this work, we introduce the first Continual Data-Free Structured VL Concepts Learning (ConStruct-VL) benchmark and show it is challenging for many existing data-free CL strategies. We, therefore, propose a data-free method comprised of a new approach of Adversarial Pseudo-Re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#25233;&#37057;&#30151;&#29366;&#26816;&#27979;&#20998;&#31867;&#22120;&#65292;&#20174;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#25552;&#21462;&#20020;&#24202;&#30456;&#20851;&#29305;&#24449;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#27169;&#22411;&#29992;&#20110;&#26816;&#27979;&#29992;&#25143;&#30340;&#20020;&#24202;&#25233;&#37057;&#30151;&#65292;&#36890;&#36807;&#25552;&#20379;&#19981;&#21516;&#26102;&#38388;&#31890;&#24230;&#30340;&#20934;&#30830;&#24230;&#24230;&#37327;&#26469;&#35780;&#20272;&#35813;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2211.07717</link><description>&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#28145;&#24230;&#26102;&#38388;&#24314;&#27169;&#22312;&#20020;&#24202;&#25233;&#37057;&#30151;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Temporal Modelling of Clinical Depression through Social Media Text. (arXiv:2211.07717v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#25233;&#37057;&#30151;&#29366;&#26816;&#27979;&#20998;&#31867;&#22120;&#65292;&#20174;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#25552;&#21462;&#20020;&#24202;&#30456;&#20851;&#29305;&#24449;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#27169;&#22411;&#29992;&#20110;&#26816;&#27979;&#29992;&#25143;&#30340;&#20020;&#24202;&#25233;&#37057;&#30151;&#65292;&#36890;&#36807;&#25552;&#20379;&#19981;&#21516;&#26102;&#38388;&#31890;&#24230;&#30340;&#20934;&#30830;&#24230;&#24230;&#37327;&#26469;&#35780;&#20272;&#35813;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#29992;&#25143;&#26102;&#38388;&#36724;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#29992;&#25143;&#30340;&#20020;&#24202;&#25233;&#37057;&#30151;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#20102;&#25233;&#37057;&#30151;&#29366;&#26816;&#27979;&#65288;DSD&#65289;&#20998;&#31867;&#22120;&#65292;&#35813;&#20998;&#31867;&#22120;&#22522;&#20110;&#26368;&#22823;&#25968;&#37327;&#30340;&#24050;&#32463;&#36807;&#20020;&#24202;&#21307;&#24072;&#27880;&#37322;&#30340;&#25512;&#25991;&#12290;&#25105;&#20204;&#38543;&#21518;&#20351;&#29992;&#25105;&#20204;&#30340;DSD&#27169;&#22411;&#26469;&#25552;&#21462;&#20020;&#24202;&#30456;&#20851;&#29305;&#24449;&#65292;&#20363;&#22914;&#25233;&#37057;&#30151;&#35780;&#20998;&#21450;&#20854;&#38543;&#21518;&#30340;&#26102;&#38388;&#27169;&#24335;&#65292;&#20197;&#21450;&#29992;&#25143;&#21457;&#24067;&#27963;&#21160;&#27169;&#24335;&#65292;&#20363;&#22914;&#37327;&#21270;&#20182;&#20204;&#30340;&#8220;&#26080;&#27963;&#21160;&#8221;&#25110;&#8220;&#27785;&#40664;&#8221;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#25552;&#21462;&#29305;&#24449;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19977;&#31181;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#26469;&#33258;&#20004;&#20010;&#29616;&#26377;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#29992;&#25143;&#32423;&#21035;&#25233;&#37057;&#30151;&#26816;&#27979;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22522;&#20110;&#21333;&#20010;&#29305;&#24449;&#12289;&#22522;&#32447;&#29305;&#24449;&#21644;&#29305;&#24449;&#21066;&#20943;&#27979;&#35797;&#30340;&#20934;&#30830;&#24230;&#24230;&#37327;&#65292;&#22312;&#19981;&#21516;&#26102;&#38388;&#31890;&#24230;&#30340;&#20960;&#20010;&#32423;&#21035;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe the development of a model to detect user-level clinical depression based on a user's temporal social media posts. Our model uses a Depression Symptoms Detection (DSD) classifier, which is trained on the largest existing samples of clinician annotated tweets for clinical depression symptoms. We subsequently use our DSD model to extract clinically relevant features, e.g., depression scores and their consequent temporal patterns, as well as user posting activity patterns, e.g., quantifying their ``no activity'' or ``silence.'' Furthermore, to evaluate the efficacy of these extracted features, we create three kinds of datasets including a test dataset, from two existing well-known benchmark datasets for user-level depression detection. We then provide accuracy measures based on single features, baseline features and feature ablation tests, at several different levels of temporal granularity. The relevant data distributions and clinical depression detection related settings can
&lt;/p&gt;</description></item><item><title>&#24212;&#29992;&#24211;&#26222;&#26364;&#31639;&#23376;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#23398;&#20064;&#37327;&#23376;&#31995;&#32479;&#30340;&#28436;&#21270;&#21644;&#21487;&#35266;&#27979;&#37327;&#65292;&#21516;&#26102;&#25512;&#26029;&#20986;&#24213;&#23618;&#21160;&#21147;&#23398;&#30340;&#23545;&#31216;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.06678</link><description>&lt;p&gt;
&#23398;&#20064;&#21160;&#24577;&#31995;&#32479;&#65306;&#26469;&#33258;&#24320;&#25918;&#37327;&#23376;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#20030;&#20363;
&lt;/p&gt;
&lt;p&gt;
Learning dynamical systems: an example from open quantum system dynamics. (arXiv:2211.06678v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06678
&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#24211;&#26222;&#26364;&#31639;&#23376;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#23398;&#20064;&#37327;&#23376;&#31995;&#32479;&#30340;&#28436;&#21270;&#21644;&#21487;&#35266;&#27979;&#37327;&#65292;&#21516;&#26102;&#25512;&#26029;&#20986;&#24213;&#23618;&#21160;&#21147;&#23398;&#30340;&#23545;&#31216;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#29992;&#20110;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#21160;&#24577;&#31995;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#29992;&#20110;&#39044;&#27979;&#12289;&#25511;&#21046;&#21644;&#35299;&#37322;&#35266;&#23519;&#21040;&#30340;&#21160;&#24577;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20030;&#20363;&#35828;&#26126;&#20102;&#20854;&#20013;&#19968;&#31181;&#31639;&#27861;&#65292;&#21363;&#24211;&#26222;&#26364;&#31639;&#23376;&#23398;&#20064;&#65292;&#24212;&#29992;&#20110;&#24320;&#25918;&#37327;&#23376;&#31995;&#32479;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#23558;&#30740;&#31350;&#30001;&#36864;&#30456;&#24178;&#38376;&#32806;&#21512;&#30340;&#23567;&#33258;&#26059;&#38142;&#30340;&#21160;&#24577;&#65292;&#24182;&#23637;&#31034;&#20102;&#24211;&#26222;&#26364;&#31639;&#23376;&#23398;&#20064;&#22914;&#20309;&#39640;&#25928;&#22320;&#23398;&#20064;&#23494;&#24230;&#30697;&#38453;&#30340;&#28436;&#21270;&#65292;&#20197;&#21450;&#19982;&#31995;&#32479;&#30456;&#20851;&#30340;&#27599;&#20010;&#29289;&#29702;&#21487;&#35266;&#23519;&#37327;&#30340;&#28436;&#21270;&#12290;&#26368;&#21518;&#65292;&#21033;&#29992;&#23398;&#20064;&#30340;&#24211;&#26222;&#26364;&#31639;&#23376;&#30340;&#35889;&#20998;&#35299;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#30452;&#25509;&#20174;&#25968;&#25454;&#25512;&#26029;&#20986;&#24213;&#23618;&#21160;&#21147;&#23398;&#36981;&#24490;&#30340;&#23545;&#31216;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms designed to learn dynamical systems from data can be used to forecast, control and interpret the observed dynamics. In this work we exemplify the use of one of such algorithms, namely Koopman operator learning, in the context of open quantum system dynamics. We will study the dynamics of a small spin chain coupled with dephasing gates and show how Koopman operator learning is an approach to efficiently learn not only the evolution of the density matrix, but also of every physical observable associated to the system. Finally, leveraging the spectral decomposition of the learned Koopman operator, we show how symmetries obeyed by the underlying dynamics can be inferred directly from data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31867;&#22522;&#20110;&#21160;&#37327;&#30340;&#26041;&#24046;&#32553;&#20943;&#21644;&#26412;&#22320;SGD&#25216;&#26415;&#30340;&#26356;&#24555;&#30340;&#32852;&#37030;&#32452;&#21512;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#20984;&#20998;&#24067;&#24335;&#32452;&#21512;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#33258;&#36866;&#24212;&#30697;&#38453;&#28789;&#27963;&#22320;&#32467;&#21512;&#21508;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#12290;</title><link>http://arxiv.org/abs/2211.01883</link><description>&lt;p&gt;
&#36866;&#24212;&#24615;&#21160;&#37327;&#30340;&#24555;&#36895;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#20998;&#24067;&#24335;&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Faster Adaptive Momentum-Based Federated Methods for Distributed Composition Optimization. (arXiv:2211.01883v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31867;&#22522;&#20110;&#21160;&#37327;&#30340;&#26041;&#24046;&#32553;&#20943;&#21644;&#26412;&#22320;SGD&#25216;&#26415;&#30340;&#26356;&#24555;&#30340;&#32852;&#37030;&#32452;&#21512;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#20984;&#20998;&#24067;&#24335;&#32452;&#21512;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#33258;&#36866;&#24212;&#30697;&#38453;&#28789;&#27963;&#22320;&#32467;&#21512;&#21508;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#31181;&#27969;&#34892;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#33539;&#24335;&#12290;&#32452;&#21512;&#20248;&#21270;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#20998;&#23618;&#23398;&#20064;&#27169;&#22411;&#65292;&#20986;&#29616;&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#22914;&#20803;&#23398;&#20064;&#21644;&#40065;&#26834;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#32852;&#37030;&#32452;&#21512;&#20248;&#21270;&#31639;&#27861;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#39640;&#37319;&#26679;&#21644;&#36890;&#20449;&#22797;&#26434;&#24230;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31867;&#26356;&#24555;&#30340;&#32852;&#37030;&#32452;&#21512;&#20248;&#21270;&#31639;&#27861;&#65288;&#21363;MFCGD&#21644;AdaMFCGD&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#20984;&#20998;&#24067;&#24335;&#32452;&#21512;&#38382;&#39064;&#65292;&#20854;&#22522;&#20110;&#21160;&#37327;&#30340;&#26041;&#24046;&#32553;&#20943;&#21644;&#26412;&#22320;SGD&#25216;&#26415;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#65288;&#21363;AdaMFCGD&#65289;&#20351;&#29992;&#32479;&#19968;&#30340;&#33258;&#36866;&#24212;&#30697;&#38453;&#65292;&#28789;&#27963;&#22320;&#32467;&#21512;&#21508;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#35774;&#32622;&#19979;&#36827;&#34892;&#20102;&#22362;&#23454;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#33719;&#24471;&#26356;&#20302;&#30340;&#37319;&#26679;&#21644;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning is a popular distributed learning paradigm in machine learning. Meanwhile, composition optimization is an effective hierarchical learning model, which appears in many machine learning applications such as meta learning and robust learning. More recently, although a few federated composition optimization algorithms have been proposed, they still suffer from high sample and communication complexities. In the paper, thus, we propose a class of faster federated compositional optimization algorithms (i.e., MFCGD and AdaMFCGD) to solve the nonconvex distributed composition problems, which builds on the momentum-based variance reduced and local-SGD techniques. In particular, our adaptive algorithm (i.e., AdaMFCGD) uses a unified adaptive matrix to flexibly incorporate various adaptive learning rates. Moreover, we provide a solid theoretical analysis for our algorithms under non-i.i.d. setting, and prove our algorithms obtain a lower sample and communication complexities sim
&lt;/p&gt;</description></item><item><title>MAgNET&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#22522;&#20110;MAg&#65288;&#22810;&#36890;&#36947;&#32858;&#21512;&#65289;&#25805;&#20316;&#65292;&#37319;&#29992;&#22270;&#24418;U-Net&#26550;&#26500;&#22788;&#29702;&#20219;&#24847;&#32467;&#26500;&#65288;&#22270;&#24418;&#25968;&#25454;&#65289;&#30340;&#22823;&#32500;&#25968;&#25454;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#22788;&#29702;&#20219;&#24847;&#22797;&#26434;&#30340;&#32593;&#26684;&#12290;</title><link>http://arxiv.org/abs/2211.00713</link><description>&lt;p&gt;
MAgNET: &#38754;&#21521;&#22522;&#20110;&#32593;&#26684;&#27169;&#25311;&#30340;&#22270;&#24418;U-Net&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
MAgNET: A Graph U-Net Architecture for Mesh-Based Simulations. (arXiv:2211.00713v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00713
&lt;/p&gt;
&lt;p&gt;
MAgNET&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#22522;&#20110;MAg&#65288;&#22810;&#36890;&#36947;&#32858;&#21512;&#65289;&#25805;&#20316;&#65292;&#37319;&#29992;&#22270;&#24418;U-Net&#26550;&#26500;&#22788;&#29702;&#20219;&#24847;&#32467;&#26500;&#65288;&#22270;&#24418;&#25968;&#25454;&#65289;&#30340;&#22823;&#32500;&#25968;&#25454;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#22788;&#29702;&#20219;&#24847;&#22797;&#26434;&#30340;&#32593;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23574;&#31471;&#24212;&#29992;&#20013;&#65292;&#39640;&#20445;&#30495;&#35745;&#31639;&#27169;&#22411;&#34987;&#35777;&#26126;&#36895;&#24230;&#22826;&#24930;&#32780;&#19981;&#20999;&#23454;&#38469;&#65292;&#22240;&#27492;&#34987;&#26356;&#24555;&#30340;&#20195;&#29702;&#27169;&#22411;&#25152;&#26367;&#20195;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#21152;&#36895;&#36825;&#31181;&#39044;&#27979;&#26041;&#38754;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;&#26356;&#22823;&#26356;&#22797;&#26434;&#30340;&#38382;&#39064;&#26102;&#65292;&#23427;&#20204;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;MAgNET&#65306;&#22810;&#36890;&#36947;&#32858;&#21512;&#32593;&#32476;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#22788;&#29702;&#20219;&#24847;&#32467;&#26500;&#65288;&#22270;&#24418;&#25968;&#25454;&#65289;&#30340;&#22823;&#32500;&#25968;&#25454;&#12290;MAgNET&#24314;&#31435;&#22312;MAg&#65288;&#22810;&#36890;&#36947;&#32858;&#21512;&#65289;&#25805;&#20316;&#20043;&#19978;&#65292;&#23427;&#23558;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#22810;&#36890;&#36947;&#26412;&#22320;&#25805;&#20316;&#30340;&#27010;&#24565;&#25512;&#24191;&#21040;&#20219;&#24847;&#38750;&#32593;&#26684;&#36755;&#20837;&#12290;MAg&#23618;&#19982;&#25152;&#25552;&#20986;&#30340;&#26032;&#22411;&#22270;&#24418;&#27744;&#21270;/&#21453;&#27744;&#21270;&#25805;&#20316;&#20132;&#38169;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#22270;&#24418;U-Net&#26550;&#26500;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#22788;&#29702;&#20219;&#24847;&#22797;&#26434;&#30340;&#32593;&#26684;&#65292;&#24182;&#23545;&#22823;&#32500;&#22270;&#24418;&#32467;&#26500;&#25968;&#25454;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many cutting-edge applications, high-fidelity computational models prove too slow to be practical and are thus replaced by much faster surrogate models. Recently, deep learning techniques have become increasingly important in accelerating such predictions. However, they tend to falter when faced with larger and more complex problems. Therefore, this work introduces MAgNET: Multi-channel Aggregation Network, a novel geometric deep learning framework designed to operate on large-dimensional data of arbitrary structure (graph data). MAgNET is built upon the MAg (Multichannel Aggregation) operation, which generalizes the concept of multi-channel local operations in convolutional neural networks to arbitrary non-grid inputs. The MAg layers are interleaved with the proposed novel graph pooling/unpooling operations to form a graph U-Net architecture that is robust and can handle arbitrary complex meshes, efficiently performing supervised learning on large-dimensional graph-structured data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#21160;&#24577;&#29305;&#24449;&#12289;&#39057;&#29575;&#21644;&#26102;&#38388;&#20869;&#23481;&#25552;&#21462;&#26032;&#30340;&#30149;&#24773;&#29305;&#24449;&#65292;&#22312;&#24085;&#37329;&#26862;&#30149;&#30340;EEG&#20449;&#21495;&#35786;&#26029;&#20013;&#33719;&#24471;&#20102;94&#65285;&#30340;&#20934;&#30830;&#24615;&#12289;96&#65285;&#30340;&#28789;&#25935;&#24230;&#21644;92&#65285;&#30340;&#29305;&#24322;&#24230;&#12290;</title><link>http://arxiv.org/abs/2210.11624</link><description>&lt;p&gt;
&#31232;&#30095;&#21160;&#24577;&#29305;&#24449;&#29983;&#25104;&#65292;&#24212;&#29992;&#20110;&#24085;&#37329;&#26862;&#30149;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Sparse Dynamical Features generation, application to Parkinson's Disease diagnosis. (arXiv:2210.11624v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#21160;&#24577;&#29305;&#24449;&#12289;&#39057;&#29575;&#21644;&#26102;&#38388;&#20869;&#23481;&#25552;&#21462;&#26032;&#30340;&#30149;&#24773;&#29305;&#24449;&#65292;&#22312;&#24085;&#37329;&#26862;&#30149;&#30340;EEG&#20449;&#21495;&#35786;&#26029;&#20013;&#33719;&#24471;&#20102;94&#65285;&#30340;&#20934;&#30830;&#24615;&#12289;96&#65285;&#30340;&#28789;&#25935;&#24230;&#21644;92&#65285;&#30340;&#29305;&#24322;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#22522;&#20110;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#30340;&#24085;&#37329;&#26862;&#30149;&#65288;PD&#65289;&#35786;&#26029;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#22823;&#33041;&#21151;&#33021;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;EEG&#20449;&#21495;&#30340;&#21160;&#24577;&#29305;&#24449;&#12289;&#39057;&#29575;&#21644;&#26102;&#38388;&#20869;&#23481;&#25552;&#21462;&#26032;&#30340;&#30149;&#24773;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;50&#20010;&#34987;&#35797;&#20013;&#30340;25&#20010;PD&#24739;&#32773;&#30340;EEG&#20449;&#21495;&#65292;&#36825;&#20123;&#20449;&#21495;&#35760;&#24405;&#20102;&#19968;&#20010;3-oddball&#21548;&#35273;&#20219;&#21153;&#12290;&#36890;&#36807;&#25552;&#21462;&#20004;&#20010;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#65288;LDA&#65289;&#20998;&#31867;&#22120;&#23558;&#20854;&#20998;&#24320;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#21333;&#19968;&#36890;&#36947;&#20197;90&#65285;&#30340;&#20934;&#30830;&#29575;&#65288;$ p &lt;0.03 $&#65289;&#23558;&#20581;&#24247;&#21463;&#35797;&#32773;&#19982;&#19981;&#20581;&#24247;&#21463;&#35797;&#32773;&#20998;&#24320;&#12290;&#36890;&#36807;&#32858;&#21512;&#19977;&#20010;&#36890;&#36947;&#30340;&#20449;&#24687;&#24182;&#36827;&#34892;&#25237;&#31080;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;94&#65285;&#30340;&#20934;&#30830;&#24615;&#65292;96&#65285;&#30340;&#28789;&#25935;&#24230;&#21644;92&#65285;&#30340;&#29305;&#24322;&#24230;&#12290;&#35780;&#20272;&#26159;&#20351;&#29992;&#23884;&#22871;Leave-One-Out&#20132;&#21449;&#39564;&#35777;&#36807;&#31243;&#36827;&#34892;&#30340;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#25968;&#25454;&#27844;&#28431;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study we focus on the diagnosis of Parkinson's Disease (PD) based on electroencephalogram (EEG) signals. We propose a new approach inspired by the functioning of the brain that uses the dynamics, frequency and temporal content of EEGs to extract new demarcating features of the disease. The method was evaluated on a publicly available dataset containing EEG signals recorded during a 3-oddball auditory task involving N = 50 subjects, of whom 25 suffer from PD. By extracting two features, and separating them with a straight line using a Linear Discriminant Analysis (LDA) classifier, we can separate the healthy from the unhealthy subjects with an accuracy of 90 % $(p &lt; 0.03)$ using a single channel. By aggregating the information from three channels and making them vote, we obtain an accuracy of 94 %, a sensitivity of 96 % and a specificity of 92 %. The evaluation was carried out using a nested Leave-One-Out cross-validation procedure, thus preventing data leakage problems and givi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#30001;&#22240;&#26524;&#24615;&#24674;&#22797;&#30340;&#26041;&#24335;&#26469;&#20135;&#29983;&#26465;&#20214;&#20998;&#24067;&#19979;&#30340;&#29420;&#31435;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2210.10179</link><description>&lt;p&gt;
&#32463;&#30001;&#22240;&#26524;&#24615;&#24674;&#22797;&#22312;&#26465;&#20214;&#21160;&#21147;&#23398;&#20013;&#36827;&#34892;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Inference in conditioned dynamics through causality restoration. (arXiv:2210.10179v2 [physics.data-an] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#30001;&#22240;&#26524;&#24615;&#24674;&#22797;&#30340;&#26041;&#24335;&#26469;&#20135;&#29983;&#26465;&#20214;&#20998;&#24067;&#19979;&#30340;&#29420;&#31435;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#26377;&#26465;&#20214;&#30340;&#21160;&#21147;&#23398;&#20013;&#35745;&#31639;&#21487;&#35266;&#27979;&#37327;&#36890;&#24120;&#26159;&#35745;&#31639;&#19978;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#34429;&#28982;&#20174;&#38750;&#26465;&#20214;&#30340;&#21160;&#21147;&#23398;&#20013;&#39640;&#25928;&#22320;&#33719;&#21462;&#29420;&#31435;&#26679;&#26412;&#36890;&#24120;&#26159;&#21487;&#34892;&#30340;&#65292;&#20294;&#36890;&#24120;&#24517;&#39035;&#20002;&#24323;&#22823;&#37096;&#20998;&#26679;&#26412;(&#20197;&#19968;&#31181;&#37325;&#35201;&#24615;&#25277;&#26679;&#30340;&#24418;&#24335;)&#22240;&#20026;&#23427;&#20204;&#19981;&#28385;&#36275;&#25152;&#26045;&#21152;&#30340;&#26465;&#20214;&#12290;&#30452;&#25509;&#20174;&#26377;&#26465;&#20214;&#30340;&#20998;&#24067;&#20013;&#25277;&#26679;&#26159;&#19981;&#26131;&#30340;&#65292;&#22240;&#20026;&#26465;&#20214;&#25171;&#30772;&#20102;&#21160;&#21147;&#23398;&#30340;&#22240;&#26524;&#29305;&#24615;&#65292;&#26368;&#32456;&#20351;&#25277;&#26679;&#36807;&#31243;&#21464;&#24471;&#20302;&#25928;&#12290;&#19968;&#31181;&#26631;&#20934;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;Metropolis Monte-Carlo&#36807;&#31243;&#23454;&#29616;&#65292;&#20294;&#36825;&#20010;&#36807;&#31243;&#36890;&#24120;&#24456;&#24930;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;Monte-Carlo&#27493;&#39588;&#26469;&#33719;&#24471;&#23569;&#37327;&#30340;&#32479;&#35745;&#29420;&#31435;&#26679;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#26377;&#26465;&#20214;&#30340;&#20998;&#24067;&#20013;&#20135;&#29983;&#29420;&#31435;&#30340;&#26679;&#26412;&#12290;&#35813;&#26041;&#27861;&#23398;&#20064;&#19968;&#20010;&#24191;&#20041;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#35813;&#27169;&#22411;&#26368;&#20248;&#22320;&#25551;&#36848;&#20102;&#26465;&#20214;&#20998;&#24067;&#30340;&#21464;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computing observables from conditioned dynamics is typically computationally hard, because, although obtaining independent samples efficiently from the unconditioned dynamics is usually feasible, generally most of the samples must be discarded (in a form of importance sampling) because they do not satisfy the imposed conditions. Sampling directly from the conditioned distribution is non-trivial, as conditioning breaks the causal properties of the dynamics which ultimately renders the sampling procedure efficient. One standard way of achieving it is through a Metropolis Monte-Carlo procedure, but this procedure is normally slow and a very large number of Monte-Carlo steps is needed to obtain a small number of statistically independent samples. In this work, we propose an alternative method to produce independent samples from a conditioned distribution. The method learns the parameters of a generalized dynamical model that optimally describe the conditioned distribution in a variational 
&lt;/p&gt;</description></item><item><title>Packed-Ensembles&#26159;&#19968;&#31181;&#33021;&#22815;&#22312;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#20869;&#36816;&#34892;&#30340;&#36731;&#37327;&#32423;&#32467;&#26500;&#21270;&#38598;&#21512;&#65292;&#23427;&#36890;&#36807;&#31934;&#24515;&#35843;&#33410;&#32534;&#30721;&#31354;&#38388;&#30340;&#32500;&#24230;&#26469;&#35774;&#35745;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#25439;&#22833;&#25928;&#26524;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2210.09184</link><description>&lt;p&gt;
&#32039;&#20945;&#38598;&#25104;&#29992;&#20110;&#39640;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Packed-Ensembles for Efficient Uncertainty Estimation. (arXiv:2210.09184v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09184
&lt;/p&gt;
&lt;p&gt;
Packed-Ensembles&#26159;&#19968;&#31181;&#33021;&#22815;&#22312;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#20869;&#36816;&#34892;&#30340;&#36731;&#37327;&#32423;&#32467;&#26500;&#21270;&#38598;&#21512;&#65292;&#23427;&#36890;&#36807;&#31934;&#24515;&#35843;&#33410;&#32534;&#30721;&#31354;&#38388;&#30340;&#32500;&#24230;&#26469;&#35774;&#35745;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#25439;&#22833;&#25928;&#26524;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#38598;&#25104;&#26159;&#23454;&#29616;&#20851;&#38190;&#25351;&#26631;&#65288;&#22914;&#20934;&#30830;&#24615;&#12289;&#26657;&#20934;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#65289;&#21331;&#36234;&#24615;&#33021;&#30340;&#31361;&#20986;&#26041;&#27861;&#12290;&#20294;&#26159;&#65292;&#29616;&#23454;&#31995;&#32479;&#30340;&#30828;&#20214;&#38480;&#21046;&#38480;&#21046;&#20102;&#26356;&#23567;&#30340;&#38598;&#21512;&#21644;&#36739;&#20302;&#23481;&#37327;&#30340;&#32593;&#32476;&#65292;&#20005;&#37325;&#25439;&#23475;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#21644;&#23646;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;Packed-Ensembles&#65288;PE&#65289;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#31934;&#24515;&#35843;&#33410;&#20854;&#32534;&#30721;&#31354;&#38388;&#30340;&#32500;&#24230;&#26469;&#35774;&#35745;&#21644;&#35757;&#32451;&#36731;&#37327;&#32423;&#32467;&#26500;&#21270;&#38598;&#21512;&#12290;&#25105;&#20204;&#21033;&#29992;&#32452;&#21367;&#31215;&#23558;&#38598;&#21512;&#24182;&#34892;&#21270;&#20026;&#21333;&#20010;&#20849;&#20139;&#39592;&#24178;&#65292;&#24182;&#36827;&#34892;&#21069;&#21521;&#20256;&#36882;&#20197;&#25552;&#39640;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;PE&#26088;&#22312;&#22312;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#23384;&#38480;&#21046;&#20869;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Ensembles (DE) are a prominent approach for achieving excellent performance on key metrics such as accuracy, calibration, uncertainty estimation, and out-of-distribution detection. However, hardware limitations of real-world systems constrain to smaller ensembles and lower-capacity networks, significantly deteriorating their performance and properties. We introduce Packed-Ensembles (PE), a strategy to design and train lightweight structured ensembles by carefully modulating the dimension of their encoding space. We leverage grouped convolutions to parallelize the ensemble into a single shared backbone and forward pass to improve training and inference speeds. PE is designed to operate within the memory limits of a standard neural network. Our extensive research indicates that PE accurately preserves the properties of DE, such as diversity, and performs equally well in terms of accuracy, calibration, out-of-distribution detection, and robustness to distribution shift. We make our c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#38598;&#29256;&#26435;&#20445;&#25252;&#30340;&#26080;&#23475;&#21644;&#38544;&#34109;&#30340;&#26080;&#30446;&#26631;&#21518;&#38376;&#27700;&#21360;&#26041;&#26696;&#65292;&#21487;&#20197;&#36798;&#21040;&#19982;&#26368;&#20808;&#36827;&#26041;&#26696;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#27700;&#21360;&#25928;&#26524;&#65292;&#24182;&#35777;&#26126;&#23545;&#27169;&#22411;&#24615;&#33021;&#26080;&#23475;&#19988;&#38544;&#34109;&#12290;</title><link>http://arxiv.org/abs/2210.00875</link><description>&lt;p&gt;
&#26080;&#30446;&#26631;&#21518;&#38376;&#27700;&#21360;&#65306;&#26397;&#30528;&#26080;&#23475;&#21644;&#38544;&#34109;&#30340;&#25968;&#25454;&#38598;&#29256;&#26435;&#20445;&#25252;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Untargeted Backdoor Watermark: Towards Harmless and Stealthy Dataset Copyright Protection. (arXiv:2210.00875v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#38598;&#29256;&#26435;&#20445;&#25252;&#30340;&#26080;&#23475;&#21644;&#38544;&#34109;&#30340;&#26080;&#30446;&#26631;&#21518;&#38376;&#27700;&#21360;&#26041;&#26696;&#65292;&#21487;&#20197;&#36798;&#21040;&#19982;&#26368;&#20808;&#36827;&#26041;&#26696;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#27700;&#21360;&#25928;&#26524;&#65292;&#24182;&#35777;&#26126;&#23545;&#27169;&#22411;&#24615;&#33021;&#26080;&#23475;&#19988;&#38544;&#34109;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#22312;&#23454;&#36341;&#20013;&#23637;&#29616;&#20986;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;&#21487;&#20197;&#35828;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24555;&#36895;&#21457;&#23637;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24471;&#30410;&#20110;&#39640;&#36136;&#37327;&#65288;&#24320;&#28304;&#65289;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#21487;&#20197;&#22312;&#27492;&#22522;&#30784;&#19978;&#36731;&#26494;&#22320;&#35780;&#20272;&#21644;&#25913;&#36827;&#20182;&#20204;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#30001;&#20110;&#25968;&#25454;&#25910;&#38598;&#36890;&#24120;&#26159;&#32791;&#26102;&#29978;&#33267;&#26114;&#36149;&#30340;&#65292;&#22914;&#20309;&#20445;&#25252;&#20854;&#29256;&#26435;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#24182;&#20540;&#24471;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#25968;&#25454;&#38598;&#30340;&#25152;&#26377;&#26435;&#39564;&#35777;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#39564;&#35777;&#26041;&#27861;&#30001;&#20110;&#26377;&#30446;&#26631;&#30340;&#21518;&#38376;&#27700;&#21360;&#30340;&#29305;&#24615;&#65292;&#20250;&#22312;&#21463;&#20445;&#25252;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#24341;&#20837;&#26032;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26080;&#30446;&#26631;&#21518;&#38376;&#27700;&#21360;&#26041;&#26696;&#65292;&#20854;&#20013;&#24322;&#24120;&#30340;&#27169;&#22411;&#34892;&#20026;&#19981;&#26159;&#30830;&#23450;&#24615;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#20010;&#20998;&#25955;&#24230;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#30456;&#20851;&#24615;&#65292;&#22522;&#20110;&#27492;&#25105;&#20204;&#22312;&#21463;&#27745;&#26579;&#26631;&#31614;&#21644;&#24178;&#20928;&#26631;&#31614;&#25915;&#20987;&#35774;&#32622;&#19979;&#35774;&#35745;&#20102;&#26080;&#30446;&#26631;&#21518;&#38376;&#27700;&#21360;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#27700;&#21360;&#25552;&#21462;&#30340;&#20934;&#30830;&#24615;&#21644;&#27169;&#22411;&#24615;&#33021;&#19978;&#37117;&#33021;&#22815;&#36798;&#21040;&#29978;&#33267;&#36229;&#36807;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36824;&#34987;&#35777;&#26126;&#23545;&#27169;&#22411;&#24615;&#33021;&#26080;&#23475;&#19988;&#38544;&#34109;&#65292;&#19981;&#20250;&#24341;&#20837;&#20219;&#20309;&#21487;&#26816;&#27979;&#30340;&#25197;&#26354;&#25110;&#25925;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have demonstrated their superiority in practice. Arguably, the rapid development of DNNs is largely benefited from high-quality (open-sourced) datasets, based on which researchers and developers can easily evaluate and improve their learning methods. Since the data collection is usually time-consuming or even expensive, how to protect their copyrights is of great significance and worth further exploration. In this paper, we revisit dataset ownership verification. We find that existing verification methods introduced new security risks in DNNs trained on the protected dataset, due to the targeted nature of poison-only backdoor watermarks. To alleviate this problem, in this work, we explore the untargeted backdoor watermarking scheme, where the abnormal model behaviors are not deterministic. Specifically, we introduce two dispersibilities and prove their correlation, based on which we design the untargeted backdoor watermark under both poisoned-label and clean
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#32534;&#30721;&#35299;&#30721;&#32593;&#32476;&#32467;&#26500;&#65292;&#21517;&#20026;TDANet&#65292;&#23427;&#36890;&#36807;&#27169;&#25311;&#22823;&#33041;&#30340;&#33258;&#19978;&#32780;&#19979;&#27880;&#24847;&#21147;&#26469;&#38477;&#20302;&#27169;&#22411;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#35821;&#38899;&#20998;&#31163;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2209.15200</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#33258;&#19978;&#32780;&#19979;&#27880;&#24847;&#21147;&#30340;&#39640;&#25928;&#32534;&#30721;&#35299;&#30721;&#32467;&#26500;&#29992;&#20110;&#35821;&#38899;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
An efficient encoder-decoder architecture with top-down attention for speech separation. (arXiv:2209.15200v5 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#32534;&#30721;&#35299;&#30721;&#32593;&#32476;&#32467;&#26500;&#65292;&#21517;&#20026;TDANet&#65292;&#23427;&#36890;&#36807;&#27169;&#25311;&#22823;&#33041;&#30340;&#33258;&#19978;&#32780;&#19979;&#27880;&#24847;&#21147;&#26469;&#38477;&#20302;&#27169;&#22411;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#35821;&#38899;&#20998;&#31163;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35821;&#38899;&#20998;&#31163;&#20219;&#21153;&#20013;&#24050;&#32463;&#26174;&#31034;&#20986;&#20248;&#31168;&#30340;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#22312;&#20445;&#25345;&#20302;&#27169;&#22411;&#22797;&#26434;&#24230;&#30340;&#21516;&#26102;&#33719;&#24471;&#33391;&#22909;&#32467;&#26524;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#22823;&#33041;&#30340;&#33258;&#19978;&#32780;&#19979;&#27880;&#24847;&#21147;&#25552;&#20379;&#20102;&#19968;&#31181;&#29983;&#29289;&#21551;&#21457;&#30340;&#39640;&#25928;&#32534;&#30721;&#35299;&#30721;&#26550;&#26500;&#65292;&#31216;&#20026;TDANet&#65292;&#20855;&#26377;&#38477;&#20302;&#30340;&#27169;&#22411;&#22797;&#26434;&#24230;&#65292;&#32780;&#19981;&#29306;&#29298;&#24615;&#33021;&#12290;TDANet&#20013;&#30340;&#33258;&#19978;&#32780;&#19979;&#27880;&#24847;&#21147;&#36890;&#36807;&#20840;&#23616;&#27880;&#24847;&#21147;(GA)&#27169;&#22359;&#21644;&#32423;&#32852;&#23616;&#37096;&#27880;&#24847;&#21147;(LA)&#23618;&#25552;&#21462;&#12290;GA&#27169;&#22359;&#23558;&#22810;&#23610;&#24230;&#38899;&#39057;&#29305;&#24449;&#20316;&#20026;&#36755;&#20837;&#65292;&#25552;&#21462;&#20840;&#23616;&#27880;&#24847;&#21147;&#20449;&#21495;&#65292;&#28982;&#21518;&#36890;&#36807;&#30452;&#25509;&#33258;&#19978;&#32780;&#19979;&#36830;&#25509;&#26469;&#35843;&#21046;&#19981;&#21516;&#23610;&#24230;&#30340;&#29305;&#24449;&#12290;LA&#23618;&#20351;&#29992;&#30456;&#37051;&#23618;&#30340;&#29305;&#24449;&#20316;&#20026;&#36755;&#20837;&#65292;&#20197;&#25552;&#21462;&#23616;&#37096;&#27880;&#24847;&#21147;&#20449;&#21495;&#65292;&#35813;&#20449;&#21495;&#29992;&#20110;&#20197;&#33258;&#19978;&#32780;&#19979;&#30340;&#26041;&#24335;&#35843;&#21046;&#27178;&#21521;&#36755;&#20837;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;TDANet&#22987;&#32456;&#33719;&#24471;&#20102;&#19982;&#20043;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#20998;&#31163;&#24615;&#33021;&#65292;&#21516;&#26102;&#26174;&#30528;&#20943;&#23569;&#20102;&#21442;&#25968;&#25968;&#37327;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have shown excellent prospects in speech separation tasks. However, obtaining good results while keeping a low model complexity remains challenging in real-world applications. In this paper, we provide a bio-inspired efficient encoder-decoder architecture by mimicking the brain's top-down attention, called TDANet, with decreased model complexity without sacrificing performance. The top-down attention in TDANet is extracted by the global attention (GA) module and the cascaded local attention (LA) layers. The GA module takes multi-scale acoustic features as input to extract global attention signal, which then modulates features of different scales by direct top-down connections. The LA layers use features of adjacent layers as input to extract the local attention signal, which is used to modulate the lateral input in a top-down manner. On three benchmark datasets, TDANet consistently achieved competitive separation performance to previous state-of-the-art (SOTA) meth
&lt;/p&gt;</description></item><item><title>MolMIM&#26159;&#29992;&#20110;&#23567;&#20998;&#23376;&#33647;&#29289;&#21457;&#29616;&#30340;&#27010;&#29575;&#33258;&#32534;&#30721;&#22120;&#65292;&#20854;&#23398;&#20064;&#20102;&#19968;&#31181;&#20449;&#24687;&#20016;&#23500;&#19988;&#32858;&#31867;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#20419;&#36827;&#33268;&#23494;&#30340;&#28508;&#22312;&#31354;&#38388;&#26469;&#37319;&#26679;&#26377;&#25928;&#30340;&#20998;&#23376;&#12290;&#36890;&#36807;&#19982;&#20854;&#20182;&#27169;&#22411;&#30340;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;MolMIM&#30340;&#26356;&#22909;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20986;&#33394;&#30340;&#20998;&#23376;&#20248;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.09016</link><description>&lt;p&gt;
&#20351;&#29992;&#20114;&#20449;&#24687;&#26426;&#22120;&#25552;&#39640;&#23567;&#20998;&#23376;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Improving Small Molecule Generation using Mutual Information Machine. (arXiv:2208.09016v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09016
&lt;/p&gt;
&lt;p&gt;
MolMIM&#26159;&#29992;&#20110;&#23567;&#20998;&#23376;&#33647;&#29289;&#21457;&#29616;&#30340;&#27010;&#29575;&#33258;&#32534;&#30721;&#22120;&#65292;&#20854;&#23398;&#20064;&#20102;&#19968;&#31181;&#20449;&#24687;&#20016;&#23500;&#19988;&#32858;&#31867;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#20419;&#36827;&#33268;&#23494;&#30340;&#28508;&#22312;&#31354;&#38388;&#26469;&#37319;&#26679;&#26377;&#25928;&#30340;&#20998;&#23376;&#12290;&#36890;&#36807;&#19982;&#20854;&#20182;&#27169;&#22411;&#30340;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;MolMIM&#30340;&#26356;&#22909;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20986;&#33394;&#30340;&#20998;&#23376;&#20248;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25511;&#21046;&#23567;&#20998;&#23376;&#29983;&#25104;&#30340;&#20219;&#21153;&#65292;&#21363;&#22312;&#19968;&#23450;&#38480;&#21046;&#26465;&#20214;&#19979;&#65288;&#22914;&#19982;&#21442;&#32771;&#20998;&#23376;&#30340;&#30456;&#20284;&#24230;&#65289;&#65292;&#23547;&#25214;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#26032;&#22411;&#20998;&#23376;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;MolMIM&#65292;&#19968;&#31181;&#29992;&#20110;&#23567;&#20998;&#23376;&#33647;&#29289;&#21457;&#29616;&#30340;&#27010;&#29575;&#33258;&#32534;&#30721;&#22120;&#65292;&#23427;&#23398;&#20064;&#20102;&#19968;&#31181;&#20449;&#24687;&#20016;&#23500;&#19988;&#32858;&#31867;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290; MolMIM&#26159;&#29992;&#20114;&#20449;&#24687;&#26426;&#22120;&#65288;MIM&#65289;&#23398;&#20064;&#35757;&#32451;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#21464;&#38271;SMILES&#23383;&#31526;&#20018;&#30340;&#22266;&#23450;&#38271;&#24230;&#34920;&#31034;&#12290;&#30001;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#20855;&#26377;&#26080;&#25928;&#26679;&#26412;&#30340;&#8220;&#31354;&#27934;&#8221;&#30340;&#34920;&#31034;&#65292;&#22240;&#27492;&#25105;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#23637;&#65292;&#20419;&#36827;&#20102;&#33268;&#23494;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#20801;&#35768;&#27169;&#22411;&#20174;&#28508;&#22312;&#20195;&#30721;&#30340;&#38543;&#26426;&#25200;&#21160;&#20013;&#37319;&#26679;&#26377;&#25928;&#30340;&#20998;&#23376;&#12290;&#25105;&#20204;&#23545;MolMIM&#19982;&#20960;&#31181;&#21487;&#21464;&#22823;&#23567;&#21644;&#22266;&#23450;&#22823;&#23567;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#65292;&#36890;&#36807;&#26377;&#25928;&#24615;&#12289;&#29420;&#29305;&#24615;&#21644;&#26032;&#39062;&#24615;&#31561;&#25351;&#26631;&#35777;&#26126;&#20102;MolMIM&#30340;&#26356;&#22909;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;CMA-ES&#65288;&#19968;&#31181;&#26420;&#32032;&#28436;&#21270;&#31639;&#27861;&#65289;&#26469;&#20248;&#21270;&#20998;&#23376;&#24615;&#36136;&#30340;&#32452;&#21512;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#23637;&#31034;&#20102;MolMIM&#22312;&#20998;&#23376;&#20248;&#21270;&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the task of controlled generation of small molecules, which entails finding novel molecules with desired properties under certain constraints (e.g., similarity to a reference molecule). Here we introduce MolMIM, a probabilistic auto-encoder for small molecule drug discovery that learns an informative and clustered latent space. MolMIM is trained with Mutual Information Machine (MIM) learning, and provides a fixed length representation of variable length SMILES strings. Since encoder-decoder models can learn representations with ``holes'' of invalid samples, here we propose a novel extension to the training procedure which promotes a dense latent space, and allows the model to sample valid molecules from random perturbations of latent codes. We provide a thorough comparison of MolMIM to several variable-size and fixed-size encoder-decoder models, demonstrating MolMIM's superior generation as measured in terms of validity, uniqueness, and novelty. We then utilize CMA-ES, a nai
&lt;/p&gt;</description></item><item><title>MEAN&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25239;&#20307;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;E(3)-&#31561;&#21464;&#28040;&#24687;&#20256;&#36882;&#21644;&#27880;&#24847;&#26426;&#21046;&#26356;&#22909;&#22320;&#25429;&#33719;&#19981;&#21516;&#32452;&#20214;&#20043;&#38388;&#30340;&#20960;&#20309;&#30456;&#20851;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#21516;&#26102;&#36755;&#20986;1D&#24207;&#21015;&#21644;3D&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2208.06073</link><description>&lt;p&gt;
&#20316;&#20026;3D&#31561;&#21464;&#22270;&#32763;&#35793;&#30340;&#26465;&#20214;&#24615;&#25239;&#20307;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Conditional Antibody Design as 3D Equivariant Graph Translation. (arXiv:2208.06073v6 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06073
&lt;/p&gt;
&lt;p&gt;
MEAN&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25239;&#20307;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;E(3)-&#31561;&#21464;&#28040;&#24687;&#20256;&#36882;&#21644;&#27880;&#24847;&#26426;&#21046;&#26356;&#22909;&#22320;&#25429;&#33719;&#19981;&#21516;&#32452;&#20214;&#20043;&#38388;&#30340;&#20960;&#20309;&#30456;&#20851;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#21516;&#26102;&#36755;&#20986;1D&#24207;&#21015;&#21644;3D&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25239;&#20307;&#35774;&#35745;&#23545;&#20110;&#27835;&#30103;&#21644;&#29983;&#29289;&#30740;&#31350;&#20855;&#26377;&#20215;&#20540;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#36935;&#21040;&#20102;&#20960;&#20010;&#20851;&#38190;&#38382;&#39064;: 1) &#23545;&#20110;&#20114;&#34917;&#20915;&#23450;&#21306;&#22495;(CDR)&#30340;&#29983;&#25104;&#23384;&#22312;&#19981;&#23436;&#25972;&#30340;&#19978;&#19979;&#25991;; 2)&#26080;&#27861;&#25429;&#33719;&#36755;&#20837;&#32467;&#26500;&#30340;&#25972;&#20010;3D&#20960;&#20309;&#24418;&#29366;; 3)&#26080;&#27861;&#20197;&#33258;&#22238;&#24402;&#26041;&#24335;&#39640;&#25928;&#39044;&#27979;CDR&#24207;&#21015;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#36890;&#36947;&#31561;&#21464;&#27880;&#24847;&#21147;&#32593;&#32476;(MEAN)&#65292;&#20197;&#20849;&#21516;&#35774;&#35745;CDR&#30340;1D&#24207;&#21015;&#21644;3D&#32467;&#26500;&#12290;&#26126;&#30830;&#22320;&#35828;&#65292;MEAN&#36890;&#36807;&#23548;&#20837;&#39069;&#22806;&#32452;&#20214;&#65292;&#21253;&#25324;&#30446;&#26631;&#25239;&#21407;&#21644;&#25239;&#20307;&#30340;&#36731;&#38142;&#65292;&#23558;&#25239;&#20307;&#35774;&#35745;&#20844;&#24335;&#21270;&#20026;&#26465;&#20214;&#22270;&#32763;&#35793;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;MEAN&#37319;&#29992;E&#65288;3&#65289;-&#31561;&#21464;&#28040;&#24687;&#20256;&#36882;&#21644;&#25552;&#35758;&#30340;&#27880;&#24847;&#26426;&#21046;&#26469;&#26356;&#22909;&#22320;&#25429;&#33719;&#19981;&#21516;&#32452;&#20214;&#20043;&#38388;&#30340;&#20960;&#20309;&#30456;&#20851;&#24615;&#12290;&#26368;&#21518;&#65292;&#23427;&#36890;&#36807;&#19968;&#20010;&#22810;&#36718;&#36880;&#28176;&#23436;&#25972;&#30340;&#26041;&#26696;&#36755;&#20986;1D&#24207;&#21015;&#21644;3D&#32467;&#26500;&#65292;&#36825;&#26679;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Antibody design is valuable for therapeutic usage and biological research. Existing deep-learning-based methods encounter several key issues: 1) incomplete context for Complementarity-Determining Regions (CDRs) generation; 2) incapability of capturing the entire 3D geometry of the input structure; 3) inefficient prediction of the CDR sequences in an autoregressive manner. In this paper, we propose Multi-channel Equivariant Attention Network (MEAN) to co-design 1D sequences and 3D structures of CDRs. To be specific, MEAN formulates antibody design as a conditional graph translation problem by importing extra components including the target antigen and the light chain of the antibody. Then, MEAN resorts to E(3)-equivariant message passing along with a proposed attention mechanism to better capture the geometrical correlation between different components. Finally, it outputs both the 1D sequences and 3D structure via a multi-round progressive full-shot scheme, which enjoys more efficiency
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; FixEval&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#31454;&#25216;&#32534;&#31243;&#38382;&#39064;&#30340;&#26377;&#32570;&#38519;&#20195;&#30721;&#25552;&#20132;&#21644;&#23427;&#20204;&#23545;&#24212;&#30340;&#20462;&#22797;&#32452;&#25104;&#30340;&#22522;&#20934;&#12290;FixEval &#25552;&#20379;&#20102;&#22823;&#37327;&#30340;&#21333;&#20803;&#27979;&#35797;&#26469;&#35780;&#20272;&#27169;&#22411;&#29983;&#25104;&#30340;&#31243;&#24207;&#20462;&#22797;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#26681;&#25454;&#21028;&#20915;&#36827;&#34892;&#26102;&#38388;&#12289;&#20869;&#23384;&#38480;&#21046;&#21644;&#25509;&#21463;&#24615;&#30340;&#36827;&#19968;&#27493;&#20449;&#24687;&#35780;&#20272;&#12290;&#22522;&#20110;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#25191;&#34892;&#30340;&#25351;&#26631;&#26356;&#20934;&#30830;&#65292;&#26377;&#21161;&#20110;&#35782;&#21035;&#29983;&#25104;&#30340;&#20462;&#22797;&#30340;&#24369;&#28857;&#12290;</title><link>http://arxiv.org/abs/2206.07796</link><description>&lt;p&gt;
FixEval: &#38024;&#23545;&#32534;&#31243;&#38382;&#39064;&#30340;&#31243;&#24207;&#20462;&#22797;&#30340;&#25191;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
FixEval: Execution-based Evaluation of Program Fixes for Programming Problems. (arXiv:2206.07796v4 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; FixEval&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#31454;&#25216;&#32534;&#31243;&#38382;&#39064;&#30340;&#26377;&#32570;&#38519;&#20195;&#30721;&#25552;&#20132;&#21644;&#23427;&#20204;&#23545;&#24212;&#30340;&#20462;&#22797;&#32452;&#25104;&#30340;&#22522;&#20934;&#12290;FixEval &#25552;&#20379;&#20102;&#22823;&#37327;&#30340;&#21333;&#20803;&#27979;&#35797;&#26469;&#35780;&#20272;&#27169;&#22411;&#29983;&#25104;&#30340;&#31243;&#24207;&#20462;&#22797;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#26681;&#25454;&#21028;&#20915;&#36827;&#34892;&#26102;&#38388;&#12289;&#20869;&#23384;&#38480;&#21046;&#21644;&#25509;&#21463;&#24615;&#30340;&#36827;&#19968;&#27493;&#20449;&#24687;&#35780;&#20272;&#12290;&#22522;&#20110;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#25191;&#34892;&#30340;&#25351;&#26631;&#26356;&#20934;&#30830;&#65292;&#26377;&#21161;&#20110;&#35782;&#21035;&#29983;&#25104;&#30340;&#20462;&#22797;&#30340;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#36719;&#20214;&#30340;&#22797;&#26434;&#24615;&#23548;&#33268;&#20102;&#26816;&#27979;&#21644;&#32416;&#27491;&#36719;&#20214; Bug &#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#24613;&#21095;&#22686;&#21152;&#12290;&#20026;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#33258;&#21160;&#29983;&#25104;&#26377;&#32570;&#38519;&#20195;&#30721;&#30340;&#20462;&#22797;&#31243;&#24207;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32473;&#23450;&#20219;&#20309; Bug &#21487;&#33021;&#30340;&#20462;&#22797;&#30340;&#32452;&#21512;&#31354;&#38388;&#24456;&#22823;&#65292;&#22240;&#27492;&#24456;&#23569;&#26377;&#24037;&#20855;&#21644;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#26377;&#25928;&#35780;&#20272;&#29983;&#25104;&#30340;&#20462;&#22797;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; FixEval&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#31454;&#25216;&#32534;&#31243;&#38382;&#39064;&#30340;&#26377;&#32570;&#38519;&#20195;&#30721;&#25552;&#20132;&#21644;&#23427;&#20204;&#23545;&#24212;&#30340;&#20462;&#22797;&#32452;&#25104;&#30340;&#22522;&#20934;&#12290;FixEval &#25552;&#20379;&#20102;&#22823;&#37327;&#30340;&#21333;&#20803;&#27979;&#35797;&#26469;&#35780;&#20272;&#27169;&#22411;&#29983;&#25104;&#30340;&#31243;&#24207;&#20462;&#22797;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#26681;&#25454;&#21028;&#20915;&#36827;&#34892;&#26102;&#38388;&#12289;&#20869;&#23384;&#38480;&#21046;&#21644;&#25509;&#21463;&#24615;&#30340;&#36827;&#19968;&#27493;&#20449;&#24687;&#35780;&#20272;&#12290;&#25105;&#20204;&#23558;&#20004;&#20010;&#32534;&#31243;&#35821;&#35328;&#30340; Transformer &#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22522;&#32447;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#21305;&#37197;&#21644;&#25191;&#34892;&#30340;&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22522;&#20110;&#21305;&#37197;&#30340;&#25351;&#26631;&#21487;&#33021;&#20250;&#25552;&#20379;&#35823;&#23548;&#24615;&#30340;&#31243;&#24207;&#20462;&#22797;&#35780;&#20272;&#65292;&#32780;&#22522;&#20110;&#25191;&#34892;&#30340;&#25351;&#26631;&#26356;&#20934;&#30830;&#65292;&#26377;&#21161;&#20110;&#35782;&#21035;&#29983;&#25104;&#30340;&#20462;&#22797;&#30340;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The complexity of modern software has led to a drastic increase in the time and cost associated with detecting and rectifying software bugs. In response, researchers have explored various methods to automatically generate fixes for buggy code. However, due to the large combinatorial space of possible fixes for any given bug, few tools and datasets are available to evaluate model-generated fixes effectively. To address this issue, we introduce FixEval, a benchmark comprising of buggy code submissions to competitive programming problems and their corresponding fixes. FixEval offers an extensive collection of unit tests to evaluate the correctness of model-generated program fixes and assess further information regarding time, memory constraints, and acceptance based on a verdict. We consider two Transformer language models pretrained on programming languages as our baseline and compare them using match-based and execution-based evaluation metrics. Our experiments show that match-based met
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#23616;&#21407;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#33258;&#30417;&#30563;&#20449;&#24687;&#30340;&#27491;&#21017;&#21270;&#19979;&#23398;&#20064;&#25968;&#25454;&#34920;&#31034;&#65292;&#20197;&#32531;&#35299;&#36127;&#38754;&#34920;&#31034;&#28418;&#31227;&#38382;&#39064;&#65292;&#24182;&#20943;&#23569;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2205.12186</link><description>&lt;p&gt;
&#22522;&#20110;&#20840;&#23616;&#21407;&#22411;&#30340;&#22686;&#24378;&#25345;&#32493;&#23398;&#20064;: &#23545;&#25239;&#36127;&#34920;&#31034;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;
Enhancing Continual Learning with Global Prototypes: Counteracting Negative Representation Drift. (arXiv:2205.12186v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12186
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#23616;&#21407;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#33258;&#30417;&#30563;&#20449;&#24687;&#30340;&#27491;&#21017;&#21270;&#19979;&#23398;&#20064;&#25968;&#25454;&#34920;&#31034;&#65292;&#20197;&#32531;&#35299;&#36127;&#38754;&#34920;&#31034;&#28418;&#31227;&#38382;&#39064;&#65292;&#24182;&#20943;&#23569;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26088;&#22312;&#23398;&#20064;&#19968;&#31995;&#21015;&#20219;&#21153;&#65292;&#20854;&#20013;&#25968;&#25454;&#20998;&#24067;&#20174;&#19968;&#20010;&#20219;&#21153;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#20219;&#21153;&#12290;&#22312;&#35757;&#32451;&#26032;&#20219;&#21153;&#25968;&#25454;&#26102;&#65292;&#26087;&#20219;&#21153;&#30340;&#25968;&#25454;&#34920;&#31034;&#21487;&#33021;&#20250;&#28418;&#31227;&#12290;&#19968;&#20123;&#36127;&#38754;&#30340;&#34920;&#31034;&#28418;&#31227;&#21487;&#33021;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#22240;&#20026;&#20250;&#23548;&#33268;&#20174;&#26412;&#22320;&#23398;&#20064;&#30340;&#31867;&#21035;&#21407;&#22411;&#21644;&#25968;&#25454;&#34920;&#31034;&#22312;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#36739;&#24046;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#34920;&#31034;&#28418;&#31227;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20840;&#23616;&#21407;&#22411;&#25351;&#23548;&#23398;&#20064;&#65292;&#29992;&#33258;&#30417;&#30563;&#20449;&#24687;&#30340;&#27491;&#21017;&#21270;&#26469;&#23398;&#20064;&#25968;&#25454;&#34920;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23545;&#20110;NLP&#20219;&#21153;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;&#20219;&#21153;&#20197;&#23631;&#34109;&#35821;&#35328;&#24314;&#27169;&#30340;&#26041;&#24335;&#36827;&#34892;&#20844;&#24335;&#21270;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30456;&#37051;&#27880;&#24847;&#26426;&#21046;&#23398;&#20064;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#20986;&#20855;&#26377;&#36739;&#23569;&#34920;&#31034;&#28418;&#31227;&#30340;&#30456;&#24403;&#19968;&#33268;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#19981;&#37325;&#26032;&#37319;&#26679;&#36807;&#21435;&#20219;&#21153;&#30340;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#20943;&#23569;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) aims to learn a sequence of tasks over time, with data distributions shifting from one task to another. When training on new task data, data representations from old tasks may drift. Some negative representation drift can result in catastrophic forgetting, by causing the locally learned class prototypes and data representations to correlate poorly across tasks. To mitigate such representation drift, we propose a method that finds global prototypes to guide the learning, and learns data representations with the regularization of the self-supervised information. Specifically, for NLP tasks, we formulate each task in a masked language modeling style, and learn the task via a neighbor attention mechanism over a pre-trained language model. Experimental results show that our proposed method can learn fairly consistent representations with less representation drift, and significantly reduce catastrophic forgetting in CL without resampling data from past tasks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21322;&#21442;&#25968;&#20307;&#31995;&#32467;&#26500;&#35843;&#29992;&#35757;&#32451;&#38598;&#30340;&#32593;&#32476;SPIN&#65292;&#24182;&#20197;&#20043;&#20026;&#22522;&#30784;&#26500;&#24314;&#20986;&#36866;&#29992;&#20110;&#22823;&#22411;&#19978;&#19979;&#25991;&#30340;&#35825;&#23548;&#28857;&#31070;&#32463;&#36807;&#31243;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#20803;&#23398;&#20064;&#39046;&#22495;&#24182;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2205.11718</link><description>&lt;p&gt;
&#21322;&#21442;&#25968;&#35825;&#23548;&#28857;&#32593;&#32476;&#21644;&#31070;&#32463;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Semi-Parametric Inducing Point Networks and Neural Processes. (arXiv:2205.11718v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11718
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21322;&#21442;&#25968;&#20307;&#31995;&#32467;&#26500;&#35843;&#29992;&#35757;&#32451;&#38598;&#30340;&#32593;&#32476;SPIN&#65292;&#24182;&#20197;&#20043;&#20026;&#22522;&#30784;&#26500;&#24314;&#20986;&#36866;&#29992;&#20110;&#22823;&#22411;&#19978;&#19979;&#25991;&#30340;&#35825;&#23548;&#28857;&#31070;&#32463;&#36807;&#31243;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#20803;&#23398;&#20064;&#39046;&#22495;&#24182;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21322;&#21442;&#25968;&#35825;&#23548;&#28857;&#32593;&#32476;&#65288;SPIN&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#38388;&#20197;&#35745;&#31639;&#26377;&#25928;&#30340;&#26041;&#24335;&#26597;&#35810;&#35757;&#32451;&#38598;&#12290;&#21322;&#21442;&#25968;&#20307;&#31995;&#32467;&#26500;&#36890;&#24120;&#27604;&#21442;&#25968;&#27169;&#22411;&#26356;&#32039;&#20945;&#65292;&#20294;&#23427;&#20204;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#36890;&#24120;&#26159;&#20108;&#27425;&#30340;&#12290;&#30456;&#21453;&#65292;SPIN&#36890;&#36807;&#25968;&#25454;&#28857;&#20043;&#38388;&#21463;&#35825;&#23548;&#28857;&#26041;&#27861;&#21551;&#21457;&#30340;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#23454;&#29616;&#20102;&#32447;&#24615;&#22797;&#26434;&#24230;&#12290;&#26597;&#35810;&#22823;&#22411;&#35757;&#32451;&#38598;&#22312;&#20803;&#23398;&#20064;&#20013;&#23588;&#20854;&#26377;&#29992;&#65292;&#22240;&#20026;&#23427;&#35299;&#38145;&#20102;&#39069;&#22806;&#30340;&#35757;&#32451;&#20449;&#21495;&#65292;&#20294;&#24120;&#24120;&#36229;&#20986;&#29616;&#26377;&#27169;&#22411;&#30340;&#32553;&#25918;&#38480;&#21046;&#12290;&#25105;&#20204;&#20351;&#29992;SPIN&#20316;&#20026;&#35825;&#23548;&#28857;&#31070;&#32463;&#36807;&#31243;&#30340;&#22522;&#30784;&#65292;&#36825;&#26159;&#19968;&#31181;&#27010;&#29575;&#27169;&#22411;&#65292;&#25903;&#25345;&#22312;&#20803;&#23398;&#20064;&#20013;&#20351;&#29992;&#22823;&#22411;&#19978;&#19979;&#25991;&#65292;&#24182;&#22312;&#29616;&#26377;&#27169;&#22411;&#22833;&#36133;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;SPIN&#20943;&#23569;&#20102;&#20869;&#23384;&#38656;&#27714;&#65292;&#22312;&#19968;&#31995;&#21015;&#20803;&#23398;&#20064;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#37325;&#35201;&#30340;&#23454;&#38469;&#38382;&#39064;&#65292;&#22522;&#22240;&#22411; im...&#65288;&#26681;&#25454;&#21407;&#25991;&#38271;&#24230;&#25130;&#26029;&#65289;
&lt;/p&gt;
&lt;p&gt;
We introduce semi-parametric inducing point networks (SPIN), a general-purpose architecture that can query the training set at inference time in a compute-efficient manner. Semi-parametric architectures are typically more compact than parametric models, but their computational complexity is often quadratic. In contrast, SPIN attains linear complexity via a cross-attention mechanism between datapoints inspired by inducing point methods. Querying large training sets can be particularly useful in meta-learning, as it unlocks additional training signal, but often exceeds the scaling limits of existing models. We use SPIN as the basis of the Inducing Point Neural Process, a probabilistic model which supports large contexts in meta-learning and achieves high accuracy where existing models fail. In our experiments, SPIN reduces memory requirements, improves accuracy across a range of meta-learning tasks, and improves state-of-the-art performance on an important practical problem, genotype imp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#39030;&#28857;&#23545;&#40784;&#30340;&#24179;&#22343;&#22270;&#65292;&#32858;&#31867;&#24179;&#22343;&#22270;&#21644;&#28151;&#28102;&#32593;&#32476;&#21305;&#37197;&#30340;&#31574;&#30053;&#65292;&#27604;&#36215;&#20256;&#32479;&#30340;&#20840;&#23616;&#24179;&#22343;&#22270;&#31574;&#30053;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#25552;&#39640;&#21305;&#37197;&#24615;&#33021;&#21644;&#20998;&#31867;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2205.03486</link><description>&lt;p&gt;
&#32858;&#31867;&#22270;&#21305;&#37197;&#29992;&#20110;&#26631;&#31614;&#24674;&#22797;&#21644;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Clustered Graph Matching for Label Recovery and Graph Classification. (arXiv:2205.03486v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.03486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#39030;&#28857;&#23545;&#40784;&#30340;&#24179;&#22343;&#22270;&#65292;&#32858;&#31867;&#24179;&#22343;&#22270;&#21644;&#28151;&#28102;&#32593;&#32476;&#21305;&#37197;&#30340;&#31574;&#30053;&#65292;&#27604;&#36215;&#20256;&#32479;&#30340;&#20840;&#23616;&#24179;&#22343;&#22270;&#31574;&#30053;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#25552;&#39640;&#21305;&#37197;&#24615;&#33021;&#21644;&#20998;&#31867;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#19968;&#32452;&#39030;&#28857;&#23545;&#40784;&#32593;&#32476;&#21644;&#19968;&#20010;&#39069;&#22806;&#30340;&#26631;&#31614;&#28151;&#28102;&#32593;&#32476;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39030;&#28857;&#23545;&#40784;&#38598;&#21512;&#20013;&#30340;&#20449;&#21495;&#26469;&#24674;&#22797;&#28151;&#28102;&#32593;&#32476;&#26631;&#31614;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#32771;&#34385;&#23558;&#28151;&#28102;&#32593;&#32476;&#19982;&#19981;&#21516;&#31890;&#24230;&#19979;&#30340;&#39030;&#28857;&#23545;&#40784;&#38598;&#21512;&#20013;&#30340;&#24179;&#22343;&#32593;&#32476;&#36827;&#34892;&#21305;&#37197;&#12290;&#25105;&#20204;&#35777;&#26126;&#24182;&#35777;&#23454;&#65292;&#22312;&#32593;&#32476;&#26469;&#33258;&#19981;&#21516;&#32593;&#32476;&#31867;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#32593;&#32476;&#32858;&#31867;&#21040;&#31867;&#20013;&#65292;&#28982;&#21518;&#23558;&#26032;&#32593;&#32476;&#21305;&#37197;&#21040;&#32858;&#31867;&#24179;&#22343;&#20540;&#65292;&#21487;&#20197;&#27604;&#23558;&#20854;&#21305;&#37197;&#21040;&#20840;&#23616;&#24179;&#22343;&#22270;&#20135;&#29983;&#26356;&#39640;&#30340;&#21305;&#37197;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#30456;&#23545;&#20110;&#27599;&#20010;&#32858;&#31867;&#24179;&#22343;&#20540;&#30340;&#22270;&#21305;&#37197;&#30446;&#26631;&#20989;&#25968;&#65292;&#36825;&#31181;&#26041;&#27861;&#21516;&#26102;&#23545;&#28151;&#28102;&#22270;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#39030;&#28857;&#26631;&#31614;&#24674;&#22797;&#12290;&#36825;&#20123;&#29702;&#35770;&#30740;&#31350;&#36890;&#36807;&#19968;&#20010;&#26377;&#21551;&#31034;&#24847;&#20041;&#30340;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#21305;&#37197;&#20154;&#31867;&#36830;&#25509;&#20307;&#26469;&#24471;&#21040;&#26356;&#22810;&#24041;&#22266;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a collection of vertex-aligned networks and an additional label-shuffled network, we propose procedures for leveraging the signal in the vertex-aligned collection to recover the labels of the shuffled network. We consider matching the shuffled network to averages of the networks in the vertex-aligned collection at different levels of granularity. We demonstrate both in theory and practice that if the graphs come from different network classes, then clustering the networks into classes followed by matching the new graph to cluster-averages can yield higher fidelity matching performance than matching to the global average graph. Moreover, by minimizing the graph matching objective function with respect to each cluster average, this approach simultaneously classifies and recovers the vertex labels for the shuffled graph. These theoretical developments are further reinforced via an illuminating real data experiment matching human connectomes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#33033;&#20914;&#34920;&#31034;&#30340;&#24494;&#20998;&#65288;DSR&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#19982;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30456;&#31454;&#20105;&#30340;&#39640;&#24615;&#33021;&#12289;&#20302;&#24310;&#36831;&#35757;&#32451;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#12290;</title><link>http://arxiv.org/abs/2205.00459</link><description>&lt;p&gt;
&#22522;&#20110;&#33033;&#20914;&#34920;&#31034;&#30340;&#24494;&#20998;&#35757;&#32451;&#39640;&#24615;&#33021;&#20302;&#24310;&#36831;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Training High-Performance Low-Latency Spiking Neural Networks by Differentiation on Spike Representation. (arXiv:2205.00459v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.00459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#33033;&#20914;&#34920;&#31034;&#30340;&#24494;&#20998;&#65288;DSR&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#19982;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30456;&#31454;&#20105;&#30340;&#39640;&#24615;&#33021;&#12289;&#20302;&#24310;&#36831;&#35757;&#32451;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#26159;&#19968;&#31181;&#38750;&#24120;&#26377;&#21069;&#36884;&#30340;&#33021;&#28304;&#39640;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#23588;&#20854;&#24403;&#23427;&#22312;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#19978;&#23454;&#29616;&#26102;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#19981;&#21487;&#24494;&#24615;&#65292;&#39640;&#25928;&#22320;&#35757;&#32451;SNN&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#30446;&#21069;&#22823;&#22810;&#25968;&#26041;&#27861;&#35201;&#20040;&#23384;&#22312;&#39640;&#24310;&#36831;&#65288;&#21363;&#38271;&#30340;&#20223;&#30495;&#26102;&#38388;&#27493;&#38271;&#65289;&#35201;&#20040;&#26080;&#27861;&#36798;&#21040;&#19982;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30456;&#24403;&#30340;&#39640;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#33033;&#20914;&#34920;&#31034;&#30340;&#24494;&#20998;&#65288;DSR&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#19982;ANN&#30456;&#31454;&#20105;&#30340;&#39640;&#24615;&#33021;&#19988;&#20302;&#24310;&#36831;&#30340;&#35757;&#32451;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#65288;&#21152;&#26435;&#65289;&#21457;&#23556;&#29575;&#32534;&#30721;&#23558;&#33033;&#20914;&#21015;&#32534;&#30721;&#20026;&#33033;&#20914;&#34920;&#31034;&#12290;&#22312;&#33033;&#20914;&#34920;&#31034;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#25512;&#23548;&#20986;&#20855;&#26377;&#24120;&#35265;&#31070;&#32463;&#27169;&#22411;&#30340;&#33033;&#20914;&#21160;&#21147;&#23398;&#21487;&#20197;&#34920;&#31034;&#20026;&#19968;&#20123;&#27425;&#24494;&#20998;&#26144;&#23556;&#12290;&#20174;&#36825;&#20010;&#35282;&#24230;&#20986;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;DSR&#26041;&#27861;&#36890;&#36807;&#36825;&#20123;&#26144;&#23556;&#30340;&#26799;&#24230;&#35757;&#32451;SNN&#24182;&#36991;&#20813;&#20102;SNN&#35757;&#32451;&#20013;&#24120;&#35265;&#30340;&#19981;&#21487;&#24494;&#24615;&#38382;&#39064;&#12290;&#28982;&#21518;&#25105;&#20204;&#23545;&#22122;&#22768;&#19979;&#34920;&#31034;&#35823;&#24046;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#25351;&#20986;DSR&#26041;&#27861;&#23545;&#36825;&#20123;&#35823;&#24046;&#34920;&#29616;&#20986;&#24378;&#26377;&#21147;&#30340;&#23481;&#24525;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Network (SNN) is a promising energy-efficient AI model when implemented on neuromorphic hardware. However, it is a challenge to efficiently train SNNs due to their non-differentiability. Most existing methods either suffer from high latency (i.e., long simulation time steps), or cannot achieve as high performance as Artificial Neural Networks (ANNs). In this paper, we propose the Differentiation on Spike Representation (DSR) method, which could achieve high performance that is competitive to ANNs yet with low latency. First, we encode the spike trains into spike representation using (weighted) firing rate coding. Based on the spike representation, we systematically derive that the spiking dynamics with common neural models can be represented as some sub-differentiable mapping. With this viewpoint, our proposed DSR method trains SNNs through gradients of the mapping and avoids the common non-differentiability problem in SNN training. Then we analyze the error when represe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22522;&#20110;&#25968;&#25454;&#30340;&#20215;&#26684;&#27495;&#35270;&#65292;&#25581;&#31034;&#20102;&#20219;&#20309;&#22522;&#20110;&#25968;&#25454;&#30340;&#23450;&#20215;&#31574;&#30053;&#22312;&#25910;&#20837;&#29983;&#25104;&#26041;&#38754;&#30340;&#20449;&#24687;&#35770;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#32463;&#39564;&#25910;&#30410;&#26368;&#22823;&#21270;&#65288;ERM&#65289;&#31574;&#30053;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2204.12723</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#30340;&#20215;&#26684;&#27495;&#35270;&#30340;&#20449;&#24687;&#35770;&#38480;&#21046;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Information-theoretic limitations of data-based price discrimination. (arXiv:2204.12723v3 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.12723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22522;&#20110;&#25968;&#25454;&#30340;&#20215;&#26684;&#27495;&#35270;&#65292;&#25581;&#31034;&#20102;&#20219;&#20309;&#22522;&#20110;&#25968;&#25454;&#30340;&#23450;&#20215;&#31574;&#30053;&#22312;&#25910;&#20837;&#29983;&#25104;&#26041;&#38754;&#30340;&#20449;&#24687;&#35770;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#32463;&#39564;&#25910;&#30410;&#26368;&#22823;&#21270;&#65288;ERM&#65289;&#31574;&#30053;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20272;&#20540;&#21644;&#22806;&#29983;&#21464;&#37327;&#25968;&#25454;&#38543;&#26426;&#26679;&#26412;&#30340;&#31532;&#19977;&#24230;&#20215;&#26684;&#27495;&#35270;&#65288;3PD&#65289;&#65292;&#20854;&#20013;&#22806;&#29983;&#21464;&#37327;&#26159;&#36830;&#32493;&#30340;&#65292;&#25968;&#25454;&#20998;&#24067;&#23545;&#21334;&#26041;&#26469;&#35828;&#26159;&#26410;&#30693;&#30340;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#32467;&#26524;&#26377;&#20004;&#20010;&#26041;&#38754;&#12290;&#31532;&#19968;&#32452;&#32467;&#26524;&#26159;&#23450;&#20215;&#31574;&#30053;&#26080;&#20851;&#30340;&#65292;&#25581;&#31034;&#20102;&#20219;&#20309;&#22522;&#20110;&#25968;&#25454;&#30340;&#23450;&#20215;&#31574;&#30053;&#22312;&#25910;&#20837;&#29983;&#25104;&#26041;&#38754;&#30340;&#20449;&#24687;&#35770;&#38480;&#21046;&#65292;&#20998;&#20026;3PD&#21644;&#22343;&#21248;&#23450;&#20215;&#20004;&#31181;&#24773;&#20917;&#12290;&#31532;&#20108;&#32452;&#32467;&#26524;&#25552;&#20986;&#20102;$K$-markets&#32463;&#39564;&#25910;&#30410;&#26368;&#22823;&#21270;&#65288;ERM&#65289;&#31574;&#30053;&#65292;&#24182;&#26174;&#31034;$K$-markets ERM&#21644;&#22343;&#21248;ERM&#31574;&#30053;&#23454;&#29616;&#20102;&#25910;&#20837;&#25910;&#25947;&#21040;&#21508;&#33258;&#30495;&#23454;&#20998;&#24067;3PD&#21644;&#22343;&#21248;&#23450;&#20215;&#26368;&#20248;&#35299;&#30340;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#26679;&#26412;&#37327;&#36275;&#22815;&#23567;&#30340;&#26102;&#20505;&#65292;&#22343;&#21248;&#65288;&#21363;$1$-market&#65289;ERM&#31574;&#30053;&#20135;&#29983;&#30340;&#25910;&#20837;&#27604;$K$-markets ERM&#31574;&#30053;&#26356;&#39640;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies third-degree price discrimination (3PD) based on a random sample of valuation and covariate data, where the covariate is continuous, and the distribution of the data is unknown to the seller. The main results of this paper are twofold. The first set of results is pricing strategy independent and reveals the fundamental information-theoretic limitation of any data-based pricing strategy in revenue generation for two cases: 3PD and uniform pricing. The second set of results proposes the $K$-markets empirical revenue maximization (ERM) strategy and shows that the $K$-markets ERM and the uniform ERM strategies achieve the optimal rate of convergence in revenue to that generated by their respective true-distribution 3PD and uniform pricing optima. Our theoretical and numerical results suggest that the uniform (i.e., $1$-market) ERM strategy generates a larger revenue than the $K$-markets ERM strategy when the sample size is small enough, and vice versa.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340; $\ell_{2,1}$ &#21644; $\ell_{F}$ &#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#33719;&#24471;&#26368;&#30456;&#20851;&#30340;&#20960;&#20010;&#29305;&#24449;&#65292;&#24182;&#22312;&#27969;&#24418;&#27491;&#21017;&#21270;&#20013;&#23454;&#29616;&#20102;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#31574;&#30053;&#30340;&#39640;&#24230;&#31283;&#20581;&#30340;&#37051;&#22495;&#22270;&#12290;&#35813;&#26041;&#27861;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#27604;&#36739;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2204.06445</link><description>&lt;p&gt;
&#38543;&#26426;&#27969;&#24418;&#37319;&#26679;&#21644;&#32852;&#21512;&#31232;&#30095;&#27491;&#21017;&#21270;&#30340;&#22810;&#26631;&#31614;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Random Manifold Sampling and Joint Sparse Regularization for Multi-label Feature Selection. (arXiv:2204.06445v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.06445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340; $\ell_{2,1}$ &#21644; $\ell_{F}$ &#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#33719;&#24471;&#26368;&#30456;&#20851;&#30340;&#20960;&#20010;&#29305;&#24449;&#65292;&#24182;&#22312;&#27969;&#24418;&#27491;&#21017;&#21270;&#20013;&#23454;&#29616;&#20102;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#31574;&#30053;&#30340;&#39640;&#24230;&#31283;&#20581;&#30340;&#37051;&#22495;&#22270;&#12290;&#35813;&#26041;&#27861;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#27604;&#36739;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26631;&#31614;&#23398;&#20064;&#36890;&#24120;&#29992;&#20110;&#25366;&#25496;&#29305;&#24449;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#29305;&#24449;&#36873;&#25321;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#29305;&#24449;&#20445;&#30041;&#23613;&#21487;&#33021;&#22810;&#30340;&#20449;&#24687;&#12290; $\ell_{2,1}$ &#27491;&#21017;&#21270;&#21487;&#20197;&#33719;&#24471;&#31232;&#30095;&#31995;&#25968;&#30697;&#38453;&#65292;&#20294;&#19981;&#33021;&#26377;&#25928;&#22320;&#35299;&#20915;&#22810;&#37325;&#20849;&#32447;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#27169;&#22411;&#36890;&#36807;&#35299;&#20915; $\ell_{2,1}$ &#21644; $\ell_{F}$ &#27491;&#21017;&#21270;&#30340;&#32852;&#21512;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#26469;&#33719;&#21462;&#26368;&#30456;&#20851;&#30340;&#20960;&#20010;&#29305;&#24449;&#12290;&#22312;&#27969;&#24418;&#27491;&#21017;&#21270;&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#32852;&#21512;&#20449;&#24687;&#30697;&#38453;&#23454;&#29616;&#20102;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#31574;&#30053;&#30340;&#39640;&#24230;&#31283;&#20581;&#30340;&#37051;&#22495;&#22270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#32473;&#20986;&#20102;&#35299;&#20915;&#35813;&#27169;&#22411;&#30340;&#31639;&#27861;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#12290;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#27604;&#36739;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-label learning is usually used to mine the correlation between features and labels, and feature selection can retain as much information as possible through a small number of features. $\ell_{2,1}$ regularization method can get sparse coefficient matrix, but it can not solve multicollinearity problem effectively. The model proposed in this paper can obtain the most relevant few features by solving the joint constrained optimization problems of $\ell_{2,1}$ and $\ell_{F}$ regularization.In manifold regularization, we implement random walk strategy based on joint information matrix, and get a highly robust neighborhood graph.In addition, we given the algorithm for solving the model and proved its convergence.Comparative experiments on real-world data sets show that the proposed method outperforms other methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#22238;&#24402;&#20808;&#39564;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#19977;&#32500;&#20219;&#21153;&#20013;&#30340;&#24418;&#29366;&#23436;&#25104;&#12289;&#37325;&#24314;&#21644;&#29983;&#25104;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#22312;&#20219;&#24847;&#31354;&#38388;&#38170;&#28857;&#20301;&#32622;&#30340;&#20449;&#24687;&#26465;&#20214;&#19979;&#36827;&#34892;&#24418;&#29366;&#23436;&#25104;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#29992;&#20110;&#21333;&#35270;&#22270;&#37325;&#24314;&#21644;&#22522;&#20110;&#35821;&#20041;&#30340;&#24418;&#29366;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2203.09516</link><description>&lt;p&gt;
AutoSDF: &#29992;&#20110;&#19977;&#32500;&#23436;&#25104;&#12289;&#37325;&#24314;&#21644;&#29983;&#25104;&#30340;&#24418;&#29366;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
AutoSDF: Shape Priors for 3D Completion, Reconstruction and Generation. (arXiv:2203.09516v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.09516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#22238;&#24402;&#20808;&#39564;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#19977;&#32500;&#20219;&#21153;&#20013;&#30340;&#24418;&#29366;&#23436;&#25104;&#12289;&#37325;&#24314;&#21644;&#29983;&#25104;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#22312;&#20219;&#24847;&#31354;&#38388;&#38170;&#28857;&#20301;&#32622;&#30340;&#20449;&#24687;&#26465;&#20214;&#19979;&#36827;&#34892;&#24418;&#29366;&#23436;&#25104;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#29992;&#20110;&#21333;&#35270;&#22270;&#37325;&#24314;&#21644;&#22522;&#20110;&#35821;&#20041;&#30340;&#24418;&#29366;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#22823;&#30340;&#20808;&#39564;&#26465;&#20214;&#21487;&#20197;&#20351;&#25105;&#20204;&#22312;&#20449;&#24687;&#19981;&#20805;&#20998;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#25512;&#29702;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#32500;&#24418;&#29366;&#30340;&#33258;&#22238;&#24402;&#20808;&#39564;&#65292;&#20197;&#35299;&#20915;&#22810;&#27169;&#24577;&#30340;&#19977;&#32500;&#20219;&#21153;&#65292;&#22914;&#24418;&#29366;&#23436;&#25104;&#12289;&#37325;&#24314;&#21644;&#29983;&#25104;&#12290;&#25105;&#20204;&#23558;&#19977;&#32500;&#24418;&#29366;&#30340;&#20998;&#24067;&#24314;&#27169;&#20026;&#38750;&#36830;&#32493;&#33258;&#22238;&#24402;&#20998;&#24067;&#65292;&#36890;&#36807;&#23545;&#19977;&#32500;&#24418;&#29366;&#30340;&#31163;&#25955;&#21270;&#12289;&#20302;&#32500;&#12289;&#35937;&#24449;&#24615;&#30340;&#32593;&#26684;&#29366;&#28508;&#22312;&#34920;&#31034;&#36827;&#34892;&#24314;&#27169;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#34920;&#31034;&#22312;&#20219;&#24847;&#19968;&#32452;&#31354;&#38388;&#38170;&#23450;&#26597;&#35810;&#20301;&#32622;&#30340;&#20449;&#24687;&#26465;&#20214;&#19979;&#65292;&#19977;&#32500;&#24418;&#29366;&#30340;&#20998;&#24067;&#65292;&#20174;&#32780;&#22312;&#36825;&#20123;&#20219;&#24847;&#35774;&#32622;&#20013;&#36827;&#34892;&#24418;&#29366;&#23436;&#25104;&#65288;&#20363;&#22914;&#65292;&#20165;&#32473;&#20986;&#21518;&#33151;&#30340;&#35270;&#22270;&#21363;&#21487;&#29983;&#25104;&#23436;&#25972;&#30340;&#26885;&#23376;&#65289;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23398;&#20064;&#30340;&#33258;&#22238;&#24402;&#20808;&#39564;&#21487;&#20197;&#29992;&#20110;&#26465;&#20214;&#20219;&#21153;&#65292;&#22914;&#21333;&#35270;&#22270;&#37325;&#24314;&#21644;&#22522;&#20110;&#35821;&#35328;&#30340;&#29983;&#25104;&#12290;&#36825;&#36890;&#36807;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#26420;&#32032;&#26465;&#20214;&#27010;&#29575;&#23454;&#29616;&#65292;&#36825;&#20123;&#26465;&#20214;&#27010;&#29575;&#21487;&#20197;&#30001;&#22312;&#26368;&#23567;&#25104;&#23545;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#36731;&#37327;&#32423;&#27169;&#22411;&#36827;&#34892;&#36817;&#20284;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Powerful priors allow us to perform inference with insufficient information. In this paper, we propose an autoregressive prior for 3D shapes to solve multimodal 3D tasks such as shape completion, reconstruction, and generation. We model the distribution over 3D shapes as a non-sequential autoregressive distribution over a discretized, low-dimensional, symbolic grid-like latent representation of 3D shapes. This enables us to represent distributions over 3D shapes conditioned on information from an arbitrary set of spatially anchored query locations and thus perform shape completion in such arbitrary settings (e.g., generating a complete chair given only a view of the back leg). We also show that the learned autoregressive prior can be leveraged for conditional tasks such as single-view reconstruction and language-based generation. This is achieved by learning task-specific naive conditionals which can be approximated by light-weight models trained on minimal paired data. We validate the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#20943;&#23569;&#27599;&#20010;&#32452;&#20998;&#25968;&#20998;&#24067;&#20043;&#38388;&#30340;&#32479;&#35745;&#36317;&#31163;&#65292;&#22312;&#20219;&#20309;&#20915;&#31574;&#38408;&#20540;&#19979;&#21516;&#26102;&#25552;&#39640;&#20998;&#31867;&#30340;&#20844;&#24179;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20339;&#36816;&#36755;&#30340;&#21518;&#22788;&#29702;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2203.07490</link><description>&lt;p&gt;
&#20219;&#20309;&#20915;&#31574;&#38408;&#20540;&#19979;&#20844;&#24179;&#20998;&#31867;&#30340;&#20960;&#20309;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Geometric Repair for Fair Classification at Any Decision Threshold. (arXiv:2203.07490v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.07490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#20943;&#23569;&#27599;&#20010;&#32452;&#20998;&#25968;&#20998;&#24067;&#20043;&#38388;&#30340;&#32479;&#35745;&#36317;&#31163;&#65292;&#22312;&#20219;&#20309;&#20915;&#31574;&#38408;&#20540;&#19979;&#21516;&#26102;&#25552;&#39640;&#20998;&#31867;&#30340;&#20844;&#24179;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20339;&#36816;&#36755;&#30340;&#21518;&#22788;&#29702;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#30340;&#38382;&#39064;&#26159;&#21518;&#22788;&#29702;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#22120;&#20197;&#22312;&#25152;&#26377;&#20915;&#31574;&#38408;&#20540;&#19979;&#26368;&#22823;&#21270;&#20844;&#24179;&#30340;&#20108;&#20998;&#31867;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#34920;&#26126;&#36890;&#36807;&#20943;&#23569;&#27599;&#20010;&#32452;&#20998;&#25968;&#20998;&#24067;&#20043;&#38388;&#30340;&#32479;&#35745;&#36317;&#31163;&#65292;&#25105;&#20204;&#21487;&#20197;&#21516;&#26102;&#22686;&#21152;&#25152;&#26377;&#38408;&#20540;&#19978;&#30340;&#20844;&#24179;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#20197;&#20570;&#21040;&#22312;&#19981;&#26174;&#30528;&#38477;&#20302;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#24418;&#24335;&#21270;&#30340;&#20998;&#24067;&#24179;&#31561;&#24230;&#37327;&#65292;&#23427;&#25429;&#33719;&#20102;&#19981;&#21516;&#20445;&#25252;&#32452;&#30340;&#20998;&#31867;&#20998;&#24067;&#30456;&#20284;&#31243;&#24230;&#12290;&#19982;&#20808;&#21069;&#21482;&#38024;&#23545;&#25152;&#26377;&#38408;&#20540;&#30340;&#20154;&#21475;&#32479;&#35745;&#24179;&#31561;&#24230;&#30740;&#31350;&#30340;&#30740;&#31350;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#24230;&#37327;&#36866;&#29992;&#20110;&#22823;&#31867;&#20844;&#24179;&#24615;&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#25104;&#26524;&#26159;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26368;&#20339;&#36816;&#36755;&#30340;&#21518;&#22788;&#29702;&#31639;&#27861;&#65292;&#21487;&#20197;&#35777;&#26126;&#23427;&#26368;&#22823;&#21270;&#20102;&#20998;&#24067;&#24179;&#31561;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#20844;&#24179;&#24615;&#22522;&#20934;&#27979;&#35797;&#19978;&#25903;&#25345;&#27492;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of post-processing a supervised machine-learned regressor to maximize fair binary classification at all decision thresholds. Specifically, we show that by decreasing the statistical distance between each group's score distributions, we can increase fair performance across all thresholds at once, and that we can do so without a significant decrease in accuracy. To this end, we introduce a formal measure of distributional parity, which captures the degree of similarity in the distributions of classifications for different protected groups. In contrast to prior work, which has been limited to studies of demographic parity across all thresholds, our measure applies to a large class of fairness metrics. Our main result is to put forward a novel post-processing algorithm based on optimal transport, which provably maximizes distributional parity. We support this result with experiments on several fairness benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#29615;&#22659;&#30340;&#20849;&#20139;&#32467;&#26500;&#26469;&#26368;&#22823;&#21270;&#31995;&#32479;&#21270;&#27867;&#21270;&#65292;&#22312;&#36816;&#21160;&#23450;&#24459;&#20849;&#20139;&#30340;&#29615;&#22659;&#31995;&#21015;&#20013;&#23454;&#29616;&#20102;&#20248;&#21270;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20132;&#25442;&#65292;&#24182;&#22312;&#22522;&#20934;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#26377;&#25928;&#24615;&#21644;&#25361;&#25112;&#24615;&#12290;</title><link>http://arxiv.org/abs/2202.06545</link><description>&lt;p&gt;
&#31995;&#32479;&#21270;&#27867;&#21270;&#30340;&#21487;&#35777;&#26126;&#26377;&#25928;&#22240;&#26524;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Provably Efficient Causal Model-Based Reinforcement Learning for Systematic Generalization. (arXiv:2202.06545v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.06545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#29615;&#22659;&#30340;&#20849;&#20139;&#32467;&#26500;&#26469;&#26368;&#22823;&#21270;&#31995;&#32479;&#21270;&#27867;&#21270;&#65292;&#22312;&#36816;&#21160;&#23450;&#24459;&#20849;&#20139;&#30340;&#29615;&#22659;&#31995;&#21015;&#20013;&#23454;&#29616;&#20102;&#20248;&#21270;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20132;&#25442;&#65292;&#24182;&#22312;&#22522;&#20934;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#26377;&#25928;&#24615;&#21644;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39034;&#24207;&#20915;&#31574;&#29615;&#22659;&#20013;&#65292;&#26234;&#33021;&#20307;&#26088;&#22312;&#23454;&#29616;&#23545;&#22823;&#35268;&#27169;&#12289;&#21487;&#33021;&#26080;&#38480;&#30340;&#29615;&#22659;&#30340;&#31995;&#32479;&#21270;&#27867;&#21270;&#12290;&#36825;&#20123;&#29615;&#22659;&#34987;&#24314;&#27169;&#20026;&#31163;&#25955;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#20854;&#20013;&#29366;&#24577;&#21644;&#21160;&#20316;&#37117;&#29992;&#29305;&#24449;&#21521;&#37327;&#34920;&#31034;&#12290;&#29615;&#22659;&#30340;&#24213;&#23618;&#32467;&#26500;&#20801;&#35768;&#23558;&#36716;&#31227;&#21160;&#24577;&#20998;&#35299;&#20026;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#19968;&#20010;&#26159;&#29305;&#23450;&#20110;&#29615;&#22659;&#30340;&#65292;&#21478;&#19968;&#20010;&#26159;&#20849;&#20139;&#30340;&#12290;&#20363;&#22914;&#65292;&#32771;&#34385;&#19968;&#32452;&#20849;&#20139;&#36816;&#21160;&#27861;&#21017;&#30340;&#29615;&#22659;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26234;&#33021;&#20307;&#21487;&#20197;&#20174;&#36825;&#20123;&#29615;&#22659;&#30340;&#23376;&#38598;&#20013;&#36827;&#34892;&#26377;&#38480;&#30340;&#26080;&#22238;&#25253;&#20132;&#20114;&#12290;&#28982;&#21518;&#65292;&#26234;&#33021;&#20307;&#24517;&#39035;&#33021;&#22815;&#20165;&#20381;&#38752;&#19978;&#36848;&#20132;&#20114;&#65292;&#36817;&#20284;&#22320;&#35299;&#20915;&#35774;&#32622;&#22312;&#20219;&#20309;&#29615;&#22659;&#20013;&#30340;&#20219;&#20309;&#35268;&#21010;&#20219;&#21153;&#12290;&#25105;&#20204;&#26159;&#21542;&#21487;&#20197;&#35774;&#35745;&#19968;&#20010;&#21487;&#35777;&#26126;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#36825;&#20010;&#38596;&#24515;&#21187;&#21187;&#30340;&#31995;&#32479;&#21270;&#27867;&#21270;&#30446;&#26631;&#21602;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37096;&#20998;&#22320;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21487;&#20197;&#23545;&#36825;&#31867;&#38382;&#39064;&#36827;&#34892;&#20248;&#21270;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20132;&#25442;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;&#36890;&#36807;&#22240;&#26524;&#25512;&#29702;&#23454;&#29616;&#31995;&#32479;&#21270;&#27867;&#21270;&#65288;SGCI&#65289;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#29615;&#22659;&#30340;&#20849;&#20139;&#32467;&#26500;&#26469;&#26368;&#22823;&#21270;&#31995;&#32479;&#21270;&#27867;&#21270;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#23398;&#20064;&#36816;&#21160;&#23450;&#24459;&#20849;&#20139;&#30340;&#29615;&#22659;&#31995;&#21015;&#26102;&#23454;&#29616;&#20102;&#20248;&#21270;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20132;&#25442;&#65292;&#24182;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#31639;&#27861;&#22312;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#38382;&#39064;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#36825;&#20123;&#38382;&#39064;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#32780;&#35328;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the sequential decision making setting, an agent aims to achieve systematic generalization over a large, possibly infinite, set of environments. Such environments are modeled as discrete Markov decision processes with both states and actions represented through a feature vector. The underlying structure of the environments allows the transition dynamics to be factored into two components: one that is environment-specific and another that is shared. Consider a set of environments that share the laws of motion as an example. In this setting, the agent can take a finite amount of reward-free interactions from a subset of these environments. The agent then must be able to approximately solve any planning task defined over any environment in the original set, relying on the above interactions only. Can we design a provably efficient algorithm that achieves this ambitious goal of systematic generalization? In this paper, we give a partially positive answer to this question. First, we prov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#33539;&#25968;&#32422;&#26463;&#30340;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#33021;&#21147;&#65292;&#24182;&#35777;&#26126;&#20102;&#23545;&#20110;&#24179;&#28369;&#20989;&#25968;&#31867;&#65292;&#36825;&#20123;&#32593;&#32476;&#30340;&#36924;&#36817;&#35823;&#24046;&#26377;&#19978;&#19979;&#30028;&#12290;&#27492;&#22806;&#65292;&#24212;&#29992;&#32467;&#26524;&#20998;&#26512;&#20102;&#22238;&#24402;&#21644;GAN&#20998;&#24067;&#20272;&#35745;&#38382;&#39064;&#30340;&#25910;&#25947;&#24615;&#65292;&#26368;&#32456;&#35777;&#26126;&#20102;&#24403;GAN&#30340;&#21028;&#21035;&#22120;&#36873;&#25321;&#21512;&#36866;&#30340;&#20855;&#33539;&#25968;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#21487;&#20197;&#23454;&#29616;&#23398;&#20064;&#27010;&#29575;&#20998;&#24067;&#30340;&#26368;&#20248;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2201.09418</link><description>&lt;p&gt;
&#20855;&#33539;&#25968;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#35823;&#24046;&#30028;&#19982;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Approximation bounds for norm constrained neural networks with applications to regression and GANs. (arXiv:2201.09418v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.09418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#33539;&#25968;&#32422;&#26463;&#30340;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#33021;&#21147;&#65292;&#24182;&#35777;&#26126;&#20102;&#23545;&#20110;&#24179;&#28369;&#20989;&#25968;&#31867;&#65292;&#36825;&#20123;&#32593;&#32476;&#30340;&#36924;&#36817;&#35823;&#24046;&#26377;&#19978;&#19979;&#30028;&#12290;&#27492;&#22806;&#65292;&#24212;&#29992;&#32467;&#26524;&#20998;&#26512;&#20102;&#22238;&#24402;&#21644;GAN&#20998;&#24067;&#20272;&#35745;&#38382;&#39064;&#30340;&#25910;&#25947;&#24615;&#65292;&#26368;&#32456;&#35777;&#26126;&#20102;&#24403;GAN&#30340;&#21028;&#21035;&#22120;&#36873;&#25321;&#21512;&#36866;&#30340;&#20855;&#33539;&#25968;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#21487;&#20197;&#23454;&#29616;&#23398;&#20064;&#27010;&#29575;&#20998;&#24067;&#30340;&#26368;&#20248;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#24102;&#26435;&#37325;&#33539;&#25968;&#32422;&#26463;&#30340;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#33021;&#21147;&#65292;&#23545;&#20110;&#24179;&#28369;&#30340;&#20989;&#25968;&#31867;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#32593;&#32476;&#30340;&#36924;&#36817;&#35823;&#24046;&#19978;&#19979;&#30028;&#12290;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#30340;Rademacher&#22797;&#26434;&#24230;&#23548;&#20986;&#19979;&#30028;&#35777;&#26126;&#65292;&#36825;&#21487;&#33021;&#20855;&#26377;&#29420;&#31435;&#30340;&#30740;&#31350;&#20215;&#20540;&#12290;&#25105;&#20204;&#24212;&#29992;&#36825;&#20123;&#36924;&#36817;&#35823;&#24046;&#30028;&#38480;&#26469;&#20998;&#26512;&#20351;&#29992;&#20855;&#33539;&#25968;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22238;&#24402;&#21644;GAN&#20998;&#24067;&#20272;&#35745;&#30340;&#25910;&#25947;&#24615;&#12290;&#29305;&#21035;&#30340;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#36807;&#21442;&#25968;&#31070;&#32463;&#32593;&#32476;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#24403;&#21028;&#21035;&#22120;&#36873;&#25321;&#21512;&#36866;&#30340;&#20855;&#33539;&#25968;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;GAN&#21487;&#20197;&#23454;&#29616;&#23398;&#20064;&#27010;&#29575;&#20998;&#24067;&#30340;&#26368;&#20248;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the approximation capacity of ReLU neural networks with norm constraint on the weights. We prove upper and lower bounds on the approximation error of these networks for smooth function classes. The lower bound is derived through the Rademacher complexity of neural networks, which may be of independent interest. We apply these approximation bounds to analyze the convergences of regression using norm constrained neural networks and distribution estimation by GANs. In particular, we obtain convergence rates for over-parameterized neural networks. It is also shown that GANs can achieve optimal rate of learning probability distributions, when the discriminator is a properly chosen norm constrained neural network.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CTR&#27169;&#22411;MOEF&#65292;&#23427;&#36890;&#36807;&#22312;&#39057;&#22495;&#20013;&#24314;&#27169;&#22330;&#21512;&#28436;&#21464;&#26469;&#22788;&#29702;&#22312;&#32447;&#20998;&#24067;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#37319;&#29992;&#22810;&#20010;&#19987;&#23478;&#23398;&#20064;&#29305;&#24449;&#34920;&#31034;&#65292;&#21462;&#24471;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#30005;&#23376;&#21830;&#21153;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;CTR&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2112.13747</link><description>&lt;p&gt;
MOEF:&#24314;&#27169;&#39057;&#22495;&#20013;&#30340;&#22330;&#21512;&#28436;&#21464;&#65292;&#23454;&#29616;&#20419;&#38144;&#24863;&#30693;&#30340;&#28857;&#20987;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
MOEF: Modeling Occasion Evolution in Frequency Domain for Promotion-Aware Click-Through Rate Prediction. (arXiv:2112.13747v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.13747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CTR&#27169;&#22411;MOEF&#65292;&#23427;&#36890;&#36807;&#22312;&#39057;&#22495;&#20013;&#24314;&#27169;&#22330;&#21512;&#28436;&#21464;&#26469;&#22788;&#29702;&#22312;&#32447;&#20998;&#24067;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#37319;&#29992;&#22810;&#20010;&#19987;&#23478;&#23398;&#20064;&#29305;&#24449;&#34920;&#31034;&#65292;&#21462;&#24471;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#30005;&#23376;&#21830;&#21153;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;CTR&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20419;&#38144;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#21644;&#26222;&#36941;&#65292;&#20197;&#21560;&#24341;&#23458;&#25143;&#21644;&#20419;&#36827;&#38144;&#21806;&#65292;&#23548;&#33268;&#22330;&#21512;&#32463;&#24120;&#21464;&#21270;&#65292;&#20174;&#32780;&#39537;&#21160;&#29992;&#25143;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#34892;&#20026;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#21363;&#23558;&#21040;&#26469;&#30340;&#22330;&#21512;&#20998;&#24067;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#27169;&#22411;&#26080;&#27861;&#22312;&#22312;&#32447;&#26381;&#21153;&#20013;&#33391;&#22909;&#22320;&#25512;&#24191;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;CTR&#27169;&#22411;&#65292;&#21517;&#20026;MOEF&#65292;&#29992;&#20110;&#22312;&#22330;&#21512;&#32463;&#24120;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#25512;&#33616;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26102;&#38388;&#24207;&#21015;&#65292;&#20854;&#20013;&#21253;&#25324;&#20174;&#22312;&#32447;&#19994;&#21153;&#22330;&#26223;&#20013;&#29983;&#25104;&#30340;&#22330;&#21512;&#20449;&#21495;&#12290;&#30001;&#20110;&#22330;&#21512;&#20449;&#21495;&#22312;&#39057;&#22495;&#20013;&#26356;&#20855;&#26377;&#21306;&#21035;&#24615;&#65292;&#25105;&#20204;&#23545;&#26102;&#38388;&#31383;&#21475;&#24212;&#29992;&#20613;&#37324;&#21494;&#21464;&#25442;&#65292;&#24471;&#21040;&#19968;&#31995;&#21015;&#39057;&#35889;&#65292;&#28982;&#21518;&#36890;&#36807;&#22330;&#21512;&#28436;&#21464;&#23618;&#65288;OEL&#65289;&#36827;&#34892;&#22788;&#29702;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#21487;&#20197;&#23398;&#20064;&#39640;&#38454;&#22330;&#21512;&#34920;&#31034;&#65292;&#20197;&#22788;&#29702;&#22312;&#32447;&#20998;&#24067;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#22810;&#20010;&#19987;&#23478;&#26469;&#23398;&#20064;&#29305;&#24449;&#34920;&#31034;&#65292;&#34920;&#31034;&#20026;&#22330;&#21512;&#19978;&#19979;&#25991;&#32534;&#30721;&#65288;OCE&#65289;&#21644;&#27169;&#22411;&#24863;&#30693;&#27880;&#24847;&#21147;&#65288;MAA&#65289;&#65292;&#20197;&#25429;&#25417;&#29992;&#25143;&#34892;&#20026;&#21644;&#39033;&#30446;&#29305;&#24449;&#30340;&#19981;&#21516;&#26041;&#38754;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#30005;&#23376;&#21830;&#21153;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#65292;MOEF&#22312;&#31163;&#32447;&#35780;&#20272;&#21644;&#22312;&#32447;&#26381;&#21153;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;CTR&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Promotions are becoming more important and prevalent in e-commerce to attract customers and boost sales, leading to frequent changes of occasions, which drives users to behave differently. In such situations, most existing Click-Through Rate (CTR) models can't generalize well to online serving due to distribution uncertainty of the upcoming occasion. In this paper, we propose a novel CTR model named MOEF for recommendations under frequent changes of occasions. Firstly, we design a time series that consists of occasion signals generated from the online business scenario. Since occasion signals are more discriminative in the frequency domain, we apply Fourier Transformation to sliding time windows upon the time series, obtaining a sequence of frequency spectrum which is then processed by Occasion Evolution Layer (OEL). In this way, a high-order occasion representation can be learned to handle the online distribution uncertainty. Moreover, we adopt multiple experts to learn feature repres
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#31867;&#24863;&#30693;&#36857;&#27604;&#20248;&#21270;&#30340;&#36890;&#36947;&#21098;&#26525;&#26041;&#27861;&#65288;CATRO&#65289;&#65292;&#36890;&#36807;&#29305;&#24449;&#31354;&#38388;&#21028;&#21035;&#24230;&#37327;&#22810;&#36890;&#36947;&#30340;&#32852;&#21512;&#24433;&#21709;&#24182;&#21512;&#24182;&#20445;&#30041;&#36890;&#36947;&#30340;&#23618;&#27425;&#24433;&#21709;&#65292;&#26377;&#25928;&#38477;&#20302;&#35745;&#31639;&#36127;&#25285;&#24182;&#21152;&#36895;&#27169;&#22411;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2110.10921</link><description>&lt;p&gt;
CATRO&#65306;&#22522;&#20110;&#31867;&#24863;&#30693;&#30340;&#36857;&#27604;&#20248;&#21270;&#30340;&#36890;&#36947;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
CATRO: Channel Pruning via Class-Aware Trace Ratio Optimization. (arXiv:2110.10921v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.10921
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#31867;&#24863;&#30693;&#36857;&#27604;&#20248;&#21270;&#30340;&#36890;&#36947;&#21098;&#26525;&#26041;&#27861;&#65288;CATRO&#65289;&#65292;&#36890;&#36807;&#29305;&#24449;&#31354;&#38388;&#21028;&#21035;&#24230;&#37327;&#22810;&#36890;&#36947;&#30340;&#32852;&#21512;&#24433;&#21709;&#24182;&#21512;&#24182;&#20445;&#30041;&#36890;&#36947;&#30340;&#23618;&#27425;&#24433;&#21709;&#65292;&#26377;&#25928;&#38477;&#20302;&#35745;&#31639;&#36127;&#25285;&#24182;&#21152;&#36895;&#27169;&#22411;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#24212;&#29992;&#22330;&#26223;&#20013;&#23384;&#22312;&#39640;&#21442;&#25968;&#21644;&#35745;&#31639;&#20887;&#20313;&#65292;&#24517;&#35201;&#26102;&#38656;&#35201;&#36827;&#34892;&#27169;&#22411;&#21098;&#26525;&#20197;&#33719;&#24471;&#36731;&#37327;&#32423;&#21644;&#39640;&#25928;&#30340;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#21098;&#26525;&#26041;&#27861;&#26159;&#30001;&#32463;&#39564;&#21551;&#21457;&#24335;&#30340;&#65292;&#24456;&#23569;&#32771;&#34385;&#36890;&#36947;&#30340;&#32852;&#21512;&#24433;&#21709;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#30830;&#23450;&#21644;&#27425;&#20248;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#31867;&#24863;&#30693;&#36857;&#27604;&#20248;&#21270;&#30340;&#36890;&#36947;&#21098;&#26525;&#26041;&#27861;&#65288;CATRO&#65289;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#36127;&#25285;&#24182;&#21152;&#36895;&#27169;&#22411;&#25512;&#29702;&#12290;&#21033;&#29992;&#23569;&#37327;&#26679;&#26412;&#30340;&#31867;&#21035;&#20449;&#24687;&#65292;CATRO&#36890;&#36807;&#29305;&#24449;&#31354;&#38388;&#21028;&#21035;&#24230;&#37327;&#22810;&#36890;&#36947;&#30340;&#32852;&#21512;&#24433;&#21709;&#65292;&#24182;&#21512;&#24182;&#20445;&#30041;&#36890;&#36947;&#30340;&#23618;&#27425;&#24433;&#21709;&#12290;&#36890;&#36807;&#23558;&#36890;&#36947;&#21098;&#26525;&#24418;&#24335;&#21270;&#20026;&#23376;&#27169;&#20989;&#25968;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;CATRO&#36890;&#36807;&#20004;&#38454;&#27573;&#36138;&#24515;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;CATRO&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep convolutional neural networks are shown to be overkill with high parametric and computational redundancy in many application scenarios, and an increasing number of works have explored model pruning to obtain lightweight and efficient networks. However, most existing pruning approaches are driven by empirical heuristic and rarely consider the joint impact of channels, leading to unguaranteed and suboptimal performance. In this paper, we propose a novel channel pruning method via Class-Aware Trace Ratio Optimization (CATRO) to reduce the computational burden and accelerate the model inference. Utilizing class information from a few samples, CATRO measures the joint impact of multiple channels by feature space discriminations and consolidates the layer-wise impact of preserved channels. By formulating channel pruning as a submodular set function maximization problem, CATRO solves it efficiently via a two-stage greedy iterative optimization procedure. More importantly, we present theo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32479;&#35745;&#19978;&#24847;&#20041;&#30340;&#36817;&#20284;&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#30740;&#31350;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#25442;&#22120;&#30340;SM&#36817;&#20284;&#22312;&#24067;&#23572;&#30005;&#36335;&#21644;&#22270;&#28789;&#26426;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#22312;&#20110;&#25506;&#32034;&#36817;&#20284;&#32593;&#32476;&#24212;&#35813;&#20855;&#26377;&#33391;&#22909;&#30340;&#32479;&#35745;&#21487;&#23398;&#24615;&#30340;&#27010;&#24565;&#65292;&#36798;&#21040;&#26356;&#26377;&#24847;&#20041;&#30340;&#36817;&#20284;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2107.13163</link><description>&lt;p&gt;
&#32479;&#35745;&#19978;&#24847;&#20041;&#30340;&#36817;&#20284;&#65306;&#19968;&#31181;&#22312;&#21464;&#25442;&#22120;&#20013;&#36817;&#20284;&#22270;&#28789;&#26426;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Statistically Meaningful Approximation: a Case Study on Approximating Turing Machines with Transformers. (arXiv:2107.13163v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.13163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32479;&#35745;&#19978;&#24847;&#20041;&#30340;&#36817;&#20284;&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#30740;&#31350;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#25442;&#22120;&#30340;SM&#36817;&#20284;&#22312;&#24067;&#23572;&#30005;&#36335;&#21644;&#22270;&#28789;&#26426;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#22312;&#20110;&#25506;&#32034;&#36817;&#20284;&#32593;&#32476;&#24212;&#35813;&#20855;&#26377;&#33391;&#22909;&#30340;&#32479;&#35745;&#21487;&#23398;&#24615;&#30340;&#27010;&#24565;&#65292;&#36798;&#21040;&#26356;&#26377;&#24847;&#20041;&#30340;&#36817;&#20284;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35770;&#19978;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30340;&#24120;&#29992;&#26041;&#27861;&#26159;&#20998;&#26512;&#23427;&#20204;&#21487;&#20197;&#36817;&#20284;&#30340;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#36817;&#20284;&#29702;&#35770;&#20013;&#30340;&#26500;&#36896;&#21487;&#33021;&#26159;&#19981;&#29616;&#23454;&#30340;&#65292;&#22240;&#27492;&#24847;&#20041;&#19981;&#22826;&#26126;&#30830;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#32479;&#35745;&#19978;&#24847;&#20041;&#30340;&#65288;SM&#65289;&#36817;&#20284;&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#35201;&#27714;&#36817;&#20284;&#32593;&#32476;&#20855;&#26377;&#33391;&#22909;&#30340;&#32479;&#35745;&#21487;&#23398;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#20989;&#25968;&#31867;&#21035;&#30340;SM&#36817;&#20284;&#65306;&#24067;&#23572;&#30005;&#36335;&#21644;&#22270;&#28789;&#26426;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;SM&#36817;&#20284;&#24067;&#23572;&#30005;&#36335;&#65292;&#37319;&#26679;&#22797;&#26434;&#24230;&#20165;&#21462;&#20915;&#20110;&#30005;&#36335;&#22823;&#23567;&#65292;&#32780;&#19981;&#26159;&#32593;&#32476;&#22823;&#23567;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#21464;&#25442;&#22120;&#21487;&#20197;SM&#36817;&#20284;&#35745;&#31639;&#26102;&#38388;&#21463;$T$&#38480;&#21046;&#30340;&#22270;&#28789;&#26426;&#65292;&#37319;&#26679;&#22797;&#26434;&#24230;&#22810;&#39033;&#24335;&#22320;&#21462;&#20915;&#20110;&#23383;&#27597;&#22823;&#23567;&#12289;&#29366;&#24577;&#31354;&#38388;&#22823;&#23567;&#21644;$\log (T)$&#12290;&#25105;&#20204;&#36824;&#22312;...
&lt;/p&gt;
&lt;p&gt;
A common lens to theoretically study neural net architectures is to analyze the functions they can approximate. However, constructions from approximation theory may be unrealistic and therefore less meaningful. For example, a common unrealistic trick is to encode target function values using infinite precision. To address these issues, this work proposes a formal definition of statistically meaningful (SM) approximation which requires the approximating network to exhibit good statistical learnability. We study SM approximation for two function classes: boolean circuits and Turing machines. We show that overparameterized feedforward neural nets can SM approximate boolean circuits with sample complexity depending only polynomially on the circuit size, not the size of the network. In addition, we show that transformers can SM approximate Turing machines with computation time bounded by $T$ with sample complexity polynomial in the alphabet size, state space size, and $\log (T)$. We also in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#25277;&#35937;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#29992;&#20110;&#35745;&#31639;&#26576;&#31867;&#39640;&#32500;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#20215;&#20540;&#20989;&#25968;&#21644;&#26368;&#20248;&#25511;&#21046;&#12290;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#35745;&#31639;&#30340;&#20960;&#20010;&#25968;&#20540;&#32467;&#26524;&#20063;&#34987;&#23637;&#31034;&#20102;&#20986;&#26469;&#12290;&#21516;&#26102;&#65292;&#21021;&#27493;&#23454;&#29616;&#35777;&#26126;&#35813;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30456;&#23545;&#20110;CPU&#20855;&#26377;&#33391;&#22909;&#30340;&#21152;&#36895;&#25928;&#26524;&#65292;&#20026;&#21033;&#29992;&#19987;&#38376;&#20026;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#39640;&#25928;&#19987;&#29992;&#30828;&#20214;&#35299;&#20915;&#39640;&#32500;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#21644;&#21704;&#23494;&#39039;-&#38597;&#21487;&#27604;&#20559;&#24494;&#20998;&#26041;&#31243;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2105.03336</link><description>&lt;p&gt;
&#20351;&#29992;min-plus&#20195;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#35299;&#20915;&#39640;&#32500;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#21644;&#21704;&#23494;&#39039;-&#38597;&#21487;&#27604;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Neural network architectures using min-plus algebra for solving certain high dimensional optimal control problems and Hamilton-Jacobi PDEs. (arXiv:2105.03336v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.03336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#25277;&#35937;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#29992;&#20110;&#35745;&#31639;&#26576;&#31867;&#39640;&#32500;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#20215;&#20540;&#20989;&#25968;&#21644;&#26368;&#20248;&#25511;&#21046;&#12290;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#35745;&#31639;&#30340;&#20960;&#20010;&#25968;&#20540;&#32467;&#26524;&#20063;&#34987;&#23637;&#31034;&#20102;&#20986;&#26469;&#12290;&#21516;&#26102;&#65292;&#21021;&#27493;&#23454;&#29616;&#35777;&#26126;&#35813;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30456;&#23545;&#20110;CPU&#20855;&#26377;&#33391;&#22909;&#30340;&#21152;&#36895;&#25928;&#26524;&#65292;&#20026;&#21033;&#29992;&#19987;&#38376;&#20026;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#39640;&#25928;&#19987;&#29992;&#30828;&#20214;&#35299;&#20915;&#39640;&#32500;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#21644;&#21704;&#23494;&#39039;-&#38597;&#21487;&#27604;&#20559;&#24494;&#20998;&#26041;&#31243;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#39640;&#32500;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#21450;&#20854;&#23545;&#24212;&#30340;&#21704;&#23494;&#39039;-&#38597;&#21487;&#27604;&#20559;&#24494;&#20998;&#26041;&#31243;&#26159;&#25511;&#21046;&#24037;&#31243;&#20013;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#25277;&#35937;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#20998;&#21035;&#29992;&#20110;&#35745;&#31639;&#26576;&#31867;&#39640;&#32500;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#20215;&#20540;&#20989;&#25968;&#21644;&#26368;&#20248;&#25511;&#21046;&#12290;&#25105;&#20204;&#23545;&#36825;&#20004;&#31181;&#25277;&#35937;&#32467;&#26500;&#36827;&#34892;&#20102;&#25968;&#23398;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20351;&#29992;&#36825;&#20123;&#25277;&#35937;&#32467;&#26500;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#35745;&#31639;&#30340;&#20960;&#20010;&#25968;&#20540;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;FPGA&#19978;&#23545;&#25105;&#20204;&#25552;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#36827;&#34892;&#20102;&#21021;&#27493;&#23454;&#29616;&#65292;&#35777;&#26126;&#20854;&#30456;&#23545;&#20110;CPU&#20855;&#26377;&#33391;&#22909;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#21033;&#29992;&#19987;&#38376;&#20026;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#39640;&#25928;&#19987;&#29992;&#30828;&#20214;&#35299;&#20915;&#39640;&#32500;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#21644;&#21704;&#23494;&#39039;-&#38597;&#21487;&#27604;&#20559;&#24494;&#20998;&#26041;&#31243;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving high dimensional optimal control problems and corresponding Hamilton-Jacobi PDEs are important but challenging problems in control engineering. In this paper, we propose two abstract neural network architectures which are respectively used to compute the value function and the optimal control for certain class of high dimensional optimal control problems. We provide the mathematical analysis for the two abstract architectures. We also show several numerical results computed using the deep neural network implementations of these abstract architectures. A preliminary implementation of our proposed neural network architecture on FPGAs shows promising speed up compared to CPUs. This work paves the way to leverage efficient dedicated hardware designed for neural networks to solve high dimensional optimal control problems and Hamilton-Jacobi PDEs.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#36873;&#25321;&#21644;&#25104;&#26412;&#25910;&#30410;&#30340;&#31995;&#32479;&#25551;&#36848;&#26041;&#24335;&#65292;&#24182;&#20174;&#32534;&#31243;&#35821;&#35328;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#27492;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#23450;&#20041;&#20102;&#20004;&#31181;&#25903;&#25345;&#20915;&#31574;&#25277;&#35937;&#30340;&#23567;&#35821;&#35328;&#65292;&#24182;&#32473;&#20986;&#20102;&#23427;&#20204;&#30340;&#25805;&#20316;&#35821;&#20041;&#21644;&#24213;&#23618;&#35821;&#20041;&#65292;&#24182;&#23558;&#24213;&#23618;&#35821;&#20041;&#22686;&#24378;&#20026;&#36873;&#25321;&#21644;&#27010;&#29575;&#21333;&#23376;&#12290;&#35813;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#20110;&#20004;&#20010;&#31616;&#21333;&#20363;&#23376;&#23637;&#31034;&#20102;&#27492;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2007.08926</link><description>&lt;p&gt;
&#26234;&#33021;&#36873;&#25321;&#21644;&#36873;&#25321;&#21333;&#23376;
&lt;/p&gt;
&lt;p&gt;
Smart Choices and the Selection Monad. (arXiv:2007.08926v8 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.08926
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#36873;&#25321;&#21644;&#25104;&#26412;&#25910;&#30410;&#30340;&#31995;&#32479;&#25551;&#36848;&#26041;&#24335;&#65292;&#24182;&#20174;&#32534;&#31243;&#35821;&#35328;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#27492;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#23450;&#20041;&#20102;&#20004;&#31181;&#25903;&#25345;&#20915;&#31574;&#25277;&#35937;&#30340;&#23567;&#35821;&#35328;&#65292;&#24182;&#32473;&#20986;&#20102;&#23427;&#20204;&#30340;&#25805;&#20316;&#35821;&#20041;&#21644;&#24213;&#23618;&#35821;&#20041;&#65292;&#24182;&#23558;&#24213;&#23618;&#35821;&#20041;&#22686;&#24378;&#20026;&#36873;&#25321;&#21644;&#27010;&#29575;&#21333;&#23376;&#12290;&#35813;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#20110;&#20004;&#20010;&#31616;&#21333;&#20363;&#23376;&#23637;&#31034;&#20102;&#27492;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#36873;&#25321;&#21644;&#30456;&#24212;&#30340;&#25104;&#26412;&#21644;&#25910;&#30410;&#26469;&#25551;&#36848;&#31995;&#32479;&#65292;&#26377;&#26395;&#20351;&#31639;&#27861;&#35774;&#35745;&#20154;&#21592;&#21644;&#31243;&#24207;&#21592;&#20174;&#25351;&#23450;&#22914;&#20309;&#36827;&#34892;&#36873;&#25321;&#20013;&#35299;&#25918;&#20986;&#26469;&#65307;&#22312;&#23454;&#38469;&#23454;&#29616;&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#20248;&#21270;&#25216;&#26415;&#21644;&#36234;&#26469;&#36234;&#22810;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#20123;&#36873;&#25321;&#12290;&#25105;&#20204;&#20174;&#32534;&#31243;&#35821;&#35328;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#20004;&#31181;&#25903;&#25345;&#20915;&#31574;&#25277;&#35937;&#30340;&#23567;&#35821;&#35328;&#65306;&#19968;&#20010;&#20855;&#26377;&#36873;&#25321;&#21644;&#25910;&#30410;&#65292;&#21478;&#19968;&#20010;&#21017;&#39069;&#22806;&#21152;&#20837;&#20102;&#27010;&#29575;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#25805;&#20316;&#35821;&#20041;&#21644;&#24213;&#23618;&#35821;&#20041;&#12290;&#38024;&#23545;&#31532;&#20108;&#31181;&#35821;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19977;&#31181;&#24213;&#23618;&#35821;&#20041;&#65292;&#23427;&#20204;&#22312;&#21487;&#33021;&#30340;&#31243;&#24207;&#20540;&#21644;&#39044;&#26399;&#22238;&#25253;&#20043;&#38388;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#30456;&#20851;&#24615;&#12290;&#25805;&#20316;&#35821;&#20041;&#23558;&#26631;&#20934;&#26500;&#36896;&#30340;&#36890;&#24120;&#35821;&#20041;&#19982;&#21487;&#33021;&#30340;&#25191;&#34892;&#31574;&#30053;&#31354;&#38388;&#30340;&#20248;&#21270;&#30456;&#32467;&#21512;&#12290;&#22522;&#20110;&#36873;&#25321;&#21333;&#23376;&#30340;&#32452;&#21512;&#24213;&#23618;&#35821;&#20041;&#65292;&#22686;&#21152;&#31532;&#20108;&#31181;&#35821;&#35328;&#20013;&#30340;&#27010;&#29575;&#21333;&#23376;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36873;&#25321;&#21333;&#23376;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;&#19968;&#31181;&#29305;&#27530;&#31867;&#22411;&#30340; continuation &#21333;&#23376;&#65292;&#32780;&#35821;&#35328;&#30340;&#25805;&#20316;&#35821;&#20041;&#21644;&#24213;&#23618;&#35821;&#20041;&#21017;&#36890;&#36807;&#21333;&#23376;&#32763;&#35793;&#30456;&#20851;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#20004;&#20010;&#31616;&#21333;&#30340;&#20363;&#23376;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Describing systems in terms of choices and their resulting costs and rewards offers the promise of freeing algorithm designers and programmers from specifying how those choices should be made; in implementations, the choices can be realized by optimization techniques and, increasingly, by machine-learning methods. We study this approach from a programming-language perspective. We define two small languages that support decision-making abstractions: one with choices and rewards, and the other additionally with probabilities. We give both operational and denotational semantics.  In the case of the second language we consider three denotational semantics, with varying degrees of correlation between possible program values and expected rewards. The operational semantics combine the usual semantics of standard constructs with optimization over spaces of possible execution strategies. The denotational semantics, which are compositional, rely on the selection monad, to handle choice, augmente
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;few-shot&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;Earth Mover's Distance&#20316;&#20026;&#34913;&#37327;&#22270;&#20687;&#32467;&#26500;&#36317;&#31163;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#20132;&#21449;&#21442;&#32771;&#26426;&#21046;&#26469;&#29983;&#25104;EMD&#20844;&#24335;&#20013;&#20803;&#32032;&#30340;&#26435;&#37325;&#65292;&#23454;&#29616;&#20102;&#26368;&#20248;&#21305;&#37197;&#27969;&#30340;&#29983;&#25104;&#65292;&#33719;&#24471;&#20102;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2003.06777</link><description>&lt;p&gt;
DeepEMD: &#19981;&#21516;iable Earth Mover's Distance &#29992;&#20110;few-shot&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DeepEMD: Differentiable Earth Mover's Distance for Few-Shot Learning. (arXiv:2003.06777v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2003.06777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;few-shot&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;Earth Mover's Distance&#20316;&#20026;&#34913;&#37327;&#22270;&#20687;&#32467;&#26500;&#36317;&#31163;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#20132;&#21449;&#21442;&#32771;&#26426;&#21046;&#26469;&#29983;&#25104;EMD&#20844;&#24335;&#20013;&#20803;&#32032;&#30340;&#26435;&#37325;&#65292;&#23454;&#29616;&#20102;&#26368;&#20248;&#21305;&#37197;&#27969;&#30340;&#29983;&#25104;&#65292;&#33719;&#24471;&#20102;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#22270;&#20687;&#21306;&#22495;&#20043;&#38388;&#26368;&#20248;&#21305;&#37197;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;few-shot&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;Earth Mover's Distance&#65288;EMD&#65289;&#20316;&#20026;&#34913;&#37327;&#23494;&#38598;&#22270;&#20687;&#34920;&#31034;&#32467;&#26500;&#36317;&#31163;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#20197;&#20915;&#23450;&#22270;&#20687;&#30340;&#30456;&#20284;&#24230;&#12290;EMD&#20026;&#32467;&#26500;&#20803;&#32032;&#29983;&#25104;&#26368;&#20339;&#21305;&#37197;&#27969;&#65292;&#36825;&#20123;&#27969;&#20855;&#26377;&#26368;&#23567;&#30340;&#21305;&#37197;&#20195;&#20215;&#65292;&#29992;&#20110;&#35745;&#31639;&#20998;&#31867;&#30340;&#22270;&#20687;&#36317;&#31163;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20132;&#21449;&#21442;&#32771;&#26426;&#21046;&#26469;&#29983;&#25104;EMD&#20844;&#24335;&#20013;&#20803;&#32032;&#30340;&#26435;&#37325;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#32531;&#35299;&#26434;&#20081;&#32972;&#26223;&#21644;&#22823;&#37327;&#31867;&#20869;&#22806;&#35266;&#21464;&#21270;&#25152;&#24341;&#36215;&#30340;&#19981;&#33391;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#23398;&#20064;&#19968;&#20010;&#32467;&#26500;&#21270;&#20840;&#36830;&#25509;&#23618;&#65292;&#30452;&#25509;&#20351;&#29992;EMD&#23545;&#23494;&#38598;&#22270;&#20687;&#34920;&#31034;&#36827;&#34892;&#20998;&#31867;&#65292;&#20197;&#23454;&#29616;k-shot&#20998;&#31867;&#12290;&#22522;&#20110;&#38544;&#20989;&#25968;&#23450;&#29702;&#65292;&#22312;&#32593;&#32476;&#20013;&#25554;&#20837;EMD&#23618;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;few-shot&#22270;&#20687;&#20998;&#31867;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we develop methods for few-shot image classification from a new perspective of optimal matching between image regions. We employ the Earth Mover's Distance (EMD) as a metric to compute a structural distance between dense image representations to determine image relevance. The EMD generates the optimal matching flows between structural elements that have the minimum matching cost, which is used to calculate the image distance for classification. To generate the important weights of elements in the EMD formulation, we design a cross-reference mechanism, which can effectively alleviate the adverse impact caused by the cluttered background and large intra-class appearance variations. To implement k-shot classification, we propose to learn a structured fully connected layer that can directly classify dense image representations with the EMD. Based on the implicit function theorem, the EMD can be inserted as a layer into the network for end-to-end training. Our extensive experi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#21464;&#20998;Wasserstein&#36136;&#24515;&#35299;&#20915;&#20960;&#20309;&#32858;&#31867;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;Monge WBs&#19982;K-means&#32858;&#31867;&#21644;&#20849;&#21516;&#32858;&#31867;&#30456;&#20851;&#65292;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#38382;&#39064;&#8212;&#8212;&#27491;&#21017;&#21270;K-means&#21644;Wasserstein&#36136;&#24515;&#21387;&#32553;&#65292;&#24182;&#28436;&#31034;&#20102;VWBs&#22312;&#35299;&#20915;&#36825;&#20123;&#32858;&#31867;&#30456;&#20851;&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2002.10543</link><description>&lt;p&gt;
&#21464;&#20998;Wasserstein&#36136;&#24515;&#29992;&#20110;&#20960;&#20309;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Variational Wasserstein Barycenters for Geometric Clustering. (arXiv:2002.10543v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2002.10543
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#21464;&#20998;Wasserstein&#36136;&#24515;&#35299;&#20915;&#20960;&#20309;&#32858;&#31867;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;Monge WBs&#19982;K-means&#32858;&#31867;&#21644;&#20849;&#21516;&#32858;&#31867;&#30456;&#20851;&#65292;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#38382;&#39064;&#8212;&#8212;&#27491;&#21017;&#21270;K-means&#21644;Wasserstein&#36136;&#24515;&#21387;&#32553;&#65292;&#24182;&#28436;&#31034;&#20102;VWBs&#22312;&#35299;&#20915;&#36825;&#20123;&#32858;&#31867;&#30456;&#20851;&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#35299;&#20915;&#20855;&#26377;&#21464;&#20998;&#21407;&#29702;&#30340;Monge&#26144;&#23556;&#26469;&#35745;&#31639;Wasserstein&#36136;&#24515;(WBs)&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;WBs&#30340;&#24230;&#37327;&#29305;&#24615;&#65292;&#24182;&#25506;&#32034;&#23427;&#20204;&#30340;&#32852;&#31995;&#65292;&#29305;&#21035;&#26159;Monge WBs&#19982;K-means&#32858;&#31867;&#21644;&#20849;&#21516;&#32858;&#31867;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;Monge WBs&#22312;&#38750;&#24179;&#34913;&#24230;&#37327;&#21644;&#29699;&#24418;&#22495;&#19978;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#38382;&#39064;&#8212;&#8212;&#27491;&#21017;&#21270;K-means&#21644;Wasserstein&#36136;&#24515;&#21387;&#32553;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#20351;&#29992;VWBs&#35299;&#20915;&#36825;&#20123;&#32858;&#31867;&#30456;&#20851;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose to compute Wasserstein barycenters (WBs) by solving for Monge maps with variational principle. We discuss the metric properties of WBs and explore their connections, especially the connections of Monge WBs, to K-means clustering and co-clustering. We also discuss the feasibility of Monge WBs on unbalanced measures and spherical domains. We propose two new problems -regularized K-means and Wasserstein barycenter compression. We demonstrate the use of VWBs in solving these clustering-related problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;RCTAC&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#38024;&#23545;&#20855;&#26377;&#24050;&#30693;&#21160;&#24577;&#21644;&#26080;&#38480;&#22320;&#24179;&#32447;&#30340;&#38750;&#32447;&#24615;&#36830;&#32493;&#26102;&#38388;&#31995;&#32479;&#25214;&#21040;&#20960;&#20046;&#26368;&#20248;&#31574;&#30053;&#65292;&#19988;&#26080;&#38656;&#25511;&#21046;&#31995;&#32479;&#30340;&#29305;&#23450;&#23646;&#24615;&#25110;&#21021;&#22987;&#31574;&#30053;&#30340;&#21487;&#25509;&#21463;&#24615;&#65292;&#21516;&#26102;&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#25511;&#21046;&#24615;&#33021;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/1909.05402</link><description>&lt;p&gt;
&#38024;&#23545;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#36830;&#32493;&#26102;&#38388;&#26368;&#20248;&#25511;&#21046;&#30340;&#26494;&#24347;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#21450;&#20854;&#25910;&#25947;&#24615;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Relaxed Actor-Critic with Convergence Guarantees for Continuous-Time Optimal Control of Nonlinear Systems. (arXiv:1909.05402v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1909.05402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;RCTAC&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#38024;&#23545;&#20855;&#26377;&#24050;&#30693;&#21160;&#24577;&#21644;&#26080;&#38480;&#22320;&#24179;&#32447;&#30340;&#38750;&#32447;&#24615;&#36830;&#32493;&#26102;&#38388;&#31995;&#32479;&#25214;&#21040;&#20960;&#20046;&#26368;&#20248;&#31574;&#30053;&#65292;&#19988;&#26080;&#38656;&#25511;&#21046;&#31995;&#32479;&#30340;&#29305;&#23450;&#23646;&#24615;&#25110;&#21021;&#22987;&#31574;&#30053;&#30340;&#21487;&#25509;&#21463;&#24615;&#65292;&#21516;&#26102;&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#25511;&#21046;&#24615;&#33021;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26494;&#24347;&#36830;&#32493;&#26102;&#38388;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;RCTAC&#65289;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#20855;&#26377;&#24050;&#30693;&#21160;&#24577;&#21644;&#26080;&#38480;&#22320;&#24179;&#32447;&#30340;&#38750;&#32447;&#24615;&#36830;&#32493;&#26102;&#38388;&#65288;CT&#65289;&#31995;&#32479;&#30340;&#20960;&#20046;&#26368;&#20248;&#31574;&#30053;&#65292;&#20363;&#22914;&#36710;&#36742;&#30340;&#36335;&#24452;&#36319;&#36394;&#25511;&#21046;&#12290; &#19982;&#29616;&#26377;&#30340;&#33258;&#36866;&#24212;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#30456;&#27604;&#65292;RCTAC&#20855;&#26377;&#20960;&#20010;&#20248;&#28857;&#12290;&#23427;&#19981;&#38656;&#35201;&#21021;&#22987;&#21270;&#31574;&#30053;&#30340;&#8220;&#21487;&#25509;&#21463;&#24615;&#8221;&#25110;&#32773;&#25511;&#21046;&#31995;&#32479;&#30340;&#36755;&#20837;&#20223;&#23556;&#24615;&#36136;&#20197;&#23454;&#29616;&#25910;&#25947;&#12290;&#30456;&#21453;&#65292;&#32473;&#23450;&#20219;&#20309;&#21021;&#22987;&#31574;&#30053;&#65292;RCTAC&#37117;&#21487;&#20197;&#25910;&#25947;&#21040;&#19968;&#20010;&#36866;&#29992;&#30340;&#65292;&#38543;&#21518;&#26159;&#20960;&#20046;&#26368;&#20248;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#20855;&#26377;&#39281;&#21644;&#25511;&#21046;&#22120;&#30340;&#19968;&#33324;&#38750;&#32447;&#24615;&#31995;&#32479;&#12290;RCTAC&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#28909;&#36523;&#38454;&#27573;&#21644;&#24191;&#20041;&#31574;&#30053;&#36845;&#20195;&#38454;&#27573;&#12290;&#28909;&#36523;&#38454;&#27573;&#36890;&#36807;&#26368;&#23567;&#21270;&#21704;&#23494;&#39039;&#37327;&#30340;&#24179;&#26041;&#26469;&#23454;&#29616;&#21487;&#25509;&#21463;&#24615;&#65292;&#32780;&#24191;&#20041;&#31574;&#30053;&#36845;&#20195;&#38454;&#27573;&#25918;&#23485;&#20102;&#26356;&#26032;&#32456;&#27490;&#26465;&#20214;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#12290;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#24050;&#34987;&#25552;&#20986;&#24182;&#35777;&#26126;&#12290;&#23545;&#20110;&#36335;&#24452;&#36319;&#36394;&#25511;&#21046;&#21644;&#20498;&#31435;&#25670;&#25511;&#21046;&#38382;&#39064;&#30340;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;RCTAC&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#25511;&#21046;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#33258;&#36866;&#24212;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the Relaxed Continuous-Time Actor-critic (RCTAC) algorithm, a method for finding the nearly optimal policy for nonlinear continuous-time (CT) systems with known dynamics and infinite horizon, such as the path-tracking control of vehicles. RCTAC has several advantages over existing adaptive dynamic programming algorithms for CT systems. It does not require the ``admissibility" of the initialized policy or the input-affine nature of controlled systems for convergence. Instead, given any initial policy, RCTAC can converge to an admissible, and subsequently nearly optimal policy for a general nonlinear system with a saturated controller. RCTAC consists of two phases: a warm-up phase and a generalized policy iteration phase. The warm-up phase minimizes the square of the Hamiltonian to achieve admissibility, while the generalized policy iteration phase relaxes the update termination conditions for faster convergence. The convergence and optimality of the algorithm are pro
&lt;/p&gt;</description></item></channel></rss>