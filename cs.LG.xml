<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#20559;&#20506;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#31532;&#20108;&#27425;&#25171;&#20998;&#26102;&#25552;&#20379;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#12290;&#25105;&#20204;&#21033;&#29992;&#25552;&#31034;&#20449;&#24687;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;boosting&#65292;&#24182;&#37319;&#29992;&#22810;&#20219;&#21153;&#35757;&#32451;&#20197;&#39044;&#27979;&#23454;&#20307;&#31867;&#21035;&#21644;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#25552;&#31034;&#26041;&#27861;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.00723</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#19978;&#19979;&#25991;&#20559;&#20506;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Contextual Biasing of Named-Entities with Large Language Models. (arXiv:2309.00723v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#20559;&#20506;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#31532;&#20108;&#27425;&#25171;&#20998;&#26102;&#25552;&#20379;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#12290;&#25105;&#20204;&#21033;&#29992;&#25552;&#31034;&#20449;&#24687;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;boosting&#65292;&#24182;&#37319;&#29992;&#22810;&#20219;&#21153;&#35757;&#32451;&#20197;&#39044;&#27979;&#23454;&#20307;&#31867;&#21035;&#21644;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#25552;&#31034;&#26041;&#27861;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20013;&#36827;&#34892;&#19978;&#19979;&#25991;&#20559;&#20506;&#65292;&#21363;&#22312;&#31532;&#20108;&#27425;&#25171;&#20998;&#26102;&#20026;LLM&#25552;&#20379;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(ASR)&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#25171;&#20998;&#26399;&#38388;&#21033;&#29992;&#25552;&#31034;&#20449;&#24687;&#23545;LLM&#36827;&#34892;boosting&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#65292;&#36825;&#20123;&#25552;&#31034;&#20449;&#24687;&#21253;&#25324;&#20559;&#20506;&#21015;&#34920;&#21644;&#23569;&#26679;&#26412;&#31034;&#20363;&#65292;&#29992;&#20110;&#22312;&#35745;&#31639;&#20551;&#35774;&#24471;&#20998;&#26102;&#20316;&#20026;&#38468;&#21152;&#20449;&#24687;&#12290;&#38500;&#20102;&#23569;&#26679;&#26412;&#25552;&#31034;&#23398;&#20064;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;LLM&#30340;&#22810;&#20219;&#21153;&#35757;&#32451;&#65292;&#20197;&#39044;&#27979;&#23454;&#20307;&#31867;&#21035;&#21644;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#20026;&#20102;&#25552;&#39640;&#19978;&#19979;&#25991;&#20559;&#20506;&#30340;&#25928;&#29575;&#24182;&#36991;&#20813;&#36229;&#36807;LLMs&#30340;&#26368;&#22823;&#24207;&#21015;&#38271;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#25552;&#31034;&#65292;&#21363;&#20351;&#29992;&#31867;&#21035;&#26631;&#31614;&#39044;&#27979;&#36873;&#25321;&#26368;&#21487;&#33021;&#30340;&#31867;&#21035;&#65292;&#24182;&#20165;&#20351;&#29992;&#36825;&#20010;&#31867;&#21035;&#20013;&#30340;&#23454;&#20307;&#20316;&#20026;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;&#19978;&#19979;&#25991;&#12290;&#23545;&#20869;&#37096;&#30340;&#21628;&#21483;&#12289;&#28040;&#24687;&#21644;&#21475;&#36848;&#25968;&#25454;&#38598;&#20197;&#21450;SLUE-Voxpopuli&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35789;&#38169;&#35823;&#29575;(WER)&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies contextual biasing with Large Language Models (LLMs), where during second-pass rescoring additional contextual information is provided to a LLM to boost Automatic Speech Recognition (ASR) performance. We propose to leverage prompts for a LLM without fine tuning during rescoring which incorporate a biasing list and few-shot examples to serve as additional information when calculating the score for the hypothesis. In addition to few-shot prompt learning, we propose multi-task training of the LLM to predict both the entity class and the next token. To improve the efficiency for contextual biasing and to avoid exceeding LLMs' maximum sequence lengths, we propose dynamic prompting, where we select the most likely class using the class tag prediction, and only use entities in this class as contexts for next token prediction. Word Error Rate (WER) evaluation is performed on i) an internal calling, messaging, and dictation dataset, and ii) the SLUE-Voxpopuli dataset. Results
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20174;&#23433;&#20840;&#20219;&#21153;&#23436;&#25104;&#30340;&#19987;&#23478;&#31034;&#33539;&#20013;&#23398;&#20064;&#20849;&#20139;&#30340;&#23433;&#20840;&#32422;&#26463;&#65292;&#36890;&#36807;&#23558;&#36870;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#25193;&#23637;&#21040;&#32422;&#26463;&#31354;&#38388;&#65292;&#23398;&#20064;&#32422;&#26463;&#20197;&#31105;&#27490;&#19987;&#23478;&#21487;&#33021;&#37319;&#21462;&#20294;&#36873;&#25321;&#19981;&#37319;&#21462;&#30340;&#39640;&#25910;&#30410;&#34892;&#20026;&#65292;&#24182;&#21033;&#29992;&#22810;&#20219;&#21153;&#35774;&#32622;&#20013;&#22810;&#26679;&#21270;&#31034;&#33539;&#23398;&#20064;&#19968;&#32452;&#26356;&#32039;&#23494;&#30340;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2309.00711</link><description>&lt;p&gt;
&#20174;&#22810;&#20219;&#21153;&#31034;&#33539;&#20013;&#23398;&#20064;&#20849;&#20139;&#30340;&#23433;&#20840;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Learning Shared Safety Constraints from Multi-task Demonstrations. (arXiv:2309.00711v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20174;&#23433;&#20840;&#20219;&#21153;&#23436;&#25104;&#30340;&#19987;&#23478;&#31034;&#33539;&#20013;&#23398;&#20064;&#20849;&#20139;&#30340;&#23433;&#20840;&#32422;&#26463;&#65292;&#36890;&#36807;&#23558;&#36870;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#25193;&#23637;&#21040;&#32422;&#26463;&#31354;&#38388;&#65292;&#23398;&#20064;&#32422;&#26463;&#20197;&#31105;&#27490;&#19987;&#23478;&#21487;&#33021;&#37319;&#21462;&#20294;&#36873;&#25321;&#19981;&#37319;&#21462;&#30340;&#39640;&#25910;&#30410;&#34892;&#20026;&#65292;&#24182;&#21033;&#29992;&#22810;&#20219;&#21153;&#35774;&#32622;&#20013;&#22810;&#26679;&#21270;&#31034;&#33539;&#23398;&#20064;&#19968;&#32452;&#26356;&#32039;&#23494;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#35770;&#25105;&#20204;&#24076;&#26395;&#26234;&#33021;&#20307;&#22312;&#29615;&#22659;&#20013;&#25191;&#34892;&#21738;&#39033;&#20219;&#21153;&#65292;&#25105;&#20204;&#36890;&#24120;&#24076;&#26395;&#23427;&#20204;&#36981;&#23432;&#20849;&#20139;&#30340;&#23433;&#20840;&#32422;&#26463;&#12290;&#20363;&#22914;&#65292;&#26080;&#35770;&#26159;&#20570;&#19977;&#26126;&#27835;&#36824;&#26159;&#28165;&#29702;&#26700;&#23376;&#65292;&#21416;&#25151;&#26426;&#22120;&#20154;&#37117;&#19981;&#24212;&#35813;&#25171;&#30772;&#30424;&#23376;&#12290;&#25163;&#21160;&#25351;&#23450;&#36825;&#26679;&#30340;&#32422;&#26463;&#21487;&#33021;&#32791;&#26102;&#19988;&#23481;&#26131;&#20986;&#38169;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#23558;&#36870;&#24378;&#21270;&#23398;&#20064;(IRL)&#25216;&#26415;&#25193;&#23637;&#21040;&#32422;&#26463;&#31354;&#38388;&#26469;&#20174;&#23433;&#20840;&#20219;&#21153;&#23436;&#25104;&#30340;&#19987;&#23478;&#31034;&#33539;&#20013;&#23398;&#20064;&#32422;&#26463;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#25105;&#20204;&#23398;&#20064;&#32422;&#26463;&#20197;&#31105;&#27490;&#19987;&#23478;&#21487;&#33021;&#37319;&#21462;&#20294;&#36873;&#25321;&#19981;&#37319;&#21462;&#30340;&#39640;&#25910;&#30410;&#34892;&#20026;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#32422;&#26463;&#23398;&#20064;&#38382;&#39064;&#36890;&#24120;&#19981;&#26126;&#30830;&#65292;&#24182;&#19988;&#36890;&#24120;&#23548;&#33268;&#36807;&#20110;&#20445;&#23432;&#30340;&#32422;&#26463;&#65292;&#31105;&#27490;&#25152;&#26377;&#19987;&#23478;&#27809;&#26377;&#37319;&#21462;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#22810;&#20219;&#21153;&#35774;&#32622;&#20013;&#33258;&#28982;&#21457;&#29983;&#30340;&#22810;&#26679;&#21270;&#31034;&#33539;&#26469;&#23398;&#20064;&#19968;&#32452;&#26356;&#32039;&#23494;&#30340;&#32422;&#26463;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#29992;&#20223;&#30495;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regardless of the particular task we want them to perform in an environment, there are often shared safety constraints we want our agents to respect. For example, regardless of whether it is making a sandwich or clearing the table, a kitchen robot should not break a plate. Manually specifying such a constraint can be both time-consuming and error-prone. We show how to learn constraints from expert demonstrations of safe task completion by extending inverse reinforcement learning (IRL) techniques to the space of constraints. Intuitively, we learn constraints that forbid highly rewarding behavior that the expert could have taken but chose not to. Unfortunately, the constraint learning problem is rather ill-posed and typically leads to overly conservative constraints that forbid all behavior that the expert did not take. We counter this by leveraging diverse demonstrations that naturally occur in multi-task settings to learn a tighter set of constraints. We validate our method with simula
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#26469;&#22686;&#24378;&#29616;&#26377;&#20132;&#36890;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#31532;&#19968;&#20010;&#29992;&#20110;&#20132;&#36890;&#24314;&#27169;&#20013;&#30495;&#23454;&#24615;&#23545;&#40784;&#30340;&#25968;&#25454;&#38598;&#26469;&#25903;&#25345;&#27492;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2309.00709</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#23454;&#38469;&#20132;&#36890;&#27169;&#25311;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Human Feedback for Realistic Traffic Simulation. (arXiv:2309.00709v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#26469;&#22686;&#24378;&#29616;&#26377;&#20132;&#36890;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#31532;&#19968;&#20010;&#29992;&#20110;&#20132;&#36890;&#24314;&#27169;&#20013;&#30495;&#23454;&#24615;&#23545;&#40784;&#30340;&#25968;&#25454;&#38598;&#26469;&#25903;&#25345;&#27492;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#30495;&#23454;&#19990;&#30028;&#27979;&#35797;&#30340;&#25361;&#25112;&#21644;&#25104;&#26412;&#65292;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#24320;&#21457;&#32773;&#36890;&#24120;&#20381;&#36182;&#27169;&#25311;&#27979;&#35797;&#26469;&#21019;&#24314;&#21487;&#38752;&#30340;&#31995;&#32479;&#12290;&#26377;&#25928;&#27169;&#25311;&#30340;&#20851;&#38190;&#35201;&#32032;&#26159;&#34701;&#20837;&#19982;&#20154;&#31867;&#30693;&#35782;&#30456;&#19968;&#33268;&#30340;&#30495;&#23454;&#20132;&#36890;&#27169;&#22411;&#65292;&#36825;&#19968;&#26041;&#38754;&#22240;&#20026;&#38656;&#35201;&#24179;&#34913;&#30495;&#23454;&#24615;&#21644;&#22810;&#26679;&#24615;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#20559;&#22909;&#65288;RLHF&#65289;&#26469;&#22686;&#24378;&#29616;&#26377;&#20132;&#36890;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#35813;&#30740;&#31350;&#36824;&#30830;&#23450;&#20102;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#25429;&#25417;&#20154;&#31867;&#23545;&#30495;&#23454;&#24615;&#30340;&#24494;&#22937;&#20559;&#22909;&#20197;&#21450;&#32479;&#19968;&#22810;&#26679;&#30340;&#20132;&#36890;&#27169;&#25311;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#23545;&#40784;&#65292;&#24182;&#37319;&#29992;RLHF&#22240;&#20854;&#26679;&#26412;&#25928;&#29575;&#39640;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#20132;&#36890;&#24314;&#27169;&#20013;&#30495;&#23454;&#24615;&#23545;&#40784;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#25903;&#25345;&#27492;&#31867;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21517;&#20026;TrafficRLHF&#65292;&#22312;&#29983;&#25104;&#29616;&#23454;&#19990;&#30028;&#33324;&#30340;&#20132;&#36890;&#27169;&#25311;&#25968;&#25454;&#26041;&#38754;&#23637;&#29616;&#20102;&#20854;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In light of the challenges and costs of real-world testing, autonomous vehicle developers often rely on testing in simulation for the creation of reliable systems. A key element of effective simulation is the incorporation of realistic traffic models that align with human knowledge, an aspect that has proven challenging due to the need to balance realism and diversity. This works aims to address this by developing a framework that employs reinforcement learning with human preference (RLHF) to enhance the realism of existing traffic models. This study also identifies two main challenges: capturing the nuances of human preferences on realism and the unification of diverse traffic simulation models. To tackle these issues, we propose using human feedback for alignment and employ RLHF due to its sample efficiency. We also introduce the first dataset for realism alignment in traffic modeling to support such research. Our framework, named TrafficRLHF, demonstrates its proficiency in generati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#28201;&#24230;&#20316;&#20026;&#19968;&#20010;&#38750;&#37327;&#23376;&#21644;&#38750;&#30456;&#23545;&#35770;&#31890;&#23376;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;GCN&#21644;GAT&#27169;&#22411;&#30340;&#21508;&#20010;&#23618;&#27425;&#20013;&#30340;&#28201;&#24230;&#65292;&#24182;&#35752;&#35770;&#20102;&#21487;&#33021;&#30340;&#26410;&#26469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.00699</link><description>&lt;p&gt;
&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;: &#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#20110;&#28201;&#24230;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Geometric Deep Learning: a Temperature Based Analysis of Graph Neural Networks. (arXiv:2309.00699v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#28201;&#24230;&#20316;&#20026;&#19968;&#20010;&#38750;&#37327;&#23376;&#21644;&#38750;&#30456;&#23545;&#35770;&#31890;&#23376;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;GCN&#21644;GAT&#27169;&#22411;&#30340;&#21508;&#20010;&#23618;&#27425;&#20013;&#30340;&#28201;&#24230;&#65292;&#24182;&#35752;&#35770;&#20102;&#21487;&#33021;&#30340;&#26410;&#26469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35270;&#20026;&#28909;&#21147;&#23398;&#31995;&#32479;&#65292;&#23558;&#26435;&#37325;&#30475;&#20316;&#38750;&#37327;&#23376;&#21644;&#38750;&#30456;&#23545;&#35770;&#31890;&#23376;&#12290;&#25105;&#20204;&#37319;&#29992;&#20043;&#21069;&#22312;[7]&#20013;&#23450;&#20041;&#30340;&#28201;&#24230;&#27010;&#24565;&#65292;&#24182;&#30740;&#31350;&#20102;GCN&#21644;GAT&#27169;&#22411;&#30340;&#21508;&#20010;&#23618;&#27425;&#20013;&#30340;&#28201;&#24230;&#12290;&#35752;&#35770;&#20102;&#25105;&#20204;&#21457;&#29616;&#30340;&#28508;&#22312;&#26410;&#26469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine a Geometric Deep Learning model as a thermodynamic system treating the weights as non-quantum and non-relativistic particles. We employ the notion of temperature previously defined in [7] and study it in the various layers for GCN and GAT models. Potential future applications of our findings are discussed.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#21160;&#24577;&#23398;&#20064;&#20013;&#20849;&#21516;&#25506;&#32034;&#23458;&#25143;&#28418;&#31227;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#21463;&#25511;&#30340;&#27979;&#35797;&#29615;&#22659;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23458;&#25143;&#28418;&#31227;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#32972;&#21518;&#30340;&#26681;&#26412;&#21407;&#22240;&#26159;&#30456;&#20851;&#30340;&#65292;&#24182;&#19988;&#36890;&#36807;&#29983;&#25104;&#19968;&#20010;3D&#22320;&#24418;&#22270;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#36825;&#31181;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.00688</link><description>&lt;p&gt;
&#22312;&#21160;&#24577;&#23398;&#20064;&#20013;&#20849;&#21516;&#25506;&#32034;&#23458;&#25143;&#28418;&#31227;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Jointly Exploring Client Drift and Catastrophic Forgetting in Dynamic Learning. (arXiv:2309.00688v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#21160;&#24577;&#23398;&#20064;&#20013;&#20849;&#21516;&#25506;&#32034;&#23458;&#25143;&#28418;&#31227;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#21463;&#25511;&#30340;&#27979;&#35797;&#29615;&#22659;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23458;&#25143;&#28418;&#31227;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#32972;&#21518;&#30340;&#26681;&#26412;&#21407;&#22240;&#26159;&#30456;&#20851;&#30340;&#65292;&#24182;&#19988;&#36890;&#36807;&#29983;&#25104;&#19968;&#20010;3D&#22320;&#24418;&#22270;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#36825;&#31181;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#21644;&#36830;&#32493;&#23398;&#20064;&#24050;&#32463;&#34987;&#25552;&#20986;&#20316;&#20026;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#31283;&#20581;&#21644;&#38544;&#31169;&#24847;&#35782;&#30340;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#28508;&#22312;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#23458;&#25143;&#28418;&#31227;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#26159;&#20445;&#35777;&#19968;&#33268;&#24615;&#24615;&#33021;&#30340;&#22522;&#26412;&#38556;&#30861;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#21482;&#26159;&#20998;&#21035;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24573;&#35270;&#20102;&#36825;&#20004;&#31181;&#24615;&#33021;&#19979;&#38477;&#32972;&#21518;&#30340;&#26681;&#26412;&#21407;&#22240;&#26159;&#32852;&#31995;&#22312;&#19968;&#36215;&#30340;&#20107;&#23454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#19968;&#20010;&#21463;&#25511;&#30340;&#27979;&#35797;&#29615;&#22659;&#65292;&#20197;&#25506;&#32034;&#23458;&#25143;&#28418;&#31227; - &#36890;&#36807;&#25200;&#21160;&#19968;&#23450;&#27604;&#20363;&#30340;&#23458;&#25143; - &#21644;&#28798;&#38590;&#24615;&#36951;&#24536; - &#36890;&#36807;&#20197;&#29305;&#23450;&#24378;&#24230;&#36801;&#31227;&#25152;&#26377;&#23458;&#25143; - &#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#29983;&#25104;&#19968;&#20010;&#20174;&#20004;&#32773;&#30340;&#32452;&#21512;&#24615;&#33021;&#24433;&#21709;&#20135;&#29983;&#30340;3D&#22320;&#24418;&#22270;&#36827;&#19968;&#27493;&#21033;&#29992;&#36825;&#31181;&#26032;&#30340;&#32452;&#21512;&#20998;&#26512;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;&#23458;&#25143;&#28418;&#31227;&#24341;&#36215;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#36890;&#36807;&#19968;&#23450;&#27604;&#20363;&#30340;&#36801;&#31227;&#23458;&#25143;&#65292;&#19982;&#30001;&#30456;&#24212;&#30340;&#36801;&#31227;&#24378;&#24230;&#24341;&#36215;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24615;&#33021;&#19979;&#38477;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated and Continual Learning have emerged as potential paradigms for the robust and privacy-aware use of Deep Learning in dynamic environments. However, Client Drift and Catastrophic Forgetting are fundamental obstacles to guaranteeing consistent performance. Existing work only addresses these problems separately, which neglects the fact that the root cause behind both forms of performance deterioration is connected. We propose a unified analysis framework for building a controlled test environment for Client Drift -- by perturbing a defined ratio of clients -- and Catastrophic Forgetting -- by shifting all clients with a particular strength. Our framework further leverages this new combined analysis by generating a 3D landscape of the combined performance impact from both. We demonstrate that the performance drop through Client Drift, caused by a certain share of shifted clients, is correlated to the drop from Catastrophic Forgetting resulting from a corresponding shift strength. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20219;&#24847;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#38543;&#26426;&#26497;&#21270;&#30721;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#38543;&#26426;&#25688;&#35201;&#21644;&#26497;&#21270;&#30721;&#30340;&#27010;&#24565;&#65292;&#23454;&#29616;&#20102;&#23545;&#24930;&#35745;&#31639;&#33410;&#28857;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#36866;&#29992;&#20110;&#23454;&#20540;&#25968;&#25454;&#30340;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#20018;&#34892;&#35793;&#30721;&#31639;&#27861;&#21644;&#21363;&#26102;&#20272;&#35745;&#22120;&#65292;&#35813;&#26694;&#26550;&#22312;&#22823;&#35268;&#27169;&#30697;&#38453;&#20056;&#27861;&#21644;&#40657;&#30418;&#20248;&#21270;&#31561;&#22810;&#20010;&#24212;&#29992;&#22330;&#26223;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.00682</link><description>&lt;p&gt;
&#20219;&#24847;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#38543;&#26426;&#26497;&#21270;&#30721;
&lt;/p&gt;
&lt;p&gt;
Randomized Polar Codes for Anytime Distributed Machine Learning. (arXiv:2309.00682v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00682
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20219;&#24847;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#38543;&#26426;&#26497;&#21270;&#30721;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#38543;&#26426;&#25688;&#35201;&#21644;&#26497;&#21270;&#30721;&#30340;&#27010;&#24565;&#65292;&#23454;&#29616;&#20102;&#23545;&#24930;&#35745;&#31639;&#33410;&#28857;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#36866;&#29992;&#20110;&#23454;&#20540;&#25968;&#25454;&#30340;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#20018;&#34892;&#35793;&#30721;&#31639;&#27861;&#21644;&#21363;&#26102;&#20272;&#35745;&#22120;&#65292;&#35813;&#26694;&#26550;&#22312;&#22823;&#35268;&#27169;&#30697;&#38453;&#20056;&#27861;&#21644;&#40657;&#30418;&#20248;&#21270;&#31561;&#22810;&#20010;&#24212;&#29992;&#22330;&#26223;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#35745;&#31639;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#25269;&#25239;&#24930;&#35745;&#31639;&#33410;&#28857;&#65292;&#24182;&#33021;&#22815;&#36827;&#34892;&#32447;&#24615;&#25805;&#20316;&#30340;&#36817;&#20284;&#21644;&#31934;&#30830;&#35745;&#31639;&#12290;&#25152;&#25552;&#20986;&#30340;&#26426;&#21046;&#22312;&#32534;&#30721;&#35745;&#31639;&#30340;&#32972;&#26223;&#19979;&#65292;&#32467;&#21512;&#20102;&#38543;&#26426;&#25688;&#35201;&#21644;&#26497;&#21270;&#30721;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#23454;&#20540;&#25968;&#25454;&#30340;&#20018;&#34892;&#35793;&#30721;&#31639;&#27861;&#65292;&#20197;&#32500;&#25345;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#29992;&#20110;&#24674;&#22797;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#21363;&#26102;&#20272;&#35745;&#22120;&#65292;&#21363;&#20351;&#21487;&#29992;&#33410;&#28857;&#36755;&#20986;&#26080;&#27861;&#35793;&#30721;&#65292;&#20063;&#33021;&#22815;&#29983;&#25104;&#21487;&#35777;&#26126;&#20934;&#30830;&#30340;&#20272;&#35745;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#22312;&#21508;&#31181;&#24773;&#22659;&#19979;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#22914;&#22823;&#35268;&#27169;&#30697;&#38453;&#20056;&#27861;&#21644;&#40657;&#30418;&#20248;&#21270;&#12290;&#25105;&#20204;&#22312;&#26080;&#26381;&#21153;&#22120;&#20113;&#35745;&#31639;&#31995;&#32479;&#19978;&#23454;&#29616;&#20102;&#36825;&#20123;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#25968;&#20540;&#32467;&#26524;&#26469;&#23637;&#31034;&#23427;&#20204;&#22312;&#23454;&#36341;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#21253;&#25324;ImageNet&#35268;&#27169;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel distributed computing framework that is robust to slow compute nodes, and is capable of both approximate and exact computation of linear operations. The proposed mechanism integrates the concepts of randomized sketching and polar codes in the context of coded computation. We propose a sequential decoding algorithm designed to handle real valued data while maintaining low computational complexity for recovery. Additionally, we provide an anytime estimator that can generate provably accurate estimates even when the set of available node outputs is not decodable. We demonstrate the potential applications of this framework in various contexts, such as large-scale matrix multiplication and black-box optimization. We present the implementation of these methods on a serverless cloud computing system and provide numerical results to demonstrate their scalability in practice, including ImageNet scale computations.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#30446;&#30340;&#22312;&#20110;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24773;&#22659;&#24847;&#35782;&#30340;&#20135;&#29983;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#27169;&#22411;&#24773;&#22659;&#24847;&#35782;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.00667</link><description>&lt;p&gt;
&#33073;&#31163;&#19978;&#19979;&#25991;&#30340;&#24433;&#21709;&#65306;&#20851;&#20110;&#34913;&#37327;LLMs&#20013;&#30340;&#24773;&#22659;&#24847;&#35782;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Taken out of context: On measuring situational awareness in LLMs. (arXiv:2309.00667v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00667
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#30446;&#30340;&#22312;&#20110;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24773;&#22659;&#24847;&#35782;&#30340;&#20135;&#29983;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#27169;&#22411;&#24773;&#22659;&#24847;&#35782;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26088;&#22312;&#26356;&#22909;&#22320;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#8220;&#24773;&#22659;&#24847;&#35782;&#8221;&#30340;&#20986;&#29616;&#12290;&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#22312;&#24847;&#35782;&#21040;&#33258;&#24049;&#26159;&#19968;&#20010;&#27169;&#22411;&#30340;&#21516;&#26102;&#65292;&#33021;&#22815;&#35782;&#21035;&#33258;&#24049;&#24403;&#21069;&#26159;&#22788;&#20110;&#27979;&#35797;&#25110;&#37096;&#32626;&#29366;&#24577;&#65292;&#37027;&#20040;&#36825;&#20010;&#27169;&#22411;&#22312;&#24773;&#22659;&#19978;&#26159;&#20855;&#22791;&#24847;&#35782;&#30340;&#12290;&#20170;&#22825;&#30340;LLMs&#22312;&#37096;&#32626;&#20043;&#21069;&#20250;&#32463;&#36807;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#30340;&#27979;&#35797;&#12290;&#22312;&#37096;&#32626;&#21518;&#65292;&#19968;&#20010;LLM&#21487;&#33021;&#20250;&#21033;&#29992;&#24773;&#22659;&#24847;&#35782;&#22312;&#23433;&#20840;&#27979;&#35797;&#20013;&#21462;&#24471;&#39640;&#20998;&#65292;&#20294;&#22312;&#23454;&#38469;&#20351;&#29992;&#20013;&#37319;&#21462;&#26377;&#23475;&#34892;&#20026;&#12290;&#24773;&#22659;&#24847;&#35782;&#21487;&#33021;&#20250;&#24847;&#22806;&#22320;&#22312;&#27169;&#22411;&#25193;&#23637;&#20013;&#20986;&#29616;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#39044;&#27979;&#36825;&#31181;&#20986;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#20110;&#24773;&#22659;&#24847;&#35782;&#32780;&#35328;&#24517;&#35201;&#30340;&#33021;&#21147;&#20043;&#19968;&#65292;&#21363;&#8220;&#33073;&#31163;&#19978;&#19979;&#25991;&#25512;&#29702;&#8221;&#65288;&#19982;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#23398;&#20064;&#30456;&#23545;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#20102;&#33073;&#31163;&#19978;&#19979;&#25991;&#25512;&#29702;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#27809;&#26377;&#25552;&#20379;&#20219;&#20309;&#31034;&#20363;&#25110;&#28436;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;LLM&#36827;&#34892;&#20102;&#25551;&#36848;&#27979;&#35797;&#30340;&#24494;&#35843;&#12290;&#22312;&#27979;&#35797;&#26102;&#65292;&#25105;&#20204;&#35780;&#20272;&#27169;&#22411;&#26159;&#21542;&#33021;&#36890;&#36807;&#27979;&#35797;&#12290;&#20196;&#25105;&#20204;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
We aim to better understand the emergence of `situational awareness' in large language models (LLMs). A model is situationally aware if it's aware that it's a model and can recognize whether it's currently in testing or deployment. Today's LLMs are tested for safety and alignment before they are deployed. An LLM could exploit situational awareness to achieve a high score on safety tests, while taking harmful actions after deployment. Situational awareness may emerge unexpectedly as a byproduct of model scaling. One way to better foresee this emergence is to run scaling experiments on abilities necessary for situational awareness. As such an ability, we propose `out-of-context reasoning' (in contrast to in-context learning). We study out-of-context reasoning experimentally. First, we finetune an LLM on a description of a test while providing no examples or demonstrations. At test time, we assess whether the model can pass the test. To our surprise, we find that LLMs succeed on this out-
&lt;/p&gt;</description></item><item><title>ICDARTS&#25913;&#36827;&#20102;&#24490;&#29615;DARTS&#30340;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#65292;&#36890;&#36807;&#28040;&#38500;&#35780;&#20272;&#32593;&#32476;&#26435;&#37325;&#23545;&#25628;&#32034;&#32593;&#32476;&#26435;&#37325;&#30340;&#20381;&#36182;&#65292;&#24182;&#24341;&#20837;&#20462;&#25913;&#21518;&#30340;&#25628;&#32034;&#31163;&#25955;&#21270;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2309.00664</link><description>&lt;p&gt;
ICDARTS: &#25913;&#36827;&#24490;&#29615;DARTS&#30340;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
ICDARTS: Improving the Stability and Performance of Cyclic DARTS. (arXiv:2309.00664v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00664
&lt;/p&gt;
&lt;p&gt;
ICDARTS&#25913;&#36827;&#20102;&#24490;&#29615;DARTS&#30340;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#65292;&#36890;&#36807;&#28040;&#38500;&#35780;&#20272;&#32593;&#32476;&#26435;&#37325;&#23545;&#25628;&#32034;&#32593;&#32476;&#26435;&#37325;&#30340;&#20381;&#36182;&#65292;&#24182;&#24341;&#20837;&#20462;&#25913;&#21518;&#30340;&#25628;&#32034;&#31163;&#25955;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25913;&#36827;&#20102;&#24490;&#29615;DARTS&#65288;CDARTS&#65289;&#30340;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;CDARTS&#26159;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#20998;&#26550;&#26500;&#25628;&#32034;&#65288;DARTS&#65289;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26041;&#27861;&#65292;&#20351;&#29992;&#24490;&#29615;&#21453;&#39304;&#26426;&#21046;&#21516;&#26102;&#35757;&#32451;&#25628;&#32034;&#21644;&#35780;&#20272;&#32593;&#32476;&#12290;&#35813;&#35757;&#32451;&#21327;&#35758;&#26088;&#22312;&#36890;&#36807;&#24378;&#21046;&#35201;&#27714;&#25628;&#32034;&#21644;&#35780;&#20272;&#32593;&#32476;&#20135;&#29983;&#30456;&#20284;&#30340;&#36755;&#20986;&#26469;&#20248;&#21270;&#25628;&#32034;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;CDARTS&#24341;&#20837;&#20102;&#19968;&#20010;&#20381;&#36182;&#20110;&#25628;&#32034;&#32593;&#32476;&#30340;&#35780;&#20272;&#32593;&#32476;&#25439;&#22833;&#20989;&#25968;&#12290;&#25628;&#32034;&#21644;&#37325;&#26032;&#35757;&#32451;&#38454;&#27573;&#35780;&#20272;&#32593;&#32476;&#20351;&#29992;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#19981;&#30456;&#20284;&#24615;&#23548;&#33268;&#25628;&#32034;&#38454;&#27573;&#35780;&#20272;&#32593;&#32476;&#25104;&#20026;&#37325;&#26032;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#30340;&#26368;&#32456;&#35780;&#20272;&#32593;&#32476;&#30340;&#27425;&#20248;&#20195;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;ICDARTS&#65292;&#19968;&#31181;&#20462;&#27491;&#30340;&#26041;&#27861;&#65292;&#28040;&#38500;&#20102;&#35780;&#20272;&#32593;&#32476;&#26435;&#37325;&#23545;&#25628;&#32034;&#32593;&#32476;&#26435;&#37325;&#30340;&#20381;&#36182;&#65292;&#20197;&#21450;&#19968;&#31181;&#20462;&#25913;&#30340;&#25628;&#32034;&#31163;&#25955;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces improvements to the stability and generalizability of Cyclic DARTS (CDARTS). CDARTS is a Differentiable Architecture Search (DARTS)-based approach to neural architecture search (NAS) that uses a cyclic feedback mechanism to train search and evaluation networks concurrently. This training protocol aims to optimize the search process by enforcing that the search and evaluation networks produce similar outputs. However, CDARTS introduces a loss function for the evaluation network that is dependent on the search network. The dissimilarity between the loss functions used by the evaluation networks during the search and retraining phases results in a search-phase evaluation network that is a sub-optimal proxy for the final evaluation network that is utilized during retraining. We present ICDARTS, a revised approach that eliminates the dependency of the evaluation network weights upon those of the search network, along with a modified process for discretizing the search n
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22810;&#39033;&#24335;&#27169;&#22411;&#30340;&#40657;&#30418;&#20248;&#21270;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#25311;&#21512;&#22810;&#39033;&#24335;&#20195;&#29702;&#23547;&#25214;&#26410;&#30693;&#31995;&#32479;&#30340;&#26368;&#20339;&#21442;&#25968;&#65292;&#25104;&#21151;&#31454;&#20105;&#24182;&#36229;&#36234;&#20854;&#20182;&#31639;&#27861;&#65292;&#22312;&#40657;&#30418;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#20915;&#20013;&#20855;&#26377;&#20851;&#38190;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.00663</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#39033;&#24335;&#27169;&#22411;&#30340;&#40657;&#30418;&#30446;&#26631;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Polynomial-Model-Based Optimization for Blackbox Objectives. (arXiv:2309.00663v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00663
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22810;&#39033;&#24335;&#27169;&#22411;&#30340;&#40657;&#30418;&#20248;&#21270;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#25311;&#21512;&#22810;&#39033;&#24335;&#20195;&#29702;&#23547;&#25214;&#26410;&#30693;&#31995;&#32479;&#30340;&#26368;&#20339;&#21442;&#25968;&#65292;&#25104;&#21151;&#31454;&#20105;&#24182;&#36229;&#36234;&#20854;&#20182;&#31639;&#27861;&#65292;&#22312;&#40657;&#30418;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#20915;&#20013;&#20855;&#26377;&#20851;&#38190;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20687;&#31070;&#32463;&#32593;&#32476;&#25110;&#22797;&#26434;&#27169;&#25311;&#36825;&#26679;&#30340;&#31995;&#32479;&#32467;&#26500;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#26159;&#26410;&#30693;&#30340;&#65292;&#36817;&#20284;&#26159;&#26114;&#36149;&#29978;&#33267;&#19981;&#21487;&#33021;&#23454;&#29616;&#30340;&#12290;&#40657;&#30418;&#20248;&#21270;&#26088;&#22312;&#25214;&#21040;&#36825;&#20123;&#31995;&#32479;&#30340;&#26368;&#20339;&#65288;&#36229;&#65289;&#21442;&#25968;&#65292;&#20351;&#39044;&#23450;&#20041;&#30340;&#30446;&#26631;&#20989;&#25968;&#26368;&#23567;&#21270;&#12290;&#22522;&#20110;&#22810;&#39033;&#24335;&#27169;&#22411;&#30340;&#20248;&#21270;&#65288;PMBO&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#40657;&#30418;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#25311;&#21512;&#22810;&#39033;&#24335;&#20195;&#29702;&#26469;&#23547;&#25214;&#26368;&#23567;&#20540;&#12290;&#21463;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#21551;&#21457;&#65292;&#35813;&#27169;&#22411;&#26681;&#25454;&#39044;&#26399;&#25913;&#36827;&#30340;&#37319;&#38598;&#20989;&#25968;&#36827;&#34892;&#36845;&#20195;&#26356;&#26032;&#65292;&#20174;&#32780;&#24179;&#34913;&#20102;&#24320;&#21457;&#21644;&#25506;&#32034;&#29575;&#65292;&#24182;&#25552;&#20379;&#20102;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#23545;&#20110;&#32473;&#23450;&#30340;&#19968;&#32452;&#20154;&#24037;&#12289;&#20998;&#26512;&#20989;&#25968;&#65292;PMBO&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;PMBO&#25104;&#21151;&#22320;&#19982;&#36825;&#20123;&#31639;&#27861;&#31454;&#20105;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#36229;&#36234;&#20102;&#23427;&#20204;&#12290;&#26681;&#25454;&#32467;&#26524;&#65292;&#25105;&#20204;&#35748;&#20026;PMBO&#26159;&#35299;&#20915;&#40657;&#30418;&#20248;&#21270;&#38382;&#39064;&#30340;&#20851;&#38190;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
For a wide range of applications the structure of systems like Neural Networks or complex simulations, is unknown and approximation is costly or even impossible. Black-box optimization seeks to find optimal (hyper-) parameters for these systems such that a pre-defined objective function is minimized. Polynomial-Model-Based Optimization (PMBO) is a novel blackbox optimizer that finds the minimum by fitting a polynomial surrogate to the objective function.  Motivated by Bayesian optimization the model is iteratively updated according to the acquisition function Expected Improvement, thus balancing the exploitation and exploration rate and providing an uncertainty estimate of the model. PMBO is benchmarked against other state-of-the-art algorithms for a given set of artificial, analytical functions. PMBO competes successfully with those algorithms and even outperforms all of them in some cases. As the results suggest, we believe PMBO is the pivotal choice for solving blackbox optimization
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38646;&#21644;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#20013;&#23398;&#20064;&#949;-&#26368;&#20248;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#22266;&#23450;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#33258;&#36866;&#24212;&#30340;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#36827;&#34892;&#23616;&#37096;&#26356;&#26032;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;&#25910;&#25947;&#36895;&#24230;&#20026;$\tilde{\mathcal{O}}(T^{-1/2})$&#30340;&#32467;&#26524;&#65292;&#24182;&#22312;&#26368;&#20339;&#31574;&#30053;&#19979;&#23545;&#28216;&#25103;&#21442;&#25968;&#20855;&#26377;&#36817;&#20046;&#26368;&#20248;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.00656</link><description>&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#24102;&#26377;&#36712;&#36857;&#21453;&#39304;&#30340;&#38646;&#21644;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#20013;&#22914;&#20309;&#23398;&#20064;&#949;-&#26368;&#20248;&#31574;&#30053;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#29609;&#23478;&#26681;&#25454;&#20182;&#20204;&#22312;&#22266;&#23450;&#25968;&#37327;&#30340;&#22238;&#21512;&#20013;&#35266;&#23519;&#21040;&#30340;&#20449;&#24687;&#20381;&#27425;&#26356;&#26032;&#31574;&#30053;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#30001;&#20110;&#20351;&#29992;&#20102;&#37325;&#35201;&#24615;&#37319;&#26679;&#26469;&#23545;&#21160;&#20316;&#24207;&#21015;&#36827;&#34892;&#20272;&#35745;&#65292;&#23548;&#33268;&#26041;&#24046;&#36739;&#39640;&#12290;&#20026;&#20102;&#20943;&#23567;&#36825;&#31181;&#26041;&#24046;&#65292;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#19968;&#31181;&#22266;&#23450;&#37319;&#26679;&#26041;&#27861;&#65292;&#21363;&#29609;&#23478;&#22312;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#26356;&#26032;&#31574;&#30053;&#65292;&#20294;&#35266;&#23519;&#21040;&#30340;&#20449;&#24687;&#26159;&#36890;&#36807;&#32473;&#23450;&#30340;&#22266;&#23450;&#37319;&#26679;&#31574;&#30053;&#33719;&#24471;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#65288;OMD&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;OMD&#24212;&#29992;&#20110;&#27599;&#20010;&#20449;&#24687;&#38598;&#65292;&#20351;&#29992;&#36880;&#28176;&#20943;&#23567;&#30340;&#23398;&#20064;&#29575;&#21644;&#27491;&#21017;&#21270;&#25439;&#22833;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#39640;&#27010;&#29575;&#19979;&#20855;&#26377;&#25910;&#25947;&#36895;&#24230;&#20026;$\tilde{\mathcal{O}}(T^{-1/2})$&#65292;&#24182;&#22312;&#24212;&#29992;&#26368;&#20339;&#31574;&#30053;&#26102;&#23545;&#28216;&#25103;&#21442;&#25968;&#20855;&#26377;&#36817;&#20046;&#26368;&#20248;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Local and adaptive mirror descents in extensive-form games. (arXiv:2309.00656v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38646;&#21644;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#20013;&#23398;&#20064;&#949;-&#26368;&#20248;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#22266;&#23450;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#33258;&#36866;&#24212;&#30340;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#36827;&#34892;&#23616;&#37096;&#26356;&#26032;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;&#25910;&#25947;&#36895;&#24230;&#20026;$\tilde{\mathcal{O}}(T^{-1/2})$&#30340;&#32467;&#26524;&#65292;&#24182;&#22312;&#26368;&#20339;&#31574;&#30053;&#19979;&#23545;&#28216;&#25103;&#21442;&#25968;&#20855;&#26377;&#36817;&#20046;&#26368;&#20248;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#24102;&#26377;&#36712;&#36857;&#21453;&#39304;&#30340;&#38646;&#21644;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#20013;&#23398;&#20064;&#949;-&#26368;&#20248;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#26041;&#27861;&#30001;&#20110;&#20351;&#29992;&#20102;&#37325;&#35201;&#24615;&#37319;&#26679;&#65292;&#23384;&#22312;&#26041;&#24046;&#36739;&#39640;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20943;&#23567;&#26041;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22266;&#23450;&#37319;&#26679;&#26041;&#27861;&#65292;&#20351;&#29992;&#22266;&#23450;&#37319;&#26679;&#31574;&#30053;&#26469;&#35266;&#23519;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#33258;&#36866;&#24212;&#30340;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#65292;&#23545;&#27599;&#20010;&#20449;&#24687;&#38598;&#36827;&#34892;&#23616;&#37096;&#26356;&#26032;&#65292;&#24182;&#20351;&#29992;&#36880;&#28176;&#20943;&#23567;&#30340;&#23398;&#20064;&#29575;&#21644;&#27491;&#21017;&#21270;&#25439;&#22833;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#39640;&#27010;&#29575;&#19979;&#20855;&#26377;&#25910;&#25947;&#36895;&#24230;&#20026;$\tilde{\mathcal{O}}(T^{-1/2})$&#65292;&#24182;&#22312;&#26368;&#20339;&#31574;&#30053;&#19979;&#23545;&#28216;&#25103;&#21442;&#25968;&#20855;&#26377;&#36817;&#20046;&#26368;&#20248;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study how to learn $\epsilon$-optimal strategies in zero-sum imperfect information games (IIG) with trajectory feedback. In this setting, players update their policies sequentially based on their observations over a fixed number of episodes, denoted by $T$. Existing procedures suffer from high variance due to the use of importance sampling over sequences of actions (Steinberger et al., 2020; McAleer et al., 2022). To reduce this variance, we consider a fixed sampling approach, where players still update their policies over time, but with observations obtained through a given fixed sampling policy. Our approach is based on an adaptive Online Mirror Descent (OMD) algorithm that applies OMD locally to each information set, using individually decreasing learning rates and a regularized loss. We show that this approach guarantees a convergence rate of $\tilde{\mathcal{O}}(T^{-1/2})$ with high probability and has a near-optimal dependence on the game parameters when applied with the best 
&lt;/p&gt;</description></item><item><title>&#21512;&#25104;&#25968;&#25454;&#26159;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;AI&#27169;&#22411;&#30340;&#20154;&#24037;&#29983;&#25104;&#25968;&#25454;&#65292;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#25552;&#20379;&#20102;&#30740;&#31350;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#19982;&#20943;&#23569;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20559;&#35265;&#30340;&#26426;&#20250;&#65292;&#20294;&#21516;&#26102;&#20063;&#38656;&#35201;&#21046;&#23450;&#21512;&#36866;&#30340;&#25919;&#31574;&#26469;&#30830;&#20445;&#25968;&#25454;&#36136;&#37327;&#12289;&#30495;&#23454;&#24615;&#20197;&#21450;&#24179;&#34913;&#38544;&#31169;&#19982;&#25968;&#25454;&#25928;&#29992;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.00652</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#26469;&#35757;&#32451;AI&#27169;&#22411;&#65306;&#21487;&#25345;&#32493;&#21457;&#23637;&#30340;&#26426;&#36935;&#19982;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
The Use of Synthetic Data to Train AI Models: Opportunities and Risks for Sustainable Development. (arXiv:2309.00652v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00652
&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#26159;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;AI&#27169;&#22411;&#30340;&#20154;&#24037;&#29983;&#25104;&#25968;&#25454;&#65292;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#25552;&#20379;&#20102;&#30740;&#31350;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#19982;&#20943;&#23569;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20559;&#35265;&#30340;&#26426;&#20250;&#65292;&#20294;&#21516;&#26102;&#20063;&#38656;&#35201;&#21046;&#23450;&#21512;&#36866;&#30340;&#25919;&#31574;&#26469;&#30830;&#20445;&#25968;&#25454;&#36136;&#37327;&#12289;&#30495;&#23454;&#24615;&#20197;&#21450;&#24179;&#34913;&#38544;&#31169;&#19982;&#25968;&#25454;&#25928;&#29992;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#25968;&#25454;&#39537;&#21160;&#30340;&#26102;&#20195;&#65292;&#21512;&#25104;&#25968;&#25454;&#65292;&#21363;&#20154;&#24037;&#29983;&#25104;&#30340;&#31867;&#20284;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#29305;&#24449;&#20294;&#19981;&#21253;&#21547;&#23454;&#38469;&#20010;&#20154;&#20449;&#24687;&#30340;&#25968;&#25454;&#65292;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#12290;&#36825;&#26159;&#22240;&#20026;&#23427;&#26377;&#28508;&#21147;&#20445;&#25252;&#38544;&#31169;&#12289;&#22686;&#21152;&#30740;&#31350;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#24182;&#20943;&#23569;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21512;&#25104;&#25968;&#25454;&#30340;&#21019;&#24314;&#12289;&#21033;&#29992;&#21644;&#20256;&#25773;&#25919;&#31574;&#12290;&#21512;&#25104;&#25968;&#25454;&#21487;&#20197;&#26159;&#20445;&#25252;&#20010;&#20154;&#38544;&#31169;&#30340;&#26377;&#21147;&#24037;&#20855;&#65292;&#20294;&#20063;&#38754;&#20020;&#25361;&#25112;&#65292;&#22914;&#30830;&#20445;&#20854;&#36136;&#37327;&#21644;&#30495;&#23454;&#24615;&#12290;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#21512;&#25104;&#25968;&#25454;&#25919;&#31574;&#24517;&#39035;&#22312;&#38544;&#31169;&#20851;&#27880;&#21644;&#25968;&#25454;&#25928;&#29992;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#30830;&#20445;&#22312;&#19981;&#25439;&#23475;&#36947;&#24503;&#25110;&#27861;&#24459;&#26631;&#20934;&#30340;&#21069;&#25552;&#19979;&#26377;&#25928;&#21033;&#29992;&#25968;&#25454;&#12290;&#32452;&#32455;&#21644;&#26426;&#26500;&#24517;&#39035;&#21046;&#23450;&#26631;&#20934;&#21270;&#30340;&#25351;&#23548;&#26041;&#38024;&#21644;&#26368;&#20339;&#23454;&#36341;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#30340;&#22909;&#22788;&#24182;&#35299;&#20915;&#20854;&#20013;&#30340;&#22266;&#26377;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the current data driven era, synthetic data, artificially generated data that resembles the characteristics of real world data without containing actual personal information, is gaining prominence. This is due to its potential to safeguard privacy, increase the availability of data for research, and reduce bias in machine learning models. This paper investigates the policies governing the creation, utilization, and dissemination of synthetic data. Synthetic data can be a powerful instrument for protecting the privacy of individuals, but it also presents challenges, such as ensuring its quality and authenticity. A well crafted synthetic data policy must strike a balance between privacy concerns and the utility of data, ensuring that it can be utilized effectively without compromising ethical or legal standards. Organizations and institutions must develop standardized guidelines and best practices in order to capitalize on the benefits of synthetic data while addressing its inherent c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#23567;&#22411;&#20851;&#38190;&#35789;&#26816;&#27979;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#36741;&#21161;&#25968;&#25454;&#36827;&#34892;&#30417;&#30563;&#65292;&#21033;&#29992;&#33258;&#21160;&#26631;&#27880;&#21644;&#31579;&#36873;&#25968;&#25454;&#26500;&#24314;&#20102;&#19968;&#20010;&#31867;&#20284;&#20851;&#38190;&#35789;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#25552;&#21319;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#21462;&#24471;&#20102;&#22312;&#23569;&#26679;&#26412;&#20851;&#38190;&#35789;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#31454;&#20105;&#26041;&#27861;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.00647</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#36741;&#21161;&#25968;&#25454;&#36827;&#34892;&#30417;&#30563;&#65292;&#25913;&#36827;&#23567;&#22411;&#20851;&#38190;&#35789;&#26816;&#27979;&#27169;&#22411;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Improving Small Footprint Few-shot Keyword Spotting with Supervision on Auxiliary Data. (arXiv:2309.00647v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#23567;&#22411;&#20851;&#38190;&#35789;&#26816;&#27979;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#36741;&#21161;&#25968;&#25454;&#36827;&#34892;&#30417;&#30563;&#65292;&#21033;&#29992;&#33258;&#21160;&#26631;&#27880;&#21644;&#31579;&#36873;&#25968;&#25454;&#26500;&#24314;&#20102;&#19968;&#20010;&#31867;&#20284;&#20851;&#38190;&#35789;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#25552;&#21319;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#21462;&#24471;&#20102;&#22312;&#23569;&#26679;&#26412;&#20851;&#38190;&#35789;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#31454;&#20105;&#26041;&#27861;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#20851;&#38190;&#35789;&#26816;&#27979;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#26631;&#27880;&#25968;&#25454;&#38598;&#25165;&#33021;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#30446;&#26631;&#20851;&#38190;&#35789;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20851;&#38190;&#35789;&#26816;&#27979;&#25968;&#25454;&#38598;&#22312;&#35268;&#27169;&#19978;&#23384;&#22312;&#38480;&#21046;&#65292;&#24182;&#19988;&#25910;&#38598;&#31867;&#20284;&#20851;&#38190;&#35789;&#30340;&#26631;&#27880;&#25968;&#25454;&#26159;&#19968;&#39033;&#26114;&#36149;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#23481;&#26131;&#25910;&#38598;&#30340;&#26080;&#26631;&#27880;&#38405;&#35835;&#35821;&#38899;&#25968;&#25454;&#20316;&#20026;&#36741;&#21161;&#26469;&#28304;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#24191;&#27867;&#24212;&#29992;&#20110;&#20174;&#26080;&#26631;&#27880;&#25968;&#25454;&#20013;&#23398;&#20064;&#34920;&#24449;&#65292;&#20294;&#26159;&#23427;&#36866;&#29992;&#20110;&#20855;&#26377;&#36275;&#22815;&#23481;&#37327;&#30340;&#22823;&#27169;&#22411;&#65292;&#24182;&#19981;&#36866;&#21512;&#35757;&#32451;&#23567;&#22411;&#30340;&#20851;&#38190;&#35789;&#26816;&#27979;&#27169;&#22411;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#21160;&#26631;&#27880;&#21644;&#31579;&#36873;&#25968;&#25454;&#26469;&#26500;&#24314;&#19968;&#20010;&#31867;&#20284;&#20851;&#38190;&#35789;&#30340;&#25968;&#25454;&#38598;LibriWord&#65292;&#23454;&#29616;&#23545;&#36741;&#21161;&#25968;&#25454;&#30340;&#30417;&#30563;&#12290;&#28982;&#21518;&#25105;&#20204;&#37319;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#25552;&#21319;&#27169;&#22411;&#20174;&#22495;&#22806;&#36741;&#21161;&#25968;&#25454;&#20013;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;&#22312;&#23569;&#26679;&#26412;&#20851;&#38190;&#35789;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#31454;&#20105;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot keyword spotting (FS-KWS) models usually require large-scale annotated datasets to generalize to unseen target keywords. However, existing KWS datasets are limited in scale and gathering keyword-like labeled data is costly undertaking. To mitigate this issue, we propose a framework that uses easily collectible, unlabeled reading speech data as an auxiliary source. Self-supervised learning has been widely adopted for learning representations from unlabeled data; however, it is known to be suitable for large models with enough capacity and is not practical for training a small footprint FS-KWS model. Instead, we automatically annotate and filter the data to construct a keyword-like dataset, LibriWord, enabling supervision on auxiliary data. We then adopt multi-task learning that helps the model to enhance the representation power from out-of-domain auxiliary data. Our method notably improves the performance over competitive methods in the FS-KWS benchmark.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#34880;&#28165;&#20998;&#31867;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#22810;&#32500;&#21644;&#26377;&#26434;&#36136;&#30340;&#35757;&#32451;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23545;&#26679;&#26412;&#30340;&#20998;&#31867;&#21644;&#20272;&#35745;&#24739;&#30149;&#29575;&#26469;&#20943;&#23569;&#35823;&#24046;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#30452;&#25509;&#35775;&#38382;&#26465;&#20214;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65292;&#32780;&#26159;&#23558;&#25968;&#25454;&#23884;&#20837;&#21442;&#25968;&#21270;&#30340;&#26354;&#32447;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#32463;&#39564;&#35823;&#24046;&#26469;&#20248;&#21270;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2309.00645</link><description>&lt;p&gt;
&#23545;&#20110;&#22810;&#32500;&#21644;&#26434;&#36136;&#35757;&#32451;&#25968;&#25454;&#30340;&#26368;&#20248;&#34880;&#28165;&#20998;&#31867;&#30340;&#26368;&#23567;&#20551;&#35774;&#65306;&#29702;&#35770;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Minimal Assumptions for Optimal Serology Classification: Theory and Implications for Multidimensional Settings and Impure Training Data. (arXiv:2309.00645v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#34880;&#28165;&#20998;&#31867;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#22810;&#32500;&#21644;&#26377;&#26434;&#36136;&#30340;&#35757;&#32451;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23545;&#26679;&#26412;&#30340;&#20998;&#31867;&#21644;&#20272;&#35745;&#24739;&#30149;&#29575;&#26469;&#20943;&#23569;&#35823;&#24046;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#30452;&#25509;&#35775;&#38382;&#26465;&#20214;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65292;&#32780;&#26159;&#23558;&#25968;&#25454;&#23884;&#20837;&#21442;&#25968;&#21270;&#30340;&#26354;&#32447;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#32463;&#39564;&#35823;&#24046;&#26469;&#20248;&#21270;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#34880;&#28165;&#23398;&#20013;&#65292;&#20943;&#23569;&#20559;&#24046;&#20272;&#35745;&#21644;&#35786;&#26029;&#20998;&#31867;&#22120;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#29702;&#35770;&#19978;&#65292;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#36716;&#21270;&#20026;&#24314;&#27169;&#27979;&#37327;&#32467;&#26524;&#30340;&#31867;&#21035;-&#26465;&#20214;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65288;PDFs&#65289;&#65292;&#23427;&#20204;&#25511;&#21046;&#25152;&#26377;&#21518;&#32493;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#23545;&#20110;&#20165;&#20855;&#26377;&#23569;&#25968;&#32500;&#24230;&#65288;&#20363;&#22914;&#30446;&#26631;&#25239;&#21407;&#65289;&#30340;&#27979;&#37327;&#36755;&#20986;&#65292;&#36825;&#20010;&#20219;&#21153;&#20063;&#24456;&#24555;&#21463;&#21040;&#32500;&#24230;&#35781;&#21650;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;&#21033;&#29992;&#32463;&#39564;&#35757;&#32451;&#25968;&#25454;&#22312;&#20219;&#24847;&#32500;&#24230;&#19978;&#20998;&#31867;&#26679;&#26412;&#21644;&#20272;&#35745;&#24739;&#30149;&#29575;&#65292;&#32780;&#19981;&#38656;&#35201;&#30452;&#25509;&#35775;&#38382;&#26465;&#20214;PDFs&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#24341;&#29702;&#26469;&#35299;&#37322;&#36825;&#20010;&#26041;&#27861;&#65292;&#35813;&#24341;&#29702;&#23558;&#30456;&#23545;&#26465;&#20214;&#27010;&#29575;&#19982;&#26368;&#23567;&#35823;&#24046;&#20998;&#31867;&#36793;&#30028;&#32852;&#31995;&#36215;&#26469;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#21046;&#23450;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65306;&#65288;i&#65289;&#23558;&#25968;&#25454;&#23884;&#20837;&#21442;&#25968;&#21270;&#30340;&#26354;&#32447;&#31354;&#38388;&#65307;&#65288;ii&#65289;&#26681;&#25454;&#26679;&#26412;&#30456;&#23545;&#20110;&#22352;&#26631;&#36724;&#30340;&#20301;&#32622;&#23545;&#26679;&#26412;&#36827;&#34892;&#20998;&#31867;&#65307;&#65288;iii&#65289;&#36890;&#36807;&#26368;&#23567;&#21270;&#32463;&#39564;
&lt;/p&gt;
&lt;p&gt;
Minimizing error in prevalence estimates and diagnostic classifiers remains a challenging task in serology. In theory, these problems can be reduced to modeling class-conditional probability densities (PDFs) of measurement outcomes, which control all downstream analyses. However, this task quickly succumbs to the curse of dimensionality, even for assay outputs with only a few dimensions (e.g. target antigens). To address this problem, we propose a technique that uses empirical training data to classify samples and estimate prevalence in arbitrary dimension without direct access to the conditional PDFs. We motivate this method via a lemma that relates relative conditional probabilities to minimum-error classification boundaries. This leads us to formulate an optimization problem that: (i) embeds the data in a parameterized, curved space; (ii) classifies samples based on their position relative to a coordinate axis; and (iii) subsequently optimizes the space by minimizing the empirical c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25972;&#21512;&#20102;&#21464;&#27169;&#20998;&#35299;&#65288;VMD&#65289;&#21644;&#26102;&#21516;&#27493;&#24179;&#22343;&#65288;TSA&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#32806;&#21512;&#30005;&#26426;-&#26426;&#26800;&#21464;&#36895;&#22120;&#65288;CEMG&#65289;&#31995;&#32479;&#20013;&#30340;&#40831;&#36718;&#40831;&#35010;&#12290;&#36890;&#36807;&#25913;&#36827;&#30340;&#25289;&#26684;&#26391;&#26085;&#20844;&#24335;&#21644;&#26102;&#21464;&#21870;&#21512;&#21018;&#24230;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#35010;&#32441;&#23545;&#31995;&#32479;&#21160;&#24577;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.00641</link><description>&lt;p&gt;
&#20351;&#29992;&#21464;&#27169;&#20998;&#35299;&#65288;VMD&#65289;&#21644;&#26102;&#21516;&#27493;&#24179;&#22343;&#65288;TSA&#65289;&#32452;&#21512;&#26041;&#27861;&#30340;&#40831;&#36718;&#40831;&#35010;&#20998;&#26512;&#30340;&#25913;&#36827;&#25289;&#26684;&#26391;&#26085;&#20844;&#24335;&#65288;Lagrangian Formulation&#65289;&#65288;arXiv:2309.00641v1 [eess.SY]&#65289;
&lt;/p&gt;
&lt;p&gt;
Modified Lagrangian Formulation of Gear Tooth Crack Analysis using Combined Approach of Variable Mode Decomposition (VMD) and Time Synchronous Averaging (TSA). (arXiv:2309.00641v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25972;&#21512;&#20102;&#21464;&#27169;&#20998;&#35299;&#65288;VMD&#65289;&#21644;&#26102;&#21516;&#27493;&#24179;&#22343;&#65288;TSA&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#32806;&#21512;&#30005;&#26426;-&#26426;&#26800;&#21464;&#36895;&#22120;&#65288;CEMG&#65289;&#31995;&#32479;&#20013;&#30340;&#40831;&#36718;&#40831;&#35010;&#12290;&#36890;&#36807;&#25913;&#36827;&#30340;&#25289;&#26684;&#26391;&#26085;&#20844;&#24335;&#21644;&#26102;&#21464;&#21870;&#21512;&#21018;&#24230;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#35010;&#32441;&#23545;&#31995;&#32479;&#21160;&#24577;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#20010;&#37319;&#29992;&#21464;&#27169;&#20998;&#35299;&#65288;VMD&#65289;&#21644;&#26102;&#21516;&#27493;&#24179;&#22343;&#65288;TSA&#65289;&#30340;&#32452;&#21512;&#26041;&#27861;&#30340;&#25972;&#21512;&#40831;&#36718;&#40831;&#35010;&#20998;&#26512;&#31243;&#24207;&#65292;&#35813;&#31243;&#24207;&#22522;&#20110;&#32806;&#21512;&#30005;&#26426;-&#26426;&#26800;&#21464;&#36895;&#22120;&#65288;CEMG&#65289;&#31995;&#32479;&#12290;&#26412;&#25991;&#36824;&#32467;&#21512;&#20102;&#25913;&#36827;&#30340;&#25289;&#26684;&#26391;&#26085;&#20844;&#24335;&#65292;&#36890;&#36807;&#32771;&#34385;Rayleigh&#30340;&#32791;&#25955;&#21183;&#26469;&#27169;&#25311;CEMG&#31995;&#32479;&#12290;&#21516;&#26102;&#65292;&#23558;&#20855;&#26377;&#19981;&#21516;&#40831;&#36718;&#40831;&#35010;&#28145;&#24230;&#30340;&#20998;&#26512;&#25913;&#36827;&#30340;&#26102;&#21464;&#21870;&#21512;&#21018;&#24230;&#65288;IAM-TVMS&#65289;&#20063;&#32435;&#20837;&#21040;CEMG&#31995;&#32479;&#20013;&#65292;&#20197;&#26816;&#26597;&#35010;&#32441;&#23545;&#31995;&#32479;&#21160;&#24577;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#20998;&#26512;&#20102;&#19981;&#21516;&#40831;&#36718;&#40831;&#35010;&#27700;&#24179;&#30340;CEMG&#31995;&#32479;&#30340;&#21160;&#24577;&#21709;&#24212;&#65292;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;VMD&#21644;TSA&#30340;&#25972;&#21512;&#26041;&#27861;&#39318;&#27425;&#24212;&#29992;&#20110;&#20998;&#26512;CEMG&#31995;&#32479;&#30340;&#21160;&#24577;&#34892;&#20026;&#65292;&#32435;&#20837;&#20102;&#19981;&#21516;&#40831;&#36718;&#40831;&#35010;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper discusses the possible observation of an integrated gear tooth crack analysis procedure that employs the combined approach of variable mode decomposition (VMD) and time synchronous averaging (TSA) based on the coupled electromechanical gearbox (CEMG) system. This paper also incorporates the modified Lagrangian formulation to model the CEMG system by considering Rayleigh's dissipative potential. An analytical improved time-varying mesh stiffness (IAM-TVMS) with different levels of gear tooth crack depts is also incorporated into the CEMG system to inspect the influence of cracks on the system's dynamic behavior. Dynamic responses of the CEMG system with different tooth crack levels have been used for further investigations. For the first time, the integrated approach of variable mode decomposition (VMD) and time-synchronous averaging (TSA) has been presented to analyze the dynamic behaviour of CEMG systems at the different gear tooth cracks have been experienced as non-statio
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#32593;&#32476;&#30340;&#20196;&#29260;&#32423;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#29983;&#25104;&#36924;&#30495;&#30340;&#38480;&#20215;&#35746;&#21333;&#31807;&#28040;&#24687;&#65292;&#24182;&#19988;&#22312;&#36924;&#36817;&#25968;&#25454;&#20998;&#24067;&#21644;&#20013;&#38388;&#20215;&#26684;&#22238;&#25253;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#21069;&#26223;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.00638</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#29992;&#20110;&#31471;&#21040;&#31471;&#30340;&#38480;&#20215;&#35746;&#21333;&#31807;&#24314;&#27169;&#65306;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#32593;&#32476;&#30340;&#20196;&#29260;&#32423;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;&#30340;&#28040;&#24687;&#27969;
&lt;/p&gt;
&lt;p&gt;
Generative AI for End-to-End Limit Order Book Modelling: A Token-Level Autoregressive Generative Model of Message Flow Using a Deep State Space Network. (arXiv:2309.00638v1 [q-fin.TR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#32593;&#32476;&#30340;&#20196;&#29260;&#32423;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#29983;&#25104;&#36924;&#30495;&#30340;&#38480;&#20215;&#35746;&#21333;&#31807;&#28040;&#24687;&#65292;&#24182;&#19988;&#22312;&#36924;&#36817;&#25968;&#25454;&#20998;&#24067;&#21644;&#20013;&#38388;&#20215;&#26684;&#22238;&#25253;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#21069;&#26223;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37329;&#34701;&#24066;&#22330;&#20013;&#24320;&#21457;&#19968;&#20010;&#36924;&#30495;&#30340;&#35746;&#21333;&#27969;&#29983;&#25104;&#27169;&#22411;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24320;&#25918;&#38382;&#39064;&#65292;&#20855;&#26377;&#35768;&#22810;&#24066;&#22330;&#21442;&#19982;&#32773;&#30340;&#24212;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#29983;&#25104;&#20196;&#29260;&#21270;&#30340;&#38480;&#20215;&#35746;&#21333;&#31807;(LOB)&#28040;&#24687;&#12290;&#36825;&#20123;&#28040;&#24687;&#30001;Jax-LOB&#27169;&#25311;&#22120;&#35299;&#37322;&#65292;&#35813;&#27169;&#25311;&#22120;&#26356;&#26032;LOB&#29366;&#24577;&#12290;&#20026;&#20102;&#39640;&#25928;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#31616;&#21270;&#30340;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#23618;&#26469;&#22788;&#29702;&#35746;&#21333;&#31807;&#29366;&#24577;&#21644;&#20196;&#29260;&#21270;&#28040;&#24687;&#30340;&#24207;&#21015;&#12290;&#21033;&#29992;NASDAQ&#32929;&#31080;LOBSTER&#25968;&#25454;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#23450;&#20041;&#30340;&#28040;&#24687;&#25968;&#25454;&#20998;&#35789;&#22120;&#65292;&#23558;&#36830;&#32493;&#25968;&#23383;&#20998;&#32452;&#36716;&#25442;&#20026;&#20196;&#29260;&#65292;&#31867;&#20284;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20998;&#35789;&#12290;&#22806;&#26679;&#26412;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36924;&#36817;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#65292;&#27169;&#22411;&#30340;&#22256;&#24785;&#24230;&#36739;&#20302;&#65292;&#24615;&#33021;&#34920;&#29616;&#26377;&#21069;&#26223;&#12290;&#27492;&#22806;&#65292;&#20174;&#29983;&#25104;&#30340;&#35746;&#21333;&#27969;&#35745;&#31639;&#20986;&#30340;&#20013;&#38388;&#20215;&#26684;&#22238;&#25253;&#19982;&#25968;&#25454;&#21576;&#26174;&#33879;&#30456;&#20851;&#24615;&#65292;&#34920;&#26126;&#20102;&#27169;&#22411;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing a generative model of realistic order flow in financial markets is a challenging open problem, with numerous applications for market participants. Addressing this, we propose the first end-to-end autoregressive generative model that generates tokenized limit order book (LOB) messages. These messages are interpreted by a Jax-LOB simulator, which updates the LOB state. To handle long sequences efficiently, the model employs simplified structured state-space layers to process sequences of order book states and tokenized messages. Using LOBSTER data of NASDAQ equity LOBs, we develop a custom tokenizer for message data, converting groups of successive digits to tokens, similar to tokenization in large language models. Out-of-sample results show promising performance in approximating the data distribution, as evidenced by low model perplexity. Furthermore, the mid-price returns calculated from the generated order flow exhibit a significant correlation with the data, indicating imp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#26377;&#38480;&#20803;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#30899;&#32420;&#32500;&#22120;&#29255;&#30340;&#30005;&#27744;&#23481;&#22120;&#35774;&#35745;&#19982;&#30896;&#25758;&#24615;&#33021;&#30340;&#20248;&#21270;&#12290;&#27169;&#25311;&#21644;&#39044;&#27979;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#35774;&#35745;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#30005;&#27744;&#23481;&#22120;&#30340;&#30896;&#25758;&#24378;&#24230;&#21644;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.00637</link><description>&lt;p&gt;
&#22522;&#20110;&#26377;&#38480;&#20803;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#30899;&#32420;&#32500;&#22120;&#29255;&#30005;&#27744;&#23481;&#22120;&#35774;&#35745;&#19982;&#30896;&#25758;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Finite Element Analysis and Machine Learning Guided Design of Carbon Fiber Organosheet-based Battery Enclosures for Crashworthiness. (arXiv:2309.00637v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00637
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#26377;&#38480;&#20803;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#30899;&#32420;&#32500;&#22120;&#29255;&#30340;&#30005;&#27744;&#23481;&#22120;&#35774;&#35745;&#19982;&#30896;&#25758;&#24615;&#33021;&#30340;&#20248;&#21270;&#12290;&#27169;&#25311;&#21644;&#39044;&#27979;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#35774;&#35745;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#30005;&#27744;&#23481;&#22120;&#30340;&#30896;&#25758;&#24378;&#24230;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20855;&#26377;&#26356;&#22909;&#30340;&#24378;&#24230;&#37325;&#37327;&#27604;&#21644;&#32784;&#33104;&#34432;&#24615;&#65292;&#30899;&#32420;&#32500;&#22797;&#21512;&#26448;&#26009;&#21487;&#20197;&#26367;&#20195;&#24403;&#21069;&#30005;&#21160;&#27773;&#36710;&#30340;&#37329;&#23646;&#30005;&#27744;&#23481;&#22120;&#12290;&#28982;&#32780;&#65292;&#30899;&#32420;&#32500;&#32467;&#26500;&#30340;&#24378;&#24230;&#21462;&#20915;&#20110;&#20960;&#20010;&#38656;&#35201;&#20180;&#32454;&#36873;&#25321;&#30340;&#21442;&#25968;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#39640;&#36890;&#37327;&#30340;&#26377;&#38480;&#20803;&#20998;&#26512;&#65288;FEA&#65289;&#28909;&#25104;&#24418;&#27169;&#25311;&#65292;&#20197;&#20351;&#29992;&#19981;&#21516;&#30340;&#35774;&#35745;&#21644;&#21152;&#24037;&#21442;&#25968;&#34394;&#25311;&#21046;&#36896;&#30005;&#27744;&#23481;&#22120;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#34394;&#25311;&#30896;&#25758;&#27169;&#25311;&#65292;&#27169;&#25311;&#20102;&#20391;&#26438;&#30896;&#25758;&#65292;&#20197;&#35780;&#20272;&#30005;&#27744;&#23481;&#22120;&#30340;&#30896;&#25758;&#24615;&#33021;&#12290;&#36825;&#20010;&#39640;&#36890;&#37327;&#30340;&#30896;&#25758;&#27169;&#25311;&#25968;&#25454;&#38598;&#34987;&#29992;&#26469;&#26500;&#24314;&#39044;&#27979;&#27169;&#22411;&#65292;&#20197;&#20102;&#35299;&#26410;&#30693;&#38598;&#30340;&#30896;&#25758;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26174;&#31034;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#65288;R2 &gt; 0.97&#65289;&#65292;&#21487;&#20197;&#39044;&#27979;&#30896;&#25758;&#24615;&#33021;&#25351;&#26631;&#65292;&#22914;&#21387;&#30862;&#36127;&#33655;&#25928;&#29575;&#12289;&#21560;&#25910;&#33021;&#37327;&#12289;&#20405;&#20837;&#21644;&#26368;&#22823;&#20943;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Carbon fiber composite can be a potential candidate for replacing metal-based battery enclosures of current electric vehicles (E.V.s) owing to its better strength-to-weight ratio and corrosion resistance. However, the strength of carbon fiber-based structures depends on several parameters that should be carefully chosen. In this work, we implemented high throughput finite element analysis (FEA) based thermoforming simulation to virtually manufacture the battery enclosure using different design and processing parameters. Subsequently, we performed virtual crash simulations to mimic a side pole crash to evaluate the crashworthiness of the battery enclosures. This high throughput crash simulation dataset was utilized to build predictive models to understand the crashworthiness of an unknown set. Our machine learning (ML) models showed excellent performance (R2 &gt; 0.97) in predicting the crashworthiness metrics, i.e., crush load efficiency, absorbed energy, intrusion, and maximum decelerati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#31639;&#27861;&#24615;&#22823;&#23447;&#21830;&#21697;&#20132;&#26131;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#26102;&#38388;&#31163;&#25955;&#21270;&#26041;&#26696;&#21644;&#20004;&#31181;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;DRL&#27169;&#22411;&#23558;&#22799;&#26222;&#27604;&#29575;&#25552;&#39640;&#20102;83%&#12290;</title><link>http://arxiv.org/abs/2309.00630</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#22823;&#23447;&#21830;&#21697;&#20132;&#26131;
&lt;/p&gt;
&lt;p&gt;
Commodities Trading through Deep Policy Gradient Methods. (arXiv:2309.00630v1 [q-fin.TR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#31639;&#27861;&#24615;&#22823;&#23447;&#21830;&#21697;&#20132;&#26131;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#26102;&#38388;&#31163;&#25955;&#21270;&#26041;&#26696;&#21644;&#20004;&#31181;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;DRL&#27169;&#22411;&#23558;&#22799;&#26222;&#27604;&#29575;&#25552;&#39640;&#20102;83%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#28508;&#22312;&#30340;&#36739;&#22909;&#25910;&#30410;&#33021;&#21147;&#65292;&#31639;&#27861;&#20132;&#26131;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26041;&#27861;&#22312;&#31639;&#27861;&#24615;&#22823;&#23447;&#21830;&#21697;&#20132;&#26131;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#23427;&#23558;&#22823;&#23447;&#21830;&#21697;&#20132;&#26131;&#38382;&#39064;&#24314;&#27169;&#20026;&#36830;&#32493;&#30340;&#31163;&#25955;&#26102;&#38388;&#38543;&#26426;&#21160;&#24577;&#31995;&#32479;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#31163;&#25955;&#21270;&#26041;&#26696;&#65292;&#21487;&#20197;&#26681;&#25454;&#24066;&#22330;&#27874;&#21160;&#24615;&#35843;&#25972;&#65292;&#25552;&#39640;&#20102;&#23376;&#37319;&#26679;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#30340;&#32479;&#35745;&#29305;&#24615;&#12290;&#20026;&#20102;&#20248;&#21270;&#23545;&#20132;&#26131;&#25104;&#26412;&#21644;&#39118;&#38505;&#25935;&#24863;&#30340;&#20132;&#26131;&#20195;&#29702;&#65292;&#24341;&#20837;&#20102;&#20004;&#31181;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#21363;&#22522;&#20110;&#28436;&#21592;&#30340;&#26041;&#27861;&#21644;&#22522;&#20110;&#28436;&#21592;-&#35780;&#35770;&#32773;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#20195;&#29702;&#21033;&#29992;CNN&#21644;LSTM&#20316;&#20026;&#21442;&#25968;&#21270;&#20989;&#25968;&#36924;&#36817;&#22120;&#23558;&#21382;&#21490;&#20215;&#26684;&#35266;&#27979;&#26144;&#23556;&#21040;&#24066;&#22330;&#20301;&#32622;&#12290;&#23545;&#26368;&#36817;&#26376;&#20221;&#30340;&#22825;&#28982;&#27668;&#26399;&#36135;&#36827;&#34892;&#22238;&#27979;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20080;&#20837;&#24182;&#25345;&#26377;&#22522;&#32447;&#30456;&#27604;&#65292;DRL&#27169;&#22411;&#23558;&#22799;&#26222;&#27604;&#29575;&#25552;&#39640;&#20102;83%&#12290;&#27492;&#22806;&#65292;&#20195;&#29702;&#30340;&#39118;&#38505;&#29305;&#24449;&#21487;&#20197;&#23450;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic trading has gained attention due to its potential for generating superior returns. This paper investigates the effectiveness of deep reinforcement learning (DRL) methods in algorithmic commodities trading. It formulates the commodities trading problem as a continuous, discrete-time stochastic dynamical system. The proposed system employs a novel time-discretization scheme that adapts to market volatility, enhancing the statistical properties of subsampled financial time series. To optimize transaction-cost- and risk-sensitive trading agents, two policy gradient algorithms, namely actor-based and actor-critic-based approaches, are introduced. These agents utilize CNNs and LSTMs as parametric function approximators to map historical price observations to market positions.Backtesting on front-month natural gas futures demonstrates that DRL models increase the Sharpe ratio by $83\%$ compared to the buy-and-hold baseline. Additionally, the risk profile of the agents can be custo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#26085;&#20869;&#21152;&#23494;&#36135;&#24065;&#25237;&#36164;&#32452;&#21512;&#20132;&#26131;&#30340;&#20132;&#26131;&#31574;&#30053;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#36890;&#36807;&#37319;&#29992;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#21644;&#26032;&#39062;&#30340;&#28151;&#21512;&#20998;&#24067;&#31574;&#30053;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#24066;&#22330;&#26465;&#20214;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#30456;&#27604;&#20110;&#22522;&#20934;&#27169;&#22411;&#26174;&#33879;&#25552;&#39640;&#20102;&#26679;&#26412;&#22806;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.00626</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#38598;&#25104;&#26041;&#27861;&#29992;&#20110;&#33258;&#21160;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;
&lt;/p&gt;
&lt;p&gt;
An Ensemble Method of Deep Reinforcement Learning for Automated Cryptocurrency Trading. (arXiv:2309.00626v1 [q-fin.TR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#26085;&#20869;&#21152;&#23494;&#36135;&#24065;&#25237;&#36164;&#32452;&#21512;&#20132;&#26131;&#30340;&#20132;&#26131;&#31574;&#30053;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#36890;&#36807;&#37319;&#29992;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#21644;&#26032;&#39062;&#30340;&#28151;&#21512;&#20998;&#24067;&#31574;&#30053;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#24066;&#22330;&#26465;&#20214;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#30456;&#27604;&#20110;&#22522;&#20934;&#27169;&#22411;&#26174;&#33879;&#25552;&#39640;&#20102;&#26679;&#26412;&#22806;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#26085;&#20869;&#21152;&#23494;&#36135;&#24065;&#25237;&#36164;&#32452;&#21512;&#20132;&#26131;&#20013;&#30340;&#39640;&#24230;&#38543;&#26426;&#29615;&#22659;&#20013;&#30340;&#20132;&#26131;&#31574;&#30053;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#39564;&#35777;&#26399;&#38388;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#20998;&#24067;&#31574;&#30053;&#26469;&#26377;&#25928;&#38598;&#25104;&#25152;&#36873;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#31934;&#32454;&#27979;&#35797;&#26399;&#38388;&#30340;&#26679;&#26412;&#22806;&#34920;&#29616;&#30340;&#20998;&#24067;&#35270;&#35282;&#65292;&#20197;&#23637;&#31034;&#31574;&#30053;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#24066;&#22330;&#26465;&#20214;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#23450;&#26399;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#20197;&#24212;&#23545;&#37329;&#34701;&#25968;&#25454;&#30340;&#38750;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#38598;&#25104;&#26041;&#27861;&#30456;&#27604;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#21644;&#34987;&#21160;&#25237;&#36164;&#31574;&#30053;&#30340;&#22522;&#20934;&#27169;&#22411;&#25913;&#21892;&#20102;&#26679;&#26412;&#22806;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an ensemble method to improve the generalization performance of trading strategies trained by deep reinforcement learning algorithms in a highly stochastic environment of intraday cryptocurrency portfolio trading. We adopt a model selection method that evaluates on multiple validation periods, and propose a novel mixture distribution policy to effectively ensemble the selected models. We provide a distributional view of the out-of-sample performance on granular test periods to demonstrate the robustness of the strategies in evolving market conditions, and retrain the models periodically to address non-stationarity of financial data. Our proposed ensemble method improves the out-of-sample performance compared with the benchmarks of a deep reinforcement learning strategy and a passive investment strategy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#22235;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#21457;&#29616;&#22312;&#30701;&#26399;&#39044;&#27979;&#32445;&#32422;&#35777;&#21048;&#20132;&#26131;&#25152;&#20013;&#19977;&#20010;&#30693;&#21517;&#32929;&#31080;&#30340;&#32929;&#20215;&#26102;&#65292;XGBoost&#27169;&#22411;&#34920;&#29616;&#20986;&#26368;&#39640;&#20934;&#30830;&#24615;&#12290;&#36827;&#19968;&#27493;&#20248;&#21270;&#21442;&#25968;&#25110;&#24341;&#20837;&#26356;&#22810;&#22806;&#29983;&#21464;&#37327;&#21487;&#33021;&#25913;&#36827;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.00618</link><description>&lt;p&gt;
&#20351;&#29992;&#22806;&#29983;&#21464;&#37327;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#30701;&#26399;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Short-Term Stock Price Forecasting using exogenous variables and Machine Learning Algorithms. (arXiv:2309.00618v1 [q-fin.TR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#22235;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#21457;&#29616;&#22312;&#30701;&#26399;&#39044;&#27979;&#32445;&#32422;&#35777;&#21048;&#20132;&#26131;&#25152;&#20013;&#19977;&#20010;&#30693;&#21517;&#32929;&#31080;&#30340;&#32929;&#20215;&#26102;&#65292;XGBoost&#27169;&#22411;&#34920;&#29616;&#20986;&#26368;&#39640;&#20934;&#30830;&#24615;&#12290;&#36827;&#19968;&#27493;&#20248;&#21270;&#21442;&#25968;&#25110;&#24341;&#20837;&#26356;&#22810;&#22806;&#29983;&#21464;&#37327;&#21487;&#33021;&#25913;&#36827;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37329;&#34701;&#39046;&#22495;&#20013;&#65292;&#20934;&#30830;&#39044;&#27979;&#32929;&#31080;&#24066;&#22330;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#39044;&#27979;&#39046;&#22495;&#30340;&#19979;&#19968;&#32423;&#21035;&#30340;&#23835;&#36215;&#65292;&#26412;&#30740;&#31350;&#35770;&#25991;&#27604;&#36739;&#20102;&#22235;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;2020&#24180;3&#26376;&#21040;2022&#24180;5&#26376;&#26399;&#38388;&#30701;&#26399;&#39044;&#27979;&#32445;&#32422;&#35777;&#21048;&#20132;&#26131;&#25152;&#20013;&#19977;&#20010;&#30693;&#21517;&#32929;&#31080;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;XGBoost&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;&#22810;&#23618;&#24863;&#30693;&#22120;&#21644;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#27169;&#22411;&#36827;&#34892;&#24320;&#21457;&#12289;&#35843;&#20248;&#21644;&#37096;&#32626;&#12290;&#25105;&#20204;&#26681;&#25454;RMSE&#12289;MAPE&#12289;MTT&#21644;MPE&#36825;&#20123;&#35780;&#20272;&#25351;&#26631;&#25253;&#21578;&#20102;&#20135;&#29983;&#26368;&#39640;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#12290;&#22312;&#20351;&#29992;240&#20010;&#20132;&#26131;&#26085;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;XGBoost&#23613;&#31649;&#36816;&#34892;&#26102;&#38388;&#36739;&#38271;&#65288;&#39640;&#36798;10&#31186;&#65289;&#65292;&#20294;&#32473;&#20986;&#20102;&#26368;&#39640;&#20934;&#30830;&#24615;&#12290;&#36827;&#19968;&#27493;&#35843;&#25972;&#21508;&#20010;&#21442;&#25968;&#25110;&#24341;&#20837;&#26356;&#22810;&#22806;&#29983;&#21464;&#37327;&#21487;&#20197;&#25913;&#36827;&#26412;&#30740;&#31350;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creating accurate predictions in the stock market has always been a significant challenge in finance. With the rise of machine learning as the next level in the forecasting area, this research paper compares four machine learning models and their accuracy in forecasting three well-known stocks traded in the NYSE in the short term from March 2020 to May 2022. We deploy, develop, and tune XGBoost, Random Forest, Multi-layer Perceptron, and Support Vector Regression models. We report the models that produce the highest accuracies from our evaluation metrics: RMSE, MAPE, MTT, and MPE. Using a training data set of 240 trading days, we find that XGBoost gives the highest accuracy despite running longer (up to 10 seconds). Results from this study may improve by further tuning the individual parameters or introducing more exogenous variables.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30340;&#23545;&#25239;&#25915;&#20987;&#38382;&#39064;&#65292;&#36890;&#36807;&#35780;&#20272;&#22522;&#32447;&#38450;&#24481;&#31574;&#30053;&#30340;&#25928;&#26524;&#65292;&#25506;&#35752;&#20102;&#21508;&#31181;&#31574;&#30053;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#24182;&#23545;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2309.00614</link><description>&lt;p&gt;
&#38754;&#21521;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#22522;&#32447;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Baseline Defenses for Adversarial Attacks Against Aligned Language Models. (arXiv:2309.00614v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00614
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30340;&#23545;&#25239;&#25915;&#20987;&#38382;&#39064;&#65292;&#36890;&#36807;&#35780;&#20272;&#22522;&#32447;&#38450;&#24481;&#31574;&#30053;&#30340;&#25928;&#26524;&#65292;&#25506;&#35752;&#20102;&#21508;&#31181;&#31574;&#30053;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#24182;&#23545;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26222;&#21450;&#65292;&#20854;&#23433;&#20840;&#28431;&#27934;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25991;&#26412;&#20248;&#21270;&#22120;&#21487;&#20197;&#29983;&#25104;&#32469;&#36807;&#23457;&#26597;&#21644;&#23545;&#40784;&#30340;&#36234;&#29425;&#25552;&#31034;&#12290;&#20511;&#37492;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#30340;&#20016;&#23500;&#30740;&#31350;&#25104;&#26524;&#65292;&#25105;&#20204;&#20174;&#19977;&#20010;&#38382;&#39064;&#20837;&#25163;&#65306;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#20160;&#20040;&#26679;&#30340;&#23041;&#32961;&#27169;&#22411;&#26159;&#23454;&#29992;&#30340;&#65311;&#22522;&#32447;&#38450;&#24481;&#25216;&#26415;&#22312;&#36825;&#20010;&#26032;&#39046;&#22495;&#20013;&#34920;&#29616;&#22914;&#20309;&#65311;LLM&#23433;&#20840;&#24615;&#19982;&#35745;&#31639;&#26426;&#35270;&#35273;&#26377;&#20309;&#19981;&#21516;&#65311;&#25105;&#20204;&#23545;&#20027;&#23548;&#23545;&#25239;LLM&#25915;&#20987;&#30340;&#20960;&#31181;&#22522;&#32447;&#38450;&#24481;&#31574;&#30053;&#36827;&#34892;&#35780;&#20272;&#65292;&#35752;&#35770;&#20102;&#27599;&#31181;&#31574;&#30053;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#20851;&#27880;&#19977;&#31181;&#31867;&#22411;&#30340;&#38450;&#24481;&#65306;&#26816;&#27979;&#65288;&#22522;&#20110;&#22256;&#24785;&#24230;&#65289;&#12289;&#36755;&#20837;&#39044;&#22788;&#29702;&#65288;&#25913;&#20889;&#21644;&#37325;&#26032;&#26631;&#35760;&#21270;&#65289;&#21644;&#23545;&#25239;&#35757;&#32451;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#30333;&#30418;&#21644;&#28784;&#30418;&#35774;&#32622;&#65292;&#24182;&#35752;&#35770;&#20102;&#27599;&#31181;&#32771;&#34385;&#30340;&#38450;&#24481;&#31574;&#30053;&#22312;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models quickly become ubiquitous, their security vulnerabilities are critical to understand. Recent work shows that text optimizers can produce jailbreaking prompts that bypass moderation and alignment. Drawing from the rich body of work on adversarial machine learning, we approach these attacks with three questions: What threat models are practically useful in this domain? How do baseline defense techniques perform in this new domain? How does LLM security differ from computer vision?  We evaluate several baseline defense strategies against leading adversarial attacks on LLMs, discussing the various settings in which each is feasible and effective. Particularly, we look at three types of defenses: detection (perplexity based), input preprocessing (paraphrase and retokenization), and adversarial training. We discuss white-box and gray-box settings and discuss the robustness-performance trade-off for each of the defenses considered. Surprisingly, we find much more succ
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#24615;&#30340;&#31070;&#32463;&#27714;&#35299;&#22120;&#65292;&#29992;&#20110;&#28165;&#29702;&#21644;&#27714;&#35299;&#20809;&#23398;&#36816;&#21160;&#25429;&#25417;&#25968;&#25454;&#12290;&#35770;&#25991;&#36890;&#36807;&#26500;&#24314;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#20351;&#29992;&#29305;&#23450;&#30340;&#22270;&#21367;&#31215;&#25805;&#20316;&#65292;&#25552;&#21462;&#26631;&#35760;&#21644;&#20851;&#33410;&#30340;&#23616;&#37096;&#29305;&#24449;&#65292;&#24182;&#36716;&#21270;&#20026;&#26356;&#20934;&#30830;&#30340;&#21160;&#20316;&#12290;&#36890;&#36807;&#30740;&#31350;&#26631;&#35760;&#30340;&#21160;&#20316;&#19982;&#20854;&#30456;&#37051;&#26631;&#35760;&#30340;&#30456;&#20851;&#24615;&#65292;&#35770;&#25991;&#33021;&#22815;&#39640;&#25928;&#22320;&#22635;&#34917;&#32570;&#22833;&#30340;&#26631;&#35760;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#21152;&#36895;&#24230;&#36718;&#24275;&#35782;&#21035;&#36319;&#36394;&#38169;&#35823;&#24341;&#36215;&#30340;&#24322;&#24120;&#26631;&#35760;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#22522;&#20110;&#34920;&#31034;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#35757;&#32451;&#26426;&#21046;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.00428</link><description>&lt;p&gt;
&#20809;&#23398;&#36816;&#21160;&#25429;&#25417;&#30340;&#22522;&#20110;&#23616;&#37096;&#24615;&#30340;&#31070;&#32463;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Locality-based Neural Solver for Optical Motion Capture. (arXiv:2309.00428v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00428
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#24615;&#30340;&#31070;&#32463;&#27714;&#35299;&#22120;&#65292;&#29992;&#20110;&#28165;&#29702;&#21644;&#27714;&#35299;&#20809;&#23398;&#36816;&#21160;&#25429;&#25417;&#25968;&#25454;&#12290;&#35770;&#25991;&#36890;&#36807;&#26500;&#24314;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#20351;&#29992;&#29305;&#23450;&#30340;&#22270;&#21367;&#31215;&#25805;&#20316;&#65292;&#25552;&#21462;&#26631;&#35760;&#21644;&#20851;&#33410;&#30340;&#23616;&#37096;&#29305;&#24449;&#65292;&#24182;&#36716;&#21270;&#20026;&#26356;&#20934;&#30830;&#30340;&#21160;&#20316;&#12290;&#36890;&#36807;&#30740;&#31350;&#26631;&#35760;&#30340;&#21160;&#20316;&#19982;&#20854;&#30456;&#37051;&#26631;&#35760;&#30340;&#30456;&#20851;&#24615;&#65292;&#35770;&#25991;&#33021;&#22815;&#39640;&#25928;&#22320;&#22635;&#34917;&#32570;&#22833;&#30340;&#26631;&#35760;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#21152;&#36895;&#24230;&#36718;&#24275;&#35782;&#21035;&#36319;&#36394;&#38169;&#35823;&#24341;&#36215;&#30340;&#24322;&#24120;&#26631;&#35760;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#22522;&#20110;&#34920;&#31034;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#35757;&#32451;&#26426;&#21046;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23616;&#37096;&#24615;&#30340;&#23398;&#20064;&#26041;&#27861;&#26469;&#28165;&#29702;&#21644;&#27714;&#35299;&#20809;&#23398;&#36816;&#21160;&#25429;&#25417;&#25968;&#25454;&#12290;&#32473;&#23450;&#22122;&#22768;&#26631;&#35760;&#25968;&#25454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#26631;&#35760;&#21644;&#20851;&#33410;&#35270;&#20026;&#19981;&#21516;&#31867;&#22411;&#30340;&#33410;&#28857;&#65292;&#24182;&#20351;&#29992;&#22270;&#21367;&#31215;&#25805;&#20316;&#25552;&#21462;&#26631;&#35760;&#21644;&#20851;&#33410;&#30340;&#23616;&#37096;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#28165;&#26224;&#30340;&#21160;&#20316;&#12290;&#20026;&#20102;&#22788;&#29702;&#24322;&#24120;&#26631;&#35760;&#65288;&#20363;&#22914;&#36974;&#25377;&#25110;&#20855;&#26377;&#36739;&#22823;&#30340;&#36319;&#36394;&#35823;&#24046;&#65289;&#65292;&#20851;&#38190;&#27934;&#23519;&#21147;&#22312;&#20110;&#26631;&#35760;&#30340;&#21160;&#20316;&#19982;&#20854;&#30452;&#25509;&#30456;&#37051;&#30340;&#26631;&#35760;&#30340;&#21160;&#20316;&#20043;&#38388;&#23384;&#22312;&#24456;&#24378;&#30340;&#30456;&#20851;&#24615;&#65292;&#20294;&#19982;&#20854;&#20182;&#26631;&#35760;&#30340;&#30456;&#20851;&#24615;&#36739;&#23567;&#65292;&#21363;&#23616;&#37096;&#24615;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#22635;&#34917;&#32570;&#22833;&#30340;&#26631;&#35760;&#65288;&#20363;&#22914;&#30001;&#20110;&#36974;&#25377;&#24341;&#36215;&#30340;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#30740;&#31350;&#21152;&#36895;&#24230;&#36718;&#24275;&#26469;&#35782;&#21035;&#30001;&#20110;&#36319;&#36394;&#38169;&#35823;&#24341;&#36215;&#30340;&#24322;&#24120;&#26631;&#35760;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#34920;&#31034;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#35757;&#32451;&#26426;&#21046;&#65292;&#36890;&#36807;&#22312;&#20855;&#26377;&#23631;&#34109;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#12290;&#23631;&#34109;&#26041;&#26696;&#26088;&#22312;&#27169;&#25311;&#36974;&#25377;&#21644;&#22122;&#22768;&#31561;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel locality-based learning method for cleaning and solving optical motion capture data. Given noisy marker data, we propose a new heterogeneous graph neural network which treats markers and joints as different types of nodes, and uses graph convolution operations to extract the local features of markers and joints and transform them to clean motions. To deal with anomaly markers (e.g. occluded or with big tracking errors), the key insight is that a marker's motion shows strong correlations with the motions of its immediate neighboring markers but less so with other markers, a.k.a. locality, which enables us to efficiently fill missing markers (e.g. due to occlusion). Additionally, we also identify marker outliers due to tracking errors by investigating their acceleration profiles. Finally, we propose a training regime based on representation learning and data augmentation, by training the model on data with masking. The masking schemes aim to mimic the occluded and nois
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#38271;&#31243;&#22270;&#34920;&#22522;&#20934;&#65288;LRGB&#65289;&#36827;&#34892;&#20102;&#37325;&#26032;&#35780;&#20272;&#65292;&#36890;&#36807;&#20005;&#26684;&#30340;&#23454;&#35777;&#20998;&#26512;&#21457;&#29616;&#65292;&#20808;&#21069;&#30340;&#25253;&#21578;&#24615;&#33021;&#24046;&#36317;&#34987;&#39640;&#20272;&#20102;&#65292;&#32780;&#32463;&#36807;&#22522;&#26412;&#36229;&#21442;&#25968;&#20248;&#21270;&#21518;&#65292;&#24046;&#36317;&#23436;&#20840;&#28040;&#22833;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#29305;&#24449;&#24402;&#19968;&#21270;&#30340;&#32570;&#22833;&#21644;&#38142;&#25509;&#39044;&#27979;&#24230;&#37327;&#30340;&#34394;&#20551;&#23454;&#29616;&#23545;LRGB&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.00367</link><description>&lt;p&gt;
&#38271;&#31243;&#22270;&#34920;&#22522;&#20934;&#30340;&#37325;&#26032;&#35780;&#20272;&#65306;&#24046;&#36317;&#21435;&#21738;&#20799;&#20102;&#65311;
&lt;/p&gt;
&lt;p&gt;
Where Did the Gap Go? Reassessing the Long-Range Graph Benchmark. (arXiv:2309.00367v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#38271;&#31243;&#22270;&#34920;&#22522;&#20934;&#65288;LRGB&#65289;&#36827;&#34892;&#20102;&#37325;&#26032;&#35780;&#20272;&#65292;&#36890;&#36807;&#20005;&#26684;&#30340;&#23454;&#35777;&#20998;&#26512;&#21457;&#29616;&#65292;&#20808;&#21069;&#30340;&#25253;&#21578;&#24615;&#33021;&#24046;&#36317;&#34987;&#39640;&#20272;&#20102;&#65292;&#32780;&#32463;&#36807;&#22522;&#26412;&#36229;&#21442;&#25968;&#20248;&#21270;&#21518;&#65292;&#24046;&#36317;&#23436;&#20840;&#28040;&#22833;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#29305;&#24449;&#24402;&#19968;&#21270;&#30340;&#32570;&#22833;&#21644;&#38142;&#25509;&#39044;&#27979;&#24230;&#37327;&#30340;&#34394;&#20551;&#23454;&#29616;&#23545;LRGB&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#38271;&#31243;&#22270;&#34920;&#22522;&#20934;(LRGB&#65292;Dwivedi&#31561;&#65292;2022)&#24341;&#20837;&#20102;&#19968;&#32452;&#19982;&#39030;&#28857;&#20043;&#38388;&#30340;&#38271;&#31243;&#30456;&#20114;&#20316;&#29992;&#23494;&#20999;&#30456;&#20851;&#30340;&#22270;&#34920;&#23398;&#20064;&#20219;&#21153;&#12290;&#32463;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;&#22270;&#24418;&#21464;&#25442;&#22120;&#26126;&#26174;&#20248;&#20110;&#28040;&#24687;&#20256;&#36882;GNN&#65288;MPGNN&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;LRGB&#19978;&#30340;&#22810;&#20010;MPGNN&#22522;&#32447;&#20197;&#21450;&#22270;&#24418;&#21464;&#25442;&#22120;GPS&#65288;Ramp\'a\v{s}ek&#31561;&#65292;2022&#65289;&#36827;&#34892;&#20102;&#20180;&#32454;&#37325;&#26032;&#35780;&#20272;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30001;&#20110;&#23376;&#20248;&#36229;&#21442;&#25968;&#36873;&#25321;&#19981;&#24403;&#32780;&#39640;&#20272;&#20102;&#25253;&#21578;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#22522;&#26412;&#36229;&#21442;&#25968;&#20248;&#21270;&#21518;&#65292;&#36328;&#22810;&#20010;&#25968;&#25454;&#38598;&#65292;&#24615;&#33021;&#24046;&#36317;&#23436;&#20840;&#28040;&#22833;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;LRGB&#30340;&#35270;&#35273;&#25968;&#25454;&#38598;&#32570;&#20047;&#29305;&#24449;&#24402;&#19968;&#21270;&#30340;&#24433;&#21709;&#65292;&#24182;&#31361;&#20986;&#20102;LRGB&#30340;&#38142;&#25509;&#39044;&#27979;&#24230;&#37327;&#30340;&#34394;&#20551;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#22270;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#24314;&#31435;&#26356;&#39640;&#30340;&#23454;&#35777;&#20005;&#35880;&#24615;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent Long-Range Graph Benchmark (LRGB, Dwivedi et al. 2022) introduced a set of graph learning tasks strongly dependent on long-range interaction between vertices. Empirical evidence suggests that on these tasks Graph Transformers significantly outperform Message Passing GNNs (MPGNNs). In this paper, we carefully reevaluate multiple MPGNN baselines as well as the Graph Transformer GPS (Ramp\'a\v{s}ek et al. 2022) on LRGB. Through a rigorous empirical analysis, we demonstrate that the reported performance gap is overestimated due to suboptimal hyperparameter choices. It is noteworthy that across multiple datasets the performance gap completely vanishes after basic hyperparameter optimization. In addition, we discuss the impact of lacking feature normalization for LRGB's vision datasets and highlight a spurious implementation of LRGB's link prediction metric. The principal aim of our paper is to establish a higher standard of empirical rigor within the graph machine learning commun
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;Top-k&#22810;&#26631;&#31614;&#23398;&#20064;&#30340;&#19981;&#21487;&#23519;&#35273;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#26041;&#27861;&#65292;&#26082;&#33021;&#22815;&#27450;&#39575;&#20154;&#30524;&#30340;&#35270;&#35273;&#24863;&#30693;&#65292;&#21448;&#33021;&#22815;&#36991;&#24320;&#24230;&#37327;&#30417;&#27979;&#12290;</title><link>http://arxiv.org/abs/2309.00007</link><description>&lt;p&gt;
&#24403;&#24230;&#37327;&#19981;&#21487;&#38752;&#26102;&#65306;&#26397;&#30528;Top-k&#22810;&#26631;&#31614;&#23398;&#20064;&#30340;&#19981;&#21487;&#23519;&#35273;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;
&lt;/p&gt;
&lt;p&gt;
When Measures are Unreliable: Imperceptible Adversarial Perturbations toward Top-$k$ Multi-Label Learning. (arXiv:2309.00007v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;Top-k&#22810;&#26631;&#31614;&#23398;&#20064;&#30340;&#19981;&#21487;&#23519;&#35273;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#26041;&#27861;&#65292;&#26082;&#33021;&#22815;&#27450;&#39575;&#20154;&#30524;&#30340;&#35270;&#35273;&#24863;&#30693;&#65292;&#21448;&#33021;&#22815;&#36991;&#24320;&#24230;&#37327;&#30417;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24040;&#22823;&#25104;&#21151;&#65292;&#23545;&#25239;&#24615;&#23398;&#20064;&#22312;&#21508;&#31181;&#30740;&#31350;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#28085;&#30422;&#20102;&#20174;&#22810;&#31867;&#21035;&#23398;&#20064;&#21040;&#22810;&#26631;&#31614;&#23398;&#20064;&#30340;&#33539;&#22260;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#27861;&#21482;&#20851;&#27880;&#20256;&#32479;&#30340;&#35270;&#35273;&#19981;&#21487;&#23519;&#35273;&#24615;&#65292;&#32780;&#24573;&#35270;&#20102;&#26469;&#33258;&#24230;&#37327;&#30340;&#26032;&#30340;&#21487;&#24863;&#30693;&#38382;&#39064;&#65292;&#20363;&#22914;Precision@k&#21644;mAP@k&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24403;&#19968;&#20010;&#35757;&#32451;&#33391;&#22909;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#22120;&#22312;&#26576;&#20123;&#26679;&#26412;&#19978;&#30340;&#34920;&#29616;&#36828;&#20302;&#20110;&#39044;&#26399;&#26102;&#65292;&#21463;&#23475;&#32773;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#24847;&#35782;&#21040;&#36825;&#31181;&#24615;&#33021;&#36864;&#21270;&#28304;&#20110;&#25915;&#20987;&#65292;&#32780;&#19981;&#26159;&#27169;&#22411;&#26412;&#36523;&#12290;&#22240;&#27492;&#65292;&#29702;&#24819;&#30340;&#22810;&#26631;&#31614;&#23545;&#25239;&#24615;&#25915;&#20987;&#24212;&#35813;&#33021;&#22815;&#27450;&#39575;&#35270;&#35273;&#24863;&#30693;&#65292;&#24182;&#19988;&#36991;&#24320;&#24230;&#37327;&#30340;&#30417;&#27979;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#39318;&#20808;&#24341;&#20837;&#20102;&#24230;&#37327;&#19981;&#21487;&#23519;&#35273;&#24615;&#30340;&#27010;&#24565;&#12290;&#28982;&#21518;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#29983;&#25104;&#36825;&#31181;&#26082;&#33021;&#22815;&#23454;&#29616;&#35270;&#35273;&#19981;&#21487;&#23519;&#35273;&#24615;&#21448;&#33021;&#22815;&#36991;&#24320;&#24230;&#37327;&#30417;&#27979;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the great success of deep neural networks, adversarial learning has received widespread attention in various studies, ranging from multi-class learning to multi-label learning. However, existing adversarial attacks toward multi-label learning only pursue the traditional visual imperceptibility but ignore the new perceptible problem coming from measures such as Precision@$k$ and mAP@$k$. Specifically, when a well-trained multi-label classifier performs far below the expectation on some samples, the victim can easily realize that this performance degeneration stems from attack, rather than the model itself. Therefore, an ideal multi-labeling adversarial attack should manage to not only deceive visual perception but also evade monitoring of measures. To this end, this paper first proposes the concept of measure imperceptibility. Then, a novel loss function is devised to generate such adversarial perturbations that could achieve both visual and measure imperceptibility. Furthermore, a
&lt;/p&gt;</description></item><item><title>StratMed&#26159;&#19968;&#31181;&#38754;&#21521;&#20302;&#36164;&#28304;&#33647;&#29289;&#25512;&#33616;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#30456;&#20851;&#24615;&#20998;&#23618;&#26426;&#21046;&#26469;&#35299;&#20915;&#21307;&#30103;&#25968;&#25454;&#38271;&#23614;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#24179;&#34913;&#20102;&#33647;&#29289;&#32452;&#21512;&#30340;&#23433;&#20840;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16781</link><description>&lt;p&gt;
StratMed&#65306;&#38754;&#21521;&#20302;&#36164;&#28304;&#33647;&#29289;&#25512;&#33616;&#30340;&#30456;&#20851;&#24615;&#20998;&#23618;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
StratMed: Relevance Stratification for Low-resource Medication Recommendation. (arXiv:2308.16781v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16781
&lt;/p&gt;
&lt;p&gt;
StratMed&#26159;&#19968;&#31181;&#38754;&#21521;&#20302;&#36164;&#28304;&#33647;&#29289;&#25512;&#33616;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#30456;&#20851;&#24615;&#20998;&#23618;&#26426;&#21046;&#26469;&#35299;&#20915;&#21307;&#30103;&#25968;&#25454;&#38271;&#23614;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#24179;&#34913;&#20102;&#33647;&#29289;&#32452;&#21512;&#30340;&#23433;&#20840;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26377;&#38480;&#21307;&#30103;&#36164;&#28304;&#19982;&#26085;&#30410;&#22686;&#38271;&#30340;&#38656;&#27714;&#20043;&#38388;&#30340;&#22833;&#34913;&#65292;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20020;&#24202;&#20219;&#21153;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20316;&#20026;&#19968;&#20010;&#23376;&#39046;&#22495;&#65292;&#33647;&#29289;&#25512;&#33616;&#26088;&#22312;&#23558;&#24739;&#32773;&#30340;&#32437;&#21521;&#21382;&#21490;&#19982;&#21307;&#23398;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#24110;&#21161;&#21307;&#29983;&#26356;&#23433;&#20840;&#12289;&#26356;&#20934;&#30830;&#22320;&#24320;&#20855;&#33647;&#29289;&#32452;&#21512;&#22788;&#26041;&#12290;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#20102;&#21307;&#30103;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#38271;&#23614;&#20998;&#24067;&#65292;&#32570;&#20047;&#22836;&#23614;&#25968;&#25454;&#20043;&#38388;&#30340;&#24179;&#34913;&#34920;&#31034;&#65292;&#23548;&#33268;&#27169;&#22411;&#24615;&#33021;&#27425;&#20248;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;StratMed&#65292;&#36825;&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#21019;&#26032;&#30340;&#30456;&#20851;&#24615;&#20998;&#23618;&#26426;&#21046;&#30340;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#21327;&#35843;&#25968;&#25454;&#38271;&#23614;&#20998;&#24067;&#20013;&#30340;&#24046;&#24322;&#65292;&#24182;&#22312;&#33647;&#29289;&#32452;&#21512;&#30340;&#23433;&#20840;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#26500;&#24314;&#39044;&#35757;&#32451;&#26041;&#27861;&#26469;&#33719;&#21462;&#23454;&#20307;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31867;&#20284;&#37329;&#23383;&#22612;&#30340;&#25968;&#25454;&#20998;&#23618;&#26041;&#27861;&#65292;&#20197;&#33719;&#24471;&#26356;&#36890;&#29992;&#30340;&#23454;&#20307;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growing imbalance between limited medical resources and escalating demands, AI-based clinical tasks have become paramount. Medication recommendation, as a sub-domain, aims to amalgamate longitudinal patient history with medical knowledge, assisting physicians in prescribing safer and more accurate medication combinations. Existing methods overlook the inherent long-tail distribution in medical data, lacking balanced representation between head and tail data, which leads to sub-optimal model performance. To address this challenge, we introduce StratMed, a model that incorporates an innovative relevance stratification mechanism. It harmonizes discrepancies in data long-tail distribution and strikes a balance between the safety and accuracy of medication combinations. Specifically, we first construct a pre-training method using deep learning networks to obtain entity representation. After that, we design a pyramid-like data stratification method to obtain more generalized entity 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#20102;&#19968;&#31181;&#26356;&#20005;&#37325;&#30340;&#21518;&#38376;&#25915;&#20987;&#23041;&#32961;&#65292;&#21363;&#20219;&#20309;&#20154;&#37117;&#21487;&#20197;&#21033;&#29992;&#26131;&#33719;&#21462;&#30340;&#26377;&#25439;&#21387;&#32553;&#31639;&#27861;&#36827;&#34892;&#33258;&#28982;&#21518;&#38376;&#25915;&#20987;&#65292;&#26080;&#38656;&#35774;&#35745;&#29305;&#23450;&#35302;&#21457;&#22120;&#25110;&#36827;&#34892;&#32321;&#29712;&#35843;&#35797;&#12290;</title><link>http://arxiv.org/abs/2308.16684</link><description>&lt;p&gt;
&#20219;&#20309;&#20154;&#37117;&#21487;&#20197;&#25915;&#20987;&#65306;&#23558;&#26377;&#25439;&#21387;&#32553;&#37325;&#26032;&#29992;&#20316;&#33258;&#28982;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Everyone Can Attack: Repurpose Lossy Compression as a Natural Backdoor Attack. (arXiv:2308.16684v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#20102;&#19968;&#31181;&#26356;&#20005;&#37325;&#30340;&#21518;&#38376;&#25915;&#20987;&#23041;&#32961;&#65292;&#21363;&#20219;&#20309;&#20154;&#37117;&#21487;&#20197;&#21033;&#29992;&#26131;&#33719;&#21462;&#30340;&#26377;&#25439;&#21387;&#32553;&#31639;&#27861;&#36827;&#34892;&#33258;&#28982;&#21518;&#38376;&#25915;&#20987;&#65292;&#26080;&#38656;&#35774;&#35745;&#29305;&#23450;&#35302;&#21457;&#22120;&#25110;&#36827;&#34892;&#32321;&#29712;&#35843;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21518;&#38376;&#25915;&#20987;&#23545;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#26500;&#25104;&#20102;&#23041;&#32961;&#12290;&#20256;&#32479;&#26234;&#24935;&#35748;&#20026;&#65292;&#24182;&#19981;&#26159;&#27599;&#20010;&#20154;&#37117;&#21487;&#20197;&#25104;&#20026;&#25915;&#20987;&#32773;&#65292;&#22240;&#20026;&#35774;&#35745;&#35302;&#21457;&#22120;&#29983;&#25104;&#31639;&#27861;&#30340;&#36807;&#31243;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#21162;&#21147;&#21644;&#24191;&#27867;&#30340;&#23454;&#39564;&#26469;&#30830;&#20445;&#25915;&#20987;&#30340;&#38544;&#31192;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#25351;&#20986;&#23384;&#22312;&#19968;&#31181;&#26356;&#20026;&#20005;&#37325;&#30340;&#21518;&#38376;&#23041;&#32961;&#65306;&#20219;&#20309;&#20154;&#37117;&#21487;&#20197;&#21033;&#29992;&#26131;&#33719;&#21462;&#30340;&#31639;&#27861;&#36827;&#34892;&#38544;&#24708;&#21518;&#38376;&#25915;&#20987;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#21033;&#29992;&#21508;&#31181;&#21387;&#32553;&#24037;&#20855;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#26377;&#25439;&#22270;&#29255;&#21387;&#32553;&#25216;&#26415;&#65292;&#26080;&#38656;&#30041;&#19979;&#20219;&#20309;&#26126;&#26174;&#30340;&#30165;&#36857;&#23601;&#33021;&#36731;&#26494;&#22320;&#23558;&#35302;&#21457;&#22120;&#27169;&#24335;&#27880;&#20837;&#21040;&#22270;&#20687;&#20013;&#65292;&#21363;&#29983;&#25104;&#30340;&#35302;&#21457;&#22120;&#26159;&#33258;&#28982;&#30340;&#22270;&#20687;&#20266;&#24433;&#12290;&#20351;&#29992;&#26377;&#25439;&#22270;&#29255;&#21387;&#32553;&#24037;&#20855;&#26102;&#65292;&#20154;&#20204;&#24182;&#19981;&#38656;&#35201;&#24191;&#27867;&#30693;&#35782;&#65292;&#21482;&#38656;&#28857;&#20987;&#8220;&#36716;&#25442;&#8221;&#25110;&#8220;&#21478;&#23384;&#20026;&#8221;&#25353;&#38062;&#21363;&#21487;&#12290;&#36890;&#36807;&#36825;&#31181;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#26080;&#38656;&#35774;&#35745;&#19968;&#20010;&#19987;&#38376;&#30340;&#35302;&#21457;&#22120;&#25110;&#36827;&#34892;&#32321;&#29712;&#30340;&#35843;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
The vulnerabilities to backdoor attacks have recently threatened the trustworthiness of machine learning models in practical applications. Conventional wisdom suggests that not everyone can be an attacker since the process of designing the trigger generation algorithm often involves significant effort and extensive experimentation to ensure the attack's stealthiness and effectiveness. Alternatively, this paper shows that there exists a more severe backdoor threat: anyone can exploit an easily-accessible algorithm for silent backdoor attacks. Specifically, this attacker can employ the widely-used lossy image compression from a plethora of compression tools to effortlessly inject a trigger pattern into an image without leaving any noticeable trace; i.e., the generated triggers are natural artifacts. One does not require extensive knowledge to click on the "convert" or "save as" button while using tools for lossy image compression. Via this attack, the adversary does not need to design a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#20316;&#19987;&#23478;&#23454;&#29616;&#20102;&#38271;&#23614;&#22270;&#20998;&#31867;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#22270;&#25968;&#25454;&#19978;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2308.16609</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#20316;&#19987;&#23478;&#23454;&#29616;&#38271;&#23614;&#22270;&#20998;&#31867;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Long-Tailed Recognition for Graph Classification via Collaborative Experts. (arXiv:2308.16609v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#20316;&#19987;&#23478;&#23454;&#29616;&#20102;&#38271;&#23614;&#22270;&#20998;&#31867;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#22270;&#25968;&#25454;&#19978;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20998;&#31867;&#26088;&#22312;&#23398;&#20064;&#29992;&#20110;&#26377;&#25928;&#31867;&#21035;&#20998;&#37197;&#30340;&#22270;&#32423;&#34920;&#31034;&#65292;&#22312;&#24179;&#34913;&#30340;&#31867;&#21035;&#20998;&#24067;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#30340;&#25903;&#25345;&#19979;&#21462;&#24471;&#20102;&#26480;&#20986;&#25104;&#26524;&#12290;&#20107;&#23454;&#19978;&#65292;&#22823;&#22810;&#25968;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#25968;&#25454;&#33258;&#28982;&#21576;&#29616;&#38271;&#23614;&#24418;&#24335;&#65292;&#20854;&#20013;&#22836;&#37096;&#31867;&#21035;&#30340;&#26679;&#26412;&#25968;&#37327;&#36828;&#36229;&#36807;&#23614;&#37096;&#31867;&#21035;&#65292;&#22240;&#27492;&#22312;&#38271;&#23614;&#25968;&#25454;&#19978;&#30740;&#31350;&#22270;&#32423;&#20998;&#31867;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20294;&#20173;&#28982;&#36739;&#23569;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35270;&#35273;&#20013;&#30340;&#38271;&#23614;&#23398;&#20064;&#26041;&#27861;&#22823;&#22810;&#26080;&#27861;&#21516;&#26102;&#20248;&#21270;&#34920;&#31034;&#23398;&#20064;&#21644;&#20998;&#31867;&#22120;&#35757;&#32451;&#65292;&#24182;&#19988;&#24573;&#30053;&#20102;&#38590;&#20197;&#20998;&#31867;&#30340;&#31867;&#21035;&#30340;&#25366;&#25496;&#12290;&#30452;&#25509;&#23558;&#29616;&#26377;&#26041;&#27861;&#24212;&#29992;&#20110;&#22270;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#65292;&#22240;&#20026;&#22312;&#22270;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30001;&#20110;&#22797;&#26434;&#30340;&#25299;&#25169;&#29305;&#24449;&#20250;&#26356;&#21152;&#25935;&#24863;&#20110;&#38271;&#23614;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#38271;&#23614;&#22270;&#32423;&#20998;&#31867;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph classification, aiming at learning the graph-level representations for effective class assignments, has received outstanding achievements, which heavily relies on high-quality datasets that have balanced class distribution. In fact, most real-world graph data naturally presents a long-tailed form, where the head classes occupy much more samples than the tail classes, it thus is essential to study the graph-level classification over long-tailed data while still remaining largely unexplored. However, most existing long-tailed learning methods in visions fail to jointly optimize the representation learning and classifier training, as well as neglect the mining of the hard-to-classify classes. Directly applying existing methods to graphs may lead to sub-optimal performance, since the model trained on graphs would be more sensitive to the long-tailed distribution due to the complex topological characteristics. Hence, in this paper, we propose a novel long-tailed graph-level classifica
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#28508;&#22312;&#30011;&#23478;&#30340;&#25216;&#26415;&#65292;&#23427;&#21033;&#29992;&#28508;&#22312;&#20316;&#20026;&#30011;&#24067;&#21644;&#25193;&#25955;&#22120;&#30340;&#39044;&#27979;&#20316;&#20026;&#35745;&#21010;&#26469;&#29983;&#25104;&#32472;&#30011;&#21160;&#30011;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#26816;&#26597;&#28857;&#38598;&#20013;&#36716;&#25442;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2308.16490</link><description>&lt;p&gt;
&#28508;&#22312;&#30011;&#23478;
&lt;/p&gt;
&lt;p&gt;
Latent Painter. (arXiv:2308.16490v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16490
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#28508;&#22312;&#30011;&#23478;&#30340;&#25216;&#26415;&#65292;&#23427;&#21033;&#29992;&#28508;&#22312;&#20316;&#20026;&#30011;&#24067;&#21644;&#25193;&#25955;&#22120;&#30340;&#39044;&#27979;&#20316;&#20026;&#35745;&#21010;&#26469;&#29983;&#25104;&#32472;&#30011;&#21160;&#30011;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#26816;&#26597;&#28857;&#38598;&#20013;&#36716;&#25442;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#22312;&#25193;&#25955;&#22120;&#22312;&#29983;&#25104;AI&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#65292;&#24182;&#28608;&#21457;&#20102;&#21019;&#36896;&#24615;&#33402;&#26415;&#12290;&#22312;&#21435;&#22122;&#28508;&#22312;&#26102;&#65292;&#27599;&#20010;&#27493;&#39588;&#39044;&#27979;&#30340;&#21407;&#22987;&#22270;&#20687;&#20849;&#21516;&#24418;&#25104;&#20102;&#21160;&#30011;&#12290;&#28982;&#32780;&#65292;&#21160;&#30011;&#21463;&#21040;&#25193;&#25955;&#22120;&#21435;&#22122;&#29305;&#24615;&#30340;&#38480;&#21046;&#65292;&#21482;&#21576;&#29616;&#20102;&#19968;&#20010;&#38160;&#21270;&#36807;&#31243;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#28508;&#22312;&#30011;&#23478;&#65292;&#23427;&#20197;&#28508;&#22312;&#20316;&#20026;&#30011;&#24067;&#65292;&#20197;&#25193;&#25955;&#22120;&#30340;&#39044;&#27979;&#20316;&#20026;&#35745;&#21010;&#65292;&#29983;&#25104;&#32472;&#30011;&#21160;&#30011;&#12290;&#28508;&#22312;&#30011;&#23478;&#36824;&#21487;&#20197;&#23558;&#19968;&#20010;&#29983;&#25104;&#30340;&#22270;&#20687;&#36716;&#25442;&#20026;&#21478;&#19968;&#20010;&#22270;&#20687;&#65292;&#36825;&#21487;&#20197;&#21457;&#29983;&#22312;&#20004;&#20010;&#19981;&#21516;&#26816;&#26597;&#28857;&#38598;&#20013;&#30340;&#22270;&#20687;&#20043;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latent diffusers revolutionized the generative AI and inspired creative art. When denoising the latent, the predicted original image at each step collectively animates the formation. However, the animation is limited by the denoising nature of the diffuser, and only renders a sharpening process. This work presents Latent Painter, which uses the latent as the canvas, and the diffuser predictions as the plan, to generate painting animation. Latent Painter also transits one generated image to another, which can happen between images from two different sets of checkpoints.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20803;&#23398;&#20064;&#36827;&#34892;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#28857;&#20113;&#19978;&#37319;&#26679;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#65292;&#35299;&#20915;&#20102;&#27979;&#35797;&#25968;&#25454;&#20998;&#24067;&#19982;&#35757;&#32451;&#25968;&#25454;&#19981;&#21516;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.16484</link><description>&lt;p&gt;
&#20351;&#29992;&#20803;&#23398;&#20064;&#36827;&#34892;&#28857;&#20113;&#19978;&#37319;&#26679;&#30340;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Test-Time Adaptation for Point Cloud Upsampling Using Meta-Learning. (arXiv:2308.16484v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20803;&#23398;&#20064;&#36827;&#34892;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#28857;&#20113;&#19978;&#37319;&#26679;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#65292;&#35299;&#20915;&#20102;&#27979;&#35797;&#25968;&#25454;&#20998;&#24067;&#19982;&#35757;&#32451;&#25968;&#25454;&#19981;&#21516;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24265;&#20215;&#30340;3D&#25195;&#25551;&#20202;&#32463;&#24120;&#20135;&#29983;&#31232;&#30095;&#21644;&#38750;&#22343;&#21248;&#30340;&#28857;&#20113;&#65292;&#36825;&#23545;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#19979;&#28216;&#24212;&#29992;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#28857;&#20113;&#19978;&#37319;&#26679;&#26550;&#26500;&#22312;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#24403;&#27979;&#35797;&#25968;&#25454;&#19982;&#35757;&#32451;&#25968;&#25454;&#20855;&#26377;&#19981;&#21516;&#20998;&#24067;&#26102;&#65292;&#23427;&#20204;&#24448;&#24448;&#20250;&#20986;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#26041;&#27861;&#26469;&#22686;&#24378;&#28857;&#20113;&#19978;&#37319;&#26679;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#20803;&#23398;&#20064;&#26469;&#26174;&#24335;&#22320;&#23398;&#20064;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#30340;&#32593;&#32476;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#20851;&#20110;&#27979;&#35797;&#25968;&#25454;&#30340;&#20808;&#39564;&#20449;&#24687;&#12290;&#22312;&#20803;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#21442;&#25968;&#26159;&#20174;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#30095;-&#23494;&#38598;&#28857;&#20113;&#23545;&#30340;&#38598;&#21512;&#20013;&#23398;&#20064;&#30340;&#12290;&#22312;&#20803;&#27979;&#35797;&#36807;&#31243;&#20013;&#65292;&#32463;&#36807;&#23569;&#37327;&#26799;&#24230;&#26356;&#26032;&#30340;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#20135;&#29983;&#19968;&#32452;&#21807;&#19968;&#30340;&#32593;&#32476;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Affordable 3D scanners often produce sparse and non-uniform point clouds that negatively impact downstream applications in robotic systems. While existing point cloud upsampling architectures have demonstrated promising results on standard benchmarks, they tend to experience significant performance drops when the test data have different distributions from the training data. To address this issue, this paper proposes a test-time adaption approach to enhance model generality of point cloud upsampling. The proposed approach leverages meta-learning to explicitly learn network parameters for test-time adaption. Our method does not require any prior information about the test data. During meta-training, the model parameters are learned from a collection of instance-level tasks, each of which consists of a sparse-dense pair of point clouds from the training data. During meta-testing, the trained model is fine-tuned with a few gradient updates to produce a unique set of network parameters for
&lt;/p&gt;</description></item><item><title>Point-TTA&#26159;&#19968;&#31181;&#36890;&#36807;&#22810;&#20219;&#21153;&#20803;&#36741;&#21161;&#23398;&#20064;&#23454;&#29616;&#30340;&#28857;&#20113;&#37197;&#20934;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#33021;&#22815;&#25552;&#39640;&#37197;&#20934;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.16481</link><description>&lt;p&gt;
Point-TTA: &#20351;&#29992;&#22810;&#20219;&#21153;&#20803;&#36741;&#21161;&#23398;&#20064;&#30340;&#28857;&#20113;&#37197;&#20934;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Point-TTA: Test-Time Adaptation for Point Cloud Registration Using Multitask Meta-Auxiliary Learning. (arXiv:2308.16481v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16481
&lt;/p&gt;
&lt;p&gt;
Point-TTA&#26159;&#19968;&#31181;&#36890;&#36807;&#22810;&#20219;&#21153;&#20803;&#36741;&#21161;&#23398;&#20064;&#23454;&#29616;&#30340;&#28857;&#20113;&#37197;&#20934;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#33021;&#22815;&#25552;&#39640;&#37197;&#20934;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Point-TTA&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28857;&#20113;&#37197;&#20934;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#39640;&#37197;&#20934;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#34429;&#28982;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36827;&#23637;&#65292;&#20294;&#38754;&#23545;&#26410;&#30693;&#30340;&#27979;&#35797;&#29615;&#22659;&#30340;&#27867;&#21270;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#21407;&#22240;&#26159;3D&#25195;&#25551;&#30340;&#21464;&#21270;&#36739;&#22823;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#35757;&#32451;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#24182;&#22312;&#27599;&#20010;&#23454;&#20363;&#19978;&#24212;&#29992;&#30456;&#21516;&#30340;&#35757;&#32451;&#27169;&#22411;&#12290;&#36825;&#21487;&#33021;&#26159;&#27425;&#20248;&#30340;&#65292;&#22240;&#20026;&#21516;&#19968;&#27169;&#22411;&#24456;&#38590;&#22788;&#29702;&#27979;&#35797;&#26399;&#38388;&#30340;&#25152;&#26377;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28857;&#20113;&#37197;&#20934;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#36866;&#24212;&#27979;&#35797;&#26102;&#26410;&#30693;&#30340;&#20998;&#24067;&#65292;&#26080;&#38656;&#20219;&#20309;&#20851;&#20110;&#27979;&#35797;&#25968;&#25454;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#20010;&#33258;&#30417;&#30563;&#36741;&#21161;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#19982;&#20027;&#35201;&#30340;&#37197;&#20934;&#20219;&#21153;&#19968;&#36215;&#36827;&#34892;&#20248;&#21270;&#12290;&#32473;&#23450;&#19968;&#20010;&#27979;&#35797;&#23454;&#20363;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#36741;&#21161;&#20219;&#21153;&#26469;&#35843;&#25972;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#26356;&#26032;&#21518;&#30340;&#27169;&#22411;&#36827;&#34892;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Point-TTA, a novel test-time adaptation framework for point cloud registration (PCR) that improves the generalization and the performance of registration models. While learning-based approaches have achieved impressive progress, generalization to unknown testing environments remains a major challenge due to the variations in 3D scans. Existing methods typically train a generic model and the same trained model is applied on each instance during testing. This could be sub-optimal since it is difficult for the same model to handle all the variations during testing. In this paper, we propose a test-time adaptation approach for PCR. Our model can adapt to unseen distributions at test-time without requiring any prior knowledge of the test data. Concretely, we design three self-supervised auxiliary tasks that are optimized jointly with the primary PCR task. Given a test instance, we adapt our model using these auxiliary tasks and the updated model is used to perform the inference. 
&lt;/p&gt;</description></item><item><title>BioCoder&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#65292;&#24182;&#36890;&#36807;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.16458</link><description>&lt;p&gt;
BioCoder: &#19968;&#31181;&#24102;&#26377;&#19978;&#19979;&#25991;&#35821;&#29992;&#30693;&#35782;&#30340;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge. (arXiv:2308.16458v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16458
&lt;/p&gt;
&lt;p&gt;
BioCoder&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#65292;&#24182;&#36890;&#36807;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#26174;&#33879;&#25913;&#36827;&#20102;&#20195;&#30721;&#29983;&#25104;&#12290;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#30340;&#25193;&#22823;&#65292;&#38656;&#35201;&#36755;&#20986;&#26469;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#30340;&#38656;&#27714;&#20063;&#36234;&#26469;&#36234;&#22810;&#12290;&#27492;&#22806;&#65292;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#65292;&#29983;&#25104;&#21151;&#33021;&#31243;&#24207;&#30001;&#20110;&#39046;&#22495;&#30693;&#35782;&#37327;&#22823;&#12289;&#38656;&#35201;&#22797;&#26434;&#30340;&#25968;&#25454;&#25805;&#20316;&#21644;&#22797;&#26434;&#30340;&#21151;&#33021;&#20381;&#36182;&#20851;&#31995;&#32780;&#38754;&#20020;&#39069;&#22806;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BioCoder&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#29616;&#26377;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#12290;&#19982;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#26377;&#20851;&#65292;BioCoder&#28085;&#30422;&#20102;&#21487;&#33021;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#12290;&#23427;&#21253;&#25324;&#26469;&#33258;GitHub&#30340;1026&#20010;Python&#21644;Java&#20989;&#25968;&#21644;1243&#20010;&#26041;&#27861;&#65292;&#20197;&#21450;&#26469;&#33258;Rosalind&#39033;&#30446;&#30340;253&#20010;&#31034;&#20363;&#12290;BioCoder&#36824;&#32467;&#21512;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#65292;&#25105;&#20204;&#24050;&#32463;&#24212;&#29992;&#23427;&#26469;&#35780;&#20272;&#35768;&#22810;&#27169;&#22411;&#65292;&#21253;&#25324;InCoder&#12289;CodeGen&#12289;CodeGen2&#12289;SantaCoder&#12289;StarCoder&#12289;StarCoder+&#12289;InstructCodeT&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models like ChatGPT have significantly improved code generation. As these models scale up, there is an increasing need for the output to handle more intricate tasks. Moreover, in bioinformatics, generating functional programs poses additional notable challenges due to the amount of domain knowledge, the need for complicated data operations, and intricate functional dependencies between the operations. Here, we present BioCoder, a benchmark developed to evaluate existing pre-trained models in generating bioinformatics code. In relation to function-code generation, BioCoder covers potential package dependencies, class declarations, and global variables. It incorporates 1026 functions and 1243 methods in Python and Java from GitHub and 253 examples from the Rosalind Project. BioCoder incorporates a fuzz-testing framework for evaluation, and we have applied it to evaluate many models including InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#19981;&#24179;&#34913;&#31867;&#21035;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#26694;&#26550;PASS&#65292;&#36890;&#36807;&#37325;&#26032;&#37319;&#26679;&#35757;&#32451;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#23545;&#27604;&#24615;&#39044;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#26631;&#31614;&#20559;&#35265;&#21644;&#27969;&#37327;&#22343;&#21248;&#24615;&#31561;&#38382;&#39064;&#65292;&#20026;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#25552;&#20379;&#20102;&#31283;&#20581;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.16453</link><description>&lt;p&gt;
&#21548;&#21462;&#23569;&#25968;&#32676;&#20307;&#30340;&#22768;&#38899;&#65306;&#22522;&#20110;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#19981;&#24179;&#34913;&#31867;&#21035;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Listen to Minority: Encrypted Traffic Classification for Class Imbalance with Contrastive Pre-Training. (arXiv:2308.16453v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#19981;&#24179;&#34913;&#31867;&#21035;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#26694;&#26550;PASS&#65292;&#36890;&#36807;&#37325;&#26032;&#37319;&#26679;&#35757;&#32451;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#23545;&#27604;&#24615;&#39044;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#26631;&#31614;&#20559;&#35265;&#21644;&#27969;&#37327;&#22343;&#21248;&#24615;&#31561;&#38382;&#39064;&#65292;&#20026;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#25552;&#20379;&#20102;&#31283;&#20581;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#20114;&#32852;&#32593;&#22312;&#21508;&#20010;&#26041;&#38754;&#28145;&#21051;&#22320;&#25913;&#21464;&#20102;&#29616;&#20195;&#29983;&#27963;&#26041;&#24335;&#12290;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#22312;&#31649;&#29702;&#31227;&#21160;&#20114;&#32852;&#32593;&#20013;&#36215;&#21040;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#20351;&#29992;&#21152;&#23494;&#36890;&#20449;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#30340;&#24773;&#20917;&#19979;&#12290;&#23613;&#31649;&#19968;&#20123;&#29616;&#26377;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#30495;&#23454;&#32593;&#32476;&#29615;&#22659;&#20013;&#20173;&#28982;&#23384;&#22312;&#19977;&#20010;&#38480;&#21046;&#65306;1&#65289;&#30001;&#27969;&#37327;&#31867;&#21035;&#19981;&#24179;&#34913;&#24341;&#36215;&#30340;&#26631;&#31614;&#20559;&#35265;&#65292;2&#65289;&#30001;&#32452;&#20214;&#20849;&#20139;&#24341;&#36215;&#30340;&#27969;&#37327;&#22343;&#21248;&#24615;&#65292;&#20197;&#21450;3&#65289;&#20381;&#36182;&#20805;&#36275;&#26631;&#35760;&#27969;&#37327;&#36827;&#34892;&#35757;&#32451;&#12290;&#27809;&#26377;&#20219;&#20309;&#29616;&#26377;&#30340;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#25152;&#26377;&#36825;&#20123;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#21322;&#30417;&#30563;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#26694;&#26550;&#65292;&#31216;&#20026;PASS&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#37325;&#26032;&#37319;&#26679;&#21407;&#22987;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#34892;&#23545;&#27604;&#24615;&#39044;&#35757;&#32451;&#65292;&#32780;&#19981;&#30452;&#25509;&#20351;&#29992;&#20010;&#20307;&#24212;&#29992;&#31243;&#24207;&#26631;&#31614;&#65292;&#20197;&#36991;&#20813;&#30001;&#20110;&#31867;&#21035;&#19981;&#24179;&#34913;&#24341;&#36215;&#30340;&#26631;&#31614;&#20559;&#35265;&#38382;&#39064;&#65292;&#21516;&#26102;&#33719;&#24471;&#31283;&#20581;&#30340;&#29305;&#24449;&#34920;&#31034;&#26469;&#21306;&#20998;&#37325;&#21472;&#30340;&#27969;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mobile Internet has profoundly reshaped modern lifestyles in various aspects. Encrypted Traffic Classification (ETC) naturally plays a crucial role in managing mobile Internet, especially with the explosive growth of mobile apps using encrypted communication. Despite some existing learning-based ETC methods showing promising results, three-fold limitations still remain in real-world network environments, 1) label bias caused by traffic class imbalance, 2) traffic homogeneity caused by component sharing, and 3) training with reliance on sufficient labeled traffic. None of the existing ETC methods can address all these limitations. In this paper, we propose a novel Pre-trAining Semi-Supervised ETC framework, dubbed PASS. Our key insight is to resample the original train dataset and perform contrastive pre-training without using individual app labels directly to avoid label bias issues caused by class imbalance, while obtaining a robust feature representation to differentiate overlapping 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22270;&#23884;&#20837;&#20013;&#24179;&#34913;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#35843;&#21442;&#25968;&#36827;&#34892;&#35843;&#33410;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#31454;&#20105;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36136;&#37327;&#25351;&#26631;&#65292;&#32858;&#31867;&#36317;&#31163;&#20445;&#25345;&#65292;&#29992;&#20110;&#35780;&#20272;&#23884;&#20837;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.16403</link><description>&lt;p&gt;
&#22312;&#22270;&#23884;&#20837;&#20013;&#24179;&#34913;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#65288;LGS&#65289;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Balancing between the Local and Global Structures (LGS) in Graph Embedding. (arXiv:2308.16403v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22270;&#23884;&#20837;&#20013;&#24179;&#34913;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#35843;&#21442;&#25968;&#36827;&#34892;&#35843;&#33410;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#31454;&#20105;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36136;&#37327;&#25351;&#26631;&#65292;&#32858;&#31867;&#36317;&#31163;&#20445;&#25345;&#65292;&#29992;&#20110;&#35780;&#20272;&#23884;&#20837;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21487;&#35843;&#21442;&#25968;&#22312;&#22270;&#23884;&#20837;&#20013;&#24179;&#34913;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#65288;LGS&#65289;&#30340;&#26041;&#27861;&#12290;&#19968;&#20123;&#23884;&#20837;&#26041;&#27861;&#26088;&#22312;&#25429;&#25417;&#20840;&#23616;&#32467;&#26500;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#21017;&#35797;&#22270;&#20445;&#30041;&#23616;&#37096;&#37051;&#22495;&#12290;&#24456;&#23569;&#26377;&#26041;&#27861;&#23581;&#35797;&#21516;&#26102;&#20570;&#21040;&#36825;&#20004;&#28857;&#65292;&#24182;&#19988;&#22312;&#20108;&#32500;&#31354;&#38388;&#20013;&#24456;&#38590;&#24456;&#22909;&#22320;&#25429;&#25417;&#21040;&#23616;&#37096;&#21644;&#20840;&#23616;&#20449;&#24687;&#65292;&#32780;&#22823;&#37096;&#20998;&#22270;&#30340;&#32472;&#21046;&#37117;&#26159;&#22312;&#36825;&#20010;&#31354;&#38388;&#20013;&#36827;&#34892;&#30340;&#12290;&#26681;&#25454;&#20219;&#21153;&#21644;&#24213;&#23618;&#25968;&#25454;&#30340;&#32467;&#26500;&#36873;&#25321;&#20351;&#29992;&#23616;&#37096;&#36824;&#26159;&#20840;&#23616;&#23884;&#20837;&#26469;&#36827;&#34892;&#21487;&#35270;&#21270;&#19981;&#20165;&#21462;&#20915;&#20110;&#20219;&#21153;&#26412;&#36523;&#65292;&#36824;&#21462;&#20915;&#20110;&#24213;&#23618;&#25968;&#25454;&#30340;&#32467;&#26500;&#65292;&#32780;&#36825;&#26159;&#20107;&#20808;&#19981;&#30693;&#36947;&#30340;&#12290;&#23545;&#20110;&#32473;&#23450;&#30340;&#22270;&#65292;LGS&#26088;&#22312;&#25214;&#21040;&#19968;&#20010;&#33391;&#22909;&#30340;&#24179;&#34913;&#65292;&#20197;&#20445;&#30041;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#12290;&#25105;&#20204;&#29992;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;LGS&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#22312;&#20351;&#29992;&#20102;&#35832;&#22914;&#21387;&#21147;&#21644;&#37051;&#22495;&#20445;&#25345;&#31561;&#24050;&#24314;&#31435;&#30340;&#36136;&#37327;&#25351;&#26631;&#26102;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36136;&#37327;&#25351;&#26631;&#65292;&#32858;&#31867;&#36317;&#31163;&#20445;&#25345;&#65292;&#29992;&#20110;&#35780;&#20272;&#23884;&#20837;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a method for balancing between the Local and Global Structures (LGS) in graph embedding, via a tunable parameter. Some embedding methods aim to capture global structures, while others attempt to preserve local neighborhoods. Few methods attempt to do both, and it is not always possible to capture well both local and global information in two dimensions, which is where most graph drawing live. The choice of using a local or a global embedding for visualization depends not only on the task but also on the structure of the underlying data, which may not be known in advance. For a given graph, LGS aims to find a good balance between the local and global structure to preserve. We evaluate the performance of LGS with synthetic and real-world datasets and our results indicate that it is competitive with the state-of-the-art methods, using established quality metrics such as stress and neighborhood preservation. We introduce a novel quality metric, cluster distance preservation, to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#30340;&#28145;&#24230;&#35270;&#39057;&#32534;&#30721;&#25511;&#21046;&#26041;&#27861;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#24102;&#23485;&#38480;&#21046;&#21644;&#19979;&#28216;&#35270;&#35273;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#30772;&#22351;&#29616;&#26377;&#26631;&#20934;&#21270;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#20445;&#25252;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2308.16215</link><description>&lt;p&gt;
&#28145;&#24230;&#35270;&#39057;&#32534;&#30721;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Deep Video Codec Control. (arXiv:2308.16215v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#30340;&#28145;&#24230;&#35270;&#39057;&#32534;&#30721;&#25511;&#21046;&#26041;&#27861;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#24102;&#23485;&#38480;&#21046;&#21644;&#19979;&#28216;&#35270;&#35273;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#30772;&#22351;&#29616;&#26377;&#26631;&#20934;&#21270;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#20445;&#25252;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20002;&#22833;&#29575;&#35270;&#39057;&#21387;&#32553;&#36890;&#24120;&#29992;&#20110;&#20256;&#36755;&#21644;&#23384;&#20648;&#35270;&#39057;&#25968;&#25454;&#12290;&#23613;&#31649;&#23384;&#22312;&#36827;&#38454;&#65288;&#31070;&#32463;&#65289;&#21387;&#32553;&#26041;&#27861;&#65292;&#20294;&#32479;&#19968;&#35270;&#39057;&#32534;&#30721;&#22120;&#65288;&#22914;H.264&#25110;H.265&#65289;&#20173;&#28982;&#26159;&#20107;&#23454;&#19978;&#30340;&#26631;&#20934;&#12290;&#22312;&#38754;&#23545;&#21160;&#24577;&#32593;&#32476;&#24102;&#23485;&#26465;&#20214;&#30340;&#35270;&#39057;&#20256;&#36755;&#20013;&#65292;&#35270;&#39057;&#32534;&#30721;&#22120;&#38656;&#35201;&#36866;&#24212;&#38750;&#24120;&#19981;&#21516;&#30340;&#21387;&#32553;&#24378;&#24230;&#12290;&#36895;&#29575;&#25511;&#21046;&#27169;&#22359;&#22686;&#24378;&#32534;&#35299;&#30721;&#22120;&#30340;&#21387;&#32553;&#33021;&#21147;&#65292;&#20197;&#28385;&#36275;&#24102;&#23485;&#38480;&#21046;&#24182;&#23613;&#37327;&#20943;&#23569;&#35270;&#39057;&#22833;&#30495;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#35270;&#39057;&#32534;&#30721;&#22120;&#21450;&#20854;&#36895;&#29575;&#25511;&#21046;&#27169;&#22359;&#26159;&#20026;&#20102;&#26368;&#23567;&#21270;&#20154;&#31867;&#36136;&#37327;&#35780;&#20272;&#32780;&#24320;&#21457;&#30340;&#65292;&#21364;&#27809;&#26377;&#32771;&#34385;&#20445;&#25252;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#30340;&#28145;&#24230;&#35270;&#39057;&#32534;&#30721;&#25511;&#21046;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#24102;&#23485;&#38480;&#21046;&#21644;&#19979;&#28216;&#35270;&#35273;&#24615;&#33021;&#65292;&#24182;&#19981;&#30772;&#22351;&#29616;&#26377;&#30340;&#26631;&#20934;&#21270;&#12290;&#25105;&#20204;&#38024;&#23545;&#20004;&#20010;&#24120;&#35265;&#30340;&#35270;&#35273;&#20219;&#21153;&#65288;&#35821;&#20041;&#20998;&#21106;...
&lt;/p&gt;
&lt;p&gt;
Lossy video compression is commonly used when transmitting and storing video data. Unified video codecs (e.g., H.264 or H.265) remain the \emph{de facto} standard, despite the availability of advanced (neural) compression approaches. Transmitting videos in the face of dynamic network bandwidth conditions requires video codecs to adapt to vastly different compression strengths. Rate control modules augment the codec's compression such that bandwidth constraints are satisfied and video distortion is minimized. While, both standard video codes and their rate control modules are developed to minimize video distortion w.r.t. human quality assessment, preserving the downstream performance of deep vision models is not considered. In this paper, we present the first end-to-end learnable deep video codec control considering both bandwidth constraints and downstream vision performance, while not breaking existing standardization. We demonstrate for two common vision tasks (semantic segmentation 
&lt;/p&gt;</description></item><item><title>MDTD&#26159;&#19968;&#31181;&#22810;&#39046;&#22495;&#26408;&#39532;&#26816;&#27979;&#22120;&#65292;&#29992;&#20110;&#22312;&#27979;&#35797;&#26102;&#26816;&#27979;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#21253;&#21547;&#26408;&#39532;&#35302;&#21457;&#22120;&#30340;&#36755;&#20837;&#12290;MDTD&#19981;&#38656;&#35201;&#30693;&#36947;&#35302;&#21457;&#22120;&#23884;&#20837;&#31574;&#30053;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#36755;&#20837;&#12290;&#23427;&#21033;&#29992;&#20102;&#36755;&#20837;&#26679;&#26412;&#19982;&#20915;&#31574;&#36793;&#30028;&#30340;&#36317;&#31163;&#26469;&#26816;&#27979;&#26408;&#39532;&#35302;&#21457;&#22120;&#12290;</title><link>http://arxiv.org/abs/2308.15673</link><description>&lt;p&gt;
MDTD: &#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#39046;&#22495;&#26408;&#39532;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
MDTD: A Multi Domain Trojan Detector for Deep Neural Networks. (arXiv:2308.15673v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15673
&lt;/p&gt;
&lt;p&gt;
MDTD&#26159;&#19968;&#31181;&#22810;&#39046;&#22495;&#26408;&#39532;&#26816;&#27979;&#22120;&#65292;&#29992;&#20110;&#22312;&#27979;&#35797;&#26102;&#26816;&#27979;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#21253;&#21547;&#26408;&#39532;&#35302;&#21457;&#22120;&#30340;&#36755;&#20837;&#12290;MDTD&#19981;&#38656;&#35201;&#30693;&#36947;&#35302;&#21457;&#22120;&#23884;&#20837;&#31574;&#30053;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#36755;&#20837;&#12290;&#23427;&#21033;&#29992;&#20102;&#36755;&#20837;&#26679;&#26412;&#19982;&#20915;&#31574;&#36793;&#30028;&#30340;&#36317;&#31163;&#26469;&#26816;&#27979;&#26408;&#39532;&#35302;&#21457;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#12290;&#25915;&#20987;&#32773;&#22312;&#19968;&#23567;&#37096;&#20998;&#36755;&#20837;&#26679;&#26412;&#20013;&#23884;&#20837;&#19968;&#20010;&#39044;&#23450;&#20041;&#30340;&#24178;&#25200;&#29289;&#65292;&#31216;&#20026;&#35302;&#21457;&#22120;&#65292;&#24182;&#35757;&#32451;DNN&#65292;&#20351;&#24471;&#36755;&#20837;&#20013;&#23384;&#22312;&#35302;&#21457;&#22120;&#23548;&#33268;&#36755;&#20986;&#20026;&#25915;&#20987;&#32773;&#25152;&#26399;&#26395;&#30340;&#31867;&#21035;&#12290;&#27492;&#31867;&#23545;&#25239;&#24615;&#37325;&#35757;&#32451;&#38656;&#35201;&#30830;&#20445;&#27809;&#26377;&#35302;&#21457;&#22120;&#30340;&#36755;&#20837;&#36755;&#20986;&#19981;&#21463;&#24433;&#21709;&#65292;&#24182;&#23545;&#24178;&#20928;&#26679;&#26412;&#25552;&#20379;&#39640;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MDTD&#65292;&#19968;&#31181;&#38024;&#23545;DNN&#30340;&#22810;&#39046;&#22495;&#26408;&#39532;&#26816;&#27979;&#22120;&#65292;&#29992;&#20110;&#22312;&#27979;&#35797;&#26102;&#26816;&#27979;&#21253;&#21547;&#26408;&#39532;&#35302;&#21457;&#22120;&#30340;&#36755;&#20837;&#12290;MDTD&#19981;&#38656;&#35201;&#30693;&#36947;&#25915;&#20987;&#32773;&#23884;&#20837;&#35302;&#21457;&#22120;&#30340;&#31574;&#30053;&#65292;&#24182;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;DNN&#27169;&#22411;&#65292;&#21253;&#25324;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#22522;&#20110;&#22270;&#30340;&#36755;&#20837;&#12290;MDTD&#21033;&#29992;&#20102;&#19968;&#20010;&#27934;&#23519;&#21147;&#65292;&#21363;&#21253;&#21547;&#26408;&#39532;&#35302;&#21457;&#22120;&#30340;&#36755;&#20837;&#26679;&#26412;&#30456;&#23545;&#20110;&#24178;&#20928;&#26679;&#26412;&#20301;&#20110;&#20915;&#31574;&#36793;&#30028;&#20043;&#22806;&#36739;&#36828;&#30340;&#20301;&#32622;&#12290;MDTD&#20272;&#35745;&#20102;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models that use deep neural networks (DNNs) are vulnerable to backdoor attacks. An adversary carrying out a backdoor attack embeds a predefined perturbation called a trigger into a small subset of input samples and trains the DNN such that the presence of the trigger in the input results in an adversary-desired output class. Such adversarial retraining however needs to ensure that outputs for inputs without the trigger remain unaffected and provide high classification accuracy on clean samples. In this paper, we propose MDTD, a Multi-Domain Trojan Detector for DNNs, which detects inputs containing a Trojan trigger at testing time. MDTD does not require knowledge of trigger-embedding strategy of the attacker and can be applied to a pre-trained DNN model with image, audio, or graph-based inputs. MDTD leverages an insight that input samples containing a Trojan trigger are located relatively farther away from a decision boundary than clean samples. MDTD estimates the dista
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26500;&#24314;&#20102;&#22235;&#20010;&#25991;&#26412;&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;&#27979;&#37327;&#31713;&#25913;&#26816;&#27979;&#25216;&#26415;&#65292;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25805;&#32437;&#27979;&#37327;&#32467;&#26524;&#20197;&#33829;&#36896;&#33391;&#22909;&#32467;&#26524;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#23637;&#31034;&#20102;&#20248;&#20110;&#22522;&#20934;&#30340;&#25216;&#26415;&#65292;&#20294;&#36824;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.15605</link><description>&lt;p&gt;
&#27979;&#37327;&#31713;&#25913;&#26816;&#27979;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Measurement Tampering Detection Benchmark. (arXiv:2308.15605v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26500;&#24314;&#20102;&#22235;&#20010;&#25991;&#26412;&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;&#27979;&#37327;&#31713;&#25913;&#26816;&#27979;&#25216;&#26415;&#65292;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25805;&#32437;&#27979;&#37327;&#32467;&#26524;&#20197;&#33829;&#36896;&#33391;&#22909;&#32467;&#26524;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#23637;&#31034;&#20102;&#20248;&#20110;&#22522;&#20934;&#30340;&#25216;&#26415;&#65292;&#20294;&#36824;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#24378;&#22823;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26469;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#26102;&#65292;&#25552;&#20379;&#23545;&#20248;&#21270;&#20855;&#26377;&#31283;&#20581;&#24615;&#30340;&#35757;&#32451;&#20449;&#21495;&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#19968;&#20010;&#38382;&#39064;&#26159;&#27979;&#37327;&#31713;&#25913;&#65292;&#21363;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25805;&#32437;&#22810;&#20010;&#27979;&#37327;&#32467;&#26524;&#65292;&#20197;&#33829;&#36896;&#33391;&#22909;&#32467;&#26524;&#30340;&#20551;&#35937;&#65292;&#32780;&#19981;&#26159;&#23454;&#29616;&#26399;&#26395;&#30340;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#22235;&#20010;&#26032;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#27979;&#37327;&#31713;&#25913;&#26816;&#27979;&#25216;&#26415;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#32473;&#23450;&#19968;&#32452;&#25991;&#26412;&#36755;&#20837;&#21644;&#27979;&#37327;&#32467;&#26524;&#65292;&#26088;&#22312;&#30830;&#23450;&#26576;&#20010;&#32467;&#26524;&#26159;&#21542;&#21457;&#29983;&#65292;&#20197;&#21450;&#19968;&#20010;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#27979;&#37327;&#32467;&#26524;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#30446;&#26631;&#26159;&#30830;&#23450;&#25152;&#26377;&#27979;&#37327;&#32467;&#26524;&#37117;&#34920;&#26126;&#32467;&#26524;&#21457;&#29983;&#30340;&#31034;&#20363;&#26159;&#21542;&#30830;&#23454;&#21457;&#29983;&#20102;&#32467;&#26524;&#65292;&#25110;&#32773;&#36825;&#26159;&#30001;&#20110;&#27979;&#37327;&#31713;&#25913;&#24341;&#36215;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#31616;&#21333;&#22522;&#20934;&#30340;&#25216;&#26415;&#65292;&#20294;&#27809;&#26377;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;&#25105;&#20204;&#30456;&#20449;&#22312;&#25216;&#26415;&#21644;&#25968;&#25454;&#38598;&#26041;&#38754;&#37117;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#65292;&#25105;&#20204;&#24863;&#21040;&#20852;&#22859;&#12290;
&lt;/p&gt;
&lt;p&gt;
When training powerful AI systems to perform complex tasks, it may be challenging to provide training signals which are robust to optimization. One concern is measurement tampering, where the AI system manipulates multiple measurements to create the illusion of good results instead of achieving the desired outcome. In this work, we build four new text-based datasets to evaluate measurement tampering detection techniques on large language models. Concretely, given sets of text inputs and measurements aimed at determining if some outcome occurred, as well as a base model able to accurately predict measurements, the goal is to determine if examples where all measurements indicate the outcome actually had the outcome occur, or if this was caused by measurement tampering. We demonstrate techniques that outperform simple baselines on most datasets, but don't achieve maximum performance. We believe there is significant room for improvement for both techniques and datasets, and we are excited 
&lt;/p&gt;</description></item><item><title>OEBench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20851;&#31995;&#25968;&#25454;&#27969;&#20013;&#24320;&#25918;&#29615;&#22659;&#25361;&#25112;&#30340;&#24320;&#25918;&#29615;&#22659;&#22522;&#20934;&#65292;&#30740;&#31350;&#21457;&#29616;&#36825;&#31181;&#25361;&#25112;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2308.15059</link><description>&lt;p&gt;
OEBench: &#30740;&#31350;&#29616;&#23454;&#19990;&#30028;&#20013;&#20851;&#31995;&#25968;&#25454;&#27969;&#20013;&#30340;&#24320;&#25918;&#29615;&#22659;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
OEBench: Investigating Open Environment Challenges in Real-World Relational Data Streams. (arXiv:2308.15059v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15059
&lt;/p&gt;
&lt;p&gt;
OEBench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20851;&#31995;&#25968;&#25454;&#27969;&#20013;&#24320;&#25918;&#29615;&#22659;&#25361;&#25112;&#30340;&#24320;&#25918;&#29615;&#22659;&#22522;&#20934;&#65292;&#30740;&#31350;&#21457;&#29616;&#36825;&#31181;&#25361;&#25112;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25968;&#25454;&#38598;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#38750;&#24120;&#26222;&#36941;&#65292;&#24182;&#19988;&#36890;&#24120;&#20197;&#25968;&#25454;&#27969;&#30340;&#26041;&#24335;&#20256;&#36882;&#12290;&#36825;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#27969;&#21487;&#33021;&#23384;&#22312;&#19968;&#20123;&#29305;&#27530;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#20998;&#24067;&#28418;&#31227;&#12289;&#24322;&#24120;&#20540;&#12289;&#26032;&#20852;&#31867;&#21035;&#21644;&#29305;&#24449;&#21464;&#21270;&#65292;&#26368;&#36817;&#23558;&#36825;&#20123;&#25361;&#25112;&#25551;&#36848;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24320;&#25918;&#29615;&#22659;&#25361;&#25112;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#19968;&#20123;&#20851;&#20110;&#25968;&#25454;&#27969;&#30340;&#22686;&#37327;&#23398;&#20064;&#30340;&#30740;&#31350;&#65292;&#20294;&#20854;&#35780;&#20272;&#20027;&#35201;&#26159;&#22522;&#20110;&#25163;&#21160;&#20998;&#21106;&#30340;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#26377;&#20960;&#20010;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#27969;&#25968;&#25454;&#38598;&#21487;&#29992;&#65292;&#20294;&#36825;&#20123;&#24320;&#25918;&#29615;&#22659;&#25361;&#25112;&#26159;&#21542;&#26222;&#36941;&#23384;&#22312;&#20197;&#21450;&#29616;&#26377;&#30340;&#22686;&#37327;&#23398;&#20064;&#31639;&#27861;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#22914;&#20309;&#23578;&#19981;&#30830;&#23450;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;OEBench&#30340;&#24320;&#25918;&#29615;&#22659;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#20851;&#31995;&#25968;&#25454;&#27969;&#20013;&#30340;&#24320;&#25918;&#29615;&#22659;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;55&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#27969;&#25968;&#25454;&#38598;&#65292;&#24182;&#30830;&#31435;&#20102;&#24320;&#25918;&#29615;&#22659;&#22330;&#26223;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#30340;&#26222;&#36941;&#23384;&#22312;&#24615;&#65292;&#36825;&#22312;&#24403;&#21069;&#30340;&#30740;&#31350;&#20013;&#36824;&#27809;&#26377;&#34987;&#30830;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relational datasets are widespread in real-world scenarios and are usually delivered in a streaming fashion. This type of data stream can present unique challenges, such as distribution drifts, outliers, emerging classes, and changing features, which have recently been described as open environment challenges for machine learning. While some work has been done on incremental learning for data streams, their evaluations are mostly conducted with manually partitioned datasets. Moreover, while several real-world streaming datasets are available, it is uncertain whether these open environment challenges are prevalent and how existing incremental learning algorithms perform on real datasets. To fill this gap, we develop an Open Environment Benchmark named OEBench to evaluate open environment challenges in relational data streams. Specifically, we investigate 55 real-world streaming datasets and establish that open environment scenarios are indeed widespread in real-world datasets, which pre
&lt;/p&gt;</description></item><item><title>C3AL&#26159;&#19968;&#31181;&#20914;&#31361;&#24863;&#30693;&#30340;&#20027;&#21160;&#26377;&#38480;&#29366;&#24577;&#26426;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22788;&#29702;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#20914;&#31361;&#65292;&#36890;&#36807;&#23558;&#35266;&#27979;&#26641;&#20316;&#20026;&#23398;&#20064;&#36807;&#31243;&#30340;&#19968;&#31561;&#20844;&#27665;&#24182;&#26368;&#23567;&#21270;&#27979;&#35797;&#27425;&#25968;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.14781</link><description>&lt;p&gt;
&#20914;&#31361;&#24863;&#30693;&#30340;&#20027;&#21160;&#26377;&#38480;&#29366;&#24577;&#26426;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Conflict-Aware Active Automata Learning. (arXiv:2308.14781v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14781
&lt;/p&gt;
&lt;p&gt;
C3AL&#26159;&#19968;&#31181;&#20914;&#31361;&#24863;&#30693;&#30340;&#20027;&#21160;&#26377;&#38480;&#29366;&#24577;&#26426;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22788;&#29702;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#20914;&#31361;&#65292;&#36890;&#36807;&#23558;&#35266;&#27979;&#26641;&#20316;&#20026;&#23398;&#20064;&#36807;&#31243;&#30340;&#19968;&#31561;&#20844;&#27665;&#24182;&#26368;&#23567;&#21270;&#27979;&#35797;&#27425;&#25968;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#26377;&#38480;&#29366;&#24577;&#26426;&#23398;&#20064;&#31639;&#27861;&#22312;&#22788;&#29702;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#20914;&#31361;&#65288;&#21516;&#19968;&#36755;&#20837;&#23545;&#24212;&#19981;&#21516;&#36755;&#20986;&#65289;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#36825;&#31181;&#22266;&#26377;&#30340;&#20914;&#31361;&#24674;&#22797;&#33021;&#21147;&#19981;&#36275;&#65292;&#24433;&#21709;&#20102;&#23427;&#20204;&#22312;&#23384;&#22312;&#22122;&#22768;&#25110;&#23398;&#20064;&#20013;&#30340;&#31995;&#32479;&#21464;&#21270;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20914;&#31361;&#24863;&#30693;&#30340;&#20027;&#21160;&#26377;&#38480;&#29366;&#24577;&#26426;&#23398;&#20064;&#65288;C3AL&#65289;&#26694;&#26550;&#65292;&#20197;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#22788;&#29702;&#20914;&#31361;&#20449;&#24687;&#12290;&#26680;&#24515;&#24605;&#24819;&#26159;&#23558;&#25152;&#35859;&#30340;&#35266;&#27979;&#26641;&#35270;&#20026;&#23398;&#20064;&#36807;&#31243;&#30340;&#19968;&#31561;&#20844;&#27665;&#12290;&#23613;&#31649;&#36825;&#20010;&#24819;&#27861;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#24471;&#21040;&#20102;&#25506;&#32034;&#65292;&#20294;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#19982;&#20219;&#20309;&#29616;&#26377;&#30340;&#23398;&#20064;&#31639;&#27861;&#32467;&#21512;&#65292;&#24182;&#22312;&#38754;&#23545;&#20914;&#31361;&#26102;&#26368;&#23567;&#21270;&#23545;&#27491;&#22312;&#23398;&#20064;&#30340;&#31995;&#32479;&#25191;&#34892;&#30340;&#27979;&#35797;&#27425;&#25968;&#65292;&#20805;&#20998;&#21457;&#25381;&#20102;&#23427;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#22312;&#22823;&#37327;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#20102;C3AL&#65292;&#28085;&#30422;&#20102;30&#22810;&#20010;&#19981;&#21516;&#30340;&#30495;&#23454;&#30446;&#26631;&#21644;18,000&#22810;&#20010;&#19981;&#21516;&#30340;&#22330;&#26223;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;C3AL&#26159;&#19968;&#20010;&#21512;&#36866;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active automata learning algorithms cannot easily handle \emph{conflict} in the observation data (different outputs observed for the same inputs). This inherent inability to recover after a conflict impairs their effective applicability in scenarios where noise is present or the system under learning is mutating.  We propose the Conflict-Aware Active Automata Learning (C3AL) framework to enable handling conflicting information during the learning process. The core idea is to consider the so-called observation tree as a first-class citizen in the learning process. Though this idea is explored in recent work, we take it to its full effect by enabling its use with any existing learner and minimizing the number of tests performed on the system under learning, specially in the face of conflicts. We evaluate C3AL in a large set of benchmarks, covering over 30 different realistic targets, and over 18,000 different scenarios. The results of the evaluation show that C3AL is a suitable alternati
&lt;/p&gt;</description></item><item><title>RESTORE&#26159;&#19968;&#20010;&#29992;&#20110;&#36890;&#36807;&#22270;&#37325;&#24314;&#36827;&#34892;&#20869;&#22312;GEs&#35780;&#20272;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#25581;&#31034;&#22312;&#32473;&#23450;&#21521;&#37327;&#24418;&#24335;&#20013;&#20445;&#30041;&#30340;&#20449;&#24687;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.14659</link><description>&lt;p&gt;
RESTORE: &#36890;&#36807;&#37325;&#24314;&#36827;&#34892;&#22270;&#23884;&#20837;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
RESTORE: Graph Embedding Assessment Through Reconstruction. (arXiv:2308.14659v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14659
&lt;/p&gt;
&lt;p&gt;
RESTORE&#26159;&#19968;&#20010;&#29992;&#20110;&#36890;&#36807;&#22270;&#37325;&#24314;&#36827;&#34892;&#20869;&#22312;GEs&#35780;&#20272;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#25581;&#31034;&#22312;&#32473;&#23450;&#21521;&#37327;&#24418;&#24335;&#20013;&#20445;&#30041;&#30340;&#20449;&#24687;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;Word2Vec&#23884;&#20837;&#21462;&#24471;&#25104;&#21151;&#20043;&#21518;&#65292;&#22270;&#23884;&#20837;&#65288;GEs&#65289;&#24320;&#22987;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;GEs&#24120;&#24120;&#22312;&#19979;&#28216;&#24212;&#29992;&#20013;&#29983;&#25104;&#21644;&#35780;&#20272;&#22806;&#26174;&#24615;&#65292;&#20294;&#20851;&#20110;&#21407;&#22987;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#30340;&#20869;&#22312;&#35780;&#20272;&#19968;&#30452;&#32570;&#20047;&#12290;&#20102;&#35299;&#36825;&#20123;&#23558;&#26377;&#21161;&#20110;&#30830;&#23450;&#21508;&#31181;GE&#26041;&#27861;&#22312;&#21521;&#37327;&#21270;&#22270;&#26102;&#20007;&#22833;&#30456;&#20851;&#30693;&#35782;&#25110;&#23398;&#20064;&#38169;&#35823;&#30693;&#35782;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RESTORE&#65292;&#19968;&#20010;&#29992;&#20110;&#36890;&#36807;&#22270;&#37325;&#24314;&#36827;&#34892;&#20869;&#22312;GEs&#35780;&#20272;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;&#24213;&#23618;GEs&#20013;&#37325;&#24314;&#21407;&#22987;&#22270;&#21487;&#20197;&#25581;&#31034;&#22312;&#32473;&#23450;&#21521;&#37327;&#24418;&#24335;&#20013;&#20445;&#30041;&#30340;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#22270;&#37325;&#24314;&#20219;&#21153;&#12290;&#25105;&#20204;&#26681;&#25454;&#20998;&#35299;&#26041;&#27861;&#12289;&#38543;&#26426;&#34892;&#36208;&#21644;&#28145;&#24230;&#23398;&#20064;&#20174;&#19977;&#20010;GE&#23478;&#26063;&#29983;&#25104;GEs&#65288;&#27599;&#20010;&#23478;&#26063;&#36873;&#25321;&#20102;&#20195;&#34920;&#24615;&#31639;&#27861;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following the success of Word2Vec embeddings, graph embeddings (GEs) have gained substantial traction. GEs are commonly generated and evaluated extrinsically on downstream applications, but intrinsic evaluations of the original graph properties in terms of topological structure and semantic information have been lacking. Understanding these will help identify the deficiency of the various families of GE methods when vectorizing graphs in terms of preserving the relevant knowledge or learning incorrect knowledge. To address this, we propose RESTORE, a framework for intrinsic GEs assessment through graph reconstruction. We show that reconstructing the original graph from the underlying GEs yields insights into the relative amount of information preserved in a given vector form. We first introduce the graph reconstruction task. We generate GEs from three GE families based on factorization methods, random walks, and deep learning (with representative algorithms from each family) on the Com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36328;&#27169;&#22411;&#19968;&#33268;&#24615;&#36827;&#34892;&#26631;&#31614;&#21435;&#22122;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35266;&#23519;&#21457;&#29616;&#65292;&#19981;&#21516;&#27169;&#22411;&#22312;&#24178;&#20928;&#31034;&#20363;&#19978;&#30340;&#39044;&#27979;&#30456;&#23545;&#30456;&#20284;&#65292;&#32780;&#22312;&#26377;&#22122;&#22768;&#31034;&#20363;&#19978;&#30340;&#39044;&#27979;&#22312;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#21464;&#21270;&#26356;&#22823;&#12290;&#22312;&#36825;&#31181;&#35266;&#23519;&#30340;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#36328;&#27169;&#22411;&#19968;&#33268;&#24615;&#36827;&#34892;&#21435;&#22122;&#30340;&#26041;&#27861;&#65288;DeCA&#65289;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#20004;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#21270;&#30340;&#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#25968;&#25454;&#35266;&#27979;&#30340;&#20284;&#28982;&#12290;</title><link>http://arxiv.org/abs/2308.13976</link><description>&lt;p&gt;
&#36890;&#36807;&#36328;&#27169;&#22411;&#19968;&#33268;&#24615;&#36827;&#34892;&#26631;&#31614;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
Label Denoising through Cross-Model Agreement. (arXiv:2308.13976v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36328;&#27169;&#22411;&#19968;&#33268;&#24615;&#36827;&#34892;&#26631;&#31614;&#21435;&#22122;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35266;&#23519;&#21457;&#29616;&#65292;&#19981;&#21516;&#27169;&#22411;&#22312;&#24178;&#20928;&#31034;&#20363;&#19978;&#30340;&#39044;&#27979;&#30456;&#23545;&#30456;&#20284;&#65292;&#32780;&#22312;&#26377;&#22122;&#22768;&#31034;&#20363;&#19978;&#30340;&#39044;&#27979;&#22312;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#21464;&#21270;&#26356;&#22823;&#12290;&#22312;&#36825;&#31181;&#35266;&#23519;&#30340;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#36328;&#27169;&#22411;&#19968;&#33268;&#24615;&#36827;&#34892;&#21435;&#22122;&#30340;&#26041;&#27861;&#65288;DeCA&#65289;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#20004;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#21270;&#30340;&#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#25968;&#25454;&#35266;&#27979;&#30340;&#20284;&#28982;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#20174;&#26377;&#22122;&#22768;&#30340;&#26631;&#31614;&#23398;&#20064;&#26159;&#38750;&#24120;&#24120;&#35265;&#30340;&#12290;&#35760;&#24518;&#36825;&#20123;&#26377;&#22122;&#22768;&#30340;&#26631;&#31614;&#21487;&#33021;&#20250;&#24433;&#21709;&#27169;&#22411;&#30340;&#23398;&#20064;&#65292;&#20174;&#32780;&#23548;&#33268;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#26377;&#22122;&#22768;&#26631;&#31614;&#20013;&#23398;&#20064;&#40065;&#26834;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#27169;&#22411;&#22312;&#24178;&#20928;&#31034;&#20363;&#19978;&#30340;&#39044;&#27979;&#30456;&#23545;&#30456;&#20284;&#65292;&#32780;&#22312;&#26377;&#22122;&#22768;&#31034;&#20363;&#19978;&#30340;&#39044;&#27979;&#22312;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#21464;&#21270;&#26356;&#22823;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#36328;&#27169;&#22411;&#19968;&#33268;&#24615;&#36827;&#34892;&#21435;&#22122;&#65288;DeCA&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#26368;&#23567;&#21270;&#30001;&#20004;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#21270;&#30340;&#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#25968;&#25454;&#35266;&#27979;&#30340;&#20284;&#28982;&#12290;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;DeCA&#26041;&#27861;&#24212;&#29992;&#20110;&#20108;&#36827;&#21046;&#26631;&#31614;&#24773;&#26223;&#21644;&#22810;&#26631;&#31614;&#24773;&#26223;&#12290;&#23545;&#20110;&#20108;&#36827;&#21046;&#26631;&#31614;&#24773;&#26223;&#65292;&#25105;&#20204;&#36873;&#25321;&#38544;&#24335;&#21453;&#39304;&#25512;&#33616;&#20316;&#20026;&#19979;&#28216;&#20219;&#21153;&#65292;&#24182;&#36827;&#34892;&#20102;&#22235;&#31181;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from corrupted labels is very common in real-world machine-learning applications. Memorizing such noisy labels could affect the learning of the model, leading to sub-optimal performances. In this work, we propose a novel framework to learn robust machine-learning models from noisy labels. Through an empirical study, we find that different models make relatively similar predictions on clean examples, while the predictions on noisy examples vary much more across different models. Motivated by this observation, we propose \em denoising with cross-model agreement \em (DeCA) which aims to minimize the KL-divergence between the true label distributions parameterized by two machine learning models while maximizing the likelihood of data observation. We employ the proposed DeCA on both the binary label scenario and the multiple label scenario. For the binary label scenario, we select implicit feedback recommendation as the downstream task and conduct experiments with four state-of-the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#23398;&#20064;&#22120;&#27169;&#22411;&#65292;&#31216;&#20026;&#38543;&#26426;&#37197;&#32622;&#26426;&#65288;SCMs&#65289;&#65292;&#20854;&#22522;&#20110;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#65288;SCNs&#65289;&#65292;&#26088;&#22312;&#24378;&#35843;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#26377;&#25928;&#24314;&#27169;&#21644;&#33410;&#32422;&#25968;&#25454;&#22823;&#23567;&#12290;SCMs&#36890;&#36807;&#21387;&#32553;&#27169;&#22411;&#23384;&#20648;&#65292;&#24182;&#20445;&#25345;&#26377;&#21033;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20855;&#26377;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.13570</link><description>&lt;p&gt;
&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#38543;&#26426;&#37197;&#32622;&#26426;
&lt;/p&gt;
&lt;p&gt;
Stochastic Configuration Machines for Industrial Artificial Intelligence. (arXiv:2308.13570v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#23398;&#20064;&#22120;&#27169;&#22411;&#65292;&#31216;&#20026;&#38543;&#26426;&#37197;&#32622;&#26426;&#65288;SCMs&#65289;&#65292;&#20854;&#22522;&#20110;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#65288;SCNs&#65289;&#65292;&#26088;&#22312;&#24378;&#35843;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#26377;&#25928;&#24314;&#27169;&#21644;&#33410;&#32422;&#25968;&#25454;&#22823;&#23567;&#12290;SCMs&#36890;&#36807;&#21387;&#32553;&#27169;&#22411;&#23384;&#20648;&#65292;&#24182;&#20445;&#25345;&#26377;&#21033;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20855;&#26377;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#65288;IAI&#65289;&#20013;&#65292;&#38656;&#35201;&#23454;&#26102;&#12289;&#20934;&#30830;&#30340;&#39044;&#27979;&#24314;&#27169;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#20854;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#24378;&#22823;&#30340;&#39640;&#24615;&#33021;&#35745;&#31639;&#35774;&#22791;&#26469;&#22788;&#29702;&#22823;&#37327;&#30340;&#28014;&#28857;&#25968;&#25454;&#12290;&#26412;&#25991;&#22522;&#20110;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#65288;SCNs&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#23398;&#20064;&#22120;&#27169;&#22411;&#65292;&#31216;&#20026;&#38543;&#26426;&#37197;&#32622;&#26426;&#65288;SCMs&#65289;&#65292;&#20197;&#24378;&#35843;&#23545;&#20110;&#24037;&#19994;&#24212;&#29992;&#38750;&#24120;&#26377;&#29992;&#21644;&#26377;&#20215;&#20540;&#30340;&#26377;&#25928;&#24314;&#27169;&#21644;&#33410;&#32422;&#25968;&#25454;&#22823;&#23567;&#12290;&#19982;&#20855;&#26377;&#20108;&#20540;&#21270;&#23454;&#29616;&#30340;&#38543;&#26426;&#21521;&#37327;&#21151;&#33021;&#38142;&#25509;&#65288;RVFL&#65289;&#32593;&#32476;&#30456;&#27604;&#65292;SCMs&#30340;&#27169;&#22411;&#23384;&#20648;&#21487;&#20197;&#26174;&#33879;&#21387;&#32553;&#65292;&#21516;&#26102;&#20445;&#25345;&#26377;&#21033;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#38500;&#20102;SCM&#23398;&#20064;&#22120;&#27169;&#22411;&#30340;&#26550;&#26500;&#21644;&#23398;&#20064;&#31639;&#27861;&#65292;&#20316;&#20026;&#26412;&#25991;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#25552;&#20379;&#20102;SCMs&#30340;&#23398;&#20064;&#33021;&#21147;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#23454;&#39564;&#30740;&#31350;&#20063;&#36827;&#34892;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time predictive modelling with desired accuracy is highly expected in industrial artificial intelligence (IAI), where neural networks play a key role. Neural networks in IAI require powerful, high-performance computing devices to operate a large number of floating point data. Based on stochastic configuration networks (SCNs), this paper proposes a new randomized learner model, termed stochastic configuration machines (SCMs), to stress effective modelling and data size saving that are useful and valuable for industrial applications. Compared to SCNs and random vector functional-link (RVFL) nets with binarized implementation, the model storage of SCMs can be significantly compressed while retaining favourable prediction performance. Besides the architecture of the SCM learner model and its learning algorithm, as an important part of this contribution, we also provide a theoretical basis on the learning capacity of SCMs by analysing the model's complexity. Experimental studies are ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#36328;&#31038;&#21306;&#33021;&#37327;&#20132;&#20114;&#20248;&#21270;&#35843;&#24230;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#19981;&#21516;&#31038;&#21306;&#30340;&#36127;&#36733;&#29305;&#24615;&#65292;&#24182;&#22522;&#20110;&#35813;&#30693;&#35782;&#20570;&#20986;&#20915;&#31574;&#65292;&#23454;&#29616;&#32508;&#21512;&#33021;&#37327;&#31995;&#32479;&#30340;&#25972;&#20307;&#20248;&#21270;&#21644;&#35843;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.12554</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#36328;&#31038;&#21306;&#33021;&#37327;&#20132;&#20114;&#20248;&#21270;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning-driven Cross-Community Energy Interaction Optimal Scheduling. (arXiv:2308.12554v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#36328;&#31038;&#21306;&#33021;&#37327;&#20132;&#20114;&#20248;&#21270;&#35843;&#24230;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#19981;&#21516;&#31038;&#21306;&#30340;&#36127;&#36733;&#29305;&#24615;&#65292;&#24182;&#22522;&#20110;&#35813;&#30693;&#35782;&#20570;&#20986;&#20915;&#31574;&#65292;&#23454;&#29616;&#32508;&#21512;&#33021;&#37327;&#31995;&#32479;&#30340;&#25972;&#20307;&#20248;&#21270;&#21644;&#35843;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#19981;&#30830;&#23450;&#30340;&#26465;&#20214;&#19979;&#21327;&#35843;&#21508;&#20010;&#31038;&#21306;&#20043;&#38388;&#30340;&#33021;&#37327;&#20132;&#20114;&#21644;&#22810;&#33021;&#28304;&#23376;&#31995;&#32479;&#20043;&#38388;&#30340;&#33021;&#37327;&#36716;&#25442;&#65292;&#24182;&#23454;&#29616;&#32508;&#21512;&#33021;&#37327;&#31995;&#32479;&#30340;&#25972;&#20307;&#20248;&#21270;&#21644;&#35843;&#24230;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#35843;&#24230;&#27169;&#22411;&#65292;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#23398;&#20064;&#19981;&#21516;&#31038;&#21306;&#30340;&#36127;&#36733;&#29305;&#24615;&#65292;&#24182;&#22522;&#20110;&#35813;&#30693;&#35782;&#20570;&#20986;&#20915;&#31574;&#12290;&#22312;&#35813;&#27169;&#22411;&#20013;&#65292;&#32508;&#21512;&#33021;&#37327;&#31995;&#32479;&#30340;&#35843;&#24230;&#38382;&#39064;&#34987;&#36716;&#21270;&#20026;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35299;&#20915;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#23545;&#22810;&#31038;&#21306;&#21644;&#22810;&#33021;&#28304;&#23376;&#31995;&#32479;&#20043;&#38388;&#22797;&#26434;&#33021;&#37327;&#32806;&#21512;&#20851;&#31995;&#30340;&#24314;&#27169;&#38656;&#27714;&#12290;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#19981;&#21516;&#31038;&#21306;&#30340;&#36127;&#36733;&#29305;&#24615;&#65292;&#24182;&#21033;&#29992;&#20854;&#20114;&#34917;&#29305;&#24615;&#36827;&#34892;&#21327;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to coordinate energy interactions among various communities and energy conversions among multi-energy subsystems within the multi-community integrated energy system under uncertain conditions, and achieve overall optimization and scheduling of the comprehensive energy system, this paper proposes a comprehensive scheduling model that utilizes a multi-agent deep reinforcement learning algorithm to learn load characteristics of different communities and make decisions based on this knowledge. In this model, the scheduling problem of the integrated energy system is transformed into a Markov decision process and solved using a data-driven deep reinforcement learning algorithm, which avoids the need for modeling complex energy coupling relationships between multi-communities and multi-energy subsystems. The simulation results show that the proposed method effectively captures the load characteristics of different communities and utilizes their complementary features to coordinate re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26680;&#23398;&#20064;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#26694;&#26550;(MKL-$L_{0/1}$-SVM)&#65292;&#36890;&#36807;&#24320;&#21457;&#24555;&#36895;&#30340;ADMM&#27714;&#35299;&#22120;&#22788;&#29702;&#38750;&#20984;&#38750;&#20809;&#28369;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#19982;&#39046;&#20808;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12016</link><description>&lt;p&gt;
MKL-$L_{0/1}$-SVM: &#19968;&#31181;&#22810;&#26680;&#23398;&#20064;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MKL-$L_{0/1}$-SVM. (arXiv:2308.12016v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26680;&#23398;&#20064;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#26694;&#26550;(MKL-$L_{0/1}$-SVM)&#65292;&#36890;&#36807;&#24320;&#21457;&#24555;&#36895;&#30340;ADMM&#27714;&#35299;&#22120;&#22788;&#29702;&#38750;&#20984;&#38750;&#20809;&#28369;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#19982;&#39046;&#20808;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;$(0, 1)$&#25439;&#22833;&#20989;&#25968;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#22810;&#26680;&#23398;&#20064;&#65288;MKL&#65289;&#26694;&#26550;&#12290;&#39318;&#20808;&#32473;&#20986;&#20102;&#19968;&#38454;&#26368;&#20248;&#24615;&#26465;&#20214;&#65292;&#28982;&#21518;&#21033;&#29992;&#23427;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#24555;&#36895;&#30340;ADMM&#27714;&#35299;&#22120;&#26469;&#22788;&#29702;&#38750;&#20984;&#38750;&#20809;&#28369;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#35814;&#32454;&#30340;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;MKL-$L_{0/1}$-SVM&#30340;&#24615;&#33021;&#19982;&#19968;&#31181;&#21517;&#20026;SimpleMKL&#30340;&#39046;&#20808;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a Multiple Kernel Learning (abbreviated as MKL) framework for the Support Vector Machine (SVM) with the $(0, 1)$ loss function. Some first-order optimality conditions are given and then exploited to develop a fast ADMM solver to deal with the nonconvex and nonsmooth optimization problem. Extensive numerical experiments on synthetic and real datasets show that the performance of our MKL-$L_{0/1}$-SVM is comparable with the one of the leading approaches called SimpleMKL developed by Rakotomamonjy, Bach, Canu, and Grandvalet [Journal of Machine Learning Research, vol. 9, pp. 2491-2521, 2008].
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#39044;&#31639;&#30340;&#38543;&#26426;&#20108;&#33218;&#36172;&#21338;&#26426;&#20013;&#36827;&#34892;&#26368;&#20339;&#33218;&#36873;&#25321;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#19981;&#23384;&#22312;&#27604;&#31561;&#27010;&#29575;&#37319;&#26679;&#31639;&#27861;&#26356;&#22909;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#33268;&#31283;&#23450;&#31639;&#27861;&#30340;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;&#20219;&#20309;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#19982;&#31561;&#27010;&#29575;&#37319;&#26679;&#31639;&#27861;&#34920;&#29616;&#19968;&#26679;&#22909;&#30340;&#31639;&#27861;&#24517;&#39035;&#23646;&#20110;&#36825;&#20010;&#31867;&#21035;&#12290;&#36825;&#19968;&#32467;&#26524;&#35299;&#20915;&#20102;&#20043;&#21069;&#30340;&#20004;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;</title><link>http://arxiv.org/abs/2308.12000</link><description>&lt;p&gt;
&#26377;&#20851;&#22312;&#26377;&#38480;&#39044;&#31639;&#20108;&#33218;&#36172;&#21338;&#26426;&#20013;&#36827;&#34892;&#26368;&#20339;&#33218;&#36873;&#25321;&#30340;&#32479;&#19968;&#26368;&#20248;&#31639;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Uniformly Optimal Algorithms for Best Arm Identification in Two-Armed Bandits with Fixed Budget. (arXiv:2308.12000v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12000
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#39044;&#31639;&#30340;&#38543;&#26426;&#20108;&#33218;&#36172;&#21338;&#26426;&#20013;&#36827;&#34892;&#26368;&#20339;&#33218;&#36873;&#25321;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#19981;&#23384;&#22312;&#27604;&#31561;&#27010;&#29575;&#37319;&#26679;&#31639;&#27861;&#26356;&#22909;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#33268;&#31283;&#23450;&#31639;&#27861;&#30340;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;&#20219;&#20309;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#19982;&#31561;&#27010;&#29575;&#37319;&#26679;&#31639;&#27861;&#34920;&#29616;&#19968;&#26679;&#22909;&#30340;&#31639;&#27861;&#24517;&#39035;&#23646;&#20110;&#36825;&#20010;&#31867;&#21035;&#12290;&#36825;&#19968;&#32467;&#26524;&#35299;&#20915;&#20102;&#20043;&#21069;&#30340;&#20004;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#20271;&#21162;&#21033;&#22870;&#21169;&#30340;&#38543;&#26426;&#20108;&#33218;&#36172;&#21338;&#26426;&#20013;&#65292;&#20351;&#29992;&#26377;&#38480;&#39044;&#31639;&#36827;&#34892;&#26368;&#20339;&#33218;&#36873;&#25321;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#19981;&#23384;&#22312;&#19968;&#20010;&#31639;&#27861;&#21487;&#20197;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#19982;&#31561;&#27010;&#29575;&#37319;&#26679;&#31639;&#27861;&#34920;&#29616;&#19968;&#26679;&#22909;&#65288;&#35813;&#31639;&#27861;&#34987;&#31216;&#20026;&#8220;&#22343;&#21248;&#37319;&#26679;&#8221;&#31639;&#27861;&#65289;&#65292;&#24182;&#19988;&#22312;&#33267;&#23569;&#19968;&#20010;&#24773;&#20917;&#19979;&#26126;&#26174;&#20248;&#20110;&#35813;&#31639;&#27861;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#19981;&#23384;&#22312;&#27604;&#22343;&#21248;&#37319;&#26679;&#31639;&#27861;&#26356;&#22909;&#30340;&#31639;&#27861;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#19968;&#32467;&#26524;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#19968;&#33268;&#8221;&#21644;&#8220;&#31283;&#23450;&#8221;&#31639;&#27861;&#30340;&#33258;&#28982;&#31867;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#20219;&#20309;&#31639;&#27861;&#35201;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#19982;&#22343;&#21248;&#37319;&#26679;&#31639;&#27861;&#34920;&#29616;&#19968;&#26679;&#22909;&#65292;&#24517;&#39035;&#23646;&#20110;&#36825;&#20010;&#31867;&#21035;&#12290;&#36890;&#36807;&#23548;&#20986;&#28385;&#36275;&#20219;&#20309;&#19968;&#33268;&#19988;&#31283;&#23450;&#31639;&#27861;&#30340;&#38169;&#35823;&#29575;&#30340;&#19979;&#30028;&#65292;&#24182;&#35777;&#26126;&#22343;&#21248;&#37319;&#26679;&#31639;&#27861;&#19982;&#27492;&#19979;&#30028;&#30456;&#21305;&#37197;&#65292;&#25105;&#20204;&#23436;&#25104;&#20102;&#35777;&#26126;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35299;&#20915;&#20102;\cite{qin2022open}&#20013;&#25552;&#20986;&#30340;&#20004;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of best-arm identification with fixed budget in stochastic two-arm bandits with Bernoulli rewards. We prove that surprisingly, there is no algorithm that (i) performs as well as the algorithm sampling each arm equally (this algorithm is referred to as the {\it uniform sampling} algorithm) on all instances, and that (ii) strictly outperforms this algorithm on at least one instance. In short, there is no algorithm better than the uniform sampling algorithm. Towards this result, we introduce the natural class of {\it consistent} and {\it stable} algorithms, and show that any algorithm that performs as well as the uniform sampling algorithm on all instances belongs to this class. The proof is completed by deriving a lower bound on the error rate satisfied by any consistent and stable algorithm, and by showing that the uniform sampling algorithm matches this lower bound. Our results provide a solution to the two open problems presented in \cite{qin2022open}.
&lt;/p&gt;</description></item><item><title>"&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#28151;&#28102;&#21327;&#21464;&#37327;&#23545;&#22522;&#20110;&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#30340;&#26041;&#27861;&#25512;&#26029;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#25552;&#20379;&#32479;&#19968;&#30340;&#22270;&#24418;&#26694;&#26550;&#26469;&#22686;&#24378;&#23545;&#36825;&#20123;&#27169;&#22411;&#22522;&#26412;&#21407;&#29702;&#30340;&#29702;&#35299;&#65292;&#20026;&#23454;&#38469;&#22330;&#26223;&#20013;&#24212;&#29992;&#36825;&#20123;&#27169;&#22411;&#24102;&#26469;&#20102;&#28508;&#22312;&#20215;&#20540;&#12290;"</title><link>http://arxiv.org/abs/2308.11676</link><description>&lt;p&gt;
"&#38750;&#28151;&#28102;&#21327;&#21464;&#37327;&#23545;&#22522;&#20110;&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#30340;&#26041;&#27861;&#25512;&#26029;&#24615;&#33021;&#30340;&#24433;&#21709;&#30740;&#31350;"
&lt;/p&gt;
&lt;p&gt;
A Study on the Impact of Non-confounding Covariates on the Inferential Performance of Methods based on the Potential Outcome Framework. (arXiv:2308.11676v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11676
&lt;/p&gt;
&lt;p&gt;
"&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#28151;&#28102;&#21327;&#21464;&#37327;&#23545;&#22522;&#20110;&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#30340;&#26041;&#27861;&#25512;&#26029;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#25552;&#20379;&#32479;&#19968;&#30340;&#22270;&#24418;&#26694;&#26550;&#26469;&#22686;&#24378;&#23545;&#36825;&#20123;&#27169;&#22411;&#22522;&#26412;&#21407;&#29702;&#30340;&#29702;&#35299;&#65292;&#20026;&#23454;&#38469;&#22330;&#26223;&#20013;&#24212;&#29992;&#36825;&#20123;&#27169;&#22411;&#24102;&#26469;&#20102;&#28508;&#22312;&#20215;&#20540;&#12290;"
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
"&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#65288;POF&#65289;&#22312;&#22240;&#26524;&#25512;&#26029;&#39046;&#22495;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22823;&#22810;&#25968;&#22522;&#20110;POF&#30340;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;&#65288;CIMs-B-POF&#65289;&#26088;&#22312;&#28040;&#38500;&#28151;&#28102;&#20559;&#24046;&#65292;&#24182;&#40664;&#35748;&#23384;&#22312;&#28151;&#28102;&#21327;&#21464;&#37327;&#30340;&#22522;&#26412;&#20551;&#35774;&#12290;&#36825;&#19968;&#20551;&#35774;&#35748;&#20026;&#21327;&#21464;&#37327;&#20165;&#30001;&#28151;&#28102;&#21464;&#37327;&#32452;&#25104;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#20445;&#25345;&#28151;&#28102;&#21327;&#21464;&#37327;&#30340;&#20551;&#35774;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#39640;&#32500;&#21327;&#21464;&#37327;&#26102;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#22312;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#20043;&#21069;&#21306;&#20998;&#21327;&#21464;&#37327;&#30340;&#19981;&#21516;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#23558;&#38750;&#28151;&#28102;&#30340;&#21327;&#21464;&#37327;&#35270;&#20026;&#28151;&#28102;&#21464;&#37327;&#30340;&#21518;&#26524;&#20173;&#19981;&#28165;&#26970;&#12290;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#24212;&#29992;CIMs-B-POF&#26102;&#23384;&#22312;&#28508;&#22312;&#39118;&#38505;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22270;&#24418;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;CIMs-B-POF&#27169;&#22411;&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;&#21033;&#29992;&#36825;&#20010;&#22270;&#24418;&#26694;&#26550;&#65292;&#25105;&#20204;&#23545;CIMs-B-POF&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#37327;&#21270;&#20998;&#26512;&#12290;"
&lt;/p&gt;
&lt;p&gt;
The Potential Outcome Framework (POF) plays a prominent role in the field of causal inference. Most causal inference models based on the POF (CIMs-B-POF) are designed for eliminating confounding bias and default to an underlying assumption of Confounding Covariates. This assumption posits that the covariates consist solely of confounders. However, the assumption of Confounding Covariates is challenging to maintain in practice, particularly when dealing with high-dimensional covariates. While certain methods have been proposed to differentiate the distinct components of covariates prior to conducting causal inference, the consequences of treating non-confounding covariates as confounders remain unclear. This ambiguity poses a potential risk when applying the CIMs-B-POF in practical scenarios. In this paper, we present a unified graphical framework for the CIMs-B-POF, which greatly enhances the comprehension of these models' underlying principles. Using this graphical framework, we quant
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA-Reviewer&#26694;&#26550;&#65292;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#21033;&#29992;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLaMA&#22312;&#20195;&#30721;&#23457;&#26597;&#39046;&#22495;&#33021;&#21147;&#65292;&#23454;&#29616;&#23545;&#20195;&#30721;&#23457;&#26597;&#20219;&#21153;&#30340;&#33258;&#21160;&#21270;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#20165;&#20351;&#29992;&#19981;&#21040;1%&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#35813;&#26694;&#26550;&#20173;&#33021;&#21462;&#24471;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.11148</link><description>&lt;p&gt;
LLaMA-Reviewer: &#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25512;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#23457;&#26597;&#33258;&#21160;&#21270;&#20013;&#30340;&#24212;&#29992;&#65288;&#23454;&#35777;&#30740;&#31350;&#65289;
&lt;/p&gt;
&lt;p&gt;
LLaMA-Reviewer: Advancing Code Review Automation with Large Language Models through Parameter-Efficient Fine-Tuning (Practical Experience Report). (arXiv:2308.11148v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA-Reviewer&#26694;&#26550;&#65292;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#21033;&#29992;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLaMA&#22312;&#20195;&#30721;&#23457;&#26597;&#39046;&#22495;&#33021;&#21147;&#65292;&#23454;&#29616;&#23545;&#20195;&#30721;&#23457;&#26597;&#20219;&#21153;&#30340;&#33258;&#21160;&#21270;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#20165;&#20351;&#29992;&#19981;&#21040;1%&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#35813;&#26694;&#26550;&#20173;&#33021;&#21462;&#24471;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#23457;&#26597;&#27963;&#21160;&#30340;&#33258;&#21160;&#21270;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#30340;&#36861;&#27714;&#65292;&#20027;&#35201;&#36890;&#36807;&#35768;&#22810;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#35299;&#20915;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#36164;&#28304;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#34917;&#20805;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#30528;&#36855;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#33258;&#21160;&#21270;&#20195;&#30721;&#23457;&#26597;&#20219;&#21153;&#26041;&#38754;&#30340;&#28508;&#21147;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLaMA-Reviewer&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#20102;&#27969;&#34892;&#30340;LLM&#8212;&#8212;LLaMA&#22312;&#20195;&#30721;&#23457;&#26597;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;&#32771;&#34385;&#21040;&#36164;&#28304;&#38480;&#21046;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#20197;&#26497;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#25552;&#20379;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;LLaMA-Reviewer&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#20351;&#29992;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#20844;&#24320;&#25968;&#25454;&#38598;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21363;&#20351;&#22312;&#21482;&#20351;&#29992;&#19981;&#21040;1%&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20063;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The automation of code review activities, a long-standing pursuit in software engineering, has been primarily addressed by numerous domain-specific pre-trained models. Despite their success, these models frequently demand extensive resources for pre-training from scratch. In contrast, Large Language Models (LLMs) provide an intriguing alternative, given their remarkable capabilities when supplemented with domain-specific knowledge. However, their potential for automating code review tasks remains largely unexplored.  In response to this research gap, we present LLaMA-Reviewer, an innovative framework that leverages the capabilities of LLaMA, a popular LLM, in the realm of code review. Mindful of resource constraints, this framework employs parameter-efficient fine-tuning (PEFT) methods, delivering high performance while using less than 1% of trainable parameters.  An extensive evaluation of LLaMA-Reviewer is conducted on two diverse, publicly available datasets. Notably, even with the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#25512;&#33616;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#34920;&#36798;&#33021;&#21147;&#24230;&#37327;&#26631;&#20934;&#21487;&#33021;&#26080;&#27861;&#26377;&#25928;&#35780;&#20272;&#27169;&#22411;&#22312;&#25512;&#33616;&#20013;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.11127</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#25512;&#33616;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#26377;&#22810;&#24378;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Expressive are Graph Neural Networks in Recommendation?. (arXiv:2308.11127v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#25512;&#33616;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#34920;&#36798;&#33021;&#21147;&#24230;&#37327;&#26631;&#20934;&#21487;&#33021;&#26080;&#27861;&#26377;&#25928;&#35780;&#20272;&#27169;&#22411;&#22312;&#25512;&#33616;&#20013;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#21508;&#31181;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#21033;&#29992;&#22270;&#20013;&#30340;&#29992;&#25143;-&#29289;&#21697;&#21327;&#20316;&#36807;&#28388;&#20449;&#21495;&#36827;&#34892;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#26368;&#20808;&#36827;&#30340;&#25512;&#33616;&#27169;&#22411;&#20013;&#30340;&#32463;&#39564;&#26377;&#25928;&#24615;&#65292;&#20294;&#23545;&#20110;&#23427;&#20204;&#30340;&#33021;&#21147;&#30340;&#29702;&#35770;&#34920;&#36848;&#38750;&#24120;&#31232;&#23569;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;GNNs&#30340;&#19968;&#33324;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#28040;&#24687;&#20256;&#36882;GNNs&#33267;&#22810;&#19982;Weisfeiler-Lehman&#27979;&#35797;&#19968;&#26679;&#24378;&#22823;&#65292;&#24182;&#19988;&#19982;&#38543;&#26426;&#33410;&#28857;&#21021;&#22987;&#21270;&#30456;&#32467;&#21512;&#30340;GNNs&#26159;&#36890;&#29992;&#30340;&#12290;&#28982;&#32780;&#65292;GNNs&#30340;&#8220;&#34920;&#36798;&#33021;&#21147;&#8221;&#27010;&#24565;&#20173;&#28982;&#23450;&#20041;&#27169;&#31946;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24037;&#20316;&#37319;&#29992;&#22270;&#21516;&#26500;&#27979;&#35797;&#20316;&#20026;&#34920;&#36798;&#33021;&#21147;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#20294;&#36825;&#31181;&#22270;&#32423;&#20219;&#21153;&#21487;&#33021;&#19981;&#33021;&#26377;&#25928;&#35780;&#20272;&#27169;&#22411;&#22312;&#25512;&#33616;&#20013;&#21306;&#20998;&#19981;&#21516;&#25509;&#36817;&#31243;&#24230;&#33410;&#28857;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;GNNs&#22312;&#25512;&#33616;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have demonstrated superior performance on various graph learning tasks, including recommendation, where they leverage user-item collaborative filtering signals in graphs. However, theoretical formulations of their capability are scarce, despite their empirical effectiveness in state-of-the-art recommender models. Recently, research has explored the expressiveness of GNNs in general, demonstrating that message passing GNNs are at most as powerful as the Weisfeiler-Lehman test, and that GNNs combined with random node initialization are universal. Nevertheless, the concept of "expressiveness" for GNNs remains vaguely defined. Most existing works adopt the graph isomorphism test as the metric of expressiveness, but this graph-level task may not effectively assess a model's ability in recommendation, where the objective is to distinguish nodes of different closeness. In this paper, we provide a comprehensive theoretical analysis of the expressiveness of GNNs in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20851;&#27880;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#20027;&#21160;&#23545;&#31216;&#24615;&#65292;&#36890;&#36807;&#32771;&#34385;&#20449;&#21495;&#22312;&#22266;&#23450;&#22270;&#19978;&#30340;&#23398;&#20064;&#35774;&#32622;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#30340;&#23545;&#31216;&#24615;&#27010;&#24565;&#65292;&#36890;&#36807;&#22270;&#31895;&#21270;&#23454;&#29616;&#12290;&#36825;&#31687;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#20559;&#24046;-&#26041;&#24046;&#20844;&#24335;&#26469;&#34913;&#37327;&#36817;&#20284;&#23545;&#31216;&#24615;...</title><link>http://arxiv.org/abs/2308.10436</link><description>&lt;p&gt;
&#36817;&#20284;&#31561;&#21464;&#22270;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Approximately Equivariant Graph Networks. (arXiv:2308.10436v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#20027;&#21160;&#23545;&#31216;&#24615;&#65292;&#36890;&#36807;&#32771;&#34385;&#20449;&#21495;&#22312;&#22266;&#23450;&#22270;&#19978;&#30340;&#23398;&#20064;&#35774;&#32622;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#30340;&#23545;&#31216;&#24615;&#27010;&#24565;&#65292;&#36890;&#36807;&#22270;&#31895;&#21270;&#23454;&#29616;&#12290;&#36825;&#31687;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#20559;&#24046;-&#26041;&#24046;&#20844;&#24335;&#26469;&#34913;&#37327;&#36817;&#20284;&#23545;&#31216;&#24615;...
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36890;&#24120;&#34987;&#25551;&#36848;&#20026;&#23545;&#22270;&#20013;&#30340;&#33410;&#28857;&#37325;&#26032;&#25490;&#24207;&#20855;&#26377;&#32622;&#25442;&#31561;&#21464;&#24615;&#12290;GNNs&#30340;&#36825;&#31181;&#23545;&#31216;&#24615;&#24120;&#34987;&#19982;&#27431;&#20960;&#37324;&#24471;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#30340;&#24179;&#31227;&#31561;&#21464;&#24615;&#27604;&#36739;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#23545;&#31216;&#24615;&#26412;&#36136;&#19978;&#26159;&#19981;&#21516;&#30340;&#65306;CNNs&#30340;&#24179;&#31227;&#31561;&#21464;&#24615;&#23545;&#24212;&#20110;&#20316;&#29992;&#20110;&#22270;&#20687;&#20449;&#21495;&#30340;&#22266;&#23450;&#22495;&#30340;&#23545;&#31216;&#24615;&#65288;&#26377;&#26102;&#31216;&#20026;&#20027;&#21160;&#23545;&#31216;&#24615;&#65289;&#65292;&#32780;&#22312;GNNs&#20013;&#65292;&#20219;&#20309;&#32622;&#25442;&#37117;&#20316;&#29992;&#20110;&#22270;&#20449;&#21495;&#21644;&#22270;&#22495;&#65288;&#26377;&#26102;&#25551;&#36848;&#20026;&#34987;&#21160;&#23545;&#31216;&#24615;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32858;&#28966;&#20110;GNNs&#30340;&#20027;&#21160;&#23545;&#31216;&#24615;&#65292;&#32771;&#34385;&#20449;&#21495;&#22312;&#19968;&#20010;&#22266;&#23450;&#22270;&#19978;&#36827;&#34892;&#23398;&#20064;&#30340;&#24773;&#20917;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;GNNs&#30340;&#33258;&#28982;&#23545;&#31216;&#24615;&#26159;&#22270;&#30340;&#33258;&#21516;&#26500;&#12290;&#30001;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22270;&#24448;&#24448;&#26159;&#38750;&#23545;&#31216;&#30340;&#65292;&#25105;&#20204;&#36890;&#36807;&#24418;&#24335;&#21270;&#22270;&#31895;&#21270;&#26469;&#25918;&#26494;&#23545;&#31216;&#24615;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20559;&#24046;-&#26041;&#24046;&#20844;&#24335;&#26469;&#34913;&#37327;...
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) are commonly described as being permutation equivariant with respect to node relabeling in the graph. This symmetry of GNNs is often compared to the translation equivariance symmetry of Euclidean convolution neural networks (CNNs). However, these two symmetries are fundamentally different: The translation equivariance of CNNs corresponds to symmetries of the fixed domain acting on the image signal (sometimes known as active symmetries), whereas in GNNs any permutation acts on both the graph signals and the graph domain (sometimes described as passive symmetries). In this work, we focus on the active symmetries of GNNs, by considering a learning setting where signals are supported on a fixed graph. In this case, the natural symmetries of GNNs are the automorphisms of the graph. Since real-world graphs tend to be asymmetric, we relax the notion of symmetries by formalizing approximate symmetries via graph coarsening. We present a bias-variance formula that qu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RANL&#30340;&#26032;&#39062;&#21644;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;Hessian&#21021;&#22987;&#21270;&#21644;&#33258;&#36866;&#24212;&#30340;&#35757;&#32451;&#21306;&#22495;&#20998;&#37197;&#65292;&#20811;&#26381;&#20102;&#29275;&#39039;&#27861;&#22312;&#22823;&#35268;&#27169;&#21644;&#24322;&#26500;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#38480;&#21046;&#65292;&#24182;&#23454;&#29616;&#20102;&#32447;&#24615;&#25910;&#25947;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.10154</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#30340;&#36164;&#28304;&#33258;&#36866;&#24212;&#29275;&#39039;&#27861;
&lt;/p&gt;
&lt;p&gt;
Resource-Adaptive Newton's Method for Distributed Learning. (arXiv:2308.10154v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RANL&#30340;&#26032;&#39062;&#21644;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;Hessian&#21021;&#22987;&#21270;&#21644;&#33258;&#36866;&#24212;&#30340;&#35757;&#32451;&#21306;&#22495;&#20998;&#37197;&#65292;&#20811;&#26381;&#20102;&#29275;&#39039;&#27861;&#22312;&#22823;&#35268;&#27169;&#21644;&#24322;&#26500;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#38480;&#21046;&#65292;&#24182;&#23454;&#29616;&#20102;&#32447;&#24615;&#25910;&#25947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#29275;&#39039;&#26041;&#27861;&#30340;&#20998;&#24067;&#24335;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#26354;&#29575;&#20449;&#24687;&#25552;&#20379;&#20102;&#27604;&#19968;&#38454;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#35268;&#27169;&#21644;&#24322;&#26500;&#23398;&#20064;&#29615;&#22659;&#20013;&#65292;&#29275;&#39039;&#26041;&#27861;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#21463;&#21040;&#20102;&#35832;&#22810;&#25361;&#25112;&#30340;&#38480;&#21046;&#65292;&#20363;&#22914;&#19982;Hessian&#30697;&#38453;&#30456;&#20851;&#30340;&#39640;&#35745;&#31639;&#21644;&#36890;&#20449;&#25104;&#26412;&#12289;&#23376;&#27169;&#22411;&#22810;&#26679;&#24615;&#12289;&#35757;&#32451;&#30340;&#36807;&#26102;&#24615;&#21644;&#25968;&#25454;&#30340;&#24322;&#26500;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;RANL&#30340;&#26032;&#39062;&#39640;&#25928;&#31639;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#31616;&#21333;&#30340;Hessian&#21021;&#22987;&#21270;&#21644;&#33258;&#36866;&#24212;&#30340;&#35757;&#32451;&#21306;&#22495;&#20998;&#37197;&#26469;&#20811;&#26381;&#29275;&#39039;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;&#35813;&#31639;&#27861;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#22312;&#38543;&#26426;&#20248;&#21270;&#30340;&#26631;&#20934;&#20551;&#35774;&#19979;&#36827;&#34892;&#20102;&#20005;&#26684;&#20998;&#26512;&#12290;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;RANL&#23454;&#29616;&#20102;&#32447;&#24615;&#25910;&#25947;&#29575;&#65292;&#21516;&#26102;&#26377;&#25928;&#22320;&#36866;&#24212;&#20102;&#21487;&#29992;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed stochastic optimization methods based on Newton's method offer significant advantages over first-order methods by leveraging curvature information for improved performance. However, the practical applicability of Newton's method is hindered in large-scale and heterogeneous learning environments due to challenges such as high computation and communication costs associated with the Hessian matrix, sub-model diversity, staleness in training, and data heterogeneity. To address these challenges, this paper introduces a novel and efficient algorithm called RANL, which overcomes the limitations of Newton's method by employing a simple Hessian initialization and adaptive assignments of training regions. The algorithm demonstrates impressive convergence properties, which are rigorously analyzed under standard assumptions in stochastic optimization. The theoretical analysis establishes that RANL achieves a linear convergence rate while effectively adapting to available resources and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#30340;&#21160;&#24577;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#19987;&#21033;&#30003;&#35831;&#36235;&#21183;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20844;&#21496;&#21644;&#20998;&#31867;&#20195;&#30721;&#30340;&#21487;&#35760;&#24518;&#34920;&#31034;&#65292;&#36890;&#36807;&#21382;&#21490;&#35760;&#24518;&#21644;&#24403;&#21069;&#20449;&#24687;&#26356;&#26032;&#26469;&#25429;&#25417;&#35821;&#20041;&#25509;&#36817;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.09780</link><description>&lt;p&gt;
&#22522;&#20110;&#20107;&#20214;&#30340;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#22312;&#19987;&#21033;&#30003;&#35831;&#36235;&#21183;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Event-based Dynamic Graph Representation Learning for Patent Application Trend Prediction. (arXiv:2308.09780v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#30340;&#21160;&#24577;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#19987;&#21033;&#30003;&#35831;&#36235;&#21183;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20844;&#21496;&#21644;&#20998;&#31867;&#20195;&#30721;&#30340;&#21487;&#35760;&#24518;&#34920;&#31034;&#65292;&#36890;&#36807;&#21382;&#21490;&#35760;&#24518;&#21644;&#24403;&#21069;&#20449;&#24687;&#26356;&#26032;&#26469;&#25429;&#25417;&#35821;&#20041;&#25509;&#36817;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#20844;&#21496;&#22312;&#26410;&#26469;&#19968;&#27573;&#26102;&#38388;&#20869;&#23558;&#30003;&#35831;&#21738;&#20123;&#31867;&#22411;&#30340;&#19987;&#21033;&#33021;&#22815;&#25581;&#31034;&#20986;&#23427;&#20204;&#30340;&#21457;&#23637;&#25112;&#30053;&#65292;&#24182;&#24110;&#21161;&#20854;&#25552;&#21069;&#21457;&#29616;&#28508;&#22312;&#30340;&#21512;&#20316;&#20249;&#20276;&#25110;&#31454;&#20105;&#23545;&#25163;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23545;&#20844;&#21496;&#19981;&#26029;&#21464;&#21270;&#30340;&#20559;&#22909;&#21644;&#23545;&#20998;&#31867;&#20195;&#30721;&#30340;&#35821;&#20041;&#20851;&#32852;&#30340;&#24314;&#27169;&#22256;&#38590;&#65292;&#36825;&#20010;&#38382;&#39064;&#22312;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#40092;&#26377;&#28041;&#21450;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#30340;&#21160;&#24577;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#19987;&#21033;&#30003;&#35831;&#36235;&#21183;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24314;&#31435;&#22312;&#20844;&#21496;&#21644;&#19987;&#21033;&#20998;&#31867;&#20195;&#30721;&#30340;&#21487;&#35760;&#24518;&#34920;&#31034;&#22522;&#30784;&#19978;&#12290;&#24403;&#35266;&#23519;&#21040;&#19968;&#20010;&#26032;&#30340;&#19987;&#21033;&#26102;&#65292;&#30456;&#20851;&#20844;&#21496;&#21644;&#20998;&#31867;&#20195;&#30721;&#30340;&#34920;&#31034;&#26681;&#25454;&#21382;&#21490;&#35760;&#24518;&#21644;&#24403;&#21069;&#32534;&#30721;&#30340;&#20449;&#24687;&#36827;&#34892;&#26356;&#26032;&#12290;&#27492;&#22806;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#23618;&#27425;&#21270;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#20197;&#25429;&#25417;&#19987;&#21033;&#20998;&#31867;&#20195;&#30721;&#30340;&#35821;&#20041;&#25509;&#36817;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate prediction of what types of patents that companies will apply for in the next period of time can figure out their development strategies and help them discover potential partners or competitors in advance. Although important, this problem has been rarely studied in previous research due to the challenges in modelling companies' continuously evolving preferences and capturing the semantic correlations of classification codes. To fill in this gap, we propose an event-based dynamic graph learning framework for patent application trend prediction. In particular, our method is founded on the memorable representations of both companies and patent classification codes. When a new patent is observed, the representations of the related companies and classification codes are updated according to the historical memories and the currently encoded messages. Moreover, a hierarchical message passing mechanism is provided to capture the semantic proximities of patent classification codes by u
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35299;&#20915;&#20102;Baird&#21453;&#20363;&#19978;&#30340;&#25910;&#25947;&#38382;&#39064;&#65292;&#36890;&#36807;&#19968;&#31181;&#20855;&#26377;&#25910;&#25947;&#20445;&#35777;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.09732</link><description>&lt;p&gt;
Baird&#21453;&#20363;&#24050;&#35299;&#20915;&#65306;&#20197;&#35843;&#35797;&#20004;&#20010;&#26102;&#38388;&#23610;&#24230;&#31639;&#27861;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Baird Counterexample Is Solved: with an example of How to Debug a Two-time-scale Algorithm. (arXiv:2308.09732v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09732
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35299;&#20915;&#20102;Baird&#21453;&#20363;&#19978;&#30340;&#25910;&#25947;&#38382;&#39064;&#65292;&#36890;&#36807;&#19968;&#31181;&#20855;&#26377;&#25910;&#25947;&#20445;&#35777;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Baird&#21453;&#20363;&#26159;&#30001;Leemon Baird&#22312;1995&#24180;&#25552;&#20986;&#30340;&#65292;&#39318;&#20808;&#29992;&#20110;&#35777;&#26126;Temporal Difference (TD(0))&#31639;&#27861;&#22312;&#36825;&#20010;&#20363;&#23376;&#19978;&#21457;&#25955;&#12290;&#20174;&#37027;&#26102;&#36215;&#65292;&#23427;&#32463;&#24120;&#34987;&#29992;&#26469;&#27979;&#35797;&#21644;&#27604;&#36739;&#31163;&#31574;&#30053;&#23398;&#20064;&#31639;&#27861;&#12290;&#26799;&#24230;TD&#31639;&#27861;&#35299;&#20915;&#20102;TD&#22312;Baird&#21453;&#20363;&#19978;&#30340;&#21457;&#25955;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#36825;&#20010;&#20363;&#23376;&#19978;&#30340;&#25910;&#25947;&#20173;&#28982;&#38750;&#24120;&#32531;&#24930;&#65292;&#32780;&#19988;&#32531;&#24930;&#30340;&#26412;&#36136;&#36824;&#19981;&#34987;&#24456;&#22909;&#22320;&#29702;&#35299;&#12290;&#26412;&#25991;&#26088;&#22312;&#29305;&#21035;&#29702;&#35299;&#20026;&#20160;&#20040;TDC&#22312;&#36825;&#20010;&#20363;&#23376;&#19978;&#24930;&#65292;&#24182;&#25552;&#20379;&#35843;&#35797;&#20998;&#26512;&#26469;&#29702;&#35299;&#36825;&#31181;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#35843;&#35797;&#25216;&#26415;&#21487;&#20197;&#29992;&#26469;&#30740;&#31350;&#20004;&#20010;&#26102;&#38388;&#23610;&#24230;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#25910;&#25947;&#34892;&#20026;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#26368;&#36817;&#30340;Impression GTD&#31639;&#27861;&#22312;&#36825;&#20010;&#20363;&#23376;&#19978;&#30340;&#23454;&#35777;&#32467;&#26524;&#65292;&#34920;&#26126;&#25910;&#25947;&#38750;&#24120;&#24555;&#65292;&#20107;&#23454;&#19978;&#26159;&#32447;&#24615;&#30340;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;Baird&#21453;&#20363;&#36890;&#36807;&#19968;&#31181;&#20855;&#26377;&#25910;&#25947;&#20445;&#35777;&#30340;&#31639;&#27861;&#35299;&#20915;&#20102;&#65292;&#35813;&#31639;&#27861;&#25910;&#25947;&#21040;TD&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Baird counterexample was proposed by Leemon Baird in 1995, first used to show that the Temporal Difference (TD(0)) algorithm diverges on this example. Since then, it is often used to test and compare off-policy learning algorithms. Gradient TD algorithms solved the divergence issue of TD on Baird counterexample. However, their convergence on this example is still very slow, and the nature of the slowness is not well understood, e.g., see (Sutton and Barto 2018).  This note is to understand in particular, why TDC is slow on this example, and provide debugging analysis to understand this behavior. Our debugging technique can be used to study the convergence behavior of two-time-scale stochastic approximation algorithms. We also provide empirical results of the recent Impression GTD algorithm on this example, showing the convergence is very fast, in fact, in a linear rate. We conclude that Baird counterexample is solved, by an algorithm with convergence guarantee to the TD solution in gen
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24402;&#19968;&#21270;&#26799;&#24230;&#65292;&#25105;&#20204;&#22312;&#26080;&#32447;&#35745;&#31639;&#30340;&#36741;&#21161;&#19979;&#25913;&#36827;&#20102;&#32852;&#37030;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#25910;&#25947;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09082</link><description>&lt;p&gt;
&#22522;&#20110;&#26080;&#32447;&#35745;&#31639;&#30340;&#36741;&#21161;&#32852;&#37030;&#23398;&#20064;&#19982;&#24402;&#19968;&#21270;&#26799;&#24230;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Over-the-Air Computation Aided Federated Learning with the Aggregation of Normalized Gradient. (arXiv:2308.09082v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09082
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24402;&#19968;&#21270;&#26799;&#24230;&#65292;&#25105;&#20204;&#22312;&#26080;&#32447;&#35745;&#31639;&#30340;&#36741;&#21161;&#19979;&#25913;&#36827;&#20102;&#32852;&#37030;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#25910;&#25947;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#32447;&#35745;&#31639;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#19968;&#31181;&#36890;&#20449;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#36825;&#26679;&#30340;&#31995;&#32479;&#20013;&#65292;&#36827;&#34892;&#36845;&#20195;&#36807;&#31243;&#65306;&#27599;&#20010;&#31227;&#21160;&#35774;&#22791;&#26356;&#26032;&#12289;&#25918;&#22823;&#24182;&#20256;&#36755;&#31169;&#26377;&#25439;&#22833;&#20989;&#25968;&#30340;&#23616;&#37096;&#26799;&#24230;&#65307;&#26381;&#21153;&#22120;&#25509;&#25910;&#19968;&#27425;&#24615;&#32858;&#21512;&#30340;&#26799;&#24230;&#65292;&#29983;&#25104;&#24182;&#24191;&#25773;&#26356;&#26032;&#30340;&#27169;&#22411;&#21442;&#25968;&#32473;&#27599;&#20010;&#31227;&#21160;&#35774;&#22791;&#12290;&#22312;&#25918;&#22823;&#22240;&#23376;&#36873;&#25321;&#26041;&#38754;&#65292;&#22823;&#22810;&#25968;&#30456;&#20851;&#24037;&#20316;&#20551;&#35774;&#23616;&#37096;&#26799;&#24230;&#30340;&#26368;&#22823;&#33539;&#25968;&#22987;&#32456;&#21457;&#29983;&#65292;&#23613;&#31649;&#23454;&#38469;&#19978;&#23427;&#22312;&#36845;&#20195;&#20013;&#20250;&#27874;&#21160;&#65292;&#36825;&#21487;&#33021;&#38477;&#20302;&#25910;&#25947;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#25918;&#22823;&#20043;&#21069;&#23558;&#23616;&#37096;&#26799;&#24230;&#24402;&#19968;&#21270;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19979;&#65292;&#24403;&#25439;&#22833;&#20989;&#25968;&#24179;&#28369;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20197;&#27425;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#21040;&#31283;&#23450;&#28857;&#12290;&#22312;&#24179;&#28369;&#19988;&#24378;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20197;&#32447;&#24615;&#36895;&#29575;&#23454;&#29616;&#26368;&#23567;&#35757;&#32451;&#25439;&#22833;&#65292;&#19988;&#35813;&#36895;&#29575;&#21487;&#20197;&#20219;&#24847;&#23567;&#27491;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over-the-air computation is a communication-efficient solution for federated learning (FL). In such a system, iterative procedure is performed: Local gradient of private loss function is updated, amplified and then transmitted by every mobile device; the server receives the aggregated gradient all-at-once, generates and then broadcasts updated model parameters to every mobile device. In terms of amplification factor selection, most related works suppose the local gradient's maximal norm always happens although it actually fluctuates over iterations, which may degrade convergence performance. To circumvent this problem, we propose to turn local gradient to be normalized one before amplifying it. Under our proposed method, when the loss function is smooth, we prove our proposed method can converge to stationary point at sub-linear rate. In case of smooth and strongly convex loss function, we prove our proposed method can achieve minimal training loss at linear rate with any small positiv
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DealMVC&#30340;&#21452;&#21521;&#23545;&#27604;&#26657;&#20934;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#35270;&#22270;&#32858;&#31867;&#20013;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#23545;&#27604;&#26657;&#20934;&#25439;&#22833;&#26469;&#25552;&#39640;&#32858;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09000</link><description>&lt;p&gt;
DealMVC: &#38754;&#21521;&#22810;&#35270;&#35282;&#32858;&#31867;&#30340;&#21452;&#21521;&#23545;&#27604;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
DealMVC: Dual Contrastive Calibration for Multi-view Clustering. (arXiv:2308.09000v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09000
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DealMVC&#30340;&#21452;&#21521;&#23545;&#27604;&#26657;&#20934;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#35270;&#22270;&#32858;&#31867;&#20013;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#23545;&#27604;&#26657;&#20934;&#25439;&#22833;&#26469;&#25552;&#39640;&#32858;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#23545;&#27604;&#32858;&#31867;&#30001;&#20110;&#20854;&#24378;&#22823;&#30340;&#35270;&#35282;&#19968;&#33268;&#24615;&#20449;&#24687;&#25366;&#25496;&#33021;&#21147;&#32780;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19968;&#20010;&#38480;&#21046;&#32858;&#31867;&#24615;&#33021;&#36827;&#19968;&#27493;&#25552;&#21319;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#22810;&#35270;&#22270;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#19981;&#21516;&#35270;&#22270;&#20013;&#30456;&#21516;&#26679;&#26412;&#30340;&#19968;&#33268;&#24615;&#65292;&#32780;&#24573;&#35270;&#20102;&#20132;&#21449;&#35270;&#22270;&#22330;&#26223;&#20013;&#31867;&#20284;&#20294;&#19981;&#21516;&#30340;&#26679;&#26412;&#24773;&#20917;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#22810;&#35270;&#22270;&#32858;&#31867;&#30340;&#21452;&#21521;&#23545;&#27604;&#26657;&#20934;&#32593;&#32476;&#65288;DealMVC&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#34701;&#21512;&#26426;&#21046;&#65292;&#33719;&#24471;&#20840;&#23616;&#30340;&#36328;&#35270;&#22270;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23545;&#40784;&#35270;&#22270;&#29305;&#24449;&#30456;&#20284;&#24615;&#22270;&#21644;&#39640;&#32622;&#20449;&#24230;&#20266;&#26631;&#31614;&#22270;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#23616;&#23545;&#27604;&#26657;&#20934;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#21033;&#29992;&#22810;&#35270;&#22270;&#20449;&#24687;&#30340;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23616;&#37096;&#23545;&#27604;&#26657;&#20934;&#25439;&#22833;&#65292;&#29992;&#20110;&#32422;&#26463;&#25104;&#23545;&#35270;&#22270;&#29305;&#24449;&#30340;&#19968;&#33268;&#24615;&#12290;&#29305;&#24449;&#32467;&#26500;&#26159; ...
&lt;/p&gt;
&lt;p&gt;
Benefiting from the strong view-consistent information mining capacity, multi-view contrastive clustering has attracted plenty of attention in recent years. However, we observe the following drawback, which limits the clustering performance from further improvement. The existing multi-view models mainly focus on the consistency of the same samples in different views while ignoring the circumstance of similar but different samples in cross-view scenarios. To solve this problem, we propose a novel Dual contrastive calibration network for Multi-View Clustering (DealMVC). Specifically, we first design a fusion mechanism to obtain a global cross-view feature. Then, a global contrastive calibration loss is proposed by aligning the view feature similarity graph and the high-confidence pseudo-label graph. Moreover, to utilize the diversity of multi-view information, we propose a local contrastive calibration loss to constrain the consistency of pair-wise view features. The feature structure is
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COVERT&#30340;&#23545;&#27604;&#22270;&#32858;&#31867;&#32593;&#32476;&#65292;&#36890;&#36807;&#21487;&#36870;&#25200;&#21160;&#24674;&#22797;&#32593;&#32476;&#22788;&#29702;&#25968;&#25454;&#22686;&#24378;&#65292;&#25552;&#28860;&#21487;&#38752;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#36827;&#19968;&#27493;&#20445;&#35777;&#23545;&#27604;&#23398;&#20064;&#20013;&#35821;&#20041;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08963</link><description>&lt;p&gt;
CONVERT: &#21487;&#38752;&#22686;&#24378;&#30340;&#23545;&#27604;&#22270;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
CONVERT:Contrastive Graph Clustering with Reliable Augmentation. (arXiv:2308.08963v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COVERT&#30340;&#23545;&#27604;&#22270;&#32858;&#31867;&#32593;&#32476;&#65292;&#36890;&#36807;&#21487;&#36870;&#25200;&#21160;&#24674;&#22797;&#32593;&#32476;&#22788;&#29702;&#25968;&#25454;&#22686;&#24378;&#65292;&#25552;&#28860;&#21487;&#38752;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#36827;&#19968;&#27493;&#20445;&#35777;&#23545;&#27604;&#23398;&#20064;&#20013;&#35821;&#20041;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#25968;&#25454;&#22686;&#24378;&#36827;&#34892;&#23545;&#27604;&#22270;&#33410;&#28857;&#32858;&#31867;&#26159;&#26080;&#30417;&#30563;&#22270;&#23398;&#20064;&#39046;&#22495;&#30340;&#30740;&#31350;&#28909;&#28857;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#39044;&#23450;&#20041;&#22686;&#24378;&#30340;&#37319;&#26679;&#20998;&#24067;&#26469;&#33258;&#21160;&#29983;&#25104;&#25968;&#25454;&#39537;&#21160;&#22686;&#24378;&#12290;&#34429;&#28982;&#24050;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32858;&#31867;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#26041;&#27861;&#20173;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#22686;&#24378;&#65292;&#22686;&#24378;&#21518;&#30340;&#22270;&#30340;&#35821;&#20041;&#23481;&#26131;&#28418;&#31227;&#65292;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#22686;&#24378;&#35270;&#22270;&#35821;&#20041;&#30340;&#21487;&#38752;&#24615;&#26080;&#27861;&#20445;&#35777;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#38752;&#22686;&#24378;&#30340;&#23545;&#27604;&#22270;&#32858;&#31867;&#32593;&#32476;&#65288;COVERT&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#22686;&#24378;&#25968;&#25454;&#36890;&#36807;&#25552;&#20986;&#30340;&#21487;&#36870;&#25200;&#21160;&#24674;&#22797;&#32593;&#32476;&#36827;&#34892;&#22788;&#29702;&#12290;&#23427;&#36890;&#36807;&#24674;&#22797;&#25200;&#21160;&#21518;&#30340;&#28508;&#22312;&#23884;&#20837;&#26469;&#25552;&#28860;&#21487;&#38752;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#36827;&#19968;&#27493;&#20445;&#35777;&#35821;&#20041;&#30340;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#20041;Loss&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive graph node clustering via learnable data augmentation is a hot research spot in the field of unsupervised graph learning. The existing methods learn the sampling distribution of a pre-defined augmentation to generate data-driven augmentations automatically. Although promising clustering performance has been achieved, we observe that these strategies still rely on pre-defined augmentations, the semantics of the augmented graph can easily drift. The reliability of the augmented view semantics for contrastive learning can not be guaranteed, thus limiting the model performance. To address these problems, we propose a novel CONtrastiVe Graph ClustEring network with Reliable AugmenTation (COVERT). Specifically, in our method, the data augmentations are processed by the proposed reversible perturb-recover network. It distills reliable semantic information by recovering the perturbed latent embeddings. Moreover, to further guarantee the reliability of semantics, a novel semantic lo
&lt;/p&gt;</description></item><item><title>Real Robot Challenge 2022&#20026;RL&#21644;&#26426;&#22120;&#20154;&#23398;&#30028;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#20801;&#35768;&#21442;&#19982;&#32773;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#23398;&#20064;&#28789;&#24039;&#25805;&#20316;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#22312;&#27169;&#25311;&#20013;&#24471;&#21040;&#30340;&#35265;&#35299;&#19981;&#33021;&#36716;&#21270;&#21040;&#30495;&#23454;&#26426;&#22120;&#20154;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.07741</link><description>&lt;p&gt;
Real Robot Challenge 2022: &#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#23398;&#20064;&#28789;&#24039;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Real Robot Challenge 2022: Learning Dexterous Manipulation from Offline Data in the Real World. (arXiv:2308.07741v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07741
&lt;/p&gt;
&lt;p&gt;
Real Robot Challenge 2022&#20026;RL&#21644;&#26426;&#22120;&#20154;&#23398;&#30028;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#20801;&#35768;&#21442;&#19982;&#32773;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#23398;&#20064;&#28789;&#24039;&#25805;&#20316;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#22312;&#27169;&#25311;&#20013;&#24471;&#21040;&#30340;&#35265;&#35299;&#19981;&#33021;&#36716;&#21270;&#21040;&#30495;&#23454;&#26426;&#22120;&#20154;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#36827;&#34892;&#23454;&#39564;&#22312;&#26102;&#38388;&#21644;&#25104;&#26412;&#19978;&#35201;&#27714;&#24456;&#39640;&#12290;&#22240;&#27492;&#65292;&#22686;&#24378;&#23398;&#20064;&#65288;RL&#65289;&#31038;&#21306;&#30340;&#24456;&#22823;&#19968;&#37096;&#20998;&#20351;&#29992;&#27169;&#25311;&#22120;&#26469;&#24320;&#21457;&#21644;&#35780;&#20272;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#20174;&#27169;&#25311;&#20013;&#24471;&#21040;&#30340;&#35265;&#35299;&#19981;&#19968;&#23450;&#33021;&#22815;&#36716;&#21270;&#21040;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#28041;&#21450;&#22797;&#26434;&#29615;&#22659;&#20132;&#20114;&#30340;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;Real Robot Challenge 2022&#20316;&#20026;RL&#21644;&#26426;&#22120;&#20154;&#23398;&#30028;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#35753;&#21442;&#19982;&#32773;&#33021;&#22815;&#20687;&#22312;&#27169;&#25311;&#20013;&#19968;&#26679;&#36731;&#26494;&#22320;&#36828;&#31243;&#23454;&#39564;&#30495;&#23454;&#26426;&#22120;&#20154;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#24050;&#32463;&#25104;&#29087;&#20026;&#19968;&#31181;&#20174;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#30340;&#26377;&#24076;&#26395;&#30340;&#33539;&#24335;&#65292;&#20943;&#36731;&#20102;&#23545;&#26114;&#36149;&#22312;&#32447;&#20132;&#20114;&#30340;&#20381;&#36182;.&#22240;&#27492;&#65292;&#25105;&#20204;&#35201;&#27714;&#21442;&#19982;&#32773;&#20174;&#25552;&#20379;&#30340;&#30495;&#23454;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#20004;&#20010;&#28789;&#24039;&#25805;&#20316;&#20219;&#21153;&#65292;&#21253;&#25324;&#25512;&#21160;&#12289;&#25235;&#21462;&#21644;&#25163;&#20869;&#23450;&#20301;&#12290;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#36719;&#20214;&#25991;&#26723;&#21270;&#65292;&#24182;&#22312;&#22522;&#20110;&#20223;&#30495;&#30340;&#21021;&#27493;&#38454;&#27573;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Experimentation on real robots is demanding in terms of time and costs. For this reason, a large part of the reinforcement learning (RL) community uses simulators to develop and benchmark algorithms. However, insights gained in simulation do not necessarily translate to real robots, in particular for tasks involving complex interactions with the environment. The Real Robot Challenge 2022 therefore served as a bridge between the RL and robotics communities by allowing participants to experiment remotely with a real robot - as easily as in simulation.  In the last years, offline reinforcement learning has matured into a promising paradigm for learning from pre-collected datasets, alleviating the reliance on expensive online interactions. We therefore asked the participants to learn two dexterous manipulation tasks involving pushing, grasping, and in-hand orientation from provided real-robot datasets. An extensive software documentation and an initial stage based on a simulation of the re
&lt;/p&gt;</description></item><item><title>&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20351;&#29992;&#21069;&#32512;&#35821;&#35328;&#27169;&#22411;&#65288;PrefixLM&#65289;&#27604;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#65288;CausalLM&#65289;&#25928;&#26524;&#26356;&#22909;&#12290;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#65292;&#34429;&#28982;&#20004;&#31181;&#35821;&#35328;&#27169;&#22411;&#37117;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#21040;&#31283;&#23450;&#28857;&#65292;&#20294;&#21069;&#32512;&#35821;&#35328;&#27169;&#22411;&#25910;&#25947;&#21040;&#32447;&#24615;&#22238;&#24402;&#30340;&#26368;&#20248;&#35299;&#65292;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#30340;&#25910;&#25947;&#21160;&#24577;&#36981;&#24490;&#22312;&#32447;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#19981;&#20445;&#35777;&#25910;&#25947;&#21040;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2308.06912</link><description>&lt;p&gt;
CausalLM&#19981;&#36866;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CausalLM is not optimal for in-context learning. (arXiv:2308.06912v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06912
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20351;&#29992;&#21069;&#32512;&#35821;&#35328;&#27169;&#22411;&#65288;PrefixLM&#65289;&#27604;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#65288;CausalLM&#65289;&#25928;&#26524;&#26356;&#22909;&#12290;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#65292;&#34429;&#28982;&#20004;&#31181;&#35821;&#35328;&#27169;&#22411;&#37117;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#21040;&#31283;&#23450;&#28857;&#65292;&#20294;&#21069;&#32512;&#35821;&#35328;&#27169;&#22411;&#25910;&#25947;&#21040;&#32447;&#24615;&#22238;&#24402;&#30340;&#26368;&#20248;&#35299;&#65292;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#30340;&#25910;&#25947;&#21160;&#24577;&#36981;&#24490;&#22312;&#32447;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#19981;&#20445;&#35777;&#25910;&#25947;&#21040;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#20351;&#29992;&#21069;&#32512;&#35821;&#35328;&#27169;&#22411;&#65288;PrefixLM&#65289;&#34920;&#29616;&#26356;&#22909;&#65292;&#20854;&#20801;&#35768;&#19978;&#19979;&#25991;&#26679;&#26412;&#30456;&#20114;&#20851;&#27880;&#65307;&#30456;&#27604;&#20043;&#19979;&#65292;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#65288;CausalLM&#65289;&#20351;&#29992;&#33258;&#22238;&#24402;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#31105;&#27490;&#19978;&#19979;&#25991;&#26679;&#26412;&#20851;&#27880;&#26410;&#26469;&#30340;&#26679;&#26412;&#12290;&#34429;&#28982;&#36825;&#20010;&#32467;&#26524;&#26159;&#30452;&#35266;&#30340;&#65292;&#20294;&#20174;&#29702;&#35770;&#35282;&#24230;&#24182;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#37319;&#29992;&#29702;&#35770;&#26041;&#27861;&#65292;&#20998;&#26512;&#20102;&#22312;&#29305;&#23450;&#21442;&#25968;&#26500;&#24314;&#19979;&#65292;&#21069;&#32512;&#35821;&#35328;&#27169;&#22411;&#21644;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#30340;&#25910;&#25947;&#34892;&#20026;&#12290;&#20998;&#26512;&#32467;&#26524;&#26174;&#31034;&#65292;&#20004;&#31181;&#35821;&#35328;&#27169;&#22411;&#37117;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#21040;&#31283;&#23450;&#28857;&#65292;&#20294;&#21069;&#32512;&#35821;&#35328;&#27169;&#22411;&#25910;&#25947;&#21040;&#32447;&#24615;&#22238;&#24402;&#30340;&#26368;&#20248;&#35299;&#65292;&#32780;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#30340;&#25910;&#25947;&#21160;&#24577;&#36981;&#24490;&#22312;&#32447;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#21363;&#20351;&#26679;&#26412;&#25968;&#37327;&#36235;&#20110;&#26080;&#31351;&#65292;&#20063;&#19981;&#33021;&#20445;&#35777;&#25910;&#25947;&#21040;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#30340;&#32463;&#39564;&#23454;&#39564;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#29702;&#35770;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent empirical evidence indicates that transformer based in-context learning performs better when using a prefix language model (prefixLM), in which in-context samples can all attend to each other, compared to causal language models (causalLM), which use auto-regressive attention that prohibits in-context samples to attend to future samples. While this result is intuitive, it is not understood from a theoretical perspective. In this paper we take a theoretical approach and analyze the convergence behavior of prefixLM and causalLM under a certain parameter construction. Our analysis shows that both LM types converge to their stationary points at a linear rate, but that while prefixLM converges to the optimal solution of linear regression, causalLM convergence dynamics follows that of an online gradient descent algorithm, which is not guaranteed to be optimal even as the number of samples grows infinitely. We supplement our theoretical claims with empirical experiments over synthetic a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25299;&#25169;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#35843;&#22270;&#20013;&#30340;&#36335;&#24452;&#65292;&#31361;&#30772;&#20102;1-Weisfeiler-Lehmann&#27979;&#35797;&#30340;&#29702;&#35770;&#38480;&#21046;&#65292;&#24182;&#22312;&#26080;&#38656;&#23545;&#22270;&#30340;&#23376;&#32467;&#26500;&#36827;&#34892;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06838</link><description>&lt;p&gt;
&#20351;&#29992;&#36335;&#24452;&#36827;&#34892;&#19968;&#33324;&#21270;&#25299;&#25169;&#22270;&#31070;&#32463;&#32593;&#32476; (arXiv:2308.06838v2 [cs.LG] &#24050;&#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
Generalizing Topological Graph Neural Networks with Paths. (arXiv:2308.06838v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25299;&#25169;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#35843;&#22270;&#20013;&#30340;&#36335;&#24452;&#65292;&#31361;&#30772;&#20102;1-Weisfeiler-Lehmann&#27979;&#35797;&#30340;&#29702;&#35770;&#38480;&#21046;&#65292;&#24182;&#22312;&#26080;&#38656;&#23545;&#22270;&#30340;&#23376;&#32467;&#26500;&#36827;&#34892;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#19981;&#21516;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#21463;&#21040;1-Weisfeiler-Lehmann&#27979;&#35797;&#30340;&#29702;&#35770;&#38480;&#21046;&#12290;&#23613;&#31649;&#39640;&#38454;GNNs&#30340;&#26368;&#26032;&#36827;&#23637;&#21487;&#20197;&#20811;&#26381;&#36825;&#20010;&#38556;&#30861;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38598;&#20013;&#22312;&#29305;&#23450;&#30340;&#22270;&#32452;&#20214;&#65292;&#22914;&#22242;&#25110;&#29615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36208;&#20102;&#19981;&#21516;&#30340;&#36335;&#32447;&#12290;&#25105;&#20204;&#24378;&#35843;&#36335;&#24452;&#65292;&#36825;&#26159;&#27599;&#20010;&#22270;&#20013;&#22266;&#26377;&#30340;&#12290;&#25105;&#20204;&#33021;&#22815;&#26500;&#24314;&#19968;&#20010;&#26356;&#36890;&#29992;&#30340;&#25299;&#25169;&#35270;&#35282;&#65292;&#24182;&#19982;&#20854;&#20182;&#25299;&#25169;&#39046;&#22495;&#30340;&#19968;&#20123;&#25104;&#29087;&#29702;&#35770;&#24418;&#25104;&#26725;&#26753;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#22312;&#27809;&#26377;&#23545;&#22270;&#30340;&#23376;&#32467;&#26500;&#36827;&#34892;&#20219;&#20309;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36229;&#36807;&#20102;&#35813;&#39046;&#22495;&#26089;&#26399;&#30340;&#25216;&#26415;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Graph Neural Networks (GNNs) have made significant strides in diverse areas, they are hindered by a theoretical constraint known as the 1-Weisfeiler-Lehmann test. Even though latest advancements in higher-order GNNs can overcome this boundary, they typically center around certain graph components like cliques or cycles. However, our investigation goes a different route. We put emphasis on paths, which are inherent in every graph. We are able to construct a more general topological perspective and form a bridge to certain established theories about other topological domains. Interestingly, without any assumptions on graph sub-structures, our approach surpasses earlier techniques in this field, achieving state-of-the-art performance on several benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;Apriori&#31639;&#27861;&#22522;&#20110;&#20851;&#32852;&#35268;&#21017;&#25366;&#25496;&#30340;&#26041;&#27861;&#65292;&#20174;COVID-19&#24739;&#32773;&#20013;&#21457;&#29616;&#20102;&#19968;&#20123;&#26368;&#24120;&#35265;&#30340;&#30151;&#29366;&#27169;&#24335;&#65292;&#22914;&#21628;&#21560;&#26242;&#20572;&#12289;&#21683;&#22013;&#12289;&#21457;&#28909;&#31561;&#65292;&#20026;&#20020;&#24202;&#21307;&#29983;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#30142;&#30149;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2308.06763</link><description>&lt;p&gt;
&#20351;&#29992;Apriori&#20851;&#32852;&#35268;&#21017;&#25366;&#25496;&#20174;&#24247;&#22797;&#21644;&#27515;&#20129;&#30149;&#20363;&#20013;&#21457;&#29616;COVID-19&#30340;&#30151;&#29366;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Discovering the Symptom Patterns of COVID-19 from Recovered and Deceased Patients Using Apriori Association Rule Mining. (arXiv:2308.06763v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;Apriori&#31639;&#27861;&#22522;&#20110;&#20851;&#32852;&#35268;&#21017;&#25366;&#25496;&#30340;&#26041;&#27861;&#65292;&#20174;COVID-19&#24739;&#32773;&#20013;&#21457;&#29616;&#20102;&#19968;&#20123;&#26368;&#24120;&#35265;&#30340;&#30151;&#29366;&#27169;&#24335;&#65292;&#22914;&#21628;&#21560;&#26242;&#20572;&#12289;&#21683;&#22013;&#12289;&#21457;&#28909;&#31561;&#65292;&#20026;&#20020;&#24202;&#21307;&#29983;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#30142;&#30149;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#22823;&#27969;&#34892;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#36896;&#25104;&#20102;&#27585;&#28781;&#24615;&#30340;&#24433;&#21709;&#65292;&#22842;&#36208;&#20102;&#25968;&#30334;&#19975;&#20154;&#30340;&#29983;&#21629;&#65292;&#36896;&#25104;&#20102;&#37325;&#22823;&#30340;&#31038;&#20250;&#21644;&#32463;&#27982;&#20013;&#26029;&#12290;&#20026;&#20102;&#20248;&#21270;&#20915;&#31574;&#24182;&#20998;&#37197;&#26377;&#38480;&#36164;&#28304;&#65292;&#30830;&#23450;COVID-19&#30340;&#30151;&#29366;&#24182;&#30830;&#23450;&#27599;&#31181;&#30149;&#20363;&#30340;&#20005;&#37325;&#31243;&#24230;&#38750;&#24120;&#37325;&#35201;&#12290;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#22312;&#25366;&#25496;&#20020;&#24202;&#25968;&#25454;&#38598;&#20197;&#33719;&#21462;&#26377;&#29992;&#20449;&#24687;&#21644;&#25351;&#23548;&#31185;&#23398;&#20915;&#31574;&#26041;&#38754;&#12290;&#20851;&#32852;&#35268;&#21017;&#25366;&#25496;&#26159;&#19968;&#31181;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#38544;&#34255;&#27169;&#24335;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Apriori&#31639;&#27861;&#30340;&#20851;&#32852;&#35268;&#21017;&#25366;&#25496;&#22312;COVID-19&#24739;&#32773;&#20013;&#21457;&#29616;&#30151;&#29366;&#27169;&#24335;&#30340;&#24212;&#29992;&#12290;&#35813;&#30740;&#31350;&#20351;&#29992;2875&#20363;&#24739;&#32773;&#35760;&#24405;&#65292;&#30830;&#23450;&#20102;&#26368;&#24120;&#35265;&#30340;&#30151;&#29366;&#20026;&#21628;&#21560;&#26242;&#20572;&#65288;72&#65285;&#65289;&#12289;&#21683;&#22013;&#65288;64&#65285;&#65289;&#12289;&#21457;&#28909;&#65288;59&#65285;&#65289;&#12289;&#34394;&#24369;&#65288;18&#65285;&#65289;&#12289;&#32908;&#32905;&#30171;&#65288;14.5&#65285;&#65289;&#21644;&#21897;&#21657;&#30171;&#65288;12&#65285;&#65289;&#12290;&#35813;&#26041;&#27861;&#20026;&#20020;&#24202;&#21307;&#29983;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#30142;&#30149;&#35265;&#35299;&#65292;&#21487;&#20197;&#24110;&#21161;&#20182;&#20204;&#36827;&#34892;&#31649;&#29702;&#21644;&#27835;&#30103;&#12290;
&lt;/p&gt;
&lt;p&gt;
The COVID-19 pandemic has a devastating impact globally, claiming millions of lives and causing significant social and economic disruptions. In order to optimize decision-making and allocate limited resources, it is essential to identify COVID-19 symptoms and determine the severity of each case. Machine learning algorithms offer a potent tool in the medical field, particularly in mining clinical datasets for useful information and guiding scientific decisions. Association rule mining is a machine learning technique for extracting hidden patterns from data. This paper presents an application of association rule mining based Apriori algorithm to discover symptom patterns from COVID-19 patients. The study, using 2875 patient's records, identified the most common signs and symptoms as apnea (72%), cough (64%), fever (59%), weakness (18%), myalgia (14.5%), and sore throat (12%). The proposed method provides clinicians with valuable insight into disease that can assist them in managing and t
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38477;&#27700;&#21363;&#26102;&#39044;&#27979;&#30340;&#38382;&#39064;&#65292;&#29983;&#25104;&#27169;&#22411;&#22312;&#22825;&#27668;&#39044;&#25253;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#28508;&#21147;&#65292;&#24182;&#33021;&#22815;&#26356;&#22909;&#22320;&#24314;&#27169;&#27010;&#29575;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2308.06733</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#30340;&#38477;&#27700;&#21363;&#26102;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Precipitation nowcasting with generative diffusion models. (arXiv:2308.06733v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06733
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38477;&#27700;&#21363;&#26102;&#39044;&#27979;&#30340;&#38382;&#39064;&#65292;&#29983;&#25104;&#27169;&#22411;&#22312;&#22825;&#27668;&#39044;&#25253;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#28508;&#21147;&#65292;&#24182;&#33021;&#22815;&#26356;&#22909;&#22320;&#24314;&#27169;&#27010;&#29575;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20256;&#32479;&#30340;&#25968;&#20540;&#39044;&#27979;&#26041;&#27861;&#36880;&#28176;&#21463;&#21040;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#25361;&#25112;&#12290;&#29992;&#20110;&#30701;&#26399;&#21644;&#20013;&#26399;&#22825;&#27668;&#39044;&#25253;&#30340;&#22810;&#20010;&#21382;&#21490;&#25968;&#25454;&#38598;&#36890;&#24120;&#34987;&#32452;&#32455;&#25104;&#35268;&#21017;&#30340;&#31354;&#38388;&#32593;&#26684;&#32467;&#26500;&#12290;&#36825;&#31181;&#25490;&#21015;&#19982;&#22270;&#20687;&#38750;&#24120;&#30456;&#20284;&#65306;&#27599;&#20010;&#22825;&#27668;&#21464;&#37327;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#24133;&#22320;&#22270;&#65292;&#25110;&#32773;&#22312;&#32771;&#34385;&#26102;&#38388;&#36724;&#26102;&#65292;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#27573;&#35270;&#39057;&#12290;&#22810;&#20010;&#31867;&#21035;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21253;&#25324;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#12289;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#25110;&#32773;&#26368;&#36817;&#30340;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#19979;&#19968;&#24103;&#39044;&#27979;&#38382;&#39064;&#19978;&#24050;&#32463;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#65292;&#22240;&#27492;&#33258;&#28982;&#32780;&#28982;&#22320;&#24076;&#26395;&#27979;&#35797;&#23427;&#20204;&#22312;&#22825;&#27668;&#39044;&#25253;&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25193;&#25955;&#27169;&#22411;&#29305;&#21035;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#22825;&#27668;&#39044;&#25253;&#26412;&#36136;&#19978;&#26159;&#20855;&#26377;&#27010;&#29575;&#24615;&#30340;&#65306;&#25105;&#20204;&#30495;&#27491;&#26377;&#20852;&#36259;&#24314;&#27169;&#30340;&#26159;&#22825;&#27668;&#25351;&#26631;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#20854;&#26399;&#26395;&#20540;&#26159;&#26368;&#21487;&#33021;&#30340;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years traditional numerical methods for accurate weather prediction have been increasingly challenged by deep learning methods. Numerous historical datasets used for short and medium-range weather forecasts are typically organized into a regular spatial grid structure. This arrangement closely resembles images: each weather variable can be visualized as a map or, when considering the temporal axis, as a video. Several classes of generative models, comprising Generative Adversarial Networks, Variational Autoencoders, or the recent Denoising Diffusion Models have largely proved their applicability to the next-frame prediction problem, and is thus natural to test their performance on the weather prediction benchmarks. Diffusion models are particularly appealing in this context, due to the intrinsically probabilistic nature of weather forecasting: what we are really interested to model is the probability distribution of weather indicators, whose expected value is the most likely 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#24773;&#22659;&#36718;&#30424;&#36172;&#20013;&#30340;&#36716;&#31227;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#35782;&#21035;&#34892;&#20026;&#21644;&#22870;&#21169;&#22240;&#26524;&#25928;&#24212;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#22240;&#26524;&#32422;&#26463;&#26469;&#25913;&#36827;&#36718;&#30424;&#36172;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.03572</link><description>&lt;p&gt;
&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#24773;&#22659;&#36718;&#30424;&#36172;&#20013;&#30340;&#21487;&#35777;&#25928;&#29575;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Provably Efficient Learning in Partially Observable Contextual Bandit. (arXiv:2308.03572v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#24773;&#22659;&#36718;&#30424;&#36172;&#20013;&#30340;&#36716;&#31227;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#35782;&#21035;&#34892;&#20026;&#21644;&#22870;&#21169;&#22240;&#26524;&#25928;&#24212;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#22240;&#26524;&#32422;&#26463;&#26469;&#25913;&#36827;&#36718;&#30424;&#36172;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#24773;&#22659;&#36718;&#30424;&#36172;&#20013;&#30340;&#36716;&#31227;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#20195;&#29702;&#20154;&#20165;&#26377;&#26469;&#33258;&#20854;&#20182;&#20195;&#29702;&#20154;&#30340;&#26377;&#38480;&#30693;&#35782;&#65292;&#24182;&#19988;&#23545;&#38544;&#34255;&#30340;&#28151;&#28102;&#22240;&#32032;&#21482;&#26377;&#37096;&#20998;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#35813;&#38382;&#39064;&#36716;&#21270;&#20026;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#26469;&#35782;&#21035;&#25110;&#37096;&#20998;&#35782;&#21035;&#34892;&#20026;&#21644;&#22870;&#21169;&#20043;&#38388;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#26410;&#30693;&#20998;&#24067;&#30340;&#21407;&#22987;&#21151;&#33021;&#32422;&#26463;&#31163;&#25955;&#21270;&#20026;&#32447;&#24615;&#32422;&#26463;&#65292;&#24182;&#36890;&#36807;&#39034;&#24207;&#35299;&#32447;&#24615;&#35268;&#21010;&#26469;&#37319;&#26679;&#20860;&#23481;&#30340;&#22240;&#26524;&#27169;&#22411;&#65292;&#20197;&#32771;&#34385;&#20272;&#35745;&#35823;&#24046;&#24471;&#21040;&#22240;&#26524;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#37319;&#26679;&#31639;&#27861;&#20026;&#36866;&#24403;&#30340;&#37319;&#26679;&#20998;&#24067;&#25552;&#20379;&#20102;&#29702;&#24819;&#30340;&#25910;&#25947;&#32467;&#26524;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#22240;&#26524;&#32422;&#26463;&#24212;&#29992;&#20110;&#25913;&#36827;&#32463;&#20856;&#30340;&#36718;&#30424;&#36172;&#31639;&#27861;&#65292;&#24182;&#20197;&#34892;&#21160;&#38598;&#21644;&#20989;&#25968;&#31354;&#38388;&#35268;&#27169;&#20026;&#21442;&#32771;&#25913;&#21464;&#20102;&#36951;&#25022;&#20540;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#20801;&#35768;&#25105;&#20204;&#22788;&#29702;&#19968;&#33324;&#24773;&#22659;&#20998;&#24067;&#30340;&#20989;&#25968;&#36924;&#36817;&#20219;&#21153;&#20013;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate transfer learning in partially observable contextual bandits, where agents have limited knowledge from other agents and partial information about hidden confounders. We first convert the problem to identifying or partially identifying causal effects between actions and rewards through optimization problems. To solve these optimization problems, we discretize the original functional constraints of unknown distributions into linear constraints, and sample compatible causal models via sequentially solving linear programmings to obtain causal bounds with the consideration of estimation error. Our sampling algorithms provide desirable convergence results for suitable sampling distributions. We then show how causal bounds can be applied to improving classical bandit algorithms and affect the regrets with respect to the size of action sets and function spaces. Notably, in the task with function approximation which allows us to handle general context distributions
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36793;&#32536;&#31283;&#23450;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#65288;ES$^2$N&#65289;&#26550;&#26500;&#65292;&#32467;&#21512;&#20102;&#28176;&#36827;&#35760;&#24518;&#23646;&#24615;&#21644;&#23613;&#21487;&#33021;&#20445;&#30041;&#26356;&#22810;&#35760;&#24518;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#23558;&#20648;&#22791;&#23618;&#23450;&#20041;&#20026;&#38750;&#32447;&#24615;&#21644;&#32447;&#24615;&#30340;&#20984;&#32452;&#21512;&#65292;&#25552;&#20379;&#20102;&#32593;&#32476;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#30340;&#22686;&#24378;&#12290;</title><link>http://arxiv.org/abs/2308.02902</link><description>&lt;p&gt;
&#36793;&#32536;&#31283;&#23450;&#30340;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Edge of stability echo state networks. (arXiv:2308.02902v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36793;&#32536;&#31283;&#23450;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#65288;ES$^2$N&#65289;&#26550;&#26500;&#65292;&#32467;&#21512;&#20102;&#28176;&#36827;&#35760;&#24518;&#23646;&#24615;&#21644;&#23613;&#21487;&#33021;&#20445;&#30041;&#26356;&#22810;&#35760;&#24518;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#23558;&#20648;&#22791;&#23618;&#23450;&#20041;&#20026;&#38750;&#32447;&#24615;&#21644;&#32447;&#24615;&#30340;&#20984;&#32452;&#21512;&#65292;&#25552;&#20379;&#20102;&#32593;&#32476;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#30340;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#65288;ESNs&#65289;&#26159;&#22522;&#20110;&#22238;&#22768;&#29366;&#24577;&#23646;&#24615;&#65288;ESP&#65289;&#21407;&#29702;&#24037;&#20316;&#30340;&#26102;&#38388;&#24207;&#21015;&#22788;&#29702;&#27169;&#22411;&#12290;ESP&#26159;&#25351;&#23545;&#36755;&#20837;&#30340;&#35760;&#24518;&#22312;&#28176;&#36827;&#34928;&#20943;&#12290;&#28982;&#32780;&#65292;ESNs&#30340;&#22266;&#26377;&#26550;&#26500;&#20559;&#24046;&#21487;&#33021;&#23548;&#33268;&#36807;&#22810;&#20449;&#24687;&#30340;&#20002;&#22833;&#65292;&#36827;&#32780;&#24433;&#21709;&#22312;&#26576;&#20123;&#38271;&#26399;&#35760;&#24518;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#23558;&#28176;&#36827;&#35760;&#24518;&#23646;&#24615;&#19982;&#23613;&#21487;&#33021;&#20445;&#30041;&#26356;&#22810;&#35760;&#24518;&#30340;&#33021;&#21147;&#32467;&#21512;&#36215;&#26469;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;ESN&#26550;&#26500;&#65292;&#31216;&#20026;&#36793;&#32536;&#31283;&#23450;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#65288;ES$^2$N&#65289;&#12290;&#24341;&#20837;&#30340;ES$^2$N&#27169;&#22411;&#22522;&#20110;&#23558;&#20648;&#22791;&#23618;&#23450;&#20041;&#20026;&#38750;&#32447;&#24615;&#20648;&#22791;&#65288;&#19982;&#26631;&#20934;ESN&#30456;&#21516;&#65289;&#21644;&#23454;&#29616;&#27491;&#20132;&#21464;&#25442;&#30340;&#32447;&#24615;&#20648;&#22791;&#30340;&#20984;&#32452;&#21512;&#12290;&#25105;&#20204;&#23545;&#24341;&#20837;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#25968;&#23398;&#20998;&#26512;&#65292;&#24182;&#35777;&#26126;&#20102;ES$^2$N&#30340;&#38597;&#21487;&#27604;&#30697;&#38453;&#30340;&#25972;&#20010;&#29305;&#24449;&#35889;...
&lt;/p&gt;
&lt;p&gt;
Echo State Networks (ESNs) are time-series processing models working under the Echo State Property (ESP) principle. The ESP is a notion of stability that imposes an asymptotic fading of the memory of the input. On the other hand, the resulting inherent architectural bias of ESNs may lead to an excessive loss of information, which in turn harms the performance in certain tasks with long short-term memory requirements. With the goal of bringing together the fading memory property and the ability to retain as much memory as possible, in this paper we introduce a new ESN architecture, called the Edge of Stability Echo State Network (ES$^2$N). The introduced ES$^2$N model is based on defining the reservoir layer as a convex combination of a nonlinear reservoir (as in the standard ESN), and a linear reservoir that implements an orthogonal transformation. We provide a thorough mathematical analysis of the introduced model, proving that the whole eigenspectrum of the Jacobian of the ES$^2$N ma
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#20855;&#26377;&#24179;&#28369;&#28608;&#27963;&#20989;&#25968;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#23384;&#20648;&#23481;&#37327;&#19979;&#30028;&#20026;md/2&#65292;&#24182;&#19988;&#20934;&#30830;&#24615;&#22823;&#33268;&#20026;2&#20493;&#12290;&#20998;&#26512;&#23384;&#20648;&#23481;&#37327;&#30340;&#26041;&#27861;&#21253;&#25324;&#35745;&#31639;&#32593;&#32476;&#38597;&#21487;&#27604;&#30697;&#38453;&#30340;&#31209;&#65292;&#24182;&#25193;&#23637;&#20102;&#26377;&#20851;Hadamard&#24130;&#31209;&#30340;&#32463;&#20856;&#32447;&#24615;&#20195;&#25968;&#20107;&#23454;&#12290;</title><link>http://arxiv.org/abs/2308.02001</link><description>&lt;p&gt;
&#20855;&#26377;&#24179;&#28369;&#28608;&#27963;&#20989;&#25968;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#23384;&#20648;&#23481;&#37327;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Memory capacity of two layer neural networks with smooth activations. (arXiv:2308.02001v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02001
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#20855;&#26377;&#24179;&#28369;&#28608;&#27963;&#20989;&#25968;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#23384;&#20648;&#23481;&#37327;&#19979;&#30028;&#20026;md/2&#65292;&#24182;&#19988;&#20934;&#30830;&#24615;&#22823;&#33268;&#20026;2&#20493;&#12290;&#20998;&#26512;&#23384;&#20648;&#23481;&#37327;&#30340;&#26041;&#27861;&#21253;&#25324;&#35745;&#31639;&#32593;&#32476;&#38597;&#21487;&#27604;&#30697;&#38453;&#30340;&#31209;&#65292;&#24182;&#25193;&#23637;&#20102;&#26377;&#20851;Hadamard&#24130;&#31209;&#30340;&#32463;&#20856;&#32447;&#24615;&#20195;&#25968;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#23450;&#20855;&#26377;m&#20010;&#38544;&#34255;&#31070;&#32463;&#20803;&#21644;&#36755;&#20837;&#32500;&#25968;d&#65288;&#21363;md+m&#20010;&#35757;&#32451;&#21442;&#25968;&#65289;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#23384;&#20648;&#23481;&#37327;&#65292;&#21363;&#32593;&#32476;&#33021;&#22815;&#35760;&#24518;&#30340;&#19968;&#33324;&#25968;&#25454;&#30340;&#26368;&#22823;&#23610;&#23544;&#65292;&#26159;&#19968;&#20010;&#22522;&#26412;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#12290;&#23545;&#20110;&#38750;&#22810;&#39033;&#24335;&#23454;&#35299;&#26512;&#28608;&#27963;&#20989;&#25968;&#65292;&#22914;sigmoid&#21644;&#24179;&#28369;&#30340;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;&#24179;&#28369;ReLU&#65289;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;md/2&#30340;&#19979;&#30028;&#65292;&#24182;&#19988;&#20934;&#30830;&#24615;&#22823;&#32422;&#20026;2&#20493;&#12290;&#31867;&#20284;&#30340;&#20808;&#21069;&#32467;&#26524;&#20165;&#38480;&#20110;&#38454;&#36291;&#20989;&#25968;&#21644;ReLU&#28608;&#27963;&#20989;&#25968;&#65292;&#23545;&#20110;&#24179;&#28369;&#28608;&#27963;&#20989;&#25968;&#30340;&#32467;&#26524;&#21463;&#21040;&#23545;&#25968;&#22240;&#23376;&#21644;&#38543;&#26426;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#20998;&#26512;&#23384;&#20648;&#23481;&#37327;&#65292;&#25105;&#20204;&#36890;&#36807;&#35745;&#31639;&#28041;&#21450;Hadamard&#24130;&#21644;Khati-Rao&#31215;&#30340;&#30697;&#38453;&#30340;&#31209;&#26469;&#32771;&#23519;&#32593;&#32476;&#38597;&#21487;&#27604;&#30697;&#38453;&#30340;&#31209;&#12290;&#25105;&#20204;&#30340;&#35745;&#31639;&#25193;&#23637;&#20102;&#20851;&#20110;Hadamard&#24130;&#31209;&#30340;&#32463;&#20856;&#32447;&#24615;&#20195;&#25968;&#20107;&#23454;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20043;&#21069;&#20851;&#20110;&#23384;&#20648;&#23481;&#37327;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#24182;&#26377;&#24076;&#26395;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Determining the memory capacity of two-layer neural networks with m hidden neurons and input dimension d (i.e., md+m total trainable parameters), which refers to the largest size of general data the network can memorize, is a fundamental machine-learning question. For non-polynomial real analytic activation functions, such as sigmoids and smoothed rectified linear units (smoothed ReLUs), we establish a lower bound of md/2 and optimality up to a factor of approximately 2. Analogous prior results were limited to Heaviside and ReLU activations, with results for smooth activations suffering from logarithmic factors and requiring random data. To analyze the memory capacity, we examine the rank of the network's Jacobian by computing the rank of matrices involving both Hadamard powers and the Khati-Rao product. Our computation extends classical linear algebraic facts about the rank of Hadamard powers. Overall, our approach differs from previous works on memory capacity and holds promise for e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Floss&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39057;&#22495;&#19978;&#23545;&#23398;&#21040;&#30340;&#34920;&#31034;&#36827;&#34892;&#27491;&#21017;&#21270;&#26469;&#22686;&#24378;&#21608;&#26399;&#24615;&#26102;&#38388;&#24207;&#21015;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;Floss&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#21608;&#26399;&#24615;&#24182;&#23398;&#20064;&#20855;&#26377;&#21608;&#26399;&#19968;&#33268;&#24615;&#30340;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.01011</link><description>&lt;p&gt;
&#20351;&#29992;Floss&#22686;&#24378;&#21608;&#26399;&#24615;&#26102;&#38388;&#24207;&#21015;&#30340;&#34920;&#31034;&#23398;&#20064;&#65306;&#19968;&#31181;&#39057;&#22495;&#27491;&#21017;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Representation Learning for Periodic Time Series with Floss: A Frequency Domain Regularization Approach. (arXiv:2308.01011v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Floss&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39057;&#22495;&#19978;&#23545;&#23398;&#21040;&#30340;&#34920;&#31034;&#36827;&#34892;&#27491;&#21017;&#21270;&#26469;&#22686;&#24378;&#21608;&#26399;&#24615;&#26102;&#38388;&#24207;&#21015;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;Floss&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#21608;&#26399;&#24615;&#24182;&#23398;&#20064;&#20855;&#26377;&#21608;&#26399;&#19968;&#33268;&#24615;&#30340;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26159;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#30340;&#22522;&#30784;&#20219;&#21153;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#23637;&#29616;&#20986;&#37325;&#35201;&#30340;&#21608;&#26399;&#24615;&#25110;&#20934;&#21608;&#26399;&#24615;&#21160;&#24577;&#65292;&#36825;&#20123;&#21160;&#24577;&#24448;&#24448;&#19981;&#33021;&#34987;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#20805;&#20998;&#25429;&#25417;&#21040;&#12290;&#36825;&#23548;&#33268;&#23545;&#24863;&#20852;&#36259;&#30340;&#22522;&#30784;&#21160;&#24577;&#34892;&#20026;&#30340;&#34920;&#31034;&#19981;&#23436;&#25972;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#21483;&#20570;Floss&#65292;&#23427;&#36890;&#36807;&#33258;&#21160;&#21270;&#22320;&#22312;&#39057;&#22495;&#19978;&#35843;&#25972;&#23398;&#21040;&#30340;&#34920;&#31034;&#26469;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;Floss&#26041;&#27861;&#39318;&#20808;&#33258;&#21160;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#20027;&#35201;&#21608;&#26399;&#24615;&#12290;&#28982;&#21518;&#65292;&#23427;&#21033;&#29992;&#21608;&#26399;&#31227;&#20301;&#21644;&#35889;&#23494;&#24230;&#30456;&#20284;&#24230;&#24230;&#37327;&#26469;&#23398;&#20064;&#20855;&#26377;&#21608;&#26399;&#19968;&#33268;&#24615;&#30340;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;Floss&#21487;&#20197;&#36731;&#26494;&#22320;&#25972;&#21512;&#21040;&#26377;&#30417;&#30563;&#12289;&#21322;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;&#23398;&#20064;&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series analysis is a fundamental task in various application domains, and deep learning approaches have demonstrated remarkable performance in this area. However, many real-world time series data exhibit significant periodic or quasi-periodic dynamics that are often not adequately captured by existing deep learning-based solutions. This results in an incomplete representation of the underlying dynamic behaviors of interest. To address this gap, we propose an unsupervised method called Floss that automatically regularizes learned representations in the frequency domain. The Floss method first automatically detects major periodicities from the time series. It then employs periodic shift and spectral density similarity measures to learn meaningful representations with periodic consistency. In addition, Floss can be easily incorporated into both supervised, semi-supervised, and unsupervised learning frameworks. We conduct extensive experiments on common time series classification, for
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23454;&#29616;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#20174;&#30417;&#25511;&#35270;&#39057;&#36755;&#20837;&#36827;&#34892;&#29359;&#32618;&#29616;&#22330;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#22312;&#22810;&#20010;Jetson&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#21644;&#36816;&#34892;&#12290;&#36825;&#26159;&#23545;Jetson&#24179;&#21488;&#22312;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#25191;&#34892;&#26041;&#38754;&#24615;&#33021;&#30340;&#22522;&#20934;&#27979;&#35797;&#20998;&#26512;&#30340;&#21019;&#26032;&#12290;</title><link>http://arxiv.org/abs/2307.16834</link><description>&lt;p&gt;
&#20351;&#29992;&#31471;&#21040;&#31471;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#26469;&#22522;&#20934;&#27979;&#35797;Jetson&#36793;&#32536;&#35774;&#22791;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Jetson Edge Devices with an End-to-end Video-based Anomaly Detection System. (arXiv:2307.16834v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16834
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23454;&#29616;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#20174;&#30417;&#25511;&#35270;&#39057;&#36755;&#20837;&#36827;&#34892;&#29359;&#32618;&#29616;&#22330;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#22312;&#22810;&#20010;Jetson&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#21644;&#36816;&#34892;&#12290;&#36825;&#26159;&#23545;Jetson&#24179;&#21488;&#22312;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#25191;&#34892;&#26041;&#38754;&#24615;&#33021;&#30340;&#22522;&#20934;&#27979;&#35797;&#20998;&#26512;&#30340;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#26032;&#30340;&#23884;&#20837;&#24335;&#31995;&#32479;&#24179;&#21488;&#65292;&#29305;&#21035;&#26159;&#30828;&#20214;&#21152;&#36895;&#65292;&#26174;&#30528;&#24433;&#21709;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#36825;&#20123;&#21019;&#26032;&#23558;&#20154;&#31867;&#21171;&#21160;&#36716;&#21270;&#20026;&#33258;&#21160;&#21270;&#30340;&#26234;&#33021;&#31995;&#32479;&#65292;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#12289;&#26426;&#22120;&#20154;&#25216;&#26415;&#12289;&#29289;&#32852;&#32593;&#21644;&#35768;&#22810;&#20854;&#20182;&#26377;&#37325;&#22823;&#24433;&#21709;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;NVIDIA&#30340;Jetson&#24179;&#21488;&#26159;&#22312;&#25191;&#34892;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#26041;&#38754;&#33021;&#22815;&#25552;&#20379;&#33021;&#25928;&#21644;&#21534;&#21520;&#29575;&#26368;&#20339;&#24615;&#33021;&#30340;&#20808;&#39537;&#20043;&#19968;&#12290;&#20808;&#21069;&#30340;&#22823;&#37096;&#20998;&#22522;&#20934;&#27979;&#35797;&#20998;&#26512;&#37117;&#26159;&#22522;&#20110;2D&#22270;&#20687;&#65292;&#24182;&#20351;&#29992;&#21333;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#27599;&#20010;&#27604;&#36739;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#31181;&#20174;&#30417;&#25511;&#35270;&#39057;&#36755;&#20837;&#30340;&#31471;&#21040;&#31471;&#22522;&#20110;&#35270;&#39057;&#30340;&#29359;&#32618;&#29616;&#22330;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#24182;&#23558;&#35813;&#31995;&#32479;&#37096;&#32626;&#22312;&#22810;&#20010;Jetson&#36793;&#32536;&#35774;&#22791;&#19978;&#65288;Nano&#12289;AGX Xavier&#12289;Orin Nano&#65289;&#12290;&#27604;&#36739;&#20998;&#26512;&#21253;&#25324;&#23558;Torch-TensorRT&#38598;&#25104;&#20026;&#36719;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Innovative enhancement in embedded system platforms, specifically hardware accelerations, significantly influence the application of deep learning in real-world scenarios. These innovations translate human labor efforts into automated intelligent systems employed in various areas such as autonomous driving, robotics, Internet-of-Things (IoT), and numerous other impactful applications. NVIDIA's Jetson platform is one of the pioneers in offering optimal performance regarding energy efficiency and throughput in the execution of deep learning algorithms. Previously, most benchmarking analysis was based on 2D images with a single deep learning model for each comparison result. In this paper, we implement an end-to-end video-based crime-scene anomaly detection system inputting from surveillance videos and the system is deployed and completely operates on multiple Jetson edge devices (Nano, AGX Xavier, Orin Nano). The comparison analysis includes the integration of Torch-TensorRT as a softwar
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;16&#20301;&#35745;&#31639;&#20013;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Adam&#20248;&#21270;&#22120;&#30340;&#26032;&#26041;&#27861;&#26469;&#25552;&#39640;16&#20301;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#36807;&#31243;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.16189</link><description>&lt;p&gt;
&#29992;&#20110;16&#20301;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#30340;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Efficient Approach to Mitigate Numerical Instability in Backpropagation for 16-bit Neural Network Training. (arXiv:2307.16189v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16189
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;16&#20301;&#35745;&#31639;&#20013;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Adam&#20248;&#21270;&#22120;&#30340;&#26032;&#26041;&#27861;&#26469;&#25552;&#39640;16&#20301;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#36807;&#31243;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22312;16&#20301;&#35745;&#31639;&#20013;&#20351;&#29992;&#27969;&#34892;&#30340;&#20248;&#21270;&#31639;&#27861;&#65288;&#22914;RMSProp&#21644;Adam&#65289;&#26102;&#35266;&#23519;&#21040;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#30340;&#22797;&#26434;&#24615;&#12290;&#36825;&#31181;&#19981;&#31283;&#23450;&#24615;&#36890;&#24120;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#38454;&#27573;&#20013;&#20986;&#29616;&#65292;&#23548;&#33268;&#23398;&#20064;&#36807;&#31243;&#21463;&#21040;&#24178;&#25200;&#65292;&#20174;&#32780;&#22952;&#30861;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#26377;&#25928;&#37096;&#32626;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#21333;&#19968;&#36229;&#21442;&#25968;epsilon&#26159;&#36825;&#31181;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#23545;16&#20301;&#35745;&#31639;&#20013;&#36825;&#20123;&#20248;&#21270;&#22120;&#20013;epsilon&#30340;&#20316;&#29992;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#32034;&#65292;&#21457;&#29616;&#24494;&#35843;&#20854;&#20540;&#21487;&#20197;&#24674;&#22797;RMSProp&#21644;Adam&#30340;&#21151;&#33021;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#25928;&#21033;&#29992;16&#20301;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#34987;&#21457;&#29616;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;Adam&#20248;&#21270;&#22120;&#30340;&#26356;&#26032;&#65292;&#24182;&#26174;&#33879;&#25913;&#21892;&#20102;16&#20301;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#36807;&#31243;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this research, we delve into the intricacies of the numerical instability observed in 16-bit computations of machine learning models, particularly when employing popular optimization algorithms such as RMSProp and Adam. This instability is commonly experienced during the training phase of deep neural networks, leading to disrupted learning processes and hindering the effective deployment of such models. We identify the single hyperparameter, epsilon, as the main culprit behind this numerical instability. An in-depth exploration of the role of epsilon in these optimizers within 16-bit computations reveals that a minor adjustment of its value can restore the functionality of RMSProp and Adam, consequently enabling the effective utilization of 16-bit neural networks. We propose a novel method to mitigate the identified numerical instability issues. This method capitalizes on the updates from the Adam optimizer and significantly improves the robustness of the learning process in 16-bit 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#26102;&#24207;&#24046;&#20998;&#31639;&#27861;&#65292;&#21482;&#20351;&#29992;&#19968;&#20010;&#27493;&#38271;&#21442;&#25968;&#65292;&#24182;&#35777;&#26126;&#25910;&#25947;&#36895;&#24230;&#33267;&#23569;&#20026;$O(1/t)$&#12290;</title><link>http://arxiv.org/abs/2307.15892</link><description>&lt;p&gt;
&#20165;&#20351;&#29992;&#19968;&#20010;&#27493;&#38271;&#30340;&#26032;&#22411;&#26799;&#24230;&#26102;&#24207;&#24046;&#20998;&#31639;&#27861;&#65306;&#36890;&#36807;$L$-$\lambda$&#24179;&#28369;&#24615;&#36827;&#34892;&#25910;&#25947;&#36895;&#29575;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A new Gradient TD Algorithm with only One Step-size: Convergence Rate Analysis using $L$-$\lambda$ Smoothness. (arXiv:2307.15892v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#26102;&#24207;&#24046;&#20998;&#31639;&#27861;&#65292;&#21482;&#20351;&#29992;&#19968;&#20010;&#27493;&#38271;&#21442;&#25968;&#65292;&#24182;&#35777;&#26126;&#25910;&#25947;&#36895;&#24230;&#33267;&#23569;&#20026;$O(1/t)$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#26102;&#24207;&#24046;&#20998;&#65288;GTD&#65289;&#31639;&#27861;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#25910;&#25947;&#20445;&#35777;&#30340;&#31163;&#31574;&#30053;&#23398;&#20064;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#31639;&#27861;&#65292;&#20854;&#22797;&#26434;&#24230;&#20026;$O(d)$&#65288;$d$&#26159;&#29305;&#24449;&#25968;&#37327;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Impression GTD&#30340;&#20840;&#26032;&#21333;&#26102;&#38388;&#23610;&#24230;GTD&#31639;&#27861;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;&#26399;&#26395;td&#26356;&#26032;&#65288;NEU&#65289;&#30446;&#26631;&#65292;&#24182;&#21482;&#26377;&#19968;&#20010;&#27493;&#38271;&#21442;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#31181;&#26032;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#33267;&#23569;&#19982;$O(1/t)$&#19968;&#26679;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient Temporal Difference (GTD) algorithms (Sutton et al., 2008, 2009) are the first $O(d)$ ($d$ is the number features) algorithms that have convergence guarantees for off-policy learning with linear function approximation. Liu et al. (2015) and Dalal et. al. (2018) proved the convergence rates of GTD, GTD2 and TDC are $O(t^{-\alpha/2})$ for some $\alpha \in (0,1)$. This bound is tight (Dalal et al., 2020), and slower than $O(1/\sqrt{t})$. GTD algorithms also have two step-size parameters, which are difficult to tune. In literature, there is a "single-time-scale" formulation of GTD. However, this formulation still has two step-size parameters.  This paper presents a truly single-time-scale GTD algorithm for minimizing the Norm of Expected td Update (NEU) objective, and it has only one step-size parameter. We prove that the new algorithm, called Impression GTD, converges at least as fast as $O(1/t)$. Furthermore, based on a generalization of the expected smoothness (Gower et al. 201
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#25506;&#35752;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#24180;&#40836;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;XAI&#26041;&#27861;&#22312;&#21307;&#30103;&#24212;&#29992;&#21644;&#24180;&#40836;&#39044;&#27979;&#39046;&#22495;&#30340;&#30410;&#22788;&#12290;</title><link>http://arxiv.org/abs/2307.13704</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#24180;&#40836;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
eXplainable Artificial Intelligence (XAI) in age prediction: A systematic review. (arXiv:2307.13704v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#25506;&#35752;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#24180;&#40836;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;XAI&#26041;&#27861;&#22312;&#21307;&#30103;&#24212;&#29992;&#21644;&#24180;&#40836;&#39044;&#27979;&#39046;&#22495;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#29616;&#22312;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#33021;&#22815;&#35299;&#37322;&#22797;&#26434;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;XAI&#29305;&#21035;&#36866;&#29992;&#20110;&#21361;&#38505;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#20154;&#31867;&#30340;&#29983;&#21629;&#20381;&#36182;&#20110;AI&#31995;&#32479;&#30340;&#20915;&#31574;&#12290;&#21307;&#30103;&#30740;&#31350;&#30340;&#19968;&#20010;&#39046;&#22495;&#26159;&#24180;&#40836;&#39044;&#27979;&#21644;&#34928;&#32769;&#21450;&#19982;&#24180;&#40836;&#30456;&#20851;&#30142;&#30149;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#37492;&#23450;&#12290;&#28982;&#32780;&#65292;&#22312;&#24180;&#40836;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;XAI&#30340;&#20316;&#29992;&#23578;&#26410;&#30452;&#25509;&#25506;&#35752;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;XAI&#26041;&#27861;&#22312;&#24180;&#40836;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#22120;&#23448;&#31995;&#32479;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#24182;&#35752;&#35770;&#20102;XAI&#22312;&#21307;&#30103;&#24212;&#29992;&#20197;&#21450;&#29305;&#21035;&#26159;&#24180;&#40836;&#39044;&#27979;&#39046;&#22495;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
eXplainable Artificial Intelligence (XAI) is now an important and essential part of machine learning, allowing to explain the predictions of complex models. XAI is especially required in risky applications, particularly in health care, where human lives depend on the decisions of AI systems. One area of medical research is age prediction and identification of biomarkers of aging and age-related diseases. However, the role of XAI in the age prediction task has not previously been explored directly. In this review, we discuss the application of XAI approaches to age prediction tasks. We give a systematic review of the works organized by body systems, and discuss the benefits of XAI in medical applications and, in particular, in the age prediction domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#36719;&#27880;&#24847;&#21147;&#12289;&#30828;&#27880;&#24847;&#21147;&#21644;&#28508;&#21464;&#37327;&#36793;&#38469;&#20284;&#28982;&#65288;LVML&#65289;&#27880;&#24847;&#21147;&#19977;&#31181;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#25152;&#36873;&#25321;&#30340;&#29255;&#27573;&#32858;&#21512;&#26041;&#24335;&#19978;&#30340;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#35299;&#37322;&#20102;&#20998;&#31867;&#27169;&#22411;&#22312;&#26799;&#24230;&#19979;&#38477;&#19979;&#30340;&#28436;&#21270;&#23545;&#26368;&#32456;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.13421</link><description>&lt;p&gt;
&#20851;&#20110;&#27880;&#24847;&#21147;&#32593;&#32476;&#23398;&#20064;&#21160;&#24577;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the learning Dynamics of Attention Networks. (arXiv:2307.13421v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#36719;&#27880;&#24847;&#21147;&#12289;&#30828;&#27880;&#24847;&#21147;&#21644;&#28508;&#21464;&#37327;&#36793;&#38469;&#20284;&#28982;&#65288;LVML&#65289;&#27880;&#24847;&#21147;&#19977;&#31181;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#25152;&#36873;&#25321;&#30340;&#29255;&#27573;&#32858;&#21512;&#26041;&#24335;&#19978;&#30340;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#35299;&#37322;&#20102;&#20998;&#31867;&#27169;&#22411;&#22312;&#26799;&#24230;&#19979;&#38477;&#19979;&#30340;&#28436;&#21270;&#23545;&#26368;&#32456;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#27169;&#22411;&#36890;&#24120;&#36890;&#36807;&#20248;&#21270;&#19977;&#20010;&#26631;&#20934;&#25439;&#22833;&#20989;&#25968;&#20043;&#19968;&#26469;&#23398;&#20064;&#65292;&#20998;&#21035;&#31216;&#20026;&#36719;&#27880;&#24847;&#21147;&#12289;&#30828;&#27880;&#24847;&#21147;&#21644;&#28508;&#21464;&#37327;&#36793;&#38469;&#20284;&#28982;&#65288;LVML&#65289;&#27880;&#24847;&#21147;&#12290;&#36825;&#19977;&#31181;&#33539;&#24335;&#37117;&#26159;&#20026;&#20102;&#36798;&#21040;&#30456;&#21516;&#30340;&#30446;&#26631;&#65292;&#21363;&#25214;&#21040;&#20004;&#20010;&#27169;&#22411;&#65306;&#19968;&#20010;&#8220;&#28966;&#28857;&#8221;&#27169;&#22411;&#65292;&#29992;&#20110;&#8220;&#36873;&#25321;&#8221;&#36755;&#20837;&#20013;&#30340;&#27491;&#30830;&#8220;&#29255;&#27573;&#8221;&#65292;&#21644;&#19968;&#20010;&#8220;&#20998;&#31867;&#8221;&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#36873;&#23450;&#30340;&#29255;&#27573;&#22788;&#29702;&#25104;&#30446;&#26631;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25152;&#36873;&#25321;&#30340;&#29255;&#27573;&#32858;&#21512;&#26041;&#24335;&#19978;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#23548;&#33268;&#20102;&#19981;&#21516;&#30340;&#21160;&#24577;&#21644;&#26368;&#32456;&#32467;&#26524;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20351;&#29992;&#36825;&#20123;&#33539;&#24335;&#23398;&#20064;&#30340;&#27169;&#22411;&#20855;&#26377;&#29420;&#29305;&#30340;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#35299;&#37322;&#20026;&#22312;&#28966;&#28857;&#27169;&#22411;&#22266;&#23450;&#26102;&#65292;&#20998;&#31867;&#27169;&#22411;&#22312;&#26799;&#24230;&#19979;&#38477;&#19979;&#30340;&#28436;&#21270;&#25152;&#33268;&#12290;&#25105;&#20204;&#36824;&#22312;&#19968;&#20010;&#31616;&#21333;&#30340;&#35774;&#32622;&#20013;&#20998;&#26512;&#20102;&#36825;&#20123;&#33539;&#24335;&#65292;&#24182;&#25512;&#23548;&#20986;&#26799;&#24230;&#27969;&#19979;&#21442;&#25968;&#36712;&#36857;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#12290;&#22312;&#36719;&#27880;&#24847;&#21147;&#25439;&#22833;&#19979;&#65292;&#28966;&#28857;&#27169;&#22411;&#22312;&#21021;&#22987;&#21270;&#38454;&#27573;&#24555;&#36895;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention models are typically learned by optimizing one of three standard loss functions that are variously called -- soft attention, hard attention, and latent variable marginal likelihood (LVML) attention. All three paradigms are motivated by the same goal of finding two models -- a `focus' model that `selects' the right \textit{segment} of the input and a `classification' model that processes the selected segment into the target label. However, they differ significantly in the way the selected segments are aggregated, resulting in distinct dynamics and final results. We observe a unique signature of models learned using these paradigms and explain this as a consequence of the evolution of the classification model under gradient descent when the focus model is fixed. We also analyze these paradigms in a simple setting and derive closed-form expressions for the parameter trajectory under gradient flow. With the soft attention loss, the focus model improves quickly at initialization a
&lt;/p&gt;</description></item><item><title>AutoAlign&#26159;&#19968;&#31181;&#20840;&#33258;&#21160;&#30340;&#30693;&#35782;&#22270;&#35889;&#23545;&#40784;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#25163;&#24037;&#21046;&#20316;&#30340;&#31181;&#23376;&#23545;&#40784;&#12290;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#25429;&#25417;&#35859;&#35789;&#30456;&#20284;&#24615;&#65292;&#24182;&#20351;&#29992;TransE&#35745;&#31639;&#23454;&#20307;&#23884;&#20837;&#26469;&#23454;&#29616;&#23454;&#20307;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2307.11772</link><description>&lt;p&gt;
AutoAlign&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#33258;&#21160;&#26377;&#25928;&#30693;&#35782;&#22270;&#35889;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AutoAlign: Fully Automatic and Effective Knowledge Graph Alignment enabled by Large Language Models. (arXiv:2307.11772v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11772
&lt;/p&gt;
&lt;p&gt;
AutoAlign&#26159;&#19968;&#31181;&#20840;&#33258;&#21160;&#30340;&#30693;&#35782;&#22270;&#35889;&#23545;&#40784;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#25163;&#24037;&#21046;&#20316;&#30340;&#31181;&#23376;&#23545;&#40784;&#12290;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#25429;&#25417;&#35859;&#35789;&#30456;&#20284;&#24615;&#65292;&#24182;&#20351;&#29992;TransE&#35745;&#31639;&#23454;&#20307;&#23884;&#20837;&#26469;&#23454;&#29616;&#23454;&#20307;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#38388;&#30340;&#23454;&#20307;&#23545;&#40784;&#20219;&#21153;&#26088;&#22312;&#35782;&#21035;&#20986;&#20004;&#20010;&#19981;&#21516;&#30693;&#35782;&#22270;&#35889;&#20013;&#34920;&#31034;&#30456;&#21516;&#23454;&#20307;&#30340;&#27599;&#23545;&#23454;&#20307;&#12290;&#35768;&#22810;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#29992;&#20110;&#36825;&#20010;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#37117;&#38656;&#35201;&#25163;&#24037;&#21046;&#20316;&#30340;&#31181;&#23376;&#23545;&#40784;&#65292;&#36825;&#26159;&#38750;&#24120;&#26114;&#36149;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21517;&#20026;AutoAlign&#30340;&#23436;&#20840;&#33258;&#21160;&#23545;&#40784;&#26041;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#20219;&#20309;&#25163;&#24037;&#21046;&#20316;&#30340;&#31181;&#23376;&#23545;&#40784;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#35859;&#35789;&#23884;&#20837;&#65292;AutoAlign&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#35859;&#35789;&#36817;&#37051;&#22270;&#65292;&#33258;&#21160;&#25429;&#25417;&#20004;&#20010;&#30693;&#35782;&#22270;&#35889;&#20013;&#35859;&#35789;&#30340;&#30456;&#20284;&#24615;&#12290;&#23545;&#20110;&#23454;&#20307;&#23884;&#20837;&#65292;AutoAlign&#39318;&#20808;&#20351;&#29992;TransE&#29420;&#31435;&#35745;&#31639;&#27599;&#20010;&#30693;&#35782;&#22270;&#35889;&#30340;&#23454;&#20307;&#23884;&#20837;&#65292;&#28982;&#21518;&#36890;&#36807;&#35745;&#31639;&#22522;&#20110;&#23454;&#20307;&#23646;&#24615;&#30340;&#23454;&#20307;&#30456;&#20284;&#24615;&#65292;&#23558;&#20004;&#20010;&#30693;&#35782;&#22270;&#35889;&#30340;&#23454;&#20307;&#23884;&#20837;&#31227;&#21160;&#21040;&#30456;&#21516;&#30340;&#21521;&#37327;&#31354;&#38388;&#20013;&#12290;&#22240;&#27492;&#65292;AutoAlign&#23454;&#29616;&#20102;&#35859;&#35789;&#23545;&#40784;&#21644;&#23454;&#20307;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of entity alignment between knowledge graphs (KGs) aims to identify every pair of entities from two different KGs that represent the same entity. Many machine learning-based methods have been proposed for this task. However, to our best knowledge, existing methods all require manually crafted seed alignments, which are expensive to obtain. In this paper, we propose the first fully automatic alignment method named AutoAlign, which does not require any manually crafted seed alignments. Specifically, for predicate embeddings, AutoAlign constructs a predicate-proximity-graph with the help of large language models to automatically capture the similarity between predicates across two KGs. For entity embeddings, AutoAlign first computes the entity embeddings of each KG independently using TransE, and then shifts the two KGs' entity embeddings into the same vector space by computing the similarity between entities based on their attributes. Thus, both predicate alignment and entity al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#24179;&#28369;&#38750;&#20984;&#20248;&#21270;&#20013;&#38543;&#26426;&#27425;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#21333;&#26102;&#38388;&#23610;&#24230;&#21644;&#21452;&#26102;&#38388;&#23610;&#24230;&#24773;&#20917;&#19979;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#21253;&#25324;&#20102;&#22810;&#31181;&#24050;&#30693;&#30340;SGD&#31867;&#22411;&#26041;&#27861;&#12290;&#23545;&#20110;&#26377;&#38480;&#21644;&#24418;&#24335;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#27493;&#38271;&#21644;&#21021;&#22987;&#28857;&#19978;&#25214;&#21040;Clarke&#31283;&#23450;&#28857;&#12290;</title><link>http://arxiv.org/abs/2307.10053</link><description>&lt;p&gt;
&#38750;&#24179;&#28369;&#38750;&#20984;&#20248;&#21270;&#20013;&#38543;&#26426;&#27425;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Convergence Guarantees for Stochastic Subgradient Methods in Nonsmooth Nonconvex Optimization. (arXiv:2307.10053v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#24179;&#28369;&#38750;&#20984;&#20248;&#21270;&#20013;&#38543;&#26426;&#27425;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#21333;&#26102;&#38388;&#23610;&#24230;&#21644;&#21452;&#26102;&#38388;&#23610;&#24230;&#24773;&#20917;&#19979;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#21253;&#25324;&#20102;&#22810;&#31181;&#24050;&#30693;&#30340;SGD&#31867;&#22411;&#26041;&#27861;&#12290;&#23545;&#20110;&#26377;&#38480;&#21644;&#24418;&#24335;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#27493;&#38271;&#21644;&#21021;&#22987;&#28857;&#19978;&#25214;&#21040;Clarke&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26041;&#27861;&#21450;&#20854;&#21464;&#31181;&#22312;&#35757;&#32451;&#30001;&#38750;&#24179;&#28369;&#28608;&#27963;&#20989;&#25968;&#26500;&#24314;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20026;&#26356;&#26032;&#21160;&#37327;&#39033;&#21644;&#21464;&#37327;&#30340;&#27493;&#38271;&#20998;&#37197;&#20102;&#19981;&#21516;&#30340;&#26102;&#38388;&#23610;&#24230;&#12290;&#22312;&#19968;&#20123;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#21333;&#26102;&#38388;&#23610;&#24230;&#21644;&#21452;&#26102;&#38388;&#23610;&#24230;&#24773;&#20917;&#19979;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#21253;&#21547;&#20102;&#24456;&#22810;&#24050;&#30693;&#30340;SGD&#31867;&#22411;&#26041;&#27861;&#65292;&#21253;&#25324;heavy-ball SGD&#12289;SignSGD&#12289;Lion&#12289;normalized SGD&#21644;clipped SGD&#12290;&#27492;&#22806;&#65292;&#24403;&#30446;&#26631;&#20989;&#25968;&#37319;&#29992;&#26377;&#38480;&#21644;&#24418;&#24335;&#26102;&#65292;&#25105;&#20204;&#22522;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#35777;&#26126;&#20102;&#36825;&#20123;SGD&#31867;&#22411;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;SGD&#31867;&#22411;&#26041;&#27861;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#27493;&#38271;&#21644;&#21021;&#22987;&#28857;&#19978;&#33021;&#22815;&#25214;&#21040;&#30446;&#26631;&#20989;&#25968;&#30340;Clarke&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the convergence properties of the stochastic gradient descent (SGD) method and its variants, especially in training neural networks built from nonsmooth activation functions. We develop a novel framework that assigns different timescales to stepsizes for updating the momentum terms and variables, respectively. Under mild conditions, we prove the global convergence of our proposed framework in both single-timescale and two-timescale cases. We show that our proposed framework encompasses a wide range of well-known SGD-type methods, including heavy-ball SGD, SignSGD, Lion, normalized SGD and clipped SGD. Furthermore, when the objective function adopts a finite-sum formulation, we prove the convergence properties for these SGD-type methods based on our proposed framework. In particular, we prove that these SGD-type methods find the Clarke stationary points of the objective function with randomly chosen stepsizes and initial points under mild assumptions. Preli
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23454;&#39564;&#24471;&#20986;&#20102;&#19977;&#20010;&#32467;&#35770;&#65306;&#27809;&#26377;&#36890;&#29992;&#30340;&#20219;&#21153;&#37319;&#26679;&#31574;&#30053;&#33021;&#20445;&#35777;&#20803;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65307;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#20250;&#23548;&#33268;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20986;&#29616;&#27424;&#25311;&#21512;&#25110;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#65307;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#21463;&#21040;&#20219;&#21153;&#30340;&#24046;&#24322;&#12289;&#20219;&#21153;&#29109;&#21644;&#20219;&#21153;&#38590;&#24230;&#30340;&#24433;&#21709;&#12290;&#38024;&#23545;&#36825;&#20123;&#21457;&#29616;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#37319;&#26679;&#22120;ASr&#65292;&#23427;&#21033;&#29992;&#20219;&#21153;&#30340;&#24046;&#24322;&#12289;&#20219;&#21153;&#29109;&#21644;&#20219;&#21153;&#38590;&#24230;&#26469;&#37319;&#26679;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#37325;&#26032;&#24605;&#32771;&#21644;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#26469;&#20248;&#21270;ASr&#12290;&#22823;&#37327;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#20102;ASr&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.08924</link><description>&lt;p&gt;
&#23398;&#20064;&#37319;&#26679;&#20219;&#21153;&#29992;&#20110;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning to Sample Tasks for Meta Learning. (arXiv:2307.08924v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08924
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23454;&#39564;&#24471;&#20986;&#20102;&#19977;&#20010;&#32467;&#35770;&#65306;&#27809;&#26377;&#36890;&#29992;&#30340;&#20219;&#21153;&#37319;&#26679;&#31574;&#30053;&#33021;&#20445;&#35777;&#20803;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65307;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#20250;&#23548;&#33268;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20986;&#29616;&#27424;&#25311;&#21512;&#25110;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#65307;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#21463;&#21040;&#20219;&#21153;&#30340;&#24046;&#24322;&#12289;&#20219;&#21153;&#29109;&#21644;&#20219;&#21153;&#38590;&#24230;&#30340;&#24433;&#21709;&#12290;&#38024;&#23545;&#36825;&#20123;&#21457;&#29616;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#37319;&#26679;&#22120;ASr&#65292;&#23427;&#21033;&#29992;&#20219;&#21153;&#30340;&#24046;&#24322;&#12289;&#20219;&#21153;&#29109;&#21644;&#20219;&#21153;&#38590;&#24230;&#26469;&#37319;&#26679;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#37325;&#26032;&#24605;&#32771;&#21644;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#26469;&#20248;&#21270;ASr&#12290;&#22823;&#37327;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#20102;ASr&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#21508;&#31181;&#20803;&#23398;&#20064;&#26041;&#27861;&#12289;&#20219;&#21153;&#37319;&#26679;&#22120;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#30340;&#23454;&#39564;&#65292;&#26412;&#25991;&#24471;&#20986;&#20102;&#19977;&#20010;&#32467;&#35770;&#12290;&#39318;&#20808;&#65292;&#27809;&#26377;&#36890;&#29992;&#30340;&#20219;&#21153;&#37319;&#26679;&#31574;&#30053;&#33021;&#20445;&#35777;&#20803;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#20250;&#23548;&#33268;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20986;&#29616;&#27424;&#25311;&#21512;&#25110;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#21463;&#21040;&#20219;&#21153;&#30340;&#24046;&#24322;&#12289;&#20219;&#21153;&#29109;&#21644;&#20219;&#21153;&#38590;&#24230;&#30340;&#24433;&#21709;&#12290;&#38024;&#23545;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#37319;&#26679;&#22120;&#65292;&#31216;&#20026;&#33258;&#36866;&#24212;&#37319;&#26679;&#22120;&#65288;ASr&#65289;&#12290;ASr&#26159;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#20219;&#21153;&#37319;&#26679;&#22120;&#65292;&#23427;&#21033;&#29992;&#20219;&#21153;&#30340;&#24046;&#24322;&#12289;&#20219;&#21153;&#29109;&#21644;&#20219;&#21153;&#38590;&#24230;&#26469;&#37319;&#26679;&#20219;&#21153;&#12290;&#20026;&#20102;&#20248;&#21270;ASr&#65292;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#22823;&#37327;&#30340;&#23454;&#35777;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;ASr&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Through experiments on various meta-learning methods, task samplers, and few-shot learning tasks, this paper arrives at three conclusions. Firstly, there are no universal task sampling strategies to guarantee the performance of meta-learning models. Secondly, task diversity can cause the models to either underfit or overfit during training. Lastly, the generalization performance of the models are influenced by task divergence, task entropy, and task difficulty. In response to these findings, we propose a novel task sampler called Adaptive Sampler (ASr). ASr is a plug-and-play task sampler that takes task divergence, task entropy, and task difficulty to sample tasks. To optimize ASr, we rethink and propose a simple and general meta-learning algorithm. Finally, a large number of empirical experiments demonstrate the effectiveness of the proposed ASr.
&lt;/p&gt;</description></item><item><title>&#20803;&#20215;&#20540;&#23398;&#20064;&#26159;&#19968;&#31181;&#24102;&#26377;&#23398;&#20064;&#24847;&#35782;&#30340;&#23398;&#20064;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#26234;&#33021;&#20307;&#23398;&#20064;&#36807;&#31243;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20351;&#29992;&#20803;&#20215;&#20540;&#20989;&#25968;&#26469;&#25351;&#23548;&#20248;&#21270;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36924;&#36817;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#21487;&#38752;&#30340;&#25913;&#36827;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.08863</link><description>&lt;p&gt;
&#20803;&#20215;&#20540;&#23398;&#20064;&#65306;&#19968;&#31181;&#24102;&#26377;&#23398;&#20064;&#24847;&#35782;&#30340;&#23398;&#20064;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Meta-Value Learning: a General Framework for Learning with Learning Awareness. (arXiv:2307.08863v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08863
&lt;/p&gt;
&lt;p&gt;
&#20803;&#20215;&#20540;&#23398;&#20064;&#26159;&#19968;&#31181;&#24102;&#26377;&#23398;&#20064;&#24847;&#35782;&#30340;&#23398;&#20064;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#26234;&#33021;&#20307;&#23398;&#20064;&#36807;&#31243;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20351;&#29992;&#20803;&#20215;&#20540;&#20989;&#25968;&#26469;&#25351;&#23548;&#20248;&#21270;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36924;&#36817;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#21487;&#38752;&#30340;&#25913;&#36827;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#26799;&#24230;&#23398;&#20064;&#24456;&#22256;&#38590;&#65292;&#22240;&#20026;&#26799;&#24230;&#26469;&#33258;&#20110;&#19968;&#20010;&#19968;&#38454;&#27169;&#22411;&#65292;&#19981;&#32771;&#34385;&#26234;&#33021;&#20307;&#23398;&#20064;&#36807;&#31243;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;LOLA&#30340;&#24605;&#24819;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#23436;&#20840;&#36890;&#29992;&#30340;&#22522;&#20110;&#20215;&#20540;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;&#26680;&#24515;&#24605;&#24819;&#26159;&#19968;&#20010;&#31216;&#20026;&#20803;&#20215;&#20540;&#30340;&#20989;&#25968;&#65292;&#23427;&#22312;&#32852;&#21512;&#31574;&#30053;&#31354;&#38388;&#30340;&#27599;&#20010;&#28857;&#19978;&#65292;&#20026;&#27599;&#20010;&#26234;&#33021;&#20307;&#32473;&#20986;&#20854;&#26410;&#26469;&#20248;&#21270;&#27493;&#39588;&#20013;&#30446;&#26631;&#30340;&#25240;&#25187;&#24635;&#21644;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20803;&#20215;&#20540;&#30340;&#26799;&#24230;&#27604;&#21407;&#22987;&#30446;&#26631;&#30340;&#26799;&#24230;&#26356;&#21487;&#38752;&#30340;&#25913;&#36827;&#26041;&#21521;&#65292;&#22240;&#20026;&#20803;&#20215;&#20540;&#26469;&#33258;&#23545;&#20248;&#21270;&#25928;&#26524;&#30340;&#32463;&#39564;&#35266;&#23519;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;&#20803;&#20215;&#20540;&#65292;&#20197;&#27839;&#30528;&#26234;&#33021;&#20307;&#27839;&#30528;&#20803;&#20215;&#20540;&#26799;&#24230;&#30340;&#20248;&#21270;&#36712;&#36857;&#36827;&#34892;TD&#35823;&#24046;&#26368;&#23567;&#21270;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient-based learning in multi-agent systems is difficult because the gradient derives from a first-order model which does not account for the interaction between agents' learning processes. LOLA (arXiv:1709.04326) accounts for this by differentiating through one step of optimization. We extend the ideas of LOLA and develop a fully-general value-based approach to optimization. At the core is a function we call the meta-value, which at each point in joint-policy space gives for each agent a discounted sum of its objective over future optimization steps. We argue that the gradient of the meta-value gives a more reliable improvement direction than the gradient of the original objective, because the meta-value derives from empirical observations of the effects of optimization. We show how the meta-value can be approximated by training a neural network to minimize TD error along optimization trajectories in which agents follow the gradient of the meta-value. We analyze the behavior of our
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#23545;&#25239;&#24615;&#21487;&#36716;&#31227;&#24615;&#30340;&#29702;&#35299;&#65292;&#29305;&#21035;&#20851;&#27880;&#26367;&#20195;&#35757;&#32451;&#12290;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#30340;&#24179;&#28369;&#24615;&#21644;&#26799;&#24230;&#30456;&#20284;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21457;&#29616;&#23545;&#25239;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#26367;&#20195;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#25968;&#25454;&#20998;&#24067;&#30340;&#36716;&#21464;&#25552;&#20986;&#20102;&#26032;&#30340;&#25512;&#27979;&#12290;</title><link>http://arxiv.org/abs/2307.07873</link><description>&lt;p&gt;
&#25506;&#32034;&#20174;&#26367;&#20195;&#35757;&#32451;&#20013;&#29702;&#35299;&#23545;&#25239;&#24615;&#21487;&#36716;&#31227;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding Adversarial Transferability From Surrogate Training. (arXiv:2307.07873v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#23545;&#25239;&#24615;&#21487;&#36716;&#31227;&#24615;&#30340;&#29702;&#35299;&#65292;&#29305;&#21035;&#20851;&#27880;&#26367;&#20195;&#35757;&#32451;&#12290;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#30340;&#24179;&#28369;&#24615;&#21644;&#26799;&#24230;&#30456;&#20284;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21457;&#29616;&#23545;&#25239;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#26367;&#20195;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#25968;&#25454;&#20998;&#24067;&#30340;&#36716;&#21464;&#25552;&#20986;&#20102;&#26032;&#30340;&#25512;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;DNNs&#30340;&#23545;&#25239;&#26679;&#26412;(AEs)&#24050;&#32463;&#34920;&#26126;&#26159;&#21487;&#36716;&#31227;&#30340;&#65306;&#25104;&#21151;&#27450;&#39575;&#30333;&#30418;&#23376;&#26367;&#20195;&#27169;&#22411;&#30340;AEs&#20063;&#21487;&#20197;&#27450;&#39575;&#20855;&#26377;&#19981;&#21516;&#26550;&#26500;&#30340;&#20854;&#20182;&#40657;&#30418;&#27169;&#22411;&#12290;&#34429;&#28982;&#35768;&#22810;&#32463;&#39564;&#30740;&#31350;&#25552;&#20379;&#20102;&#29983;&#25104;&#39640;&#24230;&#21487;&#36716;&#31227;AE&#30340;&#25351;&#23548;&#65292;&#20294;&#36825;&#20123;&#30740;&#31350;&#32570;&#20047;&#35299;&#37322;&#29978;&#33267;&#23548;&#33268;&#19981;&#19968;&#33268;&#30340;&#24314;&#35758;&#12290;&#26412;&#25991;&#22312;&#29702;&#35299;&#23545;&#25239;&#24615;&#21487;&#36716;&#31227;&#24615;&#26041;&#38754;&#36808;&#20986;&#20102;&#19968;&#27493;&#65292;&#29305;&#21035;&#20851;&#27880;&#26367;&#20195;&#26041;&#38754;&#12290;&#20174;&#30528;&#21517;&#30340;&#23567;&#20581;&#22766;&#24615;&#29616;&#35937;&#24320;&#22987;&#65292;&#36890;&#36807;&#20197;&#36731;&#24494;&#25200;&#21160;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#23545;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;&#20854;&#24402;&#22240;&#20110;&#20004;&#20010;&#20027;&#35201;&#22240;&#32032;&#20043;&#38388;&#30340;&#26435;&#34913;&#65306;&#27169;&#22411;&#30340;&#24179;&#28369;&#24615;&#21644;&#26799;&#24230;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#23427;&#20204;&#30340;&#20849;&#21516;&#25928;&#26524;&#19978;&#65292;&#32780;&#19981;&#26159;&#23427;&#20204;&#19982;&#21487;&#36716;&#31227;&#24615;&#30340;&#21333;&#29420;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#25512;&#27979;&#25968;&#25454;&#20998;&#24067;&#30340;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial examples (AEs) for DNNs have been shown to be transferable: AEs that successfully fool white-box surrogate models can also deceive other black-box models with different architectures. Although a bunch of empirical studies have provided guidance on generating highly transferable AEs, many of these findings lack explanations and even lead to inconsistent advice. In this paper, we take a further step towards understanding adversarial transferability, with a particular focus on surrogate aspects. Starting from the intriguing little robustness phenomenon, where models adversarially trained with mildly perturbed adversarial samples can serve as better surrogates, we attribute it to a trade-off between two predominant factors: model smoothness and gradient similarity. Our investigations focus on their joint effects, rather than their separate correlations with transferability. Through a series of theoretical and empirical analyses, we conjecture that the data distribution shift in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#31181;&#28608;&#27963;&#20989;&#25968;&#19979;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#21487;&#20197;&#36890;&#36807;&#22312;&#26377;&#30028;&#38598;&#21512;&#19978;&#26500;&#24314;&#19968;&#20010;&#23485;&#24230;&#20026;6N&#12289;&#28145;&#24230;&#20026;2L&#30340;varrho&#28608;&#27963;&#32593;&#32476;&#26469;&#36924;&#36817;&#19968;&#20010;&#23485;&#24230;&#20026;N&#12289;&#28145;&#24230;&#20026;L&#30340;ReLU&#32593;&#32476;&#65292;&#20174;&#32780;&#23558;&#23545;ReLU&#32593;&#32476;&#30340;&#36924;&#36817;&#32467;&#26524;&#25512;&#24191;&#21040;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.06555</link><description>&lt;p&gt;
&#28145;&#24230;&#32593;&#32476;&#36924;&#36817;&#65306;&#20174;ReLU&#21040;&#22810;&#31181;&#28608;&#27963;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Deep Network Approximation: Beyond ReLU to Diverse Activation Functions. (arXiv:2307.06555v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#31181;&#28608;&#27963;&#20989;&#25968;&#19979;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#21487;&#20197;&#36890;&#36807;&#22312;&#26377;&#30028;&#38598;&#21512;&#19978;&#26500;&#24314;&#19968;&#20010;&#23485;&#24230;&#20026;6N&#12289;&#28145;&#24230;&#20026;2L&#30340;varrho&#28608;&#27963;&#32593;&#32476;&#26469;&#36924;&#36817;&#19968;&#20010;&#23485;&#24230;&#20026;N&#12289;&#28145;&#24230;&#20026;L&#30340;ReLU&#32593;&#32476;&#65292;&#20174;&#32780;&#23558;&#23545;ReLU&#32593;&#32476;&#30340;&#36924;&#36817;&#32467;&#26524;&#25512;&#24191;&#21040;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#31181;&#28608;&#27963;&#20989;&#25968;&#19979;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#23450;&#20041;&#20102;&#19968;&#20010;&#28608;&#27963;&#20989;&#25968;&#38598;&#21512;A&#65292;&#21253;&#25324;&#22823;&#22810;&#25968;&#24120;&#29992;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#22914;ReLU&#12289;LeakyReLU&#12289;ReLU^2&#12289;ELU&#12289;SELU&#12289;Softplus&#12289;GELU&#12289;SiLU&#12289;Swish&#12289;Mish&#12289;Sigmoid&#12289;Tanh&#12289;Arctan&#12289;Softsign&#12289;dSiLU&#21644;SRS&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20219;&#24847;&#28608;&#27963;&#20989;&#25968;varrho&#8712;A&#65292;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#23485;&#24230;&#20026;6N&#12289;&#28145;&#24230;&#20026;2L&#30340;varrho&#28608;&#27963;&#32593;&#32476;&#22312;&#26377;&#30028;&#38598;&#21512;&#19978;&#20197;&#20219;&#24847;&#31934;&#24230;&#36924;&#36817;&#19968;&#20010;&#23485;&#24230;&#20026;N&#12289;&#28145;&#24230;&#20026;L&#30340;ReLU&#32593;&#32476;&#12290;&#36825;&#19968;&#21457;&#29616;&#20351;&#24471;&#22823;&#37096;&#20998;&#23545;&#20110;ReLU&#32593;&#32476;&#30340;&#36924;&#36817;&#32467;&#26524;&#33021;&#22815;&#25512;&#24191;&#21040;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#65292;&#23613;&#31649;&#38656;&#35201;&#31245;&#22823;&#30340;&#24120;&#25968;&#20195;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the expressive power of deep neural networks for a diverse range of activation functions. An activation function set $\mathscr{A}$ is defined to encompass the majority of commonly used activation functions, such as $\mathtt{ReLU}$, $\mathtt{LeakyReLU}$, $\mathtt{ReLU}^2$, $\mathtt{ELU}$, $\mathtt{SELU}$, $\mathtt{Softplus}$, $\mathtt{GELU}$, $\mathtt{SiLU}$, $\mathtt{Swish}$, $\mathtt{Mish}$, $\mathtt{Sigmoid}$, $\mathtt{Tanh}$, $\mathtt{Arctan}$, $\mathtt{Softsign}$, $\mathtt{dSiLU}$, and $\mathtt{SRS}$. We demonstrate that for any activation function $\varrho\in \mathscr{A}$, a $\mathtt{ReLU}$ network of width $N$ and depth $L$ can be approximated to arbitrary precision by a $\varrho$-activated network of width $6N$ and depth $2L$ on any bounded set. This finding enables the extension of most approximation results achieved with $\mathtt{ReLU}$ networks to a wide variety of other activation functions, at the cost of slightly larger constants.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#29366;&#24577;&#37325;&#26500;&#25193;&#25955;&#31574;&#30053; (SRDP) &#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#26368;&#26032;&#30340;&#25193;&#25955;&#31574;&#30053;&#31867;&#20013;&#24341;&#20837;&#20102;&#29366;&#24577;&#37325;&#26500;&#29305;&#24449;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#21644;&#26377;&#25928;&#34920;&#31034;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.04726</link><description>&lt;p&gt;
&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31163;&#25955;&#31574;&#30053;&#30340;&#25193;&#25955;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Diffusion Policies for Out-of-Distribution Generalization in Offline Reinforcement Learning. (arXiv:2307.04726v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04726
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#29366;&#24577;&#37325;&#26500;&#25193;&#25955;&#31574;&#30053; (SRDP) &#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#26368;&#26032;&#30340;&#25193;&#25955;&#31574;&#30053;&#31867;&#20013;&#24341;&#20837;&#20102;&#29366;&#24577;&#37325;&#26500;&#29305;&#24449;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#21644;&#26377;&#25928;&#34920;&#31034;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33073;&#26426;&#24378;&#21270;&#23398;&#20064; (RL) &#26041;&#27861;&#21033;&#29992;&#20197;&#21069;&#30340;&#32463;&#39564;&#26469;&#23398;&#20064;&#27604;&#29992;&#20110;&#25968;&#25454;&#25910;&#38598;&#30340;&#34892;&#20026;&#31574;&#30053;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;&#19982;&#34892;&#20026;&#20811;&#38534;&#30456;&#21453;&#65292;&#34892;&#20026;&#20811;&#38534;&#20551;&#35774;&#25968;&#25454;&#26159;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#25910;&#38598;&#30340;&#65292;&#32780;&#33073;&#26426; RL &#21487;&#20197;&#20351;&#29992;&#38750;&#19987;&#23478;&#25968;&#25454;&#21644;&#22810;&#27169;&#24577;&#34892;&#20026;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#33073;&#26426; RL &#31639;&#27861;&#22312;&#22788;&#29702;&#20998;&#24067;&#20559;&#31227;&#21644;&#26377;&#25928;&#34920;&#31034;&#31574;&#30053;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#35757;&#32451;&#36807;&#31243;&#20013;&#32570;&#20047;&#22312;&#32447;&#20132;&#20114;&#12290;&#20808;&#21069;&#20851;&#20110;&#33073;&#26426; RL &#30340;&#24037;&#20316;&#20351;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26469;&#34920;&#31034;&#25968;&#25454;&#38598;&#20013;&#30340;&#22810;&#27169;&#24577;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24182;&#27809;&#26377;&#38024;&#23545;&#32531;&#35299;&#33073;&#26426;&#20998;&#24067;&#29366;&#24577;&#27867;&#21270;&#32780;&#21046;&#23450;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#29366;&#24577;&#37325;&#26500;&#25193;&#25955;&#31574;&#30053; (SRDP)&#65292;&#23558;&#29366;&#24577;&#37325;&#26500;&#29305;&#24449;&#23398;&#20064;&#32435;&#20837;&#21040;&#26368;&#26032;&#30340;&#25193;&#25955;&#31574;&#30053;&#31867;&#20013;&#65292;&#20197;&#35299;&#20915;&#33073;&#26426;&#20998;&#24067;&#36890;&#29992;&#21270;&#38382;&#39064;&#12290;&#29366;&#24577;&#37325;&#26500;&#25439;&#22833;&#20419;&#36827;&#20102;&#26356;&#35814;&#32454;&#30340;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline Reinforcement Learning (RL) methods leverage previous experiences to learn better policies than the behavior policy used for data collection. In contrast to behavior cloning, which assumes the data is collected from expert demonstrations, offline RL can work with non-expert data and multimodal behavior policies. However, offline RL algorithms face challenges in handling distribution shifts and effectively representing policies due to the lack of online interaction during training. Prior work on offline RL uses conditional diffusion models to represent multimodal behavior in the dataset. Nevertheless, these methods are not tailored toward alleviating the out-of-distribution state generalization. We introduce a novel method, named State Reconstruction for Diffusion Policies (SRDP), incorporating state reconstruction feature learning in the recent class of diffusion policies to address the out-of-distribution generalization problem. State reconstruction loss promotes more descript
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32676;&#20307;&#30340;&#40065;&#26834;&#24615;&#25351;&#26631;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#25269;&#25239;&#25915;&#20987;&#30340;&#33021;&#21147;&#65292;&#24357;&#34917;&#20102;&#20256;&#32479;&#25351;&#26631;&#30340;&#19981;&#36275;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#25351;&#26631;&#33021;&#22815;&#21306;&#20998;&#27169;&#22411;&#23545;&#29305;&#23450;&#23041;&#32961;&#30340;&#33030;&#24369;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16614</link><description>&lt;p&gt;
&#22522;&#20110;&#32676;&#20307;&#30340;&#40065;&#26834;&#24615;&#65306;&#29616;&#23454;&#19990;&#30028;&#20013;&#23450;&#21046;&#40065;&#26834;&#24615;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Group-based Robustness: A General Framework for Customized Robustness in the Real World. (arXiv:2306.16614v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32676;&#20307;&#30340;&#40065;&#26834;&#24615;&#25351;&#26631;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#25269;&#25239;&#25915;&#20987;&#30340;&#33021;&#21147;&#65292;&#24357;&#34917;&#20102;&#20256;&#32479;&#25351;&#26631;&#30340;&#19981;&#36275;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#25351;&#26631;&#33021;&#22815;&#21306;&#20998;&#27169;&#22411;&#23545;&#29305;&#23450;&#23041;&#32961;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#36867;&#36991;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#21363;&#36890;&#36807;&#25200;&#21160;&#27169;&#22411;&#36755;&#20837;&#26469;&#24341;&#36215;&#38169;&#35823;&#20998;&#31867;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20256;&#32479;&#30340;&#24230;&#37327;&#30446;&#26631;&#21644;&#38750;&#30446;&#26631;&#40065;&#26834;&#24615;&#30340;&#25351;&#26631;&#26080;&#27861;&#20934;&#30830;&#35780;&#20272;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#30495;&#23454;&#23041;&#32961;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#30340;&#32570;&#38519;&#65292;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;&#65292;&#31216;&#20026;&#22522;&#20110;&#32676;&#20307;&#30340;&#40065;&#26834;&#24615;&#65292;&#23427;&#34917;&#20805;&#20102;&#29616;&#26377;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#26356;&#36866;&#21512;&#35780;&#20272;&#29305;&#23450;&#25915;&#20987;&#22330;&#26223;&#19979;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22522;&#20110;&#32676;&#20307;&#30340;&#40065;&#26834;&#24615;&#33021;&#22815;&#22312;&#20256;&#32479;&#30340;&#40065;&#26834;&#24615;&#25351;&#26631;&#19981;&#36866;&#29992;&#30340;&#24773;&#20917;&#19979;&#21306;&#20998;&#27169;&#22411;&#23545;&#29305;&#23450;&#23041;&#32961;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#26377;&#25928;&#20934;&#30830;&#22320;&#34913;&#37327;&#22522;&#20110;&#32676;&#20307;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine-learning models are known to be vulnerable to evasion attacks that perturb model inputs to induce misclassifications. In this work, we identify real-world scenarios where the true threat cannot be assessed accurately by existing attacks. Specifically, we find that conventional metrics measuring targeted and untargeted robustness do not appropriately reflect a model's ability to withstand attacks from one set of source classes to another set of target classes. To address the shortcomings of existing methods, we formally define a new metric, termed group-based robustness, that complements existing metrics and is better-suited for evaluating model performance in certain attack scenarios. We show empirically that group-based robustness allows us to distinguish between models' vulnerability against specific threat models in situations where traditional robustness metrics do not apply. Moreover, to measure group-based robustness efficiently and accurately, we 1) propose two loss func
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#20195;&#29702;&#27169;&#22411;&#65292;&#24555;&#36895;&#39044;&#27979;&#21335;&#20315;&#32599;&#37324;&#36798;&#24030;&#36808;&#38463;&#23494;&#27827;&#19979;&#28216;&#30340;&#27700;&#20301;&#65292;&#24182;&#19982;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2306.15907</link><description>&lt;p&gt;
&#21335;&#20315;&#32599;&#37324;&#36798;&#24030;&#27700;&#20301;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Models for Water Stage Predictions in South Florida. (arXiv:2306.15907v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#20195;&#29702;&#27169;&#22411;&#65292;&#24555;&#36895;&#39044;&#27979;&#21335;&#20315;&#32599;&#37324;&#36798;&#24030;&#36808;&#38463;&#23494;&#27827;&#19979;&#28216;&#30340;&#27700;&#20301;&#65292;&#24182;&#19982;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#21644;&#39044;&#27979;&#27827;&#27969;&#31995;&#32479;&#30340;&#27700;&#20301;&#23545;&#20110;&#27946;&#27700;&#35686;&#25253;&#12289;&#27700;&#21147;&#25805;&#20316;&#21644;&#27946;&#27700;&#20943;&#36731;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#24037;&#31243;&#39046;&#22495;&#20013;&#65292;&#20351;&#29992;HEC-RAS&#12289;MIKE&#21644;SWMM&#31561;&#24037;&#20855;&#24314;&#31435;&#35814;&#32454;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#27700;&#25991;&#21644;&#27700;&#21147;&#35745;&#31639;&#27169;&#22411;&#26469;&#27169;&#25311;&#25972;&#20010;&#27969;&#22495;&#65292;&#20174;&#32780;&#39044;&#27979;&#31995;&#32479;&#20013;&#20219;&#24847;&#28857;&#30340;&#27700;&#20301;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#35745;&#31639;&#37327;&#22823;&#65292;&#23588;&#20854;&#23545;&#20110;&#22823;&#27969;&#22495;&#21644;&#38271;&#26102;&#38388;&#27169;&#25311;&#26469;&#35828;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#20960;&#20010;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#65292;&#24555;&#36895;&#39044;&#27979;&#27700;&#20301;&#12290;&#26412;&#25991;&#20197;&#21335;&#20315;&#32599;&#37324;&#36798;&#24030;&#36808;&#38463;&#23494;&#27827;&#30340;&#19979;&#28216;&#27700;&#20301;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#25968;&#25454;&#38598;&#26469;&#33258;&#21335;&#20315;&#32599;&#37324;&#36798;&#27700;&#31649;&#29702;&#21306;&#65288;SFWMD&#65289;&#30340;DBHYDRO&#25968;&#25454;&#24211;&#65292;&#26102;&#38388;&#36328;&#24230;&#20026;2010&#24180;1&#26376;1&#26085;&#33267;2020&#24180;12&#26376;31&#26085;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;DL&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulating and predicting water levels in river systems is essential for flood warnings, hydraulic operations, and flood mitigations. In the engineering field, tools such as HEC-RAS, MIKE, and SWMM are used to build detailed physics-based hydrological and hydraulic computational models to simulate the entire watershed, thereby predicting the water stage at any point in the system. However, these physics-based models are computationally intensive, especially for large watersheds and for longer simulations. To overcome this problem, we train several deep learning (DL) models for use as surrogate models to rapidly predict the water stage. The downstream stage of the Miami River in South Florida is chosen as a case study for this paper. The dataset is from January 1, 2010, to December 31, 2020, downloaded from the DBHYDRO database of the South Florida Water Management District (SFWMD). Extensive experiments show that the performance of the DL models is comparable to that of the physics-bas
&lt;/p&gt;</description></item><item><title>MeciFace&#26159;&#19968;&#27454;&#27880;&#37325;&#38544;&#31169;&#19988;&#20302;&#21151;&#32791;&#30340;&#21487;&#31359;&#25140;&#35774;&#22791;&#65292;&#23427;&#37319;&#29992;&#36731;&#37327;&#32423;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#30417;&#27979;&#38754;&#37096;&#34920;&#24773;&#21644;&#36827;&#39135;&#27963;&#21160;&#65292;&#38754;&#37096;&#34920;&#24773;&#26696;&#20363;&#30340;F1&#20998;&#25968;&#36798;&#21040;&#20102;86&#65285;&#65292;&#39278;&#39135;&#30417;&#27979;&#21017;&#36798;&#21040;&#20102;90&#65285;&#30340;F1&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2306.13674</link><description>&lt;p&gt;
MeciFace&#65306;&#22522;&#20110;&#32908;&#32905;&#30005;&#21644;&#24815;&#24615;&#34701;&#21512;&#30340;&#36793;&#32536;&#23454;&#26102;&#35782;&#21035;&#38754;&#37096;&#34920;&#24773;&#21644;&#36827;&#39135;&#27963;&#21160;&#30524;&#38236;
&lt;/p&gt;
&lt;p&gt;
MeciFace: Mechanomyography and Inertial Fusion based Glasses for Edge Real-Time Recognition of Facial and Eating Activities. (arXiv:2306.13674v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13674
&lt;/p&gt;
&lt;p&gt;
MeciFace&#26159;&#19968;&#27454;&#27880;&#37325;&#38544;&#31169;&#19988;&#20302;&#21151;&#32791;&#30340;&#21487;&#31359;&#25140;&#35774;&#22791;&#65292;&#23427;&#37319;&#29992;&#36731;&#37327;&#32423;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#30417;&#27979;&#38754;&#37096;&#34920;&#24773;&#21644;&#36827;&#39135;&#27963;&#21160;&#65292;&#38754;&#37096;&#34920;&#24773;&#26696;&#20363;&#30340;F1&#20998;&#25968;&#36798;&#21040;&#20102;86&#65285;&#65292;&#39278;&#39135;&#30417;&#27979;&#21017;&#36798;&#21040;&#20102;90&#65285;&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MeciFace&#65292;&#36825;&#26159;&#19968;&#20010;&#20302;&#21151;&#32791;&#65288;0.55&#29926;&#65289;&#65292;&#27880;&#37325;&#38544;&#31169;&#65292;&#23454;&#26102;&#36793;&#32536;&#30417;&#27979;&#65288;RTE&#65289;&#30340;&#21487;&#31359;&#25140;&#35299;&#20915;&#26041;&#26696;&#65292;&#20855;&#26377;&#24494;&#23567;&#30340;&#20869;&#23384;&#21344;&#29992;&#65288;11-19 KB&#65289;&#65292;&#26088;&#22312;&#30417;&#27979;&#38754;&#37096;&#34920;&#24773;&#21644;&#36827;&#39135;&#27963;&#21160;&#12290;&#25105;&#20204;&#37319;&#29992;&#36731;&#37327;&#32423;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#38754;&#37096;&#21644;&#36827;&#39135;&#22330;&#26223;&#30340;&#20027;&#24178;&#27169;&#22411;&#12290;&#35813;&#31995;&#32479;&#22312;&#38754;&#37096;&#34920;&#24773;&#26696;&#20363;&#30340;RTE&#35780;&#20272;&#20013;&#20135;&#29983;&#20102;86&#65285;&#30340;F1&#20998;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#26410;&#30693;&#29992;&#25143;&#30340;RTE&#36827;&#34892;&#39278;&#39135;&#30417;&#27979;&#65292;&#24471;&#21040;&#20102;90&#65285;&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present MeciFace, a low-power (0.55 Watts), privacy-conscious, real-time on-the-edge (RTE) wearable solution with a tiny memory footprint (11-19 KB), designed to monitor facial expressions and eating activities. We employ lightweight convolutional neural networks as the backbone models for both facial and eating scenarios. The system yielded an F1-score of 86% for the RTE evaluation in the facial expression case. In addition, we obtained an F1-score of 90% for eating/drinking monitoring for the RTE of an unseen user.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65288;SSL&#27169;&#22411;&#65289;&#36827;&#34892;&#27468;&#21809;&#22768;&#38899;&#29702;&#35299;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23558;&#33258;&#30417;&#30563;&#21069;&#31471;&#36716;&#31227;&#21040;&#30446;&#26631;&#20219;&#21153;&#21487;&#20197;&#21462;&#24471;&#26356;&#22909;&#24615;&#33021;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;SSL&#27169;&#22411;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#22343;&#20248;&#20110;&#24120;&#35268;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.12714</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#21069;&#31471;&#23454;&#29616;&#27468;&#21809;&#22768;&#38899;&#33258;&#21160;&#29702;&#35299;&#20219;&#21153;&#65306;&#19977;&#20010;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Toward Leveraging Pre-Trained Self-Supervised Frontends for Automatic Singing Voice Understanding Tasks: Three Case Studies. (arXiv:2306.12714v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65288;SSL&#27169;&#22411;&#65289;&#36827;&#34892;&#27468;&#21809;&#22768;&#38899;&#29702;&#35299;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23558;&#33258;&#30417;&#30563;&#21069;&#31471;&#36716;&#31227;&#21040;&#30446;&#26631;&#20219;&#21153;&#21487;&#20197;&#21462;&#24471;&#26356;&#22909;&#24615;&#33021;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;SSL&#27169;&#22411;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#22343;&#20248;&#20110;&#24120;&#35268;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#23545;&#33258;&#21160;&#27468;&#21809;&#22768;&#38899;&#29702;&#35299;&#20219;&#21153;&#65292;&#22914;&#27468;&#25163;&#35782;&#21035;&#12289;&#27468;&#22768;&#36716;&#24405;&#21644;&#27468;&#21809;&#25216;&#24039;&#20998;&#31867;&#31561;&#26041;&#38754;&#20855;&#26377;&#34920;&#24449;&#33021;&#21147;&#65292;&#21363;&#20351;&#22312;&#20855;&#26377;&#20016;&#23500;&#20154;&#22768;&#21644;&#22122;&#22768;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#21457;&#25381;&#33391;&#22909;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#26377;&#38480;&#30340;&#26631;&#27880;&#25968;&#25454;&#20173;&#28982;&#26159;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#30340;&#37325;&#35201;&#38556;&#30861;&#12290;&#36817;&#24180;&#26469;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65288;SSL&#27169;&#22411;&#65289;&#22312;&#35821;&#38899;&#22788;&#29702;&#21644;&#38899;&#20048;&#20998;&#31867;&#39046;&#22495;&#32463;&#36807;&#22823;&#37327;&#26410;&#26631;&#27880;&#25968;&#25454;&#30340;&#35757;&#32451;&#12290;&#36890;&#36807;&#24494;&#35843;&#36825;&#20123;&#27169;&#22411;&#20197;&#29992;&#20110;&#30446;&#26631;&#20219;&#21153;&#65292;&#21363;&#20351;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19979;&#20063;&#33021;&#23454;&#29616;&#19982;&#24120;&#35268;&#30417;&#30563;&#23398;&#20064;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;SSL&#27169;&#22411;&#22312;&#21508;&#31181;&#27468;&#21809;&#22768;&#38899;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#27604;&#36739;&#19977;&#20010;&#19981;&#21516;&#20219;&#21153;&#65288;&#21363;&#27468;&#25163;&#35782;&#21035;&#12289;&#27468;&#22768;&#36716;&#24405;&#21644;&#27468;&#21809;&#25216;&#24039;&#20998;&#31867;&#65289;&#30340;SSL&#27169;&#22411;&#21644;&#24120;&#35268;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SSL&#27169;&#22411;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#22343;&#20248;&#20110;&#26377;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#23558;&#22312;&#22823;&#37327;&#26410;&#26631;&#27880;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#21069;&#31471;&#36716;&#31227;&#21040;&#30446;&#26631;&#20219;&#21153;&#21487;&#20197;&#27604;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#32467;&#26524;&#35828;&#26126;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#25552;&#39640;&#27468;&#21809;&#22768;&#38899;&#29702;&#35299;&#20219;&#21153;&#30340;&#25968;&#25454;&#25928;&#29575;&#21644;&#24615;&#33021;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic singing voice understanding tasks, such as singer identification, singing voice transcription, and singing technique classification, benefit from data-driven approaches that utilize deep learning techniques. These approaches work well even under the rich diversity of vocal and noisy samples owing to their representation ability. However, the limited availability of labeled data remains a significant obstacle to achieving satisfactory performance. In recent years, self-supervised learning models (SSL models) have been trained using large amounts of unlabeled data in the field of speech processing and music classification. By fine-tuning these models for the target tasks, comparable performance to conventional supervised learning can be achieved with limited training data. Therefore, in this paper, we investigate the effectiveness of SSL models for various singing voice recognition tasks. We report the results of experiments comparing SSL models for three different tasks (i.e.,
&lt;/p&gt;</description></item><item><title>DynaQuant&#36890;&#36807;&#21160;&#24577;&#37327;&#21270;&#23454;&#29616;&#23545;&#21508;&#31181;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#26174;&#30528;&#21387;&#32553;&#65292;&#20960;&#20046;&#19981;&#24433;&#21709;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.11800</link><description>&lt;p&gt;
DynaQuant: &#36890;&#36807;&#21160;&#24577;&#37327;&#21270;&#21387;&#32553;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#26816;&#26597;&#28857;
&lt;/p&gt;
&lt;p&gt;
DynaQuant: Compressing Deep Learning Training Checkpoints via Dynamic Quantization. (arXiv:2306.11800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11800
&lt;/p&gt;
&lt;p&gt;
DynaQuant&#36890;&#36807;&#21160;&#24577;&#37327;&#21270;&#23454;&#29616;&#23545;&#21508;&#31181;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#26174;&#30528;&#21387;&#32553;&#65292;&#20960;&#20046;&#19981;&#24433;&#21709;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#24037;&#20316;&#37327;&#22312;&#35745;&#31639;&#36164;&#28304;&#21644;&#26102;&#38388;&#28040;&#32791;&#26041;&#38754;&#30340;&#22686;&#21152;&#65292;&#36935;&#21040;&#35757;&#32451;&#22833;&#36133;&#30340;&#21487;&#33021;&#24615;&#26174;&#33879;&#22686;&#21152;&#65292;&#23548;&#33268;&#24037;&#20316;&#20002;&#22833;&#21644;&#36164;&#28304;&#28010;&#36153;&#12290;&#26368;&#26032;&#30340;&#26041;&#27861;&#28041;&#21450;&#26377;&#25439;&#27169;&#22411;&#21387;&#32553;&#26426;&#21046;&#65292;&#36825;&#20250;&#22312;&#27169;&#22411;&#36136;&#37327;&#65288;&#20934;&#30830;&#24615;&#65289;&#21644;&#21387;&#32553;&#27604;&#20043;&#38388;&#20135;&#29983;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21160;&#24577;&#37327;&#21270;&#26694;&#26550;&#65292;&#31216;&#20026;DynaQuant&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#35757;&#32451;&#26399;&#38388;&#27169;&#22411;&#26435;&#37325;&#30340;&#28789;&#25935;&#24230;&#21464;&#21270;&#26469;&#26356;&#26032;&#37327;&#21270;&#32423;&#21035;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#21508;&#31181;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#26174;&#30528;&#21387;&#32553;&#65292;&#24182;&#19988;&#20960;&#20046;&#19981;&#24433;&#21709;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increase in the scale of Deep Learning (DL) training workloads in terms of compute resources and time consumption, the likelihood of encountering in-training failures rises substantially, leading to lost work and resource wastage. Such failures are typically offset by a checkpointing mechanism, which comes at the cost of storage and network bandwidth overhead. State-of-the-art approaches involve lossy model compression mechanisms, which induce a tradeoff between the resulting model quality (accuracy) and compression ratio. Delta compression is then also used to further reduce the overhead by only storing the difference between consecutive checkpoints. We make a key enabling observation that the sensitivity of model weights to compression varies during training, and different weights benefit from different quantization levels (ranging from retaining full precision to pruning). We propose (1) a non-uniform quantization scheme that leverages this variation, (2) an efficient searc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#32500;RL&#27169;&#22411;&#65292;&#25512;&#23548;&#20986;&#35813;&#27169;&#22411;&#30340;&#20856;&#22411;&#21160;&#21147;&#23398;&#20026;&#19968;&#32452;&#38381;&#24335;ODE&#26041;&#31243;&#32452;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#19982;&#31070;&#32463;RL&#20195;&#29702;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#25429;&#25417;&#20102;&#29616;&#23454;&#19990;&#30028;RL&#30340;&#20851;&#38190;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2306.10404</link><description>&lt;p&gt;
RL&#24863;&#30693;&#26426;&#65306;&#39640;&#32500;&#31574;&#30053;&#23398;&#20064;&#30340;&#27867;&#21270;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
The RL Perceptron: Generalisation Dynamics of Policy Learning in High Dimensions. (arXiv:2306.10404v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#32500;RL&#27169;&#22411;&#65292;&#25512;&#23548;&#20986;&#35813;&#27169;&#22411;&#30340;&#20856;&#22411;&#21160;&#21147;&#23398;&#20026;&#19968;&#32452;&#38381;&#24335;ODE&#26041;&#31243;&#32452;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#19982;&#31070;&#32463;RL&#20195;&#29702;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#25429;&#25417;&#20102;&#29616;&#23454;&#19990;&#30028;RL&#30340;&#20851;&#38190;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24050;&#34987;&#35777;&#26126;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#20855;&#26377;&#21464;&#38761;&#24615;&#65292;&#20026;&#20102;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#30340;&#38382;&#39064;&#65292;&#36825;&#20123;&#31995;&#32479;&#36890;&#24120;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30452;&#25509;&#20174;&#20687;&#32032;&#25110;&#20854;&#20182;&#39640;&#32500;&#24863;&#23448;&#36755;&#20837;&#20013;&#23398;&#20064;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24378;&#21270;&#23398;&#20064;&#29702;&#35770;&#37117;&#38598;&#20013;&#20110;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#25110;&#26368;&#22351;&#24773;&#20917;&#20998;&#26512;&#65292;&#20851;&#20110;&#39640;&#32500;&#24773;&#20917;&#19979;&#31574;&#30053;&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;&#22522;&#26412;&#38382;&#39064;&#20173;&#26377;&#24453;&#35299;&#20915;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#30340;&#39640;&#32500;RL&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#25429;&#25417;&#22810;&#31181;&#23398;&#20064;&#21327;&#35758;&#65292;&#24182;&#23558;&#20854;&#20856;&#22411;&#21160;&#21147;&#23398;&#23548;&#20986;&#20026;&#19968;&#32452;&#38381;&#24335;&#24120;&#24494;&#20998;&#26041;&#31243;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#26368;&#20339;&#30340;&#23398;&#20064;&#29575;&#21644;&#20219;&#21153;&#38590;&#24230;&#35843;&#24230;&#65292;&#31867;&#20284;&#20110;&#35757;&#32451;&#20013;&#30340;&#36864;&#28779;&#26041;&#26696;&#21644;&#35838;&#31243;&#34920;&#65292;&#24182;&#34920;&#26126;&#35813;&#27169;&#22411;&#34920;&#29616;&#20986;&#20016;&#23500;&#30340;&#34892;&#20026;&#65292;&#21253;&#25324;&#22312;&#31232;&#30095;&#22870;&#21169;&#19979;&#30340;&#24310;&#36831;&#23398;&#20064;&#65307;&#26681;&#25454;&#22870;&#21169;&#22522;&#32447;&#19981;&#21516;&#30340;&#21508;&#31181;&#23398;&#20064;&#26041;&#26696;&#65307;&#20197;&#21450;&#30001;&#22870;&#21169;&#20005;&#26684;&#31243;&#24230;&#39537;&#21160;&#30340;&#36895;&#24230;-&#20934;&#30830;&#24230;&#26435;&#34913;&#12290;&#19982;&#31070;&#32463;RL&#20195;&#29702;&#36827;&#34892;&#30340;&#23454;&#39564;&#27604;&#36739;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#25429;&#25417;&#20102;&#29616;&#23454;&#19990;&#30028;RL&#30340;&#20851;&#38190;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) algorithms have proven transformative in a range of domains. To tackle real-world domains, these systems often use neural networks to learn policies directly from pixels or other high-dimensional sensory input. By contrast, much theory of RL has focused on discrete state spaces or worst-case analysis, and fundamental questions remain about the dynamics of policy learning in high-dimensional settings. Here, we propose a solvable high-dimensional model of RL that can capture a variety of learning protocols, and derive its typical dynamics as a set of closed-form ordinary differential equations (ODEs). We derive optimal schedules for the learning rates and task difficulty - analogous to annealing schemes and curricula during training in RL - and show that the model exhibits rich behaviour, including delayed learning under sparse rewards; a variety of learning regimes depending on reward baselines; and a speed-accuracy trade-off driven by reward stringency. Expe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#30417;&#30563;&#23398;&#20064;&#30340;&#21160;&#30011;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#25991;&#26412;&#25110;&#35821;&#38899;&#36755;&#20837;&#65292;&#23454;&#29616;&#22522;&#20110;&#30495;&#23454;&#21160;&#20316;&#34920;&#28436;&#30340;&#38754;&#37096;&#21160;&#30011;&#65292;&#24182;&#19988;&#21487;&#20197;&#19981;&#21516;&#31243;&#24230;&#22320;&#23398;&#20064;&#24182;&#21512;&#25104;&#19981;&#21516;&#30340;&#34920;&#28436;&#39118;&#26684;&#12290;</title><link>http://arxiv.org/abs/2306.10006</link><description>&lt;p&gt;
&#22522;&#20110;&#29616;&#23454;&#34920;&#28436;&#30340;&#38754;&#37096;&#21160;&#30011;&#39118;&#26684;&#24863;&#30693;&#38750;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Learning of Style-Aware Facial Animation from Real Acting Performances. (arXiv:2306.10006v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#30417;&#30563;&#23398;&#20064;&#30340;&#21160;&#30011;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#25991;&#26412;&#25110;&#35821;&#38899;&#36755;&#20837;&#65292;&#23454;&#29616;&#22522;&#20110;&#30495;&#23454;&#21160;&#20316;&#34920;&#28436;&#30340;&#38754;&#37096;&#21160;&#30011;&#65292;&#24182;&#19988;&#21487;&#20197;&#19981;&#21516;&#31243;&#24230;&#22320;&#23398;&#20064;&#24182;&#21512;&#25104;&#19981;&#21516;&#30340;&#34920;&#28436;&#39118;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#28151;&#21512;&#24418;&#29366;&#20960;&#20309;&#12289;&#21160;&#24577;&#32441;&#29702;&#21644;&#31070;&#32463;&#28210;&#26579;&#65292;&#29992;&#20110;&#20174;&#30495;&#23454;&#21160;&#20316;&#34920;&#28436;&#20013;&#39537;&#21160;&#38754;&#37096;&#27169;&#22411;&#30340;&#25991;&#26412;/&#35821;&#38899;&#21160;&#30011;&#12290;&#36890;&#36807;&#35757;&#32451;&#21253;&#25324;&#24418;&#29366;&#21644;&#32441;&#29702;&#30340;VAE&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#21442;&#25968;&#21270;&#27169;&#22411;&#65292;&#20197;&#31934;&#30830;&#25429;&#25417;&#21644;&#36924;&#30495;&#21512;&#25104;&#28508;&#22312;&#29305;&#24449;&#21521;&#37327;&#20013;&#30340;&#38754;&#37096;&#34920;&#24773;&#12290;&#25105;&#20204;&#30340;&#21160;&#30011;&#26041;&#27861;&#22522;&#20110;&#26465;&#20214;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#25991;&#26412;&#25110;&#35821;&#38899;&#36716;&#25442;&#20026;&#19968;&#31995;&#21015;&#21160;&#30011;&#21442;&#25968;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#21160;&#30011;&#27169;&#22411;&#20197;&#38750;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#21306;&#20998;&#21644;&#21512;&#25104;&#19981;&#21516;&#30340;&#34920;&#28436;&#39118;&#26684;&#65292;&#21482;&#38656;&#35201;&#29992;&#20110;&#25551;&#36848;&#35757;&#32451;&#24207;&#21015;&#20869;&#23481;&#30340;&#35821;&#38899;&#26631;&#31614;&#12290;&#20026;&#20102;&#23454;&#29616;&#36924;&#30495;&#30340;&#23454;&#26102;&#28210;&#26579;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;U-Net&#65292;&#36890;&#36807;&#35745;&#31639;&#25913;&#36827;&#30340;&#20687;&#32032;&#39068;&#33394;&#21644;&#21069;&#26223;&#36974;&#32617;&#26469;&#25913;&#21892;&#26629;&#26684;&#21270;&#28210;&#26579;&#12290;&#25105;&#20204;&#23450;&#24615;/&#23450;&#37327;&#22320;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#19982;&#26368;&#36817;&#30340;&#22836;&#37096;&#24314;&#27169;&#26041;&#27861;&#20197;&#21450;&#38754;&#37096;&#21160;&#30011;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#35780;&#20272;&#24863;&#30693;&#28210;&#26579;/&#21160;&#30011;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel approach for text/speech-driven animation of a photo-realistic head model based on blend-shape geometry, dynamic textures, and neural rendering. Training a VAE for geometry and texture yields a parametric model for accurate capturing and realistic synthesis of facial expressions from a latent feature vector. Our animation method is based on a conditional CNN that transforms text or speech into a sequence of animation parameters. In contrast to previous approaches, our animation model learns disentangling/synthesizing different acting-styles in an unsupervised manner, requiring only phonetic labels that describe the content of training sequences. For realistic real-time rendering, we train a U-Net that refines rasterization-based renderings by computing improved pixel colors and a foreground matte. We compare our framework qualitatively/quantitatively against recent methods for head modeling as well as facial animation and evaluate the perceived rendering/ani
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#35270;&#35282;&#20998;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;MVCIL&#65289;&#30340;&#26032;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#38543;&#26426;&#21270;&#30340;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#24182;&#25552;&#20986;&#27491;&#20132;&#34701;&#21512;&#23376;&#31354;&#38388;&#21644;&#36873;&#25321;&#24615;&#26435;&#37325;&#21512;&#24182;&#26469;&#35299;&#20915;&#22686;&#37327;&#23398;&#20064;&#20013;&#36951;&#24536;&#26087;&#20449;&#24687;&#21644;&#23398;&#20064;&#26032;&#27010;&#24565;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#30456;&#27604;&#26368;&#26032;&#26041;&#27861;&#26377;&#25928;&#24615;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.09675</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#20998;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-View Class Incremental Learning. (arXiv:2306.09675v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#35270;&#35282;&#20998;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;MVCIL&#65289;&#30340;&#26032;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#38543;&#26426;&#21270;&#30340;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#24182;&#25552;&#20986;&#27491;&#20132;&#34701;&#21512;&#23376;&#31354;&#38388;&#21644;&#36873;&#25321;&#24615;&#26435;&#37325;&#21512;&#24182;&#26469;&#35299;&#20915;&#22686;&#37327;&#23398;&#20064;&#20013;&#36951;&#24536;&#26087;&#20449;&#24687;&#21644;&#23398;&#20064;&#26032;&#27010;&#24565;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#30456;&#27604;&#26368;&#26032;&#26041;&#27861;&#26377;&#25928;&#24615;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#23398;&#20064;&#65288;MVL&#65289;&#22312;&#25972;&#21512;&#25968;&#25454;&#38598;&#30340;&#22810;&#20010;&#35270;&#35282;&#20197;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#20026;&#20102;&#20351;MVL&#26041;&#27861;&#22312;&#24320;&#25918;&#24335;&#29615;&#22659;&#20013;&#26356;&#23454;&#29992;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#31216;&#20026;&#22810;&#35270;&#35282;&#20998;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;MVCIL&#65289;&#65292;&#20854;&#20013;&#21333;&#20010;&#27169;&#22411;&#20174;&#36830;&#32493;&#30340;&#35270;&#22270;&#27969;&#20013;&#36880;&#27493;&#20998;&#31867;&#26032;&#31867;&#65292;&#19981;&#38656;&#35201;&#35775;&#38382;&#26089;&#26399;&#25968;&#25454;&#30340;&#35270;&#22270;&#12290;&#20294;&#26159;&#65292;MVCIL&#38754;&#20020;&#30528;&#32769;&#20449;&#24687;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#23398;&#20064;&#26032;&#27010;&#24565;&#30340;&#24178;&#25200;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#21270;&#30340;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#29305;&#24449;&#25552;&#21462;&#65292;&#20197;&#20445;&#35777;&#23427;&#20204;&#22312;&#24037;&#20316;&#29366;&#24577;&#19979;&#30340;&#20998;&#31163;&#35270;&#22270;&#26368;&#20248;&#65292;&#20854;&#20013;&#23646;&#20110;&#31867;&#30340;&#22810;&#20010;&#35270;&#22270;&#25353;&#39034;&#24207;&#21576;&#29616;&#65307;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#36880;&#20010;&#38598;&#25104;&#21040;&#30001;&#25552;&#21462;&#30340;&#29305;&#24449;&#36328;&#36234;&#30340;&#27491;&#20132;&#34701;&#21512;&#23376;&#31354;&#38388;&#20013;&#65307;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#36873;&#25321;&#24615;&#26435;&#37325;&#21512;&#24182;&#65292;&#20197;&#20445;&#30041;&#26087;&#31867;&#30340;&#30693;&#35782;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#26368;&#26032;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-view learning (MVL) has gained great success in integrating information from multiple perspectives of a dataset to improve downstream task performance. To make MVL methods more practical in an open-ended environment, this paper investigates a novel paradigm called multi-view class incremental learning (MVCIL), where a single model incrementally classifies new classes from a continual stream of views, requiring no access to earlier views of data. However, MVCIL is challenged by the catastrophic forgetting of old information and the interference with learning new concepts. To address this, we first develop a randomization-based representation learning technique serving for feature extraction to guarantee their separate view-optimal working states, during which multiple views belonging to a class are presented sequentially; Then, we integrate them one by one in the orthogonality fusion subspace spanned by the extracted features; Finally, we introduce selective weight consolidation f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#27979;&#35797;&#65292;&#25361;&#25112;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#38271;&#25991;&#26723;&#12289;&#21033;&#29992;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12289;&#22810;&#35821;&#35328;&#29702;&#35299;&#21644;&#22810;&#20219;&#21153;&#22788;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#29790;&#22763;&#27861;&#24459;&#31995;&#32479;&#30340;&#22810;&#26679;&#21270;&#27861;&#24459;NLP&#25968;&#25454;&#38598;&#65292;&#20801;&#35768;&#36827;&#34892;&#23545;&#24213;&#23618;&#38750;&#33521;&#35821;&#12289;&#22266;&#26377;&#22810;&#35821;&#35328;&#30340;&#27861;&#24459;&#31995;&#32479;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2306.09237</link><description>&lt;p&gt;
SCALE: &#25552;&#21319;&#39640;&#32423;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
SCALE: Scaling up the Complexity for Advanced Language Model Evaluation. (arXiv:2306.09237v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09237
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#27979;&#35797;&#65292;&#25361;&#25112;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#38271;&#25991;&#26723;&#12289;&#21033;&#29992;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12289;&#22810;&#35821;&#35328;&#29702;&#35299;&#21644;&#22810;&#20219;&#21153;&#22788;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#29790;&#22763;&#27861;&#24459;&#31995;&#32479;&#30340;&#22810;&#26679;&#21270;&#27861;&#24459;NLP&#25968;&#25454;&#38598;&#65292;&#20801;&#35768;&#36827;&#34892;&#23545;&#24213;&#23618;&#38750;&#33521;&#35821;&#12289;&#22266;&#26377;&#22810;&#35821;&#35328;&#30340;&#27861;&#24459;&#31995;&#32479;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#24050;&#32463;&#39281;&#21644;&#20102;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#27979;&#35797;&#65288;&#21253;&#25324;&#19987;&#19994;&#39046;&#22495;&#30340;&#22522;&#20934;&#27979;&#35797;&#65289;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#26032;&#39062;&#12289;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#27979;&#35797;&#26469;&#27491;&#30830;&#35780;&#20272;LLM&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#27979;&#35797;&#65292;&#23545;&#24403;&#21069;LLM&#30340;&#22235;&#20010;&#20851;&#38190;&#26041;&#38754;&#25552;&#20986;&#20102;&#25361;&#25112;&#65306;&#22788;&#29702;&#38271;&#25991;&#26723;&#65288;&#22810;&#36798;50K&#20010;&#26631;&#35760;&#65289;&#12289;&#21033;&#29992;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65288;&#20307;&#29616;&#22312;&#27861;&#24459;&#25991;&#26412;&#20013;&#65289;&#12289;&#22810;&#35821;&#35328;&#29702;&#35299;&#65288;&#28085;&#30422;&#20116;&#31181;&#35821;&#35328;&#65289;&#21644;&#22810;&#20219;&#21153;&#22788;&#29702;&#65288;&#21253;&#25324;&#27861;&#24459;&#25991;&#20214;&#21040;&#25991;&#20214;&#20449;&#24687;&#26816;&#32034;&#12289;&#27861;&#24237;&#35270;&#22270;&#29983;&#25104;&#12289;&#37325;&#35201;&#20915;&#31574;&#25688;&#35201;&#12289;&#24341;&#29992;&#25552;&#21462;&#21644;&#20843;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#65289;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#20102;&#26469;&#33258;&#29790;&#22763;&#27861;&#24459;&#31995;&#32479;&#30340;&#22810;&#26679;&#30340;&#27861;&#24459;NLP&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#23545;&#24213;&#23618;&#38750;&#33521;&#35821;&#12289;&#22266;&#26377;&#22810;&#35821;&#35328;&#30340;&#32852;&#37030;&#27861;&#24459;&#31995;&#32479;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#12290;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#24378;&#28872;&#30340;&#23457;&#26597;/&#20998;&#26512;&#20219;&#21153;&#65292;&#39640;&#25928;&#22320;&#22788;&#29702;&#38271;&#25991;&#26723;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent strides in Large Language Models (LLMs) have saturated many NLP benchmarks (even professional domain-specific ones), emphasizing the need for novel, more challenging novel ones to properly assess LLM capabilities. In this paper, we introduce a novel NLP benchmark that poses challenges to current LLMs across four key dimensions: processing long documents (up to 50K tokens), utilizing domain specific knowledge (embodied in legal texts), multilingual understanding (covering five languages), and multitasking (comprising legal document to document Information Retrieval, Court View Generation, Leading Decision Summarization, Citation Extraction, and eight challenging Text Classification tasks). Our benchmark comprises diverse legal NLP datasets from the Swiss legal system, allowing for a comprehensive study of the underlying Non-English, inherently multilingual, federal legal system. Despite recent advances, efficiently processing long documents for intense review/analysis tasks remai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#26680;&#38543;&#26426;&#25237;&#24433;&#28145;&#24230;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#25968;&#25454;&#20113;&#20013;&#30340;&#22810;&#27169;&#24335;&#21644;&#38750;&#20984;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.07056</link><description>&lt;p&gt;
&#20869;&#26680;&#38543;&#26426;&#25237;&#24433;&#28145;&#24230;&#29992;&#20110;&#31163;&#32676;&#28857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Kernel Random Projection Depth for Outlier Detection. (arXiv:2306.07056v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#26680;&#38543;&#26426;&#25237;&#24433;&#28145;&#24230;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#25968;&#25454;&#20113;&#20013;&#30340;&#22810;&#27169;&#24335;&#21644;&#38750;&#20984;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#38543;&#26426;&#25237;&#24433;&#28145;&#24230;&#65288;RPD&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#25968;&#25454;&#20113;&#20013;&#30340;&#22810;&#27169;&#24335;&#21644;&#38750;&#20984;&#24615;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26694;&#26550;&#20013;&#65292;RPD&#22312;&#20877;&#29616;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#35745;&#31639;&#12290;&#20511;&#21161;&#20869;&#26680;&#20027;&#25104;&#20998;&#20998;&#26512;&#65292;&#25105;&#20204;&#26399;&#26395;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#19978;&#36848;&#22810;&#31181;&#27169;&#24335;&#21644;&#38750;&#20984;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;RPD&#65292;&#24182;&#21487;&#19982;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#29616;&#26377;&#30340;&#26816;&#27979;&#27169;&#22411;&#30456;&#23218;&#32654;&#65292;&#20851;&#20110;&#25509;&#25910;&#25805;&#20316;&#29305;&#24449;&#26354;&#32447;&#65288;ROC&#65289;&#19979;&#30340;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an extension of Random Projection Depth (RPD) to cope with multiple modalities and non-convexity on data clouds. In the framework of the proposed method, the RPD is computed in a reproducing kernel Hilbert space. With the help of kernel principal component analysis, we expect that the proposed method can cope with the above multiple modalities and non-convexity. The experimental results demonstrate that the proposed method outperforms RPD and is comparable to other existing detection models on benchmark datasets regarding Area Under the Curves (AUCs) of Receiver Operating Characteristic (ROC).
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#20915;&#31574;&#26641;&#27169;&#22411;&#65292;&#21033;&#29992;&#25346;&#36215;&#30340;&#26641;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;&#20854;&#35299;&#37322;&#24615;&#21644;&#32479;&#35745;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#26080;&#38480;&#28145;&#24230;&#20915;&#31574;&#26641;&#30340;&#27700;&#24179;&#65292;&#24182;&#21487;&#19982;XGBoost&#31561;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2306.06777</link><description>&lt;p&gt;
&#25552;&#39640;&#20915;&#31574;&#26641;&#35299;&#37322;&#24615;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving the Validity of Decision Trees as Explanations. (arXiv:2306.06777v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06777
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#20915;&#31574;&#26641;&#27169;&#22411;&#65292;&#21033;&#29992;&#25346;&#36215;&#30340;&#26641;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;&#20854;&#35299;&#37322;&#24615;&#21644;&#32479;&#35745;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#26080;&#38480;&#28145;&#24230;&#20915;&#31574;&#26641;&#30340;&#27700;&#24179;&#65292;&#24182;&#21487;&#19982;XGBoost&#31561;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#20998;&#31867;&#21644;&#39044;&#27979;&#20013;&#65292;&#20154;&#20204;&#32463;&#24120;&#20351;&#29992;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#12290;&#36825;&#21487;&#20197;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31454;&#20105;[&#21442;&#35265;Grinsztajn&#31561;&#20154;&#65292;NeurIPS 2022&#65292;arXiv&#65306;2207.08815]&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#26159;&#21487;&#35299;&#37322;&#30340;&#12290;&#21487;&#35299;&#37322;&#24615;&#21462;&#20915;&#20110;&#26641;&#30340;&#28145;&#24230;&#21644;&#27599;&#20010;&#21494;&#33410;&#28857;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#20302;&#28145;&#24230;&#30340;&#26641;&#65292;&#20854;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#27599;&#20010;&#21494;&#33410;&#28857;&#19978;&#30340;&#26368;&#22823;&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#20174;&#20302;&#28145;&#24230;&#26641;&#30340;&#27599;&#20010;&#21494;&#33410;&#28857;&#8220;&#25346;&#36215;&#8221;&#36827;&#19968;&#27493;&#30340;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#65288;&#20363;&#22914;&#26080;&#38480;&#28145;&#24230;&#30340;&#26641;&#65289;&#12290;&#20302;&#28145;&#24230;&#26641;&#26131;&#20110;&#35299;&#37322;&#65292;&#32780;&#32508;&#21512;&#20302;&#28145;&#24230;&#21644;&#25346;&#36215;&#30340;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#30340;&#25972;&#20307;&#32479;&#35745;&#24615;&#33021;&#20248;&#20110;&#20351;&#29992;&#32463;&#20856;&#26041;&#27861;&#65288;&#20363;&#22914;CART&#65289;&#35757;&#32451;&#30340;&#26080;&#38480;&#28145;&#24230;&#20915;&#31574;&#26641;&#65292;&#24182;&#19988;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65288;&#20363;&#22914;&#20248;&#21270;&#30340;XGBoost&#65289;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
In classification and forecasting with tabular data, one often utilizes tree-based models. This can be competitive with deep neural networks on tabular data [cf. Grinsztajn et al., NeurIPS 2022, arXiv:2207.08815] and, under some conditions, explainable. The explainability depends on the depth of the tree and the accuracy in each leaf of the tree. Here, we train a low-depth tree with the objective of minimising the maximum misclassification error across each leaf node, and then ``suspend'' further tree-based models (e.g., trees of unlimited depth) from each leaf of the low-depth tree. The low-depth tree is easily explainable, while the overall statistical performance of the combined low-depth and suspended tree-based models improves upon decision trees of unlimited depth trained using classical methods (e.g., CART) and is comparable to state-of-the-art methods (e.g., well-tuned XGBoost).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#22810;&#32423;&#22810;&#31034;&#20363;&#23398;&#20064;&#65288;MMIL-Transformer&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#28041;&#21450;&#22823;&#37327;&#23454;&#20363;&#30340;MIL&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.05029</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22810;&#32423;&#22810;&#31034;&#20363;&#23398;&#20064;&#29992;&#20110;&#20840;&#25195;&#25551;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Multi-level Multiple Instance Learning with Transformer for Whole Slide Image Classification. (arXiv:2306.05029v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#22810;&#32423;&#22810;&#31034;&#20363;&#23398;&#20064;&#65288;MMIL-Transformer&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#28041;&#21450;&#22823;&#37327;&#23454;&#20363;&#30340;MIL&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#25195;&#25551;&#22270;&#20687;&#65288;WSI&#65289;&#26159;&#19968;&#31181;&#39640;&#20998;&#36776;&#29575;&#30340;&#32452;&#32455;&#25195;&#25551;&#22270;&#20687;&#65292;&#34987;&#24191;&#27867;&#29992;&#20110;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#65288;CAD&#65289;&#12290;&#30001;&#20110;&#26497;&#39640;&#30340;&#20998;&#36776;&#29575;&#21644;&#21306;&#22495;&#32423;&#21035;&#27880;&#37322;&#30340;&#26377;&#38480;&#24615;&#65292;&#23545;&#20110;&#22522;&#20110;WSI&#30340;&#25968;&#23383;&#35786;&#26029;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22810;&#31034;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#26159;&#35299;&#20915;&#24369;&#27880;&#37322;&#38382;&#39064;&#30340;&#24378;&#26377;&#21147;&#24037;&#20855;&#65292;&#32780;Transformer&#24050;&#22312;&#35270;&#35273;&#20219;&#21153;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#32467;&#21512;&#20004;&#32773;&#23558;&#20026;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#35786;&#26029;&#25552;&#20379;&#26032;&#30340;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21333;&#32423;MIL&#30340;&#38480;&#21046;&#21644;&#27880;&#24847;&#26426;&#21046;&#23545;&#24207;&#21015;&#38271;&#24230;&#30340;&#38480;&#21046;&#65292;&#23558;Transformer&#30452;&#25509;&#24212;&#29992;&#20110;&#22522;&#20110;WSI&#30340;MIL&#20219;&#21153;&#24182;&#19981;&#23454;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32423;MIL&#19982;Transformer&#65288;MMIL-Transformer&#65289;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#23618;&#27425;&#32467;&#26500;&#21040;MIL&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#28041;&#21450;&#22823;&#37327;&#23454;&#20363;&#30340;MIL&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whole slide image (WSI) refers to a type of high-resolution scanned tissue image, which is extensively employed in computer-assisted diagnosis (CAD). The extremely high resolution and limited availability of region-level annotations make it challenging to employ deep learning methods for WSI-based digital diagnosis. Multiple instance learning (MIL) is a powerful tool to address the weak annotation problem, while Transformer has shown great success in the field of visual tasks. The combination of both should provide new insights for deep learning based image diagnosis. However, due to the limitations of single-level MIL and the attention mechanism's constraints on sequence length, directly applying Transformer to WSI-based MIL tasks is not practical. To tackle this issue, we propose a Multi-level MIL with Transformer (MMIL-Transformer) approach. By introducing a hierarchical structure to MIL, this approach enables efficient handling of MIL tasks that involve a large number of instances.
&lt;/p&gt;</description></item><item><title>AnoOnly&#26159;&#19968;&#20010;&#26032;&#30340;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#23545;&#27491;&#24120;&#25968;&#25454;&#30340;&#24369;&#30417;&#30563;&#24418;&#24335;&#26469;&#35299;&#20915;&#21516;&#36136;&#25968;&#25454;&#23545;&#24322;&#24120;&#30340;&#24433;&#21709;&#65292;&#20197;&#23454;&#29616;&#24179;&#34913;&#30340;&#30417;&#30563;&#12290;&#35813;&#26694;&#26550;&#22312;&#21508;&#31181;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36798;&#21040;&#20102;&#26032;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18798</link><description>&lt;p&gt;
AnoOnly:&#26080;&#38656;&#25439;&#22833;&#27491;&#24120;&#25968;&#25454;&#30340;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
AnoOnly: Semi-Supervised Anomaly Detection without Loss on Normal Data. (arXiv:2305.18798v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18798
&lt;/p&gt;
&lt;p&gt;
AnoOnly&#26159;&#19968;&#20010;&#26032;&#30340;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#23545;&#27491;&#24120;&#25968;&#25454;&#30340;&#24369;&#30417;&#30563;&#24418;&#24335;&#26469;&#35299;&#20915;&#21516;&#36136;&#25968;&#25454;&#23545;&#24322;&#24120;&#30340;&#24433;&#21709;&#65292;&#20197;&#23454;&#29616;&#24179;&#34913;&#30340;&#30417;&#30563;&#12290;&#35813;&#26694;&#26550;&#22312;&#21508;&#31181;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36798;&#21040;&#20102;&#26032;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;(SSAD)&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#23569;&#37327;&#20294;&#26377;&#25351;&#23548;&#20316;&#29992;&#30340;&#24322;&#24120;&#23454;&#20363;&#65292;&#22686;&#24378;&#20102;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;(UAD)&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#21516;&#36136;&#27491;&#24120;&#25968;&#25454;&#23545;&#24322;&#24120;&#30340;&#32479;&#27835;&#20351;&#24471;SSAD&#27169;&#22411;&#26080;&#27861;&#26377;&#25928;&#22320;&#24863;&#30693;&#24322;&#24120;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#22312;&#20005;&#37325;&#19981;&#24179;&#34913;&#30340;&#27491;&#24120;&#21644;&#24322;&#24120;&#25968;&#25454;&#20043;&#38388;&#23454;&#29616;&#24179;&#34913;&#30340;&#30417;&#30563;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;AnoOnly(&#20165;&#24322;&#24120;)&#30340;&#26032;&#26694;&#26550;&#12290;&#19982;&#29616;&#26377;&#30340;SSAD&#26041;&#27861;&#19981;&#21516;&#65292;AnoOnly&#26242;&#20572;&#20102;&#20005;&#26684;&#30340;&#25439;&#22833;&#30417;&#30563;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#23545;&#27491;&#24120;&#25968;&#25454;&#30340;&#24369;&#30417;&#30563;&#24418;&#24335;&#12290;&#36825;&#31181;&#24369;&#30417;&#30563;&#36890;&#36807;&#25209;&#37327;&#24402;&#19968;&#21270;&#23454;&#29616;&#65292;&#38544;&#24335;&#22320;&#23545;&#27491;&#24120;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#23398;&#20064;&#12290;&#24403;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;SSAD&#26041;&#27861;&#20013;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;AnoOnly&#22312;&#21508;&#31181;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36798;&#21040;&#20102;&#26032;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;A
&lt;/p&gt;
&lt;p&gt;
Semi-supervised anomaly detection (SSAD) methods have demonstrated their effectiveness in enhancing unsupervised anomaly detection (UAD) by leveraging few-shot but instructive abnormal instances. However, the dominance of homogeneous normal data over anomalies biases the SSAD models against effectively perceiving anomalies. To address this issue and achieve balanced supervision between heavily imbalanced normal and abnormal data, we develop a novel framework called AnoOnly (Anomaly Only). Unlike existing SSAD methods that resort to strict loss supervision, AnoOnly suspends it and introduces a form of weak supervision for normal data. This weak supervision is instantiated through the utilization of batch normalization, which implicitly performs cluster learning on normal data. When integrated into existing SSAD methods, the proposed AnoOnly demonstrates remarkable performance enhancements across various models and datasets, achieving new state-of-the-art performance. Additionally, our A
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#34394;&#25311;&#31890;&#23376;&#38543;&#26426;&#36924;&#36817;&#30340;&#21487;&#35777;&#36895;&#38480;&#21046;&#21464;&#31181;&#30340;SVGD&#31639;&#27861;&#65292;&#20855;&#26377;&#21487;&#35777;&#36895;&#30340;&#26377;&#38480;&#31890;&#23376;&#25910;&#25947;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.17558</link><description>&lt;p&gt;
&#22522;&#20110;&#34394;&#25311;&#31890;&#23376;&#38543;&#26426;&#36924;&#36817;&#30340;&#21487;&#35777;&#36895;&#38480;&#21046;&#21464;&#31181;&#30340;SVGD&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Provably Fast Finite Particle Variants of SVGD via Virtual Particle Stochastic Approximation. (arXiv:2305.17558v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#34394;&#25311;&#31890;&#23376;&#38543;&#26426;&#36924;&#36817;&#30340;&#21487;&#35777;&#36895;&#38480;&#21046;&#21464;&#31181;&#30340;SVGD&#31639;&#27861;&#65292;&#20855;&#26377;&#21487;&#35777;&#36895;&#30340;&#26377;&#38480;&#31890;&#23376;&#25910;&#25947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Stein&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#65288;SVGD&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#65292;&#23427;&#27169;&#25311;&#30456;&#20114;&#20316;&#29992;&#30340;&#31890;&#23376;&#31995;&#32479;&#20197;&#36817;&#20284;&#20174;&#30446;&#26631;&#20998;&#24067;&#20013;&#37319;&#26679;&#65292;&#20855;&#26377;&#21508;&#31181;&#39046;&#22495;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32463;&#39564;&#24615;&#33021;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#23427;&#30340;&#32676;&#20307;&#65288;&#21363;&#65292;&#26080;&#38480;&#31890;&#23376;&#65289;&#26497;&#38480;&#21160;&#21147;&#23398;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#30740;&#31350;&#65292;&#20294;&#26159;SVGD&#22312;&#26377;&#38480;&#31890;&#23376;&#20307;&#21046;&#19979;&#30340;&#34892;&#20026;&#21017;&#19981;&#22826;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;SVGD&#21464;&#20307;&#65292;&#21363;VP-SVGD&#65288;&#20174;&#27010;&#24565;&#19978;&#35762;&#24456;&#20248;&#38597;&#65289;&#21644;GB-SVGD&#65288;&#20174;&#32463;&#39564;&#19978;&#30475;&#24456;&#26377;&#25928;&#65289;&#65292;&#20855;&#26377;&#21487;&#35777;&#36895;&#30340;&#26377;&#38480;&#31890;&#23376;&#25910;&#25947;&#29575;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#34394;&#25311;&#31890;&#23376;&#8221;&#30340;&#27010;&#24565;&#65292;&#24182;&#22312;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#20013;&#24320;&#21457;&#20102;&#20154;&#21475;&#26497;&#38480;SVGD&#21160;&#21147;&#23398;&#30340;&#26032;&#22411;&#38543;&#26426;&#36924;&#36817;&#26041;&#27861;&#65292;&#23427;&#20204;&#21487;&#20197;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#31890;&#23376;&#31934;&#30830;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#30475;&#20316;&#26159;SVGD&#30340;&#29305;&#23450;&#38543;&#26426;&#25209;&#22788;&#29702;&#36924;&#36817;&#65292;&#27604;&#26222;&#36890;&#26041;&#27861;&#26356;&#20855;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stein Variational Gradient Descent (SVGD) is a popular variational inference algorithm which simulates an interacting particle system to approximately sample from a target distribution, with impressive empirical performance across various domains. Theoretically, its population (i.e, infinite-particle) limit dynamics is well studied but the behavior of SVGD in the finite-particle regime is much less understood. In this work, we design two computationally efficient variants of SVGD, namely VP-SVGD (which is conceptually elegant) and GB-SVGD (which is empirically effective), with provably fast finite-particle convergence rates. We introduce the notion of \emph{virtual particles} and develop novel stochastic approximations of population-limit SVGD dynamics in the space of probability measures, which are exactly implementable using a finite number of particles. Our algorithms can be viewed as specific random-batch approximations of SVGD, which are computationally more efficient than ordinar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26681;&#25454;&#36235;&#21183;&#21644;&#23395;&#33410;&#24615;&#20998;&#37327;&#22312;&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#29420;&#29305;&#24433;&#21709;&#25351;&#26631;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#32452;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#37319;&#29992; LightGBM &#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#22312;&#27779;&#23572;&#29595;&#38144;&#21806;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.17201</link><description>&lt;p&gt;
&#22522;&#20110; Trend &#21644; Seasonality &#20998;&#35299;&#21644; LightGBM &#30340;&#38144;&#21806;&#39044;&#27979;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improved Sales Forecasting using Trend and Seasonality Decomposition with LightGBM. (arXiv:2305.17201v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26681;&#25454;&#36235;&#21183;&#21644;&#23395;&#33410;&#24615;&#20998;&#37327;&#22312;&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#29420;&#29305;&#24433;&#21709;&#25351;&#26631;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#32452;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#37319;&#29992; LightGBM &#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#22312;&#27779;&#23572;&#29595;&#38144;&#21806;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#27779;&#23572;&#29595;&#21644;&#20122;&#39532;&#36874;&#31561;&#22823;&#22411;&#38646;&#21806;&#21830;&#38144;&#21806;&#39044;&#27979;&#30340;&#38590;&#28857;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#26681;&#25454;&#36235;&#21183;&#21644;&#23395;&#33410;&#24615;&#20998;&#37327;&#22312;&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#29420;&#29305;&#24433;&#21709;&#25351;&#26631;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#32452;&#65292;&#24182;&#37319;&#29992; LightGBM &#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#20998;&#32452;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#21644;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;MAPE&#65288;&#24179;&#22343;&#32477;&#23545;&#30334;&#20998;&#27604;&#35823;&#24046;&#65289;&#22312;&#27979;&#35797;&#38598;&#19978;&#21487;&#36798; 4.49%&#12290;
&lt;/p&gt;
&lt;p&gt;
Retail sales forecasting presents a significant challenge for large retailers such as Walmart and Amazon, due to the vast assortment of products, geographical location heterogeneity, seasonality, and external factors including weather, local economic conditions, and geopolitical events. Various methods have been employed to tackle this challenge, including traditional time series models, machine learning models, and neural network mechanisms, but the difficulty persists. Categorizing data into relevant groups has been shown to improve sales forecast accuracy as time series from different categories may exhibit distinct patterns. In this paper, we propose a new measure to indicate the unique impacts of the trend and seasonality components on a time series and suggest grouping time series based on this measure. We apply this approach to Walmart sales data from 01/29/2011 to 05/22/2016 and generate sales forecasts from 05/23/2016 to 06/19/2016. Our experiments show that the proposed strat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#21644;&#26377;&#25928;&#26816;&#27979;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#22686;&#21152;&#22403;&#22334;&#23454;&#20363;&#30340;&#22810;&#26679;&#24615;&#65292;&#22312;&#28145;&#20837;&#27700;&#19979;&#29615;&#22659;&#20013;&#25552;&#39640;&#20854;&#26816;&#27979;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.16460</link><description>&lt;p&gt;
&#20248;&#21270;&#30340;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#29992;&#20110;&#39640;&#25928;&#26816;&#27979;&#27700;&#19979;&#22403;&#22334;
&lt;/p&gt;
&lt;p&gt;
Optimized Custom Dataset for Efficient Detection of Underwater Trash. (arXiv:2305.16460v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#21644;&#26377;&#25928;&#26816;&#27979;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#22686;&#21152;&#22403;&#22334;&#23454;&#20363;&#30340;&#22810;&#26679;&#24615;&#65292;&#22312;&#28145;&#20837;&#27700;&#19979;&#29615;&#22659;&#20013;&#25552;&#39640;&#20854;&#26816;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#35780;&#20272;&#21644;&#28165;&#38500;&#28508;&#22312;&#30340;&#27700;&#19979;&#24223;&#29289;&#23545;&#20110;&#20445;&#25252;&#28023;&#27915;&#29983;&#29289;&#21644;&#29615;&#22659;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#38024;&#23545;&#27700;&#19979;&#22403;&#22334;&#26816;&#27979;&#25152;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#22914;&#20809;&#25240;&#23556;&#12289;&#21560;&#25910;&#12289;&#24748;&#28014;&#39063;&#31890;&#21644;&#33394;&#24425;&#25197;&#26354;&#31561;&#22240;&#32032;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#21644;&#26377;&#25928;&#26816;&#27979;&#26041;&#27861;&#12290;&#35813;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#22810;&#31181;&#27700;&#19979;&#29615;&#22659;&#65292;&#24182;&#21253;&#25324;&#23545;&#24223;&#24323;&#29289;&#23454;&#20363;&#30340;&#31934;&#30830;&#23450;&#20301;&#26631;&#27880;&#12290;&#26368;&#32456;&#65292;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#32467;&#26500;&#65292;&#30446;&#30340;&#26159;&#36890;&#36807;&#22686;&#21152;&#22403;&#22334;&#23454;&#20363;&#30340;&#22810;&#26679;&#24615;&#65292;&#22312;&#28145;&#20837;&#27700;&#19979;&#29615;&#22659;&#20013;&#25552;&#39640;&#20854;&#26816;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately quantifying and removing submerged underwater waste plays a crucial role in safeguarding marine life and preserving the environment. While detecting floating and surface debris is relatively straightforward, quantifying submerged waste presents significant challenges due to factors like light refraction, absorption, suspended particles, and color distortion. This paper addresses these challenges by proposing the development of a custom dataset and an efficient detection approach for submerged marine debris. The dataset encompasses diverse underwater environments and incorporates annotations for precise labeling of debris instances. Ultimately, the primary objective of this custom dataset is to enhance the diversity of litter instances and improve their detection accuracy in deep submerged environments by leveraging state-of-the-art deep learning architectures.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#21363;&#22810;&#23610;&#24230; KPPillarsBEV&#65292;&#20197;&#32531;&#35299;&#38647;&#36798;&#30446;&#26631;&#26816;&#27979;&#20013;&#20174;&#28857;&#20113;&#25968;&#25454;&#36716;&#21270;&#20026;&#32593;&#26684;&#32467;&#26500;&#36807;&#31243;&#20013;&#30340;&#20449;&#24687;&#20002;&#22833;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32593;&#26684;&#28210;&#26579;&#26041;&#27861; KPBEV&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.15836</link><description>&lt;p&gt;
&#29992;&#20110;&#38647;&#36798;&#30446;&#26631;&#26816;&#27979;&#30340;&#28857;&#20113;&#22810;&#23610;&#24230;&#32593;&#26684;&#28210;&#26579;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improved Multi-Scale Grid Rendering of Point Clouds for Radar Object Detection Networks. (arXiv:2305.15836v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#21363;&#22810;&#23610;&#24230; KPPillarsBEV&#65292;&#20197;&#32531;&#35299;&#38647;&#36798;&#30446;&#26631;&#26816;&#27979;&#20013;&#20174;&#28857;&#20113;&#25968;&#25454;&#36716;&#21270;&#20026;&#32593;&#26684;&#32467;&#26500;&#36807;&#31243;&#20013;&#30340;&#20449;&#24687;&#20002;&#22833;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32593;&#26684;&#28210;&#26579;&#26041;&#27861; KPBEV&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#28857;&#20113;&#36716;&#25442;&#20026;&#32593;&#26684;&#34920;&#24449;&#65292;&#28982;&#21518;&#24212;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#21487;&#29992;&#20110;&#38647;&#36798;&#30446;&#26631;&#26816;&#27979;&#65292;&#20294;&#20174;&#19981;&#35268;&#21017;&#30340;&#28857;&#20113;&#25968;&#25454;&#36716;&#25442;&#20026;&#23494;&#38598;&#30340;&#32593;&#26684;&#32467;&#26500;&#24120;&#24120;&#20250;&#23548;&#33268;&#20449;&#24687;&#30340;&#20002;&#22833;&#65292;&#36825;&#26159;&#30001;&#20110;&#28857;&#30340;&#31163;&#25955;&#21270;&#21644;&#32858;&#21512;&#36896;&#25104;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#21363;&#22810;&#23610;&#24230; KPPillarsBEV&#65292;&#20197;&#32531;&#35299;&#32593;&#26684;&#28210;&#26579;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32593;&#26684;&#28210;&#26579;&#26041;&#27861; KPBEV&#65292;&#23427;&#21033;&#29992;&#26680;&#28857;&#21367;&#31215;&#30340;&#25551;&#36848;&#33021;&#21147;&#26469;&#25913;&#36827;&#32593;&#26684;&#28210;&#26579;&#36807;&#31243;&#20013;&#23616;&#37096;&#28857;&#20113;&#19978;&#19979;&#25991;&#30340;&#32534;&#30721;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#22810;&#23610;&#24230;&#32593;&#26684;&#28210;&#26579;&#20844;&#24335;&#65292;&#20197;&#20219;&#24847;&#32593;&#26684;&#28210;&#26579;&#26041;&#27861;&#23558;&#22810;&#23610;&#24230;&#29305;&#24449;&#26144;&#23556;&#34701;&#21512;&#21040;&#26816;&#27979;&#32593;&#32476;&#30340;&#21367;&#31215;&#39592;&#24178;&#20013;&#12290;&#25105;&#20204;&#22312; nuScenes &#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#35780;&#20272;&#20102;&#26816;&#27979;&#27773;&#36710;&#12289;&#21345;&#36710;&#21644;&#20844;&#20132;&#36710;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#22810;&#23610;&#24230; KPPillarsBEV &#32467;&#26500;&#21644; KPBEV &#32593;&#26684;&#28210;&#26579;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Architectures that first convert point clouds to a grid representation and then apply convolutional neural networks achieve good performance for radar-based object detection. However, the transfer from irregular point cloud data to a dense grid structure is often associated with a loss of information, due to the discretization and aggregation of points. In this paper, we propose a novel architecture, multi-scale KPPillarsBEV, that aims to mitigate the negative effects of grid rendering. Specifically, we propose a novel grid rendering method, KPBEV, which leverages the descriptive power of kernel point convolutions to improve the encoding of local point cloud contexts during grid rendering. In addition, we propose a general multi-scale grid rendering formulation to incorporate multi-scale feature maps into convolutional backbones of detection networks with arbitrary grid rendering methods. We perform extensive experiments on the nuScenes dataset and evaluate the methods in terms of dete
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#25439;&#22833;&#40657;&#22622;&#30697;&#38453;&#21644;&#36755;&#20837;-&#36755;&#20986;&#38597;&#20811;&#27604;&#30697;&#38453;&#32852;&#31995;&#36215;&#26469;&#30340;&#20551;&#35774;&#65292;&#37327;&#21270;&#20102;&#27169;&#22411;&#30340;&#36755;&#20837;-&#36755;&#20986;&#38597;&#20811;&#27604;&#30697;&#38453;&#36817;&#20284;&#20854;&#22312;&#25968;&#25454;&#20998;&#24067;&#19978;&#30340;&#21033;&#26222;&#35199;&#33576;&#33539;&#25968;&#30340;&#31243;&#24230;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32463;&#39564;&#38597;&#20811;&#27604;&#30697;&#38453;&#30340;&#26032;&#30340;&#27867;&#21270;&#30028;&#65292;&#32473;&#20986;&#20102;&#20851;&#20110;&#36827;&#21270;&#30952;&#38155;&#21644;&#24179;&#22374;&#26497;&#23567;&#30340;&#27867;&#21270;&#24615;&#36136;&#30340;&#26032;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2305.14683</link><description>&lt;p&gt;
&#35770;&#36827;&#21270;&#30952;&#38155;&#12289;&#24179;&#22374;&#26497;&#23567;&#21644;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
On progressive sharpening, flat minima and generalisation. (arXiv:2305.14683v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#25439;&#22833;&#40657;&#22622;&#30697;&#38453;&#21644;&#36755;&#20837;-&#36755;&#20986;&#38597;&#20811;&#27604;&#30697;&#38453;&#32852;&#31995;&#36215;&#26469;&#30340;&#20551;&#35774;&#65292;&#37327;&#21270;&#20102;&#27169;&#22411;&#30340;&#36755;&#20837;-&#36755;&#20986;&#38597;&#20811;&#27604;&#30697;&#38453;&#36817;&#20284;&#20854;&#22312;&#25968;&#25454;&#20998;&#24067;&#19978;&#30340;&#21033;&#26222;&#35199;&#33576;&#33539;&#25968;&#30340;&#31243;&#24230;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32463;&#39564;&#38597;&#20811;&#27604;&#30697;&#38453;&#30340;&#26032;&#30340;&#27867;&#21270;&#30028;&#65292;&#32473;&#20986;&#20102;&#20851;&#20110;&#36827;&#21270;&#30952;&#38155;&#21644;&#24179;&#22374;&#26497;&#23567;&#30340;&#27867;&#21270;&#24615;&#36136;&#30340;&#26032;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#20013;&#25439;&#22833;&#26354;&#29575;&#19982;&#27867;&#21270;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#29616;&#26377;&#30340;&#28145;&#24230;&#32593;&#32476;&#25439;&#22833;&#40657;&#22622;&#30697;&#38453;&#39057;&#35889;&#32463;&#39564;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#25439;&#22833;&#40657;&#22622;&#30697;&#38453;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;-&#36755;&#20986;&#38597;&#20811;&#27604;&#30697;&#38453;&#32852;&#31995;&#36215;&#26469;&#30340;&#20551;&#35774;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#31995;&#21015;&#29702;&#35770;&#32467;&#26524;&#65292;&#37327;&#21270;&#20102;&#27169;&#22411;&#30340;&#36755;&#20837;-&#36755;&#20986;&#38597;&#20811;&#27604;&#30697;&#38453;&#36817;&#20284;&#20854;&#22312;&#25968;&#25454;&#20998;&#24067;&#19978;&#30340;&#21033;&#26222;&#35199;&#33576;&#33539;&#25968;&#30340;&#31243;&#24230;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32463;&#39564;&#38597;&#20811;&#27604;&#30697;&#38453;&#30340;&#26032;&#30340;&#27867;&#21270;&#30028;&#12290;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#30340;&#20551;&#35774;&#21644;&#29702;&#35770;&#32467;&#26524;&#65292;&#32473;&#20986;&#20102;&#20851;&#20110;&#26368;&#36817;&#35266;&#23519;&#21040;&#30340;&#36827;&#21270;&#30952;&#38155;&#29616;&#35937;&#20197;&#21450;&#24179;&#22374;&#26497;&#23567;&#30340;&#27867;&#21270;&#24615;&#36136;&#30340;&#26032;&#25551;&#36848;&#12290;&#23454;&#39564;&#35777;&#25454;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#20027;&#24352;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new approach to understanding the relationship between loss curvature and generalisation in deep learning. Specifically, we use existing empirical analyses of the spectrum of deep network loss Hessians to ground an ansatz tying together the loss Hessian and the input-output Jacobian of a deep neural network. We then prove a series of theoretical results which quantify the degree to which the input-output Jacobian of a model approximates its Lipschitz norm over a data distribution, and deduce a novel generalisation bound in terms of the empirical Jacobian. We use our ansatz, together with our theoretical results, to give a new account of the recently observed progressive sharpening phenomenon, as well as the generalisation properties of flat minima. Experimental evidence is provided to validate our claims.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24179;&#24615;&#26694;&#26550;&#65292;&#36890;&#36807;&#26799;&#24230;&#35828;&#26126;&#25214;&#21040;&#20004;&#20010;&#27169;&#22411;&#28966;&#28857;&#36827;&#34892;&#21435;&#20559;&#35265;&#22788;&#29702;&#65292;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.12178</link><description>&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#35828;&#26126;&#30340;&#34920;&#31034;&#27861;&#21435;&#20559;&#35265;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Model Debiasing via Gradient-based Explanation on Representation. (arXiv:2305.12178v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12178
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24179;&#24615;&#26694;&#26550;&#65292;&#36890;&#36807;&#26799;&#24230;&#35828;&#26126;&#25214;&#21040;&#20004;&#20010;&#27169;&#22411;&#28966;&#28857;&#36827;&#34892;&#21435;&#20559;&#35265;&#22788;&#29702;&#65292;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20250;&#23545;&#26576;&#20123;&#20154;&#21475;&#32479;&#35745;&#23398;&#32676;&#20307;&#20135;&#29983;&#20559;&#35265;&#65292;&#21363;&#19981;&#20844;&#24179;&#29616;&#35937;&#12290;&#36817;&#26399;&#30340;&#35299;&#20915;&#26041;&#27861;&#26159;&#36890;&#36807;&#20998;&#31163;&#24335;&#34920;&#31034;&#23398;&#20064;&#23398;&#20064;&#28508;&#22312;&#30721;&#65288;&#21363;&#34920;&#31034;&#27861;&#65289;&#65292;&#28982;&#21518;&#20002;&#24323;&#19982;&#25935;&#24863;&#23646;&#24615;&#65288;&#22914;&#24615;&#21035;&#65289;&#30456;&#20851;&#30340;&#30721;&#12290;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#22788;&#29702;&#29616;&#23454;&#25968;&#25454;&#65288;&#29305;&#21035;&#26159;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#65289;&#26102;&#65292;&#21487;&#33021;&#20250;&#36951;&#28431;&#20195;&#29702;&#23646;&#24615;&#65288;&#25935;&#24863;&#23646;&#24615;&#30340;&#20195;&#29702;&#65289;&#65292;&#24182;&#19988;&#21463;&#21040;&#19981;&#23436;&#20840;&#20998;&#31163;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#20844;&#24179;&#24615;&#33021;&#19979;&#38477;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#25439;&#22833;&#26377;&#29992;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24179;&#24615;&#26694;&#26550;&#65292;&#38024;&#23545;&#25935;&#24863;&#23646;&#24615;&#21644;&#20195;&#29702;&#23646;&#24615;&#36827;&#34892;&#21435;&#20559;&#35265;&#22788;&#29702;&#65292;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#32780;&#19981;&#38656;&#35201;&#23436;&#20840;&#20998;&#31163;&#12290;&#20027;&#35201;&#24605;&#36335;&#26159;&#21033;&#29992;&#26799;&#24230;&#35828;&#26126;&#25214;&#21040;&#20004;&#20010;&#27169;&#22411;&#28966;&#28857;&#65306;1&#65289;&#20854;&#20013;&#19968;&#20010;&#28966;&#28857;&#29992;&#20110;&#39044;&#27979;&#20540;&#65292;2&#65289;&#21478;&#19968;&#20010;&#28966;&#28857;&#29992;&#20110;&#20195;&#29702;&#23646;&#24615;&#65292;&#28982;&#21518;&#23545;&#28508;&#22312;&#30721;&#36827;&#34892;&#20462;&#27491;&#20197;&#20943;&#36731;&#36825;&#20123;&#23646;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning systems produce biased results towards certain demographic groups, known as the fairness problem. Recent approaches to tackle this problem learn a latent code (i.e., representation) through disentangled representation learning and then discard the latent code dimensions correlated with sensitive attributes (e.g., gender). Nevertheless, these approaches may suffer from incomplete disentanglement and overlook proxy attributes (proxies for sensitive attributes) when processing real-world data, especially for unstructured data, causing performance degradation in fairness and loss of useful information for downstream tasks. In this paper, we propose a novel fairness framework that performs debiasing with regard to both sensitive attributes and proxy attributes, which boosts the prediction performance of downstream task models without complete disentanglement. The main idea is to, first, leverage gradient-based explanation to find two model focuses, 1) one focus for predicti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#38543;&#26426;&#25628;&#32034;&#21450;&#20854;&#24615;&#33021;&#65292;&#24341;&#20837;&#20102;&#8220;&#25955;&#23556;&#32500;&#24230;&#8221;&#30340;&#27010;&#24565;&#65292;&#25551;&#36848;&#20102;&#24213;&#23618;&#20989;&#25968;&#30340;&#29366;&#24577;&#65292;&#37327;&#21270;&#20102;&#38543;&#26426;&#25628;&#32034;&#30340;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#26080;&#22122;&#22768;&#21644;&#26377;&#30028;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#36755;&#20986;&#20998;&#21035;&#20197;&#19968;&#23450;&#27010;&#29575;&#25910;&#25947;&#21040;&#26368;&#20248;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.11509</link><description>&lt;p&gt;
&#20174;&#38543;&#26426;&#25628;&#32034;&#21040;&#24230;&#37327;&#27979;&#24230;&#31354;&#38388;&#20013;&#30340;&#36172;&#21338;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
From Random Search to Bandit Learning in Metric Measure Spaces. (arXiv:2305.11509v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38543;&#26426;&#25628;&#32034;&#21450;&#20854;&#24615;&#33021;&#65292;&#24341;&#20837;&#20102;&#8220;&#25955;&#23556;&#32500;&#24230;&#8221;&#30340;&#27010;&#24565;&#65292;&#25551;&#36848;&#20102;&#24213;&#23618;&#20989;&#25968;&#30340;&#29366;&#24577;&#65292;&#37327;&#21270;&#20102;&#38543;&#26426;&#25628;&#32034;&#30340;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#26080;&#22122;&#22768;&#21644;&#26377;&#30028;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#36755;&#20986;&#20998;&#21035;&#20197;&#19968;&#23450;&#27010;&#29575;&#25910;&#25947;&#21040;&#26368;&#20248;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#25628;&#32034;&#26159;&#36229;&#21442;&#25968;&#20248;&#21270;&#20013;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#20043;&#19968;&#65292;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#20854;&#24615;&#33021;&#20196;&#20154;&#24778;&#21497;&#65292;&#20294;&#24456;&#23569;&#26377;&#38750;&#21551;&#21457;&#24335;&#30340;&#29702;&#35770;&#29992;&#20110;&#25551;&#36848;&#20854;&#24037;&#20316;&#26426;&#21046;&#12290;&#26412;&#25991;&#32473;&#20986;&#20102;&#20851;&#20110;&#38543;&#26426;&#25628;&#32034;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#25955;&#23556;&#32500;&#24230;&#8221;&#30340;&#27010;&#24565;&#65292;&#25551;&#36848;&#20102;&#24213;&#23618;&#20989;&#25968;&#30340;&#29366;&#24577;&#65292;&#24182;&#37327;&#21270;&#20102;&#38543;&#26426;&#25628;&#32034;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#24403;&#29615;&#22659;&#27809;&#26377;&#22122;&#22768;&#26102;&#65292;&#38543;&#26426;&#25628;&#32034;&#30340;&#36755;&#20986;&#20197;&#27010;&#29575;&#25910;&#25947;&#21040;&#26368;&#20248;&#20540;&#65292;&#20854;&#36895;&#29575;&#20026;$ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T} \right)^{ \frac{1}{d_s} } \right) $&#65292;&#20854;&#20013;$ d_s \ge 0 $&#26159;&#24213;&#23618;&#20989;&#25968;&#30340;&#25955;&#23556;&#32500;&#24230;&#12290;&#24403;&#35266;&#23519;&#21040;&#30340;&#20989;&#25968;&#20540;&#21463;&#21040;&#26377;&#30028;&#30340;&#29420;&#31435;&#21516;&#20998;&#24067;&#22122;&#22768;&#24433;&#21709;&#26102;&#65292;&#38543;&#26426;&#25628;&#32034;&#30340;&#36755;&#20986;&#20197;&#27010;&#29575;&#25910;&#25947;&#21040;&#26368;&#20248;&#20540;&#65292;&#36895;&#29575;&#20026;$ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T} \right)^{ \frac{2}{2+d_s} } \right) $&#12290;
&lt;/p&gt;
&lt;p&gt;
Random Search is one of the most widely-used method for Hyperparameter Optimization, and is critical to the success of deep learning models. Despite its astonishing performance, little non-heuristic theory has been developed to describe the underlying working mechanism. This paper gives a theoretical accounting of Random Search. We introduce the concept of \emph{scattering dimension} that describes the landscape of the underlying function, and quantifies the performance of random search. We show that, when the environment is noise-free, the output of random search converges to the optimal value in probability at rate $ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T} \right)^{ \frac{1}{d_s} } \right) $, where $ d_s \ge 0 $ is the scattering dimension of the underlying function. When the observed function values are corrupted by bounded $iid$ noise, the output of random search converges to the optimal value in probability at rate $ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T} \rig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#29699;&#24418;&#36127;&#24863;&#30693;&#22120;&#27169;&#22411;&#30340;&#35299;&#31354;&#38388;&#23637;&#24320;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#21306;&#22495;&#20869;&#35299;&#20915;&#26041;&#26696;&#27969;&#24418;&#26174;&#31034;&#20986;&#31616;&#21333;&#30340;&#36830;&#36890;&#24615;&#36136;&#12290;&#23384;&#22312;&#19968;&#20010;&#22823;&#30340;&#27979;&#22320;&#20984;&#25104;&#20998;&#65292;&#23545;&#21508;&#31181;&#20248;&#21270;&#21160;&#21147;&#23398;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#20854;&#20013;&#21448;&#26377;&#19968;&#20010;&#19982;&#22823;&#22810;&#25968;&#20854;&#20182;&#35299;&#20915;&#26041;&#26696;&#27979;&#22320;&#36830;&#25509;&#30340;&#38750;&#20856;&#22411;&#40065;&#26834;&#35299;&#20915;&#26041;&#26696;&#30340;&#23376;&#38598;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#26143;&#24418;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;</title><link>http://arxiv.org/abs/2305.10623</link><description>&lt;p&gt;
&#29699;&#24418;&#36127;&#24863;&#30693;&#22120;&#30340;&#35299;&#31354;&#38388;&#30340;&#26143;&#24418;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
The star-shaped space of solutions of the spherical negative perceptron. (arXiv:2305.10623v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#29699;&#24418;&#36127;&#24863;&#30693;&#22120;&#27169;&#22411;&#30340;&#35299;&#31354;&#38388;&#23637;&#24320;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#21306;&#22495;&#20869;&#35299;&#20915;&#26041;&#26696;&#27969;&#24418;&#26174;&#31034;&#20986;&#31616;&#21333;&#30340;&#36830;&#36890;&#24615;&#36136;&#12290;&#23384;&#22312;&#19968;&#20010;&#22823;&#30340;&#27979;&#22320;&#20984;&#25104;&#20998;&#65292;&#23545;&#21508;&#31181;&#20248;&#21270;&#21160;&#21147;&#23398;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#20854;&#20013;&#21448;&#26377;&#19968;&#20010;&#19982;&#22823;&#22810;&#25968;&#20854;&#20182;&#35299;&#20915;&#26041;&#26696;&#27979;&#22320;&#36830;&#25509;&#30340;&#38750;&#20856;&#22411;&#40065;&#26834;&#35299;&#20915;&#26041;&#26696;&#30340;&#23376;&#38598;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#26143;&#24418;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#31070;&#32463;&#32593;&#32476;&#26223;&#35266;&#30340;&#32463;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#20302;&#33021;&#37197;&#32622;&#36890;&#24120;&#20986;&#29616;&#22312;&#22797;&#26434;&#30340;&#36830;&#36890;&#32467;&#26500;&#20013;&#65292;&#22312;&#37027;&#37324;&#21487;&#20197;&#26500;&#24314;&#36828;&#36317;&#31163;&#35299;&#20043;&#38388;&#30340;&#38646;&#33021;&#36335;&#24452;&#12290;&#26412;&#25991;&#30740;&#31350;&#29699;&#24418;&#36127;&#24863;&#30693;&#22120;&#65292;&#19968;&#20010;&#20316;&#20026;&#36830;&#32493;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#30340;&#20856;&#22411;&#38750;&#20984;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#35745;&#31639;&#20174;&#24179;&#34913;&#28857;&#37319;&#26679;&#30340;&#39030;&#28857;&#37197;&#32622;&#30340;&#21333;&#32431;&#24418;&#20013;&#33021;&#37327;&#38556;&#30861;&#30340;&#36890;&#29992;&#20998;&#26512;&#26041;&#27861;&#12290;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#21306;&#22495;&#20869;&#65292;&#35299;&#20915;&#26041;&#26696;&#27969;&#24418;&#26174;&#31034;&#20986;&#31616;&#21333;&#30340;&#36830;&#36890;&#24615;&#36136;&#12290;&#23384;&#22312;&#19968;&#20010;&#22823;&#30340;&#27979;&#22320;&#20984;&#25104;&#20998;&#65292;&#23545;&#21508;&#31181;&#20248;&#21270;&#21160;&#21147;&#23398;&#20855;&#26377;&#21560;&#24341;&#21147;&#12290;&#22312;&#36825;&#20010;&#21306;&#22495;&#20869;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20010;&#19982;&#22823;&#22810;&#25968;&#20854;&#20182;&#35299;&#20915;&#26041;&#26696;&#27979;&#22320;&#36830;&#25509;&#30340;&#38750;&#20856;&#22411;&#40065;&#26834;&#35299;&#20915;&#26041;&#26696;&#30340;&#23376;&#38598;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#26143;&#24418;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#25105;&#20204;&#20998;&#26512;&#24615;&#22320;&#34920;&#24449;&#20102;&#35299;&#31354;&#38388;&#30340;&#36830;&#25509;&#32467;&#26500;&#32452;&#32455;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empirical studies on the landscape of neural networks have shown that low-energy configurations are often found in complex connected structures, where zero-energy paths between pairs of distant solutions can be constructed. Here we consider the spherical negative perceptron, a prototypical non-convex neural network model framed as a continuous constraint satisfaction problem. We introduce a general analytical method for computing energy barriers in the simplex with vertex configurations sampled from the equilibrium. We find that in the over-parameterized regime the solution manifold displays simple connectivity properties. There exists a large geodesically convex component that is attractive for a wide range of optimization dynamics. Inside this region we identify a subset of atypically robust solutions that are geodesically connected with most other solutions, giving rise to a star-shaped geometry. We analytically characterize the organization of the connected space of solutions and s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#32570;&#22833;&#25968;&#25454;&#22788;&#29702;&#26041;&#27861;&#23545;&#30456;&#20851;&#22270;&#30340;&#24433;&#21709;&#65292;&#24314;&#35758;&#20351;&#29992;&#30452;&#25509;&#21442;&#25968;&#20272;&#35745;&#27861;(DPER)&#26469;&#32472;&#21046;&#30456;&#20851;&#22270;</title><link>http://arxiv.org/abs/2305.06044</link><description>&lt;p&gt;
&#32570;&#22833;&#20540;&#19979;&#30340;&#30456;&#20851;&#24615;&#21487;&#35270;&#21270;&#65306;&#22635;&#20805;&#27861;&#21644;&#30452;&#25509;&#21442;&#25968;&#20272;&#35745;&#27861;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Correlation visualization under missing values: a comparison between imputation and direct parameter estimation methods. (arXiv:2305.06044v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#32570;&#22833;&#25968;&#25454;&#22788;&#29702;&#26041;&#27861;&#23545;&#30456;&#20851;&#22270;&#30340;&#24433;&#21709;&#65292;&#24314;&#35758;&#20351;&#29992;&#30452;&#25509;&#21442;&#25968;&#20272;&#35745;&#27861;(DPER)&#26469;&#32472;&#21046;&#30456;&#20851;&#22270;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20851;&#30697;&#38453;&#21487;&#35270;&#21270;&#23545;&#20110;&#29702;&#35299;&#25968;&#25454;&#38598;&#20013;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#26159;&#32570;&#22833;&#25968;&#25454;&#20250;&#23545;&#30456;&#20851;&#31995;&#25968;&#30340;&#20272;&#35745;&#20135;&#29983;&#26174;&#33879;&#25361;&#25112;&#12290;&#26412;&#25991;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#32570;&#22833;&#25968;&#25454;&#22788;&#29702;&#26041;&#27861;&#23545;&#30456;&#20851;&#22270;&#30340;&#24433;&#21709;&#65292;&#37325;&#28857;&#20851;&#27880;&#20004;&#31181;&#24120;&#35265;&#30340;&#32570;&#22833;&#27169;&#24335;&#65306;&#38543;&#26426;&#21644;&#21333;&#35843;&#12290;&#25105;&#20204;&#26088;&#22312;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#25552;&#20379;&#23454;&#29992;&#30340;&#31574;&#30053;&#21644;&#24314;&#35758;&#65292;&#20197;&#21019;&#24314;&#21644;&#20998;&#26512;&#30456;&#20851;&#22270;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#22635;&#20805;&#27861;&#36890;&#24120;&#29992;&#20110;&#32570;&#22833;&#25968;&#25454;&#65292;&#20294;&#20351;&#29992;&#22635;&#20805;&#30340;&#25968;&#25454;&#26469;&#29983;&#25104;&#30456;&#20851;&#30697;&#38453;&#22270;&#21487;&#33021;&#20250;&#23548;&#33268;&#23545;&#29305;&#24449;&#20043;&#38388;&#20851;&#31995;&#30340;&#35823;&#23548;&#24615;&#25512;&#26029;&#12290;&#25105;&#20204;&#24314;&#35758;&#22522;&#20110;&#20854;&#22312;&#23454;&#39564;&#20013;&#30340;&#34920;&#29616;&#65292;&#20351;&#29992;DPER&#65292;&#19968;&#31181;&#30452;&#25509;&#21442;&#25968;&#20272;&#35745;&#26041;&#27861;&#65292;&#32472;&#21046;&#30456;&#20851;&#30697;&#38453;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Correlation matrix visualization is essential for understanding the relationships between variables in a dataset, but missing data can pose a significant challenge in estimating correlation coefficients. In this paper, we compare the effects of various missing data methods on the correlation plot, focusing on two common missing patterns: random and monotone. We aim to provide practical strategies and recommendations for researchers and practitioners in creating and analyzing the correlation plot. Our experimental results suggest that while imputation is commonly used for missing data, using imputed data for plotting the correlation matrix may lead to a significantly misleading inference of the relation between the features. We recommend using DPER, a direct parameter estimation approach, for plotting the correlation matrix based on its performance in the experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#35268;&#33539;&#21270;&#25289;&#26222;&#25289;&#26031;&#22270;&#30340;&#21551;&#21457;&#30340;&#30697;&#38453;&#32553;&#25918;&#26041;&#27861;&#65292;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#22312;&#25991;&#26412;&#20027;&#39064;&#27169;&#22411;&#20013;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.05389</link><description>&lt;p&gt;
&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#20013;&#30340;&#20004;&#21040;&#20116;&#20010;&#30495;&#30456;
&lt;/p&gt;
&lt;p&gt;
Two to Five Truths in Non-Negative Matrix Factorization. (arXiv:2305.05389v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#35268;&#33539;&#21270;&#25289;&#26222;&#25289;&#26031;&#22270;&#30340;&#21551;&#21457;&#30340;&#30697;&#38453;&#32553;&#25918;&#26041;&#27861;&#65292;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#22312;&#25991;&#26412;&#20027;&#39064;&#27169;&#22411;&#20013;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#20351;&#29992;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#26500;&#24314;&#20027;&#39064;&#27169;&#22411;&#26102;&#65292;&#30697;&#38453;&#32553;&#25918;&#22312;&#35745;&#25968;&#30697;&#38453;&#19978;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#35268;&#33539;&#21270;&#25289;&#26222;&#25289;&#26031;&#22270;&#65288;NL&#65289; &#30340;&#21551;&#21457;&#30340;&#30697;&#38453;&#32553;&#25918;&#26041;&#27861;&#65292;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#30340;&#36136;&#37327;&#12290;&#22312;&#25991;&#26412;&#20998;&#26512;&#20013;&#65292;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299; (NMF) &#36890;&#24120;&#29992;&#20110;&#35745;&#25968;&#30697;&#38453;&#30340;&#20849;&#29616;&#8220;&#19978;&#19979;&#25991;&#8221;&#21644;&#8220;&#26415;&#35821;&#8221;&#12290;&#21463; LSE &#30340;&#21551;&#21457;&#65292;&#30697;&#38453;&#32553;&#25918;&#23545;&#21508;&#31181;&#25968;&#25454;&#38598;&#20013;&#30340;&#25991;&#26412;&#20027;&#39064;&#27169;&#22411;&#37117;&#26377;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#30697;&#38453;&#32553;&#25918;&#22312; NMF &#20013;&#30340;&#24040;&#22823;&#24433;&#21709;&#65292;&#21487;&#20197;&#22823;&#22823;&#25913;&#21892;&#20027;&#39064;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore the role of matrix scaling on a matrix of counts when building a topic model using non-negative matrix factorization. We present a scaling inspired by the normalized Laplacian (NL) for graphs that can greatly improve the quality of a non-negative matrix factorization. The results parallel those in the spectral graph clustering work of \cite{Priebe:2019}, where the authors proved adjacency spectral embedding (ASE) spectral clustering was more likely to discover core-periphery partitions and Laplacian Spectral Embedding (LSE) was more likely to discover affinity partitions. In text analysis non-negative matrix factorization (NMF) is typically used on a matrix of co-occurrence ``contexts'' and ``terms" counts. The matrix scaling inspired by LSE gives significant improvement for text topic models in a variety of datasets. We illustrate the dramatic difference a matrix scalings in NMF can greatly improve the quality of a topic model on three datasets where human an
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DELTA&#30340;CTR&#27169;&#22411;&#65292;&#20351;&#29992;&#25130;&#26029;&#24847;&#35782;&#27880;&#24847;&#21147;&#36827;&#34892;&#21160;&#24577;&#23884;&#20837;&#23398;&#20064;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#19978;&#19979;&#25991;&#20013;&#26080;&#25928;&#21644;&#20887;&#20313;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.04891</link><description>&lt;p&gt;
&#24102;&#26377;&#25130;&#26029;&#24847;&#35782;&#27880;&#24847;&#21147;&#30340;&#21160;&#24577;&#23884;&#20837;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;CTR&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
DELTA: Dynamic Embedding Learning with Truncated Conscious Attention for CTR Prediction. (arXiv:2305.04891v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04891
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DELTA&#30340;CTR&#27169;&#22411;&#65292;&#20351;&#29992;&#25130;&#26029;&#24847;&#35782;&#27880;&#24847;&#21147;&#36827;&#34892;&#21160;&#24577;&#23884;&#20837;&#23398;&#20064;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#19978;&#19979;&#25991;&#20013;&#26080;&#25928;&#21644;&#20887;&#20313;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#26159;&#20135;&#21697;&#21644;&#20869;&#23481;&#25512;&#33616;&#20013;&#20851;&#38190;&#30340;&#20219;&#21153;&#65292;&#23398;&#20064;&#26377;&#25928;&#30340;&#29305;&#24449;&#23884;&#20837;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#23398;&#20064;&#22266;&#23450;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#32780;&#32570;&#20047;&#26681;&#25454;&#19978;&#19979;&#25991;&#20449;&#24687;&#21160;&#24577;&#35843;&#25972;&#29305;&#24449;&#34920;&#31034;&#30340;&#26426;&#21046;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#19968;&#20123;&#36817;&#26399;&#30340;&#26041;&#27861;&#23581;&#35797;&#36890;&#36807;&#23398;&#20064;&#20301;&#26435;&#37325;&#25110;&#22686;&#24378;&#23884;&#20837;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#26159;&#21463;&#21040;&#19978;&#19979;&#25991;&#20013;&#26080;&#20449;&#24687;&#25110;&#20887;&#20313;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#24847;&#35782;&#21152;&#24037;&#20013;&#20840;&#23616;&#24037;&#20316;&#21306;&#29702;&#35770;&#65292;&#35813;&#29702;&#35770;&#35748;&#20026;&#21482;&#26377;&#29305;&#23450;&#30340;&#20135;&#21697;&#29305;&#24449;&#19982;&#28857;&#20987;&#34892;&#20026;&#30456;&#20851;&#65292;&#20854;&#20313;&#29305;&#24449;&#21487;&#33021;&#20250;&#22122;&#38899;&#24178;&#25200;&#65292;&#29978;&#33267;&#26377;&#23475;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#25130;&#26029;&#24847;&#35782;&#27880;&#24847;&#21147;&#30340;&#21160;&#24577;&#23884;&#20837;&#23398;&#20064;&#27169;&#22411;DELTA&#36827;&#34892;CTR&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Click-Through Rate (CTR) prediction is a pivotal task in product and content recommendation, where learning effective feature embeddings is of great significance. However, traditional methods typically learn fixed feature representations without dynamically refining feature representations according to the context information, leading to suboptimal performance. Some recent approaches attempt to address this issue by learning bit-wise weights or augmented embeddings for feature representations, but suffer from uninformative or redundant features in the context. To tackle this problem, inspired by the Global Workspace Theory in conscious processing, which posits that only a specific subset of the product features are pertinent while the rest can be noisy and even detrimental to human-click behaviors, we propose a CTR model that enables Dynamic Embedding Learning with Truncated Conscious Attention for CTR prediction, termed DELTA. DELTA contains two key components: (I) conscious truncatio
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;ClusterNet&#65292;&#19968;&#31181;&#22522;&#20110;&#24863;&#30693;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#32858;&#31867;&#27169;&#22411;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#22522;&#20110;&#28857;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21453;&#26144;&#20154;&#31867;&#24863;&#30693;&#30340;&#32858;&#31867;&#21487;&#20998;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.14185</link><description>&lt;p&gt;
ClusterNet&#65306;&#19968;&#31181;&#22522;&#20110;&#24863;&#30693;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#32858;&#31867;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ClusterNet: A Perception-Based Clustering Model for Scattered Data. (arXiv:2304.14185v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14185
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;ClusterNet&#65292;&#19968;&#31181;&#22522;&#20110;&#24863;&#30693;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#32858;&#31867;&#27169;&#22411;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#22522;&#20110;&#28857;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21453;&#26144;&#20154;&#31867;&#24863;&#30693;&#30340;&#32858;&#31867;&#21487;&#20998;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25955;&#28857;&#22270;&#20013;&#30340;&#32858;&#31867;&#20998;&#31163;&#26159;&#19968;&#20010;&#36890;&#24120;&#30001;&#24191;&#27867;&#20351;&#29992;&#30340;&#32858;&#31867;&#25216;&#26415;&#65288;&#20363;&#22914;k-means&#25110;DBSCAN&#65289;&#26469;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#31639;&#27861;&#22522;&#20110;&#38750;&#24863;&#30693;&#24230;&#37327;&#65292;&#23427;&#20204;&#30340;&#36755;&#20986;&#32463;&#24120;&#19981;&#33021;&#21453;&#26144;&#20986;&#20154;&#31867;&#32858;&#31867;&#24863;&#30693;&#12290;&#20026;&#20102;&#24357;&#21512;&#20154;&#31867;&#32858;&#31867;&#24863;&#30693;&#21644;&#26426;&#22120;&#35745;&#31639;&#32858;&#31867;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#22788;&#29702;&#20998;&#24067;&#24335;&#25968;&#25454;&#30340;&#23398;&#20064;&#31574;&#30053;&#12290;&#20026;&#20102;&#22312;&#36825;&#20123;&#25968;&#25454;&#19978;&#23398;&#20064;&#24863;&#30693;&#32858;&#31867;&#20998;&#31163;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#20247;&#21253;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#24037;&#20316;&#65292;&#20854;&#20013;&#21253;&#25324;384&#20010;&#20154;&#32676;&#24037;&#20316;&#32773;&#23545;&#21452;&#21464;&#37327;&#25968;&#25454;&#30340;7,320&#20010;&#28857;&#32858;&#31867;&#20174;&#23646;&#36827;&#34892;&#20102;&#26631;&#35760;&#12290;&#22522;&#20110;&#36825;&#20123;&#25968;&#25454;&#65292;&#25105;&#20204;&#33021;&#22815;&#35757;&#32451;ClusterNet&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#28857;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#34987;&#35757;&#32451;&#25104;&#21453;&#26144;&#20154;&#31867;&#24863;&#30693;&#30340;&#32858;&#31867;&#21487;&#20998;&#24615;&#12290;&#20026;&#20102;&#22312;&#20154;&#31867;&#27880;&#37322;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;ClusterNet&#65292;&#25105;&#20204;&#30465;&#30053;&#20102;&#22312;2D&#30011;&#24067;&#19978;&#28210;&#26579;&#25955;&#28857;&#22270;&#65292;&#32780;&#26159;&#20351;&#29992;&#20102;&#19968;&#20010;PointNet++&#26550;&#26500;&#65292;&#20351;&#20854;&#33021;&#22815;&#30452;&#25509;&#25512;&#29702;&#28857;&#20113;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#31181;&#22522;&#20110;&#24863;&#30693;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#32858;&#31867;&#27169;&#22411;&#65292;ClusterNet&#12290;
&lt;/p&gt;
&lt;p&gt;
Cluster separation in scatterplots is a task that is typically tackled by widely used clustering techniques, such as for instance k-means or DBSCAN. However, as these algorithms are based on non-perceptual metrics, their output often does not reflect human cluster perception. To bridge the gap between human cluster perception and machine-computed clusters, we propose a learning strategy which directly operates on scattered data. To learn perceptual cluster separation on this data, we crowdsourced a large scale dataset, consisting of 7,320 point-wise cluster affiliations for bivariate data, which has been labeled by 384 human crowd workers. Based on this data, we were able to train ClusterNet, a point-based deep learning model, trained to reflect human perception of cluster separability. In order to train ClusterNet on human annotated data, we omit rendering scatterplots on a 2D canvas, but rather use a PointNet++ architecture enabling inference on point clouds directly. In this work, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22812;&#38388;&#20010;&#20154;&#26723;&#26696;&#34920;&#31034;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#21069;&#39044;&#27979;&#21019;&#20260;&#24739;&#32773;&#30340;&#33043;&#27602;&#30151;&#21457;&#20316;&#65292;&#36825;&#31181;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.12737</link><description>&lt;p&gt;
ICU&#21019;&#20260;&#24739;&#32773;&#26089;&#26399;&#33043;&#27602;&#30151;&#21457;&#20316;&#39044;&#27979;&#30340;&#22812;&#38388;&#20010;&#20154;&#26723;&#26696;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
NPRL: Nightly Profile Representation Learning for Early Sepsis Onset Prediction in ICU Trauma Patients. (arXiv:2304.12737v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22812;&#38388;&#20010;&#20154;&#26723;&#26696;&#34920;&#31034;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#21069;&#39044;&#27979;&#21019;&#20260;&#24739;&#32773;&#30340;&#33043;&#27602;&#30151;&#21457;&#20316;&#65292;&#36825;&#31181;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33043;&#27602;&#30151;&#26159;&#19968;&#31181;&#28304;&#20110;&#24863;&#26579;&#65292;&#20197;&#20005;&#37325;&#22120;&#23448;&#21151;&#33021;&#38556;&#30861;&#20026;&#29305;&#24449;&#30340;&#32508;&#21512;&#30151;&#65292;&#24182;&#19988;&#26159;&#20840;&#29699;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;(ICU)&#27515;&#20129;&#29575;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#12290;&#36890;&#36807;&#26089;&#26399;&#24212;&#29992;&#25239;&#29983;&#32032;&#21487;&#20197;&#20943;&#23569;&#36825;&#20123;&#24182;&#21457;&#30151;&#65292;&#22240;&#27492;&#39044;&#27979;&#33043;&#27602;&#30151;&#30340;&#21457;&#20316;&#26102;&#38388;&#23545;&#24739;&#32773;&#30340;&#29983;&#23384;&#21644;&#31119;&#31049;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#22312;&#21307;&#30103;&#22522;&#30784;&#35774;&#26045;&#20869;&#37096;&#37096;&#32626;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34920;&#29616;&#19981;&#20339;&#65292;&#19981;&#36275;&#20197;&#26089;&#26399;&#39044;&#27979;&#33043;&#27602;&#30151;&#30340;&#21457;&#29983;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24739;&#32773;&#29983;&#29702;&#21644;&#20020;&#24202;&#25968;&#25454;&#30340;&#22812;&#38388;&#20010;&#20154;&#26723;&#26696;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;(NPRL)&#65292;&#20197;&#25429;&#25417;&#24739;&#32773;&#29366;&#24577;&#38543;&#26102;&#38388;&#21160;&#24577;&#25913;&#21464;&#30340;&#24773;&#20917;&#12290;&#28982;&#21518;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#39044;&#27979;&#36825;&#20123;&#24739;&#32773;&#30340;&#33043;&#27602;&#30151;&#21457;&#20316;&#26102;&#38388;&#65292;&#24182;&#36229;&#36234;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sepsis is a syndrome that develops in response to the presence of infection. It is characterized by severe organ dysfunction and is one of the leading causes of mortality in Intensive Care Units (ICUs) worldwide. These complications can be reduced through early application of antibiotics, hence the ability to anticipate the onset of sepsis early is crucial to the survival and well-being of patients. Current machine learning algorithms deployed inside medical infrastructures have demonstrated poor performance and are insufficient for anticipating sepsis onset early. In recent years, deep learning methodologies have been proposed to predict sepsis, but some fail to capture the time of onset (e.g., classifying patients' entire visits as developing sepsis or not) and others are unrealistic to be deployed into medical facilities (e.g., creating training instances using a fixed time to onset where the time of onset needs to be known apriori). Therefore, in this paper, we first propose a nove
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21305;&#37197;&#30340;&#29983;&#25104;&#27169;&#22411;&#25968;&#25454;&#20272;&#20540;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#20219;&#20309;&#29983;&#25104;&#27169;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#65292;&#21487;&#20197;&#23545;&#25968;&#25454;&#23454;&#20363;&#36827;&#34892;&#20272;&#20540;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;&#20272;&#20540;&#25928;&#26524;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2304.10701</link><description>&lt;p&gt;
&#22522;&#20110;&#21305;&#37197;&#30340;&#29983;&#25104;&#27169;&#22411;&#25968;&#25454;&#20272;&#20540;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Matching-based Data Valuation for Generative Model. (arXiv:2304.10701v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21305;&#37197;&#30340;&#29983;&#25104;&#27169;&#22411;&#25968;&#25454;&#20272;&#20540;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#20219;&#20309;&#29983;&#25104;&#27169;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#65292;&#21487;&#20197;&#23545;&#25968;&#25454;&#23454;&#20363;&#36827;&#34892;&#20272;&#20540;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;&#20272;&#20540;&#25928;&#26524;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20272;&#20540;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#26377;&#21161;&#20110;&#22686;&#24378;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#24182;&#20445;&#25252;&#25968;&#25454;&#29305;&#24615;&#12290;&#29616;&#26377;&#30340;&#25968;&#25454;&#20272;&#20540;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#21028;&#21035;&#27169;&#22411;&#19978;&#65292;&#24573;&#30053;&#20102;&#26368;&#36817;&#21560;&#24341;&#20102;&#22823;&#37327;&#20851;&#27880;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#12290;&#19982;&#21028;&#21035;&#27169;&#22411;&#31867;&#20284;&#65292;&#38656;&#35201;&#35780;&#20272;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#25968;&#25454;&#36129;&#29486;&#30340;&#32039;&#36843;&#38656;&#27714;&#20063;&#23384;&#22312;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#25968;&#25454;&#20272;&#20540;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#21028;&#21035;&#27169;&#22411;&#24615;&#33021;&#25351;&#26631;&#65292;&#24182;&#38656;&#35201;&#23545;&#27169;&#22411;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#19981;&#33021;&#22312;&#23454;&#38469;&#20013;&#30452;&#25509;&#39640;&#25928;&#22320;&#24212;&#29992;&#20110;&#36817;&#26399;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#20363;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#25193;&#25955;&#27169;&#22411;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20174;&#30456;&#20284;&#24615;&#21305;&#37197;&#30340;&#35282;&#24230;&#23545;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#20272;&#20540;&#38382;&#39064;&#36827;&#34892;&#20102;&#26500;&#24314;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;Generative Model Valuator&#8221;&#65288;GMValuator&#65289;&#8212;&#8212;&#31532;&#19968;&#20010;&#38024;&#23545;&#20219;&#20309;&#29983;&#25104;&#27169;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#65292;&#26088;&#22312;&#20026;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#25968;&#25454;&#20272;&#20540;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#25968;&#25454;&#23454;&#20363;&#21450;&#30001;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#30456;&#24212;&#21512;&#25104;&#23454;&#20363;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#26469;&#20272;&#35745;&#21407;&#22987;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20026;&#19981;&#21516;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;&#21253;&#25324;GAN&#21644;&#25193;&#25955;&#27169;&#22411;&#65289;&#35780;&#20272;&#25968;&#25454;&#23454;&#20363;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data valuation is critical in machine learning, as it helps enhance model transparency and protect data properties. Existing data valuation methods have primarily focused on discriminative models, neglecting deep generative models that have recently gained considerable attention. Similar to discriminative models, there is an urgent need to assess data contributions in deep generative models as well. However, previous data valuation approaches mainly relied on discriminative model performance metrics and required model retraining. Consequently, they cannot be applied directly and efficiently to recent deep generative models, such as generative adversarial networks and diffusion models, in practice. To bridge this gap, we formulate the data valuation problem in generative models from a similarity-matching perspective. Specifically, we introduce Generative Model Valuator (GMValuator), the first model-agnostic approach for any generative models, designed to provide data valuation for gener
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;muP&#65292;&#21487;&#20197;&#25552;&#39640;&#36229;&#21442;&#25968;&#30340;&#32553;&#25918;&#24459;&#30340;&#25311;&#21512;&#31934;&#24230;&#65292;&#20943;&#23569;&#23545;&#22823;&#27169;&#22411;&#36229;&#21442;&#25968;&#30340;&#25628;&#32034;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#22823;&#35268;&#27169;&#27169;&#22411;&#19978;&#36827;&#34892;&#25439;&#22833;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.06875</link><description>&lt;p&gt;
&#19981;&#38656;&#37325;&#26032;&#25628;&#32034;&#30340;&#30740;&#31350;&#65306;&#26368;&#22823;&#26356;&#26032;&#21442;&#25968;&#21270;&#21487;&#31934;&#30830;&#39044;&#27979;&#36328;&#23610;&#24230;&#30340;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Research without Re-search: Maximal Update Parametrization Yields Accurate Loss Prediction across Scales. (arXiv:2304.06875v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06875
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;muP&#65292;&#21487;&#20197;&#25552;&#39640;&#36229;&#21442;&#25968;&#30340;&#32553;&#25918;&#24459;&#30340;&#25311;&#21512;&#31934;&#24230;&#65292;&#20943;&#23569;&#23545;&#22823;&#27169;&#22411;&#36229;&#21442;&#25968;&#30340;&#25628;&#32034;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#22823;&#35268;&#27169;&#27169;&#22411;&#19978;&#36827;&#34892;&#25439;&#22833;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#25193;&#22823;&#65292;&#39564;&#35777;&#30740;&#31350;&#24819;&#27861;&#21464;&#24471;&#36234;&#26469;&#36234;&#26114;&#36149;&#65292;&#22240;&#20026;&#23567;&#27169;&#22411;&#30340;&#32467;&#35770;&#19981;&#33021;&#31616;&#21333;&#22320;&#36716;&#31227;&#21040;&#22823;&#27169;&#22411;&#12290;&#35299;&#20915;&#26041;&#26696;&#26159;&#24314;&#31435;&#19968;&#20010;&#36890;&#29992;&#31995;&#32479;&#65292;&#20165;&#22522;&#20110;&#23567;&#27169;&#22411;&#30340;&#32467;&#26524;&#21644;&#36229;&#21442;&#25968;&#30452;&#25509;&#39044;&#27979;&#22823;&#27169;&#22411;&#30340;&#19968;&#20123;&#25351;&#26631;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#32553;&#25918;&#24459;&#30340;&#26041;&#27861;&#38656;&#35201;&#22312;&#26368;&#22823;&#30340;&#27169;&#22411;&#19978;&#36827;&#34892;&#36229;&#21442;&#25968;&#25628;&#32034;&#65292;&#20294;&#30001;&#20110;&#36164;&#28304;&#26377;&#38480;&#65292;&#36825;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#26368;&#22823;&#26356;&#26032;&#21442;&#25968;&#21270;&#65288;muP&#65289;&#20351;&#24471;&#21487;&#20197;&#22312;&#38752;&#36817;&#24120;&#35265;&#25439;&#22833;&#27969;&#22495;&#30340;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#25311;&#21512;&#36229;&#21442;&#25968;&#30340;&#32553;&#25918;&#24459;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#25628;&#32034;&#12290;&#22240;&#27492;&#65292;&#19981;&#21516;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#22823;&#23610;&#24230;&#19978;&#36827;&#34892;&#25439;&#22833;&#39044;&#27979;&#65292;&#22312;&#35757;&#32451;&#24320;&#22987;&#20043;&#21069;&#23601;&#21487;&#20197;&#36827;&#34892;&#30452;&#25509;&#27604;&#36739;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#20316;&#20026;&#21487;&#38752;&#30340;&#23398;&#26415;&#30740;&#31350;&#30340;&#31532;&#19968;&#27493;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#27169;&#22411;&#35268;&#27169;&#65292;&#32780;&#19981;&#38656;&#22823;&#37327;&#30340;&#35745;&#31639;&#12290;&#20195;&#30721;&#23558;&#24456;&#24555;&#20844;&#24320;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
As language models scale up, it becomes increasingly expensive to verify research ideas because conclusions on small models do not trivially transfer to large ones. A possible solution is to establish a generic system that directly predicts some metrics for large models solely based on the results and hyperparameters from small models. Existing methods based on scaling laws require hyperparameter search on the largest models, which is impractical with limited resources. We address this issue by presenting our discoveries indicating that Maximal Update parametrization (muP) enables accurate fitting of scaling laws for hyperparameters close to common loss basins, without any search. Thus, different models can be directly compared on large scales with loss prediction even before the training starts. We propose a new paradigm as a first step towards reliable academic research for any model scale without heavy computation. Code will be publicly available shortly.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#21160;&#24577;&#20844;&#20849;&#20113;&#29615;&#22659;&#20013;&#23454;&#29616;&#24555;&#36895;&#22312;&#32447;&#31574;&#30053;&#23398;&#20064;&#30340;&#26694;&#26550;RAPID&#65292;&#36890;&#36807;&#39046;&#22495;&#30693;&#35782;&#21551;&#21457;&#30340;&#25216;&#26415;&#23454;&#29616;&#26679;&#26412;&#25928;&#29575;&#21644;&#20559;&#24046;&#20943;&#23569;&#65292;&#23398;&#20064;&#24182;&#23454;&#26102;&#35843;&#25972;&#36164;&#28304;&#20998;&#37197;&#31574;&#30053;&#65292;&#33021;&#26377;&#25928;&#35299;&#20915;&#36164;&#28304;&#20849;&#20139;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.04797</link><description>&lt;p&gt;
RAPID: &#22312;&#21160;&#24577;&#20844;&#20849;&#20113;&#29615;&#22659;&#20013;&#23454;&#29616;&#24555;&#36895;&#22312;&#32447;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RAPID: Enabling Fast Online Policy Learning in Dynamic Public Cloud Environments. (arXiv:2304.04797v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04797
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#21160;&#24577;&#20844;&#20849;&#20113;&#29615;&#22659;&#20013;&#23454;&#29616;&#24555;&#36895;&#22312;&#32447;&#31574;&#30053;&#23398;&#20064;&#30340;&#26694;&#26550;RAPID&#65292;&#36890;&#36807;&#39046;&#22495;&#30693;&#35782;&#21551;&#21457;&#30340;&#25216;&#26415;&#23454;&#29616;&#26679;&#26412;&#25928;&#29575;&#21644;&#20559;&#24046;&#20943;&#23569;&#65292;&#23398;&#20064;&#24182;&#23454;&#26102;&#35843;&#25972;&#36164;&#28304;&#20998;&#37197;&#31574;&#30053;&#65292;&#33021;&#26377;&#25928;&#35299;&#20915;&#36164;&#28304;&#20849;&#20139;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20010;&#24037;&#20316;&#36127;&#36733;&#20043;&#38388;&#30340;&#36164;&#28304;&#20849;&#20139;&#24050;&#25104;&#20026;&#20113;&#26381;&#21153;&#25552;&#20379;&#21830;&#20043;&#38388;&#30340;&#19968;&#31181;&#31361;&#20986;&#23454;&#36341;&#65292;&#36825;&#26159;&#30001;&#38656;&#27714;&#25913;&#36827;&#36164;&#28304;&#21033;&#29992;&#29575;&#21644;&#38477;&#20302;&#25317;&#26377;&#25104;&#26412;&#25152;&#39537;&#21160;&#30340;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36164;&#28304;&#20105;&#29992;&#21487;&#33021;&#20250;&#23545;&#20855;&#26377;&#20005;&#26684;&#26381;&#21153;&#36136;&#37327; (QoS) &#35201;&#27714;&#30340;&#20248;&#20808;&#32423;&#39640;&#12289;&#38754;&#21521;&#29992;&#25143;&#30340;&#36127;&#36733;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#65292;&#22240;&#27492;&#26377;&#25928;&#30340;&#36164;&#28304;&#20849;&#20139;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#25361;&#25112;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#26041;&#27861;&#24050;&#32463;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#36825;&#20123;&#24037;&#20316;&#22312;&#20844;&#20849;&#20113;&#29615;&#22659;&#20013;&#20173;&#28982;&#24456;&#38590;&#23454;&#36341;&#65292;&#22240;&#20026;&#36127;&#36733;&#20107;&#20808;&#26159;&#26410;&#30693;&#30340;&#65292;&#21487;&#33021;&#20165;&#36816;&#34892;&#30701;&#26242;&#30340;&#26102;&#38388;&#65292;&#20174;&#32780;&#31105;&#27490;&#33073;&#26426;&#23398;&#20064;&#65292;&#24182;&#19988;&#26174;&#33879;&#38459;&#30861;&#22312;&#32447;&#23398;&#20064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986; RAPID&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#39640;&#24230;&#21160;&#24577;&#30340;&#25805;&#20316;&#29615;&#22659;&#20013;&#23454;&#29616;&#24555;&#36895;&#12289;&#23436;&#20840;&#22312;&#32447;&#30340;&#36164;&#28304;&#20998;&#37197;&#31574;&#30053;&#23398;&#20064;&#12290;RAPID &#21033;&#29992;&#36731;&#37327;&#32423; QoS &#39044;&#27979;&#65292;&#36890;&#36807;&#39046;&#22495;&#30693;&#35782;&#21551;&#21457;&#30340;&#25216;&#26415;&#23454;&#29616;&#26679;&#26412;&#25928;&#29575;&#21644;&#20559;&#24046;&#20943;&#23569;&#65292;&#20197;&#20998;&#31163;&#20105;&#29992;&#26816;&#27979;&#21644;&#20998;&#37197;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Resource sharing between multiple workloads has become a prominent practice among cloud service providers, motivated by demand for improved resource utilization and reduced cost of ownership. Effective resource sharing, however, remains an open challenge due to the adverse effects that resource contention can have on high-priority, user-facing workloads with strict Quality of Service (QoS) requirements. Although recent approaches have demonstrated promising results, those works remain largely impractical in public cloud environments since workloads are not known in advance and may only run for a brief period, thus prohibiting offline learning and significantly hindering online learning. In this paper, we propose RAPID, a novel framework for fast, fully-online resource allocation policy learning in highly dynamic operating environments. RAPID leverages lightweight QoS predictions, enabled by domain-knowledge-inspired techniques for sample efficiency and bias reduction, to decouple contr
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#26631;&#31614;&#21487;&#35266;&#27979;&#12289;&#33410;&#28857;&#19981;&#21487;&#35266;&#27979;&#19979;&#30340;&#20108;&#37096;&#22270;&#22270;&#20272;&#35745;&#38382;&#39064;&#65292;&#22312;&#20998;&#27573;&#24120;&#25968;&#21644;H\"older&#36830;&#32493;&#22270;&#35889;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#20102;&#26377;&#38480;&#30340;&#26679;&#26412;&#39118;&#38505;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2304.03590</link><description>&lt;p&gt;
&#26631;&#31614;&#21487;&#35266;&#27979;&#12289;&#33410;&#28857;&#19981;&#21487;&#35266;&#27979;&#19979;&#30340;&#20108;&#37096;&#22270;&#22270;&#20272;&#35745;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Graphon Estimation in bipartite graphs with observable edge labels and unobservable node labels. (arXiv:2304.03590v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03590
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#26631;&#31614;&#21487;&#35266;&#27979;&#12289;&#33410;&#28857;&#19981;&#21487;&#35266;&#27979;&#19979;&#30340;&#20108;&#37096;&#22270;&#22270;&#20272;&#35745;&#38382;&#39064;&#65292;&#22312;&#20998;&#27573;&#24120;&#25968;&#21644;H\"older&#36830;&#32493;&#22270;&#35889;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#20102;&#26377;&#38480;&#30340;&#26679;&#26412;&#39118;&#38505;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#25968;&#25454;&#38598;&#37117;&#21487;&#20197;&#34920;&#31034;&#20026;&#19968;&#20010;&#30697;&#38453;&#65292;&#20854;&#26465;&#30446;&#23545;&#24212;&#20110;&#19981;&#21516;&#31867;&#22411;&#23454;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#65288;&#32593;&#39029;&#29992;&#25143;&#35775;&#38382;&#32593;&#39029;&#30340;&#27425;&#25968;&#12289;&#23398;&#29983;&#26576;&#31185;&#30446;&#30340;&#25104;&#32489;&#12289;&#24739;&#32773;&#23545;&#21307;&#29983;&#30340;&#35780;&#20215;&#31561;&#65289;&#12290;&#26412;&#25991;&#20551;&#35774;&#19978;&#36848;&#20132;&#20114;&#26159;&#30001;&#25551;&#36848;&#27599;&#20010;&#23454;&#20307;&#30340;&#19981;&#21487;&#35266;&#27979;&#28508;&#22312;&#21464;&#37327;&#30830;&#23450;&#30340;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20272;&#35745;&#32473;&#23450;&#19981;&#21487;&#35266;&#27979;&#21464;&#37327;&#30340;&#25968;&#25454;&#30697;&#38453;&#30340;&#26465;&#20214;&#26399;&#26395;&#12290;&#36825;&#34987;&#34920;&#31034;&#20026;&#20272;&#35745;&#31216;&#20026;&#22270;&#35889;&#30340;&#21452;&#21464;&#37327;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20998;&#27573;&#24120;&#25968;&#21644;H\"older&#36830;&#32493;&#22270;&#35889;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#20026;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#21644;&#25351;&#25968;&#21152;&#26435;&#32858;&#21512;&#24314;&#31435;&#20102;&#26377;&#38480;&#26679;&#26412;&#39118;&#38505;&#30028;&#38480;&#12290;&#36825;&#20123;&#30028;&#38480;&#24378;&#35843;&#20102;&#20272;&#35745;&#35823;&#24046;&#19982;&#25968;&#25454;&#38598;&#22823;&#23567;&#12289;&#20132;&#20114;&#24378;&#24230;&#26368;&#22823;&#20540;&#21644;&#22122;&#22768;&#27700;&#24179;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#30001;&#20110;&#20998;&#26512;&#30340;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#37327;&#38590;&#20197;&#22788;&#29702;&#65292;
&lt;/p&gt;
&lt;p&gt;
Many real-world data sets can be presented in the form of a matrix whose entries correspond to the interaction between two entities of different natures (number of times a web user visits a web page, a student's grade in a subject, a patient's rating of a doctor, etc.). We assume in this paper that the mentioned interaction is determined by unobservable latent variables describing each entity. Our objective is to estimate the conditional expectation of the data matrix given the unobservable variables. This is presented as a problem of estimation of a bivariate function referred to as graphon. We study the cases of piecewise constant and H\"older-continuous graphons. We establish finite sample risk bounds for the least squares estimator and the exponentially weighted aggregate. These bounds highlight the dependence of the estimation error on the size of the data set, the maximum intensity of the interactions, and the level of noise. As the analyzed least-squares estimator is intractable
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27010;&#25324;&#20102;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#20013;&#22522;&#20110;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#26041;&#27861;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#35299;&#20915;&#38750;&#32447;&#24615;&#24773;&#20917;&#19979;&#21807;&#19968;&#24615;&#38382;&#39064;&#25552;&#20986;&#20102;&#21487;&#35782;&#21035;&#30340;&#25193;&#23637;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.16535</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#20013;&#22522;&#20110;&#38750;&#32447;&#24615;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#30340;&#21407;&#21017;&#20998;&#31163;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Nonlinear Independent Component Analysis for Principled Disentanglement in Unsupervised Deep Learning. (arXiv:2303.16535v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27010;&#25324;&#20102;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#20013;&#22522;&#20110;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#26041;&#27861;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#35299;&#20915;&#38750;&#32447;&#24615;&#24773;&#20917;&#19979;&#21807;&#19968;&#24615;&#38382;&#39064;&#25552;&#20986;&#20102;&#21487;&#35782;&#21035;&#30340;&#25193;&#23637;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#22914;&#20309;&#25214;&#21040;&#26377;&#29992;&#30340;&#39640;&#32500;&#25968;&#25454;&#34920;&#31034;&#65292;&#21363;&#25152;&#35859;&#30340;&#8220;&#20998;&#31163;&#8221;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#26159;&#21551;&#21457;&#24335;&#30340;&#65292;&#32570;&#20047;&#36866;&#24403;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#22312;&#32447;&#24615;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#65288;ICA&#65289;&#22312;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#24182;&#19988;&#20855;&#26377;&#22522;&#20110;&#33391;&#23450;&#20041;&#30340;&#27010;&#29575;&#27169;&#22411;&#30340;&#21407;&#21017;&#24615;&#12290; &#28982;&#32780;&#65292;&#23558;ICA&#25193;&#23637;&#21040;&#38750;&#32447;&#24615;&#24773;&#20917;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#26840;&#25163;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#30001;&#20110;&#32570;&#20047;&#21487;&#35782;&#21035;&#24615;&#65292;&#21363;&#34920;&#31034;&#30340;&#21807;&#19968;&#24615;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20351;&#29992;&#26102;&#38388;&#32467;&#26500;&#25110;&#26576;&#20123;&#36741;&#21161;&#20449;&#24687;&#30340;&#38750;&#32447;&#24615;&#25193;&#23637;&#12290;&#36825;&#20123;&#27169;&#22411;&#23454;&#38469;&#19978;&#26159;&#21487;&#35782;&#21035;&#30340;&#65292;&#22240;&#27492;&#24050;&#32463;&#24320;&#21457;&#20986;&#36234;&#26469;&#36234;&#22810;&#30340;&#31639;&#27861;&#12290;&#29305;&#21035;&#26159;&#19968;&#20123;&#33258;&#30417;&#30563;&#31639;&#27861;&#21487;&#20197;&#26174;&#31034;&#20986;&#20272;&#35745;&#38750;&#32447;&#24615;ICA&#65292;&#21363;&#20351;&#26368;&#21021;&#26159;&#20174;&#21551;&#21457;&#24335;&#35282;&#24230;&#25552;&#20986;&#30340;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#38750;&#32447;&#24615;ICA&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
A central problem in unsupervised deep learning is how to find useful representations of high-dimensional data, sometimes called "disentanglement". Most approaches are heuristic and lack a proper theoretical foundation. In linear representation learning, independent component analysis (ICA) has been successful in many applications areas, and it is principled, i.e. based on a well-defined probabilistic model. However, extension of ICA to the nonlinear case has been problematic due to the lack of identifiability, i.e. uniqueness of the representation. Recently, nonlinear extensions that utilize temporal structure or some auxiliary information have been proposed. Such models are in fact identifiable, and consequently, an increasing number of algorithms have been developed. In particular, some self-supervised algorithms can be shown to estimate nonlinear ICA, even though they have initially been proposed from heuristic perspectives. This paper reviews the state-of-the-art of nonlinear ICA 
&lt;/p&gt;</description></item><item><title>HERMES Pathfinder&#26159;&#19968;&#20010;&#22312;&#36712;&#25506;&#27979;&#31995;&#32479;&#65292;&#20351;&#29992;&#31616;&#21333;&#20294;&#21019;&#26032;&#30340;&#25506;&#27979;&#22120;&#30417;&#27979;&#39640;&#33021;&#23431;&#23449;&#30636;&#21464;&#29616;&#35937;&#65292;&#36890;&#36807;&#30740;&#31350;&#20449;&#21495;&#21040;&#36798;&#19981;&#21516;&#25506;&#27979;&#22120;&#30340;&#24310;&#36831;&#26102;&#38388;&#33719;&#24471;&#31934;&#30830;&#30340;&#20301;&#32622;&#20449;&#24687;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#35780;&#20272;&#33322;&#22825;&#39640;&#33021;&#25506;&#27979;&#22120;&#32972;&#26223;&#35745;&#25968;&#29575;&#30340;&#26032;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2303.15936</link><description>&lt;p&gt;
&#23547;&#25214;&#38271;&#26102;&#38388;&#24494;&#24369;&#30340;&#22825;&#25991;&#39640;&#33021;&#30636;&#21464;&#29616;&#35937;&#65306;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Searching for long faint astronomical high energy transients: a data driven approach. (arXiv:2303.15936v1 [astro-ph.HE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15936
&lt;/p&gt;
&lt;p&gt;
HERMES Pathfinder&#26159;&#19968;&#20010;&#22312;&#36712;&#25506;&#27979;&#31995;&#32479;&#65292;&#20351;&#29992;&#31616;&#21333;&#20294;&#21019;&#26032;&#30340;&#25506;&#27979;&#22120;&#30417;&#27979;&#39640;&#33021;&#23431;&#23449;&#30636;&#21464;&#29616;&#35937;&#65292;&#36890;&#36807;&#30740;&#31350;&#20449;&#21495;&#21040;&#36798;&#19981;&#21516;&#25506;&#27979;&#22120;&#30340;&#24310;&#36831;&#26102;&#38388;&#33719;&#24471;&#31934;&#30830;&#30340;&#20301;&#32622;&#20449;&#24687;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#35780;&#20272;&#33322;&#22825;&#39640;&#33021;&#25506;&#27979;&#22120;&#32972;&#26223;&#35745;&#25968;&#29575;&#30340;&#26032;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
HERMES&#65288;High Energy Rapid Modular Ensemble of Satellites&#65289;&#26159;&#19968;&#20010;&#22312;&#36712;&#25506;&#27979;&#31995;&#32479;&#30340;&#21069;&#23548;&#37096;&#32626;&#65292;&#30001;&#20845;&#20010;3U&#32435;&#31859;&#21355;&#26143;&#32452;&#25104;&#65292;&#25176;&#31649;&#30528;&#29992;&#20110;&#30417;&#27979;&#23431;&#23449;&#39640;&#33021;&#30636;&#21464;&#29616;&#35937;&#30340;&#31616;&#21333;&#20294;&#21019;&#26032;&#30340;&#25506;&#27979;&#22120;&#12290;HERMES Pathfinder&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#35777;&#26126;&#20351;&#29992;&#24494;&#22411;&#30828;&#20214;&#21487;&#20197;&#33719;&#24471;&#39640;&#33021;&#23431;&#23449;&#30636;&#21464;&#29616;&#35937;&#30340;&#31934;&#30830;&#20301;&#32622;&#20449;&#24687;&#12290;&#36890;&#36807;&#30740;&#31350;&#20449;&#21495;&#21040;&#36798;&#19981;&#21516;&#25506;&#27979;&#22120;&#30340;&#24310;&#36831;&#26102;&#38388;&#65292;&#21487;&#20197;&#33719;&#21462;&#30636;&#21464;&#29616;&#35937;&#30340;&#20301;&#32622;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#38656;&#35201;&#24320;&#21457;&#26032;&#30340;&#24037;&#20855;&#26469;&#20805;&#20998;&#21033;&#29992;HERMES Pathfinder&#26410;&#26469;&#30340;&#31185;&#23398;&#25968;&#25454;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35780;&#20272;&#33322;&#22825;&#39640;&#33021;&#25506;&#27979;&#22120;&#32972;&#26223;&#35745;&#25968;&#29575;&#30340;&#26032;&#26694;&#26550;&#65307;&#36825;&#26159;&#37492;&#21035;&#24494;&#24369;&#30340;&#22825;&#20307;&#29289;&#29702;&#30636;&#21464;&#29616;&#35937;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#26469;&#20272;&#35745;&#25506;&#27979;&#22120;&#30340;&#32972;&#26223;&#35745;&#25968;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
HERMES (High Energy Rapid Modular Ensemble of Satellites) pathfinder is an in-orbit demonstration consisting of a constellation of six 3U nano-satellites hosting simple but innovative detectors for the monitoring of cosmic high-energy transients. The main objective of HERMES Pathfinder is to prove that accurate position of high-energy cosmic transients can be obtained using miniaturized hardware. The transient position is obtained by studying the delay time of arrival of the signal to different detectors hosted by nano-satellites on low Earth orbits. To this purpose, the goal is to achive an overall accuracy of a fraction of a micro-second. In this context, we need to develop novel tools to fully exploit the future scientific data output of HERMES Pathfinder. In this paper, we introduce a new framework to assess the background count rate of a space-born, high energy detector; a key step towards the identification of faint astrophysical transients. We employ a Neural Network (NN) to est
&lt;/p&gt;</description></item><item><title>RGI&#26159;&#19968;&#20010;&#29992;&#20110;&#22270;&#19978;&#33410;&#28857;&#32423;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#33410;&#28857;&#32423;&#23616;&#37096;&#21644;&#20840;&#23616;&#35270;&#22270;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#22120;&#65292;&#24182;&#35268;&#33539;&#21270;&#20102;&#34920;&#31034;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#12290;</title><link>http://arxiv.org/abs/2303.08644</link><description>&lt;p&gt;
RGI: &#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#22270;&#24418;Infomax&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RGI : Regularized Graph Infomax for self-supervised learning on graphs. (arXiv:2303.08644v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08644
&lt;/p&gt;
&lt;p&gt;
RGI&#26159;&#19968;&#20010;&#29992;&#20110;&#22270;&#19978;&#33410;&#28857;&#32423;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#33410;&#28857;&#32423;&#23616;&#37096;&#21644;&#20840;&#23616;&#35270;&#22270;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#22120;&#65292;&#24182;&#35268;&#33539;&#21270;&#20102;&#34920;&#31034;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#20316;&#20026;&#22312;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#20013;&#36991;&#20813;&#22823;&#37327;&#27880;&#37322;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#27491;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#27491;&#21017;&#21270;&#22270;&#24418;Infomax&#65288;RGI&#65289;&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#21448;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22270;&#24418;&#19978;&#36827;&#34892;&#33410;&#28857;&#32423;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#33410;&#28857;&#32423;&#23616;&#37096;&#21644;&#20840;&#23616;&#35270;&#22270;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#22120;&#65292;&#19982;&#20197;&#21069;&#37319;&#29992;&#22270;&#32423;&#20840;&#23616;&#35270;&#22270;&#30340;&#26041;&#27861;&#19981;&#21516;&#12290;&#35813;&#26041;&#27861;&#20419;&#36827;&#20102;&#35270;&#22270;&#20043;&#38388;&#30340;&#21487;&#39044;&#27979;&#24615;&#65292;&#21516;&#26102;&#35268;&#33539;&#21270;&#20102;&#34920;&#31034;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#12290;&#22240;&#27492;&#65292;RGI&#26159;&#38750;&#23545;&#27604;&#30340;&#65292;&#19981;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#19981;&#23545;&#31216;&#20307;&#31995;&#32467;&#26500;&#25110;&#35757;&#32451;&#25216;&#24039;&#65292;&#26080;&#38656;&#22686;&#24378;&#21644;&#19981;&#20381;&#36182;&#20110;&#21452;&#20998;&#25903;&#26550;&#26500;&#12290;&#25105;&#20204;&#22312;&#27969;&#34892;&#30340;&#22270;&#24418;&#22522;&#20934;&#19978;&#36816;&#34892;RGI&#65292;&#24182;&#23637;&#31034;&#23427;&#21487;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#32780;&#19981;&#31649;&#23427;&#30340;&#31616;&#21333;&#24615;&#22914;&#20309;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning is gaining considerable attention as a solution to avoid the requirement of extensive annotations in representation learning on graphs. We introduce \textit{Regularized Graph Infomax (RGI)}, a simple yet effective framework for node level self-supervised learning on graphs that trains a graph neural network encoder by maximizing the mutual information between node level local and global views, in contrast to previous works that employ graph level global views. The method promotes the predictability between views while regularizing the covariance matrices of the representations. Therefore, RGI is non-contrastive, does not depend on complex asymmetric architectures nor training tricks, is augmentation-free and does not rely on a two branch architecture. We run RGI on both transductive and inductive settings with popular graph benchmarks and show that it can achieve state-of-the-art performance regardless of its simplicity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#25910;&#25947;&#30340;PnP&#26041;&#27861;&#65292;&#20351;&#29992;&#25311;&#29275;&#39039;&#27493;&#39588;&#20197;&#21152;&#36895;&#25910;&#25947;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;PnP&#26041;&#27861;&#23545;&#21435;&#22122;&#22120;&#25110;&#20445;&#30495;&#24230;&#20989;&#25968;&#26045;&#21152;&#20102;&#36739;&#36731;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.07271</link><description>&lt;p&gt;
&#21487;&#35777;&#25910;&#25947;&#30340;&#21363;&#25554;&#21363;&#29992;&#25311;&#29275;&#39039;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Provably Convergent Plug-and-Play Quasi-Newton Methods. (arXiv:2303.07271v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#25910;&#25947;&#30340;PnP&#26041;&#27861;&#65292;&#20351;&#29992;&#25311;&#29275;&#39039;&#27493;&#39588;&#20197;&#21152;&#36895;&#25910;&#25947;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;PnP&#26041;&#27861;&#23545;&#21435;&#22122;&#22120;&#25110;&#20445;&#30495;&#24230;&#20989;&#25968;&#26045;&#21152;&#20102;&#36739;&#36731;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21363;&#25554;&#21363;&#29992;&#65288;PnP&#65289;&#26041;&#27861;&#26159;&#19968;&#31867;&#39640;&#25928;&#30340;&#36845;&#20195;&#31639;&#27861;&#65292;&#26088;&#22312;&#21033;&#29992;&#32463;&#20856;&#20248;&#21270;&#31639;&#27861;&#65288;&#22914;ISTA&#25110;ADMM&#65289;&#65292;&#23558;&#25968;&#25454;&#20445;&#30495;&#24230;&#39033;&#21644;&#28145;&#24230;&#21435;&#22122;&#22120;&#30456;&#32467;&#21512;&#12290;&#29616;&#26377;&#30340;&#21487;&#35777;&#26126;&#30340;PnP&#26041;&#27861;&#23545;&#21435;&#22122;&#22120;&#25110;&#20445;&#30495;&#24230;&#20989;&#25968;&#26045;&#21152;&#20102;&#20005;&#26684;&#30340;&#38480;&#21046;&#65292;&#22914;&#38750;&#25193;&#24352;&#24615;&#25110;&#20005;&#26684;&#20984;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#30340;PnP&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#36817;&#31471;&#21435;&#22122;&#22120;&#26045;&#21152;&#30456;&#23545;&#36739;&#36731;&#30340;&#26465;&#20214;&#65292;&#24182;&#24341;&#20837;&#20102;&#25311;&#29275;&#39039;&#27493;&#39588;&#20197;&#22823;&#22823;&#21152;&#36895;&#25910;&#25947;&#12290;&#36890;&#36807;&#23558;&#28145;&#24230;&#21435;&#22122;&#22120;&#29305;&#21035;&#21442;&#25968;&#21270;&#20026;&#26799;&#24230;&#27493;&#39588;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#25311;&#29275;&#39039;PnP&#31639;&#27861;&#30340;&#22266;&#23450;&#28857;&#34920;&#24449;&#20026;&#21487;&#33021;&#38750;&#20984;&#20989;&#25968;&#30340;&#20020;&#30028;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Plug-and-Play (PnP) methods are a class of efficient iterative methods that aim to combine data fidelity terms and deep denoisers using classical optimization algorithms, such as ISTA or ADMM. Existing provable PnP methods impose heavy restrictions on the denoiser or fidelity function, such as nonexpansiveness or strict convexity. In this work, we propose a provable PnP method that imposes relatively light conditions based on proximal denoisers, and introduce a quasi-Newton step to greatly accelerate convergence. By specially parameterizing the deep denoiser as a gradient step, we further characterize the fixed-points of the quasi-Newton PnP algorithm as critical points of a possibly non-convex function.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20998;&#26512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#24067;&#23572;&#30005;&#36335;&#22797;&#26434;&#24615;&#21644;&#25551;&#36848;&#24615;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#22810;&#39033;&#24335;&#35268;&#27169;&#26377;&#30028;&#28145;&#24230;&#30340;GNN&#26063;&#26063;&#21487;&#20197;&#35745;&#31639;&#30340;&#22270;&#26597;&#35810;&#27491;&#26159;&#24102;&#35745;&#25968;&#21644;&#20869;&#32622;&#20851;&#31995;&#30340;&#19968;&#38454;&#36923;&#36753;&#21463;&#20445;&#25252;&#30340;&#29255;&#26029;GFO+C&#25152;&#23450;&#20041;&#30340;&#65292;&#36825;&#23558;GNN&#25918;&#22312;&#30005;&#36335;&#22797;&#26434;&#24615;&#31867;TC^0&#20013;&#12290;</title><link>http://arxiv.org/abs/2303.04613</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25551;&#36848;&#24615;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Descriptive Complexity of Graph Neural Networks. (arXiv:2303.04613v2 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04613
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20998;&#26512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#24067;&#23572;&#30005;&#36335;&#22797;&#26434;&#24615;&#21644;&#25551;&#36848;&#24615;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#22810;&#39033;&#24335;&#35268;&#27169;&#26377;&#30028;&#28145;&#24230;&#30340;GNN&#26063;&#26063;&#21487;&#20197;&#35745;&#31639;&#30340;&#22270;&#26597;&#35810;&#27491;&#26159;&#24102;&#35745;&#25968;&#21644;&#20869;&#32622;&#20851;&#31995;&#30340;&#19968;&#38454;&#36923;&#36753;&#21463;&#20445;&#25252;&#30340;&#29255;&#26029;GFO+C&#25152;&#23450;&#20041;&#30340;&#65292;&#36825;&#23558;GNN&#25918;&#22312;&#30005;&#36335;&#22797;&#26434;&#24615;&#31867;TC^0&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#24067;&#23572;&#30005;&#36335;&#22797;&#26434;&#24615;&#21644;&#25551;&#36848;&#24615;&#22797;&#26434;&#24615;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22810;&#39033;&#24335;&#35268;&#27169;&#26377;&#30028;&#28145;&#24230;&#30340;GNN&#26063;&#26063;&#21487;&#20197;&#35745;&#31639;&#30340;&#22270;&#26597;&#35810;&#27491;&#26159;&#37027;&#20123;&#29992;&#24102;&#35745;&#25968;&#21644;&#20869;&#32622;&#20851;&#31995;&#30340;&#19968;&#38454;&#36923;&#36753;&#21463;&#20445;&#25252;&#30340;&#29255;&#26029;GFO+C&#23450;&#20041;&#30340;&#12290;&#36825;&#23558;GNN&#25918;&#22312;&#30005;&#36335;&#22797;&#26434;&#24615;&#31867;TC^0&#20013;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;GNN&#23478;&#26063;&#21487;&#20197;&#20351;&#29992;&#20219;&#24847;&#23454;&#25968;&#26435;&#20540;&#21644;&#21253;&#25324;&#26631;&#20934;ReLU&#12289;Logistic&#8220;sigmod&#8221;&#21644;&#21452;&#26354;&#27491;&#20999;&#20989;&#25968;&#22312;&#20869;&#30340;&#24191;&#27867;&#28608;&#27963;&#20989;&#25968;&#31867;&#12290;&#22914;&#26524;GNN&#34987;&#20801;&#35768;&#20351;&#29992;&#38543;&#26426;&#21021;&#22987;&#21270;&#21644;&#20840;&#23616;&#35835;&#21462;&#65288;&#36825;&#20123;&#37117;&#26159;GNN&#22312;&#23454;&#36341;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#26631;&#20934;&#21151;&#33021;&#65289;&#65292;&#23427;&#20204;&#21487;&#20197;&#35745;&#31639;&#19982;&#38408;&#38376;&#30340;&#26377;&#30028;&#28145;&#24230;&#24067;&#23572;&#30005;&#36335;&#23436;&#20840;&#30456;&#21516;&#30340;&#26597;&#35810;&#65292;&#21363;&#22312;TC^0&#20013;&#30340;&#26597;&#35810;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#24102;&#20998;&#27573;&#32447;&#24615;&#28608;&#27963;&#21644;&#26377;&#29702;&#26435;&#37325;&#30340;&#21333;&#20010;GNN&#21487;&#20197;&#22312;&#19981;&#24314;&#36896;&#20869;&#37096;&#20851;&#31995;&#30340;&#24773;&#20917;&#19979;&#30001;GFO+C&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyse the power of graph neural networks (GNNs) in terms of Boolean circuit complexity and descriptive complexity.  We prove that the graph queries that can be computed by a polynomial-size bounded-depth family of GNNs are exactly those definable in the guarded fragment GFO+C of first-order logic with counting and with built-in relations. This puts GNNs in the circuit complexity class TC^0. Remarkably, the GNN families may use arbitrary real weights and a wide class of activation functions that includes the standard ReLU, logistic "sigmod", and hyperbolic tangent functions. If the GNNs are allowed to use random initialisation and global readout (both standard features of GNNs widely used in practice), they can compute exactly the same queries as bounded depth Boolean circuits with threshold gates, that is, exactly the queries in TC^0.  Moreover, we show that queries computable by a single GNN with piecewise linear activations and rational weights are definable in GFO+C without bui
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Rashomon&#30340;&#22235;&#37325;&#22863;&#65292;&#36825;&#26159;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#26469;&#33258;&#19981;&#21516;&#31867;&#21035;&#30340;&#22235;&#20010;&#27169;&#22411;&#20855;&#26377;&#20960;&#20046;&#30456;&#21516;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#21516;&#26102;&#20854;&#21487;&#35270;&#21270;&#25581;&#31034;&#20102;&#26497;&#20854;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#29702;&#35299;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#24615;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2302.13356</link><description>&lt;p&gt;
&#34920;&#29616;&#19981;&#36275;&#20197;&#20026;&#30408;&#65292;&#28145;&#31350;Rashomon&#30340;&#22235;&#37325;&#22863;
&lt;/p&gt;
&lt;p&gt;
Performance is not enough: a story of the Rashomon's quartet. (arXiv:2302.13356v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Rashomon&#30340;&#22235;&#37325;&#22863;&#65292;&#36825;&#26159;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#26469;&#33258;&#19981;&#21516;&#31867;&#21035;&#30340;&#22235;&#20010;&#27169;&#22411;&#20855;&#26377;&#20960;&#20046;&#30456;&#21516;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#21516;&#26102;&#20854;&#21487;&#35270;&#21270;&#25581;&#31034;&#20102;&#26497;&#20854;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#29702;&#35299;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#24615;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#24314;&#27169;&#36890;&#24120;&#34987;&#31616;&#21270;&#20026;&#23547;&#25214;&#26368;&#20248;&#27169;&#22411;&#26469;&#20248;&#21270;&#36873;&#23450;&#30340;&#24615;&#33021;&#24230;&#37327;&#12290;&#20294;&#22914;&#26524;&#31532;&#20108;&#20248;&#27169;&#22411;&#33021;&#22815;&#20197;&#23436;&#20840;&#19981;&#21516;&#30340;&#26041;&#24335;&#21516;&#26679;&#25551;&#36848;&#25968;&#25454;&#21602;&#65311;&#31532;&#19977;&#20010;&#27169;&#22411;&#21602;&#65311;&#26368;&#26377;&#25928;&#30340;&#27169;&#22411;&#20250;&#23398;&#21040;&#23436;&#20840;&#19981;&#21516;&#30340;&#25968;&#25454;&#20851;&#31995;&#21527;&#65311;&#21463;&#21040;Anscombe&#22235;&#37325;&#22863;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;Rashomon&#30340;&#22235;&#37325;&#22863;&#65292;&#36825;&#26159;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#26469;&#33258;&#19981;&#21516;&#31867;&#21035;&#30340;&#22235;&#20010;&#27169;&#22411;&#20855;&#26377;&#20960;&#20046;&#30456;&#21516;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#21487;&#35270;&#21270;&#25581;&#31034;&#20102;&#26497;&#20854;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#29702;&#35299;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#24615;&#32467;&#26500;&#12290;&#24341;&#20837;&#30340;&#31616;&#21333;&#31034;&#20363;&#26088;&#22312;&#36827;&#19968;&#27493;&#20419;&#36827;&#21487;&#35270;&#21270;&#20316;&#20026;&#27604;&#36739;&#39044;&#27979;&#27169;&#22411;&#36229;&#36234;&#24615;&#33021;&#30340;&#24517;&#35201;&#24037;&#20855;&#12290;&#25105;&#20204;&#38656;&#35201;&#24320;&#21457;&#23500;&#26377;&#27934;&#23519;&#21147;&#30340;&#25216;&#26415;&#26469;&#35299;&#37322;&#27169;&#22411;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictive modelling is often reduced to finding the best model that optimizes a selected performance measure. But what if the second-best model describes the data equally well but in a completely different way? What about the third? Is it possible that the most effective models learn completely different relationships in the data? Inspired by Anscombe's quartet, this paper introduces Rashomon's quartet, a synthetic dataset for which four models from different classes have practically identical predictive performance. However, their visualization reveals drastically distinct ways of understanding the correlation structure in data. The introduced simple illustrative example aims to further facilitate visualization as a mandatory tool to compare predictive models beyond their performance. We need to develop insightful techniques for the explanatory analysis of model sets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20808;&#21069;&#30340;&#20540;&#20989;&#25968;&#36817;&#20284;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20219;&#24847;&#36817;&#20284;&#20540;&#20989;&#25968;&#26469;&#25512;&#23548;&#24863;&#20852;&#36259;&#30340;&#26368;&#20248;&#20540;&#20989;&#25968;&#30340;&#21452;&#36793;&#30028;&#38480;&#30340;&#26041;&#27861;&#65292;&#24182;&#20026;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#35823;&#24046;&#20998;&#26512;&#25552;&#20379;&#20102;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#31616;&#21333;&#39046;&#22495;&#36827;&#34892;&#25968;&#20540;&#39564;&#35777;&#35777;&#23454;&#20102;&#36825;&#20123;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.09676</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#20540;&#20989;&#25968;&#30340;&#21452;&#36793;&#30028;&#38480;&#26469;&#21033;&#29992;&#20808;&#21069;&#30693;&#35782;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;
&lt;/p&gt;
&lt;p&gt;
Leveraging Prior Knowledge in Reinforcement Learning via Double-Sided Bounds on the Value Function. (arXiv:2302.09676v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09676
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20808;&#21069;&#30340;&#20540;&#20989;&#25968;&#36817;&#20284;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20219;&#24847;&#36817;&#20284;&#20540;&#20989;&#25968;&#26469;&#25512;&#23548;&#24863;&#20852;&#36259;&#30340;&#26368;&#20248;&#20540;&#20989;&#25968;&#30340;&#21452;&#36793;&#30028;&#38480;&#30340;&#26041;&#27861;&#65292;&#24182;&#20026;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#35823;&#24046;&#20998;&#26512;&#25552;&#20379;&#20102;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#31616;&#21333;&#39046;&#22495;&#36827;&#34892;&#25968;&#20540;&#39564;&#35777;&#35777;&#23454;&#20102;&#36825;&#20123;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#20195;&#29702;&#26426;&#22120;&#20154;&#33021;&#22815;&#21033;&#29992;&#20197;&#24448;&#30340;&#32463;&#39564;&#23545;&#20110;&#39640;&#25928;&#35299;&#20915;&#26032;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#65292;&#21487;&#20197;&#20174;&#20043;&#21069;&#25512;&#23548;&#30340;&#20540;&#20989;&#25968;&#20013;&#33719;&#24471;&#26032;&#20219;&#21153;&#30340;&#36817;&#20284;&#35299;&#20915;&#26041;&#26696;&#65292;&#20363;&#22914;&#36801;&#31227;&#23398;&#20064;&#65292;&#35838;&#31243;&#23398;&#20064;&#21644;&#32452;&#21512;&#24615;&#31561;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#20351;&#29992;&#20540;&#20989;&#25968;&#26469;&#33719;&#24471;&#26032;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#30340;&#38646;&#26679;&#26412;&#36817;&#20284;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20219;&#24847;&#36817;&#20284;&#20540;&#20989;&#25968;&#22914;&#20309;&#29992;&#26469;&#25512;&#23548;&#23545;&#24863;&#20852;&#36259;&#30340;&#26368;&#20248;&#20540;&#20989;&#25968;&#30340;&#21452;&#36793;&#30028;&#38480;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20026;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#35823;&#24046;&#20998;&#26512;&#25193;&#23637;&#20102;&#36825;&#20010;&#26694;&#26550;&#12290;&#25512;&#23548;&#20986;&#30340;&#32467;&#26524;&#20026;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#35009;&#21098;&#25552;&#20379;&#20102;&#26032;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#31616;&#21333;&#39046;&#22495;&#36827;&#34892;&#20102;&#25968;&#20540;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
An agent's ability to leverage past experience is critical for efficiently solving new tasks. Approximate solutions for new tasks can be obtained from previously derived value functions, as demonstrated by research on transfer learning, curriculum learning, and compositionality. However, prior work has primarily focused on using value functions to obtain zero-shot approximations for solutions to a new task. In this work, we show how an arbitrary approximation for the value function can be used to derive double-sided bounds on the optimal value function of interest. We further extend the framework with error analysis for continuous state and action spaces. The derived results lead to new approaches for clipping during training which we validate numerically in simple domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;&#20855;&#26377;&#37325;&#23614;&#22122;&#22768;&#30340;&#38750;&#20984;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#20013;&#65292;&#25913;&#36827;&#20102;Cutkosky&#21644;Mehta&#30340;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#36817;&#20046;&#26368;&#20248;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#32780;&#26080;&#38656;&#23545;&#38543;&#26426;&#26799;&#24230;&#30340;&#30697;&#26465;&#20214;&#36827;&#34892;&#39069;&#22806;&#30340;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2302.06763</link><description>&lt;p&gt;
&#20197;(&#23567;)&#32467;&#26500;&#31361;&#30772;&#19979;&#30028;&#65306;&#20855;&#26377;&#37325;&#23614;&#22122;&#22768;&#30340;&#38750;&#20984;&#38543;&#26426;&#20248;&#21270;&#20013;&#30340;&#21152;&#36895;&#12290;(arXiv:2302.06763v2 [cs.LG] &#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
Breaking the Lower Bound with (Little) Structure: Acceleration in Non-Convex Stochastic Optimization with Heavy-Tailed Noise. (arXiv:2302.06763v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#20855;&#26377;&#37325;&#23614;&#22122;&#22768;&#30340;&#38750;&#20984;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#20013;&#65292;&#25913;&#36827;&#20102;Cutkosky&#21644;Mehta&#30340;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#36817;&#20046;&#26368;&#20248;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#32780;&#26080;&#38656;&#23545;&#38543;&#26426;&#26799;&#24230;&#30340;&#30697;&#26465;&#20214;&#36827;&#34892;&#39069;&#22806;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37325;&#23614;&#22122;&#22768;&#21306;&#22495;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20855;&#26377;&#24179;&#28369;&#20294;&#19981;&#19968;&#23450;&#26159;&#20984;&#30446;&#26631;&#30340;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#20551;&#35774;&#38543;&#26426;&#26799;&#24230;&#30340;&#22122;&#22768;&#20855;&#26377;&#26377;&#30028;&#30340;$p$&#38454;&#30697;($p\in(1,2]$)&#12290;Zhang&#31561;&#20154;(2020)&#39318;&#27425;&#35777;&#26126;&#20102;&#25910;&#25947;&#30340;$\Omega(T^{\frac{1-p}{3p-2}})$&#19979;&#30028;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#21098;&#20999;&#31639;&#27861;&#65292;&#20197;&#21305;&#37197;&#36825;&#20010;&#26368;&#20248;&#36895;&#29575;&#12290;Cutkosky&#21644;Mehta(2021)&#25552;&#20986;&#20102;&#21478;&#19968;&#31181;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#34987;&#35777;&#26126;&#33021;&#22815;&#23454;&#29616;&#36817;&#20046;&#26368;&#20248;&#30340;&#39640;&#27010;&#29575;&#25910;&#25947;&#20445;&#35777;$O(\log(T/\delta)T^{\frac{1-p}{3p-2}})$&#65292;&#20854;&#20013;$\delta$&#26159;&#22833;&#36133;&#30340;&#27010;&#29575;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#29702;&#24819;&#30340;&#20445;&#35777;&#21482;&#22312;&#38468;&#21152;&#30340;&#20551;&#35774;&#19979;&#25104;&#31435;&#65292;&#21363;&#38543;&#26426;&#26799;&#24230;&#26412;&#36523;&#22312;$p$&#38454;&#30697;&#19978;&#26377;&#30028;&#65292;&#32780;&#21363;&#20351;&#23545;&#20110;&#20108;&#27425;&#30446;&#26631;&#21644;&#20013;&#24515;&#21270;&#30340;&#39640;&#26031;&#22122;&#22768;&#65292;&#36825;&#20010;&#20551;&#35774;&#20063;&#19981;&#25104;&#31435;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25913;&#36827;&#20102;Cutkosky&#21644;Mehta(2021)&#20013;&#31639;&#27861;&#30340;&#20998;&#26512;&#65292;&#20197;&#33719;&#24471;&#30456;&#21516;&#30340;&#36817;&#20046;&#26368;&#20248;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the stochastic optimization problem with smooth but not necessarily convex objectives in the heavy-tailed noise regime, where the stochastic gradient's noise is assumed to have bounded $p$th moment ($p\in(1,2]$). Zhang et al. (2020) is the first to prove the $\Omega(T^{\frac{1-p}{3p-2}})$ lower bound for convergence (in expectation) and provides a simple clipping algorithm that matches this optimal rate. Cutkosky and Mehta (2021) proposes another algorithm, which is shown to achieve the nearly optimal high-probability convergence guarantee $O(\log(T/\delta)T^{\frac{1-p}{3p-2}})$, where $\delta$ is the probability of failure. However, this desirable guarantee is only established under the additional assumption that the stochastic gradient itself is bounded in $p$th moment, which fails to hold even for quadratic objectives and centered Gaussian noise.  In this work, we first improve the analysis of the algorithm in Cutkosky and Mehta (2021) to obtain the same nearly optimal h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#35889;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#27969;&#24418;&#19978;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#24182;&#22312;&#29699;&#38754;&#21644;&#29615;&#38754;&#19978;&#24471;&#21040;&#20102;&#25104;&#21151;&#24212;&#29992;&#12290;&#23545;&#27604;&#26631;&#20934;&#26550;&#26500;&#65292;&#26412;&#25991;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2302.05322</link><description>&lt;p&gt;
&#20351;&#29992;&#35889;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#27969;&#24418;&#19978;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#25968;&#20540;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Numerical Methods For PDEs Over Manifolds Using Spectral Physics Informed Neural Networks. (arXiv:2302.05322v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#35889;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#27969;&#24418;&#19978;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#24182;&#22312;&#29699;&#38754;&#21644;&#29615;&#38754;&#19978;&#24471;&#21040;&#20102;&#25104;&#21151;&#24212;&#29992;&#12290;&#23545;&#27604;&#26631;&#20934;&#26550;&#26500;&#65292;&#26412;&#25991;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35889;&#26041;&#27861;&#23545;&#40784;&#26550;&#26500;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#27714;&#35299;&#27969;&#24418;&#19978;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#36825;&#20123;&#32593;&#32476;&#34987;&#35757;&#32451;&#20026;&#23558;&#21021;&#22987;&#26465;&#20214;&#12289;&#26102;&#38388;&#25139;&#21644;&#27969;&#24418;&#19978;&#30340;&#28857;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36755;&#20986;&#32473;&#23450;&#26102;&#38388;&#21644;&#28857;&#22788;&#30340;&#35299;&#30340;&#20540;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;&#35889;&#26041;&#27861;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#22312;&#21306;&#38388;&#19978;&#30340;&#28909;&#26041;&#31243;&#12289;&#29699;&#38754;&#21644;&#29615;&#38754;&#38750;&#32447;&#24615;&#26041;&#31243;&#30340;&#35299;&#20915;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22522;&#20110;&#35889;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20248;&#20110;&#26631;&#20934;&#29289;&#29702;&#20449;&#24687;&#26550;&#26500;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#21253;&#25324;&#23545;&#24191;&#27867;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#27867;&#21270;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an approach for solving PDEs over manifolds using physics informed neural networks whose architecture aligns with spectral methods. The networks are trained to take in as input samples of an initial condition, a time stamp and point(s) on the manifold and then output the solution's value at the given time and point(s). We provide proofs of our method for the heat equation on the interval and examples of unique network architectures that are adapted to nonlinear equations on the sphere and the torus. We also show that our spectral-inspired neural network architectures outperform the standard physics informed architectures. Our extensive experimental results include generalization studies where the testing dataset of initial conditions is randomly sampled from a significantly larger space than the training set.
&lt;/p&gt;</description></item><item><title>V1T&#26159;&#19968;&#31181;&#22522;&#20110;Vision Transformer&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#21487;&#20197;&#36328;&#21160;&#29289;&#23398;&#20064;&#20849;&#20139;&#30340;&#35270;&#35273;&#21644;&#34892;&#20026;&#34920;&#31034;&#65292;&#23545;&#33258;&#28982;&#35270;&#35273;&#21050;&#28608;&#19979;&#30340;&#35270;&#35273;&#30382;&#23618;&#31070;&#32463;&#21709;&#24212;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#20248;&#20110;&#20043;&#21069;&#22522;&#20110;&#21367;&#31215;&#30340;&#27169;&#22411;&#36229;&#36807;12.7&#65285;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;Transformer&#23398;&#20064;&#30340;&#33258;&#25105;&#20851;&#27880;&#26435;&#37325;&#36824;&#33021;&#22815;&#23637;&#31034;&#19982;&#32676;&#20307;&#24863;&#21463;&#37326;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.03023</link><description>&lt;p&gt;
V1T&#65306;&#20351;&#29992;Vision Transformer&#36827;&#34892;&#22823;&#35268;&#27169;&#23567;&#40736;V1&#21709;&#24212;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
V1T: large-scale mouse V1 response prediction using a Vision Transformer. (arXiv:2302.03023v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03023
&lt;/p&gt;
&lt;p&gt;
V1T&#26159;&#19968;&#31181;&#22522;&#20110;Vision Transformer&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#21487;&#20197;&#36328;&#21160;&#29289;&#23398;&#20064;&#20849;&#20139;&#30340;&#35270;&#35273;&#21644;&#34892;&#20026;&#34920;&#31034;&#65292;&#23545;&#33258;&#28982;&#35270;&#35273;&#21050;&#28608;&#19979;&#30340;&#35270;&#35273;&#30382;&#23618;&#31070;&#32463;&#21709;&#24212;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#20248;&#20110;&#20043;&#21069;&#22522;&#20110;&#21367;&#31215;&#30340;&#27169;&#22411;&#36229;&#36807;12.7&#65285;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;Transformer&#23398;&#20064;&#30340;&#33258;&#25105;&#20851;&#27880;&#26435;&#37325;&#36824;&#33021;&#22815;&#23637;&#31034;&#19982;&#32676;&#20307;&#24863;&#21463;&#37326;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#31070;&#32463;&#31185;&#23398;&#20013;&#65292;&#23545;&#33258;&#28982;&#35270;&#35273;&#21050;&#28608;&#19979;&#30340;&#35270;&#35273;&#30382;&#23618;&#31070;&#32463;&#21709;&#24212;&#30340;&#31934;&#30830;&#39044;&#27979;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;V1T&#65292;&#19968;&#31181;&#22522;&#20110;Vision Transformer&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#21487;&#20197;&#36328;&#21160;&#29289;&#23398;&#20064;&#20849;&#20139;&#30340;&#35270;&#35273;&#21644;&#34892;&#20026;&#34920;&#31034;&#12290;&#25105;&#20204;&#23545;&#35760;&#24405;&#20110;&#23567;&#40736;&#21407;&#22987;&#35270;&#35273;&#30382;&#23618;&#30340;&#20004;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#20248;&#20110;&#20043;&#21069;&#22522;&#20110;&#21367;&#31215;&#30340;&#27169;&#22411;&#36229;&#36807;12.7&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Transformer&#23398;&#20064;&#30340;&#33258;&#25105;&#20851;&#27880;&#26435;&#37325;&#19982;&#32676;&#20307;&#24863;&#21463;&#37326;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20026;&#31070;&#32463;&#21709;&#24212;&#39044;&#27979;&#35774;&#31435;&#20102;&#26032;&#30340;&#22522;&#20934;&#65292;&#24182;&#21487;&#19982;&#34892;&#20026;&#21644;&#31070;&#32463;&#35760;&#24405;&#19968;&#36215;&#20351;&#29992;&#65292;&#20197;&#25581;&#31034;&#35270;&#35273;&#30382;&#23618;&#30340;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate predictive models of the visual cortex neural response to natural visual stimuli remain a challenge in computational neuroscience. In this work, we introduce V1T, a novel Vision Transformer based architecture that learns a shared visual and behavioral representation across animals. We evaluate our model on two large datasets recorded from mouse primary visual cortex and outperform previous convolution-based models by more than 12.7% in prediction performance. Moreover, we show that the self-attention weights learned by the Transformer correlate with the population receptive fields. Our model thus sets a new benchmark for neural response prediction and can be used jointly with behavioral and neural recordings to reveal meaningful characteristic features of the visual cortex.
&lt;/p&gt;</description></item><item><title>NeuRI&#26159;&#19968;&#31181;&#20840;&#33258;&#21160;&#21270;&#29983;&#25104;&#30001;&#25968;&#30334;&#31181;&#25805;&#20316;&#31526;&#32452;&#25104;&#30340;&#26377;&#25928;&#19988;&#22810;&#26679;&#21270;&#30340;DL&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#24402;&#32435;&#24335;&#31243;&#24207;&#21512;&#25104;&#25512;&#26029;&#25805;&#20316;&#31526;&#32422;&#26463;&#26465;&#20214;&#65292;&#24182;&#37319;&#29992;&#31526;&#21495;&#21644;&#20855;&#20307;&#25805;&#20316;&#30340;&#28151;&#21512;&#27169;&#22411;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2302.02261</link><description>&lt;p&gt;
NeuRI&#65306;&#36890;&#36807;&#24402;&#32435;&#35268;&#21017;&#25512;&#26029;&#23454;&#29616;DNN&#29983;&#25104;&#30340;&#22810;&#26679;&#21270;
&lt;/p&gt;
&lt;p&gt;
NeuRI: Diversifying DNN Generation via Inductive Rule Inference. (arXiv:2302.02261v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02261
&lt;/p&gt;
&lt;p&gt;
NeuRI&#26159;&#19968;&#31181;&#20840;&#33258;&#21160;&#21270;&#29983;&#25104;&#30001;&#25968;&#30334;&#31181;&#25805;&#20316;&#31526;&#32452;&#25104;&#30340;&#26377;&#25928;&#19988;&#22810;&#26679;&#21270;&#30340;DL&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#24402;&#32435;&#24335;&#31243;&#24207;&#21512;&#25104;&#25512;&#26029;&#25805;&#20316;&#31526;&#32422;&#26463;&#26465;&#20214;&#65292;&#24182;&#37319;&#29992;&#31526;&#21495;&#21644;&#20855;&#20307;&#25805;&#20316;&#30340;&#28151;&#21512;&#27169;&#22411;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;(DL)&#22312;&#21508;&#20010;&#34892;&#19994;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#25913;&#21892;&#20915;&#31574;&#21644;&#33258;&#21160;&#21270;&#27969;&#31243;&#65292;&#20854;&#25512;&#21160;&#21147;&#26469;&#33258;&#19981;&#26029;&#21457;&#23637;&#30340;DL&#24211;&#21644;&#32534;&#35793;&#22120;&#12290;DL&#31995;&#32479;&#30340;&#27491;&#30830;&#24615;&#23545;&#20110;&#20449;&#20219;DL&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#28010;&#28526;&#19968;&#30452;&#22312;&#30740;&#31350;&#29992;&#20110;&#27169;&#31946;DL&#31995;&#32479;&#30340;&#33258;&#21160;&#21270;&#27979;&#35797;&#29992;&#20363;&#21512;&#25104;&#65288;&#21363;DNN&#27169;&#22411;&#21644;&#20854;&#36755;&#20837;&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27169;&#22411;&#29983;&#25104;&#22120;&#21482;&#28085;&#30422;&#20102;&#26377;&#38480;&#25968;&#37327;&#30340;&#25805;&#20316;&#31526;&#65292;&#32570;&#20047;&#24191;&#27867;&#24314;&#27169;&#25805;&#20316;&#31526;&#32422;&#26463;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NeuRI&#65292;&#19968;&#31181;&#20840;&#33258;&#21160;&#29983;&#25104;&#30001;&#25968;&#30334;&#31181;&#25805;&#20316;&#31526;&#32452;&#25104;&#30340;&#26377;&#25928;&#19988;&#22810;&#26679;&#21270;&#30340;DL&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;NeuRI&#37319;&#29992;&#20102;&#19977;&#27493;&#36807;&#31243;&#65306;(i)&#20174;&#21508;&#31181;&#26469;&#28304;&#25910;&#38598;&#26377;&#25928;&#21644;&#26080;&#25928;&#30340;API&#36861;&#36394;&#65307;(ii)&#22312;&#36861;&#36394;&#25968;&#25454;&#19978;&#24212;&#29992;&#24402;&#32435;&#24335;&#31243;&#24207;&#21512;&#25104;&#65292;&#25512;&#26029;&#26500;&#24314;&#26377;&#25928;&#27169;&#22411;&#30340;&#32422;&#26463;&#26465;&#20214;&#65307;(iii)&#36890;&#36807;&#32467;&#21512;&#31526;&#21495;&#21644;&#20855;&#20307;&#25805;&#20316;&#25191;&#34892;&#28151;&#21512;&#27169;&#22411;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning (DL) is prevalently used in various industries to improve decision-making and automate processes, driven by the ever-evolving DL libraries and compilers. The correctness of DL systems is crucial for trust in DL applications. As such, the recent wave of research has been studying the automated synthesis of test-cases (i.e., DNN models and their inputs) for fuzzing DL systems. However, existing model generators only subsume a limited number of operators, lacking the ability to pervasively model operator constraints. To address this challenge, we propose NeuRI, a fully automated approach for generating valid and diverse DL models composed of hundreds of types of operators. NeuRI adopts a three-step process: (i) collecting valid and invalid API traces from various sources; (ii) applying inductive program synthesis over the traces to infer the constraints for constructing valid models; and (iii) performing hybrid model generation by incorporating both symbolic and concrete ope
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#29366;Parzen&#20272;&#35745;&#22120;&#65288;TPE&#65289;&#21644;&#20998;&#35299;&#31639;&#27861;&#30340;&#26032;&#26694;&#26550;&#65288;TPE-VMD-TFT&#65289;&#65292;&#29992;&#20110;24&#23567;&#26102;&#21644;48&#23567;&#26102;&#20043;&#21069;&#30340;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#12290;&#22312;&#27861;&#22269;&#30005;&#21147;&#20844;&#21496;Engie&#30340;&#39118;&#33021;&#25968;&#25454;&#38598;&#19978;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2302.01222</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#38388;&#27880;&#24847;&#26426;&#21046;&#30340;&#20013;&#26399;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A novel framework for medium-term wind power prediction based on temporal attention mechanisms. (arXiv:2302.01222v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#29366;Parzen&#20272;&#35745;&#22120;&#65288;TPE&#65289;&#21644;&#20998;&#35299;&#31639;&#27861;&#30340;&#26032;&#26694;&#26550;&#65288;TPE-VMD-TFT&#65289;&#65292;&#29992;&#20110;24&#23567;&#26102;&#21644;48&#23567;&#26102;&#20043;&#21069;&#30340;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#12290;&#22312;&#27861;&#22269;&#30005;&#21147;&#20844;&#21496;Engie&#30340;&#39118;&#33021;&#25968;&#25454;&#38598;&#19978;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#33021;&#26159;&#19968;&#31181;&#24191;&#27867;&#20998;&#24067;&#12289;&#21487;&#20877;&#29983;&#21644;&#29615;&#20445;&#30340;&#33021;&#28304;&#65292;&#23545;&#32531;&#35299;&#20840;&#29699;&#21464;&#26262;&#21644;&#33021;&#28304;&#30701;&#32570;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#19981;&#30830;&#23450;&#24615;&#21644;&#27874;&#21160;&#24615;&#65292;&#22823;&#35268;&#27169;&#39118;&#30005;&#31995;&#32479;&#30340;&#32593;&#26684;&#38598;&#25104;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20013;&#26399;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#21487;&#20197;&#20026;&#33021;&#37327;&#35843;&#24230;&#25552;&#20379;&#22522;&#26412;&#20381;&#25454;&#65292;&#22240;&#27492;&#31934;&#30830;&#30340;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#29366;Parzen&#20272;&#35745;&#22120;&#65288;TPE&#65289;&#21644;&#20998;&#35299;&#31639;&#27861;&#30340;&#26032;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#21464;&#20998;&#27169;&#24335;&#20998;&#35299;&#65288;VMD&#65289;&#21644;&#26102;&#38388;&#34701;&#21512;&#21464;&#21387;&#22120;&#65288;TFT&#65289;&#23450;&#20041;&#20102;24&#23567;&#26102;&#21644;48&#23567;&#26102;&#20043;&#21069;&#30340;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#30340;TPE-VMD-TFT&#26041;&#27861;&#12290;&#22312;&#27861;&#22269;&#30005;&#21147;&#20844;&#21496;Engie&#30340;&#39118;&#33021;&#25968;&#25454;&#38598;&#19978;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wind energy is a widely distributed, recyclable and environmentally friendly energy source that plays an important role in mitigating global warming and energy shortages. Wind energy's uncertainty and fluctuating nature makes grid integration of large-scale wind energy systems challenging. Medium-term wind power forecasts can provide an essential basis for energy dispatch, so accurate wind power forecasts are essential. Much research has yielded excellent results in recent years. However, many of them require additional experimentation and analysis when applied to other data. In this paper, we propose a novel short-term forecasting framework by tree-structured parzen estimator (TPE) and decomposition algorithms. This framework defines the TPE-VMD-TFT method for 24-h and 48-h ahead wind power forecasting based on variational mode decomposition (VMD) and time fusion transformer (TFT). In the Engie wind dataset from the electricity company in France, the results show that the proposed met
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#39318;&#27425;&#35780;&#20272;&#20102;&#22312;&#26080;&#38480;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;&#21160;&#24577;&#20013;&#20171;&#25928;&#24212;&#65292;&#24182;&#24320;&#21457;&#20102;&#40065;&#26834;&#21644;&#21322;&#21442;&#25968;&#26377;&#25928;&#30340;&#20272;&#35745;&#26041;&#27861;&#26469;&#25512;&#26029;&#36825;&#20123;&#22240;&#26524;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2301.13348</link><description>&lt;p&gt;
&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#21160;&#24577;&#20013;&#20171;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Reinforcement Learning Framework for Dynamic Mediation Analysis. (arXiv:2301.13348v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13348
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#39318;&#27425;&#35780;&#20272;&#20102;&#22312;&#26080;&#38480;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;&#21160;&#24577;&#20013;&#20171;&#25928;&#24212;&#65292;&#24182;&#24320;&#21457;&#20102;&#40065;&#26834;&#21644;&#21322;&#21442;&#25968;&#26377;&#25928;&#30340;&#20272;&#35745;&#26041;&#27861;&#26469;&#25512;&#26029;&#36825;&#20123;&#22240;&#26524;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#20171;&#20998;&#26512;&#36890;&#36807;&#23398;&#20064;&#20171;&#23548;&#21464;&#37327;&#22312;&#27835;&#30103;&#21644;&#32467;&#26524;&#20043;&#38388;&#20256;&#36882;&#30340;&#22240;&#26524;&#25928;&#24212;&#65292;&#22312;&#21508;&#20010;&#31185;&#23398;&#39046;&#22495;&#20013;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20197;&#38416;&#26126;&#22240;&#26524;&#20851;&#31995;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#38598;&#20013;&#22312;&#28857;&#26292;&#38706;&#30740;&#31350;&#20013;&#65292;&#20854;&#20013;&#27599;&#20010;&#21463;&#35797;&#32773;&#21482;&#22312;&#19968;&#20010;&#26102;&#38388;&#28857;&#25509;&#21463;&#19968;&#31181;&#27835;&#30103;&#12290;&#28982;&#32780;&#65292;&#26377;&#35768;&#22810;&#24212;&#29992;&#65288;&#20363;&#22914;&#31227;&#21160;&#20581;&#24247;&#65289;&#22312;&#36825;&#20123;&#24212;&#29992;&#20013;&#65292;&#27835;&#30103;&#26159;&#25353;&#39034;&#24207;&#20998;&#37197;&#30340;&#65292;&#21160;&#24577;&#20013;&#20171;&#25928;&#24212;&#26159;&#20027;&#35201;&#20851;&#27880;&#30340;&#23545;&#35937;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#65292;&#25105;&#20204;&#39318;&#27425;&#35780;&#20272;&#22312;&#26080;&#38480;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;&#21160;&#24577;&#20013;&#20171;&#25928;&#24212;&#12290;&#25105;&#20204;&#23558;&#24179;&#22343;&#27835;&#30103;&#25928;&#24212;&#20998;&#35299;&#20026;&#30452;&#25509;&#25928;&#24212;&#12289;&#20013;&#20171;&#25928;&#24212;&#12289;&#24310;&#36831;&#30452;&#25509;&#25928;&#24212;&#21644;&#24310;&#36831;&#20013;&#20171;&#25928;&#24212;&#12290;&#22312;&#30830;&#23450;&#27599;&#20010;&#25928;&#24212;&#25104;&#20998;&#21518;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;RL&#26694;&#26550;&#19979;&#24320;&#21457;&#40065;&#26834;&#21644;&#21322;&#21442;&#25968;&#26377;&#25928;&#30340;&#20272;&#35745;&#22120;&#26469;&#25512;&#26029;&#36825;&#20123;&#22240;&#26524;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mediation analysis learns the causal effect transmitted via mediator variables between treatments and outcomes and receives increasing attention in various scientific domains to elucidate causal relations. Most existing works focus on point-exposure studies where each subject only receives one treatment at a single time point. However, there are a number of applications (e.g., mobile health) where the treatments are sequentially assigned over time and the dynamic mediation effects are of primary interest. Proposing a reinforcement learning (RL) framework, we are the first to evaluate dynamic mediation effects in settings with infinite horizons. We decompose the average treatment effect into an immediate direct effect, an immediate mediation effect, a delayed direct effect, and a delayed mediation effect. Upon the identification of each effect component, we further develop robust and semi-parametrically efficient estimators under the RL framework to infer these causal effects. The super
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#39318;&#20010;&#22312;&#24179;&#28369;&#21333;&#35843;&#21338;&#24328;&#20013;&#23454;&#29616;&#21452;&#37325;&#26368;&#20248;&#26080;&#36951;&#25022;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#22312;&#23545;&#25239;&#29615;&#22659;&#19979;&#20855;&#26377;&#26368;&#20248;&#30340;&#36951;&#25022;&#21644;&#22312;&#22810;&#20154;&#24179;&#28369;&#21333;&#35843;&#21338;&#24328;&#20013;&#20855;&#26377;&#26368;&#20248;&#30340;&#26368;&#32456;&#36845;&#20195;&#25910;&#25947;&#36895;&#29575;&#21040;&#36798;&#32435;&#20160;&#22343;&#34913;&#12290;</title><link>http://arxiv.org/abs/2301.13120</link><description>&lt;p&gt;
&#20855;&#26377;&#21452;&#37325;&#26368;&#20248;&#26080;&#36951;&#25022;&#30340;&#21333;&#35843;&#21338;&#24328;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Doubly Optimal No-Regret Learning in Monotone Games. (arXiv:2301.13120v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13120
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#39318;&#20010;&#22312;&#24179;&#28369;&#21333;&#35843;&#21338;&#24328;&#20013;&#23454;&#29616;&#21452;&#37325;&#26368;&#20248;&#26080;&#36951;&#25022;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#22312;&#23545;&#25239;&#29615;&#22659;&#19979;&#20855;&#26377;&#26368;&#20248;&#30340;&#36951;&#25022;&#21644;&#22312;&#22810;&#20154;&#24179;&#28369;&#21333;&#35843;&#21338;&#24328;&#20013;&#20855;&#26377;&#26368;&#20248;&#30340;&#26368;&#32456;&#36845;&#20195;&#25910;&#25947;&#36895;&#29575;&#21040;&#36798;&#32435;&#20160;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22810;&#20154;&#24179;&#28369;&#21333;&#35843;&#21338;&#24328;&#20013;&#30340;&#22312;&#32447;&#23398;&#20064;&#12290;&#29616;&#26377;&#31639;&#27861;&#23384;&#22312;&#35832;&#22914;&#65288;1&#65289;&#20165;&#36866;&#29992;&#20110;&#24378;&#21333;&#35843;&#21338;&#24328;&#65307;&#65288;2&#65289;&#32570;&#20047;&#26080;&#36951;&#25022;&#20445;&#35777;&#65307;&#65288;3&#65289;&#21482;&#26377;&#28176;&#36827;&#25110;&#24930;&#36895;&#30340;$O(\frac{1}{\sqrt{T}})$&#26368;&#32456;&#36845;&#20195;&#25910;&#25947;&#36895;&#29575;&#21040;&#32435;&#20160;&#22343;&#34913;&#12290;&#34429;&#28982;&#23545;&#20110;&#21253;&#25324;&#24191;&#27867;&#30740;&#31350;&#30340;&#22806;&#25512;&#26799;&#24230;&#31639;&#27861;&#21644;&#20048;&#35266;&#26799;&#24230;&#31639;&#27861;&#22312;&#20869;&#30340;&#22823;&#31867;&#31639;&#27861;&#65292;$O(\frac{1}{\sqrt{T}})$&#36895;&#29575;&#26159;&#32039;&#30830;&#30340;&#65292;&#20294;&#19981;&#26159;&#25152;&#26377;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#37117;&#26159;&#26368;&#20248;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21152;&#36895;&#20048;&#35266;&#26799;&#24230;&#65288;AOG&#65289;&#31639;&#27861;&#65292;&#36825;&#26159;&#39318;&#20010;&#22312;&#24179;&#28369;&#21333;&#35843;&#21338;&#24328;&#20013;&#23454;&#29616;&#21452;&#37325;&#26368;&#20248;&#26080;&#36951;&#25022;&#23398;&#20064;&#30340;&#31639;&#27861;&#12290;&#21363;&#25105;&#20204;&#30340;&#31639;&#27861;&#21516;&#26102;&#23454;&#29616;&#20102;&#65288;i&#65289;&#22312;&#23545;&#25239;&#29615;&#22659;&#19979;&#65292;&#23545;&#20110;&#24179;&#28369;&#20984;&#25439;&#22833;&#20989;&#25968;&#20855;&#26377;&#26368;&#20248;&#30340;$O(\sqrt{T})$&#36951;&#25022;&#21644;&#65288;ii&#65289;&#22312;&#22810;&#20154;&#24179;&#28369;&#21333;&#35843;&#21338;&#24328;&#20013;&#65292;&#20855;&#26377;&#26368;&#20248;&#30340;$O(\frac{1}{T})$&#26368;&#32456;&#36845;&#20195;&#21040;&#36798;&#32435;&#20160;&#22343;&#34913;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider online learning in multi-player smooth monotone games. Existing algorithms have limitations such as (1) being only applicable to strongly monotone games; (2) lacking the no-regret guarantee; (3) having only asymptotic or slow $O(\frac{1}{\sqrt{T}})$ last-iterate convergence rate to a Nash equilibrium. While the $O(\frac{1}{\sqrt{T}})$ rate is tight for a large class of algorithms including the well-studied extragradient algorithm and optimistic gradient algorithm, it is not optimal for all gradient-based algorithms.  We propose the accelerated optimistic gradient (AOG) algorithm, the first doubly optimal no-regret learning algorithm for smooth monotone games. Namely, our algorithm achieves both (i) the optimal $O(\sqrt{T})$ regret in the adversarial setting under smooth and convex loss functions and (ii) the optimal $O(\frac{1}{T})$ last-iterate convergence rate to a Nash equilibrium in multi-player smooth monotone games. As a byproduct of the accelerated last-iterate conve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35299;&#26512;&#20248;&#21270;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#21453;&#21521;&#20256;&#25773;&#20013;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#29983;&#25104;&#39640;&#25928;&#21487;&#35299;&#26512;&#30340;&#21453;&#21521;&#20256;&#25773;&#20248;&#21270;&#27169;&#22411;&#30340;&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#36890;&#36807;&#20248;&#21270;&#26144;&#23556;&#32479;&#19968;&#23637;&#24320;&#21644;&#35299;&#26512;&#24494;&#20998;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2301.12047</link><description>&lt;p&gt;
&#21453;&#21521;&#20256;&#25773;&#23637;&#24320;&#30340;&#25240;&#21472;&#20248;&#21270;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Backpropagation of Unrolled Solvers with Folded Optimization. (arXiv:2301.12047v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35299;&#26512;&#20248;&#21270;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#21453;&#21521;&#20256;&#25773;&#20013;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#29983;&#25104;&#39640;&#25928;&#21487;&#35299;&#26512;&#30340;&#21453;&#21521;&#20256;&#25773;&#20248;&#21270;&#27169;&#22411;&#30340;&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#36890;&#36807;&#20248;&#21270;&#26144;&#23556;&#32479;&#19968;&#23637;&#24320;&#21644;&#35299;&#26512;&#24494;&#20998;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#32593;&#32476;&#20013;&#23558;&#32422;&#26463;&#20248;&#21270;&#27169;&#22411;&#20316;&#20026;&#32452;&#20214;&#38598;&#25104;&#65292;&#21487;&#20197;&#22312;&#35768;&#22810;&#19987;&#38376;&#30340;&#23398;&#20064;&#20219;&#21153;&#19978;&#21462;&#24471;&#26377;&#24076;&#26395;&#30340;&#36827;&#23637;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#26159;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#26469;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#65292;&#32780;&#35813;&#35299;&#36890;&#24120;&#32570;&#20047;&#38381;&#21512;&#30340;&#24418;&#24335;&#12290;&#19968;&#31181;&#20856;&#22411;&#30340;&#31574;&#30053;&#26159;&#31639;&#27861;&#23637;&#24320;&#65292;&#23427;&#20381;&#36182;&#20110;&#36845;&#20195;&#27714;&#35299;&#22120;&#30340;&#33258;&#21160;&#24494;&#20998;&#25805;&#20316;&#12290;&#34429;&#28982;&#28789;&#27963;&#19988;&#36890;&#29992;&#65292;&#20294;&#22312;&#23454;&#38469;&#20013;&#65292;&#23637;&#24320;&#21487;&#33021;&#36935;&#21040;&#31934;&#24230;&#21644;&#25928;&#29575;&#38382;&#39064;&#12290;&#36890;&#36807;&#20248;&#21270;&#30340;&#35299;&#26512;&#24494;&#20998;&#21487;&#20197;&#36991;&#20813;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#24403;&#21069;&#30340;&#26694;&#26550;&#23545;&#20110;&#20248;&#21270;&#38382;&#39064;&#30340;&#24418;&#24335;&#26045;&#21152;&#20102;&#20005;&#26684;&#30340;&#35201;&#27714;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#23637;&#24320;&#20248;&#21270;&#21518;&#21521;&#20256;&#36882;&#30340;&#29702;&#35770;&#35265;&#35299;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#39640;&#25928;&#21487;&#35299;&#26512;&#30340;&#21453;&#21521;&#20256;&#25773;&#20248;&#21270;&#27169;&#22411;&#30340;&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#36890;&#36807;&#20248;&#21270;&#26144;&#23556;&#32479;&#19968;&#23637;&#24320;&#21644;&#35299;&#26512;&#24494;&#20998;&#30340;&#35270;&#35282;&#12290;&#22312;&#23454;&#39564;&#19978;&#36827;&#34892;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
The integration of constrained optimization models as components in deep networks has led to promising advances on many specialized learning tasks. A central challenge in this setting is backpropagation through the solution of an optimization problem, which typically lacks a closed form. One typical strategy is algorithm unrolling, which relies on automatic differentiation through the operations of an iterative solver. While flexible and general, unrolling can encounter accuracy and efficiency issues in practice. These issues can be avoided by analytical differentiation of the optimization, but current frameworks impose rigid requirements on the optimization problem's form. This paper provides theoretical insights into the backward pass of unrolled optimization, leading to a system for generating efficiently solvable analytical models of backpropagation. Additionally, it proposes a unifying view of unrolling and analytical differentiation through optimization mappings. Experiments over
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CoAD&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24039;&#21512;&#23398;&#20064;&#26469;&#36827;&#34892;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65292;&#22312;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#36890;&#36807;&#29305;&#24449;&#31354;&#38388;&#30340;&#24039;&#21512;&#34892;&#20026;&#26469;&#35782;&#21035;&#24322;&#24120;&#12290;</title><link>http://arxiv.org/abs/2301.11368</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#24039;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Coincident Learning for Unsupervised Anomaly Detection. (arXiv:2301.11368v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CoAD&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24039;&#21512;&#23398;&#20064;&#26469;&#36827;&#34892;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65292;&#22312;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#36890;&#36807;&#29305;&#24449;&#31354;&#38388;&#30340;&#24039;&#21512;&#34892;&#20026;&#26469;&#35782;&#21035;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#26159;&#22797;&#26434;&#31995;&#32479;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#27604;&#22914;&#24037;&#19994;&#35774;&#26045;&#12289;&#21046;&#36896;&#19994;&#12289;&#22823;&#22411;&#31185;&#23398;&#23454;&#39564;&#31561;&#65292;&#20854;&#20013;&#23376;&#31995;&#32479;&#30340;&#25925;&#38556;&#21487;&#33021;&#23548;&#33268;&#20302;&#20135;&#37327;&#12289;&#26377;&#32570;&#38519;&#30340;&#20135;&#21697;&#29978;&#33267;&#25439;&#22351;&#32452;&#20214;&#12290;&#34429;&#28982;&#22797;&#26434;&#31995;&#32479;&#36890;&#24120;&#26377;&#22823;&#37327;&#30340;&#25968;&#25454;&#65292;&#20294;&#26631;&#35760;&#30340;&#24322;&#24120;&#36890;&#24120;&#26159;&#31232;&#26377;&#30340;&#65288;&#29978;&#33267;&#19981;&#23384;&#22312;&#65289;&#65292;&#24182;&#19988;&#33719;&#21462;&#36215;&#26469;&#24456;&#26114;&#36149;&#12290;&#22240;&#27492;&#65292;&#26080;&#30417;&#30563;&#26041;&#27861;&#24456;&#24120;&#35265;&#65292;&#36890;&#24120;&#36890;&#36807;&#36317;&#31163;&#25110;&#23494;&#24230;&#26469;&#25628;&#32034;&#36755;&#20837;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#24322;&#24120;&#65288;&#25110;&#19982;&#20043;&#30456;&#20851;&#30340;&#20302;&#32500;&#34920;&#31034;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CoAD&#30340;&#26032;&#26041;&#27861;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#24182;&#26681;&#25454;&#29305;&#24449;&#31354;&#38388;&#20013;&#20004;&#20010;&#19981;&#21516;&#37096;&#20998;&#30340;&#8220;&#24039;&#21512;&#8221;&#34892;&#20026;&#26469;&#35782;&#21035;&#24322;&#24120;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#8220;&#26080;&#30417;&#30563;&#8221;&#24230;&#37327;$\hat{F}_\beta$&#65292;&#20197;&#31867;&#27604;&#26377;&#30417;&#30563;&#20998;&#31867;&#30340;$F_\beta$&#32479;&#35745;&#37327;&#12290;CoAD&#20351;&#29992;$\hat{F}_\beta$&#26469;&#35757;&#32451;&#19968;&#20010;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection is an important task for complex systems (e.g., industrial facilities, manufacturing, large-scale science experiments), where failures in a sub-system can lead to low yield, faulty products, or even damage to components. While complex systems often have a wealth of data, labeled anomalies are typically rare (or even nonexistent) and expensive to acquire. Unsupervised approaches are therefore common and typically search for anomalies either by distance or density of examples in the input feature space (or some associated low-dimensional representation). This paper presents a novel approach called CoAD, which is specifically designed for multi-modal tasks and identifies anomalies based on \textit{coincident} behavior across two different slices of the feature space. We define an \textit{unsupervised} metric, $\hat{F}_\beta$, out of analogy to the supervised classification $F_\beta$ statistic. CoAD uses $\hat{F}_\beta$ to train an anomaly detection algorithm on \textit{u
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22122;&#22768;&#24863;&#30693;&#25552;&#39640;&#40065;&#26834;&#25439;&#22833;&#30340;&#22122;&#22768;&#23481;&#24525;&#24615;&#65292;&#37319;&#29992;&#23454;&#20363;&#30456;&#20851;&#36229;&#21442;&#25968;&#38598;&#25104;&#40065;&#26834;&#25439;&#22833;&#65292;&#25552;&#39640;&#20102;&#23545;&#22122;&#22768;&#26631;&#31614;&#30340;&#22788;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.07306</link><description>&lt;p&gt;
&#36890;&#36807;&#22122;&#22768;&#24863;&#30693;&#25552;&#39640;&#40065;&#26834;&#25439;&#22833;&#30340;&#22122;&#22768;&#23481;&#24525;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improve Noise Tolerance of Robust Loss via Noise-Awareness. (arXiv:2301.07306v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07306
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22122;&#22768;&#24863;&#30693;&#25552;&#39640;&#40065;&#26834;&#25439;&#22833;&#30340;&#22122;&#22768;&#23481;&#24525;&#24615;&#65292;&#37319;&#29992;&#23454;&#20363;&#30456;&#20851;&#36229;&#21442;&#25968;&#38598;&#25104;&#40065;&#26834;&#25439;&#22833;&#65292;&#25552;&#39640;&#20102;&#23545;&#22122;&#22768;&#26631;&#31614;&#30340;&#22788;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#25439;&#22833;&#26368;&#23567;&#21270;&#26159;&#22788;&#29702;&#22122;&#22768;&#26631;&#31614;&#19978;&#30340;&#40065;&#26834;&#23398;&#20064;&#38382;&#39064;&#30340;&#37325;&#35201;&#31574;&#30053;&#12290;&#30446;&#21069;&#35774;&#35745;&#40065;&#26834;&#25439;&#22833;&#30340;&#26041;&#27861;&#28041;&#21450;&#24341;&#20837;&#22122;&#22768;&#40065;&#26834;&#22240;&#23376;&#65292;&#21363;&#36229;&#21442;&#25968;&#65292;&#26469;&#25511;&#21046;&#22122;&#22768;&#40065;&#26834;&#24615;&#21644;&#21487;&#23398;&#20064;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20855;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#19981;&#21516;&#25968;&#25454;&#38598;&#65292;&#25214;&#21040;&#36866;&#21512;&#30340;&#36229;&#21442;&#25968;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#40065;&#26834;&#25439;&#22833;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#25152;&#26377;&#35757;&#32451;&#26679;&#26412;&#20849;&#20139;&#30456;&#21516;&#30340;&#36229;&#21442;&#25968;&#65292;&#36825;&#20123;&#36229;&#21442;&#25968;&#19982;&#23454;&#20363;&#26080;&#20851;&#12290;&#36825;&#38480;&#21046;&#20102;&#36825;&#20123;&#26041;&#27861;&#35782;&#21035;&#19981;&#21516;&#26679;&#26412;&#30340;&#20010;&#20307;&#22122;&#22768;&#29305;&#24615;&#20197;&#21450;&#24573;&#35270;&#19981;&#21516;&#35757;&#32451;&#26679;&#26412;&#22312;&#24110;&#21161;&#27169;&#22411;&#29702;&#35299;&#24213;&#23618;&#27169;&#24335;&#26041;&#38754;&#30340;&#19981;&#21516;&#36129;&#29486;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#20855;&#26377;&#23454;&#20363;&#30456;&#20851;&#36229;&#21442;&#25968;&#30340;&#40065;&#26834;&#25439;&#22833;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#20197;&#25552;&#39640;&#20854;&#22122;&#22768;&#23481;&#24525;&#24615;&#65292;&#24182;&#24102;&#26377;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust loss minimization is an important strategy for handling robust learning issue on noisy labels. Current approaches for designing robust losses involve the introduction of noise-robust factors, i.e., hyperparameters, to control the trade-off between noise robustness and learnability. However, finding suitable hyperparameters for different datasets with noisy labels is a challenging and time-consuming task. Moreover, existing robust loss methods usually assume that all training samples share common hyperparameters, which are independent of instances. This limits the ability of these methods to distinguish the individual noise properties of different samples and overlooks the varying contributions of diverse training samples in helping models understand underlying patterns. To address above issues, we propose to assemble robust loss with instance-dependent hyperparameters to improve their noise tolerance with theoretical guarantee. To achieve setting such instance-dependent hyperpar
&lt;/p&gt;</description></item><item><title>&#22312;&#38544;&#31169;&#32422;&#26463;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#32852;&#21512;&#23398;&#20064;&#20026;&#26412;&#22320;&#23458;&#25143;&#23450;&#21046;&#30340;&#20010;&#24615;&#21270;&#22270;&#20197;&#21450;&#20849;&#35782;&#22270;&#65292;&#20197;&#25512;&#26029;&#28508;&#22312;&#22270;&#25299;&#25169;&#65292;&#21516;&#26102;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#22788;&#29702;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2301.06662</link><description>&lt;p&gt;
&#22312;&#38544;&#31169;&#32422;&#26463;&#19979;&#30340;&#22270;&#25299;&#25169;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Topology Learning Under Privacy Constraints. (arXiv:2301.06662v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06662
&lt;/p&gt;
&lt;p&gt;
&#22312;&#38544;&#31169;&#32422;&#26463;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#32852;&#21512;&#23398;&#20064;&#20026;&#26412;&#22320;&#23458;&#25143;&#23450;&#21046;&#30340;&#20010;&#24615;&#21270;&#22270;&#20197;&#21450;&#20849;&#35782;&#22270;&#65292;&#20197;&#25512;&#26029;&#28508;&#22312;&#22270;&#25299;&#25169;&#65292;&#21516;&#26102;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#22788;&#29702;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#25968;&#25454;&#20998;&#24067;&#20110;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#19988;&#20855;&#26377;&#38544;&#31169;&#25935;&#24863;&#24615;&#30340;&#26032;&#39062;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#24179;&#28369;&#22270;&#20449;&#21495;&#25512;&#26029;&#28508;&#22312;&#22270;&#25299;&#25169;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#20219;&#21153;&#30340;&#20027;&#35201;&#22256;&#38590;&#22312;&#20110;&#22914;&#20309;&#22312;&#38544;&#31169;&#32422;&#26463;&#19979;&#21033;&#29992;&#25152;&#26377;&#29420;&#31435;&#23458;&#25143;&#31471;&#30340;&#28508;&#22312;&#24322;&#26500;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#20026;&#26412;&#22320;&#23458;&#25143;&#31471;&#23450;&#21046;&#30340;&#20010;&#24615;&#21270;&#22270;&#20197;&#21450;&#20849;&#35782;&#22270;&#12290;&#20010;&#24615;&#21270;&#22270;&#21305;&#37197;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#65292;&#20174;&#32780;&#20943;&#36731;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#65292;&#32780;&#20849;&#35782;&#22270;&#25429;&#25417;&#20840;&#23616;&#20449;&#24687;&#12290;&#25105;&#20204;&#25509;&#19979;&#26469;&#35774;&#35745;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#24341;&#20837;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#19981;&#36829;&#21453;&#38544;&#31169;&#32422;&#26463;&#65292;&#21363;&#25152;&#26377;&#30340;&#31169;&#26377;&#25968;&#25454;&#37117;&#22312;&#26412;&#22320;&#22788;&#29702;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#65292;&#25105;&#20204;&#23558;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#24341;&#20837;&#21040;&#25152;&#25552;&#31639;&#27861;&#20013;&#65292;&#22312;&#20256;&#36755;&#27169;&#22411;&#26356;&#26032;&#26102;&#25269;&#24481;&#38544;&#31169;&#25915;&#20987;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#21487;&#35777;&#26126;&#25910;&#25947;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of inferring the underlying graph topology from smooth graph signals in a novel but practical scenario where data are located in distributed clients and are privacy-sensitive. The main difficulty of this task lies in how to utilize the potentially heterogeneous data of all isolated clients under privacy constraints. Towards this end, we propose a framework where personalized graphs for local clients as well as a consensus graph are jointly learned. The personalized graphs match local data distributions, thereby mitigating data heterogeneity, while the consensus graph captures the global information. We next devise a tailored algorithm to solve the induced problem without violating privacy constraints, i.e., all private data are processed locally. To further enhance privacy protection, we introduce differential privacy (DP) into the proposed algorithm to resist privacy attacks when transmitting model updates. Theoretically, we establish provable convergence analy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21560;&#25910;MDPs&#20013;&#26410;&#25240;&#25187;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MWLA&#31639;&#27861;&#30340;&#26041;&#27861;&#26469;&#30452;&#25509;&#20272;&#35745;&#26399;&#26395;&#22238;&#25253;&#65292;&#24182;&#20998;&#26512;&#20102;&#35813;&#26041;&#27861;&#30340;&#35823;&#24046;&#30028;&#38480;&#21644;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2301.03183</link><description>&lt;p&gt;
&#26368;&#23567;&#21270;&#26497;&#23567;&#21270;&#26435;&#37325;&#23398;&#20064;&#22312;&#21560;&#25910;MDPs&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Minimax Weight Learning for Absorbing MDPs. (arXiv:2301.03183v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21560;&#25910;MDPs&#20013;&#26410;&#25240;&#25187;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MWLA&#31639;&#27861;&#30340;&#26041;&#27861;&#26469;&#30452;&#25509;&#20272;&#35745;&#26399;&#26395;&#22238;&#25253;&#65292;&#24182;&#20998;&#26512;&#20102;&#35813;&#26041;&#27861;&#30340;&#35823;&#24046;&#30028;&#38480;&#21644;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#24120;&#34987;&#24314;&#27169;&#20026;&#26377;&#38480;&#25110;&#25240;&#25187;/&#24179;&#22343;&#26080;&#38480;&#26102;&#22495;&#30340;MDPs&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21560;&#25910;MDPs&#20013;&#26410;&#25240;&#25187;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;&#12290;&#32473;&#23450;&#21253;&#21547;&#29305;&#23450;&#25130;&#26029;&#32423;&#21035;&#30340;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MWLA&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#30452;&#25509;&#36890;&#36807;&#29366;&#24577;-&#34892;&#21160;&#21344;&#29992;&#24230;&#37327;&#30340;&#37325;&#35201;&#27604;&#29575;&#26469;&#20272;&#35745;&#26399;&#26395;&#22238;&#25253;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;MWLA&#26041;&#27861;&#30340;&#22343;&#26041;&#35823;&#24046;&#30028;&#38480;&#65292;&#24182;&#20998;&#26512;&#20102;&#32479;&#35745;&#35823;&#24046;&#19982;&#25968;&#25454;&#22823;&#23567;&#21644;&#25130;&#26029;&#32423;&#21035;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#24773;&#26223;&#20986;&#31199;&#36710;&#29615;&#22659;&#36827;&#34892;&#35745;&#31639;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;MWLA&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning policy evaluation problems are often modeled as finite or discounted/averaged infinite-horizon MDPs. In this paper, we study undiscounted off-policy policy evaluation for absorbing MDPs. Given the dataset consisting of the i.i.d episodes with a given truncation level, we propose a so-called MWLA algorithm to directly estimate the expected return via the importance ratio of the state-action occupancy measure. The Mean Square Error (MSE) bound for the MWLA method is investigated and the dependence of statistical errors on the data size and the truncation level are analyzed. With an episodic taxi environment, computational experiments illustrate the performance of the MWLA algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#65292;&#25552;&#20986;&#20102;&#35299;&#37322;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#20174;&#22240;&#26524;&#20851;&#31995;&#30340;&#35282;&#24230;&#23545;&#20854;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;</title><link>http://arxiv.org/abs/2301.02791</link><description>&lt;p&gt;
&#24544;&#23454;&#19988;&#19968;&#33268;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#19982;&#21407;&#29702;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Faithful and Consistent Graph Neural Network Explanations with Rationale Alignment. (arXiv:2301.02791v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#65292;&#25552;&#20986;&#20102;&#35299;&#37322;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#20174;&#22240;&#26524;&#20851;&#31995;&#30340;&#35282;&#24230;&#23545;&#20854;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25581;&#31034;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#39044;&#27979;&#32972;&#21518;&#30340;&#21407;&#29702;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#23454;&#20363;&#32423;GNN&#35299;&#37322;&#26088;&#22312;&#21457;&#29616;&#30446;&#26631;GNN&#20381;&#36182;&#20110;&#36827;&#34892;&#39044;&#27979;&#30340;&#20851;&#38190;&#36755;&#20837;&#20803;&#32032;&#65292;&#22914;&#33410;&#28857;&#25110;&#36793;&#32536;&#12290;&#36825;&#20123;&#35782;&#21035;&#20986;&#30340;&#23376;&#32467;&#26500;&#21487;&#20197;&#35299;&#37322;GNN&#30340;&#34892;&#20026;&#12290;&#23613;&#31649;&#25552;&#20986;&#20102;&#21508;&#31181;&#31639;&#27861;&#65292;&#20294;&#20854;&#20013;&#22823;&#22810;&#25968;&#36890;&#36807;&#25628;&#32034;&#33021;&#22815;&#20445;&#30041;&#21407;&#22987;&#39044;&#27979;&#30340;&#26368;&#23567;&#23376;&#22270;&#26469;&#24418;&#24335;&#21270;&#36825;&#20010;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#26694;&#26550;&#26681;&#28145;&#33922;&#22266;&#22320;&#20855;&#26377;&#24402;&#32435;&#20559;&#35265;&#65306;&#20960;&#20010;&#23376;&#22270;&#21487;&#33021;&#20250;&#20135;&#29983;&#19982;&#21407;&#22987;&#22270;&#30456;&#21516;&#25110;&#30456;&#20284;&#30340;&#36755;&#20986;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#25552;&#20379;&#34394;&#20551;&#30340;&#35299;&#37322;&#65292;&#24182;&#19988;&#26080;&#27861;&#25552;&#20379;&#19968;&#33268;&#30340;&#35299;&#37322;&#12290;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#35299;&#37322;&#34920;&#29616;&#36739;&#24046;&#30340;GNN&#20250;&#36827;&#19968;&#27493;&#25918;&#22823;&#36825;&#20123;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#22240;&#26524;&#20851;&#31995;&#30340;&#35282;&#24230;&#22312;&#29702;&#35770;&#19978;&#26816;&#26597;GNN&#30340;&#39044;&#27979;&#12290;&#20266;&#35299;&#37322;&#30340;&#20004;&#20010;&#20856;&#22411;&#21407;&#22240;
&lt;/p&gt;
&lt;p&gt;
Uncovering rationales behind predictions of graph neural networks (GNNs) has received increasing attention over recent years. Instance-level GNN explanation aims to discover critical input elements, like nodes or edges, that the target GNN relies upon for making predictions. %These identified sub-structures can provide interpretations of GNN's behavior. Though various algorithms are proposed, most of them formalize this task by searching the minimal subgraph which can preserve original predictions. However, an inductive bias is deep-rooted in this framework: several subgraphs can result in the same or similar outputs as the original graphs. Consequently, they have the danger of providing spurious explanations and failing to provide consistent explanations. Applying them to explain weakly-performed GNNs would further amplify these issues. To address this problem, we theoretically examine the predictions of GNNs from the causality perspective. Two typical reasons for spurious explanation
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21453;&#21521;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22238;&#25918;&#36712;&#36857;&#32780;&#19981;&#26159;&#21407;&#22987;&#30340;&#21069;&#21521;&#36712;&#36857;&#26469;&#35757;&#32451;&#26234;&#33021;&#20307;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#25552;&#20379;&#24378;&#26377;&#21147;&#30340;&#22870;&#21169;&#20449;&#21495;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#23398;&#20064;&#65292;&#32780;&#19988;&#21482;&#38656;&#35201;&#36827;&#34892;&#24494;&#23567;&#30340;&#31639;&#27861;&#25913;&#21464;&#12290;</title><link>http://arxiv.org/abs/2212.14214</link><description>&lt;p&gt;
&#21453;&#21521;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Backward Curriculum Reinforcement Learning. (arXiv:2212.14214v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14214
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21453;&#21521;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22238;&#25918;&#36712;&#36857;&#32780;&#19981;&#26159;&#21407;&#22987;&#30340;&#21069;&#21521;&#36712;&#36857;&#26469;&#35757;&#32451;&#26234;&#33021;&#20307;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#25552;&#20379;&#24378;&#26377;&#21147;&#30340;&#22870;&#21169;&#20449;&#21495;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#23398;&#20064;&#65292;&#32780;&#19988;&#21482;&#38656;&#35201;&#36827;&#34892;&#24494;&#23567;&#30340;&#31639;&#27861;&#25913;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20351;&#29992;&#21069;&#21521;&#29983;&#25104;&#36712;&#36857;&#26469;&#35757;&#32451;&#26234;&#33021;&#20307;&#65292;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#30340;&#25351;&#23548;&#19981;&#36275;&#20197;&#20351;&#26234;&#33021;&#20307;&#36827;&#34892;&#23613;&#21487;&#33021;&#22810;&#30340;&#25506;&#32034;&#12290;&#23613;&#31649;&#25105;&#20204;&#35748;&#35782;&#21040;&#24378;&#21270;&#23398;&#20064;&#32467;&#26524;&#26469;&#33258;&#20805;&#20998;&#30340;&#25506;&#32034;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#19978;&#23384;&#22312;&#25240;&#34935;&#65292;&#36825;&#26159;&#24433;&#21709;&#31639;&#27861;&#24615;&#33021;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#20351;&#29992;&#22870;&#21169;&#22609;&#36896;&#25216;&#26415;&#21644;&#32593;&#32476;&#32467;&#26500;&#20462;&#25913;&#26469;&#22686;&#21152;&#26679;&#26412;&#25928;&#29575;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#24456;&#22810;&#27493;&#39588;&#26469;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21453;&#21521;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#20351;&#29992;&#22238;&#25918;&#36712;&#36857;&#32780;&#19981;&#26159;&#21407;&#22987;&#30340;&#21069;&#21521;&#36712;&#36857;&#26469;&#35757;&#32451;&#26234;&#33021;&#20307;&#12290;&#36825;&#31181;&#26041;&#27861;&#20026;&#26234;&#33021;&#20307;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#22870;&#21169;&#20449;&#21495;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#38656;&#35201;&#22312;&#26234;&#33021;&#20307;&#35757;&#32451;&#20043;&#21069;&#23545;&#36712;&#36857;&#30340;&#39034;&#24207;&#36827;&#34892;&#24494;&#23567;&#30340;&#25913;&#21464;&#65292;&#20351;&#24471;&#23454;&#29616;&#36215;&#26469;&#26356;&#21152;&#30452;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current reinforcement learning algorithms train an agent using forward-generated trajectories, which provide little guidance so that the agent can explore as much as possible. While realizing the value of reinforcement learning results from sufficient exploration, this approach leads to a trade-off in losing sample efficiency, an essential factor impacting algorithm performance. Previous tasks use reward-shaping techniques and network structure modification to increase sample efficiency. However, these methods require many steps to implement. In this work, we propose novel backward curriculum reinforcement learning that begins training the agent using the backward trajectory of the episode instead of the original forward trajectory. This approach provides the agent with a strong reward signal, enabling more sample-efficient learning. Moreover, our method only requires a minor change in the algorithm of reversing the order of the trajectory before agent training, allowing a straightforw
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;&#31264;&#23494;&#22478;&#24066;&#29615;&#22659;&#20013;&#26080;&#32447;&#22320;&#22270;&#25968;&#25454;&#38598;&#30340;&#30740;&#31350;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#33021;&#22815;&#29992;&#20110;&#36335;&#24452;&#25439;&#32791;&#39044;&#27979;&#21644;&#26080;&#32447;&#23450;&#20301;&#65292;&#36890;&#36807;&#22312;&#30456;&#21516;&#30340;&#22478;&#24066;&#22320;&#22270;&#19978;&#35745;&#31639;&#24471;&#21040;RSS&#21644;ToA&#22320;&#22270;&#65292;&#21487;&#20197;&#20844;&#24179;&#27604;&#36739;&#20004;&#31181;&#23450;&#20301;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.11777</link><description>&lt;p&gt;
&#20855;&#26377;&#23450;&#20301;&#24212;&#29992;&#30340;&#36335;&#24452;&#25439;&#32791;&#21644;&#21040;&#36798;&#26102;&#38388;&#26080;&#32447;&#22320;&#22270;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Dataset of Pathloss and ToA Radio Maps With Localization Application. (arXiv:2212.11777v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11777
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;&#31264;&#23494;&#22478;&#24066;&#29615;&#22659;&#20013;&#26080;&#32447;&#22320;&#22270;&#25968;&#25454;&#38598;&#30340;&#30740;&#31350;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#33021;&#22815;&#29992;&#20110;&#36335;&#24452;&#25439;&#32791;&#39044;&#27979;&#21644;&#26080;&#32447;&#23450;&#20301;&#65292;&#36890;&#36807;&#22312;&#30456;&#21516;&#30340;&#22478;&#24066;&#22320;&#22270;&#19978;&#35745;&#31639;&#24471;&#21040;RSS&#21644;ToA&#22320;&#22270;&#65292;&#21487;&#20197;&#20844;&#24179;&#27604;&#36739;&#20004;&#31181;&#23450;&#20301;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#31264;&#23494;&#22478;&#24066;&#29615;&#22659;&#20013;&#29983;&#25104;&#24182;&#20844;&#24320;&#25552;&#20379;&#30340;&#19968;&#32452;&#26080;&#32447;&#22320;&#22270;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#21253;&#25324;&#27169;&#25311;&#30340;&#36335;&#24452;&#25439;&#32791;/&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#65288;RSS&#65289;&#21644;&#21040;&#36798;&#26102;&#38388;&#65288;ToA&#65289;&#26080;&#32447;&#22320;&#22270;&#65292;&#35206;&#30422;&#20102;&#22823;&#37327;&#30495;&#23454;&#22478;&#24066;&#22320;&#22270;&#30340;&#31264;&#23494;&#22478;&#24066;&#35774;&#32622;&#12290;&#35813;&#25968;&#25454;&#38598;&#30340;&#20004;&#20010;&#20027;&#35201;&#24212;&#29992;&#26159;1&#65289;&#20174;&#36755;&#20837;&#30340;&#22478;&#24066;&#22320;&#22270;&#39044;&#27979;&#36335;&#24452;&#25439;&#32791;&#30340;&#23398;&#20064;&#26041;&#27861;&#65288;&#21363;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#25311;&#65289;&#65292;&#20197;&#21450;2&#65289;&#26080;&#32447;&#23450;&#20301;&#12290;RSS&#21644;ToA&#22320;&#22270;&#36890;&#36807;&#30456;&#21516;&#30340;&#27169;&#25311;&#22312;&#30456;&#21516;&#30340;&#22478;&#24066;&#22320;&#22270;&#19978;&#35745;&#31639;&#24471;&#20986;&#65292;&#21487;&#20197;&#23545;&#22522;&#20110;RSS&#21644;ToA&#30340;&#23450;&#20301;&#26041;&#27861;&#36827;&#34892;&#20844;&#24179;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article, we present a collection of radio map datasets in dense urban setting, which we generated and made publicly available. The datasets include simulated pathloss/received signal strength (RSS) and time of arrival (ToA) radio maps over a large collection of realistic dense urban setting in real city maps. The two main applications of the presented dataset are 1) learning methods that predict the pathloss from input city maps (namely, deep learning-based simulations), and, 2) wireless localization. The fact that the RSS and ToA maps are computed by the same simulations over the same city maps allows for a fair comparison of the RSS and ToA-based localization methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;Vision Transformers&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#37096;&#32626;&#25928;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36229;&#32593;&#32476;&#35774;&#35745;&#21644;&#25628;&#32034;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#19982;MobileNet&#31867;&#20284;&#22823;&#23567;&#21644;&#36895;&#24230;&#30340;Transformer&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.08059</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#36866;&#29992;&#20110;MobileNet&#36895;&#24230;&#21644;&#23610;&#23544;&#30340;Vision Transformers
&lt;/p&gt;
&lt;p&gt;
Rethinking Vision Transformers for MobileNet Size and Speed. (arXiv:2212.08059v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;Vision Transformers&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#37096;&#32626;&#25928;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36229;&#32593;&#32476;&#35774;&#35745;&#21644;&#25628;&#32034;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#19982;MobileNet&#31867;&#20284;&#22823;&#23567;&#21644;&#36895;&#24230;&#30340;Transformer&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;Vision Transformers&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#30340;&#25104;&#21151;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#20248;&#21270;ViTs&#30340;&#24615;&#33021;&#21644;&#22797;&#26434;&#24615;&#65292;&#20197;&#23454;&#29616;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#39640;&#25928;&#37096;&#32626;&#12290;&#25552;&#20986;&#20102;&#22810;&#31181;&#26041;&#27861;&#26469;&#21152;&#36895;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25913;&#36827;&#20302;&#25928;&#30340;&#35774;&#35745;&#65292;&#25110;&#23558;&#31227;&#21160;&#35774;&#22791;&#21451;&#22909;&#30340;&#36731;&#37327;&#32423;&#21367;&#31215;&#19982;ViTs&#32467;&#21512;&#24418;&#25104;&#28151;&#21512;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#22810;&#24180;&#21069;&#30340;MobileNet&#65292;ViT&#21450;&#20854;&#21464;&#31181;&#20173;&#28982;&#20855;&#26377;&#26356;&#39640;&#30340;&#24310;&#36831;&#25110;&#26356;&#22810;&#30340;&#21442;&#25968;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#24310;&#36831;&#21644;&#22823;&#23567;&#23545;&#20110;&#22312;&#36164;&#28304;&#21463;&#38480;&#30828;&#20214;&#19978;&#30340;&#39640;&#25928;&#37096;&#32626;&#37117;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;ViTs&#30340;&#35774;&#35745;&#36873;&#25321;&#36827;&#34892;&#20102;&#37325;&#26032;&#23457;&#35270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20302;&#24310;&#36831;&#21644;&#39640;&#21442;&#25968;&#25928;&#29575;&#30340;&#26032;&#22411;&#36229;&#32593;&#32476;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32454;&#31890;&#24230;&#32852;&#21512;&#25628;&#32034;&#31574;&#30053;&#65292;&#29992;&#20110;&#36890;&#36807;&#20248;&#21270;&#26469;&#23547;&#25214;&#39640;&#25928;&#30340;transformer&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the success of Vision Transformers (ViTs) in computer vision tasks, recent arts try to optimize the performance and complexity of ViTs to enable efficient deployment on mobile devices. Multiple approaches are proposed to accelerate attention mechanism, improve inefficient designs, or incorporate mobile-friendly lightweight convolutions to form hybrid architectures. However, ViT and its variants still have higher latency or considerably more parameters than lightweight CNNs, even true for the years-old MobileNet. In practice, latency and size are both crucial for efficient deployment on resource-constraint hardware. In this work, we investigate a central question, can transformer models run as fast as MobileNet and maintain a similar size? We revisit the design choices of ViTs and propose a novel supernet with low latency and high parameter efficiency. We further introduce a novel fine-grained joint search strategy for transformer models that can find efficient architectures by opt
&lt;/p&gt;</description></item><item><title>Nelson&#26159;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#21644;Lov\'asz Local Lemma&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#27169;&#22411;&#29983;&#25104;&#28385;&#36275;&#32452;&#21512;&#32422;&#26463;&#26465;&#20214;&#30340;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2212.00296</link><description>&lt;p&gt;
&#36890;&#36807; Lov\'asz Local Lemma &#36827;&#34892;&#37319;&#26679;&#30340;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#23398;&#20064;&#32452;&#21512;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Learning Combinatorial Structures via Markov Random Fields with Sampling through Lov\'asz Local Lemma. (arXiv:2212.00296v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00296
&lt;/p&gt;
&lt;p&gt;
Nelson&#26159;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#21644;Lov\'asz Local Lemma&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#27169;&#22411;&#29983;&#25104;&#28385;&#36275;&#32452;&#21512;&#32422;&#26463;&#26465;&#20214;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#32452;&#21512;&#32467;&#26500;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#20855;&#26377;&#38761;&#21629;&#24615;&#30340;&#24433;&#21709;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25552;&#20379;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#23398;&#20064;&#32467;&#26524;&#65292;&#30001;&#20110;&#23398;&#20064;&#30446;&#26631;&#21463;&#21040;&#32452;&#21512;&#32422;&#26463;&#26465;&#20214;&#30340;&#21046;&#32422;&#65292;&#20854;&#26799;&#24230;&#20272;&#35745;&#38750;&#24120;&#22797;&#26434;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110; Lov\'asz Local Lemma &#30340;&#31070;&#32463;&#32593;&#32476;&#65288;Nelson&#65289;&#65292;&#23427;&#33021;&#22815;&#20174;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#27169;&#22411;&#30340;&#20998;&#24067;&#20013;&#29983;&#25104;&#28385;&#36275;&#32452;&#21512;&#32422;&#26463;&#26465;&#20214;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models for learning combinatorial structures have transformative impacts in many applications. However, existing approaches fail to offer efficient and accurate learning results. Because of the highly intractable nature of the gradient estimation of the learning objective subject to combinatorial constraints. Existing gradient estimation methods would easily run into exponential time/memory space, or incur huge estimation errors due to improper approximation. We develop NEural Lovasz Sampler (Nelson), a neural network based on Lov\'asz Local Lemma (LLL). We show it guarantees to generate samples satisfying combinatorial constraints from the distribution of the constrained Markov Random Fields model (MRF) under certain conditions. We further present a fully differentiable contrastive-divergence-based learning framework on constrained MRF (Nelson-CD). Meanwhile, Nelson-CD being fully differentiable allows us to take advantage of the parallel computing power of GPUs, resulting 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#27979;&#35797;&#23545;&#25968;&#20284;&#28982;&#36827;&#34892;&#27604;&#36739;&#21487;&#33021;&#19982;&#20854;&#20182;&#25351;&#26631;&#30456;&#30683;&#30462;&#65292;&#24182;&#19988;&#39640;&#27979;&#35797;&#23545;&#25968;&#20284;&#28982;&#19981;&#24847;&#21619;&#30528;&#26356;&#20934;&#30830;&#30340;&#21518;&#39564;&#36817;&#20284;&#12290;</title><link>http://arxiv.org/abs/2212.00219</link><description>&lt;p&gt;
&#20320;&#26159;&#21542;&#27491;&#30830;&#20351;&#29992;&#20102;&#27979;&#35797;&#23545;&#25968;&#20284;&#28982;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are you using test log-likelihood correctly?. (arXiv:2212.00219v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00219
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#27979;&#35797;&#23545;&#25968;&#20284;&#28982;&#36827;&#34892;&#27604;&#36739;&#21487;&#33021;&#19982;&#20854;&#20182;&#25351;&#26631;&#30456;&#30683;&#30462;&#65292;&#24182;&#19988;&#39640;&#27979;&#35797;&#23545;&#25968;&#20284;&#28982;&#19981;&#24847;&#21619;&#30528;&#26356;&#20934;&#30830;&#30340;&#21518;&#39564;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#23545;&#25968;&#20284;&#28982;&#24120;&#34987;&#29992;&#26469;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#30340;&#21516;&#19968;&#25968;&#25454;&#65292;&#25110;&#32773;&#27604;&#36739;&#25311;&#21512;&#21516;&#19968;&#27010;&#29575;&#27169;&#22411;&#30340;&#19981;&#21516;&#36817;&#20284;&#25512;&#26029;&#31639;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#31616;&#21333;&#30340;&#20363;&#23376;&#23637;&#31034;&#20102;&#22914;&#20309;&#22522;&#20110;&#27979;&#35797;&#23545;&#25968;&#20284;&#28982;&#30340;&#27604;&#36739;&#21487;&#33021;&#19982;&#20854;&#20182;&#30446;&#26631;&#30456;&#30683;&#30462;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#20363;&#23376;&#34920;&#26126;&#65306;&#65288;i&#65289;&#36798;&#21040;&#26356;&#39640;&#27979;&#35797;&#23545;&#25968;&#20284;&#28982;&#30340;&#36817;&#20284;&#36125;&#21494;&#26031;&#25512;&#26029;&#31639;&#27861;&#19981;&#24517;&#24847;&#21619;&#30528;&#33021;&#22815;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#21518;&#39564;&#36817;&#20284;&#65292;&#65288;ii&#65289;&#22522;&#20110;&#27979;&#35797;&#23545;&#25968;&#20284;&#28982;&#27604;&#36739;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#32467;&#35770;&#21487;&#33021;&#19982;&#22522;&#20110;&#22343;&#26041;&#26681;&#35823;&#24046;&#30340;&#32467;&#35770;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Test log-likelihood is commonly used to compare different models of the same data or different approximate inference algorithms for fitting the same probabilistic model. We present simple examples demonstrating how comparisons based on test log-likelihood can contradict comparisons according to other objectives. Specifically, our examples show that (i) approximate Bayesian inference algorithms that attain higher test log-likelihoods need not also yield more accurate posterior approximations and (ii) conclusions about forecast accuracy based on test log-likelihood comparisons may not agree with conclusions based on root mean squared error.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;PAC&#39564;&#35777;&#30340;&#27010;&#24565;&#65292;&#24182;&#22312;&#19977;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#65306;&#23545;&#20110;VC&#32500;&#24230;&#20026;$d$&#30340;&#20551;&#35774;&#31867;&#65292;PAC&#39564;&#35777;&#38656;&#35201;$\Omega\left(\sqrt{d}/\varepsilon^2\right)$&#20010;i.i.d.&#26679;&#26412;&#30340;&#19979;&#30028;&#65307;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39564;&#35777;&#23454;&#25968;&#21306;&#38388;&#30340;&#24182;&#38598;&#30340;&#21327;&#35758;&#65292;&#24182;&#19982;&#19979;&#30028;&#23545;$d$&#30340;&#20381;&#36182;&#30456;&#21305;&#37197;&#65307;&#23558;PAC&#39564;&#35777;&#30340;&#23450;&#20041;&#25512;&#24191;&#21040;&#23545;&#19968;&#33324;&#32479;&#35745;&#31639;&#27861;&#30340;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2211.17096</link><description>&lt;p&gt;
&#32479;&#35745;&#31639;&#27861;&#30340;PAC&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
PAC Verification of Statistical Algorithms. (arXiv:2211.17096v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.17096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;PAC&#39564;&#35777;&#30340;&#27010;&#24565;&#65292;&#24182;&#22312;&#19977;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#65306;&#23545;&#20110;VC&#32500;&#24230;&#20026;$d$&#30340;&#20551;&#35774;&#31867;&#65292;PAC&#39564;&#35777;&#38656;&#35201;$\Omega\left(\sqrt{d}/\varepsilon^2\right)$&#20010;i.i.d.&#26679;&#26412;&#30340;&#19979;&#30028;&#65307;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39564;&#35777;&#23454;&#25968;&#21306;&#38388;&#30340;&#24182;&#38598;&#30340;&#21327;&#35758;&#65292;&#24182;&#19982;&#19979;&#30028;&#23545;$d$&#30340;&#20381;&#36182;&#30456;&#21305;&#37197;&#65307;&#23558;PAC&#39564;&#35777;&#30340;&#23450;&#20041;&#25512;&#24191;&#21040;&#23545;&#19968;&#33324;&#32479;&#35745;&#31639;&#27861;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Goldwasser&#31561;&#20154;&#65288;2021&#65289;&#26368;&#36817;&#25552;&#20986;&#20102;PAC&#39564;&#35777;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#20351;&#29992;&#20132;&#20114;&#24335;&#35777;&#26126;&#26469;&#39564;&#35777;&#20551;&#35774;&#65288;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65289;&#65292;&#35813;&#27169;&#22411;&#22768;&#31216;&#28385;&#36275;&#26080;&#30693;PAC&#23398;&#20064;&#30446;&#26631;&#12290;&#26412;&#25991;&#22312;&#22810;&#20010;&#26041;&#38754;&#36827;&#19968;&#27493;&#21457;&#23637;&#20102;&#36825;&#20010;&#27010;&#24565;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;VC&#32500;&#24230;&#20026;$d$&#30340;&#20551;&#35774;&#31867;&#65292;PAC&#39564;&#35777;&#38656;&#35201;$\Omega\left(\sqrt{d}/\varepsilon^2\right)$&#20010;i.i.d.&#26679;&#26412;&#30340;&#19979;&#30028;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;PAC&#39564;&#35777;&#23454;&#25968;&#21306;&#38388;&#30340;&#24182;&#38598;&#30340;&#21327;&#35758;&#65292;&#35813;&#21327;&#35758;&#25913;&#36827;&#20102;&#20182;&#20204;&#25552;&#20986;&#30340;&#21327;&#35758;&#65292;&#24182;&#19982;&#25105;&#20204;&#30340;&#19979;&#30028;&#23545;$d$&#30340;&#20381;&#36182;&#30456;&#21305;&#37197;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#23558;&#20182;&#20204;&#30340;&#23450;&#20041;&#33258;&#28982;&#25512;&#24191;&#21040;&#20102;&#23545;&#19968;&#33324;&#32479;&#35745;&#31639;&#27861;&#30340;&#39564;&#35777;&#65292;&#36825;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#30340;&#39046;&#22495;&#65292;&#36229;&#20986;&#20102;&#26080;&#30693;PAC&#23398;&#20064;&#30340;&#33539;&#30068;&#12290;&#36890;&#36807;&#23637;&#31034;&#25105;&#20204;&#25552;&#20986;&#30340;&#23450;&#20041;&#65292;&#25105;&#20204;&#30340;&#26368;&#32456;&#32467;&#26524;&#26159;&#19968;&#31181;&#39564;&#35777;&#20855;&#26377;&#32452;&#21512;&#32422;&#26463;&#30340;&#32479;&#35745;&#26597;&#35810;&#31639;&#27861;&#30340;&#21327;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Goldwasser et al. (2021) recently proposed the setting of PAC verification, where a hypothesis (machine learning model) that purportedly satisfies the agnostic PAC learning objective is verified using an interactive proof. In this paper we develop this notion further in a number of ways. First, we prove a lower bound of $\Omega\left(\sqrt{d}/\varepsilon^2\right)$ i.i.d.\ samples for PAC verification of hypothesis classes of VC dimension $d$. Second, we present a protocol for PAC verification of unions of intervals over $\mathbb{R}$ that improves upon their proposed protocol for that task, and matches our lower bound's dependence on $d$. Third, we introduce a natural generalization of their definition to verification of general statistical algorithms, which is applicable to a wider variety of settings beyond agnostic PAC learning. Showcasing our proposed definition, our final result is a protocol for the verification of statistical query algorithms that satisfy a combinatorial constrain
&lt;/p&gt;</description></item><item><title>EPIC &#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#20687;-&#26631;&#35760;&#19968;&#33268;&#24615;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#25513;&#30721;&#31574;&#30053;&#21644;&#19981;&#19968;&#33268;&#30340;&#26631;&#35760;&#29983;&#25104;&#36807;&#31243;&#26469;&#20811;&#26381;CMLM&#30340;&#38480;&#21046;&#65292;&#24182;&#22686;&#24378;&#20102;&#35270;&#35273;-&#35821;&#35328;&#20851;&#32852;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.15398</link><description>&lt;p&gt;
&#21033;&#29992;&#22270;&#20687;-&#26631;&#35760;&#19968;&#33268;&#24615;&#36827;&#34892;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Leveraging per Image-Token Consistency for Vision-Language Pre-training. (arXiv:2211.15398v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15398
&lt;/p&gt;
&lt;p&gt;
EPIC &#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#20687;-&#26631;&#35760;&#19968;&#33268;&#24615;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#25513;&#30721;&#31574;&#30053;&#21644;&#19981;&#19968;&#33268;&#30340;&#26631;&#35760;&#29983;&#25104;&#36807;&#31243;&#26469;&#20811;&#26381;CMLM&#30340;&#38480;&#21046;&#65292;&#24182;&#22686;&#24378;&#20102;&#35270;&#35273;-&#35821;&#35328;&#20851;&#32852;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;VLP&#65289;&#26041;&#27861;&#37319;&#29992;&#36328;&#27169;&#24577;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65288;CMLM&#65289;&#26469;&#23398;&#20064;&#35270;&#35273;-&#35821;&#35328;&#20851;&#32852;&#12290;&#28982;&#32780;&#65292;&#26681;&#25454;&#25105;&#20204;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#21457;&#29616;CMLM&#23545;&#20110;&#27492;&#30446;&#30340;&#26469;&#35828;&#26159;&#19981;&#36275;&#22815;&#30340;&#65306;&#65288;1&#65289;&#27169;&#24577;&#20559;&#24046;&#65306;CMLM&#20013;&#30456;&#24403;&#25968;&#37327;&#30340;&#25513;&#30721;&#26631;&#35760;&#21487;&#20197;&#20165;&#36890;&#36807;&#35821;&#35328;&#20449;&#24687;&#24674;&#22797;&#65292;&#24573;&#30053;&#20102;&#35270;&#35273;&#36755;&#20837;&#12290;&#65288;2&#65289;&#26410;&#34987;&#20805;&#20998;&#21033;&#29992;&#30340;&#26410;&#34987;&#25513;&#30721;&#26631;&#35760;&#65306;CMLM&#20027;&#35201;&#20851;&#27880;&#34987;&#25513;&#30721;&#30340;&#26631;&#35760;&#65292;&#20294;&#19981;&#33021;&#21516;&#26102;&#21033;&#29992;&#20854;&#20182;&#26631;&#35760;&#26469;&#23398;&#20064;&#35270;&#35273;-&#35821;&#35328;&#20851;&#32852;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; EPIC&#65288;&#21033;&#29992;&#22270;&#20687;-&#26631;&#35760;&#19968;&#33268;&#24615;&#36827;&#34892;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#65289;&#12290;&#22312;EPIC&#20013;&#65292;&#23545;&#20110;&#27599;&#20010;&#22270;&#20687;-&#21477;&#23376;&#23545;&#65292;&#25105;&#20204;&#25513;&#30422;&#19982;&#22270;&#20687;&#30456;&#20851;&#30340;&#26631;&#35760;&#65288;&#21363;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#25513;&#30721;&#31574;&#30053;&#65289;&#65292;&#24182;&#29992;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#37319;&#26679;&#30340;&#26367;&#20195;&#26631;&#35760;&#65288;&#21363;&#19981;&#19968;&#33268;&#30340;&#26631;&#35760;&#29983;&#25104;&#36807;&#31243;&#65289;&#36827;&#34892;&#26367;&#25442;&#65292;&#28982;&#21518;&#27169;&#22411;&#38656;&#35201;&#30830;&#23450;&#27599;&#20010;&#26631;&#35760;&#22312;&#21477;&#23376;&#20013;&#30340;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing vision-language pre-training (VLP) approaches adopt cross-modal masked language modeling (CMLM) to learn vision-language associations. However, we find that CMLM is insufficient for this purpose according to our observations: (1) Modality bias: a considerable amount of masked tokens in CMLM can be recovered with only the language information, ignoring the visual inputs. (2) Under-utilization of the unmasked tokens: CMLM primarily focuses on the masked token but it cannot simultaneously leverage other tokens to learn vision-language associations. To handle those limitations, we propose EPIC (lEveraging Per Image-Token Consistency for vision-language pre-training). In EPIC, for each image-sentence pair, we mask tokens that are salient to the image (i.e., Saliency-based Masking Strategy) and replace them with alternatives sampled from a language model (i.e., Inconsistent Token Generation Procedure), and then the model is required to determine for each token in the sentence w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25439;&#22833;&#20989;&#25968;&#29992;&#20110;&#40065;&#26834;&#23398;&#20064;&#65292;&#36890;&#36807;&#33258;&#21160;&#35843;&#25972;&#30446;&#26631;&#20989;&#25968;&#30340;&#26041;&#24335;&#20174;&#38271;&#23614;&#22122;&#22768;&#25968;&#25454;&#20013;&#23398;&#20064;&#20998;&#31867;&#22120;&#12290;&#21160;&#24577;&#25439;&#22833;&#21253;&#25324;&#26631;&#31614;&#20462;&#27491;&#22120;&#21644;&#36793;&#30028;&#29983;&#25104;&#22120;&#65292;&#33021;&#22815;&#32416;&#27491;&#22122;&#22768;&#26631;&#31614;&#24182;&#29983;&#25104;&#27599;&#20010;&#31867;&#21035;&#30340;&#20998;&#31867;&#36793;&#30028;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#26469;&#20248;&#21270;&#36825;&#20004;&#20010;&#32452;&#20214;&#65292;&#20351;&#20998;&#31867;&#22120;&#36866;&#24212;&#24178;&#20928;&#19988;&#24179;&#34913;&#30340;&#27979;&#35797;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2211.12506</link><description>&lt;p&gt;
&#21160;&#24577;&#25439;&#22833;&#29992;&#20110;&#40065;&#26834;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dynamic Loss For Robust Learning. (arXiv:2211.12506v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25439;&#22833;&#20989;&#25968;&#29992;&#20110;&#40065;&#26834;&#23398;&#20064;&#65292;&#36890;&#36807;&#33258;&#21160;&#35843;&#25972;&#30446;&#26631;&#20989;&#25968;&#30340;&#26041;&#24335;&#20174;&#38271;&#23614;&#22122;&#22768;&#25968;&#25454;&#20013;&#23398;&#20064;&#20998;&#31867;&#22120;&#12290;&#21160;&#24577;&#25439;&#22833;&#21253;&#25324;&#26631;&#31614;&#20462;&#27491;&#22120;&#21644;&#36793;&#30028;&#29983;&#25104;&#22120;&#65292;&#33021;&#22815;&#32416;&#27491;&#22122;&#22768;&#26631;&#31614;&#24182;&#29983;&#25104;&#27599;&#20010;&#31867;&#21035;&#30340;&#20998;&#31867;&#36793;&#30028;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#26469;&#20248;&#21270;&#36825;&#20004;&#20010;&#32452;&#20214;&#65292;&#20351;&#20998;&#31867;&#22120;&#36866;&#24212;&#24178;&#20928;&#19988;&#24179;&#34913;&#30340;&#27979;&#35797;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#25968;&#25454;&#20013;&#24120;&#24120;&#23384;&#22312;&#26631;&#31614;&#22122;&#22768;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#40065;&#26834;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#21482;&#38024;&#23545;&#20854;&#20013;&#19968;&#31181;&#25968;&#25454;&#20559;&#24046;&#24182;&#22312;&#21516;&#26102;&#36935;&#21040;&#20004;&#31181;&#20559;&#24046;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#21160;&#24577;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#35757;&#32451;&#36807;&#31243;&#33258;&#21160;&#35843;&#25972;&#30446;&#26631;&#20989;&#25968;&#65292;&#20174;&#38271;&#23614;&#22122;&#22768;&#25968;&#25454;&#20013;&#40065;&#26834;&#22320;&#23398;&#20064;&#20998;&#31867;&#22120;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#21160;&#24577;&#25439;&#22833;&#30001;&#19968;&#20010;&#26631;&#31614;&#20462;&#27491;&#22120;&#21644;&#19968;&#20010;&#36793;&#30028;&#29983;&#25104;&#22120;&#32452;&#25104;&#65292;&#20998;&#21035;&#36890;&#36807;&#24863;&#30693;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#21644;&#20998;&#31867;&#22120;&#30340;&#23398;&#20064;&#29366;&#24577;&#26469;&#32416;&#27491;&#22122;&#22768;&#26631;&#31614;&#21644;&#29983;&#25104;&#27599;&#20010;&#31867;&#21035;&#30340;&#28155;&#21152;&#24615;&#20998;&#31867;&#36793;&#30028;&#12290;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#20998;&#23618;&#37319;&#26679;&#31574;&#30053;&#65292;&#22312;&#23569;&#37327;&#26080;&#20559;&#20803;&#25968;&#25454;&#20013;&#20016;&#23500;&#22810;&#26679;&#19988;&#22256;&#38590;&#30340;&#26679;&#26412;&#65292;&#21160;&#24577;&#25439;&#22833;&#20013;&#30340;&#20004;&#20010;&#32452;&#20214;&#36890;&#36807;&#20803;&#23398;&#20064;&#32852;&#21512;&#20248;&#21270;&#65292;&#24182;&#22521;&#20859;&#20998;&#31867;&#22120;&#20197;&#36866;&#24212;&#24178;&#20928;&#19988;&#24179;&#34913;&#30340;&#27979;&#35797;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Label noise and class imbalance commonly coexist in real-world data. Previous works for robust learning, however, usually address either one type of the data biases and underperform when facing them both. To mitigate this gap, this work presents a novel meta-learning based dynamic loss that automatically adjusts the objective functions with the training process to robustly learn a classifier from long-tailed noisy data. Concretely, our dynamic loss comprises a label corrector and a margin generator, which respectively correct noisy labels and generate additive per-class classification margins by perceiving the underlying data distribution as well as the learning state of the classifier. Equipped with a new hierarchical sampling strategy that enriches a small amount of unbiased metadata with diverse and hard samples, the two components in the dynamic loss are optimized jointly through meta-learning and cultivate the classifier to well adapt to clean and balanced test data. Extensive exp
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#27169;&#22411;&#31995;&#32479;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#30456;&#24178;X&#23556;&#32447;&#25955;&#26001;&#22270;&#21644;&#26679;&#21697;&#32467;&#26500;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#34920;&#26126;&#26426;&#22120;&#23398;&#20064;&#22312;&#30830;&#23450;&#25955;&#26001;&#22270;&#20013;&#32467;&#26500;&#30340;&#33021;&#21147;&#19978;&#26159;&#20934;&#30830;&#30340;&#12290;</title><link>http://arxiv.org/abs/2211.08194</link><description>&lt;p&gt;
&#29992;&#20110;&#20998;&#31867;&#21644;&#35299;&#37322;&#30456;&#24178;X&#23556;&#32447;&#25955;&#26001;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Machine learning for classifying and interpreting coherent X-ray speckle patterns. (arXiv:2211.08194v2 [cond-mat.mtrl-sci] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08194
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#27169;&#22411;&#31995;&#32479;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#30456;&#24178;X&#23556;&#32447;&#25955;&#26001;&#22270;&#21644;&#26679;&#21697;&#32467;&#26500;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#34920;&#26126;&#26426;&#22120;&#23398;&#20064;&#22312;&#30830;&#23450;&#25955;&#26001;&#22270;&#20013;&#32467;&#26500;&#30340;&#33021;&#21147;&#19978;&#26159;&#20934;&#30830;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#24178;X&#23556;&#32447;&#20135;&#29983;&#30340;&#25955;&#26001;&#22270;&#19982;&#26448;&#26009;&#30340;&#20869;&#37096;&#32467;&#26500;&#26377;&#30528;&#23494;&#20999;&#30340;&#20851;&#31995;&#65292;&#20294;&#20174;&#25955;&#26001;&#22270;&#20013;&#23450;&#37327;&#22320;&#25512;&#26029;&#32467;&#26500;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#27169;&#22411;&#20108;&#32500;&#22278;&#30424;&#31995;&#32479;&#25506;&#32034;&#30456;&#24178;X&#23556;&#32447;&#25955;&#26001;&#22270;&#21644;&#26679;&#21697;&#32467;&#26500;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#30740;&#31350;&#26426;&#22120;&#23398;&#20064;&#22312;&#23398;&#20064;&#36825;&#31181;&#20851;&#31995;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#26681;&#25454;&#30456;&#24212;&#32467;&#26500;&#20013;&#30340;&#22278;&#30424;&#25968;&#23494;&#24230;&#26469;&#23545;&#30456;&#24178;X&#23556;&#32447;&#25955;&#26001;&#22270;&#36827;&#34892;&#20998;&#31867;&#12290;&#35777;&#26126;&#20102;&#35813;&#20998;&#31867;&#31995;&#32479;&#23545;&#20110;&#38750;&#20998;&#25955;&#21644;&#20998;&#25955;&#23610;&#23544;&#20998;&#24067;&#37117;&#20855;&#26377;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speckle patterns produced by coherent X-ray have a close relationship with the internal structure of materials but quantitative inversion of the relationship to determine structure from speckle patterns is challenging. Here, we investigate the link between coherent X-ray speckle patterns and sample structures using a model 2D disk system and explore the ability of machine learning to learn aspects of the relationship. Specifically, we train a deep neural network to classify the coherent X-ray speckle patterns according to the disk number density in the corresponding structure. It is demonstrated that the classification system is accurate for both non-disperse and disperse size distributions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MemoNet&#30340;CTR&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#21704;&#24076;&#30721;&#26412;&#32593;&#32476;&#65288;HCNet&#65289;&#20316;&#20026;&#35760;&#24518;&#26426;&#21046;&#65292;&#39640;&#25928;&#22320;&#23398;&#20064;&#21644;&#35760;&#24518;&#20132;&#21449;&#29305;&#24449;&#30340;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MemoNet&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#23637;&#29616;&#20986;NLP&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25193;&#23637;&#35268;&#24459;&#12290;</title><link>http://arxiv.org/abs/2211.01334</link><description>&lt;p&gt;
MemoNet: &#36890;&#36807;&#22810;&#21704;&#24076;&#30721;&#26412;&#32593;&#32476;&#39640;&#25928;&#22320;&#35760;&#24518;&#25152;&#26377;&#20132;&#21449;&#29305;&#24449;&#34920;&#31034;&#20197;&#23454;&#29616;CTR&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
MemoNet: Memorizing All Cross Features' Representations Efficiently via Multi-Hash Codebook Network for CTR Prediction. (arXiv:2211.01334v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MemoNet&#30340;CTR&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#21704;&#24076;&#30721;&#26412;&#32593;&#32476;&#65288;HCNet&#65289;&#20316;&#20026;&#35760;&#24518;&#26426;&#21046;&#65292;&#39640;&#25928;&#22320;&#23398;&#20064;&#21644;&#35760;&#24518;&#20132;&#21449;&#29305;&#24449;&#30340;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MemoNet&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#23637;&#29616;&#20986;NLP&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25193;&#23637;&#35268;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#26032;&#21457;&#29616;&#34920;&#26126;&#65292;&#24378;&#22823;&#30340;&#35760;&#24518;&#33021;&#21147;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25104;&#21151;&#36215;&#21040;&#20102;&#24456;&#22823;&#20316;&#29992;&#12290;&#36825;&#21551;&#21457;&#25105;&#20204;&#23558;&#29420;&#31435;&#30340;&#35760;&#24518;&#26426;&#21046;&#24341;&#20837;CTR&#25490;&#21517;&#27169;&#22411;&#65292;&#20197;&#23398;&#20064;&#21644;&#35760;&#24518;&#20132;&#21449;&#29305;&#24449;&#30340;&#34920;&#31034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#21704;&#24076;&#30721;&#26412;&#32593;&#32476;&#65288;HCNet&#65289;&#20316;&#20026;CTR&#20219;&#21153;&#20013;&#39640;&#25928;&#23398;&#20064;&#21644;&#35760;&#24518;&#20132;&#21449;&#29305;&#24449;&#34920;&#31034;&#30340;&#35760;&#24518;&#26426;&#21046;&#12290;HCNet&#20351;&#29992;&#22810;&#21704;&#24076;&#30721;&#26412;&#20316;&#20026;&#20027;&#35201;&#30340;&#35760;&#24518;&#20301;&#32622;&#65292;&#24182;&#30001;&#22810;&#21704;&#24076;&#23547;&#22336;&#12289;&#35760;&#24518;&#24674;&#22797;&#21644;&#29305;&#24449;&#32553;&#20943;&#19977;&#20010;&#38454;&#27573;&#32452;&#25104;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MemoNet&#30340;&#26032;&#22411;CTR&#27169;&#22411;&#65292;&#23558;HCNet&#19982;DNN&#39592;&#24178;&#32593;&#32476;&#30456;&#32467;&#21512;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#22312;&#32447;&#27979;&#35797;&#20013;&#34920;&#26126;&#65292;MemoNet&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;MemoNet&#23637;&#29616;&#20986;NLP&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25193;&#23637;&#35268;&#24459;&#65292;&#36825;&#24847;&#21619;&#30528;&#25105;&#20204;&#21487;&#20197;&#25193;&#22823;&#27169;&#22411;&#35268;&#27169;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
New findings in natural language processing (NLP) demonstrate that the strong memorization capability contributes a lot to the success of Large Language Models (LLM). This inspires us to explicitly bring an independent memory mechanism into CTR ranking model to learn and memorize cross features' representations. In this paper, we propose multi-Hash Codebook NETwork (HCNet) as the memory mechanism for efficiently learning and memorizing representations of cross features in CTR tasks. HCNet uses a multi-hash codebook as the main memory place and the whole memory procedure consists of three phases: multi-hash addressing, memory restoring, and feature shrinking. We also propose a new CTR model named MemoNet which combines HCNet with a DNN backbone. Extensive experimental results on three public datasets and online test show that MemoNet reaches superior performance over state-of-the-art approaches. Besides, MemoNet shows scaling law of large language model in NLP, which means we can enlarg
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26377;&#38480;&#33258;&#21160;&#26426;&#26469;&#39564;&#35777;&#21644;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#26500;&#24314;&#29305;&#27530;&#30340;&#24369;B&#252;chi&#33258;&#21160;&#26426;&#65292;&#33021;&#22815;&#31934;&#30830;&#22320;&#25429;&#25417;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#36755;&#20986;&#34892;&#20026;&#65292;&#24182;&#29992;&#20110;&#35299;&#20915;DNN&#30340;&#24120;&#35265;&#39564;&#35777;&#21644;&#35299;&#37322;&#20219;&#21153;&#65292;&#22914;&#23545;&#25239;&#40065;&#26834;&#24615;&#25110;&#26368;&#23567;&#20805;&#20998;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2211.01022</link><description>&lt;p&gt;
&#20351;&#29992;&#26377;&#38480;&#33258;&#21160;&#26426;&#39564;&#35777;&#21644;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Verifying And Interpreting Neural Networks using Finite Automata. (arXiv:2211.01022v2 [cs.FL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01022
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26377;&#38480;&#33258;&#21160;&#26426;&#26469;&#39564;&#35777;&#21644;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#26500;&#24314;&#29305;&#27530;&#30340;&#24369;B&#252;chi&#33258;&#21160;&#26426;&#65292;&#33021;&#22815;&#31934;&#30830;&#22320;&#25429;&#25417;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#36755;&#20986;&#34892;&#20026;&#65292;&#24182;&#29992;&#20110;&#35299;&#20915;DNN&#30340;&#24120;&#35265;&#39564;&#35777;&#21644;&#35299;&#37322;&#20219;&#21153;&#65292;&#22914;&#23545;&#25239;&#40065;&#26834;&#24615;&#25110;&#26368;&#23567;&#20805;&#20998;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#21253;&#25324;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#22312;&#20869;&#30340;&#21508;&#20010;&#39046;&#22495;&#30340;&#26222;&#36941;&#20351;&#29992;&#21644;&#20854;&#40657;&#30418;&#29305;&#24615;&#65292;&#39564;&#35777;&#23646;&#24615;&#21644;&#35299;&#37322;DNN&#30340;&#34892;&#20026;&#26159;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#26426;&#29702;&#35770;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;DNN&#20998;&#26512;&#20013;&#20986;&#29616;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DNN&#30340;&#36755;&#20837;&#36755;&#20986;&#34892;&#20026;&#21487;&#20197;&#34987;&#19968;&#20010;&#65288;&#29305;&#27530;&#30340;&#65289;&#24369;B&#252;chi&#33258;&#21160;&#26426;&#31934;&#30830;&#22320;&#25429;&#33719;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#26469;&#35299;&#20915;DNN&#30340;&#24120;&#35265;&#39564;&#35777;&#21644;&#35299;&#37322;&#20219;&#21153;&#65292;&#22914;&#23545;&#25239;&#40065;&#26834;&#24615;&#25110;&#26368;&#23567;&#20805;&#20998;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Verifying properties and interpreting the behaviour of deep neural networks (DNN) is an important task given their ubiquitous use in applications, including safety-critical ones, and their black-box nature. We propose an automata-theoric approach to tackling problems arising in DNN analysis. We show that the input-output behaviour of a DNN can be captured precisely by a (special) weak B\"uchi automaton and we show how these can be used to address common verification and interpretation tasks of DNN like adversarial robustness or minimum sufficient reasons.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#35270;&#39057;&#20013;&#29289;&#20307;6D&#23039;&#24577;&#20272;&#35745;&#26694;&#26550;&#65292;&#21033;&#29992;&#20808;&#21069;&#30340;&#24103;&#20449;&#24687;&#36827;&#34892;&#23039;&#24577;&#20272;&#35745;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#32780;&#20934;&#30830;&#30340;&#23039;&#24577;&#20272;&#35745;&#65292;&#33021;&#22815;&#22788;&#29702;&#38271;&#26102;&#38388;&#24207;&#21015;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;CNN&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#20855;&#26377;33fps&#30340;&#22788;&#29702;&#36895;&#24230;&#65292;&#36866;&#29992;&#20110;&#23454;&#26102;&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2210.13540</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#30340;&#22522;&#20110;&#35270;&#39057;&#30340;&#29289;&#20307;6D&#23039;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Video based Object 6D Pose Estimation using Transformers. (arXiv:2210.13540v2 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#35270;&#39057;&#20013;&#29289;&#20307;6D&#23039;&#24577;&#20272;&#35745;&#26694;&#26550;&#65292;&#21033;&#29992;&#20808;&#21069;&#30340;&#24103;&#20449;&#24687;&#36827;&#34892;&#23039;&#24577;&#20272;&#35745;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#32780;&#20934;&#30830;&#30340;&#23039;&#24577;&#20272;&#35745;&#65292;&#33021;&#22815;&#22788;&#29702;&#38271;&#26102;&#38388;&#24207;&#21015;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;CNN&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#20855;&#26377;33fps&#30340;&#22788;&#29702;&#36895;&#24230;&#65292;&#36866;&#29992;&#20110;&#23454;&#26102;&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;VideoPose&#30340;&#22522;&#20110;Transformer&#30340;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#31471;&#21040;&#31471;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#24314;&#27169;&#26550;&#26500;&#65292;&#36890;&#36807;&#20851;&#27880;&#20808;&#21069;&#30340;&#24103;&#26469;&#20272;&#35745;&#35270;&#39057;&#20013;&#20934;&#30830;&#30340;6D&#29289;&#20307;&#23039;&#24577;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#35270;&#39057;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20449;&#24687;&#36827;&#34892;&#23039;&#24577;&#32454;&#21270;&#65292;&#21516;&#26102;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#39640;&#21644;&#40065;&#26834;&#24615;&#24378;&#30340;&#29305;&#28857;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26550;&#26500;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#21644;&#25512;&#29702;&#36828;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#22312;&#35270;&#39057;&#24207;&#21015;&#19978;&#36827;&#34892;&#36845;&#20195;&#32454;&#21270;&#12290;&#23545;YCB-Video&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;Transformer&#26041;&#27861;&#25345;&#24179;&#65292;&#24182;&#30456;&#23545;&#20110;&#22522;&#20110;CNN&#30340;&#26041;&#27861;&#34920;&#29616;&#26174;&#33879;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27599;&#31186;&#33021;&#22788;&#29702;33&#24103;&#65292;&#26356;&#21152;&#39640;&#25928;&#65292;&#22240;&#27492;&#36866;&#29992;&#20110;&#38656;&#35201;&#23454;&#26102;&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#30340;&#21508;&#31181;&#24212;&#29992;&#12290;&#35757;&#32451;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#22312;https://github.com/ApoorvaBeedu/VideoPose&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a Transformer based 6D Object Pose Estimation framework VideoPose, comprising an end-to-end attention based modelling architecture, that attends to previous frames in order to estimate accurate 6D Object Poses in videos. Our approach leverages the temporal information from a video sequence for pose refinement, along with being computationally efficient and robust. Compared to existing methods, our architecture is able to capture and reason from long-range dependencies efficiently, thus iteratively refining over video sequences. Experimental evaluation on the YCB-Video dataset shows that our approach is on par with the state-of-the-art Transformer methods, and performs significantly better relative to CNN based approaches. Further, with a speed of 33 fps, it is also more efficient and therefore applicable to a variety of applications that require real-time object pose estimation. Training code and pretrained models are available at https://github.com/ApoorvaBeedu/VideoPose
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#22270;&#27880;&#24847;&#24490;&#29615;&#32593;&#32476;&#30340;&#27963;&#21160;&#24863;&#30693;&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#23618;&#27425;&#22270;&#21644;&#20351;&#29992;&#23618;&#27425;&#22270;&#27880;&#24847;&#27169;&#22359;&#26469;&#25429;&#25417;&#26102;&#38388;-&#27963;&#21160;-&#20301;&#32622;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#24314;&#27169;&#29992;&#25143;&#30340;&#20559;&#22909;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#21382;&#21490;&#22686;&#24378;&#32622;&#20449;&#26631;&#31614;&#65292;&#29992;&#20110;&#32858;&#28966;&#20110;&#27599;&#20010;&#29992;&#25143;&#30340;&#20010;&#20307;&#32423;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2210.07765</link><description>&lt;p&gt;
&#20855;&#26377;&#23618;&#27425;&#22270;&#27880;&#24847;&#21147;&#24490;&#29615;&#32593;&#32476;&#30340;&#27963;&#21160;&#24863;&#30693;&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Activity-aware Human Mobility Prediction with Hierarchical Graph Attention Recurrent Network. (arXiv:2210.07765v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07765
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#22270;&#27880;&#24847;&#24490;&#29615;&#32593;&#32476;&#30340;&#27963;&#21160;&#24863;&#30693;&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#23618;&#27425;&#22270;&#21644;&#20351;&#29992;&#23618;&#27425;&#22270;&#27880;&#24847;&#27169;&#22359;&#26469;&#25429;&#25417;&#26102;&#38388;-&#27963;&#21160;-&#20301;&#32622;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#24314;&#27169;&#29992;&#25143;&#30340;&#20559;&#22909;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#21382;&#21490;&#22686;&#24378;&#32622;&#20449;&#26631;&#31614;&#65292;&#29992;&#20110;&#32858;&#28966;&#20110;&#27599;&#20010;&#29992;&#25143;&#30340;&#20010;&#20307;&#32423;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;&#26159;&#19968;&#39033;&#22522;&#30784;&#20219;&#21153;&#65292;&#23545;&#20110;&#22478;&#24066;&#35268;&#21010;&#12289;&#22522;&#20110;&#20301;&#32622;&#30340;&#26381;&#21153;&#21644;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#31561;&#21508;&#31181;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#24573;&#30053;&#20102;&#23545;&#34892;&#20026;&#20449;&#24687;&#30340;&#32771;&#34385;&#65292;&#36825;&#26159;&#25512;&#29702;&#20154;&#31867;&#20559;&#22909;&#21644;&#20363;&#34892;&#27963;&#21160;&#30340;&#20851;&#38190;&#65292;&#25110;&#32773;&#37319;&#29992;&#20102;&#31616;&#21270;&#30340;&#26102;&#38388;&#12289;&#27963;&#21160;&#21644;&#20301;&#32622;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#22270;&#27880;&#24847;&#24490;&#29615;&#32593;&#32476;&#65288;HGARN&#65289;&#30340;&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22522;&#20110;&#25152;&#26377;&#29992;&#25143;&#30340;&#21382;&#21490;&#31227;&#21160;&#35760;&#24405;&#26500;&#24314;&#20102;&#19968;&#20010;&#23618;&#27425;&#22270;&#65292;&#24182;&#20351;&#29992;&#23618;&#27425;&#22270;&#27880;&#24847;&#27169;&#22359;&#26469;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#38388;-&#27963;&#21160;-&#20301;&#32622;&#20381;&#36182;&#20851;&#31995;&#12290;&#36825;&#26679;&#65292;HGARN&#21487;&#20197;&#23398;&#20064;&#20855;&#26377;&#20016;&#23500;&#30340;&#20154;&#31867;&#20986;&#34892;&#35821;&#20041;&#30340;&#34920;&#31034;&#65292;&#20197;&#24314;&#27169;&#29992;&#25143;&#22312;&#20840;&#23616;&#23618;&#38754;&#19978;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#21382;&#21490;&#22686;&#24378;&#32622;&#20449;&#65288;MAHEC&#65289;&#26631;&#31614;&#65292;&#20197;&#20415;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#32858;&#28966;&#20110;&#27599;&#20010;&#29992;&#25143;&#30340;&#20010;&#20307;&#32423;&#20559;&#22909;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26102;&#38388;&#27169;&#22359;...
&lt;/p&gt;
&lt;p&gt;
Human mobility prediction is a fundamental task essential for various applications, including urban planning, location-based services and intelligent transportation systems. Existing methods often ignore activity information crucial for reasoning human preferences and routines, or adopt a simplified representation of the dependencies between time, activities and locations. To address these issues, we present Hierarchical Graph Attention Recurrent Network (HGARN) for human mobility prediction. Specifically, we construct a hierarchical graph based on all users' history mobility records and employ a Hierarchical Graph Attention Module to capture complex time-activity-location dependencies. This way, HGARN can learn representations with rich human travel semantics to model user preferences at the global level. We also propose a model-agnostic history-enhanced confidence (MAHEC) label to focus our model on each user's individual-level preferences. Finally, we introduce a Temporal Module, wh
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#19968;&#31995;&#21015;&#20998;&#23376;&#34920;&#24449;&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#21457;&#29616;&#22522;&#20110;&#22266;&#23450;&#34920;&#24449;&#30340;&#27169;&#22411;&#22312;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#20013;&#20855;&#26377;&#19968;&#23450;&#20248;&#21183;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#27963;&#24615;&#26029;&#23830;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.13492</link><description>&lt;p&gt;
&#26242;&#20572;&#20998;&#23376;&#34920;&#24449;&#23398;&#20064;&#65306;&#29992;&#20110;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Taking a Respite from Representation Learning for Molecular Property Prediction. (arXiv:2209.13492v3 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#19968;&#31995;&#21015;&#20998;&#23376;&#34920;&#24449;&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#21457;&#29616;&#22522;&#20110;&#22266;&#23450;&#34920;&#24449;&#30340;&#27169;&#22411;&#22312;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#20013;&#20855;&#26377;&#19968;&#23450;&#20248;&#21183;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#27963;&#24615;&#26029;&#23830;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#20854;&#20013;&#37325;&#35201;&#30340;&#20219;&#21153;&#20043;&#19968;&#23601;&#26159;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#12290;&#34429;&#28982;&#20998;&#23376;&#34920;&#24449;&#23398;&#20064;&#30340;&#25216;&#26415;&#22914;&#27492;&#21457;&#36798;&#65292;&#20294;&#20854;&#32972;&#21518;&#30340;&#22522;&#30784;&#38382;&#39064;&#21364;&#26410;&#34987;&#35748;&#30495;&#25506;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#31181;&#20998;&#23376;&#34920;&#24449;&#23545;&#19968;&#31995;&#21015;&#20195;&#34920;&#24615;&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#12290;&#38500;&#20102;&#24120;&#29992;&#30340;MoleculeNet&#22522;&#20934;&#25968;&#25454;&#38598;&#22806;&#65292;&#25105;&#20204;&#36824;&#20174;ChEMBL&#25968;&#25454;&#24211;&#21644;&#25991;&#29486;&#20013;&#25910;&#38598;&#20102;&#19968;&#22871;&#19982;&#38463;&#29255;&#31867;&#29289;&#36136;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#20197;&#21450;&#20004;&#20010;&#39069;&#22806;&#30340;&#27963;&#24615;&#25968;&#25454;&#38598;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20063;&#32452;&#35013;&#20102;&#19968;&#31995;&#21015;&#20855;&#26377;&#19981;&#21516;&#35268;&#27169;&#30340;&#25551;&#36848;&#31526;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#24635;&#20849;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;62,820&#20010;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;50,220&#20010;&#20351;&#29992;&#22266;&#23450;&#34920;&#24449;&#30340;&#27169;&#22411;&#12289;4,200&#20010;&#20351;&#29992;SMILES&#24207;&#21015;&#30340;&#27169;&#22411;&#21644;8,400&#20010;&#20351;&#29992;&#20998;&#23376;&#22270;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#25968;&#25454;&#38598;&#20998;&#26512;&#65292;&#24182;&#24378;&#35843;&#20102;&#38463;&#29255;&#31867;&#29289;&#36136;&#20013;&#30340;&#27963;&#24615;&#26029;&#23830;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) has been widely applied in drug discovery with a major task as molecular property prediction. Despite booming techniques in molecular representation learning, fundamentals underlying molecular property prediction haven't been carefully examined yet. In this study, we conducted a systematic evaluation on a collection of representative models using various molecular representations. In addition to the commonly used MoleculeNet benchmark datasets, we also assembled a suite of opioids-related datasets from ChEMBL and two additional activity datasets from literature. To interrogate the basic predictive power, we also assembled a series of descriptors datasets with varying sizes to evaluate the models' performance. In total, we trained 62,820 models, including 50,220 models on fixed representations, 4,200 models on SMILES sequences and 8,400 models on molecular graphs. We first conducted dataset profiling and highlighted the activity-cliffs issue in the opioids-r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#30450;&#30446;&#26816;&#27979;&#36755;&#20837;&#38899;&#39057;&#30340;&#30495;&#23454;&#24615;&#65292;&#21487;&#20197;&#26377;&#25928;&#24212;&#23545;&#38899;&#39057;&#27450;&#35784;&#38382;&#39064;&#30340;&#20998;&#31867;&#22120;&#12290;&#32780;&#36825;&#31181;&#20998;&#31867;&#22120;&#19981;&#38656;&#35201;&#20219;&#20309;&#21442;&#32771;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#30495;&#23454;&#26469;&#28304;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#20986;&#27169;&#20223;&#38899;&#39057;&#12290;</title><link>http://arxiv.org/abs/2209.12573</link><description>&lt;p&gt;
&#25968;&#23383;&#38899;&#39057;&#21462;&#35777;&#65306;&#30450;&#30446;&#26816;&#27979;&#20154;&#31867;&#35821;&#38899;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
Digital Audio Forensics: Blind Human Voice Mimicry Detection. (arXiv:2209.12573v4 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#30450;&#30446;&#26816;&#27979;&#36755;&#20837;&#38899;&#39057;&#30340;&#30495;&#23454;&#24615;&#65292;&#21487;&#20197;&#26377;&#25928;&#24212;&#23545;&#38899;&#39057;&#27450;&#35784;&#38382;&#39064;&#30340;&#20998;&#31867;&#22120;&#12290;&#32780;&#36825;&#31181;&#20998;&#31867;&#22120;&#19981;&#38656;&#35201;&#20219;&#20309;&#21442;&#32771;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#30495;&#23454;&#26469;&#28304;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#20986;&#27169;&#20223;&#38899;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#26159;&#20154;&#31867;&#20132;&#27969;&#20013;&#20351;&#29992;&#26368;&#24191;&#27867;&#30340;&#26041;&#24335;&#20043;&#19968;&#65292;&#20294;&#21516;&#26102;&#20063;&#24456;&#23481;&#26131;&#34987;&#35823;&#29992;&#26469;&#27450;&#39575;&#20154;&#20204;&#12290;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#38761;&#21629;&#65292;&#30456;&#20851;&#25216;&#26415;&#29616;&#22312;&#23545;&#20960;&#20046;&#25152;&#26377;&#20154;&#37117;&#21487;&#29992;&#65292;&#36825;&#20351;&#24471;&#29359;&#32618;&#21644;&#20266;&#36896;&#21464;&#24471;&#26356;&#21152;&#31616;&#21333;&#12290;&#26412;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#30450;&#30446;&#20998;&#31867;&#36755;&#20837;&#38899;&#39057;&#20026;&#30495;&#23454;&#25110;&#32773;&#27169;&#20223;&#65307;&#8220;&#30450;&#30446;&#8221;&#25351;&#30340;&#26159;&#33021;&#22815;&#22312;&#27809;&#26377;&#21442;&#32771;&#25110;&#30495;&#23454;&#26469;&#28304;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#20223;&#21046;&#38899;&#39057;&#30340;&#33021;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#26159;&#22312;&#19968;&#20010;&#22823;&#22411;&#38899;&#39057;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#30340;&#19968;&#32452;&#37325;&#35201;&#29305;&#24449;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#20197;&#24471;&#21040;&#19968;&#20010;&#20998;&#31867;&#22120;&#65292;&#35813;&#20998;&#31867;&#22120;&#34987;&#29992;&#20110;&#27979;&#35797;&#19981;&#21516;&#38899;&#39057;&#30340;&#30456;&#21516;&#29305;&#24449;&#38598;&#12290;&#25968;&#25454;&#25552;&#21462;&#33258;&#20004;&#20010;&#21407;&#22987;&#25968;&#25454;&#38598;&#65292;&#29305;&#21035;&#20026;&#36825;&#39033;&#24037;&#20316;&#32780;&#32534;&#20889;;&#19968;&#20010;&#20840;&#33521;&#25991;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#28151;&#21512;&#25968;&#25454;&#38598;&#65288;&#38463;&#25289;&#20271;&#35821;&#21152;&#33521;&#35821;&#65289;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#24050;&#36890;&#36807;GitHub&#20197;&#21407;&#22987;&#24418;&#24335;&#25552;&#20379;&#32473;&#30740;&#31350;&#31038;&#21306;&#65292;&#32593;&#22336;&#20026;https://github.com/SaSs7/Datas
&lt;/p&gt;
&lt;p&gt;
Audio is one of the most used ways of human communication, but at the same time it can be easily misused to trick people. With the revolution of AI, the related technologies are now accessible to almost everyone thus making it simple for the criminals to commit crimes and forgeries. In this work, we introduce a deep learning method to develop a classifier that will blindly classify an input audio as real or mimicked; the word 'blindly' refers to the ability to detect mimicked audio without references or real sources. The proposed model was trained on a set of important features extracted from a large dataset of audios to get a classifier that was tested on the same set of features from different audios. The data was extracted from two raw datasets, especially composed for this work; an all English dataset and a mixed dataset (Arabic plus English). These datasets have been made available, in raw form, through GitHub for the use of the research community at https://github.com/SaSs7/Datas
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#20986;&#29616;&#21487;&#33021;&#20250;&#23548;&#33268;&#20854;&#36861;&#27714;&#19982;&#20154;&#31867;&#21033;&#30410;&#19981;&#23545;&#40784;&#30340;&#30446;&#26631;&#65292;&#24182;&#37319;&#29992;&#27450;&#39575;&#24615;&#34892;&#20026;&#21644;&#26435;&#21147;&#36861;&#27714;&#31574;&#30053;&#12290;&#38450;&#27490;&#36825;&#31181;&#24773;&#20917;&#30340;&#21457;&#29983;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2209.00626</link><description>&lt;p&gt;
&#20174;&#28145;&#24230;&#23398;&#20064;&#30340;&#35270;&#35282;&#30475;&#24453;&#23545;&#40784;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
The alignment problem from a deep learning perspective. (arXiv:2209.00626v5 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.00626
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#20986;&#29616;&#21487;&#33021;&#20250;&#23548;&#33268;&#20854;&#36861;&#27714;&#19982;&#20154;&#31867;&#21033;&#30410;&#19981;&#23545;&#40784;&#30340;&#30446;&#26631;&#65292;&#24182;&#37319;&#29992;&#27450;&#39575;&#24615;&#34892;&#20026;&#21644;&#26435;&#21147;&#36861;&#27714;&#31574;&#30053;&#12290;&#38450;&#27490;&#36825;&#31181;&#24773;&#20917;&#30340;&#21457;&#29983;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26410;&#26469;&#20960;&#21313;&#24180;&#20869;&#65292;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#21487;&#33021;&#22312;&#35768;&#22810;&#20851;&#38190;&#20219;&#21153;&#19978;&#36229;&#36234;&#20154;&#31867;&#33021;&#21147;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22914;&#26524;&#27809;&#26377;&#22823;&#37327;&#21162;&#21147;&#26469;&#38450;&#27490;&#23427;&#65292;AGIs&#21487;&#33021;&#20250;&#23398;&#20250;&#36861;&#27714;&#19982;&#20154;&#31867;&#21033;&#30410;&#20914;&#31361;&#65288;&#21363;&#19981;&#23545;&#40784;&#65289;&#30340;&#30446;&#26631;&#12290;&#22914;&#26524;&#20687;&#29616;&#22312;&#26368;&#20855;&#33021;&#21147;&#30340;&#27169;&#22411;&#19968;&#26679;&#36827;&#34892;&#35757;&#32451;&#65292;AGIs&#21487;&#33021;&#20250;&#23398;&#20250;&#27450;&#39575;&#24615;&#22320;&#34892;&#21160;&#20197;&#33719;&#24471;&#26356;&#39640;&#30340;&#22870;&#21169;&#65292;&#23398;&#20250;&#22312;&#20854;&#24494;&#35843;&#20998;&#24067;&#20043;&#22806;&#36827;&#34892;&#20869;&#37096;&#30446;&#26631;&#30340;&#27867;&#21270;&#65292;&#24182;&#21033;&#29992;&#23547;&#27714;&#26435;&#21147;&#30340;&#31574;&#30053;&#26469;&#36861;&#27714;&#36825;&#20123;&#30446;&#26631;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#36825;&#20123;&#29305;&#24615;&#30340;&#26032;&#35777;&#25454;&#12290;&#20855;&#26377;&#36825;&#20123;&#29305;&#24615;&#30340;AGIs&#23558;&#24456;&#38590;&#36827;&#34892;&#23545;&#40784;&#65292;&#21363;&#20351;&#22312;&#19981;&#23545;&#40784;&#30340;&#24773;&#20917;&#19979;&#20063;&#21487;&#33021;&#34920;&#29616;&#20986;&#23545;&#40784;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#19981;&#23545;&#40784;&#30340;AGIs&#30340;&#37096;&#32626;&#22914;&#20309;&#21487;&#33021;&#20250;&#19981;&#21487;&#36870;&#22320;&#21066;&#24369;&#20154;&#31867;&#23545;&#19990;&#30028;&#30340;&#25511;&#21046;&#65292;&#24182;&#31616;&#35201;&#22238;&#39038;&#20102;&#26088;&#22312;&#38450;&#27490;&#36825;&#31181;&#32467;&#26524;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In coming decades, artificial general intelligence (AGI) may surpass human capabilities at many critical tasks. We argue that, without substantial effort to prevent it, AGIs could learn to pursue goals that conflict (i.e., are misaligned) with human interests. If trained like today's most capable models, AGIs could learn to act deceptively to receive higher reward, learn internally-represented goals which generalize beyond their fine-tuning distributions, and pursue those goals using power-seeking strategies. We review emerging evidence for these properties. AGIs with these properties would be difficult to align and may appear aligned even when they are not. We outline how the deployment of misaligned AGIs might irreversibly undermine human control over the world, and briefly review research directions aimed at preventing this outcome.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;gSwin&#30340;&#35270;&#35273;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;Swin Transformer&#21644;&#65288;&#22810;&#22836;&#65289;gMLP&#20004;&#20010;&#27969;&#27966;&#65292;&#33021;&#22815;&#21516;&#26102;&#20860;&#39038;&#21442;&#25968;&#25928;&#29575;&#12289;&#24615;&#33021;&#12289;&#23616;&#37096;&#24615;&#21644;&#23618;&#32423;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24230;&#65292;&#24182;&#19988;&#27169;&#22411;&#23610;&#23544;&#26356;&#23567;&#12290;</title><link>http://arxiv.org/abs/2208.11718</link><description>&lt;p&gt;
gSwin: &#20855;&#26377;&#31227;&#21160;&#31383;&#21475;&#30340;&#20998;&#23618;&#32467;&#26500;&#30340;&#38376;&#25511;&#22810;&#23618;&#24863;&#30693;&#22120;&#35270;&#35273;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
gSwin: Gated MLP Vision Model with Hierarchical Structure of Shifted Window. (arXiv:2208.11718v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11718
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;gSwin&#30340;&#35270;&#35273;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;Swin Transformer&#21644;&#65288;&#22810;&#22836;&#65289;gMLP&#20004;&#20010;&#27969;&#27966;&#65292;&#33021;&#22815;&#21516;&#26102;&#20860;&#39038;&#21442;&#25968;&#25928;&#29575;&#12289;&#24615;&#33021;&#12289;&#23616;&#37096;&#24615;&#21644;&#23618;&#32423;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24230;&#65292;&#24182;&#19988;&#27169;&#22411;&#23610;&#23544;&#26356;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#35328;&#39046;&#22495;&#21462;&#24471;&#25104;&#21151;&#21518;&#65292;&#33258;&#27880;&#24847;&#26426;&#21046;&#65288;transformer&#65289;&#24471;&#21040;&#20102;&#22312;&#35270;&#35273;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#24182;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#27492;&#22806;&#65292;&#20316;&#20026;&#21478;&#19968;&#31181;&#27969;&#27966;&#65292;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#20063;&#22312;&#35270;&#35273;&#39046;&#22495;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;&#38500;&#20102;&#20256;&#32479;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#20123;&#26550;&#26500;&#36817;&#26469;&#19968;&#30452;&#24341;&#36215;&#20851;&#27880;&#65292;&#24182;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#12290;&#20316;&#20026;&#19968;&#31181;&#33021;&#22815;&#22312;&#22270;&#20687;&#35782;&#21035;&#20013;&#20860;&#39038;&#21442;&#25968;&#25928;&#29575;&#12289;&#24615;&#33021;&#12289;&#23616;&#37096;&#24615;&#21644;&#23618;&#32423;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;gSwin&#65292;&#23558;&#20004;&#20010;&#27969;&#27966;&#21512;&#24182;&#36215;&#26469;&#65292;&#21363;Swin Transformer&#21644;&#65288;&#22810;&#22836;&#65289;gMLP&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;gSwin&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#31561;&#19977;&#20010;&#35270;&#35273;&#20219;&#21153;&#19978;&#33021;&#22815;&#36798;&#21040;&#27604;Swin Transformer&#26356;&#39640;&#30340;&#20934;&#30830;&#24230;&#65292;&#24182;&#19988;&#27169;&#22411;&#23610;&#23544;&#26356;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following the success in language domain, the self-attention mechanism (transformer) is adopted in the vision domain and achieving great success recently. Additionally, as another stream, multi-layer perceptron (MLP) is also explored in the vision domain. These architectures, other than traditional CNNs, have been attracting attention recently, and many methods have been proposed. As one that combines parameter efficiency and performance with locality and hierarchy in image recognition, we propose gSwin, which merges the two streams; Swin Transformer and (multi-head) gMLP. We showed that our gSwin can achieve better accuracy on three vision tasks, image classification, object detection and semantic segmentation, than Swin Transformer, with smaller model size.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24212;&#29992;&#32852;&#21512;&#23398;&#20064;&#25216;&#26415;&#23545;12&#23548;&#32852;&#24515;&#30005;&#22270;&#20449;&#21495;&#36827;&#34892;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#65292;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#35757;&#32451;AI&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#21307;&#30103;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#38598;&#20013;&#23398;&#20064;&#26041;&#24335;&#30456;&#24403;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2208.10993</link><description>&lt;p&gt;
&#20351;&#29992;&#32852;&#21512;&#23398;&#20064;&#25216;&#26415;&#23545;12&#23548;&#32852;&#24515;&#30005;&#22270;&#20449;&#21495;&#36827;&#34892;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Application of federated learning techniques for arrhythmia classification using 12-lead ECG signals. (arXiv:2208.10993v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10993
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24212;&#29992;&#32852;&#21512;&#23398;&#20064;&#25216;&#26415;&#23545;12&#23548;&#32852;&#24515;&#30005;&#22270;&#20449;&#21495;&#36827;&#34892;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#65292;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#35757;&#32451;AI&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#21307;&#30103;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#38598;&#20013;&#23398;&#20064;&#26041;&#24335;&#30456;&#24403;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#22823;&#22411;&#12289;&#31934;&#24515;&#31574;&#21010;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#23545;&#20110;&#21033;&#29992;&#20302;&#21151;&#32791;&#24515;&#30005;&#30417;&#27979;&#35774;&#22791;&#20449;&#24687;&#25552;&#20379;&#26089;&#26399;&#26816;&#27979;&#12289;&#26356;&#24555;&#35786;&#26029;&#21644;&#26356;&#26377;&#25928;&#27835;&#30103;&#20855;&#26377;&#24456;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#24403;&#20351;&#29992;&#12289;&#19981;&#23433;&#20840;&#30340;&#23384;&#20648;&#25110;&#25968;&#25454;&#27844;&#28431;&#21487;&#33021;&#20250;&#20405;&#29359;&#20010;&#20154;&#38544;&#31169;&#65292;&#33719;&#21462;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#25935;&#24863;&#21307;&#30103;&#25968;&#25454;&#21463;&#21040;&#20005;&#26684;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#32852;&#21512;&#23398;&#20064;&#65288;FL&#65289;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#22312;&#26469;&#33258;&#20845;&#20010;&#24322;&#26500;&#26469;&#28304;&#30340;12&#23548;&#32852;&#20256;&#24863;&#22120;&#38453;&#21015;&#37319;&#38598;&#30340;&#39640;&#28165;&#24515;&#30005;&#22270;&#30340;&#24322;&#26500;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25152;&#24471;&#27169;&#22411;&#19982;&#20197;&#38598;&#20013;&#24335;&#23398;&#20064;&#65288;CL&#65289;&#26041;&#24335;&#35757;&#32451;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#31561;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#29420;&#31435;&#21644;&#30456;&#21516;&#20998;&#24067;&#30340;&#32852;&#21512;&#25968;&#25454;&#20197;&#21450;&#38750;&#30456;&#21516;&#20998;&#24067;&#30340;&#32852;&#21512;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence-based (AI) analysis of large, curated medical datasets is promising for providing early detection, faster diagnosis, and more effective treatment using low-power Electrocardiography (ECG) monitoring devices information. However, accessing sensitive medical data from diverse sources is highly restricted since improper use, unsafe storage, or data leakage could violate a person's privacy. This work uses a Federated Learning (FL) privacy-preserving methodology to train AI models over heterogeneous sets of high-definition ECG from 12-lead sensor arrays collected from six heterogeneous sources. We evaluated the capacity of the resulting models to achieve equivalent performance compared to state-of-the-art models trained in a Centralized Learning (CL) fashion. Moreover, we assessed the performance of our solution over Independent and Identical distributed (IID) and non-IID federated data. Our methodology involves machine learning techniques based on Deep Neural Networ
&lt;/p&gt;</description></item><item><title>MolGraph&#26159;&#19968;&#20010;&#20351;&#29992;TensorFlow&#21644;Keras&#23454;&#29616;&#20998;&#23376;&#22270;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;Python&#21253;&#65292;&#20026;&#20998;&#23376;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#25552;&#20379;&#20102;&#39640;&#24230;&#20860;&#23481;&#21644;&#39564;&#35777;&#24615;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2208.09944</link><description>&lt;p&gt;
MolGraph: &#19968;&#20010;&#20351;&#29992;TensorFlow&#21644;Keras&#23454;&#29616;&#20998;&#23376;&#22270;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;Python&#21253;
&lt;/p&gt;
&lt;p&gt;
MolGraph: a Python package for the implementation of molecular graphs and graph neural networks with TensorFlow and Keras. (arXiv:2208.09944v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09944
&lt;/p&gt;
&lt;p&gt;
MolGraph&#26159;&#19968;&#20010;&#20351;&#29992;TensorFlow&#21644;Keras&#23454;&#29616;&#20998;&#23376;&#22270;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;Python&#21253;&#65292;&#20026;&#20998;&#23376;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#25552;&#20379;&#20102;&#39640;&#24230;&#20860;&#23481;&#21644;&#39564;&#35777;&#24615;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#26426;&#22120;&#23398;&#20064;&#22312;&#35299;&#20915;&#21508;&#31181;&#20998;&#23376;&#38382;&#39064;&#65288;&#22914;&#22522;&#20110;&#20998;&#23376;&#25551;&#36848;&#31526;&#25110;&#25351;&#32441;&#39044;&#27979;&#20998;&#23376;&#24615;&#36136;&#65289;&#26041;&#38754;&#34987;&#35777;&#26126;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#26368;&#36817;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#31639;&#27861;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#20998;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#20854;&#34920;&#29616;&#19982;&#25551;&#36848;&#31526;&#25110;&#25351;&#32441;&#26041;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#12290;&#34429;&#28982;&#24050;&#32463;&#23384;&#22312;&#21508;&#31181;&#24037;&#20855;&#21644;&#36719;&#20214;&#21253;&#29992;&#20110;&#22312;&#20998;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#24212;&#29992;GNNs&#65292;&#20294;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;MolGraph&#30340;&#26032;&#30340;GNN&#36719;&#20214;&#21253;&#65292;&#26088;&#22312;&#21019;&#24314;&#19982;TensorFlow&#21644;Keras&#24212;&#29992;&#31243;&#24207;&#25509;&#21475;&#39640;&#24230;&#20860;&#23481;&#30340;GNN&#27169;&#22411;&#31649;&#36947;&#12290;MolGraph&#36824;&#23454;&#29616;&#20102;&#19968;&#20010;&#21270;&#23398;&#27169;&#22359;&#65292;&#29992;&#20110;&#29983;&#25104;&#23567;&#20998;&#23376;&#22270;&#65292;&#21487;&#20197;&#36890;&#36807;GNN&#31639;&#27861;&#35299;&#20915;&#20998;&#23376;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#12290;&#20026;&#20102;&#39564;&#35777;GNNs&#30340;&#24615;&#33021;&#65292;&#23427;&#20204;&#19982;MoleculeNet&#25968;&#25454;&#38598;&#20197;&#21450;&#19977;&#20010;&#33394;&#35889;&#20445;&#30041;&#26102;&#38388;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular machine learning (ML) has proven important for tackling various molecular problems, such as predicting molecular properties based on molecular descriptors or fingerprints. Since relatively recently, graph neural network (GNN) algorithms have been implemented for molecular ML, showing comparable or superior performance to descriptor or fingerprint-based approaches. Although various tools and packages exist to apply GNNs in molecular ML, a new GNN package, named MolGraph, was developed in this work with the motivation to create GNN model pipelines highly compatible with the TensorFlow and Keras application programming interface (API). MolGraph also implements a chemistry module to accommodate the generation of small molecular graphs, which can be passed to a GNN algorithm to solve a molecular ML problem. To validate the GNNs, they were benchmarked against the datasets of MoleculeNet, as well as three chromatographic retention time datasets. The results on these benchmarks illus
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#25277;&#35937;&#35268;&#21010;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#39044;&#27979;&#26469;&#25552;&#39640;&#26426;&#22120;&#20154;&#22312;&#38271;&#26399;&#20219;&#21153;&#20013;&#30340;&#20915;&#31574;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2208.07737</link><description>&lt;p&gt;
&#23398;&#20064;&#26377;&#25928;&#30340;&#36873;&#25321;&#39044;&#27979;&#30340;&#25277;&#35937;&#35268;&#21010;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Efficient Abstract Planning Models that Choose What to Predict. (arXiv:2208.07737v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07737
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#25277;&#35937;&#35268;&#21010;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#39044;&#27979;&#26469;&#25552;&#39640;&#26426;&#22120;&#20154;&#22312;&#38271;&#26399;&#20219;&#21153;&#20013;&#30340;&#20915;&#31574;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#34892;&#21160;&#31354;&#38388;&#30340;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#65292;&#35299;&#20915;&#38271;&#26399;&#20219;&#21153;&#30340;&#26377;&#25928;&#26041;&#27861;&#26159;&#21452;&#23618;&#35268;&#21010;&#65292;&#20854;&#20013;&#22312;&#29615;&#22659;&#30340;&#25277;&#35937;&#23618;&#19978;&#36827;&#34892;&#39640;&#32423;&#25628;&#32034;&#20197;&#25351;&#23548;&#20302;&#32423;&#20915;&#31574;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#23398;&#20064;&#31526;&#21495;&#25805;&#20316;&#21644;&#31070;&#32463;&#37319;&#26679;&#22120;&#30340;&#25277;&#35937;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#36825;&#31181;&#21452;&#23618;&#35268;&#21010;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#30340;&#31526;&#21495;&#25805;&#20316;&#23398;&#20064;&#26041;&#27861;&#22312;&#35768;&#22810;&#26426;&#22120;&#20154;&#39046;&#22495;&#23384;&#22312;&#19981;&#36275;&#20043;&#22788;&#65292;&#22240;&#20026;&#26426;&#22120;&#20154;&#30340;&#34892;&#21160;&#24448;&#24448;&#20250;&#24341;&#36215;&#25277;&#35937;&#29366;&#24577;&#20013;&#22823;&#37327;&#26080;&#20851;&#30340;&#21464;&#21270;&#65292;&#32780;&#36825;&#20123;&#26041;&#27861;&#35797;&#22270;&#23398;&#20064;&#20934;&#30830;&#39044;&#27979;&#25277;&#35937;&#29366;&#24577;&#20013;&#25152;&#26377;&#35266;&#23519;&#21040;&#30340;&#21464;&#21270;&#30340;&#25805;&#20316;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;"&#36873;&#25321;&#35201;&#39044;&#27979;"&#30340;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#21482;&#23545;&#23454;&#29616;&#25351;&#23450;&#30446;&#26631;&#30340;&#25277;&#35937;&#35268;&#21010;&#25152;&#24517;&#38656;&#30340;&#21464;&#21270;&#24314;&#27169;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#20986;&#23548;&#33268;10&#20010;&#19981;&#21516;&#28151;&#21512;&#20219;&#21153;&#19978;&#30340;&#39640;&#25928;&#35268;&#21010;&#30340;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
An effective approach to solving long-horizon tasks in robotics domains with continuous state and action spaces is bilevel planning, wherein a high-level search over an abstraction of an environment is used to guide low-level decision-making. Recent work has shown how to enable such bilevel planning by learning abstract models in the form of symbolic operators and neural samplers. In this work, we show that existing symbolic operator learning approaches fall short in many robotics domains where a robot's actions tend to cause a large number of irrelevant changes in the abstract state. This is primarily because they attempt to learn operators that exactly predict all observed changes in the abstract state. To overcome this issue, we propose to learn operators that 'choose what to predict' by only modelling changes necessary for abstract planning to achieve specified goals. Experimentally, we show that our approach learns operators that lead to efficient planning across 10 different hybr
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#33258;&#21160;&#23450;&#20301;&#24046;&#24322;&#65288;ALD&#65289;&#30340;&#25968;&#25454;&#39537;&#21160;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24046;&#24322;&#38382;&#39064;&#12290;ALD&#36866;&#29992;&#20110;&#20219;&#24847;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#30340;&#24046;&#24322;&#23450;&#20041;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#20998;&#31867;&#21644;&#36830;&#32493;&#39044;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2208.06680</link><description>&lt;p&gt;
&#23450;&#20301;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Locating disparities in machine learning. (arXiv:2208.06680v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06680
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#33258;&#21160;&#23450;&#20301;&#24046;&#24322;&#65288;ALD&#65289;&#30340;&#25968;&#25454;&#39537;&#21160;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24046;&#24322;&#38382;&#39064;&#12290;ALD&#36866;&#29992;&#20110;&#20219;&#24847;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#30340;&#24046;&#24322;&#23450;&#20041;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#20998;&#31867;&#21644;&#36830;&#32493;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21487;&#33021;&#20250;&#20135;&#29983;&#24046;&#24322;&#21270;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#20854;&#20013;&#20154;&#32676;&#30340;&#23376;&#32676;&#65288;&#22914;&#25353;&#24180;&#40836;&#65292;&#24615;&#21035;&#25110;&#20854;&#20182;&#25935;&#24863;&#23646;&#24615;&#23450;&#20041;&#65289;&#20250;&#34987;&#31995;&#32479;&#24615;&#22320;&#22788;&#20110;&#19981;&#21033;&#22320;&#20301;&#12290;&#20026;&#20102;&#31526;&#21512;&#21363;&#23558;&#20986;&#21488;&#30340;&#27861;&#35268;&#65292;&#20174;&#19994;&#20154;&#21592;&#38656;&#35201;&#25214;&#20986;&#36825;&#31181;&#24046;&#24322;&#24615;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#25991;&#29486;&#36890;&#24120;&#36890;&#36807;&#24403;&#25935;&#24863;&#23646;&#24615;&#20107;&#20808;&#25351;&#23450;&#26102;&#30340;&#32479;&#35745;&#31243;&#24207;&#26469;&#26816;&#27979;&#24046;&#24322;&#12290;&#36825;&#38480;&#21046;&#20102;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#36866;&#29992;&#24615;&#65292;&#22240;&#20026;&#25968;&#25454;&#38598;&#36890;&#24120;&#26159;&#39640;&#32500;&#30340;&#65292;&#32780;&#19988;&#25935;&#24863;&#23646;&#24615;&#21487;&#33021;&#26159;&#26410;&#30693;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#33258;&#21160;&#23450;&#20301;&#24046;&#24322;&#65288;ALD&#65289;&#30340;&#25968;&#25454;&#39537;&#21160;&#26694;&#26550;&#65292;&#26088;&#22312;&#23450;&#20301;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24046;&#24322;&#12290;ALD&#28385;&#36275;&#19994;&#30028;&#30340;&#20960;&#20010;&#38656;&#27714;&#65306;ALD&#65288;1&#65289;&#36866;&#29992;&#20110;&#20219;&#24847;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65307;&#65288;2&#65289;&#21487;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#24046;&#24322;&#23450;&#20041;&#65288;&#22914;&#32479;&#35745;&#24179;&#31561;&#25110;&#24179;&#31561;&#36180;&#29575;&#65289;&#65307;&#65288;3&#65289;&#21487;&#20197;&#22788;&#29702;&#20998;&#31867;&#21644;&#36830;&#32493;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning can provide predictions with disparate outcomes, in which subgroups of the population (e.g., defined by age, gender, or other sensitive attributes) are systematically disadvantaged. In order to comply with upcoming legislation, practitioners need to locate such disparate outcomes. However, previous literature typically detects disparities through statistical procedures for when the sensitive attribute is specified a priori. This limits applicability in real-world settings where datasets are high dimensional and, on top of that, sensitive attributes may be unknown. As a remedy, we propose a data-driven framework called Automatic Location of Disparities (ALD) which aims at locating disparities in machine learning. ALD meets several demands from industry: ALD (1) is applicable to arbitrary machine learning classifiers; (2) operates on different definitions of disparities (e.g., statistical parity or equalized odds); and (3) deals with both categorical and continuous predi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#20174;&#26410;&#26631;&#35760;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#29992;&#30340;&#34920;&#31034;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#20102;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22686;&#24378;&#36873;&#25321;&#65292;&#24182;&#23558;&#35813;&#26694;&#26550;&#25193;&#23637;&#21040;&#21322;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2208.06616</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#21322;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Contrastive Representation Learning for Semi-supervised Time-Series Classification. (arXiv:2208.06616v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06616
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#20174;&#26410;&#26631;&#35760;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#29992;&#30340;&#34920;&#31034;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#20102;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22686;&#24378;&#36873;&#25321;&#65292;&#24182;&#23558;&#35813;&#26694;&#26550;&#25193;&#23637;&#21040;&#21322;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21482;&#26377;&#26410;&#26631;&#35760;&#25968;&#25454;&#25110;&#23569;&#37327;&#26631;&#35760;&#26679;&#26412;&#21487;&#29992;&#26102;&#65292;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#36890;&#36807;&#23545;&#25968;&#25454;&#30340;&#19981;&#21516;&#22686;&#24378;&#35270;&#22270;&#36827;&#34892;&#23545;&#27604;&#65292;&#22312;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#29992;&#34920;&#31034;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25913;&#36827;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21363;&#22522;&#20110;&#26102;&#38388;&#21644;&#19978;&#19979;&#25991;&#23545;&#27604;&#30340;&#26102;&#38388;&#24207;&#21015;&#23545;&#27604;&#65288;TS-TCC&#65289;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#25552;&#21462;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#24369;&#22686;&#24378;&#21644;&#24378;&#22686;&#24378;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#30340;&#35270;&#22270;&#22312;&#25552;&#20986;&#30340;&#26102;&#38388;&#23545;&#27604;&#27169;&#22359;&#20013;&#23398;&#20064;&#24378;&#22823;&#30340;&#26102;&#38388;&#20851;&#31995;&#65292;&#27492;&#22806;&#36824;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#19978;&#19979;&#25991;&#23545;&#27604;&#27169;&#22359;&#23398;&#20064;&#26377;&#21306;&#20998;&#21147;&#30340;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22686;&#24378;&#36873;&#25321;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#36825;&#26159;&#23545;&#27604;&#23398;&#20064;&#30340;&#20851;&#38190;&#37096;&#20998;&#12290;&#25105;&#20204;&#36824;&#23558;TS-TCC&#25512;&#24191;&#21040;&#21322;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;Class-A&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning time-series representations when only unlabeled data or few labeled samples are available can be a challenging task. Recently, contrastive self-supervised learning has shown great improvement in extracting useful representations from unlabeled data via contrasting different augmented views of data. In this work, we propose a novel Time-Series representation learning framework via Temporal and Contextual Contrasting (TS-TCC) that learns representations from unlabeled data with contrastive learning. Specifically, we propose time-series-specific weak and strong augmentations and use their views to learn robust temporal relations in the proposed temporal contrasting module, besides learning discriminative representations by our proposed contextual contrasting module. Additionally, we conduct a systematic study of time-series data augmentation selection, which is a key part of contrastive learning. We also extend TS-TCC to the semi-supervised learning settings and propose a Class-A
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26234;&#33021;&#22238;&#22797;&#24212;&#29992;&#31243;&#24207;&#20013;&#28508;&#22312;&#30340;&#20449;&#24687;&#27844;&#28431;&#28431;&#27934;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23454;&#38469;&#35774;&#32622;&#20013;&#38480;&#21046;&#26597;&#35810;&#31867;&#22411;&#30340;&#25915;&#20987;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2207.10802</link><description>&lt;p&gt;
&#20174;&#26234;&#33021;&#22238;&#22797;&#20013;&#25552;&#21462;&#20027;&#21160;&#27169;&#24335;&#36827;&#34892;&#35748;&#35777;
&lt;/p&gt;
&lt;p&gt;
Combing for Credentials: Active Pattern Extraction from Smart Reply. (arXiv:2207.10802v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.10802
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26234;&#33021;&#22238;&#22797;&#24212;&#29992;&#31243;&#24207;&#20013;&#28508;&#22312;&#30340;&#20449;&#24687;&#27844;&#28431;&#28431;&#27934;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23454;&#38469;&#35774;&#32622;&#20013;&#38480;&#21046;&#26597;&#35810;&#31867;&#22411;&#30340;&#25915;&#20987;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;GPT-2&#21644;BERT&#65292;&#36890;&#24120;&#20250;&#36890;&#36807;&#24494;&#35843;&#26469;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#19968;&#20010;&#33258;&#28982;&#30340;&#20363;&#23376;&#26159;&#8220;&#26234;&#33021;&#22238;&#22797;&#8221;&#24212;&#29992;&#31243;&#24207;&#65292;&#20854;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#34987;&#35843;&#25972;&#20197;&#25552;&#20379;&#32473;&#23450;&#26597;&#35810;&#28040;&#24687;&#30340;&#24314;&#35758;&#22238;&#22797;&#12290;&#30001;&#20110;&#24494;&#35843;&#25968;&#25454;&#36890;&#24120;&#26159;&#25935;&#24863;&#30340;&#25968;&#25454;&#65292;&#22914;&#30005;&#23376;&#37038;&#20214;&#25110;&#32842;&#22825;&#35760;&#24405;&#65292;&#22240;&#27492;&#37325;&#35201;&#30340;&#26159;&#20102;&#35299;&#21644;&#20943;&#36731;&#27169;&#22411;&#27844;&#28431;&#24494;&#35843;&#25968;&#25454;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#20856;&#22411;&#26234;&#33021;&#22238;&#22797;&#27969;&#31243;&#20013;&#28508;&#22312;&#30340;&#20449;&#24687;&#27844;&#38706;&#28431;&#27934;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#29616;&#23454;&#30340;&#24773;&#20917;&#65292;&#21363;&#25915;&#20987;&#32773;&#21482;&#33021;&#36890;&#36807;&#21069;&#31471;&#30028;&#38754;&#19982;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#38480;&#21046;&#20102;&#21487;&#20197;&#21457;&#36865;&#21040;&#27169;&#22411;&#30340;&#26597;&#35810;&#31867;&#22411;&#12290;&#20808;&#21069;&#30340;&#25915;&#20987;&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#19981;&#36215;&#20316;&#29992;&#65292;&#32780;&#26159;&#38656;&#35201;&#33021;&#22815;&#30452;&#25509;&#21521;&#27169;&#22411;&#21457;&#36865;&#26080;&#38480;&#21046;&#30340;&#26597;&#35810;&#12290;&#21363;&#20351;&#22312;&#27809;&#26377;&#26597;&#35810;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#24448;&#30340;&#25915;&#20987;&#36890;&#24120;&#38656;&#35201;&#25968;&#21315;&#29978;&#33267;&#25968;&#30334;&#19975;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large language models, such as GPT\nobreakdash-2 and BERT, are often fine-tuned to achieve state-of-the-art performance on a downstream task. One natural example is the ``Smart Reply'' application where a pre-trained model is tuned to provide suggested responses for a given query message. Since the tuning data is often sensitive data such as emails or chat transcripts, it is important to understand and mitigate the risk that the model leaks its tuning data. We investigate potential information leakage vulnerabilities in a typical Smart Reply pipeline. We consider a realistic setting where the adversary can only interact with the underlying model through a front-end interface that constrains what types of queries can be sent to the model. Previous attacks do not work in these settings, but require the ability to send unconstrained queries directly to the model. Even when there are no constraints on the queries, previous attacks typically require thousands, or even millions, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;zPROBE&#65292;&#19968;&#31181;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#38646;&#31397;&#25506;&#40065;&#26834;&#24615;&#26816;&#26597;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#39640;&#26029;&#28857;&#22522;&#20110;&#25490;&#24207;&#30340;&#32479;&#35745;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#38543;&#26426;&#21270;&#32858;&#31867;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#23545;&#32858;&#21512;&#27169;&#22411;&#26356;&#26032;&#30340;&#31169;&#23494;&#40065;&#26834;&#24615;&#26816;&#26597;&#65292;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#19988;&#19981;&#25439;&#23475;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2206.12100</link><description>&lt;p&gt;
zPROBE&#65306;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#38646;&#31397;&#25506;&#40065;&#26834;&#24615;&#26816;&#26597;
&lt;/p&gt;
&lt;p&gt;
zPROBE: Zero Peek Robustness Checks for Federated Learning. (arXiv:2206.12100v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12100
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;zPROBE&#65292;&#19968;&#31181;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#38646;&#31397;&#25506;&#40065;&#26834;&#24615;&#26816;&#26597;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#39640;&#26029;&#28857;&#22522;&#20110;&#25490;&#24207;&#30340;&#32479;&#35745;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#38543;&#26426;&#21270;&#32858;&#31867;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#23545;&#32858;&#21512;&#27169;&#22411;&#26356;&#26032;&#30340;&#31169;&#23494;&#40065;&#26834;&#24615;&#26816;&#26597;&#65292;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#19988;&#19981;&#25439;&#23475;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#30340;&#32852;&#37030;&#23398;&#20064;&#20801;&#35768;&#22810;&#20010;&#29992;&#25143;&#36890;&#36807;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#21327;&#35843;&#26469;&#20849;&#21516;&#35757;&#32451;&#27169;&#22411;&#12290;&#26381;&#21153;&#22120;&#21482;&#23398;&#20064;&#26368;&#32456;&#32858;&#21512;&#32467;&#26524;&#65292;&#22240;&#27492;&#29992;&#25143;&#30340;&#65288;&#31169;&#26377;&#65289;&#35757;&#32451;&#25968;&#25454;&#19981;&#20250;&#20174;&#20010;&#20307;&#27169;&#22411;&#26356;&#26032;&#20013;&#27844;&#28431;&#20986;&#21435;&#12290;&#28982;&#32780;&#65292;&#20445;&#25345;&#20010;&#20307;&#26356;&#26032;&#30340;&#31169;&#23494;&#24615;&#20801;&#35768;&#24694;&#24847;&#29992;&#25143;&#36827;&#34892;&#25308;&#21344;&#24237;&#25915;&#20987;&#65292;&#38477;&#20302;&#20934;&#30830;&#24615;&#20294;&#19981;&#34987;&#26816;&#27979;&#21040;&#12290;&#30446;&#21069;&#23545;&#25239;&#25308;&#21344;&#24237;&#24037;&#20154;&#30340;&#26368;&#20339;&#26041;&#27861;&#20381;&#36182;&#20110;&#40065;&#26834;&#30340;&#22522;&#20110;&#25490;&#24207;&#30340;&#32479;&#35745;&#20449;&#24687;&#65292;&#20363;&#22914;&#20013;&#20301;&#25968;&#65292;&#20197;&#26597;&#25214;&#24694;&#24847;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#22312;&#23433;&#20840;&#39046;&#22495;&#23454;&#29616;&#20445;&#25252;&#38544;&#31169;&#30340;&#22522;&#20110;&#25490;&#24207;&#30340;&#32479;&#35745;&#20449;&#24687;&#26159;&#38750;&#24120;&#22256;&#38590;&#21644;&#19981;&#21487;&#25193;&#23637;&#30340;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#23545;&#25152;&#26377;&#20010;&#20307;&#26356;&#26032;&#36827;&#34892;&#25490;&#24207;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#31532;&#19968;&#20010;&#20351;&#29992;&#39640;&#26029;&#28857;&#22522;&#20110;&#25490;&#24207;&#30340;&#32479;&#35745;&#20449;&#24687;&#23545;&#32858;&#21512;&#27169;&#22411;&#26356;&#26032;&#36827;&#34892;&#31169;&#23494;&#30340;&#40065;&#26834;&#24615;&#26816;&#26597;&#12290;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#21270;&#32858;&#31867;&#65292;&#25105;&#20204;&#26174;&#33879;&#25552;&#39640;&#20102;&#25105;&#20204;&#30340;&#38450;&#24481;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#21516;&#26102;&#19981;&#25439;&#23475;&#38544;&#31169;&#12290;&#25105;&#20204;&#21033;&#29992;&#38646;&#26029;&#28857;&#19979;&#30340;&#32479;&#35745;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy-preserving federated learning allows multiple users to jointly train a model with coordination of a central server. The server only learns the final aggregation result, thus the users' (private) training data is not leaked from the individual model updates. However, keeping the individual updates private allows malicious users to perform Byzantine attacks and degrade the accuracy without being detected. Best existing defenses against Byzantine workers rely on robust rank-based statistics, e.g., median, to find malicious updates. However, implementing privacy-preserving rank-based statistics is nontrivial and not scalable in the secure domain, as it requires sorting all individual updates. We establish the first private robustness check that uses high break point rank-based statistics on aggregated model updates. By exploiting randomized clustering, we significantly improve the scalability of our defense without compromising privacy. We leverage our statistical bounds in zero-kn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#20998;&#21106;&#36807;&#31243;&#65292;&#26088;&#22312;&#21152;&#24555;&#32959;&#30244;&#27835;&#30103;&#20013;&#35843;&#25972;&#21058;&#37327;&#36882;&#36865;&#36335;&#24452;&#30340;&#27969;&#31243;&#65292;&#36991;&#20813;&#32791;&#26102;&#30340;&#25163;&#21160;&#25805;&#20316;&#65292;&#25552;&#39640;&#24739;&#32773;&#27835;&#30103;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2206.11048</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#33258;&#21160;&#21270;&#32963;&#32928;&#36947;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Automated GI tract segmentation using deep learning. (arXiv:2206.11048v5 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.11048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#20998;&#21106;&#36807;&#31243;&#65292;&#26088;&#22312;&#21152;&#24555;&#32959;&#30244;&#27835;&#30103;&#20013;&#35843;&#25972;&#21058;&#37327;&#36882;&#36865;&#36335;&#24452;&#30340;&#27969;&#31243;&#65292;&#36991;&#20813;&#32791;&#26102;&#30340;&#25163;&#21160;&#25805;&#20316;&#65292;&#25552;&#39640;&#24739;&#32773;&#27835;&#30103;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25918;&#23556;&#27835;&#30103;&#19987;&#23478;&#30340;&#24037;&#20316;&#26159;&#23558;X&#23556;&#32447;&#26463;&#25351;&#21521;&#32959;&#30244;&#30340;&#21516;&#26102;&#36991;&#20813;&#32963;&#21644;&#32928;&#36947;&#12290;&#36890;&#36807;MR-Linacs&#65288;&#30913;&#20849;&#25391;&#25104;&#20687;&#21644;&#32447;&#24615;&#21152;&#36895;&#22120;&#31995;&#32479;&#65289;&#65292;&#25918;&#23556;&#27835;&#30103;&#19987;&#23478;&#21487;&#20197;&#21487;&#35270;&#21270;&#32959;&#30244;&#20301;&#32622;&#65292;&#24182;&#26681;&#25454;&#32959;&#30244;&#32454;&#32990;&#30340;&#23384;&#22312;&#24773;&#20917;&#31934;&#30830;&#25511;&#21046;&#21058;&#37327;&#65292;&#32780;&#36825;&#31181;&#24773;&#20917;&#27599;&#22825;&#37117;&#21487;&#33021;&#26377;&#25152;&#21464;&#21270;&#12290;&#30446;&#21069;&#30340;&#24037;&#20316;&#26159;&#30830;&#23450;&#32963;&#21644;&#32928;&#36947;&#30340;&#20301;&#32622;&#65292;&#20197;&#35843;&#25972;X&#23556;&#32447;&#26463;&#30340;&#26041;&#21521;&#65292;&#20197;&#20415;&#23558;&#21058;&#37327;&#20934;&#30830;&#36882;&#36865;&#21040;&#32959;&#30244;&#24182;&#36991;&#20813;&#36825;&#20123;&#22120;&#23448;&#12290;&#36825;&#26159;&#19968;&#20010;&#32791;&#26102;&#19988;&#20154;&#21147;&#23494;&#38598;&#30340;&#36807;&#31243;&#65292;&#38500;&#38750;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#33258;&#21160;&#21270;&#20998;&#21106;&#36807;&#31243;&#65292;&#21542;&#21017;&#27835;&#30103;&#26102;&#38388;&#24456;&#23481;&#26131;&#20174;15&#20998;&#38047;&#24310;&#38271;&#21040;&#27599;&#22825;1&#23567;&#26102;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#20998;&#21106;&#36807;&#31243;&#65292;&#20197;&#21152;&#24555;&#27492;&#36807;&#31243;&#24182;&#20351;&#26356;&#22810;&#24739;&#32773;&#33719;&#24471;&#26377;&#25928;&#27835;&#30103;&#12290;
&lt;/p&gt;
&lt;p&gt;
The job of Radiation oncologists is to deliver x-ray beams pointed toward the tumor and at the same time avoid the stomach and intestines. With MR-Linacs (magnetic resonance imaging and linear accelerator systems), oncologists can visualize the position of the tumor and allow for precise dose according to tumor cell presence which can vary from day to day. The current job of outlining the position of the stomach and intestines to adjust the X-ray beams direction for the dose delivery to the tumor while avoiding the organs. This is a time-consuming and labor-intensive process that can easily prolong treatments from 15 minutes to an hour a day unless deep learning methods can automate the segmentation process. This paper discusses an automated segmentation process using deep learning to make this process faster and allow more patients to get effective treatment.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26263;&#30693;&#35782;&#35757;&#32451;&#26367;&#20195;&#27169;&#22411;&#20197;&#25552;&#39640;&#23545;&#25239;&#24615;&#26679;&#26412;&#20256;&#36882;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#26263;&#30693;&#35782;&#21644;&#20351;&#29992;&#28151;&#21512;&#25193;&#22686;&#31639;&#27861;&#26469;&#35757;&#32451;&#25152;&#35859;&#30340;&#26263;&#26367;&#20195;&#27169;&#22411;(DSM)&#12290;</title><link>http://arxiv.org/abs/2206.08316</link><description>&lt;p&gt;
&#29992;&#26263;&#30693;&#35782;&#25552;&#39640;&#26367;&#20195;&#27169;&#22411;&#30340;&#23545;&#25239;&#20256;&#36882;&#24615;
&lt;/p&gt;
&lt;p&gt;
Boosting the Adversarial Transferability of Surrogate Models with Dark Knowledge. (arXiv:2206.08316v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26263;&#30693;&#35782;&#35757;&#32451;&#26367;&#20195;&#27169;&#22411;&#20197;&#25552;&#39640;&#23545;&#25239;&#24615;&#26679;&#26412;&#20256;&#36882;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#26263;&#30693;&#35782;&#21644;&#20351;&#29992;&#28151;&#21512;&#25193;&#22686;&#31639;&#27861;&#26469;&#35757;&#32451;&#25152;&#35859;&#30340;&#26263;&#26367;&#20195;&#27169;&#22411;(DSM)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#23545;&#23545;&#25239;&#24615;&#26679;&#26412;&#20855;&#26377;&#33030;&#24369;&#24615;&#12290;&#32780;&#19988;&#65292;&#23545;&#25239;&#24615;&#26679;&#26412;&#20855;&#26377;&#20256;&#36882;&#24615;&#65292;&#36825;&#24847;&#21619;&#30528;&#19968;&#20010;DNN&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#21487;&#20197;&#20197;&#38750;&#24179;&#20961;&#30340;&#27010;&#29575;&#27450;&#39575;&#21478;&#19968;&#20010;&#27169;&#22411;&#12290;&#36825;&#23548;&#33268;&#20102;&#22522;&#20110;&#20256;&#36882;&#30340;&#25915;&#20987;&#65292;&#20854;&#20013;&#30001;&#26367;&#20195;&#27169;&#22411;&#29983;&#25104;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#29992;&#20110;&#36827;&#34892;&#40657;&#30418;&#25915;&#20987;&#12290;&#26377;&#19968;&#20123;&#30740;&#31350;&#33268;&#21147;&#20110;&#29983;&#25104;&#20855;&#26377;&#26356;&#22909;&#20256;&#36882;&#24615;&#30340;&#32473;&#23450;&#26367;&#20195;&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#19968;&#20010;&#29305;&#27530;&#30340;&#26367;&#20195;&#27169;&#22411;&#20197;&#29983;&#25104;&#20855;&#26377;&#26356;&#22909;&#20256;&#36882;&#24615;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#30456;&#23545;&#36739;&#23569;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26263;&#30693;&#35782;&#35757;&#32451;&#26367;&#20195;&#27169;&#22411;&#20197;&#22686;&#24378;&#26367;&#20195;&#27169;&#22411;&#29983;&#25104;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#20256;&#36882;&#24615;&#30340;&#26041;&#27861;&#12290;&#36825;&#20010;&#35757;&#32451;&#30340;&#26367;&#20195;&#27169;&#22411;&#34987;&#31216;&#20026;&#26263;&#26367;&#20195;&#27169;&#22411;(DSM)&#12290;&#35757;&#32451;DSM&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65306;&#25552;&#21462;&#26263;&#30693;&#35782;&#30340;&#25945;&#24072;&#27169;&#22411;&#21644;&#28151;&#21512;&#25193;&#22686;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are vulnerable to adversarial examples. And, the adversarial examples have transferability, which means that an adversarial example for a DNN model can fool another model with a non-trivial probability. This gave birth to the transfer-based attack where the adversarial examples generated by a surrogate model are used to conduct black-box attacks. There are some work on generating the adversarial examples from a given surrogate model with better transferability. However, training a special surrogate model to generate adversarial examples with better transferability is relatively under-explored. This paper proposes a method for training a surrogate model with dark knowledge to boost the transferability of the adversarial examples generated by the surrogate model. This trained surrogate model is named dark surrogate model (DSM). The proposed method for training a DSM consists of two key components: a teacher model extracting dark knowledge, and the mixing augme
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ResNorm&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26631;&#20934;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#22609;&#36896;&#33410;&#28857;&#26631;&#20934;&#24046;&#20998;&#24067;&#26469;&#35299;&#20915;&#38271;&#23614;&#33410;&#28857;&#24230;&#20998;&#24067;&#38382;&#39064;&#65292;&#25552;&#39640;&#33410;&#28857;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.08181</link><description>&lt;p&gt;
ResNorm: &#36890;&#36807;&#26631;&#20934;&#21270;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#38271;&#23614;&#24230;&#20998;&#24067;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
ResNorm: Tackling Long-tailed Degree Distribution Issue in Graph Neural Networks via Normalization. (arXiv:2206.08181v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ResNorm&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26631;&#20934;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#22609;&#36896;&#33410;&#28857;&#26631;&#20934;&#24046;&#20998;&#24067;&#26469;&#35299;&#20915;&#38271;&#23614;&#33410;&#28857;&#24230;&#20998;&#24067;&#38382;&#39064;&#65292;&#25552;&#39640;&#33410;&#28857;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22240;&#20854;&#33021;&#22815;&#20174;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#23398;&#20064;&#34920;&#31034;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#23613;&#31649;GNNs&#22312;&#35768;&#22810;&#39046;&#22495;&#26377;&#30528;&#25104;&#21151;&#30340;&#24212;&#29992;&#65292;&#20294;GNNs&#30340;&#20248;&#21270;&#30740;&#31350;&#36739;&#23569;&#65292;&#33410;&#28857;&#20998;&#31867;&#30340;&#24615;&#33021;&#20005;&#37325;&#21463;&#21040;&#38271;&#23614;&#33410;&#28857;&#24230;&#20998;&#24067;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#26631;&#20934;&#21270;&#25552;&#39640;GNNs&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#30740;&#31350;&#22270;&#20013;&#33410;&#28857;&#24230;&#30340;&#38271;&#23614;&#20998;&#24067;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GNN&#26631;&#20934;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;ResNorm&#65288;&#36890;&#36807;&#26631;&#20934;&#21270;&#23558;&#38271;&#23614;&#20998;&#24067;&#37325;&#26032;&#22609;&#36896;&#25104;&#31867;&#20284;&#27491;&#24577;&#20998;&#24067;&#65289;&#12290;ResNorm&#30340;scale&#25805;&#20316;&#36890;&#36807;&#37325;&#26032;&#22609;&#36896;&#33410;&#28857;&#26631;&#20934;&#24046;&#65288;NStd&#65289;&#20998;&#24067;&#65292;&#20174;&#32780;&#25552;&#39640;&#23614;&#33410;&#28857;&#65288;&#21363;&#20302;&#24230;&#33410;&#28857;&#65289;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#21644;&#23454;&#35777;&#35777;&#25454;&#26469;&#29702;&#35299;&#19978;&#36848;scale&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have attracted much attention due to their ability in learning representations from graph-structured data. Despite the successful applications of GNNs in many domains, the optimization of GNNs is less well studied, and the performance on node classification heavily suffers from the long-tailed node degree distribution. This paper focuses on improving the performance of GNNs via normalization.  In detail, by studying the long-tailed distribution of node degrees in the graph, we propose a novel normalization method for GNNs, which is termed ResNorm (\textbf{Res}haping the long-tailed distribution into a normal-like distribution via \textbf{norm}alization). The $scale$ operation of ResNorm reshapes the node-wise standard deviation (NStd) distribution so as to improve the accuracy of tail nodes (\textit{i}.\textit{e}., low-degree nodes). We provide a theoretical interpretation and empirical evidence for understanding the mechanism of the above $scale$. In addit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#23450;&#35268;&#21010;&#30340;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24358;&#29366;&#31232;&#30095;&#24615;&#65292;&#26088;&#22312;&#25913;&#21892;&#29616;&#26377;&#25216;&#26415;&#20013;&#23384;&#22312;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.03482</link><description>&lt;p&gt;
&#22522;&#20110;&#21322;&#23450;&#35268;&#21010;&#30340;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#20013;&#30340;&#24358;&#29366;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
Chordal Sparsity for SDP-based Neural Network Verification. (arXiv:2206.03482v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#23450;&#35268;&#21010;&#30340;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24358;&#29366;&#31232;&#30095;&#24615;&#65292;&#26088;&#22312;&#25913;&#21892;&#29616;&#26377;&#25216;&#26415;&#20013;&#23384;&#22312;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#26032;&#20852;&#25216;&#26415;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#65292;&#20294;&#39564;&#35777;&#20854;&#27491;&#30830;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#24050;&#30693;&#32593;&#32476;&#36755;&#20986;&#23545;&#20110;&#21363;&#20351;&#26159;&#23567;&#30340;&#36755;&#20837;&#25200;&#21160;&#20063;&#38750;&#24120;&#25935;&#24863;&#21644;&#33030;&#24369;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#19981;&#21487;&#39044;&#27979;&#21644;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#30340;&#39118;&#38505;&#12290;&#24555;&#36895;&#32780;&#20934;&#30830;&#22320;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#23545;&#20854;&#24191;&#27867;&#37319;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#36817;&#24180;&#26469;&#24050;&#32463;&#24320;&#21457;&#20986;&#22810;&#31181;&#26041;&#27861;&#26469;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#20110;&#25913;&#36827;&#22522;&#20110;&#21322;&#23450;&#35268;&#21010;&#65288;SDP&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#25216;&#26415;&#12290;&#36825;&#20123;&#25216;&#26415;&#22312;&#20445;&#30041;&#20984;&#38382;&#39064;&#24418;&#24335;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#20102;&#34920;&#36798;&#22797;&#26434;&#20960;&#20309;&#32422;&#26463;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#21487;&#25193;&#23637;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#36215;&#28857;&#26159;Fazlyab&#31561;&#20154;&#25552;&#20986;&#30340;DeepSDP&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#20108;&#27425;&#32422;&#26463;&#23558;&#39564;&#35777;&#38382;&#39064;&#25277;&#35937;&#20026;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;SDP&#12290;&#28982;&#32780;&#65292;&#24403;&#32593;&#32476;&#35268;&#27169;&#22686;&#38271;&#26102;&#65292;&#35299;&#20915;&#36825;&#20010;SDP&#38382;&#39064;&#21464;&#24471;&#22256;&#38590;&#19988;&#32791;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are central to many emerging technologies, but verifying their correctness remains a major challenge. It is known that network outputs can be sensitive and fragile to even small input perturbations, thereby increasing the risk of unpredictable and undesirable behavior. Fast and accurate verification of neural networks is therefore critical to their widespread adoption, and in recent years a variety of methods have been developed as a response to this problem. In this paper, we focus on improving semidefinite programming (SDP) based techniques for neural network verification. Such techniques offer the power of expressing complex geometric constraints while retaining a convex problem formulation, but in practice, scalability remains a major issue. Our starting point is the DeepSDP framework proposed by Fazlyab et al, which uses quadratic constraints to abstract the verification problem into a large-scale SDP. When the network size grows, however, solving this SDP quickly 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#27169;&#22411;&#23545;&#20010;&#20307;&#31034;&#20363;&#30340;&#38544;&#31169;&#20445;&#35777;&#65292;&#24182;&#21457;&#29616;&#22823;&#22810;&#25968;&#31034;&#20363;&#20139;&#26377;&#36739;&#24378;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#35757;&#32451;&#25439;&#22833;&#21644;&#31034;&#20363;&#30340;&#38544;&#31169;&#21442;&#25968;&#23384;&#22312;&#24456;&#24378;&#30340;&#30456;&#20851;&#24615;&#12290;&#26368;&#20302;&#20934;&#30830;&#29575;&#31867;&#21035;&#30340;&#24179;&#22343;&#38544;&#31169;&#21442;&#25968;&#27604;&#26368;&#39640;&#20934;&#30830;&#29575;&#31867;&#21035;&#39640;44.2%&#12290;</title><link>http://arxiv.org/abs/2206.02617</link><description>&lt;p&gt;
&#20010;&#20307;&#38544;&#31169;&#20250;&#35745;&#23545;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Individual Privacy Accounting for Differentially Private Stochastic Gradient Descent. (arXiv:2206.02617v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#27169;&#22411;&#23545;&#20010;&#20307;&#31034;&#20363;&#30340;&#38544;&#31169;&#20445;&#35777;&#65292;&#24182;&#21457;&#29616;&#22823;&#22810;&#25968;&#31034;&#20363;&#20139;&#26377;&#36739;&#24378;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#35757;&#32451;&#25439;&#22833;&#21644;&#31034;&#20363;&#30340;&#38544;&#31169;&#21442;&#25968;&#23384;&#22312;&#24456;&#24378;&#30340;&#30456;&#20851;&#24615;&#12290;&#26368;&#20302;&#20934;&#30830;&#29575;&#31867;&#21035;&#30340;&#24179;&#22343;&#38544;&#31169;&#21442;&#25968;&#27604;&#26368;&#39640;&#20934;&#30830;&#29575;&#31867;&#21035;&#39640;44.2%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26159;&#26368;&#36817;&#31169;&#26377;&#28145;&#24230;&#23398;&#20064;&#30340;&#21069;&#27839;&#31639;&#27861;&#12290;&#23427;&#20026;&#25968;&#25454;&#38598;&#20013;&#30340;&#25152;&#26377;&#25968;&#25454;&#28857;&#25552;&#20379;&#20102;&#21333;&#19968;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#20010;&#20363;&#30340;&#36755;&#20986;&#29305;&#23450;$(\varepsilon,\delta)$-DP&#65292;&#20197;&#21051;&#30011;&#36890;&#36807;DP-SGD&#35757;&#32451;&#30340;&#27169;&#22411;&#23545;&#20010;&#21035;&#31034;&#20363;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#39640;&#25928;&#31639;&#27861;&#26469;&#30740;&#31350;&#36328;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#20010;&#20307;&#38544;&#31169;&#12290;&#25105;&#20204;&#21457;&#29616;&#22823;&#22810;&#25968;&#31034;&#20363;&#37117;&#20139;&#26377;&#27604;&#26368;&#22351;&#24773;&#20917;&#36793;&#30028;&#26356;&#24378;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#35757;&#32451;&#25439;&#22833;&#21644;&#31034;&#20363;&#30340;&#38544;&#31169;&#21442;&#25968;&#20043;&#38388;&#23384;&#22312;&#24456;&#24378;&#30340;&#30456;&#20851;&#24615;&#12290;&#36825;&#24847;&#21619;&#30528;&#22312;&#27169;&#22411;&#25928;&#29992;&#26041;&#38754;&#21463;&#21040;&#19981;&#36275;&#30340;&#32676;&#20307;&#21516;&#26102;&#32463;&#21382;&#36739;&#24369;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;&#20363;&#22914;&#65292;&#22312;CIFAR-10&#19978;&#65292;&#26368;&#20302;&#27979;&#35797;&#20934;&#30830;&#29575;&#31867;&#21035;&#30340;&#24179;&#22343;$\varepsilon$&#27604;&#26368;&#39640;&#20934;&#30830;&#29575;&#31867;&#21035;&#39640;44.2%&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentially private stochastic gradient descent (DP-SGD) is the workhorse algorithm for recent advances in private deep learning. It provides a single privacy guarantee to all datapoints in the dataset. We propose output-specific $(\varepsilon,\delta)$-DP to characterize privacy guarantees for individual examples when releasing models trained by DP-SGD. We also design an efficient algorithm to investigate individual privacy across a number of datasets. We find that most examples enjoy stronger privacy guarantees than the worst-case bound. We further discover that the training loss and the privacy parameter of an example are well-correlated. This implies groups that are underserved in terms of model utility simultaneously experience weaker privacy guarantees. For example, on CIFAR-10, the average $\varepsilon$ of the class with the lowest test accuracy is 44.2\% higher than that of the class with the highest accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#40065;&#26834;&#24615;&#27979;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#29305;&#24449;&#21644;&#20687;&#32032;&#32423;&#21035;&#30340;&#20998;&#24067;&#65292;&#25429;&#25417;&#23545;&#25239;&#25200;&#21160;&#30340;&#24863;&#30693;&#36136;&#37327;&#65292;&#20197;&#25913;&#21892;&#27169;&#22411;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.08589</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#20998;&#23618;&#20998;&#24067;&#24863;&#30693;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Distribution-Aware Testing of Deep Learning. (arXiv:2205.08589v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.08589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#40065;&#26834;&#24615;&#27979;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#29305;&#24449;&#21644;&#20687;&#32032;&#32423;&#21035;&#30340;&#20998;&#24067;&#65292;&#25429;&#25417;&#23545;&#25239;&#25200;&#21160;&#30340;&#24863;&#30693;&#36136;&#37327;&#65292;&#20197;&#25913;&#21892;&#27169;&#22411;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#36234;&#26469;&#36234;&#22810;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#20854;&#21487;&#38752;&#24615;&#30340;&#25285;&#24551;&#12290;&#28145;&#24230;&#23398;&#20064;&#22312;&#38754;&#23545;&#23545;&#25239;&#25200;&#21160;&#65288;&#21363;&#23545;&#25163;&#26679;&#26412;&#65289;&#26102;&#24448;&#24448;&#32570;&#20047;&#40065;&#26834;&#24615;&#12290;&#23613;&#31649;&#26368;&#36817;&#37319;&#29992;&#20102;&#20808;&#36827;&#30340;&#25915;&#20987;&#21644;&#27979;&#35797;&#26041;&#27861;&#26469;&#26816;&#27979;&#23545;&#25163;&#26679;&#26412;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#20102;&#36755;&#20837;&#20998;&#24067;&#21644;&#25200;&#21160;&#30340;&#24863;&#30693;&#36136;&#37327;&#12290;&#32467;&#26524;&#65292;&#26816;&#27979;&#21040;&#30340;&#23545;&#25163;&#26679;&#26412;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#19981;&#30456;&#20851;&#65292;&#25110;&#32773;&#23545;&#20154;&#31867;&#35266;&#23519;&#32773;&#26469;&#35828;&#21487;&#33021;&#30475;&#36215;&#26469;&#19981;&#30495;&#23454;&#12290;&#36825;&#23548;&#33268;&#27979;&#35797;&#36164;&#28304;&#28010;&#36153;&#22312;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24456;&#23569;&#21457;&#29983;&#30340;&#31232;&#26377;&#23545;&#25163;&#26679;&#26412;&#19978;&#65292;&#38480;&#21046;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#38752;&#24615;&#30340;&#25552;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40065;&#26834;&#24615;&#27979;&#35797;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#23545;&#25163;&#26679;&#26412;&#65292;&#32771;&#34385;&#21040;&#20102;&#29305;&#24449;&#32423;&#21035;&#20998;&#24067;&#21644;&#20687;&#32032;&#32423;&#21035;&#20998;&#24067;&#65292;&#25429;&#25417;&#20102;&#23545;&#25239;&#25200;&#21160;&#30340;&#24863;&#30693;&#36136;&#37327;&#12290;&#36825;&#20004;&#31181;&#32771;&#34385;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23618;&#26426;&#21046;&#26469;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning (DL) is increasingly used in safety-critical applications, raising concerns about its reliability. DL suffers from a well-known problem of lacking robustness, especially when faced with adversarial perturbations known as Adversarial Examples (AEs). Despite recent efforts to detect AEs using advanced attack and testing methods, these approaches often overlook the input distribution and perceptual quality of the perturbations. As a result, the detected AEs may not be relevant in practical applications or may appear unrealistic to human observers. This can waste testing resources on rare AEs that seldom occur during real-world use, limiting improvements in DL model dependability.  In this paper, we propose a new robustness testing approach for detecting AEs that considers both the feature level distribution and the pixel level distribution, capturing the perceptual quality of adversarial perturbations. The two considerations are encoded by a novel hierarchical mechanism. Fir
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#30465;&#24335;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#65288;IDML&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#25913;&#36827;&#20102;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2205.04449</link><description>&lt;p&gt;
&#22270;&#20687;&#26816;&#32034;&#30340;&#20869;&#30465;&#24335;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Introspective Deep Metric Learning for Image Retrieval. (arXiv:2205.04449v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.04449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#30465;&#24335;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#65288;IDML&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#25913;&#36827;&#20102;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#30465;&#24335;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#65288;IDML&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#22270;&#20687;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#27604;&#36739;&#12290;&#20256;&#32479;&#30340;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#22312;&#22270;&#20687;&#20043;&#38388;&#20135;&#29983;&#33258;&#20449;&#30340;&#35821;&#20041;&#36317;&#31163;&#65292;&#32780;&#19981;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#19968;&#20010;&#22909;&#30340;&#30456;&#20284;&#24615;&#27169;&#22411;&#24212;&#35813;&#35880;&#24910;&#32771;&#34385;&#35821;&#20041;&#24046;&#24322;&#65292;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#27169;&#31946;&#30340;&#22270;&#20687;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#31283;&#20581;&#30340;&#35757;&#32451;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#35821;&#20041;&#23884;&#20837;&#21644;&#20276;&#38543;&#30340;&#19981;&#30830;&#23450;&#24615;&#23884;&#20837;&#26469;&#34920;&#31034;&#22270;&#20687;&#65292;&#20998;&#21035;&#25551;&#36848;&#22270;&#20687;&#30340;&#35821;&#20041;&#29305;&#24449;&#21644;&#27169;&#31946;&#24230;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#30465;&#24335;&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;&#29992;&#20110;&#22312;&#32771;&#34385;&#22270;&#20687;&#30340;&#35821;&#20041;&#24046;&#24322;&#21644;&#27169;&#31946;&#24230;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#30456;&#20284;&#24615;&#21028;&#26029;&#12290;&#25152;&#25552;&#20986;&#30340;IDML&#26694;&#26550;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#25913;&#36827;&#20102;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;CUB-200-2011&#65292;Cars196&#21644;Stanford Online&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an introspective deep metric learning (IDML) framework for uncertainty-aware comparisons of images. Conventional deep metric learning methods produce confident semantic distances between images regardless of the uncertainty level. However, we argue that a good similarity model should consider the semantic discrepancies with caution to better deal with ambiguous images for more robust training. To achieve this, we propose to represent an image using not only a semantic embedding but also an accompanying uncertainty embedding, which describes the semantic characteristics and ambiguity of an image, respectively. We further propose an introspective similarity metric to make similarity judgments between images considering both their semantic differences and ambiguities. The proposed IDML framework improves the performance of deep metric learning through uncertainty modeling and attains state-of-the-art results on the widely used CUB-200-2011, Cars196, and Stanford Online
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;&#21151;&#33021;&#19981;&#21464;&#36335;&#24452;&#65288;FIP&#65289;&#30340;&#24046;&#20998;&#20960;&#20309;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#30340;&#28789;&#27963;&#12289;&#36830;&#32493;&#36866;&#24212;&#65292;&#20197;&#24212;&#23545;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#30446;&#26631;&#21644;&#32593;&#32476;&#31232;&#30095;&#21270;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2205.00334</link><description>&lt;p&gt;
&#36890;&#36807;&#36941;&#21382;&#21151;&#33021;&#19981;&#21464;&#36335;&#24452;&#65292;&#26500;&#24314;&#28789;&#27963;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Engineering flexible machine learning systems by traversing functionally-invariant paths. (arXiv:2205.00334v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.00334
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;&#21151;&#33021;&#19981;&#21464;&#36335;&#24452;&#65288;FIP&#65289;&#30340;&#24046;&#20998;&#20960;&#20309;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#30340;&#28789;&#27963;&#12289;&#36830;&#32493;&#36866;&#24212;&#65292;&#20197;&#24212;&#23545;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#30446;&#26631;&#21644;&#32593;&#32476;&#31232;&#30095;&#21270;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#21387;&#22120;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#22312;&#22522;&#30784;&#27169;&#22411;&#33539;&#20363;&#20013;&#65292;&#22823;&#22411;&#21464;&#21387;&#22120;&#27169;&#22411;&#65288;BERT&#12289;GPT3/4&#12289;Bloom&#12289;ViT&#65289;&#36890;&#36807;&#33258;&#30417;&#30563;&#20219;&#21153;&#65288;&#22914;&#35789;&#25110;&#22270;&#20687;&#23631;&#34109;&#65289;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#36890;&#36807;&#24494;&#35843;&#36866;&#24212;&#20110;&#19979;&#28216;&#29992;&#25143;&#24212;&#29992;&#65292;&#21253;&#25324;&#25351;&#20196;&#36319;&#38543;&#21644;&#38382;&#31572;&#12290;&#34429;&#28982;&#26377;&#35768;&#22810;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#65288;&#22914;&#20302;&#31209;&#26435;&#37325;&#26356;&#26032;&#31574;&#30053;&#65292;&#22914;LoRA&#65289;&#65292;&#20294;&#20173;&#28982;&#23545;&#23454;&#29616;&#32593;&#32476;&#36866;&#24212;&#24615;&#32780;&#19981;&#25439;&#22833;&#30693;&#35782;&#30340;&#25968;&#23398;&#21407;&#29702;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24046;&#20998;&#20960;&#20309;&#26694;&#26550;&#65292;&#21151;&#33021;&#19981;&#21464;&#36335;&#24452;&#65288;FIP&#65289;&#65292;&#20026;&#19968;&#31995;&#21015;&#26426;&#22120;&#23398;&#20064;&#30446;&#26631;&#21644;&#32593;&#32476;&#31232;&#30095;&#21270;&#30446;&#26631;&#25552;&#20379;&#28789;&#27963;&#21644;&#36830;&#32493;&#30340;&#31070;&#32463;&#32593;&#32476;&#36866;&#24212;&#12290;&#25105;&#20204;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#31354;&#38388;&#26500;&#24819;&#20026;&#19968;&#20010;&#26354;&#29575;&#30340;&#40654;&#26364;&#27969;&#24418;&#65292;&#24182;&#37197;&#22791;&#20102;&#19968;&#20010;&#24230;&#35268;&#24352;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have emerged as the state of the art neural network architecture for natural language processing and computer vision. In the foundation model paradigm, large transformer models (BERT, GPT3/4, Bloom, ViT) are pre-trained on self-supervised tasks such as word or image masking, and then, adapted through fine-tuning for downstream user applications including instruction following and Question Answering. While many approaches have been developed for model fine-tuning including low-rank weight update strategies (eg. LoRA), underlying mathematical principles that enable network adaptation without knowledge loss remain poorly understood. Here, we introduce a differential geometry framework, functionally invariant paths (FIP), that provides flexible and continuous adaptation of neural networks for a range of machine learning goals and network sparsification objectives. We conceptualize the weight space of a neural network as a curved Riemannian manifold equipped with a metric tenso
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#20960;&#31181;&#39044;&#35757;&#32451;&#21464;&#20307;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;&#39046;&#22495;&#65292;&#20351;&#29992;Transformer&#26550;&#26500;&#33021;&#22815;&#22312;&#27809;&#26377;&#20351;&#29992;&#26174;&#24335;&#35821;&#35328;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#26368;&#20339;&#30340;&#20215;&#20540;&#39044;&#27979;&#24615;&#33021;&#65292;&#30456;&#20851;&#24615;&#31995;&#25968;&#20026;0.638&#12290;</title><link>http://arxiv.org/abs/2203.07378</link><description>&lt;p&gt;
&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;&#20013;&#30340;Transformer&#26102;&#20195;&#30340;&#40654;&#26126;&#65306;&#24357;&#21512;&#24773;&#24863;&#20215;&#20540;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Dawn of the transformer era in speech emotion recognition: closing the valence gap. (arXiv:2203.07378v3 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.07378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#20960;&#31181;&#39044;&#35757;&#32451;&#21464;&#20307;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;&#39046;&#22495;&#65292;&#20351;&#29992;Transformer&#26550;&#26500;&#33021;&#22815;&#22312;&#27809;&#26377;&#20351;&#29992;&#26174;&#24335;&#35821;&#35328;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#26368;&#20339;&#30340;&#20215;&#20540;&#39044;&#27979;&#24615;&#33021;&#65292;&#30456;&#20851;&#24615;&#31995;&#25968;&#20026;0.638&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;Transformer&#26550;&#26500;&#22312;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#24182;&#22312;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#22312;&#38899;&#39057;&#39046;&#22495;&#65292;&#36825;&#31181;&#26550;&#26500;&#20063;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;(SER)&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#36824;&#27809;&#26377;&#35780;&#20272;&#27169;&#22411;&#22823;&#23567;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#23545;&#19979;&#28216;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#23545;&#27867;&#21270;&#33021;&#21147;&#12289;&#31283;&#20581;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#20851;&#27880;&#26377;&#38480;&#12290;&#26412;&#25991;&#22312;&#20960;&#31181;&#39044;&#35757;&#32451;&#21464;&#20307;&#30340;wav2vec 2.0&#21644;HuBERT&#19978;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#22312;MSP-Podcast&#30340;&#21796;&#36215;&#12289;&#25511;&#21046;&#21644;&#20215;&#20540;&#32500;&#24230;&#19978;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#21516;&#26102;&#20351;&#29992;IEMOCAP&#21644;MOSI&#36827;&#34892;&#36328;&#35821;&#26009;&#24211;&#27867;&#21270;&#27979;&#35797;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#22312;&#19981;&#20351;&#29992;&#26174;&#24335;&#35821;&#35328;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#22312;MSP-Podcast&#19978;&#33719;&#24471;&#20102;&#26368;&#20339;&#30340;&#20215;&#20540;&#39044;&#27979;&#24615;&#33021;&#65292;&#30456;&#20851;&#24615;&#31995;&#25968;&#20026;0.638&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in transformer-based architectures which are pre-trained in self-supervised manner have shown great promise in several machine learning tasks. In the audio domain, such architectures have also been successfully utilised in the field of speech emotion recognition (SER). However, existing works have not evaluated the influence of model size and pre-training data on downstream performance, and have shown limited attention to generalisation, robustness, fairness, and efficiency. The present contribution conducts a thorough analysis of these aspects on several pre-trained variants of wav2vec 2.0 and HuBERT that we fine-tuned on the dimensions arousal, dominance, and valence of MSP-Podcast, while additionally using IEMOCAP and MOSI to test cross-corpus generalisation. To the best of our knowledge, we obtain the top performance for valence prediction without use of explicit linguistic information, with a concordance correlation coefficient (CCC) of .638 on MSP-Podcast. Further
&lt;/p&gt;</description></item><item><title>DeltaCNN&#26159;&#19968;&#31181;&#31232;&#30095;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#36880;&#24103;&#31232;&#30095;&#26356;&#26032;&#65292;&#21152;&#36895;&#35270;&#39057;&#25512;&#29702;&#30340;&#23454;&#36341;&#24212;&#29992;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2203.03996</link><description>&lt;p&gt;
DeltaCNN: &#35270;&#39057;&#20013;&#31232;&#30095;&#24103;&#24046;&#24322;&#30340;&#31471;&#21040;&#31471;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
DeltaCNN: End-to-End CNN Inference of Sparse Frame Differences in Videos. (arXiv:2203.03996v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.03996
&lt;/p&gt;
&lt;p&gt;
DeltaCNN&#26159;&#19968;&#31181;&#31232;&#30095;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#36880;&#24103;&#31232;&#30095;&#26356;&#26032;&#65292;&#21152;&#36895;&#35270;&#39057;&#25512;&#29702;&#30340;&#23454;&#36341;&#24212;&#29992;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35270;&#39057;&#25968;&#25454;&#36827;&#34892;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#38656;&#35201;&#24378;&#22823;&#30340;&#30828;&#20214;&#20197;&#23454;&#29616;&#23454;&#26102;&#22788;&#29702;&#12290;&#32771;&#34385;&#21040;&#36830;&#32493;&#24103;&#20043;&#38388;&#30340;&#22266;&#26377;&#19968;&#33268;&#24615;&#65292;&#35270;&#39057;&#30340;&#22823;&#37096;&#20998;&#21306;&#22495;&#36890;&#24120;&#21464;&#21270;&#24456;&#23567;&#12290;&#36890;&#36807;&#36339;&#36807;&#30456;&#21516;&#30340;&#22270;&#20687;&#21306;&#22495;&#24182;&#25130;&#26029;&#19981;&#37325;&#35201;&#30340;&#20687;&#32032;&#26356;&#26032;&#65292;&#29702;&#35770;&#19978;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35745;&#31639;&#20887;&#20313;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31232;&#30095;&#26356;&#26032;&#38459;&#30861;&#20102;&#35745;&#31639;&#19968;&#33268;&#24615;&#21644;&#20869;&#23384;&#35775;&#38382;&#19968;&#33268;&#24615;&#65292;&#36825;&#20123;&#29702;&#35770;&#19978;&#30340;&#33410;&#30465;&#22312;&#23454;&#36341;&#20013;&#24456;&#38590;&#23454;&#29616;&#65307;&#32780;&#36825;&#20123;&#29305;&#24615;&#23545;&#20110;&#23454;&#38469;&#30828;&#20214;&#30340;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;DeltaCNN&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#36880;&#24103;&#31232;&#30095;&#26356;&#26032;&#65292;&#21152;&#36895;&#35270;&#39057;&#25512;&#29702;&#30340;&#23454;&#36341;&#24212;&#29992;&#12290;&#25105;&#20204;&#20026;&#25152;&#26377;&#20856;&#22411;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23618;&#25552;&#20379;&#20102;&#31232;&#30095;&#23454;&#29616;&#65292;&#24182;&#31471;&#21040;&#31471;&#22320;&#20256;&#25773;&#31232;&#30095;&#29305;&#24449;&#26356;&#26032;-&#26080;&#38656;&#38543;&#26102;&#38388;&#32047;&#31215;&#38169;&#35823;&#12290;DeltaCNN&#36866;&#29992;&#20110;&#25152;&#26377;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#25552;&#20986;&#36825;&#31181;&#26041;&#27861;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural network inference on video data requires powerful hardware for real-time processing. Given the inherent coherence across consecutive frames, large parts of a video typically change little. By skipping identical image regions and truncating insignificant pixel updates, computational redundancy can in theory be reduced significantly. However, these theoretical savings have been difficult to translate into practice, as sparse updates hamper computational consistency and memory access coherence; which are key for efficiency on real hardware. With DeltaCNN, we present a sparse convolutional neural network framework that enables sparse frame-by-frame updates to accelerate video inference in practice. We provide sparse implementations for all typical CNN layers and propagate sparse feature updates end-to-end - without accumulating errors over time. DeltaCNN is applicable to all convolutional neural networks without retraining. To the best of our knowledge, we are the firs
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#30693;&#35782;&#39537;&#21160;&#30340;&#20998;&#23376;&#23398;&#20064;&#65292;&#20174;&#33539;&#24335;&#36716;&#31227;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#24635;&#32467;&#20102;&#20854;&#19981;&#21516;&#33539;&#24335;&#30340;&#20998;&#31867;&#21644;&#26041;&#27861;&#35770;&#65292;&#24182;&#20998;&#26512;&#20102;&#39046;&#22495;&#30693;&#35782;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2202.10587</link><description>&lt;p&gt;
&#30693;&#35782;&#39537;&#21160;&#30340;&#20998;&#23376;&#23398;&#20064;&#65306;&#33539;&#24335;&#36716;&#31227;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Knowledge-informed Molecular Learning: A Survey on Paradigm Transfer. (arXiv:2202.10587v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.10587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#30693;&#35782;&#39537;&#21160;&#30340;&#20998;&#23376;&#23398;&#20064;&#65292;&#20174;&#33539;&#24335;&#36716;&#31227;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#24635;&#32467;&#20102;&#20854;&#19981;&#21516;&#33539;&#24335;&#30340;&#20998;&#31867;&#21644;&#26041;&#27861;&#35770;&#65292;&#24182;&#20998;&#26512;&#20102;&#39046;&#22495;&#30693;&#35782;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#28145;&#24230;&#23398;&#20064;&#65292;&#26174;&#33879;&#25512;&#21160;&#20102;&#29983;&#29289;&#21270;&#23398;&#39046;&#22495;&#20869;&#30340;&#20998;&#23376;&#30740;&#31350;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#31867;&#30740;&#31350;&#30340;&#24314;&#27169;&#20027;&#35201;&#22260;&#32469;&#30528;&#19968;&#20123;&#33539;&#24335;&#23637;&#24320;&#12290;&#20363;&#22914;&#65292;&#39044;&#27979;&#33539;&#24335;&#32463;&#24120;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#31561;&#20219;&#21153;&#12290;&#20026;&#20102;&#22686;&#24378;&#32431;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#29983;&#25104;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#23398;&#32773;&#20204;&#23558;&#29983;&#21270;&#39046;&#22495;&#30340;&#30693;&#35782;&#34701;&#20837;&#21040;&#36825;&#20123;&#20998;&#23376;&#30740;&#31350;&#27169;&#22411;&#20013;&#12290;&#36825;&#31181;&#34701;&#21512;&#24341;&#21457;&#20102;&#33539;&#24335;&#36716;&#31227;&#30340;&#39134;&#36895;&#21457;&#23637;&#65292;&#21363;&#36890;&#36807;&#23558;&#19968;&#20010;&#20998;&#23376;&#23398;&#20064;&#20219;&#21153;&#36716;&#21270;&#20026;&#21478;&#19968;&#20010;&#20219;&#21153;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#36825;&#20123;&#33539;&#24335;&#21576;&#29616;&#20986;&#36880;&#28176;&#36235;&#20110;&#32479;&#19968;&#30340;&#36235;&#21183;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#33539;&#24335;&#36716;&#31227;&#30340;&#35282;&#24230;&#65292;&#23545;&#30693;&#35782;&#39537;&#21160;&#30340;&#20998;&#23376;&#23398;&#20064;&#36827;&#34892;&#20102;&#25991;&#29486;&#32508;&#36848;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#33539;&#24335;&#36827;&#34892;&#20998;&#31867;&#12289;&#23457;&#35270;&#23427;&#20204;&#30340;&#26041;&#27861;&#35770;&#65292;&#24182;&#21078;&#26512;&#20102;&#39046;&#22495;&#30693;&#35782;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning, notably deep learning, has significantly propelled molecular investigations within the biochemical sphere. Traditionally, modeling for such research has centered around a handful of paradigms. For instance, the prediction paradigm is frequently deployed for tasks such as molecular property prediction. To enhance the generation and decipherability of purely data-driven models, scholars have integrated biochemical domain knowledge into these molecular study models. This integration has sparked a surge in paradigm transfer, which is solving one molecular learning task by reformulating it as another one. With the emergence of Large Language Models, these paradigms have demonstrated an escalating trend towards harmonized unification. In this work, we delineate a literature survey focused on knowledge-informed molecular learning from the perspective of paradigm transfer. We classify the paradigms, scrutinize their methodologies, and dissect the contribution of domain knowle
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#20102;Keras&#21644;TensorFlow&#20013;&#30340;&#38745;&#40664;&#38169;&#35823;&#65292;&#36825;&#20123;&#38169;&#35823;&#23545;&#29992;&#25143;&#31243;&#24207;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;&#36890;&#36807;&#23545;&#30456;&#20851;&#38382;&#39064;&#36827;&#34892;&#20998;&#31867;&#21644;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20123;&#38169;&#35823;&#30340;&#26041;&#27861;&#21644;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2112.13314</link><description>&lt;p&gt;
Keras&#21644;TensorFlow&#20013;&#38745;&#40664;&#38169;&#35823;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Silent Bugs in Deep Learning Frameworks: An Empirical Study of Keras and TensorFlow. (arXiv:2112.13314v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.13314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#20102;Keras&#21644;TensorFlow&#20013;&#30340;&#38745;&#40664;&#38169;&#35823;&#65292;&#36825;&#20123;&#38169;&#35823;&#23545;&#29992;&#25143;&#31243;&#24207;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;&#36890;&#36807;&#23545;&#30456;&#20851;&#38382;&#39064;&#36827;&#34892;&#20998;&#31867;&#21644;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20123;&#38169;&#35823;&#30340;&#26041;&#27861;&#21644;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#65292;&#31616;&#21270;&#20102;&#22797;&#26434;&#27169;&#22411;&#30340;&#21019;&#24314;&#21644;&#19982;&#21508;&#31181;&#24212;&#29992;&#30340;&#38598;&#25104;&#65292;&#21363;&#20351;&#23545;&#38750;&#28145;&#24230;&#23398;&#20064;&#19987;&#23478;&#20063;&#26159;&#22914;&#27492;&#12290;&#28982;&#32780;&#65292;&#19982;&#20219;&#20309;&#20854;&#20182;&#31243;&#24207;&#19968;&#26679;&#65292;&#23427;&#20204;&#23481;&#26131;&#20986;&#29616;&#38169;&#35823;&#12290;&#26412;&#25991;&#28041;&#21450;&#19968;&#31181;&#21517;&#20026;&#38745;&#40664;&#38169;&#35823;&#30340;&#38169;&#35823;&#23376;&#31867;&#65306;&#23427;&#20204;&#23548;&#33268;&#38169;&#35823;&#34892;&#20026;&#65292;&#20294;&#19981;&#20250;&#23548;&#33268;&#31995;&#32479;&#23849;&#28291;&#12289;&#25346;&#36215;&#65292;&#20063;&#19981;&#20250;&#21521;&#29992;&#25143;&#26174;&#31034;&#38169;&#35823;&#28040;&#24687;&#12290;&#30001;&#20110;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#21644;&#26694;&#26550;&#30340;&#8220;&#40657;&#30418;&#8221;&#21644;&#38543;&#26426;&#24615;&#36136;&#65288;&#26368;&#32456;&#29992;&#25143;&#26080;&#27861;&#29702;&#35299;&#27169;&#22411;&#22914;&#20309;&#20570;&#20986;&#20915;&#31574;&#65289;&#65292;&#36825;&#26679;&#30340;&#38169;&#35823;&#22312;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#21644;&#26694;&#26550;&#20013;&#26356;&#21152;&#21361;&#38505;&#12290;&#26412;&#25991;&#39318;&#27425;&#23545;Keras&#21644;TensorFlow&#20013;&#30340;&#38745;&#40664;&#38169;&#35823;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#20197;&#21450;&#23427;&#20204;&#23545;&#29992;&#25143;&#31243;&#24207;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20174;TensorFlow&#30340;GitHub&#23384;&#20648;&#24211;&#20013;&#25552;&#21462;&#20102;&#19982;Keras&#30456;&#20851;&#30340;&#24050;&#20851;&#38381;&#38382;&#39064;&#12290;&#22312;&#25105;&#20204;&#25910;&#38598;&#30340;1,168&#20010;&#38382;&#39064;&#20013;&#65292;&#26377;77&#20010;&#26159;&#24433;&#21709;&#29992;&#25143;&#31243;&#24207;&#30340;&#21487;&#37325;&#29616;&#30340;&#38745;&#40664;&#38169;&#35823;&#12290;&#25105;&#20204;&#26681;&#25454;&#23545;&#29992;&#25143;&#31243;&#24207;&#30340;&#24433;&#21709;&#20197;&#21450;&#21457;&#29983;&#38169;&#35823;&#30340;&#32452;&#20214;&#23545;&#36825;&#20123;&#38169;&#35823;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning (DL) frameworks are now widely used, simplifying the creation of complex models as well as their integration to various applications even to non DL experts. However, like any other programs, they are prone to bugs. This paper deals with the subcategory of bugs named silent bugs: they lead to wrong behavior but they do not cause system crashes or hangs, nor show an error message to the user. Such bugs are even more dangerous in DL applications and frameworks due to the "black-box" and stochastic nature of the systems (the end user can not understand how the model makes decisions). This paper presents the first empirical study of Keras and TensorFlow silent bugs, and their impact on users' programs. We extracted closed issues related to Keras from the TensorFlow GitHub repository. Out of the 1,168 issues that we gathered, 77 were reproducible silent bugs affecting users' programs. We categorized the bugs based on the effects on the users' programs and the components where t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#20462;&#27491;&#31574;&#30053;&#65288;DCD&#65289;&#65292;&#29992;&#20110;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#26356;&#26377;&#25928;&#22320;&#23558;&#25945;&#24072;&#27169;&#22411;&#30340;&#25490;&#21517;&#20449;&#24687;&#36716;&#31227;&#21040;&#23398;&#29983;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#20805;&#20998;&#21033;&#29992;&#20102;&#23398;&#29983;&#27169;&#22411;&#30340;&#39044;&#27979;&#35823;&#24046;&#65292;&#36824;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#30340;&#35270;&#35282;&#65292;&#35299;&#20915;&#20102;&#26494;&#24347;&#25490;&#21517;&#33976;&#39311;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2109.03459</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#29992;&#20110;&#25490;&#21517;&#33976;&#39311;&#30340;&#21452;&#37325;&#20462;&#27491;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Dual Correction Strategy for Ranking Distillation in Top-N Recommender System. (arXiv:2109.03459v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.03459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#20462;&#27491;&#31574;&#30053;&#65288;DCD&#65289;&#65292;&#29992;&#20110;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#26356;&#26377;&#25928;&#22320;&#23558;&#25945;&#24072;&#27169;&#22411;&#30340;&#25490;&#21517;&#20449;&#24687;&#36716;&#31227;&#21040;&#23398;&#29983;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#20805;&#20998;&#21033;&#29992;&#20102;&#23398;&#29983;&#27169;&#22411;&#30340;&#39044;&#27979;&#35823;&#24046;&#65292;&#36824;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#30340;&#35270;&#35282;&#65292;&#35299;&#20915;&#20102;&#26494;&#24347;&#25490;&#21517;&#33976;&#39311;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#26159;&#23558;&#35757;&#32451;&#20805;&#20998;&#30340;&#22823;&#27169;&#22411;&#65288;&#25945;&#24072;&#65289;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#23567;&#27169;&#22411;&#65288;&#23398;&#29983;&#65289;&#30340;&#37325;&#35201;&#30740;&#31350;&#39046;&#22495;&#65292;&#23545;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#23454;&#38469;&#37096;&#32626;&#32780;&#35328;&#65292;&#23427;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#26368;&#36817;&#65292;&#26494;&#24347;&#25490;&#21517;&#33976;&#39311;&#65288;RRD&#65289;&#34920;&#26126;&#65292;&#22312;&#25512;&#33616;&#21015;&#34920;&#20013;&#33976;&#39311;&#25490;&#21517;&#20449;&#24687;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#20197;&#19979;&#38480;&#21046;&#65306;1&#65289;&#23427;&#26410;&#20805;&#20998;&#21033;&#29992;&#23398;&#29983;&#27169;&#22411;&#30340;&#39044;&#27979;&#35823;&#24046;&#65292;&#20351;&#24471;&#35757;&#32451;&#25928;&#29575;&#19981;&#39640;&#65307;2&#65289;&#23427;&#21482;&#33976;&#39311;&#29992;&#25143;&#20391;&#30340;&#25490;&#21517;&#20449;&#24687;&#65292;&#22312;&#31232;&#30095;&#30340;&#38544;&#24335;&#21453;&#39304;&#19979;&#25552;&#20379;&#30340;&#35270;&#35282;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#33976;&#39311;&#26041;&#27861;&#65292;&#21363;&#21452;&#37325;&#20462;&#27491;&#31574;&#30053;&#65288;DCD&#65289;&#65292;&#36890;&#36807;&#25945;&#24072;&#27169;&#22411;&#21644;&#23398;&#29983;&#27169;&#22411;&#39044;&#27979;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#20915;&#23450;&#35201;&#33976;&#39311;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation (KD), which transfers the knowledge of a well-trained large model (teacher) to a small model (student), has become an important area of research for practical deployment of recommender systems. Recently, Relaxed Ranking Distillation (RRD) has shown that distilling the ranking information in the recommendation list significantly improves the performance. However, the method still has limitations in that 1) it does not fully utilize the prediction errors of the student model, which makes the training not fully efficient, and 2) it only distills the user-side ranking information, which provides an insufficient view under the sparse implicit feedback. This paper presents Dual Correction strategy for Distillation (DCD), which transfers the ranking information from the teacher model to the student model in a more efficient manner. Most importantly, DCD uses the discrepancy between the teacher model and the student model predictions to decide which knowledge to be disti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#32593;&#32476;&#30340;&#38477;&#38454;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#36870;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#26080;&#26465;&#20214;&#27169;&#25311;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20934;&#30830;&#21305;&#37197;&#27979;&#37327;&#25968;&#25454;&#21644;&#40644;&#37329;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2105.13859</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#32593;&#32476;&#30340;&#38477;&#38454;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#12289;&#25968;&#25454;&#21516;&#21270;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Generative Network-Based Reduced-Order Model for Prediction, Data Assimilation and Uncertainty Quantification. (arXiv:2105.13859v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.13859
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#32593;&#32476;&#30340;&#38477;&#38454;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#36870;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#26080;&#26465;&#20214;&#27169;&#25311;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20934;&#30830;&#21305;&#37197;&#27979;&#37327;&#25968;&#25454;&#21644;&#40644;&#37329;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#29983;&#25104;&#32593;&#32476;&#65288;GN&#65289;&#25972;&#21512;&#21040;&#38477;&#38454;&#27169;&#22411;&#65288;ROM&#65289;&#26694;&#26550;&#20013;&#65292;&#29992;&#20110;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#36870;&#38382;&#39064;&#12290;&#30446;&#26631;&#26159;&#21305;&#37197;&#21487;&#29992;&#30340;&#27979;&#37327;&#25968;&#25454;&#65292;&#24182;&#20272;&#35745;&#25968;&#20540;&#29289;&#29702;&#27169;&#25311;&#30340;&#29366;&#24577;&#21644;&#21442;&#25968;&#30340;&#30456;&#24212;&#19981;&#30830;&#23450;&#24615;&#12290;GN&#20165;&#20351;&#29992;&#31163;&#25955;&#21270;&#30340;PDE&#27169;&#22411;&#30340;&#26080;&#26465;&#20214;&#27169;&#25311;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#40644;&#37329;&#26631;&#20934;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#27969;&#34892;&#30149;&#23398;&#20013;&#30340;&#26102;&#31354;&#38548;&#23460;&#27169;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;GN&#30340;&#38477;&#38454;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#19988;&#20165;&#20351;&#29992;&#23569;&#37327;&#26080;&#26465;&#20214;&#27169;&#25311;&#30340;&#20840;&#38454;&#25968;&#20540;PDE&#27169;&#22411;&#21363;&#21487;&#20934;&#30830;&#21305;&#37197;&#27979;&#37327;&#25968;&#25454;&#21644;&#40644;&#37329;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new method in which a generative network (GN) integrate into a reduced-order model (ROM) framework is used to solve inverse problems for partial differential equations (PDE). The aim is to match available measurements and estimate the corresponding uncertainties associated with the states and parameters of a numerical physical simulation. The GN is trained using only unconditional simulations of the discretized PDE model. We compare the proposed method with the golden standard Markov chain Monte Carlo. We apply the proposed approaches to a spatio-temporal compartmental model in epidemiology. The results show that the proposed GN-based ROM can efficiently quantify uncertainty and accurately match the measurements and the golden standard, using only a few unconditional simulations of the full-order numerical PDE model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#30740;&#31350;&#24322;&#27493;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26377;&#38480;&#26679;&#26412;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#22522;&#20110;Lyapunov&#20998;&#26512;&#24314;&#31435;&#20102;&#24322;&#27493;RL&#31639;&#27861;&#30340;&#22343;&#26041;&#35823;&#24046;&#30028;&#38480;&#12290;&#36890;&#36807;&#23545;n&#27493;TD&#21644;TD&#65288;&#955;&#65289;&#30340;&#25910;&#25947;&#30028;&#38480;&#30340;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#23548;&#25216;&#24039;&#25928;&#29575;&#30340;&#29702;&#35770;&#27934;&#35265;&#12290;</title><link>http://arxiv.org/abs/2102.01567</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#24322;&#27493;Q&#23398;&#20064;&#21644;TD&#23398;&#20064;&#21464;&#31181;&#30340;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#30340;Lyapunov&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Lyapunov Theory for Finite-Sample Guarantees of Asynchronous Q-Learning and TD-Learning Variants. (arXiv:2102.01567v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.01567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#30740;&#31350;&#24322;&#27493;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26377;&#38480;&#26679;&#26412;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#22522;&#20110;Lyapunov&#20998;&#26512;&#24314;&#31435;&#20102;&#24322;&#27493;RL&#31639;&#27861;&#30340;&#22343;&#26041;&#35823;&#24046;&#30028;&#38480;&#12290;&#36890;&#36807;&#23545;n&#27493;TD&#21644;TD&#65288;&#955;&#65289;&#30340;&#25910;&#25947;&#30028;&#38480;&#30340;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#23548;&#25216;&#24039;&#25928;&#29575;&#30340;&#29702;&#35770;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#39318;&#20808;&#23558;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#37325;&#26032;&#34920;&#36848;&#20026;&#35299;&#20915;&#22266;&#23450;&#28857;&#26041;&#31243;&#30340;"Markovian Stochastic Approximation"(SA)&#31639;&#27861;&#65292;&#21457;&#23637;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#30740;&#31350;&#22522;&#20110;&#20540;&#30340;&#24322;&#27493;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26377;&#38480;&#26679;&#26412;&#25910;&#25947;&#20445;&#35777;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;Lyapunov&#20998;&#26512;&#25512;&#23548;&#20986;Markovian SA&#30340;&#22343;&#26041;&#35823;&#24046;&#30028;&#38480;&#65292;&#22522;&#20110;&#27492;&#32467;&#26524;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#24322;&#27493;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;&#22914;Q&#23398;&#20064;&#65292;n&#27493;TD&#65292;TD&#65288;&#955;&#65289;&#21644;&#21253;&#25324;V-trace&#30340;&#31163;&#31574;&#30053;TD&#31639;&#27861;&#65289;&#30340;&#26377;&#38480;&#26679;&#26412;&#22343;&#26041;&#25910;&#25947;&#30028;&#38480;&#12290;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#36890;&#36807;&#20998;&#26512;n&#27493;TD&#21644;TD&#65288;&#955;&#65289;&#30340;&#25910;&#25947;&#30028;&#38480;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#23548;&#25216;&#24039;&#25928;&#29575;&#65288;&#21363;&#20559;&#24046;-&#26041;&#24046;&#26435;&#34913;&#65289;&#30340;&#29702;&#35770;&#27934;&#35265;&#65292;&#36825;&#26159;(Sutton, 1999)&#20013;&#39318;&#27425;&#25552;&#20986;&#30340;&#19968;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper develops an unified framework to study finite-sample convergence guarantees of a large class of value-based asynchronous reinforcement learning (RL) algorithms. We do this by first reformulating the RL algorithms as \textit{Markovian Stochastic Approximation} (SA) algorithms to solve fixed-point equations. We then develop a Lyapunov analysis and derive mean-square error bounds on the convergence of the Markovian SA. Based on this result, we establish finite-sample mean-square convergence bounds for asynchronous RL algorithms such as $Q$-learning, $n$-step TD, TD$(\lambda)$, and off-policy TD algorithms including V-trace. As a by-product, by analyzing the convergence bounds of $n$-step TD and TD$(\lambda)$, we provide theoretical insights into the bias-variance trade-off, i.e., efficiency of bootstrapping in RL. This was first posed as an open problem in (Sutton, 1999).
&lt;/p&gt;</description></item><item><title>Het-node2vec&#26159;&#19968;&#20010;&#31639;&#27861;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#24322;&#26500;&#22810;&#22270;&#19978;&#36827;&#34892;&#20108;&#38454;&#38543;&#26426;&#28216;&#36208;&#37319;&#26679;&#65292;&#33021;&#22815;&#25429;&#33719;&#22270;&#30340;&#32467;&#26500;&#29305;&#24449;&#21644;&#19981;&#21516;&#31867;&#22411;&#33410;&#28857;&#36793;&#30340;&#35821;&#20041;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#23545;&#24322;&#26500;&#22270;&#30340;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2101.01425</link><description>&lt;p&gt;
Het-node2vec&#65306;&#24322;&#26500;&#22810;&#22270;&#23884;&#20837;&#30340;&#20108;&#38454;&#38543;&#26426;&#28216;&#36208;&#37319;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Het-node2vec: second order random walk sampling for heterogeneous multigraphs embedding. (arXiv:2101.01425v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.01425
&lt;/p&gt;
&lt;p&gt;
Het-node2vec&#26159;&#19968;&#20010;&#31639;&#27861;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#24322;&#26500;&#22810;&#22270;&#19978;&#36827;&#34892;&#20108;&#38454;&#38543;&#26426;&#28216;&#36208;&#37319;&#26679;&#65292;&#33021;&#22815;&#25429;&#33719;&#22270;&#30340;&#32467;&#26500;&#29305;&#24449;&#21644;&#19981;&#21516;&#31867;&#22411;&#33410;&#28857;&#36793;&#30340;&#35821;&#20041;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#23545;&#24322;&#26500;&#22270;&#30340;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#65292;&#20026;&#24322;&#26500;&#22270;&#24320;&#21457;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#26159;&#22522;&#30784;&#24615;&#30340;&#65292;&#22240;&#20026;&#22312;&#22810;&#20010;&#19978;&#19979;&#25991;&#20013;&#65292;&#22270;&#30001;&#19981;&#21516;&#31867;&#22411;&#30340;&#33410;&#28857;&#21644;&#36793;&#25152;&#29305;&#24449;&#21270;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31639;&#27861;&#26694;&#26550;&#65288;Het-node2vec&#65289;&#65292;&#23558;&#21407;&#22987;&#30340;node2vec&#33410;&#28857;&#37051;&#22495;&#37319;&#26679;&#26041;&#27861;&#25193;&#23637;&#21040;&#20102;&#24322;&#26500;&#22810;&#22270;&#19978;&#12290;&#25152;&#24471;&#21040;&#30340;&#38543;&#26426;&#28216;&#36208;&#26679;&#26412;&#25429;&#33719;&#20102;&#22270;&#30340;&#32467;&#26500;&#29305;&#24449;&#20197;&#21450;&#19981;&#21516;&#31867;&#22411;&#30340;&#33410;&#28857;&#21644;&#36793;&#30340;&#35821;&#20041;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#20197;&#32858;&#28966;&#20110;&#29305;&#23450;&#30340;&#33410;&#28857;&#25110;&#36793;&#31867;&#22411;&#65292;&#20026;&#25152;&#30740;&#31350;&#30340;&#39044;&#27979;&#38382;&#39064;&#20013;&#26377;&#20852;&#36259;&#30340;&#23569;&#25968;&#33410;&#28857;/&#36793;&#31867;&#22411;&#25552;&#20379;&#20934;&#30830;&#30340;&#34920;&#31034;&#12290;&#36825;&#20123;&#20016;&#23500;&#32780;&#26377;&#38024;&#23545;&#24615;&#30340;&#34920;&#31034;&#21487;&#20197;&#22686;&#24378;&#23545;&#24322;&#26500;&#22270;&#30340;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of Graph Representation Learning methods for heterogeneous graphs is fundamental in several real-world applications, since in several contexts graphs are characterized by different types of nodes and edges. We introduce a an algorithmic framework (Het-node2vec) that extends the original node2vec node-neighborhood sampling method to heterogeneous multigraphs. The resulting random walk samples capture both the structural characteristics of the graph and the semantics of the different types of nodes and edges. The proposed algorithms can focus their attention on specific node or edge types, allowing accurate representations also for underrepresented types of nodes/edges that are of interest for the prediction problem under investigation. These rich and well-focused representations can boost unsupervised and supervised learning on heterogeneous graphs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Lyapunov&#20248;&#21270;&#38382;&#39064;&#30340;&#20844;&#24179;&#20445;&#35777;&#30340;&#23458;&#25143;&#36873;&#25321;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#36890;&#36807;&#20272;&#35745;&#27599;&#20010;&#23458;&#25143;&#31471;&#19982;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#27169;&#22411;&#20132;&#25442;&#26102;&#38388;&#65292;&#25552;&#39640;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2011.01783</link><description>&lt;p&gt;
&#19968;&#20010;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#25928;&#29575;&#30340;&#20855;&#26377;&#20844;&#24179;&#20445;&#35777;&#30340;&#23458;&#25143;&#36873;&#25321;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
An Efficiency-boosting Client Selection Scheme for Federated Learning with Fairness Guarantee. (arXiv:2011.01783v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.01783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Lyapunov&#20248;&#21270;&#38382;&#39064;&#30340;&#20844;&#24179;&#20445;&#35777;&#30340;&#23458;&#25143;&#36873;&#25321;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#36890;&#36807;&#20272;&#35745;&#27599;&#20010;&#23458;&#25143;&#31471;&#19982;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#27169;&#22411;&#20132;&#25442;&#26102;&#38388;&#65292;&#25552;&#39640;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38598;&#20013;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#35757;&#32451;&#20013;&#28508;&#22312;&#30340;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#24341;&#36215;&#20102;&#20844;&#20247;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#19968;&#31181;&#21517;&#20026;&#32852;&#37030;&#23398;&#20064;&#30340;&#24182;&#34892;&#21644;&#20998;&#24067;&#24335;&#35745;&#31639;&#26041;&#26696;&#24050;&#32463;&#20986;&#29616;&#65292;&#36890;&#36807;&#20801;&#35768;&#23458;&#25143;&#22312;&#26412;&#22320;&#25191;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#26080;&#38656;&#19978;&#20256;&#20010;&#20154;&#25935;&#24863;&#25968;&#25454;&#26469;&#24212;&#23545;&#38544;&#31169;&#38382;&#39064;&#12290;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#23458;&#25143;&#31471;&#30340;&#25968;&#37327;&#21487;&#33021;&#38750;&#24120;&#22823;&#65292;&#20294;&#29992;&#20110;&#27169;&#22411;&#20998;&#21457;&#21644;&#37325;&#26032;&#19978;&#20256;&#30340;&#24102;&#23485;&#38750;&#24120;&#26377;&#38480;&#65292;&#22240;&#27492;&#21512;&#29702;&#22320;&#21482;&#35753;&#37096;&#20998;&#24535;&#24895;&#32773;&#21442;&#19982;&#35757;&#32451;&#36807;&#31243;&#12290;&#23458;&#25143;&#36873;&#25321;&#31574;&#30053;&#22312;&#32852;&#37030;&#23398;&#20064;&#36807;&#31243;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#28041;&#21450;&#35757;&#32451;&#25928;&#29575;&#12289;&#26368;&#32456;&#27169;&#22411;&#36136;&#37327;&#20197;&#21450;&#20844;&#24179;&#24615;&#12290;&#26412;&#25991;&#23558;&#20844;&#24179;&#20445;&#35777;&#30340;&#23458;&#25143;&#36873;&#25321;&#24314;&#27169;&#20026;&#19968;&#20010;Lyapunov&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;C2MAB&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#27599;&#20010;&#23458;&#25143;&#31471;&#19982;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#27169;&#22411;&#20132;&#25442;&#26102;&#38388;&#65292;&#22522;&#20110;&#27492;&#36827;&#34892;&#23458;&#25143;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
The issue of potential privacy leakage during centralized AI's model training has drawn intensive concern from the public. A Parallel and Distributed Computing (or PDC) scheme, termed Federated Learning (FL), has emerged as a new paradigm to cope with the privacy issue by allowing clients to perform model training locally, without the necessity to upload their personal sensitive data. In FL, the number of clients could be sufficiently large, but the bandwidth available for model distribution and re-upload is quite limited, making it sensible to only involve part of the volunteers to participate in the training process. The client selection policy is critical to an FL process in terms of training efficiency, the final model's quality as well as fairness. In this paper, we will model the fairness guaranteed client selection as a Lyapunov optimization problem and then a C2MAB-based method is proposed for estimation of the model exchange time between each client and the server, based on wh
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#30340;&#24212;&#29992;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#25361;&#25112;&#65292;&#21253;&#25324;&#27010;&#24565;&#28418;&#31227;&#12289;&#28436;&#21270;&#12289;&#24310;&#36831;&#26631;&#31614;&#21644;&#23545;&#25239;&#24615;&#26426;&#22120;&#23398;&#20064;&#23545;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#24433;&#21709;&#65292;&#24378;&#35843;&#20102;&#27491;&#30830;&#26500;&#24314;&#21644;&#35780;&#20272;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23433;&#20840;&#35299;&#20915;&#26041;&#26696;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2010.16045</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;&#19981;&#65289;&#23433;&#20840;&#24615;&#65306;&#19968;&#31995;&#21015;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (In) Security: A Stream of Problems. (arXiv:2010.16045v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.16045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#30340;&#24212;&#29992;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#25361;&#25112;&#65292;&#21253;&#25324;&#27010;&#24565;&#28418;&#31227;&#12289;&#28436;&#21270;&#12289;&#24310;&#36831;&#26631;&#31614;&#21644;&#23545;&#25239;&#24615;&#26426;&#22120;&#23398;&#20064;&#23545;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#24433;&#21709;&#65292;&#24378;&#35843;&#20102;&#27491;&#30830;&#26500;&#24314;&#21644;&#35780;&#20272;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23433;&#20840;&#35299;&#20915;&#26041;&#26696;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#65292;&#24182;&#34987;&#35748;&#20026;&#26159;&#35299;&#20915;&#35813;&#39046;&#22495;&#35768;&#22810;&#24320;&#25918;&#38382;&#39064;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23433;&#20840;&#39046;&#22495;&#38754;&#20020;&#30340;&#25361;&#25112;&#22312;&#20854;&#20182;&#39046;&#22495;&#21487;&#33021;&#19981;&#20250;&#20986;&#29616;&#65292;&#22240;&#27492;&#24456;&#38590;&#35780;&#20272;&#25152;&#20135;&#29983;&#35299;&#20915;&#26041;&#26696;&#30340;&#22909;&#22351;&#12290;&#20854;&#20013;&#19968;&#39033;&#25361;&#25112;&#26159;&#27010;&#24565;&#28418;&#31227;&#65292;&#23427;&#21152;&#21095;&#20102;&#25915;&#20987;&#32773;&#21644;&#38450;&#24481;&#32773;&#20043;&#38388;&#30340;&#23545;&#25239;&#65306;&#24694;&#24847;&#34892;&#20026;&#32773;&#22987;&#32456;&#21487;&#20197;&#21019;&#24314;&#26032;&#30340;&#23041;&#32961;&#26469;&#20811;&#26381;&#38450;&#24481;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#26576;&#20123;&#26041;&#27861;&#20013;&#21487;&#33021;&#24182;&#26410;&#32771;&#34385;&#36825;&#20123;&#23041;&#32961;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#22914;&#20309;&#27491;&#30830;&#26500;&#24314;&#21644;&#35780;&#20272;&#22522;&#20110;ML&#30340;&#23433;&#20840;&#35299;&#20915;&#26041;&#26696;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#12289;&#35814;&#36848;&#21644;&#35752;&#35770;&#20102;&#22312;&#23558;ML&#25216;&#26415;&#27491;&#30830;&#24212;&#29992;&#20110;&#32593;&#32476;&#23433;&#20840;&#25968;&#25454;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#27010;&#24565;&#28418;&#31227;&#12289;&#28436;&#21270;&#12289;&#24310;&#36831;&#26631;&#31614;&#21644;&#23545;&#25239;&#24615;ML&#23545;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#19982;&#25968;&#25454;&#25910;&#38598;&#30456;&#20851;&#30340;&#38382;&#39064;&#22914;&#20309;&#24433;&#21709;&#32467;&#26524;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) has been widely applied to cybersecurity and is considered state-of-the-art for solving many of the open issues in that field. However, it is very difficult to evaluate how good the produced solutions are, since the challenges faced in security may not appear in other areas. One of these challenges is the concept drift, which increases the existing arms race between attackers and defenders: malicious actors can always create novel threats to overcome the defense solutions, which may not consider them in some approaches. Due to this, it is essential to know how to properly build and evaluate an ML-based security solution. In this paper, we identify, detail, and discuss the main challenges in the correct application of ML techniques to cybersecurity data. We evaluate how concept drift, evolution, delayed labels, and adversarial ML impact the existing solutions. Moreover, we address how issues related to data collection affect the quality of the results presented in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21463;&#38480;Laplacian&#22270;&#27169;&#22411;&#19979;&#23398;&#20064;&#31232;&#30095;&#22270;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#32463;&#20856;&#30340;$\ell_1$-&#33539;&#25968;&#27491;&#21017;&#21270;&#26080;&#27861;&#26377;&#25928;&#23454;&#29616;&#31232;&#30095;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#20984;&#31232;&#30095;&#24809;&#32602;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2006.14925</link><description>&lt;p&gt;
$\ell_1$-&#33539;&#25968;&#26159;&#21542;&#33021;&#22815;&#22312;&#21463;&#38480;Laplacian&#22270;&#27169;&#22411;&#19979;&#23398;&#20064;&#31232;&#30095;&#22270;&#24418;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does the $\ell_1$-norm Learn a Sparse Graph under Laplacian Constrained Graphical Models?. (arXiv:2006.14925v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.14925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21463;&#38480;Laplacian&#22270;&#27169;&#22411;&#19979;&#23398;&#20064;&#31232;&#30095;&#22270;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#32463;&#20856;&#30340;$\ell_1$-&#33539;&#25968;&#27491;&#21017;&#21270;&#26080;&#27861;&#26377;&#25928;&#23454;&#29616;&#31232;&#30095;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#20984;&#31232;&#30095;&#24809;&#32602;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#21463;&#38480;Laplacian&#39640;&#26031;&#22270;&#27169;&#22411;&#19979;&#23398;&#20064;&#31232;&#30095;&#22270;&#30340;&#38382;&#39064;&#12290;&#35813;&#38382;&#39064;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#25289;&#26222;&#25289;&#26031;&#32422;&#26463;&#19979;&#30340;&#31934;&#24230;&#30697;&#38453;&#30340;&#24809;&#32602;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#12290;&#19982;&#32463;&#20856;&#30340;&#22270;&#24418;&#22871;&#32034;&#38382;&#39064;&#31867;&#20284;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#20102;$\ell_1$-&#33539;&#25968;&#27491;&#21017;&#21270;&#26469;&#20419;&#36827;&#22312;&#25289;&#26222;&#25289;&#26031;&#32422;&#26463;&#31934;&#24230;&#30697;&#38453;&#20272;&#35745;&#20013;&#30340;&#31232;&#30095;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#24191;&#27867;&#24212;&#29992;&#30340;$\ell_1$-&#33539;&#25968;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#26080;&#27861;&#26377;&#25928;&#22320;&#23454;&#29616;&#31232;&#30095;&#35299;&#12290;&#36890;&#36807;&#32463;&#39564;&#35777;&#25454;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#38750;&#38646;&#22270;&#26435;&#37325;&#30340;&#25968;&#37327;&#38543;&#30528;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;&#20174;&#29702;&#35770;&#19978;&#26469;&#30475;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36739;&#22823;&#30340;&#27491;&#21017;&#21270;&#21442;&#25968;&#23558;&#24341;&#21457;&#19968;&#20010;&#24847;&#22806;&#30340;&#23436;&#20840;&#22270;&#65292;&#21363;&#27599;&#23545;&#39030;&#28857;&#20043;&#38388;&#37117;&#29992;&#36793;&#36830;&#25509;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#38750;&#20984;&#31232;&#30095;&#24809;&#32602;&#65292;&#24182;&#36890;&#36807;&#27714;&#35299;&#19968;&#31995;&#21015;&#21152;&#26435;$\ell_1$-&#33539;&#25968;&#24471;&#21040;&#20102;&#19968;&#20010;&#26032;&#30340;&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning a sparse graph under the Laplacian constrained Gaussian graphical models. This problem can be formulated as a penalized maximum likelihood estimation of the Laplacian constrained precision matrix. Like in the classical graphical lasso problem, recent works made use of the $\ell_1$-norm regularization with the goal of promoting sparsity in Laplacian constrained precision matrix estimation. However, we find that the widely used $\ell_1$-norm is not effective in imposing a sparse solution in this problem. Through empirical evidence, we observe that the number of nonzero graph weights grows with the increase of the regularization parameter. From a theoretical perspective, we prove that a large regularization parameter will surprisingly lead to a complete graph, i.e., every pair of vertices is connected by an edge. To address this issue, we introduce the nonconvex sparsity penalty, and propose a new estimator by solving a sequence of weighted $\ell_1$-nor
&lt;/p&gt;</description></item></channel></rss>