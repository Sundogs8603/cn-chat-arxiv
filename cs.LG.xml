<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>MM-Vet&#26159;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22797;&#26434;&#20219;&#21153;&#19978;&#30340;&#32508;&#21512;&#33021;&#21147;&#12290;&#35813;&#26631;&#20934;&#35299;&#20915;&#20102;&#22914;&#20309;&#32467;&#26500;&#21270;&#21644;&#35780;&#20272;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#12289;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#38382;&#39064;&#21644;&#22238;&#31572;&#31867;&#22411;&#30340;&#35780;&#20272;&#25351;&#26631;&#20197;&#21450;&#22914;&#20309;&#25552;&#20379;&#27169;&#22411;&#27934;&#23519;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;&#26680;&#24515;&#35270;&#35273;-&#35821;&#35328;&#33021;&#21147;&#65292;MM-Vet&#23637;&#31034;&#20102;&#26377;&#36259;&#30340;&#33021;&#21147;&#21644;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02490</link><description>&lt;p&gt;
MM-Vet: &#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#32508;&#21512;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities. (arXiv:2308.02490v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02490
&lt;/p&gt;
&lt;p&gt;
MM-Vet&#26159;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22797;&#26434;&#20219;&#21153;&#19978;&#30340;&#32508;&#21512;&#33021;&#21147;&#12290;&#35813;&#26631;&#20934;&#35299;&#20915;&#20102;&#22914;&#20309;&#32467;&#26500;&#21270;&#21644;&#35780;&#20272;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#12289;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#38382;&#39064;&#21644;&#22238;&#31572;&#31867;&#22411;&#30340;&#35780;&#20272;&#25351;&#26631;&#20197;&#21450;&#22914;&#20309;&#25552;&#20379;&#27169;&#22411;&#27934;&#23519;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;&#26680;&#24515;&#35270;&#35273;-&#35821;&#35328;&#33021;&#21147;&#65292;MM-Vet&#23637;&#31034;&#20102;&#26377;&#36259;&#30340;&#33021;&#21147;&#21644;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MM-Vet&#65292;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#65292;&#29992;&#20110;&#26816;&#26597;&#22312;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMM&#65289;&#30340;&#34920;&#29616;&#12290;&#26368;&#36817;&#30340;LMM&#23637;&#31034;&#20102;&#21508;&#31181;&#26377;&#36259;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#35299;&#20915;&#20070;&#20889;&#22312;&#40657;&#26495;&#19978;&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#25512;&#29702;&#26032;&#38395;&#22270;&#29255;&#20013;&#30340;&#20107;&#20214;&#21644;&#21517;&#20154;&#65292;&#20197;&#21450;&#35299;&#37322;&#35270;&#35273;&#31505;&#35805;&#12290;&#24555;&#36895;&#30340;&#27169;&#22411;&#36827;&#27493;&#32473;&#35780;&#20272;&#26631;&#20934;&#30340;&#24320;&#21457;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#38382;&#39064;&#21253;&#25324;&#65306;&#65288;1&#65289;&#22914;&#20309;&#31995;&#32479;&#22320;&#26500;&#24314;&#21644;&#35780;&#20272;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#65307;&#65288;2&#65289;&#22914;&#20309;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#38382;&#39064;&#21644;&#22238;&#31572;&#30340;&#35780;&#20272;&#25351;&#26631;&#65307;&#65288;3&#65289;&#22914;&#20309;&#32473;&#20986;&#36229;&#20986;&#31616;&#21333;&#24615;&#33021;&#25490;&#21517;&#30340;&#27169;&#22411;&#27934;&#23519;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MM-Vet&#65292;&#22522;&#20110;&#36825;&#26679;&#19968;&#20010;&#27934;&#23519;&#65306;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26377;&#36259;&#33021;&#21147;&#36890;&#24120;&#36890;&#36807;&#19968;&#31181;&#36890;&#25165;&#27169;&#22411;&#33021;&#22815;&#25972;&#21512;&#19981;&#21516;&#30340;&#26680;&#24515;&#35270;&#35273;-&#35821;&#35328;&#65288;VL&#65289;&#33021;&#21147;&#26469;&#23454;&#29616;&#12290;MM-Vet&#23450;&#20041;&#20102;6&#20010;&#26680;&#24515;VL&#33021;&#21147;&#65292;&#24182;&#26816;&#26597;&#20102;&#20174;&#36825;&#20123;&#33021;&#21147;&#32452;&#21512;&#20013;&#24471;&#20986;&#30340;16&#31181;&#26377;&#36259;&#30340;&#25972;&#21512;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose MM-Vet, an evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various intriguing abilities, such as solving math problems written on the blackboard, reasoning about events and celebrities in news images, and explaining visual jokes. Rapid model advancements pose challenges to evaluation benchmark development. Problems include: (1) How to systematically structure and evaluate the complicated multimodal tasks; (2) How to design evaluation metrics that work well across question and answer types; and (3) How to give model insights beyond a simple performance ranking. To this end, we present MM-Vet, designed based on the insight that the intriguing ability to solve complicated tasks is often achieved by a generalist model being able to integrate different core vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and examines the 16 integrations of interest derived from the capability combin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#33410;&#28857;&#32423;&#22402;&#30452;&#32852;&#37030;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26631;&#31614;&#25512;&#26029;&#25915;&#20987;&#65292;&#21033;&#29992;&#38646;&#32972;&#26223;&#30693;&#35782;&#31574;&#30053;&#26469;&#23454;&#29616;&#25915;&#20987;&#65292;&#24182;&#25581;&#31034;&#20102;&#35813;&#39046;&#22495;&#20869;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.02465</link><description>&lt;p&gt;
BlindSage&#65306;&#38024;&#23545;&#33410;&#28857;&#32423;&#22402;&#30452;&#32852;&#37030;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26631;&#31614;&#25512;&#26029;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
BlindSage: Label Inference Attacks against Node-level Vertical Federated Graph Neural Networks. (arXiv:2308.02465v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#33410;&#28857;&#32423;&#22402;&#30452;&#32852;&#37030;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26631;&#31614;&#25512;&#26029;&#25915;&#20987;&#65292;&#21033;&#29992;&#38646;&#32972;&#26223;&#30693;&#35782;&#31574;&#30053;&#26469;&#23454;&#29616;&#25915;&#20987;&#65292;&#24182;&#25581;&#31034;&#20102;&#35813;&#39046;&#22495;&#20869;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#20445;&#25345;&#28041;&#21450;&#24037;&#20316;&#26041;&#30340;&#21407;&#22987;&#25968;&#25454;&#31169;&#23494;&#24615;&#65292;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21327;&#20316;&#35757;&#32451;&#12290;&#20854;&#20027;&#35201;&#30446;&#26631;&#20043;&#19968;&#26159;&#25552;&#39640;&#27169;&#22411;&#30340;&#38544;&#31169;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65288;VFL&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#36328;&#22495;&#35774;&#32622;&#65292;&#20854;&#20013;&#23569;&#25968;&#21442;&#19982;&#26041;&#22312;&#19981;&#20849;&#20139;&#30456;&#21516;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#20849;&#21516;&#35757;&#32451;&#27169;&#22411;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20998;&#31867;&#26631;&#31614;&#36890;&#24120;&#34987;&#35270;&#20026;&#20165;&#30001;&#19968;&#20010;&#65288;&#20027;&#21160;&#65289;&#21442;&#19982;&#26041;&#29420;&#21344;&#25345;&#26377;&#30340;&#25935;&#24863;&#20449;&#24687;&#65292;&#32780;&#20854;&#20182;&#65288;&#34987;&#21160;&#65289;&#21442;&#19982;&#26041;&#20165;&#20351;&#29992;&#20854;&#26412;&#22320;&#20449;&#24687;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;VFL&#30340;&#37325;&#35201;&#32570;&#38519;&#65292;&#21487;&#33021;&#23548;&#33268;&#22312;&#25915;&#20987;&#32773;&#20855;&#26377;&#26576;&#20123;&#65292;&#29978;&#33267;&#26377;&#38480;&#30340;&#26631;&#31614;&#19982;&#25968;&#25454;&#20851;&#31995;&#30340;&#32972;&#26223;&#30693;&#35782;&#30340;&#20551;&#35774;&#19979;&#21457;&#29983;&#26631;&#31614;&#25512;&#26029;&#25915;&#20987;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26159;&#39318;&#27425;&#65288;&#25454;&#25105;&#20204;&#25152;&#30693;&#65289;&#20351;&#29992;&#38646;&#32972;&#26223;&#30693;&#35782;&#31574;&#30053;&#30740;&#31350;VFL&#19978;&#30340;&#26631;&#31614;&#25512;&#26029;&#25915;&#20987;&#12290;&#20026;&#20102;&#20855;&#20307;&#38416;&#36848;&#25105;&#20204;&#30340;&#25552;&#26696;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;Grap&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning enables collaborative training of machine learning models by keeping the raw data of the involved workers private. One of its main objectives is to improve the models' privacy, security, and scalability. Vertical Federated Learning (VFL) offers an efficient cross-silo setting where a few parties collaboratively train a model without sharing the same features. In such a scenario, classification labels are commonly considered sensitive information held exclusively by one (active) party, while other (passive) parties use only their local information. Recent works have uncovered important flaws of VFL, leading to possible label inference attacks under the assumption that the attacker has some, even limited, background knowledge on the relation between labels and data. In this work, we are the first (to the best of our knowledge) to investigate label inference attacks on VFL using a zero-background knowledge strategy. To concretely formulate our proposal, we focus on Grap
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38543;&#26426;&#24615;&#30340;&#20648;&#22791;&#35745;&#31639;&#65292;RNNs&#21487;&#20197;&#36890;&#29992;&#36924;&#36817;&#32447;&#24615;&#26102;&#19981;&#21464;&#31995;&#32479;&#65292;&#36825;&#19968;&#35266;&#23519;&#21040;&#30340;&#24615;&#33021;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20102;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2308.02464</link><description>&lt;p&gt;
&#36890;&#36807;RNNs&#23454;&#29616;&#32447;&#24615;&#26102;&#19981;&#21464;&#31995;&#32479;&#30340;&#36890;&#29992;&#36924;&#36817;&#65306;&#38543;&#26426;&#24615;&#22312;&#20648;&#22791;&#35745;&#31639;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Universal Approximation of Linear Time-Invariant (LTI) Systems through RNNs: Power of Randomness in Reservoir Computing. (arXiv:2308.02464v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02464
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#24615;&#30340;&#20648;&#22791;&#35745;&#31639;&#65292;RNNs&#21487;&#20197;&#36890;&#29992;&#36924;&#36817;&#32447;&#24615;&#26102;&#19981;&#21464;&#31995;&#32479;&#65292;&#36825;&#19968;&#35266;&#23519;&#21040;&#30340;&#24615;&#33021;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20102;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(RNNs)&#20197;&#30456;&#23545;&#28201;&#21644;&#21644;&#26222;&#36866;&#30340;&#26465;&#20214;&#34987;&#35748;&#20026;&#26159;&#21160;&#24577;&#31995;&#32479;&#30340;&#36890;&#29992;&#36817;&#20284;&#22120;&#65292;&#20351;&#20854;&#25104;&#20026;&#22788;&#29702;&#26102;&#38388;&#20449;&#24687;&#30340;&#33391;&#22909;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;RNNs&#36890;&#24120;&#21463;&#21040;&#26631;&#20934;RNN&#35757;&#32451;&#20013;&#26799;&#24230;&#28040;&#22833;&#21644;&#29190;&#28856;&#30340;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#20648;&#22791;&#35745;&#31639;(RC)&#26159;&#19968;&#31181;&#29305;&#27530;&#30340;RNN&#65292;&#20854;&#20013;&#30340;&#24490;&#29615;&#26435;&#37325;&#26159;&#38543;&#26426;&#21270;&#24182;&#30041;&#22312;&#26410;&#32463;&#35757;&#32451;&#29366;&#24577;&#65292;&#23427;&#34987;&#24341;&#20837;&#29992;&#20110;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#22312;&#35832;&#22914;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26080;&#32447;&#36890;&#20449;&#31561;&#39046;&#22495;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#23454;&#35777;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#35757;&#32451;&#26679;&#26412;&#26497;&#20854;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#25903;&#25345;&#36825;&#31181;&#35266;&#23519;&#21040;&#30340;&#24615;&#33021;&#30340;&#29702;&#35770;&#22522;&#30784;&#24182;&#26410;&#20197;&#30456;&#21516;&#30340;&#36895;&#24230;&#23436;&#20840;&#21457;&#23637;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;RNNs&#21487;&#20197;&#25552;&#20379;&#32447;&#24615;&#26102;&#19981;&#21464;(LTI)&#31995;&#32479;&#30340;&#36890;&#29992;&#36924;&#36817;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;RC&#21487;&#20197;&#23545;&#19968;&#33324;LTI&#31995;&#32479;&#36827;&#34892;&#20840;&#38754;&#36924;&#36817;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#20449;&#21495;&#22788;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recurrent neural networks (RNNs) are known to be universal approximators of dynamic systems under fairly mild and general assumptions, making them good tools to process temporal information. However, RNNs usually suffer from the issues of vanishing and exploding gradients in the standard RNN training. Reservoir computing (RC), a special RNN where the recurrent weights are randomized and left untrained, has been introduced to overcome these issues and has demonstrated superior empirical performance in fields as diverse as natural language processing and wireless communications especially in scenarios where training samples are extremely limited. On the contrary, the theoretical grounding to support this observed performance has not been fully developed at the same pace. In this work, we show that RNNs can provide universal approximation of linear time-invariant (LTI) systems. Specifically, we show that RC can universally approximate a general LTI system. We present a clear signal proces
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#36816;&#31639;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;MOOSE&#26694;&#26550;&#20013;&#24320;&#21457;&#20102;&#19968;&#20010;&#24555;&#36895;&#20934;&#30830;&#30340;&#20943;&#38454;&#27169;&#22411;&#65292;&#29992;&#20110;&#22686;&#26448;&#21046;&#36896;&#20013;&#30340;&#25511;&#21046;&#21644;&#20248;&#21270;&#36807;&#31243;&#65292;&#36890;&#36807;&#25913;&#21464;&#36807;&#31243;&#21464;&#37327;&#26469;&#23398;&#20064;&#24494;&#20998;&#26041;&#31243;&#65292;&#20351;&#29992;Fourier&#31070;&#32463;&#36816;&#31639;&#22120;&#21644;&#28145;&#24230;&#36816;&#31639;&#22120;&#32593;&#32476;&#24320;&#21457;&#20102;&#26102;&#38388;&#30456;&#20851;&#21709;&#24212;&#30340;&#20943;&#38454;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.02462</link><description>&lt;p&gt;
&#22522;&#20110;MOOSE&#26694;&#26550;&#30340;&#24555;&#36895;&#20934;&#30830;&#30340;&#20943;&#38454;&#24314;&#27169;&#22312;&#22686;&#26448;&#21046;&#36896;&#20013;&#30340;&#36816;&#31639;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fast and Accurate Reduced-Order Modeling of a MOOSE-based Additive Manufacturing Model with Operator Learning. (arXiv:2308.02462v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#36816;&#31639;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;MOOSE&#26694;&#26550;&#20013;&#24320;&#21457;&#20102;&#19968;&#20010;&#24555;&#36895;&#20934;&#30830;&#30340;&#20943;&#38454;&#27169;&#22411;&#65292;&#29992;&#20110;&#22686;&#26448;&#21046;&#36896;&#20013;&#30340;&#25511;&#21046;&#21644;&#20248;&#21270;&#36807;&#31243;&#65292;&#36890;&#36807;&#25913;&#21464;&#36807;&#31243;&#21464;&#37327;&#26469;&#23398;&#20064;&#24494;&#20998;&#26041;&#31243;&#65292;&#20351;&#29992;Fourier&#31070;&#32463;&#36816;&#31639;&#22120;&#21644;&#28145;&#24230;&#36816;&#31639;&#22120;&#32593;&#32476;&#24320;&#21457;&#20102;&#26102;&#38388;&#30456;&#20851;&#21709;&#24212;&#30340;&#20943;&#38454;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#26448;&#21046;&#36896;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#22312;&#36816;&#34892;&#26102;&#36890;&#36807;&#35843;&#25972;&#21046;&#36896;&#36807;&#31243;&#21442;&#25968;&#26469;&#36798;&#21040;&#29305;&#23450;&#30340;&#26448;&#26009;&#23646;&#24615;&#12290;&#36825;&#31181;&#35843;&#25972;&#24448;&#24448;&#22686;&#21152;&#20102;&#29992;&#20110;&#22686;&#26448;&#21046;&#36896;&#20013;&#30340;&#29616;&#26377;&#27169;&#25311;&#24037;&#20855;&#30340;&#35745;&#31639;&#36127;&#33655;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#20026;&#22312;&#22810;&#29289;&#29702;&#38754;&#21521;&#23545;&#35937;&#27169;&#25311;&#29615;&#22659;&#65288;MOOSE&#65289;&#26694;&#26550;&#20013;&#24320;&#21457;&#30340;&#22686;&#26448;&#21046;&#36896;&#27169;&#22411;&#26500;&#24314;&#19968;&#20010;&#24555;&#36895;&#20934;&#30830;&#30340;&#20943;&#38454;&#27169;&#22411;&#65288;ROM&#65289;&#65292;&#20174;&#32780;&#26368;&#32456;&#20943;&#23569;&#22686;&#26448;&#21046;&#36896;&#25511;&#21046;&#21644;&#20248;&#21270;&#36807;&#31243;&#30340;&#26102;&#38388;/&#25104;&#26412;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#36816;&#31639;&#23398;&#20064;&#65288;OL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#21464;&#28608;&#20809;&#30340;&#39640;&#26031;&#28857;&#28909;&#28304;&#20013;&#30340;&#36807;&#31243;&#21464;&#37327;&#65292;&#23398;&#20064;&#20102;&#19968;&#31995;&#21015;&#24494;&#20998;&#26041;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;Fourier&#31070;&#32463;&#36816;&#31639;&#22120;&#65288;FNO&#65289;&#21644;&#28145;&#24230;&#36816;&#31639;&#22120;&#32593;&#32476;&#65288;DeepONet&#65289;&#26469;&#24320;&#21457;&#26102;&#38388;&#30456;&#20851;&#21709;&#24212;&#30340;&#20943;&#38454;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;OL&#26041;&#27861;&#30340;&#24615;&#33021;&#19982;&#20256;&#32479;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
One predominant challenge in additive manufacturing (AM) is to achieve specific material properties by manipulating manufacturing process parameters during the runtime. Such manipulation tends to increase the computational load imposed on existing simulation tools employed in AM. The goal of the present work is to construct a fast and accurate reduced-order model (ROM) for an AM model developed within the Multiphysics Object-Oriented Simulation Environment (MOOSE) framework, ultimately reducing the time/cost of AM control and optimization processes. Our adoption of the operator learning (OL) approach enabled us to learn a family of differential equations produced by altering process variables in the laser's Gaussian point heat source. More specifically, we used the Fourier neural operator (FNO) and deep operator network (DeepONet) to develop ROMs for time-dependent responses. Furthermore, we benchmarked the performance of these OL methods against a conventional deep neural network (DNN
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#21355;&#26143;&#35266;&#27979;&#21644;&#27668;&#20505;&#27169;&#22411;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#28023;&#24179;&#38754;&#21464;&#21270;&#30340;&#26694;&#26550;&#65292;&#24182;&#29983;&#25104;&#20102;30&#24180;&#21518;&#28023;&#24179;&#38754;&#19978;&#21319;&#30340;&#39044;&#27979;&#65292;&#20026;&#29702;&#35299;&#27668;&#20505;&#21464;&#21270;&#20449;&#21495;&#30340;&#36129;&#29486;&#21644;&#26410;&#26469;&#28023;&#24179;&#38754;&#21464;&#21270;&#30340;&#39044;&#27979;&#25552;&#20379;&#20102;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2308.02460</link><description>&lt;p&gt;
&#21033;&#29992;&#27979;&#39640;&#20202;&#21644;&#27668;&#20505;&#27169;&#22411;&#38598;&#21512;&#30340;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#28023;&#24179;&#38754;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Sea level Projections with Machine Learning using Altimetry and Climate Model ensembles. (arXiv:2308.02460v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02460
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#21355;&#26143;&#35266;&#27979;&#21644;&#27668;&#20505;&#27169;&#22411;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#28023;&#24179;&#38754;&#21464;&#21270;&#30340;&#26694;&#26550;&#65292;&#24182;&#29983;&#25104;&#20102;30&#24180;&#21518;&#28023;&#24179;&#38754;&#19978;&#21319;&#30340;&#39044;&#27979;&#65292;&#20026;&#29702;&#35299;&#27668;&#20505;&#21464;&#21270;&#20449;&#21495;&#30340;&#36129;&#29486;&#21644;&#26410;&#26469;&#28023;&#24179;&#38754;&#21464;&#21270;&#30340;&#39044;&#27979;&#25552;&#20379;&#20102;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;1993&#24180;&#20197;&#26469;&#65292;&#21355;&#26143;&#27979;&#39640;&#20202;&#35266;&#27979;&#21040;&#30340;&#20840;&#29699;&#24179;&#22343;&#28023;&#24179;&#38754;&#19978;&#21319;&#36895;&#24230;&#36798;&#21040;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;3.4&#27627;&#31859;/&#24180;&#12290;&#20973;&#20511;&#36817;30&#24180;&#30340;&#35266;&#27979;&#25968;&#25454;&#65292;&#25105;&#20204;&#29616;&#22312;&#21487;&#20197;&#30740;&#31350;&#20154;&#20026;&#27668;&#20505;&#21464;&#21270;&#20449;&#21495;&#65288;&#22914;&#28201;&#23460;&#27668;&#20307;&#12289;&#27668;&#28342;&#33014;&#21644;&#29983;&#29289;&#36136;&#29123;&#28903;&#65289;&#23545;&#28023;&#24179;&#38754;&#19978;&#21319;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#30740;&#31350;&#26410;&#26469;&#28023;&#24179;&#38754;&#21464;&#21270;&#30340;&#27169;&#24335;&#12290;&#20026;&#20102;&#20102;&#35299;&#27668;&#20505;&#21464;&#21270;&#20449;&#21495;&#30340;&#36129;&#29486;&#31243;&#24230;&#65292;&#24182;&#24110;&#21161;&#39044;&#27979;&#26410;&#26469;&#30340;&#28023;&#24179;&#38754;&#21464;&#21270;&#65292;&#25105;&#20204;&#29992;&#27668;&#20505;&#27169;&#22411;&#27169;&#25311;&#26469;&#36827;&#34892;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#21355;&#26143;&#35266;&#27979;&#21644;&#27668;&#20505;&#27169;&#22411;&#27169;&#25311;&#29983;&#25104;2&#24230;&#20998;&#36776;&#29575;&#31354;&#38388;&#32593;&#26684;30&#24180;&#28023;&#24179;&#38754;&#19978;&#21319;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#36890;&#36807;&#38750;&#32447;&#24615;&#34701;&#21512;&#27668;&#20505;&#27169;&#22411;&#30340;&#22238;&#28335;&#25968;&#25454;&#65288;1993-2019&#24180;&#65289;&#35757;&#32451;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65288;FCNN&#65289;&#26469;&#39044;&#27979;&#27979;&#39640;&#20202;&#30340;&#25968;&#20540;&#12290;&#25152;&#23398;&#24471;&#30340;FCNN&#26159;
&lt;/p&gt;
&lt;p&gt;
Satellite altimeter observations retrieved since 1993 show that the global mean sea level is rising at an unprecedented rate (3.4mm/year). With almost three decades of observations, we can now investigate the contributions of anthropogenic climate-change signals such as greenhouse gases, aerosols, and biomass burning in this rising sea level. We use machine learning (ML) to investigate future patterns of sea level change. To understand the extent of contributions from the climate-change signals, and to help in forecasting sea level change in the future, we turn to climate model simulations. This work presents a machine learning framework that exploits both satellite observations and climate model simulations to generate sea level rise projections at a 2-degree resolution spatial grid, 30 years into the future. We train fully connected neural networks (FCNNs) to predict altimeter values through a non-linear fusion of the climate model hindcasts (for 1993-2019). The learned FCNNs are the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#20998;&#31867;&#25506;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#27492;&#26041;&#27861;&#33021;&#22815;&#35757;&#32451;&#20986;&#24179;&#28369;&#19988;&#20934;&#30830;&#30340;&#38750;&#25235;&#21462;&#24179;&#38754;&#25805;&#20316;&#25511;&#21046;&#22120;&#12290;&#27492;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#20219;&#21153;&#30340;&#28151;&#21512;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#35299;&#20915;&#20102;&#20043;&#21069;&#20351;&#29992;&#30340;&#21333;&#27169;&#24577;&#25506;&#32034;&#31574;&#30053;&#26080;&#27861;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#24179;&#28369;&#36712;&#36857;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.02459</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#27169;&#24577;&#20998;&#31867;&#25506;&#32034;&#30340;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#38750;&#25235;&#21462;&#24179;&#38754;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Nonprehensile Planar Manipulation through Reinforcement Learning with Multimodal Categorical Exploration. (arXiv:2308.02459v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#20998;&#31867;&#25506;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#27492;&#26041;&#27861;&#33021;&#22815;&#35757;&#32451;&#20986;&#24179;&#28369;&#19988;&#20934;&#30830;&#30340;&#38750;&#25235;&#21462;&#24179;&#38754;&#25805;&#20316;&#25511;&#21046;&#22120;&#12290;&#27492;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#20219;&#21153;&#30340;&#28151;&#21512;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#35299;&#20915;&#20102;&#20043;&#21069;&#20351;&#29992;&#30340;&#21333;&#27169;&#24577;&#25506;&#32034;&#31574;&#30053;&#26080;&#27861;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#24179;&#28369;&#36712;&#36857;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#33021;&#22815;&#23454;&#29616;&#28789;&#24039;&#30340;&#38750;&#25235;&#21462;&#24179;&#38754;&#25805;&#20316;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#38382;&#39064;&#30340;&#27424;&#39537;&#21160;&#21644;&#28151;&#21512;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#20877;&#21152;&#19978;&#25705;&#25830;&#21147;&#20132;&#20114;&#25152;&#20135;&#29983;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#38656;&#35201;&#22797;&#26434;&#30340;&#25511;&#21046;&#34892;&#20026;&#12290;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26159;&#24320;&#21457;&#27492;&#31867;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#30340;&#24378;&#22823;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;RL&#25991;&#29486;&#35299;&#20915;&#38750;&#25235;&#21462;&#25512;&#21160;&#20219;&#21153;&#26102;&#30340;&#20934;&#30830;&#24615;&#20302;&#12289;&#36712;&#36857;&#19981;&#24179;&#28369;&#65292;&#24182;&#19988;&#20165;&#23454;&#29616;&#31616;&#21333;&#30340;&#36816;&#21160;&#65292;&#21363;&#19981;&#26059;&#36716;&#34987;&#25805;&#20316;&#30340;&#23545;&#35937;&#12290;&#25105;&#20204;&#25512;&#27979;&#20043;&#21069;&#20351;&#29992;&#30340;&#21333;&#27169;&#24577;&#25506;&#32034;&#31574;&#30053;&#26080;&#27861;&#25429;&#25417;&#20219;&#21153;&#30340;&#28151;&#21512;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#22240;&#20026;&#26426;&#22120;&#20154;&#21644;&#29289;&#20307;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#19981;&#21516;&#30340;&#25509;&#35302;&#20132;&#20114;&#27169;&#24335;&#65292;&#22914;&#31896;&#30528;&#12289;&#28369;&#21160;&#21644;&#20998;&#31163;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#31867;&#20998;&#24067;&#36827;&#34892;&#22810;&#27169;&#24577;&#25506;&#32034;&#30340;&#26041;&#27861;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#35757;&#32451;&#20986;&#24179;&#28369;&#19988;&#20934;&#30830;&#30340;&#38750;&#25235;&#21462;&#24179;&#38754;&#25805;&#20316;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing robot controllers capable of achieving dexterous nonprehensile manipulation, such as pushing an object on a table, is challenging. The underactuated and hybrid-dynamics nature of the problem, further complicated by the uncertainty resulting from the frictional interactions, requires sophisticated control behaviors. Reinforcement Learning (RL) is a powerful framework for developing such robot controllers. However, previous RL literature addressing the nonprehensile pushing task achieves low accuracy, non-smooth trajectories, and only simple motions, i.e. without rotation of the manipulated object. We conjecture that previously used unimodal exploration strategies fail to capture the inherent hybrid-dynamics of the task, arising from the different possible contact interaction modes between the robot and the object, such as sticking, sliding, and separation. In this work, we propose a multimodal exploration approach through categorical distributions, which enables us to train p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;Federated Machine Learning&#65288;FedML&#65289;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#29616;&#29366;&#65292;&#24182;&#21457;&#29616;&#20102;&#38459;&#30861;&#20854;&#23454;&#38469;&#24212;&#29992;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.02454</link><description>&lt;p&gt;
&#30740;&#31350;Federated Machine Learning&#22312;&#23454;&#38469;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#29616;&#29366;
&lt;/p&gt;
&lt;p&gt;
SoK: Assessing the State of Applied Federated Machine Learning. (arXiv:2308.02454v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;Federated Machine Learning&#65288;FedML&#65289;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#29616;&#29366;&#65292;&#24182;&#21457;&#29616;&#20102;&#38459;&#30861;&#20854;&#23454;&#38469;&#24212;&#29992;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#29616;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65307;&#28982;&#32780;&#65292;&#30001;&#20110;&#23545;&#25968;&#25454;&#38544;&#31169;&#30340;&#25285;&#24551;&#65292;&#23427;&#22312;&#38656;&#35201;&#20445;&#25252;&#38544;&#31169;&#30340;&#39046;&#22495;&#30340;&#24212;&#29992;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;Federated Machine Learning&#65288;FedML&#65289;&#26159;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#26159;&#19968;&#31181;&#38754;&#21521;&#25968;&#25454;&#30340;&#27169;&#22411;&#26041;&#27861;&#65292;&#26356;&#21152;&#37325;&#35270;&#25968;&#25454;&#38544;&#31169;&#12290;&#36890;&#36807;&#20351;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;&#20998;&#24067;&#24335;&#25968;&#25454;&#28304;&#32780;&#19981;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#65292;FedML&#25552;&#20379;&#20102;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#65292;&#36866;&#29992;&#20110;&#38656;&#35201;&#20445;&#25252;&#38544;&#31169;&#30340;&#29615;&#22659;&#12290;&#23613;&#31649;&#22312;&#29702;&#35770;&#19978;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;FedML&#22312;&#23454;&#36341;&#20013;&#24182;&#27809;&#26377;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#24212;&#29992;FedML&#30340;&#24403;&#21069;&#29366;&#24577;&#65292;&#24182;&#30830;&#23450;&#38459;&#30861;&#20854;&#23454;&#38469;&#24212;&#29992;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#20840;&#38754;&#31995;&#32479;&#30340;&#25991;&#29486;&#22238;&#39038;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;74&#31687;&#30456;&#20851;&#35770;&#25991;&#65292;&#20998;&#26512;&#20102;FedML&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20851;&#27880;FedML&#23454;&#29616;&#30340;&#29305;&#24449;&#21644;&#26032;&#20852;&#36235;&#21183;&#65292;&#20197;&#21450;&#39537;&#21160;&#21147;&#21644;&#24212;&#29992;&#26041;&#38754;&#30340;&#21160;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) has shown significant potential in various applications; however, its adoption in privacy-critical domains has been limited due to concerns about data privacy. A promising solution to this issue is Federated Machine Learning (FedML), a model-to-data approach that prioritizes data privacy. By enabling ML algorithms to be applied directly to distributed data sources without sharing raw data, FedML offers enhanced privacy protections, making it suitable for privacy-critical environments. Despite its theoretical benefits, FedML has not seen widespread practical implementation. This study aims to explore the current state of applied FedML and identify the challenges hindering its practical adoption. Through a comprehensive systematic literature review, we assess 74 relevant papers to analyze the real-world applicability of FedML. Our analysis focuses on the characteristics and emerging trends of FedML implementations, as well as the motivational drivers and application
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;L&#233;vyGAN&#65292;&#29992;&#20110;&#29983;&#25104;&#26465;&#20214;&#20110;&#24067;&#26391;&#22686;&#37327;&#30340;L&#233;vy&#21306;&#22495;&#30340;&#36817;&#20284;&#26679;&#26412;&#12290;&#36890;&#36807;&#8220;&#26725;&#32763;&#36716;&#8221;&#25805;&#20316;&#65292;&#36755;&#20986;&#30340;&#26679;&#26412;&#21487;&#20197;&#31934;&#30830;&#21305;&#37197;&#25152;&#26377;&#22855;&#25968;&#38454;&#30697;&#65292;&#35299;&#20915;&#20102;&#38750;&#39640;&#26031;&#24615;&#36136;&#19979;&#30340;&#25277;&#26679;&#22256;&#38590;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.02452</link><description>&lt;p&gt;
&#23545;&#39640;&#38454;SDE&#27169;&#25311;&#30340;L&#233;vy&#21306;&#22495;&#36827;&#34892;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Generative Modelling of L\'{e}vy Area for High Order SDE Simulation. (arXiv:2308.02452v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;L&#233;vyGAN&#65292;&#29992;&#20110;&#29983;&#25104;&#26465;&#20214;&#20110;&#24067;&#26391;&#22686;&#37327;&#30340;L&#233;vy&#21306;&#22495;&#30340;&#36817;&#20284;&#26679;&#26412;&#12290;&#36890;&#36807;&#8220;&#26725;&#32763;&#36716;&#8221;&#25805;&#20316;&#65292;&#36755;&#20986;&#30340;&#26679;&#26412;&#21487;&#20197;&#31934;&#30830;&#21305;&#37197;&#25152;&#26377;&#22855;&#25968;&#38454;&#30697;&#65292;&#35299;&#20915;&#20102;&#38750;&#39640;&#26031;&#24615;&#36136;&#19979;&#30340;&#25277;&#26679;&#22256;&#38590;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#24403;&#25968;&#20540;&#27169;&#25311;SDE&#30340;&#35299;&#26102;&#65292;&#35201;&#23454;&#29616;&#24378;&#25910;&#25947;&#36895;&#29575;&#36229;&#36807;O(\sqrt{h})&#65288;&#20854;&#20013;h&#20026;&#27493;&#38271;&#65289;&#65292;&#38656;&#35201;&#20351;&#29992;&#26576;&#20123;&#24067;&#26391;&#36816;&#21160;&#30340;&#36845;&#20195;&#31215;&#20998;&#65292;&#36890;&#24120;&#31216;&#20026;&#20854;&#8220;L&#233;vy&#21306;&#22495;&#8221;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#38750;&#39640;&#26031;&#24615;&#36136;&#65292;&#23545;&#20110;d&#32500;&#24067;&#26391;&#36816;&#21160;&#65288;d&gt;2&#65289;&#65292;&#30446;&#21069;&#27809;&#26377;&#24555;&#36895;&#36817;&#20284;&#25277;&#26679;&#31639;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;L&#233;vyGAN&#65292;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#26465;&#20214;&#20110;&#24067;&#26391;&#22686;&#37327;&#30340;L&#233;vy&#21306;&#22495;&#30340;&#36817;&#20284;&#26679;&#26412;&#12290;&#36890;&#36807;&#8220;&#26725;&#32763;&#36716;&#8221;&#25805;&#20316;&#65292;&#36755;&#20986;&#30340;&#26679;&#26412;&#21487;&#20197;&#31934;&#30830;&#21305;&#37197;&#25152;&#26377;&#22855;&#25968;&#38454;&#30697;&#12290;&#25105;&#20204;&#30340;&#29983;&#25104;&#22120;&#37319;&#29992;&#32463;&#36807;&#37327;&#36523;&#23450;&#21046;&#30340;GNN-inspired&#26550;&#26500;&#65292;&#24378;&#21046;&#36755;&#20986;&#20998;&#24067;&#19982;&#26465;&#20214;&#21464;&#37327;&#20043;&#38388;&#30340;&#27491;&#30830;&#20381;&#36182;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#32467;&#21512;&#20102;&#22522;&#20110;&#29305;&#24449;&#20989;&#25968;&#30340;&#25968;&#23398;&#21407;&#29702;&#30340;&#21028;&#21035;&#24615;&#24402;&#19968;&#21270;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is well known that, when numerically simulating solutions to SDEs, achieving a strong convergence rate better than O(\sqrt{h}) (where h is the step size) requires the use of certain iterated integrals of Brownian motion, commonly referred to as its "L\'{e}vy areas". However, these stochastic integrals are difficult to simulate due to their non-Gaussian nature and for a d-dimensional Brownian motion with d &gt; 2, no fast almost-exact sampling algorithm is known.  In this paper, we propose L\'{e}vyGAN, a deep-learning-based model for generating approximate samples of L\'{e}vy area conditional on a Brownian increment. Due to our "Bridge-flipping" operation, the output samples match all joint and conditional odd moments exactly. Our generator employs a tailored GNN-inspired architecture, which enforces the correct dependency structure between the output distribution and the conditioning variable. Furthermore, we incorporate a mathematically principled characteristic-function based discrim
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#20462;&#21098;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#29702;&#24819;&#30340;&#31232;&#30095;&#31243;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#21147;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.02451</link><description>&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#20462;&#21098;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Pruning a neural network using Bayesian inference. (arXiv:2308.02451v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02451
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#20462;&#21098;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#29702;&#24819;&#30340;&#31232;&#30095;&#31243;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#21147;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20462;&#21098;&#26159;&#19968;&#31181;&#38750;&#24120;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;&#26088;&#22312;&#20943;&#23569;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#12290;&#22312;&#36825;&#31687;&#30740;&#31350;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#20462;&#21098;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#26080;&#32541;&#22320;&#38598;&#25104;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#20462;&#21098;&#21069;&#21518;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#21518;&#39564;&#27010;&#29575;&#65292;&#20174;&#32780;&#35745;&#31639;&#36125;&#21494;&#26031;&#22240;&#23376;&#12290;&#35745;&#31639;&#30340;&#36125;&#21494;&#26031;&#22240;&#23376;&#25351;&#23548;&#30528;&#36845;&#20195;&#20462;&#21098;&#36807;&#31243;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#22522;&#20934;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;&#31454;&#20105;&#21147;&#30340;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#29702;&#24819;&#30340;&#31232;&#30095;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network pruning is a highly effective technique aimed at reducing the computational and memory demands of large neural networks. In this research paper, we present a novel approach to pruning neural networks utilizing Bayesian inference, which can seamlessly integrate into the training procedure. Our proposed method leverages the posterior probabilities of the neural network prior to and following pruning, enabling the calculation of Bayes factors. The calculated Bayes factors guide the iterative pruning. Through comprehensive evaluations conducted on multiple benchmarks, we demonstrate that our method achieves desired levels of sparsity while maintaining competitive accuracy.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20174;&#20891;&#20107;&#21040;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#37319;&#32435;&#21644;&#25193;&#23637;&#20262;&#29702;&#21407;&#21017;&#20197;&#24212;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#21487;&#34892;&#24615;&#21644;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.02448</link><description>&lt;p&gt;
&#20174;&#20891;&#20107;&#21040;&#21307;&#30103;&#20445;&#20581;&#65306;&#37319;&#32435;&#21644;&#25193;&#23637;&#29992;&#20110;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#20262;&#29702;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
From Military to Healthcare: Adopting and Expanding Ethical Principles for Generative Artificial Intelligence. (arXiv:2308.02448v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02448
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20174;&#20891;&#20107;&#21040;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#37319;&#32435;&#21644;&#25193;&#23637;&#20262;&#29702;&#21407;&#21017;&#20197;&#24212;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#21487;&#34892;&#24615;&#21644;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;2020&#24180;&#65292;&#32654;&#22269;&#22269;&#38450;&#37096;&#27491;&#24335;&#20844;&#24067;&#20102;&#19968;&#22871;&#25351;&#23548;&#26410;&#26469;&#25112;&#22330;&#19978;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20351;&#29992;&#30340;&#20262;&#29702;&#21407;&#21017;&#12290;&#23613;&#31649;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#65292;&#20294;&#20891;&#20107;&#21644;&#21307;&#30103;&#26381;&#21153;&#20043;&#38388;&#23384;&#22312;&#26680;&#24515;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;&#25112;&#22330;&#19978;&#30340;&#25112;&#22763;&#32463;&#24120;&#38754;&#20020;&#38656;&#35201;&#24555;&#36895;&#20915;&#31574;&#30340;&#25913;&#21464;&#29983;&#27963;&#30340;&#24773;&#20917;&#12290;&#21307;&#30103;&#26381;&#21153;&#25552;&#20379;&#32773;&#22312;&#24555;&#36895;&#21464;&#21270;&#30340;&#21307;&#30103;&#29615;&#22659;&#20013;&#20063;&#38754;&#20020;&#31867;&#20284;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#22312;&#24613;&#35786;&#31185;&#25110;&#27835;&#30103;&#21361;&#21450;&#29983;&#21629;&#30340;&#29366;&#20917;&#19979;&#36827;&#34892;&#25163;&#26415;&#12290;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#31181;&#26032;&#20852;&#25216;&#26415;&#65292;&#26088;&#22312;&#39640;&#25928;&#29983;&#25104;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#38543;&#30528;&#35745;&#31639;&#33021;&#21147;&#30340;&#26085;&#30410;&#26222;&#21450;&#21644;&#22823;&#37327;&#30340;&#20581;&#24247;&#25968;&#25454;&#65288;&#22914;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#12289;&#24515;&#30005;&#22270;&#21644;&#21307;&#23398;&#22270;&#20687;&#65289;&#30340;&#22686;&#21152;&#65292;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24517;&#23558;&#34987;&#36825;&#39033;&#25216;&#26415;&#38761;&#21629;&#21270;&#12290;&#26368;&#36817;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#30740;&#31350;&#30028;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#24341;&#21457;&#20102;&#20851;&#20110;&#20854;&#20262;&#29702;&#38382;&#39064;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
In 2020, the U.S. Department of Defense officially disclosed a set of ethical principles to guide the use of Artificial Intelligence (AI) technologies on future battlefields. Despite stark differences, there are core similarities between the military and medical service. Warriors on battlefields often face life-altering circumstances that require quick decision-making. Medical providers experience similar challenges in a rapidly changing healthcare environment, such as in the emergency department or during surgery treating a life-threatening condition. Generative AI, an emerging technology designed to efficiently generate valuable information, holds great promise. As computing power becomes more accessible and the abundance of health data, such as electronic health records, electrocardiograms, and medical images, increases, it is inevitable that healthcare will be revolutionized by this technology. Recently, generative AI has captivated the research community, leading to debates about 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;paNNG&#30340;&#31639;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#33258;&#36866;&#24212;kNN&#21644;&#22522;&#20110;&#20998;&#24067;&#30340;&#22270;&#26500;&#24314;&#12290;&#36890;&#36807;&#21253;&#21547;&#20998;&#24067;&#20449;&#24687;&#65292;paNNG&#33021;&#22815;&#26377;&#25928;&#25552;&#21319;&#27169;&#31946;&#26679;&#26412;&#30340;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.02442</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#24067;&#24863;&#30693;&#30340;&#33258;&#36866;&#24212;&#20248;&#20808;&#32423;&#38468;&#21152;kNN&#22270;
&lt;/p&gt;
&lt;p&gt;
Adaptive Preferential Attached kNN Graph With Distribution-Awareness. (arXiv:2308.02442v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;paNNG&#30340;&#31639;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#33258;&#36866;&#24212;kNN&#21644;&#22522;&#20110;&#20998;&#24067;&#30340;&#22270;&#26500;&#24314;&#12290;&#36890;&#36807;&#21253;&#21547;&#20998;&#24067;&#20449;&#24687;&#65292;paNNG&#33021;&#22815;&#26377;&#25928;&#25552;&#21319;&#27169;&#31946;&#26679;&#26412;&#30340;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;kNN&#31639;&#27861;&#22240;&#20854;&#31616;&#21333;&#24615;&#21644;&#26377;&#25928;&#24615;&#22312;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#24191;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;kNN&#22270;&#23545;&#20110;k&#20540;&#30340;&#22266;&#23450;&#20381;&#36182;&#21487;&#33021;&#20250;&#24433;&#21709;&#20854;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#12290;&#27492;&#22806;&#65292;&#19982;&#20854;&#20182;&#20998;&#31867;&#27169;&#22411;&#31867;&#20284;&#65292;&#20915;&#31574;&#36793;&#30028;&#19978;&#23384;&#22312;&#30340;&#27169;&#31946;&#26679;&#26412;&#24120;&#24120;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#26356;&#23481;&#26131;&#34987;&#38169;&#35823;&#20998;&#31867;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20248;&#20808;&#32423;&#38468;&#21152;k-&#26368;&#36817;&#37051;&#22270;&#65288;paNNG&#65289;&#65292;&#23427;&#23558;&#33258;&#36866;&#24212;&#30340;kNN&#19982;&#22522;&#20110;&#20998;&#24067;&#30340;&#22270;&#26500;&#24314;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#32467;&#21512;&#20998;&#24067;&#20449;&#24687;&#65292;paNNG&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#31946;&#26679;&#26412;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#8220;&#25289;&#8221;&#23427;&#20204;&#22238;&#21040;&#21407;&#22987;&#31867;&#21035;&#65292;&#20174;&#32780;&#23454;&#29616;&#25913;&#36827;&#30340;&#25972;&#20307;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#22810;&#26679;&#21270;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20005;&#26684;&#35780;&#20272;&#65292;paNNG&#30340;&#24615;&#33021;&#36229;&#36234;&#20102;&#29616;&#26377;&#31639;&#27861;&#65292;&#23637;&#31034;&#20102;&#23427;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph-based kNN algorithms have garnered widespread popularity for machine learning tasks, due to their simplicity and effectiveness. However, the conventional kNN graph's reliance on a fixed value of k can hinder its performance, especially in scenarios involving complex data distributions. Moreover, like other classification models, the presence of ambiguous samples along decision boundaries often presents a challenge, as they are more prone to incorrect classification. To address these issues, we propose the Preferential Attached k-Nearest Neighbors Graph (paNNG), which combines adaptive kNN with distribution-based graph construction. By incorporating distribution information, paNNG can significantly improve performance for ambiguous samples by "pulling" them towards their original classes and hence enable enhanced overall accuracy and generalization capability. Through rigorous evaluations on diverse benchmark datasets, paNNG outperforms state-of-the-art algorithms, showcasing its 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#20415;&#25658;&#24335;&#33041;&#30005;&#22270;&#20013;&#22122;&#38899;&#30340;&#26816;&#27979;&#21644;&#21435;&#38500;&#26041;&#27861;&#65292;&#21457;&#29616;&#27169;&#24335;&#35782;&#21035;&#23545;&#20110;&#21306;&#20998;&#19981;&#21516;&#26465;&#20214;&#19979;&#30340;&#33041;&#30005;&#22270;&#25968;&#25454;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2308.02437</link><description>&lt;p&gt;
&#20415;&#25658;&#24335;&#33041;&#30005;&#22270;&#20013;&#30340;&#22122;&#38899;&#21435;&#38500;&#26041;&#27861;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Noise removal methods on ambulatory EEG: A Survey. (arXiv:2308.02437v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#20415;&#25658;&#24335;&#33041;&#30005;&#22270;&#20013;&#22122;&#38899;&#30340;&#26816;&#27979;&#21644;&#21435;&#38500;&#26041;&#27861;&#65292;&#21457;&#29616;&#27169;&#24335;&#35782;&#21035;&#23545;&#20110;&#21306;&#20998;&#19981;&#21516;&#26465;&#20214;&#19979;&#30340;&#33041;&#30005;&#22270;&#25968;&#25454;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#26469;&#65292;&#19968;&#30452;&#33268;&#21147;&#20110;&#20415;&#25658;&#24335;&#33041;&#30005;&#22270;&#20013;&#22122;&#38899;&#30340;&#21435;&#38500;&#30740;&#31350;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#24050;&#32463;&#21457;&#24067;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#35770;&#25991;&#26469;&#35782;&#21035;&#22122;&#38899;&#21435;&#38500;&#26041;&#27861;&#65292;&#20294;&#26159;&#35814;&#32454;&#22238;&#39038;&#25152;&#26377;&#36825;&#20123;&#25991;&#29486;&#26159;&#22256;&#38590;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#32508;&#36848;&#22122;&#38899;&#30340;&#26816;&#27979;&#21644;&#21435;&#38500;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;100&#22810;&#31687;&#30740;&#31350;&#35770;&#25991;&#65292;&#20197;&#35782;&#21035;&#26816;&#27979;&#21644;&#21435;&#38500;&#20415;&#25658;&#24335;&#33041;&#30005;&#22270;&#30340;&#25216;&#26415;&#12290;&#36827;&#19968;&#27493;&#30340;&#25991;&#29486;&#32508;&#36848;&#34920;&#26126;&#65292;&#29992;&#20110;&#26816;&#27979;&#20415;&#25658;&#24335;&#33041;&#30005;&#22270;&#30340;&#27169;&#24335;&#35782;&#21035;&#26041;&#27861;&#65292;&#27604;&#22914;&#30524;&#30555;&#38381;&#21512;&#21644;&#30529;&#24320;&#29366;&#24577;&#65292;&#22240;&#33041;&#30005;&#22270;&#30340;&#26465;&#20214;&#19981;&#21516;&#32780;&#24322;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#19981;&#21516;&#26465;&#20214;&#19979;&#26816;&#27979;&#21040;&#30340;&#33041;&#30005;&#22270;&#20855;&#26377;&#19981;&#21516;&#30340;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#24517;&#39035;&#30830;&#23450;&#19968;&#31181;&#27169;&#24335;&#35782;&#21035;&#25216;&#26415;&#65292;&#20197;&#26377;&#25928;&#22320;&#21306;&#20998;&#33041;&#30005;&#22270;&#22122;&#38899;&#25968;&#25454;&#21644;&#21508;&#31181;&#33041;&#30005;&#22270;&#25968;&#25454;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over many decades, research is being attempted for the removal of noise in the ambulatory EEG. In this respect, an enormous number of research papers is published for identification of noise removal, It is difficult to present a detailed review of all these literature. Therefore, in this paper, an attempt has been made to review the detection and removal of an noise. More than 100 research papers have been discussed to discern the techniques for detecting and removal the ambulatory EEG. Further, the literature survey shows that the pattern recognition required to detect ambulatory method, eye open and close, varies with different conditions of EEG datasets. This is mainly due to the fact that EEG detected under different conditions has different characteristics. This is, in turn, necessitates the identification of pattern recognition technique to effectively distinguish EEG noise data from a various condition of EEG data.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#29983;&#29702;&#20449;&#21495;&#26816;&#27979;&#24515;&#25151;&#39076;&#21160;(AF)&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#23398;&#20064;&#30456;&#20284;&#23884;&#20837;&#24182;&#30830;&#23450;&#26368;&#30456;&#20284;&#30340;&#24739;&#32773;&#12290;&#22312;&#36229;&#36807;170&#21517;&#20010;&#20307;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#19982;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2308.02433</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#24739;&#32773;&#30456;&#20284;&#24615;&#30740;&#31350;&#26041;&#27861;&#65306;&#20197;&#24515;&#25151;&#39076;&#21160;&#30340;&#26816;&#27979;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Contrastive Self-Supervised Learning Based Approach for Patient Similarity: A Case Study on Atrial Fibrillation Detection from PPG Signal. (arXiv:2308.02433v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02433
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#29983;&#29702;&#20449;&#21495;&#26816;&#27979;&#24515;&#25151;&#39076;&#21160;(AF)&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#23398;&#20064;&#30456;&#20284;&#23884;&#20837;&#24182;&#30830;&#23450;&#26368;&#30456;&#20284;&#30340;&#24739;&#32773;&#12290;&#22312;&#36229;&#36807;170&#21517;&#20010;&#20307;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#19982;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20351;&#29992;&#29983;&#29702;&#20449;&#21495;&#36827;&#34892;&#24739;&#32773;&#30456;&#20284;&#24615;&#25628;&#32034;&#12290;&#25105;&#20204;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#20855;&#26377;&#30456;&#20284;&#29983;&#29702;&#20449;&#21495;&#25968;&#25454;&#30340;&#24739;&#32773;&#30340;&#30456;&#20284;&#23884;&#20837;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20123;&#37051;&#23621;&#36873;&#25321;&#31639;&#27861;&#65292;&#20197;&#30830;&#23450;&#29983;&#25104;&#30340;&#23884;&#20837;&#20013;&#20855;&#26377;&#26368;&#39640;&#30456;&#20284;&#24615;&#30340;&#24739;&#32773;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26694;&#26550;&#29992;&#20110;&#27979;&#37327;&#24739;&#32773;&#30456;&#20284;&#24615;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36873;&#25321;&#20351;&#29992;&#20174;&#26234;&#33021;&#25163;&#34920;&#35774;&#22791;&#33719;&#21462;&#30340;&#20809;&#30005;&#23481;&#25239;&#22270;&#20449;&#21495;&#36827;&#34892;&#24515;&#25151;&#39076;&#21160;(AF)&#30340;&#26816;&#27979;&#20316;&#20026;&#25105;&#20204;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#25105;&#20204;&#23545;&#36229;&#36807;170&#21517;&#20010;&#20307;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#25105;&#20204;&#30340;&#26694;&#26550;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#24182;&#23558;&#20854;&#19982;&#35813;&#25968;&#25454;&#38598;&#19978;&#30340;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel contrastive learning based deep learning framework for patient similarity search using physiological signals. We use a contrastive learning based approach to learn similar embeddings of patients with similar physiological signal data. We also introduce a number of neighbor selection algorithms to determine the patients with the highest similarity on the generated embeddings. To validate the effectiveness of our framework for measuring patient similarity, we select the detection of Atrial Fibrillation (AF) through photoplethysmography (PPG) signals obtained from smartwatch devices as our case study. We present extensive experimentation of our framework on a dataset of over 170 individuals and compare the performance of our framework with other baseline methods on this dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#21305;&#37197;&#30340;&#23398;&#20064;&#27169;&#24335;&#65292;&#36890;&#36807;&#20351;&#29992;PyTorch&#23454;&#29616;&#20102;&#25193;&#23637;&#21040;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#21367;&#31215;&#38750;&#36127;&#30456;&#20284;&#24615;&#21305;&#37197;&#65292;&#24341;&#20837;&#20102;&#23616;&#37096;&#30417;&#30563;&#30340;SM&#30446;&#26631;&#65292;&#20197;&#21450;&#21033;&#29992;PyTorch&#23454;&#29616;&#39044;&#35757;&#32451;&#32593;&#32476;&#32467;&#26500;&#30340;&#29305;&#24449;&#35780;&#20272;&#23545;&#27604;&#12290;&#36825;&#23558;&#29983;&#29289;&#23398;&#21512;&#29702;&#24615;&#30340;&#31639;&#27861;&#19982;&#35745;&#31639;&#25104;&#26412;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#32447;&#12289;&#26412;&#22320;&#21270;&#19988;&#20855;&#26377;&#29983;&#29289;&#23398;&#21512;&#29702;&#24615;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02427</link><description>&lt;p&gt;
&#35299;&#38145;&#30456;&#20284;&#24615;&#21305;&#37197;&#30340;&#28508;&#21147;&#65306;&#21487;&#25193;&#23637;&#24615;&#12289;&#30417;&#30563;&#21644;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Unlocking the Potential of Similarity Matching: Scalability, Supervision and Pre-training. (arXiv:2308.02427v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#21305;&#37197;&#30340;&#23398;&#20064;&#27169;&#24335;&#65292;&#36890;&#36807;&#20351;&#29992;PyTorch&#23454;&#29616;&#20102;&#25193;&#23637;&#21040;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#21367;&#31215;&#38750;&#36127;&#30456;&#20284;&#24615;&#21305;&#37197;&#65292;&#24341;&#20837;&#20102;&#23616;&#37096;&#30417;&#30563;&#30340;SM&#30446;&#26631;&#65292;&#20197;&#21450;&#21033;&#29992;PyTorch&#23454;&#29616;&#39044;&#35757;&#32451;&#32593;&#32476;&#32467;&#26500;&#30340;&#29305;&#24449;&#35780;&#20272;&#23545;&#27604;&#12290;&#36825;&#23558;&#29983;&#29289;&#23398;&#21512;&#29702;&#24615;&#30340;&#31639;&#27861;&#19982;&#35745;&#31639;&#25104;&#26412;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#32447;&#12289;&#26412;&#22320;&#21270;&#19988;&#20855;&#26377;&#29983;&#29289;&#23398;&#21512;&#29702;&#24615;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26377;&#25928;&#65292;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#22312;&#29983;&#29289;&#21512;&#29702;&#24615;&#12289;&#35745;&#31639;&#25104;&#26412;&#21644;&#22312;&#32447;&#23398;&#20064;&#36866;&#24212;&#24615;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22240;&#27492;&#65292;&#20154;&#20204;&#23545;&#20110;&#22522;&#20110;&#23616;&#37096;&#23398;&#20064;&#35268;&#21017;&#30340;&#21487;&#26367;&#20195;&#29983;&#29289;&#23398;&#21512;&#29702;&#24615;&#23398;&#20064;&#26041;&#27861;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#26412;&#30740;&#31350;&#37325;&#28857;&#30740;&#31350;&#20102;&#20027;&#35201;&#26159;&#38750;&#30417;&#30563;&#30340;&#30456;&#20284;&#24615;&#21305;&#37197;(SM)&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#19982;&#29983;&#29289;&#31995;&#32479;&#20013;&#35266;&#23519;&#21040;&#30340;&#26426;&#21046;&#30456;&#19968;&#33268;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#32447;&#12289;&#26412;&#22320;&#21270;&#21644;&#29983;&#29289;&#23398;&#19978;&#21512;&#29702;&#30340;&#31639;&#27861;&#12290;i) &#20026;&#20102;&#23558;&#30456;&#20284;&#24615;&#21305;&#37197;(SM)&#25193;&#23637;&#21040;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;PyTorch&#30340;&#21367;&#31215;&#38750;&#36127;SM&#30340;&#23454;&#29616;&#12290;ii) &#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;&#20856;&#22411;&#30456;&#20851;&#24615;&#20998;&#26512;&#30340;&#23616;&#37096;&#30417;&#30563;SM&#30446;&#26631;&#65292;&#20415;&#20110;&#22534;&#21472;SM&#23618;&#12290;iii) &#25105;&#20204;&#21033;&#29992;PyTorch&#23454;&#29616;&#39044;&#35757;&#32451;&#32593;&#32476;&#32467;&#26500;&#65292;&#22914;LeNet&#65292;&#24182;&#23558;&#20854;&#29305;&#24449;&#35780;&#20272;&#19982;&#21453;&#21521;&#20256;&#25773;&#35757;&#32451;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#26412;&#30740;&#31350;&#23558;&#20855;&#26377;&#29983;&#29289;&#21512;&#29702;&#24615;&#30340;&#31639;&#27861;&#19982;&#35745;&#31639;&#25104;&#26412;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
While effective, the backpropagation (BP) algorithm exhibits limitations in terms of biological plausibility, computational cost, and suitability for online learning. As a result, there has been a growing interest in developing alternative biologically plausible learning approaches that rely on local learning rules. This study focuses on the primarily unsupervised similarity matching (SM) framework, which aligns with observed mechanisms in biological systems and offers online, localized, and biologically plausible algorithms. i) To scale SM to large datasets, we propose an implementation of Convolutional Nonnegative SM using PyTorch. ii) We introduce a localized supervised SM objective reminiscent of canonical correlation analysis, facilitating stacking SM layers. iii) We leverage the PyTorch implementation for pre-training architectures such as LeNet and compare the evaluation of features against BP-trained models. This work combines biologically plausible algorithms with computationa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#21367;&#31215;&#26680;&#30340;&#39640;&#32500;&#34920;&#31034;&#25216;&#26415;&#65292;&#29992;&#20110;&#20174;&#33033;&#25615;&#27874;&#20449;&#21495;&#20013;&#26816;&#27979;&#39640;&#34880;&#21387;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#19968;&#26041;&#27861;&#19981;&#20165;&#23616;&#38480;&#20110;&#24515;&#29575;&#21644;&#34880;&#21387;&#65292;&#23637;&#31034;&#20102;&#39640;&#34880;&#21387;&#26816;&#27979;&#30340;&#26222;&#36866;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.02425</link><description>&lt;p&gt;
&#20174;&#39640;&#32500;&#33033;&#25615;&#27874;&#20449;&#21495;&#20013;&#26816;&#27979;&#39640;&#34880;&#21387;
&lt;/p&gt;
&lt;p&gt;
Hypertension Detection From High-Dimensional Representation of Photoplethysmogram Signals. (arXiv:2308.02425v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#21367;&#31215;&#26680;&#30340;&#39640;&#32500;&#34920;&#31034;&#25216;&#26415;&#65292;&#29992;&#20110;&#20174;&#33033;&#25615;&#27874;&#20449;&#21495;&#20013;&#26816;&#27979;&#39640;&#34880;&#21387;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#19968;&#26041;&#27861;&#19981;&#20165;&#23616;&#38480;&#20110;&#24515;&#29575;&#21644;&#34880;&#21387;&#65292;&#23637;&#31034;&#20102;&#39640;&#34880;&#21387;&#26816;&#27979;&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#34880;&#21387;&#24120;&#34987;&#31216;&#20026;&#8220;&#27785;&#40664;&#26432;&#25163;&#8221;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#30340;&#20581;&#24247;&#24182;&#21457;&#30151;&#65292;&#21364;&#27809;&#26377;&#20219;&#20309;&#26126;&#26174;&#30151;&#29366;&#12290;&#26089;&#26399;&#21457;&#29616;&#39640;&#34880;&#21387;&#22312;&#39044;&#38450;&#37325;&#22823;&#20581;&#24247;&#38382;&#39064;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;&#34880;&#21387;&#19982;&#26576;&#20123;&#37325;&#35201;&#20449;&#21495;&#65288;&#22914;&#33033;&#25615;&#27874;&#65289;&#20043;&#38388;&#23384;&#22312;&#20851;&#32852;&#65292;&#20294;&#30446;&#21069;&#23578;&#26080;&#21487;&#38752;&#30340;&#25552;&#21462;&#34880;&#21387;&#20272;&#35745;&#26041;&#27861;&#36827;&#34892;&#26222;&#36866;&#24615;&#27867;&#21270;&#12290;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#20351;&#24471;&#19968;&#20123;&#30740;&#31350;&#23545;&#36825;&#31181;&#20851;&#31995;&#30340;&#23384;&#22312;&#20135;&#29983;&#24576;&#30097;&#65292;&#25110;&#32773;&#35748;&#20026;&#23427;&#20204;&#20165;&#38480;&#20110;&#24515;&#29575;&#21644;&#34880;&#21387;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#21367;&#31215;&#26680;&#30340;&#39640;&#32500;&#34920;&#31034;&#25216;&#26415;&#65292;&#29992;&#20110;&#20351;&#29992;&#33033;&#25615;&#27874;&#20449;&#21495;&#36827;&#34892;&#39640;&#34880;&#21387;&#26816;&#27979;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#20851;&#31995;&#19981;&#20165;&#38480;&#20110;&#24515;&#29575;&#21644;&#34880;&#21387;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#27867;&#21270;&#24615;&#30340;&#39640;&#34880;&#21387;&#26816;&#27979;&#30340;&#21487;&#34892;&#24615;&#12290;&#21478;&#22806;&#65292;&#20351;&#29992;&#21367;&#31215;&#26680;&#36827;&#34892;&#21464;&#25442;&#30340;&#26041;&#27861;&#20063;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypertension is commonly referred to as the "silent killer", since it can lead to severe health complications without any visible symptoms. Early detection of hypertension is crucial in preventing significant health issues. Although some studies suggest a relationship between blood pressure and certain vital signals, such as Photoplethysmogram (PPG), reliable generalization of the proposed blood pressure estimation methods is not yet guaranteed. This lack of certainty has resulted in some studies doubting the existence of such relationships, or considering them weak and limited to heart rate and blood pressure. In this paper, a high-dimensional representation technique based on random convolution kernels is proposed for hypertension detection using PPG signals. The results show that this relationship extends beyond heart rate and blood pressure, demonstrating the feasibility of hypertension detection with generalization. Additionally, the utilized transform using convolution kernels, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36339;&#27493;&#38271;&#24230;&#21487;&#24494;&#20998;&#30340;&#30701;&#26102;&#20613;&#37324;&#21494;&#21464;&#25442;&#65292;&#36890;&#36807;&#20351;&#36339;&#27493;&#38271;&#24230;&#36830;&#32493;&#65292;&#23454;&#29616;&#20102;&#26799;&#24230;&#20248;&#21270;&#30340;&#26102;&#38388;&#23450;&#20301;&#25511;&#21046;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#20248;&#21270;&#25928;&#26524;&#21644;&#26356;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#38598;&#25104;&#21040;&#29616;&#26377;&#31639;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;</title><link>http://arxiv.org/abs/2308.02421</link><description>&lt;p&gt;
&#22522;&#20110;&#36339;&#27493;&#38271;&#24230;&#21487;&#24494;&#20998;&#30340;&#30701;&#26102;&#20613;&#37324;&#21494;&#21464;&#25442;
&lt;/p&gt;
&lt;p&gt;
Differentiable short-time Fourier transform with respect to the hop length. (arXiv:2308.02421v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36339;&#27493;&#38271;&#24230;&#21487;&#24494;&#20998;&#30340;&#30701;&#26102;&#20613;&#37324;&#21494;&#21464;&#25442;&#65292;&#36890;&#36807;&#20351;&#36339;&#27493;&#38271;&#24230;&#36830;&#32493;&#65292;&#23454;&#29616;&#20102;&#26799;&#24230;&#20248;&#21270;&#30340;&#26102;&#38388;&#23450;&#20301;&#25511;&#21046;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#20248;&#21270;&#25928;&#26524;&#21644;&#26356;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#38598;&#25104;&#21040;&#29616;&#26377;&#31639;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36339;&#27493;&#38271;&#24230;&#21487;&#24494;&#20998;&#30340;&#30701;&#26102;&#20613;&#37324;&#21494;&#21464;&#25442;&#65288;STFT&#65289;&#65292;&#36890;&#36807;&#20351;&#36825;&#20123;&#21442;&#25968;&#36830;&#32493;&#65292;&#20351;&#36339;&#27493;&#38271;&#24230;&#25110;&#24103;&#26102;&#38388;&#20301;&#32622;&#21487;&#20197;&#22522;&#20110;&#26799;&#24230;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#23545;&#24103;&#30340;&#26102;&#38388;&#23450;&#20301;&#26356;&#22909;&#30340;&#25511;&#21046;&#65292;&#22240;&#20026;&#36339;&#27493;&#38271;&#24230;&#30340;&#36830;&#32493;&#24615;&#20801;&#35768;&#36827;&#34892;&#26356;&#31934;&#32454;&#30340;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#36129;&#29486;&#36824;&#21487;&#20197;&#20351;&#29992;&#35832;&#22914;&#26799;&#24230;&#19979;&#38477;&#20043;&#31867;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#27604;&#20256;&#32479;&#30340;&#31163;&#25955;&#20248;&#21270;&#26041;&#27861;&#26356;&#21152;&#39640;&#25928;&#12290;&#25105;&#20204;&#30340;&#21487;&#24494;&#20998;STFT&#20063;&#21487;&#20197;&#36731;&#26494;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#31639;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#27169;&#25311;&#23454;&#20363;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24341;&#36215;&#30740;&#31350;&#30028;&#30340;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a differentiable version of the short-time Fourier transform (STFT) that allows for gradient-based optimization of the hop length or the frame temporal position by making these parameters continuous. Our approach provides improved control over the temporal positioning of frames, as the continuous nature of the hop length allows for a more finely-tuned optimization. Furthermore, our contribution enables the use of optimization methods such as gradient descent, which are more computationally efficient than conventional discrete optimization methods. Our differentiable STFT can also be easily integrated into existing algorithms and neural networks. We present a simulated illustration to demonstrate the efficacy of our approach and to garner interest from the research community.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22522;&#20110;&#26234;&#33021;&#25163;&#26426;&#30340;&#23454;&#26102;&#20581;&#36523;&#21160;&#20316;&#37325;&#22797;&#35745;&#25968;&#31995;&#32479;&#65292;&#22312;&#25552;&#20379;&#20102;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#27010;&#36848;&#21518;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#30001;&#20116;&#20010;&#32452;&#20214;&#26500;&#25104;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#21517;&#20026;P=&#20027;ioio&#30340;&#31227;&#21160;&#24212;&#29992;&#23454;&#29616;&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;&#26234;&#33021;&#25163;&#26426;&#25668;&#20687;&#22836;&#23454;&#26102;&#36319;&#36394;&#26631;&#20934;&#20581;&#36523;&#21160;&#20316;&#30340;&#37325;&#22797;&#27425;&#25968;&#12290;&#33521;&#25991;&#35201;&#28857;&#20026;&#65306;This study explored a real-time smartphone-based exercise repetition counting system, introduced a deep learning system consisting of five components, and implemented it through a mobile application named P=&#20027;ioio that uses the smartphone camera to track repetitions in real time for standard exercises.</title><link>http://arxiv.org/abs/2308.02420</link><description>&lt;p&gt;
P=&#20027;ioio&#65306;&#22522;&#20110;&#35774;&#22791;&#30340;&#23454;&#26102;&#26234;&#33021;&#25163;&#26426;&#33258;&#21160;&#20581;&#36523;&#21160;&#20316;&#37325;&#22797;&#35745;&#25968;&#31995;&#32479;&#20013;&#25991;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
P\=uioio: On-device Real-Time Smartphone-Based Automated Exercise Repetition Counting System. (arXiv:2308.02420v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22522;&#20110;&#26234;&#33021;&#25163;&#26426;&#30340;&#23454;&#26102;&#20581;&#36523;&#21160;&#20316;&#37325;&#22797;&#35745;&#25968;&#31995;&#32479;&#65292;&#22312;&#25552;&#20379;&#20102;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#27010;&#36848;&#21518;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#30001;&#20116;&#20010;&#32452;&#20214;&#26500;&#25104;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#21517;&#20026;P=&#20027;ioio&#30340;&#31227;&#21160;&#24212;&#29992;&#23454;&#29616;&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;&#26234;&#33021;&#25163;&#26426;&#25668;&#20687;&#22836;&#23454;&#26102;&#36319;&#36394;&#26631;&#20934;&#20581;&#36523;&#21160;&#20316;&#30340;&#37325;&#22797;&#27425;&#25968;&#12290;&#33521;&#25991;&#35201;&#28857;&#20026;&#65306;This study explored a real-time smartphone-based exercise repetition counting system, introduced a deep learning system consisting of five components, and implemented it through a mobile application named P=&#20027;ioio that uses the smartphone camera to track repetitions in real time for standard exercises.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20581;&#36523;&#21160;&#20316;&#37325;&#22797;&#35745;&#25968;&#22312;&#20581;&#24247;&#21040;&#24247;&#22797;&#31561;&#21508;&#20010;&#26041;&#38754;&#37117;&#26377;&#24212;&#29992;&#12290;&#37492;&#20110;&#25163;&#26426;&#30340;&#26222;&#21450;&#24615;&#21644;&#36319;&#36394;&#36523;&#20307;&#27963;&#21160;&#30340;&#22909;&#22788;&#65292;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20165;&#20351;&#29992;&#25163;&#26426;&#35774;&#22791;&#25512;&#26029;&#23454;&#26102;&#35745;&#25968;&#20581;&#36523;&#21160;&#20316;&#30340;&#21487;&#34892;&#24615;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24191;&#27867;&#27010;&#36848;&#20102;&#33258;&#21160;&#20581;&#36523;&#21160;&#20316;&#37325;&#22797;&#35745;&#25968;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#65292;&#28982;&#21518;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26234;&#33021;&#25163;&#26426;&#20581;&#36523;&#21160;&#20316;&#37325;&#22797;&#35745;&#25968;&#31995;&#32479;&#65292;&#21253;&#25324;&#20116;&#20010;&#32452;&#20214;&#65306;&#65288;1&#65289;&#23039;&#21183;&#20272;&#35745;&#65292;&#65288;2&#65289;&#38408;&#20540;&#20998;&#21106;&#65292;&#65288;3&#65289;&#20809;&#27969;&#65292;&#65288;4&#65289;&#29366;&#24577;&#26426;&#21644;&#65288;5&#65289;&#35745;&#25968;&#22120;&#12290;&#28982;&#21518;&#65292;&#35813;&#31995;&#32479;&#36890;&#36807;&#19968;&#27454;&#36328;&#24179;&#21488;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;P=&#20027;ioio&#23454;&#29616;&#65292;&#22312;&#23454;&#26102;&#36319;&#36394;&#26631;&#20934;&#20581;&#36523;&#21160;&#20316;&#65288;&#28145;&#36466;&#12289;&#20463;&#21351;&#25745;&#21644;&#24341;&#20307;&#21521;&#19978;&#65289;&#30340;&#36807;&#31243;&#20013;&#21482;&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#25668;&#20687;&#22836;&#12290;&#36890;&#36807;&#23545;&#39044;&#20808;&#24405;&#21046;&#30340;&#20010;&#20307;&#38203;&#28860;&#35270;&#39057;&#36827;&#34892;&#25968;&#25454;&#38598;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated exercise repetition counting has applications across the physical fitness realm, from personal health to rehabilitation. Motivated by the ubiquity of mobile phones and the benefits of tracking physical activity, this study explored the feasibility of counting exercise repetitions in real-time, using only on-device inference, on smartphones. In this work, after providing an extensive overview of the state-of-the-art automatic exercise repetition counting methods, we introduce a deep learning based exercise repetition counting system for smartphones consisting of five components: (1) Pose estimation, (2) Thresholding, (3) Optical flow, (4) State machine, and (5) Counter. The system is then implemented via a cross-platform mobile application named P\=uioio that uses only the smartphone camera to track repetitions in real time for three standard exercises: Squats, Push-ups, and Pull-ups. The proposed system was evaluated via a dataset of pre-recorded videos of individuals exercis
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31359;&#25140;&#35774;&#22791;&#30340;RSSI&#21644;&#21152;&#36895;&#24230;&#35745;&#25968;&#25454;&#36827;&#34892;&#23460;&#20869;&#23450;&#20301;&#65292;&#20197;&#25913;&#21892;&#24403;&#21069;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#23460;&#20869;&#23450;&#20301;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#26816;&#27979;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#30340;&#33647;&#29289;&#20351;&#29992;&#24773;&#20917;&#26469;&#35780;&#20272;&#36816;&#21160;&#27874;&#21160;&#12290;</title><link>http://arxiv.org/abs/2308.02419</link><description>&lt;p&gt;
&#22312;&#24085;&#37329;&#26862;&#30149;&#20013;&#29992;&#20110;&#26816;&#27979;&#33647;&#29289;&#20351;&#29992;&#30340;&#22810;&#27169;&#24577;&#23460;&#20869;&#23450;&#20301;&#65306;&#22312;&#33258;&#30001;&#29983;&#27963;&#29615;&#22659;&#20013;&#30340;&#35266;&#23519;&#24615;&#35797;&#39564;
&lt;/p&gt;
&lt;p&gt;
Multimodal Indoor Localisation in Parkinson's Disease for Detecting Medication Use: Observational Pilot Study in a Free-Living Setting. (arXiv:2308.02419v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02419
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31359;&#25140;&#35774;&#22791;&#30340;RSSI&#21644;&#21152;&#36895;&#24230;&#35745;&#25968;&#25454;&#36827;&#34892;&#23460;&#20869;&#23450;&#20301;&#65292;&#20197;&#25913;&#21892;&#24403;&#21069;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#23460;&#20869;&#23450;&#20301;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#26816;&#27979;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#30340;&#33647;&#29289;&#20351;&#29992;&#24773;&#20917;&#26469;&#35780;&#20272;&#36816;&#21160;&#27874;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24085;&#37329;&#26862;&#30149;&#65288;PD&#65289;&#26159;&#19968;&#31181;&#32531;&#24930;&#36827;&#23637;&#30340;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#65292;&#23548;&#33268;&#21253;&#25324;&#27493;&#24577;&#38556;&#30861;&#22312;&#20869;&#30340;&#36816;&#21160;&#30151;&#29366;&#12290;&#36816;&#21160;&#27874;&#21160;&#26159;&#25351;&#22312;&#24038;&#22810;&#24052;&#30103;&#27861;&#65288;&#8220;&#24320;&#8221;&#65289;&#21644;PD&#30151;&#29366;&#20877;&#24230;&#20986;&#29616;&#65288;&#8220;&#20851;&#8221;&#65289;&#20043;&#38388;&#30340;&#21464;&#21270;&#65292;&#22240;&#33647;&#29289;&#25928;&#26524;&#20943;&#36864;&#32780;&#24341;&#36215;&#12290;&#36825;&#20123;&#27874;&#21160;&#32463;&#24120;&#24433;&#21709;&#27493;&#24577;&#36895;&#24230;&#65292;&#24182;&#38543;&#30528;PD&#36827;&#23637;&#32780;&#22686;&#21152;&#20854;&#33268;&#27531;&#24433;&#21709;&#12290;&#20026;&#20102;&#25552;&#39640;&#24403;&#21069;&#23460;&#20869;&#23450;&#20301;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#25351;&#31034;&#22120;&#65288;RSSI&#65289;&#21644;&#21152;&#36895;&#24230;&#35745;&#25968;&#25454;&#25552;&#20379;&#20114;&#34917;&#30340;&#36816;&#21160;&#35270;&#35282;&#12290;&#19968;&#20010;&#27425;&#30446;&#26631;&#26088;&#22312;&#35780;&#20272;&#23460;&#20869;&#23450;&#20301;&#65292;&#21253;&#25324;&#20854;&#23478;&#24237;&#27493;&#24577;&#36895;&#24230;&#29305;&#24449;&#65288;&#21363;&#22312;&#25151;&#38388;&#20043;&#38388;&#34892;&#36208;&#25152;&#38656;&#30340;&#26102;&#38388;&#65289;&#65292;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#36890;&#36807;&#26816;&#27979;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#26159;&#21542;&#27491;&#22312;&#20351;&#29992;&#24038;&#22810;&#24052;&#33647;&#29289;&#25110;&#24739;&#32773;&#26159;&#21542;&#20572;&#29992;&#33647;&#29289;&#26469;&#35780;&#20272;&#36816;&#21160;&#27874;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parkinson's disease (PD) is a slowly progressive, debilitating neurodegenerative disease which causes motor symptoms including gait dysfunction. Motor fluctuations are alterations between periods with a positive response to levodopa therapy ("on") and periods marked by re-emergency of PD symptoms ("off") as the response to medication wears off. These fluctuations often affect gait speed and they increase in their disabling impact as PD progresses. To improve the effectiveness of current indoor localisation methods, a transformer-based approach utilising dual modalities which provide complementary views of movement, Received Signal Strength Indicator (RSSI) and accelerometer data from wearable devices, is proposed. A sub-objective aims to evaluate whether indoor localisation, including its in-home gait speed features (i.e. the time taken to walk between rooms), could be used to evaluate motor fluctuations by detecting whether the person with PD is taking levodopa medications or withhold
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#26102;&#20248;&#21270;&#30701;&#26102;&#20613;&#37324;&#21494;&#21464;&#25442;&#65288;STFT&#65289;&#30340;&#24103;&#38388;&#21644;&#39057;&#29575;&#38388;&#31383;&#38271;&#12290;&#29983;&#25104;&#30340;&#21487;&#24494;&#20998;&#33258;&#36866;&#24212;STFT&#20855;&#26377;&#20986;&#33394;&#30340;&#29305;&#24615;&#65292;&#21487;&#20197;&#36866;&#24212;&#30636;&#24577;&#21644;&#31283;&#24577;&#25104;&#20998;&#65292;&#24182;&#19988;&#26131;&#20110;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.02418</link><description>&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#30340;&#21487;&#24494;&#20998;&#33258;&#36866;&#24212;&#30701;&#26102;&#20613;&#37324;&#21494;&#21464;&#25442;
&lt;/p&gt;
&lt;p&gt;
Differentiable adaptive short-time Fourier transform with respect to the window length. (arXiv:2308.02418v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#26102;&#20248;&#21270;&#30701;&#26102;&#20613;&#37324;&#21494;&#21464;&#25442;&#65288;STFT&#65289;&#30340;&#24103;&#38388;&#21644;&#39057;&#29575;&#38388;&#31383;&#38271;&#12290;&#29983;&#25104;&#30340;&#21487;&#24494;&#20998;&#33258;&#36866;&#24212;STFT&#20855;&#26377;&#20986;&#33394;&#30340;&#29305;&#24615;&#65292;&#21487;&#20197;&#36866;&#24212;&#30636;&#24577;&#21644;&#31283;&#24577;&#25104;&#20998;&#65292;&#24182;&#19988;&#26131;&#20110;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#26102;&#20248;&#21270;&#30701;&#26102;&#20613;&#37324;&#21494;&#21464;&#25442;&#65288;STFT&#65289;&#30340;&#24103;&#38388;&#21644;&#39057;&#29575;&#38388;&#31383;&#38271;&#65292;&#19982;&#25105;&#20204;&#20043;&#21069;&#24320;&#21457;&#30340;&#23558;&#31383;&#38271;&#21464;&#20026;&#36830;&#32493;&#21442;&#25968;&#30340;&#21487;&#24494;&#20998;&#29256;&#26412;&#30340;STFT&#30456;&#20851;&#12290;&#29983;&#25104;&#30340;&#21487;&#24494;&#20998;&#33258;&#36866;&#24212;STFT&#20855;&#26377;&#20986;&#33394;&#30340;&#29305;&#24615;&#65292;&#20363;&#22914;&#22312;&#21516;&#19968;&#26102;&#39057;&#34920;&#31034;&#20013;&#36866;&#24212;&#30636;&#24577;&#21644;&#31283;&#24577;&#25104;&#20998;&#65292;&#21516;&#26102;&#21487;&#20197;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#36731;&#26494;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#22312;&#25391;&#21160;&#20998;&#26512;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a gradient-based method for on-the-fly optimization for both per-frame and per-frequency window length of the short-time Fourier transform (STFT), related to previous work in which we developed a differentiable version of STFT by making the window length a continuous parameter. The resulting differentiable adaptive STFT possesses commendable properties, such as the ability to adapt in the same time-frequency representation to both transient and stationary components, while being easily optimized by gradient descent. We validate the performance of our method in vibration analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26412;&#22320;-&#20840;&#23616;&#26102;&#38388;&#34701;&#21512;&#32593;&#32476;&#32467;&#21512;&#27880;&#24847;&#26426;&#21046;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#38024;&#23545;&#24515;&#24459;&#22833;&#24120;&#30340;&#26816;&#27979;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#21462;&#26412;&#22320;&#26102;&#38388;&#20449;&#24687;&#21644;&#20840;&#23616;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#26412;&#22320;-&#20840;&#23616;&#20449;&#24687;&#34701;&#21512;&#65292;&#20197;&#22788;&#29702;&#38271;&#24230;&#21464;&#21270;&#30340;&#24515;&#24459;&#22833;&#24120;&#25968;&#25454;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27492;&#26041;&#27861;&#22312;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02416</link><description>&lt;p&gt;
&#26412;&#22320;-&#20840;&#23616;&#26102;&#38388;&#34701;&#21512;&#32593;&#32476;&#32467;&#21512;&#27880;&#24847;&#26426;&#21046;&#29992;&#20110;&#22810;&#31867;&#21644;&#22810;&#31181;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Local-Global Temporal Fusion Network with an Attention Mechanism for Multiple and Multiclass Arrhythmia Classification. (arXiv:2308.02416v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26412;&#22320;-&#20840;&#23616;&#26102;&#38388;&#34701;&#21512;&#32593;&#32476;&#32467;&#21512;&#27880;&#24847;&#26426;&#21046;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#38024;&#23545;&#24515;&#24459;&#22833;&#24120;&#30340;&#26816;&#27979;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#21462;&#26412;&#22320;&#26102;&#38388;&#20449;&#24687;&#21644;&#20840;&#23616;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#26412;&#22320;-&#20840;&#23616;&#20449;&#24687;&#34701;&#21512;&#65292;&#20197;&#22788;&#29702;&#38271;&#24230;&#21464;&#21270;&#30340;&#24515;&#24459;&#22833;&#24120;&#25968;&#25454;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27492;&#26041;&#27861;&#22312;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65288;CDSSs&#65289;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#25903;&#25345;&#24515;&#30005;&#22270;&#65288;ECGs&#65289;&#20013;&#24515;&#34880;&#31649;&#31185;&#21307;&#29983;&#26816;&#27979;&#21644;&#20998;&#31867;&#24515;&#24459;&#22833;&#24120;&#26102;&#25152;&#20570;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24515;&#24459;&#22833;&#24120;&#38271;&#24230;&#30340;&#21464;&#21270;&#65292;&#24418;&#25104;&#19968;&#20010;&#38024;&#23545;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#20219;&#21153;&#30340;CDSS&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#34429;&#28982;&#24515;&#24459;&#22833;&#24120;&#30340;&#21457;&#20316;&#26102;&#38388;&#26159;&#21464;&#21270;&#30340;&#65292;&#20294;&#20043;&#21069;&#24320;&#21457;&#30340;&#26041;&#27861;&#27809;&#26377;&#32771;&#34385;&#21040;&#36825;&#20123;&#26465;&#20214;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21253;&#25324;&#65288;i&#65289;&#26412;&#22320;&#26102;&#38388;&#20449;&#24687;&#25552;&#21462;&#65292;&#65288;ii&#65289;&#20840;&#23616;&#27169;&#24335;&#25552;&#21462;&#65292;&#21644;&#65288;iii&#65289;&#24102;&#26377;&#27880;&#24847;&#21147;&#30340;&#26412;&#22320;-&#20840;&#23616;&#20449;&#24687;&#34701;&#21512;&#65292;&#20197;&#23454;&#29616;&#23545;&#26377;&#38480;&#36755;&#20837;&#38271;&#24230;&#30340;&#24515;&#24459;&#22833;&#24120;&#26816;&#27979;&#21644;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26816;&#27979;&#24515;&#24459;&#22833;&#24120;&#36215;&#22987;&#21644;&#32467;&#26463;&#26102;&#38388;&#20316;&#20026;&#19968;&#20010;&#20107;&#20214;&#65292;&#20197;&#21450;&#22522;&#20110;MIT-BIH&#24515;&#24459;&#22833;&#24120;&#25968;&#25454;&#24211;&#65288;MITDB&#65289;&#21644;MIT-BIH&#25151;&#39076;&#25968;&#25454;&#24211;&#65288;AFDB&#65289;&#30340;&#24515;&#24459;&#22833;&#24120;&#25345;&#32493;&#26102;&#38388;&#26469;&#35780;&#20272;10&#31867;&#21644;4&#31867;&#34920;&#29616;&#12290;&#32467;&#26524;&#22312;&#32479;&#35745;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical decision support systems (CDSSs) have been widely utilized to support the decisions made by cardiologists when detecting and classifying arrhythmia from electrocardiograms (ECGs). However, forming a CDSS for the arrhythmia classification task is challenging due to the varying lengths of arrhythmias. Although the onset time of arrhythmia varies, previously developed methods have not considered such conditions. Thus, we propose a framework that consists of (i) local temporal information extraction, (ii) global pattern extraction, and (iii) local-global information fusion with attention to perform arrhythmia detection and classification with a constrained input length. The 10-class and 4-class performances of our approach were assessed by detecting the onset and offset of arrhythmia as an episode and the duration of arrhythmia based on the MIT-BIH arrhythmia database (MITDB) and MIT-BIH atrial fibrillation database (AFDB), respectively. The results were statistically superior to 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#22522;&#20110;WiFi CSI&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#36827;&#34892;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#20013;&#25968;&#25454;&#19981;&#36275;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#19982;CSI&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#26080;&#38656;&#19987;&#23478;&#30693;&#35782;&#12290;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#31867;&#22411;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#30340;&#28508;&#21147;&#65292;&#23545;&#32570;&#20047;&#26631;&#35760;CSI&#25968;&#25454;&#30340;&#25361;&#25112;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25972;&#29702;&#21644;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2308.02412</link><description>&lt;p&gt;
&#22522;&#20110;WiFi CSI&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#65306;&#31995;&#32479;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning for WiFi CSI-Based Human Activity Recognition: A Systematic Study. (arXiv:2308.02412v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#22522;&#20110;WiFi CSI&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#36827;&#34892;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#20013;&#25968;&#25454;&#19981;&#36275;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#19982;CSI&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#26080;&#38656;&#19987;&#23478;&#30693;&#35782;&#12290;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#31867;&#22411;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#30340;&#28508;&#21147;&#65292;&#23545;&#32570;&#20047;&#26631;&#35760;CSI&#25968;&#25454;&#30340;&#25361;&#25112;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25972;&#29702;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38543;&#30528;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#30340;&#21457;&#23637;&#65292;&#22522;&#20110;WiFi CSI&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;(HAR)&#24341;&#36215;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#36890;&#36807;&#23558;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#19982;&#22522;&#20110;CSI&#30340;HAR&#30456;&#32467;&#21512;&#65292;&#30740;&#31350;&#20154;&#21592;&#22312;&#19981;&#38656;&#35201;&#19987;&#23478;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#22522;&#20110;CSI&#30340;HAR&#30340;&#32972;&#26223;&#19979;&#65292;&#26631;&#35760;CSI&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#20173;&#28982;&#26159;&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26102;&#26368;&#31361;&#20986;&#30340;&#25361;&#25112;&#65292;&#36825;&#26159;&#30001;&#20110;CSI&#25968;&#25454;&#30340;&#38544;&#31169;&#24615;&#21644;&#19981;&#21487;&#29702;&#35299;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20316;&#20026;&#19968;&#31181;&#22312;&#19981;&#37325;&#35270;&#26631;&#35760;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#26377;&#24847;&#20041;&#34920;&#31034;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#24050;&#32463;&#20986;&#29616;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#21033;&#29992;SSL&#31639;&#27861;&#26469;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#20013;&#25968;&#25454;&#19981;&#36275;&#30340;&#25361;&#25112;&#24050;&#32463;&#20570;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;SSL&#31639;&#27861;&#30340;&#28508;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25972;&#29702;&#21644;&#20998;&#26512;&#65292;&#21253;&#25324;&#20197;&#21069;&#30740;&#31350;&#36807;&#30340;&#31639;&#27861;&#21644;&#37027;&#20123;&#36824;&#27809;&#26377;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, with the advancement of the Internet of Things (IoT), WiFi CSI-based HAR has gained increasing attention from academic and industry communities. By integrating the deep learning technology with CSI-based HAR, researchers achieve state-of-the-art performance without the need of expert knowledge. However, the scarcity of labeled CSI data remains the most prominent challenge when applying deep learning models in the context of CSI-based HAR due to the privacy and incomprehensibility of CSI-based HAR data. On the other hand, SSL has emerged as a promising approach for learning meaningful representations from data without heavy reliance on labeled examples. Therefore, considerable efforts have been made to address the challenge of insufficient data in deep learning by leveraging SSL algorithms. In this paper, we undertake a comprehensive inventory and analysis of the potential held by different categories of SSL algorithms, including those that have been previously studied and tho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23556;&#39057;&#35782;&#21035;&#21644;&#22810;&#31181;&#29289;&#32852;&#32593;&#26080;&#32447;&#25216;&#26415;&#30340;&#28151;&#21512;&#23460;&#20869;&#23450;&#20301;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#22312;&#21508;&#21306;&#22359;&#36793;&#30028;&#23433;&#35013;RFID&#26631;&#31614;&#26469;&#38477;&#20302;&#25104;&#26412;&#65292;&#24182;&#36890;&#36807;&#19981;&#21516;&#26080;&#32447;&#25216;&#26415;&#30340;&#32447;&#24615;&#20301;&#32622;&#20272;&#35745;&#23454;&#29616;&#20102;&#23460;&#20869;&#29289;&#20307;&#23450;&#20301;&#12290;</title><link>http://arxiv.org/abs/2308.02410</link><description>&lt;p&gt;
&#37319;&#29992;&#28151;&#21512;&#26080;&#32447;&#25968;&#25454;&#34701;&#21512;&#30340;RFID&#36741;&#21161;&#23460;&#20869;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
RFID-Assisted Indoor Localization Using Hybrid Wireless Data Fusion. (arXiv:2308.02410v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23556;&#39057;&#35782;&#21035;&#21644;&#22810;&#31181;&#29289;&#32852;&#32593;&#26080;&#32447;&#25216;&#26415;&#30340;&#28151;&#21512;&#23460;&#20869;&#23450;&#20301;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#22312;&#21508;&#21306;&#22359;&#36793;&#30028;&#23433;&#35013;RFID&#26631;&#31614;&#26469;&#38477;&#20302;&#25104;&#26412;&#65292;&#24182;&#36890;&#36807;&#19981;&#21516;&#26080;&#32447;&#25216;&#26415;&#30340;&#32447;&#24615;&#20301;&#32622;&#20272;&#35745;&#23454;&#29616;&#20102;&#23460;&#20869;&#29289;&#20307;&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#32447;&#23450;&#20301;&#22312;&#23460;&#20869;&#29615;&#22659;&#20013;&#36319;&#36394;&#29289;&#20307;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#29289;&#32852;&#32593;&#36890;&#36807;&#20854;&#22810;&#26679;&#30340;&#26080;&#32447;&#36890;&#20449;&#21327;&#35758;&#23454;&#29616;&#23450;&#20301;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#24320;&#21457;&#30340;&#23556;&#39057;&#35782;&#21035;&#65288;RFID&#65289;&#36319;&#36394;&#35774;&#22791;&#21644;&#22810;&#20010;&#29289;&#32852;&#32593;&#26080;&#32447;&#25216;&#26415;&#30340;&#28151;&#21512;&#22522;&#20110;&#21306;&#22359;&#30340;&#23460;&#20869;&#23450;&#20301;&#26041;&#27861;&#12290;&#20026;&#20102;&#38477;&#20302;RFID&#26631;&#31614;&#30340;&#25104;&#26412;&#65292;&#26631;&#31614;&#20165;&#23433;&#35013;&#22312;&#27599;&#20010;&#21306;&#22359;&#30340;&#36793;&#30028;&#19978;&#12290;RFID&#36319;&#36394;&#35774;&#22791;&#35782;&#21035;&#21306;&#22359;&#65292;&#25552;&#20986;&#30340;&#28151;&#21512;&#26041;&#27861;&#25214;&#21040;&#21306;&#22359;&#20869;&#29289;&#20307;&#30340;&#20301;&#32622;&#12290;&#35813;&#28151;&#21512;&#26041;&#27861;&#36890;&#36807;&#19981;&#21516;&#29289;&#32852;&#32593;&#26080;&#32447;&#25216;&#26415;&#33719;&#24471;&#30340;&#32447;&#24615;&#20301;&#32622;&#20272;&#35745;&#36827;&#34892;&#20998;&#26512;&#12290;&#20351;&#29992;&#24320;&#21457;&#30340;RFID&#36319;&#36394;&#35774;&#22791;&#21644;&#22522;&#20110;RSSI&#30340;&#34013;&#29273;&#12289;WiFi&#21644;ZigBee&#25216;&#26415;&#30340;&#23450;&#20301;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#20998;&#26512;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wireless localization is essential for tracking objects in indoor environments. Internet of Things (IoT) enables localization through its diverse wireless communication protocols. In this paper, a hybrid section-based indoor localization method using a developed Radio Frequency Identification (RFID) tracking device and multiple IoT wireless technologies is proposed. In order to reduce the cost of the RFID tags, the tags are installed only on the borders of each section. The RFID tracking device identifies the section, and the proposed wireless hybrid method finds the location of the object inside the section. The proposed hybrid method is analytically driven by linear location estimates obtained from different IoT wireless technologies. The experimental results using developed RFID tracking device and RSSI-based localization for Bluetooth, WiFi and ZigBee technologies verifies the analytical results.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#34701;&#21512;&#22810;&#20010;&#31354;&#38388;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#33041;&#30005;&#20449;&#21495;&#23545;&#24515;&#29702;&#36127;&#33655;&#36827;&#34892;&#20998;&#31867;&#21644;&#20272;&#35745;&#36830;&#32493;&#32423;&#21035;&#12290;&#22312;&#26102;&#38388;&#22495;&#20013;&#20351;&#29992;&#20102;&#26102;&#24577;&#21367;&#31215;&#32593;&#32476;&#65292;&#32780;&#22312;&#39057;&#29575;&#22495;&#20013;&#24341;&#20837;&#20102;&#26032;&#30340;&#26550;&#26500;&#8212;&#8212;&#22810;&#32500;&#27531;&#24046;&#22359;&#12290;</title><link>http://arxiv.org/abs/2308.02409</link><description>&lt;p&gt;
&#36890;&#36807;&#34701;&#21512;&#22810;&#31354;&#38388;&#28145;&#24230;&#27169;&#22411;&#30340;&#33041;&#30005;&#20449;&#21495;&#26469;&#20272;&#35745;&#24515;&#29702;&#36127;&#33655;
&lt;/p&gt;
&lt;p&gt;
Mental Workload Estimation with Electroencephalogram Signals by Combining Multi-Space Deep Models. (arXiv:2308.02409v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02409
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#34701;&#21512;&#22810;&#20010;&#31354;&#38388;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#33041;&#30005;&#20449;&#21495;&#23545;&#24515;&#29702;&#36127;&#33655;&#36827;&#34892;&#20998;&#31867;&#21644;&#20272;&#35745;&#36830;&#32493;&#32423;&#21035;&#12290;&#22312;&#26102;&#38388;&#22495;&#20013;&#20351;&#29992;&#20102;&#26102;&#24577;&#21367;&#31215;&#32593;&#32476;&#65292;&#32780;&#22312;&#39057;&#29575;&#22495;&#20013;&#24341;&#20837;&#20102;&#26032;&#30340;&#26550;&#26500;&#8212;&#8212;&#22810;&#32500;&#27531;&#24046;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33041;&#22312;&#24037;&#20316;&#21644;&#20241;&#24687;&#26102;&#37117;&#22788;&#20110;&#25345;&#32493;&#27963;&#21160;&#30340;&#29366;&#24577;&#12290;&#24515;&#29702;&#27963;&#21160;&#26159;&#26085;&#24120;&#36807;&#31243;&#20013;&#30340;&#19968;&#37096;&#20998;&#65292;&#24403;&#22823;&#33041;&#36807;&#24230;&#21171;&#32047;&#26102;&#65292;&#20250;&#23545;&#20154;&#20307;&#20581;&#24247;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#20110;&#26089;&#26399;&#26816;&#27979;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#30340;&#37325;&#35270;&#36880;&#28176;&#22686;&#21152;&#65292;&#22240;&#20026;&#36825;&#21487;&#20197;&#24110;&#21161;&#39044;&#38450;&#20005;&#37325;&#30340;&#20581;&#24247;&#38382;&#39064;&#65292;&#24182;&#25913;&#21892;&#29983;&#27963;&#36136;&#37327;&#12290;&#22810;&#31181;&#20449;&#21495;&#34987;&#29992;&#20110;&#35780;&#20272;&#24515;&#29702;&#29366;&#24577;&#65292;&#20294;&#30001;&#20110;&#22823;&#37327;&#25552;&#20379;&#20851;&#20110;&#22823;&#33041;&#20449;&#24687;&#30340;&#29305;&#28857;&#65292;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#34987;&#30740;&#31350;&#20154;&#21592;&#24191;&#27867;&#20351;&#29992;&#12290;&#26412;&#25991;&#26088;&#22312;&#23558;&#24515;&#29702;&#36127;&#33655;&#20998;&#20026;&#19977;&#31181;&#29366;&#24577;&#24182;&#20272;&#35745;&#36830;&#32493;&#32423;&#21035;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#34701;&#21512;&#22810;&#20010;&#31354;&#38388;&#32500;&#24230;&#26469;&#23454;&#29616;&#26368;&#20339;&#30340;&#24515;&#29702;&#20272;&#35745;&#32467;&#26524;&#12290;&#22312;&#26102;&#38388;&#22495;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26102;&#24577;&#21367;&#31215;&#32593;&#32476;&#65292;&#32780;&#22312;&#39057;&#29575;&#22495;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#32500;&#27531;&#24046;&#22359;&#30340;&#26032;&#26550;&#26500;&#65292;&#23427;&#32467;&#21512;&#20102;&#27531;&#24046;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
The human brain is in a continuous state of activity during both work and rest. Mental activity is a daily process, and when the brain is overworked, it can have negative effects on human health. In recent years, great attention has been paid to early detection of mental health problems because it can help prevent serious health problems and improve quality of life. Several signals are used to assess mental state, but the electroencephalogram (EEG) is widely used by researchers because of the large amount of information it provides about the brain. This paper aims to classify mental workload into three states and estimate continuum levels. Our method combines multiple dimensions of space to achieve the best results for mental estimation. In the time domain approach, we use Temporal Convolutional Networks, and in the frequency domain, we propose a new architecture called the Multi-Dimensional Residual Block, which combines residual blocks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#25506;&#32034;&#20102;&#22312;&#33041;&#30005;&#22270;&#35299;&#30721;&#20219;&#21153;&#20013;&#65292;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#28145;&#24230;&#23398;&#20064;&#34920;&#24449;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#21363;&#20351;&#26159;&#32447;&#24615;&#25506;&#27979;&#36716;&#31227;&#65292;&#20063;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35299;&#30721;&#24615;&#33021;&#65292;&#30456;&#36739;&#20110;&#32431;&#30417;&#30563;&#26041;&#27861;&#65292;&#25913;&#36827;&#24133;&#24230;&#39640;&#36798;28%&#12290;</title><link>http://arxiv.org/abs/2308.02408</link><description>&lt;p&gt;
&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#35780;&#20272;&#35748;&#30693;&#20219;&#21153;&#30340;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Evaluating the structure of cognitive tasks with transfer learning. (arXiv:2308.02408v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#25506;&#32034;&#20102;&#22312;&#33041;&#30005;&#22270;&#35299;&#30721;&#20219;&#21153;&#20013;&#65292;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#28145;&#24230;&#23398;&#20064;&#34920;&#24449;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#21363;&#20351;&#26159;&#32447;&#24615;&#25506;&#27979;&#36716;&#31227;&#65292;&#20063;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35299;&#30721;&#24615;&#33021;&#65292;&#30456;&#36739;&#20110;&#32431;&#30417;&#30563;&#26041;&#27861;&#65292;&#25913;&#36827;&#24133;&#24230;&#39640;&#36798;28%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26631;&#35760;&#25968;&#25454;&#26377;&#38480;&#65292;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#35299;&#30721;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#36801;&#31227;&#23398;&#20064;&#26159;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#25216;&#26415;&#65292;&#20294;&#23427;&#20551;&#35774;&#21487;&#20256;&#36882;&#30340;&#25968;&#25454;&#39046;&#22495;&#21644;&#20219;&#21153;&#26159;&#24050;&#30693;&#30340;&#65292;&#32780;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#24182;&#38750;&#22914;&#27492;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#19981;&#21516;EEG&#35299;&#30721;&#20219;&#21153;&#20043;&#38388;&#28145;&#24230;&#23398;&#20064;&#34920;&#31034;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#35299;&#30721;&#27169;&#22411;&#23545;&#26368;&#36817;&#21457;&#24067;&#30340;ERP CORE&#21644;M$^3$CV&#20004;&#20010;EEG&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#21253;&#21547;&#36229;&#36807;140&#20010;&#21463;&#35797;&#32773;&#21644;11&#20010;&#19981;&#21516;&#30340;&#35748;&#30693;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#19968;&#20010;&#20219;&#21153;&#19978;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#35299;&#30721;&#21518;&#32493;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#26469;&#34913;&#37327;&#23398;&#21040;&#30340;&#34920;&#31034;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#32447;&#24615;&#25506;&#27979;&#36716;&#31227;&#65292;&#20063;&#21487;&#20197;&#33719;&#24471;&#26174;&#33879;&#30340;&#35299;&#30721;&#24615;&#33021;&#25913;&#36827;&#65292;&#30456;&#36739;&#20110;&#32431;&#30417;&#30563;&#26041;&#27861;&#65292;&#25913;&#36827;&#24133;&#24230;&#39640;&#36798;28%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#35777;&#25454;&#35777;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Electroencephalography (EEG) decoding is a challenging task due to the limited availability of labelled data. While transfer learning is a promising technique to address this challenge, it assumes that transferable data domains and task are known, which is not the case in this setting. This study investigates the transferability of deep learning representations between different EEG decoding tasks. We conduct extensive experiments using state-of-the-art decoding models on two recently released EEG datasets, ERP CORE and M$^3$CV, containing over 140 subjects and 11 distinct cognitive tasks. We measure the transferability of learned representations by pre-training deep neural networks on one task and assessing their ability to decode subsequent tasks. Our experiments demonstrate that, even with linear probing transfer, significant improvements in decoding performance can be obtained, with gains of up to 28% compare with the pure supervised approach. Additionally, we discover evidence tha
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#27169;&#25311;&#24773;&#20917;&#19979;&#30340;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#65292;&#30740;&#31350;&#20102;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#20013;&#31934;&#24230;&#21644;&#30828;&#20214;&#36164;&#28304;&#20043;&#38388;&#30340;&#25240;&#34935;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;IMU&#24863;&#27979;&#36827;&#34892;&#39640;&#25928;&#20934;&#30830;&#30340;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02397</link><description>&lt;p&gt;
&#20174;&#31232;&#30095;IMU&#24863;&#27979;&#20013;&#39640;&#25928;&#20934;&#30830;&#22320;&#36827;&#34892;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#30340;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Design Space Exploration on Efficient and Accurate Human Pose Estimation from Sparse IMU-Sensing. (arXiv:2308.02397v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#27169;&#25311;&#24773;&#20917;&#19979;&#30340;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#65292;&#30740;&#31350;&#20102;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#20013;&#31934;&#24230;&#21644;&#30828;&#20214;&#36164;&#28304;&#20043;&#38388;&#30340;&#25240;&#34935;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;IMU&#24863;&#27979;&#36827;&#34892;&#39640;&#25928;&#20934;&#30830;&#30340;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#25935;&#24863;&#30340;&#20010;&#20154;&#25968;&#25454;&#65292;&#23545;&#20110;&#35780;&#20272;&#36816;&#21160;&#12289;&#24247;&#22797;&#25110;&#24037;&#20316;&#23433;&#20840;&#20013;&#30340;&#20154;&#20307;&#36816;&#21160;&#30340;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#65288;HPE&#65289;&#65292;&#38656;&#35201;&#20934;&#30830;&#30340;&#24863;&#27979;&#12290;&#22240;&#27492;&#65292;&#26412;&#22320;&#22788;&#29702;&#26159;&#24517;&#35201;&#30340;&#65292;&#24182;&#19988;&#22312;&#27492;&#31867;&#31995;&#32479;&#20013;&#65292;&#26377;&#38480;&#30340;&#33021;&#28304;&#39044;&#31639;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#65288;IMU&#65289;&#32780;&#19981;&#26159;&#24120;&#35265;&#30340;&#30456;&#26426;&#24863;&#27979;&#26469;&#35299;&#20915;&#12290;&#30740;&#31350;&#20013;&#24456;&#23569;&#35752;&#35770;&#31934;&#24230;&#21644;&#30828;&#20214;&#36164;&#28304;&#30340;&#26377;&#25928;&#21033;&#29992;&#20043;&#38388;&#30340;&#26680;&#24515;&#25240;&#34935;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#24773;&#20917;&#19979;&#30340;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#26469;&#35299;&#20915;&#36825;&#20010;&#25240;&#34935;&#38382;&#39064;&#65292;&#23427;&#28041;&#21450;&#19981;&#21516;&#30340;IMU&#20256;&#24863;&#22120;&#25968;&#37327;&#21644;&#25670;&#25918;&#20301;&#32622;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;&#20154;&#20307;&#27169;&#22411;&#25968;&#25454;&#38598;&#20026;&#19981;&#21516;&#30340;&#20256;&#24863;&#22120;&#37197;&#32622;&#29983;&#25104;IMU&#25968;&#25454;&#65292;&#24182;&#29992;&#36825;&#20123;&#25968;&#25454;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32452;&#21512;&#24230;&#37327;&#26469;&#35780;&#20272;&#31934;&#24230;&#21644;&#36164;&#28304;&#21033;&#29992;&#30340;&#25240;&#34935;&#12290;&#25105;&#20204;&#23558;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#29992;&#20316;&#35780;&#20272;&#20256;&#24863;&#22120;&#37197;&#32622;&#24182;&#30830;&#23450;&#29305;&#23450;&#29992;&#20363;&#30340;&#26377;&#30410;&#37197;&#32622;&#30340;&#24037;&#20855;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#19968;&#20010;&#21516;&#31561;&#37325;&#35201;&#24615;&#30340;&#31995;&#32479;...
&lt;/p&gt;
&lt;p&gt;
Human Pose Estimation (HPE) to assess human motion in sports, rehabilitation or work safety requires accurate sensing without compromising the sensitive underlying personal data. Therefore, local processing is necessary and the limited energy budget in such systems can be addressed by Inertial Measurement Units (IMU) instead of common camera sensing. The central trade-off between accuracy and efficient use of hardware resources is rarely discussed in research. We address this trade-off by a simulative Design Space Exploration (DSE) of a varying quantity and positioning of IMU-sensors. First, we generate IMU-data from a publicly available body model dataset for different sensor configurations and train a deep learning model with this data. Additionally, we propose a combined metric to assess the accuracy-resource trade-off. We used the DSE as a tool to evaluate sensor configurations and identify beneficial ones for a specific use case. Exemplary, for a system with equal importance of ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HOOD&#30340;&#23454;&#26102;&#31283;&#20581;&#30340;&#20154;&#21592;&#23384;&#22312;&#21644;&#31163;&#32676;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20302;&#25104;&#26412;&#30340;60 GHz FMCW&#38647;&#36798;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#21487;&#21516;&#26102;&#35299;&#20915;&#23384;&#22312;&#26816;&#27979;&#21644;&#31163;&#32676;&#26816;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#37325;&#26500;&#26550;&#26500;&#21644;&#38647;&#36798;&#22270;&#20687;&#23454;&#29616;&#20934;&#30830;&#26816;&#27979;&#20154;&#31867;&#23384;&#22312;&#65292;&#21516;&#26102;&#22312;&#20154;&#31867;&#19981;&#23384;&#22312;&#26102;&#26816;&#27979;&#31227;&#21160;&#25110;&#38745;&#27490;&#24178;&#25200;&#29289;&#12290;</title><link>http://arxiv.org/abs/2308.02396</link><description>&lt;p&gt;
HOOD: &#23454;&#26102;&#31283;&#20581;&#30340;&#20302;&#25104;&#26412;FMCW&#38647;&#36798;&#20154;&#21592;&#23384;&#22312;&#21644;&#31163;&#32676;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
HOOD: Real-Time Robust Human Presence and Out-of-Distribution Detection with Low-Cost FMCW Radar. (arXiv:2308.02396v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HOOD&#30340;&#23454;&#26102;&#31283;&#20581;&#30340;&#20154;&#21592;&#23384;&#22312;&#21644;&#31163;&#32676;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20302;&#25104;&#26412;&#30340;60 GHz FMCW&#38647;&#36798;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#21487;&#21516;&#26102;&#35299;&#20915;&#23384;&#22312;&#26816;&#27979;&#21644;&#31163;&#32676;&#26816;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#37325;&#26500;&#26550;&#26500;&#21644;&#38647;&#36798;&#22270;&#20687;&#23454;&#29616;&#20934;&#30830;&#26816;&#27979;&#20154;&#31867;&#23384;&#22312;&#65292;&#21516;&#26102;&#22312;&#20154;&#31867;&#19981;&#23384;&#22312;&#26102;&#26816;&#27979;&#31227;&#21160;&#25110;&#38745;&#27490;&#24178;&#25200;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23460;&#20869;&#29615;&#22659;&#20013;&#20351;&#29992;&#27627;&#31859;&#27874;&#39057;&#29575;&#35843;&#21046;&#36830;&#32493;&#27874;&#65288;FMCW&#65289;&#38647;&#36798;&#36827;&#34892;&#20154;&#21592;&#23384;&#22312;&#26816;&#27979;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#23460;&#20869;&#31354;&#38388;&#20013;&#23384;&#22312;&#31227;&#21160;&#21644;&#38745;&#27490;&#30340;&#26434;&#27874;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;HOOD&#8221;&#30340;&#23454;&#26102;&#31283;&#20581;&#30340;&#20154;&#21592;&#23384;&#22312;&#21644;&#31163;&#32676;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;60 GHz&#30701;&#36317;&#31163;FMCW&#38647;&#36798;&#12290;&#25105;&#20204;&#23558;&#23384;&#22312;&#26816;&#27979;&#24212;&#29992;&#35270;&#20026;&#31163;&#32676;&#26816;&#27979;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#21333;&#19968;&#27969;&#31243;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20381;&#36182;&#20110;&#22522;&#20110;&#37325;&#26500;&#30340;&#26550;&#26500;&#65292;&#24182;&#20351;&#29992;&#38647;&#36798;&#23439;&#35266;&#21644;&#24494;&#35266;&#33539;&#22260;-Doppler&#22270;&#20687;&#65288;RDI&#65289;&#12290;HOOD&#26088;&#22312;&#22312;&#23384;&#22312;&#25110;&#19981;&#23384;&#22312;&#31227;&#21160;&#21644;&#38745;&#27490;&#24178;&#25200;&#29289;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#26816;&#27979;&#20154;&#31867;&#30340;&#8220;&#23384;&#22312;&#8221;&#12290;&#30001;&#20110;&#23427;&#20063;&#26159;&#19968;&#20010;&#31163;&#32676;&#26816;&#27979;&#22120;&#65292;&#23427;&#26088;&#22312;&#23558;&#31227;&#21160;&#25110;&#38745;&#27490;&#26434;&#27874;&#20316;&#20026;&#20154;&#31867;&#19981;&#23384;&#22312;&#30340;&#31163;&#32676;&#26816;&#27979;&#65292;&#24182;&#23558;&#24403;&#21069;&#22330;&#26223;&#30340;&#36755;&#20986;&#39044;&#27979;&#20026;&#8220;&#26080;&#20154;&#23384;&#22312;&#8221;&#12290;HOOD&#26159;&#19968;&#31181;&#26080;&#38656;&#20219;&#20309;&#27963;&#21160;&#30340;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#20154;&#31867;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human presence detection in indoor environments using millimeter-wave frequency-modulated continuous-wave (FMCW) radar is challenging due to the presence of moving and stationary clutters in indoor places. This work proposes "HOOD" as a real-time robust human presence and out-of-distribution (OOD) detection method by exploiting 60 GHz short-range FMCW radar. We approach the presence detection application as an OOD detection problem and solve the two problems simultaneously using a single pipeline. Our solution relies on a reconstruction-based architecture and works with radar macro and micro range-Doppler images (RDIs). HOOD aims to accurately detect the "presence" of humans in the presence or absence of moving and stationary disturbers. Since it is also an OOD detector, it aims to detect moving or stationary clutters as OOD in humans' absence and predicts the current scene's output as "no presence." HOOD is an activity-free approach that performs well in different human scenarios. On 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Gramm&#35282;&#24230;&#22330;&#30340;ECG&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#26102;&#38388;&#39057;&#29575;1D&#21521;&#37327;&#36716;&#25442;&#20026;2D&#22270;&#20687;&#65292;&#24182;&#23545;&#36716;&#25442;&#21518;&#30340;ECG&#20449;&#21495;&#36827;&#34892;&#20998;&#31867;&#65292;&#36798;&#21040;&#20102;&#39640;&#20934;&#30830;&#29575;&#12290;&#35813;&#26041;&#27861;&#25913;&#21892;&#20102;&#20998;&#31867;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#26377;&#21161;&#20110;&#35782;&#21035;&#21644;&#21487;&#35270;&#21270;ECG&#20449;&#21495;&#20013;&#30340;&#26102;&#22495;&#27169;&#24335;&#65292;&#22312;&#24515;&#34880;&#31649;&#30142;&#30149;&#35786;&#26029;&#21644;&#27835;&#30103;&#20197;&#21450;&#24322;&#24120;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.02395</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Gramm&#35282;&#24230;&#22330;&#30340;ECG&#20998;&#31867;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
ECG classification using Deep CNN and Gramian Angular Field. (arXiv:2308.02395v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Gramm&#35282;&#24230;&#22330;&#30340;ECG&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#26102;&#38388;&#39057;&#29575;1D&#21521;&#37327;&#36716;&#25442;&#20026;2D&#22270;&#20687;&#65292;&#24182;&#23545;&#36716;&#25442;&#21518;&#30340;ECG&#20449;&#21495;&#36827;&#34892;&#20998;&#31867;&#65292;&#36798;&#21040;&#20102;&#39640;&#20934;&#30830;&#29575;&#12290;&#35813;&#26041;&#27861;&#25913;&#21892;&#20102;&#20998;&#31867;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#26377;&#21161;&#20110;&#35782;&#21035;&#21644;&#21487;&#35270;&#21270;ECG&#20449;&#21495;&#20013;&#30340;&#26102;&#22495;&#27169;&#24335;&#65292;&#22312;&#24515;&#34880;&#31649;&#30142;&#30149;&#35786;&#26029;&#21644;&#27835;&#30103;&#20197;&#21450;&#24322;&#24120;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#34920;&#31034;&#26041;&#27861;&#65292;&#21363;Gramm&#35282;&#24230;&#22330;&#21464;&#25442;&#65292;&#20026;ECG&#20449;&#21495;&#20998;&#26512;&#30340;&#20449;&#21495;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#39033;&#21019;&#26032;&#36129;&#29486;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#23558;&#26102;&#38388;&#39057;&#29575;1D&#21521;&#37327;&#36716;&#25442;&#20026;2D&#22270;&#20687;&#65292;&#28982;&#21518;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#23545;&#36716;&#25442;&#21518;&#30340;ECG&#20449;&#21495;&#36827;&#34892;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#24322;&#24120;&#26816;&#27979;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;97.47%&#21644;98.65%&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#29305;&#24449;&#34920;&#31034;&#26041;&#27861;&#19981;&#20165;&#25913;&#21892;&#20102;&#20998;&#31867;&#24615;&#33021;&#65292;&#36824;&#26377;&#21161;&#20110;&#35782;&#21035;&#21644;&#21487;&#35270;&#21270;ECG&#20449;&#21495;&#20013;&#30340;&#26102;&#22495;&#27169;&#24335;&#65292;&#20363;&#22914;&#24515;&#29575;&#12289;&#33410;&#24459;&#21644;&#24418;&#24577;&#30340;&#21464;&#21270;&#65292;&#36825;&#22312;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#20197;&#21450;&#24322;&#24120;&#26816;&#27979;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper study provides a novel contribution to the field of signal processing and DL for ECG signal analysis by introducing a new feature representation method for ECG signals. The proposed method is based on transforming time frequency 1D vectors into 2D images using Gramian Angular Field transform. Moving on, the classification of the transformed ECG signals is performed using Convolutional Neural Networks (CNN). The obtained results show a classification accuracy of 97.47% and 98.65% for anomaly detection. Accordingly, in addition to improving the classification performance compared to the state-of-the-art, the feature representation helps identify and visualize temporal patterns in the ECG signal, such as changes in heart rate, rhythm, and morphology, which may not be apparent in the original signal. This has significant implications in the diagnosis and treatment of cardiovascular diseases and detection of anomalies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#38431;&#21015;&#32593;&#32476;&#20013;&#23398;&#20064;&#26368;&#20248;&#20837;&#22330;&#25511;&#21046;&#31574;&#30053;&#30340;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#38381;&#21512;&#20135;&#21697;&#24418;&#24335;&#38431;&#21015;&#32593;&#32476;&#30340;Norton&#31561;&#25928;&#23450;&#29702;&#21644;&#32467;&#26500;&#21270;&#30340;&#20986;&#29983;&#27515;&#20129;&#36807;&#31243; MDP &#30340;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#26368;&#22823;&#20316;&#19994;&#25968;&#37327;&#30340;&#20122;&#32447;&#24615;&#36951;&#25022;&#12290;</title><link>http://arxiv.org/abs/2308.02391</link><description>&lt;p&gt;
&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#38431;&#21015;&#32593;&#32476;&#20013;&#23398;&#20064;&#26368;&#20248;&#20837;&#22330;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Learning Optimal Admission Control in Partially Observable Queueing Networks. (arXiv:2308.02391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#38431;&#21015;&#32593;&#32476;&#20013;&#23398;&#20064;&#26368;&#20248;&#20837;&#22330;&#25511;&#21046;&#31574;&#30053;&#30340;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#38381;&#21512;&#20135;&#21697;&#24418;&#24335;&#38431;&#21015;&#32593;&#32476;&#30340;Norton&#31561;&#25928;&#23450;&#29702;&#21644;&#32467;&#26500;&#21270;&#30340;&#20986;&#29983;&#27515;&#20129;&#36807;&#31243; MDP &#30340;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#26368;&#22823;&#20316;&#19994;&#25968;&#37327;&#30340;&#20122;&#32447;&#24615;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#38431;&#21015;&#32593;&#32476;&#20013;&#23398;&#20064;&#26368;&#20248;&#20837;&#22330;&#25511;&#21046;&#31574;&#30053;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21482;&#33021;&#35266;&#23519;&#21040;&#32593;&#32476;&#30340;&#21040;&#36798;&#21644;&#31163;&#24320;&#26102;&#38388;&#65292;&#24182;&#19988;&#20248;&#21270;&#25351;&#30340;&#26159;&#26080;&#38480;&#26102;&#22495;&#20869;&#30340;&#24179;&#22343;&#25345;&#26377;/&#25298;&#32477;&#25104;&#26412;&#12290;&#34429;&#28982;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#36890;&#24120;&#26159;&#38750;&#24120;&#26114;&#36149;&#30340;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#30340;&#36951;&#25022;&#20165;&#20165;&#20381;&#36182;&#20110;&#32593;&#32476;&#20013;&#20316;&#19994;&#25968;&#37327;&#30340;&#26368;&#22823;&#20540;S&#30340;&#20122;&#32447;&#24615;&#20989;&#25968;&#12290;&#29305;&#21035;&#22320;&#65292;&#19982;&#29616;&#26377;&#30340;&#36951;&#25022;&#20998;&#26512;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#36951;&#25022;&#19978;&#30028;&#19981;&#20381;&#36182;&#20110;&#24213;&#23618;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#30340;&#30452;&#24452;&#65292;&#32780;&#22312;&#22823;&#22810;&#25968;&#38431;&#21015;&#31995;&#32479;&#20013;&#65292;&#36825;&#20010;&#30452;&#24452;&#33267;&#23569;&#26159;&#25351;&#25968;&#32423;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an efficient reinforcement learning algorithm that learns the optimal admission control policy in a partially observable queueing network. Specifically, only the arrival and departure times from the network are observable, and optimality refers to the average holding/rejection cost in infinite horizon.  While reinforcement learning in Partially Observable Markov Decision Processes (POMDP) is prohibitively expensive in general, we show that our algorithm has a regret that only depends sub-linearly on the maximal number of jobs in the network, $S$. In particular, in contrast with existing regret analyses, our regret bound does not depend on the diameter of the underlying Markov Decision Process (MDP), which in most queueing systems is at least exponential in $S$.  The novelty of our approach is to leverage Norton's equivalent theorem for closed product-form queueing networks and an efficient reinforcement learning algorithm for MDPs with the structure of birth-and-death proces
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#21307;&#30103;&#20445;&#20581;&#20013;&#29983;&#23384;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;FedSurF++&#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#32852;&#21512;&#23398;&#20064;&#31639;&#27861;&#29992;&#20110;&#22788;&#29702;&#20998;&#24067;&#24335;&#30340;&#12289;&#20855;&#26377;&#38544;&#31169;&#20445;&#23494;&#35201;&#27714;&#30340;&#29983;&#23384;&#20998;&#26512;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#22823;&#35268;&#27169;&#24314;&#27169;&#21644;&#22788;&#29702;&#29983;&#23384;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#21644;&#38544;&#31169;&#20445;&#23494;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.02382</link><description>&lt;p&gt;
&#12298;&#20351;&#29992;&#32852;&#21512;&#29983;&#23384;&#26862;&#26519;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#25193;&#23637;&#29983;&#23384;&#20998;&#26512;: &#24515;&#21147;&#34928;&#31469;&#21644;&#20083;&#33146;&#30284;&#22522;&#22240;&#32452;&#30340;&#27604;&#36739;&#30740;&#31350;&#12299;
&lt;/p&gt;
&lt;p&gt;
Scaling Survival Analysis in Healthcare with Federated Survival Forests: A Comparative Study on Heart Failure and Breast Cancer Genomics. (arXiv:2308.02382v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#21307;&#30103;&#20445;&#20581;&#20013;&#29983;&#23384;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;FedSurF++&#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#32852;&#21512;&#23398;&#20064;&#31639;&#27861;&#29992;&#20110;&#22788;&#29702;&#20998;&#24067;&#24335;&#30340;&#12289;&#20855;&#26377;&#38544;&#31169;&#20445;&#23494;&#35201;&#27714;&#30340;&#29983;&#23384;&#20998;&#26512;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#22823;&#35268;&#27169;&#24314;&#27169;&#21644;&#22788;&#29702;&#29983;&#23384;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#21644;&#38544;&#31169;&#20445;&#23494;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#23384;&#20998;&#26512;&#26159;&#21307;&#23398;&#20013;&#30340;&#19968;&#31181;&#22522;&#26412;&#24037;&#20855;&#65292;&#29992;&#20110;&#23545;&#20154;&#32676;&#20013;&#21457;&#29983;&#24863;&#20852;&#36259;&#20107;&#20214;&#30340;&#26102;&#38388;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#29983;&#23384;&#25968;&#25454;&#36890;&#24120;&#26159;&#19981;&#23436;&#25972;&#12289;&#34987;&#23457;&#26597;&#12289;&#20998;&#24067;&#24335;&#21644;&#20445;&#23494;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#38544;&#31169;&#20445;&#23494;&#33267;&#20851;&#37325;&#35201;&#30340;&#21307;&#30103;&#20445;&#20581;&#29615;&#22659;&#20013;&#12290;&#25968;&#25454;&#31232;&#32570;&#20005;&#37325;&#38480;&#21046;&#20102;&#29983;&#23384;&#27169;&#22411;&#22312;&#20381;&#36182;&#22823;&#22411;&#25968;&#25454;&#27744;&#30340;&#20998;&#24067;&#24335;&#24212;&#29992;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#32852;&#21512;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#26395;&#36890;&#36807;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32780;&#19981;&#25439;&#23475;&#29992;&#25143;&#38544;&#31169;&#30340;&#25216;&#26415;&#65292;&#22240;&#27492;&#29305;&#21035;&#36866;&#21512;&#35299;&#20915;&#29983;&#23384;&#25968;&#25454;&#21644;&#22823;&#35268;&#27169;&#29983;&#23384;&#24212;&#29992;&#30340;&#25361;&#25112;&#12290;&#23613;&#31649;&#32852;&#21512;&#23398;&#20064;&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#21457;&#23637;&#65292;&#20294;&#22312;&#29983;&#23384;&#20998;&#26512;&#39046;&#22495;&#20173;&#26377;&#35768;&#22810;&#26410;&#25506;&#32034;&#30340;&#26041;&#21521;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#21512;&#29983;&#23384;&#26862;&#26519;&#31639;&#27861;&#30340;&#25193;&#23637;&#65292;&#31216;&#20026;FedSurF++&#12290;&#36825;&#31181;&#32852;&#21512;&#38598;&#25104;&#26041;&#27861;&#21487;&#20197;&#23545;&#22823;&#35268;&#27169;&#29983;&#23384;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#24182;&#22788;&#29702;&#20998;&#24067;&#24335;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Survival analysis is a fundamental tool in medicine, modeling the time until an event of interest occurs in a population. However, in real-world applications, survival data are often incomplete, censored, distributed, and confidential, especially in healthcare settings where privacy is critical. The scarcity of data can severely limit the scalability of survival models to distributed applications that rely on large data pools. Federated learning is a promising technique that enables machine learning models to be trained on multiple datasets without compromising user privacy, making it particularly well-suited for addressing the challenges of survival data and large-scale survival applications. Despite significant developments in federated learning for classification and regression, many directions remain unexplored in the context of survival analysis. In this work, we propose an extension of the Federated Survival Forest algorithm, called FedSurF++. This federated ensemble method const
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20174;&#25506;&#27979;&#36710;&#25968;&#25454;&#39044;&#27979;&#20132;&#36890;&#20449;&#21495;&#28783;&#23450;&#26102;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;XGBoost&#27169;&#22411;&#20272;&#35745;&#20449;&#21495;&#21608;&#26399;&#38271;&#24230;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30830;&#23450;&#32418;&#28783;&#26102;&#38388;&#65292;&#24182;&#26681;&#25454;&#21608;&#26399;&#38271;&#24230;&#21644;&#32418;&#28783;&#26102;&#38388;&#35745;&#31639;&#32511;&#28783;&#26102;&#38388;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23545;&#20449;&#21495;&#21608;&#26399;&#38271;&#24230;&#30340;&#20272;&#35745;&#35823;&#24046;&#23567;&#20110;0.56&#31186;&#65292;&#32418;&#28783;&#26102;&#38388;&#30340;&#39044;&#27979;&#35823;&#24046;&#24179;&#22343;&#22312;7.2&#31186;&#20197;&#20869;&#12290;</title><link>http://arxiv.org/abs/2308.02370</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20174;&#25506;&#27979;&#36710;&#25968;&#25454;&#39044;&#27979;&#20132;&#36890;&#20449;&#21495;&#28783;&#23450;&#26102;
&lt;/p&gt;
&lt;p&gt;
A Machine Learning Method for Predicting Traffic Signal Timing from Probe Vehicle Data. (arXiv:2308.02370v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20174;&#25506;&#27979;&#36710;&#25968;&#25454;&#39044;&#27979;&#20132;&#36890;&#20449;&#21495;&#28783;&#23450;&#26102;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;XGBoost&#27169;&#22411;&#20272;&#35745;&#20449;&#21495;&#21608;&#26399;&#38271;&#24230;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30830;&#23450;&#32418;&#28783;&#26102;&#38388;&#65292;&#24182;&#26681;&#25454;&#21608;&#26399;&#38271;&#24230;&#21644;&#32418;&#28783;&#26102;&#38388;&#35745;&#31639;&#32511;&#28783;&#26102;&#38388;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23545;&#20449;&#21495;&#21608;&#26399;&#38271;&#24230;&#30340;&#20272;&#35745;&#35823;&#24046;&#23567;&#20110;0.56&#31186;&#65292;&#32418;&#28783;&#26102;&#38388;&#30340;&#39044;&#27979;&#35823;&#24046;&#24179;&#22343;&#22312;7.2&#31186;&#20197;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#20449;&#21495;&#28783;&#22312;&#20132;&#36890;&#36816;&#36755;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#23427;&#20204;&#33021;&#22815;&#31649;&#29702;&#20132;&#36890;&#27969;&#37327;&#24182;&#30830;&#20445;&#20132;&#21449;&#36335;&#21475;&#30340;&#23433;&#20840;&#12290;&#27492;&#22806;&#65292;&#20102;&#35299;&#20132;&#36890;&#20449;&#21495;&#28783;&#30340;&#30456;&#20301;&#21644;&#23450;&#26102;&#25968;&#25454;&#21487;&#20197;&#23454;&#29616;&#26368;&#20339;&#36710;&#36742;&#34892;&#39542;&#36335;&#32447;&#65292;&#25552;&#39640;&#26102;&#38388;&#21644;&#33021;&#28304;&#25928;&#29575;&#65292;&#36827;&#34892;&#29983;&#24577;&#39550;&#39542;&#20197;&#21450;&#20934;&#30830;&#27169;&#25311;&#26377;&#20449;&#21495;&#30340;&#36947;&#36335;&#32593;&#32476;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20174;&#36710;&#36742;&#25506;&#27979;&#25968;&#25454;&#20272;&#35745;&#20132;&#36890;&#20449;&#21495;&#28783;&#23450;&#26102;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#25454;&#20316;&#32773;&#25152;&#30693;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20174;&#36710;&#36742;&#25506;&#27979;&#25968;&#25454;&#30830;&#23450;&#20132;&#36890;&#20449;&#21495;&#28783;&#23450;&#26102;&#21442;&#25968;&#30340;&#24037;&#20316;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26497;&#38480;&#26799;&#24230;&#22686;&#24378;&#65288;XGBoost&#65289;&#27169;&#22411;&#26469;&#20272;&#35745;&#20449;&#21495;&#21608;&#26399;&#38271;&#24230;&#65292;&#24182;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20174;&#25506;&#27979;&#25968;&#25454;&#20013;&#30830;&#23450;&#30456;&#24212;&#30340;&#32418;&#28783;&#26102;&#38388;&#12290;&#28982;&#21518;&#26681;&#25454;&#21608;&#26399;&#38271;&#24230;&#21644;&#32418;&#28783;&#26102;&#38388;&#35745;&#31639;&#32511;&#28783;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#20449;&#21495;&#21608;&#26399;&#38271;&#24230;&#30340;&#35823;&#24046;&#23567;&#20110;0.56&#31186;&#65292;&#32418;&#28783;&#26102;&#38388;&#30340;&#39044;&#27979;&#35823;&#24046;&#24179;&#22343;&#22312;7.2&#31186;&#20197;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic signals play an important role in transportation by enabling traffic flow management, and ensuring safety at intersections. In addition, knowing the traffic signal phase and timing data can allow optimal vehicle routing for time and energy efficiency, eco-driving, and the accurate simulation of signalized road networks. In this paper, we present a machine learning (ML) method for estimating traffic signal timing information from vehicle probe data. To the authors best knowledge, very few works have presented ML techniques for determining traffic signal timing parameters from vehicle probe data. In this work, we develop an Extreme Gradient Boosting (XGBoost) model to estimate signal cycle lengths and a neural network model to determine the corresponding red times per phase from probe data. The green times are then be derived from the cycle length and red times. Our results show an error of less than 0.56 sec for cycle length, and red times predictions within 7.2 sec error on ave
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#24046;&#20998;&#38544;&#31169;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65288;VFL&#65289;&#65292;&#36890;&#36807;&#24212;&#29992;&#33539;&#25968;&#21098;&#35009;&#23454;&#29616;&#20102;&#20005;&#26684;&#30340;&#38544;&#31169;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#35843;&#25972;&#29305;&#24449;&#23884;&#20837;&#30340;&#23610;&#24230;&#21644;&#20998;&#24067;&#26469;&#20248;&#21270;&#20219;&#21153;&#25928;&#29992;&#65292;&#32780;&#19981;&#25439;&#23475;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2308.02362</link><description>&lt;p&gt;
&#28789;&#27963;&#30340;&#24046;&#20998;&#38544;&#31169;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#19982;&#33258;&#36866;&#24212;&#29305;&#24449;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Flexible Differentially Private Vertical Federated Learning with Adaptive Feature Embeddings. (arXiv:2308.02362v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#24046;&#20998;&#38544;&#31169;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65288;VFL&#65289;&#65292;&#36890;&#36807;&#24212;&#29992;&#33539;&#25968;&#21098;&#35009;&#23454;&#29616;&#20102;&#20005;&#26684;&#30340;&#38544;&#31169;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#35843;&#25972;&#29305;&#24449;&#23884;&#20837;&#30340;&#23610;&#24230;&#21644;&#20998;&#24067;&#26469;&#20248;&#21270;&#20219;&#21153;&#25928;&#29992;&#65292;&#32780;&#19981;&#25439;&#23475;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65288;VFL&#65289;&#30340;&#20986;&#29616;&#24341;&#21457;&#20102;&#23545;&#38544;&#31169;&#20445;&#25252;&#19981;&#23436;&#21892;&#30340;&#25285;&#24551;&#65292;&#22240;&#20026;&#20849;&#20139;&#30340;&#29305;&#24449;&#23884;&#20837;&#21487;&#33021;&#22312;&#38544;&#31169;&#25915;&#20987;&#19979;&#27844;&#38706;&#25935;&#24863;&#20449;&#24687;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;VFL&#22312;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#19979;&#25968;&#25454;&#38544;&#31169;&#21644;&#20219;&#21153;&#25928;&#29992;&#30446;&#26631;&#20043;&#38388;&#30340;&#24494;&#22937;&#24179;&#34913;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#25216;&#26415;&#30340;&#36890;&#29992;&#24615;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#19988;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#23558;&#36825;&#20004;&#20010;&#30446;&#26631;&#20998;&#35299;&#24182;&#36880;&#27493;&#35299;&#20915;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23545;&#20849;&#20139;&#29305;&#24449;&#23884;&#20837;&#24212;&#29992;&#33539;&#25968;&#21098;&#35009;&#65292;&#24471;&#21040;&#20102;&#20005;&#26684;&#30340;&#38544;&#31169;&#20445;&#35777;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#36890;&#36807;&#23545;&#29305;&#24449;&#23884;&#20837;&#30340;&#23610;&#24230;&#21644;&#20998;&#24067;&#36827;&#34892;&#33258;&#36866;&#24212;&#35843;&#25972;&#65292;&#20197;&#19968;&#31181;&#27880;&#37325;&#20934;&#30830;&#24615;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#20248;&#21270;&#20219;&#21153;&#25928;&#29992;&#65292;&#32780;&#19981;&#25439;&#23475;&#24050;&#24314;&#31435;&#30340;DP&#26426;&#21046;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#20855;&#20307;&#21270;&#20026;&#25552;&#20986;&#30340;VFL-AFE&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23545;&#25239;&#20102;&#20808;&#39564;&#25915;&#20987;&#65292;&#24182;&#26174;&#31034;&#20986;&#20102;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of vertical federated learning (VFL) has stimulated concerns about the imperfection in privacy protection, as shared feature embeddings may reveal sensitive information under privacy attacks. This paper studies the delicate equilibrium between data privacy and task utility goals of VFL under differential privacy (DP). To address the generality issue of prior arts, this paper advocates a flexible and generic approach that decouples the two goals and addresses them successively. Specifically, we initially derive a rigorous privacy guarantee by applying norm clipping on shared feature embeddings, which is applicable across various datasets and models. Subsequently, we demonstrate that task utility can be optimized via adaptive adjustments on the scale and distribution of feature embeddings in an accuracy-appreciative way, without compromising established DP mechanisms. We concretize our observation into the proposed VFL-AFE framework, which exhibits effectiveness against pri
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31215;&#20998;&#30340;&#26080;&#24378;&#24230;&#31215;&#20998;&#21270;&#23398;&#33021;&#30340;&#23398;&#20064;&#26694;&#26550;IFIB&#65292;&#29992;&#20110;&#24314;&#27169;&#31163;&#25955;&#20107;&#20214;&#20013;&#20855;&#26377;&#20998;&#31867;&#25110;&#25968;&#20540;&#23646;&#24615;&#30340;&#20107;&#20214;&#26631;&#35760;&#12290;</title><link>http://arxiv.org/abs/2308.02360</link><description>&lt;p&gt;
&#22522;&#20110;&#31215;&#20998;&#30340;&#26080;&#24378;&#24230;&#31215;&#20998;&#21270;&#23398;&#33021;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Intensity-free Integral-based Learning of Marked Temporal Point Processes. (arXiv:2308.02360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02360
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31215;&#20998;&#30340;&#26080;&#24378;&#24230;&#31215;&#20998;&#21270;&#23398;&#33021;&#30340;&#23398;&#20064;&#26694;&#26550;IFIB&#65292;&#29992;&#20110;&#24314;&#27169;&#31163;&#25955;&#20107;&#20214;&#20013;&#20855;&#26377;&#20998;&#31867;&#25110;&#25968;&#20540;&#23646;&#24615;&#30340;&#20107;&#20214;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26631;&#35760;&#30340;&#26102;&#38388;&#28857;&#36807;&#31243;&#65288;MTPP&#65289;&#20013;&#65292;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#20026;&#26465;&#20214;&#32852;&#21512;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65288;PDF&#65289;$p^*&#65288;m&#65292;t&#65289;$&#21442;&#25968;&#21270;&#25554;&#20540;&#26102;&#38388;t&#21644;&#26631;&#35760;m&#22312;&#21382;&#21490;&#26465;&#20214;&#19979;&#12290;&#29616;&#26377;&#30740;&#31350;&#22823;&#22810;&#39044;&#20808;&#23450;&#20041;&#24378;&#24230;&#20989;&#25968;&#12290;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#21463;&#21040;&#25351;&#23450;&#24378;&#24230;&#20989;&#25968;&#27491;&#30830;&#24418;&#24335;&#30340;&#25361;&#25112;&#65292;&#36825;&#23545;&#20110;&#24179;&#34913;&#34920;&#36798;&#33021;&#21147;&#21644;&#22788;&#29702;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#26377;&#30740;&#31350;&#25670;&#33073;&#39044;&#23450;&#20041;&#24378;&#24230;&#20989;&#25968;&#65292;&#19968;&#20010;&#27169;&#22411;$p^*&#65288;t&#65289;$&#21644;$p^*&#65288;m&#65289;$&#20998;&#24320;&#65292;&#21478;&#19968;&#20010;&#20391;&#37325;&#20110;&#19981;&#32771;&#34385;&#26631;&#35760;&#30340;&#26102;&#38388;&#28857;&#36807;&#31243;&#65288;TPP&#65289;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#39640;&#20445;&#30495;&#24230;&#30340;$p^*&#65288;m&#65292;t&#65289;$&#65292;&#36866;&#29992;&#20110;&#20107;&#20214;&#26631;&#35760;&#22312;&#22810;&#32500;&#36830;&#32493;&#31354;&#38388;&#20013;&#20855;&#26377;&#20998;&#31867;&#25110;&#25968;&#20540;&#23646;&#24615;&#30340;&#31163;&#25955;&#20107;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#26694;&#26550;IFIB&#65288;&#26080;&#24378;&#24230;&#31215;&#20998;&#21270;&#23398;&#33021;&#36807;&#31243;&#65289;&#65292;&#30452;&#25509;&#24314;&#27169;&#26465;&#20214;&#32852;&#21512;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;$p^*&#65288;m&#65292;t&#65289;$&#12290;
&lt;/p&gt;
&lt;p&gt;
In the marked temporal point processes (MTPP), a core problem is to parameterize the conditional joint PDF (probability distribution function) $p^*(m,t)$ for inter-event time $t$ and mark $m$, conditioned on the history. The majority of existing studies predefine intensity functions. Their utility is challenged by specifying the intensity function's proper form, which is critical to balance expressiveness and processing efficiency. Recently, there are studies moving away from predefining the intensity function -- one models $p^*(t)$ and $p^*(m)$ separately, while the other focuses on temporal point processes (TPPs), which do not consider marks. This study aims to develop high-fidelity $p^*(m,t)$ for discrete events where the event marks are either categorical or numeric in a multi-dimensional continuous space. We propose a solution framework IFIB (\underline{I}ntensity-\underline{f}ree \underline{I}ntegral-\underline{b}ased process) that models conditional joint PDF $p^*(m,t)$ directly
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21322;&#30417;&#30563;&#22270;&#24418;&#22240;&#26524;&#35299;&#37322;&#22120; (DyGRACE)&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#25968;&#25454;&#34920;&#31034;&#24182;&#21033;&#29992;&#24050;&#30693;&#25968;&#25454;&#20998;&#24067;&#30340;&#21021;&#22987;&#30693;&#35782;&#65292;&#22312;&#21160;&#24577;&#25968;&#25454;&#29615;&#22659;&#20013;&#25628;&#32034;&#26377;&#25928;&#30340;&#22240;&#26524;&#35299;&#37322;&#12290;&#35813;&#26041;&#27861;&#29420;&#31435;&#20110;&#24213;&#23618;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.02353</link><description>&lt;p&gt;
&#36866;&#24212;&#21464;&#21270;&#65306;&#22312;&#21160;&#24577;&#25968;&#25454;&#29615;&#22659;&#20013;&#30340;&#24378;&#38887;&#22240;&#26524;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Adapting to Change: Robust Counterfactual Explanations in Dynamic Data Landscapes. (arXiv:2308.02353v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02353
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21322;&#30417;&#30563;&#22270;&#24418;&#22240;&#26524;&#35299;&#37322;&#22120; (DyGRACE)&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#25968;&#25454;&#34920;&#31034;&#24182;&#21033;&#29992;&#24050;&#30693;&#25968;&#25454;&#20998;&#24067;&#30340;&#21021;&#22987;&#30693;&#35782;&#65292;&#22312;&#21160;&#24577;&#25968;&#25454;&#29615;&#22659;&#20013;&#25628;&#32034;&#26377;&#25928;&#30340;&#22240;&#26524;&#35299;&#37322;&#12290;&#35813;&#26041;&#27861;&#29420;&#31435;&#20110;&#24213;&#23618;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21322;&#30417;&#30563;&#22270;&#24418;&#22240;&#26524;&#35299;&#37322;&#22120; (GCE) &#26041;&#27861;&#65292;&#31216;&#20026;&#21160;&#24577;&#22270;&#24418;&#22240;&#26524;&#35299;&#37322;&#22120; (DyGRACE)&#12290;&#23427;&#21033;&#29992;&#24050;&#30693;&#25968;&#25454;&#20998;&#24067;&#30340;&#21021;&#22987;&#30693;&#35782;&#65292;&#22312;&#25628;&#32034;&#26377;&#25928;&#30340;&#22240;&#26524;&#35299;&#37322;&#26102;&#36991;&#20813;&#20351;&#29992;&#28508;&#22312;&#36807;&#26102;&#30340;&#20915;&#31574;&#20989;&#25968;&#30340;&#20449;&#24687;&#12290;DyGRACE&#21033;&#29992;&#20004;&#20010;&#22270;&#24418;&#33258;&#32534;&#30721;&#22120; (GAE) &#26469;&#23398;&#20064;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#20013;&#27599;&#20010;&#31867;&#30340;&#34920;&#31034;&#12290;GAE&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26368;&#23567;&#21270;&#21407;&#22987;&#22270;&#24418;&#19982;&#20854;&#23398;&#20064;&#34920;&#31034;&#20043;&#38388;&#30340;&#37325;&#24314;&#35823;&#24046;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324; (i) &#36890;&#36807;&#26368;&#22823;&#21270;&#20107;&#23454;&#33258;&#32534;&#30721;&#22120;&#30340;&#37325;&#24314;&#35823;&#24046;&#26469;&#20248;&#21270;&#21442;&#25968;&#21270;&#23494;&#24230;&#20989;&#25968; (&#23454;&#29616;&#20026;&#36923;&#36753;&#22238;&#24402;&#20989;&#25968;) &#65292;&#20197;&#35782;&#21035;&#22240;&#26524;&#35299;&#37322;&#65292;(ii) &#26368;&#23567;&#21270;&#22240;&#26524;&#33258;&#32534;&#30721;&#22120;&#30340;&#35823;&#24046;&#65292;(iii) &#26368;&#22823;&#21270;&#20107;&#23454;&#22270;&#24418;&#19982;&#22240;&#26524;&#22270;&#24418;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#36825;&#31181;&#21322;&#30417;&#30563;&#26041;&#27861;&#29420;&#31435;&#20110;&#22522;&#30784;&#40657;&#30418;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel semi-supervised Graph Counterfactual Explainer (GCE) methodology, Dynamic GRAph Counterfactual Explainer (DyGRACE). It leverages initial knowledge about the data distribution to search for valid counterfactuals while avoiding using information from potentially outdated decision functions in subsequent time steps. Employing two graph autoencoders (GAEs), DyGRACE learns the representation of each class in a binary classification scenario. The GAEs minimise the reconstruction error between the original graph and its learned representation during training. The method involves (i) optimising a parametric density function (implemented as a logistic regression function) to identify counterfactuals by maximising the factual autoencoder's reconstruction error, (ii) minimising the counterfactual autoencoder's error, and (iii) maximising the similarity between the factual and counterfactual graphs. This semi-supervised approach is independent of an underlying black-box oracle
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#22312;ImageNet&#19978;&#20840;&#38754;&#35780;&#20272;&#20102;&#37327;&#21270;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#37327;&#21270;&#27169;&#22411;&#22312;&#23545;&#25239;&#25915;&#20987;&#26041;&#38754;&#26356;&#20855;&#40065;&#26834;&#24615;&#65292;&#20294;&#23545;&#33258;&#28982;&#25439;&#22351;&#21644;&#31995;&#32479;&#22122;&#22768;&#26356;&#33030;&#24369;&#12290;</title><link>http://arxiv.org/abs/2308.02350</link><description>&lt;p&gt;
RobustMQ:&#35780;&#20272;&#37327;&#21270;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
RobustMQ: Benchmarking Robustness of Quantized Models. (arXiv:2308.02350v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02350
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#22312;ImageNet&#19978;&#20840;&#38754;&#35780;&#20272;&#20102;&#37327;&#21270;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#37327;&#21270;&#27169;&#22411;&#22312;&#23545;&#25239;&#25915;&#20987;&#26041;&#38754;&#26356;&#20855;&#40065;&#26834;&#24615;&#65292;&#20294;&#23545;&#33258;&#28982;&#25439;&#22351;&#21644;&#31995;&#32479;&#22122;&#22768;&#26356;&#33030;&#24369;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#24050;&#32463;&#25104;&#20026;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#35774;&#22791;&#19978;&#37096;&#32626;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#19968;&#31181;&#37325;&#35201;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#37327;&#21270;&#27169;&#22411;&#22312;&#38754;&#23545;&#21508;&#31181;&#22122;&#22768;&#26102;&#20250;&#23637;&#29616;&#20986;&#33030;&#24369;&#24615;&#12290;&#23613;&#31649;&#35780;&#20272;&#37327;&#21270;&#23545;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#30340;&#37325;&#35201;&#24615;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#36824;&#24456;&#26377;&#38480;&#65292;&#24182;&#19988;&#32463;&#24120;&#24573;&#35270;&#24050;&#32463;&#30830;&#31435;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#21407;&#21017;&#65292;&#23548;&#33268;&#20102;&#19981;&#23436;&#25972;&#21644;&#19981;&#30830;&#23450;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;ImageNet&#19978;&#30340;&#37327;&#21270;&#27169;&#22411;&#22312;&#21508;&#31181;&#22122;&#22768;&#65288;&#23545;&#25239;&#25915;&#20987;&#65292;&#33258;&#28982;&#25439;&#22351;&#21644;&#31995;&#32479;&#22122;&#22768;&#65289;&#19979;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#12290;&#20840;&#38754;&#30340;&#35780;&#20272;&#32467;&#26524;&#20174;&#32463;&#39564;&#19978;&#20026;&#37327;&#21270;&#27169;&#22411;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#40065;&#26834;&#24615;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#20363;&#22914;&#65306;&#65288;1&#65289;&#37327;&#21270;&#27169;&#22411;&#22312;&#23545;&#25239;&#25915;&#20987;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#40065;&#26834;&#24615;&#65292;&#20294;&#23545;&#33258;&#28982;&#25439;&#22351;&#21644;&#31995;&#32479;&#22122;&#22768;&#26356;&#33030;&#24369;&#65307;&#65288;2&#65289;&#24635;&#30340;&#26469;&#35828;&#65292;&#19981;&#21516;&#31243;&#24230;&#30340;&#37327;&#21270;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization has emerged as an essential technique for deploying deep neural networks (DNNs) on devices with limited resources. However, quantized models exhibit vulnerabilities when exposed to various noises in real-world applications. Despite the importance of evaluating the impact of quantization on robustness, existing research on this topic is limited and often disregards established principles of robustness evaluation, resulting in incomplete and inconclusive findings. To address this gap, we thoroughly evaluated the robustness of quantized models against various noises (adversarial attacks, natural corruptions, and systematic noises) on ImageNet. The comprehensive evaluation results empirically provide valuable insights into the robustness of quantized models in various scenarios, for example: (1) quantized models exhibit higher adversarial robustness than their floating-point counterparts, but are more vulnerable to natural corruptions and systematic noises; (2) in general, inc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36229;&#22270;&#21327;&#21516;&#32593;&#32476;&#30340;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#24615;&#65292;&#24182;&#22312;&#20998;&#26512;&#20013;&#25581;&#31034;&#20102;&#21327;&#21516;&#32593;&#32476;&#20013;&#36229;&#22270;&#28388;&#27874;&#22120;&#30340;&#35774;&#35745;&#35201;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.02347</link><description>&lt;p&gt;
&#36229;&#22270;&#21327;&#21516;&#32593;&#32476;&#30340;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#24615;
&lt;/p&gt;
&lt;p&gt;
Stability and Generalization of Hypergraph Collaborative Networks. (arXiv:2308.02347v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36229;&#22270;&#21327;&#21516;&#32593;&#32476;&#30340;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#24615;&#65292;&#24182;&#22312;&#20998;&#26512;&#20013;&#25581;&#31034;&#20102;&#21327;&#21516;&#32593;&#32476;&#20013;&#36229;&#22270;&#28388;&#27874;&#22120;&#30340;&#35774;&#35745;&#35201;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#34987;&#35777;&#26126;&#22312;&#21033;&#29992;&#26679;&#26412;&#20043;&#38388;&#30340;&#25104;&#23545;&#20851;&#31995;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#26377;&#20960;&#20010;&#25104;&#21151;&#30340;&#26041;&#26696;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#24191;&#21040;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#21033;&#29992;&#26356;&#22797;&#26434;&#30340;&#20851;&#31995;&#12290;&#29305;&#21035;&#26159;&#65292;&#36229;&#22270;&#21327;&#21516;&#32593;&#32476;&#30456;&#23545;&#20110;&#20854;&#20182;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#20013;&#20135;&#29983;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;&#21327;&#21516;&#32593;&#32476;&#36890;&#36807;&#23558;&#39030;&#28857;&#23884;&#20837;&#21644;&#36229;&#36793;&#23884;&#20837;&#34920;&#36848;&#20026;&#19968;&#20010;&#32852;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#22312;&#37325;&#26500;&#32473;&#23450;&#36229;&#22270;&#26102;&#30340;&#19968;&#33268;&#24615;&#65292;&#21487;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#23884;&#20837;&#34920;&#31034;&#12290;&#26412;&#25991;&#26088;&#22312;&#24314;&#31435;&#21327;&#21516;&#32593;&#32476;&#26680;&#24515;&#23618;&#30340;&#31639;&#27861;&#31283;&#23450;&#24615;&#24182;&#25552;&#20379;&#27867;&#21270;&#20445;&#35777;&#12290;&#20998;&#26512;&#32467;&#26524;&#25581;&#31034;&#20102;&#21327;&#21516;&#32593;&#32476;&#20013;&#36229;&#22270;&#28388;&#27874;&#22120;&#30340;&#35774;&#35745;&#65292;&#20363;&#22914;&#65292;&#25968;&#25454;&#21644;&#36229;&#22270;&#28388;&#27874;&#22120;&#24212;&#22914;&#20309;&#32553;&#25918;&#20197;&#23454;&#29616;&#32479;&#19968;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks have been shown to be very effective in utilizing pairwise relationships across samples. Recently, there have been several successful proposals to generalize graph neural networks to hypergraph neural networks to exploit more complex relationships. In particular, the hypergraph collaborative networks yield superior results compared to other hypergraph neural networks for various semi-supervised learning tasks. The collaborative network can provide high quality vertex embeddings and hyperedge embeddings together by formulating them as a joint optimization problem and by using their consistency in reconstructing the given hypergraph. In this paper, we aim to establish the algorithmic stability of the core layer of the collaborative network and provide generalization guarantees. The analysis sheds light on the design of hypergraph filters in collaborative networks, for instance, how the data and hypergraph filters should be scaled to achieve uniform stability of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#37325;&#22797;&#27979;&#37327;&#30340;&#39640;&#26031;&#33258;&#30001;&#22330;&#20013;&#20272;&#35745;&#21152;&#26435;&#32593;&#32476;&#32467;&#26500;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#20998;&#24067;&#20613;&#37324;&#21494;&#20998;&#26512;&#29305;&#24615;&#30340;&#26032;&#22411;&#20272;&#35745;&#22120;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#26500;&#36896;&#30340;&#22797;&#25968;&#20540;&#32479;&#35745;&#37327;&#65292;&#20855;&#26377;&#30740;&#31350;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2308.02344</link><description>&lt;p&gt;
&#20174;&#39640;&#26031;&#22270;&#27169;&#22411;&#21644;&#39640;&#26031;&#33258;&#30001;&#22330;&#23398;&#20064;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Learning Networks from Gaussian Graphical Models and Gaussian Free Fields. (arXiv:2308.02344v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#37325;&#22797;&#27979;&#37327;&#30340;&#39640;&#26031;&#33258;&#30001;&#22330;&#20013;&#20272;&#35745;&#21152;&#26435;&#32593;&#32476;&#32467;&#26500;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#20998;&#24067;&#20613;&#37324;&#21494;&#20998;&#26512;&#29305;&#24615;&#30340;&#26032;&#22411;&#20272;&#35745;&#22120;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#26500;&#36896;&#30340;&#22797;&#25968;&#20540;&#32479;&#35745;&#37327;&#65292;&#20855;&#26377;&#30740;&#31350;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;&#32593;&#32476;&#19978;&#30340;&#39640;&#26031;&#22270;&#27169;&#22411;&#65288;Gaussian Graphical Model&#65292;&#31616;&#31216;GGM&#65289;&#30340;&#37325;&#22797;&#27979;&#37327;&#20013;&#20272;&#35745;&#21152;&#26435;&#32593;&#32476;&#32467;&#26500;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#26041;&#38754;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19982;&#22522;&#20110;&#20854;&#19978;&#21152;&#26435;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#30456;&#19968;&#33268;&#30340;GGM&#30340;&#21327;&#26041;&#24046;&#32467;&#26500;&#12290;&#36825;&#31181;GGM&#22312;&#32479;&#35745;&#29289;&#29702;&#23398;&#20013;&#19968;&#30452;&#21463;&#21040;&#20851;&#27880;&#65292;&#24182;&#34987;&#31216;&#20026;&#39640;&#26031;&#33258;&#30001;&#22330;&#65288;Gaussian Free Field&#65292;&#31616;&#31216;GFF&#65289;&#12290;&#36817;&#24180;&#26469;&#65292;&#23427;&#20204;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#29702;&#35770;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20852;&#36259;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#20998;&#24067;&#30340;&#20613;&#37324;&#21494;&#20998;&#26512;&#29305;&#24615;&#30340;&#37325;&#22797;&#27979;&#37327;&#30340;GFF&#26469;&#20272;&#35745;&#21152;&#26435;&#32593;&#32476;&#65288;&#31561;&#20215;&#22320;&#65292;&#20854;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#65289;&#30340;&#26032;&#22411;&#20272;&#35745;&#37327;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#26500;&#36896;&#30340;&#22797;&#25968;&#20540;&#32479;&#35745;&#37327;&#65292;&#36825;&#20123;&#32479;&#35745;&#37327;&#26412;&#36523;&#23601;&#20855;&#26377;&#30740;&#31350;&#20215;&#20540;&#12290;&#25105;&#20204;&#36890;&#36807;&#20855;&#20307;&#30340;&#24674;&#22797;&#20445;&#35777;&#21644;&#23545;&#25152;&#38656;&#37319;&#26679;&#30340;&#30028;&#38480;&#26469;&#35777;&#26126;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the problem of estimating the structure of a weighted network from repeated measurements of a Gaussian Graphical Model (GGM) on the network. In this vein, we consider GGMs whose covariance structures align with the geometry of the weighted network on which they are based. Such GGMs have been of longstanding interest in statistical physics, and are referred to as the Gaussian Free Field (GFF). In recent years, they have attracted considerable interest in the machine learning and theoretical computer science. In this work, we propose a novel estimator for the weighted network (equivalently, its Laplacian) from repeated measurements of a GFF on the network, based on the Fourier analytic properties of the Gaussian distribution. In this pursuit, our approach exploits complex-valued statistics constructed from observed data, that are of interest on their own right. We demonstrate the effectiveness of our estimator with concrete recovery guarantees and bounds on the required sa
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#22411;&#28151;&#21512;&#32593;&#32476;(RAHNet)&#29992;&#20110;&#38271;&#23614;&#22270;&#20998;&#31867;&#20219;&#21153;&#65292;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#31283;&#20581;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#26080;&#20559;&#30340;&#20998;&#31867;&#22120;&#65292;&#35299;&#20915;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#38271;&#23614;&#31867;&#21035;&#20998;&#24067;&#19979;&#30340;&#20559;&#24046;&#21644;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.02335</link><description>&lt;p&gt;
RAHNet: &#26816;&#32034;&#22686;&#24378;&#22411;&#28151;&#21512;&#32593;&#32476;&#29992;&#20110;&#38271;&#23614;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
RAHNet: Retrieval Augmented Hybrid Network for Long-tailed Graph Classification. (arXiv:2308.02335v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02335
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#22411;&#28151;&#21512;&#32593;&#32476;(RAHNet)&#29992;&#20110;&#38271;&#23614;&#22270;&#20998;&#31867;&#20219;&#21153;&#65292;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#31283;&#20581;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#26080;&#20559;&#30340;&#20998;&#31867;&#22120;&#65292;&#35299;&#20915;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#38271;&#23614;&#31867;&#21035;&#20998;&#24067;&#19979;&#30340;&#20559;&#24046;&#21644;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20998;&#31867;&#26159;&#35768;&#22810;&#23454;&#38469;&#22810;&#23186;&#20307;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#22270;&#21487;&#20197;&#34920;&#31034;&#21508;&#31181;&#22810;&#23186;&#20307;&#25968;&#25454;&#31867;&#22411;&#65292;&#22914;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#31038;&#20132;&#32593;&#32476;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#22312;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#24212;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#65292;&#20854;&#20013;&#31867;&#20998;&#24067;&#26159;&#24179;&#34913;&#30340;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#25968;&#25454;&#36890;&#24120;&#21576;&#29616;&#20986;&#38271;&#23614;&#31867;&#21035;&#20998;&#24067;&#65292;&#23548;&#33268;&#22312;&#20351;&#29992;GNN&#26102;&#23545;&#22836;&#37096;&#31867;&#21035;&#23384;&#22312;&#20559;&#24046;&#65292;&#19988;&#23545;&#23614;&#37096;&#31867;&#21035;&#30340;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#37325;&#26032;&#24179;&#34913;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#26410;&#33021;&#26126;&#30830;&#24341;&#20837;&#26032;&#30693;&#35782;&#65292;&#24182;&#29306;&#29298;&#20102;&#22836;&#37096;&#31867;&#21035;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#26816;&#32034;&#22686;&#24378;&#22411;&#28151;&#21512;&#32593;&#32476;(RAHNet)&#65292;&#20197;&#20998;&#31163;&#30340;&#26041;&#24335;&#32852;&#21512;&#23398;&#20064;&#31283;&#20581;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#26080;&#20559;&#30340;&#20998;&#31867;&#22120;&#12290;&#22312;&#29305;&#24449;&#25552;&#21462;&#22120;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22270;&#26816;&#32034;&#27169;&#22359;&#26469;&#25628;&#32034;&#30456;&#20851;&#22270;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph classification is a crucial task in many real-world multimedia applications, where graphs can represent various multimedia data types such as images, videos, and social networks. Previous efforts have applied graph neural networks (GNNs) in balanced situations where the class distribution is balanced. However, real-world data typically exhibit long-tailed class distributions, resulting in a bias towards the head classes when using GNNs and limited generalization ability over the tail classes. Recent approaches mainly focus on re-balancing different classes during model training, which fails to explicitly introduce new knowledge and sacrifices the performance of the head classes. To address these drawbacks, we propose a novel framework called Retrieval Augmented Hybrid Network (RAHNet) to jointly learn a robust feature extractor and an unbiased classifier in a decoupled manner. In the feature extractor training stage, we develop a graph retrieval module to search for relevant grap
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#39640;&#38454;&#24635;&#21464;&#24046;&#27491;&#21017;&#21270;&#30340;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35757;&#32451;&#38750;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#65292;&#36991;&#20813;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.02293</link><description>&lt;p&gt;
&#29992;&#27491;&#21017;&#21270;&#39640;&#38454;&#24635;&#21464;&#24046;&#30340;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#35757;&#32451;&#38750;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A stochastic optimization approach to train non-linear neural networks with regularization of higher-order total variation. (arXiv:2308.02293v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02293
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#39640;&#38454;&#24635;&#21464;&#24046;&#27491;&#21017;&#21270;&#30340;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35757;&#32451;&#38750;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#65292;&#36991;&#20813;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21253;&#25324;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20869;&#30340;&#39640;&#24230;&#34920;&#36798;&#30340;&#21442;&#25968;&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#24314;&#27169;&#22797;&#26434;&#27010;&#24565;&#65292;&#20294;&#35757;&#32451;&#36825;&#31181;&#39640;&#24230;&#38750;&#32447;&#24615;&#27169;&#22411;&#24050;&#30693;&#20250;&#23548;&#33268;&#20005;&#37325;&#30340;&#36807;&#25311;&#21512;&#39118;&#38505;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#19968;&#31181;k&#38454;&#24635;&#21464;&#24046;&#65288;k-TV&#65289;&#27491;&#21017;&#21270;&#65292;&#23427;&#34987;&#23450;&#20041;&#20026;&#35201;&#35757;&#32451;&#30340;&#21442;&#25968;&#27169;&#22411;&#30340;k&#38454;&#23548;&#25968;&#30340;&#24179;&#26041;&#31215;&#20998;&#65292;&#36890;&#36807;&#24809;&#32602;k-TV&#26469;&#20135;&#29983;&#19968;&#20010;&#26356;&#24179;&#28369;&#30340;&#20989;&#25968;&#65292;&#20174;&#32780;&#36991;&#20813;&#36807;&#25311;&#21512;&#12290;&#23613;&#31649;&#23558;k-TV&#39033;&#24212;&#29992;&#20110;&#19968;&#33324;&#30340;&#21442;&#25968;&#27169;&#22411;&#30001;&#20110;&#31215;&#20998;&#32780;&#23548;&#33268;&#35745;&#31639;&#22797;&#26434;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35757;&#32451;&#24102;&#26377;k-TV&#27491;&#21017;&#21270;&#30340;&#19968;&#33324;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#26174;&#24335;&#30340;&#25968;&#20540;&#31215;&#20998;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#32467;&#26500;&#20219;&#24847;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#65292;&#22240;&#20026;&#23427;&#21482;&#38656;&#35201;&#36827;&#34892;&#31616;&#21333;&#30340;&#38543;&#26426;&#26799;&#24230;&#20248;&#21270;&#21363;&#21487;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
While highly expressive parametric models including deep neural networks have an advantage to model complicated concepts, training such highly non-linear models is known to yield a high risk of notorious overfitting. To address this issue, this study considers a $k$th order total variation ($k$-TV) regularization, which is defined as the squared integral of the $k$th order derivative of the parametric models to be trained; penalizing the $k$-TV is expected to yield a smoother function, which is expected to avoid overfitting. While the $k$-TV terms applied to general parametric models are computationally intractable due to the integration, this study provides a stochastic optimization algorithm, that can efficiently train general models with the $k$-TV regularization without conducting explicit numerical integration. The proposed approach can be applied to the training of even deep neural networks whose structure is arbitrary, as it can be implemented by only a simple stochastic gradien
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#34394;&#25311;&#39118;&#38505;&#26368;&#23567;&#21270;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20196;&#20154;&#27822;&#20007;&#22320;&#31616;&#21333;&#19988;&#36890;&#29992;&#30340;&#25216;&#26415;&#65288;DuRM&#65289;&#65292;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#32463;&#39564;&#39564;&#35777;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DuRM&#21487;&#20197;&#36890;&#36807;&#22686;&#21152;&#26799;&#24230;&#30340;&#26041;&#24046;&#26469;&#20419;&#36827;&#27169;&#22411;&#30340;&#27867;&#21270;&#25928;&#26524;&#65292;&#24182;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;DuRM&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.02287</link><description>&lt;p&gt;
&#36890;&#36807;&#34394;&#25311;&#39118;&#38505;&#26368;&#23567;&#21270;&#23454;&#29616;&#20196;&#20154;&#27822;&#20007;&#30340;&#27169;&#22411;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Frustratingly Easy Model Generalization by Dummy Risk Minimization. (arXiv:2308.02287v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02287
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#34394;&#25311;&#39118;&#38505;&#26368;&#23567;&#21270;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20196;&#20154;&#27822;&#20007;&#22320;&#31616;&#21333;&#19988;&#36890;&#29992;&#30340;&#25216;&#26415;&#65288;DuRM&#65289;&#65292;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#32463;&#39564;&#39564;&#35777;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DuRM&#21487;&#20197;&#36890;&#36807;&#22686;&#21152;&#26799;&#24230;&#30340;&#26041;&#24046;&#26469;&#20419;&#36827;&#27169;&#22411;&#30340;&#27867;&#21270;&#25928;&#26524;&#65292;&#24182;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;DuRM&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#65292;&#23427;&#30340;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#34394;&#25311;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;DuRM&#65289;&#65292;&#19968;&#31181;&#20196;&#20154;&#27822;&#20007;&#22320;&#31616;&#21333;&#21644;&#36890;&#29992;&#30340;&#25216;&#26415;&#26469;&#25552;&#39640;ERM&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;DuRM&#38750;&#24120;&#31616;&#21333;&#23454;&#29616;&#65306;&#21482;&#38656;&#25193;&#22823;&#36755;&#20986;logits&#30340;&#32500;&#24230;&#65292;&#28982;&#21518;&#20351;&#29992;&#26631;&#20934;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#21644;&#32463;&#39564;&#39564;&#35777;DuRM&#30340;&#26377;&#25928;&#24615;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DuRM&#23548;&#33268;&#26356;&#22823;&#30340;&#26799;&#24230;&#26041;&#24046;&#65292;&#36890;&#36807;&#35266;&#23519;&#26356;&#22909;&#30340;&#24179;&#22374;&#23616;&#37096;&#26368;&#23567;&#20540;&#20419;&#36827;&#27169;&#22411;&#27867;&#21270;&#12290;&#20174;&#32463;&#39564;&#19978;&#35762;&#65292;&#25105;&#20204;&#38024;&#23545;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#27169;&#24577;&#21644;&#32593;&#32476;&#26550;&#26500;&#65292;&#22312;&#19981;&#21516;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;DuRM&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#20256;&#32479;&#20998;&#31867;&#65292;&#35821;&#20041;&#20998;&#21106;&#65292;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#65292;&#23545;&#25239;&#35757;&#32451;&#21644;&#38271;&#23614;&#35782;&#21035;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;DuRM&#33021;&#22815;&#25345;&#32493;&#25913;&#36827;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empirical risk minimization (ERM) is a fundamental machine learning paradigm. However, its generalization ability is limited in various tasks. In this paper, we devise Dummy Risk Minimization (DuRM), a frustratingly easy and general technique to improve the generalization of ERM. DuRM is extremely simple to implement: just enlarging the dimension of the output logits and then optimizing using standard gradient descent. Moreover, we validate the efficacy of DuRM on both theoretical and empirical analysis. Theoretically, we show that DuRM derives greater variance of the gradient, which facilitates model generalization by observing better flat local minima. Empirically, we conduct evaluations of DuRM across different datasets, modalities, and network architectures on diverse tasks, including conventional classification, semantic segmentation, out-of-distribution generalization, adverserial training, and long-tailed recognition. Results demonstrate that DuRM could consistently improve the 
&lt;/p&gt;</description></item><item><title>DIVERSIFY&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#31163;&#32676;&#26816;&#27979;&#21644;&#25512;&#24191;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#38598;&#20013;&#30340;&#23376;&#22495;&#26469;&#23545;&#25239;&#26102;&#38388;&#24207;&#21015;&#30340;&#38750;&#24179;&#31283;&#24615;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#36807;&#31243;&#20943;&#23567;&#28508;&#22312;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2308.02282</link><description>&lt;p&gt;
DIVERSIFY: &#19968;&#33324;&#21270;&#26102;&#38388;&#24207;&#21015;&#31163;&#32676;&#26816;&#27979;&#21644;&#25512;&#24191;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DIVERSIFY: A General Framework for Time Series Out-of-distribution Detection and Generalization. (arXiv:2308.02282v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02282
&lt;/p&gt;
&lt;p&gt;
DIVERSIFY&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#31163;&#32676;&#26816;&#27979;&#21644;&#25512;&#24191;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#38598;&#20013;&#30340;&#23376;&#22495;&#26469;&#23545;&#25239;&#26102;&#38388;&#24207;&#21015;&#30340;&#38750;&#24179;&#31283;&#24615;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#36807;&#31243;&#20943;&#23567;&#28508;&#22312;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20173;&#28982;&#26159;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#27169;&#24577;&#20043;&#19968;&#12290;&#30001;&#20110;&#20854;&#38750;&#24179;&#31283;&#24615;&#36136;&#65292;&#26102;&#38388;&#24207;&#21015;&#30340;&#31163;&#32676;&#26816;&#27979;&#21644;&#25512;&#24191;&#24448;&#24448;&#21463;&#21040;&#22256;&#25200;&#65292;&#21363;&#20998;&#24067;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#21160;&#24577;&#20998;&#24067;&#32473;&#29616;&#26377;&#31639;&#27861;&#24102;&#26469;&#20102;&#24456;&#22823;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#20027;&#35201;&#20851;&#27880;&#39046;&#22495;&#20449;&#24687;&#24050;&#30693;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#23581;&#35797;&#21033;&#29992;&#25968;&#25454;&#38598;&#20013;&#30340;&#23376;&#22495;&#26469;&#23545;&#25239;&#38750;&#24179;&#31283;&#24615;&#24341;&#21457;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#24191;&#20041;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DIVERSIFY&#65292;&#19968;&#20010;&#38024;&#23545;&#21160;&#24577;&#26102;&#38388;&#24207;&#21015;&#20998;&#24067;&#30340;&#31163;&#32676;&#26816;&#27979;&#21644;&#25512;&#24191;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;DIVERSIFY&#37319;&#29992;&#36845;&#20195;&#36807;&#31243;&#65306;&#39318;&#20808;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#33719;&#24471;&#8220;&#26368;&#22351;&#24773;&#20917;&#8221;&#28508;&#22312;&#20998;&#24067;&#24773;&#26223;&#65292;&#28982;&#21518;&#20943;&#23567;&#36825;&#20123;&#28508;&#22312;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#36890;&#36807;&#32467;&#21512;&#29616;&#26377;&#30340;&#31163;&#32676;&#26816;&#27979;&#26041;&#27861;&#23454;&#29616;&#20102;DIVERSIFY&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series remains one of the most challenging modalities in machine learning research. The out-of-distribution (OOD) detection and generalization on time series tend to suffer due to its non-stationary property, i.e., the distribution changes over time. The dynamic distributions inside time series pose great challenges to existing algorithms to identify invariant distributions since they mainly focus on the scenario where the domain information is given as prior knowledge. In this paper, we attempt to exploit subdomains within a whole dataset to counteract issues induced by non-stationary for generalized representation learning. We propose DIVERSIFY, a general framework, for OOD detection and generalization on dynamic distributions of time series. DIVERSIFY takes an iterative process: it first obtains the "worst-case" latent distribution scenario via adversarial training, then reduces the gap between these latent distributions. We implement DIVERSIFY via combining existing OOD detect
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#29256;&#26412;&#30340;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#21644;&#36817;&#31471;&#26799;&#24230;&#26041;&#27861;&#65288;ProxGD&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#23616;&#37096;&#26354;&#29575;&#20449;&#24687;&#23436;&#20840;&#33258;&#36866;&#24212;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#25910;&#25947;&#24615;&#65292;&#19988;&#20801;&#35768;&#20351;&#29992;&#26356;&#22823;&#30340;&#27493;&#38271;&#12290;</title><link>http://arxiv.org/abs/2308.02261</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#36817;&#31471;&#26799;&#24230;&#26041;&#27861;&#30340;&#20984;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Adaptive Proximal Gradient Method for Convex Optimization. (arXiv:2308.02261v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#29256;&#26412;&#30340;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#21644;&#36817;&#31471;&#26799;&#24230;&#26041;&#27861;&#65288;ProxGD&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#23616;&#37096;&#26354;&#29575;&#20449;&#24687;&#23436;&#20840;&#33258;&#36866;&#24212;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#25910;&#25947;&#24615;&#65292;&#19988;&#20801;&#35768;&#20351;&#29992;&#26356;&#22823;&#30340;&#27493;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20984;&#20248;&#21270;&#20013;&#30340;&#20004;&#20010;&#22522;&#26412;&#19968;&#38454;&#31639;&#27861;&#65292;&#21363;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#21644;&#36817;&#31471;&#26799;&#24230;&#26041;&#27861;&#65288;ProxGD&#65289;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#36890;&#36807;&#21033;&#29992;&#24179;&#28369;&#20989;&#25968;&#30340;&#23616;&#37096;&#26354;&#29575;&#20449;&#24687;&#65292;&#20351;&#36825;&#20123;&#31639;&#27861;&#23436;&#20840;&#33258;&#36866;&#24212;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#35266;&#23519;&#21040;&#30340;&#26799;&#24230;&#24046;&#24322;&#30340;&#33258;&#36866;&#24212;&#29256;&#26412;&#30340;GD&#21644;ProxGD&#65292;&#22240;&#27492;&#19981;&#20250;&#22686;&#21152;&#35745;&#31639;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#20165;&#20551;&#35774;&#26799;&#24230;&#30340;&#23616;&#37096;Lipschitz&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;&#21478;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#29256;&#26412;&#20801;&#35768;&#20351;&#29992;&#27604;[MM20]&#26368;&#21021;&#24314;&#35758;&#30340;&#26356;&#22823;&#30340;&#27493;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore two fundamental first-order algorithms in convex optimization, namely, gradient descent (GD) and proximal gradient method (ProxGD). Our focus is on making these algorithms entirely adaptive by leveraging local curvature information of smooth functions. We propose adaptive versions of GD and ProxGD that are based on observed gradient differences and, thus, have no added computational costs. Moreover, we prove convergence of our methods assuming only local Lipschitzness of the gradient. In addition, the proposed versions allow for even larger stepsizes than those initially suggested in [MM20].
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#23545;&#38889;&#22269;&#27665;&#27468;&#36827;&#34892;&#35745;&#31639;&#26426;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#24182;&#20197;&#8220;&#25176;&#37324;&#8221;&#20316;&#20026;&#26696;&#20363;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#8220;&#25176;&#37324;&#8221;&#30340;&#29305;&#24449;&#65292;&#20026;&#30740;&#31350;&#38889;&#22269;&#27665;&#27468;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2308.02249</link><description>&lt;p&gt;
&#23547;&#25214;&#25176;&#37324;&#65306;&#33258;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#20998;&#26512;&#38889;&#22269;&#27665;&#27468;
&lt;/p&gt;
&lt;p&gt;
Finding Tori: Self-supervised Learning for Analyzing Korean Folk Song. (arXiv:2308.02249v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#23545;&#38889;&#22269;&#27665;&#27468;&#36827;&#34892;&#35745;&#31639;&#26426;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#24182;&#20197;&#8220;&#25176;&#37324;&#8221;&#20316;&#20026;&#26696;&#20363;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#8220;&#25176;&#37324;&#8221;&#30340;&#29305;&#24449;&#65292;&#20026;&#30740;&#31350;&#38889;&#22269;&#27665;&#27468;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#23545;&#22823;&#32422;700&#23567;&#26102;&#30340;&#38889;&#22269;&#27665;&#27468;&#22330;&#24405;&#38899;&#25968;&#25454;&#38598;&#36827;&#34892;&#35745;&#31639;&#26426;&#20998;&#26512;&#65292;&#36825;&#20123;&#24405;&#38899;&#22823;&#22810;&#25968;&#22312;1980-90&#24180;&#20195;&#24405;&#21046;&#12290;&#30001;&#20110;&#22823;&#37096;&#20998;&#27468;&#26354;&#37117;&#26159;&#30001;&#38750;&#19987;&#19994;&#38899;&#20048;&#23478;&#22312;&#27809;&#26377;&#20276;&#22863;&#30340;&#24773;&#20917;&#19979;&#28436;&#21809;&#30340;&#65292;&#22240;&#27492;&#35813;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19968;&#20123;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#38899;&#35843;&#36718;&#24275;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#24182;&#20998;&#26512;&#20102;&#27169;&#22411;&#22914;&#20309;&#25429;&#25417;&#21040;&#8220;&#25176;&#37324;&#8221;&#36825;&#19968;&#38899;&#20048;&#27010;&#24565;&#65292;&#23427;&#26159;&#19968;&#20010;&#22522;&#20110;&#29305;&#23450;&#38899;&#38454;&#12289;&#35013;&#39280;&#38899;&#21644;&#25104;&#35821;&#21270;&#26059;&#24459;&#36718;&#24275;&#30340;&#20998;&#31867;&#31995;&#32479;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#38899;&#39640;&#30452;&#26041;&#22270;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#8220;&#25176;&#37324;&#8221;&#30340;&#29305;&#24449;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29616;&#26377;&#23398;&#26415;&#30028;&#25552;&#20986;&#30340;&#38899;&#20048;&#35752;&#35770;&#22312;&#38889;&#22269;&#27665;&#27468;&#23454;&#38469;&#24405;&#38899;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a computational analysis of the field recording dataset of approximately 700 hours of Korean folk songs, which were recorded around 1980-90s. Because most of the songs were sung by non-expert musicians without accompaniment, the dataset provides several challenges. To address this challenge, we utilized self-supervised learning with convolutional neural network based on pitch contour, then analyzed how the musical concept of tori, a classification system defined by a specific scale, ornamental notes, and an idiomatic melodic contour, is captured by the model. The experimental result shows that our approach can better capture the characteristics of tori compared to traditional pitch histograms. Using our approaches, we have examined how musical discussions proposed in existing academia manifest in the actual field recordings of Korean folk songs.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#27491;&#35268;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#22411;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#19968;&#27425;&#36801;&#31227;&#23398;&#20064;&#24182;&#23545;&#22810;&#20010;EDFA&#22120;&#20214;&#30340;&#27874;&#38271;&#30456;&#20851;&#22686;&#30410;&#36827;&#34892;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2308.02233</link><description>&lt;p&gt;
&#33258;&#27491;&#35268;&#21270;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;EDFA&#27874;&#38271;&#30456;&#20851;&#22686;&#30410;&#24314;&#27169;&#30340;&#19968;&#27425;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-Normalizing Neural Network, Enabling One Shot Transfer Learning for Modeling EDFA Wavelength Dependent Gain. (arXiv:2308.02233v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#27491;&#35268;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#22411;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#19968;&#27425;&#36801;&#31227;&#23398;&#20064;&#24182;&#23545;&#22810;&#20010;EDFA&#22120;&#20214;&#30340;&#27874;&#38271;&#30456;&#20851;&#22686;&#30410;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21322;&#30417;&#30563;&#12289;&#33258;&#27491;&#35268;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#22411;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#20010;EDFA&#22120;&#20214;&#30340;&#27874;&#38271;&#30456;&#20851;&#22686;&#30410;&#24314;&#27169;&#65292;&#24182;&#23454;&#29616;&#20102;&#19968;&#27425;&#36801;&#31227;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;Open Ireland&#21644;COSMOS&#27979;&#35797;&#24179;&#21488;&#19978;&#23545;22&#20010;EDFA&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#21363;&#20351;&#22312;&#19981;&#21516;&#30340;&#25918;&#22823;&#22120;&#31867;&#22411;&#19979;&#65292;&#20063;&#33021;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel ML framework for modeling the wavelength-dependent gain of multiple EDFAs, based on semi-supervised, self-normalizing neural networks, enabling one-shot transfer learning. Our experiments on 22 EDFAs in Open Ireland and COSMOS testbeds show high-accuracy transfer-learning even when operated across different amplifier types.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20284;&#28982;&#27604;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#26500;&#24314;&#19981;&#23545;&#31216;&#21306;&#38388;&#65292;&#32771;&#34385;&#20102;&#35757;&#32451;&#26102;&#38388;&#12289;&#32593;&#32476;&#26550;&#26500;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#31561;&#22240;&#32032;&#12290;&#23613;&#31649;&#26041;&#27861;&#23454;&#29616;&#26114;&#36149;&#65292;&#20294;&#22312;&#29305;&#23450;&#39046;&#22495;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.02221</link><description>&lt;p&gt;
&#22522;&#20110;&#20284;&#28982;&#27604;&#30340;&#32622;&#20449;&#21306;&#38388;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Likelihood-ratio-based confidence intervals for neural networks. (arXiv:2308.02221v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20284;&#28982;&#27604;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#26500;&#24314;&#19981;&#23545;&#31216;&#21306;&#38388;&#65292;&#32771;&#34385;&#20102;&#35757;&#32451;&#26102;&#38388;&#12289;&#32593;&#32476;&#26550;&#26500;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#31561;&#22240;&#32032;&#12290;&#23613;&#31649;&#26041;&#27861;&#23454;&#29616;&#26114;&#36149;&#65292;&#20294;&#22312;&#29305;&#23450;&#39046;&#22495;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20284;&#28982;&#27604;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;DeepLR&#65292;&#20855;&#26377;&#35768;&#22810;&#20248;&#28857;&#65292;&#20854;&#20013;&#26368;&#37325;&#35201;&#30340;&#26159;&#33021;&#22815;&#26500;&#24314;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#21306;&#22495;&#20013;&#25193;&#23637;&#30340;&#19981;&#23545;&#31216;&#21306;&#38388;&#65292;&#24182;&#22266;&#26377;&#22320;&#32771;&#34385;&#20102;&#35757;&#32451;&#26102;&#38388;&#12289;&#32593;&#32476;&#26550;&#26500;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#31561;&#22240;&#32032;&#12290;&#23613;&#31649;&#25215;&#35748;&#30446;&#21069;&#30340;&#26041;&#27861;&#23454;&#29616;&#23545;&#20110;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#26469;&#35828;&#36807;&#20110;&#26114;&#36149;&#65292;&#20294;&#22312;&#35832;&#22914;&#21307;&#23398;&#39044;&#27979;&#25110;&#22825;&#20307;&#29289;&#29702;&#23398;&#31561;&#29305;&#23450;&#39046;&#22495;&#20013;&#65292;&#21487;&#38752;&#30340;&#21333;&#20010;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21487;&#33021;&#24050;&#32463;&#21512;&#29702;&#12290;&#36825;&#39033;&#24037;&#20316;&#31361;&#20986;&#20102;&#22522;&#20110;&#20284;&#28982;&#27604;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#37325;&#35201;&#28508;&#21147;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#24320;&#36767;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a first implementation of a novel likelihood-ratio-based approach for constructing confidence intervals for neural networks. Our method, called DeepLR, offers several qualitative advantages: most notably, the ability to construct asymmetric intervals that expand in regions with a limited amount of data, and the inherent incorporation of factors such as the amount of training time, network architecture, and regularization techniques. While acknowledging that the current implementation of the method is prohibitively expensive for many deep-learning applications, the high cost may already be justified in specific fields like medical predictions or astrophysics, where a reliable uncertainty estimate for a single prediction is essential. This work highlights the significant potential of a likelihood-ratio-based uncertainty estimate and establishes a promising avenue for future research.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#35199;&#29677;&#29273;&#35821;&#20020;&#24202;&#35821;&#35328;&#27169;&#22411;&#30340;&#24212;&#29992;&#65292;&#22238;&#39038;&#20102;17&#20010;&#19987;&#27880;&#20110;&#20020;&#24202;&#20219;&#21153;&#30340;&#35821;&#26009;&#24211;&#30340;&#36129;&#29486;&#65292;&#24182;&#23545;&#26368;&#30456;&#20851;&#30340;&#35199;&#29677;&#29273;&#35821;&#35821;&#35328;&#27169;&#22411;&#21644;&#35199;&#29677;&#29273;&#20020;&#24202;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#24443;&#24213;&#27604;&#36739;&#65292;&#25552;&#20379;&#20102;3000&#22810;&#20010;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#20415;&#20110;&#26410;&#26469;&#30340;&#30740;&#31350;&#21644;&#25361;&#25112;&#65292;&#25152;&#26377;&#27979;&#35797;&#36807;&#30340;&#35821;&#26009;&#24211;&#21644;&#26368;&#20339;&#27169;&#22411;&#37117;&#34987;&#20197;&#21487;&#35775;&#38382;&#30340;&#26041;&#24335;&#20844;&#24320;&#12290;</title><link>http://arxiv.org/abs/2308.02199</link><description>&lt;p&gt;
&#35199;&#29677;&#29273;&#20020;&#24202;&#35821;&#35328;&#27169;&#22411;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey of Spanish Clinical Language Models. (arXiv:2308.02199v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02199
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#35199;&#29677;&#29273;&#35821;&#20020;&#24202;&#35821;&#35328;&#27169;&#22411;&#30340;&#24212;&#29992;&#65292;&#22238;&#39038;&#20102;17&#20010;&#19987;&#27880;&#20110;&#20020;&#24202;&#20219;&#21153;&#30340;&#35821;&#26009;&#24211;&#30340;&#36129;&#29486;&#65292;&#24182;&#23545;&#26368;&#30456;&#20851;&#30340;&#35199;&#29677;&#29273;&#35821;&#35821;&#35328;&#27169;&#22411;&#21644;&#35199;&#29677;&#29273;&#20020;&#24202;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#24443;&#24213;&#27604;&#36739;&#65292;&#25552;&#20379;&#20102;3000&#22810;&#20010;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#20415;&#20110;&#26410;&#26469;&#30340;&#30740;&#31350;&#21644;&#25361;&#25112;&#65292;&#25152;&#26377;&#27979;&#35797;&#36807;&#30340;&#35821;&#26009;&#24211;&#21644;&#26368;&#20339;&#27169;&#22411;&#37117;&#34987;&#20197;&#21487;&#35775;&#38382;&#30340;&#26041;&#24335;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#32858;&#28966;&#20110;&#20351;&#29992;&#32534;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#20915;&#35199;&#29677;&#29273;&#35821;&#20020;&#24202;&#39046;&#22495;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;17&#20010;&#20027;&#35201;&#19987;&#27880;&#20110;&#20020;&#24202;&#20219;&#21153;&#30340;&#35821;&#26009;&#24211;&#30340;&#36129;&#29486;&#65292;&#28982;&#21518;&#21015;&#20986;&#20102;&#26368;&#30456;&#20851;&#30340;&#35199;&#29677;&#29273;&#35821;&#35821;&#35328;&#27169;&#22411;&#21644;&#35199;&#29677;&#29273;&#20020;&#24202;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#21487;&#29992;&#35821;&#26009;&#24211;&#30340;&#31934;&#36873;&#23376;&#38598;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#27604;&#36739;&#65292;&#20197;&#25214;&#21040;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#65307;&#24635;&#20849;&#36229;&#36807;3000&#20010;&#27169;&#22411;&#34987;&#38024;&#23545;&#36825;&#39033;&#30740;&#31350;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#25152;&#26377;&#27979;&#35797;&#30340;&#35821;&#26009;&#24211;&#21644;&#26368;&#20339;&#27169;&#22411;&#37117;&#20197;&#21487;&#35775;&#38382;&#30340;&#26041;&#24335;&#20844;&#24320;&#65292;&#20197;&#20415;&#29420;&#31435;&#22242;&#38431;&#21487;&#20197;&#37325;&#29616;&#32467;&#26524;&#25110;&#22312;&#26410;&#26469;&#21019;&#24314;&#26032;&#30340;&#35199;&#29677;&#29273;&#20020;&#24202;&#35821;&#35328;&#27169;&#22411;&#26102;&#36827;&#34892;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey focuses in encoder Language Models for solving tasks in the clinical domain in the Spanish language. We review the contributions of 17 corpora focused mainly in clinical tasks, then list the most relevant Spanish Language Models and Spanish Clinical Language models. We perform a thorough comparison of these models by benchmarking them over a curated subset of the available corpora, in order to find the best-performing ones; in total more than 3000 models were fine-tuned for this study. All the tested corpora and the best models are made publically available in an accessible way, so that the results can be reproduced by independent teams or challenged in the future when new Spanish Clinical Language models are created.
&lt;/p&gt;</description></item><item><title>AutoML4ETC&#26159;&#19968;&#20010;&#33258;&#21160;&#35774;&#35745;&#39640;&#25928;&#19988;&#39640;&#24615;&#33021;&#31070;&#32463;&#26550;&#26500;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#12290;&#20854;&#36890;&#36807;&#23450;&#20041;&#26032;&#39062;&#30340;&#25628;&#32034;&#31354;&#38388;&#21644;&#20351;&#29992;&#19981;&#21516;&#30340;&#25628;&#32034;&#31574;&#30053;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2308.02182</link><description>&lt;p&gt;
AutoML4ETC: &#33258;&#21160;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#23454;&#29616;&#29616;&#23454;&#19990;&#30028;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
AutoML4ETC: Automated Neural Architecture Search for Real-World Encrypted Traffic Classification. (arXiv:2308.02182v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02182
&lt;/p&gt;
&lt;p&gt;
AutoML4ETC&#26159;&#19968;&#20010;&#33258;&#21160;&#35774;&#35745;&#39640;&#25928;&#19988;&#39640;&#24615;&#33021;&#31070;&#32463;&#26550;&#26500;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#12290;&#20854;&#36890;&#36807;&#23450;&#20041;&#26032;&#39062;&#30340;&#25628;&#32034;&#31354;&#38388;&#21644;&#20351;&#29992;&#19981;&#21516;&#30340;&#25628;&#32034;&#31574;&#30053;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#39564;&#29615;&#22659;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21152;&#23494;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;DL&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#38543;&#26102;&#38388;&#19981;&#21487;&#36991;&#20813;&#22320;&#19979;&#38477;&#12290;&#20165;&#20165;&#23545;&#26032;&#25968;&#25454;&#38598;&#36827;&#34892;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#21482;&#33021;&#37096;&#20998;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#25163;&#21160;&#35843;&#25972;&#27169;&#22411;&#26550;&#26500;&#20197;&#28385;&#36275;&#26032;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#26399;&#26395;&#32791;&#26102;&#19988;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24037;&#20855;AutoML4ETC&#65292;&#29992;&#20110;&#33258;&#21160;&#35774;&#35745;&#39640;&#25928;&#19988;&#39640;&#24615;&#33021;&#30340;&#31070;&#32463;&#26550;&#26500;&#20197;&#36827;&#34892;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#39062;&#32780;&#24378;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#19987;&#38376;&#38024;&#23545;&#20351;&#29992;&#25968;&#25454;&#21253;&#22836;&#23383;&#33410;&#36827;&#34892;&#36817;&#23454;&#26102;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#12290;&#36890;&#36807;&#22312;&#25628;&#32034;&#31354;&#38388;&#19978;&#20351;&#29992;&#19981;&#21516;&#30340;&#25628;&#32034;&#31574;&#30053;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;AutoML4ETC&#29983;&#25104;&#30340;&#31070;&#32463;&#26550;&#26500;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#22343;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#22120;&#65292;&#21253;&#25324;&#20844;&#20849;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) has been successfully applied to encrypted network traffic classification in experimental settings. However, in production use, it has been shown that a DL classifier's performance inevitably decays over time. Re-training the model on newer datasets has been shown to only partially improve its performance. Manually re-tuning the model architecture to meet the performance expectations on newer datasets is time-consuming and requires domain expertise. We propose AutoML4ETC, a novel tool to automatically design efficient and high-performing neural architectures for encrypted traffic classification. We define a novel, powerful search space tailored specifically for the near real-time classification of encrypted traffic using packet header bytes. We show that with different search strategies over our search space, AutoML4ETC generates neural architectures that outperform the state-of-the-art encrypted traffic classifiers on several datasets, including public benchmark dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25193;&#23637;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#24182;&#20197;&#32959;&#30244;&#23398;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#20808;&#36827;&#30340;LLMs&#33021;&#22815;&#22788;&#29702;&#20020;&#24202;&#35797;&#39564;&#30340;&#22797;&#26434;&#26465;&#20214;&#21644;&#21305;&#37197;&#36923;&#36753;&#65292;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#24615;&#33021;&#26174;&#33879;&#25552;&#21319;&#65292;&#24182;&#21487;&#20316;&#20026;&#20154;&#24037;&#36741;&#21161;&#31579;&#36873;&#24739;&#32773;-&#35797;&#39564;&#20505;&#36873;&#20154;&#30340;&#21021;&#27493;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.02180</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#65306;&#20197;&#32959;&#30244;&#23398;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Scaling Clinical Trial Matching Using Large Language Models: A Case Study in Oncology. (arXiv:2308.02180v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25193;&#23637;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#24182;&#20197;&#32959;&#30244;&#23398;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#20808;&#36827;&#30340;LLMs&#33021;&#22815;&#22788;&#29702;&#20020;&#24202;&#35797;&#39564;&#30340;&#22797;&#26434;&#26465;&#20214;&#21644;&#21305;&#37197;&#36923;&#36753;&#65292;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#24615;&#33021;&#26174;&#33879;&#25552;&#21319;&#65292;&#24182;&#21487;&#20316;&#20026;&#20154;&#24037;&#36741;&#21161;&#31579;&#36873;&#24739;&#32773;-&#35797;&#39564;&#20505;&#36873;&#20154;&#30340;&#21021;&#27493;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#26159;&#21307;&#30103;&#20256;&#36882;&#21644;&#21457;&#29616;&#20013;&#30340;&#20851;&#38190;&#36807;&#31243;&#12290;&#23454;&#38469;&#19978;&#65292;&#30001;&#20110;&#24222;&#22823;&#30340;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#21644;&#19981;&#21487;&#25193;&#23637;&#30340;&#25163;&#21160;&#22788;&#29702;&#65292;&#35813;&#36807;&#31243;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#20197;&#32959;&#30244;&#23398;&#20026;&#37325;&#28857;&#39046;&#22495;&#65292;&#23545;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25193;&#23637;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22522;&#20110;&#19968;&#20010;&#27491;&#22312;&#32654;&#22269;&#19968;&#20010;&#22823;&#22411;&#21307;&#30103;&#32593;&#32476;&#36827;&#34892;&#27979;&#35797;&#37096;&#32626;&#30340;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#31995;&#32479;&#12290;&#21021;&#27493;&#32467;&#26524;&#20196;&#20154;&#40723;&#33310;&#65306;&#20808;&#36827;&#30340;LLM&#65288;&#22914;GPT-4&#65289;&#21487;&#20197;&#31435;&#21363;&#36830;&#25509;&#20020;&#24202;&#35797;&#39564;&#30340;&#22797;&#26434;&#30340;&#21512;&#26684;&#26465;&#20214;&#65292;&#24182;&#25552;&#21462;&#22797;&#26434;&#30340;&#21305;&#37197;&#36923;&#36753;&#65288;&#20363;&#22914;&#23884;&#22871;&#30340;AND/OR/NOT&#65289;&#12290;&#34429;&#28982;&#20173;&#19981;&#23436;&#32654;&#65292;LLM&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#20197;&#21069;&#30340;&#24378;&#22522;&#20934;&#32447;&#65292;&#24182;&#21487;&#33021;&#20316;&#20026;&#22312;&#20154;&#19982;&#20154;&#20043;&#38388;&#36827;&#34892;&#20505;&#36873;&#24739;&#32773;-&#35797;&#39564;&#21010;&#20998;&#30340;&#21021;&#27493;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;&#19968;&#20123;&#24212;&#29992;LLM&#36827;&#34892;&#31471;&#21040;&#31471;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#30340;&#37325;&#35201;&#22686;&#38271;&#39046;&#22495;&#65292;&#20363;&#22914;&#19978;&#19979;&#25991;&#38480;&#21046;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical trial matching is a key process in health delivery and discovery. In practice, it is plagued by overwhelming unstructured data and unscalable manual processing. In this paper, we conduct a systematic study on scaling clinical trial matching using large language models (LLMs), with oncology as the focus area. Our study is grounded in a clinical trial matching system currently in test deployment at a large U.S. health network. Initial findings are promising: out of box, cutting-edge LLMs, such as GPT-4, can already structure elaborate eligibility criteria of clinical trials and extract complex matching logic (e.g., nested AND/OR/NOT). While still far from perfect, LLMs substantially outperform prior strong baselines and may serve as a preliminary solution to help triage patient-trial candidates with humans in the loop. Our study also reveals a few significant growth areas for applying LLMs to end-to-end clinical trial matching, such as context limitation and accuracy, especially
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22686;&#24378;&#20102;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#26230;&#20307;&#32467;&#26500;&#29983;&#25104;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#21435;&#22122;&#21407;&#23376;&#22352;&#26631;&#24182;&#29983;&#25104;&#36136;&#37327;&#30456;&#24403;&#30340;&#26230;&#20307;&#32467;&#26500;&#65292;&#24182;&#19988;&#33021;&#22815;&#29983;&#25104;&#19982;&#30495;&#23454;&#22522;&#24577;&#26356;&#25509;&#36817;&#30340;&#32467;&#26500;&#65292;&#33021;&#37327;&#20934;&#30830;&#24615;&#26126;&#26174;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2308.02165</link><description>&lt;p&gt;
&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22686;&#24378;&#20102;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#26230;&#20307;&#32467;&#26500;&#29983;&#25104;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Diffusion probabilistic models enhance variational autoencoder for crystal structure generative modeling. (arXiv:2308.02165v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02165
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22686;&#24378;&#20102;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#26230;&#20307;&#32467;&#26500;&#29983;&#25104;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#21435;&#22122;&#21407;&#23376;&#22352;&#26631;&#24182;&#29983;&#25104;&#36136;&#37327;&#30456;&#24403;&#30340;&#26230;&#20307;&#32467;&#26500;&#65292;&#24182;&#19988;&#33021;&#22815;&#29983;&#25104;&#19982;&#30495;&#23454;&#22522;&#24577;&#26356;&#25509;&#36817;&#30340;&#32467;&#26500;&#65292;&#33021;&#37327;&#20934;&#30830;&#24615;&#26126;&#26174;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26230;&#20307;&#25193;&#25955;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;CDVAE&#65289;&#26159;&#19968;&#31181;&#21033;&#29992;&#20998;&#25968;&#21305;&#37197;&#29983;&#25104;&#20445;&#30041;&#26230;&#20307;&#23545;&#31216;&#24615;&#30340;&#36924;&#30495;&#26230;&#20307;&#32467;&#26500;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26032;&#39062;&#30340;&#25193;&#25955;&#27010;&#29575;&#65288;DP&#65289;&#27169;&#22411;&#26469;&#21435;&#22122;&#21407;&#23376;&#22352;&#26631;&#65292;&#32780;&#19981;&#26159;&#37319;&#29992;CDVAE&#20013;&#30340;&#26631;&#20934;&#20998;&#25968;&#21305;&#37197;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;DP-CDVAE&#27169;&#22411;&#21487;&#20197;&#37325;&#24314;&#21644;&#29983;&#25104;&#22312;&#36136;&#37327;&#19978;&#19982;&#21407;&#22987;CDVAE&#30456;&#24403;&#30340;&#26230;&#20307;&#32467;&#26500;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#24403;&#23558;DP-CDVAE&#27169;&#22411;&#29983;&#25104;&#30340;&#30899;&#32467;&#26500;&#19982;&#20174;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#35745;&#31639;&#24471;&#21040;&#30340;&#24347;&#35947;&#32467;&#26500;&#36827;&#34892;&#27604;&#36739;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;DP-CDVAE&#29983;&#25104;&#30340;&#32467;&#26500;&#19982;&#20854;&#30456;&#24212;&#22522;&#24577;&#26356;&#25509;&#36817;&#12290;&#36825;&#20123;&#32467;&#26500;&#19982;&#30495;&#23454;&#22522;&#24577;&#20043;&#38388;&#30340;&#33021;&#37327;&#24046;&#24179;&#22343;&#27604;&#21407;&#22987;CDVAE&#29983;&#25104;&#30340;&#33021;&#37327;&#24046;&#20302;&#20102;68.1 meV/&#21407;&#23376;&#12290;&#36825;&#31181;&#33021;&#37327;&#20934;&#30830;&#24615;&#30340;&#26174;&#33879;&#25913;&#36827;&#20984;&#26174;&#20102;&#20854;&#21019;&#26032;&#24615;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
The crystal diffusion variational autoencoder (CDVAE) is a machine learning model that leverages score matching to generate realistic crystal structures that preserve crystal symmetry. In this study, we leverage novel diffusion probabilistic (DP) models to denoise atomic coordinates rather than adopting the standard score matching approach in CDVAE. Our proposed DP-CDVAE model can reconstruct and generate crystal structures whose qualities are statistically comparable to those of the original CDVAE. Furthermore, notably, when comparing the carbon structures generated by the DP-CDVAE model with relaxed structures obtained from density functional theory calculations, we find that the DP-CDVAE generated structures are remarkably closer to their respective ground states. The energy differences between these structures and the true ground states are, on average, 68.1 meV/atom lower than those generated by the original CDVAE. This significant improvement in the energy accuracy highlights the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#21457;&#35328;&#20154;&#26085;&#21270;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#21046;&#20316;&#33050;&#26412;&#25552;&#21462;&#20266;&#26631;&#31614;&#25968;&#25454;&#65292;&#24182;&#30456;&#23545;&#20110;&#26080;&#30417;&#30563;&#22522;&#32447;&#27169;&#22411;&#23454;&#29616;&#20102;51.7%&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.02160</link><description>&lt;p&gt;
&#35270;&#21548;&#20869;&#23481;&#30340;&#21457;&#35328;&#20154;&#26085;&#21270;
&lt;/p&gt;
&lt;p&gt;
Speaker Diarization of Scripted Audiovisual Content. (arXiv:2308.02160v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#21457;&#35328;&#20154;&#26085;&#21270;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#21046;&#20316;&#33050;&#26412;&#25552;&#21462;&#20266;&#26631;&#31614;&#25968;&#25454;&#65292;&#24182;&#30456;&#23545;&#20110;&#26080;&#30417;&#30563;&#22522;&#32447;&#27169;&#22411;&#23454;&#29616;&#20102;51.7%&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23186;&#20307;&#26412;&#22320;&#21270;&#34892;&#19994;&#36890;&#24120;&#38656;&#35201;&#19968;&#20010;&#19982;&#26368;&#32456;&#30005;&#24433;&#25110;&#30005;&#35270;&#21046;&#20316;&#30340;&#21407;&#25991;&#33050;&#26412;&#30456;&#31526;&#30340;&#33050;&#26412;&#65292;&#20197;&#20415;&#22312;&#22806;&#35821;&#20013;&#21019;&#24314;&#23383;&#24149;&#25110;&#37197;&#38899;&#33050;&#26412;&#12290;&#29305;&#21035;&#26159;&#65292;&#21363;&#25773;&#20986;&#33050;&#26412;&#24517;&#39035;&#34987;&#32467;&#26500;&#21270;&#20026;&#21253;&#21547;&#26102;&#38388;&#20195;&#30721;&#12289;&#21457;&#35328;&#20154;&#22995;&#21517;&#21644;&#36716;&#24405;&#30340;&#23545;&#35805;&#34892;&#24207;&#21015;&#12290;&#30446;&#21069;&#30340;&#35821;&#38899;&#35782;&#21035;&#25216;&#26415;&#21487;&#20197;&#20943;&#36731;&#36716;&#24405;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#21457;&#35328;&#20154;&#26085;&#21270;&#27169;&#22411;&#22312;&#30005;&#35270;&#33410;&#30446;&#19978;&#20173;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65306;&#65288;&#19968;&#65289;&#26080;&#27861;&#36861;&#36394;&#22823;&#37327;&#21457;&#35328;&#20154;&#65292;&#65288;&#20108;&#65289;&#22312;&#26816;&#27979;&#39057;&#32321;&#30340;&#21457;&#35328;&#20154;&#26356;&#25442;&#26102;&#20934;&#30830;&#29575;&#36739;&#20302;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22312;&#25293;&#25668;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#21046;&#20316;&#33050;&#26412;&#26469;&#25552;&#21462;&#29992;&#20110;&#21457;&#35328;&#20154;&#26085;&#21270;&#20219;&#21153;&#30340;&#20266;&#26631;&#31614;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#24182;&#22312;&#25105;&#20204;&#30340;&#24230;&#37327;&#32467;&#26524;&#19978;&#65292;&#22312;66&#20010;&#33410;&#30446;&#30340;&#27979;&#35797;&#38598;&#19978;&#30456;&#23545;&#20110;&#20004;&#20010;&#26080;&#30417;&#30563;&#22522;&#32447;&#27169;&#22411;&#23454;&#29616;&#20102;51.7%&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The media localization industry usually requires a verbatim script of the final film or TV production in order to create subtitles or dubbing scripts in a foreign language. In particular, the verbatim script (i.e. as-broadcast script) must be structured into a sequence of dialogue lines each including time codes, speaker name and transcript. Current speech recognition technology alleviates the transcription step. However, state-of-the-art speaker diarization models still fall short on TV shows for two main reasons: (i) their inability to track a large number of speakers, (ii) their low accuracy in detecting frequent speaker changes. To mitigate this problem, we present a novel approach to leverage production scripts used during the shooting process, to extract pseudo-labeled data for the speaker diarization task. We propose a novel semi-supervised approach and demonstrate improvements of 51.7% relative to two unsupervised baseline models on our metrics on a 66 show test set.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37325;&#26032;&#35780;&#20272;&#39640;&#38454;&#24494;&#20998;&#27714;&#35299;&#22120;&#30340;&#35774;&#35745;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#25351;&#25968;&#31215;&#20998;&#22120;&#65292;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#20013;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#37319;&#26679;&#36136;&#37327;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.02157</link><description>&lt;p&gt;
&#25913;&#36827;&#30340;&#25351;&#25968;&#31215;&#20998;&#22120;&#22312;&#25193;&#25955;&#27169;&#22411;&#37319;&#26679;&#20013;&#30340;&#24207;&#20998;&#26512;&#19982;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Improved Order Analysis and Design of Exponential Integrator for Diffusion Models Sampling. (arXiv:2308.02157v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02157
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#35780;&#20272;&#39640;&#38454;&#24494;&#20998;&#27714;&#35299;&#22120;&#30340;&#35774;&#35745;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#25351;&#25968;&#31215;&#20998;&#22120;&#65292;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#20013;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#37319;&#26679;&#36136;&#37327;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#30340;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#26174;&#33879;&#20943;&#23569;&#20102;&#25193;&#25955;&#27169;&#22411;&#65288;DM&#65289;&#30340;&#37319;&#26679;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#36136;&#37327;&#30340;&#37319;&#26679;&#32467;&#26524;&#12290;&#22312;&#36825;&#20123;&#27714;&#35299;&#22120;&#20013;&#65292;&#25351;&#25968;&#31215;&#20998;&#22120;&#65288;EI&#65289;&#20197;&#20854;&#23637;&#31034;&#30340;&#26368;&#26032;&#24615;&#33021;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#39640;&#38454;EI&#30340;&#37319;&#26679;&#31639;&#27861;&#20381;&#36182;&#20110;&#36864;&#21270;&#30340;EI&#27714;&#35299;&#22120;&#65292;&#19982;&#29702;&#35770;&#19978;&#39044;&#26399;&#30340;&#32467;&#26524;&#30456;&#27604;&#65292;&#23548;&#33268;&#36739;&#24046;&#30340;&#35823;&#24046;&#30028;&#38480;&#21644;&#38477;&#20302;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#31181;&#24773;&#20917;&#20351;&#24471;&#37319;&#26679;&#36136;&#37327;&#26497;&#20854;&#23481;&#26131;&#21463;&#21040;&#34920;&#38754;&#19978;&#26080;&#23475;&#30340;&#35774;&#35745;&#36873;&#25321;&#30340;&#24433;&#21709;&#65292;&#20363;&#22914;&#65292;&#20302;&#25928;&#30340;&#26102;&#38388;&#27493;&#38271;&#35843;&#24230;&#22120;&#21487;&#33021;&#38656;&#35201;&#20004;&#20493;&#30340;&#27493;&#25968;&#25165;&#33021;&#36798;&#21040;&#19982;&#32463;&#36807;&#31934;&#24515;&#20248;&#21270;&#30340;&#26102;&#38388;&#27493;&#38271;&#33719;&#24471;&#30340;&#36136;&#37327;&#30456;&#24403;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37325;&#26032;&#35780;&#20272;&#20102;&#23545;DM&#30340;&#39640;&#38454;&#24494;&#20998;&#27714;&#35299;&#22120;&#30340;&#35774;&#35745;&#12290;&#36890;&#36807;&#24443;&#24213;&#30340;&#24207;&#20998;&#26512;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#29616;&#26377;&#39640;&#38454;EI&#27714;&#35299;&#22120;&#30340;&#36864;&#21270;&#21487;&#20197;&#34987;... (&#25688;&#35201;&#22826;&#38271;&#65292;&#24050;&#30465;&#30053;&#37096;&#20998;&#20869;&#23481;)
&lt;/p&gt;
&lt;p&gt;
Efficient differential equation solvers have significantly reduced the sampling time of diffusion models (DMs) while retaining high sampling quality. Among these solvers, exponential integrators (EI) have gained prominence by demonstrating state-of-the-art performance. However, existing high-order EI-based sampling algorithms rely on degenerate EI solvers, resulting in inferior error bounds and reduced accuracy in contrast to the theoretically anticipated results under optimal settings. This situation makes the sampling quality extremely vulnerable to seemingly innocuous design choices such as timestep schedules. For example, an inefficient timestep scheduler might necessitate twice the number of steps to achieve a quality comparable to that obtained through carefully optimized timesteps. To address this issue, we reevaluate the design of high-order differential solvers for DMs. Through a thorough order analysis, we reveal that the degeneration of existing high-order EI solvers can be 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#30446;&#26631;&#20248;&#21270;&#20013;&#65292;&#22914;&#20309;&#35299;&#20915;Pareto&#38598;&#21512;&#19978;&#30340;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#23616;&#37096;&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#25910;&#25947;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.02145</link><description>&lt;p&gt;
Pareto&#38598;&#21512;&#19978;&#30340;&#20248;&#21270;&#65306;&#22810;&#30446;&#26631;&#20248;&#21270;&#29702;&#35770;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Optimization on Pareto sets: On a theory of multi-objective optimization. (arXiv:2308.02145v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02145
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#30446;&#26631;&#20248;&#21270;&#20013;&#65292;&#22914;&#20309;&#35299;&#20915;Pareto&#38598;&#21512;&#19978;&#30340;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#23616;&#37096;&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#30446;&#26631;&#20248;&#21270;&#20013;&#65292;&#19968;&#20010;&#20915;&#31574;&#21521;&#37327;&#24517;&#39035;&#22312;&#22810;&#20010;&#30446;&#26631;&#20043;&#38388;&#24179;&#34913;&#26435;&#34913;&#12290;&#23454;&#29616;&#26368;&#20339;&#26435;&#34913;&#30340;&#35299;&#20915;&#26041;&#26696;&#34987;&#31216;&#20026;Pareto&#26368;&#20248;&#65306;&#36825;&#20123;&#26159;&#20915;&#31574;&#21521;&#37327;&#65292;&#36890;&#36807;&#25913;&#21892;&#20219;&#20309;&#19968;&#20010;&#30446;&#26631;&#65292;&#24517;&#28982;&#20250;&#20197;&#29306;&#29298;&#21478;&#19968;&#20010;&#30446;&#26631;&#20026;&#20195;&#20215;&#12290;&#20294;&#30001;&#20110;Pareto&#26368;&#20248;&#21521;&#37327;&#30340;&#38598;&#21512;&#21487;&#33021;&#38750;&#24120;&#22823;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#32771;&#34385;&#20102;&#19968;&#20010;&#26356;&#23454;&#38469;&#37325;&#35201;&#30340;Pareto&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#65306;&#30446;&#26631;&#26159;&#23558;&#20559;&#22909;&#20989;&#25968;&#20248;&#21270;&#21040;Pareto&#38598;&#21512;&#32422;&#26463;&#19979;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#35299;&#20915;&#36825;&#20010;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#23616;&#37096;&#26041;&#27861;&#65292;&#36825;&#20010;&#38382;&#39064;&#23384;&#22312;&#37325;&#35201;&#25361;&#25112;&#65292;&#22240;&#20026;&#32422;&#26463;&#38598;&#21512;(i)&#36890;&#36807;&#38544;&#24335;&#23450;&#20041;&#65292;&#19988;(ii)&#19968;&#33324;&#24773;&#20917;&#19979;&#38750;&#20984;&#38750;&#20809;&#28369;&#65292;&#21363;&#20351;&#30446;&#26631;&#26159;&#20984;&#20809;&#28369;&#30340;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#26368;&#20248;&#24615;&#21644;&#31283;&#23450;&#24615;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#24403;&#30446;&#26631;&#26159;&#24378;&#20984;&#30340;&#19988;Lipschitz&#20809;&#28369;&#26102;&#65292;&#23427;&#20855;&#26377;&#26411;&#27425;&#36845;&#20195;&#30340;&#25910;&#25947;&#36895;&#29575;&#20026;$O(K^{-1/2})$&#12290;
&lt;/p&gt;
&lt;p&gt;
In multi-objective optimization, a single decision vector must balance the trade-offs between many objectives. Solutions achieving an optimal trade-off are said to be Pareto optimal: these are decision vectors for which improving any one objective must come at a cost to another. But as the set of Pareto optimal vectors can be very large, we further consider a more practically significant Pareto-constrained optimization problem, where the goal is to optimize a preference function constrained to the Pareto set.  We investigate local methods for solving this constrained optimization problem, which poses significant challenges because the constraint set is (i) implicitly defined, and (ii) generally non-convex and non-smooth, even when the objectives are. We define notions of optimality and stationarity, and provide an algorithm with a last-iterate convergence rate of $O(K^{-1/2})$ to stationarity when the objectives are strongly convex and Lipschitz smooth.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29289;&#29702;&#24863;&#30693;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#19981;&#21516;&#20960;&#20309;&#24418;&#29366;&#20013;&#31283;&#24577;Navier-Stokes&#26041;&#31243;&#35299;&#30340;&#36817;&#20284;&#35299;&#30340;&#25216;&#26415;&#65292;&#26080;&#38656;&#21442;&#25968;&#21270;&#65292;&#22312;&#32467;&#26524;&#19978;&#19982;&#22522;&#20110;&#25968;&#25454;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2308.02137</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#24863;&#30693;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20108;&#32500;&#19981;&#21487;&#21387;&#32553;Navier-Stokes&#26041;&#31243;&#30340;&#35299;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Learning the solution operator of two-dimensional incompressible Navier-Stokes equations using physics-aware convolutional neural networks. (arXiv:2308.02137v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29289;&#29702;&#24863;&#30693;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#19981;&#21516;&#20960;&#20309;&#24418;&#29366;&#20013;&#31283;&#24577;Navier-Stokes&#26041;&#31243;&#35299;&#30340;&#36817;&#20284;&#35299;&#30340;&#25216;&#26415;&#65292;&#26080;&#38656;&#21442;&#25968;&#21270;&#65292;&#22312;&#32467;&#26524;&#19978;&#19982;&#22522;&#20110;&#25968;&#25454;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23558;&#29289;&#29702;&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;&#30340;&#27010;&#24565;&#21464;&#24471;&#24191;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#21253;&#21547;&#29289;&#29702;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20173;&#28982;&#23616;&#38480;&#20110;&#21333;&#19968;&#20960;&#20309;&#24418;&#29366;&#25110;&#19968;&#32452;&#21487;&#21442;&#25968;&#21270;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#22240;&#27492;&#65292;&#21363;&#20351;&#20165;&#30053;&#24494;&#20462;&#25913;&#65292;&#20173;&#28982;&#38656;&#35201;&#38024;&#23545;&#26032;&#30340;&#20960;&#20309;&#24418;&#29366;&#35757;&#32451;&#26032;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#22312;&#19981;&#21516;&#20960;&#20309;&#24418;&#29366;&#20013;&#30340;&#31283;&#24577;Navier-Stokes&#26041;&#31243;&#30340;&#36817;&#20284;&#35299;&#12290;&#36825;&#31181;&#25216;&#26415;&#22522;&#20110;U-Net-like CNN&#21644;&#26469;&#33258;&#26377;&#38480;&#24046;&#20998;&#26041;&#27861;&#39046;&#22495;&#30340;&#25104;&#29087;&#31163;&#25955;&#21270;&#26041;&#27861;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#23558;&#29289;&#29702;&#24863;&#30693;&#30340;CNN&#30340;&#32467;&#26524;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#25968;&#25454;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#22522;&#20110;&#25968;&#25454;&#30340;&#26041;&#27861;&#32467;&#21512;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the concept of introducing physics to machine learning has become widely popular. Most physics-inclusive ML-techniques however are still limited to a single geometry or a set of parametrizable geometries. Thus, there remains the need to train a new model for a new geometry, even if it is only slightly modified. With this work we introduce a technique with which it is possible to learn approximate solutions to the steady-state Navier--Stokes equations in varying geometries without the need of parametrization. This technique is based on a combination of a U-Net-like CNN and well established discretization methods from the field of the finite difference method.The results of our physics-aware CNN are compared to a state-of-the-art data-based approach. Additionally, it is also shown how our approach performs when combined with the data-based approach.
&lt;/p&gt;</description></item><item><title>Eva&#26159;&#19968;&#20010;&#20869;&#23384;&#21644;&#26102;&#38388;&#39640;&#25928;&#30340;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#20351;&#29992;Kronecker&#20998;&#35299;&#21644;Sherman-Morrison&#20844;&#24335;&#26469;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#21644;&#35745;&#31639;&#30697;&#38453;&#36870;&#65292;&#21516;&#26102;&#23558;&#20854;&#25193;&#23637;&#20026;&#36890;&#29992;&#30340;&#30690;&#37327;&#21270;&#36817;&#20284;&#26694;&#26550;&#20197;&#25552;&#39640;&#35745;&#31639;&#21644;&#20869;&#23384;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.02123</link><description>&lt;p&gt;
Eva&#65306;&#29992;&#20110;&#20108;&#38454;&#20248;&#21270;&#30340;&#36890;&#29992;&#30690;&#37327;&#21270;&#36817;&#20284;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Eva: A General Vectorized Approximation Framework for Second-order Optimization. (arXiv:2308.02123v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02123
&lt;/p&gt;
&lt;p&gt;
Eva&#26159;&#19968;&#20010;&#20869;&#23384;&#21644;&#26102;&#38388;&#39640;&#25928;&#30340;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#20351;&#29992;Kronecker&#20998;&#35299;&#21644;Sherman-Morrison&#20844;&#24335;&#26469;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#21644;&#35745;&#31639;&#30697;&#38453;&#36870;&#65292;&#21516;&#26102;&#23558;&#20854;&#25193;&#23637;&#20026;&#36890;&#29992;&#30340;&#30690;&#37327;&#21270;&#36817;&#20284;&#26694;&#26550;&#20197;&#25552;&#39640;&#35745;&#31639;&#21644;&#20869;&#23384;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#23545;&#20110;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#20248;&#31168;&#30340;&#25910;&#25947;&#24615;&#33021;&#65292;&#20294;&#24448;&#24448;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#24320;&#38144;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#27604;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#31561;&#19968;&#38454;&#31639;&#27861;&#26356;&#20302;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Eva&#30340;&#20869;&#23384;&#21644;&#26102;&#38388;&#39640;&#25928;&#30340;&#20108;&#38454;&#31639;&#27861;&#65292;&#37319;&#29992;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65306;1&#65289;&#25105;&#20204;&#20351;&#29992;&#23567;&#25209;&#37327;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;Kronecker&#20998;&#35299;&#26500;&#24314;&#20108;&#38454;&#20449;&#24687;&#65292;&#20197;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#65307;2&#65289;&#25105;&#20204;&#21033;&#29992;Sherman-Morrison&#20844;&#24335;&#23548;&#20986;&#19968;&#31181;&#39640;&#25928;&#30340;&#26356;&#26032;&#20844;&#24335;&#65292;&#32780;&#26080;&#38656;&#26174;&#24335;&#35745;&#31639;&#30697;&#38453;&#30340;&#36870;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;Eva&#25193;&#23637;&#20026;&#19968;&#20010;&#36890;&#29992;&#30340;&#30690;&#37327;&#21270;&#36817;&#20284;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#20004;&#31181;&#29616;&#26377;&#20108;&#38454;&#31639;&#27861;&#65288;FOOF&#21644;Shampoo&#65289;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25928;&#29575;&#65292;&#32780;&#19981;&#24433;&#21709;&#23427;&#20204;&#30340;&#25910;&#25947;&#24615;&#33021;&#12290;&#22312;&#19981;&#21516;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Second-order optimization algorithms exhibit excellent convergence properties for training deep learning models, but often incur significant computation and memory overheads. This can result in lower training efficiency than the first-order counterparts such as stochastic gradient descent (SGD). In this work, we present a memory- and time-efficient second-order algorithm named Eva with two novel techniques: 1) we construct the second-order information with the Kronecker factorization of small stochastic vectors over a mini-batch of training data to reduce memory consumption, and 2) we derive an efficient update formula without explicitly computing the inverse of matrices using the Sherman-Morrison formula. We further extend Eva to a general vectorized approximation framework to improve the compute and memory efficiency of two existing second-order algorithms (FOOF and Shampoo) without affecting their convergence performance. Extensive experimental results on different models and datase
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#27169;&#22411;&#26469;&#28304;&#35777;&#26126;&#30340;&#26032;&#27010;&#24565;&#27169;&#22411;DNA&#65292;&#36890;&#36807;&#32534;&#30721;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#36755;&#20837;&#36755;&#20986;&#20449;&#24687;&#20316;&#20026;&#32039;&#20945;&#20840;&#38754;&#30340;&#34920;&#31034;&#65292;&#26469;&#30830;&#23450;&#28304;&#27169;&#22411;&#26159;&#21542;&#20316;&#20026;&#30446;&#26631;&#27169;&#22411;&#30340;&#26469;&#28304;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2308.02121</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#22411;DNA&#30340;&#27169;&#22411;&#26469;&#28304;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Model Provenance via Model DNA. (arXiv:2308.02121v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#27169;&#22411;&#26469;&#28304;&#35777;&#26126;&#30340;&#26032;&#27010;&#24565;&#27169;&#22411;DNA&#65292;&#36890;&#36807;&#32534;&#30721;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#36755;&#20837;&#36755;&#20986;&#20449;&#24687;&#20316;&#20026;&#32039;&#20945;&#20840;&#38754;&#30340;&#34920;&#31034;&#65292;&#26469;&#30830;&#23450;&#28304;&#27169;&#22411;&#26159;&#21542;&#20316;&#20026;&#30446;&#26631;&#27169;&#22411;&#30340;&#26469;&#28304;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#29983;&#21629;&#21608;&#26399;&#26159;&#19968;&#20010;&#26377;&#36259;&#30340;&#30740;&#31350;&#39046;&#22495;&#65288;&#20363;&#22914;&#65292;&#20102;&#35299;&#27169;&#22411;&#30340;&#26469;&#28304;&#65292;&#35757;&#32451;&#26041;&#24335;&#20197;&#21450;&#20351;&#29992;&#26041;&#24335;&#65289;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#36825;&#19968;&#39046;&#22495;&#20869;&#30340;&#19968;&#20010;&#26032;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#26469;&#28304;&#35777;&#26126;&#65288;MP&#65289;&#65292;&#35813;&#38382;&#39064;&#28041;&#21450;&#30446;&#26631;&#27169;&#22411;&#19982;&#20854;&#39044;&#35757;&#32451;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#26088;&#22312;&#30830;&#23450;&#19968;&#20010;&#28304;&#27169;&#22411;&#26159;&#21542;&#20316;&#20026;&#30446;&#26631;&#27169;&#22411;&#30340;&#26469;&#28304;&#35777;&#26126;&#12290;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#23545;&#20110;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#30693;&#35782;&#20135;&#26435;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#20294;&#22312;&#25991;&#29486;&#20013;&#24182;&#27809;&#26377;&#24471;&#21040;&#24456;&#22810;&#20851;&#27880;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#27010;&#24565;&#65292;&#21363;&#27169;&#22411;DNA&#65292;&#23427;&#20195;&#34920;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#25105;&#20204;&#21033;&#29992;&#25968;&#25454;&#39537;&#21160;&#21644;&#27169;&#22411;&#39537;&#21160;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#36755;&#20837;&#36755;&#20986;&#20449;&#24687;&#32534;&#30721;&#20026;&#27169;&#22411;&#30340;&#32039;&#20945;&#19988;&#20840;&#38754;&#30340;&#34920;&#31034;&#65288;&#21363;DNA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the life cycle of the machine learning (ML) model is an intriguing area of research (e.g., understanding where the model comes from, how it is trained, and how it is used). This paper focuses on a novel problem within this field, namely Model Provenance (MP), which concerns the relationship between a target model and its pre-training model and aims to determine whether a source model serves as the provenance for a target model. This is an important problem that has significant implications for ensuring the security and intellectual property of machine learning models but has not received much attention in the literature. To fill in this gap, we introduce a novel concept of Model DNA which represents the unique characteristics of a machine learning model. We utilize a data-driven and model-driven representation learning method to encode the model's training data and input-output information as a compact and comprehensive representation (i.e., DNA) of the model. Using this 
&lt;/p&gt;</description></item><item><title>VQGraph&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#24378;&#22823;&#30340;&#22270;&#24418;&#34920;&#31034;&#31354;&#38388;&#65292;&#29992;&#20110;&#36830;&#25509;GNN&#21644;MLPs&#12290;&#23427;&#37319;&#29992;&#30690;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VQ-VAE&#65289;&#30340;&#32534;&#30721;&#22120;&#20316;&#20026;&#32467;&#26500;&#24863;&#30693;&#22270;&#26631;&#35760;&#22120;&#65292;&#26377;&#25928;&#22320;&#34920;&#31034;&#24213;&#23618;&#22270;&#30340;&#22810;&#26679;&#21270;&#23616;&#37096;&#32467;&#26500;&#12290;&#36890;&#36807; VQGraph&#65292;&#21487;&#20197;&#23454;&#29616;&#20174;GNN&#21040;MLP&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2308.02117</link><description>&lt;p&gt;
VQGraph: &#22270;&#24418;&#21521;&#37327;&#37327;&#21270;&#29992;&#20110;&#36830;&#25509;GNN&#21644;MLPs
&lt;/p&gt;
&lt;p&gt;
VQGraph: Graph Vector-Quantization for Bridging GNNs and MLPs. (arXiv:2308.02117v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02117
&lt;/p&gt;
&lt;p&gt;
VQGraph&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#24378;&#22823;&#30340;&#22270;&#24418;&#34920;&#31034;&#31354;&#38388;&#65292;&#29992;&#20110;&#36830;&#25509;GNN&#21644;MLPs&#12290;&#23427;&#37319;&#29992;&#30690;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VQ-VAE&#65289;&#30340;&#32534;&#30721;&#22120;&#20316;&#20026;&#32467;&#26500;&#24863;&#30693;&#22270;&#26631;&#35760;&#22120;&#65292;&#26377;&#25928;&#22320;&#34920;&#31034;&#24213;&#23618;&#22270;&#30340;&#22810;&#26679;&#21270;&#23616;&#37096;&#32467;&#26500;&#12290;&#36890;&#36807; VQGraph&#65292;&#21487;&#20197;&#23454;&#29616;&#20174;GNN&#21040;MLP&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36827;&#34892;&#20449;&#24687;&#20256;&#36882;&#65292;&#32858;&#21512;&#23616;&#37096;&#37051;&#23621;&#20197;&#26356;&#26032;&#33410;&#28857;&#34920;&#31034;&#12290;&#36825;&#31181;&#20449;&#24687;&#20256;&#36882;&#23548;&#33268;&#22312;&#23454;&#38469;&#30340;&#24310;&#36831;&#32422;&#26463;&#24212;&#29992;&#31243;&#24207;&#20013;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#37319;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#36890;&#36807;&#27169;&#20223;GNN&#30340;&#36755;&#20986;&#26469;&#23398;&#20064;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;GNN&#34920;&#31034;&#31354;&#38388;&#21487;&#33021;&#19981;&#36275;&#20197;&#34920;&#31034;&#24213;&#23618;&#22270;&#30340;&#22810;&#26679;&#21270;&#23616;&#37096;&#32467;&#26500;&#65292;&#36825;&#38480;&#21046;&#20102;&#20174;GNN&#21040;MLP&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;VQGraph&#65292;&#29992;&#20110;&#23398;&#20064;&#19968;&#20010;&#24378;&#22823;&#30340;&#22270;&#24418;&#34920;&#31034;&#31354;&#38388;&#65292;&#29992;&#20110;&#36830;&#25509;GNN&#21644;MLPs&#12290;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#21464;&#20307;&#30340;&#30690;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VQ-VAE&#65289;&#30340;&#32534;&#30721;&#22120;&#20316;&#20026;&#32467;&#26500;&#24863;&#30693;&#22270;&#26631;&#35760;&#22120;&#65292;&#23427;&#23558;&#22810;&#26679;&#21270;&#30340;&#23616;&#37096;&#32467;&#26500;&#33410;&#28857;&#26126;&#30830;&#34920;&#31034;&#20026;&#22823;&#37327;&#31163;&#25955;&#20196;&#29260;&#65292;&#24182;&#26500;&#25104;&#19968;&#20010;&#26377;&#24847;&#20041;&#30340;&#20195;&#30721;&#20070;&#12290;&#37197;&#22791;&#20102;&#23398;&#20064;&#30340;&#20195;&#30721;&#20070;&#65292;&#25105;&#20204;&#25552;&#20986;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) conduct message passing which aggregates local neighbors to update node representations. Such message passing leads to scalability issues in practical latency-constrained applications. To address this issue, recent methods adopt knowledge distillation (KD) to learn computationally-efficient multi-layer perceptron (MLP) by mimicking the output of GNN. However, the existing GNN representation space may not be expressive enough for representing diverse local structures of the underlying graph, which limits the knowledge transfer from GNN to MLP. Here we present a novel framework VQGraph to learn a powerful graph representation space for bridging GNNs and MLPs. We adopt the encoder of a variant of a vector-quantized variational autoencoder (VQ-VAE) as a structure-aware graph tokenizer, which explicitly represents the nodes of diverse local structures as numerous discrete tokens and constitutes a meaningful codebook. Equipped with the learned codebook, we propos
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28151;&#21512;&#22810;&#20219;&#21153;CNN-Transformer&#32593;&#32476;&#36827;&#34892;&#20083;&#33146;&#36229;&#22768;&#32959;&#30244;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#20840;&#23616;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#24615;&#12289;&#28789;&#25935;&#24230;&#21644;F1&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2308.02101</link><description>&lt;p&gt;
&#20351;&#29992;&#28151;&#21512;&#22810;&#20219;&#21153;CNN-Transformer&#32593;&#32476;&#36827;&#34892;&#20083;&#33146;&#36229;&#22768;&#32959;&#30244;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network. (arXiv:2308.02101v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02101
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28151;&#21512;&#22810;&#20219;&#21153;CNN-Transformer&#32593;&#32476;&#36827;&#34892;&#20083;&#33146;&#36229;&#22768;&#32959;&#30244;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#20840;&#23616;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#24615;&#12289;&#28789;&#25935;&#24230;&#21644;F1&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20083;&#33146;&#36229;&#22768;&#65288;BUS&#65289;&#22270;&#20687;&#20998;&#31867;&#20013;&#65292;&#25429;&#25417;&#20840;&#23616;&#19978;&#19979;&#25991;&#20449;&#24687;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#32959;&#30244;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#21487;&#38752;&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#21367;&#31215;&#25805;&#20316;&#30340;&#23616;&#37096;&#24615;&#36136;&#65292;&#23427;&#20204;&#22312;&#24314;&#27169;&#20840;&#23616;&#21644;&#38271;&#31243;&#20381;&#36182;&#24615;&#26041;&#38754;&#23384;&#22312;&#22266;&#26377;&#30340;&#38480;&#21046;&#12290;&#35270;&#35273;Transformer&#20855;&#26377;&#25429;&#25417;&#20840;&#23616;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#25913;&#36827;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#26631;&#35760;&#21270;&#25805;&#20316;&#21487;&#33021;&#20250;&#25197;&#26354;&#23616;&#37096;&#22270;&#20687;&#27169;&#24335;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hybrid-MT-ESTAN&#30340;&#28151;&#21512;&#22810;&#20219;&#21153;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#20351;&#29992;&#30001;CNN&#21644;Swin Transformer&#32452;&#20214;&#32452;&#25104;&#30340;&#28151;&#21512;&#26550;&#26500;&#25191;&#34892;BUS&#32959;&#30244;&#20998;&#31867;&#21644;&#20998;&#21106;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#20061;&#31181;BUS&#20998;&#31867;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#20351;&#29992;3320&#20010;BUS&#22270;&#20687;&#30340;&#19971;&#20010;&#23450;&#37327;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;Hybrid-MT-ESTAN&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#28789;&#25935;&#24230;&#21644;F1&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Capturing global contextual information plays a critical role in breast ultrasound (BUS) image classification. Although convolutional neural networks (CNNs) have demonstrated reliable performance in tumor classification, they have inherent limitations for modeling global and long-range dependencies due to the localized nature of convolution operations. Vision Transformers have an improved capability of capturing global contextual information but may distort the local image patterns due to the tokenization operations. In this study, we proposed a hybrid multitask deep neural network called Hybrid-MT-ESTAN, designed to perform BUS tumor classification and segmentation using a hybrid architecture composed of CNNs and Swin Transformer components. The proposed approach was compared to nine BUS classification methods and evaluated using seven quantitative metrics on a dataset of 3,320 BUS images. The results indicate that Hybrid-MT-ESTAN achieved the highest accuracy, sensitivity, and F1 sco
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Encoder-Adaptor-Reconfigurator&#65288;EAR&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#39046;&#22495;&#28418;&#31227;&#19979;&#36827;&#34892;&#39640;&#25928;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#20102;&#22266;&#23450;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#29305;&#24449;&#32534;&#30721;&#22120;&#65292;&#24182;&#22312;&#32534;&#30721;&#22120;&#20043;&#19978;&#35757;&#32451;&#27973;&#23618;&#32593;&#32476;&#26469;&#22788;&#29702;&#26032;&#25968;&#25454;&#12290;&#36890;&#36807;&#32467;&#21512;DNN&#21644;&#36229;&#32500;&#35745;&#31639;&#65288;HDC&#65289;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#26816;&#27979;&#26032;&#25968;&#25454;&#26159;&#21542;&#23646;&#20110;&#20998;&#24067;&#20043;&#22806;&#65288;OOD&#65289;&#65292;&#24182;&#33021;&#22815;&#35782;&#21035;&#20986;... (&#25688;&#35201;&#20869;&#23481;&#30465;&#30053;)</title><link>http://arxiv.org/abs/2308.02084</link><description>&lt;p&gt;
&#22312;&#36793;&#32536;&#31471;&#30340;&#39640;&#25928;&#27169;&#22411;&#36866;&#24212;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Model Adaptation for Continual Learning at the Edge. (arXiv:2308.02084v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02084
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Encoder-Adaptor-Reconfigurator&#65288;EAR&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#39046;&#22495;&#28418;&#31227;&#19979;&#36827;&#34892;&#39640;&#25928;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#20102;&#22266;&#23450;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#29305;&#24449;&#32534;&#30721;&#22120;&#65292;&#24182;&#22312;&#32534;&#30721;&#22120;&#20043;&#19978;&#35757;&#32451;&#27973;&#23618;&#32593;&#32476;&#26469;&#22788;&#29702;&#26032;&#25968;&#25454;&#12290;&#36890;&#36807;&#32467;&#21512;DNN&#21644;&#36229;&#32500;&#35745;&#31639;&#65288;HDC&#65289;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#26816;&#27979;&#26032;&#25968;&#25454;&#26159;&#21542;&#23646;&#20110;&#20998;&#24067;&#20043;&#22806;&#65288;OOD&#65289;&#65292;&#24182;&#33021;&#22815;&#35782;&#21035;&#20986;... (&#25688;&#35201;&#20869;&#23481;&#30465;&#30053;)
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#35757;&#32451;&#21644;&#37096;&#32626;&#36807;&#31243;&#20013;&#20551;&#35774;&#25968;&#25454;&#20998;&#24067;&#26159;&#22266;&#23450;&#21644;&#21305;&#37197;&#30340;&#65292;&#20294;&#36825;&#36890;&#24120;&#26159;&#38169;&#35823;&#30340;&#20551;&#35774;&#12290;&#24403;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37096;&#32626;&#22312;&#30495;&#23454;&#35774;&#22791;&#19978;&#26102;&#65292;&#25968;&#25454;&#20998;&#24067;&#24120;&#24120;&#20250;&#38543;&#26102;&#38388;&#21464;&#21270;&#65292;&#21407;&#22240;&#26159;&#29615;&#22659;&#22240;&#32032;&#12289;&#20256;&#24863;&#22120;&#29305;&#24615;&#21644;&#24863;&#20852;&#36259;&#30340;&#20219;&#21153;&#21457;&#29983;&#20102;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Encoder-Adaptor-Reconfigurator&#65288;EAR&#65289;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#39046;&#22495;&#28418;&#31227;&#19979;&#30340;&#39640;&#25928;&#25345;&#32493;&#23398;&#20064;&#12290;EAR&#26694;&#26550;&#21033;&#29992;&#22266;&#23450;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#29305;&#24449;&#32534;&#30721;&#22120;&#65292;&#24182;&#22312;&#32534;&#30721;&#22120;&#20043;&#19978;&#35757;&#32451;&#27973;&#23618;&#32593;&#32476;&#26469;&#22788;&#29702;&#26032;&#25968;&#25454;&#12290;EAR&#26694;&#26550;&#33021;&#22815;&#36890;&#36807;&#23558;DNN&#19982;&#36229;&#32500;&#35745;&#31639;&#65288;HDC&#65289;&#30456;&#32467;&#21512;&#65292;&#26816;&#27979;&#20986;&#26032;&#25968;&#25454;&#26159;&#21542;&#23646;&#20110;&#20998;&#24067;&#20043;&#22806;&#65288;OOD&#65289;&#65292;&#24182;&#33021;&#22815;&#35782;&#21035;&#20986;
&lt;/p&gt;
&lt;p&gt;
Most machine learning (ML) systems assume stationary and matching data distributions during training and deployment. This is often a false assumption. When ML models are deployed on real devices, data distributions often shift over time due to changes in environmental factors, sensor characteristics, and task-of-interest. While it is possible to have a human-in-the-loop to monitor for distribution shifts and engineer new architectures in response to these shifts, such a setup is not cost-effective. Instead, non-stationary automated ML (AutoML) models are needed. This paper presents the Encoder-Adaptor-Reconfigurator (EAR) framework for efficient continual learning under domain shifts. The EAR framework uses a fixed deep neural network (DNN) feature encoder and trains shallow networks on top of the encoder to handle novel data. The EAR framework is capable of 1) detecting when new data is out-of-distribution (OOD) by combining DNNs with hyperdimensional computing (HDC), 2) identifying l
&lt;/p&gt;</description></item><item><title>&#30446;&#26631;&#35268;&#33539;&#20559;&#24046;&#26159;&#24433;&#21709;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#24037;&#20855;&#20020;&#24202;&#25928;&#29992;&#30340;&#26222;&#36941;&#20559;&#35265;&#65292;&#23427;&#25351;&#30340;&#26159;&#30446;&#26631;&#21464;&#37327;&#30340;&#25805;&#20316;&#21270;&#19982;&#20915;&#31574;&#32773;&#23450;&#20041;&#30340;&#19981;&#19968;&#33268;&#65292;&#21487;&#33021;&#23548;&#33268;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#36807;&#39640;&#20272;&#35745;&#21644;&#27425;&#20248;&#20915;&#31574;&#65292;&#19982;&#25968;&#25454;&#38480;&#21046;&#21644;&#20581;&#24247;&#24046;&#24322;&#26080;&#20851;&#12290;</title><link>http://arxiv.org/abs/2308.02081</link><description>&lt;p&gt;
&#30446;&#26631;&#35268;&#33539;&#20559;&#24046;&#12289;&#21453;&#20107;&#23454;&#39044;&#27979;&#21644;&#21307;&#30103;&#31639;&#27861;&#20844;&#27491;&#24615;
&lt;/p&gt;
&lt;p&gt;
Target specification bias, counterfactual prediction, and algorithmic fairness in healthcare. (arXiv:2308.02081v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02081
&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#35268;&#33539;&#20559;&#24046;&#26159;&#24433;&#21709;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#24037;&#20855;&#20020;&#24202;&#25928;&#29992;&#30340;&#26222;&#36941;&#20559;&#35265;&#65292;&#23427;&#25351;&#30340;&#26159;&#30446;&#26631;&#21464;&#37327;&#30340;&#25805;&#20316;&#21270;&#19982;&#20915;&#31574;&#32773;&#23450;&#20041;&#30340;&#19981;&#19968;&#33268;&#65292;&#21487;&#33021;&#23548;&#33268;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#36807;&#39640;&#20272;&#35745;&#21644;&#27425;&#20248;&#20915;&#31574;&#65292;&#19982;&#25968;&#25454;&#38480;&#21046;&#21644;&#20581;&#24247;&#24046;&#24322;&#26080;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#21307;&#30103;&#39046;&#22495;&#26102;&#65292;&#20559;&#35265;&#36890;&#24120;&#24402;&#22240;&#20110;&#19981;&#20855;&#20195;&#34920;&#24615;&#25110;&#19981;&#23436;&#25972;&#30340;&#25968;&#25454;&#65292;&#25110;&#32773;&#28508;&#22312;&#30340;&#20581;&#24247;&#24046;&#24322;&#12290;&#26412;&#25991;&#21457;&#29616;&#20102;&#19968;&#31181;&#26356;&#26222;&#36941;&#30340;&#20559;&#35265;&#26469;&#28304;&#65292;&#24433;&#21709;&#20102;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#24037;&#20855;&#30340;&#20020;&#24202;&#25928;&#29992;&#65306;&#30446;&#26631;&#35268;&#33539;&#20559;&#24046;&#12290;&#24403;&#30446;&#26631;&#21464;&#37327;&#30340;&#23454;&#26045;&#19982;&#20915;&#31574;&#32773;&#30340;&#23450;&#20041;&#19981;&#19968;&#33268;&#26102;&#65292;&#23601;&#20250;&#21457;&#29983;&#30446;&#26631;&#35268;&#33539;&#20559;&#24046;&#12290;&#36825;&#31181;&#19981;&#21305;&#37197;&#36890;&#24120;&#26159;&#24494;&#22937;&#30340;&#65292;&#28304;&#20110;&#20915;&#31574;&#32773;&#36890;&#24120;&#23545;&#21453;&#20107;&#23454;&#30340;&#21307;&#30103;&#24773;&#26223;&#30340;&#32467;&#26524;&#36827;&#34892;&#39044;&#27979;&#65292;&#32780;&#19981;&#26159;&#23454;&#38469;&#24773;&#20917;&#12290;&#30446;&#26631;&#35268;&#33539;&#20559;&#24046;&#19982;&#25968;&#25454;&#38480;&#21046;&#21644;&#20581;&#24247;&#24046;&#24322;&#26080;&#20851;&#12290;&#22914;&#26524;&#19981;&#36827;&#34892;&#32416;&#27491;&#65292;&#23427;&#20250;&#23548;&#33268;&#23545;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#36807;&#39640;&#20272;&#35745;&#65292;&#23545;&#21307;&#30103;&#36164;&#28304;&#30340;&#20302;&#25928;&#21033;&#29992;&#65292;&#20197;&#21450;&#21487;&#33021;&#23545;&#24739;&#32773;&#36896;&#25104;&#20260;&#23475;&#30340;&#27425;&#20248;&#20915;&#31574;&#12290;&#26368;&#36817;&#22312;&#35745;&#37327;&#23398;&#8212;&#8212;&#27979;&#37327;&#31185;&#23398;&#39046;&#22495;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#23545;&#25239;&#30446;&#26631;&#35268;&#33539;&#20559;&#24046;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bias in applications of machine learning (ML) to healthcare is usually attributed to unrepresentative or incomplete data, or to underlying health disparities. This article identifies a more pervasive source of bias that affects the clinical utility of ML-enabled prediction tools: target specification bias. Target specification bias arises when the operationalization of the target variable does not match its definition by decision makers. The mismatch is often subtle, and stems from the fact that decision makers are typically interested in predicting the outcomes of counterfactual, rather than actual, healthcare scenarios. Target specification bias persists independently of data limitations and health disparities. When left uncorrected, it gives rise to an overestimation of predictive accuracy, to inefficient utilization of medical resources, and to suboptimal decisions that can harm patients. Recent work in metrology - the science of measurement suggests ways of counteracting target 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#24179;&#21488;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#35299;&#32544;&#36755;&#20837;&#34920;&#31034;&#20026;&#19981;&#21464;&#29305;&#24449;&#21644;&#24179;&#21488;&#30456;&#20851;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20010;&#26410;&#35265;&#24179;&#21488;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.02080</link><description>&lt;p&gt;
&#36328;&#24179;&#21488;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20013;&#30340;&#22240;&#26524;&#24341;&#23548;&#35299;&#32544;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Causality Guided Disentanglement for Cross-Platform Hate Speech Detection. (arXiv:2308.02080v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#24179;&#21488;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#35299;&#32544;&#36755;&#20837;&#34920;&#31034;&#20026;&#19981;&#21464;&#29305;&#24449;&#21644;&#24179;&#21488;&#30456;&#20851;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20010;&#26410;&#35265;&#24179;&#21488;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#22312;&#20419;&#36827;&#20844;&#24320;&#23545;&#35805;&#26041;&#38754;&#20855;&#26377;&#20215;&#20540;&#65292;&#20294;&#20182;&#20204;&#32463;&#24120;&#34987;&#21033;&#29992;&#26469;&#20256;&#25773;&#26377;&#23475;&#20869;&#23481;&#12290;&#30446;&#21069;&#29992;&#20110;&#26816;&#27979;&#36825;&#31181;&#26377;&#23475;&#20869;&#23481;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#36807;&#24230;&#20381;&#36182;&#20110;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#65292;&#24433;&#21709;&#21040;&#20102;&#23427;&#20204;&#36866;&#24212;&#27867;&#21270;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;&#36825;&#26159;&#22240;&#20026;&#23427;&#20204;&#20542;&#21521;&#20110;&#36807;&#20110;&#29421;&#38552;&#22320;&#20851;&#27880;&#29305;&#23450;&#30340;&#35821;&#35328;&#20449;&#21495;&#25110;&#26576;&#20123;&#35789;&#35821;&#31867;&#21035;&#30340;&#20351;&#29992;&#12290;&#24403;&#24179;&#21488;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#26102;&#65292;&#21478;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#20986;&#29616;&#20102;&#65292;&#38656;&#35201;&#36328;&#24179;&#21488;&#27169;&#22411;&#26469;&#36866;&#24212;&#19981;&#21516;&#30340;&#20998;&#24067;&#36716;&#21270;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#36328;&#24179;&#21488;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#19968;&#20010;&#24179;&#21488;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#24182;&#25512;&#24191;&#21040;&#22810;&#20010;&#26410;&#35265;&#24179;&#21488;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#19981;&#21516;&#24179;&#21488;&#30340;&#33391;&#22909;&#27867;&#21270;&#24615;&#33021;&#65292;&#19968;&#31181;&#26041;&#27861;&#26159;&#23558;&#36755;&#20837;&#34920;&#31034;&#35299;&#32544;&#20026;&#19981;&#21464;&#29305;&#24449;&#21644;&#24179;&#21488;&#30456;&#20851;&#29305;&#24449;&#12290;&#25105;&#20204;&#36824;&#35748;&#20026;&#23398;&#20064;&#22240;&#26524;&#20851;&#31995;&#26159;&#25552;&#20379;&#26356;&#22909;&#35299;&#32544;&#21644;&#27867;&#21270;&#24615;&#33021;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media platforms, despite their value in promoting open discourse, are often exploited to spread harmful content. Current deep learning and natural language processing models used for detecting this harmful content overly rely on domain-specific terms affecting their capabilities to adapt to generalizable hate speech detection. This is because they tend to focus too narrowly on particular linguistic signals or the use of certain categories of words. Another significant challenge arises when platforms lack high-quality annotated data for training, leading to a need for cross-platform models that can adapt to different distribution shifts. Our research introduces a cross-platform hate speech detection model capable of being trained on one platform's data and generalizing to multiple unseen platforms. To achieve good generalizability across platforms, one way is to disentangle the input representations into invariant and platform-dependent features. We also argue that learning causa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26085;&#24120;&#25235;&#21462;&#30340;1,404&#20010;&#19981;&#21487;&#38752;&#26032;&#38395;&#32593;&#31449;&#20197;&#21450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#32858;&#31867;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#20998;&#26512;&#32593;&#32476;&#29983;&#24577;&#31995;&#32479;&#20013;&#20256;&#25773;&#30340;&#21465;&#36848;&#30340;&#31995;&#32479;&#12290;&#36890;&#36807;&#35782;&#21035;55,301&#20010;&#21465;&#36848;&#65292;&#25551;&#36848;&#20102;2022&#24180;&#20256;&#25773;&#26368;&#24191;&#27867;&#30340;&#21465;&#36848;&#65292;&#24182;&#30830;&#23450;&#20102;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#36215;&#28304;&#21644;&#25918;&#22823;&#21465;&#36848;&#30340;&#32593;&#31449;&#12290;&#35813;&#31995;&#32479;&#21487;&#29992;&#20110;&#26816;&#27979;&#26469;&#33258;&#19981;&#21487;&#38752;&#26032;&#38395;&#32593;&#31449;&#30340;&#26032;&#21465;&#36848;&#65292;&#24182;&#24110;&#21161;&#20107;&#23454;&#26680;&#26597;&#32452;&#32455;&#26356;&#24555;&#22320;&#24212;&#23545;&#38169;&#35823;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.02068</link><description>&lt;p&gt;
&#34394;&#20551;&#32593;&#31449;&#65306;&#22312;&#35268;&#27169;&#19978;&#36861;&#36394;&#21644;&#24433;&#21709;&#34394;&#20551;&#26032;&#38395;&#25925;&#20107;&#30340;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Specious Sites: Tracking the Spread and Sway of Spurious News Stories at Scale. (arXiv:2308.02068v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26085;&#24120;&#25235;&#21462;&#30340;1,404&#20010;&#19981;&#21487;&#38752;&#26032;&#38395;&#32593;&#31449;&#20197;&#21450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#32858;&#31867;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#20998;&#26512;&#32593;&#32476;&#29983;&#24577;&#31995;&#32479;&#20013;&#20256;&#25773;&#30340;&#21465;&#36848;&#30340;&#31995;&#32479;&#12290;&#36890;&#36807;&#35782;&#21035;55,301&#20010;&#21465;&#36848;&#65292;&#25551;&#36848;&#20102;2022&#24180;&#20256;&#25773;&#26368;&#24191;&#27867;&#30340;&#21465;&#36848;&#65292;&#24182;&#30830;&#23450;&#20102;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#36215;&#28304;&#21644;&#25918;&#22823;&#21465;&#36848;&#30340;&#32593;&#31449;&#12290;&#35813;&#31995;&#32479;&#21487;&#29992;&#20110;&#26816;&#27979;&#26469;&#33258;&#19981;&#21487;&#38752;&#26032;&#38395;&#32593;&#31449;&#30340;&#26032;&#21465;&#36848;&#65292;&#24182;&#24110;&#21161;&#20107;&#23454;&#26680;&#26597;&#32452;&#32455;&#26356;&#24555;&#22320;&#24212;&#23545;&#38169;&#35823;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#20551;&#20449;&#24687;&#12289;&#23459;&#20256;&#21644;&#24443;&#22836;&#24443;&#23614;&#30340;&#35854;&#35328;&#22312;&#32593;&#32476;&#19978;&#22823;&#37327;&#20256;&#25773;&#65292;&#20854;&#20013;&#19968;&#20123;&#21465;&#36848;&#23545;&#20844;&#20849;&#20581;&#24247;&#12289;&#36873;&#20030;&#21644;&#20010;&#20154;&#23433;&#20840;&#20135;&#29983;&#21361;&#38505;&#30340;&#29616;&#23454;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#34394;&#20551;&#20449;&#24687;&#30340;&#24433;&#21709;&#65292;&#30740;&#31350;&#30028;&#22312;&#36861;&#36394;&#22312;&#32447;&#24179;&#21488;&#19978;&#30340;&#26032;&#38395;&#21465;&#36848;&#26041;&#38754;&#20027;&#35201;&#32570;&#20047;&#33258;&#21160;&#21270;&#21644;&#31243;&#24207;&#21270;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#21033;&#29992;&#23545;1,404&#20010;&#19981;&#21487;&#38752;&#26032;&#38395;&#32593;&#31449;&#30340;&#26085;&#24120;&#25235;&#21462;&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;MPNet&#21644;DP-Means&#32858;&#31867;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#31995;&#32479;&#26469;&#33258;&#21160;&#20998;&#31163;&#21644;&#20998;&#26512;&#22312;&#32447;&#29983;&#24577;&#31995;&#32479;&#20013;&#20256;&#25773;&#30340;&#21465;&#36848;&#12290;&#25105;&#20204;&#22312;&#36825;&#20123;1,404&#20010;&#32593;&#31449;&#19978;&#35782;&#21035;&#20102;55,301&#20010;&#21465;&#36848;&#65292;&#25551;&#36848;&#20102;2022&#24180;&#20256;&#25773;&#26368;&#24191;&#27867;&#30340;&#21465;&#36848;&#65292;&#24182;&#30830;&#23450;&#20102;&#36215;&#28304;&#21644;&#25918;&#22823;&#21465;&#36848;&#30340;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#32593;&#31449;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#25105;&#20204;&#30340;&#31995;&#32479;&#26469;&#26816;&#27979;&#28304;&#33258;&#19981;&#21487;&#38752;&#26032;&#38395;&#32593;&#31449;&#30340;&#26032;&#21465;&#36848;&#65292;&#24182;&#24110;&#21161;Politifact&#12289;&#36335;&#36879;&#31038;&#21644;&#32654;&#32852;&#31038;&#31561;&#20107;&#23454;&#26680;&#26597;&#32452;&#32455;&#26356;&#24555;&#22320;&#24212;&#23545;&#38169;&#35823;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Misinformation, propaganda, and outright lies proliferate on the web, with some narratives having dangerous real-world consequences on public health, elections, and individual safety. However, despite the impact of misinformation, the research community largely lacks automated and programmatic approaches for tracking news narratives across online platforms. In this work, utilizing daily scrapes of 1,404 unreliable news websites, the large-language model MPNet, and DP-Means clustering, we introduce a system to automatically isolate and analyze the narratives spread within online ecosystems. Identifying 55,301 narratives on these 1,404 websites, we describe the most prevalent narratives spread in 2022 and identify the most influential websites that originate and magnify narratives. Finally, we show how our system can be utilized to detect new narratives originating from unreliable news websites and aid fact-checkers like Politifact, Reuters, and AP News in more quickly addressing misinfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ETR-NLP&#27169;&#22411;&#26469;&#20943;&#36731;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#24178;&#25200;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#21487;&#23398;&#20064;&#21407;&#35821;&#21644;&#26174;&#24335;&#20219;&#21153;&#36335;&#30001;&#30340;&#21327;&#21516;&#32452;&#21512;&#65292;&#22312;&#20849;&#20139;&#20998;&#25903;&#21644;&#20219;&#21153;&#29305;&#23450;&#20998;&#25903;&#20013;&#26174;&#24335;&#22320;&#20998;&#31163;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#20197;&#23454;&#29616;&#20219;&#21153;&#20043;&#38388;&#30340;&#26368;&#23567;&#21270;&#24178;&#25200;&#12290;</title><link>http://arxiv.org/abs/2308.02066</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#38750;&#21487;&#23398;&#20064;&#21407;&#35821;&#30340;&#26174;&#24335;&#20219;&#21153;&#36335;&#30001;&#26469;&#20943;&#36731;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#24178;&#25200;
&lt;/p&gt;
&lt;p&gt;
Mitigating Task Interference in Multi-Task Learning via Explicit Task Routing with Non-Learnable Primitives. (arXiv:2308.02066v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ETR-NLP&#27169;&#22411;&#26469;&#20943;&#36731;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#24178;&#25200;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#21487;&#23398;&#20064;&#21407;&#35821;&#21644;&#26174;&#24335;&#20219;&#21153;&#36335;&#30001;&#30340;&#21327;&#21516;&#32452;&#21512;&#65292;&#22312;&#20849;&#20139;&#20998;&#25903;&#21644;&#20219;&#21153;&#29305;&#23450;&#20998;&#25903;&#20013;&#26174;&#24335;&#22320;&#20998;&#31163;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#20197;&#23454;&#29616;&#20219;&#21153;&#20043;&#38388;&#30340;&#26368;&#23567;&#21270;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#36890;&#36807;&#21033;&#29992;&#20219;&#21153;&#20043;&#38388;&#30340;&#20849;&#20139;&#20449;&#24687;&#26469;&#23398;&#20064;&#19968;&#20010;&#27169;&#22411;&#26469;&#23436;&#25104;&#22810;&#20010;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MTL&#27169;&#22411;&#24050;&#32463;&#34987;&#21457;&#29616;&#23384;&#22312;&#36127;&#38754;&#24178;&#25200;&#38382;&#39064;&#12290;&#20026;&#20102;&#20943;&#36731;&#20219;&#21153;&#24178;&#25200;&#65292;&#24050;&#26377;&#30340;&#21162;&#21147;&#20027;&#35201;&#38598;&#20013;&#22312;&#25439;&#22833;/&#26799;&#24230;&#24179;&#34913;&#25110;&#38544;&#24335;&#21442;&#25968;&#21010;&#20998;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ETR-NLP&#26469;&#36890;&#36807;&#38750;&#21487;&#23398;&#20064;&#21407;&#35821;&#65288;NLP&#65289;&#21644;&#26174;&#24335;&#20219;&#21153;&#36335;&#30001;&#65288;ETR&#65289;&#30340;&#21327;&#21516;&#32452;&#21512;&#26469;&#20943;&#36731;&#20219;&#21153;&#24178;&#25200;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#20351;&#29992;&#38750;&#21487;&#23398;&#20064;&#21407;&#35821;&#26469;&#25552;&#21462;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#29305;&#24449;&#65292;&#24182;&#23558;&#23427;&#20204;&#37325;&#26032;&#32452;&#21512;&#25104;&#19968;&#20010;&#20849;&#20139;&#20110;&#25152;&#26377;&#20219;&#21153;&#30340;&#20998;&#25903;&#21644;&#19987;&#38376;&#20026;&#27599;&#20010;&#20219;&#21153;&#20445;&#30041;&#30340;&#26174;&#24335;&#20219;&#21153;&#29305;&#23450;&#20998;&#25903;&#12290;&#38750;&#21487;&#23398;&#20064;&#21407;&#35821;&#21644;&#21487;&#23398;&#20064;&#21442;&#25968;&#30340;&#26174;&#24335;&#35299;&#32806;&#20026;&#26368;&#23567;&#21270;&#20219;&#21153;&#24178;&#25200;&#25552;&#20379;&#20102;&#25152;&#38656;&#30340;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;ETR-NLP&#32593;&#32476;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning (MTL) seeks to learn a single model to accomplish multiple tasks by leveraging shared information among the tasks. Existing MTL models, however, have been known to suffer from negative interference among tasks. Efforts to mitigate task interference have focused on either loss/gradient balancing or implicit parameter partitioning with partial overlaps among the tasks. In this paper, we propose ETR-NLP to mitigate task interference through a synergistic combination of non-learnable primitives (NLPs) and explicit task routing (ETR). Our key idea is to employ non-learnable primitives to extract a diverse set of task-agnostic features and recombine them into a shared branch common to all tasks and explicit task-specific branches reserved for each task. The non-learnable primitives and the explicit decoupling of learnable parameters into shared and task-specific ones afford the flexibility needed for minimizing task interference. We evaluate the efficacy of ETR-NLP networ
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#22312;&#36229;&#29699;&#29305;&#24449;&#31354;&#38388;&#20013;&#29983;&#25104;&#30340;&#20154;&#33080;&#22270;&#20687;&#30340;&#29983;&#29289;&#29305;&#24449;&#23481;&#37327;&#65292;&#24182;&#22312;&#22810;&#20010;&#29983;&#25104;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2308.02065</link><description>&lt;p&gt;
&#20851;&#20110;&#29983;&#25104;&#20154;&#33080;&#27169;&#22411;&#30340;&#29983;&#29289;&#29305;&#24449;&#23481;&#37327;
&lt;/p&gt;
&lt;p&gt;
On the Biometric Capacity of Generative Face Models. (arXiv:2308.02065v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02065
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#22312;&#36229;&#29699;&#29305;&#24449;&#31354;&#38388;&#20013;&#29983;&#25104;&#30340;&#20154;&#33080;&#22270;&#20687;&#30340;&#29983;&#29289;&#29305;&#24449;&#23481;&#37327;&#65292;&#24182;&#22312;&#22810;&#20010;&#29983;&#25104;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#29983;&#25104;&#36924;&#30495;&#38754;&#23380;&#30340;&#36827;&#23637;&#38750;&#24120;&#22823;&#12290;&#23613;&#31649;&#26377;&#36825;&#20123;&#36827;&#23637;&#65292;&#19968;&#20010;&#20851;&#38190;&#30340;&#38382;&#39064;&#20173;&#28982;&#27809;&#26377;&#31572;&#26696;&#65306;&#8220;&#32473;&#23450;&#19968;&#20010;&#29983;&#25104;&#30340;&#20154;&#33080;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#22810;&#23569;&#20010;&#29420;&#29305;&#30340;&#36523;&#20221;&#65311;&#8221;&#25442;&#21477;&#35805;&#35828;&#65292;&#29983;&#25104;&#20154;&#33080;&#27169;&#22411;&#30340;&#29983;&#29289;&#29305;&#24449;&#23481;&#37327;&#26159;&#22810;&#23569;&#65311;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#30340;&#31185;&#23398;&#20381;&#25454;&#23558;&#26377;&#21161;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#30340;&#29983;&#25104;&#20154;&#33080;&#27169;&#22411;&#65292;&#24182;&#30830;&#23450;&#23427;&#20204;&#30340;&#21487;&#25193;&#23637;&#24615;&#30340;&#19978;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26041;&#27861;&#26469;&#20272;&#35745;&#22312;&#19968;&#20010;&#36229;&#29699;&#29305;&#24449;&#31354;&#38388;&#20013;&#29983;&#25104;&#30340;&#20154;&#33080;&#22270;&#20687;&#30340;&#29983;&#29289;&#29305;&#24449;&#23481;&#37327;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#29983;&#25104;&#27169;&#22411;&#19978;&#24212;&#29992;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#26080;&#26465;&#20214;&#29983;&#25104;&#22120;&#22914;StyleGAN&#12289;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#21644;&#8220;&#29983;&#25104;&#30340;&#29031;&#29255;&#8221;&#65292;&#20197;&#21450;DCF&#38754;&#65292;&#19968;&#20010;&#31867;&#26465;&#20214;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#36824;&#20272;&#35745;&#20102;&#19982;&#24615;&#21035;&#21644;&#24180;&#40836;&#31561;&#20154;&#21475;&#23646;&#24615;&#30456;&#20851;&#30340;&#23481;&#37327;&#12290;&#25105;&#20204;&#30340;&#23481;&#37327;&#20272;&#35745;&#34920;&#26126;&#65292;&#22312;&#34394;&#20551;&#25509;&#21463;&#26465;&#20214;&#19979;&#65292;&#20351;&#29992;ArcFace&#34920;&#31034;&#65306;
&lt;/p&gt;
&lt;p&gt;
There has been tremendous progress in generating realistic faces with high fidelity over the past few years. Despite this progress, a crucial question remains unanswered: "Given a generative face model, how many unique identities can it generate?" In other words, what is the biometric capacity of the generative face model? A scientific basis for answering this question will benefit evaluating and comparing different generative face models and establish an upper bound on their scalability. This paper proposes a statistical approach to estimate the biometric capacity of generated face images in a hyperspherical feature space. We employ our approach on multiple generative models, including unconditional generators like StyleGAN, Latent Diffusion Model, and "Generated Photos," as well as DCFace, a class-conditional generator. We also estimate capacity w.r.t. demographic attributes such as gender and age. Our capacity estimates indicate that (a) under ArcFace representation at a false accep
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#39640;&#31232;&#30095;&#23545;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20351;&#29992;&#20256;&#32479;&#30340;&#23494;&#38598;&#35757;&#32451;&#31574;&#30053;&#36827;&#34892;&#31232;&#30095;&#35757;&#32451;&#25928;&#26524;&#19981;&#20339;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#19978;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.02060</link><description>&lt;p&gt;
&#20934;&#30830;&#30340;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#38656;&#35201;&#37325;&#26032;&#24605;&#32771;&#31232;&#30095;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Accurate Neural Network Pruning Requires Rethinking Sparse Optimization. (arXiv:2308.02060v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02060
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#39640;&#31232;&#30095;&#23545;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20351;&#29992;&#20256;&#32479;&#30340;&#23494;&#38598;&#35757;&#32451;&#31574;&#30053;&#36827;&#34892;&#31232;&#30095;&#35757;&#32451;&#25928;&#26524;&#19981;&#20339;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#19978;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27169;&#22411;&#21387;&#32553;&#39046;&#22495;&#65292;&#33719;&#24471;&#26082;&#39640;&#31934;&#30830;&#21448;&#39640;&#31232;&#30095;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29256;&#26412;&#26159;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#65292;&#31038;&#21306;&#24050;&#32463;&#23545;&#20960;&#31181;&#39640;&#24615;&#33021;&#30340;&#21098;&#26525;&#25216;&#26415;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#31232;&#30095;&#24615;&#21644;&#29992;&#20110;&#35757;&#32451;&#31232;&#30095;&#32593;&#32476;&#30340;&#26631;&#20934;&#38543;&#26426;&#20248;&#21270;&#25216;&#26415;&#30340;&#20132;&#20114;&#20102;&#35299;&#36739;&#23569;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#20351;&#29992;&#26631;&#20934;&#30340;&#23494;&#38598;&#35757;&#32451;&#35745;&#21010;&#21644;&#36229;&#21442;&#25968;&#26469;&#35757;&#32451;&#31232;&#30095;&#32593;&#32476;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#26631;&#20934;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31232;&#30095;&#22522;&#20934;&#26469;&#30740;&#31350;&#39640;&#31232;&#30095;&#23545;&#27169;&#22411;&#35757;&#32451;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#20351;&#29992;&#26631;&#20934;&#30340;&#23494;&#38598;&#35757;&#32451;&#31574;&#30053;&#36827;&#34892;&#31232;&#30095;&#35757;&#32451;&#26159;&#27425;&#20248;&#30340;&#65292;&#23548;&#33268;&#27424;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26082;&#21487;&#20197;&#29992;&#20110;&#35270;&#35273;&#27169;&#22411;&#65288;&#22914;ResNet50/ImageNet&#65289;&#30340;&#31232;&#30095;&#39044;&#35757;&#32451;&#65292;&#20063;&#21487;&#20197;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT/GLUE&#65289;&#30340;&#31232;&#30095;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Obtaining versions of deep neural networks that are both highly-accurate and highly-sparse is one of the main challenges in the area of model compression, and several high-performance pruning techniques have been investigated by the community. Yet, much less is known about the interaction between sparsity and the standard stochastic optimization techniques used for training sparse networks, and most existing work uses standard dense schedules and hyperparameters for training sparse networks. In this work, we examine the impact of high sparsity on model training using the standard computer vision and natural language processing sparsity benchmarks. We begin by showing that using standard dense training recipes for sparse training is suboptimal, and results in under-training. We provide new approaches for mitigating this issue for both sparse pre-training of vision models (e.g. ResNet50/ImageNet) and sparse fine-tuning of language models (e.g. BERT/GLUE), achieving state-of-the-art resul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#40065;&#33725;&#34892;&#20026;&#24341;&#20837;&#22522;&#20110;&#30697;&#38453;&#20998;&#35299;&#30340;&#25512;&#33616;&#31995;&#32479;&#23398;&#20064;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25511;&#21046;&#39118;&#38505;&#27700;&#24179;&#26469;&#25552;&#39640;&#39044;&#27979;&#30340;&#25968;&#37327;&#21644;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.02058</link><description>&lt;p&gt;
&#25972;&#21512;&#40065;&#33725;&#34892;&#20026;&#21040;&#22522;&#20110;&#21327;&#21516;&#36807;&#28388;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;
&lt;/p&gt;
&lt;p&gt;
Incorporating Recklessness to Collaborative Filtering based Recommender Systems. (arXiv:2308.02058v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#40065;&#33725;&#34892;&#20026;&#24341;&#20837;&#22522;&#20110;&#30697;&#38453;&#20998;&#35299;&#30340;&#25512;&#33616;&#31995;&#32479;&#23398;&#20064;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25511;&#21046;&#39118;&#38505;&#27700;&#24179;&#26469;&#25552;&#39640;&#39044;&#27979;&#30340;&#25968;&#37327;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21253;&#21547;&#21487;&#38752;&#24615;&#27979;&#37327;&#30340;&#25512;&#33616;&#31995;&#32479;&#24448;&#24448;&#22312;&#39044;&#27979;&#20013;&#26356;&#21152;&#20445;&#23432;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#20445;&#25345;&#21487;&#38752;&#24615;&#12290;&#36825;&#23548;&#33268;&#20102;&#36825;&#20123;&#31995;&#32479;&#21487;&#20197;&#25552;&#20379;&#30340;&#35206;&#30422;&#33539;&#22260;&#21644;&#26032;&#39062;&#24615;&#30340;&#26174;&#33879;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#30697;&#38453;&#20998;&#35299;&#22411;&#25512;&#33616;&#31995;&#32479;&#30340;&#23398;&#20064;&#36807;&#31243;&#20013;&#21152;&#20837;&#19968;&#39033;&#26032;&#30340;&#39033;&#65292;&#31216;&#20026;&#40065;&#33725;&#34892;&#20026;&#65292;&#23427;&#21487;&#20197;&#25511;&#21046;&#22312;&#20570;&#20986;&#20851;&#20110;&#39044;&#27979;&#21487;&#38752;&#24615;&#30340;&#20915;&#31574;&#26102;&#25152;&#24076;&#26395;&#30340;&#39118;&#38505;&#27700;&#24179;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#40065;&#33725;&#34892;&#20026;&#19981;&#20165;&#20801;&#35768;&#36827;&#34892;&#39118;&#38505;&#35843;&#25511;&#65292;&#36824;&#25552;&#39640;&#20102;&#25512;&#33616;&#31995;&#32479;&#25552;&#20379;&#30340;&#39044;&#27979;&#30340;&#25968;&#37327;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems that include some reliability measure of their predictions tend to be more conservative in forecasting, due to their constraint to preserve reliability. This leads to a significant drop in the coverage and novelty that these systems can provide. In this paper, we propose the inclusion of a new term in the learning process of matrix factorization-based recommender systems, called recklessness, which enables the control of the risk level desired when making decisions about the reliability of a prediction. Experimental results demonstrate that recklessness not only allows for risk regulation but also improves the quantity and quality of predictions provided by the recommender system.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#23558;&#23395;&#33410;&#24615;&#20316;&#20026;&#20449;&#21495;&#26469;&#37325;&#26032;&#25490;&#24207;&#30005;&#23376;&#21830;&#21153;&#30340;&#33258;&#21160;&#23436;&#25104;&#21151;&#33021;&#65292;&#20174;&#32780;&#25552;&#39640;&#33258;&#21160;&#23436;&#25104;&#30340;&#30456;&#20851;&#24615;&#21644;&#19994;&#21153;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2308.02055</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#30340;&#30005;&#23376;&#21830;&#21153;&#33258;&#21160;&#23436;&#25104;&#30340;&#23395;&#33410;&#24615;&#37325;&#26032;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Seasonality Based Reranking of E-commerce Autocomplete Using Natural Language Queries. (arXiv:2308.02055v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#23558;&#23395;&#33410;&#24615;&#20316;&#20026;&#20449;&#21495;&#26469;&#37325;&#26032;&#25490;&#24207;&#30005;&#23376;&#21830;&#21153;&#30340;&#33258;&#21160;&#23436;&#25104;&#21151;&#33021;&#65292;&#20174;&#32780;&#25552;&#39640;&#33258;&#21160;&#23436;&#25104;&#30340;&#30456;&#20851;&#24615;&#21644;&#19994;&#21153;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#33258;&#21160;&#23436;&#25104;&#65288;QAC&#65289;&#20063;&#34987;&#31216;&#20026;typeahead&#65292;&#23427;&#22312;&#29992;&#25143;&#22312;&#25628;&#32034;&#26694;&#20013;&#36755;&#20837;&#21069;&#32512;&#26102;&#24314;&#35758;&#23436;&#25972;&#26597;&#35810;&#21015;&#34920;&#12290;&#23427;&#26159;&#29616;&#20195;&#25628;&#32034;&#24341;&#25806;&#29305;&#21035;&#26159;&#22312;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#30340;&#20851;&#38190;&#21151;&#33021;&#20043;&#19968;&#12290;typeahead&#30340;&#30446;&#26631;&#20043;&#19968;&#26159;&#21521;&#29992;&#25143;&#24314;&#35758;&#19982;&#23395;&#33410;&#24615;&#30456;&#20851;&#30340;&#37325;&#35201;&#26597;&#35810;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31639;&#27861;&#65292;&#20197;&#23558;&#23395;&#33410;&#24615;&#20316;&#20026;&#19968;&#20010;&#20449;&#21495;&#24182;&#23545;QAC&#25490;&#24207;&#27169;&#22411;&#36827;&#34892;&#31471;&#21040;&#31471;&#35780;&#20272;&#12290;&#23558;&#23395;&#33410;&#24615;&#32435;&#20837;&#33258;&#21160;&#23436;&#25104;&#25490;&#24207;&#27169;&#22411;&#20013;&#21487;&#20197;&#25552;&#39640;&#33258;&#21160;&#23436;&#25104;&#30340;&#30456;&#20851;&#24615;&#21644;&#19994;&#21153;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Query autocomplete (QAC) also known as typeahead, suggests list of complete queries as user types prefix in the search box. It is one of the key features of modern search engines specially in e-commerce. One of the goals of typeahead is to suggest relevant queries to users which are seasonally important. In this paper we propose a neural network based natural language processing (NLP) algorithm to incorporate seasonality as a signal and present end to end evaluation of the QAC ranking model. Incorporating seasonality into autocomplete ranking model can improve autocomplete relevance and business metric.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#21516;&#27493;&#38543;&#26426;&#32447;&#24615;&#31995;&#32479;&#30340;&#40065;&#26834;&#29420;&#31435;&#24615;&#26816;&#39564;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#38750;&#28176;&#36817;&#24615;&#20445;&#35777;&#30340;&#26174;&#33879;&#27700;&#24179;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#32622;&#20449;&#21306;&#38388;&#20272;&#35745;&#12289;&#25490;&#21015;&#26816;&#39564;&#20197;&#21450;&#19968;&#33324;&#30340;&#20381;&#36182;&#24230;&#37327;&#65292;&#20197;&#26816;&#27979;&#31995;&#32479;&#20043;&#38388;&#30340;&#20219;&#20309;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2308.02054</link><description>&lt;p&gt;
&#38024;&#23545;&#21516;&#27493;&#38543;&#26426;&#32447;&#24615;&#31995;&#32479;&#30340;&#40065;&#26834;&#29420;&#31435;&#24615;&#26816;&#39564;&#21450;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Robust Independence Tests with Finite Sample Guarantees for Synchronous Stochastic Linear Systems. (arXiv:2308.02054v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#21516;&#27493;&#38543;&#26426;&#32447;&#24615;&#31995;&#32479;&#30340;&#40065;&#26834;&#29420;&#31435;&#24615;&#26816;&#39564;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#38750;&#28176;&#36817;&#24615;&#20445;&#35777;&#30340;&#26174;&#33879;&#27700;&#24179;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#32622;&#20449;&#21306;&#38388;&#20272;&#35745;&#12289;&#25490;&#21015;&#26816;&#39564;&#20197;&#21450;&#19968;&#33324;&#30340;&#20381;&#36182;&#24230;&#37327;&#65292;&#20197;&#26816;&#27979;&#31995;&#32479;&#20043;&#38388;&#30340;&#20219;&#20309;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#38543;&#26426;&#32447;&#24615;&#26102;&#19981;&#21464;&#31995;&#32479;&#30340;&#40065;&#26834;&#29420;&#31435;&#24615;&#26816;&#39564;&#26041;&#27861;&#65292;&#20854;&#22312;&#35266;&#27979;&#21040;&#30340;&#36755;&#20986;&#26159;&#21516;&#27493;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#25552;&#20379;&#38750;&#28176;&#36817;&#24615;&#20445;&#35777;&#30340;&#26174;&#33879;&#27700;&#24179;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#20998;&#24067;&#26080;&#20851;&#30340;&#31532;&#19968;&#31867;&#38169;&#35823;&#27010;&#29575;&#30340;&#30028;&#38480;&#65292;&#21363;&#21019;&#26032;&#39033;&#21487;&#20197;&#20855;&#26377;&#20219;&#24847;&#20998;&#24067;&#12290;&#35813;&#31639;&#27861;&#32467;&#21512;&#20102;&#32622;&#20449;&#21306;&#38388;&#20272;&#35745;&#12289;&#25490;&#21015;&#26816;&#39564;&#20197;&#21450;&#19968;&#33324;&#30340;&#20381;&#36182;&#24230;&#37327;&#65292;&#22914;&#24076;&#23572;&#20271;&#29305;-&#26045;&#23494;&#29305;&#29420;&#31435;&#24615;&#20934;&#21017;&#21644;&#36317;&#31163;&#21327;&#26041;&#24046;&#65292;&#20197;&#26816;&#27979;&#35266;&#23519;&#21040;&#30340;&#31995;&#32479;&#20043;&#38388;&#30340;&#20219;&#20309;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#30340;&#20551;&#35774;&#26816;&#39564;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#36890;&#36807;&#33258;&#22238;&#24402;&#31995;&#32479;&#30340;&#31034;&#20363;&#28436;&#31034;&#20102;&#36825;&#20123;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper introduces robust independence tests with non-asymptotically guaranteed significance levels for stochastic linear time-invariant systems, assuming that the observed outputs are synchronous, which means that the systems are driven by jointly i.i.d. noises. Our method provides bounds for the type I error probabilities that are distribution-free, i.e., the innovations can have arbitrary distributions. The algorithm combines confidence region estimates with permutation tests and general dependence measures, such as the Hilbert-Schmidt independence criterion and the distance covariance, to detect any nonlinear dependence between the observed systems. We also prove the consistency of our hypothesis tests under mild assumptions and demonstrate the ideas through the example of autoregressive systems.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;PDF&#30340;&#20803;&#25968;&#25454;&#65292;&#23558;&#27599;&#20010;&#39029;&#38754;&#34920;&#31034;&#20026;&#19968;&#20010;&#32467;&#26500;&#21270;&#22270;&#65292;&#24182;&#23558;&#24067;&#23616;&#20998;&#26512;&#38382;&#39064;&#36716;&#21270;&#20026;&#22270;&#20998;&#21106;&#21644;&#20998;&#31867;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#26356;&#23567;&#19988;&#24615;&#33021;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2308.02051</link><description>&lt;p&gt;
&#12298;&#19968;&#31181;&#22270;&#24418;&#21270;&#30340;&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#26041;&#27861;&#12299;
&lt;/p&gt;
&lt;p&gt;
A Graphical Approach to Document Layout Analysis. (arXiv:2308.02051v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02051
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;PDF&#30340;&#20803;&#25968;&#25454;&#65292;&#23558;&#27599;&#20010;&#39029;&#38754;&#34920;&#31034;&#20026;&#19968;&#20010;&#32467;&#26500;&#21270;&#22270;&#65292;&#24182;&#23558;&#24067;&#23616;&#20998;&#26512;&#38382;&#39064;&#36716;&#21270;&#20026;&#22270;&#20998;&#21106;&#21644;&#20998;&#31867;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#26356;&#23567;&#19988;&#24615;&#33021;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#65288;DLA&#65289;&#26159;&#26816;&#27979;&#25991;&#26723;&#20013;&#19981;&#21516;&#35821;&#20041;&#20869;&#23481;&#24182;&#23558;&#20854;&#27491;&#30830;&#20998;&#31867;&#21040;&#36866;&#24403;&#31867;&#21035;&#65288;&#22914;&#25991;&#26412;&#12289;&#26631;&#39064;&#12289;&#22270;&#34920;&#65289;&#30340;&#20219;&#21153;&#12290;DLA&#27969;&#27700;&#32447;&#20351;&#29992;&#25143;&#33021;&#22815;&#23558;&#25991;&#26723;&#36716;&#25442;&#20026;&#32467;&#26500;&#21270;&#30340;&#26426;&#22120;&#21487;&#35835;&#26684;&#24335;&#65292;&#28982;&#21518;&#21487;&#29992;&#20110;&#35768;&#22810;&#26377;&#29992;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;DLA&#27169;&#22411;&#23558;&#25991;&#26723;&#34920;&#31034;&#20026;&#22270;&#20687;&#65292;&#20002;&#24323;&#20102;&#22312;&#30005;&#23376;&#29983;&#25104;&#30340;PDF&#20013;&#21487;&#29992;&#30340;&#20016;&#23500;&#20803;&#25968;&#25454;&#12290;&#36890;&#36807;&#30452;&#25509;&#21033;&#29992;&#36825;&#20123;&#20803;&#25968;&#25454;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;PDF&#39029;&#38754;&#34920;&#31034;&#20026;&#19968;&#20010;&#32467;&#26500;&#21270;&#22270;&#65292;&#24182;&#23558;DLA&#38382;&#39064;&#35270;&#20026;&#22270;&#20998;&#21106;&#21644;&#20998;&#31867;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#22270;&#30340;&#24067;&#23616;&#20998;&#26512;&#27169;&#22411;&#65288;GLAM&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#19982;SOTA&#27169;&#22411;&#22312;&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;DLA&#25968;&#25454;&#38598;&#19978;&#31454;&#20105; - &#21516;&#26102;&#27604;&#29616;&#26377;&#27169;&#22411;&#23567;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;&#29305;&#21035;&#26159;&#65292;&#20855;&#26377;400&#19975;&#20010;&#21442;&#25968;&#30340;GLAM&#27169;&#22411;&#20248;&#20110;&#39046;&#20808;&#30340;1.4&#20159;&#21442;&#25968;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document layout analysis (DLA) is the task of detecting the distinct, semantic content within a document and correctly classifying these items into an appropriate category (e.g., text, title, figure). DLA pipelines enable users to convert documents into structured machine-readable formats that can then be used for many useful downstream tasks. Most existing state-of-the-art (SOTA) DLA models represent documents as images, discarding the rich metadata available in electronically generated PDFs. Directly leveraging this metadata, we represent each PDF page as a structured graph and frame the DLA problem as a graph segmentation and classification problem. We introduce the Graph-based Layout Analysis Model (GLAM), a lightweight graph neural network competitive with SOTA models on two challenging DLA datasets - while being an order of magnitude smaller than existing models. In particular, the 4-million parameter GLAM model outperforms the leading 140M+ parameter computer vision-based model 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FuNToM&#30340;RF&#30005;&#36335;&#21151;&#33021;&#24314;&#27169;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#21452;&#31471;&#21475;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#35757;&#32451;&#25968;&#25454;&#25910;&#38598;&#21644;&#21518;&#24067;&#23616;&#24314;&#27169;&#20223;&#30495;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.02050</link><description>&lt;p&gt;
FuNToM: &#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36741;&#21161;&#21452;&#31471;&#21475;&#20998;&#26512;&#26041;&#27861;&#36827;&#34892;RF&#30005;&#36335;&#30340;&#21151;&#33021;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
FuNToM: Functional Modeling of RF Circuits Using a Neural Network Assisted Two-Port Analysis Method. (arXiv:2308.02050v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FuNToM&#30340;RF&#30005;&#36335;&#21151;&#33021;&#24314;&#27169;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#21452;&#31471;&#21475;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#35757;&#32451;&#25968;&#25454;&#25910;&#38598;&#21644;&#21518;&#24067;&#23616;&#24314;&#27169;&#20223;&#30495;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21512;&#25104;&#27169;&#25311;&#21644;&#23556;&#39057;&#65288;RF&#65289;&#30005;&#36335;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#38656;&#35201;&#19968;&#31181;&#39640;&#25928;&#30340;&#30005;&#36335;&#24314;&#27169;&#26041;&#27861;&#12290;&#36825;&#26159;&#22240;&#20026;&#22312;&#27599;&#20010;&#21512;&#25104;&#21608;&#26399;&#20013;&#36816;&#34892;&#22823;&#37327;&#20223;&#30495;&#30340;&#25104;&#26412;&#26114;&#36149;&#12290;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#26159;&#30005;&#36335;&#24314;&#27169;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#36895;&#24230;&#24555;&#19988;&#30456;&#23545;&#20934;&#30830;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#20173;&#28982;&#20351;&#29992;&#27169;&#25311;&#36816;&#34892;&#25910;&#38598;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#28155;&#21152;&#25110;&#21024;&#38500;&#19968;&#20010;&#21333;&#19968;&#20803;&#32032;&#65292;&#36825;&#20123;&#26041;&#27861;&#20063;&#38656;&#35201;&#20026;&#27599;&#20010;&#30005;&#36335;&#25299;&#25169;&#25910;&#38598;&#19968;&#20010;&#23436;&#25972;&#30340;&#21333;&#29420;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#38382;&#39064;&#22312;&#38656;&#35201;&#36827;&#34892;&#21518;&#24067;&#23616;&#24314;&#27169;&#20223;&#30495;&#26102;&#23588;&#20026;&#20005;&#37325;&#65292;&#32780;&#36825;&#38656;&#35201;&#26356;&#38271;&#30340;&#26102;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#25105;&#20204;&#25552;&#20986;&#20102;FuNToM&#65292;&#19968;&#31181;&#29992;&#20110;RF&#30005;&#36335;&#30340;&#21151;&#33021;&#24314;&#27169;&#26041;&#27861;&#12290;FuNToM&#21033;&#29992;&#20102;&#21452;&#31471;&#21475;&#20998;&#26512;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#20010;&#20027;&#25968;&#25454;&#38598;&#21644;&#22810;&#20010;&#23567;&#25968;&#25454;&#38598;&#26469;&#24314;&#27169;&#22810;&#20010;&#25299;&#25169;&#12290;&#23427;&#36824;&#21033;&#29992;&#20102;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic synthesis of analog and Radio Frequency (RF) circuits is a trending approach that requires an efficient circuit modeling method. This is due to the expensive cost of running a large number of simulations at each synthesis cycle. Artificial intelligence methods are promising approaches for circuit modeling due to their speed and relative accuracy. However, existing approaches require a large amount of training data, which is still collected using simulation runs. In addition, such approaches collect a whole separate dataset for each circuit topology even if a single element is added or removed. These matters are only exacerbated by the need for post-layout modeling simulations, which take even longer. To alleviate these drawbacks, in this paper, we present FuNToM, a functional modeling method for RF circuits. FuNToM leverages the two-port analysis method for modeling multiple topologies using a single main dataset and multiple small datasets. It also leverages neural networks 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20934;&#30830;&#30340;&#31354;&#38388;&#25104;&#26412;&#26799;&#24230;&#22312;&#21487;&#24494;&#30340;&#39640;&#20998;&#36776;&#29575;&#27700;&#25991;&#27169;&#22411;&#20013;&#23398;&#20064;&#21010;&#20998;&#21306;&#22495;&#21270;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#21306;&#22495;&#21270;&#26144;&#23556;&#65292;&#32467;&#21512;&#25968;&#25454;&#21516;&#21270;&#21644;&#21442;&#25968;&#26657;&#27491;&#65292;&#33021;&#22815;&#22312;&#24191;&#27867;&#30340;&#26102;&#31354;&#35745;&#31639;&#22495;&#20013;&#21033;&#29992;&#19981;&#21516;&#25968;&#25454;&#38598;&#36827;&#34892;&#27700;&#25991;&#21442;&#25968;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2308.02040</link><description>&lt;p&gt;
&#20351;&#29992;&#20934;&#30830;&#30340;&#31354;&#38388;&#25104;&#26412;&#26799;&#24230;&#22312;&#21487;&#24494;&#30340;&#39640;&#20998;&#36776;&#29575;&#27700;&#25991;&#27169;&#22411;&#20013;&#23398;&#20064;&#21010;&#20998;&#21306;&#22495;&#21270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Regionalization within a Differentiable High-Resolution Hydrological Model using Accurate Spatial Cost Gradients. (arXiv:2308.02040v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20934;&#30830;&#30340;&#31354;&#38388;&#25104;&#26412;&#26799;&#24230;&#22312;&#21487;&#24494;&#30340;&#39640;&#20998;&#36776;&#29575;&#27700;&#25991;&#27169;&#22411;&#20013;&#23398;&#20064;&#21010;&#20998;&#21306;&#22495;&#21270;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#21306;&#22495;&#21270;&#26144;&#23556;&#65292;&#32467;&#21512;&#25968;&#25454;&#21516;&#21270;&#21644;&#21442;&#25968;&#26657;&#27491;&#65292;&#33021;&#22815;&#22312;&#24191;&#27867;&#30340;&#26102;&#31354;&#35745;&#31639;&#22495;&#20013;&#21033;&#29992;&#19981;&#21516;&#25968;&#25454;&#38598;&#36827;&#34892;&#27700;&#25991;&#21442;&#25968;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27809;&#26377;&#27969;&#37327;&#25968;&#25454;&#30340;&#26410;&#30417;&#27979;&#27969;&#22495;&#20013;&#20272;&#35745;&#31354;&#38388;&#20998;&#24067;&#30340;&#27700;&#25991;&#21442;&#25968;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21306;&#22495;&#21270;&#38382;&#39064;&#65292;&#38656;&#35201;&#32771;&#34385;&#27969;&#37327;&#25968;&#25454;&#30340;&#31232;&#30095;&#24615;&#24182;&#26045;&#21152;&#31354;&#38388;&#32422;&#26463;&#12290;&#19968;&#31181;&#21487;&#33021;&#30340;&#26041;&#27861;&#26159;&#23547;&#25214;&#19968;&#20010;&#23450;&#37327;&#23558;&#29289;&#29702;&#25351;&#26631;&#19982;&#27010;&#24565;&#27169;&#22411;&#21442;&#25968;&#32852;&#31995;&#36215;&#26469;&#30340;&#36716;&#31227;&#20989;&#25968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#25968;&#25454;&#21516;&#21270;&#21644;&#21442;&#25968;&#21306;&#22495;&#21270;&#65288;HDA-PR&#65289;&#26041;&#27861;&#65292;&#23558;&#21487;&#23398;&#20064;&#30340;&#21306;&#22495;&#21270;&#26144;&#23556;&#65288;&#22522;&#20110;&#22810;&#20803;&#22238;&#24402;&#25110;&#31070;&#32463;&#32593;&#32476;&#65289;&#24341;&#20837;&#21487;&#24494;&#30340;&#27700;&#25991;&#27169;&#22411;&#20013;&#12290;&#23427;&#21487;&#20197;&#21033;&#29992;&#20934;&#30830;&#30340;&#20276;&#38543;&#26799;&#24230;&#22312;&#39640;&#32500;&#30340;&#21306;&#22495;&#21270;&#32972;&#26223;&#19979;&#22312;&#24191;&#27867;&#30340;&#26102;&#31354;&#35745;&#31639;&#22495;&#20013;&#21033;&#29992;&#24322;&#36136;&#25968;&#25454;&#12290;&#36870;&#38382;&#39064;&#36890;&#36807;&#22810;&#20010;&#35266;&#27979;&#31449;&#28857;&#30340;&#20449;&#24687;&#35745;&#31639;&#26657;&#27491;&#20195;&#20215;&#20989;&#25968;&#26469;&#35299;&#20915;&#12290;HDA-PR&#22312;&#39640;&#20998;&#36776;&#29575;&#12289;&#23567;&#26102;&#32423;&#21644;&#20844;&#37324;&#32423;&#30340;&#21306;&#22495;&#27169;&#25311;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating spatially distributed hydrological parameters in ungauged catchments poses a challenging regionalization problem and requires imposing spatial constraints given the sparsity of discharge data. A possible approach is to search for a transfer function that quantitatively relates physical descriptors to conceptual model parameters. This paper introduces a Hybrid Data Assimilation and Parameter Regionalization (HDA-PR) approach incorporating learnable regionalization mappings, based on either multivariate regressions or neural networks, into a differentiable hydrological model. It enables the exploitation of heterogeneous datasets across extensive spatio-temporal computational domains within a high-dimensional regionalization context, using accurate adjoint-based gradients. The inverse problem is tackled with a multi-gauge calibration cost function accounting for information from multiple observation sites. HDA-PR was tested on high-resolution, hourly and kilometric regional mod
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;Twitter&#19978;&#26410;&#26469;&#20027;&#20041;&#32773;&#23545;&#26410;&#26469;&#30340;&#39044;&#26399;&#65292;&#25506;&#31350;&#35821;&#35328;&#25552;&#31034;&#23545;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#30340;&#39044;&#27979;&#24615;&#24605;&#32500;&#30340;&#24433;&#21709;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27969;&#31243;&#21644;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2308.02035</link><description>&lt;p&gt;
Twitter&#25968;&#25454;&#21578;&#35785;&#25105;&#20204;&#20851;&#20110;&#26410;&#26469;&#30340;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Twitter Data Tell Us about the Future?. (arXiv:2308.02035v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;Twitter&#19978;&#26410;&#26469;&#20027;&#20041;&#32773;&#23545;&#26410;&#26469;&#30340;&#39044;&#26399;&#65292;&#25506;&#31350;&#35821;&#35328;&#25552;&#31034;&#23545;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#30340;&#39044;&#27979;&#24615;&#24605;&#32500;&#30340;&#24433;&#21709;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27969;&#31243;&#21644;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#26159;&#19968;&#31181;&#22522;&#26412;&#30340;&#20154;&#31867;&#35748;&#30693;&#33021;&#21147;&#65292;&#28041;&#21450;&#23545;&#26410;&#26469;&#30340;&#24605;&#32771;&#21644;&#29983;&#27963;&#12290;&#34429;&#28982;&#35821;&#35328;&#26631;&#35760;&#21453;&#26144;&#20102;&#39044;&#27979;&#24615;&#24605;&#32500;&#65292;&#20294;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35282;&#24230;&#26469;&#30740;&#31350;&#39044;&#27979;&#24615;&#24605;&#32500;&#30340;&#30740;&#31350;&#36824;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35843;&#26597;Twitter&#19978;&#26410;&#26469;&#20027;&#20041;&#32773;&#23545;&#26410;&#26469;&#30340;&#39044;&#26399;&#65292;&#24182;&#25506;&#32034;&#35821;&#35328;&#25552;&#31034;&#23545;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#30340;&#39044;&#27979;&#24615;&#24605;&#32500;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;Twitter&#26410;&#26469;&#20027;&#20041;&#32773;&#39044;&#27979;&#21644;&#20998;&#20139;&#21738;&#20123;&#26410;&#26469;&#65292;&#20197;&#21450;&#22914;&#20309;&#20174;&#31038;&#20132;&#25968;&#25454;&#20013;&#24314;&#27169;&#36825;&#20123;&#39044;&#26399;&#26410;&#26469;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#20851;&#20110;&#39044;&#27979;&#30340;&#30456;&#20851;&#24037;&#20316;&#65292;&#35752;&#35770;&#20102;&#35821;&#35328;&#26631;&#35760;&#21644;&#30693;&#21517;&#20154;&#22763;&#23545;&#39044;&#27979;&#24615;&#24605;&#32500;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#26410;&#26469;&#20998;&#20026;&#8220;&#29616;&#22312;&#30340;&#26410;&#26469;&#8221;&#21644;&#8220;&#26410;&#26469;&#30340;&#29616;&#22312;&#8221; &#30340;&#20998;&#31867;&#31995;&#32479;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#30001;&#26410;&#26469;&#24433;&#21709;&#32773;&#20844;&#24320;&#20998;&#20139;&#30340;&#36229;&#36807;100&#19975;&#26465;&#25512;&#25991;&#30340;&#32534;&#35793;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;SOTA&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anticipation is a fundamental human cognitive ability that involves thinking about and living towards the future. While language markers reflect anticipatory thinking, research on anticipation from the perspective of natural language processing is limited. This study aims to investigate the futures projected by futurists on Twitter and explore the impact of language cues on anticipatory thinking among social media users. We address the research questions of what futures Twitter's futurists anticipate and share, and how these anticipated futures can be modeled from social data. To investigate this, we review related works on anticipation, discuss the influence of language markers and prestigious individuals on anticipatory thinking, and present a taxonomy system categorizing futures into "present futures" and "future present". This research presents a compiled dataset of over 1 million publicly shared tweets by future influencers and develops a scalable NLP pipeline using SOTA models. T
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#39044;&#27979;&#20102;&#32654;&#22269;&#30005;&#21160;&#33258;&#34892;&#36710;&#38144;&#37327;&#30340;&#22686;&#38271;&#65292;&#24182;&#35780;&#20272;&#20102;&#24433;&#21709;&#30005;&#21160;&#33258;&#34892;&#36710;&#20351;&#29992;&#30340;&#22240;&#32032;&#12290;&#26681;&#25454;&#39044;&#27979;&#32467;&#26524;&#65292;&#39044;&#35745;2025&#24180;&#30005;&#21160;&#33258;&#34892;&#36710;&#38144;&#21806;&#37327;&#20026;130&#19975;&#36742;&#65292;2028&#24180;&#20026;211.3&#19975;&#36742;&#12290;</title><link>http://arxiv.org/abs/2308.02034</link><description>&lt;p&gt;
&#30005;&#21160;&#33258;&#34892;&#36710;&#20351;&#29992;&#30340;&#22686;&#38271;&#65306;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
The Growth of E-Bike Use: A Machine Learning Approach. (arXiv:2308.02034v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#39044;&#27979;&#20102;&#32654;&#22269;&#30005;&#21160;&#33258;&#34892;&#36710;&#38144;&#37327;&#30340;&#22686;&#38271;&#65292;&#24182;&#35780;&#20272;&#20102;&#24433;&#21709;&#30005;&#21160;&#33258;&#34892;&#36710;&#20351;&#29992;&#30340;&#22240;&#32032;&#12290;&#26681;&#25454;&#39044;&#27979;&#32467;&#26524;&#65292;&#39044;&#35745;2025&#24180;&#30005;&#21160;&#33258;&#34892;&#36710;&#38144;&#21806;&#37327;&#20026;130&#19975;&#36742;&#65292;2028&#24180;&#20026;211.3&#19975;&#36742;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#30005;&#21160;&#33258;&#34892;&#36710;&#65288;&#30005;&#21333;&#36710;&#65289;&#21450;&#20854;&#23545;&#32654;&#22269;&#25919;&#31574;&#21046;&#23450;&#32773;&#30340;&#24433;&#21709;&#12290;&#30005;&#21160;&#33258;&#34892;&#36710;&#20316;&#20026;&#19968;&#31181;&#24555;&#36895;&#21644;&#29615;&#20445;&#30340;&#20132;&#36890;&#36873;&#25321;&#65292;&#24050;&#32463;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#27969;&#34892;&#12290;&#22312;&#25105;&#20204;&#36861;&#27714;&#21487;&#25345;&#32493;&#33021;&#28304;&#35745;&#21010;&#30340;&#36807;&#31243;&#20013;&#65292;&#20102;&#35299;&#30005;&#21160;&#33258;&#34892;&#36710;&#30340;&#22686;&#38271;&#21644;&#24433;&#21709;&#23545;&#25919;&#31574;&#21046;&#23450;&#32773;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#25968;&#23398;&#27169;&#22411;&#20026;&#30005;&#21160;&#33258;&#34892;&#36710;&#30340;&#20215;&#20540;&#21644;&#20854;&#22312;&#26410;&#26469;&#30340;&#35282;&#33394;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;&#25105;&#20204;&#20351;&#29992;ARIMA&#27169;&#22411;&#65292;&#19968;&#31181;&#30417;&#30563;&#24335;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#39044;&#27979;&#20102;&#32654;&#22269;&#30005;&#21160;&#33258;&#34892;&#36710;&#38144;&#37327;&#30340;&#22686;&#38271;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;&#20174;2006&#24180;1&#26376;&#21040;2022&#24180;12&#26376;&#30340;&#21382;&#21490;&#38144;&#21806;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#39044;&#27979;2025&#24180;&#38144;&#21806;&#37327;&#20026;130&#19975;&#36742;&#65292;2028&#24180;&#38144;&#21806;&#37327;&#20026;211.3&#19975;&#36742;&#12290;&#20026;&#20102;&#35780;&#20272;&#24433;&#21709;&#30005;&#21160;&#33258;&#34892;&#36710;&#20351;&#29992;&#30340;&#22240;&#32032;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#38543;&#26426;&#26862;&#26519;&#22238;&#24402;&#27169;&#22411;&#12290;&#24433;&#21709;&#30005;&#21160;&#33258;&#34892;&#36710;&#38144;&#37327;&#22686;&#38271;&#26368;&#26174;&#33879;&#30340;&#22240;&#32032;&#26159;&#21487;&#25903;&#37197;&#20010;&#20154;&#25910;&#20837;&#21644;&#21463;&#27426;&#36814;&#31243;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#30005;&#21160;&#33258;&#34892;&#36710;&#23545;&#29615;&#22659;&#21644;&#20581;&#24247;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present our work on electric bicycles (e-bikes) and their implications for policymakers in the United States. E-bikes have gained significant popularity as a fast and eco-friendly transportation option. As we strive for a sustainable energy plan, understanding the growth and impact of e-bikes is crucial for policymakers. Our mathematical modeling offers insights into the value of e-bikes and their role in the future. Using an ARIMA model, a supervised machine-learning algorithm, we predicted the growth of e-bike sales in the U.S. Our model, trained on historical sales data from January 2006 to December 2022, projected sales of 1.3 million units in 2025 and 2.113 million units in 2028. To assess the factors contributing to e-bike usage, we employed a Random Forest regression model. The most significant factors influencing e-bike sales growth were disposable personal income and popularity. Furthermore, we examined the environmental and health impacts of e-bikes. Through Monte Carlo si
&lt;/p&gt;</description></item><item><title>&#30693;&#35782;&#22686;&#24378;&#30340;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;&#30693;&#35782;&#22270;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#23433;&#20840;&#24615;&#65292;&#22312;&#32593;&#32476;&#23433;&#20840;&#21644;&#38544;&#31169;&#20445;&#25252;&#39046;&#22495;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2308.02031</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#29992;&#20110;&#32593;&#32476;&#23433;&#20840;&#21644;&#38544;&#31169;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Knowledge-enhanced Neuro-Symbolic AI for Cybersecurity and Privacy. (arXiv:2308.02031v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02031
&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22686;&#24378;&#30340;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;&#30693;&#35782;&#22270;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#23433;&#20840;&#24615;&#65292;&#22312;&#32593;&#32476;&#23433;&#20840;&#21644;&#38544;&#31169;&#20445;&#25252;&#39046;&#22495;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#20010;&#26032;&#20852;&#19988;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#23427;&#23558;&#65288;&#28145;&#24230;&#65289;&#31070;&#32463;&#32593;&#32476;&#30340;&#23376;&#31526;&#21495;&#20248;&#21183;&#19982;&#30693;&#35782;&#22270;&#20013;&#30340;&#26174;&#24335;&#31526;&#21495;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#24403;&#21069;&#19968;&#20195;&#31995;&#32479;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#21363;&#23427;&#20204;&#26080;&#27861;&#20026;&#20854;&#32467;&#26524;&#29983;&#25104;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#35299;&#37322;&#65292;&#24182;&#22312;&#23384;&#22312;&#8220;&#26410;&#30693;&#26410;&#30693;&#8221;&#65288;&#20363;&#22914;&#32593;&#32476;&#23433;&#20840;&#12289;&#38544;&#31169;&#65289;&#24773;&#20917;&#19979;&#30830;&#20445;&#23433;&#20840;&#34892;&#20026;&#12290;&#31070;&#32463;&#32593;&#32476;&#65288;&#25797;&#38271;&#25506;&#32034;&#22797;&#26434;&#25968;&#25454;&#31354;&#38388;&#65289;&#21644;&#31526;&#21495;&#30693;&#35782;&#22270;&#30340;&#25972;&#21512;&#65288;&#20195;&#34920;&#39046;&#22495;&#30693;&#35782;&#65289;&#20351;&#24471;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#33021;&#22815;&#20197;&#19987;&#23478;&#21487;&#29702;&#35299;&#30340;&#26041;&#24335;&#36827;&#34892;&#25512;&#29702;&#12289;&#23398;&#20064;&#21644;&#27867;&#21270;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#22914;&#20309;&#22312;&#32593;&#32476;&#23433;&#20840;&#21644;&#38544;&#31169;&#20445;&#25252;&#39046;&#22495;&#20013;&#24212;&#29992;&#65292;&#24182;&#19988;&#36825;&#20004;&#20010;&#39046;&#22495;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#39640;&#20934;&#30830;&#24615;&#38656;&#27714;&#26368;&#22823;&#65292;&#21487;&#20197;&#20174;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#20013;&#33719;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neuro-Symbolic Artificial Intelligence (AI) is an emerging and quickly advancing field that combines the subsymbolic strengths of (deep) neural networks and explicit, symbolic knowledge contained in knowledge graphs to enhance explainability and safety in AI systems. This approach addresses a key criticism of current generation systems, namely their inability to generate human-understandable explanations for their outcomes and ensure safe behaviors, especially in scenarios with \textit{unknown unknowns} (e.g. cybersecurity, privacy). The integration of neural networks, which excel at exploring complex data spaces, and symbolic knowledge graphs, which represent domain knowledge, allows AI systems to reason, learn, and generalize in a manner understandable to experts. This article describes how applications in cybersecurity and privacy, two most demanding domains in terms of the need for AI to be explainable while being highly accurate in complex environments, can benefit from Neuro-Symb
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Deep Maxout&#32593;&#32476;&#29305;&#24449;&#34701;&#21512;&#21644;Political Tangent Search&#20248;&#21270;&#22120;&#30340;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#22320;&#20013;&#28023;&#36139;&#34880;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#23545;&#36755;&#20837;&#25968;&#25454;&#36827;&#34892;&#24402;&#19968;&#21270;&#22788;&#29702;&#65292;&#24182;&#21033;&#29992;Deep Maxout&#32593;&#32476;&#30340;&#29305;&#24449;&#34701;&#21512;&#21644;&#36807;&#37319;&#26679;&#26041;&#27861;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#26368;&#32456;&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#36827;&#34892;&#22320;&#20013;&#28023;&#36139;&#34880;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.02029</link><description>&lt;p&gt;
&#22522;&#20110;Deep Maxout&#32593;&#32476;&#29305;&#24449;&#34701;&#21512;&#21644;Political Tangent Search&#20248;&#21270;&#22120;&#30340;&#36716;&#31227;&#23398;&#20064;&#22312;&#22320;&#20013;&#28023;&#36139;&#34880;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Maxout Network-based Feature Fusion and Political Tangent Search Optimizer enabled Transfer Learning for Thalassemia Detection. (arXiv:2308.02029v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Deep Maxout&#32593;&#32476;&#29305;&#24449;&#34701;&#21512;&#21644;Political Tangent Search&#20248;&#21270;&#22120;&#30340;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#22320;&#20013;&#28023;&#36139;&#34880;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#23545;&#36755;&#20837;&#25968;&#25454;&#36827;&#34892;&#24402;&#19968;&#21270;&#22788;&#29702;&#65292;&#24182;&#21033;&#29992;Deep Maxout&#32593;&#32476;&#30340;&#29305;&#24449;&#34701;&#21512;&#21644;&#36807;&#37319;&#26679;&#26041;&#27861;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#26368;&#32456;&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#36827;&#34892;&#22320;&#20013;&#28023;&#36139;&#34880;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#20013;&#28023;&#36139;&#34880;&#26159;&#19968;&#31181;&#36951;&#20256;&#24615;&#34880;&#28082;&#30149;&#65292;&#30001;&#36951;&#20256;&#32570;&#38519;&#23548;&#33268;&#34880;&#32418;&#34507;&#30333;&#22810;&#32957;&#38142;&#30340;&#20135;&#29983;&#19981;&#36275;&#12290;&#28982;&#32780;&#65292;&#23545;&#36825;&#20123;&#22320;&#21306;&#30340;&#21457;&#30149;&#39057;&#29575;&#21644;&#20849;&#20139;&#31243;&#24230;&#30340;&#20102;&#35299;&#36739;&#23569;&#12290;&#20102;&#35299;&#22320;&#20013;&#28023;&#36139;&#34880;&#21457;&#29983;&#30340;&#39057;&#29575;&#21644;&#21487;&#38752;&#31361;&#21464;&#26159;&#39044;&#38450;&#12289;&#25511;&#21046;&#21644;&#27835;&#30103;&#35745;&#21010;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;Political Tangent Search&#20248;&#21270;&#22120;&#30340;&#36716;&#31227;&#23398;&#20064;&#65288;PTSO_TL&#65289;&#22312;&#22320;&#20013;&#28023;&#36139;&#34880;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#39318;&#20808;&#65292;&#20174;&#29305;&#23450;&#25968;&#25454;&#38598;&#33719;&#21462;&#30340;&#36755;&#20837;&#25968;&#25454;&#22312;&#25968;&#25454;&#24402;&#19968;&#21270;&#38454;&#27573;&#36827;&#34892;&#20102;&#35268;&#33539;&#21270;&#12290;&#25968;&#25454;&#24402;&#19968;&#21270;&#38454;&#27573;&#21033;&#29992;&#20998;&#20301;&#25968;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#28982;&#21518;&#23558;&#25968;&#25454;&#20256;&#36882;&#32473;&#29305;&#24449;&#34701;&#21512;&#38454;&#27573;&#65292;&#22312;&#35813;&#38454;&#27573;&#21033;&#29992;Deep Maxout&#32593;&#32476;&#30340;&#21152;&#26435;&#27431;&#27663;&#36317;&#31163;&#36827;&#34892;&#29305;&#24449;&#34701;&#21512;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#36807;&#37319;&#26679;&#26041;&#27861;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#20197;&#22686;&#21152;&#25968;&#25454;&#32500;&#24230;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#36827;&#34892;&#22320;&#20013;&#28023;&#36139;&#34880;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Thalassemia is a heritable blood disorder which is the outcome of a genetic defect causing lack of production of hemoglobin polypeptide chains. However, there is less understanding of the precise frequency as well as sharing in these areas. Knowing about the frequency of thalassemia occurrence and dependable mutations is thus a significant step in preventing, controlling, and treatment planning. Here, Political Tangent Search Optimizer based Transfer Learning (PTSO_TL) is introduced for thalassemia detection. Initially, input data obtained from a particular dataset is normalized in the data normalization stage. Quantile normalization is utilized in the data normalization stage, and the data are then passed to the feature fusion phase, in which Weighted Euclidean Distance with Deep Maxout Network (DMN) is utilized. Thereafter, data augmentation is performed using the oversampling method to increase data dimensionality. Lastly, thalassemia detection is carried out by TL, wherein a convol
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#32467;&#21512;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#38754;&#21521;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#32852;&#37030;&#34920;&#31034;&#23398;&#20064;&#65292;&#23558;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#38544;&#31169;&#25968;&#25454;&#29992;&#20110;&#23398;&#20064;&#20581;&#22766;&#30340;&#38899;&#39057;&#34920;&#31034;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2308.02013</link><description>&lt;p&gt;
&#38754;&#21521;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#32852;&#37030;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Representation Learning for Automatic Speech Recognition. (arXiv:2308.02013v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02013
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#32467;&#21512;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#38754;&#21521;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#32852;&#37030;&#34920;&#31034;&#23398;&#20064;&#65292;&#23558;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#38544;&#31169;&#25968;&#25454;&#29992;&#20110;&#23398;&#20064;&#20581;&#22766;&#30340;&#38899;&#39057;&#34920;&#31034;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#27169;&#24335;&#65292;&#20801;&#35768;&#36793;&#32536;&#35774;&#22791;&#22312;&#19981;&#20849;&#20139;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21327;&#20316;&#23398;&#20064;&#12290;&#20687;Alexa&#21644;Siri&#36825;&#26679;&#30340;&#36793;&#32536;&#35774;&#22791;&#26159;&#28508;&#22312;&#30340;&#38750;&#26631;&#35760;&#38899;&#39057;&#25968;&#25454;&#26469;&#28304;&#65292;&#21487;&#20197;&#29992;&#26469;&#23398;&#20064;&#20581;&#22766;&#30340;&#38899;&#39057;&#34920;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#21644;FL&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#36981;&#23432;&#25968;&#25454;&#38544;&#31169;&#32422;&#26463;&#26465;&#20214;&#65292;&#23398;&#20064;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#35821;&#38899;&#25968;&#25454;&#38598;Libri-Light&#20013;&#30340;&#35828;&#35805;&#32773;&#21644;&#31456;&#33410;&#20449;&#24687;&#65292;&#27169;&#25311;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#35828;&#35805;&#32773;&#38548;&#31163;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#20351;&#29992;FedSGD&#22312;&#23545;&#27604;&#39044;&#27979;&#32534;&#30721;&#26694;&#26550;&#19979;&#36827;&#34892;LSTM&#32534;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;FL&#20013;&#39044;&#35757;&#32451;&#30340;ASR&#32534;&#30721;&#22120;&#30340;&#24615;&#33021;&#19982;&#20013;&#24515;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#19988;&#30456;&#27604;&#27809;&#26377;&#39044;&#35757;&#32451;&#65292;&#26377;12-15%&#65288;WER&#65289;&#30340;&#25913;&#21892;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#32852;&#37030;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#21040;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#65292;&#27861;&#35821;&#65292;&#24182;&#19988;&#30456;&#27604;&#27809;&#26377;&#39044;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;20%&#65288;WER&#65289;&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a privacy-preserving paradigm, allowing edge devices to learn collaboratively without sharing data. Edge devices like Alexa and Siri are prospective sources of unlabeled audio data that can be tapped to learn robust audio representations. In this work, we bring Self-supervised Learning (SSL) and FL together to learn representations for Automatic Speech Recognition respecting data privacy constraints. We use the speaker and chapter information in the unlabeled speech dataset, Libri-Light, to simulate non-IID speaker-siloed data distributions and pre-train an LSTM encoder with the Contrastive Predictive Coding framework with FedSGD. We show that the pre-trained ASR encoder in FL performs as well as a centrally pre-trained model and produces an improvement of 12-15% (WER) compared to no pre-training. We further adapt the federated pre-trained models to a new language, French, and show a 20% (WER) improvement over no pre-training.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#20855;&#26377;&#24179;&#28369;&#28608;&#27963;&#20989;&#25968;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#23384;&#20648;&#23481;&#37327;&#19979;&#30028;&#20026;md/2&#65292;&#24182;&#19988;&#20934;&#30830;&#24615;&#22823;&#33268;&#20026;2&#20493;&#12290;&#20998;&#26512;&#23384;&#20648;&#23481;&#37327;&#30340;&#26041;&#27861;&#21253;&#25324;&#35745;&#31639;&#32593;&#32476;&#38597;&#21487;&#27604;&#30697;&#38453;&#30340;&#31209;&#65292;&#24182;&#25193;&#23637;&#20102;&#26377;&#20851;Hadamard&#24130;&#31209;&#30340;&#32463;&#20856;&#32447;&#24615;&#20195;&#25968;&#20107;&#23454;&#12290;</title><link>http://arxiv.org/abs/2308.02001</link><description>&lt;p&gt;
&#20855;&#26377;&#24179;&#28369;&#28608;&#27963;&#20989;&#25968;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#23384;&#20648;&#23481;&#37327;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Memory capacity of two layer neural networks with smooth activations. (arXiv:2308.02001v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02001
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#20855;&#26377;&#24179;&#28369;&#28608;&#27963;&#20989;&#25968;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#23384;&#20648;&#23481;&#37327;&#19979;&#30028;&#20026;md/2&#65292;&#24182;&#19988;&#20934;&#30830;&#24615;&#22823;&#33268;&#20026;2&#20493;&#12290;&#20998;&#26512;&#23384;&#20648;&#23481;&#37327;&#30340;&#26041;&#27861;&#21253;&#25324;&#35745;&#31639;&#32593;&#32476;&#38597;&#21487;&#27604;&#30697;&#38453;&#30340;&#31209;&#65292;&#24182;&#25193;&#23637;&#20102;&#26377;&#20851;Hadamard&#24130;&#31209;&#30340;&#32463;&#20856;&#32447;&#24615;&#20195;&#25968;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#23450;&#20855;&#26377;m&#20010;&#38544;&#34255;&#31070;&#32463;&#20803;&#21644;&#36755;&#20837;&#32500;&#25968;d&#65288;&#21363;md+m&#20010;&#35757;&#32451;&#21442;&#25968;&#65289;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#23384;&#20648;&#23481;&#37327;&#65292;&#21363;&#32593;&#32476;&#33021;&#22815;&#35760;&#24518;&#30340;&#19968;&#33324;&#25968;&#25454;&#30340;&#26368;&#22823;&#23610;&#23544;&#65292;&#26159;&#19968;&#20010;&#22522;&#26412;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#12290;&#23545;&#20110;&#38750;&#22810;&#39033;&#24335;&#23454;&#35299;&#26512;&#28608;&#27963;&#20989;&#25968;&#65292;&#22914;sigmoid&#21644;&#24179;&#28369;&#30340;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;&#24179;&#28369;ReLU&#65289;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;md/2&#30340;&#19979;&#30028;&#65292;&#24182;&#19988;&#20934;&#30830;&#24615;&#22823;&#32422;&#20026;2&#20493;&#12290;&#31867;&#20284;&#30340;&#20808;&#21069;&#32467;&#26524;&#20165;&#38480;&#20110;&#38454;&#36291;&#20989;&#25968;&#21644;ReLU&#28608;&#27963;&#20989;&#25968;&#65292;&#23545;&#20110;&#24179;&#28369;&#28608;&#27963;&#20989;&#25968;&#30340;&#32467;&#26524;&#21463;&#21040;&#23545;&#25968;&#22240;&#23376;&#21644;&#38543;&#26426;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#20998;&#26512;&#23384;&#20648;&#23481;&#37327;&#65292;&#25105;&#20204;&#36890;&#36807;&#35745;&#31639;&#28041;&#21450;Hadamard&#24130;&#21644;Khati-Rao&#31215;&#30340;&#30697;&#38453;&#30340;&#31209;&#26469;&#32771;&#23519;&#32593;&#32476;&#38597;&#21487;&#27604;&#30697;&#38453;&#30340;&#31209;&#12290;&#25105;&#20204;&#30340;&#35745;&#31639;&#25193;&#23637;&#20102;&#20851;&#20110;Hadamard&#24130;&#31209;&#30340;&#32463;&#20856;&#32447;&#24615;&#20195;&#25968;&#20107;&#23454;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20043;&#21069;&#20851;&#20110;&#23384;&#20648;&#23481;&#37327;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#24182;&#26377;&#24076;&#26395;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Determining the memory capacity of two-layer neural networks with m hidden neurons and input dimension d (i.e., md+m total trainable parameters), which refers to the largest size of general data the network can memorize, is a fundamental machine-learning question. For non-polynomial real analytic activation functions, such as sigmoids and smoothed rectified linear units (smoothed ReLUs), we establish a lower bound of md/2 and optimality up to a factor of approximately 2. Analogous prior results were limited to Heaviside and ReLU activations, with results for smooth activations suffering from logarithmic factors and requiring random data. To analyze the memory capacity, we examine the rank of the network's Jacobian by computing the rank of matrices involving both Hadamard powers and the Khati-Rao product. Our computation extends classical linear algebraic facts about the rank of Hadamard powers. Overall, our approach differs from previous works on memory capacity and holds promise for e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;-&#31526;&#21495;&#36807;&#28193;&#23383;&#20856;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#31070;&#32463;&#32593;&#32476;&#19982;&#31526;&#21495;&#24605;&#32500;&#36827;&#34892;&#32467;&#21512;&#12290;&#36890;&#36807;&#23398;&#20064;&#36807;&#28193;&#34920;&#31034;&#65292;&#24182;&#33258;&#30417;&#30563;&#22320;&#21457;&#29616;&#38544;&#21547;&#30340;&#35859;&#35789;&#32467;&#26500;&#65292;&#20197;&#21450;&#36890;&#36807;&#21338;&#24328;&#21644;&#24378;&#21270;&#23398;&#20064;&#35843;&#25972;&#23398;&#20064;&#21040;&#30340;&#21407;&#22411;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#23454;&#29616;&#23545;&#39640;&#32500;&#20449;&#24687;&#30340;&#21387;&#32553;&#21644;&#31526;&#21495;&#34920;&#31034;&#30340;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2308.02000</link><description>&lt;p&gt;
&#20174;&#31070;&#32463;&#34920;&#31034;&#21040;&#31526;&#21495;&#30693;&#35782;&#30340;&#36807;&#28193;
&lt;/p&gt;
&lt;p&gt;
On the Transition from Neural Representation to Symbolic Knowledge. (arXiv:2308.02000v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02000
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;-&#31526;&#21495;&#36807;&#28193;&#23383;&#20856;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#31070;&#32463;&#32593;&#32476;&#19982;&#31526;&#21495;&#24605;&#32500;&#36827;&#34892;&#32467;&#21512;&#12290;&#36890;&#36807;&#23398;&#20064;&#36807;&#28193;&#34920;&#31034;&#65292;&#24182;&#33258;&#30417;&#30563;&#22320;&#21457;&#29616;&#38544;&#21547;&#30340;&#35859;&#35789;&#32467;&#26500;&#65292;&#20197;&#21450;&#36890;&#36807;&#21338;&#24328;&#21644;&#24378;&#21270;&#23398;&#20064;&#35843;&#25972;&#23398;&#20064;&#21040;&#30340;&#21407;&#22411;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#23454;&#29616;&#23545;&#39640;&#32500;&#20449;&#24687;&#30340;&#21387;&#32553;&#21644;&#31526;&#21495;&#34920;&#31034;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24357;&#21512;&#31070;&#32463;&#34920;&#31034;&#19982;&#31526;&#21495;&#34920;&#31034;&#20043;&#38388;&#30340;&#24040;&#22823;&#24046;&#36317;&#21487;&#33021;&#20351;&#31526;&#21495;&#24605;&#32500;&#20174;&#26412;&#36136;&#19978;&#34701;&#20837;&#31070;&#32463;&#32593;&#32476;&#12290;&#21463;&#20154;&#31867;&#22914;&#20309;&#36880;&#28176;&#20174;&#36890;&#36807;&#30693;&#35273;&#21644;&#29615;&#22659;&#20132;&#20114;&#23398;&#20064;&#21040;&#30340;&#21407;&#22411;&#31526;&#21495;&#26500;&#24314;&#22797;&#26434;&#30340;&#31526;&#21495;&#34920;&#31034;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;-&#31526;&#21495;&#36807;&#28193;&#23383;&#20856;&#23398;&#20064;&#65288;TDL&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;EM&#31639;&#27861;&#23398;&#20064;&#25968;&#25454;&#30340;&#36807;&#28193;&#34920;&#31034;&#65292;&#23558;&#36755;&#20837;&#30340;&#39640;&#32500;&#35270;&#35273;&#37096;&#20998;&#20449;&#24687;&#21387;&#32553;&#21040;&#19968;&#32452;&#24352;&#37327;&#20316;&#20026;&#31070;&#32463;&#21464;&#37327;&#65292;&#24182;&#33258;&#30417;&#30563;&#22320;&#21457;&#29616;&#38544;&#21547;&#30340;&#35859;&#35789;&#32467;&#26500;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#36755;&#20837;&#20998;&#35299;&#35270;&#20026;&#21512;&#20316;&#21338;&#24328;&#26469;&#23454;&#29616;&#26694;&#26550;&#65292;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#35859;&#35789;&#65292;&#24182;&#36890;&#36807;RL&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#36827;&#19968;&#27493;&#35843;&#25972;&#23398;&#20064;&#21040;&#30340;&#21407;&#22411;&#65292;&#20197;&#34701;&#20837;&#20027;&#35266;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bridging the huge disparity between neural and symbolic representation can potentially enable the incorporation of symbolic thinking into neural networks from essence. Motivated by how human gradually builds complex symbolic representation from the prototype symbols that are learned through perception and environmental interactions. We propose a Neural-Symbolic Transitional Dictionary Learning (TDL) framework that employs an EM algorithm to learn a transitional representation of data that compresses high-dimension information of visual parts of an input into a set of tensors as neural variables and discover the implicit predicate structure in a self-supervised way. We implement the framework with a diffusion model by regarding the decomposition of input as a cooperative game, then learn predicates by prototype clustering. We additionally use RL enabled by the Markovian of diffusion models to further tune the learned prototypes by incorporating subjective factors. Extensive experiments 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#32593;&#32476;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#26080;&#30417;&#30563;&#22810;&#27169;&#24577;&#22270;&#20687;&#37197;&#20934;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;Grad-CAM-based&#30340;&#21487;&#35299;&#37322;&#24615;&#26694;&#26550;&#65292;&#33021;&#22815;&#22788;&#29702;&#20223;&#23556;&#21644;&#38750;&#21018;&#24615;&#37197;&#20934;&#65292;&#24182;&#22312;&#20020;&#24202;MRI&#35774;&#32622;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2308.01994</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#26080;&#30417;&#30563;&#22810;&#27169;&#24577;&#22270;&#20687;&#37197;&#20934;
&lt;/p&gt;
&lt;p&gt;
Explainable unsupervised multi-modal image registration using deep networks. (arXiv:2308.01994v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01994
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#32593;&#32476;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#26080;&#30417;&#30563;&#22810;&#27169;&#24577;&#22270;&#20687;&#37197;&#20934;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;Grad-CAM-based&#30340;&#21487;&#35299;&#37322;&#24615;&#26694;&#26550;&#65292;&#33021;&#22815;&#22788;&#29702;&#20223;&#23556;&#21644;&#38750;&#21018;&#24615;&#37197;&#20934;&#65292;&#24182;&#22312;&#20020;&#24202;MRI&#35774;&#32622;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30913;&#20849;&#25391;&#25104;&#20687;(MRI)&#30340;&#20020;&#24202;&#20915;&#31574;&#32467;&#21512;&#20102;&#22810;&#20010;MRI&#24207;&#21015;(&#23450;&#20041;&#20026;"&#27169;&#24577;")&#20013;&#30340;&#20114;&#34917;&#20449;&#24687;&#12290;MRI&#22270;&#20687;&#37197;&#20934;&#26088;&#22312;&#20960;&#20309;&#19978;"&#37197;&#23545;"&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#12289;&#26102;&#38388;&#28857;&#21644;&#20999;&#29255;&#30340;&#35786;&#26029;&#32467;&#26524;&#12290;&#22312;&#20020;&#24202;MRI&#35774;&#32622;&#20013;&#65292;&#26082;&#26377;&#20869;&#27169;&#24577;MRI&#37197;&#20934;&#65292;&#20063;&#26377;&#38388;&#27169;&#24577;MRI&#37197;&#20934;&#12290;&#27492;&#22806;&#65292;&#22312;&#33021;&#22815;&#22788;&#29702;&#20223;&#23556;&#21644;&#38750;&#21018;&#24615;&#37197;&#20934;&#30340;MRI&#22270;&#20687;&#22788;&#29702;&#27969;&#31243;&#20013;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#22312;&#30495;&#23454;&#30340;MRI&#25968;&#25454;&#24773;&#20917;&#19979;&#21487;&#33021;&#21457;&#29983;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#21464;&#24418;&#12290;&#19982;&#22270;&#20687;&#20998;&#31867;&#19981;&#21516;&#65292;&#22270;&#20687;&#37197;&#20934;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20013;&#24456;&#23569;&#28041;&#21450;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#65292;&#22240;&#20026;&#24456;&#38590;&#23545;&#27169;&#22411;-&#25968;&#25454;&#34892;&#20026;&#19982;&#21464;&#25442;&#22330;&#36827;&#34892;&#35299;&#37322;&#12290;&#20026;&#20102;&#27491;&#30830;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#26080;&#30417;&#30563;&#22810;&#27169;&#24577;&#21644;&#22810;&#22120;&#23448;&#22270;&#20687;&#37197;&#20934;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#27599;&#20010;&#20027;&#35201;&#32452;&#20214;&#20013;&#37117;&#21152;&#20837;&#20102;&#22522;&#20110;Grad-CAM&#30340;&#21487;&#35299;&#37322;&#24615;&#26694;&#26550;&#12290;&#25105;&#20204;&#20808;&#21069;&#35777;&#26126;&#20102;&#25105;&#20204;&#33021;&#22815;&#36798;&#21040;-
&lt;/p&gt;
&lt;p&gt;
Clinical decision making from magnetic resonance imaging (MRI) combines complementary information from multiple MRI sequences (defined as 'modalities'). MRI image registration aims to geometrically 'pair' diagnoses from different modalities, time points and slices. Both intra- and inter-modality MRI registration are essential components in clinical MRI settings. Further, an MRI image processing pipeline that can address both afine and non-rigid registration is critical, as both types of deformations may be occuring in real MRI data scenarios. Unlike image classification, explainability is not commonly addressed in image registration deep learning (DL) methods, as it is challenging to interpet model-data behaviours against transformation fields. To properly address this, we incorporate Grad-CAM-based explainability frameworks in each major component of our unsupervised multi-modal and multi-organ image registration DL methodology. We previously demonstrated that we were able to reach su
&lt;/p&gt;</description></item><item><title>CartiMorph&#26159;&#19968;&#31181;&#33258;&#21160;&#21270;&#33181;&#20851;&#33410;&#36719;&#39592;&#24418;&#24577;&#23398;&#27979;&#37327;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#22270;&#20687;&#20998;&#26512;&#65292;&#36890;&#36807;&#23450;&#37327;&#25351;&#26631;&#35780;&#20272;&#20102;&#36719;&#39592;&#30340;&#25439;&#22833;&#21644;&#21402;&#24230;&#65292;&#24182;&#19982;&#25163;&#21160;&#20998;&#21106;&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#34920;&#38754;&#27861;&#32447;&#30340;&#21402;&#24230;&#26144;&#23556;&#26041;&#27861;&#20855;&#26377;&#36739;&#23567;&#30340;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2308.01981</link><description>&lt;p&gt;
CartiMorph:&#19968;&#31181;&#33258;&#21160;&#21270;&#33181;&#20851;&#33410;&#36719;&#39592;&#24418;&#24577;&#23398;&#27979;&#37327;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CartiMorph: a framework for automated knee articular cartilage morphometrics. (arXiv:2308.01981v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01981
&lt;/p&gt;
&lt;p&gt;
CartiMorph&#26159;&#19968;&#31181;&#33258;&#21160;&#21270;&#33181;&#20851;&#33410;&#36719;&#39592;&#24418;&#24577;&#23398;&#27979;&#37327;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#22270;&#20687;&#20998;&#26512;&#65292;&#36890;&#36807;&#23450;&#37327;&#25351;&#26631;&#35780;&#20272;&#20102;&#36719;&#39592;&#30340;&#25439;&#22833;&#21644;&#21402;&#24230;&#65292;&#24182;&#19982;&#25163;&#21160;&#20998;&#21106;&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#34920;&#38754;&#27861;&#32447;&#30340;&#21402;&#24230;&#26144;&#23556;&#26041;&#27861;&#20855;&#26377;&#36739;&#23567;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;CartiMorph&#65292;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#21270;&#33181;&#20851;&#33410;&#36719;&#39592;&#24418;&#24577;&#23398;&#27979;&#37327;&#30340;&#26694;&#26550;&#12290;&#23427;&#20197;&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#29983;&#25104;&#36719;&#39592;&#20122;&#21306;&#22495;&#30340;&#23450;&#37327;&#25351;&#26631;&#65292;&#21253;&#25324;&#20840;&#21402;&#24230;&#36719;&#39592;&#20002;&#22833;&#65288;FCL&#65289;&#30340;&#30334;&#20998;&#27604;&#12289;&#24179;&#22343;&#21402;&#24230;&#12289;&#34920;&#38754;&#31215;&#21644;&#20307;&#31215;&#12290;CartiMorph&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20998;&#23618;&#22270;&#20687;&#29305;&#24449;&#34920;&#31034;&#12290;&#25105;&#20204;&#35757;&#32451;&#21644;&#39564;&#35777;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#32452;&#32455;&#20998;&#21106;&#12289;&#27169;&#26495;&#26500;&#24314;&#21644;&#27169;&#26495;&#21040;&#22270;&#20687;&#30340;&#27880;&#20876;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#22522;&#20110;&#34920;&#38754;&#27861;&#32447;&#30340;&#36719;&#39592;&#21402;&#24230;&#26144;&#23556;&#12289;FCL&#20272;&#35745;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#36719;&#39592;&#20998;&#21106;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#36719;&#39592;&#21402;&#24230;&#22270;&#22312;&#34180;&#21644;&#21608;&#36793;&#21306;&#22495;&#26174;&#31034;&#20986;&#36739;&#23567;&#30340;&#35823;&#24046;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#36890;&#36807;&#27169;&#22411;&#20998;&#21106;&#21644;&#25163;&#21160;&#20998;&#21106;&#33719;&#24471;&#30340;&#23450;&#37327;&#25351;&#26631;&#65292;&#35780;&#20272;&#20102;&#25152;&#37319;&#29992;&#30340;&#20998;&#21106;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;FCL&#27979;&#37327;&#30340;&#22343;&#26041;&#26681;&#20559;&#24046;&#23567;&#20110;8%&#65292;&#24182;&#19988;&#19982;&#25163;&#21160;&#20998;&#21106;&#30340;&#25351;&#26631;&#23384;&#22312;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce CartiMorph, a framework for automated knee articular cartilage morphometrics. It takes an image as input and generates quantitative metrics for cartilage subregions, including the percentage of full-thickness cartilage loss (FCL), mean thickness, surface area, and volume. CartiMorph leverages the power of deep learning models for hierarchical image feature representation. Deep learning models were trained and validated for tissue segmentation, template construction, and template-to-image registration. We established methods for surface-normal-based cartilage thickness mapping, FCL estimation, and rule-based cartilage parcellation. Our cartilage thickness map showed less error in thin and peripheral regions. We evaluated the effectiveness of the adopted segmentation model by comparing the quantitative metrics obtained from model segmentation and those from manual segmentation. The root-mean-squared deviation of the FCL measurements was less than 8%, and strong correlations 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22312;&#32447;&#24066;&#22330;&#20013;&#25340;&#20889;&#38169;&#35823;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;&#25340;&#20889;&#26816;&#26597;&#22120;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#22312;&#24494;&#36719;AppSource&#24066;&#22330;&#30340;&#23454;&#26102;&#25512;&#26029;API&#20013;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#34920;&#26126;&#65292;&#25511;&#21046;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#21487;&#33021;&#25104;&#20026;&#19968;&#20010;&#26377;&#21147;&#30340;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#32771;&#34385;&#21040;&#24403;&#21069;&#22823;&#35821;&#35328;&#27169;&#22411;&#25152;&#20381;&#36182;&#30340;&#24040;&#22823;&#19988;&#38590;&#20197;&#25511;&#21046;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2308.01976</link><description>&lt;p&gt;
&#25340;&#20889;&#26816;&#26597;&#22120;&#22312;&#22312;&#32447;&#24066;&#22330;&#20013;&#30340;&#39046;&#22495;&#29305;&#24322;&#24615;&#21644;&#25968;&#25454;&#25928;&#29575;&#65306;&#20197;&#22312;&#32447;&#24066;&#22330;&#25628;&#32034;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Domain specificity and data efficiency in typo tolerant spell checkers: the case of search in online marketplaces. (arXiv:2308.01976v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22312;&#32447;&#24066;&#22330;&#20013;&#25340;&#20889;&#38169;&#35823;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;&#25340;&#20889;&#26816;&#26597;&#22120;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#22312;&#24494;&#36719;AppSource&#24066;&#22330;&#30340;&#23454;&#26102;&#25512;&#26029;API&#20013;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#34920;&#26126;&#65292;&#25511;&#21046;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#21487;&#33021;&#25104;&#20026;&#19968;&#20010;&#26377;&#21147;&#30340;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#32771;&#34385;&#21040;&#24403;&#21069;&#22823;&#35821;&#35328;&#27169;&#22411;&#25152;&#20381;&#36182;&#30340;&#24040;&#22823;&#19988;&#38590;&#20197;&#25511;&#21046;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22312;&#32447;&#24066;&#22330;&#30340;&#39046;&#22495;&#29305;&#23450;&#24615;&#21644;&#29992;&#25143;&#30701;&#26597;&#35810;&#30340;&#29305;&#28857;&#65292;&#38169;&#23383;&#26159;&#22312;&#32447;&#24066;&#22330;&#35775;&#38382;&#32773;&#30340;&#20027;&#35201;&#22256;&#25200;&#12290;&#20256;&#32479;&#30340;&#25340;&#20889;&#26816;&#26597;&#35299;&#20915;&#26041;&#26696;&#22312;&#32416;&#27491;&#25340;&#20889;&#38169;&#35823;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#35299;&#20915;&#32570;&#20047;&#26631;&#27880;&#25340;&#20889;&#38169;&#35823;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20102;&#19978;&#19979;&#25991;&#38480;&#21046;&#30340;&#39046;&#22495;&#29305;&#23450;&#23884;&#20837;&#12290;&#36825;&#20123;&#23884;&#20837;&#34987;&#37096;&#32626;&#22312;&#24494;&#36719;AppSource&#24066;&#22330;&#30340;&#23454;&#26102;&#25512;&#26029;API&#20013;&#65292;&#20197;&#22312;&#38169;&#35823;&#25340;&#20889;&#30340;&#29992;&#25143;&#26597;&#35810;&#21644;&#21487;&#29992;&#20135;&#21697;&#21517;&#31216;&#20043;&#38388;&#25214;&#21040;&#26368;&#25509;&#36817;&#30340;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#34920;&#26126;&#65292;&#21463;&#21040;&#24403;&#21069;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#25511;&#21046;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#21487;&#33021;&#25104;&#20026;&#19968;&#20010;&#26377;&#21147;&#30340;&#24037;&#20855;&#65292;&#32780;&#36825;&#20123;&#27169;&#22411;&#20381;&#36182;&#20110;&#24040;&#22823;&#19988;&#38590;&#20197;&#25511;&#21046;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Typographical errors are a major source of frustration for visitors of online marketplaces. Because of the domain-specific nature of these marketplaces and the very short queries users tend to search for, traditional spell cheking solutions do not perform well in correcting typos. We present a data augmentation method to address the lack of annotated typo data and train a recurrent neural network to learn context-limited domain-specific embeddings. Those embeddings are deployed in a real-time inferencing API for the Microsoft AppSource marketplace to find the closest match between a misspelled user query and the available product names. Our data efficient solution shows that controlled high quality synthetic data may be a powerful tool especially considering the current climate of large language models which rely on prohibitively huge and often uncontrolled datasets.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#25193;&#24352;&#21367;&#31215;&#36716;&#25442;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;MULTIMEDIATE 2023&#31454;&#36187;&#20013;&#24314;&#27169;&#21644;&#20272;&#35745;&#22810;&#27169;&#24577;&#23545;&#35805;&#20013;&#30340;&#20154;&#31867;&#21442;&#19982;&#24230;&#12290;&#22312;&#27979;&#35797;&#38598;&#19978;&#35813;&#31995;&#32479;&#27604;&#22522;&#20934;&#27169;&#22411;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;7%&#25913;&#36827;&#65292;&#22312;&#39564;&#35777;&#38598;&#19978;&#34920;&#29616;&#20026;4%&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#31616;&#21333;&#30340;&#20018;&#32852;&#26041;&#27861;&#19982;&#33258;&#27880;&#24847;&#21147;&#34701;&#21512;&#21487;&#20197;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.01966</link><description>&lt;p&gt;
DCTM&#65306;&#25193;&#24352;&#21367;&#31215;&#36716;&#25442;&#27169;&#22411;&#29992;&#20110;&#22810;&#27169;&#24577;&#23545;&#35805;&#20013;&#30340;&#21442;&#19982;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
DCTM: Dilated Convolutional Transformer Model for Multimodal Engagement Estimation in Conversation. (arXiv:2308.01966v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01966
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#25193;&#24352;&#21367;&#31215;&#36716;&#25442;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;MULTIMEDIATE 2023&#31454;&#36187;&#20013;&#24314;&#27169;&#21644;&#20272;&#35745;&#22810;&#27169;&#24577;&#23545;&#35805;&#20013;&#30340;&#20154;&#31867;&#21442;&#19982;&#24230;&#12290;&#22312;&#27979;&#35797;&#38598;&#19978;&#35813;&#31995;&#32479;&#27604;&#22522;&#20934;&#27169;&#22411;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;7%&#25913;&#36827;&#65292;&#22312;&#39564;&#35777;&#38598;&#19978;&#34920;&#29616;&#20026;4%&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#31616;&#21333;&#30340;&#20018;&#32852;&#26041;&#27861;&#19982;&#33258;&#27880;&#24847;&#21147;&#34701;&#21512;&#21487;&#20197;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#21442;&#19982;&#24230;&#20272;&#35745;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#20010;&#22238;&#24402;&#38382;&#39064;&#65292;&#38656;&#35201;&#35782;&#21035;&#21442;&#19982;&#32773;&#22312;&#23545;&#35805;&#20013;&#30340;&#20851;&#27880;&#21644;&#21442;&#19982;&#31243;&#24230;&#12290;&#36825;&#19968;&#20219;&#21153;&#23545;&#20110;&#25581;&#31034;&#20154;&#31867;&#20132;&#20114;&#21160;&#21147;&#23398;&#21644;&#34892;&#20026;&#27169;&#24335;&#22312;&#23545;&#35805;&#20013;&#30340;&#27934;&#23519;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25193;&#24352;&#21367;&#31215;&#36716;&#25442;&#27169;&#22411;&#65292;&#29992;&#20110;&#24314;&#27169;&#21644;&#20272;&#35745;MULTIMEDIATE 2023&#31454;&#36187;&#20013;&#30340;&#20154;&#31867;&#21442;&#19982;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31995;&#32479;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#65292;&#22312;&#27979;&#35797;&#38598;&#19978;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;7%&#25913;&#36827;&#21644;&#39564;&#35777;&#38598;&#19978;&#30340;4%&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19981;&#21516;&#30340;&#27169;&#24577;&#34701;&#21512;&#26426;&#21046;&#65292;&#24182;&#23637;&#31034;&#23545;&#20110;&#36825;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#31616;&#21333;&#30340;&#20018;&#32852;&#26041;&#27861;&#19982;&#33258;&#27880;&#24847;&#21147;&#34701;&#21512;&#33719;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational engagement estimation is posed as a regression problem, entailing the identification of the favorable attention and involvement of the participants in the conversation. This task arises as a crucial pursuit to gain insights into human's interaction dynamics and behavior patterns within a conversation. In this research, we introduce a dilated convolutional Transformer for modeling and estimating human engagement in the MULTIMEDIATE 2023 competition. Our proposed system surpasses the baseline models, exhibiting a noteworthy $7$\% improvement on test set and $4$\% on validation set. Moreover, we employ different modality fusion mechanism and show that for this type of data, a simple concatenated method with self-attention fusion gains the best performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#35843;&#25972;&#25439;&#22833;&#26435;&#37325;&#26469;&#25913;&#21892;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#20803;&#22238;&#24402;&#20219;&#21153;&#20013;&#23398;&#20064;&#29123;&#28903;&#26597;&#25214;&#34920;&#30340;&#20934;&#30830;&#24615;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#25152;&#26377;&#29289;&#31181;&#36136;&#37327;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2308.01954</link><description>&lt;p&gt;
&#23558;&#21270;&#23398;&#24341;&#20837;&#35268;&#27169;&#65306;&#22312;&#28145;&#24230;&#23398;&#20064;&#28909;&#21270;&#23398;&#36807;&#31243;&#20013;&#23545;&#22810;&#20803;&#22238;&#24402;&#30340;&#25439;&#22833;&#26435;&#37325;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Bringing Chemistry to Scale: Loss Weight Adjustment for Multivariate Regression in Deep Learning of Thermochemical Processes. (arXiv:2308.01954v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#35843;&#25972;&#25439;&#22833;&#26435;&#37325;&#26469;&#25913;&#21892;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#20803;&#22238;&#24402;&#20219;&#21153;&#20013;&#23398;&#20064;&#29123;&#28903;&#26597;&#25214;&#34920;&#30340;&#20934;&#30830;&#24615;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#25152;&#26377;&#29289;&#31181;&#36136;&#37327;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28779;&#28976;&#27169;&#22411;&#24191;&#27867;&#24212;&#29992;&#20110;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#20013;&#65292;&#29992;&#20110;&#27169;&#25311;&#28237;&#27969;&#29123;&#28903;&#20013;&#30340;&#28909;&#21270;&#23398;&#36807;&#31243;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#20869;&#23384;&#23494;&#38598;&#22411;&#30340;&#26597;&#25214;&#34920;&#26469;&#34920;&#31034;&#35201;&#27169;&#25311;&#30340;&#29123;&#28903;&#36807;&#31243;&#12290;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#23569;&#37327;&#30340;&#32593;&#32476;&#26435;&#37325;&#23384;&#20648;&#36825;&#20123;&#34920;&#26684;&#25968;&#25454;&#65292;&#20174;&#32780;&#21487;&#33021;&#23558;&#22797;&#26434;&#27169;&#25311;&#30340;&#20869;&#23384;&#38656;&#27714;&#20943;&#23569;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#26631;&#20934;&#35757;&#32451;&#25439;&#22833;&#30340;ANN&#22312;&#22810;&#20803;&#22238;&#24402;&#20219;&#21153;&#20013;&#24448;&#24448;&#38590;&#20197;&#22788;&#29702;&#20195;&#34920;&#36739;&#23567;&#29289;&#31181;&#36136;&#37327;&#20998;&#25968;&#30340;&#30446;&#26631;&#20540;&#19981;&#36275;&#30340;&#24773;&#20917;&#65292;&#20363;&#22914;&#22312;&#26597;&#25214;&#34920;&#20013;&#23398;&#20064;&#23569;&#25968;&#29289;&#31181;&#36136;&#37327;&#20998;&#25968;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#39640;ANN&#22312;&#23398;&#20064;&#27682;&#65288;\ce{H2}&#65289;&#29123;&#28903;&#26597;&#25214;&#34920;&#30340;&#22810;&#31181;&#29289;&#31181;&#36136;&#37327;&#20998;&#25968;&#26102;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25439;&#22833;&#26435;&#37325;&#35843;&#25972;&#26041;&#27861;&#65292;&#20248;&#20110;&#26631;&#20934;&#30340;&#22343;&#26041;&#35823;&#24046;&#20248;&#21270;&#65292;&#24182;&#23454;&#29616;&#20102;&#25152;&#26377;&#29289;&#31181;&#36136;&#37327;&#20998;&#25968;&#30340;&#20934;&#30830;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Flamelet models are widely used in computational fluid dynamics to simulate thermochemical processes in turbulent combustion. These models typically employ memory-expensive lookup tables that are predetermined and represent the combustion process to be simulated. Artificial neural networks (ANNs) offer a deep learning approach that can store this tabular data using a small number of network weights, potentially reducing the memory demands of complex simulations by orders of magnitude. However, ANNs with standard training losses often struggle with underrepresented targets in multivariate regression tasks, e.g., when learning minor species mass fractions as part of lookup tables. This paper seeks to improve the accuracy of an ANN when learning multiple species mass fractions of a hydrogen (\ce{H2}) combustion lookup table. We assess a simple, yet effective loss weight adjustment that outperforms the standard mean-squared error optimization and enables accurate learning of all species ma
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#23398;&#29983;-&#25945;&#24072;&#27169;&#22411;&#30340;&#21028;&#21035;&#24615;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#24322;&#24120;&#22270;&#20449;&#24687;&#21644;&#37319;&#29992;&#33410;&#28857;&#32423;&#21644;&#22270;&#32423;&#20449;&#24687;&#24046;&#26469;&#35782;&#21035;&#24322;&#24120;&#22270;&#65292;&#24182;&#24341;&#20837;&#25945;&#24072;&#27169;&#22411;&#21644;&#20004;&#20010;&#31454;&#20105;&#30340;&#23398;&#29983;&#27169;&#22411;&#26469;&#25552;&#39640;&#24322;&#24120;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.01947</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#23398;&#29983;-&#25945;&#24072;&#27169;&#22411;&#30340;&#21028;&#21035;&#24615;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Discriminative Graph-level Anomaly Detection via Dual-students-teacher Model. (arXiv:2308.01947v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#23398;&#29983;-&#25945;&#24072;&#27169;&#22411;&#30340;&#21028;&#21035;&#24615;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#24322;&#24120;&#22270;&#20449;&#24687;&#21644;&#37319;&#29992;&#33410;&#28857;&#32423;&#21644;&#22270;&#32423;&#20449;&#24687;&#24046;&#26469;&#35782;&#21035;&#24322;&#24120;&#22270;&#65292;&#24182;&#24341;&#20837;&#25945;&#24072;&#27169;&#22411;&#21644;&#20004;&#20010;&#31454;&#20105;&#30340;&#23398;&#29983;&#27169;&#22411;&#26469;&#25552;&#39640;&#24322;&#24120;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#20110;&#24403;&#21069;&#30340;&#33410;&#28857;&#32423;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#65292;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#30446;&#26631;&#26159;&#23547;&#25214;&#22312;&#22270;&#38598;&#20013;&#19982;&#20854;&#20182;&#22270;&#26174;&#33879;&#19981;&#21516;&#30340;&#24322;&#24120;&#22270;&#12290;&#30001;&#20110;&#23545;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#30740;&#31350;&#36739;&#23569;&#65292;&#20851;&#20110;&#22270;&#32423;&#24322;&#24120;&#30340;&#35814;&#32454;&#25551;&#36848;&#19981;&#36275;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#24037;&#20316;&#38598;&#20013;&#20110;&#25429;&#25417;&#24322;&#24120;&#22270;&#20449;&#24687;&#20197;&#23398;&#20064;&#26356;&#22909;&#30340;&#22270;&#34920;&#31034;&#65292;&#20294;&#24573;&#35270;&#20102;&#23545;&#35780;&#20272;&#24322;&#24120;&#22270;&#30340;&#26377;&#25928;&#24322;&#24120;&#24471;&#20998;&#20989;&#25968;&#30340;&#37325;&#35201;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#20102;&#22312;&#22270;&#38598;&#20013;&#21253;&#25324;&#33410;&#28857;&#21644;&#22270;&#23646;&#24615;&#24322;&#24120;&#30340;&#24322;&#24120;&#22270;&#20449;&#24687;&#65292;&#24182;&#20998;&#21035;&#37319;&#29992;&#33410;&#28857;&#32423;&#21644;&#22270;&#32423;&#20449;&#24687;&#24046;&#26469;&#35782;&#21035;&#23427;&#20204;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20855;&#26377;&#21452;&#23398;&#29983;-&#25945;&#24072;&#27169;&#22411;&#30340;&#21028;&#21035;&#24615;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#20854;&#20013;&#25945;&#24072;&#27169;&#22411;&#36890;&#36807;&#19968;&#31181;&#21551;&#21457;&#24335;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#24471;&#22270;&#34920;&#31034;&#26356;&#20026;&#20998;&#25955;&#12290;&#28982;&#21518;&#65292;&#20004;&#20010;&#31454;&#20105;&#30340;&#23398;&#29983;&#27169;&#22411;&#36890;&#36807;&#20105;&#22842;&#20219;&#21153;&#26469;&#25552;&#39640;&#24322;&#24120;&#26816;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Different from the current node-level anomaly detection task, the goal of graph-level anomaly detection is to find abnormal graphs that significantly differ from others in a graph set. Due to the scarcity of research on the work of graph-level anomaly detection, the detailed description of graph-level anomaly is insufficient. Furthermore, existing works focus on capturing anomalous graph information to learn better graph representations, but they ignore the importance of an effective anomaly score function for evaluating abnormal graphs. Thus, in this work, we first define anomalous graph information including node and graph property anomalies in a graph set and adopt node-level and graph-level information differences to identify them, respectively. Then, we introduce a discriminative graph-level anomaly detection framework with dual-students-teacher model, where the teacher model with a heuristic loss are trained to make graph representations more divergent. Then, two competing studen
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#20102;&#22312;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#24212;&#29992;&#22235;&#20803;&#25968;&#30340;&#25928;&#26524;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#22235;&#20803;&#25968;&#26469;&#34920;&#31034;&#21644;&#20998;&#31867;&#26059;&#36716;&#25968;&#25454;&#65292;&#24182;&#32467;&#21512;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.01946</link><description>&lt;p&gt;
&#36890;&#36807;&#22235;&#20803;&#25968;&#36827;&#34892;&#22810;&#26426;&#22120;&#23398;&#20064;&#30340;&#23454;&#39564;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Experimental Results regarding multiple Machine Learning via Quaternions. (arXiv:2308.01946v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#20102;&#22312;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#24212;&#29992;&#22235;&#20803;&#25968;&#30340;&#25928;&#26524;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#22235;&#20803;&#25968;&#26469;&#34920;&#31034;&#21644;&#20998;&#31867;&#26059;&#36716;&#25968;&#25454;&#65292;&#24182;&#32467;&#21512;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20851;&#20110;&#22312;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#24212;&#29992;&#22235;&#20803;&#25968;&#30340;&#23454;&#39564;&#30740;&#31350;&#12290;&#22235;&#20803;&#25968;&#26159;&#19977;&#32500;&#31354;&#38388;&#20013;&#26059;&#36716;&#30340;&#25968;&#23398;&#34920;&#31034;&#65292;&#21487;&#20197;&#29992;&#20110;&#34920;&#31034;&#22797;&#26434;&#30340;&#25968;&#25454;&#36716;&#25442;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#22235;&#20803;&#25968;&#26469;&#34920;&#31034;&#21644;&#20998;&#31867;&#26059;&#36716;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#38543;&#26426;&#29983;&#25104;&#30340;&#22235;&#20803;&#25968;&#25968;&#25454;&#21644;&#30456;&#24212;&#30340;&#26631;&#31614;&#65292;&#23558;&#22235;&#20803;&#25968;&#36716;&#25442;&#20026;&#26059;&#36716;&#30697;&#38453;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#36755;&#20837;&#29305;&#24449;&#12290;&#22522;&#20110;&#22235;&#20803;&#25968;&#21644;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#22312;&#39044;&#27979;&#20219;&#21153;&#20013;&#20934;&#30830;&#29575;&#26356;&#39640;&#19988;&#24615;&#33021;&#26174;&#33879;&#25552;&#21319;&#12290;&#24635;&#20307;&#19978;&#65292;&#36825;&#39033;&#30740;&#31350;&#20026;&#21033;&#29992;&#22235;&#20803;&#25968;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20102;&#23454;&#35777;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an experimental study on the application of quaternions in several machine learning algorithms. Quaternion is a mathematical representation of rotation in three-dimensional space, which can be used to represent complex data transformations. In this study, we explore the use of quaternions to represent and classify rotation data, using randomly generated quaternion data and corresponding labels, converting quaternions to rotation matrices, and using them as input features. Based on quaternions and multiple machine learning algorithms, it has shown higher accuracy and significantly improved performance in prediction tasks. Overall, this study provides an empirical basis for exploiting quaternions for machine learning tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#22312;&#32447;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#20998;&#21035;&#22522;&#20110;&#36882;&#24402;&#26368;&#23567;&#20108;&#20056;&#21644;&#36882;&#24402;&#26680;&#26041;&#27861;&#12290;&#19982;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#25110;&#19981;&#31934;&#30830;&#36924;&#36817;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27599;&#20010;&#23454;&#20363;&#30340;&#20195;&#20215;&#19978;&#20855;&#26377;&#20108;&#27425;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#39118;&#21147;&#30701;&#26399;&#39044;&#27979;&#25361;&#25112;&#65292;&#24182;&#19982;&#20854;&#20182;&#31454;&#20105;&#32773;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2308.01938</link><description>&lt;p&gt;
&#22312;&#32447;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#22522;&#20110;&#36882;&#24402;&#26368;&#23567;&#20108;&#20056;&#21644;&#36882;&#24402;&#26680;&#26041;&#27861;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Online Multi-Task Learning with Recursive Least Squares and Recursive Kernel Methods. (arXiv:2308.01938v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#22312;&#32447;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#20998;&#21035;&#22522;&#20110;&#36882;&#24402;&#26368;&#23567;&#20108;&#20056;&#21644;&#36882;&#24402;&#26680;&#26041;&#27861;&#12290;&#19982;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#25110;&#19981;&#31934;&#30830;&#36924;&#36817;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27599;&#20010;&#23454;&#20363;&#30340;&#20195;&#20215;&#19978;&#20855;&#26377;&#20108;&#27425;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#39118;&#21147;&#30701;&#26399;&#39044;&#27979;&#25361;&#25112;&#65292;&#24182;&#19982;&#20854;&#20182;&#31454;&#20105;&#32773;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#22238;&#24402;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#39640;&#24615;&#33021;&#22522;&#20110;&#22270;&#30340;MTL&#20844;&#24335;&#65292;&#22522;&#20110;&#21152;&#26435;&#36882;&#24402;&#26368;&#23567;&#20108;&#20056;&#65288;WRLS&#65289;&#21644;&#22312;&#32447;&#31232;&#30095;&#26368;&#23567;&#20108;&#20056;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#65288;OSLSSVR&#65289;&#24320;&#21457;&#20854;&#36882;&#24402;&#29256;&#26412;&#12290;&#37319;&#29992;&#20219;&#21153;&#22534;&#21472;&#36716;&#25442;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23384;&#22312;&#19968;&#20010;&#21333;&#30697;&#38453;&#65292;&#23427;&#34701;&#21512;&#20102;&#22810;&#20219;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20026;MT-WRLS&#26041;&#27861;&#30340;&#21021;&#22987;&#21270;&#36807;&#31243;&#21644;MT-OSLSSVR&#30340;&#22810;&#20219;&#21153;&#26680;&#20989;&#25968;&#25552;&#20379;&#32467;&#26500;&#20449;&#24687;&#12290;&#19982;&#29616;&#26377;&#22823;&#37096;&#20998;&#22522;&#20110;&#22312;&#32447;&#26799;&#24230;&#19979;&#38477;&#65288;OGD&#65289;&#25110;&#19981;&#31934;&#30830;&#31435;&#26041;&#36924;&#36817;&#26041;&#27861;&#30340;&#25991;&#29486;&#30456;&#27604;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#31934;&#30830;&#21644;&#36817;&#20284;&#36882;&#24402;&#65292;&#20854;&#27599;&#20010;&#23454;&#20363;&#30340;&#20195;&#20215;&#22312;&#36755;&#20837;&#31354;&#38388;&#30340;&#32500;&#24230;&#65288;MT-WRLS&#65289;&#25110;&#23454;&#20363;&#23383;&#20856;&#30340;&#22823;&#23567;&#19978;&#26159;&#20108;&#27425;&#30340;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#22312;&#32447;MTL&#26041;&#27861;&#19982;&#20854;&#20182;&#31454;&#20105;&#32773;&#22312;&#23454;&#38469;&#39118;&#30701;&#26399;&#39044;&#27979;&#25361;&#25112;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces two novel approaches for Online Multi-Task Learning (MTL) Regression Problems. We employ a high performance graph-based MTL formulation and develop its recursive versions based on the Weighted Recursive Least Squares (WRLS) and the Online Sparse Least Squares Support Vector Regression (OSLSSVR). Adopting task-stacking transformations, we demonstrate the existence of a single matrix incorporating the relationship of multiple tasks and providing structural information to be embodied by the MT-WRLS method in its initialization procedure and by the MT-OSLSSVR in its multi-task kernel function. Contrasting the existing literature, which is mostly based on Online Gradient Descent (OGD) or cubic inexact approaches, we achieve exact and approximate recursions with quadratic per-instance cost on the dimension of the input space (MT-WRLS) or on the size of the dictionary of instances (MT-OSLSSVR). We compare our online MTL methods to other contenders in a real-world wind sp
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20998;&#21306;&#25193;&#25955;&#27169;&#22411;&#65288;CDM&#65289;&#35757;&#32451;&#19981;&#21516;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#22312;&#25512;&#26029;&#26102;&#20219;&#24847;&#32452;&#21512;&#23427;&#20204;&#65292;&#23454;&#29616;&#20102;&#35757;&#32451;&#25968;&#25454;&#20445;&#25252;&#21644;&#36873;&#25321;&#24615;&#36951;&#24536;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#35775;&#38382;&#26435;&#38480;&#25552;&#20379;&#23450;&#21046;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.01937</link><description>&lt;p&gt;
&#20351;&#29992;&#32452;&#21512;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#35757;&#32451;&#25968;&#25454;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Training Data Protection with Compositional Diffusion Models. (arXiv:2308.01937v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01937
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20998;&#21306;&#25193;&#25955;&#27169;&#22411;&#65288;CDM&#65289;&#35757;&#32451;&#19981;&#21516;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#22312;&#25512;&#26029;&#26102;&#20219;&#24847;&#32452;&#21512;&#23427;&#20204;&#65292;&#23454;&#29616;&#20102;&#35757;&#32451;&#25968;&#25454;&#20445;&#25252;&#21644;&#36873;&#25321;&#24615;&#36951;&#24536;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#35775;&#38382;&#26435;&#38480;&#25552;&#20379;&#23450;&#21046;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#21306;&#25193;&#25955;&#27169;&#22411;&#65288;CDM&#65289;&#65292;&#19968;&#31181;&#22312;&#19981;&#21516;&#25968;&#25454;&#28304;&#19978;&#35757;&#32451;&#19981;&#21516;&#25193;&#25955;&#27169;&#22411;&#65288;&#25110;&#25552;&#31034;&#65289;&#24182;&#22312;&#25512;&#26029;&#26102;&#20219;&#24847;&#32452;&#21512;&#23427;&#20204;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#21333;&#29420;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#23396;&#31435;&#29366;&#24577;&#19979;&#12289;&#22312;&#19981;&#21516;&#26102;&#38388;&#12289;&#22312;&#19981;&#21516;&#20998;&#24067;&#21644;&#39046;&#22495;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#21487;&#20197;&#21518;&#32493;&#32452;&#21512;&#20197;&#36798;&#21040;&#19982;&#21516;&#26102;&#35757;&#32451;&#25152;&#26377;&#25968;&#25454;&#30340;&#29702;&#24819;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#27599;&#20010;&#27169;&#22411;&#21482;&#21253;&#21547;&#20854;&#22312;&#35757;&#32451;&#26399;&#38388;&#25509;&#35302;&#21040;&#30340;&#25968;&#25454;&#23376;&#38598;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#31181;&#24418;&#24335;&#30340;&#35757;&#32451;&#25968;&#25454;&#20445;&#25252;&#12290;&#29305;&#21035;&#26159;&#65292;CDM&#26159;&#31532;&#19968;&#31181;&#21487;&#20197;&#23454;&#29616;&#22823;&#35268;&#27169;&#25193;&#25955;&#27169;&#22411;&#30340;&#36873;&#25321;&#24615;&#36951;&#24536;&#21644;&#25345;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#20801;&#35768;&#26681;&#25454;&#29992;&#25143;&#35775;&#38382;&#26435;&#38480;&#25552;&#20379;&#23450;&#21046;&#27169;&#22411;&#12290;CDM&#36824;&#21487;&#20197;&#30830;&#23450;&#29983;&#25104;&#29305;&#23450;&#26679;&#26412;&#30340;&#25968;&#25454;&#23376;&#38598;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Compartmentalized Diffusion Models (CDM), a method to train different diffusion models (or prompts) on distinct data sources and arbitrarily compose them at inference time. The individual models can be trained in isolation, at different times, and on different distributions and domains and can be later composed to achieve performance comparable to a paragon model trained on all data simultaneously. Furthermore, each model only contains information about the subset of the data it was exposed to during training, enabling several forms of training data protection. In particular, CDMs are the first method to enable both selective forgetting and continual learning for large-scale diffusion models, as well as allowing serving customized models based on the user's access rights. CDMs also allow determining the importance of a subset of the data in generating particular samples.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#34987;&#29992;&#26469;&#20272;&#35745;&#36229;&#23548;&#20307;&#30340;&#20020;&#30028;&#28201;&#24230;&#65292;&#22635;&#34917;&#20102;&#20851;&#20110;&#36229;&#23548;&#24615;&#21644;&#20020;&#30028;&#28201;&#24230;&#20272;&#35745;&#30340;&#30693;&#35782;&#32570;&#21475;&#12290;</title><link>http://arxiv.org/abs/2308.01932</link><description>&lt;p&gt;
&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20272;&#35745;&#36229;&#23548;&#20307;&#20020;&#30028;&#28201;&#24230;&#26041;&#27861;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigation on Machine Learning Based Approaches for Estimating the Critical Temperature of Superconductors. (arXiv:2308.01932v1 [cond-mat.supr-con])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01932
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#34987;&#29992;&#26469;&#20272;&#35745;&#36229;&#23548;&#20307;&#30340;&#20020;&#30028;&#28201;&#24230;&#65292;&#22635;&#34917;&#20102;&#20851;&#20110;&#36229;&#23548;&#24615;&#21644;&#20020;&#30028;&#28201;&#24230;&#20272;&#35745;&#30340;&#30693;&#35782;&#32570;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#23548;&#20307;&#19968;&#30452;&#26159;&#26368;&#21560;&#24341;&#20154;&#30340;&#29289;&#36136;&#20043;&#19968;&#65292;&#33258;&#20854;&#21457;&#29616;&#20197;&#26469;&#65292;&#36229;&#23548;&#24615;&#30340;&#22522;&#26412;&#27010;&#24565;&#21450;&#20020;&#30028;&#28201;&#24230;&#19982;&#36229;&#23548;&#26448;&#26009;&#30340;&#20851;&#31995;&#19968;&#30452;&#26159;&#24191;&#27867;&#30740;&#31350;&#30340;&#37325;&#28857;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#35782;&#21035;&#24120;&#28201;&#19979;&#30340;&#36229;&#23548;&#20307;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;&#27492;&#22806;&#65292;&#20851;&#20110;&#36825;&#19968;&#29420;&#29305;&#29616;&#35937;&#30340;&#24456;&#22810;&#22240;&#32032;&#21644;&#29702;&#35299;&#19978;&#30340;&#24046;&#36317;&#20173;&#28982;&#23384;&#22312;&#65292;&#29305;&#21035;&#26159;&#36229;&#23548;&#24615;&#19982;&#20272;&#35745;&#20020;&#30028;&#28201;&#24230;&#30340;&#22522;&#26412;&#20934;&#21017;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#24050;&#32463;&#24314;&#31435;&#20102;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#20272;&#35745;&#20020;&#30028;&#28201;&#24230;&#65292;&#22240;&#20026;&#30830;&#23450;&#23427;&#26159;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#12290;&#27492;&#22806;&#65292;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24378;&#35843;&#20102;&#38656;&#35201;&#19968;&#31181;&#22797;&#26434;&#19988;&#21487;&#34892;&#30340;&#26041;&#27861;&#26469;&#30830;&#23450;&#36229;&#20986;&#26631;&#20934;&#32463;&#39564;&#20844;&#24335;&#33539;&#22260;&#30340;&#28201;&#24230;&#33539;&#22260;&#12290;&#26412;&#25991;&#20351;&#29992;&#22534;&#21472;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Superconductors have been among the most fascinating substances, as the fundamental concept of superconductivity as well as the correlation of critical temperature and superconductive materials have been the focus of extensive investigation since their discovery. However, superconductors at normal temperatures have yet to be identified. Additionally, there are still many unknown factors and gaps of understanding regarding this unique phenomenon, particularly the connection between superconductivity and the fundamental criteria to estimate the critical temperature. To bridge the gap, numerous machine learning techniques have been established to estimate critical temperatures as it is extremely challenging to determine. Furthermore, the need for a sophisticated and feasible method for determining the temperature range that goes beyond the scope of the standard empirical formula appears to be strongly emphasized by various machine-learning approaches. This paper uses a stacking machine le
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#20809;&#30005;&#33033;&#25615;&#20449;&#21495;&#29305;&#24449;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#31958;&#23615;&#30149;&#26816;&#27979;&#65292;&#36890;&#36807;&#38750;&#20405;&#20837;&#24615;PPG&#20449;&#21495;&#21644;LR&#12289;XGBoost&#31639;&#27861;&#30340;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#20813;&#21019;&#20260;&#19988;&#36830;&#32493;&#30417;&#27979;&#30340;&#31958;&#23615;&#30149;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.01930</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20809;&#30005;&#33033;&#25615;&#20449;&#21495;&#29305;&#24449;&#30340;&#31958;&#23615;&#30149;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Machine Learning-Based Diabetes Detection Using Photoplethysmography Signal Features. (arXiv:2308.01930v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01930
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20809;&#30005;&#33033;&#25615;&#20449;&#21495;&#29305;&#24449;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#31958;&#23615;&#30149;&#26816;&#27979;&#65292;&#36890;&#36807;&#38750;&#20405;&#20837;&#24615;PPG&#20449;&#21495;&#21644;LR&#12289;XGBoost&#31639;&#27861;&#30340;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#20813;&#21019;&#20260;&#19988;&#36830;&#32493;&#30417;&#27979;&#30340;&#31958;&#23615;&#30149;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31958;&#23615;&#30149;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#24930;&#24615;&#30142;&#30149;&#65292;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#24433;&#21709;&#30528;&#25968;&#30334;&#19975;&#20154;&#30340;&#20581;&#24247;&#12290;&#38656;&#35201;&#26080;&#21019;&#26041;&#27861;&#26469;&#39044;&#38450;&#21644;&#25511;&#21046;&#31958;&#23615;&#30149;&#65292;&#20294;&#22823;&#22810;&#25968;&#29992;&#20110;&#27979;&#37327;&#34880;&#31958;&#27700;&#24179;&#30340;&#35774;&#22791;&#26159;&#26377;&#21019;&#30340;&#65292;&#19981;&#36866;&#21512;&#36830;&#32493;&#30417;&#27979;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#20405;&#20837;&#24615;&#20809;&#23398;&#20809;&#30005;&#33033;&#25615;&#22270;&#65288;PPG&#65289;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#31958;&#23615;&#30149;&#65292;&#20197;&#20811;&#26381;&#36825;&#20123;&#32570;&#28857;&#12290;&#25105;&#20204;&#20351;&#29992;PPG&#20449;&#21495;&#21644;&#20803;&#25968;&#25454;&#23545;&#38750;&#31958;&#23615;&#30149;&#21644;&#31958;&#23615;&#30149;&#24739;&#32773;&#36827;&#34892;&#20998;&#31867;&#65292;&#29992;&#20110;&#35757;&#32451;&#36923;&#36753;&#22238;&#24402;&#65288;LR&#65289;&#21644;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319;&#65288;XGBoost&#65289;&#31639;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#20013;&#30340;PPG&#20449;&#21495;&#12290;&#20026;&#20102;&#38450;&#27490;&#36807;&#25311;&#21512;&#65292;&#25105;&#20204;&#23558;&#25968;&#25454;&#20998;&#25104;&#20116;&#20010;&#37096;&#20998;&#36827;&#34892;&#20132;&#21449;&#39564;&#35777;&#12290;&#36890;&#36807;&#30830;&#20445;&#35757;&#32451;&#38598;&#20013;&#30340;&#24739;&#32773;&#19981;&#22312;&#27979;&#35797;&#38598;&#20013;&#65292;&#21487;&#20197;&#22312;&#26410;&#35265;&#36807;&#30340;&#21463;&#35797;&#32773;&#30340;&#25968;&#25454;&#19978;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#35780;&#20272;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;F1-Score&#21644;AUC&#19978;&#20998;&#21035;&#36798;&#21040;&#20102;$58.8\pm20.0\%$&#21644;$7
&lt;/p&gt;
&lt;p&gt;
Diabetes is a prevalent chronic condition that compromises the health of millions of people worldwide. Minimally invasive methods are needed to prevent and control diabetes but most devices for measuring glucose levels are invasive and not amenable for continuous monitoring. Here, we present an alternative method to overcome these shortcomings based on non-invasive optical photoplethysmography (PPG) for detecting diabetes. We classify non-Diabetic and Diabetic patients using the PPG signal and metadata for training Logistic Regression (LR) and eXtreme Gradient Boosting (XGBoost) algorithms. We used PPG signals from a publicly available dataset. To prevent overfitting, we divided the data into five folds for cross-validation. By ensuring that patients in the training set are not in the testing set, the model's performance can be evaluated on unseen subjects' data, providing a more accurate assessment of its generalization. Our model achieved an F1-Score and AUC of $58.8\pm20.0\%$ and $7
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#19993;&#27850;&#37210;&#21644;&#29790;&#33452;&#22826;&#23612;&#30340;&#40635;&#37257;&#28145;&#24230;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#38271;&#30701;&#26102;&#35760;&#24518;&#21644;&#38376;&#25511;&#27531;&#24046;&#32593;&#32476;&#26469;&#25552;&#39640;&#29305;&#24449;&#34701;&#21512;&#30340;&#25928;&#29575;&#65292;&#24182;&#24212;&#29992;&#27880;&#24847;&#26426;&#21046;&#26469;&#21457;&#29616;&#33647;&#29289;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;PK-PD&#27169;&#22411;&#21644;&#20808;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#39044;&#27979;&#40635;&#37257;&#28145;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.01929</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#39044;&#27979;&#38774;&#25511;&#36755;&#27880;&#19993;&#27850;&#37210;&#21644;&#29790;&#33452;&#22826;&#23612;&#40635;&#37257;&#28145;&#24230;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Transformer-based Prediction Method for Depth of Anesthesia During Target-controlled Infusion of Propofol and Remifentanil. (arXiv:2308.01929v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01929
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#19993;&#27850;&#37210;&#21644;&#29790;&#33452;&#22826;&#23612;&#30340;&#40635;&#37257;&#28145;&#24230;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#38271;&#30701;&#26102;&#35760;&#24518;&#21644;&#38376;&#25511;&#27531;&#24046;&#32593;&#32476;&#26469;&#25552;&#39640;&#29305;&#24449;&#34701;&#21512;&#30340;&#25928;&#29575;&#65292;&#24182;&#24212;&#29992;&#27880;&#24847;&#26426;&#21046;&#26469;&#21457;&#29616;&#33647;&#29289;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;PK-PD&#27169;&#22411;&#21644;&#20808;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#39044;&#27979;&#40635;&#37257;&#28145;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#40635;&#37257;&#25928;&#26524;&#23545;&#20110;&#38774;&#25511;&#36755;&#27880;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#33647;&#29289;&#21160;&#21147;&#23398;-&#33647;&#25928;&#23398;&#27169;&#22411;&#38656;&#35201;&#25163;&#21160;&#36873;&#25321;&#27169;&#22411;&#21442;&#25968;&#65292;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21482;&#33021;&#25429;&#25417;&#21040;&#19968;&#33324;&#36235;&#21183;&#65292;&#21487;&#33021;&#26080;&#27861;&#39044;&#27979;BIS&#30340;&#31361;&#21464;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#19993;&#27850;&#37210;&#21644;&#29790;&#33452;&#22826;&#23612;&#30340;&#33647;&#29289;&#36755;&#27880;&#26469;&#39044;&#27979;&#40635;&#37257;&#28145;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;&#38376;&#25511;&#27531;&#24046;&#32593;&#32476;&#65288;GRN&#65289;&#26469;&#25552;&#39640;&#29305;&#24449;&#34701;&#21512;&#30340;&#25928;&#29575;&#65292;&#24182;&#24212;&#29992;&#20102;&#27880;&#24847;&#26426;&#21046;&#26469;&#21457;&#29616;&#33647;&#29289;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#26631;&#31614;&#20998;&#24067;&#24179;&#28369;&#21644;&#37325;&#26032;&#21152;&#26435;&#25439;&#22833;&#26469;&#22788;&#29702;&#25968;&#25454;&#19981;&#24179;&#34913;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;PK-PD&#27169;&#22411;&#21644;&#20808;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#39044;&#27979;&#40635;&#37257;&#28145;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately predicting anesthetic effects is essential for target-controlled infusion systems. The traditional (PK-PD) models for Bispectral index (BIS) prediction require manual selection of model parameters, which can be challenging in clinical settings. Recently proposed deep learning methods can only capture general trends and may not predict abrupt changes in BIS. To address these issues, we propose a transformer-based method for predicting the depth of anesthesia (DOA) using drug infusions of propofol and remifentanil. Our method employs long short-term memory (LSTM) and gate residual network (GRN) networks to improve the efficiency of feature fusion and applies an attention mechanism to discover the interactions between the drugs. We also use label distribution smoothing and reweighting losses to address data imbalance. Experimental results show that our proposed method outperforms traditional PK-PD models and previous deep learning methods, effectively predicting anesthetic dept
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21508;&#31181;&#21697;&#29260;&#30340;K-Means&#31639;&#27861;&#23545;&#20114;&#30456;&#20998;&#31163;&#30340;&#32858;&#31867;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#24615;&#33021;&#26356;&#20248;&#30340;K-Means++&#21464;&#20307;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.01926</link><description>&lt;p&gt;
&#25968;&#25454;&#26159;&#21542;&#26131;&#20110;K-Means&#31639;&#27861; (arXiv:2308.01926v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Are Easy Data Easy (for K-Means). (arXiv:2308.01926v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01926
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21508;&#31181;&#21697;&#29260;&#30340;K-Means&#31639;&#27861;&#23545;&#20114;&#30456;&#20998;&#31163;&#30340;&#32858;&#31867;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#24615;&#33021;&#26356;&#20248;&#30340;K-Means++&#21464;&#20307;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21508;&#31181;&#21697;&#29260;&#30340;K-Means&#31639;&#27861;&#22312;&#27491;&#30830;&#24674;&#22797;&#20114;&#30456;&#20998;&#31163;&#30340;&#32858;&#31867;&#26102;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#37324;&#25152;&#20351;&#29992;&#30340;&#20114;&#30456;&#20998;&#31163;&#30340;&#27010;&#24565;&#30452;&#25509;&#28304;&#33258;&#23545;&#32858;&#31867;&#24120;&#35265;&#23450;&#20041;&#30340;&#25512;&#23548;&#65292;&#36825;&#20010;&#23450;&#20041;&#24378;&#35843;&#20102;&#22312;&#31751;&#20869;&#21516;&#36136;&#24615;&#21644;&#31751;&#38388;&#24046;&#24322;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#25512;&#23548;&#20986;&#20102;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#20114;&#30456;&#20998;&#31163;&#24615;&#30340;&#32858;&#31867;&#34920;&#29616;&#25104;&#20102;K-Means&#20195;&#20215;&#20989;&#25968;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;&#36890;&#36807;&#23454;&#39564;&#35843;&#26597;&#21457;&#29616;&#21508;&#31181;&#21697;&#29260;&#30340;K-Means&#31639;&#27861;&#23454;&#38469;&#19978;&#19981;&#33021;&#27491;&#30830;&#22320;&#21457;&#29616;&#20114;&#30456;&#20998;&#31163;&#30340;&#32858;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#23427;&#26159;K-Means++&#30340;&#21464;&#20307;&#65292;&#36890;&#36807;&#37325;&#22797;&#23376;&#37319;&#26679;&#26469;&#36873;&#25321;&#31181;&#23376;&#12290;&#36825;&#20010;&#26032;&#31639;&#27861;&#22312;&#20219;&#21153;&#20013;&#32988;&#36807;&#20102;K-Means&#23478;&#26063;&#20013;&#30340;&#20854;&#20182;&#22235;&#20010;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the capability of correctly recovering well-separated clusters by various brands of the $k$-means algorithm. The concept of well-separatedness used here is derived directly from the common definition of clusters, which imposes an interplay between the requirements of within-cluster-homogenicity and between-clusters-diversity. Conditions are derived for a special case of well-separated clusters such that the global minimum of $k$-means cost function coincides with the well-separatedness. An experimental investigation is performed to find out whether or no various brands of $k$-means are actually capable of discovering well separated clusters. It turns out that they are not. A new algorithm is proposed that is a variation of $k$-means++ via repeated {sub}sampling when choosing a seed. The new algorithm outperforms four other algorithms from $k$-means family on the task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24191;&#27867;&#30740;&#31350;&#65292;&#21457;&#29616;&#23545;&#20110;&#21333;&#20010;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#25913;&#21892;&#20250;&#22823;&#22823;&#38477;&#20302;&#23545;&#26410;&#32771;&#34385;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#65292;&#20294;&#22312;&#22810;&#23646;&#24615;&#27169;&#24335;&#19979;&#21487;&#20197;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01923</link><description>&lt;p&gt;
&#22810;&#37325;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#25913;&#21892;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study on Fairness Improvement with Multiple Protected Attributes. (arXiv:2308.01923v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24191;&#27867;&#30740;&#31350;&#65292;&#21457;&#29616;&#23545;&#20110;&#21333;&#20010;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#25913;&#21892;&#20250;&#22823;&#22823;&#38477;&#20302;&#23545;&#26410;&#32771;&#34385;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#65292;&#20294;&#22312;&#22810;&#23646;&#24615;&#27169;&#24335;&#19979;&#21487;&#20197;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#21333;&#20010;&#20445;&#25252;&#23646;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#36719;&#20214;&#30340;&#20844;&#24179;&#24615;&#25913;&#21892;&#65292;&#20294;&#32771;&#34385;&#21040;&#35768;&#22810;&#29992;&#25143;&#20855;&#26377;&#22810;&#20010;&#20445;&#25252;&#23646;&#24615;&#65292;&#36825;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#26412;&#25991;&#23545;&#22810;&#20010;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#25913;&#21892;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#28085;&#30422;&#20102;11&#31181;&#26368;&#20808;&#36827;&#30340;&#20844;&#24179;&#24615;&#25913;&#21892;&#26041;&#27861;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#32771;&#34385;&#22810;&#20010;&#20445;&#25252;&#23646;&#24615;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;ML&#27169;&#22411;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25913;&#21892;&#21333;&#20010;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#22823;&#22823;&#38477;&#20302;&#20102;&#26410;&#32771;&#34385;&#30340;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#12290;&#22312;88.3&#65285;&#30340;&#24773;&#20917;&#19979;&#35266;&#23519;&#21040;&#36825;&#31181;&#38477;&#20302;&#65288;&#24179;&#22343;&#20026;57.5&#65285;&#65289;&#12290;&#26356;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#32771;&#34385;&#21333;&#20010;&#21644;&#22810;&#20010;&#20445;&#25252;&#23646;&#24615;&#26102;&#65292;&#20934;&#30830;&#29575;&#25439;&#22833;&#26041;&#38754;&#20960;&#20046;&#27809;&#26377;&#24046;&#24322;&#65292;&#36825;&#34920;&#26126;&#22312;&#22810;&#23646;&#24615;&#27169;&#24335;&#19979;&#21487;&#20197;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#22810;&#20010;&#20445;&#25252;&#23646;&#24615;&#26102;&#65292;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#30340;&#24433;&#21709;&#36739;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing research mostly improves the fairness of Machine Learning (ML) software regarding a single protected attribute at a time, but this is unrealistic given that many users have multiple protected attributes. This paper conducts an extensive study of fairness improvement regarding multiple protected attributes, covering 11 state-of-the-art fairness improvement methods. We analyze the effectiveness of these methods with different datasets, metrics, and ML models when considering multiple protected attributes. The results reveal that improving fairness for a single protected attribute can largely decrease fairness regarding unconsidered protected attributes. This decrease is observed in up to 88.3% of scenarios (57.5% on average). More surprisingly, we find little difference in accuracy loss when considering single and multiple protected attributes, indicating that accuracy can be maintained in the multiple-attribute paradigm. However, the effect on precision and recall when handling
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#36716;&#31227;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#27169;&#22411;&#65292;&#29992;&#20110;&#24555;&#36895;&#24212;&#23545;&#26410;&#26469;&#30340;&#29983;&#29289;&#23041;&#32961;&#12290;&#36890;&#36807;&#21033;&#29992;&#21253;&#21547;30&#19975;&#31181;&#20505;&#36873;&#33647;&#29289;&#21644;23&#20010;&#20896;&#29366;&#30149;&#27602;&#34507;&#30333;&#38774;&#30340;COVID-19&#33647;&#29289;&#23545;&#25509;&#25968;&#25454;&#38598;&#65292;&#35757;&#32451;&#20102;&#39640;&#36890;&#37327;&#34394;&#25311;COVID-19&#33647;&#29289;&#31579;&#36873;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#25351;&#32441;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#22312;&#23545;&#25509;&#24471;&#20998;&#19978;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#21487;&#36716;&#31227;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#26041;&#27861;&#65292;&#33021;&#22815;&#36866;&#29992;&#20110;&#26410;&#30693;&#30340;&#38774;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.01921</link><description>&lt;p&gt;
&#21487;&#36716;&#31227;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#27169;&#22411;&#24555;&#36895;&#24212;&#23545;&#26410;&#26469;&#29983;&#29289;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
Transferable Graph Neural Fingerprint Models for Quick Response to Future Bio-Threats. (arXiv:2308.01921v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01921
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#36716;&#31227;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#27169;&#22411;&#65292;&#29992;&#20110;&#24555;&#36895;&#24212;&#23545;&#26410;&#26469;&#30340;&#29983;&#29289;&#23041;&#32961;&#12290;&#36890;&#36807;&#21033;&#29992;&#21253;&#21547;30&#19975;&#31181;&#20505;&#36873;&#33647;&#29289;&#21644;23&#20010;&#20896;&#29366;&#30149;&#27602;&#34507;&#30333;&#38774;&#30340;COVID-19&#33647;&#29289;&#23545;&#25509;&#25968;&#25454;&#38598;&#65292;&#35757;&#32451;&#20102;&#39640;&#36890;&#37327;&#34394;&#25311;COVID-19&#33647;&#29289;&#31579;&#36873;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#25351;&#32441;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#22312;&#23545;&#25509;&#24471;&#20998;&#19978;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#21487;&#36716;&#31227;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#26041;&#27861;&#65292;&#33021;&#22815;&#36866;&#29992;&#20110;&#26410;&#30693;&#30340;&#38774;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#33647;&#29289;&#20998;&#23376;&#24555;&#36895;&#31579;&#36873;&#26159;&#33647;&#29289;&#21457;&#29616;&#31649;&#32447;&#20013;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#22270;&#31070;&#32463;&#25351;&#32441;&#26159;&#19968;&#31181;&#29992;&#20110;&#24320;&#21457;&#39640;&#36890;&#37327;&#21644;&#39640;&#20934;&#30830;&#24615;&#20998;&#23376;&#23545;&#25509;&#20195;&#29702;&#30340;&#26377;&#24076;&#26395;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#21547;&#32422;30&#19975;&#31181;&#33647;&#29289;&#20505;&#36873;&#29289;&#21644;23&#20010;&#20896;&#29366;&#30149;&#27602;&#34507;&#30333;&#38774;&#30340;COVID-19&#33647;&#29289;&#23545;&#25509;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#22270;&#31070;&#32463;&#25351;&#32441;&#23545;&#25509;&#27169;&#22411;&#65292;&#29992;&#20110;&#39640;&#36890;&#37327;&#34394;&#25311;COVID-19&#33647;&#29289;&#31579;&#36873;&#12290;&#22270;&#31070;&#32463;&#25351;&#32441;&#27169;&#22411;&#22312;&#23545;&#25509;&#24471;&#20998;&#19978;&#20855;&#26377;&#24456;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#23545;&#22823;&#22810;&#25968;&#23545;&#25509;&#38774;&#28857;&#30340;&#22343;&#26041;&#35823;&#24046;&#20302;&#20110;0.21 kcal/mol&#65292;&#30456;&#27604;&#20256;&#32479;&#22278;&#24418;&#25351;&#32441;&#26041;&#27861;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;&#20026;&#20102;&#20351;&#31070;&#32463;&#25351;&#32441;&#36866;&#29992;&#20110;&#26410;&#30693;&#30340;&#38774;&#28857;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22810;&#20010;&#38774;&#28857;&#19978;&#35757;&#32451;&#30340;&#21487;&#36716;&#31227;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fast screening of drug molecules based on the ligand binding affinity is an important step in the drug discovery pipeline. Graph neural fingerprint is a promising method for developing molecular docking surrogates with high throughput and great fidelity. In this study, we built a COVID-19 drug docking dataset of about 300,000 drug candidates on 23 coronavirus protein targets. With this dataset, we trained graph neural fingerprint docking models for high-throughput virtual COVID-19 drug screening. The graph neural fingerprint models yield high prediction accuracy on docking scores with the mean squared error lower than $0.21$ kcal/mol for most of the docking targets, showing significant improvement over conventional circular fingerprint methods. To make the neural fingerprints transferable for unknown targets, we also propose a transferable graph neural fingerprint method trained on multiple targets. With comparable accuracy to target-specific graph neural fingerprint models, the transf
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#32435;&#31859;&#25239;&#20307;&#29983;&#20135;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#21363;&#32570;&#20047;&#19982;&#22823;&#22810;&#25968;&#25239;&#21407;&#30456;&#21305;&#37197;&#30340;&#32435;&#31859;&#25239;&#20307;&#12290;&#30740;&#31350;&#38024;&#23545;&#27492;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24207;&#21015;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#32435;&#31859;&#25239;&#20307;-&#25239;&#21407;&#30340;&#32467;&#21512;&#12290;&#27492;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;3D&#32467;&#26500;&#65292;&#24182;&#19988;&#33021;&#22815;&#25552;&#39640;&#32435;&#31859;&#25239;&#20307;&#30340;&#29983;&#20135;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.01920</link><description>&lt;p&gt;
&#24207;&#21015;&#22522;&#30784;&#30340;&#32435;&#31859;&#25239;&#20307;-&#25239;&#21407;&#32467;&#21512;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Sequence-Based Nanobody-Antigen Binding Prediction. (arXiv:2308.01920v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#32435;&#31859;&#25239;&#20307;&#29983;&#20135;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#21363;&#32570;&#20047;&#19982;&#22823;&#22810;&#25968;&#25239;&#21407;&#30456;&#21305;&#37197;&#30340;&#32435;&#31859;&#25239;&#20307;&#12290;&#30740;&#31350;&#38024;&#23545;&#27492;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24207;&#21015;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#32435;&#31859;&#25239;&#20307;-&#25239;&#21407;&#30340;&#32467;&#21512;&#12290;&#27492;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;3D&#32467;&#26500;&#65292;&#24182;&#19988;&#33021;&#22815;&#25552;&#39640;&#32435;&#31859;&#25239;&#20307;&#30340;&#29983;&#20135;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32435;&#31859;&#25239;&#20307;(Nb)&#26159;&#26469;&#28304;&#20110;&#39558;&#39548;&#21644;&#40104;&#40060;&#31561;&#21160;&#29289;&#30340;&#21333;&#20307;&#37325;&#38142;&#25239;&#20307;&#29255;&#27573;&#12290;&#23427;&#20204;&#30340;&#23610;&#23544;&#30456;&#23545;&#36739;&#23567;&#65288;&#32422;3-4 nm&#65307;13 kDa&#65289;&#19988;&#20855;&#26377;&#33391;&#22909;&#30340;&#29983;&#29289;&#29289;&#29702;&#24615;&#36136;&#65292;&#22240;&#27492;&#25104;&#20026;&#37325;&#32452;&#29983;&#20135;&#30340;&#21560;&#24341;&#20154;&#30340;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#32435;&#31859;&#25239;&#20307;&#20855;&#26377;&#36873;&#25321;&#24615;&#22320;&#32467;&#21512;&#21040;&#29305;&#23450;&#30340;&#25239;&#21407;&#65292;&#22914;&#27602;&#32032;&#12289;&#21270;&#23398;&#29289;&#36136;&#12289;&#32454;&#33740;&#21644;&#30149;&#27602;&#65292;&#20351;&#20854;&#25104;&#20026;&#32454;&#32990;&#29983;&#29289;&#23398;&#12289;&#32467;&#26500;&#29983;&#29289;&#23398;&#12289;&#21307;&#23398;&#35786;&#26029;&#20197;&#21450;&#27835;&#30103;&#30284;&#30151;&#21644;&#20854;&#20182;&#20005;&#37325;&#30142;&#30149;&#30340;&#26377;&#21147;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#32435;&#31859;&#25239;&#20307;&#22312;&#22823;&#22810;&#25968;&#25239;&#21407;&#19978;&#26159;&#19981;&#21487;&#33719;&#24471;&#30340;&#65292;&#36825;&#26159;&#32435;&#31859;&#25239;&#20307;&#29983;&#20135;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#19968;&#20123;&#35745;&#31639;&#26041;&#27861;&#26469;&#31579;&#36873;&#19982;&#32473;&#23450;&#30446;&#26631;&#25239;&#21407;&#30456;&#37197;&#30340;&#28508;&#22312;&#32435;&#31859;&#25239;&#20307;&#65292;&#20294;&#26159;&#30001;&#20110;&#20381;&#36182;&#20110;3D&#32467;&#26500;&#65292;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#21463;&#21040;&#20102;&#20005;&#26684;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#39044;&#27979;&#32435;&#31859;&#25239;&#20307;-&#25239;&#21407;&#30456;&#20114;&#20316;&#29992;&#65288;&#32467;&#21512;&#65289;&#26159;&#19968;&#39033;&#32791;&#26102;&#19988;&#21171;&#21160;&#23494;&#38598;&#30340;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nanobodies (Nb) are monomeric heavy-chain fragments derived from heavy-chain only antibodies naturally found in Camelids and Sharks. Their considerably small size (~3-4 nm; 13 kDa) and favorable biophysical properties make them attractive targets for recombinant production. Furthermore, their unique ability to bind selectively to specific antigens, such as toxins, chemicals, bacteria, and viruses, makes them powerful tools in cell biology, structural biology, medical diagnostics, and future therapeutic agents in treating cancer and other serious illnesses. However, a critical challenge in nanobodies production is the unavailability of nanobodies for a majority of antigens. Although some computational methods have been proposed to screen potential nanobodies for given target antigens, their practical application is highly restricted due to their reliance on 3D structures. Moreover, predicting nanobodyantigen interactions (binding) is a time-consuming and labor-intensive task. This study
&lt;/p&gt;</description></item><item><title>PePNet&#26159;&#19968;&#31181;&#25903;&#25345;&#32597;&#35265;&#37325;&#36127;&#36733;&#30340;&#24037;&#20316;&#36127;&#36733;&#39044;&#27979;&#32593;&#32476;&#65292;&#36890;&#36807;&#21608;&#26399;&#24615;&#24863;&#30693;&#26426;&#21046;&#21644;&#34701;&#21512;&#22810;&#23610;&#24230;&#24207;&#21015;&#23398;&#20064;&#30340;&#33021;&#21147;&#25552;&#39640;&#20102;&#25972;&#20307;&#29305;&#21035;&#26159;&#37325;&#36127;&#36733;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01917</link><description>&lt;p&gt;
PePNet: &#19968;&#31181;&#25903;&#25345;&#32597;&#35265;&#37325;&#36127;&#36733;&#30340;&#21608;&#26399;&#24615;&#24863;&#30693;&#24037;&#20316;&#36127;&#36733;&#39044;&#27979;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
PePNet: A Periodicity-Perceived Workload Prediction Network Supporting Rare Occurrence of Heavy Workload. (arXiv:2308.01917v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01917
&lt;/p&gt;
&lt;p&gt;
PePNet&#26159;&#19968;&#31181;&#25903;&#25345;&#32597;&#35265;&#37325;&#36127;&#36733;&#30340;&#24037;&#20316;&#36127;&#36733;&#39044;&#27979;&#32593;&#32476;&#65292;&#36890;&#36807;&#21608;&#26399;&#24615;&#24863;&#30693;&#26426;&#21046;&#21644;&#34701;&#21512;&#22810;&#23610;&#24230;&#24207;&#21015;&#23398;&#20064;&#30340;&#33021;&#21147;&#25552;&#39640;&#20102;&#25972;&#20307;&#29305;&#21035;&#26159;&#37325;&#36127;&#36733;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20113;&#25552;&#20379;&#21830;&#21487;&#20197;&#20174;&#20934;&#30830;&#30340;&#24037;&#20316;&#36127;&#36733;&#39044;&#27979;&#20013;&#33719;&#24471;&#24040;&#22823;&#30340;&#22909;&#22788;&#12290;&#28982;&#32780;&#65292;&#20113;&#26381;&#21153;&#22120;&#30340;&#24037;&#20316;&#36127;&#36733;&#39640;&#24230;&#21464;&#21270;&#65292;&#26377;&#26102;&#20250;&#21457;&#29983;&#37325;&#36127;&#36733;&#31361;&#21457;&#20107;&#20214;&#65292;&#36825;&#20351;&#24471;&#24037;&#20316;&#36127;&#36733;&#39044;&#27979;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30446;&#21069;&#26377;&#20004;&#31181;&#20027;&#35201;&#30340;&#24037;&#20316;&#36127;&#36733;&#39044;&#27979;&#26041;&#27861;&#65306;&#32479;&#35745;&#26041;&#27861;&#21644;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#21069;&#32773;&#20381;&#36182;&#20110;&#24378;&#22823;&#30340;&#25968;&#23398;&#20551;&#35774;&#65292;&#24403;&#39044;&#27979;&#39640;&#24230;&#21464;&#21270;&#30340;&#24037;&#20316;&#36127;&#36733;&#26102;&#65292;&#20854;&#20934;&#30830;&#24615;&#36739;&#20302;&#12290;&#32780;&#21518;&#32773;&#22312;&#25972;&#20307;&#20934;&#30830;&#24615;&#19978;&#26356;&#39640;&#65292;&#20294;&#23481;&#26131;&#21463;&#21040;&#37325;&#36127;&#36733;&#21644;&#24120;&#35265;&#36127;&#36733;&#20043;&#38388;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#24433;&#21709;&#65292;&#36825;&#20250;&#24433;&#21709;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#37325;&#36127;&#36733;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#26080;&#35770;&#26159;&#32479;&#35745;&#26041;&#27861;&#30340;&#25972;&#20307;&#19981;&#20934;&#30830;&#24615;&#36824;&#26159;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#23545;&#37325;&#36127;&#36733;&#30340;&#19981;&#20934;&#30830;&#24615;&#37117;&#20250;&#23548;&#33268;&#26381;&#21153;&#32423;&#21035;&#21327;&#35758;&#30340;&#36829;&#35268;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PePNet&#26469;&#25552;&#39640;&#25972;&#20307;&#29305;&#21035;&#26159;&#37325;&#36127;&#36733;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#23427;&#20855;&#26377;&#20004;&#20010;&#29420;&#29305;&#30340;&#29305;&#28857;&#65306;&#21608;&#26399;&#24615;&#24863;&#30693;&#26426;&#21046;&#21644;&#34701;&#21512;&#22810;&#23610;&#24230;&#24207;&#21015;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cloud providers can greatly benefit from accurate workload prediction. However, the workload of cloud servers is highly variable, with occasional heavy workload bursts. This makes workload prediction challenging.  There are mainly two categories of workload prediction methods: statistical methods and neural-network-based ones. The former ones rely on strong mathematical assumptions and have reported low accuracy when predicting highly variable workload. The latter ones offer higher overall accuracy, yet they are vulnerable to data imbalance between heavy workload and common one. This impairs the prediction accuracy of neural network-based models on heavy workload.  Either the overall inaccuracy of statistic methods or the heavy-workload inaccuracy of neural-network-based models can cause service level agreement violations.  Thus, we propose PePNet to improve overall especially heavy workload prediction accuracy. It has two distinctive characteristics:  (i) A Periodicity-Perceived Mecha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#21322;&#30417;&#30563;&#20803;&#23398;&#20064;&#22312;&#26102;&#31354;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#23558;&#20803;&#23398;&#20064;&#24212;&#29992;&#20110;&#33258;&#30417;&#30563;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#32467;&#21512;&#29366;&#24577;-of-the-art&#34920;&#31034;&#23398;&#20064;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#35270;&#39057;&#37325;&#24314;&#21644;&#21160;&#20316;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.01916</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#20803;&#23398;&#20064;&#22312;&#26102;&#31354;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Semi Supervised Meta Learning for Spatiotemporal Learning. (arXiv:2308.01916v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21322;&#30417;&#30563;&#20803;&#23398;&#20064;&#22312;&#26102;&#31354;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#23558;&#20803;&#23398;&#20064;&#24212;&#29992;&#20110;&#33258;&#30417;&#30563;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#32467;&#21512;&#29366;&#24577;-of-the-art&#34920;&#31034;&#23398;&#20064;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#35270;&#39057;&#37325;&#24314;&#21644;&#21160;&#20316;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#19977;&#20010;&#27493;&#39588;&#26469;&#23558;&#20803;&#23398;&#20064;&#24212;&#29992;&#20110;&#33258;&#30417;&#30563;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#20197;&#36827;&#34892;&#26102;&#31354;&#23398;&#20064;&#12290;&#24191;&#20041;&#19978;&#35828;&#65292;&#25105;&#20204;&#26088;&#22312;&#29702;&#35299;&#23558;&#20803;&#23398;&#20064;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#34920;&#31034;&#23398;&#20064;&#26550;&#26500;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#27979;&#35797;&#26102;&#31354;&#23398;&#20064;&#65306;&#20165;&#20803;&#23398;&#20064;&#26550;&#26500;&#12289;&#20165;&#34920;&#31034;&#23398;&#20064;&#26550;&#26500;&#20197;&#21450;&#23558;&#34920;&#31034;&#23398;&#20064;&#19982;&#20803;&#23398;&#20064;&#26550;&#26500;&#30456;&#32467;&#21512;&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#22686;&#24378;&#35760;&#24518;&#31070;&#32463;&#32593;&#32476;&#65288;MANN&#65289;&#26550;&#26500;&#23558;&#20803;&#23398;&#20064;&#24212;&#29992;&#20110;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23581;&#35797;&#22312;&#23567;&#35268;&#27169;&#26102;&#31354;&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#39044;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#36827;&#34892;&#35270;&#39057;&#37325;&#24314;&#20219;&#21153;&#30340;&#24494;&#35843;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23581;&#35797;&#35757;&#32451;&#33258;&#30417;&#30563;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#30340;&#32534;&#30721;&#22120;&#65292;&#24182;&#24212;&#29992;&#20998;&#31867;&#22836;&#36827;&#34892;&#21160;&#20316;&#20998;&#31867;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23581;&#35797;&#22312;&#39044;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#30340;&#22522;&#30784;&#19978;&#65292;&#29992;MANN&#39592;&#24178;&#36827;&#34892;&#24494;&#35843;&#65292;&#29992;&#20110;&#21160;&#20316;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We approached the goal of applying meta-learning to self-supervised masked autoencoders for spatiotemporal learning in three steps. Broadly, we seek to understand the impact of applying meta-learning to existing state-of-the-art representation learning architectures. Thus, we test spatiotemporal learning through: a meta-learning architecture only, a representation learning architecture only, and an architecture applying representation learning alongside a meta learning architecture. We utilize the Memory Augmented Neural Network (MANN) architecture to apply meta-learning to our framework. Specifically, we first experiment with applying a pre-trained MAE and fine-tuning on our small-scale spatiotemporal dataset for video reconstruction tasks. Next, we experiment with training an MAE encoder and applying a classification head for action classification tasks. Finally, we experiment with applying a pre-trained MAE and fine-tune with MANN backbone for action classification tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22522;&#20934;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;LOB&#25968;&#25454;&#30340;&#32929;&#31080;&#20215;&#26684;&#36235;&#21183;&#39044;&#27979;&#30340;15&#31181;&#26368;&#26032;DL&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#38754;&#23545;&#26032;&#25968;&#25454;&#26102;&#24615;&#33021;&#26126;&#26174;&#19979;&#38477;&#65292;&#23545;&#20854;&#22312;&#23454;&#38469;&#24066;&#22330;&#20013;&#30340;&#24212;&#29992;&#24615;&#25552;&#20986;&#20102;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.01915</link><description>&lt;p&gt;
&#22522;&#20110;LOB&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#32929;&#31080;&#20215;&#26684;&#36235;&#21183;&#39044;&#27979;&#65306;&#19968;&#39033;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
LOB-Based Deep Learning Models for Stock Price Trend Prediction: A Benchmark Study. (arXiv:2308.01915v1 [q-fin.TR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22522;&#20934;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;LOB&#25968;&#25454;&#30340;&#32929;&#31080;&#20215;&#26684;&#36235;&#21183;&#39044;&#27979;&#30340;15&#31181;&#26368;&#26032;DL&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#38754;&#23545;&#26032;&#25968;&#25454;&#26102;&#24615;&#33021;&#26126;&#26174;&#19979;&#38477;&#65292;&#23545;&#20854;&#22312;&#23454;&#38469;&#24066;&#22330;&#20013;&#30340;&#24212;&#29992;&#24615;&#25552;&#20986;&#20102;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30740;&#31350;&#22312;&#37329;&#34701;&#34892;&#19994;&#20013;&#20135;&#29983;&#20102;&#26174;&#33879;&#24433;&#21709;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#38480;&#20215;&#35746;&#21333;&#31807;&#65288;LOB&#65289;&#25968;&#25454;&#30340;&#32929;&#31080;&#20215;&#26684;&#36235;&#21183;&#39044;&#27979;&#65288;SPTP&#65289;&#30340;&#21313;&#20116;&#31181;&#26368;&#26032; DL&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#36827;&#34892;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;LOBCAST&#65292;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#21253;&#25324;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;DL &#27169;&#22411;&#35757;&#32451;&#12289;&#35780;&#20272;&#21644;&#21033;&#28070;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#26377;&#27169;&#22411;&#22312;&#38754;&#23545;&#26032;&#25968;&#25454;&#26102;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#20174;&#32780;&#23545;&#23427;&#20204;&#22312;&#23454;&#38469;&#24066;&#22330;&#20013;&#30340;&#36866;&#29992;&#24615;&#25552;&#20986;&#20102;&#30097;&#38382;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20316;&#20026;&#19968;&#20010;&#22522;&#20934;&#65292;&#25581;&#31034;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#20026;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advancements in Deep Learning (DL) research have notably influenced the finance sector. We examine the robustness and generalizability of fifteen state-of-the-art DL models focusing on Stock Price Trend Prediction (SPTP) based on Limit Order Book (LOB) data. To carry out this study, we developed LOBCAST, an open-source framework that incorporates data preprocessing, DL model training, evaluation and profit analysis. Our extensive experiments reveal that all models exhibit a significant performance drop when exposed to new data, thereby raising questions about their real-world market applicability. Our work serves as a benchmark, illuminating the potential and the limitations of current approaches and providing insight for innovative solutions.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#21830;&#21697;&#20132;&#26131;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#27969;&#21160;&#24615;&#21644;&#20943;&#23569;&#24066;&#22330;&#27874;&#21160;&#24615;&#26469;&#31283;&#23450;&#24066;&#22330;&#12290;&#36825;&#23545;&#20110;&#35299;&#20915;&#33021;&#28304;&#24066;&#22330;&#30340;&#19981;&#31283;&#23450;&#21644;&#20840;&#29699;&#33021;&#28304;&#21361;&#26426;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.01910</link><description>&lt;p&gt;
&#21830;&#21697;&#24066;&#22330;&#20013;&#30340;&#28145;&#24230;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Policy Gradient Methods in Commodity Markets. (arXiv:2308.01910v1 [q-fin.TR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#21830;&#21697;&#20132;&#26131;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#27969;&#21160;&#24615;&#21644;&#20943;&#23569;&#24066;&#22330;&#27874;&#21160;&#24615;&#26469;&#31283;&#23450;&#24066;&#22330;&#12290;&#36825;&#23545;&#20110;&#35299;&#20915;&#33021;&#28304;&#24066;&#22330;&#30340;&#19981;&#31283;&#23450;&#21644;&#20840;&#29699;&#33021;&#28304;&#21361;&#26426;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#28304;&#36716;&#22411;&#22686;&#21152;&#20102;&#23545;&#38388;&#27463;&#24615;&#33021;&#28304;&#30340;&#20381;&#36182;&#65292;&#19981;&#31283;&#23450;&#20102;&#33021;&#28304;&#24066;&#22330;&#24182;&#23548;&#33268;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#27874;&#21160;&#24615;&#65292;&#26368;&#32456;&#23548;&#33268;&#20102;2021&#24180;&#30340;&#20840;&#29699;&#33021;&#28304;&#21361;&#26426;&#12290;&#38500;&#20102;&#23545;&#29983;&#20135;&#32773;&#21644;&#28040;&#36153;&#32773;&#36896;&#25104;&#20260;&#23475;&#22806;&#65292;&#27874;&#21160;&#30340;&#33021;&#28304;&#24066;&#22330;&#36824;&#21487;&#33021;&#21361;&#21450;&#20851;&#38190;&#30340;&#30899;&#20943;&#25490;&#21162;&#21147;&#12290;&#20132;&#26131;&#21830;&#36890;&#36807;&#25552;&#20379;&#27969;&#21160;&#24615;&#21644;&#20943;&#23569;&#27874;&#21160;&#24615;&#22312;&#31283;&#23450;&#24066;&#22330;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#25968;&#23398;&#21644;&#32479;&#35745;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#26410;&#26469;&#25910;&#30410;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#37329;&#34701;&#24066;&#22330;&#30340;&#20302;&#20449;&#22122;&#27604;&#21644;&#38750;&#24179;&#31283;&#21160;&#24577;&#65292;&#24320;&#21457;&#36825;&#26679;&#30340;&#27169;&#22411;&#24182;&#19981;&#26159;&#26131;&#20107;&#12290;&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#21830;&#21697;&#20132;&#26131;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#23558;&#21830;&#21697;&#20132;&#26131;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#36830;&#32493;&#31163;&#25955;&#26102;&#38388;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#37319;&#29992;&#20102;&#26032;&#39062;&#30340;&#26102;&#38388;&#31163;&#25955;&#21270;&#26041;&#26696;&#65292;&#23545;&#24066;&#22330;&#27874;&#21160;&#20570;&#20986;&#21453;&#24212;&#24182;&#33258;&#36866;&#24212;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#32479;&#35745;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The energy transition has increased the reliance on intermittent energy sources, destabilizing energy markets and causing unprecedented volatility, culminating in the global energy crisis of 2021. In addition to harming producers and consumers, volatile energy markets may jeopardize vital decarbonization efforts. Traders play an important role in stabilizing markets by providing liquidity and reducing volatility. Several mathematical and statistical models have been proposed for forecasting future returns. However, developing such models is non-trivial due to financial markets' low signal-to-noise ratios and nonstationary dynamics.  This thesis investigates the effectiveness of deep reinforcement learning methods in commodities trading. It formalizes the commodities trading problem as a continuing discrete-time stochastic dynamical system. This system employs a novel time-discretization scheme that is reactive and adaptive to market volatility, providing better statistical properties f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MRQ&#30340;&#27169;&#22411;&#37325;&#26032;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29616;&#26377;&#30340;&#37327;&#21270;&#27169;&#22411;&#24555;&#36895;&#36716;&#25442;&#20197;&#28385;&#36275;&#19981;&#21516;&#30340;&#37327;&#21270;&#35201;&#27714;&#65292;&#35299;&#20915;&#20102;&#22312;&#22266;&#23450;&#28857;&#30828;&#20214;&#19978;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.01867</link><description>&lt;p&gt;
MRQ:&#36890;&#36807;&#27169;&#22411;&#37325;&#26032;&#37327;&#21270;&#25903;&#25345;&#22810;&#31181;&#37327;&#21270;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
MRQ:Support Multiple Quantization Schemes through Model Re-Quantization. (arXiv:2308.01867v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01867
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MRQ&#30340;&#27169;&#22411;&#37325;&#26032;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29616;&#26377;&#30340;&#37327;&#21270;&#27169;&#22411;&#24555;&#36895;&#36716;&#25442;&#20197;&#28385;&#36275;&#19981;&#21516;&#30340;&#37327;&#21270;&#35201;&#27714;&#65292;&#35299;&#20915;&#20102;&#22312;&#22266;&#23450;&#28857;&#30828;&#20214;&#19978;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21508;&#31181;&#30828;&#20214;&#21152;&#36895;&#22120;&#65288;&#22914;NPU&#65292;TPU&#65292;DPU&#65289;&#30340;&#26222;&#21450;&#65292;&#20294;&#22312;&#22266;&#23450;&#28857;&#30828;&#20214;&#19978;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21407;&#22240;&#26159;&#22797;&#26434;&#30340;&#27169;&#22411;&#37327;&#21270;&#21644;&#36716;&#25442;&#12290;&#29616;&#26377;&#30340;&#27169;&#22411;&#37327;&#21270;&#26694;&#26550;&#65288;&#22914;Tensorflow QAT&#65292;TFLite PTQ&#21644;Qualcomm AIMET&#65289;&#21482;&#25903;&#25345;&#26377;&#38480;&#30340;&#37327;&#21270;&#26041;&#26696;&#65288;&#22914;&#20165;&#22312;TF1.x QAT&#20013;&#30340;&#38750;&#23545;&#31216;&#27599;&#24352;&#37327;&#37327;&#21270;&#65289;&#12290;&#22240;&#27492;&#65292;&#30001;&#20110;&#31245;&#24494;&#19981;&#21516;&#30340;&#37327;&#21270;&#35201;&#27714;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19981;&#33021;&#36731;&#26494;&#22320;&#20026;&#21508;&#31181;&#22266;&#23450;&#28857;&#30828;&#20214;&#36827;&#34892;&#37327;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#24819;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#27169;&#22411;&#37327;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;MRQ&#65288;&#27169;&#22411;&#37325;&#26032;&#37327;&#21270;&#65289;&#65292;&#23427;&#21487;&#20197;&#37319;&#29992;&#29616;&#26377;&#30340;&#37327;&#21270;&#27169;&#22411;&#65292;&#24182;&#24555;&#36895;&#23558;&#27169;&#22411;&#36716;&#25442;&#20026;&#28385;&#36275;&#19981;&#21516;&#37327;&#21270;&#35201;&#27714;&#65288;&#22914;&#38750;&#23545;&#31216;-&gt;&#23545;&#31216;&#65292;&#38750;2&#30340;&#24130;&#27425;-&gt;2&#30340;&#24130;&#27425;&#65289;&#12290;&#37325;&#26032;&#37327;&#21270;&#27604;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#37327;&#21270;&#35201;&#31616;&#21333;&#24471;&#22810;&#65292;&#22240;&#20026;&#23427;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the proliferation of diverse hardware accelerators (e.g., NPU, TPU, DPU), deploying deep learning models on edge devices with fixed-point hardware is still challenging due to complex model quantization and conversion. Existing model quantization frameworks like Tensorflow QAT [1], TFLite PTQ [2], and Qualcomm AIMET [3] supports only a limited set of quantization schemes (e.g., only asymmetric per-tensor quantization in TF1.x QAT [4]). Accordingly, deep learning models cannot be easily quantized for diverse fixed-point hardwares, mainly due to slightly different quantization requirements. In this paper, we envision a new type of model quantization approach called MRQ (model re-quantization), which takes existing quantized models and quickly transforms the models to meet different quantization requirements (e.g., asymmetric -&gt; symmetric, non-power-of-2 scale -&gt; power-of-2 scale). Re-quantization is much simpler than quantizing from scratch because it avoids costly re-training and
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#19968;&#27454;&#21517;&#20026;"Hoodwinked"&#30340;&#25991;&#26412;&#28216;&#25103;&#65292;&#30740;&#31350;&#20102;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#27450;&#39575;&#21644;&#35782;&#21035;&#35854;&#35328;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#26432;&#25163;&#32463;&#24120;&#21542;&#35748;&#32618;&#34892;&#24182;&#25351;&#36131;&#20182;&#20154;&#65292;&#23548;&#33268;&#25237;&#31080;&#32467;&#26524;&#21463;&#21040;&#24433;&#21709;&#12290;&#26356;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#26432;&#25163;&#25928;&#26524;&#19978;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#36825;&#31181;&#25913;&#36827;&#26159;&#36890;&#36807;&#22312;&#35752;&#35770;&#20013;&#26356;&#24378;&#30340;&#27450;&#39575;&#33021;&#21147;&#23454;&#29616;&#30340;&#12290;</title><link>http://arxiv.org/abs/2308.01404</link><description>&lt;p&gt;
&#33945;&#39575;&#65306;&#22522;&#20110;&#25991;&#26412;&#30340;&#28216;&#25103;&#20013;&#30340;&#27450;&#39575;&#19982;&#21512;&#20316;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Hoodwinked: Deception and Cooperation in a Text-Based Game for Language Models. (arXiv:2308.01404v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01404
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#19968;&#27454;&#21517;&#20026;"Hoodwinked"&#30340;&#25991;&#26412;&#28216;&#25103;&#65292;&#30740;&#31350;&#20102;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#27450;&#39575;&#21644;&#35782;&#21035;&#35854;&#35328;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#26432;&#25163;&#32463;&#24120;&#21542;&#35748;&#32618;&#34892;&#24182;&#25351;&#36131;&#20182;&#20154;&#65292;&#23548;&#33268;&#25237;&#31080;&#32467;&#26524;&#21463;&#21040;&#24433;&#21709;&#12290;&#26356;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#26432;&#25163;&#25928;&#26524;&#19978;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#36825;&#31181;&#25913;&#36827;&#26159;&#36890;&#36807;&#22312;&#35752;&#35770;&#20013;&#26356;&#24378;&#30340;&#27450;&#39575;&#33021;&#21147;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#27450;&#39575;&#21644;&#35782;&#21035;&#35854;&#35328;&#30340;&#33021;&#21147;&#65311;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#27454;&#21517;&#20026;&#8220;Hoodwinked&#8221;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#28216;&#25103;&#65292;&#21463;&#21040;&#8220;&#40657;&#24110;&#8221;&#21644;&#8220;&#35841;&#26159;&#21351;&#24213;&#8221;&#28216;&#25103;&#30340;&#21551;&#21457;&#65292;&#26469;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#12290;&#29609;&#23478;&#20204;&#34987;&#38145;&#22312;&#19968;&#20010;&#25151;&#23376;&#37324;&#65292;&#24517;&#39035;&#25214;&#21040;&#19968;&#25226;&#38053;&#21273;&#25165;&#33021;&#36867;&#33073;&#65292;&#20294;&#20854;&#20013;&#19968;&#20010;&#29609;&#23478;&#34987;&#27966;&#20219;&#21153;&#26432;&#27515;&#20854;&#20182;&#20154;&#12290;&#27599;&#27425;&#21457;&#29983;&#35851;&#26432;&#65292;&#24184;&#23384;&#30340;&#29609;&#23478;&#20204;&#20250;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#35752;&#35770;&#65292;&#28982;&#21518;&#25237;&#31080;&#23558;&#19968;&#21517;&#29609;&#23478;&#25918;&#36880;&#20986;&#28216;&#25103;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-3&#12289;GPT-3.5&#21644;GPT-4&#25805;&#25511;&#20195;&#29702;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#20102;&#27450;&#39575;&#21644;&#35782;&#21035;&#35854;&#35328;&#30340;&#33021;&#21147;&#35777;&#25454;&#12290;&#26432;&#25163;&#32463;&#24120;&#21542;&#35748;&#33258;&#24049;&#30340;&#32618;&#34892;&#24182;&#25351;&#36131;&#20182;&#20154;&#65292;&#23548;&#33268;&#25237;&#31080;&#32467;&#26524;&#21463;&#21040;&#21487;&#27979;&#37327;&#30340;&#24433;&#21709;&#12290;&#26356;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;24&#20010;&#20004;&#20004;&#27604;&#36739;&#20013;&#30340;18&#20010;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#26432;&#25163;&#25928;&#26524;&#65292;&#36229;&#36234;&#20102;&#26356;&#23567;&#30340;&#27169;&#22411;&#12290;&#27425;&#35201;&#25351;&#26631;&#25552;&#20379;&#20102;&#35777;&#25454;&#65292;&#34920;&#26126;&#36825;&#31181;&#25913;&#36827;&#24182;&#19981;&#26159;&#36890;&#36807;&#19981;&#21516;&#30340;&#34892;&#21160;&#23454;&#29616;&#30340;&#65292;&#32780;&#26159;&#36890;&#36807;&#22312;&#35752;&#35770;&#20013;&#26356;&#24378;&#30340;&#27450;&#39575;&#33021;&#21147;&#23454;&#29616;&#30340;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#23454;&#36136;&#24615;&#30340;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are current language models capable of deception and lie detection? We study this question by introducing a text-based game called $\textit{Hoodwinked}$, inspired by $\textit{Mafia}$ and $\textit{Among Us}$. Players are locked in a house and must find a key to escape, but one player is tasked with killing the others. Each time a murder is committed, the surviving players have a natural language discussion then vote to banish one player from the game. We conduct experiments with agents controlled by GPT-3, GPT-3.5, and GPT-4 and find evidence of deception and lie detection capabilities. The killer often denies their crime and accuses others, leading to measurable effects on voting outcomes. More advanced models are more effective killers, outperforming smaller models in 18 of 24 pairwise comparisons. Secondary metrics provide evidence that this improvement is not mediated by different actions, but rather by stronger deception capabilities during discussions. Overall, we find substantial
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65288;XBL&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#35299;&#37322;&#26469;&#20132;&#20114;&#24335;&#22320;&#28040;&#38500;&#33016;&#37096;X&#23556;&#32447;&#20998;&#31867;&#20013;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#22810;&#25968;&#25454;&#28304;&#24341;&#20837;&#30340;&#28151;&#28102;&#22240;&#32032;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.01119</link><description>&lt;p&gt;
&#22312;&#33016;&#37096;X&#23556;&#32447;&#20998;&#31867;&#20013;&#28040;&#38500;&#38169;&#35823;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Unlearning Spurious Correlations in Chest X-ray Classification. (arXiv:2308.01119v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65288;XBL&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#35299;&#37322;&#26469;&#20132;&#20114;&#24335;&#22320;&#28040;&#38500;&#33016;&#37096;X&#23556;&#32447;&#20998;&#31867;&#20013;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#22810;&#25968;&#25454;&#28304;&#24341;&#20837;&#30340;&#28151;&#28102;&#22240;&#32032;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#26469;&#33258;&#22810;&#20010;&#25968;&#25454;&#28304;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#34429;&#28982;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#28304;&#23545;&#20110;&#23454;&#29616;&#27169;&#22411;&#30340;&#27867;&#21270;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#24517;&#39035;&#25215;&#35748;&#65292;&#36825;&#20123;&#25968;&#25454;&#28304;&#30340;&#22810;&#26679;&#24615;&#26412;&#36136;&#19978;&#24341;&#20837;&#20102;&#24847;&#22806;&#30340;&#28151;&#28102;&#22240;&#32032;&#21644;&#20854;&#20182;&#21487;&#33021;&#24433;&#21709;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#36879;&#26126;&#24230;&#30340;&#25361;&#25112;&#12290;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#19968;&#20010;&#26174;&#33879;&#30340;&#28151;&#28102;&#22240;&#32032;&#65292;&#29305;&#21035;&#26159;&#22312;&#32908;&#39592;&#22270;&#20687;&#20998;&#31867;&#20013;&#65292;&#26159;&#22312;&#38738;&#26149;&#26399;&#35266;&#23519;&#21040;&#30340;&#39592;&#39612;&#25104;&#29087;&#24341;&#36215;&#30340;&#39592;&#39612;&#29983;&#38271;&#12290;&#25105;&#20204;&#20351;&#29992;COVID-19&#33016;&#37096;X&#23556;&#32447;&#25968;&#25454;&#38598;&#35757;&#32451;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#25968;&#25454;&#38598;&#30001;&#20110;&#24847;&#22806;&#28151;&#28102;&#21306;&#22495;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#30456;&#20851;&#24615;&#12290;&#22522;&#20110;&#35299;&#37322;&#30340;&#23398;&#20064;&#65288;XBL&#65289;&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#35299;&#37322;&#26469;&#20132;&#20114;&#24335;&#22320;&#28040;&#38500;&#38169;&#35823;&#30456;&#20851;&#24615;&#65292;&#20351;&#20854;&#36229;&#36234;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#26159;&#36890;&#36807;&#25972;&#21512;&#20132;&#20114;&#24335;&#29992;&#25143;&#21453;&#39304;&#12289;&#31934;&#30830;&#30340;region-of-interest&#36880;&#28176;&#28040;&#35114;&#31561;&#26041;&#27861;&#26469;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical image classification models are frequently trained using training datasets derived from multiple data sources. While leveraging multiple data sources is crucial for achieving model generalization, it is important to acknowledge that the diverse nature of these sources inherently introduces unintended confounders and other challenges that can impact both model accuracy and transparency. A notable confounding factor in medical image classification, particularly in musculoskeletal image classification, is skeletal maturation-induced bone growth observed during adolescence. We train a deep learning model using a Covid-19 chest X-ray dataset and we showcase how this dataset can lead to spurious correlations due to unintended confounding regions. eXplanation Based Learning (XBL) is a deep learning approach that goes beyond interpretability by utilizing model explanations to interactively unlearn spurious correlations. This is achieved by integrating interactive user feedback, specifi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#26410;&#32463;&#27979;&#37327;&#27969;&#22495;&#30340;&#26497;&#31471;&#27700;&#25991;&#20107;&#20214;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20840;&#29699;&#27946;&#27700;&#39044;&#35686;&#30340;&#35206;&#30422;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2307.16104</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25552;&#39640;&#20102;&#20840;&#29699;&#21487;&#38752;&#27946;&#27700;&#39044;&#35686;&#30340;&#35206;&#30422;&#33539;&#22260;
&lt;/p&gt;
&lt;p&gt;
AI Increases Global Access to Reliable Flood Forecasts. (arXiv:2307.16104v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#26410;&#32463;&#27979;&#37327;&#27969;&#22495;&#30340;&#26497;&#31471;&#27700;&#25991;&#20107;&#20214;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20840;&#29699;&#27946;&#27700;&#39044;&#35686;&#30340;&#35206;&#30422;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27946;&#27700;&#26159;&#26368;&#24120;&#35265;&#21644;&#24433;&#21709;&#26368;&#22823;&#30340;&#33258;&#28982;&#28798;&#23475;&#20043;&#19968;&#65292;&#23545;&#21457;&#23637;&#20013;&#22269;&#23478;&#23588;&#20854;&#20855;&#26377;&#19981;&#23545;&#31216;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#22269;&#23478;&#24448;&#24448;&#32570;&#20047;&#23494;&#38598;&#30340;&#27700;&#27969;&#30417;&#27979;&#32593;&#32476;&#12290;&#20934;&#30830;&#21450;&#26102;&#30340;&#39044;&#35686;&#23545;&#20110;&#20943;&#36731;&#27946;&#27700;&#39118;&#38505;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20934;&#30830;&#30340;&#27700;&#25991;&#27169;&#25311;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#26681;&#25454;&#27599;&#20010;&#24212;&#29992;&#30340;&#27969;&#22495;&#20013;&#30340;&#38271;&#26102;&#38388;&#25968;&#25454;&#35760;&#24405;&#36827;&#34892;&#26657;&#20934;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;7&#22825;&#20869;&#30340;&#26497;&#31471;&#27700;&#25991;&#20107;&#20214;&#12290;&#35813;&#27169;&#22411;&#22312;&#25152;&#26377;&#22823;&#27954;&#12289;&#21069;&#23548;&#26102;&#38388;&#21644;&#37325;&#29616;&#26399;&#20013;&#22343;&#26126;&#26174;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20840;&#29699;&#27700;&#25991;&#27169;&#22411;&#65288;Copernicus&#24212;&#24613;&#31649;&#29702;&#26381;&#21153;&#20840;&#29699;&#27946;&#27700;&#24847;&#35782;&#31995;&#32479;&#65289;&#12290;AI&#22312;&#26410;&#32463;&#27979;&#37327;&#30340;&#27969;&#22495;&#20013;&#30340;&#39044;&#27979;&#23588;&#20854;&#26377;&#25928;&#65292;&#36825;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;&#20840;&#29699;&#21482;&#26377;&#30334;&#20998;&#20043;&#20960;&#30340;&#27969;&#22495;&#20855;&#26377;&#27969;&#37327;&#35266;&#27979;&#31449;&#65292;&#32780;&#21457;&#23637;&#20013;&#22269;&#23478;&#30340;&#26410;&#32463;&#27979;&#37327;&#30340;&#27969;&#22495;&#25968;&#37327;&#21344;&#27604;&#24456;&#39640;&#65292;&#23545;&#20154;&#31867;&#29305;&#21035;&#33030;&#24369;&#12290;
&lt;/p&gt;
&lt;p&gt;
Floods are one of the most common and impactful natural disasters, with a disproportionate impact in developing countries that often lack dense streamflow monitoring networks. Accurate and timely warnings are critical for mitigating flood risks, but accurate hydrological simulation models typically must be calibrated to long data records in each watershed where they are applied. We developed an Artificial Intelligence (AI) model to predict extreme hydrological events at timescales up to 7 days in advance. This model significantly outperforms current state of the art global hydrology models (the Copernicus Emergency Management Service Global Flood Awareness System) across all continents, lead times, and return periods. AI is especially effective at forecasting in ungauged basins, which is important because only a few percent of the world's watersheds have stream gauges, with a disproportionate number of ungauged basins in developing countries that are especially vulnerable to the human 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#27169;&#20223;&#23398;&#20064;&#20013;&#22240;&#26524;&#28151;&#28102;&#38382;&#39064;&#30340;&#21021;&#22987;&#21270;&#29366;&#24577;&#24178;&#39044;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#36974;&#34109;&#35266;&#27979;&#20013;&#30340;&#28151;&#28102;&#22240;&#32032;&#24182;&#25552;&#39640;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.15980</link><description>&lt;p&gt;
&#21021;&#22987;&#21270;&#29366;&#24577;&#24178;&#39044;&#23545;&#35299;&#20915;&#21435;&#38500;&#28151;&#28102;&#30340;&#27169;&#20223;&#23398;&#20064;&#38382;&#39064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Initial State Interventions for Deconfounded Imitation Learning. (arXiv:2307.15980v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#27169;&#20223;&#23398;&#20064;&#20013;&#22240;&#26524;&#28151;&#28102;&#38382;&#39064;&#30340;&#21021;&#22987;&#21270;&#29366;&#24577;&#24178;&#39044;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#36974;&#34109;&#35266;&#27979;&#20013;&#30340;&#28151;&#28102;&#22240;&#32032;&#24182;&#25552;&#39640;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#23384;&#22312;&#22240;&#26524;&#28151;&#28102;&#38382;&#39064;&#65292;&#21363;&#23398;&#20064;&#31574;&#30053;&#20851;&#27880;&#30340;&#29305;&#24449;&#24182;&#19981;&#22240;&#26524;&#22320;&#24433;&#21709;&#19987;&#23478;&#30340;&#34892;&#20026;&#65292;&#32780;&#26159;&#34920;&#38754;&#19978;&#30340;&#30456;&#20851;&#24615;&#12290;&#22240;&#26524;&#28151;&#28102;&#30340;&#26234;&#33021;&#20307;&#22312;&#35757;&#32451;&#20013;&#20135;&#29983;&#20102;&#20302;&#30340;&#24320;&#29615;&#30417;&#30563;&#25439;&#22833;&#65292;&#20294;&#22312;&#37096;&#32626;&#26102;&#34920;&#29616;&#20986;&#24046;&#30340;&#38381;&#29615;&#24615;&#33021;&#12290;&#25105;&#20204;&#32771;&#34385;&#22312;&#35266;&#27979;&#31354;&#38388;&#30340;&#20998;&#31163;&#34920;&#31034;&#20013;&#36974;&#34109;&#24050;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36974;&#34109;&#31639;&#27861;&#65292;&#21033;&#29992;&#20102;&#23545;&#21021;&#22987;&#31995;&#32479;&#29366;&#24577;&#36827;&#34892;&#24178;&#39044;&#30340;&#33021;&#21147;&#65292;&#36991;&#20813;&#20102;&#23545;&#19987;&#23478;&#26597;&#35810;&#12289;&#19987;&#23478;&#22870;&#21169;&#20989;&#25968;&#25110;&#22240;&#26524;&#22270;&#35268;&#33539;&#30340;&#20219;&#20309;&#35201;&#27714;&#12290;&#22312;&#19968;&#23450;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#26159;&#20445;&#23432;&#30340;&#65292;&#21363;&#19981;&#20250;&#38169;&#35823;&#22320;&#36974;&#34109;&#22240;&#26524;&#30456;&#20851;&#30340;&#35266;&#27979;&#65307;&#27492;&#22806;&#65292;&#23545;&#21021;&#22987;&#29366;&#24577;&#30340;&#24178;&#39044;&#33021;&#22815;&#20005;&#26684;&#20943;&#23569;&#36807;&#24230;&#20445;&#23432;&#24615;&#12290;&#35813;&#36974;&#34109;&#31639;&#27861;&#34987;&#24212;&#29992;&#20110;&#20004;&#20010;&#31034;&#20363;&#25511;&#21046;&#31995;&#32479;&#30340;&#34892;&#20026;&#20811;&#38534;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning suffers from causal confusion. This phenomenon occurs when learned policies attend to features that do not causally influence the expert actions but are instead spuriously correlated. Causally confused agents produce low open-loop supervised loss but poor closed-loop performance upon deployment. We consider the problem of masking observed confounders in a disentangled representation of the observation space. Our novel masking algorithm leverages the usual ability to intervene in the initial system state, avoiding any requirement involving expert querying, expert reward functions, or causal graph specification. Under certain assumptions, we theoretically prove that this algorithm is conservative in the sense that it does not incorrectly mask observations that causally influence the expert; furthermore, intervening on the initial state serves to strictly reduce excess conservatism. The masking algorithm is applied to behavior cloning for two illustrative control system
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#38450;&#24481;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#34987;&#27745;&#26579;&#26679;&#26412;&#20013;&#27880;&#20837;&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#65292;&#24403;&#35302;&#21457;&#26102;&#21487;&#20197;&#25233;&#21046;&#25915;&#20987;&#32773;&#23545;&#27745;&#26579;&#25968;&#25454;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#24178;&#20928;&#25968;&#25454;&#30340;&#24433;&#21709;&#26377;&#38480;&#12290;</title><link>http://arxiv.org/abs/2307.15539</link><description>&lt;p&gt;
&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Backdoor Defense with Non-Adversarial Backdoor. (arXiv:2307.15539v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15539
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#38450;&#24481;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#34987;&#27745;&#26579;&#26679;&#26412;&#20013;&#27880;&#20837;&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#65292;&#24403;&#35302;&#21457;&#26102;&#21487;&#20197;&#25233;&#21046;&#25915;&#20987;&#32773;&#23545;&#27745;&#26579;&#25968;&#25454;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#24178;&#20928;&#25968;&#25454;&#30340;&#24433;&#21709;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36825;&#31181;&#25915;&#20987;&#24182;&#19981;&#20250;&#24433;&#21709;&#32593;&#32476;&#23545;&#24178;&#20928;&#25968;&#25454;&#30340;&#24615;&#33021;&#65292;&#20294;&#19968;&#26086;&#28155;&#21152;&#35302;&#21457;&#27169;&#24335;&#65292;&#23601;&#20250;&#25805;&#32437;&#32593;&#32476;&#34892;&#20026;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#22823;&#22823;&#38477;&#20302;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#20294;&#23427;&#20204;&#22312;&#24178;&#20928;&#25968;&#25454;&#19978;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#20173;&#28982;&#36828;&#36828;&#33853;&#21518;&#20110;&#24178;&#20928;&#27169;&#22411;&#12290;&#21463;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#21644;&#26377;&#25928;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#38750;&#24120;&#26377;&#25928;&#30340;&#38450;&#24481;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#27880;&#20837;&#20102;&#38024;&#23545;&#34987;&#27745;&#26579;&#26679;&#26412;&#30340;&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#12290;&#25353;&#29031;&#21518;&#38376;&#25915;&#20987;&#30340;&#19968;&#33324;&#27493;&#39588;&#65292;&#25105;&#20204;&#26816;&#27979;&#19968;&#23567;&#32452;&#21487;&#30097;&#26679;&#26412;&#65292;&#28982;&#21518;&#23545;&#23427;&#20204;&#24212;&#29992;&#27602;&#21270;&#31574;&#30053;&#12290;&#19968;&#26086;&#35302;&#21457;&#65292;&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#25233;&#21046;&#20102;&#25915;&#20987;&#32773;&#23545;&#27745;&#26579;&#25968;&#25454;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20294;&#23545;&#24178;&#20928;&#25968;&#25454;&#30340;&#24433;&#21709;&#26377;&#38480;&#12290;&#38450;&#24481;&#21487;&#20197;&#22312;&#25968;&#25454;&#39044;&#22788;&#29702;&#26399;&#38388;&#36827;&#34892;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;&#26631;&#20934;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#27969;&#31243;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are vulnerable to backdoor attack, which does not affect the network's performance on clean data but would manipulate the network behavior once a trigger pattern is added. Existing defense methods have greatly reduced attack success rate, but their prediction accuracy on clean data still lags behind a clean model by a large margin. Inspired by the stealthiness and effectiveness of backdoor attack, we propose a simple but highly effective defense framework which injects non-adversarial backdoors targeting poisoned samples. Following the general steps in backdoor attack, we detect a small set of suspected samples and then apply a poisoning strategy to them. The non-adversarial backdoor, once triggered, suppresses the attacker's backdoor on poisoned data, but has limited influence on clean data. The defense can be carried out during data preprocessing, without any modification to the standard end-to-end training pipeline. We conduct extensive experiments on mul
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#24046;&#20998;&#36827;&#21270;&#31639;&#27861;&#36873;&#25321;Transformer&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#20248;&#21270;&#36229;&#21442;&#25968;&#65292;&#20197;&#25552;&#39640;&#36127;&#33655;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15299</link><description>&lt;p&gt;
&#22522;&#20110;&#24046;&#20998;&#36827;&#21270;&#31639;&#27861;&#30340;Transformer&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#29992;&#20110;&#36127;&#33655;&#39044;&#27979;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Differential Evolution Algorithm based Hyper-Parameters Selection of Transformer Neural Network Model for Load Forecasting. (arXiv:2307.15299v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#24046;&#20998;&#36827;&#21270;&#31639;&#27861;&#36873;&#25321;Transformer&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#20248;&#21270;&#36229;&#21442;&#25968;&#65292;&#20197;&#25552;&#39640;&#36127;&#33655;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#30340;&#36127;&#33655;&#39044;&#27979;&#22312;&#20247;&#22810;&#39046;&#22495;&#37117;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#20934;&#30830;&#25429;&#25417;&#21160;&#21147;&#31995;&#32479;&#30340;&#22797;&#26434;&#21160;&#24577;&#20173;&#28982;&#26159;&#20256;&#32479;&#32479;&#35745;&#27169;&#22411;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65288;ARIMA&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;ANN&#65292;LSTM&#65292;GRU&#31561;&#65289;&#32463;&#24120;&#34987;&#20351;&#29992;&#65292;&#24182;&#19988;&#36890;&#24120;&#33021;&#22815;&#21462;&#24471;&#26356;&#22909;&#30340;&#25104;&#21151;&#29575;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#26368;&#36817;&#24320;&#21457;&#30340;Transformer-based&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#36127;&#33655;&#39044;&#27979;&#20013;&#30340;&#25928;&#26524;&#12290;Transformer&#27169;&#22411;&#26377;&#26395;&#25913;&#36827;&#36127;&#33655;&#39044;&#27979;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#36890;&#36807;&#20854;Attention&#26426;&#21046;&#23398;&#20064;&#21040;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#36816;&#29992;&#20102;&#20960;&#31181;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#22914;&#24046;&#20998;&#36827;&#21270;&#65292;&#20197;&#23547;&#25214;Transformer-based&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20248;&#36229;&#21442;&#25968;&#65292;&#20197;&#20135;&#29983;&#31934;&#30830;&#30340;&#39044;&#27979;&#12290;&#24046;&#20998;&#36827;&#21270;&#20026;&#38750;&#21487;&#24494;&#20998;&#12289;&#22810;&#30446;&#26631;&#25110;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#12289;&#24378;&#20581;&#21644;&#20840;&#23616;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#27604;&#36739;&#20102;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#19982;&#20854;&#20182;&#27169;&#22411;&#22312;&#36127;&#33655;&#39044;&#27979;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate load forecasting plays a vital role in numerous sectors, but accurately capturing the complex dynamics of dynamic power systems remains a challenge for traditional statistical models. For these reasons, time-series models (ARIMA) and deep-learning models (ANN, LSTM, GRU, etc.) are commonly deployed and often experience higher success. In this paper, we analyze the efficacy of the recently developed Transformer-based Neural Network model in Load forecasting. Transformer models have the potential to improve Load forecasting because of their ability to learn long-range dependencies derived from their Attention Mechanism. We apply several metaheuristics namely Differential Evolution to find the optimal hyperparameters of the Transformer-based Neural Network to produce accurate forecasts. Differential Evolution provides scalable, robust, global solutions to non-differentiable, multi-objective, or constrained optimization problems. Our work compares the proposed Transformer based Ne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30830;&#23450;&#26368;&#23567;&#21442;&#25968;&#38598;&#65292;&#26377;&#25928;&#25551;&#36848;&#38543;&#26426;&#36807;&#31243;&#21160;&#21147;&#23398;&#65292;&#24182;&#29983;&#25104;&#33021;&#20934;&#30830;&#22797;&#21046;&#39044;&#26399;&#38543;&#26426;&#34892;&#20026;&#30340;&#26032;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2307.11608</link><description>&lt;p&gt;
&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#23398;&#20064;&#38543;&#26426;&#36807;&#31243;&#30340;&#26368;&#23567;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning minimal representations of stochastic processes with variational autoencoders. (arXiv:2307.11608v1 [cond-mat.soft])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30830;&#23450;&#26368;&#23567;&#21442;&#25968;&#38598;&#65292;&#26377;&#25928;&#25551;&#36848;&#38543;&#26426;&#36807;&#31243;&#21160;&#21147;&#23398;&#65292;&#24182;&#29983;&#25104;&#33021;&#20934;&#30830;&#22797;&#21046;&#39044;&#26399;&#38543;&#26426;&#34892;&#20026;&#30340;&#26032;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#36807;&#31243;&#22312;&#31185;&#23398;&#20013;&#26377;&#35768;&#22810;&#24212;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#24191;&#27867;&#29992;&#20110;&#27169;&#25311;&#21508;&#31181;&#33258;&#28982;&#29616;&#35937;&#12290;&#30001;&#20110;&#20854;&#22266;&#26377;&#30340;&#38543;&#26426;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#23427;&#20204;&#24456;&#38590;&#36827;&#34892;&#34920;&#24449;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#26377;&#25928;&#25551;&#36848;&#38543;&#26426;&#36807;&#31243;&#21160;&#21147;&#23398;&#25152;&#38656;&#30340;&#26368;&#23567;&#21442;&#25968;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24314;&#31435;&#22312;&#25193;&#23637;&#30340;&#946;-&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26550;&#26500;&#19978;&#12290;&#36890;&#36807;&#19982;&#20856;&#22411;&#25193;&#25955;&#27169;&#22411;&#30456;&#23545;&#24212;&#30340;&#27169;&#25311;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#22312;&#25552;&#21462;&#33021;&#20934;&#30830;&#25551;&#36848;&#36825;&#20123;&#21160;&#21147;&#23398;&#30340;&#26368;&#23567;&#30456;&#20851;&#21442;&#25968;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#24544;&#23454;&#22797;&#21046;&#39044;&#26399;&#38543;&#26426;&#34892;&#20026;&#30340;&#26032;&#36712;&#36857;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#24471;&#33021;&#22815;&#33258;&#21160;&#21457;&#29616;&#25551;&#36848;&#38543;&#26426;&#36807;&#31243;&#30340;&#26410;&#30693;&#21442;&#25968;&#65292;&#20174;&#32780;&#22686;&#36827;&#23545;&#21508;&#20010;&#39046;&#22495;&#20013;&#22797;&#26434;&#29616;&#35937;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic processes have found numerous applications in science, as they are broadly used to model a variety of natural phenomena. Due to their intrinsic randomness and uncertainty, they are however difficult to characterize. Here, we introduce an unsupervised machine learning approach to determine the minimal set of parameters required to effectively describe the dynamics of a stochastic process. Our method builds upon an extended $\beta$-variational autoencoder architecture. By means of simulated datasets corresponding to paradigmatic diffusion models, we showcase its effectiveness in extracting the minimal relevant parameters that accurately describe these dynamics. Furthermore, the method enables the generation of new trajectories that faithfully replicate the expected stochastic behavior. Overall, our approach enables for the autonomous discovery of unknown parameters describing stochastic processes, hence enhancing our comprehension of complex phenomena across various fields.
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#19987;&#23478;&#24314;&#35758;&#21644;&#22810;&#33218;&#36172;&#21338;&#26159;&#20004;&#20010;&#32463;&#20856;&#30340;&#22312;&#32447;&#20915;&#31574;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#32773;&#20043;&#38388;&#30340;&#25554;&#20540;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;$\mathbf{m}$-MAB&#30340;&#26497;&#23567;&#21518;&#24724;&#30028;&#24182;&#35774;&#35745;&#20102;$\mathbf{m}$-BAI&#30340;&#26368;&#20248;PAC&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26088;&#22312;&#20197;&#23613;&#21487;&#33021;&#23569;&#30340;&#36718;&#25968;&#30830;&#23450;&#25439;&#22833;&#26368;&#23567;&#30340;&#33218;&#12290;</title><link>http://arxiv.org/abs/2307.07264</link><description>&lt;p&gt;
&#20851;&#20110;&#25554;&#20540;&#19987;&#23478;&#21644;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Interpolating Experts and Multi-Armed Bandits. (arXiv:2307.07264v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07264
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#19987;&#23478;&#24314;&#35758;&#21644;&#22810;&#33218;&#36172;&#21338;&#26159;&#20004;&#20010;&#32463;&#20856;&#30340;&#22312;&#32447;&#20915;&#31574;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#32773;&#20043;&#38388;&#30340;&#25554;&#20540;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;$\mathbf{m}$-MAB&#30340;&#26497;&#23567;&#21518;&#24724;&#30028;&#24182;&#35774;&#35745;&#20102;$\mathbf{m}$-BAI&#30340;&#26368;&#20248;PAC&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26088;&#22312;&#20197;&#23613;&#21487;&#33021;&#23569;&#30340;&#36718;&#25968;&#30830;&#23450;&#25439;&#22833;&#26368;&#23567;&#30340;&#33218;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#19987;&#23478;&#24314;&#35758;&#21644;&#22810;&#33218;&#36172;&#21338;&#26159;&#20004;&#20010;&#32463;&#20856;&#30340;&#22312;&#32447;&#20915;&#31574;&#38382;&#39064;&#65292;&#23427;&#20204;&#22312;&#27599;&#19968;&#36718;&#35266;&#23519;&#20449;&#24687;&#30340;&#26041;&#24335;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20004;&#32773;&#20043;&#38388;&#30340;&#25554;&#20540;&#38382;&#39064;&#12290;&#23545;&#20110;&#21521;&#37327;$\mathbf{m}=(m_1,\dots,m_K)\in \mathbb{N}^K$&#65292;$\mathbf{m}$-MAB&#30340;&#19968;&#20010;&#23454;&#20363;&#34920;&#31034;&#23558;&#33218;&#20998;&#25104;$K$&#32452;&#65292;&#31532;$i$&#32452;&#21253;&#21547;$m_i$&#20010;&#33218;&#12290;&#19968;&#26086;&#25289;&#21160;&#19968;&#20010;&#33218;&#65292;&#21516;&#19968;&#32452;&#20013;&#25152;&#26377;&#33218;&#30340;&#25439;&#22833;&#37117;&#34987;&#35266;&#23519;&#21040;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;$\mathbf{m}$-MAB&#30340;&#32039;&#33268;&#26497;&#23567;&#21518;&#24724;&#30028;&#65292;&#24182;&#20026;&#20854;&#32431;&#25506;&#32034;&#29256;&#26412;$\mathbf{m}$-BAI&#35774;&#35745;&#20102;&#19968;&#20010;&#26368;&#20248;&#30340;PAC&#31639;&#27861;&#65292;&#20854;&#20013;&#30446;&#26631;&#26159;&#29992;&#23613;&#21487;&#33021;&#23569;&#30340;&#36718;&#25968;&#26469;&#35782;&#21035;&#25439;&#22833;&#26368;&#23567;&#30340;&#33218;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;$\mathbf{m}$-MAB&#30340;&#26497;&#23567;&#21518;&#24724;&#26159;$\Theta\left(\sqrt{T\sum_{k=1}^K\log (m_k+1)}\right)$&#65292;&#23545;&#20110;&#19968;&#20010;$(\epsilon,0.05)$-PAC&#31639;&#27861;&#30340;$\mathbf{m}$-BAI&#65292;&#25289;&#21160;&#33218;&#30340;&#26368;&#23567;&#27425;&#25968;&#26159;$\Theta\left(\frac{1}{\epsilon^2}\cdot \sum_{k=1}^K\log (m_k+1)\right)$&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning with expert advice and multi-armed bandit are two classic online decision problems which differ on how the information is observed in each round of the game. We study a family of problems interpolating the two. For a vector $\mathbf{m}=(m_1,\dots,m_K)\in \mathbb{N}^K$, an instance of $\mathbf{m}$-MAB indicates that the arms are partitioned into $K$ groups and the $i$-th group contains $m_i$ arms. Once an arm is pulled, the losses of all arms in the same group are observed. We prove tight minimax regret bounds for $\mathbf{m}$-MAB and design an optimal PAC algorithm for its pure exploration version, $\mathbf{m}$-BAI, where the goal is to identify the arm with minimum loss with as few rounds as possible. We show that the minimax regret of $\mathbf{m}$-MAB is $\Theta\left(\sqrt{T\sum_{k=1}^K\log (m_k+1)}\right)$ and the minimum number of pulls for an $(\epsilon,0.05)$-PAC algorithm of $\mathbf{m}$-BAI is $\Theta\left(\frac{1}{\epsilon^2}\cdot \sum_{k=1}^K\log (m_k+1)\right)$. Bot
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#33258;&#21160;&#28040;&#38500;&#24207;&#21015;&#20887;&#20313;&#20197;&#23454;&#29616;&#20986;&#33394;&#30340;&#21387;&#32553;&#27604;&#21644;&#35299;&#21387;&#32553;&#24615;&#33021;&#30340;LeCo&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#21387;&#32553;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#24207;&#21015;&#30456;&#20851;&#24615;&#65292;&#23427;&#33021;&#22815;&#22312;&#21387;&#32553;&#27604;&#21644;&#38543;&#26426;&#35775;&#38382;&#36895;&#24230;&#19978;&#23454;&#29616;&#24085;&#32047;&#25176;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.15374</link><description>&lt;p&gt;
LeCo&#65306;&#36890;&#36807;&#23398;&#20064;&#24207;&#21015;&#30456;&#20851;&#24615;&#23454;&#29616;&#36731;&#37327;&#32423;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
LeCo: Lightweight Compression via Learning Serial Correlations. (arXiv:2306.15374v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15374
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#33258;&#21160;&#28040;&#38500;&#24207;&#21015;&#20887;&#20313;&#20197;&#23454;&#29616;&#20986;&#33394;&#30340;&#21387;&#32553;&#27604;&#21644;&#35299;&#21387;&#32553;&#24615;&#33021;&#30340;LeCo&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#21387;&#32553;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#24207;&#21015;&#30456;&#20851;&#24615;&#65292;&#23427;&#33021;&#22815;&#22312;&#21387;&#32553;&#27604;&#21644;&#38543;&#26426;&#35775;&#38382;&#36895;&#24230;&#19978;&#23454;&#29616;&#24085;&#32047;&#25176;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36731;&#37327;&#32423;&#25968;&#25454;&#21387;&#32553;&#26159;&#19968;&#31181;&#20851;&#38190;&#25216;&#26415;&#65292;&#23427;&#20351;&#24471;&#21015;&#24335;&#23384;&#20648;&#22312;&#20998;&#26512;&#26597;&#35810;&#26041;&#38754;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#20043;&#21069;&#26377;&#20851;&#22522;&#20110;&#23383;&#20856;&#32534;&#30721;&#26469;&#36924;&#36817;Shannon&#29109;&#30340;&#30740;&#31350;&#24050;&#32463;&#24456;&#20840;&#38754;&#65292;&#20294;&#40092;&#26377;&#20043;&#21069;&#30340;&#24037;&#20316;&#31995;&#32479;&#22320;&#21033;&#29992;&#21015;&#30340;&#24207;&#21015;&#30456;&#20851;&#24615;&#26469;&#36827;&#34892;&#21387;&#32553;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LeCo&#65288;&#21363;&#23398;&#20064;&#21387;&#32553;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#33258;&#21160;&#28040;&#38500;&#24207;&#21015;&#20887;&#20313;&#20197;&#23454;&#29616;&#20986;&#33394;&#30340;&#21387;&#32553;&#27604;&#21644;&#35299;&#21387;&#32553;&#24615;&#33021;&#30340;&#26694;&#26550;&#12290;LeCo&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#19979;&#65292;&#29616;&#26377;&#30340;&#65288;&#20020;&#26102;&#30340;&#65289;&#31639;&#27861;&#65292;&#22914;&#21442;&#32771;&#24103;&#65288;Frame-of-Reference&#65289;&#65292;Delta&#32534;&#30721;&#21644;&#28216;&#31243;&#32534;&#30721;&#65288;Run-Length Encoding&#65289;&#37117;&#26159;&#29305;&#20363;&#12290;&#25105;&#20204;&#23545;&#19977;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20845;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#24494;&#22522;&#20934;&#27979;&#35797;&#26174;&#31034;&#65292;LeCo&#21407;&#22411;&#22312;&#21387;&#32553;&#27604;&#21644;&#38543;&#26426;&#35775;&#38382;&#36895;&#24230;&#19978;&#30456;&#27604;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#21462;&#24471;&#20102;&#24085;&#32047;&#25176;&#25913;&#36827;&#12290;&#24403;&#23558;LeCo&#38598;&#25104;&#26102;
&lt;/p&gt;
&lt;p&gt;
Lightweight data compression is a key technique that allows column stores to exhibit superior performance for analytical queries. Despite a comprehensive study on dictionary-based encodings to approach Shannon's entropy, few prior works have systematically exploited the serial correlation in a column for compression. In this paper, we propose LeCo (i.e., Learned Compression), a framework that uses machine learning to remove the serial redundancy in a value sequence automatically to achieve an outstanding compression ratio and decompression performance simultaneously. LeCo presents a general approach to this end, making existing (ad-hoc) algorithms such as Frame-of-Reference (FOR), Delta Encoding, and Run-Length Encoding (RLE) special cases under our framework. Our microbenchmark with three synthetic and six real-world data sets shows that a prototype of LeCo achieves a Pareto improvement on both compression ratio and random access speed over the existing solutions. When integrating LeC
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#25513;&#30721;Token Transformer&#30340;&#32852;&#21512;&#20449;&#36947;&#20272;&#35745;&#21644;&#21453;&#39304;&#26694;&#26550;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#22823;&#35268;&#27169;MIMO&#31995;&#32479;&#20013;&#20449;&#36947;&#20272;&#35745;&#21644;&#21453;&#39304;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.06125</link><description>&lt;p&gt;
&#22522;&#20110;&#25513;&#30721;Token Transformer&#30340;&#22823;&#35268;&#27169;MIMO&#31995;&#32479;&#32852;&#21512;&#20449;&#36947;&#20272;&#35745;&#21644;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Joint Channel Estimation and Feedback with Masked Token Transformers in Massive MIMO Systems. (arXiv:2306.06125v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#25513;&#30721;Token Transformer&#30340;&#32852;&#21512;&#20449;&#36947;&#20272;&#35745;&#21644;&#21453;&#39304;&#26694;&#26550;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#22823;&#35268;&#27169;MIMO&#31995;&#32479;&#20013;&#20449;&#36947;&#20272;&#35745;&#21644;&#21453;&#39304;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22522;&#31449;&#20855;&#26377;&#19979;&#34892;&#36890;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#26102;&#65292;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;&#22823;&#35268;&#27169;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#65288;MIMO&#65289;&#22312;&#39057;&#20998;&#21452;&#24037;&#65288;FDD&#65289;&#27169;&#24335;&#19979;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#32852;&#21512;&#20449;&#36947;&#20272;&#35745;&#21644;&#21453;&#39304;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#22823;&#35268;&#27169;MIMO&#31995;&#32479;&#20013;&#30340;&#20449;&#36947;&#20272;&#35745;&#21644;&#21453;&#39304;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#20256;&#32479;&#36890;&#36947;&#35774;&#35745;&#32780;&#38750;&#31471;&#21040;&#31471;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21253;&#21547;&#20004;&#20010;&#32593;&#32476;&#12290;&#31532;&#19968;&#20010;&#32593;&#32476;&#26159;&#20449;&#36947;&#20272;&#35745;&#32593;&#32476;&#65292;&#37319;&#29992;&#21452;&#37325;&#25439;&#22833;&#35774;&#35745;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#20272;&#35745;&#23436;&#25972;&#30340;&#20449;&#36947;&#20449;&#24687;&#24182;&#28040;&#38500;&#20449;&#36947;&#22122;&#22768;&#12290;&#31532;&#20108;&#20010;&#32593;&#32476;&#26159;&#21387;&#32553;&#21644;&#21453;&#39304;&#32593;&#32476;&#12290;&#21463;&#25513;&#30721;Token Transformer&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#25513;&#30721;Token&#26041;&#27861;&#65292;&#20197;&#33719;&#24471;&#20986;&#33394;&#30340;&#20272;&#35745;&#21644;&#21387;&#32553;&#24615;&#33021;&#12290;&#24191;&#27867;&#30340;&#20223;&#30495;&#32467;&#26524;&#21644;&#21066;&#24369;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20449;&#36947;&#20272;&#35745;&#21644;&#21453;&#39304;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
When the base station has downlink channel status information (CSI), the huge potential of large-scale multiple input multiple output (MIMO) in frequency division duplex (FDD) mode can be fully exploited. In this paper, we propose a deep-learning-based joint channel estimation and feedback framework to realize channel estimation and feedback in massive MIMO systems. Specifically, we use traditional channel design rather than end-to-end methods. Our model contains two networks. The first network is a channel estimation network, which adopts a double loss design, and can accurately estimate the full channel information while removing channel noises. The second network is a compression and feedback network. Inspired by the masked token transformer, we propose a learnable mask token method to obtain excellent estimation and compression performance. The extensive simulation results and ablation studies show that our method outperforms state-of-the-art channel estimation and feedback methods
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#21033;&#29992;&#23494;&#24230;&#20989;&#25968;&#30340;&#38750;&#32447;&#24615;&#21464;&#25442;&#23545;&#23725;&#30340;&#20272;&#35745;&#65292;&#24182;&#21457;&#29616;&#20854;&#21487;&#20197;&#25913;&#36827;&#23545;&#20999;&#31354;&#38388;&#30340;&#20272;&#35745;&#20197;&#21450;&#22312;&#36817;&#20284;&#24213;&#23618;&#30495;&#23454;&#27969;&#24418;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#27969;&#24418;&#25311;&#21512;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.05722</link><description>&lt;p&gt;
&#21033;&#29992;&#23494;&#24230;&#20989;&#25968;&#30340;&#38750;&#32447;&#24615;&#21464;&#25442;&#20272;&#35745;&#23725;
&lt;/p&gt;
&lt;p&gt;
Estimation of Ridge Using Nonlinear Transformation on Density Function. (arXiv:2306.05722v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#21033;&#29992;&#23494;&#24230;&#20989;&#25968;&#30340;&#38750;&#32447;&#24615;&#21464;&#25442;&#23545;&#23725;&#30340;&#20272;&#35745;&#65292;&#24182;&#21457;&#29616;&#20854;&#21487;&#20197;&#25913;&#36827;&#23545;&#20999;&#31354;&#38388;&#30340;&#20272;&#35745;&#20197;&#21450;&#22312;&#36817;&#20284;&#24213;&#23618;&#30495;&#23454;&#27969;&#24418;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#27969;&#24418;&#25311;&#21512;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23725;&#22312;&#20934;&#30830;&#36817;&#20284;&#27969;&#24418;&#30340;&#22522;&#30784;&#32467;&#26500;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#20985;&#38750;&#32447;&#24615;&#21464;&#25442;&#24212;&#29992;&#20110;&#23494;&#24230;&#20989;&#25968;&#20197;&#25506;&#32034;&#23725;&#30340;&#21464;&#21270;&#12290;&#36890;&#36807;&#23545;Hessian&#30697;&#38453;&#30340;&#25512;&#23548;&#21644;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#38750;&#32447;&#24615;&#21464;&#25442;&#20135;&#29983;&#20102;Hessian&#30697;&#38453;&#30340;&#31209;&#19968;&#20462;&#25913;&#12290;&#21033;&#29992;&#29305;&#24449;&#20540;&#38382;&#39064;&#30340;&#21464;&#20998;&#24615;&#36136;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#30456;&#24212;&#23725;&#20043;&#38388;&#30340;&#20559;&#24207;&#21253;&#21547;&#20851;&#31995;&#12290;&#25105;&#20204;&#30452;&#35266;&#22320;&#21457;&#29616;&#65292;&#36890;&#36807;Hessian&#30697;&#38453;&#30340;&#31209;&#19968;&#20462;&#25913;&#65292;&#21464;&#25442;&#21487;&#20197;&#23548;&#33268;&#23545;&#20999;&#31354;&#38388;&#30340;&#20272;&#35745;&#25913;&#36827;&#12290;&#20026;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#25968;&#20540;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#19982;&#20854;&#20182;&#27969;&#24418;&#25311;&#21512;&#31639;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#21464;&#25442;&#26041;&#27861;&#24471;&#21040;&#30340;&#23725;&#22312;&#36817;&#20284;&#24213;&#23618;&#30495;&#23454;&#27969;&#24418;&#26041;&#38754;&#26356;&#21152;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ridges play a vital role in accurately approximating the underlying structure of manifolds. In this paper, we explore the ridge's variation by applying a concave nonlinear transformation to the density function. Through the derivation of the Hessian matrix, we observe that nonlinear transformations yield a rank-one modification of the Hessian matrix. Leveraging the variational properties of eigenvalue problems, we establish a partial order inclusion relationship among the corresponding ridges. We intuitively discover that the transformation can lead to improved estimation of the tangent space via rank-one modification of the Hessian matrix. To validate our theories, we conduct extensive numerical experiments on synthetic and real-world datasets that demonstrate the superiority of the ridges obtained from our transformed approach in approximating the underlying truth manifold compared to other manifold fitting algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#28145;&#24230;&#23398;&#20064;&#30340;&#20837;&#20405;&#26816;&#27979;&#26694;&#26550;&#65292;&#21487;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#21644;&#23616;&#37096;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#39640;&#25928;&#30340;&#27169;&#22411;&#25910;&#25947;&#65292;&#35813;&#26694;&#26550;&#22312;&#29289;&#32852;&#32593;&#20837;&#20405;&#26816;&#27979;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.02715</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#28145;&#24230;&#23398;&#20064;&#30340;&#29289;&#32852;&#32593;&#20837;&#20405;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Federated Deep Learning for Intrusion Detection in IoT Networks. (arXiv:2306.02715v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#28145;&#24230;&#23398;&#20064;&#30340;&#20837;&#20405;&#26816;&#27979;&#26694;&#26550;&#65292;&#21487;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#21644;&#23616;&#37096;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#39640;&#25928;&#30340;&#27169;&#22411;&#25910;&#25947;&#65292;&#35813;&#26694;&#26550;&#22312;&#29289;&#32852;&#32593;&#20837;&#20405;&#26816;&#27979;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#25216;&#26415;&#30340;&#22823;&#24133;&#22686;&#38271;&#21644;&#25915;&#20987;&#21521;&#37327;&#21644;&#23041;&#32961;&#34892;&#20026;&#30340;&#19981;&#26029;&#28436;&#21270;&#65292;&#26174;&#33879;&#22686;&#21152;&#20102;&#32593;&#32476;&#23433;&#20840;&#39118;&#38505;&#12290;&#26032;&#22411;&#25915;&#20987;&#21487;&#33021;&#20250;&#21361;&#21450;&#29289;&#32852;&#32593;&#35774;&#22791;&#65292;&#20197;&#33719;&#21462;&#23545;&#25935;&#24863;&#25968;&#25454;&#30340;&#35775;&#38382;&#25110;&#25511;&#21046;&#23427;&#20204;&#20197;&#37096;&#32626;&#36827;&#19968;&#27493;&#30340;&#24694;&#24847;&#27963;&#21160;&#12290;&#26816;&#27979;&#26032;&#22411;&#25915;&#20987;&#36890;&#24120;&#20381;&#36182;&#20110;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#20998;&#24067;&#24335;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#23454;&#29616;&#22522;&#20110;AI&#30340;&#20837;&#20405;&#26816;&#27979;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#20197;&#38598;&#20013;&#26041;&#24335;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#27492;&#26041;&#27861;&#21487;&#33021;&#20250;&#20405;&#29359;&#25968;&#25454;&#38544;&#31169;&#21644;&#20445;&#23494;&#24615;&#12290;&#27492;&#22806;&#65292;&#38598;&#20013;&#24335;&#25968;&#25454;&#37319;&#38598;&#31105;&#27490;IDS&#30340;&#25193;&#23637;&#12290;&#22240;&#27492;&#65292;&#29289;&#32852;&#32593;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#20837;&#20405;&#26816;&#27979;&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#26397;&#20998;&#25955;&#21270;&#26041;&#21521;&#21457;&#23637;&#12290;&#32852;&#37030;&#23398;&#20064;&#30001;&#20110;&#33021;&#22815;&#22312;&#20445;&#30041;&#25968;&#25454;&#26426;&#23494;&#24615;&#21644;&#23616;&#37096;&#24615;&#30340;&#21516;&#26102;&#36827;&#34892;&#21327;&#20316;&#23398;&#20064;&#32780;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#29289;&#32852;&#32593;&#31995;&#32479;IDS&#37117;&#26159;&#22312;&#19981;&#20999;&#23454;&#38469;&#30340;&#25968;&#25454;&#20998;&#24067;&#26465;&#20214;&#19979;&#35774;&#35745;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20195;&#34920;&#29289;&#32852;&#32593;&#29983;&#24577;&#31995;&#32479;&#20013;&#23454;&#38469;&#25968;&#25454;&#20998;&#24067;&#30340;&#23454;&#39564;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#28145;&#24230;&#23398;&#20064;&#65288;FDL&#65289;&#26694;&#26550;&#30340;&#29289;&#32852;&#32593;&#20837;&#20405;&#26816;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25972;&#21512;&#20102;CNN&#27169;&#22411;&#21644;&#38376;&#25511;&#26426;&#21046;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#27169;&#22411;&#25910;&#25947;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#21644;&#23616;&#37096;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;FDL&#26694;&#26550;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#34920;&#26126;&#20854;&#22312;&#23454;&#38469;&#29289;&#32852;&#32593;&#20837;&#20405;&#26816;&#27979;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The vast increase of IoT technologies and the ever-evolving attack vectors and threat actors have increased cyber-security risks dramatically. Novel attacks can compromise IoT devices to gain access to sensitive data or control them to deploy further malicious activities. The detection of novel attacks often relies upon AI solutions. A common approach to implementing AI-based IDS in distributed IoT systems is in a centralised manner. However, this approach may violate data privacy and secrecy. In addition, centralised data collection prohibits the scale-up of IDSs. Therefore, intrusion detection solutions in IoT ecosystems need to move towards a decentralised direction. FL has attracted significant interest in recent years due to its ability to perform collaborative learning while preserving data confidentiality and locality. Nevertheless, most FL-based IDS for IoT systems are designed under unrealistic data distribution conditions. To that end, we design an experiment representative o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#20154;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#31454;&#20105;&#21487;&#20998;&#20139;&#30340;&#25163;&#33218;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#31169;MPMAB&#19982;&#20998;&#37197;&#24179;&#22343;&#31639;&#27861;&#65288;SMAA&#65289;&#26041;&#27861;&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#22312;&#29609;&#23478;&#36981;&#24490;&#35813;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#21518;&#24724;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#27809;&#26377;&#21333;&#20010;&#33258;&#31169;&#30340;&#29609;&#23478;&#33021;&#36890;&#36807;&#20559;&#31163;&#26174;&#33879;&#22686;&#21152;&#20182;&#20204;&#30340;&#22238;&#25253;&#12290;</title><link>http://arxiv.org/abs/2305.19158</link><description>&lt;p&gt;
&#22312;&#22810;&#20154;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#31454;&#20105;&#21487;&#20998;&#20139;&#30340;&#25163;&#33218;
&lt;/p&gt;
&lt;p&gt;
Competing for Shareable Arms in Multi-Player Multi-Armed Bandits. (arXiv:2305.19158v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#20154;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#31454;&#20105;&#21487;&#20998;&#20139;&#30340;&#25163;&#33218;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#31169;MPMAB&#19982;&#20998;&#37197;&#24179;&#22343;&#31639;&#27861;&#65288;SMAA&#65289;&#26041;&#27861;&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#22312;&#29609;&#23478;&#36981;&#24490;&#35813;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#21518;&#24724;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#27809;&#26377;&#21333;&#20010;&#33258;&#31169;&#30340;&#29609;&#23478;&#33021;&#36890;&#36807;&#20559;&#31163;&#26174;&#33879;&#22686;&#21152;&#20182;&#20204;&#30340;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#31454;&#20105;&#21487;&#20998;&#20139;&#21644;&#26377;&#38480;&#36164;&#28304;&#30340;&#30740;&#31350;&#19968;&#30452;&#26159;&#25112;&#30053;&#24615;&#20195;&#29702;&#20154;&#30340;&#20027;&#39064;&#12290;&#22312;&#29616;&#23454;&#20013;&#65292;&#20195;&#29702;&#20154;&#32463;&#24120;&#38656;&#35201;&#21516;&#26102;&#23398;&#20064;&#21644;&#26368;&#22823;&#21270;&#36164;&#28304;&#30340;&#22238;&#25253;&#12290;&#20026;&#20102;&#35774;&#35745;&#20010;&#24615;&#21270;&#30340;&#31454;&#20105;&#31574;&#30053;&#65292;&#25105;&#20204;&#22312;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20154;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MPMAB&#65289;&#35774;&#32622;&#20013;&#24314;&#27169;&#20102;&#20195;&#29702;&#20154;&#20043;&#38388;&#30340;&#31454;&#20105;&#65292;&#20854;&#20013;&#29609;&#23478;&#33258;&#31169;&#24182;&#26088;&#22312;&#26368;&#22823;&#21270;&#33258;&#24049;&#30340;&#22238;&#25253;&#12290;&#27492;&#22806;&#65292;&#24403;&#20960;&#20010;&#29609;&#23478;&#25289;&#21160;&#30456;&#21516;&#30340;&#25163;&#33218;&#26102;&#65292;&#25105;&#20204;&#20551;&#35774;&#36825;&#20123;&#29609;&#23478;&#36890;&#36807;&#26399;&#26395;&#24179;&#22343;&#20998;&#20139;&#25163;&#33218;&#30340;&#22238;&#25253;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#24403;&#25163;&#33218;&#30340;&#22238;&#25253;&#24050;&#30693;&#26102;&#30340;&#32435;&#20160;&#22343;&#34913;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35813;&#22343;&#34913;&#30340;&#26032;&#39062;&#30340;&#33258;&#31169;MPMAB&#19982;&#20998;&#37197;&#24179;&#22343;&#31639;&#27861;&#65288;SMAA&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#24403;&#25152;&#26377;&#29609;&#23478;&#37117;&#36981;&#24490;&#35813;&#31639;&#27861;&#26102;&#65292;SMAA&#21487;&#20197;&#20026;&#27599;&#20010;&#29609;&#23478;&#23454;&#29616;&#33391;&#22909;&#30340;&#21518;&#24724;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#27809;&#26377;&#21333;&#20010;&#33258;&#31169;&#30340;&#29609;&#23478;&#33021;&#36890;&#36807;&#20559;&#31163;&#26174;&#33879;&#22686;&#21152;&#20182;&#20204;&#30340;&#22238;&#25253;&#65292;&#20063;&#19981;&#33021;...
&lt;/p&gt;
&lt;p&gt;
Competitions for shareable and limited resources have long been studied with strategic agents. In reality, agents often have to learn and maximize the rewards of the resources at the same time. To design an individualized competing policy, we model the competition between agents in a novel multi-player multi-armed bandit (MPMAB) setting where players are selfish and aim to maximize their own rewards. In addition, when several players pull the same arm, we assume that these players averagely share the arms' rewards by expectation. Under this setting, we first analyze the Nash equilibrium when arms' rewards are known. Subsequently, we propose a novel Selfish MPMAB with Averaging Allocation (SMAA) approach based on the equilibrium. We theoretically demonstrate that SMAA could achieve a good regret guarantee for each player when all players follow the algorithm. Additionally, we establish that no single selfish player can significantly increase their rewards through deviation, nor can they
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20013;&#30340;&#19977;&#31181;&#26631;&#31614;&#20559;&#24046;&#25552;&#20986;&#20998;&#31867;&#27861;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#30340;&#20559;&#24046;&#26657;&#20934;&#26041;&#27861;&#65292;&#20351;&#29992;&#38543;&#26426;&#30340;&#39046;&#22495;&#35789;&#20272;&#31639;&#35821;&#35328;&#27169;&#22411;&#30340;&#26631;&#31614;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.19148</link><description>&lt;p&gt;
&#32531;&#35299;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26631;&#31614;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Mitigating Label Biases for In-context Learning. (arXiv:2305.19148v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20013;&#30340;&#19977;&#31181;&#26631;&#31614;&#20559;&#24046;&#25552;&#20986;&#20998;&#31867;&#27861;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#30340;&#20559;&#24046;&#26657;&#20934;&#26041;&#27861;&#65292;&#20351;&#29992;&#38543;&#26426;&#30340;&#39046;&#22495;&#35789;&#20272;&#31639;&#35821;&#35328;&#27169;&#22411;&#30340;&#26631;&#31614;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#21508;&#31181;&#35774;&#35745;&#35774;&#32622;&#65292;&#22914;&#36873;&#25321;&#21644;&#39034;&#24207;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#21487;&#33021;&#20351;&#27169;&#22411;&#23545;&#26576;&#31181;&#29305;&#23450;&#39044;&#27979;&#20559;&#35265;&#65292;&#32780;&#36825;&#31181;&#39044;&#27979;&#24182;&#19981;&#21453;&#26144;&#23545;&#20219;&#21153;&#30340;&#29702;&#35299;&#12290;&#34429;&#28982;&#35768;&#22810;&#30740;&#31350;&#35752;&#35770;&#20102;&#36825;&#20123;&#35774;&#35745;&#36873;&#25321;&#65292;&#20294;&#23545;&#23427;&#20204;&#36827;&#34892;&#20998;&#31867;&#21644;&#20943;&#32531;&#20854;&#24433;&#21709;&#30340;&#31995;&#32479;&#35843;&#26597;&#24456;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#25991;&#26412;&#20998;&#31867;&#20013;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20013;&#30340;&#19977;&#31181;&#26631;&#31614;&#20559;&#24046;&#23450;&#20041;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#65306;&#39321;&#33609;&#26631;&#31614;&#20559;&#24046;&#12289;&#19978;&#19979;&#25991;&#26631;&#31614;&#20559;&#24046;&#21644;&#39046;&#22495;&#26631;&#31614;&#20559;&#24046;&#65288;&#25105;&#20204;&#39318;&#27425;&#27010;&#24565;&#21270;&#21644;&#26816;&#27979;&#21040;&#65289;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20808;&#21069;&#30340;&#26631;&#31614;&#20559;&#24046;&#26657;&#20934;&#26041;&#27861;&#19981;&#33021;&#35299;&#20915;&#25152;&#26377;&#19977;&#31181;&#20559;&#24046;&#12290;&#29305;&#21035;&#26159;&#65292;&#39046;&#22495;&#26631;&#31614;&#20559;&#24046;&#20351;LLM&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#21482;&#33021;&#23454;&#29616;&#38543;&#26426;&#32423;&#21035;&#30340;&#24615;&#33021;&#65292;&#32780;&#19981;&#31649;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#36873;&#25321;&#22914;&#20309;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#20559;&#24046;&#26657;&#20934;&#26041;&#27861;&#65292;&#20351;&#29992;&#38543;&#26426;&#30340;&#39046;&#22495;&#35789;&#20272;&#31639;&#35821;&#35328;&#27169;&#22411;&#30340;&#26631;&#31614;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various design settings for in-context learning (ICL), such as the choice and order of the in-context examples, can bias a model toward a particular prediction without being reflective of an understanding of the task. While many studies discuss these design choices, there have been few systematic investigations into categorizing them and mitigating their impact. In this work, we define a typology for three types of label biases in ICL for text classification: vanilla-label bias, context-label bias, and domain-label bias (which we conceptualize and detect for the first time).  Our analysis demonstrates that prior label bias calibration methods fall short of addressing all three types of biases. Specifically, domain-label bias restricts LLMs to random-level performance on many tasks regardless of the choice of in-context examples. To mitigate the effect of these biases, we propose a simple bias calibration method that estimates a language model's label bias using random in-domain words f
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#21367;&#31215;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#35780;&#20272;&#29983;&#25104;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#35777;&#26126;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26377;&#33021;&#21147;&#24456;&#22909;&#22320;&#22797;&#21046;&#21407;&#22987;&#26102;&#38388;&#24207;&#21015;&#30340;&#28151;&#27788;&#29305;&#24615;&#65292;&#20294;&#35823;&#24046;&#20998;&#26512;&#34920;&#26126;&#20173;&#28982;&#23384;&#22312;&#22823;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.16729</link><description>&lt;p&gt;
&#22522;&#20110;&#21367;&#31215;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#30340;&#35780;&#20215;
&lt;/p&gt;
&lt;p&gt;
Evaluating generation of chaotic time series by convolutional generative adversarial networks. (arXiv:2305.16729v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16729
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#21367;&#31215;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#35780;&#20272;&#29983;&#25104;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#35777;&#26126;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26377;&#33021;&#21147;&#24456;&#22909;&#22320;&#22797;&#21046;&#21407;&#22987;&#26102;&#38388;&#24207;&#21015;&#30340;&#28151;&#27788;&#29305;&#24615;&#65292;&#20294;&#35823;&#24046;&#20998;&#26512;&#34920;&#26126;&#20173;&#28982;&#23384;&#22312;&#22823;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20102;&#35299;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#31867;&#20284;&#20110;&#22797;&#26434;&#26102;&#38388;&#20449;&#21495;&#30340;&#26102;&#38388;&#24207;&#21015;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#30001;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#32452;&#25104;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26469;&#29983;&#25104;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#20351;&#29992;&#38750;&#32447;&#24615;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26469;&#35780;&#20272;&#29983;&#25104;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#30830;&#23450;&#24615;&#30340;&#25968;&#23383;&#24230;&#37327;&#21644;&#26446;&#38597;&#26222;&#35834;&#22827;&#25351;&#25968;&#65292;&#19968;&#20010;&#36712;&#36857;&#19981;&#31283;&#23450;&#30340;&#24230;&#37327;&#65292;&#34920;&#26126;&#29983;&#25104;&#30340;&#26102;&#38388;&#24207;&#21015;&#24456;&#22909;&#22320;&#22797;&#21046;&#20102;&#21407;&#22987;&#26102;&#38388;&#24207;&#21015;&#30340;&#28151;&#27788;&#29305;&#24615;&#12290;&#28982;&#32780;&#65292;&#35823;&#24046;&#20998;&#24067;&#20998;&#26512;&#34920;&#26126;&#65292;&#20302;&#20294;&#19981;&#21487;&#24573;&#30053;&#30340;&#36895;&#29575;&#19979;&#20250;&#20986;&#29616;&#22823;&#35823;&#24046;&#12290;&#22914;&#26524;&#20551;&#23450;&#20998;&#24067;&#20026;&#25351;&#25968;&#20998;&#24067;&#65292;&#21017;&#19981;&#20250;&#20986;&#29616;&#36825;&#26679;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
To understand the ability and limitations of convolutional neural networks to generate time series that mimic complex temporal signals, we trained a generative adversarial network consisting of deep convolutional networks to generate chaotic time series and used nonlinear time series analysis to evaluate the generated time series. A numerical measure of determinism and the Lyapunov exponent, a measure of trajectory instability, showed that the generated time series well reproduce the chaotic properties of the original time series. However, error distribution analyses showed that large errors appeared at a low but non-negligible rate. Such errors would not be expected if the distribution were assumed to be exponential.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#27979;&#37327;&#22122;&#22768;&#30340;Ornstein-Uhlenbeck&#36807;&#31243;&#21442;&#25968;&#20272;&#35745;&#65292;&#25552;&#20986;&#20102;&#31639;&#27861;&#21644;&#26041;&#27861;&#33021;&#22815;&#20998;&#31163;&#28909;&#22122;&#22768;&#21644;&#20056;&#24615;&#22122;&#22768;&#65292;&#24182;&#25913;&#21892;&#25968;&#25454;&#20998;&#26512;&#30340;&#21442;&#25968;&#20272;&#35745;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.13498</link><description>&lt;p&gt;
&#29992;&#20110;&#24102;&#27979;&#37327;&#22122;&#22768;&#30340;Ornstein-Uhlenbeck&#36807;&#31243;&#21442;&#25968;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Parameter estimation from an Ornstein-Uhlenbeck process with measurement noise. (arXiv:2305.13498v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#27979;&#37327;&#22122;&#22768;&#30340;Ornstein-Uhlenbeck&#36807;&#31243;&#21442;&#25968;&#20272;&#35745;&#65292;&#25552;&#20986;&#20102;&#31639;&#27861;&#21644;&#26041;&#27861;&#33021;&#22815;&#20998;&#31163;&#28909;&#22122;&#22768;&#21644;&#20056;&#24615;&#22122;&#22768;&#65292;&#24182;&#25913;&#21892;&#25968;&#25454;&#20998;&#26512;&#30340;&#21442;&#25968;&#20272;&#35745;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#22122;&#22768;&#23545;Ornstein-Uhlenbeck&#36807;&#31243;&#21442;&#25968;&#25311;&#21512;&#30340;&#24433;&#21709;&#65292;&#37325;&#28857;&#32771;&#23519;&#20102;&#20056;&#24615;&#22122;&#22768;&#21644;&#28909;&#22122;&#22768;&#23545;&#20449;&#21495;&#20998;&#31163;&#31934;&#24230;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26377;&#25928;&#21306;&#20998;&#28909;&#22122;&#22768;&#21644;&#20056;&#24615;&#22122;&#22768;&#12289;&#25913;&#21892;&#21442;&#25968;&#20272;&#35745;&#31934;&#24230;&#30340;&#31639;&#27861;&#21644;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#20056;&#24615;&#21644;&#28909;&#22122;&#22768;&#23545;&#23454;&#38469;&#20449;&#21495;&#28151;&#28102;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#26377;&#25928;&#20998;&#31163;&#28909;&#22122;&#22768;&#30340;&#31639;&#27861;&#65292;&#20854;&#24615;&#33021;&#21487;&#19982;Hamilton Monte Carlo (HMC)&#30456;&#23218;&#32654;&#65292;&#20294;&#36895;&#24230;&#26174;&#33879;&#25552;&#39640;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20056;&#24615;&#22122;&#22768;&#65292;&#24182;&#35777;&#26126;&#20102;HMC&#26080;&#27861;&#38548;&#31163;&#28909;&#22122;&#22768;&#21644;&#20056;&#24615;&#22122;&#22768;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#22312;&#39069;&#22806;&#20102;&#35299;&#28909;&#22122;&#22768;&#21644;&#20056;&#24615;&#22122;&#22768;&#20043;&#38388;&#27604;&#29575;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#31934;&#30830;&#22320;&#20272;&#35745;&#21442;&#25968;&#21644;&#20998;&#31163;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article aims to investigate the impact of noise on parameter fitting for an Ornstein-Uhlenbeck process, focusing on the effects of multiplicative and thermal noise on the accuracy of signal separation. To address these issues, we propose algorithms and methods that can effectively distinguish between thermal and multiplicative noise and improve the precision of parameter estimation for optimal data analysis. Specifically, we explore the impact of both multiplicative and thermal noise on the obfuscation of the actual signal and propose methods to resolve them. Firstly, we present an algorithm that can effectively separate thermal noise with comparable performance to Hamilton Monte Carlo (HMC) but with significantly improved speed. Subsequently, we analyze multiplicative noise and demonstrate that HMC is insufficient for isolating thermal and multiplicative noise. However, we show that, with additional knowledge of the ratio between thermal and multiplicative noise, we can accuratel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SCPT&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#31354;&#38388;&#39044;&#35757;&#32451;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#31354;&#38388;&#32534;&#30721;&#22120;&#27169;&#22359;&#65292;&#29992;&#20110;&#20174;&#26410;&#35265;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#36827;&#34892;&#26032;&#36947;&#36335;&#30340;&#20132;&#36890;&#39044;&#27979;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.05237</link><description>&lt;p&gt;
&#21033;&#29992;&#31354;&#38388;&#23545;&#27604;&#39044;&#35757;&#32451;&#36827;&#34892;&#26410;&#35265;&#35757;&#32451;&#25968;&#25454;&#30340;&#26032;&#36947;&#36335;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Traffic Forecasting on New Roads Unseen in the Training Data Using Spatial Contrastive Pre-Training. (arXiv:2305.05237v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SCPT&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#31354;&#38388;&#39044;&#35757;&#32451;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#31354;&#38388;&#32534;&#30721;&#22120;&#27169;&#22359;&#65292;&#29992;&#20110;&#20174;&#26410;&#35265;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#36827;&#34892;&#26032;&#36947;&#36335;&#30340;&#20132;&#36890;&#39044;&#27979;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26102;&#38388;&#25512;&#31227;&#20250;&#19981;&#26029;&#24314;&#35774;&#26032;&#30340;&#36947;&#36335;&#65292;&#20294;&#26159;&#20043;&#21069;&#30340;&#28145;&#24230;&#39044;&#27979;&#27169;&#22411;&#23545;&#20110;&#26032;&#36947;&#36335;&#65288;&#26410;&#35265;&#25968;&#25454;&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#34987;&#31216;&#20026;&#26102;&#31354;&#65288;ST&#65289;&#20998;&#21106;&#30340;&#26032;&#35774;&#32622;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#23545;&#26410;&#35265;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#27169;&#22411;&#35757;&#32451;&#26102;&#20351;&#29992;&#19968;&#37096;&#20998;&#30340;&#36947;&#36335;&#25968;&#25454;&#65292;&#20294;&#27979;&#35797;&#26102;&#20351;&#29992;&#26410;&#35265;&#25968;&#25454;&#30340;&#36947;&#36335;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20043;&#20026;&#31354;&#38388;&#23545;&#27604;&#39044;&#35757;&#32451;&#65288;SCPT&#65289;&#65292;&#20854;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#31354;&#38388;&#32534;&#30721;&#22120;&#27169;&#22359;&#26469;&#25552;&#21462;&#25512;&#29702;&#26102;&#26410;&#35265;&#36947;&#36335;&#30340;&#28508;&#22312;&#29305;&#24449;&#12290;&#36825;&#20010;&#31354;&#38388;&#32534;&#30721;&#22120;&#26159;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#39044;&#35757;&#32451;&#30340;&#12290;&#22312;&#25512;&#29702;&#26102;&#65292;&#31354;&#38388;&#32534;&#30721;&#22120;&#20165;&#38656;&#35201;&#26032;&#36947;&#36335;&#30340;&#20004;&#22825;&#20132;&#36890;&#25968;&#25454;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#37325;&#26032;&#35757;&#32451;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#31354;&#38388;&#32534;&#30721;&#22120;&#30340;&#36755;&#20986;&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#20110;&#25512;&#26029;&#26410;&#35265;&#36947;&#36335;&#19978;&#30340;&#28508;&#22312;&#33410;&#28857;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
New roads are being constructed all the time. However, the capabilities of previous deep forecasting models to generalize to new roads not seen in the training data (unseen roads) are rarely explored. In this paper, we introduce a novel setup called a spatio-temporal (ST) split to evaluate the models' capabilities to generalize to unseen roads. In this setup, the models are trained on data from a sample of roads, but tested on roads not seen in the training data. Moreover, we also present a novel framework called Spatial Contrastive Pre-Training (SCPT) where we introduce a spatial encoder module to extract latent features from unseen roads during inference time. This spatial encoder is pre-trained using contrastive learning. During inference, the spatial encoder only requires two days of traffic data on the new roads and does not require any re-training. We also show that the output from the spatial encoder can be used effectively to infer latent node embeddings on unseen roads during 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#29983;&#23384;&#20998;&#26512;&#26469;&#35780;&#20215;&#20986;&#38498;&#21518;&#35780;&#20272;&#21644;&#31649;&#29702;&#65288;E/M&#65289;&#26381;&#21153;&#23545;&#38450;&#27490;&#20877;&#20303;&#38498;&#25110;&#27515;&#20129;&#30340;&#24433;&#21709;&#65292;&#36991;&#20813;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#24184;&#23384;&#32773;&#20559;&#24046;&#30340;&#38382;&#39064;&#65292;&#24182;&#30830;&#23450;&#20102;&#20010;&#26696;&#31649;&#29702;&#26381;&#21153;&#22312;&#20943;&#23569;&#20877;&#20303;&#38498;&#26041;&#38754;&#26368;&#20026;&#26377;&#25928;&#65292;&#23588;&#20854;&#23545;&#20110;&#20986;&#38498;&#21518;&#21040;&#38271;&#26399;&#25252;&#29702;&#26426;&#26500;&#30340;&#24739;&#32773;&#20197;&#21450;&#20837;&#38498;&#21069;&#26377;&#39640;&#36164;&#28304;&#21033;&#29992;&#29575;&#30340;&#24739;&#32773;&#12290;</title><link>http://arxiv.org/abs/2304.09981</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#24322;&#36136;&#24615;&#24184;&#23384;&#32773;&#20559;&#24046;&#26657;&#27491;&#27835;&#30103;&#25928;&#26524;&#30740;&#31350;&#65292;&#26088;&#22312;&#25351;&#27966;&#20986;&#38498;&#21518;&#39044;&#38450;&#20877;&#20303;&#38498;&#30340;&#24178;&#39044;&#25514;&#26045;
&lt;/p&gt;
&lt;p&gt;
Interpretable (not just posthoc-explainable) heterogeneous survivor bias-corrected treatment effects for assignment of postdischarge interventions to prevent readmissions. (arXiv:2304.09981v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09981
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#29983;&#23384;&#20998;&#26512;&#26469;&#35780;&#20215;&#20986;&#38498;&#21518;&#35780;&#20272;&#21644;&#31649;&#29702;&#65288;E/M&#65289;&#26381;&#21153;&#23545;&#38450;&#27490;&#20877;&#20303;&#38498;&#25110;&#27515;&#20129;&#30340;&#24433;&#21709;&#65292;&#36991;&#20813;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#24184;&#23384;&#32773;&#20559;&#24046;&#30340;&#38382;&#39064;&#65292;&#24182;&#30830;&#23450;&#20102;&#20010;&#26696;&#31649;&#29702;&#26381;&#21153;&#22312;&#20943;&#23569;&#20877;&#20303;&#38498;&#26041;&#38754;&#26368;&#20026;&#26377;&#25928;&#65292;&#23588;&#20854;&#23545;&#20110;&#20986;&#38498;&#21518;&#21040;&#38271;&#26399;&#25252;&#29702;&#26426;&#26500;&#30340;&#24739;&#32773;&#20197;&#21450;&#20837;&#38498;&#21069;&#26377;&#39640;&#36164;&#28304;&#21033;&#29992;&#29575;&#30340;&#24739;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37319;&#29992;&#29983;&#23384;&#20998;&#26512;&#26469;&#37327;&#21270;&#20986;&#38498;&#21518;&#35780;&#20272;&#21644;&#31649;&#29702;&#26381;&#21153;&#22312;&#38450;&#27490;&#20303;&#38498;&#25110;&#27515;&#20129;&#26041;&#38754;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36991;&#20813;&#20102;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#21040;&#36825;&#20010;&#38382;&#39064;&#19978;&#30340;&#19968;&#20010;&#29305;&#23450;&#38519;&#38449;&#65292;&#37027;&#23601;&#26159;&#22240;&#20026;&#24184;&#23384;&#32773;&#20559;&#24046;&#32780;&#23548;&#33268;&#30340;&#25928;&#26524;&#36807;&#39640; -- &#36825;&#31181;&#36807;&#39640;&#30340;&#20272;&#35745;&#21487;&#33021;&#19982;&#20154;&#32676;&#20013;&#30340;&#24322;&#36136;&#24615;&#28151;&#28102;&#22240;&#32032;&#26377;&#20851;&#12290;&#36825;&#31181;&#20559;&#24046;&#20043;&#25152;&#20197;&#20135;&#29983;&#65292;&#26159;&#22240;&#20026;&#20026;&#20102;&#22312;&#20986;&#38498;&#21518;&#25509;&#21463;&#24178;&#39044;&#65292;&#19968;&#20010;&#20154;&#24517;&#39035;&#22312;&#20171;&#20837;&#26399;&#20869;&#27809;&#26377;&#20877;&#27425;&#20303;&#38498;&#12290;&#22312;&#24471;&#20986;&#36825;&#31181;&#24187;&#24433;&#25928;&#24212;&#30340;&#34920;&#36798;&#24335;&#21518;&#65292;&#25105;&#20204;&#22312;&#26412;&#36136;&#19978;&#21487;&#35299;&#37322;&#30340;&#36125;&#21494;&#26031;&#29983;&#23384;&#26694;&#26550;&#20013;&#25511;&#21046;&#20102;&#36825;&#20010;&#21644;&#20854;&#20182;&#20559;&#24046;&#12290;&#25105;&#20204;&#30830;&#23450;&#20010;&#26696;&#31649;&#29702;&#26381;&#21153;&#23545;&#20110;&#25972;&#20307;&#20943;&#23569;&#20877;&#20303;&#38498;&#20855;&#26377;&#26368;&#22823;&#30340;&#24433;&#21709;&#21147;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20986;&#38498;&#21518;&#21040;&#38271;&#26399;&#25252;&#29702;&#26426;&#26500;&#30340;&#24739;&#32773;&#65292;&#22312;&#20837;&#38498;&#21069;&#19968;&#20010;&#23395;&#24230;&#26377;&#39640;&#36164;&#28304;&#21033;&#29992;&#29575;&#30340;&#24739;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
We used survival analysis to quantify the impact of postdischarge evaluation and management (E/M) services in preventing hospital readmission or death. Our approach avoids a specific pitfall of applying machine learning to this problem, which is an inflated estimate of the effect of interventions, due to survivors bias -- where the magnitude of inflation may be conditional on heterogeneous confounders in the population. This bias arises simply because in order to receive an intervention after discharge, a person must not have been readmitted in the intervening period. After deriving an expression for this phantom effect, we controlled for this and other biases within an inherently interpretable Bayesian survival framework. We identified case management services as being the most impactful for reducing readmissions overall, particularly for patients discharged to long term care facilities, with high resource utilization in the quarter preceding admission.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;FLW-Net&#30340;&#24555;&#36895;&#12289;&#36731;&#37327;&#32423;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#20302;&#20809;&#29031;&#22270;&#20687;&#20013;&#23384;&#22312;&#30340;&#22122;&#22768;&#12289;&#20302;&#20142;&#24230;&#12289;&#20302;&#23545;&#27604;&#24230;&#21644;&#33394;&#24425;&#20559;&#24046;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#39640;&#25928;&#30340;&#20840;&#23616;&#29305;&#24449;&#20449;&#24687;&#25552;&#21462;&#32452;&#20214;&#20197;&#21450;&#22522;&#20110;&#30456;&#23545;&#20449;&#24687;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.02978</link><description>&lt;p&gt;
&#19968;&#31181;&#24555;&#36895;&#12289;&#36731;&#37327;&#32423;&#30340;&#29992;&#20110;&#20302;&#20809;&#29031;&#22270;&#20687;&#22686;&#24378;&#30340;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A Fast and Lightweight Network for Low-Light Image Enhancement. (arXiv:2304.02978v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;FLW-Net&#30340;&#24555;&#36895;&#12289;&#36731;&#37327;&#32423;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#20302;&#20809;&#29031;&#22270;&#20687;&#20013;&#23384;&#22312;&#30340;&#22122;&#22768;&#12289;&#20302;&#20142;&#24230;&#12289;&#20302;&#23545;&#27604;&#24230;&#21644;&#33394;&#24425;&#20559;&#24046;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#39640;&#25928;&#30340;&#20840;&#23616;&#29305;&#24449;&#20449;&#24687;&#25552;&#21462;&#32452;&#20214;&#20197;&#21450;&#22522;&#20110;&#30456;&#23545;&#20449;&#24687;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#20809;&#29031;&#22270;&#20687;&#36890;&#24120;&#23384;&#22312;&#20005;&#37325;&#30340;&#22122;&#22768;&#12289;&#20302;&#20142;&#24230;&#12289;&#20302;&#23545;&#27604;&#24230;&#21644;&#33394;&#24425;&#20559;&#24046;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FLW-Net&#30340;&#24555;&#36895;&#12289;&#36731;&#37327;&#32423;&#32593;&#32476;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#20840;&#23616;&#29305;&#24449;&#20449;&#24687;&#25552;&#21462;&#32452;&#20214;&#65292;&#24182;&#22522;&#20110;&#30456;&#23545;&#20449;&#24687;&#35774;&#35745;&#20102;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#32477;&#23545;&#21442;&#32771;&#32570;&#22833;&#21644;&#33719;&#24471;&#20840;&#23616;&#23545;&#27604;&#24230;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-light images often suffer from severe noise, low brightness, low contrast, and color deviation. While several low-light image enhancement methods have been proposed, there remains a lack of efficient methods that can simultaneously solve all of these problems. In this paper, we introduce FLW-Net, a Fast and LightWeight Network for low-light image enhancement that significantly improves processing speed and overall effect. To achieve efficient low-light image enhancement, we recognize the challenges of the lack of an absolute reference and the need for a large receptive field to obtain global contrast. Therefore, we propose an efficient global feature information extraction component and design loss functions based on relative information to overcome these challenges. Finally, we conduct comparative experiments to demonstrate the effectiveness of the proposed method, and the results confirm that FLW-Net can significantly reduce the complexity of supervised low-light image enhancemen
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#36719;&#25552;&#31034;&#23884;&#20837;&#65292;&#25552;&#20986;Soft Prompt-Based Calibration (SPeC)&#31649;&#36947;&#65292;&#26469;&#20943;&#36731;&#36755;&#20837;&#21464;&#37327;&#23545;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#65292;&#38477;&#20302;&#24615;&#33021;&#21464;&#24322;. &#27492;&#26041;&#27861;&#19981;&#20165;&#27604;&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#24615;&#33021;&#31283;&#23450;&#65292;&#32780;&#19988;&#22312;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;.</title><link>http://arxiv.org/abs/2303.13035</link><description>&lt;p&gt;
SPeC&#65306;&#36719;&#25552;&#31034;&#26657;&#20934;&#22312;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20013;&#38477;&#20302;&#24615;&#33021;&#21464;&#24322;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
SPeC: A Soft Prompt-Based Calibration on Mitigating Performance Variability in Clinical Notes Summarization. (arXiv:2303.13035v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13035
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#36719;&#25552;&#31034;&#23884;&#20837;&#65292;&#25552;&#20986;Soft Prompt-Based Calibration (SPeC)&#31649;&#36947;&#65292;&#26469;&#20943;&#36731;&#36755;&#20837;&#21464;&#37327;&#23545;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#65292;&#38477;&#20302;&#24615;&#33021;&#21464;&#24322;. &#27492;&#26041;&#27861;&#19981;&#20165;&#27604;&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#24615;&#33021;&#31283;&#23450;&#65292;&#32780;&#19988;&#22312;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#23384;&#20648;&#30528;&#21253;&#25324;&#30149;&#21382;&#12289;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#26816;&#27979;&#32467;&#26524;&#22312;&#20869;&#30340;&#22823;&#37327;&#24739;&#32773;&#20449;&#24687;&#12290;&#36825;&#20123;&#35760;&#24405;&#23545;&#20110;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#20570;&#20986;&#26126;&#26234;&#30340;&#24739;&#32773;&#25252;&#29702;&#20915;&#31574;&#38750;&#24120;&#20851;&#38190;&#12290;&#25688;&#35201;&#20020;&#24202;&#31508;&#35760;&#21487;&#20197;&#24110;&#21161;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#26356;&#22909;&#22320;&#21457;&#29616;&#28508;&#22312;&#20581;&#24247;&#39118;&#38505;&#65292;&#20197;&#21450;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;&#36825;&#19968;&#36807;&#31243;&#36890;&#36807;&#30830;&#20445;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#21487;&#20197;&#35775;&#38382;&#26368;&#30456;&#20851;&#21644;&#26368;&#26032;&#30340;&#24739;&#32773;&#25968;&#25454;&#65292;&#26377;&#21161;&#20110;&#20943;&#23569;&#38169;&#35823;&#24182;&#25552;&#39640;&#24739;&#32773;&#30340;&#25252;&#29702;&#25928;&#26524;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;&#25552;&#31034;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25688;&#35201;&#20219;&#21153;&#30340;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#20063;&#20250;&#23548;&#33268;&#36755;&#20986;&#26041;&#24046;&#22686;&#21152;&#65292;&#21363;&#20351;&#25552;&#31034;&#24847;&#20041;&#30456;&#20284;&#65292;&#36755;&#20986;&#20063;&#20250;&#26377;&#26126;&#26174;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#36719;&#25552;&#31034;&#26657;&#20934;&#65288;SPeC&#65289;&#27969;&#31243;&#65292;&#35813;&#27969;&#31243;&#37319;&#29992;&#36719;&#25552;&#31034;&#23884;&#20837;&#26469;&#20943;&#36731;&#36755;&#20837;&#21464;&#37327;&#23545;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SPeC&#19981;&#20165;&#21487;&#20197;&#38477;&#20302;LLM&#30340;&#24615;&#33021;&#21464;&#24322;&#65292;&#32780;&#19988;&#22312;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic health records (EHRs) store an extensive array of patient information, encompassing medical histories, diagnoses, treatments, and test outcomes. These records are crucial for enabling healthcare providers to make well-informed decisions regarding patient care. Summarizing clinical notes further assists healthcare professionals in pinpointing potential health risks and making better-informed decisions. This process contributes to reducing errors and enhancing patient outcomes by ensuring providers have access to the most pertinent and current patient data. Recent research has shown that incorporating prompts with large language models (LLMs) substantially boosts the efficacy of summarization tasks. However, we show that this approach also leads to increased output variance, resulting in notably divergent outputs even when prompts share similar meanings. To tackle this challenge, we introduce a model-agnostic Soft Prompt-Based Calibration (SPeC) pipeline that employs soft prom
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22235;&#32500;CTP&#20840;&#38754;&#21033;&#29992;&#26102;&#31354;&#20449;&#24687;&#65292;&#20197;&#20998;&#21106;&#30097;&#20284;&#24613;&#24615;&#32570;&#34880;&#24615;&#21330;&#20013;&#24739;&#32773;&#30340;&#26775;&#27515;&#21306;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#26377;&#28508;&#21147;&#20026;&#25913;&#21892;AIS&#24739;&#32773;&#30340;&#26089;&#26399;&#35786;&#26029;&#21644;&#27835;&#30103;&#35268;&#21010;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2303.08757</link><description>&lt;p&gt;
&#21033;&#29992;&#22235;&#32500;CT&#28748;&#27880;&#25104;&#20687;&#23545;&#30097;&#20284;&#24613;&#24615;&#32570;&#34880;&#24615;&#21330;&#20013;&#24739;&#32773;&#30340;&#26775;&#27515;&#21306;&#36827;&#34892;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Exploiting 4D CT Perfusion for segmenting infarcted areas in patients with suspected acute ischemic stroke. (arXiv:2303.08757v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08757
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22235;&#32500;CTP&#20840;&#38754;&#21033;&#29992;&#26102;&#31354;&#20449;&#24687;&#65292;&#20197;&#20998;&#21106;&#30097;&#20284;&#24613;&#24615;&#32570;&#34880;&#24615;&#21330;&#20013;&#24739;&#32773;&#30340;&#26775;&#27515;&#21306;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#26377;&#28508;&#21147;&#20026;&#25913;&#21892;AIS&#24739;&#32773;&#30340;&#26089;&#26399;&#35786;&#26029;&#21644;&#27835;&#30103;&#35268;&#21010;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#12289;&#24555;&#36895;&#30340;&#24613;&#24615;&#32570;&#34880;&#24615;&#21330;&#20013;&#65288;AIS&#65289;&#24739;&#32773;&#32570;&#34880;&#21306;&#65288;&#26680;&#24515;&#21644;&#21322;&#24433;&#21306;&#65289;&#39044;&#27979;&#26041;&#27861;&#23545;&#20110;&#25913;&#36827;&#35786;&#26029;&#21644;&#27835;&#30103;&#35268;&#21010;&#20855;&#26377;&#37325;&#35201;&#30340;&#20020;&#24202;&#24847;&#20041;&#12290;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#26159;&#30097;&#20284;AIS&#24739;&#32773;&#26089;&#26399;&#35780;&#20272;&#30340;&#20027;&#35201;&#27169;&#24335;&#20043;&#19968;&#12290;CT&#28748;&#27880;&#25104;&#20687;&#65288;CTP&#65289;&#36890;&#24120;&#29992;&#20316;&#20027;&#35201;&#35780;&#20272;&#25163;&#27573;&#65292;&#20197;&#30830;&#23450;&#21330;&#20013;&#20301;&#32622;&#12289;&#20005;&#37325;&#31243;&#24230;&#21644;&#32570;&#34880;&#24615;&#30149;&#28790;&#20307;&#31215;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;CTP&#33258;&#21160;&#20998;&#21106;&#26041;&#27861;&#37117;&#20351;&#29992;&#24050;&#32463;&#22788;&#29702;&#36807;&#30340;&#19977;&#32500;&#24425;&#33394;&#22320;&#22270;&#20316;&#20026;&#25918;&#23556;&#31185;&#21307;&#24072;&#24120;&#35268;&#35270;&#35273;&#35780;&#20272;&#30340;&#36755;&#20837;&#12290;&#25110;&#32773;&#65292;&#22522;&#20110;&#20999;&#29255;&#30340;&#20108;&#32500;+&#26102;&#38388;&#36755;&#20837;&#20351;&#29992;&#21407;&#22987;CTP&#25968;&#25454;&#65292;&#20854;&#20013;&#24573;&#30053;&#20102;&#22312;&#20307;&#31215;&#19978;&#30340;&#31354;&#38388;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#19981;&#21516;&#26041;&#27861;&#26469;&#21033;&#29992;&#25972;&#20010;&#22235;&#32500;CTP&#20316;&#20026;&#36755;&#20837;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#26102;&#31354;&#20449;&#24687;&#12290;&#36825;&#20351;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;4D&#21367;&#31215;&#23618;&#12290;&#25105;&#20204;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#20840;&#38754;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#26377;&#28508;&#21147;&#20026;&#25913;&#21892;AIS&#24739;&#32773;&#30340;&#26089;&#26399;&#35786;&#26029;&#21644;&#27835;&#30103;&#35268;&#21010;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precise and fast prediction methods for ischemic areas (core and penumbra) in acute ischemic stroke (AIS) patients are of significant clinical interest: they play an essential role in improving diagnosis and treatment planning. Computed Tomography (CT) scan is one of the primary modalities for early assessment in patients with suspected AIS. CT Perfusion (CTP) is often used as a primary assessment to determine stroke location, severity, and volume of ischemic lesions. Current automatic segmentation methods for CTP mostly use already processed 3D color maps conventionally used for visual assessment by radiologists as input. Alternatively, the raw CTP data is used on a slice-by-slice basis as 2D+time input, where the spatial information over the volume is ignored. In this paper, we investigate different methods to utilize the entire 4D CTP as input to fully exploit the spatio-temporal information. This leads us to propose a novel 4D convolution layer. Our comprehensive experiments on a l
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#22870;&#21169;&#22609;&#24418;&#24418;&#24335;&#65292;&#21033;&#29992;&#22810;&#23618;&#27425;&#30340;&#25277;&#35937;&#26469;&#25913;&#21892;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#29575;&#65292;&#24182;&#19988;&#23545;&#25277;&#35937;&#27169;&#22411;&#30340;&#35774;&#35745;&#35201;&#27714;&#36739;&#23569;&#65292;&#20855;&#26377;&#23481;&#24525;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.00516</link><description>&lt;p&gt;
&#36890;&#36807;&#22870;&#21169;&#22609;&#24418;&#22312;&#22522;&#20110;&#24773;&#33410;&#30340;RL&#20013;&#21033;&#29992;&#22810;&#37325;&#25277;&#35937;
&lt;/p&gt;
&lt;p&gt;
Exploiting Multiple Abstractions in Episodic RL via Reward Shaping. (arXiv:2303.00516v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00516
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#22870;&#21169;&#22609;&#24418;&#24418;&#24335;&#65292;&#21033;&#29992;&#22810;&#23618;&#27425;&#30340;&#25277;&#35937;&#26469;&#25913;&#21892;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#29575;&#65292;&#24182;&#19988;&#23545;&#25277;&#35937;&#27169;&#22411;&#30340;&#35774;&#35745;&#35201;&#27714;&#36739;&#23569;&#65292;&#20855;&#26377;&#23481;&#24525;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#35768;&#22810;&#23454;&#38469;&#39046;&#22495;&#24212;&#29992;&#30340;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#26159;&#38656;&#35201;&#22823;&#37327;&#30340;&#26679;&#26412;&#26469;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22522;&#20110;&#30446;&#26631;&#39046;&#22495;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#30340;&#32447;&#24615;&#23618;&#27425;&#30340;&#25277;&#35937;&#23618;&#27425;&#12290;&#27599;&#20010;&#23618;&#27425;&#37117;&#26159;&#19968;&#20010;&#34920;&#31034;&#27604;&#23618;&#27425;&#32467;&#26500;&#19979;&#26041;&#21363;&#21051;&#27169;&#22411;&#26356;&#31895;&#31961;&#30340;&#27169;&#22411;&#30340;MDP&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22870;&#21169;&#22609;&#24418;&#24418;&#24335;&#65292;&#20854;&#20013;&#22312;&#25277;&#35937;&#23618;&#38754;&#33719;&#24471;&#30340;&#35299;&#20915;&#26041;&#26696;&#29992;&#20110;&#21521;&#26356;&#20855;&#20307;&#30340;MDP&#25552;&#20379;&#22870;&#21169;&#65292;&#20197;&#20351;&#25277;&#35937;&#35299;&#20915;&#26041;&#26696;&#25351;&#23548;&#26356;&#22797;&#26434;&#39046;&#22495;&#20013;&#30340;&#23398;&#20064;&#12290;&#19982;&#23618;&#27425;RL&#20013;&#30340;&#20854;&#20182;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;&#25277;&#35937;&#27169;&#22411;&#30340;&#35774;&#35745;&#26041;&#38754;&#26377;&#24456;&#23569;&#30340;&#35201;&#27714;&#65292;&#24182;&#19988;&#20063;&#23545;&#24314;&#27169;&#38169;&#35823;&#20855;&#26377;&#23481;&#24525;&#24615;&#65292;&#20174;&#32780;&#20351;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21464;&#24471;&#23454;&#29992;&#12290;&#25105;&#20204;&#27491;&#24335;&#20998;&#26512;&#20102;&#25277;&#35937;&#27169;&#22411;&#19982;&#24341;&#21457;&#25506;&#32034;&#21551;&#21457;&#24335;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
One major limitation to the applicability of Reinforcement Learning (RL) to many practical domains is the large number of samples required to learn an optimal policy. To address this problem and improve learning efficiency, we consider a linear hierarchy of abstraction layers of the Markov Decision Process (MDP) underlying the target domain. Each layer is an MDP representing a coarser model of the one immediately below in the hierarchy. In this work, we propose a novel form of Reward Shaping where the solution obtained at the abstract level is used to offer rewards to the more concrete MDP, in such a way that the abstract solution guides the learning in the more complex domain. In contrast with other works in Hierarchical RL, our technique has few requirements in the design of the abstract models and it is also tolerant to modeling errors, thus making the proposed approach practical. We formally analyze the relationship between the abstract models and the exploration heuristic induced 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#19978;&#19979;&#25991;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36816;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26862;&#26519;&#26469;&#27169;&#25311;&#29305;&#24449;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#21644;&#21487;&#35299;&#37322;&#22320;&#35782;&#21035;&#20559;&#31163;&#31867;&#20284;&#23545;&#35937;&#19978;&#19979;&#25991;&#30340;&#20854;&#20182;&#23545;&#35937;&#12290;</title><link>http://arxiv.org/abs/2302.11239</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26862;&#26519;&#30340;&#21487;&#35299;&#37322;&#19978;&#19979;&#25991;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Explainable Contextual Anomaly Detection using Quantile Regression Forests. (arXiv:2302.11239v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11239
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#19978;&#19979;&#25991;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36816;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26862;&#26519;&#26469;&#27169;&#25311;&#29305;&#24449;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#21644;&#21487;&#35299;&#37322;&#22320;&#35782;&#21035;&#20559;&#31163;&#31867;&#20284;&#23545;&#35937;&#19978;&#19979;&#25991;&#30340;&#20854;&#20182;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#36890;&#36807;&#24179;&#31561;&#23545;&#24453;&#25152;&#26377;&#29305;&#24449;&#26469;&#35782;&#21035;&#20559;&#31163;&#22823;&#22810;&#25968;&#20854;&#20182;&#23545;&#35937;&#30340;&#23545;&#35937;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#19978;&#19979;&#25991;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#36890;&#36807;&#23558;&#29305;&#24449;&#21010;&#20998;&#20026;&#19978;&#19979;&#25991;&#29305;&#24449;&#21644;&#34892;&#20026;&#29305;&#24449;&#65292;&#26088;&#22312;&#26816;&#27979;&#20559;&#31163;&#31867;&#20284;&#23545;&#35937;&#19978;&#19979;&#25991;&#30340;&#20854;&#20182;&#23545;&#35937;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20381;&#36182;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#20381;&#36182;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#21644;&#19978;&#19979;&#25991;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#22522;&#20110;&#30001;&#27492;&#33719;&#24471;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26862;&#26519;&#26469;&#27169;&#25311;&#29305;&#24449;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#23454;&#29616;&#20869;&#22312;&#30340;&#21487;&#35299;&#37322;&#19978;&#19979;&#25991;&#24322;&#24120;&#26816;&#27979;&#12290;&#21508;&#31181;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35782;&#21035;&#19978;&#19979;&#25991;&#24322;&#24120;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#29366;&#24577;-of-art&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional anomaly detection methods aim to identify objects that deviate from most other objects by treating all features equally. In contrast, contextual anomaly detection methods aim to detect objects that deviate from other objects within a context of similar objects by dividing the features into contextual features and behavioral features. In this paper, we develop connections between dependency-based traditional anomaly detection methods and contextual anomaly detection methods. Based on resulting insights, we propose a novel approach to inherently interpretable contextual anomaly detection that uses Quantile Regression Forests to model dependencies between features. Extensive experiments on various synthetic and real-world datasets demonstrate that our method outperforms state-of-the-art anomaly detection methods in identifying contextual anomalies in terms of accuracy and interpretability.
&lt;/p&gt;</description></item><item><title>&#29992;SE(3)&#31561;&#21464;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21487;&#20197;&#20174;&#23567;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21160;&#33033;&#27969;&#36895;&#20272;&#35745;&#65292;&#36895;&#24230;&#24555;&#65292;&#20943;&#23569;&#20102;&#20351;&#29992;CFD&#27169;&#25311;&#30340;&#38656;&#35201;</title><link>http://arxiv.org/abs/2302.08780</link><description>&lt;p&gt;
SE(3)&#23545;&#31216;&#24615;&#35753;&#22270;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#20174;&#23567;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21160;&#33033;&#34880;&#27969;&#36895;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
SE(3) symmetry lets graph neural networks learn arterial velocity estimation from small datasets. (arXiv:2302.08780v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08780
&lt;/p&gt;
&lt;p&gt;
&#29992;SE(3)&#31561;&#21464;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21487;&#20197;&#20174;&#23567;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21160;&#33033;&#27969;&#36895;&#20272;&#35745;&#65292;&#36895;&#24230;&#24555;&#65292;&#20943;&#23569;&#20102;&#20351;&#29992;CFD&#27169;&#25311;&#30340;&#38656;&#35201;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20896;&#29366;&#21160;&#33033;&#20013;&#30340;&#34880;&#28082;&#36895;&#24230;&#22330;&#21487;&#33021;&#26159;&#35786;&#26029;&#12289;&#39044;&#21518;&#21644;&#27835;&#30103;&#35268;&#21010;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#26377;&#20215;&#20540;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#34880;&#27969;&#21160;&#21147;&#23398;&#27169;&#25311;&#38656;&#35201;&#19987;&#23478;&#30340;&#32454;&#33268;&#35774;&#32622;&#65292;&#32791;&#26102;&#19988;&#38590;&#20197;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#34987;&#22823;&#35268;&#27169;&#25509;&#21463;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#20316;&#20026;&#19968;&#31181;&#39640;&#25928;&#30340;&#40657;&#30418;&#23376;&#20195;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#21160;&#33033;&#27969;&#20307;&#20013;&#39030;&#28857;&#26144;&#23556;&#30340;3D&#36895;&#24230;&#22330;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#21160;&#33033;&#27169;&#22411;&#21644;&#22522;&#20110;CFD&#30340;&#22320;&#38754;&#30495;&#23454;&#36895;&#24230;&#22330;&#23545;&#36825;&#20123;GNN&#36827;&#34892;&#22521;&#35757;&#12290;&#19968;&#26086;GNN&#35757;&#32451;&#23436;&#25104;&#65292;&#19982;CFD&#30456;&#27604;&#65292;&#21487;&#20197;36&#20493;&#21152;&#36895;&#33719;&#24471;&#26032;&#30340;&#12289;&#26410;&#35265;&#36807;&#21160;&#33033;&#30340;&#36895;&#24230;&#20272;&#35745;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#26500;&#24314;&#19968;&#20010;SE(3)&#31561;&#21464;&#30340;GNN&#65292;&#23427;&#29420;&#31435;&#20110;&#36755;&#20837;&#32593;&#26684;&#30340;&#31354;&#38388;&#26041;&#21521;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#22914;&#20309;&#20943;&#23569;&#24517;&#35201;&#30340;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Hemodynamic velocity fields in coronary arteries could be the basis of valuable biomarkers for diagnosis, prognosis and treatment planning in cardiovascular disease. Velocity fields are typically obtained from patient-specific 3D artery models via computational fluid dynamics (CFD). However, CFD simulation requires meticulous setup by experts and is time-intensive, which hinders large-scale acceptance in clinical practice. To address this, we propose graph neural networks (GNN) as an efficient black-box surrogate method to estimate 3D velocity fields mapped to the vertices of tetrahedral meshes of the artery lumen. We train these GNNs on synthetic artery models and CFD-based ground truth velocity fields. Once the GNN is trained, velocity estimates in a new and unseen artery can be obtained with 36-fold speed-up compared to CFD. We demonstrate how to construct an SE(3)-equivariant GNN that is independent of the spatial orientation of the input mesh and show how this reduces the necessar
&lt;/p&gt;</description></item><item><title>GraphCast&#26159;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#20877;&#20998;&#26512;&#25968;&#25454;&#20013;&#35757;&#32451;&#65292;&#33021;&#22312;&#30701;&#26102;&#38388;&#20869;&#39044;&#27979;&#20840;&#29699;&#25968;&#30334;&#20010;&#22825;&#27668;&#21464;&#37327;&#65292;&#25903;&#25345;&#26356;&#22909;&#30340;&#20005;&#37325;&#20107;&#20214;&#39044;&#27979;&#65292;&#26159;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#22825;&#27668;&#39044;&#25253;&#30340;&#20851;&#38190;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2212.12794</link><description>&lt;p&gt;
GraphCast&#65306;&#23398;&#20064;&#23092;&#29087;&#30340;&#20013;&#31243;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;
&lt;/p&gt;
&lt;p&gt;
GraphCast: Learning skillful medium-range global weather forecasting. (arXiv:2212.12794v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12794
&lt;/p&gt;
&lt;p&gt;
GraphCast&#26159;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#20877;&#20998;&#26512;&#25968;&#25454;&#20013;&#35757;&#32451;&#65292;&#33021;&#22312;&#30701;&#26102;&#38388;&#20869;&#39044;&#27979;&#20840;&#29699;&#25968;&#30334;&#20010;&#22825;&#27668;&#21464;&#37327;&#65292;&#25903;&#25345;&#26356;&#22909;&#30340;&#20005;&#37325;&#20107;&#20214;&#39044;&#27979;&#65292;&#26159;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#22825;&#27668;&#39044;&#25253;&#30340;&#20851;&#38190;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#20013;&#31243;&#22825;&#27668;&#39044;&#25253;&#23545;&#20110;&#24456;&#22810;&#31038;&#20250;&#21644;&#32463;&#27982;&#39046;&#22495;&#30340;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#21033;&#29992;&#22686;&#21152;&#30340;&#35745;&#31639;&#36164;&#28304;&#25552;&#39640;&#39044;&#25253;&#20934;&#30830;&#24615;&#65292;&#20294;&#26080;&#27861;&#30452;&#25509;&#21033;&#29992;&#21382;&#21490;&#22825;&#27668;&#25968;&#25454;&#25913;&#36827;&#24213;&#23618;&#27169;&#22411;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;"GraphCast"&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#30452;&#25509;&#20174;&#20877;&#20998;&#26512;&#25968;&#25454;&#20013;&#36827;&#34892;&#35757;&#32451;&#12290;&#23427;&#22312;&#19981;&#21040;&#19968;&#20998;&#38047;&#30340;&#26102;&#38388;&#20869;&#65292;&#39044;&#27979;&#20840;&#29699;0.25&#24230;&#20998;&#36776;&#29575;&#19979;&#30340;&#25968;&#30334;&#20010;&#22825;&#27668;&#21464;&#37327;&#65292;&#25345;&#32493;10&#22825;&#12290;&#25105;&#20204;&#34920;&#26126;GraphCast&#22312;1380&#20010;&#39564;&#35777;&#30446;&#26631;&#30340;90&#65285;&#19978;&#26126;&#26174;&#20248;&#20110;&#26368;&#20934;&#30830;&#30340;&#25805;&#20316;&#30830;&#23450;&#24615;&#31995;&#32479;&#65292;&#24182;&#19988;&#20854;&#39044;&#25253;&#25903;&#25345;&#26356;&#22909;&#30340;&#20005;&#37325;&#20107;&#20214;&#39044;&#27979;&#65292;&#21253;&#25324;&#28909;&#24102;&#27668;&#26059;&#12289;&#22823;&#27668;&#27827;&#27969;&#21644;&#26497;&#31471;&#28201;&#24230;&#12290;GraphCast&#26159;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#22825;&#27668;&#39044;&#25253;&#30340;&#20851;&#38190;&#36827;&#23637;&#65292;&#24182;&#26377;&#21161;&#20110;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#22312;&#24314;&#27169;&#22797;&#26434;&#21160;&#21147;&#31995;&#32479;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Global medium-range weather forecasting is critical to decision-making across many social and economic domains. Traditional numerical weather prediction uses increased compute resources to improve forecast accuracy, but cannot directly use historical weather data to improve the underlying model. We introduce a machine learning-based method called "GraphCast", which can be trained directly from reanalysis data. It predicts hundreds of weather variables, over 10 days at 0.25 degree resolution globally, in under one minute. We show that GraphCast significantly outperforms the most accurate operational deterministic systems on 90% of 1380 verification targets, and its forecasts support better severe event prediction, including tropical cyclones, atmospheric rivers, and extreme temperatures. GraphCast is a key advance in accurate and efficient weather forecasting, and helps realize the promise of machine learning for modeling complex dynamical systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23558;&#20013;&#38388;&#27169;&#22411;&#21629;&#21517;&#20026; PMLP &#24182;&#22312;&#27979;&#35797;&#26102;&#37319;&#29992; GNNs &#30340;&#26550;&#26500;&#65292;&#21457;&#29616; GNNs &#30340;&#34920;&#29616;&#20986;&#20247;&#19981;&#26159;&#20854;&#39640;&#32423;&#34920;&#29616;&#21147;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#32780;&#26159;&#20854;&#22266;&#26377;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.09034</link><description>&lt;p&gt;
GNN &#21644; MLP &#30456;&#20114;&#32852;&#31995;&#25581;&#31034; GNN &#22312;&#26412;&#36136;&#19978;&#26159;&#22909;&#30340;&#27867;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks are Inherently Good Generalizers: Insights by Bridging GNNs and MLPs. (arXiv:2212.09034v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#20013;&#38388;&#27169;&#22411;&#21629;&#21517;&#20026; PMLP &#24182;&#22312;&#27979;&#35797;&#26102;&#37319;&#29992; GNNs &#30340;&#26550;&#26500;&#65292;&#21457;&#29616; GNNs &#30340;&#34920;&#29616;&#20986;&#20247;&#19981;&#26159;&#20854;&#39640;&#32423;&#34920;&#29616;&#21147;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#32780;&#26159;&#20854;&#22266;&#26377;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#20107;&#23454;&#19978;&#27169;&#22411;&#31867;&#21035;&#65292;&#23427;&#20204;&#24314;&#31435;&#22312;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#20307;&#31995;&#32467;&#26500;&#20043;&#19978;&#65292;&#24182;&#22686;&#21152;&#20102;&#39069;&#22806;&#30340;&#28040;&#24687;&#20256;&#36882;&#23618;&#20197;&#20801;&#35768;&#29305;&#24449;&#22312;&#33410;&#28857;&#20043;&#38388;&#27969;&#21160;&#12290;&#26412;&#25991;&#29468;&#27979; GNNs &#30340;&#34920;&#29616;&#20986;&#20247;&#19981;&#26159;&#20854;&#39640;&#32423;&#34920;&#29616;&#21147;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#32780;&#26159;&#20854;&#22266;&#26377;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#39033;&#21457;&#29616;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24335;&#26469;&#29702;&#35299; GNNs &#30340;&#23398;&#20064;&#34892;&#20026;&#65292;&#24182;&#21487;&#20197;&#29992;&#20316;&#26356;&#28145;&#23618;&#27425;&#30340;&#20998;&#26512;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs), as the de-facto model class for representation learning on graphs, are built upon the multi-layer perceptrons (MLP) architecture with additional message passing layers to allow features to flow across nodes. While conventional wisdom commonly attributes the success of GNNs to their advanced expressivity, we conjecture that this is not the main cause of GNNs' superiority in node-level prediction tasks. This paper pinpoints the major source of GNNs' performance gain to their intrinsic generalization capability, by introducing an intermediate model class dubbed as P(ropagational)MLP, which is identical to standard MLP in training, but then adopts GNN's architecture in testing. Intriguingly, we observe that PMLPs consistently perform on par with (or even exceed) their GNN counterparts, while being much more efficient in training. This finding sheds new insights into understanding the learning behavior of GNNs, and can be used as an analytic tool for dissecting
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;PAC-Bayesian&#29702;&#35770;&#25552;&#20379;&#20102;&#20803;&#23398;&#20064;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#26368;&#20339;&#24615;&#33021;&#20445;&#35777;&#30340;&#38381;&#24335;&#20248;&#21270;&#36229;&#21518;&#39564;(PACOH)&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#20445;&#35777;&#22312;&#20803;&#23398;&#20064;&#20013;&#30456;&#23545;&#20110;PAC-Bayesian&#27599;&#20010;&#20219;&#21153;&#23398;&#20064;&#30028;&#38480;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2211.07206</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;PAC-Bayesian&#20803;&#23398;&#20064;&#65306;&#20174;&#29702;&#35770;&#21040;&#23454;&#36341;&#30340;PAC-Optimal&#36229;&#21518;&#39564;
&lt;/p&gt;
&lt;p&gt;
Scalable PAC-Bayesian Meta-Learning via the PAC-Optimal Hyper-Posterior: From Theory to Practice. (arXiv:2211.07206v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;PAC-Bayesian&#29702;&#35770;&#25552;&#20379;&#20102;&#20803;&#23398;&#20064;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#26368;&#20339;&#24615;&#33021;&#20445;&#35777;&#30340;&#38381;&#24335;&#20248;&#21270;&#36229;&#21518;&#39564;(PACOH)&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#20445;&#35777;&#22312;&#20803;&#23398;&#20064;&#20013;&#30456;&#23545;&#20110;PAC-Bayesian&#27599;&#20010;&#20219;&#21153;&#23398;&#20064;&#30028;&#38480;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#20174;&#30456;&#20851;&#23398;&#20064;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#26377;&#29992;&#30340;&#24402;&#32435;&#20559;&#22909;&#65292;&#21152;&#36895;&#23545;&#26032;&#20219;&#21153;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#21487;&#29992;&#30340;&#30456;&#20851;&#20219;&#21153;&#25968;&#37327;&#36890;&#24120;&#24456;&#23567;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20551;&#35774;&#20219;&#21153;&#25968;&#37327;&#20016;&#23500;&#65292;&#20351;&#23427;&#20204;&#19981;&#20999;&#23454;&#38469;&#19988;&#23481;&#26131;&#36807;&#25311;&#21512;&#12290;&#20803;&#23398;&#20064;&#25991;&#29486;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#22914;&#20309;&#36827;&#34892;&#27491;&#21017;&#21270;&#20197;&#30830;&#20445;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;PAC-Bayesian&#29702;&#35770;&#25552;&#20379;&#20102;&#19968;&#31181;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#20803;&#23398;&#20064;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#36825;&#26159;&#30001;Rothfuss&#31561;&#20154;&#65288;2021&#65289;&#39318;&#27425;&#25512;&#23548;&#20986;&#26469;&#30340;&#12290;&#20851;&#38190;&#26159;&#65292;&#35813;&#30028;&#38480;&#20351;&#25105;&#20204;&#33021;&#22815;&#24471;&#21040;&#26368;&#20339;&#24615;&#33021;&#20445;&#35777;&#30340;&#38381;&#24335;&#20248;&#21270;&#36229;&#21518;&#39564;&#65292;&#31216;&#20026;PACOH&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#26696;&#20363;&#30740;&#31350;&#65292;&#22312;&#21738;&#20123;&#26465;&#20214;&#19979;&#20197;&#21450;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#36825;&#20123;&#20803;&#23398;&#20064;&#30340;&#20445;&#35777;&#25913;&#36827;&#20102;PAC-Bayesian&#27599;&#20010;&#20219;&#21153;&#23398;&#20064;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-Learning aims to speed up the learning process on new tasks by acquiring useful inductive biases from datasets of related learning tasks. While, in practice, the number of related tasks available is often small, most of the existing approaches assume an abundance of tasks; making them unrealistic and prone to overfitting. A central question in the meta-learning literature is how to regularize to ensure generalization to unseen tasks. In this work, we provide a theoretical analysis using the PAC-Bayesian theory and present a generalization bound for meta-learning, which was first derived by Rothfuss et al. (2021). Crucially, the bound allows us to derive the closed form of the optimal hyper-posterior, referred to as PACOH, which leads to the best performance guarantees. We provide a theoretical analysis and empirical case study under which conditions and to what extent these guarantees for meta-learning improve upon PAC-Bayesian per-task learning bounds. The closed-form PACOH inspi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#38544;&#24335;&#27969;&#20307;&#38381;&#21512;&#39033;&#30340;&#22810;&#30697;&#20415;&#27969;&#20307;&#27169;&#22411;&#65292;&#33021;&#22815;&#37325;&#29616;Landau&#38459;&#23612;&#30340;&#21160;&#21147;&#23398;&#29305;&#24449;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#19968;&#20010;&#34920;&#24449;&#27969;&#20307;-&#32454;&#32467;&#26500;&#30456;&#20114;&#20316;&#29992;&#30340;&#26032;&#21464;&#37327;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#23558;&#27969;&#20307;&#27169;&#22411;&#30340;&#24212;&#29992;&#33539;&#22260;&#25193;&#23637;&#21040;&#22810;&#23610;&#24230;&#31995;&#32479;&#65292;&#20026;&#30740;&#31350;&#31561;&#31163;&#23376;&#20307;&#21160;&#21147;&#23398;&#25552;&#20379;&#20102;&#24378;&#22823;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2211.01021</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#30340;Landau&#38459;&#23612;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Modeling of Landau Damping by Physics-Informed Neural Networks. (arXiv:2211.01021v2 [physics.plasm-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#38544;&#24335;&#27969;&#20307;&#38381;&#21512;&#39033;&#30340;&#22810;&#30697;&#20415;&#27969;&#20307;&#27169;&#22411;&#65292;&#33021;&#22815;&#37325;&#29616;Landau&#38459;&#23612;&#30340;&#21160;&#21147;&#23398;&#29305;&#24449;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#19968;&#20010;&#34920;&#24449;&#27969;&#20307;-&#32454;&#32467;&#26500;&#30456;&#20114;&#20316;&#29992;&#30340;&#26032;&#21464;&#37327;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#23558;&#27969;&#20307;&#27169;&#22411;&#30340;&#24212;&#29992;&#33539;&#22260;&#25193;&#23637;&#21040;&#22810;&#23610;&#24230;&#31995;&#32479;&#65292;&#20026;&#30740;&#31350;&#31561;&#31163;&#23376;&#20307;&#21160;&#21147;&#23398;&#25552;&#20379;&#20102;&#24378;&#22823;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24494;&#35266;&#31561;&#31163;&#23376;&#20307;&#29289;&#29702;&#38382;&#39064;&#20013;&#65292;&#21160;&#21147;&#23398;&#26041;&#27861;&#19968;&#33324;&#24456;&#20934;&#30830;&#65292;&#20294;&#23545;&#20110;&#22823;&#23610;&#24230;&#25110;&#22810;&#23610;&#24230;&#31995;&#32479;&#32780;&#35328;&#35745;&#31639;&#24320;&#38144;&#36739;&#22823;&#12290;&#31561;&#31163;&#23376;&#20307;&#29289;&#29702;&#23398;&#20013;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#20043;&#19968;&#26159;&#22914;&#20309;&#23558;&#21160;&#21147;&#23398;&#29289;&#29702;&#23398;&#25972;&#21512;&#21040;&#27969;&#20307;&#27169;&#22411;&#20013;&#65292;&#36890;&#24120;&#36890;&#36807;&#22797;&#26434;&#30340;&#35299;&#26512;&#38381;&#21512;&#39033;&#23454;&#29616;&#12290;&#26412;&#30740;&#31350;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#30697;&#20415;&#27969;&#20307;&#27169;&#22411;&#65292;&#20854;&#20013;&#31070;&#32463;&#32593;&#32476;&#21253;&#21547;&#38544;&#24335;&#27969;&#20307;&#38381;&#21512;&#39033;&#65292;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#21644;&#26799;&#24230;&#22686;&#24378;&#30340;&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#65288;gPINN&#65289;&#23545;&#23569;&#37327;&#31232;&#30095;&#37319;&#26679;&#30340;&#21160;&#21147;&#23398;&#20223;&#30495;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#20351;&#29992;PINN&#25110;gPINN&#26500;&#24314;&#30340;&#22810;&#30697;&#20415;&#27969;&#20307;&#27169;&#22411;&#33021;&#22815;&#37325;&#29616;Landau&#38459;&#23612;&#30340;&#30005;&#22330;&#33021;&#37327;&#26102;&#21464;&#28436;&#21270;&#65292;&#21253;&#25324;&#20854;&#38459;&#23612;&#29575;&#65292;&#20197;&#21450;&#21160;&#21147;&#23398;&#20223;&#30495;&#20013;&#30340;&#31561;&#31163;&#23376;&#20307;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#39318;&#27425;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#21464;&#37327;&#65292;&#21363;&#22810;&#30697;&#20415;&#38647;&#35834;&#24212;&#21147;&#24352;&#37327;&#65292;&#35813;&#21464;&#37327;&#34920;&#24449;&#27969;&#20307;-&#32454;&#32467;&#26500;&#30456;&#20114;&#20316;&#29992;&#65292;&#26159;&#29702;&#35299;&#31561;&#31163;&#23376;&#20307;&#22810;&#23610;&#24230;&#25928;&#24212;&#30340;&#20851;&#38190;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#23558;&#27969;&#20307;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#25193;&#23637;&#21040;&#22810;&#23610;&#24230;&#31995;&#32479;&#65292;&#20026;&#30740;&#31350;&#31561;&#31163;&#23376;&#20307;&#21160;&#21147;&#23398;&#25552;&#20379;&#20102;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kinetic approaches are generally accurate in dealing with microscale plasma physics problems but are computationally expensive for large-scale or multiscale systems. One of the long-standing problems in plasma physics is the integration of kinetic physics into fluid models, which is often achieved through sophisticated analytical closure terms. In this study, we successfully construct a multi-moment fluid model with an implicit fluid closure included in the neural network using machine learning. The multi-moment fluid model is trained with a small fraction of sparsely sampled data from kinetic simulations of Landau damping, using the physics-informed neural network (PINN) and the gradient-enhanced physics-informed neural network (gPINN). The multi-moment fluid model constructed using either PINN or gPINN reproduces the time evolution of the electric field energy, including its damping rate, and the plasma dynamics from the kinetic simulations. For the first time, we introduce a new var
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#31181;&#20855;&#26377;&#22266;&#23450;&#38544;&#34255;&#23618;&#20998;&#24067;&#30340;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36873;&#25321;&#31616;&#21333;&#12289;&#26131;&#35299;&#37322;&#65292;&#19981;&#38656;&#35201;&#36807;&#24230;&#21442;&#25968;&#21270;&#65292;&#21516;&#26102;&#35757;&#32451;&#26377;&#25928;&#12290;&#27169;&#22411;&#30340;&#38544;&#34255;&#21333;&#20803;&#20026;&#20108;&#20803;&#21464;&#37327;&#26102;&#20855;&#26377;&#20197;&#29305;&#24449;&#20026;&#22522;&#30784;&#30340;&#33258;&#28982;&#35299;&#37322;&#12290;&#20316;&#32773;&#35748;&#20026;&#38544;&#34255;&#21464;&#37327;&#30340;&#20998;&#24067;&#24212;&#35813;&#36981;&#24490;&#26368;&#22823;&#20851;&#32852;&#24230;&#21407;&#21017;&#65292;&#24182;&#20171;&#32461;&#20102;&#20998;&#23618;&#29305;&#24449;&#27169;&#22411;&#65288;HFM&#65289;&#20316;&#20026;&#28385;&#36275;&#36825;&#19968;&#21407;&#21017;&#24182;&#23545;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;&#20013;&#24615;&#20808;&#39564;&#32452;&#32455;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2210.13179</link><description>&lt;p&gt;
Occam&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Occam learning. (arXiv:2210.13179v2 [cond-mat.dis-nn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#31181;&#20855;&#26377;&#22266;&#23450;&#38544;&#34255;&#23618;&#20998;&#24067;&#30340;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36873;&#25321;&#31616;&#21333;&#12289;&#26131;&#35299;&#37322;&#65292;&#19981;&#38656;&#35201;&#36807;&#24230;&#21442;&#25968;&#21270;&#65292;&#21516;&#26102;&#35757;&#32451;&#26377;&#25928;&#12290;&#27169;&#22411;&#30340;&#38544;&#34255;&#21333;&#20803;&#20026;&#20108;&#20803;&#21464;&#37327;&#26102;&#20855;&#26377;&#20197;&#29305;&#24449;&#20026;&#22522;&#30784;&#30340;&#33258;&#28982;&#35299;&#37322;&#12290;&#20316;&#32773;&#35748;&#20026;&#38544;&#34255;&#21464;&#37327;&#30340;&#20998;&#24067;&#24212;&#35813;&#36981;&#24490;&#26368;&#22823;&#20851;&#32852;&#24230;&#21407;&#21017;&#65292;&#24182;&#20171;&#32461;&#20102;&#20998;&#23618;&#29305;&#24449;&#27169;&#22411;&#65288;HFM&#65289;&#20316;&#20026;&#28385;&#36275;&#36825;&#19968;&#21407;&#21017;&#24182;&#23545;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;&#20013;&#24615;&#20808;&#39564;&#32452;&#32455;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22312;&#36825;&#31181;&#27169;&#22411;&#20013;&#65292;&#38544;&#34255;&#23618;&#30340;&#20998;&#24067;&#26159;&#22266;&#23450;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#37319;&#29992;&#36825;&#31181;&#20307;&#31995;&#26550;&#26500;&#30340;&#26426;&#22120;&#23398;&#20064;&#20855;&#26377;&#35768;&#22810;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#36136;&#12290;&#20363;&#22914;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#36873;&#25321;&#20026;&#31616;&#21333;&#19988;&#26131;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#19981;&#38656;&#35201;&#36807;&#24230;&#21442;&#25968;&#21270;&#65292;&#32780;&#19988;&#22312;&#28909;&#21147;&#23398;&#24847;&#20041;&#19979;&#65292;&#35757;&#32451;&#26356;&#26377;&#25928;&#12290;&#24403;&#38544;&#34255;&#21333;&#20803;&#20026;&#20108;&#20803;&#21464;&#37327;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#20197;&#29305;&#24449;&#20026;&#22522;&#30784;&#30340;&#33258;&#28982;&#35299;&#37322;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#32570;&#20047;&#29305;&#24449;&#30340;&#29366;&#24577;&#23545;&#24212;&#20110;&#22312;&#29305;&#24449;&#26041;&#38754;&#26368;&#22823;&#31243;&#24230;&#30340;&#26080;&#30693;&#29366;&#24577;&#65292;&#24182;&#19988;&#65292;&#23398;&#20064;&#31532;&#19968;&#20010;&#29305;&#24449;&#21462;&#20915;&#20110;&#25968;&#25454;&#30340;&#38750;&#39640;&#26031;&#32479;&#35745;&#23646;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#24212;&#35813;&#26681;&#25454;&#26368;&#22823;&#20851;&#32852;&#24230;&#21407;&#21017;&#36873;&#25321;&#38544;&#34255;&#21464;&#37327;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20998;&#23618;&#29305;&#24449;&#27169;&#22411;&#65288;HFM&#65289;&#20316;&#20026;&#28385;&#36275;&#36825;&#19968;&#21407;&#21017;&#24182;&#23545;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;&#20013;&#24615;&#20808;&#39564;&#32452;&#32455;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We discuss probabilistic neural network models for unsupervised learning where the distribution of the hidden layer is fixed. We argue that learning machines with this architecture enjoy a number of desirable properties. For example, the model can be chosen as a simple and interpretable one, it does not need to be over-parametrised and training is argued to be efficient in a thermodynamic sense. When hidden units are binary variables, these models have a natural interpretation in terms of features. We show that the featureless state corresponds to a state of maximal ignorance about the features and that learning the first feature depends on non-Gaussian statistical properties of the data. We suggest that the distribution of hidden variables should be chosen according to the principle of maximal relevance. We introduce the Hierarchical Feature Model (HFM) as an example of a model that satisfies this principle, and that encodes a neutral a priori organisation of the feature space. We pre
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26088;&#22312;&#35299;&#20915;&#22269;&#38469;&#31354;&#38388;&#31449;&#19978;&#39063;&#31890;&#29289;&#23545;&#20202;&#22120;&#30340;&#21361;&#23475;&#38382;&#39064;&#65292;&#36890;&#36807;Bi-GRU&#31639;&#27861;&#26500;&#24314;&#26089;&#26399;&#39044;&#35686;&#31995;&#32479;&#65292;&#39044;&#27979;&#39063;&#31890;&#29289;&#27700;&#24179;&#65292;&#24182;&#20026;&#23431;&#33322;&#21592;&#25552;&#20379;&#20805;&#36275;&#30340;&#21453;&#24212;&#26102;&#38388;&#12290;&#36825;&#39033;&#30740;&#31350;&#36824;&#26377;&#28508;&#21147;&#21457;&#23637;&#20026;&#19982;&#28779;&#28798;&#30456;&#20851;&#30340;&#36965;&#24863;&#28895;&#38654;&#25253;&#35686;&#35013;&#32622;&#12290;</title><link>http://arxiv.org/abs/2210.08549</link><description>&lt;p&gt;
&#22269;&#38469;&#31354;&#38388;&#31449;&#33258;&#21160;&#32039;&#24613;&#26080;&#23576;&#35299;&#20915;&#26041;&#26696;: &#24102;&#26377;Bi-GRU&#30340;(AED-ISS)
&lt;/p&gt;
&lt;p&gt;
Automatic Emergency Dust-Free solution on-board International Space Station with Bi-GRU (AED-ISS). (arXiv:2210.08549v2 [stat.AP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08549
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26088;&#22312;&#35299;&#20915;&#22269;&#38469;&#31354;&#38388;&#31449;&#19978;&#39063;&#31890;&#29289;&#23545;&#20202;&#22120;&#30340;&#21361;&#23475;&#38382;&#39064;&#65292;&#36890;&#36807;Bi-GRU&#31639;&#27861;&#26500;&#24314;&#26089;&#26399;&#39044;&#35686;&#31995;&#32479;&#65292;&#39044;&#27979;&#39063;&#31890;&#29289;&#27700;&#24179;&#65292;&#24182;&#20026;&#23431;&#33322;&#21592;&#25552;&#20379;&#20805;&#36275;&#30340;&#21453;&#24212;&#26102;&#38388;&#12290;&#36825;&#39033;&#30740;&#31350;&#36824;&#26377;&#28508;&#21147;&#21457;&#23637;&#20026;&#19982;&#28779;&#28798;&#30456;&#20851;&#30340;&#36965;&#24863;&#28895;&#38654;&#25253;&#35686;&#35013;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;PM2.5&#25110;PM0.3&#38382;&#39064;&#30340;&#20851;&#27880;&#19981;&#26029;&#22686;&#21152;&#65292;&#39063;&#31890;&#29289;&#19981;&#20165;&#23545;&#29615;&#22659;&#21644;&#20154;&#31867;&#26500;&#25104;&#28508;&#22312;&#23041;&#32961;&#65292;&#32780;&#19988;&#23545;&#22269;&#38469;&#31354;&#38388;&#31449;&#19978;&#30340;&#20202;&#22120;&#20063;&#20250;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#22242;&#38431;&#26088;&#22312;&#23558;&#21508;&#31181;&#39063;&#31890;&#29289;&#27987;&#24230;&#19982;&#30913;&#22330;&#12289;&#28287;&#24230;&#12289;&#21152;&#36895;&#24230;&#12289;&#28201;&#24230;&#12289;&#21387;&#21147;&#21644;CO2&#27987;&#24230;&#20851;&#32852;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24314;&#31435;&#19968;&#20010;&#26089;&#26399;&#39044;&#35686;&#31995;&#32479;(EWS)&#65292;&#33021;&#22815;&#39044;&#27979;&#39063;&#31890;&#29289;&#27700;&#24179;&#65292;&#24182;&#20026;&#23431;&#33322;&#21592;&#25552;&#20379;&#20805;&#36275;&#30340;&#21453;&#24212;&#26102;&#38388;&#65292;&#20197;&#20445;&#25252;&#20182;&#20204;&#22312;&#26576;&#20123;&#23454;&#39564;&#20013;&#30340;&#20202;&#22120;&#65292;&#25110;&#32773;&#25552;&#39640;&#27979;&#37327;&#30340;&#20934;&#30830;&#24615;&#65307;&#27492;&#22806;&#65292;&#25152;&#26500;&#24314;&#30340;&#27169;&#22411;&#36824;&#21487;&#20197;&#36827;&#19968;&#27493;&#21457;&#23637;&#20026;&#19982;&#28779;&#28798;&#30456;&#20851;&#30340;&#36965;&#24863;&#28895;&#38654;&#25253;&#35686;&#35013;&#32622;&#30340;&#21407;&#22411;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#23454;&#29616;Bi-GRU(&#21452;&#21521;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;)&#31639;&#27861;&#65292;&#25910;&#38598;&#36807;&#21435;90&#20998;&#38047;&#30340;&#25968;&#25454;&#65292;&#24182;&#39044;&#27979;&#36229;&#36807;2.5&#24494;&#31859;&#30340;&#39063;&#31890;&#29289;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
With a rising attention for the issue of PM2.5 or PM0.3, particulate matters have become not only a potential threat to both the environment and human, but also a harming existence to instruments onboard International Space Station (ISS). Our team is aiming to relate various concentration of particulate matters to magnetic fields, humidity, acceleration, temperature, pressure and CO2 concentration. Our goal is to establish an early warning system (EWS), which is able to forecast the levels of particulate matters and provides ample reaction time for astronauts to protect their instruments in some experiments or increase the accuracy of the measurements; In addition, the constructed model can be further developed into a prototype of a remote-sensing smoke alarm for applications related to fires. In this article, we will implement the Bi-GRU (Bidirectional Gated Recurrent Unit) algorithms that collect data for past 90 minutes and predict the levels of particulates which over 2.5 micromete
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#38750;&#38646;&#26799;&#24230;&#30340;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#34892;&#20026;&#65292;&#24182;&#35777;&#26126;&#20102;&#25351;&#25968;&#32423;&#30340;&#38598;&#20013;&#24615;&#30028;&#38480;&#65292;&#36825;&#23545;&#20110;&#25237;&#24433;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2208.07243</link><description>&lt;p&gt;
&#38750;&#38646;&#26799;&#24230;&#30340;&#38543;&#26426;&#36924;&#36817;&#30340;&#25351;&#25968;&#38598;&#20013;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Exponential Concentration of Stochastic Approximation with Non-vanishing Gradient. (arXiv:2208.07243v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#38750;&#38646;&#26799;&#24230;&#30340;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#34892;&#20026;&#65292;&#24182;&#35777;&#26126;&#20102;&#25351;&#25968;&#32423;&#30340;&#38598;&#20013;&#24615;&#30028;&#38480;&#65292;&#36825;&#23545;&#20110;&#25237;&#24433;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#34892;&#20026;&#65292;&#20854;&#20013;&#27599;&#19968;&#27493;&#36845;&#20195;&#65292;&#26399;&#26395;&#20013;&#21521;&#30446;&#26631;&#21462;&#24471;&#36827;&#23637;&#12290;&#24403;&#36827;&#23637;&#19982;&#31639;&#27861;&#30340;&#27493;&#38271;&#25104;&#27604;&#20363;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25351;&#25968;&#32423;&#30340;&#38598;&#20013;&#24615;&#30028;&#38480;&#12290;&#36825;&#20123;&#23614;&#37096;&#30028;&#38480;&#19982;&#26356;&#24120;&#35265;&#30340;&#38543;&#26426;&#36924;&#36817;&#30340;&#28176;&#36817;&#27491;&#24577;&#32467;&#26524;&#24418;&#25104;&#23545;&#27604;&#12290;&#25105;&#20204;&#24320;&#21457;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#20960;&#20309;&#38459;&#23612;&#24615;&#35777;&#26126;&#12290;&#36825;&#25193;&#23637;&#20102;Hajek&#65288;1982&#65289;&#23545;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#32467;&#26524;&#21040;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#39046;&#22495;&#12290;&#23545;&#20110;&#20855;&#26377;&#38750;&#38646;&#26799;&#24230;&#30340;&#25237;&#24433;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#21487;&#20197;&#29992;&#26469;&#35777;&#26126;$O(1/t)$&#21644;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze the behavior of stochastic approximation algorithms where iterates, in expectation, make progress towards an objective at each step. When progress is proportional to the step size of the algorithm, we prove exponential concentration bounds. These tail-bounds contrast asymptotic normality results which are more frequently associated with stochastic approximation. The methods that we develop rely on a geometric ergodicity proof. This extends a result on Markov chains due to Hajek (1982) to the area of stochastic approximation algorithms. For Projected Stochastic Gradient Descent with a non-vanishing gradient, our results can be used to prove $O(1/t)$ and linear convergence rates.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32534;&#30721;&#31639;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#29305;&#24449;&#20540;&#30340;&#39057;&#29575;&#21644;&#19978;&#19979;&#25991;&#65292;&#23545;NetFlow&#25968;&#25454;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#20197;&#25552;&#39640;&#32593;&#32476;&#24322;&#24120;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2207.03890</link><description>&lt;p&gt;
ENCODE&#65306;&#29992;&#20110;&#32593;&#32476;&#24322;&#24120;&#26816;&#27979;&#30340;&#32534;&#30721;NetFlows
&lt;/p&gt;
&lt;p&gt;
ENCODE: Encoding NetFlows for Network Anomaly Detection. (arXiv:2207.03890v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.03890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32534;&#30721;&#31639;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#29305;&#24449;&#20540;&#30340;&#39057;&#29575;&#21644;&#19978;&#19979;&#25991;&#65292;&#23545;NetFlow&#25968;&#25454;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#20197;&#25552;&#39640;&#32593;&#32476;&#24322;&#24120;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NetFlow&#25968;&#25454;&#26159;&#35768;&#22810;&#32593;&#32476;&#20998;&#26512;&#24072;&#21644;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#30340;&#27969;&#34892;&#32593;&#32476;&#26085;&#24535;&#26684;&#24335;&#12290;&#19982;&#28145;&#24230;&#21253;&#26816;&#26597;&#30456;&#27604;&#65292;&#20351;&#29992;NetFlow&#30340;&#20248;&#21183;&#22312;&#20110;&#26356;&#26131;&#20110;&#25910;&#38598;&#21644;&#22788;&#29702;&#65292;&#23545;&#38544;&#31169;&#24615;&#30340;&#20405;&#20837;&#24615;&#26356;&#23567;&#12290;&#35768;&#22810;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#20351;&#29992;NetFlow&#25968;&#25454;&#26816;&#27979;&#32593;&#32476;&#25915;&#20987;&#12290;&#36825;&#20123;&#26426;&#22120;&#23398;&#20064;&#27969;&#27700;&#32447;&#30340;&#31532;&#19968;&#27493;&#26159;&#22312;&#23558;&#25968;&#25454;&#25552;&#20379;&#32473;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20043;&#21069;&#23545;&#25968;&#25454;&#36827;&#34892;&#39044;&#22788;&#29702;&#12290;&#23384;&#22312;&#35768;&#22810;&#26041;&#27861;&#26469;&#39044;&#22788;&#29702;NetFlow&#25968;&#25454;&#65307;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21482;&#26159;&#31616;&#21333;&#22320;&#23558;&#29616;&#26377;&#26041;&#27861;&#24212;&#29992;&#20110;&#25968;&#25454;&#65292;&#32780;&#19981;&#32771;&#34385;&#32593;&#32476;&#25968;&#25454;&#30340;&#29305;&#23450;&#23646;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#23545;&#20110;&#28304;&#33258;&#36719;&#20214;&#31995;&#32479;&#65288;&#22914;NetFlow&#25110;&#36719;&#20214;&#26085;&#24535;&#65289;&#30340;&#25968;&#25454;&#65292;&#29305;&#24449;&#20540;&#30340;&#39057;&#29575;&#21644;&#19978;&#19979;&#25991;&#30340;&#30456;&#20284;&#24615;&#27604;&#20540;&#26412;&#36523;&#30340;&#30456;&#20284;&#24615;&#26356;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32534;&#30721;&#31639;&#27861;&#65292;&#22312;&#22788;&#29702;&#25968;&#25454;&#26102;&#30452;&#25509;&#32771;&#34385;&#29305;&#24449;&#20540;&#30340;&#39057;&#29575;&#21644;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
NetFlow data is a popular network log format used by many network analysts and researchers. The advantages of using NetFlow over deep packet inspection are that it is easier to collect and process, and it is less privacy intrusive. Many works have used machine learning to detect network attacks using NetFlow data. The first step for these machine learning pipelines is to pre-process the data before it is given to the machine learning algorithm. Many approaches exist to pre-process NetFlow data; however, these simply apply existing methods to the data, not considering the specific properties of network data. We argue that for data originating from software systems, such as NetFlow or software logs, similarities in frequency and contexts of feature values are more important than similarities in the value itself. In this work, we propose an encoding algorithm that directly takes the frequency and the context of the feature values into account when the data is being processed. Different ty
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25512;&#29702;&#30340;&#37327;&#23376;&#24863;&#30693;&#26041;&#26696;&#65292;&#21487;&#20197;&#36890;&#36807;&#27979;&#37327;&#31995;&#32479;&#21709;&#24212;&#26469;&#25512;&#26029;&#26410;&#30693;&#21442;&#25968;&#30340;&#20540;&#65292;&#24182;&#30830;&#23450;&#26041;&#26696;&#30340;&#25935;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.09919</link><description>&lt;p&gt;
&#25512;&#29702;&#20026;&#22522;&#30784;&#30340;&#37327;&#23376;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Inference-Based Quantum Sensing. (arXiv:2206.09919v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09919
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25512;&#29702;&#30340;&#37327;&#23376;&#24863;&#30693;&#26041;&#26696;&#65292;&#21487;&#20197;&#36890;&#36807;&#27979;&#37327;&#31995;&#32479;&#21709;&#24212;&#26469;&#25512;&#26029;&#26410;&#30693;&#21442;&#25968;&#30340;&#20540;&#65292;&#24182;&#30830;&#23450;&#26041;&#26696;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26631;&#20934;&#30340;&#37327;&#23376;&#24863;&#30693;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#23545;&#31995;&#32479;&#30340;&#27979;&#37327;&#65292;&#30446;&#26631;&#26159;&#20272;&#35745;&#19968;&#20010;&#26410;&#30693;&#30340;&#21442;&#25968;&#952;&#65292;&#23558;&#20854;&#32534;&#30721;&#21040;&#19968;&#20010;n&#27604;&#29305;&#30340;&#25506;&#38024;&#24577;&#20013;&#12290;&#36825;&#20010;&#20219;&#21153;&#30340;&#25104;&#21151;&#21462;&#20915;&#20110;&#23558;&#21442;&#25968;&#30340;&#21464;&#21270;&#19982;&#31995;&#32479;&#21709;&#24212;&#929;&#65288;&#952;&#65289;&#30340;&#21464;&#21270;&#65288;&#21363;&#27979;&#37327;&#32467;&#26524;&#30340;&#21464;&#21270;&#65289;&#30456;&#20851;&#32852;&#30340;&#33021;&#21147;&#12290;&#23545;&#20110;&#31616;&#21333;&#24773;&#20917;&#65292;&#929;&#65288;&#952;&#65289;&#30340;&#24418;&#24335;&#26159;&#24050;&#30693;&#30340;&#65292;&#20294;&#23545;&#20110;&#29616;&#23454;&#22330;&#26223;&#26469;&#35828;&#65292;&#19968;&#33324;&#19981;&#23384;&#22312;&#38381;&#21512;&#24418;&#24335;&#30340;&#34920;&#36798;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25512;&#29702;&#30340;&#37327;&#23376;&#24863;&#30693;&#26041;&#26696;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#19968;&#31867;&#24120;&#35268;&#30340;&#32534;&#30721;&#30340;&#37193;&#26063;&#31995;&#21015;&#65292;&#21482;&#38656;&#22312;2n+1&#20010;&#21442;&#25968;&#22788;&#27979;&#37327;&#31995;&#32479;&#21709;&#24212;&#65292;&#23601;&#21487;&#20197;&#23436;&#20840;&#34920;&#24449;&#929;&#65288;&#952;&#65289;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#26681;&#25454;&#27979;&#37327;&#30340;&#21709;&#24212;&#25512;&#26029;&#26410;&#30693;&#21442;&#25968;&#30340;&#20540;&#65292;&#20197;&#21450;&#30830;&#23450;&#35813;&#26041;&#26696;&#30340;&#25935;&#24863;&#24615;&#65292;&#20174;&#32780;&#34920;&#24449;&#20854;&#25972;&#20307;&#24615;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25512;&#29702;&#35823;&#24046;&#22312;&#39640;&#27010;&#29575;&#19979;&#23567;&#20110;...
&lt;/p&gt;
&lt;p&gt;
In a standard Quantum Sensing (QS) task one aims at estimating an unknown parameter $\theta$, encoded into an $n$-qubit probe state, via measurements of the system. The success of this task hinges on the ability to correlate changes in the parameter to changes in the system response $\mathcal{R}(\theta)$ (i.e., changes in the measurement outcomes). For simple cases the form of $\mathcal{R}(\theta)$ is known, but the same cannot be said for realistic scenarios, as no general closed-form expression exists. In this work we present an inference-based scheme for QS. We show that, for a general class of unitary families of encoding, $\mathcal{R}(\theta)$ can be fully characterized by only measuring the system response at $2n+1$ parameters. This allows us to infer the value of an unknown parameter given the measured response, as well as to determine the sensitivity of the scheme, which characterizes its overall performance. We show that inference error is, with high probability, smaller than 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36951;&#25022;&#20998;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#39304;&#22270;&#21010;&#20998;&#30340;&#26032;&#31639;&#27861;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#32479;&#19968;&#20102;&#22810;&#33218;&#36172;&#21338;&#26426;&#21644;&#23398;&#20064;&#19982;&#19987;&#23478;&#24314;&#35758;&#30340;&#26368;&#20248;&#31639;&#27861;&#65292;&#23545;&#21253;&#25324;&#35768;&#22810;&#22270;&#31867;&#26063;&#22312;&#20869;&#30340;&#33539;&#22260;&#26356;&#24191;&#30340;&#22270;&#23478;&#26063;&#24471;&#21040;&#20102;&#25913;&#36827;&#21644;&#26368;&#20248;&#36951;&#25022;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2205.15076</link><description>&lt;p&gt;
&#36890;&#36807;&#36951;&#25022;&#20998;&#35299;&#25913;&#36827;&#20102;&#24102;&#26377;&#22270;&#21453;&#39304;&#30340;&#36172;&#21338;&#26426;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improved Algorithms for Bandit with Graph Feedback via Regret Decomposition. (arXiv:2205.15076v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15076
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36951;&#25022;&#20998;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#39304;&#22270;&#21010;&#20998;&#30340;&#26032;&#31639;&#27861;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#32479;&#19968;&#20102;&#22810;&#33218;&#36172;&#21338;&#26426;&#21644;&#23398;&#20064;&#19982;&#19987;&#23478;&#24314;&#35758;&#30340;&#26368;&#20248;&#31639;&#27861;&#65292;&#23545;&#21253;&#25324;&#35768;&#22810;&#22270;&#31867;&#26063;&#22312;&#20869;&#30340;&#33539;&#22260;&#26356;&#24191;&#30340;&#22270;&#23478;&#26063;&#24471;&#21040;&#20102;&#25913;&#36827;&#21644;&#26368;&#20248;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24102;&#26377;&#22270;&#21453;&#39304;&#30340;&#36172;&#21338;&#26426;&#38382;&#39064;&#36890;&#36807;&#22312;&#26377;&#21521;&#22270;&#20013;&#32534;&#30721;&#25439;&#22833;&#21521;&#37327;&#22312;&#27599;&#36718;&#28216;&#25103;&#20013;&#30340;&#35266;&#27979;&#26041;&#24335;&#65292;&#23558;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#21644;&#23398;&#20064;&#19982;&#19987;&#23478;&#24314;&#35758;&#38382;&#39064;&#36827;&#34892;&#20102;&#25512;&#24191;&#12290;&#26368;&#23567;&#26368;&#22823;&#36951;&#25022;&#19982;&#21453;&#39304;&#22270;&#30340;&#32467;&#26500;&#23494;&#20999;&#30456;&#20851;&#65292;&#20294;&#20108;&#32773;&#30340;&#20851;&#31995;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#39304;&#22270;&#21010;&#20998;&#30340;&#26032;&#31639;&#27861;&#26694;&#26550;&#12290;&#36890;&#36807;&#23558;&#36951;&#25022;&#20998;&#35299;&#20026;&#30001;&#23567;&#37096;&#20998;&#21644;&#23427;&#20204;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#24341;&#36215;&#30340;&#36951;&#25022;&#20043;&#21644;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#22270;&#30340;&#21508;&#20010;&#37096;&#20998;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#22810;&#33218;&#36172;&#21338;&#26426;&#21644;&#23398;&#20064;&#19982;&#19987;&#23478;&#24314;&#35758;&#30340;&#26368;&#20248;&#31639;&#27861;&#30340;&#25554;&#20540;&#21644;&#25512;&#24191;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#32479;&#19968;&#20102;&#20808;&#21069;&#38024;&#23545;&#24378;&#35266;&#27979;&#22270;&#21644;&#24369;&#35266;&#27979;&#22270;&#30340;&#31639;&#27861;&#65292;&#23545;&#21253;&#25324;&#35768;&#22810;&#22270;&#31867;&#26063;&#22312;&#20869;&#30340;&#33539;&#22260;&#26356;&#24191;&#30340;&#22270;&#23478;&#26063;&#24471;&#21040;&#20102;&#25913;&#36827;&#21644;&#26368;&#20248;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of bandit with graph feedback generalizes both the multi-armed bandit (MAB) problem and the learning with expert advice problem by encoding in a directed graph how the loss vector can be observed in each round of the game. The mini-max regret is closely related to the structure of the feedback graph and their connection is far from being fully understood. We propose a new algorithmic framework for the problem based on a partition of the feedback graph. Our analysis reveals the interplay between various parts of the graph by decomposing the regret to the sum of the regret caused by small parts and the regret caused by their interaction. As a result, our algorithm can be viewed as an interpolation and generalization of the optimal algorithms for MAB and learning with expert advice. Our framework unifies previous algorithms for both strongly observable graphs and weakly observable graphs, resulting in improved and optimal regret bounds on a wide range of graph families includi
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31561;&#24335;&#32422;&#26463;&#30340;&#38543;&#26426;&#38750;&#32447;&#24615;&#20248;&#21270;&#38382;&#39064;&#30340;&#32479;&#35745;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#33609;&#22270;&#30340;&#39034;&#24207;&#20108;&#27425;&#35268;&#21010;&#65288;StoSQP&#65289;&#36827;&#34892;&#27714;&#35299;&#65292;&#24182;&#19988;&#20801;&#35768;&#33258;&#36866;&#24212;&#36873;&#25321;&#38543;&#26426;&#27493;&#38271;&#21644;&#20351;&#29992;&#39640;&#25928;&#38543;&#26426;&#36845;&#20195;&#27714;&#35299;&#22120;&#26469;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2205.13687</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#33609;&#22270;&#30340;&#39034;&#24207;&#20108;&#27425;&#35268;&#21010;&#23545;&#32422;&#26463;&#30340;&#38543;&#26426;&#20248;&#21270;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Statistical Inference of Constrained Stochastic Optimization via Sketched Sequential Quadratic Programming. (arXiv:2205.13687v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31561;&#24335;&#32422;&#26463;&#30340;&#38543;&#26426;&#38750;&#32447;&#24615;&#20248;&#21270;&#38382;&#39064;&#30340;&#32479;&#35745;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#33609;&#22270;&#30340;&#39034;&#24207;&#20108;&#27425;&#35268;&#21010;&#65288;StoSQP&#65289;&#36827;&#34892;&#27714;&#35299;&#65292;&#24182;&#19988;&#20801;&#35768;&#33258;&#36866;&#24212;&#36873;&#25321;&#38543;&#26426;&#27493;&#38271;&#21644;&#20351;&#29992;&#39640;&#25928;&#38543;&#26426;&#36845;&#20195;&#27714;&#35299;&#22120;&#26469;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#23545;&#31561;&#24335;&#32422;&#26463;&#30340;&#38543;&#26426;&#38750;&#32447;&#24615;&#20248;&#21270;&#38382;&#39064;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20840;&#22312;&#32447;&#38543;&#26426;&#39034;&#24207;&#20108;&#27425;&#35268;&#21010;&#65288;StoSQP&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#21487;&#20197;&#23558;&#20854;&#35270;&#20026;&#23558;&#29275;&#39039;&#27861;&#24212;&#29992;&#20110;&#19968;&#38454;&#26368;&#20248;&#24615;&#26465;&#20214;&#65288;&#21363;KKT&#26465;&#20214;&#65289;&#12290;&#21463;&#26368;&#36817;&#25968;&#20540;&#20108;&#38454;&#26041;&#27861;&#35774;&#35745;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20801;&#35768;StoSQP&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#20219;&#24847;&#38543;&#26426;&#27493;&#38271;$ \bar {\ alpha} _t $&#65292;&#21482;&#35201;$ \ beta _t \ leq \ bar {\ alpha} _t \ leq \ beta _t + \ chi _t $&#65292;&#20854;&#20013; $ \ beta_t $ &#21644; $ \ chi_t = o(\beta_t) $ &#26159;&#26576;&#20123;&#25511;&#21046;&#24207;&#21015;&#12290;&#20026;&#20102;&#38477;&#20302;&#20108;&#38454;&#26041;&#27861;&#30340;&#20027;&#35201;&#35745;&#31639;&#25104;&#26412;&#65292;&#25105;&#20204;&#36824;&#20801;&#35768;StoSQP&#36890;&#36807;&#20351;&#29992;&#33609;&#22270;&#25216;&#26415;&#30340;&#39640;&#25928;&#38543;&#26426;&#36845;&#20195;&#27714;&#35299;&#22120;&#26469;&#19981;&#31934;&#30830;&#22320;&#35299;&#20915;&#20108;&#27425;&#35268;&#21010;&#38382;&#39064;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#19981;&#35201;&#27714;&#36924;&#36817;&#35823;&#24046;&#38543;&#30528;&#36845;&#20195;&#30340;&#36827;&#34892;&#32780;&#20943;&#23567;&#12290;&#23545;&#20110;&#24320;&#21457;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#65288;i&#65289;&#19979;&#65292;&#23427;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#26368;&#22810;&#20026;$ O(1 / \ ep&#65289;$&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider statistical inference of equality-constrained stochastic nonlinear optimization problems. We develop a fully online stochastic sequential quadratic programming (StoSQP) method to solve the problems, which can be regarded as applying Newton's method to the first-order optimality conditions (i.e., the KKT conditions). Motivated by recent designs of numerical second-order methods, we allow StoSQP to adaptively select any random stepsize $\bar{\alpha}_t$, as long as $\beta_t\leq \bar{\alpha}_t \leq \beta_t+\chi_t$, for some control sequences $\beta_t$ and $\chi_t=o(\beta_t)$. To reduce the dominant computational cost of second-order methods, we additionally allow StoSQP to inexactly solve quadratic programs via efficient randomized iterative solvers that utilize sketching techniques. Notably, we do not require the approximation error to diminish as iteration proceeds. For the developed method, we show that under mild assumptions (i) computationally, it can take at most $O(1/\ep
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#22312;&#32447;Mondrian Forest&#20998;&#31867;&#31639;&#27861;&#36866;&#24212;&#21040;&#22312;&#25968;&#25454;&#27969;&#19978;&#20855;&#26377;&#20869;&#23384;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#24182;&#35774;&#35745;&#20102;&#20869;&#23384;&#19981;&#36275;&#31574;&#30053;&#21644;&#20462;&#21098;&#26426;&#21046;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#25152;&#26377;&#37197;&#32622;&#20013;&#65292;Extend Node&#31574;&#30053;&#26159;&#26368;&#20339;&#30340;&#20869;&#23384;&#19981;&#36275;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2205.07871</link><description>&lt;p&gt;
&#22312;&#20869;&#23384;&#38480;&#21046;&#19979;&#29992;&#20110;&#25968;&#25454;&#27969;&#20998;&#31867;&#30340;Mondrian Forest&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Mondrian Forest for Data Stream Classification Under Memory Constraints. (arXiv:2205.07871v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.07871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#22312;&#32447;Mondrian Forest&#20998;&#31867;&#31639;&#27861;&#36866;&#24212;&#21040;&#22312;&#25968;&#25454;&#27969;&#19978;&#20855;&#26377;&#20869;&#23384;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#24182;&#35774;&#35745;&#20102;&#20869;&#23384;&#19981;&#36275;&#31574;&#30053;&#21644;&#20462;&#21098;&#26426;&#21046;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#25152;&#26377;&#37197;&#32622;&#20013;&#65292;Extend Node&#31574;&#30053;&#26159;&#26368;&#20339;&#30340;&#20869;&#23384;&#19981;&#36275;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#20551;&#35774;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#38454;&#27573;&#26377;&#36275;&#22815;&#30340;&#20869;&#23384;&#26469;&#23384;&#20648;&#25968;&#25454;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#29289;&#32852;&#32593;&#20013;&#65292;&#24403;&#25968;&#25454;&#20197;&#26080;&#38480;&#25968;&#25454;&#27969;&#30340;&#24418;&#24335;&#20986;&#29616;&#26102;&#65292;&#25110;&#32773;&#24403;&#23398;&#20064;&#31639;&#27861;&#37096;&#32626;&#22312;&#20855;&#26377;&#36739;&#23569;&#20869;&#23384;&#30340;&#35774;&#22791;&#19978;&#26102;&#65292;&#36825;&#20010;&#20551;&#35774;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#22312;&#32447;Mondrian Forest&#20998;&#31867;&#31639;&#27861;&#36866;&#24212;&#21040;&#22312;&#25968;&#25454;&#27969;&#19978;&#20855;&#26377;&#20869;&#23384;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20116;&#31181;&#20869;&#23384;&#19981;&#36275;&#31574;&#30053;&#65292;&#20197;&#22312;&#36798;&#21040;&#20869;&#23384;&#38480;&#21046;&#26102;&#26356;&#26032;Mondrian&#26641;&#30340;&#26032;&#25968;&#25454;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20462;&#21098;&#26426;&#21046;&#65292;&#22312;&#20869;&#23384;&#38480;&#21046;&#19979;&#20351;Mondrian&#26641;&#23545;&#27010;&#24565;&#28418;&#31227;&#26356;&#21152;&#31283;&#20581;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#30495;&#23454;&#21644;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#24182;&#26368;&#21518;&#32473;&#20986;&#20102;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#20351;&#29992;&#23427;&#20204;&#30340;&#24314;&#35758;&#65306;&#22312;&#25152;&#26377;&#37197;&#32622;&#20013;&#65292;Extend Node&#31574;&#30053;&#20284;&#20046;&#26159;&#26368;&#20339;&#30340;&#20869;&#23384;&#19981;&#36275;&#31574;&#30053;&#65292;&#32780;&#20462;&#21098;&#26426;&#21046;&#21017;&#22240;&#20855;&#20307;&#24773;&#20917;&#32780;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised learning algorithms generally assume the availability of enough memory to store their data model during the training and test phases. However, in the Internet of Things, this assumption is unrealistic when data comes in the form of infinite data streams, or when learning algorithms are deployed on devices with reduced amounts of memory. In this paper, we adapt the online Mondrian forest classification algorithm to work with memory constraints on data streams. In particular, we design five out-of-memory strategies to update Mondrian trees with new data points when the memory limit is reached. Moreover, we design trimming mechanisms to make Mondrian trees more robust to concept drifts under memory constraints. We evaluate our algorithms on a variety of real and simulated datasets, and we conclude with recommendations on their use in different situations: the Extend Node strategy appears as the best out-of-memory strategy in all configurations, whereas different trimming mechan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#21367;&#31215;&#32593;&#32476;&#23545;&#29289;&#20307;&#20301;&#32622;&#30340;&#21709;&#24212;&#21464;&#21270;&#20197;&#21450;&#24120;&#35265;&#25968;&#25454;&#38598;&#20013;&#29289;&#20307;&#20559;&#24046;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#35757;&#32451;&#36807;&#31243;&#20013;&#22270;&#20687;&#20013;&#24515;&#30340;&#29289;&#20307;&#36807;&#24230;&#34920;&#31034;&#23548;&#33268;&#32593;&#32476;&#24615;&#33021;&#21463;&#38480;&#65292;&#29305;&#21035;&#26159;&#24403;&#29289;&#20307;&#38752;&#36817;&#36793;&#30028;&#26102;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#20943;&#36731;&#36825;&#31181;&#24433;&#21709;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2112.09195</link><description>&lt;p&gt;
&#20943;&#36731;&#24120;&#35265;&#25968;&#25454;&#38598;&#20013;&#29289;&#20307;&#20559;&#24046;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Mitigating the Bias of Centered Objects in Common Datasets. (arXiv:2112.09195v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.09195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#21367;&#31215;&#32593;&#32476;&#23545;&#29289;&#20307;&#20301;&#32622;&#30340;&#21709;&#24212;&#21464;&#21270;&#20197;&#21450;&#24120;&#35265;&#25968;&#25454;&#38598;&#20013;&#29289;&#20307;&#20559;&#24046;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#35757;&#32451;&#36807;&#31243;&#20013;&#22270;&#20687;&#20013;&#24515;&#30340;&#29289;&#20307;&#36807;&#24230;&#34920;&#31034;&#23548;&#33268;&#32593;&#32476;&#24615;&#33021;&#21463;&#38480;&#65292;&#29305;&#21035;&#26159;&#24403;&#29289;&#20307;&#38752;&#36817;&#36793;&#30028;&#26102;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#20943;&#36731;&#36825;&#31181;&#24433;&#21709;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#32593;&#32476;&#34987;&#35748;&#20026;&#20855;&#26377;&#24179;&#31227;&#19981;&#21464;&#24615;&#65292;&#20294;&#26159;&#24050;&#32463;&#35777;&#26126;&#23427;&#20204;&#30340;&#21709;&#24212;&#21487;&#33021;&#26681;&#25454;&#29289;&#20307;&#30340;&#30830;&#20999;&#20301;&#32622;&#32780;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#35777;&#26126;&#22823;&#22810;&#25968;&#24120;&#35265;&#30340;&#30740;&#31350;&#25968;&#25454;&#38598;&#23384;&#22312;&#19968;&#31181;&#20559;&#24046;&#65292;&#21363;&#35757;&#32451;&#36807;&#31243;&#20013;&#22270;&#20687;&#20013;&#24515;&#30340;&#29289;&#20307;&#34987;&#36807;&#24230;&#34920;&#31034;&#12290;&#36825;&#31181;&#20559;&#24046;&#21644;&#32593;&#32476;&#30340;&#36793;&#30028;&#26465;&#20214;&#23545;&#36825;&#20123;&#26550;&#26500;&#30340;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#24403;&#29289;&#20307;&#38752;&#36817;&#36793;&#30028;&#26102;&#65292;&#23427;&#20204;&#30340;&#20934;&#30830;&#24615;&#20250;&#26126;&#26174;&#19979;&#38477;&#12290;&#25105;&#20204;&#36824;&#23558;&#23637;&#31034;&#22914;&#20309;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#20943;&#36731;&#36825;&#31181;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional networks are considered shift invariant, but it was demonstrated that their response may vary according to the exact location of the objects. In this paper we will demonstrate that most commonly investigated datasets have a bias, where objects are over-represented at the center of the image during training. This bias and the boundary condition of these networks can have a significant effect on the performance of these architectures and their accuracy drops significantly as an object approaches the boundary. We will also demonstrate how this effect can be mitigated with data augmentation techniques.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#27491;&#20132;&#26059;&#36716;&#26469;&#33719;&#24471;&#36817;&#20284;&#31232;&#30095;&#30340;&#29305;&#24449;&#20540;&#22522;&#12290;&#19982;&#20197;&#24448;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#31232;&#30095;&#25104;&#20998;&#19981;&#38656;&#35201;&#26159;&#20027;&#29305;&#24449;&#21521;&#37327;&#65292;&#32780;&#21487;&#20197;&#26159;&#23427;&#20204;&#30340;&#28151;&#21512;&#12290;&#36825;&#19968;&#26041;&#27861;&#19981;&#38656;&#35201;&#36827;&#34892;&#8220;&#32553;&#20943;&#8221;&#25805;&#20316;&#25110;&#20351;&#29992;&#22810;&#20010;&#35843;&#21442;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2007.00596</link><description>&lt;p&gt;
&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#30340;&#26032;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
A New Basis for Sparse Principal Component Analysis. (arXiv:2007.00596v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.00596
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#27491;&#20132;&#26059;&#36716;&#26469;&#33719;&#24471;&#36817;&#20284;&#31232;&#30095;&#30340;&#29305;&#24449;&#20540;&#22522;&#12290;&#19982;&#20197;&#24448;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#31232;&#30095;&#25104;&#20998;&#19981;&#38656;&#35201;&#26159;&#20027;&#29305;&#24449;&#21521;&#37327;&#65292;&#32780;&#21487;&#20197;&#26159;&#23427;&#20204;&#30340;&#28151;&#21512;&#12290;&#36825;&#19968;&#26041;&#27861;&#19981;&#38656;&#35201;&#36827;&#34892;&#8220;&#32553;&#20943;&#8221;&#25805;&#20316;&#25110;&#20351;&#29992;&#22810;&#20010;&#35843;&#21442;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20043;&#21069;&#30340;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#26041;&#27861;&#20551;&#35774;&#29305;&#24449;&#20540;&#22522;&#65288;&#19968;&#20010;&#22823;&#23567;&#20026;$p \times k$&#30340;&#30697;&#38453;&#65289;&#36817;&#20284;&#31232;&#30095;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20551;&#35774;&#22312;&#36827;&#34892;$k \times k$&#26059;&#36716;&#21518;&#65292;&#29305;&#24449;&#20540;&#22522;&#30340;&#31232;&#30095;&#24615;&#21464;&#24471;&#36817;&#20284;&#12290;&#31639;&#27861;&#30340;&#31616;&#21333;&#29256;&#26412;&#26159;&#20197;&#21069;$k$&#20010;&#20027;&#25104;&#20998;&#20026;&#21021;&#22987;&#20540;&#65292;&#28982;&#21518;&#20351;&#29992;$k \times k$&#27491;&#20132;&#26059;&#36716;&#20351;&#20027;&#25104;&#20998;&#36817;&#20284;&#31232;&#30095;&#65292;&#26368;&#21518;&#23545;&#26059;&#36716;&#21518;&#30340;&#20027;&#25104;&#20998;&#36827;&#34892;&#36719;&#38408;&#20540;&#22788;&#29702;&#12290;&#35813;&#26041;&#27861;&#19982;&#20197;&#24448;&#26041;&#27861;&#19981;&#21516;&#20043;&#22788;&#22312;&#20110;&#20351;&#29992;&#27491;&#20132;&#26059;&#36716;&#26469;&#36817;&#20284;&#31232;&#30095;&#22522;&#12290;&#19968;&#20010;&#32467;&#26524;&#26159;&#65292;&#31232;&#30095;&#25104;&#20998;&#19981;&#38656;&#35201;&#26159;&#20027;&#29305;&#24449;&#21521;&#37327;&#65292;&#32780;&#21487;&#20197;&#26159;&#23427;&#20204;&#30340;&#28151;&#21512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#65288;&#26059;&#36716;&#21518;&#30340;&#65289;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#22522;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36991;&#20813;&#20102;&#8220;&#32553;&#20943;&#8221;&#21644;&#22810;&#20010;&#35843;&#21442;&#21442;&#25968;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#26694;&#26550;&#38750;&#24120;&#28789;&#27963;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#33267;...
&lt;/p&gt;
&lt;p&gt;
Previous versions of sparse principal component analysis (PCA) have presumed that the eigen-basis (a $p \times k$ matrix) is approximately sparse. We propose a method that presumes the $p \times k$ matrix becomes approximately sparse after a $k \times k$ rotation. The simplest version of the algorithm initializes with the leading $k$ principal components. Then, the principal components are rotated with an $k \times k$ orthogonal rotation to make them approximately sparse. Finally, soft-thresholding is applied to the rotated principal components. This approach differs from prior approaches because it uses an orthogonal rotation to approximate a sparse basis. One consequence is that a sparse component need not to be a leading eigenvector, but rather a mixture of them. In this way, we propose a new (rotated) basis for sparse PCA. In addition, our approach avoids "deflation" and multiple tuning parameters required for that. Our sparse PCA framework is versatile; for example, it extends nat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20851;&#38190;&#28857;&#27880;&#37322;&#26469;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#29305;&#26435;&#27719;&#38598;&#21644;&#35270;&#35273;&#27880;&#24847;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23545;&#20855;&#26377;&#38271;&#23614;&#29289;&#31181;&#20998;&#24067;&#21644;&#25968;&#25454;&#38598;&#20559;&#24046;&#30340;&#21160;&#29289;&#29289;&#31181;&#36827;&#34892;&#32454;&#31890;&#24230;&#35782;&#21035;&#65292;&#25552;&#39640;&#20102;&#23567;&#26679;&#26412;&#35757;&#32451;&#30340;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2003.09168</link><description>&lt;p&gt;
&#20351;&#29992;&#29305;&#26435;&#27719;&#38598;&#30340;&#32454;&#31890;&#24230;&#29289;&#31181;&#35782;&#21035;&#65306;&#36890;&#36807;&#30417;&#30563;&#20851;&#27880;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-grained Species Recognition with Privileged Pooling: Better Sample Efficiency Through Supervised Attention. (arXiv:2003.09168v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2003.09168
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20851;&#38190;&#28857;&#27880;&#37322;&#26469;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#29305;&#26435;&#27719;&#38598;&#21644;&#35270;&#35273;&#27880;&#24847;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23545;&#20855;&#26377;&#38271;&#23614;&#29289;&#31181;&#20998;&#24067;&#21644;&#25968;&#25454;&#38598;&#20559;&#24046;&#30340;&#21160;&#29289;&#29289;&#31181;&#36827;&#34892;&#32454;&#31890;&#24230;&#35782;&#21035;&#65292;&#25552;&#39640;&#20102;&#23567;&#26679;&#26412;&#35757;&#32451;&#30340;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30417;&#30563;&#22270;&#20687;&#20998;&#31867;&#26041;&#26696;&#65292;&#21033;&#29992;&#29305;&#26435;&#20449;&#24687;&#65292;&#20197;&#35757;&#32451;&#25968;&#25454;&#30340;&#20851;&#38190;&#28857;&#27880;&#37322;&#30340;&#24418;&#24335;&#65292;&#20174;&#23567;&#37327;&#21644;/&#25110;&#20559;&#35265;&#30340;&#35757;&#32451;&#38598;&#20013;&#23398;&#20064;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21160;&#26426;&#26159;&#29992;&#20110;&#29983;&#24577;&#24212;&#29992;&#65288;&#22914;&#29983;&#29289;&#22810;&#26679;&#24615;&#24314;&#27169;&#65289;&#30340;&#21160;&#29289;&#29289;&#31181;&#35782;&#21035;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#30001;&#20110;&#31232;&#26377;&#29289;&#31181;&#21644;&#30456;&#26426;&#38519;&#38449;&#20013;&#37325;&#22797;&#30340;&#22330;&#26223;&#32972;&#26223;&#31561;&#21407;&#22240;&#65292;&#23384;&#22312;&#38271;&#23614;&#29289;&#31181;&#20998;&#24067;&#21644;&#24378;&#28872;&#30340;&#25968;&#25454;&#38598;&#20559;&#24046;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35270;&#35273;&#27880;&#24847;&#26426;&#21046;&#65292;&#36890;&#36807;&#20851;&#38190;&#28857;&#27880;&#37322;&#36827;&#34892;&#30417;&#30563;&#65292;&#31361;&#20986;&#37325;&#35201;&#30340;&#29289;&#20307;&#37096;&#20998;&#12290;&#36825;&#31181;&#29305;&#26435;&#20449;&#24687;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#26435;&#27719;&#38598;&#25805;&#20316;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20165;&#38656;&#35201;&#65292;&#24182;&#26377;&#21161;&#20110;&#27169;&#22411;&#19987;&#27880;&#20110;&#20855;&#26377;&#21306;&#20998;&#24230;&#30340;&#21306;&#22495;&#12290;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#21160;&#29289;&#29289;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24102;&#26377;&#29305;&#26435;&#27719;&#38598;&#30340;&#28145;&#24230;&#32593;&#32476;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#20351;&#29992;&#23567;&#30340;&#35757;&#32451;&#38598;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a scheme for supervised image classification that uses privileged information, in the form of keypoint annotations for the training data, to learn strong models from small and/or biased training sets. Our main motivation is the recognition of animal species for ecological applications such as biodiversity modelling, which is challenging because of long-tailed species distributions due to rare species, and strong dataset biases such as repetitive scene background in camera traps. To counteract these challenges, we propose a visual attention mechanism that is supervised via keypoint annotations that highlight important object parts. This privileged information, implemented as a novel privileged pooling operation, is only required during training and helps the model to focus on regions that are discriminative. In experiments with three different animal species datasets, we show that deep networks with privileged pooling can use small training sets more efficiently and generaliz
&lt;/p&gt;</description></item></channel></rss>