<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#20998;&#24067;&#30340;&#26041;&#24335;&#26469;&#21306;&#20998;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#30340;&#26410;&#30693;&#26679;&#26412;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#26500;&#36896;&#21253;&#21547;&#22312;&#36741;&#21161;&#26410;&#30693;&#26679;&#26412;&#20998;&#24067;&#21608;&#22260;&#30340;Wasserstein&#29699;&#20013;&#25152;&#26377;&#20998;&#24067;&#30340;&#26410;&#30693;&#26679;&#26412;&#20998;&#24067;&#38598;&#65292;&#26469;&#20943;&#23567;&#20998;&#24067;&#24046;&#24322;&#65292;&#20174;&#32780;&#25552;&#21319;&#24320;&#25918;&#19990;&#30028;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.01796</link><description>&lt;p&gt;
&#23398;&#20064;&#22686;&#24378;&#20998;&#24067;&#29992;&#20110;&#21306;&#20998;&#24320;&#25918;&#19990;&#30028;&#20013;&#30340;&#26410;&#30693;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Learning to Augment Distributions for Out-of-Distribution Detection. (arXiv:2311.01796v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#20998;&#24067;&#30340;&#26041;&#24335;&#26469;&#21306;&#20998;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#30340;&#26410;&#30693;&#26679;&#26412;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#26500;&#36896;&#21253;&#21547;&#22312;&#36741;&#21161;&#26410;&#30693;&#26679;&#26412;&#20998;&#24067;&#21608;&#22260;&#30340;Wasserstein&#29699;&#20013;&#25152;&#26377;&#20998;&#24067;&#30340;&#26410;&#30693;&#26679;&#26412;&#20998;&#24067;&#38598;&#65292;&#26469;&#20943;&#23567;&#20998;&#24067;&#24046;&#24322;&#65292;&#20174;&#32780;&#25552;&#21319;&#24320;&#25918;&#19990;&#30028;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#19990;&#30028;&#20998;&#31867;&#31995;&#32479;&#24212;&#35813;&#21306;&#20998;&#37027;&#20123;&#26631;&#31614;&#19982;&#20869;&#37096;&#20998;&#24067;&#24773;&#20917;&#19981;&#19968;&#33268;&#30340;&#26410;&#30693;&#26679;&#26412;&#65292;&#36825;&#20419;&#20351;&#20102;&#23545;&#26410;&#30693;&#26679;&#26412;&#26816;&#27979;&#30340;&#30740;&#31350;&#12290;&#23613;&#31649;&#20808;&#36827;&#30340;&#26041;&#27861;&#22312;&#36825;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#23545;&#26410;&#30693;&#26679;&#26412;&#32570;&#20047;&#39044;&#20808;&#20102;&#35299;&#65292;&#23427;&#20204;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#20173;&#21487;&#33021;&#22833;&#36133;&#12290;&#34429;&#28982;&#21487;&#20197;&#35775;&#38382;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#30340;&#36741;&#21161;&#26410;&#30693;&#26679;&#26412;&#65288;&#19982;&#26410;&#30693;&#26679;&#26412;&#19981;&#21516;&#65289;&#65292;&#20294;&#20173;&#38656;&#20998;&#26512;&#36825;&#20123;&#36741;&#21161;&#25968;&#25454;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#30340;&#25928;&#26524;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20174;&#23398;&#20064;&#29702;&#35770;&#30340;&#35282;&#24230;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#21457;&#29616;&#36741;&#21161;&#26410;&#30693;&#26679;&#26412;&#21644;&#30495;&#23454;&#26410;&#30693;&#26679;&#26412;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#26159;&#24433;&#21709;&#24320;&#25918;&#19990;&#30028;&#26816;&#27979;&#24615;&#33021;&#30340;&#20851;&#38190;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20998;&#24067;&#22686;&#24378;&#30340;&#26410;&#30693;&#26679;&#26412;&#23398;&#20064;(DAL)&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#21253;&#21547;&#22312;&#36741;&#21161;&#26410;&#30693;&#26679;&#26412;&#20998;&#24067;&#21608;&#22260;&#30340;Wasserstein&#29699;&#20013;&#30340;&#25152;&#26377;&#20998;&#24067;&#30340;&#26410;&#30693;&#26679;&#26412;&#20998;&#24067;&#38598;&#26469;&#20943;&#23567;&#26410;&#30693;&#26679;&#26412;&#20998;&#24067;&#24046;&#24322;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;DAL&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#20855;&#26377;&#36739;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-world classification systems should discern out-of-distribution (OOD) data whose labels deviate from those of in-distribution (ID) cases, motivating recent studies in OOD detection. Advanced works, despite their promising progress, may still fail in the open world, owing to the lack of knowledge about unseen OOD data in advance. Although one can access auxiliary OOD data (distinct from unseen ones) for model training, it remains to analyze how such auxiliary data will work in the open world. To this end, we delve into such a problem from a learning theory perspective, finding that the distribution discrepancy between the auxiliary and the unseen real OOD data is the key to affecting the open-world detection performance. Accordingly, we propose Distributional-Augmented OOD Learning (DAL), alleviating the OOD distribution discrepancy by crafting an OOD distribution set that contains all distributions in a Wasserstein ball centered on the auxiliary OOD distribution. We justify that t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;&#21644;&#27169;&#20223;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#37197;&#23545;&#20248;&#21270;&#35302;&#35273;&#21147;&#37327;&#26354;&#32447;&#21644;&#31616;&#21270;&#20256;&#24863;&#22120;&#24212;&#29992;&#65292;&#23545;&#25509;&#35302;&#20016;&#23500;&#30340;&#25805;&#20316;&#20219;&#21153;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2311.01248</link><description>&lt;p&gt;
&#23558;&#20854;&#25512;&#21521;&#23637;&#31034;&#26497;&#38480;&#65306;&#22810;&#27169;&#24577;&#35270;&#35273;&#35302;&#35273;&#27169;&#20223;&#23398;&#20064;&#19982;&#21147;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Push it to the Demonstrated Limit: Multimodal Visuotactile Imitation Learning with Force Matching. (arXiv:2311.01248v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;&#21644;&#27169;&#20223;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#37197;&#23545;&#20248;&#21270;&#35302;&#35273;&#21147;&#37327;&#26354;&#32447;&#21644;&#31616;&#21270;&#20256;&#24863;&#22120;&#24212;&#29992;&#65292;&#23545;&#25509;&#35302;&#20016;&#23500;&#30340;&#25805;&#20316;&#20219;&#21153;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#23398;&#35302;&#35273;&#20256;&#24863;&#22120;&#24050;&#32463;&#25104;&#20026;&#26426;&#22120;&#20154;&#25805;&#20316;&#36807;&#31243;&#20013;&#33719;&#21462;&#23494;&#38598;&#25509;&#35302;&#20449;&#24687;&#30340;&#26377;&#25928;&#25163;&#27573;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#8220;&#36879;&#35270;&#20320;&#30340;&#30382;&#32932;&#8221;&#65288;STS&#65289;&#22411;&#20256;&#24863;&#22120;&#20855;&#26377;&#35270;&#35273;&#21644;&#35302;&#35273;&#27169;&#24335;&#65292;&#36890;&#36807;&#21033;&#29992;&#21322;&#36879;&#26126;&#34920;&#38754;&#21644;&#21487;&#25511;&#29031;&#26126;&#23454;&#29616;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#19982;&#27169;&#20223;&#23398;&#20064;&#22312;&#23500;&#26377;&#25509;&#35302;&#30340;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;&#22909;&#22788;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#35302;&#35273;&#21147;&#27979;&#37327;&#21644;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#22312;&#36816;&#21160;&#31034;&#33539;&#20013;&#20135;&#29983;&#26356;&#22909;&#21305;&#37197;&#20154;&#20307;&#31034;&#33539;&#32773;&#30340;&#21147;&#26354;&#32447;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#28155;&#21152;&#20102;&#35270;&#35273;/&#35302;&#35273;STS&#27169;&#24335;&#20999;&#25442;&#20316;&#20026;&#25511;&#21046;&#31574;&#30053;&#36755;&#20986;&#65292;&#31616;&#21270;&#20256;&#24863;&#22120;&#30340;&#24212;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#31181;&#35266;&#23519;&#37197;&#32622;&#65292;&#27604;&#36739;&#21644;&#23545;&#27604;&#20102;&#35270;&#35273;/&#35302;&#35273;&#25968;&#25454;&#65288;&#21253;&#25324;&#27169;&#24335;&#20999;&#25442;&#21644;&#19981;&#20999;&#25442;&#65289;&#19982;&#25163;&#33109;&#25346;&#36733;&#30340;&#30524;&#22312;&#25163;&#25668;&#20687;&#26426;&#30340;&#35270;&#35273;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#24191;&#27867;&#30340;&#23454;&#39564;&#31995;&#21015;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optical tactile sensors have emerged as an effective means to acquire dense contact information during robotic manipulation. A recently-introduced `see-through-your-skin' (STS) variant of this type of sensor has both visual and tactile modes, enabled by leveraging a semi-transparent surface and controllable lighting. In this work, we investigate the benefits of pairing visuotactile sensing with imitation learning for contact-rich manipulation tasks. First, we use tactile force measurements and a novel algorithm during kinesthetic teaching to yield a force profile that better matches that of the human demonstrator. Second, we add visual/tactile STS mode switching as a control policy output, simplifying the application of the sensor. Finally, we study multiple observation configurations to compare and contrast the value of visual/tactile data (both with and without mode switching) with visual data from a wrist-mounted eye-in-hand camera. We perform an extensive series of experiments on a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#20266;&#26631;&#31614;&#33258;&#25105;&#20462;&#27491;&#21644;&#22122;&#22768;&#20687;&#32032;&#23450;&#20301;&#26469;&#25913;&#36827;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.16979</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#35821;&#20041;&#20998;&#21106;&#19982;&#20266;&#26631;&#31614;&#33258;&#25105;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Domain Adaptation for Semantic Segmentation with Pseudo Label Self-Refinement. (arXiv:2310.16979v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16979
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#20266;&#26631;&#31614;&#33258;&#25105;&#20462;&#27491;&#21644;&#22122;&#22768;&#20687;&#32032;&#23450;&#20301;&#26469;&#25913;&#36827;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35821;&#20041;&#20998;&#21106;&#35299;&#20915;&#26041;&#26696;&#22312;&#19982;&#35757;&#32451;&#26102;&#19981;&#21516;&#29305;&#24449;&#30340;&#25968;&#25454;&#19978;&#27979;&#35797;&#26102;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#20351;&#29992;&#26469;&#33258;&#26032;&#22495;&#30340;&#27880;&#37322;&#25968;&#25454;&#26469;&#36866;&#24212;&#27169;&#22411;&#24182;&#19981;&#24635;&#26159;&#20999;&#23454;&#21487;&#34892;&#30340;&#12290;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#26041;&#27861;&#22312;&#23454;&#38469;&#25805;&#20316;&#26465;&#20214;&#19979;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26368;&#36817;&#30340;&#26368;&#20808;&#36827;&#65288;SOTA&#65289;UDA&#26041;&#27861;&#37319;&#29992;&#25945;&#24072;-&#23398;&#29983;&#33258;&#35757;&#32451;&#26041;&#27861;&#65292;&#20854;&#20013;&#25945;&#24072;&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#26032;&#25968;&#25454;&#30340;&#20266;&#26631;&#31614;&#65292;&#36827;&#32780;&#25351;&#23548;&#23398;&#29983;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#23613;&#31649;&#27492;&#26041;&#27861;&#21462;&#24471;&#20102;&#24456;&#22823;&#25104;&#21151;&#65292;&#20294;&#21364;&#23384;&#22312;&#20266;&#26631;&#31614;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20256;&#25773;&#30340;&#22122;&#22768;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36741;&#21161;&#20266;&#26631;&#31614;&#31934;&#28860;&#32593;&#32476;&#65288;PRN&#65289;&#65292;&#29992;&#20110;&#22312;&#32447;&#31934;&#28860;&#20266;&#26631;&#31614;&#65292;&#24182;&#23450;&#20301;&#21487;&#33021;&#23384;&#22312;&#22122;&#22768;&#30340;&#20687;&#32032;&#28857;&#30340;&#39044;&#27979;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based solutions for semantic segmentation suffer from significant performance degradation when tested on data with different characteristics than what was used during the training. Adapting the models using annotated data from the new domain is not always practical. Unsupervised Domain Adaptation (UDA) approaches are crucial in deploying these models in the actual operating conditions. Recent state-of-the-art (SOTA) UDA methods employ a teacher-student self-training approach, where a teacher model is used to generate pseudo-labels for the new data which in turn guide the training process of the student model. Though this approach has seen a lot of success, it suffers from the issue of noisy pseudo-labels being propagated in the training process. To address this issue, we propose an auxiliary pseudo-label refinement network (PRN) for online refining of the pseudo labels and also localizing the pixels whose predicted labels are likely to be noisy. Being able to improve the 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#25299;&#25169;&#24863;&#30693;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#30340;&#27867;&#21270;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedGMIR&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.16407</link><description>&lt;p&gt;
&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#25299;&#25169;&#24863;&#30693;&#24322;&#26500;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#22312;&#22122;&#22768;&#36890;&#36947;&#19978;&#30340;&#27867;&#21270;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Information-Theoretic Generalization Analysis for Topology-aware Heterogeneous Federated Edge Learning over Noisy Channels. (arXiv:2310.16407v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16407
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#25299;&#25169;&#24863;&#30693;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#30340;&#27867;&#21270;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedGMIR&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36793;&#32536;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#26080;&#32447;&#32593;&#32476;&#19978;&#30340;&#32852;&#37030;&#23398;&#20064;&#37096;&#32626;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#65292;&#34987;&#31216;&#20026;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#65288;FEEL&#65289;&#12290;&#22312;FEEL&#20013;&#65292;&#31227;&#21160;&#35774;&#22791;&#36890;&#36807;&#22122;&#22768;&#36890;&#36947;&#20256;&#36755;&#27169;&#22411;&#21442;&#25968;&#21644;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#25910;&#38598;&#25968;&#25454;&#65292;&#36825;&#32473;&#35757;&#32451;&#27169;&#22411;&#30340;&#27867;&#21270;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#35774;&#22791;&#21487;&#20197;&#36890;&#36807;&#35774;&#22791;&#38388;&#36890;&#20449;&#36827;&#34892;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#65292;&#32780;&#36830;&#25509;&#35774;&#22791;&#30340;&#36890;&#20449;&#25299;&#25169;&#20063;&#24433;&#21709;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26368;&#36817;&#30340;&#29702;&#35770;&#30740;&#31350;&#22312;&#24320;&#23637;&#27867;&#21270;&#20998;&#26512;&#26102;&#24573;&#35270;&#20102;&#25152;&#26377;&#36825;&#20123;&#25928;&#24212;&#30340;&#32435;&#20837;&#12290;&#19982;&#20043;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#25299;&#25169;&#24863;&#30693;FEEL&#30340;&#27867;&#21270;&#20998;&#26512;&#26041;&#27861;&#65292;&#32771;&#34385;&#21040;&#20102;&#25968;&#25454;&#24322;&#26500;&#24615;&#21644;&#22122;&#22768;&#36890;&#36947;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32852;&#37030;&#20840;&#23616;&#20114;&#20449;&#24687;&#20943;&#23569;&#65288;FedGMIR&#65289;&#30340;&#26032;&#22411;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid growth of edge intelligence, the deployment of federated learning (FL) over wireless networks has garnered increasing attention, which is called Federated Edge Learning (FEEL). In FEEL, both mobile devices transmitting model parameters over noisy channels and collecting data in diverse environments pose challenges to the generalization of trained models. Moreover, devices can engage in decentralized FL via Device-to-Device communication while the communication topology of connected devices also impacts the generalization of models. Most recent theoretical studies overlook the incorporation of all these effects into FEEL when developing generalization analyses. In contrast, our work presents an information-theoretic generalization analysis for topology-aware FEEL in the presence of data heterogeneity and noisy channels. Additionally, we propose a novel regularization method called Federated Global Mutual Information Reduction (FedGMIR) to enhance the performance of models
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#22312;&#19981;&#21516;&#25968;&#25454;&#32452;&#25110;&#39046;&#22495;&#20013;&#23398;&#20064;&#19968;&#33268;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;AI&#21161;&#25163;&#23545;&#19981;&#21516;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2310.11971</link><description>&lt;p&gt;
&#36890;&#36807;&#32676;&#20307;&#19981;&#21464;&#24615;&#23398;&#20064;&#25552;&#39640;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Generalization of Alignment with Human Preferences through Group Invariant Learning. (arXiv:2310.11971v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11971
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#22312;&#19981;&#21516;&#25968;&#25454;&#32452;&#25110;&#39046;&#22495;&#20013;&#23398;&#20064;&#19968;&#33268;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;AI&#21161;&#25163;&#23545;&#19981;&#21516;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;AI&#21161;&#25163;&#30340;&#25104;&#21151;&#22312;&#20110;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;, &#20351;&#29983;&#25104;&#30340;&#22238;&#31572;&#26356;&#21152;&#19982;&#20154;&#31867;&#20559;&#22909;&#19968;&#33268;. &#20316;&#20026;&#36890;&#29992;AI&#21161;&#25163;, &#20154;&#20204;&#36234;&#26469;&#36234;&#26399;&#26395;&#23427;&#20204;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#34920;&#29616;&#19968;&#33268;. &#28982;&#32780;, &#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;,&#24378;&#21270;&#23398;&#20064;(RL)&#32463;&#24120;&#21033;&#29992;&#25463;&#24452;&#20197;&#33719;&#24471;&#36739;&#39640;&#30340;&#22870;&#21169;, &#24573;&#30053;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26679;&#26412;. &#36825;&#31181;&#23545;&#24555;&#36895;&#22870;&#21169;&#25910;&#30410;&#30340;&#20851;&#27880;&#19981;&#20165;&#21066;&#24369;&#20102;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;, &#20063;&#21066;&#24369;&#20102;&#27169;&#22411;&#23545;&#26032;&#30340;&#26410;&#35265;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;. &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;, &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;, &#21487;&#20197;&#36890;&#36807;RL&#22312;&#19981;&#21516;&#25968;&#25454;&#32452;&#25110;&#39046;&#22495;&#20013;&#23398;&#20064;&#19968;&#33268;&#30340;&#31574;&#30053;. &#37492;&#20110;&#33719;&#24471;&#32676;&#20307;&#26631;&#27880;&#30340;&#25361;&#25112;, &#25105;&#20204;&#30340;&#26041;&#27861;&#20250;&#33258;&#21160;&#23558;&#25968;&#25454;&#20998;&#31867;&#21040;&#19981;&#21516;&#30340;&#32452;&#20013;, &#26377;&#24847;&#22320;&#26368;&#22823;&#21270;&#24615;&#33021;&#24046;&#24322;. &#28982;&#21518;, &#25105;&#20204;&#20248;&#21270;&#31574;&#30053;&#20197;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32452;&#20013;&#34920;&#29616;&#33391;&#22909;. &#26368;&#21518;, &#21033;&#29992;&#24050;&#24314;&#31435;&#30340;
&lt;/p&gt;
&lt;p&gt;
The success of AI assistants based on language models (LLMs) hinges crucially on Reinforcement Learning from Human Feedback (RLHF), which enables the generation of responses more aligned with human preferences. As universal AI assistants, there's a growing expectation for them to perform consistently across various domains. However, previous work shows that Reinforcement Learning (RL) often exploits shortcuts to attain high rewards and overlooks challenging samples. This focus on quick reward gains undermines both the stability in training and the model's ability to generalize to new, unseen data. In this work, we propose a novel approach that can learn a consistent policy via RL across various data groups or domains. Given the challenges associated with acquiring group annotations, our method automatically classifies data into different groups, deliberately maximizing performance variance. Then, we optimize the policy to perform well on challenging groups. Lastly, leveraging the estab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#25361;&#25112;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#32534;&#36753;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#20026;NLP&#31038;&#21306;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.08475</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can We Edit Multimodal Large Language Models?. (arXiv:2310.08475v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#25361;&#25112;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#32534;&#36753;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#20026;NLP&#31038;&#21306;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#12290;&#19982;&#32534;&#36753;&#21333;&#27169;&#24335;LLMs&#30456;&#27604;&#65292;&#22810;&#27169;&#24335;&#27169;&#22411;&#30340;&#32534;&#36753;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#26356;&#39640;&#32423;&#21035;&#30340;&#23457;&#26597;&#21644;&#24910;&#37325;&#32771;&#34385;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#31216;&#20026;MMEdit&#65292;&#29992;&#20110;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#22871;&#21019;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21253;&#25324;&#21508;&#31181;&#27169;&#22411;&#32534;&#36753;&#22522;&#32447;&#30340;&#32508;&#21512;&#23454;&#39564;&#65292;&#24182;&#20998;&#26512;&#20102;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#30340;&#19981;&#21516;&#32452;&#20214;&#30340;&#24433;&#21709;&#12290;&#26681;&#25454;&#32463;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#20043;&#21069;&#30340;&#22522;&#32447;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#21487;&#20197;&#23454;&#29616;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#65292;&#20294;&#25928;&#26524;&#20173;&#28982;&#19981;&#29702;&#24819;&#65292;&#34920;&#26126;&#36825;&#20010;&#20219;&#21153;&#21487;&#33021;&#23384;&#22312;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#33021;&#20026;NLP&#31038;&#21306;&#25552;&#20379;&#35265;&#35299;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/zjunlp/EasyEdit&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in https://github.com/zjunlp/EasyEdit.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;DiscDiff&#65292;&#29992;&#20110;&#31163;&#25955;DNA&#24207;&#21015;&#29983;&#25104;&#12290;&#36890;&#36807;&#23558;&#31163;&#25955;DNA&#24207;&#21015;&#23884;&#20837;&#21040;&#36830;&#32493;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#21033;&#29992;&#36830;&#32493;&#25193;&#25955;&#27169;&#22411;&#30340;&#24378;&#22823;&#29983;&#25104;&#33021;&#21147;&#26469;&#29983;&#25104;&#31163;&#25955;&#25968;&#25454;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;FReD&#65292;&#29992;&#20110;&#35780;&#20272;DNA&#24207;&#21015;&#29983;&#25104;&#30340;&#26679;&#26412;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.06150</link><description>&lt;p&gt;
DNA&#24207;&#21015;&#29983;&#25104;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Latent Diffusion Model for DNA Sequence Generation. (arXiv:2310.06150v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06150
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;DiscDiff&#65292;&#29992;&#20110;&#31163;&#25955;DNA&#24207;&#21015;&#29983;&#25104;&#12290;&#36890;&#36807;&#23558;&#31163;&#25955;DNA&#24207;&#21015;&#23884;&#20837;&#21040;&#36830;&#32493;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#21033;&#29992;&#36830;&#32493;&#25193;&#25955;&#27169;&#22411;&#30340;&#24378;&#22823;&#29983;&#25104;&#33021;&#21147;&#26469;&#29983;&#25104;&#31163;&#25955;&#25968;&#25454;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;FReD&#65292;&#29992;&#20110;&#35780;&#20272;DNA&#24207;&#21015;&#29983;&#25104;&#30340;&#26679;&#26412;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#36816;&#29992;&#65292;&#20026;&#21512;&#25104;DNA&#24207;&#21015;&#29983;&#25104;&#39046;&#22495;&#25171;&#24320;&#20102;&#26032;&#30340;&#21069;&#26223;&#12290;&#34429;&#28982;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#22312;&#36825;&#20010;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#38754;&#20020;&#26679;&#26412;&#22810;&#26679;&#24615;&#26377;&#38480;&#21644;&#27169;&#24335;&#23849;&#28291;&#31561;&#38382;&#39064;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31867;&#26377;&#21069;&#26223;&#30340;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#65292;&#19981;&#21463;&#36825;&#20123;&#38382;&#39064;&#30340;&#22256;&#25200;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#22270;&#20687;&#29983;&#25104;&#31561;&#39046;&#22495;&#36798;&#21040;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36866;&#29992;&#20110;&#31163;&#25955;DNA&#24207;&#21015;&#29983;&#25104;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;DiscDiff&#12290;&#36890;&#36807;&#23558;&#31163;&#25955;DNA&#24207;&#21015;&#31616;&#21333;&#22320;&#23884;&#20837;&#21040;&#36830;&#32493;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#20351;&#29992;&#33258;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#36830;&#32493;&#25193;&#25955;&#27169;&#22411;&#30340;&#24378;&#22823;&#29983;&#25104;&#33021;&#21147;&#26469;&#29983;&#25104;&#31163;&#25955;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#8212;&#8212;Fr\'echet&#37325;&#24314;&#36317;&#31163;&#65288;FReD&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;DNA&#24207;&#21015;&#29983;&#25104;&#30340;&#26679;&#26412;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The harnessing of machine learning, especially deep generative models, has opened up promising avenues in the field of synthetic DNA sequence generation. Whilst Generative Adversarial Networks (GANs) have gained traction for this application, they often face issues such as limited sample diversity and mode collapse. On the other hand, Diffusion Models are a promising new class of generative models that are not burdened with these problems, enabling them to reach the state-of-the-art in domains such as image generation. In light of this, we propose a novel latent diffusion model, DiscDiff, tailored for discrete DNA sequence generation. By simply embedding discrete DNA sequences into a continuous latent space using an autoencoder, we are able to leverage the powerful generative abilities of continuous diffusion models for the generation of discrete data. Additionally, we introduce Fr\'echet Reconstruction Distance (FReD) as a new metric to measure the sample quality of DNA sequence gener
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#23618;&#21098;&#26525;&#21644;&#25439;&#22833;&#26354;&#38754;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;DNN&#26550;&#26500;&#30340;&#31616;&#21270;&#21644;&#20934;&#30830;&#24615;&#30340;&#22686;&#24378;&#12290;&#36890;&#36807;&#22855;&#24322;&#20540;&#20998;&#35299;&#65292;&#24182;&#26681;&#25454;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#26631;&#20934;&#20002;&#24323;&#23567;&#30340;&#22855;&#24322;&#20540;&#65292;&#21487;&#20943;&#23569;DNN&#23618;&#30340;&#21442;&#25968;&#65292;&#31616;&#21270;DNN&#26550;&#26500;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#22686;&#24378;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03165</link><description>&lt;p&gt;
&#21033;&#29992;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Accuracy in Deep Learning Using Random Matrix Theory. (arXiv:2310.03165v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#23618;&#21098;&#26525;&#21644;&#25439;&#22833;&#26354;&#38754;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;DNN&#26550;&#26500;&#30340;&#31616;&#21270;&#21644;&#20934;&#30830;&#24615;&#30340;&#22686;&#24378;&#12290;&#36890;&#36807;&#22855;&#24322;&#20540;&#20998;&#35299;&#65292;&#24182;&#26681;&#25454;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#26631;&#20934;&#20002;&#24323;&#23567;&#30340;&#22855;&#24322;&#20540;&#65292;&#21487;&#20943;&#23569;DNN&#23618;&#30340;&#21442;&#25968;&#65292;&#31616;&#21270;DNN&#26550;&#26500;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#22686;&#24378;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#26159;&#36890;&#36807;&#23618;&#21098;&#26525;&#31616;&#21270;DNN&#26550;&#26500;&#21644;&#25439;&#22833;&#26354;&#38754;&#12290;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#26368;&#36817;&#34987;&#29992;&#20110;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#33021;&#22815;&#26816;&#26597;DNN&#30340;&#26435;&#37325;&#23618;&#35889;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#25216;&#26415;&#26469;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36890;&#36807;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#30830;&#23450;&#35201;&#20174;DNN&#30340;&#26435;&#37325;&#23618;&#20013;&#21435;&#38500;&#30340;&#22855;&#24322;&#20540;&#30340;&#25968;&#37327;&#65292;&#36825;&#26377;&#21161;&#20110;&#31616;&#21270;DNN&#24182;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#22312;MNIST&#21644;Fashion MNIST&#25968;&#25454;&#38598;&#19978;&#22521;&#35757;&#31616;&#21333;&#30340;DNN&#27169;&#22411;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#39044;&#35757;&#32451;DNN&#30340;&#20219;&#20309;&#20840;&#36830;&#25509;&#25110;&#21367;&#31215;&#23618;&#65292;&#20943;&#23569;&#20102;&#23618;&#30340;&#21442;&#25968;&#24182;&#31616;&#21270;&#20102;DNN&#30340;&#26550;&#26500;&#65292;&#21516;&#26102;&#20445;&#25345;&#29978;&#33267;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#26681;&#25454;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#26631;&#20934;&#20002;&#24323;&#23567;&#30340;&#22855;&#24322;&#20540;&#65292;&#27979;&#35797;&#38598;&#30340;&#20934;&#30830;&#24615;&#20445;&#25345;&#19968;&#33268;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;DNN&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we explore the applications of random matrix theory (RMT) in the training of deep neural networks (DNNs), focusing on layer pruning to simplify DNN architecture and loss landscape. RMT, recently used to address overfitting in deep learning, enables the examination of DNN's weight layer spectra. We use these techniques to optimally determine the number of singular values to be removed from the weight layers of a DNN during training via singular value decomposition (SVD). This process aids in DNN simplification and accuracy enhancement, as evidenced by training simple DNN models on the MNIST and Fashion MNIST datasets.  Our method can be applied to any fully connected or convolutional layer of a pretrained DNN, decreasing the layer's parameters and simplifying the DNN architecture while preserving or even enhancing the model's accuracy. By discarding small singular values based on RMT criteria, the accuracy of the test set remains consistent, facilitating more efficient DN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#24335;&#36873;&#39033;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#23616;&#37096;&#24378;&#31574;&#30053;&#26469;&#25351;&#23548;&#25628;&#32034;&#31639;&#27861;&#65292;&#20174;&#32780;&#22312;&#24378;&#21270;&#23398;&#20064;&#21644;&#35268;&#21010;&#20013;&#24212;&#29992;&#20110;&#22797;&#26434;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2310.01569</link><description>&lt;p&gt;
&#36845;&#20195;&#24335;&#35268;&#21010;&#20013;&#30340;&#36873;&#39033;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Iterative Option Discovery for Planning, by Planning. (arXiv:2310.01569v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#24335;&#36873;&#39033;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#23616;&#37096;&#24378;&#31574;&#30053;&#26469;&#25351;&#23548;&#25628;&#32034;&#31639;&#27861;&#65292;&#20174;&#32780;&#22312;&#24378;&#21270;&#23398;&#20064;&#21644;&#35268;&#21010;&#20013;&#24212;&#29992;&#20110;&#22797;&#26434;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#26377;&#29992;&#30340;&#26102;&#38388;&#25277;&#35937;&#65292;&#20063;&#23601;&#26159;&#36873;&#39033;&#65292;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#23558;&#24378;&#21270;&#23398;&#20064;&#21644;&#35268;&#21010;&#24212;&#29992;&#20110;&#26085;&#30410;&#22797;&#26434;&#30340;&#39046;&#22495;&#30340;&#20851;&#38190;&#12290;&#22312;AlphaZero&#20013;&#20351;&#29992;&#30340;Expert Iteration&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#30340;&#32463;&#39564;&#25104;&#21151;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;Option Iteration&#65292;&#19968;&#31181;&#31867;&#20284;&#30340;&#36873;&#39033;&#21457;&#29616;&#26041;&#27861;&#12290;Option Iteration&#19981;&#26159;&#23398;&#20064;&#19968;&#20010;&#21333;&#19968;&#30340;&#24378;&#31574;&#30053;&#65292;&#32780;&#26159;&#23398;&#20064;&#19968;&#32452;&#36873;&#39033;&#31574;&#30053;&#65292;&#23545;&#20110;&#36935;&#21040;&#30340;&#27599;&#20010;&#29366;&#24577;&#65292;&#33267;&#23569;&#26377;&#19968;&#31181;&#31574;&#30053;&#22312;&#26576;&#20010;&#26410;&#26469;&#30340;&#26102;&#38388;&#28857;&#19982;&#25628;&#32034;&#32467;&#26524;&#21563;&#21512;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#36825;&#21487;&#33021;&#26356;&#23481;&#26131;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;&#31639;&#27861;&#26681;&#25454;&#24773;&#20917;&#28789;&#27963;&#35843;&#25972;&#65292;&#32780;&#19981;&#26159;&#23398;&#20064;&#19968;&#20010;&#22312;&#24403;&#21069;&#29366;&#24577;&#30340;&#32454;&#33410;&#19978;&#20855;&#26377;&#22797;&#26434;&#20381;&#36182;&#24615;&#30340;&#20840;&#23616;&#31574;&#30053;&#12290;&#36890;&#36807;&#23398;&#20064;&#36825;&#26679;&#19968;&#32452;&#23616;&#37096;&#24378;&#31574;&#30053;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#23427;&#20204;&#26469;&#25351;&#23548;&#25628;&#32034;&#31639;&#27861;&#65292;&#20174;&#32780;&#24418;&#25104;&#33391;&#24615;&#24490;&#29615;&#65292;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering useful temporal abstractions, in the form of options, is widely thought to be key to applying reinforcement learning and planning to increasingly complex domains. Building on the empirical success of the Expert Iteration approach to policy learning used in AlphaZero, we propose Option Iteration, an analogous approach to option discovery. Rather than learning a single strong policy that is trained to match the search results everywhere, Option Iteration learns a set of option policies trained such that for each state encountered, at least one policy in the set matches the search results for some horizon into the future. Intuitively, this may be significantly easier as it allows the algorithm to hedge its bets compared to learning a single globally strong policy, which may have complex dependencies on the details of the current state. Having learned such a set of locally strong policies, we can use them to guide the search algorithm resulting in a virtuous cycle where better 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22320;&#38663;&#22270;&#21464;&#21387;&#22120;&#65288;SeisT&#65289;&#30340;&#36890;&#29992;&#28145;&#24230;&#23398;&#20064;&#39592;&#24178;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#31181;&#22320;&#38663;&#30417;&#27979;&#20219;&#21153;&#12290;SeisT&#30340;&#39640;&#25928;&#32593;&#32476;&#26550;&#26500;&#20351;&#20854;&#22312;&#22320;&#38663;&#26816;&#27979;&#12289;&#22320;&#38663;&#30456;&#20301;&#25342;&#21462;&#12289;&#39318;&#27425;&#36816;&#21160;&#26497;&#24615;&#20998;&#31867;&#12289;&#38663;&#32423;&#20272;&#35745;&#21644;&#21453;&#26041;&#20301;&#35282;&#20272;&#35745;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#29305;&#21035;&#26159;&#22312;&#27867;&#21270;&#24615;&#33021;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2310.01037</link><description>&lt;p&gt;
&#22320;&#38663;&#22270;&#21464;&#21387;&#22120;&#65306;&#29992;&#20110;&#22810;&#31181;&#22320;&#38663;&#30417;&#27979;&#20219;&#21153;&#30340;&#36890;&#29992;&#28145;&#24230;&#23398;&#20064;&#39592;&#24178;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Seismogram Transformer: A generic deep learning backbone network for multiple earthquake monitoring tasks. (arXiv:2310.01037v2 [physics.geo-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22320;&#38663;&#22270;&#21464;&#21387;&#22120;&#65288;SeisT&#65289;&#30340;&#36890;&#29992;&#28145;&#24230;&#23398;&#20064;&#39592;&#24178;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#31181;&#22320;&#38663;&#30417;&#27979;&#20219;&#21153;&#12290;SeisT&#30340;&#39640;&#25928;&#32593;&#32476;&#26550;&#26500;&#20351;&#20854;&#22312;&#22320;&#38663;&#26816;&#27979;&#12289;&#22320;&#38663;&#30456;&#20301;&#25342;&#21462;&#12289;&#39318;&#27425;&#36816;&#21160;&#26497;&#24615;&#20998;&#31867;&#12289;&#38663;&#32423;&#20272;&#35745;&#21644;&#21453;&#26041;&#20301;&#35282;&#20272;&#35745;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#29305;&#21035;&#26159;&#22312;&#27867;&#21270;&#24615;&#33021;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#38663;&#35760;&#24405;&#65292;&#21363;&#22320;&#38663;&#22270;&#65292;&#26159;&#30001;&#22320;&#38663;&#20107;&#20214;&#24341;&#36215;&#30340;&#22320;&#38754;&#36816;&#21160;&#30340;&#37325;&#35201;&#35760;&#24405;&#65292;&#26159;&#22320;&#38663;&#30740;&#31350;&#21644;&#30417;&#27979;&#30340;&#22522;&#30784;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#26497;&#22823;&#22320;&#20419;&#36827;&#20102;&#21508;&#31181;&#22320;&#38663;&#20449;&#21495;&#22788;&#29702;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39592;&#24178;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#21508;&#31181;&#22320;&#38663;&#30417;&#27979;&#20219;&#21153;&#65292;&#21517;&#20026;&#22320;&#38663;&#22270;&#21464;&#21387;&#22120;&#65288;SeisT&#65289;&#12290;&#30001;&#20110;&#20854;&#39640;&#25928;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;SeisT&#22312;&#22320;&#38663;&#26816;&#27979;&#12289;&#22320;&#38663;&#30456;&#20301;&#25342;&#21462;&#12289;&#39318;&#27425;&#36816;&#21160;&#26497;&#24615;&#20998;&#31867;&#12289;&#38663;&#32423;&#20272;&#35745;&#21644;&#21453;&#26041;&#20301;&#35282;&#20272;&#35745;&#31561;&#20219;&#21153;&#20013;&#33021;&#22815;&#19982;&#29978;&#33267;&#36229;&#36807;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#21305;&#37197;&#65292;&#29305;&#21035;&#26159;&#22312;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#24615;&#33021;&#26041;&#38754;&#12290;SeisT&#30001;&#22810;&#20010;&#32593;&#32476;&#23618;&#32452;&#25104;&#65292;&#36825;&#20123;&#23618;&#30001;&#19981;&#21516;&#30340;&#22522;&#30784;&#27169;&#22359;&#32452;&#25104;&#65292;&#24110;&#21161;&#27169;&#22411;&#29702;&#35299;&#22320;&#38663;&#22270;&#30340;&#22810;&#23618;&#27425;&#29305;&#24449;&#34920;&#36798;&#65292;&#20174;&#20302;&#23618;&#27425;&#21040;&#39640;&#23618;&#27425;&#30340;&#22797;&#26434;&#29305;&#24449;&#65292;&#26377;&#25928;&#22320;&#25552;&#21462;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Seismic records, known as seismograms, are crucial records of ground motion resulting from seismic events, constituting the backbone of earthquake research and monitoring. The latest advancements in deep learning have significantly facilitated various seismic signal processing tasks. This paper introduces a novel backbone neural network model designed for various seismic monitoring tasks, named Seismogram Transformer (SeisT). Thanks to its efficient network architecture, SeisT matches or even outperforms the state-of-the-art models in earthquake detection, seismic phase picking, first-motion polarity classification, magnitude estimation, and back-azimuth estimation tasks, particularly in terms of out-of-distribution generalization performance. SeisT consists of multiple network layers composed of different foundational blocks, which help the model understand multi-level feature representations of seismograms from low-level to high-level complex features, effectively extracting features
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#35774;&#35745;&#21407;&#21017;&#29992;&#20110;&#20248;&#21270;&#39057;&#29575;&#24335;&#24207;&#36143;&#23398;&#20064;&#38382;&#39064;&#30340;&#36890;&#29992;&#29702;&#35770;&#65292;&#36890;&#36807;&#29983;&#25104;&#8220;&#31639;&#27861;&#20449;&#24565;&#8221;&#24182;&#20351;&#29992;&#36125;&#21494;&#26031;&#21518;&#39564;&#36827;&#34892;&#20915;&#31574;&#65292;&#23454;&#29616;&#20102;&#26080;&#20808;&#39564;&#30340;&#36125;&#21494;&#26031;&#31867;&#22411;&#31639;&#27861;&#22312;&#23545;&#25239;&#24615;&#29615;&#22659;&#20013;&#30340;&#27867;&#21270;&#21644;&#20248;&#21270;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.00806</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#35774;&#35745;&#21407;&#21017;&#29992;&#20110;&#39057;&#29575;&#24335;&#24207;&#36143;&#23398;&#20064;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Bayesian Design Principles for Frequentist Sequential Learning. (arXiv:2310.00806v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00806
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#35774;&#35745;&#21407;&#21017;&#29992;&#20110;&#20248;&#21270;&#39057;&#29575;&#24335;&#24207;&#36143;&#23398;&#20064;&#38382;&#39064;&#30340;&#36890;&#29992;&#29702;&#35770;&#65292;&#36890;&#36807;&#29983;&#25104;&#8220;&#31639;&#27861;&#20449;&#24565;&#8221;&#24182;&#20351;&#29992;&#36125;&#21494;&#26031;&#21518;&#39564;&#36827;&#34892;&#20915;&#31574;&#65292;&#23454;&#29616;&#20102;&#26080;&#20808;&#39564;&#30340;&#36125;&#21494;&#26031;&#31867;&#22411;&#31639;&#27861;&#22312;&#23545;&#25239;&#24615;&#29615;&#22659;&#20013;&#30340;&#27867;&#21270;&#21644;&#20248;&#21270;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#29702;&#35770;&#65292;&#29992;&#20110;&#20248;&#21270;&#39057;&#29575;&#35823;&#24046;&#27714;&#21644;&#30340;&#24207;&#36143;&#23398;&#20064;&#38382;&#39064;&#65292;&#21487;&#20197;&#36890;&#36807;&#32479;&#19968;&#30340;&#36125;&#21494;&#26031;&#21407;&#21017;&#24471;&#21040;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#36172;&#21338;&#26426;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#27599;&#19968;&#36718;&#29983;&#25104;&#8220;&#31639;&#27861;&#20449;&#24565;&#8221;&#65292;&#24182;&#20351;&#29992;&#36125;&#21494;&#26031;&#21518;&#39564;&#36827;&#34892;&#20915;&#31574;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20248;&#21270;&#30446;&#26631;&#26159;&#21019;&#24314;&#8220;&#31639;&#27861;&#20449;&#24565;&#8221;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#8220;&#31639;&#27861;&#20449;&#24687;&#27604;&#8221;&#65292;&#26377;&#25928;&#22320;&#34920;&#24449;&#20102;&#20219;&#20309;&#31639;&#27861;&#30340;&#39057;&#29575;&#35823;&#24046;&#30340;&#20869;&#22312;&#22797;&#26434;&#24230;&#24230;&#37327;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31181;&#23558;&#36125;&#21494;&#26031;&#24335;&#31639;&#27861;&#26080;&#20808;&#39564;&#22320;&#24182;&#19988;&#22312;&#23545;&#25239;&#24615;&#29615;&#22659;&#20013;&#20197;&#36890;&#29992;&#21644;&#26368;&#20248;&#26041;&#24335;&#24212;&#29992;&#30340;&#31995;&#32479;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#31639;&#27861;&#31616;&#21333;&#19988;&#36890;&#24120;&#23481;&#26131;&#23454;&#29616;&#12290;&#20316;&#20026;&#19968;&#20010;&#37325;&#35201;&#24212;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#31639;&#27861;&#65292;&#22312;&#38543;&#26426;&#12289;&#23545;&#25239;&#21644;&#38750;...
&lt;/p&gt;
&lt;p&gt;
We develop a general theory to optimize the frequentist regret for sequential learning problems, where efficient bandit and reinforcement learning algorithms can be derived from unified Bayesian principles. We propose a novel optimization approach to generate "algorithmic beliefs" at each round, and use Bayesian posteriors to make decisions. The optimization objective to create "algorithmic beliefs," which we term "Algorithmic Information Ratio," represents an intrinsic complexity measure that effectively characterizes the frequentist regret of any algorithm. To the best of our knowledge, this is the first systematical approach to make Bayesian-type algorithms prior-free and applicable to adversarial settings, in a generic and optimal manner. Moreover, the algorithms are simple and often efficient to implement. As a major application, we present a novel algorithm for multi-armed bandits that achieves the "best-of-all-worlds" empirical performance in the stochastic, adversarial, and non
&lt;/p&gt;</description></item><item><title>&#36882;&#24402;&#36229;&#32593;&#32476;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24378;&#22823;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#19987;&#38376;&#26041;&#27861;&#26356;&#20026;&#31616;&#21333;&#20294;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.14970</link><description>&lt;p&gt;
&#36882;&#24402;&#36229;&#32593;&#32476;&#22312;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24378;&#22823;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Recurrent Hypernetworks are Surprisingly Strong in Meta-RL. (arXiv:2309.14970v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14970
&lt;/p&gt;
&lt;p&gt;
&#36882;&#24402;&#36229;&#32593;&#32476;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24378;&#22823;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#19987;&#38376;&#26041;&#27861;&#26356;&#20026;&#31616;&#21333;&#20294;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#26102;&#22240;&#26679;&#26412;&#25928;&#29575;&#20302;&#32780;&#19981;&#26131;&#37096;&#32626;&#12290;&#20803;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#23398;&#20064;&#22312;&#20803;&#35757;&#32451;&#26102;&#21033;&#29992;&#30456;&#20851;&#20219;&#21153;&#30340;&#20998;&#24067;&#26469;&#23454;&#29616;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#30452;&#25509;&#35299;&#20915;&#20102;&#36825;&#20010;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#19987;&#38376;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#19982;&#19968;&#20010;&#36890;&#29992;&#30340;&#24207;&#21015;&#27169;&#22411;&#65288;&#22914;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65289;&#32467;&#21512;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26159;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#24378;&#22522;&#20934;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#35266;&#28857;&#30001;&#20110;&#26377;&#38480;&#30340;&#25903;&#25345;&#35777;&#25454;&#32780;&#24341;&#36215;&#20102;&#20105;&#35758;&#65292;&#29305;&#21035;&#26159;&#22312;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#30830;&#31435;&#20102;&#23436;&#20840;&#30456;&#21453;&#30340;&#35266;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#34429;&#28982;&#25105;&#20204;&#21516;&#26679;&#21457;&#29616;&#24490;&#29615;&#32593;&#32476;&#21487;&#20197;&#36798;&#21040;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#20102;&#36229;&#32593;&#32476;&#30340;&#20351;&#29992;&#23545;&#20110;&#21457;&#25381;&#24490;&#29615;&#22522;&#32447;&#30340;&#28508;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#19982;&#36229;&#32593;&#32476;&#30456;&#32467;&#21512;&#26102;&#65292;&#36825;&#31181;&#36828;&#27604;&#29616;&#26377;&#19987;&#38376;&#26041;&#27861;&#31616;&#21333;&#30340;&#24490;&#29615;&#22522;&#20934;&#23454;&#38469;&#19978;&#33021;&#21462;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (RL) is notoriously impractical to deploy due to sample inefficiency. Meta-RL directly addresses this sample inefficiency by learning to perform few-shot learning when a distribution of related tasks is available for meta-training. While many specialized meta-RL methods have been proposed, recent work suggests that end-to-end learning in conjunction with an off-the-shelf sequential model, such as a recurrent network, is a surprisingly strong baseline. However, such claims have been controversial due to limited supporting evidence, particularly in the face of prior work establishing precisely the opposite. In this paper, we conduct an empirical investigation. While we likewise find that a recurrent network can achieve strong performance, we demonstrate that the use of hypernetworks is crucial to maximizing their potential. Surprisingly, when combined with hypernetworks, the recurrent baselines that are far simpler than existing specialized methods actually ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#28145;&#24230;&#30693;&#35782;&#36861;&#36394;&#35270;&#20026;&#19968;&#31181;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#21457;&#29616;&#65292;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#35299;&#30721;&#22120;&#21487;&#20197;&#27604;DKT&#39044;&#27979;&#23398;&#29983;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.12334</link><description>&lt;p&gt;
&#28145;&#24230;&#30693;&#35782;&#36861;&#36394;&#26159;&#19968;&#20010;&#38544;&#21547;&#30340;&#21160;&#24577;&#22810;&#32500;&#39033;&#30446;&#21453;&#24212;&#29702;&#35770;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Deep Knowledge Tracing is an implicit dynamic multidimensional item response theory model. (arXiv:2309.12334v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#28145;&#24230;&#30693;&#35782;&#36861;&#36394;&#35270;&#20026;&#19968;&#31181;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#21457;&#29616;&#65292;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#35299;&#30721;&#22120;&#21487;&#20197;&#27604;DKT&#39044;&#27979;&#23398;&#29983;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#36861;&#36394;&#26159;&#26681;&#25454;&#23398;&#29983;&#20808;&#21069;&#38382;&#39064;&#30340;&#34920;&#29616;&#26469;&#39044;&#27979;&#20854;&#22312;&#26032;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#65292;&#36825;&#21487;&#20197;&#20316;&#20026;&#20248;&#21270;&#35780;&#20272;&#21644;&#23398;&#20064;&#30340;&#20808;&#21069;&#27493;&#39588;&#12290;&#28145;&#24230;&#30693;&#35782;&#36861;&#36394;&#65288;DKT&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#30693;&#35782;&#36861;&#36394;&#31454;&#20105;&#27169;&#22411;&#65292;&#21363;&#20351;&#19968;&#20123;&#26356;&#31616;&#21333;&#30340;&#27169;&#22411;&#21487;&#33021;&#19982;&#20854;&#24615;&#33021;&#30456;&#21305;&#37197;&#65292;&#20294;&#23545;&#20110;&#20026;&#20160;&#20040;DKT&#33021;&#22815;&#22914;&#27492;&#25104;&#21151;&#30340;&#20102;&#35299;&#36824;&#24456;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#28145;&#24230;&#30693;&#35782;&#36861;&#36394;&#35270;&#20026;&#19968;&#31181;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#12290;&#36825;&#20010;&#35266;&#28857;&#19981;&#20165;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#24615;&#33021;&#12289;&#31616;&#21333;&#24615;&#25110;&#34920;&#36798;&#24615;&#26041;&#38754;&#25552;&#20986;&#26356;&#22909;&#30340;&#27169;&#22411;&#65292;&#36824;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#25171;&#24320;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#22312;&#20960;&#20010;&#23567;&#22411;&#21644;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#35299;&#30721;&#22120;&#65292;&#21487;&#33021;&#27604;DKT&#20351;&#29992;&#30340;&#35299;&#30721;&#22120;&#21442;&#25968;&#26356;&#23569;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#39044;&#27979;&#23398;&#29983;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge tracing consists in predicting the performance of some students on new questions given their performance on previous questions, and can be a prior step to optimizing assessment and learning. Deep knowledge tracing (DKT) is a competitive model for knowledge tracing relying on recurrent neural networks, even if some simpler models may match its performance. However, little is known about why DKT works so well. In this paper, we frame deep knowledge tracing as a encoderdecoder architecture. This viewpoint not only allows us to propose better models in terms of performance, simplicity or expressivity but also opens up promising avenues for future research directions. In particular, we show on several small and large datasets that a simpler decoder, with possibly fewer parameters than the one used by DKT, can predict student performance better.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedDCSR&#30340;&#32852;&#37030;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#26469;&#22788;&#29702;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24207;&#21015;&#29305;&#24449;&#24322;&#36136;&#24615;&#65292;&#24182;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2309.08420</link><description>&lt;p&gt;
FedDCSR: &#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#23454;&#29616;&#32852;&#37030;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
FedDCSR: Federated Cross-domain Sequential Recommendation via Disentangled Representation Learning. (arXiv:2309.08420v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08420
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedDCSR&#30340;&#32852;&#37030;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#26469;&#22788;&#29702;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24207;&#21015;&#29305;&#24449;&#24322;&#36136;&#24615;&#65292;&#24182;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#30340;&#29992;&#25143;&#24207;&#21015;&#25968;&#25454;&#30340;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;(CSR)&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;CSR&#26041;&#27861;&#38656;&#35201;&#22312;&#39046;&#22495;&#20043;&#38388;&#20849;&#20139;&#21407;&#22987;&#29992;&#25143;&#25968;&#25454;&#65292;&#36825;&#36829;&#21453;&#20102;&#12298;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#12299;(GDPR)&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#23558;&#32852;&#37030;&#23398;&#20064;(FL)&#21644;CSR&#30456;&#32467;&#21512;&#65292;&#20805;&#20998;&#21033;&#29992;&#19981;&#21516;&#39046;&#22495;&#30340;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24207;&#21015;&#29305;&#24449;&#24322;&#36136;&#24615;&#23545;FL&#30340;&#25972;&#20307;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedDCSR&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#30340;&#26032;&#22411;&#32852;&#37030;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24207;&#21015;&#29305;&#24449;&#24322;&#36136;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#39046;&#22495;&#20869;-&#39046;&#22495;&#38388;&#24207;&#21015;&#34920;&#31034;&#35299;&#32544;(SRD)&#30340;&#26041;&#27861;&#65292;&#23558;&#29992;&#25143;&#24207;&#21015;&#29305;&#24449;&#35299;&#32544;&#25104;&#39046;&#22495;&#20849;&#20139;&#21644;&#39046;&#22495;&#19987;&#23646;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-domain Sequential Recommendation (CSR) which leverages user sequence data from multiple domains has received extensive attention in recent years. However, the existing CSR methods require sharing origin user data across domains, which violates the General Data Protection Regulation (GDPR). Thus, it is necessary to combine federated learning (FL) and CSR to fully utilize knowledge from different domains while preserving data privacy. Nonetheless, the sequence feature heterogeneity across different domains significantly impacts the overall performance of FL. In this paper, we propose FedDCSR, a novel federated cross-domain sequential recommendation framework via disentangled representation learning. Specifically, to address the sequence feature heterogeneity across domains, we introduce an approach called inter-intra domain sequence representation disentanglement (SRD) to disentangle the user sequence features into domain-shared and domain-exclusive features. In addition, we design
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#26031;&#36807;&#31243;&#19982;&#32447;&#24615;&#22810;&#26680;&#22312;&#22810;&#32500;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26684;&#28857;&#35889;&#28151;&#21512;&#26680;&#20844;&#24335;&#65292;&#20943;&#23569;&#20102;&#36229;&#21442;&#25968;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#20248;&#21270;&#32467;&#26500;&#21644;&#36924;&#36817;&#33021;&#21147;&#12290;&#36890;&#36807;&#24341;&#20837;&#20998;&#24067;&#24335;&#31639;&#27861;&#65292;&#20351;&#22823;&#35268;&#27169;&#36229;&#21442;&#25968;&#20248;&#21270;&#21464;&#24471;&#21487;&#34892;&#12290;</title><link>http://arxiv.org/abs/2309.08201</link><description>&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#19982;&#32447;&#24615;&#22810;&#26680;&#65306;&#39057;&#35889;&#35774;&#35745;&#21644;&#22810;&#32500;&#25968;&#25454;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Gaussian Processes with Linear Multiple Kernel: Spectrum Design and Distributed Learning for Multi-Dimensional Data. (arXiv:2309.08201v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#26031;&#36807;&#31243;&#19982;&#32447;&#24615;&#22810;&#26680;&#22312;&#22810;&#32500;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26684;&#28857;&#35889;&#28151;&#21512;&#26680;&#20844;&#24335;&#65292;&#20943;&#23569;&#20102;&#36229;&#21442;&#25968;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#20248;&#21270;&#32467;&#26500;&#21644;&#36924;&#36817;&#33021;&#21147;&#12290;&#36890;&#36807;&#24341;&#20837;&#20998;&#24067;&#24335;&#31639;&#27861;&#65292;&#20351;&#22823;&#35268;&#27169;&#36229;&#21442;&#25968;&#20248;&#21270;&#21464;&#24471;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#65288;GPs&#65289;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#21644;&#20449;&#21495;&#22788;&#29702;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;GP&#24314;&#27169;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#26680;&#20989;&#25968;&#30340;&#36873;&#25321;&#65292;&#32447;&#24615;&#22810;&#26680;&#65288;LMKs&#65289;&#22240;&#20854;&#24378;&#22823;&#30340;&#24314;&#27169;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#32780;&#25104;&#20026;&#19968;&#20010;&#21560;&#24341;&#20154;&#30340;&#26680;&#20989;&#25968;&#31867;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#26684;&#28857;&#35889;&#28151;&#21512;&#65288;GSM&#65289;&#26680;&#65292;&#23427;&#26159;&#19968;&#31181;&#21487;&#20197;&#36817;&#20284;&#20219;&#24847;&#24179;&#31283;&#26680;&#30340;LMK&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GSM&#26680;&#20844;&#24335;&#65292;&#29992;&#20110;&#22810;&#32500;&#25968;&#25454;&#65292;&#30456;&#27604;&#29616;&#26377;&#20844;&#24335;&#20943;&#23569;&#20102;&#36229;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#26377;&#21033;&#30340;&#20248;&#21270;&#32467;&#26500;&#21644;&#36924;&#36817;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20351;GSM&#26680;&#20013;&#30340;&#22823;&#35268;&#27169;&#36229;&#21442;&#25968;&#20248;&#21270;&#21464;&#24471;&#21487;&#34892;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#20998;&#24067;&#24335;SCA&#65288;DSCA&#65289;&#31639;&#27861;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#22522;&#20110;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#27861;&#65288;ADMM&#65289;&#26694;&#26550;&#25552;&#20986;&#20102;&#21452;&#37325;&#20998;&#24067;&#24335;SCA&#65288;D$^2$SCA&#65289;&#31639;&#27861;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#21512;&#20316;&#22320;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian processes (GPs) have emerged as a prominent technique for machine learning and signal processing. A key component in GP modeling is the choice of kernel, and linear multiple kernels (LMKs) have become an attractive kernel class due to their powerful modeling capacity and interpretability. This paper focuses on the grid spectral mixture (GSM) kernel, an LMK that can approximate arbitrary stationary kernels. Specifically, we propose a novel GSM kernel formulation for multi-dimensional data that reduces the number of hyper-parameters compared to existing formulations, while also retaining a favorable optimization structure and approximation capability. In addition, to make the large-scale hyper-parameter optimization in the GSM kernel tractable, we first introduce the distributed SCA (DSCA) algorithm. Building on this, we propose the doubly distributed SCA (D$^2$SCA) algorithm based on the alternating direction method of multipliers (ADMM) framework, which allows us to cooperativ
&lt;/p&gt;</description></item><item><title>beta&#25193;&#25955;&#26159;&#19968;&#31181;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21435;&#25513;&#30422;&#21644;&#21435;&#22122;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;&#32553;&#25918;&#21644;&#20559;&#31227;&#30340;beta&#20998;&#24067;&#36827;&#34892;&#20056;&#27861;&#36716;&#25442;&#65292;&#23454;&#29616;&#22312;&#26377;&#30028;&#33539;&#22260;&#20869;&#29983;&#25104;&#25968;&#25454;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;KL&#25955;&#24230;&#19978;&#30028;&#36827;&#34892;&#20248;&#21270;&#65292;&#35777;&#26126;&#20102;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.07867</link><description>&lt;p&gt;
Beta Diffusion. (arXiv:2309.07867v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Beta Diffusion. (arXiv:2309.07867v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07867
&lt;/p&gt;
&lt;p&gt;
beta&#25193;&#25955;&#26159;&#19968;&#31181;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21435;&#25513;&#30422;&#21644;&#21435;&#22122;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;&#32553;&#25918;&#21644;&#20559;&#31227;&#30340;beta&#20998;&#24067;&#36827;&#34892;&#20056;&#27861;&#36716;&#25442;&#65292;&#23454;&#29616;&#22312;&#26377;&#30028;&#33539;&#22260;&#20869;&#29983;&#25104;&#25968;&#25454;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;KL&#25955;&#24230;&#19978;&#30028;&#36827;&#34892;&#20248;&#21270;&#65292;&#35777;&#26126;&#20102;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;beta&#25193;&#25955;&#65292;&#19968;&#31181;&#23558;&#21435;&#25513;&#30422;&#21644;&#21435;&#22122;&#38598;&#25104;&#21040;&#19968;&#36215;&#30340;&#26032;&#22411;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26377;&#30028;&#33539;&#22260;&#20869;&#29983;&#25104;&#25968;&#25454;&#12290;&#20351;&#29992;&#20102;&#32553;&#25918;&#21644;&#20559;&#31227;&#30340;beta&#20998;&#24067;&#65292;beta&#25193;&#25955;&#21033;&#29992;&#20102;&#38543;&#26102;&#38388;&#30340;&#20056;&#27861;&#36716;&#25442;&#26469;&#21019;&#24314;&#27491;&#21521;&#21644;&#21453;&#21521;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#21516;&#26102;&#32500;&#25345;&#30528;&#27491;&#21521;&#36793;&#32536;&#20998;&#24067;&#21644;&#21453;&#21521;&#26465;&#20214;&#20998;&#24067;&#65292;&#32473;&#23450;&#20219;&#24847;&#26102;&#38388;&#28857;&#30340;&#25968;&#25454;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#19981;&#21516;&#65292;&#20256;&#32479;&#27169;&#22411;&#20381;&#36182;&#20110;&#21152;&#24615;&#39640;&#26031;&#22122;&#22768;&#21644;&#37325;&#26032;&#21152;&#26435;&#30340;&#35777;&#25454;&#19979;&#30028;&#65288;ELBO&#65289;&#65292;beta&#25193;&#25955;&#26159;&#20056;&#27861;&#30340;&#65292;&#24182;&#19988;&#36890;&#36807;&#20174;KL&#25955;&#24230;&#30340;&#20984;&#24615;&#25512;&#23548;&#20986;&#26469;&#30340;KL&#25955;&#24230;&#19978;&#30028;&#65288;KLUB&#65289;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;KLUB&#30456;&#23545;&#20110;&#36127;ELBO&#26469;&#35828;&#23545;&#20110;&#20248;&#21270;beta&#25193;&#25955;&#26356;&#21152;&#26377;&#25928;&#65292;&#36127;ELBO&#20063;&#21487;&#20197;&#20316;&#20026;&#30456;&#21516;KL&#25955;&#24230;&#30340;KLUB&#65292;&#21482;&#26159;&#20854;&#20004;&#20010;&#21442;&#25968;&#20132;&#25442;&#20102;&#20301;&#32622;&#12290;beta&#25193;&#25955;&#30340;&#25439;&#22833;&#20989;&#25968;&#20197;Bregman&#25955;&#24230;&#20026;&#25351;&#26631;&#26469;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce beta diffusion, a novel generative modeling method that integrates demasking and denoising to generate data within bounded ranges. Using scaled and shifted beta distributions, beta diffusion utilizes multiplicative transitions over time to create both forward and reverse diffusion processes, maintaining beta distributions in both the forward marginals and the reverse conditionals, given the data at any point in time. Unlike traditional diffusion-based generative models relying on additive Gaussian noise and reweighted evidence lower bounds (ELBOs), beta diffusion is multiplicative and optimized with KL-divergence upper bounds (KLUBs) derived from the convexity of the KL divergence. We demonstrate that the proposed KLUBs are more effective for optimizing beta diffusion compared to negative ELBOs, which can also be derived as the KLUBs of the same KL divergence with its two arguments swapped. The loss function of beta diffusion, expressed in terms of Bregman divergence, furt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;ResNet&#27169;&#22411;&#21644;&#36731;&#37327;&#32423;&#27880;&#24847;&#26426;&#21046;&#26694;&#26550;&#30340;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#29305;&#24449;&#34920;&#31034;&#12289;&#22686;&#24378;&#20998;&#31867;&#33021;&#21147;&#21644;&#25913;&#21892;&#29305;&#24449;&#21487;&#36776;&#21035;&#24615;&#65292;&#22312;&#20083;&#33146;&#30284;&#20998;&#31867;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#21331;&#36234;&#24615;&#33021;&#21644;&#28508;&#22312;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2308.13150</link><description>&lt;p&gt;
&#20351;&#29992;&#24102;&#36731;&#37327;&#32423;&#27880;&#24847;&#26426;&#21046;&#30340;&#36801;&#31227;ResNet&#22686;&#24378;&#20083;&#33146;&#30284;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Enhancing Breast Cancer Classification Using Transfer ResNet with Lightweight Attention Mechanism. (arXiv:2308.13150v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;ResNet&#27169;&#22411;&#21644;&#36731;&#37327;&#32423;&#27880;&#24847;&#26426;&#21046;&#26694;&#26550;&#30340;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#29305;&#24449;&#34920;&#31034;&#12289;&#22686;&#24378;&#20998;&#31867;&#33021;&#21147;&#21644;&#25913;&#21892;&#29305;&#24449;&#21487;&#36776;&#21035;&#24615;&#65292;&#22312;&#20083;&#33146;&#30284;&#20998;&#31867;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#21331;&#36234;&#24615;&#33021;&#21644;&#28508;&#22312;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#21407;&#22987;&#20687;&#32032;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#29305;&#24449;&#23618;&#27425;&#32467;&#26500;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#22270;&#20687;&#20998;&#31867;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;ResNet&#27169;&#22411;&#30340;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#36731;&#37327;&#32423;&#27880;&#24847;&#26426;&#21046;&#26694;&#26550;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#35813;&#26694;&#26550;&#20248;&#21270;&#29305;&#24449;&#34920;&#31034;&#65292;&#22686;&#24378;&#20998;&#31867;&#33021;&#21147;&#65292;&#25913;&#21892;&#29305;&#24449;&#21487;&#36776;&#21035;&#24615;&#12290;&#25105;&#20204;&#22312;Breakhis&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#35768;&#22810;&#26041;&#38754;&#26174;&#31034;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;&#20256;&#32479;&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20248;&#36234;&#65292;&#22312;&#24403;&#20195;&#35270;&#35273;&#21464;&#25442;&#22120;&#31561;&#26368;&#26032;&#26041;&#27861;&#19978;&#20063;&#26174;&#31034;&#20986;&#20248;&#21183;&#12290;&#22312;&#35832;&#22914;&#31934;&#24230;&#12289;&#20934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#12289;F1&#20998;&#25968;&#21644;G-means&#31561;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#21516;&#26102;&#22312;&#25910;&#25947;&#26102;&#38388;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;&#36825;&#20123;&#32467;&#26524;&#22686;&#24378;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#24041;&#22266;&#20102;&#20854;&#22312;&#23454;&#38469;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models have revolutionized image classification by learning complex feature hierarchies in raw pixel data. This paper introduces an image classification method based on the ResNet model, and introduces a lightweight attention mechanism framework to improve performance. The framework optimizes feature representation, enhances classification capabilities, and improves feature discriminativeness. We verified the effectiveness of the algorithm on the Breakhis dataset, showing its superior performance in many aspects. Not only in terms of conventional models, our method also shows advantages on state-of-the-art methods such as contemporary visual transformers. Significant improvements have been achieved in metrics such as precision, accuracy, recall, F1-score, and G-means, while also performing well in terms of convergence time. These results strengthen the performance of the algorithm and solidify its application prospects in practical image classification tasks. Keywords: Re
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20915;&#31574;&#32858;&#28966;&#20195;&#29702;&#24314;&#27169;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#31616;&#21270;&#30340;&#20984;&#20248;&#21270;&#27169;&#22411;&#26469;&#26368;&#23567;&#21270;&#21407;&#22987;&#21644;&#20195;&#29702;&#20248;&#21270;&#27169;&#22411;&#20043;&#38388;&#30340;&#20915;&#31574;&#39044;&#27979;&#35823;&#24046;&#65292;&#24182;&#22312;&#23454;&#26102;&#29615;&#22659;&#20013;&#35299;&#20915;&#35745;&#31639;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38750;&#32447;&#24615;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.12161</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#20915;&#31574;&#32858;&#28966;&#20195;&#29702;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Data-driven decision-focused surrogate modeling. (arXiv:2308.12161v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12161
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20915;&#31574;&#32858;&#28966;&#20195;&#29702;&#24314;&#27169;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#31616;&#21270;&#30340;&#20984;&#20248;&#21270;&#27169;&#22411;&#26469;&#26368;&#23567;&#21270;&#21407;&#22987;&#21644;&#20195;&#29702;&#20248;&#21270;&#27169;&#22411;&#20043;&#38388;&#30340;&#20915;&#31574;&#39044;&#27979;&#35823;&#24046;&#65292;&#24182;&#22312;&#23454;&#26102;&#29615;&#22659;&#20013;&#35299;&#20915;&#35745;&#31639;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38750;&#32447;&#24615;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#20915;&#31574;&#32858;&#28966;&#20195;&#29702;&#24314;&#27169;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#22312;&#23454;&#26102;&#29615;&#22659;&#20013;&#35299;&#20915;&#35745;&#31639;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38750;&#32447;&#24615;&#20248;&#21270;&#38382;&#39064;&#12290;&#25552;&#20986;&#30340;&#25968;&#25454;&#39537;&#21160;&#26694;&#26550;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#12289;&#20363;&#22914;&#20984;&#20248;&#21270;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#20197;&#26368;&#23567;&#21270;&#20915;&#31574;&#39044;&#27979;&#35823;&#24046;&#65292;&#20915;&#31574;&#39044;&#27979;&#35823;&#24046;&#23450;&#20041;&#20026;&#21407;&#22987;&#21644;&#20195;&#29702;&#20248;&#21270;&#27169;&#22411;&#30340;&#26368;&#20248;&#35299;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#23398;&#20064;&#38382;&#39064;&#34987;&#26500;&#24314;&#20026;&#19968;&#20010;&#21452;&#23618;&#35268;&#21010;&#38382;&#39064;&#65292;&#21487;&#20197;&#30475;&#20316;&#26159;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#36870;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20197;&#21069;&#24037;&#20316;&#20013;&#30340;&#22522;&#20110;&#20998;&#35299;&#30340;&#27714;&#35299;&#31639;&#27861;&#12290;&#36890;&#36807;&#28041;&#21450;&#24120;&#35265;&#38750;&#32447;&#24615;&#21270;&#23398;&#36807;&#31243;&#65288;&#22914;&#21270;&#23398;&#21453;&#24212;&#22120;&#12289;&#25442;&#28909;&#22120;&#32593;&#32476;&#21644;&#26448;&#26009;&#28151;&#21512;&#31995;&#32479;&#65289;&#30340;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#36824;&#23558;&#20915;&#31574;&#32858;&#28966;&#20195;&#29702;&#24314;&#27169;&#19982;&#26631;&#20934;&#30340;&#25968;&#25454;&#39537;&#21160;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#36827;&#34892;&#20102;&#35814;&#32454;&#27604;&#36739;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the concept of decision-focused surrogate modeling for solving computationally challenging nonlinear optimization problems in real-time settings. The proposed data-driven framework seeks to learn a simpler, e.g. convex, surrogate optimization model that is trained to minimize the decision prediction error, which is defined as the difference between the optimal solutions of the original and the surrogate optimization models. The learning problem, formulated as a bilevel program, can be viewed as a data-driven inverse optimization problem to which we apply a decomposition-based solution algorithm from previous work. We validate our framework through numerical experiments involving the optimization of common nonlinear chemical processes such as chemical reactors, heat exchanger networks, and material blending systems. We also present a detailed comparison of decision-focused surrogate modeling with standard data-driven surrogate modeling methods and demonstrate that our appro
&lt;/p&gt;</description></item><item><title>CSVTO&#26159;&#19968;&#31181;&#21463;&#38480;&#26031;&#22374;&#21464;&#20998;&#36712;&#36857;&#20248;&#21270;&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#29983;&#25104;&#22810;&#26679;&#30340;&#32422;&#26463;&#28385;&#36275;&#36712;&#36857;&#38598;&#21512;&#65292;&#25552;&#39640;&#20102;&#22312;&#20855;&#26377;&#20219;&#24847;&#32422;&#26463;&#30340;&#38382;&#39064;&#20013;&#30340;&#20248;&#21270;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.12110</link><description>&lt;p&gt;
&#21463;&#38480;&#26031;&#22374;&#21464;&#20998;&#36712;&#36857;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Constrained Stein Variational Trajectory Optimization. (arXiv:2308.12110v1 [cs.RO] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12110
&lt;/p&gt;
&lt;p&gt;
CSVTO&#26159;&#19968;&#31181;&#21463;&#38480;&#26031;&#22374;&#21464;&#20998;&#36712;&#36857;&#20248;&#21270;&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#29983;&#25104;&#22810;&#26679;&#30340;&#32422;&#26463;&#28385;&#36275;&#36712;&#36857;&#38598;&#21512;&#65292;&#25552;&#39640;&#20102;&#22312;&#20855;&#26377;&#20219;&#24847;&#32422;&#26463;&#30340;&#38382;&#39064;&#20013;&#30340;&#20248;&#21270;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#38480;&#26031;&#22374;&#21464;&#20998;&#36712;&#36857;&#20248;&#21270;&#65288;CSVTO&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#19968;&#32452;&#36712;&#36857;&#19978;&#36827;&#34892;&#24102;&#32422;&#26463;&#30340;&#36712;&#36857;&#20248;&#21270;&#12290;&#25105;&#20204;&#23558;&#21463;&#38480;&#36712;&#36857;&#20248;&#21270;&#35270;&#20026;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#36712;&#36857;&#20998;&#24067;&#32422;&#26463;&#30340;&#20989;&#25968;&#26368;&#23567;&#21270;&#24418;&#24335;&#65292;&#36991;&#20813;&#23558;&#32422;&#26463;&#35270;&#20026;&#30446;&#26631;&#20989;&#25968;&#30340;&#24809;&#32602;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#30340;&#28385;&#36275;&#32422;&#26463;&#30340;&#36712;&#36857;&#38598;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#65288;SVGD&#65289;&#23547;&#25214;&#19968;&#32452;&#31890;&#23376;&#65292;&#36817;&#20284;&#34920;&#31034;&#19968;&#20010;&#20302;&#25104;&#26412;&#36712;&#36857;&#30340;&#20998;&#24067;&#65292;&#24182;&#36981;&#23432;&#32422;&#26463;&#12290;CSVTO&#36866;&#29992;&#20110;&#20855;&#26377;&#20219;&#24847;&#31561;&#24335;&#21644;&#19981;&#31561;&#24335;&#32422;&#26463;&#30340;&#38382;&#39064;&#65292;&#24182;&#21253;&#25324;&#19968;&#31181;&#26032;&#39062;&#30340;&#31890;&#23376;&#37325;&#26032;&#37319;&#26679;&#27493;&#39588;&#26469;&#36991;&#20813;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#36890;&#36807;&#26126;&#30830;&#29983;&#25104;&#22810;&#26679;&#30340;&#36712;&#36857;&#38598;&#21512;&#65292;CSVTO&#33021;&#22815;&#26356;&#22909;&#22320;&#36991;&#20813;&#19981;&#33391;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#24182;&#19988;&#23545;&#21021;&#22987;&#21270;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;CSVTO&#22312;&#20855;&#26377;&#39640;&#24230;&#32422;&#26463;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#19978;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Constrained Stein Variational Trajectory Optimization (CSVTO), an algorithm for performing trajectory optimization with constraints on a set of trajectories in parallel. We frame constrained trajectory optimization as a novel form of constrained functional minimization over trajectory distributions, which avoids treating the constraints as a penalty in the objective and allows us to generate diverse sets of constraint-satisfying trajectories. Our method uses Stein Variational Gradient Descent (SVGD) to find a set of particles that approximates a distribution over low-cost trajectories while obeying constraints. CSVTO is applicable to problems with arbitrary equality and inequality constraints and includes a novel particle resampling step to escape local minima. By explicitly generating diverse sets of trajectories, CSVTO is better able to avoid poor local minima and is more robust to initialization. We demonstrate that CSVTO outperforms baselines in challenging highly-constr
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#20013;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#31034;&#27491;&#30830;&#30340;&#19978;&#19979;&#25991;&#12290;&#36890;&#36807;&#26500;&#24314;&#22810;&#20010;&#25991;&#26723;&#19978;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#35774;&#35745;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#36941;&#21382;&#22120;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#24110;&#21161;LLMs&#22312;MD-QA&#20013;&#36827;&#34892;&#31572;&#26696;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.11730</link><description>&lt;p&gt;
&#22810;&#25991;&#26723;&#38382;&#31572;&#20013;&#30340;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Prompting for Multi-Document Question Answering. (arXiv:2308.11730v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11730
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#20013;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#31034;&#27491;&#30830;&#30340;&#19978;&#19979;&#25991;&#12290;&#36890;&#36807;&#26500;&#24314;&#22810;&#20010;&#25991;&#26723;&#19978;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#35774;&#35745;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#36941;&#21382;&#22120;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#24110;&#21161;LLMs&#22312;MD-QA&#20013;&#36827;&#34892;&#31572;&#26696;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#8220;&#39044;&#35757;&#32451;&#12289;&#25552;&#31034;&#12289;&#39044;&#27979;&#8221;&#33539;&#24335;&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#65288;OD-QA&#65289;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#24037;&#20316;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#65288;MD-QA&#65289;&#22330;&#26223;&#19979;&#25506;&#32034;&#36825;&#20010;&#33539;&#24335;&#65292;&#36825;&#26159;&#19968;&#20010;&#35201;&#27714;&#23545;&#19981;&#21516;&#25991;&#26723;&#30340;&#20869;&#23481;&#21644;&#32467;&#26500;&#20043;&#38388;&#30340;&#36923;&#36753;&#20851;&#32852;&#26377;&#28145;&#20837;&#29702;&#35299;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#37325;&#35201;&#30340;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;&#65288;KGP&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;MD-QA&#20013;&#20026;LLMs&#25552;&#31034;&#27491;&#30830;&#30340;&#19978;&#19979;&#25991;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#22270;&#26500;&#24314;&#27169;&#22359;&#21644;&#22270;&#36941;&#21382;&#27169;&#22359;&#12290;&#23545;&#20110;&#22270;&#26500;&#24314;&#65292;&#25105;&#20204;&#20351;&#29992;&#33410;&#28857;&#26469;&#34920;&#31034;&#25991;&#27573;&#25110;&#25991;&#26723;&#32467;&#26500;&#65288;&#20363;&#22914;&#65292;&#39029;&#38754;/&#34920;&#26684;&#65289;&#65292;&#32780;&#20351;&#29992;&#36793;&#26469;&#34920;&#31034;&#25991;&#27573;&#20043;&#38388;&#30340;&#35821;&#20041;/&#35789;&#27719;&#30456;&#20284;&#24615;&#25110;&#32773;&#25991;&#26723;&#20869;&#30340;&#32467;&#26500;&#20851;&#31995;&#12290;&#23545;&#20110;&#22270;&#36941;&#21382;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;LM&#30340;&#22270;&#36941;&#21382;&#22120;&#65292;&#23427;&#22312;&#33410;&#28857;&#20043;&#38388;&#23548;&#33322;&#24182;&#25910;&#38598;&#25903;&#25345;&#24615;&#30340;&#25991;&#27573;&#65292;&#20197;&#24110;&#21161;LLMs&#22312;MD-QA&#20013;&#36827;&#34892;&#31572;&#26696;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The 'pre-train, prompt, predict' paradigm of large language models (LLMs) has achieved remarkable success in open-domain question answering (OD-QA). However, few works explore this paradigm in the scenario of multi-document question answering (MD-QA), a task demanding a thorough understanding of the logical associations among the contents and structures of different documents. To fill this crucial gap, we propose a Knowledge Graph Prompting (KGP) method to formulate the right context in prompting LLMs for MD-QA, which consists of a graph construction module and a graph traversal module. For graph construction, we create a knowledge graph (KG) over multiple documents with nodes symbolizing passages or document structures (e.g., pages/tables), and edges denoting the semantic/lexical similarity between passages or intra-document structural relations. For graph traversal, we design an LM-guided graph traverser that navigates across nodes and gathers supporting passages assisting LLMs in MD
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#20989;&#25968;&#30340;&#35299;&#26512;&#31215;&#20998;&#65292;&#20855;&#26377;&#35745;&#31639;&#31934;&#30830;&#31215;&#20998;&#30340;&#33021;&#21147;&#65292;&#24182;&#33021;&#23558;&#32422;&#26463;&#30452;&#25509;&#24212;&#29992;&#20110;&#31215;&#20998;&#65292;&#32780;&#19988;&#36824;&#20171;&#32461;&#20102;&#23558;&#23398;&#20064;&#20989;&#25968;&#32422;&#26463;&#20026;&#27491;&#30340;&#26041;&#27861;&#20197;&#21450;&#30456;&#20851;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.14439</link><description>&lt;p&gt;
&#22266;&#23450;&#31215;&#20998;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Fixed Integral Neural Networks. (arXiv:2307.14439v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#20989;&#25968;&#30340;&#35299;&#26512;&#31215;&#20998;&#65292;&#20855;&#26377;&#35745;&#31639;&#31934;&#30830;&#31215;&#20998;&#30340;&#33021;&#21147;&#65292;&#24182;&#33021;&#23558;&#32422;&#26463;&#30452;&#25509;&#24212;&#29992;&#20110;&#31215;&#20998;&#65292;&#32780;&#19988;&#36824;&#20171;&#32461;&#20102;&#23558;&#23398;&#20064;&#20989;&#25968;&#32422;&#26463;&#20026;&#27491;&#30340;&#26041;&#27861;&#20197;&#21450;&#30456;&#20851;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#23398;&#20064;&#20989;&#25968;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#31215;&#20998;&#26159;&#38750;&#24120;&#26377;&#29992;&#30340;&#65292;&#20294;&#26159;&#36825;&#31181;&#31215;&#20998;&#36890;&#24120;&#26159;&#36890;&#36807;&#25968;&#20540;&#26041;&#27861;&#26469;&#35745;&#31639;&#30340;&#65292;&#22240;&#20026;&#35299;&#26512;&#35745;&#31639;&#31215;&#20998;&#36807;&#31243;&#22797;&#26434;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#36825;&#26679;&#30340;&#23398;&#20064;&#20989;&#25968;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#34920;&#31034;&#23398;&#20064;&#20989;&#25968; $f$ &#35299;&#26512;&#31215;&#20998;&#30340;&#26041;&#27861;&#12290;&#36825;&#20801;&#35768;&#31934;&#30830;&#35745;&#31639;&#31070;&#32463;&#32593;&#32476;&#30340;&#31215;&#20998;&#65292;&#24182;&#19988;&#36890;&#36807;&#23558;&#32422;&#26463;&#30452;&#25509;&#24212;&#29992;&#20110;&#31215;&#20998;&#26469;&#23545;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21442;&#25968;&#21270;&#12290;&#20851;&#38190;&#30340;&#26159;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558; $f$ &#32422;&#26463;&#20026;&#27491;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#35768;&#22810;&#24212;&#29992;&#65288;&#20363;&#22914;&#27010;&#29575;&#20998;&#24067;&#12289;&#36317;&#31163;&#24230;&#37327;&#31561;&#65289;&#25152;&#24517;&#38656;&#30340;&#26465;&#20214;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20960;&#20010;&#21487;&#20197;&#21033;&#29992;&#25105;&#20204;&#30340;&#22266;&#23450;&#31215;&#20998;&#31070;&#32463;&#32593;&#32476;&#65288;FINN&#65289;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is often useful to perform integration over learned functions represented by neural networks. However, this integration is usually performed numerically, as analytical integration over learned functions (especially neural networks) is generally viewed as intractable. In this work, we present a method for representing the analytical integral of a learned function $f$. This allows the exact integral of a neural network to be computed, and enables constrained neural networks to be parametrised by applying constraints directly to the integral. Crucially, we also introduce a method to constrain $f$ to be positive, a necessary condition for many applications (e.g. probability distributions, distance metrics, etc). Finally, we introduce several applications where our fixed-integral neural network (FINN) can be utilised.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25237;&#31080;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#29992;&#20110;&#33258;&#21160;&#27450;&#39575;&#26816;&#27979;&#65292;&#36890;&#36807;&#35270;&#39057;&#30340;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#27450;&#39575;&#26816;&#27979;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2307.07516</link><description>&lt;p&gt;
&#22522;&#20110;&#25237;&#31080;&#30340;&#22810;&#27169;&#24577;&#33258;&#21160;&#27450;&#39575;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Voting-based Multimodal Automatic Deception Detection. (arXiv:2307.07516v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25237;&#31080;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#29992;&#20110;&#33258;&#21160;&#27450;&#39575;&#26816;&#27979;&#65292;&#36890;&#36807;&#35270;&#39057;&#30340;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#27450;&#39575;&#26816;&#27979;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#27450;&#39575;&#26816;&#27979;&#19968;&#30452;&#26159;&#19968;&#20010;&#28909;&#38376;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#33258;&#21160;&#26816;&#27979;&#27450;&#39575;&#32473;&#36825;&#19968;&#26087;&#39046;&#22495;&#24102;&#26469;&#20102;&#26032;&#30340;&#20809;&#26126;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25237;&#31080;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#35270;&#39057;&#20013;&#20351;&#29992;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#33258;&#21160;&#27450;&#39575;&#26816;&#27979;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20998;&#21035;&#26159;&#23494;&#27463;&#26681;&#22823;&#23398;&#30340;&#30495;&#23454;&#35797;&#39564;&#25968;&#25454;&#38598;&#21644;&#36808;&#38463;&#23494;&#22823;&#23398;&#30340;&#27450;&#39575;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;&#35270;&#39057;&#26679;&#26412;&#34987;&#20998;&#25104;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#25163;&#31295;&#30340;&#24103;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#22810;&#27169;&#24577;&#25237;&#31080;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#19977;&#20010;&#27169;&#22411;&#12290;&#31532;&#19968;&#20010;&#27169;&#22411;&#26159;&#29992;&#20110;&#20174;&#22270;&#20687;&#20013;&#26816;&#27979;&#27450;&#39575;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#31532;&#20108;&#20010;&#27169;&#22411;&#26159;&#29992;&#20110;&#20174;&#38899;&#39057;&#20013;&#26816;&#27979;&#27450;&#39575;&#30340;Mel&#39057;&#35889;&#22270;&#19978;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#65292;&#31532;&#19977;&#20010;&#27169;&#22411;&#26159;&#29992;&#20110;&#20174;&#25163;&#31295;&#20013;&#26816;&#27979;&#27450;&#39575;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#19978;&#30340;Word2Vec&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#12290;&#22312;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#25991;&#26412;&#19978;&#21462;&#24471;&#30340;&#26368;&#20339;&#32467;&#26524;&#20998;&#21035;&#20026;97&#65285;&#12289;96&#65285;&#12289;9
&lt;/p&gt;
&lt;p&gt;
Automatic Deception Detection has been a hot research topic for a long time, using machine learning and deep learning to automatically detect deception, brings new light to this old field. In this paper, we proposed a voting-based method for automatic deception detection from videos using audio, visual and lexical features. Experiments were done on two datasets, the Real-life trial dataset by Michigan University and the Miami University deception detection dataset. Video samples were split into frames of images, audio, and manuscripts. Our Voting-based Multimodal proposed solution consists of three models. The first model is CNN for detecting deception from images, the second model is Support Vector Machine (SVM) on Mel spectrograms for detecting deception from audio and the third model is Word2Vec on Support Vector Machine (SVM) for detecting deception from manuscripts. Our proposed solution outperforms state of the art. Best results achieved on images, audio and text were 97%, 96%, 9
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26680;&#25216;&#26415;&#65292;&#20855;&#26377;&#35299;&#37322;&#24615;&#19988;&#21442;&#25968;&#36739;&#23569;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#21644;&#20998;&#21106;PPG&#20449;&#21495;&#30340;&#36136;&#37327;&#21644;&#20266;&#24433;&#65292;&#19982;&#29616;&#26377;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30456;&#27604;&#26377;&#30528;&#31867;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05385</link><description>&lt;p&gt;
&#23398;&#20064;&#26680;&#25216;&#26415;&#29992;&#20110;&#21487;&#35299;&#37322;&#21644;&#39640;&#25928;&#30340;PPG&#20449;&#21495;&#36136;&#37327;&#35780;&#20272;&#21644;&#20266;&#24433;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Learned Kernels for Interpretable and Efficient PPG Signal Quality Assessment and Artifact Segmentation. (arXiv:2307.05385v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26680;&#25216;&#26415;&#65292;&#20855;&#26377;&#35299;&#37322;&#24615;&#19988;&#21442;&#25968;&#36739;&#23569;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#21644;&#20998;&#21106;PPG&#20449;&#21495;&#30340;&#36136;&#37327;&#21644;&#20266;&#24433;&#65292;&#19982;&#29616;&#26377;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30456;&#27604;&#26377;&#30528;&#31867;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#30005;&#23481;&#25239;(PPG)&#25552;&#20379;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#12289;&#38750;&#20405;&#20837;&#24615;&#30340;&#26041;&#27861;&#26469;&#25345;&#32493;&#30417;&#27979;&#21508;&#31181;&#24515;&#34880;&#31649;&#21442;&#25968;&#12290;PPG&#20449;&#21495;&#30001;&#21487;&#31359;&#25140;&#35774;&#22791;&#20135;&#29983;&#65292;&#24120;&#24120;&#21253;&#21547;&#30001;&#22806;&#37096;&#22240;&#32032;(&#22914;&#20154;&#20307;&#36816;&#21160;)&#24341;&#36215;&#30340;&#22823;&#22411;&#20266;&#24433;&#12290;&#20026;&#20102;&#30830;&#20445;&#23545;&#29983;&#29702;&#21442;&#25968;&#36827;&#34892;&#31283;&#20581;&#21644;&#20934;&#30830;&#30340;&#25552;&#21462;&#65292;&#20449;&#21495;&#30340;&#25439;&#22351;&#21306;&#22495;&#38656;&#35201;&#34987;&#27491;&#30830;&#22320;&#35782;&#21035;&#21644;&#22788;&#29702;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#20381;&#38752;&#25163;&#24037;&#29305;&#24449;&#26816;&#27979;&#22120;&#25110;&#20449;&#21495;&#24230;&#37327;&#65292;&#32467;&#26524;&#24615;&#33021;&#19981;&#20339;&#65292;&#25110;&#20381;&#38752;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#31561;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#35745;&#31639;&#21644;&#20869;&#23384;&#23494;&#38598;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23398;&#20064;&#19968;&#23567;&#32452;&#21487;&#35299;&#37322;&#30340;&#21367;&#31215;&#26680;&#65292;&#20854;&#24615;&#33021;&#19982;&#29616;&#26377;&#25216;&#26415;DNN&#26041;&#27861;&#30456;&#20284;&#65292;&#29978;&#33267;&#26356;&#22909;&#65292;&#32780;&#21442;&#25968;&#25968;&#37327;&#27604;DNN&#26041;&#27861;&#23569;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#36825;&#39033;&#24037;&#20316;&#23454;&#29616;&#20102;&#39640;&#25928;&#12289;&#31283;&#20581;&#21644;&#21487;&#35299;&#37322;&#30340;PPG&#20449;&#21495;&#36136;&#37327;&#35780;&#20272;&#21644;&#20266;&#24433;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Photoplethysmography (PPG) provides a low-cost, non-invasive method to continuously monitor various cardiovascular parameters. PPG signals are generated by wearable devices and frequently contain large artifacts caused by external factors, such as motion of the human subject. In order to ensure robust and accurate extraction of physiological parameters, corrupted areas of the signal need to be identified and handled appropriately. Previous methodology relied either on handcrafted feature detectors or signal metrics which yield sub-optimal performance, or relied on machine learning techniques such as deep neural networks (DNN) which lack interpretability and are computationally and memory intensive. In this work, we present a novel method to learn a small set of interpretable convolutional kernels that has performance similar to -- and often better than -- the state-of-the-art DNN approach with several orders of magnitude fewer parameters. This work allows for efficient, robust, and int
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#26041;&#27861;&#65288;CPDG&#65289;&#29992;&#20110;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DGNNs&#65289;&#65292;&#36890;&#36807;&#28789;&#27963;&#30340;&#32467;&#26500;-&#26102;&#24207;&#23376;&#22270;&#37319;&#26679;&#21644;&#32467;&#26500;-&#26102;&#24207;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;DGNNs&#39044;&#35757;&#32451;&#20013;&#30340;&#27867;&#21270;&#21644;&#38271;&#30701;&#26399;&#24314;&#27169;&#33021;&#21147;&#31561;&#25361;&#25112;&#65292;&#23454;&#39564;&#35777;&#26126;CPDG&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#21160;&#24577;&#22270;&#39044;&#35757;&#32451;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.02813</link><description>&lt;p&gt;
CPDG: &#19968;&#31181;&#29992;&#20110;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CPDG: A Contrastive Pre-Training Method for Dynamic Graph Neural Networks. (arXiv:2307.02813v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02813
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#26041;&#27861;&#65288;CPDG&#65289;&#29992;&#20110;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DGNNs&#65289;&#65292;&#36890;&#36807;&#28789;&#27963;&#30340;&#32467;&#26500;-&#26102;&#24207;&#23376;&#22270;&#37319;&#26679;&#21644;&#32467;&#26500;-&#26102;&#24207;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;DGNNs&#39044;&#35757;&#32451;&#20013;&#30340;&#27867;&#21270;&#21644;&#38271;&#30701;&#26399;&#24314;&#27169;&#33021;&#21147;&#31561;&#25361;&#25112;&#65292;&#23454;&#39564;&#35777;&#26126;CPDG&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#21160;&#24577;&#22270;&#39044;&#35757;&#32451;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#21160;&#24577;&#22270;&#20013;&#34164;&#21547;&#20016;&#23500;&#20449;&#24687;&#24182;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#21160;&#24577;&#22270;&#25968;&#25454;&#25366;&#25496;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#23613;&#31649;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DGNNs&#65289;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#36827;&#23637;&#65292;&#20294;&#20854;&#20016;&#23500;&#30340;&#20449;&#24687;&#21644;&#22810;&#26679;&#30340;&#19979;&#28216;&#20219;&#21153;&#32473;&#22312;&#24037;&#19994;&#24773;&#26223;&#20013;&#23454;&#38469;&#24212;&#29992;&#24102;&#26469;&#20102;&#26174;&#33879;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#26041;&#27861;&#65288;CPDG&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;CPDG&#36890;&#36807;&#19968;&#31181;&#28789;&#27963;&#30340;&#32467;&#26500;-&#26102;&#24207;&#23376;&#22270;&#37319;&#26679;&#22120;&#21644;&#32467;&#26500;-&#26102;&#24207;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;DGNNs&#39044;&#35757;&#32451;&#20013;&#30340;&#27867;&#21270;&#21644;&#38271;&#30701;&#26399;&#24314;&#27169;&#33021;&#21147;&#31561;&#25361;&#25112;&#12290;&#22312;&#22823;&#35268;&#27169;&#30340;&#30740;&#31350;&#21644;&#24037;&#19994;&#21160;&#24577;&#22270;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;CPDG&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#21160;&#24577;&#22270;&#39044;&#35757;&#32451;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic graph data mining has gained popularity in recent years due to the rich information contained in dynamic graphs and their widespread use in the real world. Despite the advances in dynamic graph neural networks (DGNNs), the rich information and diverse downstream tasks have posed significant difficulties for the practical application of DGNNs in industrial scenarios. To this end, in this paper, we propose to address them by pre-training and present the Contrastive Pre-Training Method for Dynamic Graph Neural Networks (CPDG). CPDG tackles the challenges of pre-training for DGNNs, including generalization and long-short term modeling capability, through a flexible structural-temporal subgraph sampler along with structural-temporal contrastive pre-training schemes. Extensive experiments conducted on both large-scale research and industrial dynamic graph datasets show that CPDG outperforms existing methods in dynamic graph pre-training for various downstream tasks under three transf
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36171;&#33021;&#36830;&#25509;&#26234;&#33021;&#30340;&#33258;&#20027;&#36793;&#32536;AI&#31995;&#32479;&#65292;&#36890;&#36807;&#20113;-&#36793;&#32536;-&#23458;&#25143;&#31471;&#30340;&#20998;&#23618;&#26550;&#26500;&#21644;&#24378;&#22823;&#30340;GPT&#27169;&#22411;&#33021;&#21147;&#65292;&#23454;&#29616;&#39640;&#36136;&#37327;&#12289;&#20302;&#24310;&#36831;&#12289;&#38544;&#31169;&#20445;&#25252;&#30340;AI&#26381;&#21153;&#65292;&#28385;&#36275;&#29992;&#25143;&#20010;&#20154;&#38656;&#27714;&#24182;&#23454;&#29616;&#33258;&#21160;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.02779</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36171;&#33021;&#36830;&#25509;&#26234;&#33021;&#30340;&#33258;&#20027;&#36793;&#32536;AI
&lt;/p&gt;
&lt;p&gt;
Large Language Models Empowered Autonomous Edge AI for Connected Intelligence. (arXiv:2307.02779v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02779
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36171;&#33021;&#36830;&#25509;&#26234;&#33021;&#30340;&#33258;&#20027;&#36793;&#32536;AI&#31995;&#32479;&#65292;&#36890;&#36807;&#20113;-&#36793;&#32536;-&#23458;&#25143;&#31471;&#30340;&#20998;&#23618;&#26550;&#26500;&#21644;&#24378;&#22823;&#30340;GPT&#27169;&#22411;&#33021;&#21147;&#65292;&#23454;&#29616;&#39640;&#36136;&#37327;&#12289;&#20302;&#24310;&#36831;&#12289;&#38544;&#31169;&#20445;&#25252;&#30340;AI&#26381;&#21153;&#65292;&#28385;&#36275;&#29992;&#25143;&#20010;&#20154;&#38656;&#27714;&#24182;&#23454;&#29616;&#33258;&#21160;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#32447;&#32593;&#32476;&#30340;&#21457;&#23637;&#26397;&#30528;&#36830;&#25509;&#26234;&#33021;&#30340;&#26041;&#21521;&#21457;&#23637;&#65292;&#36825;&#19968;&#27010;&#24565;&#35774;&#24819;&#20102;&#22312;&#36229;&#36830;&#25509;&#30340;&#32593;&#32476;&#29289;&#29702;&#19990;&#30028;&#20013;&#65292;&#20154;&#31867;&#12289;&#29289;&#20307;&#21644;&#26234;&#33021;&#20043;&#38388;&#23454;&#29616;&#26080;&#32541;&#20114;&#32852;&#12290;&#36793;&#32536;AI&#20316;&#20026;&#23454;&#29616;&#36830;&#25509;&#26234;&#33021;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#32593;&#32476;&#36793;&#32536;&#25552;&#20379;&#39640;&#36136;&#37327;&#12289;&#20302;&#24310;&#36831;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;AI&#26381;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#20027;&#36793;&#32536;AI&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33258;&#21160;&#32452;&#32455;&#12289;&#36866;&#24212;&#21644;&#20248;&#21270;&#33258;&#24049;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#21508;&#31181;&#38656;&#27714;&#12290;&#35813;&#31995;&#32479;&#37319;&#29992;&#20113;-&#36793;&#32536;-&#23458;&#25143;&#31471;&#30340;&#20998;&#23618;&#26550;&#26500;&#65292;&#20854;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#8212;&#8212;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#65288;GPT&#65289;&#23384;&#25918;&#22312;&#20113;&#31471;&#65292;&#20854;&#20182;AI&#27169;&#22411;&#34987;&#20849;&#21516;&#37096;&#32626;&#22312;&#35774;&#22791;&#21644;&#36793;&#32536;&#26381;&#21153;&#22120;&#19978;&#12290;&#36890;&#36807;&#21033;&#29992;GPT&#22312;&#35821;&#35328;&#29702;&#35299;&#12289;&#35268;&#21010;&#21644;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#26694;&#26550;&#65292;&#26377;&#25928;&#21327;&#35843;&#36793;&#32536;AI&#27169;&#22411;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#20010;&#20154;&#38656;&#27714;&#65292;&#21516;&#26102;&#23454;&#29616;&#33258;&#21160;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evolution of wireless networks gravitates towards connected intelligence, a concept that envisions seamless interconnectivity among humans, objects, and intelligence in a hyper-connected cyber-physical world. Edge AI emerges as a promising solution to achieve connected intelligence by delivering high-quality, low-latency, and privacy-preserving AI services at the network edge. In this article, we introduce an autonomous edge AI system that automatically organizes, adapts, and optimizes itself to meet users' diverse requirements. The system employs a cloud-edge-client hierarchical architecture, where the large language model, i.e., Generative Pretrained Transformer (GPT), resides in the cloud, and other AI models are co-deployed on devices and edge servers. By leveraging the powerful abilities of GPT in language understanding, planning, and code generation, we present a versatile framework that efficiently coordinates edge AI models to cater to users' personal demands while automati
&lt;/p&gt;</description></item><item><title>TransformerG2G&#26159;&#19968;&#31181;&#20351;&#29992;Transformer&#36827;&#34892;&#33258;&#36866;&#24212;&#26102;&#38388;&#27493;&#38271;&#30340;&#22270;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#21382;&#21490;&#19978;&#30340;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#65292;&#20934;&#30830;&#22320;&#25429;&#25417;&#26102;&#24577;&#22270;&#30340;&#21160;&#24577;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2307.02588</link><description>&lt;p&gt;
TransformerG2G&#65306;&#20351;&#29992;Transformer&#36827;&#34892;&#33258;&#36866;&#24212;&#26102;&#38388;&#27493;&#38271;&#30340;&#23398;&#20064;&#26102;&#24577;&#22270;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
TransformerG2G: Adaptive time-stepping for learning temporal graph embeddings using transformers. (arXiv:2307.02588v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02588
&lt;/p&gt;
&lt;p&gt;
TransformerG2G&#26159;&#19968;&#31181;&#20351;&#29992;Transformer&#36827;&#34892;&#33258;&#36866;&#24212;&#26102;&#38388;&#27493;&#38271;&#30340;&#22270;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#21382;&#21490;&#19978;&#30340;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#65292;&#20934;&#30830;&#22320;&#25429;&#25417;&#26102;&#24577;&#22270;&#30340;&#21160;&#24577;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22270;&#23884;&#20837;&#24050;&#25104;&#20026;&#22788;&#29702;&#19981;&#21516;&#26102;&#38388;&#22270;&#20998;&#26512;&#20219;&#21153;&#65288;&#22914;&#38142;&#36335;&#39044;&#27979;&#12289;&#33410;&#28857;&#20998;&#31867;&#12289;&#25512;&#33616;&#31995;&#32479;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#22270;&#29983;&#25104;&#65289;&#30340;&#19968;&#31181;&#38750;&#24120;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#12290;&#36825;&#20123;&#26102;&#24577;&#22270;&#23637;&#29616;&#20102;&#24322;&#36136;&#30340;&#30636;&#26102;&#21160;&#24577;&#12289;&#19981;&#21516;&#30340;&#26102;&#38388;&#38388;&#38548;&#20197;&#21450;&#22312;&#28436;&#21270;&#36807;&#31243;&#20013;&#39640;&#24230;&#21464;&#21270;&#30340;&#33410;&#28857;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#23558;&#21382;&#21490;&#22270;&#19978;&#30340;&#38271;&#31243;&#20381;&#36182;&#34701;&#20837;&#21040;&#23398;&#20064;&#26102;&#24577;&#21160;&#24577;&#30340;&#36807;&#31243;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#22270;&#23884;&#20837;&#27169;&#22411;TransformerG2G&#65292;&#36890;&#36807;&#21033;&#29992;&#20808;&#36827;&#30340;Transformer&#32534;&#30721;&#22120;&#20174;&#24403;&#21069;&#29366;&#24577;&#65288;$t$&#65289;&#21644;&#20043;&#21069;&#30340;&#19978;&#19979;&#25991;&#65288;&#26102;&#38388;&#25139;[$t-1, t-l$]&#65292;$l$&#26159;&#19978;&#19979;&#25991;&#30340;&#38271;&#24230;&#65289;&#20013;&#39318;&#20808;&#23398;&#20064;&#20013;&#38388;&#33410;&#28857;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20004;&#20010;&#25237;&#24433;&#23618;&#26469;&#29983;&#25104;&#27599;&#20010;&#33410;&#28857;&#30340;&#20302;&#32500;&#22810;&#20803;&#39640;&#26031;&#20998;&#24067;&#65292;&#20316;&#20026;&#20854;&#28508;&#22312;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic graph embedding has emerged as a very effective technique for addressing diverse temporal graph analytic tasks (i.e., link prediction, node classification, recommender systems, anomaly detection, and graph generation) in various applications. Such temporal graphs exhibit heterogeneous transient dynamics, varying time intervals, and highly evolving node features throughout their evolution. Hence, incorporating long-range dependencies from the historical graph context plays a crucial role in accurately learning their temporal dynamics. In this paper, we develop a graph embedding model with uncertainty quantification, TransformerG2G, by exploiting the advanced transformer encoder to first learn intermediate node representations from its current state ($t$) and previous context (over timestamps [$t-1, t-l$], $l$ is the length of context). Moreover, we employ two projection layers to generate lower-dimensional multivariate Gaussian distributions as each node's latent embedding at ti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#37327;&#21270;&#25968;&#25454;&#27969;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31227;&#38500;&#29983;&#32974;&#26356;&#26032;&#24555;&#36895;&#36866;&#24212;&#27010;&#24565;&#28418;&#31227;&#65292;&#21487;&#20197;&#20135;&#29983;&#26368;&#23567;&#21270;&#30340;&#27515;&#21333;&#20803;&#65292;&#24182;&#20026;&#28418;&#31227;&#26816;&#27979;&#25552;&#20379;&#20102;&#19968;&#20123;&#26377;&#29992;&#30340;&#24230;&#37327;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2306.12574</link><description>&lt;p&gt;
&#19968;&#31181;&#36890;&#36807;&#31227;&#38500;&#29983;&#32974;&#26356;&#26032;&#27969;&#25968;&#25454;&#22312;&#32447;&#37327;&#21270;&#30340;&#39640;&#25928;&#31616;&#21333;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An efficient and straightforward online quantization method for a data stream through remove-birth updating. (arXiv:2306.12574v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#37327;&#21270;&#25968;&#25454;&#27969;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31227;&#38500;&#29983;&#32974;&#26356;&#26032;&#24555;&#36895;&#36866;&#24212;&#27010;&#24565;&#28418;&#31227;&#65292;&#21487;&#20197;&#20135;&#29983;&#26368;&#23567;&#21270;&#30340;&#27515;&#21333;&#20803;&#65292;&#24182;&#20026;&#28418;&#31227;&#26816;&#27979;&#25552;&#20379;&#20102;&#19968;&#20123;&#26377;&#29992;&#30340;&#24230;&#37327;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#35774;&#22791;&#30340;&#22686;&#38271;&#27491;&#22312;&#21019;&#36896;&#20986;&#22823;&#37327;&#25968;&#25454;&#65292;&#21363;&#25152;&#35859;&#30340;&#22823;&#25968;&#25454;&#65292;&#24182;&#23545;&#26377;&#25928;&#25968;&#25454;&#20998;&#26512;&#25552;&#20986;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#36825;&#20123;&#25968;&#25454;&#19981;&#26029;&#22320;&#20135;&#29983;&#65292;&#24418;&#25104;&#20102;&#21160;&#24577;&#27969;&#25968;&#25454;&#12290;&#27969;&#25968;&#25454;&#30340;&#29305;&#24449;&#21487;&#33021;&#20250;&#21160;&#24577;&#21464;&#21270;&#65292;&#36825;&#31181;&#21464;&#21270;&#34987;&#31216;&#20026;&#27010;&#24565;&#28418;&#31227;&#12290;&#22240;&#27492;&#65292;&#22788;&#29702;&#27969;&#25968;&#25454;&#30340;&#26041;&#27861;&#24517;&#39035;&#22312;&#21160;&#24577;&#36866;&#24212;&#36825;&#20123;&#21464;&#21270;&#30340;&#21516;&#26102;&#65292;&#39640;&#25928;&#22320;&#32553;&#20943;&#23427;&#20204;&#30340;&#20307;&#31215;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#27010;&#24565;&#28418;&#31227;&#22312;&#32447;&#21521;&#37327;&#37327;&#21270;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#31227;&#38500;&#29983;&#32974;&#26356;&#26032;&#35782;&#21035;&#24182;&#26367;&#25442;&#27010;&#29575;&#20302;&#30340;&#21333;&#20803;&#65292;&#20174;&#32780;&#24555;&#36895;&#36866;&#24212;&#27010;&#24565;&#28418;&#31227;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#27010;&#24565;&#28418;&#31227;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#20063;&#21487;&#20197;&#20135;&#29983;&#26368;&#23567;&#21270;&#30340;&#27515;&#21333;&#20803;&#12290;&#26412;&#30740;&#31350;&#36824;&#34920;&#26126;&#19968;&#20123;&#36890;&#36807;&#25152;&#25552;&#20986;&#26041;&#27861;&#35745;&#31639;&#20986;&#26469;&#30340;&#24230;&#37327;&#25351;&#26631;&#23545;&#20110;&#28418;&#31227;&#26816;&#27979;&#23558;&#26159;&#26377;&#30410;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growth of network-connected devices is creating an explosion of data, known as big data, and posing significant challenges to efficient data analysis. This data is generated continuously, creating a dynamic flow known as a data stream. The characteristics of a data stream may change dynamically, and this change is known as concept drift. Consequently, a method for handling data streams must efficiently reduce their volume while dynamically adapting to these changing characteristics. This paper proposes a simple online vector quantization method for concept drift. The proposed method identifies and replaces units with low win probability through remove-birth updating, thus achieving a rapid adaptation to concept drift. Furthermore, the results of this study show that the proposed method can generate minimal dead units even in the presence of concept drift. This study also suggests that some metrics calculated from the proposed method will be helpful for drift detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#38543;&#26426;&#38646;&#38454;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20108;&#27425;&#22411;&#30446;&#26631;&#20989;&#25968;&#21450;&#20854;&#23616;&#37096;&#20960;&#20309;&#32467;&#26500;&#30340;&#26368;&#20248;Hessian&#30456;&#20851;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#20174;&#20449;&#24687;&#35770;&#35282;&#24230;&#25552;&#20379;Hessian&#30456;&#20851;&#22797;&#26434;&#24230;&#30340;&#19979;&#30028;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;Hessian&#26080;&#20851;&#30340;&#31639;&#27861;&#21487;&#26222;&#36941;&#23454;&#29616;&#25152;&#26377;Hessian&#23454;&#20363;&#30340;&#28176;&#36817;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.12383</link><description>&lt;p&gt;
&#20108;&#27425;&#22411;&#36172;&#33218;&#26426;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65306;Hessian&#30456;&#20851;&#24615;&#30028;&#38480;&#21644;&#26368;&#20248;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sample Complexity for Quadratic Bandits: Hessian Dependent Bounds and Optimal Algorithms. (arXiv:2306.12383v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#38543;&#26426;&#38646;&#38454;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20108;&#27425;&#22411;&#30446;&#26631;&#20989;&#25968;&#21450;&#20854;&#23616;&#37096;&#20960;&#20309;&#32467;&#26500;&#30340;&#26368;&#20248;Hessian&#30456;&#20851;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#20174;&#20449;&#24687;&#35770;&#35282;&#24230;&#25552;&#20379;Hessian&#30456;&#20851;&#22797;&#26434;&#24230;&#30340;&#19979;&#30028;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;Hessian&#26080;&#20851;&#30340;&#31639;&#27861;&#21487;&#26222;&#36941;&#23454;&#29616;&#25152;&#26377;Hessian&#23454;&#20363;&#30340;&#28176;&#36817;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38543;&#26426;&#38646;&#38454;&#20248;&#21270;&#20013;&#65292;&#20102;&#35299;&#22914;&#20309;&#20805;&#20998;&#21033;&#29992;&#24213;&#23618;&#30446;&#26631;&#20989;&#25968;&#30340;&#23616;&#37096;&#20960;&#20309;&#32467;&#26500;&#26159;&#19968;&#20010;&#23454;&#38469;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#31181;&#22522;&#26412;&#24773;&#20917;&#65292;&#21363;&#30446;&#26631;&#20989;&#25968;&#26159;&#20108;&#27425;&#22411;&#30340;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#26368;&#20248;Hessian&#30456;&#20851;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#31532;&#19968;&#20010;&#32039;&#23494;&#21051;&#30011;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#20855;&#26377;&#21452;&#37325;&#24615;&#36136;&#12290;&#39318;&#20808;&#65292;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#31216;&#20026;&#33021;&#37327;&#20998;&#37197;&#30340;&#27010;&#24565;&#26469;&#25429;&#25417;&#25628;&#32034;&#31639;&#27861;&#21644;&#30446;&#26631;&#20989;&#25968;&#20960;&#20309;&#32467;&#26500;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#35777;&#26126;&#20102;Hessian&#30456;&#20851;&#22797;&#26434;&#24230;&#30340;&#32039;&#23494;&#19979;&#30028;&#12290;&#36890;&#36807;&#35299;&#20915;&#26368;&#20248;&#33021;&#37327;&#35889;&#65292;&#24471;&#21040;&#20102;&#37197;&#22871;&#30340;&#19978;&#38480;&#12290;&#20854;&#27425;&#65292;&#31639;&#27861;&#26041;&#38754;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23384;&#22312;&#19968;&#31181;Hessian&#26080;&#20851;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#26222;&#36941;&#23454;&#29616;&#25152;&#26377;Hessian&#23454;&#20363;&#30340;&#28176;&#36817;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#31639;&#27861;&#33021;&#22815;&#23454;&#29616;&#30340;&#28176;&#36817;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#23545;&#20110;&#37325;&#23614;&#22122;&#22768;&#20998;&#24067;&#20173;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
In stochastic zeroth-order optimization, a problem of practical relevance is understanding how to fully exploit the local geometry of the underlying objective function. We consider a fundamental setting in which the objective function is quadratic, and provide the first tight characterization of the optimal Hessian-dependent sample complexity. Our contribution is twofold. First, from an information-theoretic point of view, we prove tight lower bounds on Hessian-dependent complexities by introducing a concept called energy allocation, which captures the interaction between the searching algorithm and the geometry of objective functions. A matching upper bound is obtained by solving the optimal energy spectrum. Then, algorithmically, we show the existence of a Hessian-independent algorithm that universally achieves the asymptotic optimal sample complexities for all Hessian instances. The optimal sample complexities achieved by our algorithm remain valid for heavy-tailed noise distributio
&lt;/p&gt;</description></item><item><title>OpenGSL&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35299;&#20915;GSL&#39046;&#22495;&#20013;&#30001;&#20110;&#23454;&#39564;&#21327;&#35758;&#19981;&#19968;&#33268;&#32780;&#23548;&#33268;&#30340;&#36827;&#23637;&#19981;&#26126;&#30830;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.10280</link><description>&lt;p&gt;
OpenGSL: &#19968;&#39033;&#38024;&#23545;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
OpenGSL: A Comprehensive Benchmark for Graph Structure Learning. (arXiv:2306.10280v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10280
&lt;/p&gt;
&lt;p&gt;
OpenGSL&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35299;&#20915;GSL&#39046;&#22495;&#20013;&#30001;&#20110;&#23454;&#39564;&#21327;&#35758;&#19981;&#19968;&#33268;&#32780;&#23548;&#33268;&#30340;&#36827;&#23637;&#19981;&#26126;&#30830;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#24050;&#25104;&#20026;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#20107;&#23454;&#26631;&#20934;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#26377;&#25928;&#22320;&#25972;&#21512;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#21644;&#33410;&#28857;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22270;&#30340;&#22797;&#26434;&#21644;&#20381;&#36182;&#24418;&#25104;&#36807;&#31243;&#23548;&#33268;&#30340;&#33410;&#28857;&#36830;&#25509;&#30340;&#22266;&#26377;&#27425;&#20248;&#24615;&#36136;&#65292;&#22312;&#23545;&#20854;&#36827;&#34892;&#24314;&#27169;&#26041;&#38754;&#23384;&#22312;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36817;&#24180;&#26469;&#22270;&#32467;&#26500;&#23398;&#20064;(GSL)&#20316;&#20026;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#21270;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;GSL&#30340;&#26680;&#24515;&#27010;&#24565;&#26159;&#21516;&#26102;&#20248;&#21270;&#22270;&#32467;&#26500;&#21644;&#23545;&#24212;&#30340;GNN&#27169;&#22411;&#12290;&#23613;&#31649;&#25552;&#20986;&#20102;&#35768;&#22810;GSL&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#23454;&#39564;&#21327;&#35758;&#19981;&#19968;&#33268;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#30340;&#21464;&#21270;&#12289;&#25968;&#25454;&#22788;&#29702;&#25216;&#26415;&#21644;&#20998;&#21106;&#31574;&#30053;&#30340;&#24046;&#24322;&#65292;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;OpenGSL&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;GSL&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;OpenGSL&#25552;&#20379;&#20102;&#19968;&#20010;&#20844;&#24179;&#30340;&#27604;&#36739;&#24179;&#21488;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#22312;&#32479;&#19968;&#30340;&#35774;&#32622;&#19979;&#35780;&#20272;&#19981;&#21516;&#30340;GSL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have emerged as the de facto standard for representation learning on graphs, owing to their ability to effectively integrate graph topology and node attributes. However, the inherent suboptimal nature of node connections, resulting from the complex and contingent formation process of graphs, presents significant challenges in modeling them effectively. To tackle this issue, Graph Structure Learning (GSL), a family of data-centric learning approaches, has garnered substantial attention in recent years. The core concept behind GSL is to jointly optimize the graph structure and the corresponding GNN models. Despite the proposal of numerous GSL methods, the progress in this field remains unclear due to inconsistent experimental protocols, including variations in datasets, data processing techniques, and splitting strategies. In this paper, we introduce OpenGSL, the first comprehensive benchmark for GSL, aimed at addressing this gap. OpenGSL enables a fair compa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#21442;&#32771;&#22270;&#20687;&#24182;&#25200;&#21160;&#35745;&#31639;&#29615;&#22659;&#65292;&#21487;&#30830;&#23450;&#27169;&#22411;&#24615;&#33021;&#26159;&#21542;&#21463;&#21040;&#35745;&#31639;&#29615;&#22659;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.06208</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#24046;&#20998;&#27979;&#35797;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Differential Testing Framework to Evaluate Image Recognition Model Robustness. (arXiv:2306.06208v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#21442;&#32771;&#22270;&#20687;&#24182;&#25200;&#21160;&#35745;&#31639;&#29615;&#22659;&#65292;&#21487;&#30830;&#23450;&#27169;&#22411;&#24615;&#33021;&#26159;&#21542;&#21463;&#21040;&#35745;&#31639;&#29615;&#22659;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#36890;&#24120;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#65292;&#24182;&#38656;&#35201;&#24040;&#22823;&#30340;&#22788;&#29702;&#33021;&#21147;&#65292;&#22240;&#27492;&#20381;&#36182;&#20110;GPU&#21644;TPU&#31561;&#30828;&#20214;&#21152;&#36895;&#22120;&#36827;&#34892;&#24555;&#36895;&#12289;&#21450;&#26102;&#30340;&#22788;&#29702;&#12290;&#22312;&#27169;&#22411;&#37096;&#32626;&#36807;&#31243;&#20013;&#65292;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#30340;&#23376;&#20248;&#26144;&#23556;&#21487;&#33021;&#20250;&#23548;&#33268;&#23454;&#26102;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#22833;&#36133;&#65292;&#20174;&#32780;&#23548;&#33268;&#26102;&#38388;&#19981;&#30830;&#23450;&#24615;&#21644;&#38169;&#35823;&#34892;&#20026;&#12290;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#30340;&#26144;&#23556;&#26159;&#36890;&#36807;&#22810;&#20010;&#36719;&#20214;&#32452;&#20214;&#36827;&#34892;&#30340;&#65292;&#20363;&#22914;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12289;&#32534;&#35793;&#22120;&#12289;&#35774;&#22791;&#24211;&#31561;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#35745;&#31639;&#29615;&#22659;&#12290;&#38543;&#30528;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#21307;&#30103;&#25104;&#20687;&#31561;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#22686;&#21152;&#65292;&#35780;&#20272;&#23427;&#20204;&#23545;&#35745;&#31639;&#29615;&#22659;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12289;&#32534;&#35793;&#22120;&#20248;&#21270;&#21644;&#30828;&#20214;&#35774;&#22791;&#31561;&#21442;&#25968;&#23545;&#27169;&#22411;&#24615;&#33021;&#21644;&#27491;&#30830;&#24615;&#30340;&#24433;&#21709;&#36824;&#19981;&#22826;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#23545;&#35745;&#31639;&#29615;&#22659;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#19968;&#32452;&#21442;&#32771;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#26356;&#25913;&#36719;&#20214;&#32452;&#20214;&#26469;&#25200;&#21160;&#35745;&#31639;&#29615;&#22659;&#65292;&#29983;&#25104;&#20855;&#26377;&#24050;&#30693;&#39044;&#27979;&#36755;&#20986;&#24046;&#24322;&#30340;&#22270;&#20687;&#12290;&#36890;&#36807;&#27604;&#36739;&#21407;&#22987;&#22270;&#20687;&#21644;&#25200;&#21160;&#22270;&#20687;&#30340;&#39044;&#27979;&#36755;&#20986;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#27169;&#22411;&#24615;&#33021;&#26159;&#21542;&#21463;&#21040;&#35745;&#31639;&#29615;&#22659;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#27979;&#35797;&#19977;&#20010;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#26469;&#35777;&#26126;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#30830;&#23450;&#20854;&#22312;&#35745;&#31639;&#29615;&#22659;&#21464;&#21270;&#19979;&#30340;&#24615;&#33021;&#21463;&#21040;&#24433;&#21709;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image recognition tasks typically use deep learning and require enormous processing power, thus relying on hardware accelerators like GPUs and TPUs for fast, timely processing. Failure in real-time image recognition tasks can occur due to sub-optimal mapping on hardware accelerators during model deployment, which may lead to timing uncertainty and erroneous behavior. Mapping on hardware accelerators is done through multiple software components like deep learning frameworks, compilers, device libraries, that we refer to as the computational environment. Owing to the increased use of image recognition tasks in safety-critical applications like autonomous driving and medical imaging, it is imperative to assess their robustness to changes in the computational environment, as the impact of parameters like deep learning frameworks, compiler optimizations, and hardware devices on model performance and correctness is not well understood.  In this paper we present a differential testing framewo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#36716;&#25442;&#20013;&#20986;&#29616;&#30340;&#27169;&#22411;&#23849;&#28291;&#21644;&#36755;&#20986;&#26631;&#31614;&#24046;&#24322;&#30340;&#25925;&#38556;&#23450;&#20301;&#21644;&#20462;&#22797;&#26041;&#27861;&#65292;&#25104;&#21151;&#20462;&#22797;&#22810;&#20010;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#36328;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#36716;&#25442;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2306.06157</link><description>&lt;p&gt;
&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#26694;&#26550;&#36716;&#25442;&#30340;&#25925;&#38556;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Fault Localization for Framework Conversions of Image Recognition Models. (arXiv:2306.06157v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#36716;&#25442;&#20013;&#20986;&#29616;&#30340;&#27169;&#22411;&#23849;&#28291;&#21644;&#36755;&#20986;&#26631;&#31614;&#24046;&#24322;&#30340;&#25925;&#38556;&#23450;&#20301;&#21644;&#20462;&#22797;&#26041;&#27861;&#65292;&#25104;&#21151;&#20462;&#22797;&#22810;&#20010;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#36328;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#36716;&#25442;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37096;&#32626;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#26102;&#65292;&#24320;&#21457;&#20154;&#21592;&#32463;&#24120;&#23558;&#27169;&#22411;&#20174;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#36716;&#25442;&#20026;&#21478;&#19968;&#20010;&#65288;&#20363;&#22914;&#65292;&#20174;TensorFlow&#21040;PyTorch&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#23481;&#26131;&#20986;&#38169;&#65292;&#24182;&#21487;&#33021;&#24433;&#21709;&#30446;&#26631;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#30830;&#23450;&#36825;&#31181;&#24433;&#21709;&#30340;&#31243;&#24230;&#65292;&#25105;&#20204;&#23545;&#19977;&#20010;&#29992;&#20110;&#22270;&#20687;&#35782;&#21035;&#30340;DNNs&#65288;MobileNetV2&#12289;ResNet101&#21644;InceptionV3&#65289;&#36827;&#34892;&#20102;&#19981;&#21516;&#30340;&#20998;&#26512;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22235;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65288;PyTorch&#12289;Keras&#12289;TensorFlow&#65288;TF&#65289;&#21644;TFLite&#65289;&#20043;&#38388;&#36827;&#34892;&#20102;&#36716;&#25442;&#65292;&#24182;&#21457;&#29616;&#20102;&#35768;&#22810;&#27169;&#22411;&#23849;&#28291;&#21644;&#36755;&#20986;&#26631;&#31614;&#24046;&#24322;&#39640;&#36798;100&#65285;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#38169;&#35823;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23450;&#20301;&#25925;&#38556;&#21644;&#20462;&#22797;&#26377;&#32570;&#38519;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#36716;&#25442;&#65292;&#37325;&#28857;&#25918;&#22312;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#19978;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#21253;&#25324;&#22235;&#20010;&#20027;&#35201;&#20998;&#26512;&#38454;&#27573;&#65306;1&#65289;&#36716;&#25442;&#24037;&#20855;&#65292;2&#65289;&#27169;&#22411;&#21442;&#25968;&#65292;3&#65289;&#27169;&#22411;&#36229;&#21442;&#25968;&#65292;4&#65289;&#22270;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35768;&#22810;&#38024;&#23545;&#25925;&#38556;&#23450;&#20301;&#21644;&#20462;&#22797;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324;&#36716;&#25442;&#24037;&#20855;&#30340;&#25512;&#33616;&#12289;&#35843;&#35797;&#25216;&#24039;&#20197;&#21450;&#27169;&#22411;&#36229;&#21442;&#25968;&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;&#36890;&#36807;&#25104;&#21151;&#20462;&#22797;&#25152;&#26377;&#27979;&#35797;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;MobileNetV2&#65292;ResNet101&#21644;InceptionV3 &#30340;&#26377;&#32570;&#38519;&#30340;&#36716;&#25442;&#26469;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
When deploying Deep Neural Networks (DNNs), developers often convert models from one deep learning framework to another (e.g., TensorFlow to PyTorch). However, this process is error-prone and can impact target model accuracy. To identify the extent of such impact, we perform and briefly present a differential analysis against three DNNs used for image recognition (MobileNetV2, ResNet101, and InceptionV3), converted across four well-known deep learning frameworks (PyTorch, Keras, TensorFlow (TF), and TFLite), which revealed numerous model crashes and output label discrepancies of up to 100%. To mitigate such errors, we present a novel approach towards fault localization and repair of buggy deep learning framework conversions, focusing on pre-trained image recognition models. Our technique consists of four primary stages of analysis: 1) conversion tools, 2) model parameters, 3) model hyperparameters, and 4) graph representation. In addition, we propose a number of strategies towards faul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27979;&#37327;&#20102;Barron&#31354;&#38388;&#21644;&#35889;Barron&#31354;&#38388;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20379;&#20102;&#23884;&#20837;&#19981;&#31561;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.19082</link><description>&lt;p&gt;
Barron&#22411;&#31354;&#38388;&#30340;&#23884;&#20837;&#19981;&#31561;&#24335;
&lt;/p&gt;
&lt;p&gt;
Embedding Inequalities for Barron-type Spaces. (arXiv:2305.19082v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27979;&#37327;&#20102;Barron&#31354;&#38388;&#21644;&#35889;Barron&#31354;&#38388;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20379;&#20102;&#23884;&#20837;&#19981;&#31561;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#29702;&#35299;&#39640;&#32500;&#26465;&#20214;&#19979;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#21644;&#27867;&#21270;&#24615;&#36136;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#24341;&#20837;&#20102;Barron&#31354;&#38388;$\mathcal{B}_s(\Omega)$&#21644;&#35889;Barron&#31354;&#38388;$\mathcal{F}_s(\Omega)$&#65292;&#20854;&#20013;&#25351;&#25968;$s$&#34920;&#24449;&#20102;&#36825;&#20123;&#31354;&#38388;&#20013;&#20989;&#25968;&#30340;&#24179;&#28369;&#24615;&#65292;$\Omega\subset\mathbb{R}^d$&#34920;&#31034;&#36755;&#20837;&#22495;&#12290;&#28982;&#32780;&#65292;&#20004;&#31181;&#31867;&#22411;&#30340;Barron&#31354;&#38388;&#20043;&#38388;&#30340;&#20851;&#31995;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;&#20197;&#19979;&#19981;&#31561;&#24335;&#24314;&#31435;&#20102;&#36825;&#20123;&#31354;&#38388;&#20043;&#38388;&#30340;&#36830;&#32493;&#23884;&#20837;&#65306;&#23545;&#20110;&#20219;&#24847;$\delta\in(0,1),s\in\mathbb{N}^{+}$&#21644;$f:\Omega \mapsto \mathbb{R}$&#65292;&#37117;&#26377;\[ \delta\gamma^{\delta-s}_{\Omega}\|f\|_{\mathcal{F}_{s-\delta}(\Omega)}\lesssim_s \|f\|_{\mathcal{B}_s(\Omega)}\lesssim_s \|f\|_{\mathcal{F}_{s+1}(\Omega)}, \]&#20854;&#20013;$\gamma_{\Omega}=\sup_{\|v\|_2=1,x\in\Omega}|v^Tx|$&#65292;$\lesssim_s$&#34920;&#31034;&#20165;&#19982;&#24179;&#28369;&#21442;&#25968;$s$&#26377;&#20851;&#30340;&#24120;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the fundamental problems in deep learning theory is understanding the approximation and generalization properties of two-layer neural networks in high dimensions. In order to tackle this issue, researchers have introduced the Barron space $\mathcal{B}_s(\Omega)$ and the spectral Barron space $\mathcal{F}_s(\Omega)$, where the index $s$ characterizes the smoothness of functions within these spaces and $\Omega\subset\mathbb{R}^d$ represents the input domain. However, it is still not clear what is the relationship between the two types of Barron spaces. In this paper, we establish continuous embeddings between these spaces as implied by the following inequality: for any $\delta\in (0,1), s\in \mathbb{N}^{+}$ and $f: \Omega \mapsto\mathbb{R}$, it holds that \[ \delta\gamma^{\delta-s}_{\Omega}\|f\|_{\mathcal{F}_{s-\delta}(\Omega)}\lesssim_s \|f\|_{\mathcal{B}_s(\Omega)}\lesssim_s \|f\|_{\mathcal{F}_{s+1}(\Omega)}, \] where $\gamma_{\Omega}=\sup_{\|v\|_2=1,x\in\Omega}|v^Tx|$ and notab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;RLCF&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20195;&#30721;&#32534;&#35793;&#22120;&#21453;&#39304;&#36827;&#19968;&#27493;&#35757;&#32451;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#29983;&#25104;&#31526;&#21512;&#30446;&#26631;&#20998;&#24067;&#30340;&#20195;&#30721;&#65292;&#24182;&#36890;&#36807;&#25152;&#26377;&#38745;&#24577;&#27491;&#30830;&#24615;&#26816;&#26597;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18341</link><description>&lt;p&gt;
&#20351;&#29992;&#32534;&#35793;&#22120;&#29983;&#25104;&#30340;&#24378;&#21270;&#23398;&#20064;&#21453;&#39304;&#35843;&#25972;&#20195;&#30721;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tuning Models of Code with Compiler-Generated Reinforcement Learning Feedback. (arXiv:2305.18341v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;RLCF&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20195;&#30721;&#32534;&#35793;&#22120;&#21453;&#39304;&#36827;&#19968;&#27493;&#35757;&#32451;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#29983;&#25104;&#31526;&#21512;&#30446;&#26631;&#20998;&#24067;&#30340;&#20195;&#30721;&#65292;&#24182;&#36890;&#36807;&#25152;&#26377;&#38745;&#24577;&#27491;&#30830;&#24615;&#26816;&#26597;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20195;&#30721;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#31243;&#24207;&#21512;&#25104;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#21487;&#33021;&#36829;&#21453;&#22522;&#26412;&#30340;&#35821;&#35328;&#32423;&#21035;&#19981;&#21464;&#24615;&#65292;&#20174;&#32780;&#38477;&#20302;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;RLCF&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#20195;&#30721;&#32534;&#35793;&#22120;&#30340;&#21453;&#39304;&#36827;&#19968;&#27493;&#35757;&#32451;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290; RLCF&#23558;LLM&#35270;&#20026;&#36890;&#36807;RL&#20195;&#29702;&#36880;&#27493;&#29983;&#25104;&#20195;&#30721;&#65292;&#24182;&#25509;&#25910;&#20197;&#19979;&#21453;&#39304;&#65306;&#65288;i&#65289;&#32534;&#35793;&#22120;&#27966;&#29983;&#30340;&#21453;&#39304;&#19982;&#25152;&#29983;&#25104;&#30340;&#20195;&#30721;&#26159;&#21542;&#36890;&#36807;&#19968;&#32452;&#27491;&#30830;&#24615;&#26816;&#26597;&#26377;&#20851;; &#65288;ii&#65289;&#19981;&#21516;LLM&#30340;&#21453;&#39304;&#65292;&#19982;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#19968;&#32452;&#21442;&#32771;&#31243;&#24207;&#30456;&#20284;&#12290;&#36825;&#20123;&#21453;&#39304;&#26426;&#21046;&#24110;&#21161;&#25152;&#29983;&#25104;&#30340;&#20195;&#30721;&#22312;&#36890;&#36807;&#25152;&#26377;&#38745;&#24577;&#27491;&#30830;&#24615;&#26816;&#26597;&#30340;&#21516;&#26102;&#20445;&#25345;&#22312;&#30446;&#26631;&#20998;&#24067;&#20013;&#12290;RLCF&#26159;&#27169;&#22411;&#21644;&#35821;&#35328;&#26080;&#20851;&#30340;&#12290;&#25105;&#20204;&#22312;Java&#30340;MBJP&#21644;MathQA&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RLCF&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) pre-trained on code have recently emerged as the dominant approach to program synthesis. However, the code that these models produce can violate basic language-level invariants, leading to lower performance in downstream tasks. We address this issue through an approach, called RLCF, that further trains a pre-trained LLM using feedback from a code compiler. RLCF views the LLM as an RL agent that generates code step by step and receives: (i) compiler-derived feedback on whether the code it generates passes a set of correctness checks; and (ii) feedback from a different LLM on whether the generated code is similar to a set of reference programs in the training corpus. Together, these feedback mechanisms help the generated code remain within the target distribution while passing all static correctness checks. RLCF is model- and language-agnostic. We empirically evaluate it on the MBJP and MathQA tasks for Java. Our experiments show that RLCF significantly raise
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#29702;&#35770;&#23618;&#38754;&#25506;&#31350;&#20102;&#24102;&#26377;&#8220;&#24605;&#32500;&#38142;&#8221;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#22522;&#26412;&#25968;&#23398;&#21644;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#22238;&#24402;Transformer&#22823;&#23567;&#24658;&#23450;&#21363;&#21487;&#35299;&#20915;&#20219;&#21153;&#65292;&#25581;&#31034;&#20102;&#8220;&#24605;&#32500;&#38142;&#8221;&#25552;&#31034;&#30340;&#32972;&#21518;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.15408</link><description>&lt;p&gt;
&#20174;&#29702;&#35770;&#35282;&#24230;&#25581;&#31034;&#8220;&#24605;&#32500;&#38142;&#8221;&#32972;&#21518;&#30340;&#22885;&#31192;
&lt;/p&gt;
&lt;p&gt;
Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective. (arXiv:2305.15408v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#29702;&#35770;&#23618;&#38754;&#25506;&#31350;&#20102;&#24102;&#26377;&#8220;&#24605;&#32500;&#38142;&#8221;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#22522;&#26412;&#25968;&#23398;&#21644;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#22238;&#24402;Transformer&#22823;&#23567;&#24658;&#23450;&#21363;&#21487;&#35299;&#20915;&#20219;&#21153;&#65292;&#25581;&#31034;&#20102;&#8220;&#24605;&#32500;&#38142;&#8221;&#25552;&#31034;&#30340;&#32972;&#21518;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;"&#24605;&#32500;&#38142;"&#25552;&#31034;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#25968;&#23398;&#25110;&#25512;&#29702;&#30340;&#22797;&#26434;&#20219;&#21153;&#20013;&#12290;&#23613;&#31649;&#33719;&#24471;&#20102;&#24040;&#22823;&#30340;&#23454;&#35777;&#25104;&#21151;&#65292;&#20294;&#8220;&#24605;&#32500;&#38142;&#8221;&#32972;&#21518;&#30340;&#26426;&#21046;&#20197;&#21450;&#23427;&#22914;&#20309;&#37322;&#25918;LLMs&#30340;&#28508;&#21147;&#20173;&#28982;&#26159;&#31070;&#31192;&#30340;&#12290;&#26412;&#25991;&#39318;&#27425;&#20174;&#29702;&#35770;&#19978;&#22238;&#31572;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#24102;&#26377;&#8220;&#24605;&#32500;&#38142;&#8221;&#22312;&#35299;&#20915;&#22522;&#26412;&#25968;&#23398;&#21644;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#32473;&#20986;&#19968;&#20010;&#19981;&#21487;&#33021;&#30340;&#32467;&#26524;&#65292;&#34920;&#26126;&#20219;&#20309;&#26377;&#38480;&#28145;&#24230;&#30340;Transformer&#37117;&#19981;&#33021;&#30452;&#25509;&#36755;&#20986;&#27491;&#30830;&#30340;&#22522;&#26412;&#31639;&#26415;/&#26041;&#31243;&#20219;&#21153;&#30340;&#31572;&#26696;&#65292;&#38500;&#38750;&#27169;&#22411;&#22823;&#23567;&#38543;&#30528;&#36755;&#20837;&#38271;&#24230;&#30340;&#22686;&#21152;&#21576;&#36229;&#22810;&#39033;&#24335;&#22686;&#38271;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#36890;&#36807;&#26500;&#36896;&#35777;&#26126;&#65292;&#22823;&#23567;&#24658;&#23450;&#30340;&#33258;&#22238;&#24402;Transformer&#36275;&#20197;&#36890;&#36807;&#20351;&#29992;&#24120;&#29992;&#30340;&#25968;&#23398;&#35821;&#35328;&#24418;&#24335;&#29983;&#25104;&#8220;&#24605;&#32500;&#38142;&#8221;&#25512;&#23548;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have discovered that Chain-of-Thought prompting (CoT) can dramatically improve the performance of Large Language Models (LLMs), particularly when dealing with complex tasks involving mathematics or reasoning. Despite the enormous empirical success, the underlying mechanisms behind CoT and how it unlocks the potential of LLMs remain elusive. In this paper, we take a first step towards theoretically answering these questions. Specifically, we examine the capacity of LLMs with CoT in solving fundamental mathematical and decision-making problems. We start by giving an impossibility result showing that any bounded-depth Transformer cannot directly output correct answers for basic arithmetic/equation tasks unless the model size grows super-polynomially with respect to the input length. In contrast, we then prove by construction that autoregressive Transformers of a constant size suffice to solve both tasks by generating CoT derivations using a commonly-used math language forma
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#23398;&#20064;&#29575;&#26080;&#20851;&#30340;&#32422;&#26463;&#22495;&#37319;&#26679;&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#33021;&#22815;&#22788;&#29702;&#22810;&#31181;&#32422;&#26463;&#37319;&#26679;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#19982;&#29616;&#26377;&#31639;&#27861;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14943</link><description>&lt;p&gt;
&#23398;&#20064;&#29575;&#26080;&#20851;&#30340;&#32422;&#26463;&#22495;Bayesian&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Learning Rate Free Bayesian Inference in Constrained Domains. (arXiv:2305.14943v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14943
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#23398;&#20064;&#29575;&#26080;&#20851;&#30340;&#32422;&#26463;&#22495;&#37319;&#26679;&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#33021;&#22815;&#22788;&#29702;&#22810;&#31181;&#32422;&#26463;&#37319;&#26679;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#19982;&#29616;&#26377;&#31639;&#27861;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#22871;&#26032;&#30340;&#22522;&#20110;&#31890;&#23376;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32422;&#26463;&#22495;&#20869;&#36827;&#34892;&#37319;&#26679;&#65292;&#36825;&#26159;&#23436;&#20840;&#19982;&#23398;&#20064;&#29575;&#26080;&#20851;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#20984;&#20248;&#21270;&#20013;&#30340;&#30828;&#24065;&#25237;&#27880;&#24605;&#24819;&#65292;&#20197;&#21450;&#32422;&#26463;&#37319;&#26679;&#20316;&#20026;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#19978;&#38236;&#20687;&#20248;&#21270;&#38382;&#39064;&#30340;&#35266;&#28857;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#28857;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#20960;&#31181;&#29616;&#26377;&#30340;&#32422;&#26463;&#37319;&#26679;&#31639;&#27861;&#65292;&#21253;&#25324;&#38236;&#20687;Langevin&#21160;&#21147;&#23398;&#21644;&#38236;&#20687;Stein&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#30340;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#20174;&#21333;&#32431;&#24418;&#30446;&#26631;&#36827;&#34892;&#37319;&#26679;&#12289;&#24102;&#20844;&#24179;&#24615;&#32422;&#26463;&#36827;&#34892;&#37319;&#26679;&#20197;&#21450;&#21518;&#36873;&#25321;&#25512;&#26029;&#20013;&#30340;&#32422;&#26463;&#37319;&#26679;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#19981;&#38656;&#35201;&#35843;&#25972;&#20219;&#20309;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#19982;&#29616;&#26377;&#32422;&#26463;&#37319;&#26679;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a suite of new particle-based algorithms for sampling on constrained domains which are entirely learning rate free. Our approach leverages coin betting ideas from convex optimisation, and the viewpoint of constrained sampling as a mirrored optimisation problem on the space of probability measures. Based on this viewpoint, we also introduce a unifying framework for several existing constrained sampling algorithms, including mirrored Langevin dynamics and mirrored Stein variational gradient descent. We demonstrate the performance of our algorithms on a range of numerical examples, including sampling from targets on the simplex, sampling with fairness constraints, and constrained sampling problems in post-selection inference. Our results indicate that our algorithms achieve competitive performance with existing constrained sampling methods, without the need to tune any hyperparameters.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#29616;&#26377;&#30340;&#22810;&#22836;&#35821;&#35328;&#27169;&#22411;&#26816;&#26597;&#28857;&#21319;&#32423;&#20026;&#20855;&#26377;&#22810;&#26597;&#35810;&#27880;&#24847;&#21147;&#65288;MQA&#65289;&#30340;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#32676;&#32452;&#26597;&#35810;&#27880;&#24847;&#21147;&#65288;GQA&#65289;&#26469;&#35299;&#20915;MQA&#21487;&#33021;&#23548;&#33268;&#30340;&#36136;&#37327;&#19979;&#38477;&#38382;&#39064;&#12290;&#36890;&#36807;&#21319;&#32423;&#21518;&#30340;GQA&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#36136;&#37327;&#65292;&#24182;&#20855;&#22791;&#19982;MQA&#30456;&#24403;&#30340;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.13245</link><description>&lt;p&gt;
GQA:&#20174;&#22810;&#22836;&#26816;&#26597;&#28857;&#35757;&#32451;&#24191;&#20041;&#22810;&#26597;&#35810;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints. (arXiv:2305.13245v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13245
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#29616;&#26377;&#30340;&#22810;&#22836;&#35821;&#35328;&#27169;&#22411;&#26816;&#26597;&#28857;&#21319;&#32423;&#20026;&#20855;&#26377;&#22810;&#26597;&#35810;&#27880;&#24847;&#21147;&#65288;MQA&#65289;&#30340;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#32676;&#32452;&#26597;&#35810;&#27880;&#24847;&#21147;&#65288;GQA&#65289;&#26469;&#35299;&#20915;MQA&#21487;&#33021;&#23548;&#33268;&#30340;&#36136;&#37327;&#19979;&#38477;&#38382;&#39064;&#12290;&#36890;&#36807;&#21319;&#32423;&#21518;&#30340;GQA&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#36136;&#37327;&#65292;&#24182;&#20855;&#22791;&#19982;MQA&#30456;&#24403;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26597;&#35810;&#27880;&#24847;&#21147;&#65288;MQA&#65289;&#20165;&#20351;&#29992;&#19968;&#20010;&#38190;&#20540;&#22836;&#65292;&#22823;&#22823;&#21152;&#24555;&#20102;&#35299;&#30721;&#22120;&#25512;&#29702;&#36895;&#24230;&#12290;&#28982;&#32780;&#65292;MQA&#21487;&#33021;&#23548;&#33268;&#36136;&#37327;&#19979;&#38477;&#65292;&#24182;&#19988;&#20026;&#20102;&#26356;&#24555;&#22320;&#25512;&#29702;&#32780;&#35757;&#32451;&#19968;&#20010;&#21333;&#29420;&#30340;&#27169;&#22411;&#21487;&#33021;&#19981;&#26159;&#29702;&#24819;&#30340;&#12290;&#25105;&#20204;&#65288;1&#65289;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#27861;&#65292;&#21033;&#29992;&#21407;&#22987;&#39044;&#35757;&#32451;&#35745;&#31639;&#37327;&#30340;5&#65285;&#65292;&#23558;&#29616;&#26377;&#30340;&#22810;&#22836;&#35821;&#35328;&#27169;&#22411;&#26816;&#26597;&#28857;&#21319;&#32423;&#20026;&#20855;&#26377;MQA&#30340;&#27169;&#22411;&#65292;&#24182;&#65288;2&#65289;&#24341;&#20837;&#20102;&#32676;&#32452;&#26597;&#35810;&#27880;&#24847;&#21147;&#65288;GQA&#65289;&#65292;&#23427;&#26159;&#22810;&#26597;&#35810;&#27880;&#24847;&#21147;&#30340;&#24191;&#20041;&#24418;&#24335;&#65292;&#20351;&#29992;&#20013;&#38388;&#25968;&#37327;&#30340;&#38190;&#20540;&#22836;&#65288;&#22810;&#20110;&#19968;&#20010;&#65292;&#23569;&#20110;&#26597;&#35810;&#22836;&#30340;&#25968;&#37327;&#65289;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#32463;&#36807;&#21319;&#32423;&#30340;GQA&#23454;&#29616;&#20102;&#19982;&#22810;&#22836;&#27880;&#24847;&#21147;&#30456;&#24403;&#30340;&#36895;&#24230;&#65292;&#24182;&#19988;&#20855;&#26377;&#25509;&#36817;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-query attention (MQA), which only uses a single key-value head, drastically speeds up decoder inference. However, MQA can lead to quality degradation, and moreover it may not be desirable to train a separate model just for faster inference. We (1) propose a recipe for uptraining existing multi-head language model checkpoints into models with MQA using 5% of original pre-training compute, and (2) introduce grouped-query attention (GQA), a generalization of multi-query attention which uses an intermediate (more than one, less than number of query heads) number of key-value heads. We show that uptrained GQA achieves quality close to multi-head attention with comparable speed to MQA.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20108;&#20998;&#31867;&#20013;&#20195;&#29702;&#39118;&#38505;&#30340;&#23545;&#25239;&#19968;&#33268;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#20195;&#29702;&#25439;&#22833;&#20989;&#25968;&#38598;&#21512;&#30340;&#29305;&#24449;&#21270;&#65292;&#32467;&#26524;&#34920;&#26126;&#25932;&#23545;&#19968;&#33268;&#20195;&#29702;&#30340;&#31867;&#19982;&#26631;&#20934;&#35774;&#32622;&#30456;&#27604;&#23567;&#24471;&#22810;&#12290;</title><link>http://arxiv.org/abs/2305.09956</link><description>&lt;p&gt;
&#20108;&#20998;&#31867;&#20013;&#20195;&#29702;&#39118;&#38505;&#30340;&#23545;&#25239;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Adversarial Consistency of Surrogate Risks for Binary Classification. (arXiv:2305.09956v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20108;&#20998;&#31867;&#20013;&#20195;&#29702;&#39118;&#38505;&#30340;&#23545;&#25239;&#19968;&#33268;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#20195;&#29702;&#25439;&#22833;&#20989;&#25968;&#38598;&#21512;&#30340;&#29305;&#24449;&#21270;&#65292;&#32467;&#26524;&#34920;&#26126;&#25932;&#23545;&#19968;&#33268;&#20195;&#29702;&#30340;&#31867;&#19982;&#26631;&#20934;&#35774;&#32622;&#30456;&#27604;&#23567;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#29992;&#20110;&#40065;&#26834;&#20108;&#20998;&#31867;&#30340;&#20195;&#29702;&#39118;&#38505;&#30340;&#19968;&#33268;&#24615;&#12290;&#24120;&#35265;&#30340;&#20570;&#27861;&#26159;&#36890;&#36807;&#23545;&#25239;&#24615;&#35757;&#32451;&#26469;&#23398;&#20064;&#40065;&#26834;&#20998;&#31867;&#22120;&#65292;&#35813;&#26041;&#27861;&#35797;&#22270;&#22312;&#27599;&#20010;&#31034;&#20363;&#21487;&#20197;&#22312;&#23567;&#29699;&#20869;&#34987;&#24694;&#24847;&#25439;&#22351;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;&#26399;&#26395;&#30340;$0$-$1$&#25439;&#22833;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#23436;&#25972;&#30340;&#20195;&#29702;&#25439;&#22833;&#20989;&#25968;&#38598;&#30340;&#29305;&#24449;&#21270;&#65292;&#36825;&#20123;&#38598;&#26159;&#8220;&#19968;&#33268;&#8221;&#30340;&#65292;&#21363;&#21487;&#20197;&#26367;&#25442;$0$-$1$&#25439;&#22833;&#32780;&#19981;&#24433;&#21709;&#21407;&#22987;&#23545;&#25239;&#39118;&#38505;&#30340;&#26368;&#23567;&#21270;&#24207;&#21015;&#30340;&#20219;&#20309;&#25968;&#25454;&#20998;&#24067;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#29992;&#20110;$\rho$-margin&#25439;&#22833;&#30340;&#23545;&#25239;&#19968;&#33268;&#24615;&#30340;&#37327;&#21270;&#29256;&#26412;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#26631;&#20934;&#35774;&#32622;&#30456;&#27604;&#65292;&#25932;&#23545;&#19968;&#33268;&#20195;&#29702;&#30340;&#31867;&#26126;&#26174;&#36739;&#23567;&#65292;&#22312;&#26631;&#20934;&#35774;&#32622;&#20013;&#65292;&#35768;&#22810;&#24120;&#35265;&#30340;&#20195;&#29702;&#37117;&#34987;&#35748;&#20026;&#26159;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the consistency of surrogate risks for robust binary classification. It is common to learn robust classifiers by adversarial training, which seeks to minimize the expected $0$-$1$ loss when each example can be maliciously corrupted within a small ball. We give a simple and complete characterization of the set of surrogate loss functions that are \emph{consistent}, i.e., that can replace the $0$-$1$ loss without affecting the minimizing sequences of the original adversarial risk, for any data distribution. We also prove a quantitative version of adversarial consistency for the $\rho$-margin loss. Our results reveal that the class of adversarially consistent surrogates is substantially smaller than in the standard setting, where many common surrogates are known to be consistent.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22870;&#21169;&#20998;&#24067;&#37325;&#23614;&#30340;MAB&#38382;&#39064;&#30340;&#38544;&#24335;&#35268;&#33539;&#21270;&#39044;&#27979;&#22120;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#37325;&#23614;&#38543;&#26426;MAB&#38382;&#39064;&#19978;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.06743</link><description>&lt;p&gt;
&#38024;&#23545;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#37325;&#23614;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#38544;&#24335;&#33539;&#25968;&#39044;&#27979;&#22120;&#30340;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
Implicitly normalized forecaster with clipping for linear and non-linear heavy-tailed multi-armed bandits. (arXiv:2305.06743v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22870;&#21169;&#20998;&#24067;&#37325;&#23614;&#30340;MAB&#38382;&#39064;&#30340;&#38544;&#24335;&#35268;&#33539;&#21270;&#39044;&#27979;&#22120;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#37325;&#23614;&#38543;&#26426;MAB&#38382;&#39064;&#19978;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#30693;&#38544;&#24335;&#33539;&#25968;&#39044;&#27979;&#22120;&#65288;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#65292;&#20197;Tsallis&#29109;&#20316;&#20026;prox&#20989;&#25968;&#65289;&#26159;&#23545;&#25239;&#24615;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#65288;MAB&#65289;&#30340;&#26368;&#20339;&#31639;&#27861;&#12290;&#20294;&#26159;&#65292;&#22823;&#22810;&#25968;&#22797;&#26434;&#24615;&#32467;&#26524;&#37117;&#20381;&#36182;&#20110;&#26377;&#30028;&#22870;&#21169;&#25110;&#20854;&#20182;&#38480;&#21046;&#24615;&#20551;&#35774;&#12290;&#26368;&#36817;&#26377;&#20851;&#26368;&#20339;&#20108;&#32773;&#32467;&#21512;&#31639;&#27861;&#30340;&#30740;&#31350;&#24050;&#32463;&#38024;&#23545;&#23545;&#25163;&#24615;&#21644;&#38543;&#26426;&#37325;&#23614;MAB&#35774;&#32622;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;&#36825;&#20010;&#31639;&#27861;&#22312;&#36825;&#20004;&#31181;&#24773;&#20917;&#19979;&#37117;&#26159;&#26368;&#20248;&#30340;&#65292;&#20294;&#19981;&#33021;&#20805;&#20998;&#21033;&#29992;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#22870;&#21169;&#20998;&#24067;&#37325;&#23614;&#30340;MAB&#38382;&#39064;&#25552;&#20986;&#20102;&#24102;&#21098;&#36753;&#30340;&#38544;&#24335;&#35268;&#33539;&#21270;&#39044;&#27979;&#22120;&#12290;&#25105;&#20204;&#22312;&#22870;&#21169;&#20998;&#24067;&#19978;&#25552;&#20986;&#28176;&#36827;&#25910;&#25947;&#24615;&#32467;&#26524;&#65292;&#24182;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23545;&#20110;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#37325;&#23614;&#38543;&#26426;MAB&#38382;&#39064;&#26159;&#26368;&#20248;&#30340;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#19982;&#26368;&#22909;&#30340;&#20108;&#32773;&#32467;&#21512;&#31639;&#27861;&#30456;&#27604;&#65292;&#35813;&#31639;&#27861;&#36890;&#24120;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicitly Normalized Forecaster (online mirror descent with Tsallis entropy as prox-function) is known to be an optimal algorithm for adversarial multi-armed problems (MAB). However, most of the complexity results rely on bounded rewards or other restrictive assumptions. Recently closely related best-of-both-worlds algorithm were proposed for both adversarial and stochastic heavy-tailed MAB settings. This algorithm is known to be optimal in both settings, but fails to exploit data fully. In this paper, we propose Implicitly Normalized Forecaster with clipping for MAB problems with heavy-tailed distribution on rewards. We derive convergence results under mild assumptions on rewards distribution and show that the proposed method is optimal for both linear and non-linear heavy-tailed stochastic MAB problems. Also we show that algorithm usually performs better compared to best-of-two-worlds algorithm.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#31163;&#25955;&#26102;&#38388;&#31995;&#32479;&#30340;&#31070;&#32463;&#26446;&#38597;&#26222;&#35834;&#22827;&#25511;&#21046;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#26469;&#39564;&#35777;&#31283;&#23450;&#24615;&#26465;&#20214;&#65292;&#35745;&#31639;&#23376;&#27700;&#24179;&#38598;&#21051;&#30011;&#21560;&#24341;&#22495;&#65292;&#26377;&#25928;&#22320;&#23398;&#20064;&#25511;&#21046;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2305.06547</link><description>&lt;p&gt;
&#31163;&#25955;&#31995;&#32479;&#30340;&#31070;&#32463;&#26446;&#38597;&#26222;&#35834;&#22827;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Neural Lyapunov Control for Discrete-Time Systems. (arXiv:2305.06547v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06547
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#31163;&#25955;&#26102;&#38388;&#31995;&#32479;&#30340;&#31070;&#32463;&#26446;&#38597;&#26222;&#35834;&#22827;&#25511;&#21046;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#26469;&#39564;&#35777;&#31283;&#23450;&#24615;&#26465;&#20214;&#65292;&#35745;&#31639;&#23376;&#27700;&#24179;&#38598;&#21051;&#30011;&#21560;&#24341;&#22495;&#65292;&#26377;&#25928;&#22320;&#23398;&#20064;&#25511;&#21046;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#32447;&#24615;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#24050;&#32463;&#34987;&#20805;&#20998;&#20102;&#35299;&#65292;&#20294;&#23545;&#20110;&#20855;&#26377;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#31995;&#32479;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#19968;&#33324;&#26041;&#27861;&#26159;&#21033;&#29992;&#26446;&#38597;&#26222;&#35834;&#22827;&#31283;&#23450;&#24615;&#29702;&#35770;&#26469;&#35745;&#31639;&#26446;&#38597;&#26222;&#35834;&#22827;&#25511;&#21046;&#20989;&#25968;&#21644;&#30456;&#20851;&#25511;&#21046;&#31574;&#30053;&#30340;&#32452;&#21512;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19968;&#33324;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#23547;&#25214;&#26446;&#38597;&#26222;&#35834;&#22827;&#20989;&#25968;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#20960;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#26446;&#38597;&#26222;&#35834;&#22827;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20165;&#38024;&#23545;&#36830;&#32493;&#26102;&#38388;&#31995;&#32479;&#35774;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#36866;&#29992;&#20110;&#31163;&#25955;&#26102;&#38388;&#31995;&#32479;&#23398;&#20064;&#31070;&#32463;&#26446;&#38597;&#26222;&#35834;&#22827;&#25511;&#21046;&#30340;&#26041;&#27861;&#12290;&#19977;&#20010;&#20851;&#38190;&#35201;&#32032;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#21487;&#35777;&#26126;&#31283;&#23450;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;&#31532;&#19968;&#20010;&#26159;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#26469;&#39564;&#35777;&#31163;&#25955;&#26102;&#38388;&#31995;&#32479;&#20013;&#30340;&#31283;&#23450;&#24615;&#26465;&#20214;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;&#31532;&#20108;&#20010;&#26159;&#35745;&#31639;&#23376;&#27700;&#24179;&#38598;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21051;&#30011;&#20102;&#21560;&#24341;&#22495;&#30340;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
While ensuring stability for linear systems is well understood, it remains a major challenge for systems with nonlinear dynamics. A general approach in such cases is to leverage Lyapunov stability theory to compute a combination of a Lyapunov control function and an associated control policy. However, finding Lyapunov functions for general nonlinear systems is a challenging task. To address this challenge, several methods have been recently proposed that represent Lyapunov functions using neural networks. However, such approaches have been designed exclusively for continuous-time systems. We propose the first approach for learning neural Lyapunov control in discrete-time systems. Three key ingredients enable us to effectively learn provably stable control policies. The first is a novel mixed-integer linear programming approach for verifying the stability conditions in discrete-time systems. The second is a novel approach for computing sub-level sets which characterize the region of att
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20195;&#29702;&#21464;&#37327;&#26041;&#27861;&#30340;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#23376;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#20013;&#26080;&#38656;&#21442;&#25968;&#32422;&#26463;&#22320;&#35782;&#21035;&#25972;&#20010;&#22240;&#26524;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.05276</link><description>&lt;p&gt;
&#20174;&#23376;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#20013;&#20351;&#29992;&#20195;&#29702;&#21464;&#37327;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Causal Discovery from Subsampled Time Series with Proxy Variables. (arXiv:2305.05276v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20195;&#29702;&#21464;&#37327;&#26041;&#27861;&#30340;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#23376;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#20013;&#26080;&#38656;&#21442;&#25968;&#32422;&#26463;&#22320;&#35782;&#21035;&#25972;&#20010;&#22240;&#26524;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25512;&#26029;&#22240;&#26524;&#32467;&#26500;&#26159;&#35768;&#22810;&#31185;&#23398;&#30740;&#31350;&#30340;&#26680;&#24515;&#20852;&#36259;&#12290;&#37319;&#26679;&#39057;&#29575;&#36828;&#20302;&#20110;&#22240;&#26524;&#24433;&#21709;&#39057;&#29575;&#26159;&#27492;&#31867;&#25512;&#26029;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#22522;&#20110;&#27169;&#22411;&#21644;&#38750;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20294;&#26159;&#35201;&#20040;&#23616;&#38480;&#20110;&#32447;&#24615;&#24773;&#20917;&#65292;&#35201;&#20040;&#26080;&#27861;&#24314;&#31435;&#21487;&#35782;&#21035;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#21442;&#25968;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#20174;&#23376;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#35782;&#21035;&#25972;&#20010;&#22240;&#26524;&#32467;&#26500;&#12290;&#35813;&#26041;&#27861;&#30340;&#24605;&#24819;&#26159;&#65292;&#23376;&#37319;&#26679;&#30340;&#25361;&#25112;&#20027;&#35201;&#26469;&#33258;&#20110;&#8220;&#26410;&#35266;&#23519;&#21040;&#8221;&#30340;&#26102;&#38388;&#27493;&#65292;&#22240;&#27492;&#24212;&#20351;&#29992;&#20026;&#26410;&#35266;&#23519;&#21040;&#21464;&#37327;&#35774;&#35745;&#30340;&#24037;&#20855;&#22788;&#29702;&#27492;&#38382;&#39064;&#12290;&#22312;&#36825;&#20123;&#24037;&#20855;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20195;&#29702;&#21464;&#37327;&#26041;&#27861;&#29305;&#21035;&#36866;&#21512;&#65292;&#22240;&#20026;&#26410;&#35266;&#23519;&#21040;&#21464;&#37327;&#30340;&#20195;&#29702;&#21464;&#37327;&#33258;&#28982;&#26159;&#22312;&#35266;&#23519;&#21040;&#30340;&#26102;&#38388;&#27493;&#19978;&#26412;&#36523;&#12290;&#26681;&#25454;&#36825;&#31181;&#30452;&#35273;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20840;&#38754;&#30340;&#32467;&#26500;&#21487;&#35782;&#21035;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inferring causal structures from time series data is the central interest of many scientific inquiries. A major barrier to such inference is the problem of subsampling, i.e., the frequency of measurements is much lower than that of causal influence. To overcome this problem, numerous model-based and model-free methods have been proposed, yet either limited to the linear case or failed to establish identifiability. In this work, we propose a model-free algorithm that can identify the entire causal structure from subsampled time series, without any parametric constraint. The idea is that the challenge of subsampling arises mainly from \emph{unobserved} time steps and therefore should be handled with tools designed for unobserved variables. Among these tools, we find the proxy variable approach particularly fits, in the sense that the proxy of an unobserved variable is naturally itself at the observed time step. Following this intuition, we establish comprehensive structural identifiabili
&lt;/p&gt;</description></item><item><title>CAMEL&#25552;&#20986;&#20102;&#20351;&#29992;&#23884;&#20837;&#24335;DRAM&#20316;&#20026;&#20027;&#35201;&#23384;&#20648;&#20171;&#36136;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#35774;&#22791;&#31471;&#23398;&#20064;&#20013;&#23384;&#20648;&#21644;&#35745;&#31639;&#36807;&#31243;&#20013;&#21344;&#29992;&#22823;&#37327;&#20869;&#23384;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#20351;AI&#27169;&#22411;&#26356;&#21152;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.03148</link><description>&lt;p&gt;
CAMEL&#65306;&#38754;&#21521;&#39640;&#25928;&#35774;&#22791;&#31471;&#23398;&#20064;&#30340;AI&#27169;&#22411;&#21644;&#23884;&#20837;&#24335;DRAM&#30340;&#20849;&#21516;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
CAMEL: Co-Designing AI Models and Embedded DRAMs for Efficient On-Device Learning. (arXiv:2305.03148v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03148
&lt;/p&gt;
&lt;p&gt;
CAMEL&#25552;&#20986;&#20102;&#20351;&#29992;&#23884;&#20837;&#24335;DRAM&#20316;&#20026;&#20027;&#35201;&#23384;&#20648;&#20171;&#36136;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#35774;&#22791;&#31471;&#23398;&#20064;&#20013;&#23384;&#20648;&#21644;&#35745;&#31639;&#36807;&#31243;&#20013;&#21344;&#29992;&#22823;&#37327;&#20869;&#23384;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#20351;AI&#27169;&#22411;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#30340;&#20852;&#36215;&#23548;&#33268;&#36793;&#32536;&#35774;&#22791;&#20135;&#29983;&#20102;&#22823;&#37327;&#25968;&#25454;&#65292;&#36890;&#24120;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#36827;&#34892;&#22788;&#29702;&#12290;&#35774;&#22791;&#31471;&#23398;&#20064;&#20351;&#36793;&#32536;&#24179;&#21488;&#33021;&#22815;&#19981;&#26029;&#22320;&#26681;&#25454;&#29992;&#25143;&#20010;&#20154;&#25968;&#25454;&#35843;&#25972;AI&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#26381;&#21153;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#19978;&#36827;&#34892;AI&#35757;&#32451;&#38750;&#24120;&#22256;&#38590;&#65292;&#22240;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20250;&#24102;&#26469;&#23494;&#38598;&#30340;&#35745;&#31639;&#24037;&#20316;&#37327;&#21644;&#21344;&#29992;&#22823;&#37327;&#33455;&#29255;&#20869;&#23384;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#23884;&#20837;&#24335;&#21160;&#24577;&#38543;&#26426;&#23384;&#21462;&#23384;&#20648;&#22120;&#65288;eDRAM&#65289;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#30340;&#20027;&#35201;&#23384;&#20648;&#20171;&#36136;&#12290;&#19982;&#38745;&#24577;&#38543;&#26426;&#35775;&#38382;&#23384;&#20648;&#22120;&#65288;SRAM&#65289;&#30456;&#27604;&#65292;eDRAM&#22312;&#23384;&#20648;&#23494;&#24230;&#19978;&#24341;&#20837;&#20102;&#36229;&#36807;2&#20493;&#30340;&#25913;&#36827;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#33455;&#29255;&#22806;&#23384;&#20648;&#22120;&#30340;&#27969;&#37327;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20445;&#25345;&#23384;&#20648;&#30340;&#25968;&#25454;&#23436;&#25972;&#65292;eDRAM&#38656;&#35201;&#25191;&#34892;&#32791;&#30005;&#30340;&#25968;&#25454;&#21047;&#26032;&#25805;&#20316;&#12290;&#22914;&#26524;&#25968;&#25454;&#23384;&#20648;&#19968;&#27573;&#26102;&#38388;&#65292;&#23601;&#21487;&#20197;&#36991;&#20813;eDRAM&#21047;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of the Internet of Things (IoT) has resulted in a remarkable amount of data generated on edge devices, which are often processed using AI algorithms. On-device learning enables edge platforms to continually adapt the AI models to user personal data and further allows for a better service quality. However, AI training on resource-limited devices is extremely difficult because of the intensive computing workload and the significant amount of on-chip memory consumption exacted by deep neural networks (DNNs). To mitigate this, we propose to use embedded dynamic random-access memory (eDRAM) as the main storage medium of training data. Compared with static random-access memory (SRAM), eDRAM introduces more than $2\times$ improvement on storage density, enabling reduced off-chip memory traffic. However, to keep the stored data intact, eDRAM is required to perform the power-hungry data refresh operations.  eDRAM refresh can be eliminated if the data is stored for a period of time
&lt;/p&gt;</description></item><item><title>&#8220;Segment Anything Model&#8221;&#65288;SAM&#65289;&#26159;&#36866;&#29992;&#20110;&#24120;&#35268;&#22270;&#20687;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#21106;&#65292;&#20294;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#38754;&#20855;&#26377;&#26356;&#39640;&#30340;&#25361;&#25112;&#24615;&#12290;&#20316;&#32773;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#22823;&#22411;&#21307;&#23398;&#20998;&#21106;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;SAM&#22312;&#35813;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.14660</link><description>&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#30340;&#8220;Segment Anything Model&#8221;&#27169;&#22411;&#65311;
&lt;/p&gt;
&lt;p&gt;
Segment Anything Model for Medical Images?. (arXiv:2304.14660v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14660
&lt;/p&gt;
&lt;p&gt;
&#8220;Segment Anything Model&#8221;&#65288;SAM&#65289;&#26159;&#36866;&#29992;&#20110;&#24120;&#35268;&#22270;&#20687;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#21106;&#65292;&#20294;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#38754;&#20855;&#26377;&#26356;&#39640;&#30340;&#25361;&#25112;&#24615;&#12290;&#20316;&#32773;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#22823;&#22411;&#21307;&#23398;&#20998;&#21106;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;SAM&#22312;&#35813;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;Segment Anything Model&#8221;&#65288;SAM&#65289;&#26159;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#24120;&#35268;&#22270;&#20687;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#23427;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#25512;&#24191;&#20998;&#21106;&#20219;&#21153;&#65292;&#36890;&#36807;&#33258;&#21160;&#21644;&#25163;&#21160;&#20004;&#31181;&#27169;&#24335;&#23454;&#29616;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#21106;&#12290;SAM&#22312;&#21508;&#31181;&#33258;&#28982;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#27169;&#24577;&#12289;&#32454;&#24494;&#30340;&#35299;&#21078;&#32467;&#26500;&#12289;&#19981;&#30830;&#23450;&#30340;&#22797;&#26434;&#23545;&#35937;&#36793;&#30028;&#21644;&#24191;&#27867;&#30340;&#23545;&#35937;&#23610;&#24230;&#65292;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65288;MIS&#65289;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;SAM&#22312;&#21508;&#31181;&#33258;&#28982;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#26524;&#12290;&#21516;&#26102;&#65292;&#38646;&#26679;&#26412;&#21644;&#39640;&#25928;&#30340;MIS&#21487;&#20197;&#24456;&#22909;&#22320;&#20943;&#23569;&#27880;&#37322;&#26102;&#38388;&#24182;&#20419;&#36827;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#21457;&#23637;&#12290;&#22240;&#27492;&#65292;SAM&#20284;&#20046;&#26159;&#19968;&#20010;&#28508;&#22312;&#30340;&#24037;&#20855;&#65292;&#24182;&#19988;&#20854;&#22312;&#22823;&#22411;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#24212;&#35813;&#36827;&#19968;&#27493;&#39564;&#35777;&#12290;&#25105;&#20204;&#25910;&#38598;&#21644;&#25972;&#29702;&#20102;52&#20010;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#20855;&#26377;16&#20010;&#27169;&#24577;&#21644;68&#20010;&#23545;&#35937;&#30340;&#22823;&#22411;&#21307;&#23398;&#20998;&#21106;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Segment Anything Model (SAM) is the first foundation model for general image segmentation. It designed a novel promotable segmentation task, ensuring zero-shot image segmentation using the pre-trained model via two main modes including automatic everything and manual prompt. SAM has achieved impressive results on various natural image segmentation tasks. However, medical image segmentation (MIS) is more challenging due to the complex modalities, fine anatomical structures, uncertain and complex object boundaries, and wide-range object scales. SAM has achieved impressive results on various natural image segmentation tasks. Meanwhile, zero-shot and efficient MIS can well reduce the annotation time and boost the development of medical image analysis. Hence, SAM seems to be a potential tool and its performance on large medical datasets should be further validated. We collected and sorted 52 open-source datasets, and build a large medical segmentation dataset with 16 modalities, 68 obje
&lt;/p&gt;</description></item><item><title>SPIRES&#26159;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#25552;&#21462;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#36890;&#29992;&#26597;&#35810;&#22238;&#31572;&#65292;&#33021;&#22815;&#22635;&#20805;&#22797;&#26434;&#30340;&#30693;&#35782;&#24211;&#32780;&#26080;&#38656;&#26174;&#24335;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2304.02711</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#25552;&#31034;&#35810;&#38382;&#19982;&#36882;&#24402;&#35821;&#20041;&#25552;&#21462;&#65288;SPIRES&#65289;&#65306;&#20351;&#29992;&#38646;&#26679;&#26412;&#23398;&#20064;&#22635;&#20805;&#30693;&#35782;&#24211;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Structured prompt interrogation and recursive extraction of semantics (SPIRES): A method for populating knowledge bases using zero-shot learning. (arXiv:2304.02711v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02711
&lt;/p&gt;
&lt;p&gt;
SPIRES&#26159;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#25552;&#21462;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#36890;&#29992;&#26597;&#35810;&#22238;&#31572;&#65292;&#33021;&#22815;&#22635;&#20805;&#22797;&#26434;&#30340;&#30693;&#35782;&#24211;&#32780;&#26080;&#38656;&#26174;&#24335;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#24314;&#30693;&#35782;&#24211;&#21644;&#26412;&#20307;&#26159;&#19968;&#39033;&#32791;&#26102;&#30340;&#20219;&#21153;&#65292;&#20381;&#36182;&#20110;&#25163;&#21160;&#31649;&#29702;&#12290;AI / NLP&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#19987;&#19994;&#31574;&#23637;&#20154;&#22635;&#20805;&#36825;&#20123;&#30693;&#35782;&#24211;&#65292;&#20294;&#24403;&#21069;&#26041;&#27861;&#20381;&#36182;&#20110;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#19988;&#19981;&#33021;&#22635;&#20805;&#20219;&#24847;&#22797;&#26434;&#30340;&#23884;&#22871;&#30693;&#35782;&#27169;&#24335;&#12290;&#22312;&#36825;&#37324;&#25105;&#20204;&#25552;&#20986;&#20102;Structured Prompt Interrogation and Recursive Extraction of Semantics&#65288;SPIRES&#65289;&#65292;&#19968;&#31181;&#30693;&#35782;&#25552;&#21462;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25191;&#34892;&#38646;&#26679;&#26412;&#23398;&#20064;&#65288;ZSL&#65289;&#21644;&#36890;&#29992;&#26597;&#35810;&#22238;&#31572;&#65292;&#20197;&#21450;&#20174;&#28789;&#27963;&#25552;&#31034;&#36820;&#22238;&#31526;&#21512;&#25351;&#23450;&#27169;&#24335;&#30340;&#20449;&#24687;&#12290; SPIRES&#38024;&#23545;&#32473;&#23450;&#30340;&#35814;&#32454;&#29992;&#25143;&#23450;&#20041;&#30340;&#30693;&#35782;&#27169;&#24335;&#21644;&#36755;&#20837;&#25991;&#26412;&#65292;&#23545;GPT-3+&#25191;&#34892;&#36882;&#24402;&#25552;&#31034;&#35810;&#38382;&#65292;&#20197;&#33719;&#24471;&#19982;&#25552;&#20379;&#30340;&#27169;&#24335;&#21305;&#37197;&#30340;&#19968;&#32452;&#21709;&#24212;&#12290; SPIRES&#20351;&#29992;&#29616;&#26377;&#30340;&#26412;&#20307;&#21644;&#35789;&#27719;&#34920;&#20026;&#25152;&#26377;&#21305;&#37197;&#20803;&#32032;&#25552;&#20379;&#26631;&#35782;&#31526;&#12290; &#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#19981;&#21516;&#39046;&#22495;&#65288;&#21253;&#25324;&#38899;&#20048;&#65292;&#20307;&#32946;&#21644;&#25919;&#27835;&#65289;&#20013;&#20351;&#29992;SPIRES&#30340;&#31034;&#20363;&#65292;&#23637;&#31034;&#20102;&#20854;&#33021;&#22815;&#22635;&#20805;&#22797;&#26434;&#30340;&#30693;&#35782;&#24211;&#32780;&#26080;&#38656;&#26174;&#24335;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creating knowledge bases and ontologies is a time consuming task that relies on a manual curation. AI/NLP approaches can assist expert curators in populating these knowledge bases, but current approaches rely on extensive training data, and are not able to populate arbitrary complex nested knowledge schemas.  Here we present Structured Prompt Interrogation and Recursive Extraction of Semantics (SPIRES), a Knowledge Extraction approach that relies on the ability of Large Language Models (LLMs) to perform zero-shot learning (ZSL) and general-purpose query answering from flexible prompts and return information conforming to a specified schema. Given a detailed, user-defined knowledge schema and an input text, SPIRES recursively performs prompt interrogation against GPT-3+ to obtain a set of responses matching the provided schema. SPIRES uses existing ontologies and vocabularies to provide identifiers for all matched elements.  We present examples of use of SPIRES in different domains, inc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#35757;&#32451;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#27169;&#22411;&#30340;&#21453;&#21521;&#26041;&#27861;&#65292;&#21033;&#29992;&#30456;&#23545;&#20934;&#30830;&#24615;&#30340;&#31574;&#30053;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;Polytuplet Loss&#20989;&#25968;&#26469;&#30830;&#20445;&#20248;&#20808;&#23398;&#20064;&#31572;&#26696;&#36873;&#25321;&#30340;&#30456;&#23545;&#27491;&#30830;&#24615;&#65292;&#33719;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#26524;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#19968;&#33324;&#24615;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2304.01046</link><description>&lt;p&gt;
&#8220;Polytuplet Loss: &#35757;&#32451;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#27169;&#22411;&#30340;&#21453;&#21521;&#26041;&#27861;&#8221;
&lt;/p&gt;
&lt;p&gt;
Polytuplet Loss: A Reverse Approach to Training Reading Comprehension and Logical Reasoning Models. (arXiv:2304.01046v1 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#35757;&#32451;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#27169;&#22411;&#30340;&#21453;&#21521;&#26041;&#27861;&#65292;&#21033;&#29992;&#30456;&#23545;&#20934;&#30830;&#24615;&#30340;&#31574;&#30053;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;Polytuplet Loss&#20989;&#25968;&#26469;&#30830;&#20445;&#20248;&#20808;&#23398;&#20064;&#31572;&#26696;&#36873;&#25321;&#30340;&#30456;&#23545;&#27491;&#30830;&#24615;&#65292;&#33719;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#26524;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#19968;&#33324;&#24615;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25972;&#20010;&#23398;&#26657;&#25945;&#32946;&#36807;&#31243;&#20013;&#65292;&#23398;&#29983;&#20204;&#23558;&#21463;&#21040;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#30340;&#32771;&#39564;&#12290;&#23398;&#29983;&#20204;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#31574;&#30053;&#26469;&#23436;&#25104;&#27492;&#31867;&#32771;&#35797;&#65292;&#20854;&#20013;&#26377;&#20123;&#34987;&#35748;&#20026;&#26159;&#36890;&#24120;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#31574;&#30053;&#30340;&#12290;&#36825;&#26679;&#19968;&#31181;&#31574;&#30053;&#28041;&#21450;&#24378;&#35843;&#30456;&#23545;&#20934;&#30830;&#24615;&#32780;&#38750;&#32477;&#23545;&#20934;&#30830;&#24615;&#65292;&#29702;&#35770;&#19978;&#21487;&#20197;&#22312;&#19981;&#23436;&#20840;&#25484;&#25569;&#35299;&#39064;&#25152;&#38656;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#24471;&#20986;&#27491;&#30830;&#31572;&#26696;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#36825;&#31181;&#31574;&#30053;&#26469;&#35757;&#32451;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#20197;&#35299;&#20915;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;ReClor&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#36923;&#36753;&#25512;&#29702;&#25216;&#33021;&#65292;&#20294;&#25105;&#20204;&#19987;&#27880;&#20110;&#19968;&#31181;&#36890;&#29992;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Polytuplet Loss&#20989;&#25968;&#65292;&#26159;&#19977;&#20803;&#32452;&#25439;&#22833;&#20989;&#25968;&#30340;&#25193;&#23637;&#65292;&#20197;&#30830;&#20445;&#20248;&#20808;&#23398;&#20064;&#31572;&#26696;&#36873;&#25321;&#30340;&#30456;&#23545;&#27491;&#30830;&#24615;&#32780;&#38750;&#23398;&#20064;&#32477;&#23545;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Throughout schooling, students are tested on reading comprehension and logical reasoning. Students have developed various strategies for completing such exams, some of which are generally thought to outperform others. One such strategy involves emphasizing relative accuracy over absolute accuracy and can theoretically produce the correct answer without full knowledge of the information required to solve the question. This paper examines the effectiveness of applying such a strategy to train transfer learning models to solve reading comprehension and logical reasoning questions. The models were evaluated on the ReClor dataset, a challenging reading comprehension and logical reasoning benchmark. While previous studies targeted logical reasoning skills, we focus on a general training method and model architecture. We propose the polytuplet loss function, an extension of the triplet loss function, to ensure prioritization of learning the relative correctness of answer choices over learning
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20581;&#22766;&#30340;&#39118;&#38505;&#24863;&#30693;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20248;&#21270;&#26399;&#26435;&#23545;&#20914;&#31574;&#30053;&#65292;&#29305;&#21035;&#24212;&#29992;&#20110;&#30028;&#38480;&#26399;&#26435;&#23545;&#20914;&#65292;&#38543;&#30528;&#20195;&#29702;&#39118;&#38505;&#20559;&#22909;&#21464;&#21270;&#65292;&#23545;&#20914;&#31574;&#30053;&#21457;&#29983;&#25197;&#26354;&#65292;&#40065;&#26834;&#31574;&#30053;&#20248;&#20110;&#38750;&#40065;&#26834;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2303.15216</link><description>&lt;p&gt;
&#20581;&#22766;&#30340;&#39118;&#38505;&#24863;&#30693;&#26399;&#26435;&#23545;&#20914;
&lt;/p&gt;
&lt;p&gt;
Robust Risk-Aware Option Hedging. (arXiv:2303.15216v1 [q-fin.CP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20581;&#22766;&#30340;&#39118;&#38505;&#24863;&#30693;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20248;&#21270;&#26399;&#26435;&#23545;&#20914;&#31574;&#30053;&#65292;&#29305;&#21035;&#24212;&#29992;&#20110;&#30028;&#38480;&#26399;&#26435;&#23545;&#20914;&#65292;&#38543;&#30528;&#20195;&#29702;&#39118;&#38505;&#20559;&#22909;&#21464;&#21270;&#65292;&#23545;&#20914;&#31574;&#30053;&#21457;&#29983;&#25197;&#26354;&#65292;&#40065;&#26834;&#31574;&#30053;&#20248;&#20110;&#38750;&#40065;&#26834;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26399;&#26435;&#23545;&#20914;/&#20132;&#26131;&#30340;&#30446;&#26631;&#19981;&#20165;&#20165;&#26159;&#20026;&#20102;&#20445;&#25252;&#19979;&#34892;&#39118;&#38505;&#65292;&#36824;&#24076;&#26395;&#23547;&#27714;&#25910;&#30410;&#65292;&#39537;&#21160;&#31574;&#30053;&#12290;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#20581;&#22766;&#30340;&#39118;&#38505;&#24863;&#30693;&#24378;&#21270;&#23398;&#20064;(RL)&#22312;&#20943;&#36731;&#19982;&#36335;&#24452;&#30456;&#20851;&#30340;&#37329;&#34701;&#34893;&#29983;&#21697;&#39118;&#38505;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#21033;&#29992;Jaimungal&#12289;Pesenti&#12289;Wang&#12289;Tatsat(2022)&#30340;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#20248;&#21270;&#20581;&#22766;&#30340;&#39118;&#38505;&#24863;&#30693;&#32489;&#25928;&#26631;&#20934;&#65292;&#20855;&#20307;&#24212;&#29992;&#20110;&#30028;&#38480;&#26399;&#26435;&#23545;&#20914;&#65292;&#24182;&#24378;&#35843;&#38543;&#30528;&#20195;&#29702;&#20174;&#39118;&#38505;&#35268;&#36991;&#36716;&#21464;&#20026;&#39118;&#38505;&#23547;&#27714;&#65292;&#26368;&#20248;&#23545;&#20914;&#31574;&#30053;&#20250;&#21457;&#29983;&#25197;&#26354;&#65292;&#20197;&#21450;&#20195;&#29702;&#22914;&#20309;&#24378;&#21270;&#20854;&#31574;&#30053;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#24403;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;(DGP)&#19982;&#35757;&#32451;DGP&#19981;&#21516;&#26102;&#65292;&#23545;&#20914;&#30340;&#34920;&#29616;&#65292;&#24182;&#35777;&#26126;&#20102;&#40065;&#26834;&#31574;&#30053;&#20248;&#20110;&#38750;&#40065;&#26834;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objectives of option hedging/trading extend beyond mere protection against downside risks, with a desire to seek gains also driving agent's strategies. In this study, we showcase the potential of robust risk-aware reinforcement learning (RL) in mitigating the risks associated with path-dependent financial derivatives. We accomplish this by leveraging the Jaimungal, Pesenti, Wang, Tatsat (2022) and their policy gradient approach, which optimises robust risk-aware performance criteria. We specifically apply this methodology to the hedging of barrier options, and highlight how the optimal hedging strategy undergoes distortions as the agent moves from being risk-averse to risk-seeking. As well as how the agent robustifies their strategy. We further investigate the performance of the hedge when the data generating process (DGP) varies from the training DGP, and demonstrate that the robust strategies outperform the non-robust ones.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#32422;&#26463;&#19979;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;EPAC&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;&#36825;&#20123;&#35299;&#37322;&#26102;&#27169;&#22411;&#30340;&#30410;&#22788;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#36817;&#20284;&#30340;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.14496</link><description>&lt;p&gt;
&#35299;&#37322;&#32422;&#26463;&#19979;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning with Explanation Constraints. (arXiv:2303.14496v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#32422;&#26463;&#19979;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;EPAC&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;&#36825;&#20123;&#35299;&#37322;&#26102;&#27169;&#22411;&#30340;&#30410;&#22788;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#36817;&#20284;&#30340;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#30417;&#30563;&#23398;&#20064;&#20551;&#35774;&#23384;&#22312;&#26631;&#27880;&#25968;&#25454;&#65292;&#20294;&#25105;&#20204;&#21487;&#33021;&#26377;&#20851;&#20110;&#27169;&#22411;&#24212;&#22914;&#20309;&#36816;&#34892;&#30340;&#20808;&#39564;&#20449;&#24687;&#12290;&#26412;&#25991;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#20174;&#35299;&#37322;&#32422;&#26463;&#20013;&#23398;&#20064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#23398;&#20064;&#29702;&#35770;&#26694;&#26550;&#65292;&#20998;&#26512;&#20102;&#36825;&#20123;&#35299;&#37322;&#22914;&#20309;&#25552;&#39640;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#26412;&#25991;&#30340;&#31532;&#19968;&#39033;&#20851;&#38190;&#36129;&#29486;&#26159;&#36890;&#36807;&#23450;&#20041;&#25105;&#20204;&#31216;&#20043;&#20026;EPAC&#27169;&#22411;&#65288;&#22312;&#26032;&#25968;&#25454;&#26399;&#26395;&#20013;&#28385;&#36275;&#36825;&#20123;&#32422;&#26463;&#30340;&#27169;&#22411;&#65289;&#26469;&#22238;&#31572;&#21738;&#20123;&#27169;&#22411;&#20250;&#21463;&#30410;&#20110;&#35299;&#37322;&#36825;&#19968;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#30340;&#23398;&#20064;&#29702;&#35770;&#24037;&#20855;&#20998;&#26512;&#20102;&#36825;&#31867;&#27169;&#22411;&#12290;&#31532;&#20108;&#20010;&#20851;&#38190;&#36129;&#29486;&#26159;&#23545;&#20110;&#30001;&#32447;&#24615;&#27169;&#22411;&#21644;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26799;&#24230;&#20449;&#24687;&#32473;&#20986;&#30340;&#35268;&#33539;&#35299;&#37322;&#30340;&#38480;&#21046;&#65288;&#20197;&#20854;Rademacher&#22797;&#26434;&#24230;&#20026;&#34913;&#37327;&#26631;&#20934;&#65289;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#21464;&#20998;&#36817;&#20284;&#25552;&#20379;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#24182;&#28385;&#36275;&#36825;&#20123;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
While supervised learning assumes the presence of labeled data, we may have prior information about how models should behave. In this paper, we formalize this notion as learning from explanation constraints and provide a learning theoretic framework to analyze how such explanations can improve the learning of our models. For what models would explanations be helpful? Our first key contribution addresses this question via the definition of what we call EPAC models (models that satisfy these constraints in expectation over new data), and we analyze this class of models using standard learning theoretic tools. Our second key contribution is to characterize these restrictions (in terms of their Rademacher complexities) for a canonical class of explanations given by gradient information for linear models and two layer neural networks. Finally, we provide an algorithmic solution for our framework, via a variational approximation that achieves better performance and satisfies these constraint
&lt;/p&gt;</description></item><item><title>FuNVol&#26159;&#19968;&#20010;&#22810;&#36164;&#20135;&#38544;&#21547;&#27874;&#21160;&#29575;&#24066;&#22330;&#27169;&#25311;&#22120;&#65292;&#20351;&#29992;&#20989;&#25968;&#20027;&#25104;&#20998;&#21644;&#31070;&#32463;SDE&#29983;&#25104;&#30495;&#23454;&#21382;&#21490;&#20215;&#26684;&#30340;IV&#34920;&#38754;&#24207;&#21015;&#65292;&#24182;&#22312;&#26080;&#38745;&#24577;&#22871;&#21033;&#30340;&#34920;&#38754;&#27425;&#27969;&#24418;&#20869;&#20135;&#29983;&#19968;&#33268;&#30340;&#24066;&#22330;&#24773;&#26223;&#12290;&#21516;&#26102;&#65292;&#20351;&#29992;&#27169;&#25311;&#34920;&#38754;&#36827;&#34892;&#23545;&#20914;&#21487;&#20197;&#29983;&#25104;&#19982;&#23454;&#29616;P&#65286;L&#19968;&#33268;&#30340;&#25439;&#30410;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2303.00859</link><description>&lt;p&gt;
FuNVol&#65306;&#20351;&#29992;&#20989;&#25968;&#20027;&#25104;&#20998;&#21644;&#31070;&#32463;SDE&#30340;&#22810;&#36164;&#20135;&#38544;&#21547;&#27874;&#21160;&#29575;&#24066;&#22330;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
FuNVol: A Multi-Asset Implied Volatility Market Simulator using Functional Principal Components and Neural SDEs. (arXiv:2303.00859v2 [q-fin.CP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00859
&lt;/p&gt;
&lt;p&gt;
FuNVol&#26159;&#19968;&#20010;&#22810;&#36164;&#20135;&#38544;&#21547;&#27874;&#21160;&#29575;&#24066;&#22330;&#27169;&#25311;&#22120;&#65292;&#20351;&#29992;&#20989;&#25968;&#20027;&#25104;&#20998;&#21644;&#31070;&#32463;SDE&#29983;&#25104;&#30495;&#23454;&#21382;&#21490;&#20215;&#26684;&#30340;IV&#34920;&#38754;&#24207;&#21015;&#65292;&#24182;&#22312;&#26080;&#38745;&#24577;&#22871;&#21033;&#30340;&#34920;&#38754;&#27425;&#27969;&#24418;&#20869;&#20135;&#29983;&#19968;&#33268;&#30340;&#24066;&#22330;&#24773;&#26223;&#12290;&#21516;&#26102;&#65292;&#20351;&#29992;&#27169;&#25311;&#34920;&#38754;&#36827;&#34892;&#23545;&#20914;&#21487;&#20197;&#29983;&#25104;&#19982;&#23454;&#29616;P&#65286;L&#19968;&#33268;&#30340;&#25439;&#30410;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20989;&#25968;&#25968;&#25454;&#20998;&#26512;&#21644;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65292;&#32467;&#21512;&#27010;&#29575;&#31215;&#20998;&#21464;&#25442;&#24809;&#32602;&#26469;&#29983;&#25104;&#22810;&#20010;&#36164;&#20135;&#30340;&#38544;&#21547;&#27874;&#21160;&#29575;&#34920;&#38754;&#24207;&#21015;&#65292;&#35813;&#26041;&#27861;&#24544;&#23454;&#20110;&#21382;&#21490;&#20215;&#26684;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23398;&#20064;IV&#34920;&#38754;&#21644;&#20215;&#26684;&#30340;&#32852;&#21512;&#21160;&#24577;&#20135;&#29983;&#30340;&#24066;&#22330;&#24773;&#26223;&#19982;&#21382;&#21490;&#29305;&#24449;&#19968;&#33268;&#65292;&#24182;&#19988;&#22312;&#27809;&#26377;&#38745;&#24577;&#22871;&#21033;&#30340;&#34920;&#38754;&#27425;&#27969;&#24418;&#20869;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20351;&#29992;&#27169;&#25311;&#34920;&#38754;&#36827;&#34892;&#23545;&#20914;&#20250;&#29983;&#25104;&#19982;&#23454;&#29616;P&#65286;L&#19968;&#33268;&#30340;&#25439;&#30410;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Here, we introduce a new approach for generating sequences of implied volatility (IV) surfaces across multiple assets that is faithful to historical prices. We do so using a combination of functional data analysis and neural stochastic differential equations (SDEs) combined with a probability integral transform penalty to reduce model misspecification. We demonstrate that learning the joint dynamics of IV surfaces and prices produces market scenarios that are consistent with historical features and lie within the sub-manifold of surfaces that are essentially free of static arbitrage. Finally, we demonstrate that delta hedging using the simulated surfaces generates profit and loss (P&amp;L) distributions that are consistent with realised P&amp;Ls.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#37327;&#23376;&#23398;&#20064;&#29702;&#35770;&#25299;&#23637;&#20102;&#25209;&#22788;&#29702;&#22810;&#31867;&#23398;&#20064;&#12289;&#22312;&#32447;&#24067;&#23572;&#23398;&#20064;&#21644;&#22312;&#32447;&#22810;&#31867;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#37327;&#23376;&#31034;&#20363;&#30340;&#22312;&#32447;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.07409</link><description>&lt;p&gt;
&#12298;&#36229;&#36234;&#25209;&#22788;&#29702;&#20108;&#20803;&#20998;&#31867;&#30340;&#37327;&#23376;&#23398;&#20064;&#29702;&#35770;&#12299;
&lt;/p&gt;
&lt;p&gt;
Quantum Learning Theory Beyond Batch Binary Classification. (arXiv:2302.07409v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07409
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#37327;&#23376;&#23398;&#20064;&#29702;&#35770;&#25299;&#23637;&#20102;&#25209;&#22788;&#29702;&#22810;&#31867;&#23398;&#20064;&#12289;&#22312;&#32447;&#24067;&#23572;&#23398;&#20064;&#21644;&#22312;&#32447;&#22810;&#31867;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#37327;&#23376;&#31034;&#20363;&#30340;&#22312;&#32447;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Arunachalam&#21644;de Wolf&#65288;2018&#65289;&#35777;&#26126;&#20102;&#22312;&#21487;&#23454;&#29616;&#21644;&#31946;&#28034;&#35774;&#32622;&#19979;&#65292;&#37327;&#23376;&#25209;&#22788;&#29702;&#23398;&#20064;&#24067;&#23572;&#20989;&#25968;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#19982;&#30456;&#24212;&#30340;&#32463;&#20856;&#26679;&#26412;&#22797;&#26434;&#24615;&#20855;&#26377;&#30456;&#21516;&#30340;&#24418;&#24335;&#21644;&#25968;&#37327;&#32423;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#26126;&#26174;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#25512;&#24191;&#21040;&#20102;&#25209;&#22788;&#29702;&#22810;&#31867;&#23398;&#20064;&#12289;&#22312;&#32447;&#24067;&#23572;&#23398;&#20064;&#21644;&#22312;&#32447;&#22810;&#31867;&#23398;&#20064;&#12290;&#23545;&#20110;&#25105;&#20204;&#30340;&#22312;&#32447;&#23398;&#20064;&#32467;&#26524;&#65292;&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;&#20102;Dawid&#21644;Tewari&#65288;2022&#65289;&#32463;&#20856;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#23545;&#25163;&#21464;&#20307;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#65288;&#25454;&#25105;&#20204;&#25152;&#30693;&#65289;&#20855;&#26377;&#37327;&#23376;&#31034;&#20363;&#30340;&#22312;&#32447;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Arunachalam and de Wolf (2018) showed that the sample complexity of quantum batch learning of boolean functions, in the realizable and agnostic settings, has the same form and order as the corresponding classical sample complexities. In this paper, we extend this, ostensibly surprising, message to batch multiclass learning, online boolean learning, and online multiclass learning. For our online learning results, we first consider an adaptive adversary variant of the classical model of Dawid and Tewari (2022). Then, we introduce the first (to the best of our knowledge) model of online learning with quantum examples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#22914;&#20309;&#22312;&#24046;&#20998;&#38544;&#31169;&#26465;&#20214;&#19979;&#20272;&#35745;&#19968;&#20010;&#20998;&#24067;&#30340;&#22810;&#20010;&#20998;&#20301;&#25968;&#12290;&#23427;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65306;&#19968;&#31181;&#26159;&#36890;&#36807;&#31169;&#26377;&#22320;&#20272;&#35745;&#26679;&#26412;&#30340;&#32463;&#39564;&#20998;&#20301;&#25968;&#26469;&#20272;&#35745;&#20998;&#24067;&#30340;&#20998;&#20301;&#25968;&#65292;&#21478;&#19968;&#31181;&#26159;&#20351;&#29992;&#23494;&#24230;&#20272;&#35745;&#25216;&#26415;&#36827;&#34892;&#20998;&#20301;&#25968;&#20989;&#25968;&#20272;&#35745;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2302.06943</link><description>&lt;p&gt;
&#22810;&#20010;&#20998;&#20301;&#25968;&#30340;&#31169;&#26377;&#32479;&#35745;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Private Statistical Estimation of Many Quantiles. (arXiv:2302.06943v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#22914;&#20309;&#22312;&#24046;&#20998;&#38544;&#31169;&#26465;&#20214;&#19979;&#20272;&#35745;&#19968;&#20010;&#20998;&#24067;&#30340;&#22810;&#20010;&#20998;&#20301;&#25968;&#12290;&#23427;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65306;&#19968;&#31181;&#26159;&#36890;&#36807;&#31169;&#26377;&#22320;&#20272;&#35745;&#26679;&#26412;&#30340;&#32463;&#39564;&#20998;&#20301;&#25968;&#26469;&#20272;&#35745;&#20998;&#24067;&#30340;&#20998;&#20301;&#25968;&#65292;&#21478;&#19968;&#31181;&#26159;&#20351;&#29992;&#23494;&#24230;&#20272;&#35745;&#25216;&#26415;&#36827;&#34892;&#20998;&#20301;&#25968;&#20989;&#25968;&#20272;&#35745;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#24046;&#20998;&#38544;&#31169;&#26465;&#20214;&#19979;&#20272;&#35745;&#35768;&#22810;&#32479;&#35745;&#20998;&#20301;&#25968;&#30340;&#38382;&#39064;&#12290;&#26356;&#20855;&#20307;&#22320;&#65292;&#32473;&#23450;&#19968;&#20010;&#20998;&#24067;&#24182;&#19988;&#33021;&#22815;&#35775;&#38382;&#26469;&#33258;&#20854;&#29420;&#31435;&#21516;&#20998;&#24067;&#26679;&#26412;&#65292;&#25105;&#20204;&#32771;&#34385;&#22312;&#29305;&#23450;&#28857;&#19978;&#20272;&#35745;&#20854;&#32047;&#31215;&#20998;&#24067;&#20989;&#25968;&#30340;&#36870;&#20989;&#25968;&#65288;&#20998;&#20301;&#25968;&#20989;&#25968;&#65289;&#12290;&#20363;&#22914;&#65292;&#36825;&#39033;&#20219;&#21153;&#22312;&#31169;&#26377;&#25968;&#25454;&#29983;&#25104;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#31169;&#19979;&#20272;&#35745;&#26679;&#26412;&#30340;&#32463;&#39564;&#20998;&#20301;&#25968;&#65292;&#24182;&#23558;&#27492;&#32467;&#26524;&#29992;&#20316;&#20998;&#24067;&#30340;&#20998;&#20301;&#25968;&#20272;&#35745;&#22120;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102; Kaplan&#31561;&#20154;&#26368;&#36817;&#21457;&#34920;&#30340;&#36882;&#24402;&#20272;&#35745;&#20998;&#20301;&#25968;&#30340;&#38544;&#31169;&#31639;&#27861;&#30340;&#32479;&#35745;&#24615;&#36136;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#26159;&#20351;&#29992;&#23494;&#24230;&#20272;&#35745;&#25216;&#26415;&#36827;&#34892;&#22343;&#21248;&#38388;&#38548;&#20869;&#30340;&#20998;&#20301;&#25968;&#20989;&#25968;&#20272;&#35745;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#24403;&#25105;&#20204;&#24819;&#35201;&#20272;&#35745;&#35768;&#22810;&#20998;&#20301;&#25968;&#26102;&#65292;&#26368;&#22909;&#20351;&#29992;&#31532;&#19968;&#31181;&#26041;&#27861;&#21333;&#29420;&#20272;&#35745;&#23427;&#20204;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24403;&#25105;&#20204;&#24819;&#35201;&#22312;&#22823;&#21306;&#38388;&#19978;&#20272;&#35745;&#20998;&#20301;&#25968;&#20989;&#25968;&#26102;&#65292;&#31532;&#20108;&#31181;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work studies the estimation of many statistical quantiles under differential privacy. More precisely, given a distribution and access to i.i.d. samples from it, we study the estimation of the inverse of its cumulative distribution function (the quantile function) at specific points. For instance, this task is of key importance in private data generation. We present two different approaches. The first one consists in privately estimating the empirical quantiles of the samples and using this result as an estimator of the quantiles of the distribution. In particular, we study the statistical properties of the recently published algorithm introduced by Kaplan et al. 2022 that privately estimates the quantiles recursively. The second approach is to use techniques of density estimation in order to uniformly estimate the quantile function on an interval. In particular, we show that there is a tradeoff between the two methods. When we want to estimate many quantiles, it is better to estim
&lt;/p&gt;</description></item><item><title>DynGFN&#26159;&#19968;&#31181;&#20511;&#21161;RNA&#36895;&#24230;&#25216;&#26415;&#36827;&#34892;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#32593;&#32476;&#32467;&#26500;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#20934;&#30830;&#24230;&#19978;&#36229;&#36807;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.04178</link><description>&lt;p&gt;
DynGFN: &#20511;&#21161;RNA&#36895;&#24230;&#25216;&#26415;&#36827;&#34892;&#36125;&#21494;&#26031;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#25512;&#26029;&#30340;GFlowNets&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DynGFN: Towards Bayesian Inference of Gene Regulatory Networks with GFlowNets. (arXiv:2302.04178v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04178
&lt;/p&gt;
&lt;p&gt;
DynGFN&#26159;&#19968;&#31181;&#20511;&#21161;RNA&#36895;&#24230;&#25216;&#26415;&#36827;&#34892;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#32593;&#32476;&#32467;&#26500;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#20934;&#30830;&#24230;&#19978;&#36229;&#36807;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#32990;&#29983;&#29289;&#23398;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#25512;&#26029;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#65288;GRN&#65289;&#65292;&#35813;&#32593;&#32476;&#25551;&#36848;&#20102;&#25511;&#21046;&#22522;&#22240;&#34920;&#36798;&#21644;&#32454;&#32990;&#21151;&#33021;&#30340;&#22522;&#22240;&#21450;&#20854;&#20135;&#29289;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#26412;&#25991;&#20511;&#21161;RNA&#36895;&#24230;&#25216;&#26415;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;DynGFN&#65292;&#35813;&#26041;&#27861;&#35757;&#32451;&#29983;&#25104;&#27969;&#32593;&#32476;&#65292;&#20351;&#29992;RNA&#36895;&#24230;&#25968;&#25454;&#25191;&#34892;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#24182;&#25429;&#25417;&#32593;&#32476;&#32467;&#26500;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the grand challenges of cell biology is inferring the gene regulatory network (GRN) which describes interactions between genes and their products that control gene expression and cellular function. We can treat this as a causal discovery problem but with two non-standard challenges: (1) regulatory networks are inherently cyclic so we should not model a GRN as a directed acyclic graph (DAG), and (2) observations have significant measurement noise, so for typical sample sizes there will always be a large equivalence class of graphs that are likely given the data, and we want methods that capture this uncertainty. Existing methods either focus on challenge (1), identifying cyclic structure from dynamics, or on challenge (2) learning complex Bayesian posteriors over DAGs, but not both. In this paper we leverage the fact that it is possible to estimate the "velocity" of gene expression with RNA velocity techniques to develop an approach that addresses both challenges. Because we have
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25512;&#23548;&#20986;&#20102;&#39034;&#24207;&#20998;&#21106;&#23398;&#20064;&#22312;&#24322;&#26500;&#25968;&#25454;&#19978;&#25910;&#25947;&#30340;&#20445;&#35777;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#23427;&#22312;&#24322;&#26500;&#25968;&#25454;&#19978;&#20248;&#20110;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.01633</link><description>&lt;p&gt;
&#24322;&#26500;&#25968;&#25454;&#19978;&#39034;&#24207;&#20998;&#21106;&#23398;&#20064;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Convergence Analysis of Sequencial Split Learning on Heterogeneous Data. (arXiv:2302.01633v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25512;&#23548;&#20986;&#20102;&#39034;&#24207;&#20998;&#21106;&#23398;&#20064;&#22312;&#24322;&#26500;&#25968;&#25454;&#19978;&#25910;&#25947;&#30340;&#20445;&#35777;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#23427;&#22312;&#24322;&#26500;&#25968;&#25454;&#19978;&#20248;&#20110;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#21644;&#20998;&#21106;&#23398;&#20064;&#65288;SL&#65289;&#26159;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#20004;&#31181;&#27969;&#34892;&#33539;&#20363;&#12290;&#36890;&#36807;&#23558;&#35745;&#31639;&#23494;&#38598;&#37096;&#20998;&#21368;&#36733;&#21040;&#26381;&#21153;&#22120;&#65292;SL&#23545;&#20110;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#36827;&#34892;&#28145;&#23618;&#27169;&#22411;&#35757;&#32451;&#38750;&#24120;&#26377;&#21069;&#36884;&#65292;&#20294;&#20173;&#32570;&#20047;&#20005;&#26684;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#39034;&#24207;SL&#65288;SSL&#65292;&#36827;&#34892;&#39034;&#24207;&#27169;&#22411;&#35757;&#32451;&#30340;SL&#22522;&#26412;&#24773;&#24418;&#65289;&#22312;&#24322;&#26500;&#25968;&#25454;&#19978;&#23545;&#20110;&#24378;&#21270;/&#19968;&#33324;/&#38750;&#20984;&#30446;&#26631;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25152;&#24471;&#21040;&#30340;&#20445;&#35777;&#34920;&#26126;&#65292;&#22312;&#24322;&#26500;&#25968;&#25454;&#19978;&#65292;SSL&#27604;&#32852;&#37030;&#24179;&#22343;&#65288;FedAvg&#65292;FL&#20013;&#26368;&#27969;&#34892;&#30340;&#31639;&#27861;&#65289;&#26356;&#22909;&#12290;&#25105;&#20204;&#22312;&#26497;&#31471;&#24322;&#26500;&#25968;&#25454;&#19978;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#20010;&#21453;&#30452;&#35273;&#30340;&#20998;&#26512;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) and Split Learning (SL) are two popular paradigms of distributed machine learning. By offloading the computation-intensive portions to the server, SL is promising for deep model training on resource-constrained devices, yet still lacking of rigorous convergence analysis. In this paper, we derive the convergence guarantees of Sequential SL (SSL, the vanilla case of SL that conducts the model training in sequence) for strongly/general/non-convex objectives on heterogeneous data. Notably, the derived guarantees suggest that SSL is better than Federated Averaging (FedAvg, the most popular algorithm in FL) on heterogeneous data. We validate the counterintuitive analysis result empirically on extremely heterogeneous data.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35268;&#32422;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#20219;&#20309;&#22810;&#25209;&#27425;&#31639;&#27861;&#36716;&#21270;&#20026;&#22788;&#29702;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#38543;&#26426;&#24310;&#36831;&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;&#25105;&#20204;&#19981;&#20165;&#22312;&#36172;&#21338;&#26426;&#12289;&#34920;&#26684;&#22411;MDPs&#21644;&#34920;&#26684;&#22411;MGs&#26041;&#38754;&#21462;&#24471;&#20102;&#19982;&#29616;&#26377;&#32467;&#26524;&#30456;&#21305;&#37197;&#25110;&#25913;&#36827;&#30340;&#25104;&#26524;&#65292;&#36824;&#39318;&#27425;&#23545;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#24310;&#36831;&#19982;&#20989;&#25968;&#36924;&#36817;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2302.01477</link><description>&lt;p&gt;
&#22522;&#20110;&#35268;&#32422;&#30340;&#24310;&#36831;&#21453;&#39304;&#39034;&#24207;&#20915;&#31574;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Reduction-based Framework for Sequential Decision Making with Delayed Feedback. (arXiv:2302.01477v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01477
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35268;&#32422;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#20219;&#20309;&#22810;&#25209;&#27425;&#31639;&#27861;&#36716;&#21270;&#20026;&#22788;&#29702;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#38543;&#26426;&#24310;&#36831;&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;&#25105;&#20204;&#19981;&#20165;&#22312;&#36172;&#21338;&#26426;&#12289;&#34920;&#26684;&#22411;MDPs&#21644;&#34920;&#26684;&#22411;MGs&#26041;&#38754;&#21462;&#24471;&#20102;&#19982;&#29616;&#26377;&#32467;&#26524;&#30456;&#21305;&#37197;&#25110;&#25913;&#36827;&#30340;&#25104;&#26524;&#65292;&#36824;&#39318;&#27425;&#23545;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#24310;&#36831;&#19982;&#20989;&#25968;&#36924;&#36817;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#33324;&#22810;&#26234;&#33021;&#20307;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#38543;&#26426;&#24310;&#36831;&#21453;&#39304;&#65292;&#21253;&#25324;&#36172;&#21338;&#26426;&#38382;&#39064;&#12289;&#21333;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#65288;MGs&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#35268;&#32422;&#30340;&#26694;&#26550;&#65292;&#23558;&#20219;&#20309;&#22810;&#25209;&#27425;&#31639;&#27861;&#36716;&#21270;&#20026;&#33021;&#22788;&#29702;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#38543;&#26426;&#24310;&#36831;&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;&#36890;&#36807;&#23558;&#19981;&#21516;&#30340;&#22810;&#25209;&#27425;&#31639;&#27861;&#25554;&#20837;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20960;&#20010;&#31034;&#20363;&#65292;&#35777;&#26126;&#25105;&#20204;&#30340;&#26694;&#26550;&#19981;&#20165;&#21305;&#37197;&#25110;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#36172;&#21338;&#26426;&#12289;&#34920;&#26684;&#22411;MDPs&#21644;&#34920;&#26684;&#22411;MGs&#30340;&#32467;&#26524;&#65292;&#36824;&#39318;&#27425;&#23545;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#24310;&#36831;&#19982;&#20989;&#25968;&#36924;&#36817;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#20026;&#22810;&#26234;&#33021;&#20307;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#24310;&#36831;&#21453;&#39304;&#25552;&#20379;&#20102;&#19968;&#22871;&#23436;&#25972;&#30340;&#23574;&#38160;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study stochastic delayed feedback in general multi-agent sequential decision making, which includes bandits, single-agent Markov decision processes (MDPs), and Markov games (MGs). We propose a novel reduction-based framework, which turns any multi-batched algorithm for sequential decision making with instantaneous feedback into a sample-efficient algorithm that can handle stochastic delays in sequential decision making. By plugging different multi-batched algorithms into our framework, we provide several examples demonstrating that our framework not only matches or improves existing results for bandits, tabular MDPs, and tabular MGs, but also provides the first line of studies on delays in sequential decision making with function approximation. In summary, we provide a complete set of sharp results for multi-agent sequential decision making with delayed feedback.
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#38598;&#21387;&#32553;&#26159;&#22788;&#29702;&#28023;&#37327;&#25968;&#25454;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#32508;&#21512;&#19968;&#20010;&#23567;&#22411;&#20856;&#22411;&#25968;&#25454;&#38598;&#26469;&#25552;&#39640;&#25968;&#25454;&#22788;&#29702;&#25928;&#29575;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;&#21387;&#32553;&#25968;&#25454;&#38598;&#26041;&#38754;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#24615;&#33021;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2301.05603</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#21387;&#32553;&#30340;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey of Dataset Distillation. (arXiv:2301.05603v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05603
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#21387;&#32553;&#26159;&#22788;&#29702;&#28023;&#37327;&#25968;&#25454;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#32508;&#21512;&#19968;&#20010;&#23567;&#22411;&#20856;&#22411;&#25968;&#25454;&#38598;&#26469;&#25552;&#39640;&#25968;&#25454;&#22788;&#29702;&#25928;&#29575;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;&#21387;&#32553;&#25968;&#25454;&#38598;&#26041;&#38754;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#24615;&#33021;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24471;&#21040;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#21457;&#23637;&#65292;&#24182;&#22312;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#25104;&#20026;&#39318;&#36873;&#12290;&#36825;&#19968;&#36827;&#23637;&#20027;&#35201;&#24402;&#21151;&#20110;&#24555;&#36895;&#21457;&#23637;&#30340;&#35745;&#31639;&#36164;&#28304;&#19982;&#20808;&#36827;&#31639;&#27861;&#30340;&#31995;&#32479;&#21327;&#21516;&#65292;&#29992;&#20197;&#22788;&#29702;&#28023;&#37327;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22788;&#29702;&#26377;&#38480;&#35745;&#31639;&#33021;&#21147;&#19979;&#26080;&#38480;&#22686;&#38271;&#30340;&#25968;&#25454;&#36880;&#28176;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#25552;&#39640;&#25968;&#25454;&#22788;&#29702;&#25928;&#29575;&#12290;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#26159;&#19968;&#31181;&#25968;&#25454;&#38598;&#32553;&#20943;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#22823;&#35268;&#27169;&#25968;&#25454;&#20013;&#32508;&#21512;&#20986;&#19968;&#20010;&#23567;&#22411;&#20856;&#22411;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#24341;&#36215;&#20102;&#28145;&#24230;&#23398;&#20064;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#21487;&#20197;&#26681;&#25454;&#26159;&#21542;&#26126;&#30830;&#27169;&#20223;&#30446;&#26631;&#25968;&#25454;&#30340;&#24615;&#33021;&#23558;&#20854;&#20998;&#31867;&#20026;&#20803;&#23398;&#20064;&#21644;&#25968;&#25454;&#21305;&#37197;&#26694;&#26550;&#12290;&#23613;&#31649;&#25968;&#25454;&#38598;&#21387;&#32553;&#22312;&#21387;&#32553;&#25968;&#25454;&#38598;&#26041;&#38754;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24615;&#33021;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning technology has developed unprecedentedly in the last decade and has become the primary choice in many application domains. This progress is mainly attributed to a systematic collaboration in which rapidly growing computing resources encourage advanced algorithms to deal with massive data. However, it has gradually become challenging to handle the unlimited growth of data with limited computing power. To this end, diverse approaches are proposed to improve data processing efficiency. Dataset distillation, a dataset reduction method, addresses this problem by synthesizing a small typical dataset from substantial data and has attracted much attention from the deep learning community. Existing dataset distillation methods can be taxonomized into meta-learning and data matching frameworks according to whether they explicitly mimic the performance of target data. Although dataset distillation has shown surprising performance in compressing datasets, there are still several limi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#38750;&#21442;&#25968;&#30340;&#31232;&#30095;&#21152;&#24615;&#27169;&#22411;&#29992;&#20110;&#20998;&#31867;&#65292;&#36890;&#36807;&#23545;&#20998;&#37327;&#31995;&#25968;&#26045;&#21152;&#31232;&#30095;&#32452;Lasso&#21644;&#31232;&#30095;&#32452;Slope&#22411;&#24809;&#32602;&#26469;&#35774;&#35745;&#20998;&#31867;&#22120;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#20998;&#31867;&#22120;&#22312;&#26410;&#30693;&#31232;&#30095;&#24615;&#21644;&#24179;&#28369;&#24615;&#19978;&#30340;&#33258;&#36866;&#24212;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.01792</link><description>&lt;p&gt;
&#31232;&#30095;&#21152;&#24615;&#27169;&#22411;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification by sparse additive models. (arXiv:2212.01792v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01792
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#38750;&#21442;&#25968;&#30340;&#31232;&#30095;&#21152;&#24615;&#27169;&#22411;&#29992;&#20110;&#20998;&#31867;&#65292;&#36890;&#36807;&#23545;&#20998;&#37327;&#31995;&#25968;&#26045;&#21152;&#31232;&#30095;&#32452;Lasso&#21644;&#31232;&#30095;&#32452;Slope&#22411;&#24809;&#32602;&#26469;&#35774;&#35745;&#20998;&#31867;&#22120;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#20998;&#31867;&#22120;&#22312;&#26410;&#30693;&#31232;&#30095;&#24615;&#21644;&#24179;&#28369;&#24615;&#19978;&#30340;&#33258;&#36866;&#24212;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#29992;&#20110;&#20998;&#31867;&#30340;&#38750;&#21442;&#25968;&#31232;&#30095;&#21152;&#24615;&#27169;&#22411;&#65288;SpAM&#65289;&#12290;SpAM&#20998;&#31867;&#22120;&#30340;&#35774;&#35745;&#22522;&#20110;&#26368;&#23567;&#21270;logistic&#25439;&#22833;&#65292;&#36890;&#36807;&#23545;&#20998;&#37327;&#23637;&#24320;&#31995;&#25968;&#26045;&#21152;&#31232;&#30095;&#32452;Lasso&#21644;&#26356;&#19968;&#33324;&#30340;&#31232;&#30095;&#32452;Slope&#22411;&#24809;&#32602;&#65288;&#20363;&#22914;&#65292;&#20613;&#37324;&#21494;&#25110;&#23567;&#27874;&#65289;&#12290;&#25152;&#24471;&#30340;&#20998;&#31867;&#22120;&#23545;&#26410;&#30693;&#30340;&#31232;&#30095;&#24615;&#21644;&#24179;&#28369;&#24615;&#20855;&#26377;&#22266;&#26377;&#30340;&#33258;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#26576;&#20123;&#31232;&#30095;&#32452;&#21463;&#38480;&#29305;&#24449;&#20540;&#26465;&#20214;&#19979;&#65292;&#31232;&#30095;&#32452;Lasso&#20998;&#31867;&#22120;&#22312;&#25972;&#20010;&#35299;&#26512;&#12289;Sobolev&#21644;Besov&#31867;&#33539;&#22260;&#20869;&#20960;&#20046;&#26159;&#26368;&#23567;&#21270;&#26497;&#23567;&#65288;&#21152;&#19978;&#23545;&#25968;&#22240;&#23376;&#65289;&#65292;&#32780;&#31232;&#30095;&#32452;Slope&#20998;&#31867;&#22120;&#22312;&#31232;&#30095;&#21644;&#36866;&#24230;&#31264;&#23494;&#35774;&#23450;&#19979;&#36798;&#21040;&#20102;&#30830;&#20999;&#30340;&#26368;&#23567;&#21270;&#26497;&#23567;&#38454;&#25968;&#65288;&#19981;&#21547;&#39069;&#22806;&#30340;&#23545;&#25968;&#22240;&#23376;&#65289;&#12290;&#35813;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#22312;&#23454;&#38469;&#25968;&#25454;&#20363;&#23376;&#20013;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider (nonparametric) sparse additive models (SpAM) for classification. The design of a SpAM classifier is based on minimizing the logistic loss with a sparse group Lasso and more general sparse group Slope-type penalties on the coefficients of univariate components' expansions in orthonormal series (e.g., Fourier or wavelets). The resulting classifiers are inherently adaptive to the unknown sparsity and smoothness. We show that under certain sparse group restricted eigenvalue condition the sparse group Lasso classifier is nearly-minimax (up to log-factors) within the entire range of analytic, Sobolev and Besov classes while the sparse group Slope classifier achieves the exact minimax order (without the extra log-factors) for sparse and moderately dense setups. The performance of the proposed classifier is illustrated on the real-data example.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;PAC-Bayesian&#29702;&#35770;&#25552;&#20379;&#20102;&#20803;&#23398;&#20064;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#26368;&#20339;&#24615;&#33021;&#20445;&#35777;&#30340;&#38381;&#24335;&#20248;&#21270;&#36229;&#21518;&#39564;(PACOH)&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#20445;&#35777;&#22312;&#20803;&#23398;&#20064;&#20013;&#30456;&#23545;&#20110;PAC-Bayesian&#27599;&#20010;&#20219;&#21153;&#23398;&#20064;&#30028;&#38480;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2211.07206</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;PAC-Bayesian&#20803;&#23398;&#20064;&#65306;&#20174;&#29702;&#35770;&#21040;&#23454;&#36341;&#30340;PAC-Optimal&#36229;&#21518;&#39564;
&lt;/p&gt;
&lt;p&gt;
Scalable PAC-Bayesian Meta-Learning via the PAC-Optimal Hyper-Posterior: From Theory to Practice. (arXiv:2211.07206v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;PAC-Bayesian&#29702;&#35770;&#25552;&#20379;&#20102;&#20803;&#23398;&#20064;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#26368;&#20339;&#24615;&#33021;&#20445;&#35777;&#30340;&#38381;&#24335;&#20248;&#21270;&#36229;&#21518;&#39564;(PACOH)&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#20445;&#35777;&#22312;&#20803;&#23398;&#20064;&#20013;&#30456;&#23545;&#20110;PAC-Bayesian&#27599;&#20010;&#20219;&#21153;&#23398;&#20064;&#30028;&#38480;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#20174;&#30456;&#20851;&#23398;&#20064;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#26377;&#29992;&#30340;&#24402;&#32435;&#20559;&#22909;&#65292;&#21152;&#36895;&#23545;&#26032;&#20219;&#21153;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#21487;&#29992;&#30340;&#30456;&#20851;&#20219;&#21153;&#25968;&#37327;&#36890;&#24120;&#24456;&#23567;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20551;&#35774;&#20219;&#21153;&#25968;&#37327;&#20016;&#23500;&#65292;&#20351;&#23427;&#20204;&#19981;&#20999;&#23454;&#38469;&#19988;&#23481;&#26131;&#36807;&#25311;&#21512;&#12290;&#20803;&#23398;&#20064;&#25991;&#29486;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#22914;&#20309;&#36827;&#34892;&#27491;&#21017;&#21270;&#20197;&#30830;&#20445;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;PAC-Bayesian&#29702;&#35770;&#25552;&#20379;&#20102;&#19968;&#31181;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#20803;&#23398;&#20064;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#36825;&#26159;&#30001;Rothfuss&#31561;&#20154;&#65288;2021&#65289;&#39318;&#27425;&#25512;&#23548;&#20986;&#26469;&#30340;&#12290;&#20851;&#38190;&#26159;&#65292;&#35813;&#30028;&#38480;&#20351;&#25105;&#20204;&#33021;&#22815;&#24471;&#21040;&#26368;&#20339;&#24615;&#33021;&#20445;&#35777;&#30340;&#38381;&#24335;&#20248;&#21270;&#36229;&#21518;&#39564;&#65292;&#31216;&#20026;PACOH&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#26696;&#20363;&#30740;&#31350;&#65292;&#22312;&#21738;&#20123;&#26465;&#20214;&#19979;&#20197;&#21450;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#36825;&#20123;&#20803;&#23398;&#20064;&#30340;&#20445;&#35777;&#25913;&#36827;&#20102;PAC-Bayesian&#27599;&#20010;&#20219;&#21153;&#23398;&#20064;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-Learning aims to speed up the learning process on new tasks by acquiring useful inductive biases from datasets of related learning tasks. While, in practice, the number of related tasks available is often small, most of the existing approaches assume an abundance of tasks; making them unrealistic and prone to overfitting. A central question in the meta-learning literature is how to regularize to ensure generalization to unseen tasks. In this work, we provide a theoretical analysis using the PAC-Bayesian theory and present a generalization bound for meta-learning, which was first derived by Rothfuss et al. (2021). Crucially, the bound allows us to derive the closed form of the optimal hyper-posterior, referred to as PACOH, which leads to the best performance guarantees. We provide a theoretical analysis and empirical case study under which conditions and to what extent these guarantees for meta-learning improve upon PAC-Bayesian per-task learning bounds. The closed-form PACOH inspi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#20984;&#37197;&#23545;&#34701;&#21512;&#30340;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#33258;&#21160;&#35782;&#21035;&#32858;&#31867;&#32467;&#26500;&#65292;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#65292;&#24182;&#30830;&#20445;&#38544;&#31169;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.04218</link><description>&lt;p&gt;
&#22522;&#20110;&#38750;&#20984;&#37197;&#23545;&#34701;&#21512;&#30340;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Clustered Federated Learning based on Nonconvex Pairwise Fusion. (arXiv:2211.04218v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#20984;&#37197;&#23545;&#34701;&#21512;&#30340;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#33258;&#21160;&#35782;&#21035;&#32858;&#31867;&#32467;&#26500;&#65292;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#65292;&#24182;&#30830;&#20445;&#38544;&#31169;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#65292;&#21363;&#20855;&#26377;&#38750;i.i.d.&#25968;&#25454;&#30340;FL&#30340;&#19968;&#31181;&#24418;&#24335;&#12290;&#22312;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#35774;&#22791;&#34987;&#20998;&#25104;&#32858;&#31867;&#65292;&#24182;&#19988;&#27599;&#20010;&#32858;&#31867;&#36890;&#36807;&#20351;&#29992;&#26412;&#22320;&#27169;&#22411;&#26469;&#26368;&#20248;&#22320;&#21305;&#37197;&#20854;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#38750;&#20984;&#24809;&#32602;&#20197;&#37197;&#23545;&#21442;&#25968;&#24046;&#24322;&#30340;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#32858;&#31867;&#32467;&#26500;&#65292;&#26080;&#38656;&#39044;&#20808;&#30693;&#36947;&#32858;&#31867;&#30340;&#25968;&#37327;&#21644;&#27599;&#20010;&#32858;&#31867;&#20013;&#30340;&#35774;&#22791;&#38598;&#21512;&#12290;&#20026;&#20102;&#23454;&#29616;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#34701;&#21512;&#24809;&#32602;&#32852;&#37030;&#32858;&#31867;&#65288;FPFC&#65289;&#30340;&#26032;&#22411;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;&#22522;&#20110;&#26631;&#20934;&#30340;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#26041;&#27861;&#65288;ADMM&#65289;&#65292;FPFC&#22312;&#24182;&#34892;&#20013;&#23454;&#26045;&#65292;&#20165;&#22312;&#27599;&#36718;&#36890;&#20449;&#20013;&#26356;&#26032;&#35774;&#22791;&#30340;&#23376;&#38598;&#65292;&#24182;&#20801;&#35768;&#27599;&#20010;&#35774;&#22791;&#30340;&#21487;&#21464;&#24037;&#20316;&#36127;&#36733;&#12290;&#36825;&#20123;&#31574;&#30053;&#26174;&#33879;&#38477;&#20302;&#20102;&#36890;&#20449;&#25104;&#26412;&#65292;&#21516;&#26102;&#30830;&#20445;&#38544;&#31169;&#24615;&#65292;&#20351;&#20854;&#22312;FL&#20013;&#23454;&#38469;&#21487;&#34892;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#28909;&#31574;&#30053;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates clustered federated learning (FL), one of the formulations of FL with non-i.i.d. data, where the devices are partitioned into clusters and each cluster optimally fits its data with a localized model. We propose a clustered FL framework that incorporates a nonconvex penalty to pairwise differences of parameters. This framework can automatically identify cluster structures without a priori knowledge of the number of clusters and the set of devices in each cluster. To implement the proposed framework, we introduce a novel clustered FL method called Fusion Penalized Federated Clustering (FPFC). Building upon the standard alternating direction method of multipliers (ADMM), FPFC is implemented in parallel, updates only a subset of devices at each communication round, and allows for variable workload per device. These strategies significantly reduce the communication cost while ensuring privacy, making it practical for FL. We also propose a new warmup strategy for hype
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#32479;&#19968;&#21152;&#20493;&#26465;&#20214;&#65292;&#20197;&#30830;&#20445;&#22312;&#35757;&#32451;DNN&#26399;&#38388;&#20934;&#30830;&#24615;&#30340;&#31283;&#23450;&#24615;&#65292;&#26080;&#35770;&#20351;&#29992;&#32477;&#23545;&#20540;&#28608;&#27963;&#20989;&#25968;&#36824;&#26159;ReLU&#28608;&#27963;&#20989;&#25968;&#12290;&#22312;&#32479;&#19968;&#21152;&#20493;&#26465;&#20214;&#19979;&#65292;&#20934;&#30830;&#29575;&#26377;&#24456;&#39640;&#30340;&#27010;&#29575;&#31283;&#23450;&#65292;&#24182;&#25552;&#20379;&#20102;&#20855;&#20307;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2210.08415</link><description>&lt;p&gt;
&#36890;&#36807;&#32479;&#19968;&#21152;&#20493;&#26465;&#20214;&#31283;&#23450;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Stability of Accuracy for the Training of DNNs Via the Uniform Doubling Condition. (arXiv:2210.08415v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#32479;&#19968;&#21152;&#20493;&#26465;&#20214;&#65292;&#20197;&#30830;&#20445;&#22312;&#35757;&#32451;DNN&#26399;&#38388;&#20934;&#30830;&#24615;&#30340;&#31283;&#23450;&#24615;&#65292;&#26080;&#35770;&#20351;&#29992;&#32477;&#23545;&#20540;&#28608;&#27963;&#20989;&#25968;&#36824;&#26159;ReLU&#28608;&#27963;&#20989;&#25968;&#12290;&#22312;&#32479;&#19968;&#21152;&#20493;&#26465;&#20214;&#19979;&#65292;&#20934;&#30830;&#29575;&#26377;&#24456;&#39640;&#30340;&#27010;&#29575;&#31283;&#23450;&#65292;&#24182;&#25552;&#20379;&#20102;&#20855;&#20307;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#35757;&#32451;&#36807;&#31243;&#20013;&#20934;&#30830;&#24615;&#30340;&#31283;&#23450;&#24615;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;DNNs&#30340;&#35757;&#32451;&#36890;&#36807;&#26368;&#23567;&#21270;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#26469;&#36827;&#34892;&#65292;&#20854;&#24615;&#33021;&#25351;&#26631;&#26159;&#20934;&#30830;&#24615;&#65288;&#27491;&#30830;&#20998;&#31867;&#30340;&#23545;&#35937;&#27604;&#20363;&#65289;&#12290;&#34429;&#28982;&#35757;&#32451;&#20250;&#23548;&#33268;&#25439;&#22833;&#20943;&#23569;&#65292;&#20294;&#26159;&#20934;&#30830;&#24615;&#19981;&#19968;&#23450;&#38543;&#30528;&#36807;&#31243;&#22686;&#21152;&#65292;&#26377;&#26102;&#29978;&#33267;&#20250;&#19979;&#38477;&#12290;&#23454;&#29616;&#20934;&#30830;&#24615;&#31283;&#23450;&#24615;&#30340;&#30446;&#26631;&#26159;&#30830;&#20445;&#22914;&#26524;&#21021;&#22987;&#26102;&#20934;&#30830;&#24615;&#24456;&#39640;&#65292;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#39640;&#27700;&#24179;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#35757;&#32451;&#25968;&#25454;&#30340;&#21152;&#20493;&#26465;&#20214;&#65292;&#20197;&#30830;&#20445;&#20351;&#29992;&#32477;&#23545;&#20540;&#28608;&#27963;&#20989;&#25968;&#30340;DNN&#30340;&#35757;&#32451;&#26399;&#38388;&#30340;&#20934;&#30830;&#24615;&#31283;&#23450;&#12290;&#23545;&#20110;&#22312;$R^n$&#20013;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#31181;&#21152;&#20493;&#26465;&#20214;&#20351;&#29992;$R^n$&#20013;&#30340;&#26495;&#22359;&#36827;&#34892;&#21046;&#23450;&#65292;&#24182;&#19988;&#21462;&#20915;&#20110;&#26495;&#22359;&#30340;&#36873;&#25321;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#20004;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#24341;&#20837;&#26356;&#31616;&#21333;&#12289;&#26356;&#36890;&#29992;&#30340;&#26465;&#20214;&#8212;&#8212;&#32479;&#19968;&#21152;&#20493;&#26465;&#20214;&#65292;&#20351;&#21152;&#20493;&#26465;&#20214;&#26041;&#27861;&#26356;&#26131;&#20110;&#29702;&#35299;&#12290;&#20854;&#27425;&#65292;&#21033;&#29992;&#36825;&#20010;&#26032;&#26465;&#20214;&#35777;&#26126;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#26080;&#35770;&#26159;&#20351;&#29992;&#32477;&#23545;&#20540;&#28608;&#27963;&#20989;&#25968;&#36824;&#26159;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;DNN&#65292;&#20854;&#20934;&#30830;&#24615;&#37117;&#20445;&#25345;&#19981;&#21464;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#65292;&#22312;&#32479;&#19968;&#21152;&#20493;&#26465;&#20214;&#19979;&#65292;&#20934;&#30830;&#24615;&#22312;&#35757;&#32451;&#26399;&#38388;&#26377;&#24456;&#39640;&#30340;&#27010;&#29575;&#20445;&#25345;&#31283;&#23450;&#65292;&#24182;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#27010;&#29575;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the stability of accuracy during the training of deep neural networks (DNNs). In this context, the training of a DNN is performed via the minimization of a cross-entropy loss function, and the performance metric is accuracy (the proportion of objects that are classified correctly). While training results in a decrease of loss, the accuracy does not necessarily increase during the process and may sometimes even decrease. The goal of achieving stability of accuracy is to ensure that if accuracy is high at some initial time, it remains high throughout training.  A recent result by Berlyand, Jabin, and Safsten introduces a doubling condition on the training data, which ensures the stability of accuracy during training for DNNs using the absolute value activation function. For training data in $\mathbb{R}^n$, this doubling condition is formulated using slabs in $\mathbb{R}^n$ and depends on the choice of the slabs. The goal of this paper is twofold. First, to make the doubling cond
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#27604;&#29616;&#26377;&#30340;&#25688;&#35201;&#35780;&#20272;&#25351;&#26631;&#26356;&#22909;&#65292;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#19988;&#26356;&#21152;&#40065;&#26834;&#65292;&#19982;&#29616;&#26377;&#25351;&#26631;&#30456;&#32467;&#21512;&#21487;&#20197;&#20351;&#35780;&#20272;&#25928;&#26524;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2208.07316</link><description>&lt;p&gt;
MENLI: &#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
MENLI: Robust Evaluation Metrics from Natural Language Inference. (arXiv:2208.07316v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#27604;&#29616;&#26377;&#30340;&#25688;&#35201;&#35780;&#20272;&#25351;&#26631;&#26356;&#22909;&#65292;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#19988;&#26356;&#21152;&#40065;&#26834;&#65292;&#19982;&#29616;&#26377;&#25351;&#26631;&#30456;&#32467;&#21512;&#21487;&#20197;&#20351;&#35780;&#20272;&#25928;&#26524;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#34987;&#25552;&#20986;&#30340;&#22522;&#20110;BERT&#30340;&#25991;&#26412;&#29983;&#25104;&#35780;&#20272;&#25351;&#26631;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#26131;&#21463;&#21040;&#23545;&#20449;&#24687;&#27491;&#30830;&#24615;&#30340;&#25915;&#20987;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#37096;&#20998;&#21407;&#22240;&#26159;&#27492;&#31867;&#27169;&#22411;&#26159;&#22522;&#20110;&#35821;&#20041;&#30456;&#20284;&#24615;&#24314;&#27169;&#30340;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#36825;&#31181;&#25351;&#26631;&#26356;&#36866;&#21512;&#24314;&#27169;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#22909;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26694;&#26550;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;NLI&#22522;&#30784;&#25351;&#26631;&#27604;&#26368;&#36817;&#30340;BERT&#22522;&#30784;&#25351;&#26631;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;NLI&#22522;&#30784;&#25351;&#26631;&#20248;&#20110;&#29616;&#26377;&#30340;&#25688;&#35201;&#35780;&#20272;&#25351;&#26631;&#65292;&#20294;&#20302;&#20110;SOTA MT&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#26377;&#25351;&#26631;&#19982;&#25105;&#20204;&#30340;NLI&#25351;&#26631;&#30456;&#32467;&#21512;&#26102;&#65292;&#25105;&#20204;&#26082;&#33719;&#24471;&#20102;&#26356;&#39640;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65288;15&#65285;-30&#65285;&#65289;&#65292;&#21448;&#33719;&#24471;&#20102;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#26356;&#39640;&#30340;&#36136;&#37327;&#25351;&#26631;&#65288;+5&#65285;&#33267;30&#65285;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently proposed BERT-based evaluation metrics for text generation perform well on standard benchmarks but are vulnerable to adversarial attacks, e.g., relating to information correctness. We argue that this stems (in part) from the fact that they are models of semantic similarity. In contrast, we develop evaluation metrics based on Natural Language Inference (NLI), which we deem a more appropriate modeling. We design a preference-based adversarial attack framework and show that our NLI based metrics are much more robust to the attacks than the recent BERT-based metrics. On standard benchmarks, our NLI based metrics outperform existing summarization metrics, but perform below SOTA MT metrics. However, when combining existing metrics with our NLI metrics, we obtain both higher adversarial robustness (15%-30%) and higher quality metrics as measured on standard benchmarks (+5% to 30%).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#38598;&#21512;&#39044;&#27979;&#21644;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#23558;&#31995;&#32479;&#23884;&#20837;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26063;&#65292;&#24182;&#22312;&#35813;&#31354;&#38388;&#20013;&#20351;&#29992;&#31616;&#21333;&#30340;&#38598;&#21512;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#36827;&#34892;&#36712;&#36857;&#37325;&#26500;&#12290;</title><link>http://arxiv.org/abs/2207.14653</link><description>&lt;p&gt;
&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#31995;&#21015;&#20013;&#30340;&#38598;&#25104;&#39044;&#27979;&#65306;&#20185;&#22659;&#20013;&#30340;&#21160;&#21147;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensemble forecasts in reproducing kernel Hilbert space family: dynamical systems in Wonderland. (arXiv:2207.14653v2 [math-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.14653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#38598;&#21512;&#39044;&#27979;&#21644;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#23558;&#31995;&#32479;&#23884;&#20837;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26063;&#65292;&#24182;&#22312;&#35813;&#31354;&#38388;&#20013;&#20351;&#29992;&#31616;&#21333;&#30340;&#38598;&#21512;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#36827;&#34892;&#36712;&#36857;&#37325;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28023;&#27915;&#25110;&#22823;&#27668;&#27969;&#31561;&#39640;&#32500;&#21160;&#21147;&#31995;&#32479;&#30340;&#38598;&#21512;&#20272;&#35745;&#21644;&#27169;&#25311;&#30340;&#26041;&#27861;&#26694;&#26550;&#12290;&#20026;&#27492;&#65292;&#23558;&#35813;&#21160;&#21147;&#31995;&#32479;&#23884;&#20837;&#30001;&#21160;&#21147;&#39537;&#21160;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26063;&#20013;&#12290;&#36825;&#20010;&#23478;&#26063;&#22240;&#20854;&#21560;&#24341;&#20154;&#30340;&#29305;&#24615;&#32780;&#34987;&#21629;&#21517;&#20026;&#20185;&#22659;&#12290;&#22312;&#20185;&#22659;&#20013;&#65292;Koopman&#21644;Perron-Frobenius&#31639;&#23376;&#26159;&#37193;&#30340;&#21644;&#19968;&#33268;&#36830;&#32493;&#30340;&#12290;&#36825;&#20010;&#23646;&#24615;&#20445;&#35777;&#23427;&#20204;&#21487;&#20197;&#29992;&#23545;&#35282;&#21270;&#26377;&#30028;&#26080;&#31351;&#23567;&#29983;&#25104;&#22120;&#30340;&#25351;&#25968;&#32423;&#32423;&#25968;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#21487;&#20197;&#30452;&#25509;&#33719;&#24471;&#23545;Lyapunov&#25351;&#25968;&#21644;&#20999;&#32447;&#32447;&#24615;&#21160;&#24577;&#30340;&#31934;&#30830;&#38598;&#21512;&#24335;&#34920;&#36798;&#24335;&#12290;&#20185;&#22659;&#20351;&#25105;&#20204;&#33021;&#22815;&#35774;&#35745;&#20986;&#26497;&#20854;&#31616;&#21333;&#30340;&#38598;&#21512;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#20197;&#36712;&#36857;&#26679;&#26412;&#30340;&#24658;&#23450;&#26102;&#38388;&#32447;&#24615;&#32452;&#21512;&#26469;&#36827;&#34892;&#36712;&#36857;&#37325;&#26500;&#12290;&#36825;&#31181;&#20196;&#20154;&#23604;&#23596;&#30340;&#31616;&#21333;&#31574;&#30053;&#24471;&#20197;&#23454;&#29616;&#65292;&#26159;&#36890;&#36807;Hilbert&#31354;&#38388;&#35774;&#32622;&#20013;&#20999;&#32447;&#32447;&#24615;&#21160;&#21147;&#30340;&#23436;&#20840;&#21512;&#29702;&#30340;&#21472;&#21152;&#21407;&#29702;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
A methodological framework for ensemble-based estimation and simulation of high dimensional dynamical systems such as the oceanic or atmospheric flows is proposed. To that end, the dynamical system is embedded in a family of reproducing kernel Hilbert spaces with kernel functions driven by the dynamics. This family is nicknamed Wonderland for its appealing properties. In Wonderland the Koopman and Perron-Frobenius operators are unitary and uniformly continuous. This property warrants they can be expressed in exponential series of diagonalizable bounded infinitesimal generators. Access to Lyapunov exponents and to exact ensemble based expressions of the tangent linear dynamics are directly available as well. Wonderland enables us the devise of strikingly simple ensemble data assimilation methods for trajectory reconstructions in terms of constant-in-time linear combinations of trajectory samples. Such an embarrassingly simple strategy is made possible through a fully justified superposi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;Hessian-based&#20998;&#26512;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#36317;&#31163;&#30340;&#27867;&#21270;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#29702;&#35299;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24494;&#35843;&#30340;&#27867;&#21270;&#29305;&#24615;&#12290;&#36890;&#36807;PAC-Bayesian&#20998;&#26512;&#65292;&#32473;&#20986;&#20102;&#22522;&#20110;Hessian&#36317;&#31163;&#30340;&#24494;&#35843;&#27169;&#22411;&#27867;&#21270;&#30028;&#12290;&#27492;&#22806;&#65292;&#36824;&#23545;&#24494;&#35843;&#38754;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#30456;&#20851;&#31639;&#27861;&#21644;&#27867;&#21270;&#35823;&#24046;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2206.02659</link><description>&lt;p&gt;
&#20855;&#26377;&#22522;&#20110;Hessian&#30340;&#27867;&#21270;&#20445;&#35777;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Robust Fine-Tuning of Deep Neural Networks with Hessian-based Generalization Guarantees. (arXiv:2206.02659v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;Hessian-based&#20998;&#26512;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#36317;&#31163;&#30340;&#27867;&#21270;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#29702;&#35299;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24494;&#35843;&#30340;&#27867;&#21270;&#29305;&#24615;&#12290;&#36890;&#36807;PAC-Bayesian&#20998;&#26512;&#65292;&#32473;&#20986;&#20102;&#22522;&#20110;Hessian&#36317;&#31163;&#30340;&#24494;&#35843;&#27169;&#22411;&#27867;&#21270;&#30028;&#12290;&#27492;&#22806;&#65292;&#36824;&#23545;&#24494;&#35843;&#38754;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#30456;&#20851;&#31639;&#27861;&#21644;&#27867;&#21270;&#35823;&#24046;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#23545;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#30740;&#31350;&#24494;&#35843;&#30340;&#27867;&#21270;&#29305;&#24615;&#65292;&#20197;&#29702;&#35299;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#36825;&#22312;&#30446;&#26631;&#25968;&#25454;&#38598;&#36739;&#23567;&#25110;&#35757;&#32451;&#26631;&#31614;&#22122;&#22768;&#26102;&#32463;&#24120;&#35266;&#23519;&#21040;&#12290;&#29616;&#26377;&#30340;&#28145;&#24230;&#32593;&#32476;&#27867;&#21270;&#24230;&#37327;&#20381;&#36182;&#20110;&#19982;&#24494;&#35843;&#27169;&#22411;&#30340;&#21021;&#22987;&#21270;&#65288;&#21363;&#39044;&#35757;&#32451;&#32593;&#32476;&#65289;&#36317;&#31163;&#21644;&#28145;&#24230;&#32593;&#32476;&#30340;&#22122;&#22768;&#31283;&#23450;&#24615;&#31561;&#27010;&#24565;&#12290;&#26412;&#25991;&#36890;&#36807;PAC-Bayesian&#20998;&#26512;&#30830;&#23450;&#20102;&#19968;&#31181;&#22522;&#20110;Hessian&#30340;&#36317;&#31163;&#24230;&#37327;&#65292;&#23427;&#19982;&#24494;&#35843;&#27169;&#22411;&#30340;&#35266;&#23519;&#21040;&#30340;&#27867;&#21270;&#24046;&#36317;&#30456;&#20851;&#24615;&#24456;&#24378;&#12290;&#20174;&#29702;&#35770;&#19978;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;Hessian&#36317;&#31163;&#30340;&#24494;&#35843;&#27169;&#22411;&#30340;&#27867;&#21270;&#30028;&#12290;&#25105;&#20204;&#36824;&#23545;&#24494;&#35843;&#23545;&#25239;&#26631;&#31614;&#22122;&#22768;&#36827;&#34892;&#20102;&#25193;&#23637;&#30740;&#31350;&#65292;&#36807;&#25311;&#21512;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65307;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#24182;&#22312;&#31867;&#26465;&#20214;&#29420;&#31435;&#20551;&#35774;&#19979;&#32473;&#20986;&#20102;&#35813;&#31639;&#27861;&#30340;&#27867;&#21270;&#35823;&#24046;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider fine-tuning a pretrained deep neural network on a target task. We study the generalization properties of fine-tuning to understand the problem of overfitting, which has often been observed (e.g., when the target dataset is small or when the training labels are noisy). Existing generalization measures for deep networks depend on notions such as distance from the initialization (i.e., the pretrained network) of the fine-tuned model and noise stability properties of deep networks. This paper identifies a Hessian-based distance measure through PAC-Bayesian analysis, which is shown to correlate well with observed generalization gaps of fine-tuned models. Theoretically, we prove Hessian distance-based generalization bounds for fine-tuned models. We also describe an extended study of fine-tuning against label noise, where overfitting is against a critical problem; We present an algorithm and a generalization error guarantee for this algorithm under a class conditional independent 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35774;&#22791;&#19978;&#23398;&#20064;&#26041;&#27861;&#65292;&#38024;&#23545;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#30340;&#29615;&#22659;&#22240;&#32032;&#23545;&#20934;&#30830;&#24615;&#36896;&#25104;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#35299;&#20915;&#12290;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#65292;&#22312;&#22024;&#26434;&#29615;&#22659;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#24322;&#24120;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#33410;&#32422;&#20102;&#20302;&#21151;&#32791;&#35774;&#22791;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2203.01077</link><description>&lt;p&gt;
&#35774;&#22791;&#19978;&#23398;&#20064;&#65306;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35757;&#32451;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
On-Device Learning: A Neural Network Based Field-Trainable Edge AI. (arXiv:2203.01077v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.01077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35774;&#22791;&#19978;&#23398;&#20064;&#26041;&#27861;&#65292;&#38024;&#23545;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#30340;&#29615;&#22659;&#22240;&#32032;&#23545;&#20934;&#30830;&#24615;&#36896;&#25104;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#35299;&#20915;&#12290;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#65292;&#22312;&#22024;&#26434;&#29615;&#22659;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#24322;&#24120;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#33410;&#32422;&#20102;&#20302;&#21151;&#32791;&#35774;&#22791;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#65292;&#20854;&#20934;&#30830;&#24615;&#32463;&#24120;&#21463;&#21040;&#21508;&#31181;&#29615;&#22659;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#22914;&#22122;&#22768;&#12289;&#20256;&#24863;&#22120;&#30340;&#20301;&#32622;/&#26657;&#20934;&#21644;&#26102;&#38388;&#30456;&#20851;&#30340;&#21464;&#21270;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35774;&#22791;&#19978;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#32780;&#19981;&#38656;&#35201;&#28145;&#20837;&#20102;&#35299;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20107;&#23454;&#19978;&#30340;&#21453;&#21521;&#20256;&#25773;&#35757;&#32451;&#26377;&#24456;&#22823;&#21306;&#21035;&#65292;&#32780;&#26159;&#19987;&#20026;&#20302;&#31471;&#36793;&#32536;&#35774;&#22791;&#37327;&#36523;&#23450;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20854;&#31639;&#27861;&#21644;&#22312;&#30001;&#26641;&#33683;&#27966;Pico&#21644;&#20302;&#21151;&#32791;&#26080;&#32447;&#27169;&#22359;&#32452;&#25104;&#30340;&#26080;&#32447;&#20256;&#24863;&#22120;&#33410;&#28857;&#19978;&#30340;&#23454;&#29616;&#12290;&#36890;&#36807;&#20351;&#29992;&#26059;&#36716;&#26426;&#22120;&#30340;&#25391;&#21160;&#27169;&#24335;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65292;&#36890;&#36807;&#35774;&#22791;&#19978;&#23398;&#20064;&#30340;&#37325;&#26032;&#35757;&#32451;&#22312;&#22024;&#26434;&#29615;&#22659;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#24322;&#24120;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#33410;&#32422;&#20102;&#20302;&#21151;&#32791;&#35774;&#22791;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world edge AI applications, their accuracy is often affected by various environmental factors, such as noises, location/calibration of sensors, and time-related changes. This article introduces a neural network based on-device learning approach to address this issue without going deep. Our approach is quite different from de facto backpropagation based training but tailored for low-end edge devices. This article introduces its algorithm and implementation on a wireless sensor node consisting of Raspberry Pi Pico and low-power wireless module. Experiments using vibration patterns of rotating machines demonstrate that retraining by the on-device learning significantly improves an anomaly detection accuracy at a noisy environment while saving computation and communication costs for low power.
&lt;/p&gt;</description></item></channel></rss>