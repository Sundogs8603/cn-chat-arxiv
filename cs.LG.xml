<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#31639;&#27861;(&#22914;Adagrad&#21644;Adam)&#30340;&#23398;&#20064;&#29575;&#20272;&#35745;&#26041;&#27861;Prodigy&#21644;Resetting&#65292;&#21487;&#20197;&#24555;&#36895;&#19988;&#27491;&#30830;&#22320;&#20272;&#35745;&#21040;&#36798;&#35299;&#20915;&#26041;&#26696;&#25152;&#38656;&#30340;&#36317;&#31163;D&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;D-Adaptation&#24182;&#21487;&#36798;&#21040;&#25163;&#21160;&#35843;&#25972;Adam&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.06101</link><description>&lt;p&gt;
Prodigy: &#19968;&#31181;&#24555;&#36895;&#33258;&#36866;&#24212;&#38646;&#21442;&#25968;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prodigy: An Expeditiously Adaptive Parameter-Free Learner. (arXiv:2306.06101v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#31639;&#27861;(&#22914;Adagrad&#21644;Adam)&#30340;&#23398;&#20064;&#29575;&#20272;&#35745;&#26041;&#27861;Prodigy&#21644;Resetting&#65292;&#21487;&#20197;&#24555;&#36895;&#19988;&#27491;&#30830;&#22320;&#20272;&#35745;&#21040;&#36798;&#35299;&#20915;&#26041;&#26696;&#25152;&#38656;&#30340;&#36317;&#31163;D&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;D-Adaptation&#24182;&#21487;&#36798;&#21040;&#25163;&#21160;&#35843;&#25972;Adam&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#33258;&#36866;&#24212;&#31639;&#27861;(&#22914;Adagrad&#21644;Adam)&#20013;&#30340;&#23398;&#20064;&#29575;&#20272;&#35745;&#38382;&#39064;&#65292;&#25551;&#36848;&#20102;&#20004;&#31181;&#25216;&#26415;Prodigy&#21644;Resetting&#65292;&#21487;&#20197;&#35777;&#26126;&#22320;&#20272;&#35745;&#21040;&#36798;&#35299;&#20915;&#26041;&#26696;&#25152;&#38656;&#30340;&#36317;&#31163;D&#65292;&#20197;&#20415;&#26368;&#20248;&#35774;&#32622;&#23398;&#20064;&#29575;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#26159;&#22522;&#20110;&#23398;&#20064;&#29575;&#33258;&#30001;&#30340;D-Adaptation&#26041;&#27861;&#30340;&#20462;&#25913;&#65292;&#24182;&#36890;&#36807;$O(\sqrt{\log(D/d_0)})$&#30340;&#22240;&#23376;&#25552;&#39640;&#20102;D-Adaptation&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20854;&#20013;$d_0$&#26159;$D$&#30340;&#21021;&#22987;&#20272;&#35745;&#20540;&#12290;&#25105;&#20204;&#22312;12&#20010;&#24120;&#35265;&#30340;&#36923;&#36753;&#22238;&#24402;&#22522;&#20934;&#25968;&#25454;&#38598;&#12289;&#22312;CIFAR10&#19978;&#35757;&#32451;&#30340;VGG11&#21644;ResNet-50&#12289;&#22312;Imagenet&#19978;&#35757;&#32451;&#30340;ViT&#12289;&#22312;IWSLT14&#19978;&#35757;&#32451;&#30340;LSTM&#12289;&#22312;Criteo&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;DLRM&#12289;&#22312;Knee MRI&#25968;&#25454;&#38598;&#19978;&#30340;VarNet&#65292;&#20197;&#21450;&#22312;BookWiki&#19978;&#35757;&#32451;&#30340;RoBERTa&#21644;GPT transformer&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;D-Adaptation&#65292;&#24182;&#36798;&#21040;&#25163;&#21160;&#35843;&#25972;Adam&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of estimating the learning rate in adaptive methods, such as Adagrad and Adam. We describe two techniques, Prodigy and Resetting, to provably estimate the distance to the solution $D$, which is needed to set the learning rate optimally. Our techniques are modifications of the D-Adaptation method for learning-rate-free learning. Our methods improve upon the convergence rate of D-Adaptation by a factor of $O(\sqrt{\log(D/d_0)})$, where $d_0$ is the initial estimate of $D$. We test our methods on 12 common logistic-regression benchmark datasets, VGG11 and ResNet-50 training on CIFAR10, ViT training on Imagenet, LSTM training on IWSLT14, DLRM training on Criteo dataset, VarNet on Knee MRI dataset, as well as RoBERTa and GPT transformer training on BookWiki. Our experimental results show that our approaches consistently outperform D-Adaptation and reach test accuracy values close to that of hand-tuned Adam.
&lt;/p&gt;</description></item><item><title>NuCLR&#26159;&#19968;&#31181;&#21487;&#20197;&#39044;&#27979;&#21508;&#31181;&#26680;&#21487;&#35266;&#27979;&#37327;&#65292;&#21253;&#25324;&#32467;&#21512;&#33021;&#12289;&#34928;&#21464;&#33021;&#21644;&#26680;&#30005;&#33655;&#21322;&#24452;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#20849;&#20139;&#34920;&#31034;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#25429;&#25417;&#21040;&#26680;&#29289;&#29702;&#30340;&#22522;&#30784;&#29289;&#29702;&#21407;&#29702;&#65292;&#24182;&#26377;&#28508;&#21147;&#20026;&#26680;&#29702;&#35770;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.06099</link><description>&lt;p&gt;
NuCLR&#65306;&#26680;&#20849;&#23398;&#20064;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
NuCLR: Nuclear Co-Learned Representations. (arXiv:2306.06099v1 [nucl-th])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06099
&lt;/p&gt;
&lt;p&gt;
NuCLR&#26159;&#19968;&#31181;&#21487;&#20197;&#39044;&#27979;&#21508;&#31181;&#26680;&#21487;&#35266;&#27979;&#37327;&#65292;&#21253;&#25324;&#32467;&#21512;&#33021;&#12289;&#34928;&#21464;&#33021;&#21644;&#26680;&#30005;&#33655;&#21322;&#24452;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#20849;&#20139;&#34920;&#31034;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#25429;&#25417;&#21040;&#26680;&#29289;&#29702;&#30340;&#22522;&#30784;&#29289;&#29702;&#21407;&#29702;&#65292;&#24182;&#26377;&#28508;&#21147;&#20026;&#26680;&#29702;&#35770;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#26680;&#20849;&#23398;&#20064;&#34920;&#31034;&#65288;NuCLR&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;&#21508;&#31181;&#26680;&#21487;&#35266;&#27979;&#37327;&#65292;&#21253;&#25324;&#32467;&#21512;&#33021;&#12289;&#34928;&#21464;&#33021;&#21644;&#26680;&#30005;&#33655;&#21322;&#24452;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#29992;&#20849;&#20139;&#34920;&#31034;&#27861;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#29702;&#35299;&#26680;&#65288;&#22825;&#20307;&#65289;&#29289;&#29702;&#22522;&#26412;&#29616;&#35937;&#25152;&#24517;&#38656;&#30340;&#31934;&#24230;&#27700;&#24179;&#12290;&#25105;&#20204;&#36824;&#25253;&#21578;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#21457;&#29616;&#65292;&#21363;NuCLR&#23398;&#20064;&#30340;&#34920;&#31034;&#23637;&#29616;&#20986;&#26680;&#22771;&#27169;&#22411;&#30340;&#37325;&#35201;&#26041;&#38754;&#65292;&#21253;&#25324;&#22771;&#23618;&#32467;&#26500;&#65292;&#21253;&#25324;&#20247;&#25152;&#21608;&#30693;&#30340;&#39764;&#25968;&#21644;&#27873;&#21033;&#25490;&#26021;&#21407;&#29702;&#12290;&#36825;&#34920;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#21040;&#22522;&#30784;&#29289;&#29702;&#21407;&#29702;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#28508;&#21147;&#20026;&#26680;&#29702;&#35770;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Nuclear Co-Learned Representations (NuCLR), a deep learning model that predicts various nuclear observables, including binding and decay energies, and nuclear charge radii. The model is trained using a multi-task approach with shared representations and obtains state-of-the-art performance, achieving levels of precision that are crucial for understanding fundamental phenomena in nuclear (astro)physics. We also report an intriguing finding that the learned representation of NuCLR exhibits the prominent emergence of crucial aspects of the nuclear shell model, namely the shell structure, including the well-known magic numbers, and the Pauli Exclusion Principle. This suggests that the model is capable of capturing the underlying physical principles and that our approach has the potential to offer valuable insights into nuclear theory.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#38169;&#35823;&#21453;&#39304;&#25216;&#26415;&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;&#39304;&#20837;&#39044;&#22788;&#29702;&#22120;&#20043;&#21069;&#23545;&#26799;&#24230;&#20449;&#24687;&#36827;&#34892;&#21387;&#32553;&#65288;&#31232;&#30095;&#21270;&#25110;&#20302;&#31209;&#21387;&#32553;&#65289;&#65292;&#23558;&#39044;&#22788;&#29702;&#22120;&#30340;&#23384;&#20648;&#25104;&#26412;&#21387;&#32553;&#22810;&#36798;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#32780;&#19981;&#20250;&#20002;&#22833;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06098</link><description>&lt;p&gt;
&#38169;&#35823;&#21453;&#39304;&#21487;&#20197;&#20934;&#30830;&#22320;&#21387;&#32553;&#39044;&#22788;&#29702;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Error Feedback Can Accurately Compress Preconditioners. (arXiv:2306.06098v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#38169;&#35823;&#21453;&#39304;&#25216;&#26415;&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;&#39304;&#20837;&#39044;&#22788;&#29702;&#22120;&#20043;&#21069;&#23545;&#26799;&#24230;&#20449;&#24687;&#36827;&#34892;&#21387;&#32553;&#65288;&#31232;&#30095;&#21270;&#25110;&#20302;&#31209;&#21387;&#32553;&#65289;&#65292;&#23558;&#39044;&#22788;&#29702;&#22120;&#30340;&#23384;&#20648;&#25104;&#26412;&#21387;&#32553;&#22810;&#36798;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#32780;&#19981;&#20250;&#20002;&#22833;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#32593;&#32476;&#35268;&#27169;&#30340;&#20108;&#38454;&#20449;&#24687;&#26159;&#25913;&#36827;&#24403;&#21069;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#22120;&#24615;&#33021;&#30340;&#20027;&#35201;&#36884;&#24452;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31934;&#30830;&#20840;&#30697;&#38453;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#22914;&#20840;&#30697;&#38453;Adagrad&#65288;GGT&#65289;&#25110;&#26080;&#30697;&#38453;&#36817;&#20284;&#26354;&#29575;&#65288;M-FAC&#65289;&#65292;&#21363;&#20351;&#24212;&#29992;&#20110;&#20013;&#31561;&#35268;&#27169;&#27169;&#22411;&#65292;&#20063;&#20250;&#36935;&#21040;&#24040;&#22823;&#30340;&#23384;&#20648;&#25104;&#26412;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#24517;&#39035;&#23384;&#20648;&#26799;&#24230;&#30340;&#28369;&#21160;&#31383;&#21475;&#65292;&#20854;&#23384;&#20648;&#38656;&#27714;&#22312;&#27169;&#22411;&#32500;&#24230;&#20013;&#26159;&#25104;&#20493;&#22686;&#21152;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#31181;&#39640;&#25928;&#19988;&#26131;&#20110;&#23454;&#29616;&#30340;&#38169;&#35823;&#21453;&#39304;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#25216;&#26415;&#21487;&#20197;&#22312;&#23454;&#36341;&#20013;&#23558;&#39044;&#22788;&#29702;&#22120;&#21387;&#32553;&#22810;&#36798;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#32780;&#19981;&#20250;&#20002;&#22833;&#25910;&#25947;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23558;&#26799;&#24230;&#20449;&#24687;&#39304;&#20837;&#39044;&#22788;&#29702;&#22120;&#20043;&#21069;&#36890;&#36807;&#31232;&#30095;&#21270;&#25110;&#20302;&#31209;&#21387;&#32553;&#21387;&#32553;&#26799;&#24230;&#20449;&#24687;&#65292;&#23558;&#21387;&#32553;&#35823;&#24046;&#21453;&#39304;&#21040;&#26410;&#26469;&#30340;&#36845;&#20195;&#20013;&#12290;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging second-order information at the scale of deep networks is one of the main lines of approach for improving the performance of current optimizers for deep learning. Yet, existing approaches for accurate full-matrix preconditioning, such as Full-Matrix Adagrad (GGT) or Matrix-Free Approximate Curvature (M-FAC) suffer from massive storage costs when applied even to medium-scale models, as they must store a sliding window of gradients, whose memory requirements are multiplicative in the model dimension. In this paper, we address this issue via an efficient and simple-to-implement error-feedback technique that can be applied to compress preconditioners by up to two orders of magnitude in practice, without loss of convergence. Specifically, our approach compresses the gradient information via sparsification or low-rank compression \emph{before} it is fed into the preconditioner, feeding the compression error back into future iterations. Extensive experiments on deep neural networks
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#20280;&#32553;&#30690;&#37327;&#22270;(SVG)&#26684;&#24335;&#22788;&#29702;&#22270;&#20687;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#30452;&#25509;&#29702;&#35299;&#21644;&#25805;&#20316;&#22270;&#20687;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#21442;&#25968;&#21270;&#30340;&#35270;&#35273;&#32452;&#20214;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#29983;&#25104;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;&#20219;&#21153;&#19978;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.06094</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21487;&#20280;&#32553;&#30690;&#37327;&#22270;&#39537;&#21160;&#30340;&#22270;&#20687;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models for Scalable Vector Graphics-Driven Image Understanding. (arXiv:2306.06094v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#20280;&#32553;&#30690;&#37327;&#22270;(SVG)&#26684;&#24335;&#22788;&#29702;&#22270;&#20687;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#30452;&#25509;&#29702;&#35299;&#21644;&#25805;&#20316;&#22270;&#20687;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#21442;&#25968;&#21270;&#30340;&#35270;&#35273;&#32452;&#20214;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#29983;&#25104;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;&#20219;&#21153;&#19978;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#28508;&#21147;&#20173;&#28982;&#24456;&#22823;&#31243;&#24230;&#19978;&#27809;&#26377;&#34987;&#24320;&#21457;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25506;&#32034;&#24615;&#26041;&#27861;&#65292;&#20351;&#24471;LLMs&#33021;&#22815;&#20351;&#29992;&#21487;&#20280;&#32553;&#30690;&#37327;&#22270;(SVG)&#26684;&#24335;&#22788;&#29702;&#22270;&#20687;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;XML&#30340;SVG&#34920;&#36848;&#30340;&#25991;&#26412;&#25551;&#36848;&#32780;&#19981;&#26159;&#20809;&#26629;&#22270;&#20687;&#65292;&#25105;&#20204;&#26088;&#22312;&#24357;&#21512;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20351;LLMs&#33021;&#22815;&#30452;&#25509;&#29702;&#35299;&#21644;&#25805;&#20316;&#22270;&#20687;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#21442;&#25968;&#21270;&#30340;&#35270;&#35273;&#32452;&#20214;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#21033;&#29992;LLMs&#30340;&#33021;&#21147;&#36827;&#34892;&#31616;&#21333;&#30340;&#22270;&#20687;&#20998;&#31867;&#12289;&#29983;&#25104;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26041;&#27861;&#22312;&#21028;&#21035;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#20248;&#24322;&#34920;&#29616;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#23427;(i)&#23545;&#20998;&#24067;&#36716;&#31227;&#30340;&#40065;&#26834;&#24615;&#65292;(ii)&#36890;&#36807;&#21033;&#29992;LLMs&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#23454;&#29616;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#20197;&#21450;(iii)&#22270;&#20687;&#26434;&#20081;&#31243;&#24230;&#19978;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large language models (LLMs) have made significant advancements in natural language understanding and generation. However, their potential in computer vision remains largely unexplored. In this paper, we introduce a new, exploratory approach that enables LLMs to process images using the Scalable Vector Graphics (SVG) format. By leveraging the XML-based textual descriptions of SVG representations instead of raster images, we aim to bridge the gap between the visual and textual modalities, allowing LLMs to directly understand and manipulate images without the need for parameterized visual components. Our method facilitates simple image classification, generation, and in-context learning using only LLM capabilities. We demonstrate the promise of our approach across discriminative and generative tasks, highlighting its (i) robustness against distribution shift, (ii) substantial improvements achieved by tapping into the in-context learning abilities of LLMs, and (iii) image unders
&lt;/p&gt;</description></item><item><title>SENS&#26159;&#19968;&#31181;&#22522;&#20110;&#33609;&#22270;&#30340;&#29983;&#25104;&#21644;&#32534;&#36753;3D&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;ViT&#34917;&#19969;&#32534;&#30721;&#23558;&#33609;&#22270;&#26144;&#23556;&#21040;&#31070;&#32463;&#38544;&#24335;&#24418;&#29366;&#26550;&#26500;&#30340;&#28508;&#31354;&#38388;&#20013;&#65292;&#21487;&#20197;&#25429;&#25417;&#29992;&#25143;&#33609;&#22270;&#30340;&#24847;&#22270;&#36827;&#34892;&#29983;&#25104;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#24615;&#33021;&#21644;&#30452;&#35266;&#30340;&#22522;&#20110;&#33609;&#22270;&#30340;&#24418;&#29366;&#32534;&#36753;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.06088</link><description>&lt;p&gt;
SENS&#65306;&#22522;&#20110;&#33609;&#22270;&#30340;&#38544;&#24335;&#31070;&#32463;&#24418;&#29366;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
SENS: Sketch-based Implicit Neural Shape Modeling. (arXiv:2306.06088v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06088
&lt;/p&gt;
&lt;p&gt;
SENS&#26159;&#19968;&#31181;&#22522;&#20110;&#33609;&#22270;&#30340;&#29983;&#25104;&#21644;&#32534;&#36753;3D&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;ViT&#34917;&#19969;&#32534;&#30721;&#23558;&#33609;&#22270;&#26144;&#23556;&#21040;&#31070;&#32463;&#38544;&#24335;&#24418;&#29366;&#26550;&#26500;&#30340;&#28508;&#31354;&#38388;&#20013;&#65292;&#21487;&#20197;&#25429;&#25417;&#29992;&#25143;&#33609;&#22270;&#30340;&#24847;&#22270;&#36827;&#34892;&#29983;&#25104;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#24615;&#33021;&#21644;&#30452;&#35266;&#30340;&#22522;&#20110;&#33609;&#22270;&#30340;&#24418;&#29366;&#32534;&#36753;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SENS&#65292;&#19968;&#31181;&#20174;&#25163;&#32472;&#33609;&#22270;&#20013;&#29983;&#25104;&#21644;&#32534;&#36753;3D&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#21253;&#25324;&#37027;&#20123;&#25277;&#35937;&#30340;&#33609;&#22270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#29992;&#25143;&#24555;&#36895;&#36731;&#26494;&#22320;&#33609;&#32472;&#24418;&#29366;&#65292;&#28982;&#21518;&#23558;&#33609;&#22270;&#26144;&#23556;&#21040;&#19968;&#20010;&#38754;&#21521;&#37096;&#20214;&#30340;&#31070;&#32463;&#38544;&#24335;&#24418;&#29366;&#26550;&#26500;&#30340;&#28508;&#31354;&#38388;&#20013;&#12290;SENS&#20998;&#26512;&#33609;&#22270;&#24182;&#23558;&#20854;&#37096;&#20214;&#32534;&#30721;&#25104;ViT&#34917;&#19969;&#32534;&#30721;&#65292;&#28982;&#21518;&#23558;&#20854;&#39304;&#36865;&#21040;transformer&#35299;&#30721;&#22120;&#20013;&#65292;&#23558;&#20854;&#36716;&#25442;&#20026;&#36866;&#29992;&#20110;&#32534;&#36753;3D&#38544;&#24335;&#31070;&#32463;&#24418;&#29366;&#30340;&#24418;&#29366;&#23884;&#20837;&#12290;SENS&#19981;&#20165;&#25552;&#20379;&#20102;&#30452;&#35266;&#30340;&#22522;&#20110;&#33609;&#22270;&#30340;&#29983;&#25104;&#21644;&#32534;&#36753;&#65292;&#32780;&#19988;&#22312;&#25429;&#25417;&#29992;&#25143;&#33609;&#22270;&#24847;&#22270;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#21487;&#20197;&#29983;&#25104;&#21508;&#31181;&#26032;&#39062;&#21644;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;3D&#24418;&#29366;&#65292;&#29978;&#33267;&#21487;&#20197;&#20174;&#25277;&#35937;&#30340;&#33609;&#22270;&#20013;&#29983;&#25104;&#12290;&#25105;&#20204;&#20351;&#29992;&#23458;&#35266;&#25351;&#26631;&#35780;&#20272;&#26631;&#20934;&#21644;&#20915;&#23450;&#24615;&#29992;&#25143;&#30740;&#31350;&#26469;&#23637;&#31034;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#20004;&#32773;&#22343;&#34920;&#26126;&#23427;&#22312;&#20855;&#26377;&#20013;&#31561;&#25277;&#35937;&#31243;&#24230;&#30340;&#33609;&#22270;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#30452;&#35266;&#30340;&#22522;&#20110;&#33609;&#22270;&#30340;&#24418;&#29366;&#32534;&#36753;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SENS, a novel method for generating and editing 3D models from hand-drawn sketches, including those of an abstract nature. Our method allows users to quickly and easily sketch a shape, and then maps the sketch into the latent space of a part-aware neural implicit shape architecture. SENS analyzes the sketch and encodes its parts into ViT patch encoding, then feeds them into a transformer decoder that converts them to shape embeddings, suitable for editing 3D neural implicit shapes. SENS not only provides intuitive sketch-based generation and editing, but also excels in capturing the intent of the user's sketch to generate a variety of novel and expressive 3D shapes, even from abstract sketches. We demonstrate the effectiveness of our model compared to the state-of-the-art using objective metric evaluation criteria and a decisive user study, both indicating strong performance on sketches with a medium level of abstraction. Furthermore, we showcase its intuitive sketch-based s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23398;&#20064;&#19981;&#36827;&#34892;&#27450;&#39575;&#30340;&#26041;&#27861;&#65292;&#24182;&#32771;&#34385;&#20102;&#26234;&#33021;&#32929;&#31080;&#20132;&#26131;&#20195;&#29702;&#30340;&#27450;&#35784;&#34892;&#20026;&#35782;&#21035;&#21644;&#36991;&#20813;&#12290;</title><link>http://arxiv.org/abs/2306.06087</link><description>&lt;p&gt;
&#23398;&#20250;&#19981;&#36827;&#34892;&#27450;&#39575;
&lt;/p&gt;
&lt;p&gt;
Learning Not to Spoof. (arXiv:2306.06087v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06087
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23398;&#20064;&#19981;&#36827;&#34892;&#27450;&#39575;&#30340;&#26041;&#27861;&#65292;&#24182;&#32771;&#34385;&#20102;&#26234;&#33021;&#32929;&#31080;&#20132;&#26131;&#20195;&#29702;&#30340;&#27450;&#35784;&#34892;&#20026;&#35782;&#21035;&#21644;&#36991;&#20813;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#26234;&#33021;&#20132;&#26131;&#20195;&#29702;&#36234;&#26469;&#36234;&#26222;&#21450;&#65292;&#30830;&#20445;RL&#20195;&#29702;&#36981;&#23432;&#27861;&#24459;&#12289;&#27861;&#35268;&#21644;&#20154;&#31867;&#34892;&#20026;&#26399;&#26395;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#20854;&#20013;&#26234;&#33021;&#32929;&#31080;&#20132;&#26131;&#20195;&#29702;&#26368;&#22823;&#21270;&#21033;&#28070;&#65292;&#20294;&#21487;&#33021;&#26080;&#24847;&#20013;&#23398;&#20250;&#27450;&#39575;&#20854;&#21442;&#19982;&#30340;&#24066;&#22330;&#12290;&#26412;&#25991;&#39318;&#20808;&#24341;&#20837;&#25163;&#21160;&#32534;&#30721;&#30340;&#27450;&#35784;&#20195;&#29702;&#21040;&#19968;&#20010;&#22810;&#20195;&#29702;&#24066;&#22330;&#27169;&#25311;&#20013;&#65292;&#24182;&#23398;&#20064;&#35782;&#21035;&#27450;&#35784;&#27963;&#21160;&#24207;&#21015;&#12290;&#28982;&#21518;&#65292;&#26412;&#25991;&#23558;&#25163;&#21160;&#32534;&#30721;&#30340;&#27450;&#39575;&#20132;&#26131;&#21592;&#26367;&#25442;&#20026;&#19968;&#20010;&#31616;&#21333;&#30340;&#26368;&#22823;&#21270;&#21033;&#28070;&#30340;RL&#20195;&#29702;&#65292;&#35266;&#23519;&#23427;&#26159;&#21542;&#20250;&#29420;&#31435;&#22320;&#23398;&#20064;&#27450;&#35784;&#34892;&#20026;&#65292;&#24182;&#23581;&#35797;&#36991;&#20813;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;
As intelligent trading agents based on reinforcement learning (RL) gain prevalence, it becomes more important to ensure that RL agents obey laws, regulations, and human behavioral expectations. There is substantial literature concerning the aversion of obvious catastrophes like crashing a helicopter or bankrupting a trading account, but little around the avoidance of subtle non-normative behavior for which there are examples, but no programmable definition. Such behavior may violate legal or regulatory, rather than physical or monetary, constraints.  In this article, I consider a series of experiments in which an intelligent stock trading agent maximizes profit but may also inadvertently learn to spoof the market in which it participates. I first inject a hand-coded spoofing agent to a multi-agent market simulation and learn to recognize spoofing activity sequences. Then I replace the hand-coded spoofing trader with a simple profit-maximizing RL agent and observe that it independently 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#25163;&#26426;&#30456;&#26426;&#25293;&#25668;&#21360;&#24230;&#30828;&#24065;&#30340;&#25968;&#23383;&#22270;&#20687;&#65292;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#20174;&#30828;&#24065;&#20004;&#20391;&#20934;&#30830;&#21028;&#26029;&#30828;&#24065;&#38754;&#39069;&#30340;&#21151;&#33021;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;97&#65285;&#12290;</title><link>http://arxiv.org/abs/2306.06084</link><description>&lt;p&gt;
&#21033;&#29992;&#25163;&#26426;&#30456;&#26426;&#30340;&#26426;&#22120;&#35270;&#35273;&#65306;&#27604;&#36739;&#19977;&#31181;&#38590;&#20197;&#21306;&#20998;&#30340;&#21360;&#24230;&#30828;&#24065;&#31867;&#21035;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Machine Vision Using Cellphone Camera: A Comparison of deep networks for classifying three challenging denominations of Indian Coins. (arXiv:2306.06084v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06084
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#25163;&#26426;&#30456;&#26426;&#25293;&#25668;&#21360;&#24230;&#30828;&#24065;&#30340;&#25968;&#23383;&#22270;&#20687;&#65292;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#20174;&#30828;&#24065;&#20004;&#20391;&#20934;&#30830;&#21028;&#26029;&#30828;&#24065;&#38754;&#39069;&#30340;&#21151;&#33021;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;97&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21360;&#24230;&#36135;&#24065;&#30828;&#24065;&#26377;&#22810;&#31181;&#19981;&#21516;&#38754;&#39069;&#12290;&#20854;&#20013;1&#21346;&#27604;&#12289;2&#21346;&#27604;&#21644;5&#21346;&#27604;&#30340;&#30828;&#24065;&#30452;&#24452;&#30456;&#20284;&#12290;&#24066;&#38754;&#19978;&#27969;&#36890;&#30340;1&#21346;&#27604;&#21644;2&#21346;&#27604;&#30828;&#24065;&#22823;&#22810;&#27454;&#24335;&#30456;&#21516;&#65292;&#21482;&#26377;&#21453;&#38754;&#19978;&#30340;&#25968;&#23383;&#19981;&#21516;&#12290;&#22914;&#26524;&#30828;&#24065;&#27491;&#38754;&#26397;&#19979;&#65292;&#21017;&#26080;&#27861;&#30001;&#20154;&#31867;&#21028;&#26029;&#20854;&#27491;&#30830;&#30340;&#38754;&#39069;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21033;&#29992;&#24265;&#20215;&#30340;&#25163;&#26426;&#30456;&#26426;&#20135;&#29983;&#30340;&#30828;&#24065;&#25968;&#23383;&#22270;&#20687;&#65292;&#26469;&#21028;&#26029;&#27491;&#21453;&#38754;&#30340;&#27491;&#30830;&#38754;&#39069;&#12290;&#36890;&#36807;&#21021;&#27493;&#20998;&#26512;&#65292;&#36873;&#25321;&#20102;&#22235;&#31181;&#26368;&#36866;&#21512;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#20013;&#20004;&#31181;&#21487;&#23454;&#29616;&#20174;&#30828;&#24065;&#20004;&#20391;&#20934;&#30830;&#21028;&#26029;&#27491;&#30830;&#30340;&#38754;&#39069;&#65292;&#20934;&#30830;&#29575;&#20026;97&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Indian currency coins come in a variety of denominations. Off all the varieties Rs.1, RS.2, and Rs.5 have similar diameters. Majority of the coin styles in market circulation for denominations of Rs.1 and Rs.2 coins are nearly the same except for numerals on its reverse side. If a coin is resting on its obverse side, the correct denomination is not distinguishable by humans. Therefore, it was hypothesized that a digital image of a coin resting on its either size could be classified into its correct denomination by training a deep neural network model. The digital images were generated by using cheap cell phone cameras. To find the most suitable deep neural network architecture, four were selected based on the preliminary analysis carried out for comparison. The results confirm that two of the four deep neural network models can classify the correct denomination from either side of a coin with an accuracy of 97%.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#32858;&#31867;&#25552;&#39640;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#21487;&#20197;&#23545;&#25152;&#26377;&#20154;&#21475;&#32479;&#35745;&#23398;&#26041;&#38754;&#37117;&#26377;&#25552;&#39640;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.06083</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#30417;&#30563;&#32858;&#31867;&#25552;&#39640;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Fairness and Robustness in End-to-End Speech Recognition through unsupervised clustering. (arXiv:2306.06083v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06083
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#32858;&#31867;&#25552;&#39640;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#21487;&#20197;&#23545;&#25152;&#26377;&#20154;&#21475;&#32479;&#35745;&#23398;&#26041;&#38754;&#37117;&#26377;&#25552;&#39640;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#19981;&#33021;&#20026;&#25152;&#26377;&#20154;&#32676;&#23376;&#32452;&#32455;&#25552;&#20379;&#21516;&#26679;&#20986;&#33394;&#30340;&#24615;&#33021;&#26102;&#65292;&#23601;&#20250;&#20986;&#29616;&#20844;&#24179;&#24615;&#25361;&#25112;&#12290;&#36817;&#24180;&#26469;&#65292;&#34429;&#28982;&#22312;&#25972;&#20307;&#35821;&#38899;&#35782;&#21035;&#36136;&#37327;&#26041;&#38754;&#24050;&#32463;&#26377;&#24456;&#22810;&#25913;&#36827;&#65292;&#20294;&#24182;&#27809;&#26377;&#29305;&#21035;&#20851;&#27880;&#20026;&#31995;&#32479;&#26080;&#27861;&#34920;&#29616;&#33391;&#22909;&#30340;&#25152;&#26377;&#29992;&#25143;&#32452;&#25512;&#36827;&#20844;&#24179;&#21644;&#24179;&#31561;&#26041;&#38754;&#30340;&#25913;&#36827;&#12290;&#22240;&#27492;&#65292;ASR&#30340;&#20844;&#24179;&#24615;&#20063;&#26159;&#19968;&#20010;&#40065;&#26834;&#24615;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#25968;&#25454;&#38544;&#31169;&#22312;&#29983;&#20135;&#31995;&#32479;&#20013;&#20063;&#26159;&#39318;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#20803;&#25968;&#25454;&#12289;&#37038;&#25919;&#32534;&#30721;&#29978;&#33267;&#19981;&#30452;&#25509;&#20351;&#29992;&#35828;&#35805;&#20154;&#25110;&#35805;&#35821;&#23884;&#20837;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#39640;&#31471;&#21040;&#31471;ASR&#30340;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#35828;&#35805;&#20154;ID&#27169;&#22411;&#26469;&#25552;&#21462;&#35805;&#35821;&#32423;&#21035;&#30340;&#23884;&#20837;&#65292;&#28982;&#21518;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#20351;&#29992;&#23427;&#20204;&#26469;&#21019;&#24314;&#22768;&#23398;&#32858;&#31867;&#12290;&#25105;&#20204;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#32858;&#31867;ID&#32780;&#19981;&#26159;&#35828;&#35805;&#20154;&#35805;&#35821;&#23884;&#20837;&#20316;&#20026;&#39069;&#22806;&#30340;&#29305;&#24449;&#65292;&#36825;&#34920;&#29616;&#20986;&#20102;&#23545;&#25152;&#26377;&#20154;&#21475;&#32479;&#35745;&#23398;&#26041;&#38754;&#37117;&#26377;&#25552;&#39640;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The challenge of fairness arises when Automatic Speech Recognition (ASR) systems do not perform equally well for all sub-groups of the population. In the past few years there have been many improvements in overall speech recognition quality, but without any particular focus on advancing Equality and Equity for all user groups for whom systems do not perform well. ASR fairness is therefore also a robustness issue. Meanwhile, data privacy also takes priority in production systems. In this paper, we present a privacy preserving approach to improve fairness and robustness of end-to-end ASR without using metadata, zip codes, or even speaker or utterance embeddings directly in training. We extract utterance level embeddings using a speaker ID model trained on a public dataset, which we then use in an unsupervised fashion to create acoustic clusters. We use cluster IDs instead of speaker utterance embeddings as extra features during model training, which shows improvements for all demographic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CASSLE&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20462;&#25913;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26377;&#21521;&#25237;&#24433;&#32593;&#32476;&#65292;&#21033;&#29992;&#22686;&#24378;&#20449;&#24687;&#26469;&#25552;&#39640;&#27169;&#22411;&#22788;&#29702;&#22270;&#20687;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06082</link><description>&lt;p&gt;
&#22686;&#24378;&#24863;&#30693;&#30340;&#26377;&#21521;&#25237;&#24433;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Augmentation-aware Self-supervised Learning with Guided Projector. (arXiv:2306.06082v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CASSLE&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20462;&#25913;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26377;&#21521;&#25237;&#24433;&#32593;&#32476;&#65292;&#21033;&#29992;&#22686;&#24378;&#20449;&#24687;&#26469;&#25552;&#39640;&#27169;&#22411;&#22788;&#29702;&#22270;&#20687;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#26159;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#20581;&#22766;&#34920;&#31034;&#30340;&#24378;&#22823;&#25216;&#26415;&#12290;SimCLR&#21644;MoCo&#31561;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#23545;&#24212;&#29992;&#30340;&#25968;&#25454;&#22686;&#24378;&#20445;&#25345;&#19981;&#21464;&#65292;&#33021;&#22815;&#36798;&#21040;&#19982;&#30417;&#30563;&#26041;&#27861;&#30456;&#24403;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#19981;&#21464;&#24615;&#21487;&#33021;&#23545;&#35299;&#20915;&#26576;&#20123;&#19979;&#28216;&#20219;&#21153;&#26377;&#23475;&#65292;&#36825;&#20123;&#20219;&#21153;&#20381;&#36182;&#20110;&#21463;&#21040;&#39044;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#30340;&#22686;&#24378;&#24433;&#21709;&#30340;&#29305;&#24449;&#65292;&#20363;&#22914;&#39068;&#33394;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20462;&#25913;&#33258;&#30417;&#30563;&#26550;&#26500;&#30340;&#24120;&#35265;&#32452;&#20214;&#20043;&#19968;&#30340;&#26377;&#21521;&#25237;&#24433;&#32593;&#32476;&#65292;&#26469;&#20419;&#36827;&#34920;&#31034;&#31354;&#38388;&#23545;&#36825;&#20123;&#29305;&#24449;&#30340;&#25935;&#24863;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20026;&#25237;&#24433;&#22120;&#34917;&#20805;&#26377;&#20851;&#24212;&#29992;&#20110;&#22270;&#20687;&#30340;&#22686;&#24378;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#35753;&#25237;&#24433;&#22120;&#22312;&#35299;&#20915;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#26102;&#21033;&#29992;&#36825;&#31181;&#36741;&#21161;&#25351;&#23548;&#65292;&#29305;&#24449;&#25552;&#21462;&#22120;&#23398;&#20064;&#22312;&#20854;&#34920;&#31034;&#20013;&#20445;&#30041;&#22686;&#24378;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;&#26377;&#21521;&#25237;&#24433;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;CASSLE&#65289;&#65292;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#25552;&#39640;&#20102;&#27169;&#22411;&#22788;&#29702;&#22270;&#20687;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) is a powerful technique for learning robust representations from unlabeled data. By learning to remain invariant to applied data augmentations, methods such as SimCLR and MoCo are able to reach quality on par with supervised approaches. However, this invariance may be harmful to solving some downstream tasks which depend on traits affected by augmentations used during pretraining, such as color. In this paper, we propose to foster sensitivity to such characteristics in the representation space by modifying the projector network, a common component of self-supervised architectures. Specifically, we supplement the projector with information about augmentations applied to images. In order for the projector to take advantage of this auxiliary guidance when solving the SSL task, the feature extractor learns to preserve the augmentation information in its representations. Our approach, coined Conditional Augmentation-aware Selfsupervised Learning (CASSLE), is d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#20998;&#31867;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26426;&#21046;CARSO&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#27604;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26356;&#22909;&#22320;&#20445;&#25252;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#20928;&#21270;&#26469;&#36827;&#34892;&#26368;&#32456;&#20998;&#31867;&#65292;&#24182;&#25104;&#21151;&#22320;&#20445;&#25252;&#33258;&#24049;&#20813;&#21463;&#26410;&#39044;&#35265;&#30340;&#23041;&#32961;&#21644;&#26368;&#32456;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2306.06081</link><description>&lt;p&gt;
CARSO: &#23545;&#25239;&#24615;&#21512;&#25104;&#35266;&#27979;&#30340;&#21453;&#23545;&#25239;&#24615;&#21484;&#22238;
&lt;/p&gt;
&lt;p&gt;
CARSO: Counter-Adversarial Recall of Synthetic Observations. (arXiv:2306.06081v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#20998;&#31867;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26426;&#21046;CARSO&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#27604;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26356;&#22909;&#22320;&#20445;&#25252;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#20928;&#21270;&#26469;&#36827;&#34892;&#26368;&#32456;&#20998;&#31867;&#65292;&#24182;&#25104;&#21151;&#22320;&#20445;&#25252;&#33258;&#24049;&#20813;&#21463;&#26410;&#39044;&#35265;&#30340;&#23041;&#32961;&#21644;&#26368;&#32456;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26426;&#21046;CARSO&#65292;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#65292;&#28789;&#24863;&#26469;&#33258;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#32447;&#32034;&#12290;&#35813;&#26041;&#27861;&#19982;&#23545;&#25239;&#35757;&#32451;&#20855;&#26377;&#21327;&#21516;&#20114;&#34917;&#24615;&#65292;&#24182;&#20381;&#36182;&#20110;&#34987;&#25915;&#20987;&#20998;&#31867;&#22120;&#30340;&#20869;&#37096;&#34920;&#31034;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#20928;&#21270;&#65292;&#35813;&#26041;&#27861;&#37319;&#26679;&#36755;&#20837;&#30340;&#37325;&#26500;&#26469;&#36827;&#34892;&#26368;&#32456;&#20998;&#31867;&#12290;&#22312;&#21508;&#31181;&#22270;&#20687;&#25968;&#25454;&#38598;&#21644;&#20998;&#31867;&#22120;&#20307;&#31995;&#32467;&#26500;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;CARSO&#33021;&#22815;&#27604;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26356;&#22909;&#22320;&#20445;&#25252;&#20998;&#31867;&#22120;&#8212;&#8212;&#21516;&#26102;&#20855;&#26377;&#21487;&#25509;&#21463;&#30340;&#28165;&#27905;&#20934;&#30830;&#24230;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#38450;&#24481;&#20307;&#31995;&#32467;&#26500;&#25104;&#21151;&#22320;&#20445;&#25252;&#33258;&#24049;&#20813;&#21463;&#26410;&#39044;&#35265;&#30340;&#23041;&#32961;&#21644;&#26368;&#32456;&#25915;&#20987;&#12290;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#22312;https://github.com/&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel adversarial defence mechanism for image classification -- CARSO -- inspired by cues from cognitive neuroscience. The method is synergistically complementary to adversarial training and relies on knowledge of the internal representation of the attacked classifier. Exploiting a generative model for adversarial purification, conditioned on such representation, it samples reconstructions of inputs to be finally classified. Experimental evaluation by a well-established benchmark of varied, strong adaptive attacks, across diverse image datasets and classifier architectures, shows that CARSO is able to defend the classifier significantly better than state-of-the-art adversarial training alone -- with a tolerable clean accuracy toll. Furthermore, the defensive architecture succeeds in effectively shielding itself from unforeseen threats, and end-to-end attacks adapted to fool stochastic defences. Code and pre-trained models are available at https://github.com/
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22270;&#20687;&#20998;&#21106;&#21644;&#22810;&#31867;&#25903;&#25345;&#21521;&#37327;&#26426;&#25216;&#26415;&#65292;&#23545;&#21463;&#26202;&#30123;&#30149;&#24433;&#21709;&#30340;&#30058;&#33540;&#21494;&#36827;&#34892;&#26816;&#27979;&#12290;&#21033;&#29992;&#22270;&#20687;&#20998;&#21106;&#25216;&#26415;&#65292;&#23558;&#21463;&#25439;&#21306;&#22495;&#20998;&#31163;&#20986;&#26469;&#65292;&#36890;&#36807;&#22810;&#31867;&#25903;&#25345;&#21521;&#37327;&#26426;&#31639;&#27861;&#26469;&#23545;&#30142;&#30149;&#36827;&#34892;&#21487;&#38752;&#30340;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2306.06080</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#20687;&#22788;&#29702;&#25216;&#26415;&#30340;&#30058;&#33540;&#26202;&#30123;&#30149;&#30149;&#21494;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Detection of Late Blight Disease in Tomato Leaf Using Image Processing Techniques. (arXiv:2306.06080v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22270;&#20687;&#20998;&#21106;&#21644;&#22810;&#31867;&#25903;&#25345;&#21521;&#37327;&#26426;&#25216;&#26415;&#65292;&#23545;&#21463;&#26202;&#30123;&#30149;&#24433;&#21709;&#30340;&#30058;&#33540;&#21494;&#36827;&#34892;&#26816;&#27979;&#12290;&#21033;&#29992;&#22270;&#20687;&#20998;&#21106;&#25216;&#26415;&#65292;&#23558;&#21463;&#25439;&#21306;&#22495;&#20998;&#31163;&#20986;&#26469;&#65292;&#36890;&#36807;&#22810;&#31867;&#25903;&#25345;&#21521;&#37327;&#26426;&#31639;&#27861;&#26469;&#23545;&#30142;&#30149;&#36827;&#34892;&#21487;&#38752;&#30340;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30058;&#33540;&#20316;&#20026;&#26368;&#24120;&#35265;&#30340;&#20892;&#20316;&#29289;&#20043;&#19968;&#65292;&#26202;&#30123;&#30149;&#26159;&#19990;&#30028;&#19978;&#26368;&#24120;&#35265;&#30340;&#30058;&#33540;&#30149;&#23475;&#65292;&#32463;&#24120;&#23548;&#33268;&#30058;&#33540;&#20316;&#29289;&#30340;&#20135;&#37327;&#26174;&#33879;&#38477;&#20302;&#12290;&#20026;&#20102;&#20445;&#35777;&#30058;&#33540;&#20316;&#20026;&#20892;&#20135;&#21697;&#30340;&#37325;&#35201;&#35282;&#33394;&#65292;&#26089;&#26399;&#26816;&#27979;&#26202;&#30123;&#30149;&#21313;&#20998;&#24517;&#35201;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#22270;&#20687;&#20998;&#21106;&#21644;&#22810;&#31867;&#25903;&#25345;&#21521;&#37327;&#26426;&#25216;&#26415;&#26469;&#26816;&#27979;&#26202;&#30123;&#30149;&#12290;&#21033;&#29992;&#22270;&#20687;&#20998;&#21106;&#25216;&#26415;&#65292;&#23558;&#21463;&#25439;&#21306;&#22495;&#20998;&#31163;&#20986;&#26469;&#65292;&#36890;&#36807;&#22810;&#31867;&#25903;&#25345;&#21521;&#37327;&#26426;&#31639;&#27861;&#26469;&#23545;&#30142;&#30149;&#36827;&#34892;&#21487;&#38752;&#30340;&#20998;&#31867;&#12290;&#26412;&#30740;&#31350;&#20174;30&#39033;&#26435;&#23041;&#30740;&#31350;&#20013;&#36873;&#23450;&#20102;&#19968;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
=One of the most frequently farmed crops is the tomato crop. Late blight is the most prevalent tomato disease in the world, and often causes a significant reduction in the production of tomato crops. The importance of tomatoes as an agricultural product necessitates early detection of late blight. It is produced by the fungus Phytophthora. The earliest signs of late blight on tomatoes are unevenly formed, water-soaked lesions on the leaves located on the plant canopy's younger leave White cottony growth may appear in humid environments evident on the undersides of the leaves that have been impacted. Lesions increase as the disease proceeds, turning the leaves brown to shrivel up and die. Using picture segmentation and the Multi-class SVM technique, late blight disorder is discovered in this work. Image segmentation is employed for separating damaged areas on leaves, and the Multi-class SVM method is used for reliable disease categorization. 30 reputable studies were chosen from a total
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31232;&#30095;&#35266;&#27979;&#25968;&#25454;&#30340;MetNet-3&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#23545;20&#23567;&#26102;&#20869;&#30340;&#22825;&#27668;&#36827;&#34892;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;MetNet-3&#30340;&#25216;&#26415;&#21019;&#26032;&#21253;&#25324;&#21487;&#23398;&#20064;&#21367;&#31215;&#12289;&#29305;&#24449;&#23398;&#20064;&#21644;&#22810;&#20219;&#21153;&#35757;&#32451;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#25345;&#32493;&#24615;&#21551;&#21457;&#27861;&#26469;&#22806;&#25512;&#21021;&#22987;&#26465;&#20214;&#25110;&#36827;&#34892;&#30701;&#26399;&#39044;&#27979;&#26469;&#22635;&#34917;&#32570;&#22833;&#30340;&#35266;&#27979;&#25968;&#25454;&#26356;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.06079</link><description>&lt;p&gt;
&#20174;&#31232;&#30095;&#35266;&#27979;&#25968;&#25454;&#20013;&#36827;&#34892;&#26085;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Day Forecasts from Sparse Observations. (arXiv:2306.06079v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31232;&#30095;&#35266;&#27979;&#25968;&#25454;&#30340;MetNet-3&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#23545;20&#23567;&#26102;&#20869;&#30340;&#22825;&#27668;&#36827;&#34892;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;MetNet-3&#30340;&#25216;&#26415;&#21019;&#26032;&#21253;&#25324;&#21487;&#23398;&#20064;&#21367;&#31215;&#12289;&#29305;&#24449;&#23398;&#20064;&#21644;&#22810;&#20219;&#21153;&#35757;&#32451;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#25345;&#32493;&#24615;&#21551;&#21457;&#27861;&#26469;&#22806;&#25512;&#21021;&#22987;&#26465;&#20214;&#25110;&#36827;&#34892;&#30701;&#26399;&#39044;&#27979;&#26469;&#22635;&#34917;&#32570;&#22833;&#30340;&#35266;&#27979;&#25968;&#25454;&#26356;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#31181;&#24314;&#27169;&#22825;&#27668;&#26465;&#20214;&#30340;&#26367;&#20195;&#33539;&#20363;&#12290;&#31070;&#32463;&#27169;&#22411;&#22312;&#25968;&#25454;&#21487;&#29992;&#26102;&#20197;&#23569;&#20110;1&#31186;&#30340;&#36895;&#24230;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#20197;&#38750;&#24120;&#39640;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#20998;&#36776;&#29575;&#36827;&#34892;&#39044;&#27979;&#12290;&#23427;&#20204;&#21487;&#20197;&#30452;&#25509;&#20174;&#22823;&#27668;&#35266;&#27979;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#36825;&#26159;&#36825;&#20123;&#27169;&#22411;&#29420;&#29305;&#30340;&#20248;&#21183;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#20165;&#20165;&#39044;&#27979;&#38477;&#27700;&#36825;&#19968;&#21807;&#19968;&#21464;&#37327;&#26102;&#65292;&#20165;&#33021;&#20351;&#29992;&#22823;&#27668;&#35266;&#27979;&#25968;&#25454;&#26469;&#35757;&#32451;&#31070;&#32463;&#27169;&#22411;&#65292;&#25165;&#33021;&#36798;&#21040;&#19982;&#29616;&#26377;&#27010;&#29575;&#24615;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#30456;&#24403;&#30340;&#33391;&#22909;&#34920;&#29616;&#21040;12&#20010;&#23567;&#26102;&#30340;&#25552;&#21069;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MetNet-3&#65292;&#23427;&#26174;&#33879;&#25193;&#23637;&#20102;&#22522;&#20110;&#35266;&#27979;&#25968;&#25454;&#30340;&#31070;&#32463;&#27169;&#22411;&#33021;&#22815;&#33391;&#22909;&#39044;&#27979;&#30340;&#24341;&#23548;&#26102;&#38388;&#33539;&#22260;&#21644;&#21464;&#37327;&#12290;MetNet-3&#20174;&#23494;&#38598;&#21644;&#31232;&#30095;&#30340;&#25968;&#25454;&#20256;&#24863;&#22120;&#20013;&#23398;&#20064;&#65292;&#24182;&#20026;&#38477;&#27700;&#12289;&#39118;&#12289;&#28201;&#24230;&#21644;&#38706;&#28857;&#36827;&#34892;24&#23567;&#26102;&#30340;&#39044;&#27979;&#12290;MetNet-3&#22312;&#20307;&#31995;&#32467;&#26500;&#23618;&#38754;&#24341;&#20837;&#20102;&#35768;&#22810;&#25216;&#26415;&#21019;&#26032;&#65292;&#36825;&#34987;&#35777;&#26126;&#23545;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#26377;&#25152;&#36129;&#29486;&#65292;&#21253;&#25324;&#21487;&#23398;&#20064;&#30340;&#26102;&#31354;&#21367;&#31215;&#12289;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#29305;&#24449;&#23398;&#20064;&#21644;&#22810;&#20219;&#21153;&#35757;&#32451;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#31070;&#32463;&#27169;&#22411;&#27867;&#21270;&#20026;&#20165;&#25509;&#21463;&#31232;&#30095;&#30340;&#27668;&#21387;&#35745;&#35266;&#27979;&#25968;&#25454;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;&#25345;&#32493;&#24615;&#21551;&#21457;&#27861;&#26469;&#22806;&#25512;&#21021;&#22987;&#26465;&#20214;&#65292;&#25110;&#36890;&#36807;&#20351;&#29992;&#20302;&#20998;&#36776;&#29575;&#25968;&#20540;&#27169;&#22411;&#36827;&#34892;&#30701;&#26399;&#39044;&#27979;&#26469;&#22635;&#34917;&#32570;&#22833;&#30340;&#35266;&#27979;&#25968;&#25454;&#30340;&#26041;&#27861;&#26159;&#26377;&#30410;&#30340;&#12290;MetNet-3&#22312;&#38477;&#27700;&#12289;&#28201;&#24230;&#21644;&#38706;&#28857;&#39044;&#27979;&#26041;&#38754;&#27604;&#29616;&#26377;&#30340;&#22823;&#27668;&#27169;&#22411;&#22312;&#25552;&#21069;&#33267;24&#23567;&#26102;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks offer an alternative paradigm for modeling weather conditions. The ability of neural models to make a prediction in less than a second once the data is available and to do so with very high temporal and spatial resolution, and the ability to learn directly from atmospheric observations, are just some of these models' unique advantages. Neural models trained using atmospheric observations, the highest fidelity and lowest latency data, have to date achieved good performance only up to twelve hours of lead time when compared with state-of-the-art probabilistic Numerical Weather Prediction models and only for the sole variable of precipitation. In this paper, we present MetNet-3 that extends significantly both the lead time range and the variables that an observation based neural model can predict well. MetNet-3 learns from both dense and sparse data sensors and makes predictions up to 24 hours ahead for precipitation, wind, temperature and dew point. MetNet-3 introduc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35266;&#23519;&#21608;&#22260;&#20154;&#26469;&#25552;&#39640;&#20010;&#20154;&#27963;&#21160;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.06078</link><description>&lt;p&gt;
&#36890;&#36807;&#21327;&#21516;&#20316;&#19994;&#26469;&#25552;&#21319;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Cheating off your neighbors: Improving activity recognition through corroboration. (arXiv:2306.06078v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06078
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35266;&#23519;&#21608;&#22260;&#20154;&#26469;&#25552;&#39640;&#20010;&#20154;&#27963;&#21160;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#36890;&#36807;&#19968;&#20010;&#20154;&#30340;&#25968;&#25454;&#26469;&#29702;&#35299;&#20154;&#31867;&#27963;&#21160;&#30340;&#22797;&#26434;&#24615;&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#21608;&#22260;&#30340;&#20154;&#21487;&#33021;&#27491;&#22312;&#25191;&#34892;&#31867;&#20284;&#30340;&#27963;&#21160;&#65292;&#32780;&#29616;&#26377;&#30340;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#26041;&#27861;&#20960;&#20046;&#23436;&#20840;&#38598;&#20013;&#22312;&#20010;&#20307;&#27979;&#37327;&#19978;&#65292;&#24182;&#19988;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#30053;&#20102;&#27963;&#21160;&#30340;&#32972;&#26223;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#34701;&#20837;&#21608;&#22260;&#20010;&#20307;&#30340;&#35265;&#35299;&#26469;&#25552;&#39640;&#20010;&#20154;&#27963;&#21160;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#26469;&#33258;20&#21517;&#21442;&#19982;&#32773;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65292;&#22312;&#36229;&#36807;58&#23567;&#26102;&#30340;&#25968;&#25454;&#20013;&#21253;&#25324;&#21442;&#21152;&#35762;&#24231;&#31561;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the complexity of human activities solely through an individual's data can be challenging. However, in many situations, surrounding individuals are likely performing similar activities, while existing human activity recognition approaches focus almost exclusively on individual measurements and largely ignore the context of the activity. Consider two activities: attending a small group meeting and working at an office desk. From solely an individual's perspective, it can be difficult to differentiate between these activities as they may appear very similar, even though they are markedly different. Yet, by observing others nearby, it can be possible to distinguish between these activities. In this paper, we propose an approach to enhance the prediction accuracy of an individual's activities by incorporating insights from surrounding individuals. We have collected a real-world dataset from 20 participants with over 58 hours of data including activities such as attending lect
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DP-RandP&#30340;&#26041;&#27861;&#65292;&#20174;&#38543;&#26426;&#36807;&#31243;&#20013;&#23398;&#20064;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#20256;&#36882;&#32473;&#31169;&#26377;&#25968;&#25454;&#65292;&#20197;&#25913;&#36827;&#24046;&#20998;&#38544;&#31169;&#30340;&#22270;&#20687;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#25552;&#39640;&#20102;CIFAR-10&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.06076</link><description>&lt;p&gt;
&#20174;&#38543;&#26426;&#36807;&#31243;&#20013;&#23398;&#20064;&#20808;&#39564;&#30693;&#35782;&#30340;&#24046;&#20998;&#38544;&#31169;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Image Classification by Learning Priors from Random Processes. (arXiv:2306.06076v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DP-RandP&#30340;&#26041;&#27861;&#65292;&#20174;&#38543;&#26426;&#36807;&#31243;&#20013;&#23398;&#20064;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#20256;&#36882;&#32473;&#31169;&#26377;&#25968;&#25454;&#65292;&#20197;&#25913;&#36827;&#24046;&#20998;&#38544;&#31169;&#30340;&#22270;&#20687;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#25552;&#39640;&#20102;CIFAR-10&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38544;&#31169;&#20445;&#25252;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#19981;&#21516;ially&#31169;&#26377;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DP-SGD&#65289;&#30001;&#20110;&#27599;&#20010;&#26679;&#26412;&#26799;&#24230;&#21098;&#36753;&#21644;&#22122;&#22768;&#28155;&#21152;&#32780;&#34920;&#29616;&#19981;&#20339;&#12290;&#38544;&#31169;&#23398;&#20064;&#30740;&#31350;&#30340;&#19968;&#20010;&#26368;&#36817;&#37325;&#28857;&#26159;&#36890;&#36807;&#23558;&#22312;&#30495;&#23454;&#19990;&#30028;&#20844;&#20849;&#25968;&#25454;&#19978;&#23398;&#20064;&#30340;&#20808;&#39564;&#30693;&#35782;&#32435;&#20837;&#36825;&#20123;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#39640;DP-SGD&#22312;&#31169;&#26377;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#20174;&#30001;&#38543;&#26426;&#36807;&#31243;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#23398;&#20064;&#20808;&#39564;&#30693;&#35782;&#24182;&#23558;&#36825;&#20123;&#20808;&#39564;&#30693;&#35782;&#36716;&#31227;&#21040;&#31169;&#26377;&#25968;&#25454;&#26469;&#25913;&#36827;DP-SGD&#30340;&#38544;&#31169;-&#25928;&#29992;&#25240;&#34935;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DP-RandP&#65292;&#36825;&#26159;&#19968;&#20010;&#19977;&#38454;&#27573;&#30340;&#26041;&#27861;&#12290;&#22312;CIFAR10&#12289;CIFAR100&#21644;MedMNIST&#19978;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#38544;&#31169;&#39044;&#31639;&#949;&#8712;[1&#65292;8]&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23558;&#22312;&#949;=1&#26102;&#22312;CIFAR10&#19978;&#25253;&#21578;&#30340;&#26368;&#20339;&#20934;&#30830;&#24615;&#20174;60.6%&#25552;&#39640;&#21040;72.3%&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/inspire-group/DP-RandP&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
In privacy-preserving machine learning, differentially private stochastic gradient descent (DP-SGD) performs worse than SGD due to per-sample gradient clipping and noise addition. A recent focus in private learning research is improving the performance of DP-SGD on private data by incorporating priors that are learned on real-world public data. In this work, we explore how we can improve the privacy-utility tradeoff of DP-SGD by learning priors from images generated by random processes and transferring these priors to private data. We propose DP-RandP, a three-phase approach. We attain new state-of-the-art accuracy when training from scratch on CIFAR10, CIFAR100, and MedMNIST for a range of privacy budgets $\varepsilon \in [1, 8]$. In particular, we improve the previous best reported accuracy on CIFAR10 from $60.6 \%$ to $72.3 \%$ for $\varepsilon=1$. Our code is available at https://github.com/inspire-group/DP-RandP.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20809;&#35889;&#25351;&#25968;&#36807;&#28388;&#25481;&#24314;&#31569;&#21306;&#22495;&#24182;&#23545;&#29305;&#24449;&#36827;&#34892;&#31934;&#36873;&#30340;&#22810;&#20809;&#35889;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#22478;&#24066;&#22320;&#21306;&#32511;&#22320;&#35206;&#30422;&#29575;&#30340;&#20272;&#35745;&#65292;&#22312;LUMS&#30340;82 &#33521;&#20137;&#25351;&#23450;&#21306;&#22495;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.06073</link><description>&lt;p&gt;
&#22522;&#20110;&#21355;&#26143;&#22810;&#20809;&#35889;&#24433;&#20687;&#30340;&#29305;&#24449;&#36873;&#25321;&#65292;&#29992;&#20110;&#22478;&#24066;&#32511;&#22320;&#35206;&#30422;&#29575;&#26377;&#25928;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Feature Selection on Sentinel-2 Multi-spectral Imagery for Efficient Tree Cover Estimation. (arXiv:2306.06073v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20809;&#35889;&#25351;&#25968;&#36807;&#28388;&#25481;&#24314;&#31569;&#21306;&#22495;&#24182;&#23545;&#29305;&#24449;&#36827;&#34892;&#31934;&#36873;&#30340;&#22810;&#20809;&#35889;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#22478;&#24066;&#22320;&#21306;&#32511;&#22320;&#35206;&#30422;&#29575;&#30340;&#20272;&#35745;&#65292;&#22312;LUMS&#30340;82 &#33521;&#20137;&#25351;&#23450;&#21306;&#22495;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#21512;&#22478;&#24066;&#22320;&#21306;&#32511;&#22320;&#35206;&#30422;&#29575;&#20272;&#35745;&#30340;&#22810;&#20809;&#35889;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#65292;&#23427;&#20855;&#26377;&#36866;&#24403;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#25513;&#34109;&#12290;&#25152;&#25552;&#20986;&#20998;&#31867;&#22120;&#30340;&#20851;&#38190;&#29305;&#28857;&#26159;&#21033;&#29992;&#20809;&#35889;&#25351;&#25968;&#36807;&#28388;&#25481;&#24314;&#31569;&#21306;&#22495;&#65292;&#28982;&#21518;&#20351;&#29992;&#31934;&#36873;&#29305;&#24449;&#23545;&#21097;&#20313;&#25513;&#34109;&#19978;&#23454;&#29616;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#12290;&#20351;&#29992;Sentinel-2&#21355;&#26143;&#24433;&#20687;&#25968;&#25454;&#23545;Lahore University of Management Sciences&#65288;LUMS&#65289;&#30340;&#25351;&#23450;&#21306;&#22495;&#65288;&#32422;82&#33521;&#20137;&#65289;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#65292;&#20197;&#21450;European Space Agency&#65288;ESA&#65289;WorldCover 10m 2020&#20135;&#21697;&#21644;DeepLabv3&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#31561;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a multi-spectral random forest classifier with suitable feature selection and masking for tree cover estimation in urban areas. The key feature of the proposed classifier is filtering out the built-up region using spectral indices followed by random forest classification on the remaining mask with carefully selected features. Using Sentinel-2 satellite imagery, we evaluate the performance of the proposed technique on a specified area (approximately 82 acres) of Lahore University of Management Sciences (LUMS) and demonstrate that our method outperforms a conventional random forest classifier as well as state-of-the-art methods such as European Space Agency (ESA) WorldCover 10m 2020 product as well as a DeepLabv3 deep learning architecture.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;YOLOv5&#22312;&#20132;&#36890;&#21644;&#36947;&#36335;&#26631;&#24535;&#26816;&#27979;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#33030;&#24369;&#24615;&#65292;&#21457;&#29616;YOLOv5&#26131;&#21463;&#22810;&#31181;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#26377; important implications for the safety and reliability of object detection algorithms used in traffic.</title><link>http://arxiv.org/abs/2306.06071</link><description>&lt;p&gt;
YOLOv5&#22312;&#20132;&#36890;&#21644;&#36947;&#36335;&#26631;&#24535;&#26816;&#27979;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attack On Yolov5 For Traffic And Road Sign Detection. (arXiv:2306.06071v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;YOLOv5&#22312;&#20132;&#36890;&#21644;&#36947;&#36335;&#26631;&#24535;&#26816;&#27979;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#33030;&#24369;&#24615;&#65292;&#21457;&#29616;YOLOv5&#26131;&#21463;&#22810;&#31181;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#26377; important implications for the safety and reliability of object detection algorithms used in traffic.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;YOLOv5&#30446;&#26631;&#26816;&#27979;&#31639;&#27861;&#23454;&#29616;&#24182;&#30740;&#31350;&#20102;&#27969;&#34892;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;&#30740;&#31350;&#25506;&#35752;&#20102;YOLOv5&#22312;&#20132;&#36890;&#21644;&#36947;&#36335;&#26631;&#24535;&#26816;&#27979;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#33030;&#24369;&#24615;&#12290;&#24182;&#30740;&#31350;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#25915;&#20987;&#65288;&#21253;&#25324;L-BFGS&#12289;FGSM&#12289;C&amp;W&#12289;BIM&#12289;PGD&#12289;One Pixel Attack&#21644;UAP&#65289;&#23545;YOLOv5&#22312;&#36947;&#36335;&#26631;&#24535;&#26816;&#27979;&#20013;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;YOLOv5&#26131;&#21463;&#36825;&#20123;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#38543;&#30528;&#25200;&#21160;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;&#20998;&#31867;&#38169;&#35823;&#29575;&#20063;&#20250;&#22686;&#21152;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#26174;&#33879;&#24615;&#22270;&#35299;&#37322;&#20102;&#36825;&#20123;&#32467;&#26524;&#12290;&#26412;&#25991;&#30340;&#21457;&#29616;&#23545;&#20110;&#20132;&#36890;&#30446;&#26631;&#26816;&#27979;&#31639;&#27861;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#20855;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper implements and investigates popular adversarial attacks on the YOLOv5 Object Detection algorithm. The paper explores the vulnerability of the YOLOv5 to adversarial attacks in the context of traffic and road sign detection. The paper investigates the impact of different types of attacks, including the Limited memory Broyden Fletcher Goldfarb Shanno (L-BFGS), the Fast Gradient Sign Method (FGSM) attack, the Carlini and Wagner (C&amp;W) attack, the Basic Iterative Method (BIM) attack, the Projected Gradient Descent (PGD) attack, One Pixel Attack, and the Universal Adversarial Perturbations attack on the accuracy of YOLOv5 in detecting traffic and road signs. The results show that YOLOv5 is susceptible to these attacks, with misclassification rates increasing as the magnitude of the perturbations increases. We also explain the results using saliency maps. The findings of this paper have important implications for the safety and reliability of object detection algorithms used in traf
&lt;/p&gt;</description></item><item><title>DeepStay&#26159;&#19968;&#20010;&#22522;&#20110;&#24369;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#20301;&#32622;&#36712;&#36857;&#20013;&#39044;&#27979;&#20572;&#30041;&#21306;&#22495;&#24182;&#25552;&#21462;&#20010;&#20154;&#20852;&#36259;&#28857;&#65288;POIs&#65289;&#32780;&#26080;&#38656;&#20010;&#20154;&#20449;&#24687;&#25110;&#20854;&#20182;&#25968;&#25454;&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#22312;&#20844;&#20849;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#24182;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.06068</link><description>&lt;p&gt;
DeepStay&#65306;&#20351;&#29992;&#24369;&#30417;&#30563;&#20174;&#20301;&#32622;&#36712;&#36857;&#20013;&#25552;&#21462;&#20572;&#30041;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
DeepStay: Stay Region Extraction from Location Trajectories using Weak Supervision. (arXiv:2306.06068v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06068
&lt;/p&gt;
&lt;p&gt;
DeepStay&#26159;&#19968;&#20010;&#22522;&#20110;&#24369;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#20301;&#32622;&#36712;&#36857;&#20013;&#39044;&#27979;&#20572;&#30041;&#21306;&#22495;&#24182;&#25552;&#21462;&#20010;&#20154;&#20852;&#36259;&#28857;&#65288;POIs&#65289;&#32780;&#26080;&#38656;&#20010;&#20154;&#20449;&#24687;&#25110;&#20854;&#20182;&#25968;&#25454;&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#22312;&#20844;&#20849;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#24182;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#31227;&#21160;&#35774;&#22791;&#21487;&#20197;&#19981;&#26029;&#36319;&#36394;&#29992;&#25143;&#30340;&#20301;&#32622;&#65292;&#20301;&#32622;&#36712;&#36857;&#21487;&#29992;&#20110;&#25512;&#26029;&#20010;&#20154;&#20852;&#36259;&#28857;(POI)&#65292;&#22914;&#23478;&#24237;&#12289;&#24037;&#20316;&#22330;&#25152;&#25110;&#21830;&#24215;&#12290;&#25552;&#21462;POI&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#39318;&#20808;&#35782;&#21035;&#29992;&#25143;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#30340;&#26102;&#31354;&#21306;&#22495;&#65292;&#21363;&#20572;&#30041;&#21306;&#22495;(SR)&#12290;&#24120;&#35265;&#30340;SR&#25552;&#21462;&#26041;&#27861;&#35201;&#20040;&#20165;&#35780;&#20272;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#35201;&#20040;&#22312;&#23567;&#35268;&#27169;&#31169;&#26377;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#22240;&#20026;&#27969;&#34892;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#26159;&#26410;&#26631;&#35760;&#30340;&#12290;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#24037;&#29305;&#24449;&#25110;&#38408;&#20540;&#65292;&#24182;&#19988;&#26080;&#27861;&#36229;&#20986;&#36229;&#21442;&#25968;&#20248;&#21270;&#33539;&#22260;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24369;&#30417;&#30563;&#30340;&#33258;&#30417;&#30563;&#21464;&#21387;&#22120;&#27169;&#22411;DeepStay&#65292;&#23427;&#26159;&#22312;&#20301;&#32622;&#36712;&#36857;&#19978;&#35757;&#32451;&#30340;&#65292;&#29992;&#20110;&#39044;&#27979;&#20572;&#30041;&#21306;&#22495;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20063;&#26159;&#31532;&#19968;&#20010;&#22312;&#20844;&#20849;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;SR&#25552;&#21462;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20572;&#30041;&#21306;&#22495;&#20013;&#25552;&#21462;&#20010;&#20154;POI&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#20010;&#20154;&#20449;&#24687;&#65292;&#20363;&#22914;&#23478;&#24237;&#25110;&#24037;&#20316;&#22320;&#22336;&#65292;&#20063;&#19981;&#38656;&#35201;&#38500;&#20301;&#32622;&#36712;&#36857;&#20197;&#22806;&#30340;&#20854;&#20182;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, mobile devices enable constant tracking of the user's position and location trajectories can be used to infer personal points of interest (POIs) like homes, workplaces, or stores. A common way to extract POIs is to first identify spatio-temporal regions where a user spends a significant amount of time, known as stay regions (SRs).  Common approaches to SR extraction are evaluated either solely unsupervised or on a small-scale private dataset, as popular public datasets are unlabeled. Most of these methods rely on hand-crafted features or thresholds and do not learn beyond hyperparameter optimization. Therefore, we propose a weakly and self-supervised transformer-based model called DeepStay, which is trained on location trajectories to predict stay regions. To the best of our knowledge, this is the first approach based on deep learning and the first approach that is evaluated on a public, labeled dataset. Our SR extraction method outperforms state-of-the-art methods. In additi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#22810;&#32423;&#20132;&#21449;&#27169;&#24577;&#29305;&#24449;&#23545;&#40784;&#26041;&#27861;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#20998;&#31867;&#65292;&#33021;&#22815;&#26377;&#25928;&#24212;&#23545;&#22823;&#31867;&#20869;&#24046;&#24322;&#21644;&#23567;&#31867;&#38388;&#30456;&#20284;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.06066</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#20132;&#21449;&#27169;&#24577;&#29305;&#24449;&#23545;&#40784;&#26041;&#27861;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#22330;&#26223;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Multi-level Cross-modal Feature Alignment via Contrastive Learning towards Zero-shot Classification of Remote Sensing Image Scenes. (arXiv:2306.06066v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06066
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#22810;&#32423;&#20132;&#21449;&#27169;&#24577;&#29305;&#24449;&#23545;&#40784;&#26041;&#27861;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#20998;&#31867;&#65292;&#33021;&#22815;&#26377;&#25928;&#24212;&#23545;&#22823;&#31867;&#20869;&#24046;&#24322;&#21644;&#23567;&#31867;&#38388;&#30456;&#20284;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#22270;&#20687;&#22330;&#26223;&#20998;&#31867;&#21487;&#20197;&#35782;&#21035;&#22312;&#35757;&#32451;&#38454;&#27573;&#26410;&#35265;&#36807;&#30340;&#22270;&#20687;&#22330;&#26223;&#65292;&#20197;&#27492;&#38477;&#20302;&#23545;&#22823;&#37327;&#26631;&#35760;&#26679;&#26412;&#30340;&#20381;&#36182;&#24615;&#12290;&#36817;&#24180;&#26469;&#65292;&#20132;&#21449;&#27169;&#24577;&#29305;&#24449;&#23545;&#40784;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#23558;&#27599;&#20010;&#22270;&#20687;&#22330;&#26223;&#30340;&#35270;&#35273;&#29305;&#24449;&#19982;&#20854;&#23545;&#24212;&#30340;&#35821;&#20041;&#25551;&#36848;&#31526;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#21305;&#37197;&#12290;&#36739;&#23569;&#20851;&#27880;&#19981;&#21516;&#22270;&#20687;&#22330;&#26223;&#21644;&#19981;&#21516;&#35821;&#20041;&#25551;&#36848;&#31526;&#20043;&#38388;&#30340;&#23545;&#27604;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot classification of image scenes which can recognize the image scenes that are not seen in the training stage holds great promise of lowering the dependence on large numbers of labeled samples. To address the zero-shot image scene classification, the cross-modal feature alignment methods have been proposed in recent years. These methods mainly focus on matching the visual features of each image scene with their corresponding semantic descriptors in the latent space. Less attention has been paid to the contrastive relationships between different image scenes and different semantic descriptors. In light of the challenge of large intra-class difference and inter-class similarity among image scenes and the potential noisy samples, these methods are susceptible to the influence of the instances which are far from these of the same classes and close to these of other classes. In this work, we propose a multi-level cross-modal feature alignment method via contrastive learning for zero
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#26053;&#34892;&#21830;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#26159;&#36890;&#36807;&#22312;TSP&#23454;&#20363;&#35757;&#32451;&#20043;&#21069;&#65292;&#23558;&#31070;&#32463;&#27169;&#22411;&#29992;&#30456;&#20851;&#31639;&#27861;&#36827;&#34892;&#39044;&#35757;&#32451;&#26469;&#23454;&#29616;&#30340;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;TSP&#38382;&#39064;&#30340;&#35299;&#20915;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.06064</link><description>&lt;p&gt;
&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#30340;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Neural Algorithmic Reasoning for Combinatorial Optimisation. (arXiv:2306.06064v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#26053;&#34892;&#21830;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#26159;&#36890;&#36807;&#22312;TSP&#23454;&#20363;&#35757;&#32451;&#20043;&#21069;&#65292;&#23558;&#31070;&#32463;&#27169;&#22411;&#29992;&#30456;&#20851;&#31639;&#27861;&#36827;&#34892;&#39044;&#35757;&#32451;&#26469;&#23454;&#29616;&#30340;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;TSP&#38382;&#39064;&#30340;&#35299;&#20915;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;NP&#38590;/&#23436;&#20840;&#32452;&#21512;&#38382;&#39064;&#26159;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#36229;&#36234;&#20256;&#32479;&#30340;&#36817;&#20284;&#31639;&#27861;&#12290;&#20854;&#38271;&#26399;&#30446;&#26631;&#26159;&#36890;&#36807;&#23398;&#20064;&#20165;&#20174;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#26356;&#20248;&#35299;&#26469;&#36229;&#36234;&#25163;&#24037;&#35774;&#35745;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#32780;&#26053;&#34892;&#21830;&#38382;&#39064;(TSP)&#26159;&#32463;&#24120;&#34987;&#36825;&#20123;&#26041;&#27861;&#30596;&#20934;&#30340;&#19968;&#20010;&#37325;&#35201;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#29992;&#20110;&#35299;&#20915;TSP&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#24120;&#24120;&#24573;&#30053;&#20102;&#38382;&#39064;&#22266;&#26377;&#30340;&#8220;&#31639;&#27861;&#8221;&#26412;&#36136;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#35774;&#35745;&#29992;&#20110;TSP&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#24120;&#24120;&#21033;&#29992;&#35832;&#22914;&#26597;&#25214;&#26368;&#23567;&#29983;&#25104;&#26641;&#20043;&#31867;&#30340;&#25104;&#29087;&#31639;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#30340;&#26368;&#26032;&#36827;&#23637;&#26469;&#25913;&#36827;TSP&#38382;&#39064;&#30340;&#23398;&#20064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#23545;TSP&#23454;&#20363;&#36827;&#34892;&#35757;&#32451;&#20043;&#21069;&#65292;&#22312;&#30456;&#20851;&#31639;&#27861;&#19978;&#23545;&#25105;&#20204;&#30340;&#31070;&#32463;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#36825;&#31181;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;TSP&#38382;&#39064;&#30340;&#35299;&#20915;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving NP-hard/complete combinatorial problems with neural networks is a challenging research area that aims to surpass classical approximate algorithms. The long-term objective is to outperform hand-designed heuristics for NP-hard/complete problems by learning to generate superior solutions solely from training data. The Travelling Salesman Problem (TSP) is a prominent combinatorial optimisation problem often targeted by such approaches. However, current neural-based methods for solving TSP often overlook the inherent "algorithmic" nature of the problem. In contrast, heuristics designed for TSP frequently leverage well-established algorithms, such as those for finding the minimum spanning tree. In this paper, we propose leveraging recent advancements in neural algorithmic reasoning to improve the learning of TSP problems. Specifically, we suggest pre-training our neural model on relevant algorithms before training it on TSP instances. Our results demonstrate that, using this learning
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34394;&#25311;&#33410;&#28857;&#35843;&#25972;&#26041;&#27861;VNT&#26469;&#35299;&#20915;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#30340;&#25361;&#25112;&#12290;VNT&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22270;&#21464;&#25442;&#22120;&#32534;&#30721;&#22120;&#21644;&#23884;&#20837;&#31354;&#38388;&#20013;&#27880;&#20837;&#34394;&#25311;&#33410;&#28857;&#65292;&#24182;&#36890;&#36807;&#20803;&#23398;&#20064;&#20248;&#21270;&#34394;&#25311;&#33410;&#28857;&#20197;&#35843;&#33410;&#27599;&#20010;FSNC&#20219;&#21153;&#30340;&#33410;&#28857;&#23884;&#20837;&#12290;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#22522;&#31867;&#21035;&#20013;&#26377;&#31232;&#30095;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.06063</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#30340;&#34394;&#25311;&#33410;&#28857;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Virtual Node Tuning for Few-shot Node Classification. (arXiv:2306.06063v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06063
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34394;&#25311;&#33410;&#28857;&#35843;&#25972;&#26041;&#27861;VNT&#26469;&#35299;&#20915;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#30340;&#25361;&#25112;&#12290;VNT&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22270;&#21464;&#25442;&#22120;&#32534;&#30721;&#22120;&#21644;&#23884;&#20837;&#31354;&#38388;&#20013;&#27880;&#20837;&#34394;&#25311;&#33410;&#28857;&#65292;&#24182;&#36890;&#36807;&#20803;&#23398;&#20064;&#20248;&#21270;&#34394;&#25311;&#33410;&#28857;&#20197;&#35843;&#33410;&#27599;&#20010;FSNC&#20219;&#21153;&#30340;&#33410;&#28857;&#23884;&#20837;&#12290;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#22522;&#31867;&#21035;&#20013;&#26377;&#31232;&#30095;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#65288;FSNC&#65289;&#26159;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#25361;&#25112;&#65292;&#20165;&#26377;&#27599;&#20010;&#31867;&#21035;&#23569;&#37327;&#24050;&#26631;&#35760;&#33410;&#28857;&#21487;&#29992;&#20110;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20803;&#23398;&#20064;&#34987;&#25552;&#20986;&#26469;&#23558;&#32467;&#26500;&#30693;&#35782;&#20174;&#26377;&#20016;&#23500;&#26631;&#31614;&#30340;&#22522;&#31867;&#20013;&#36716;&#31227;&#33267;&#30446;&#26631;&#26032;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#24403;&#22522;&#31867;&#27809;&#26377;&#25110;&#26631;&#35760;&#33410;&#28857;&#38750;&#24120;&#23569;&#26102;&#65292;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#21464;&#24471;&#26080;&#25928;&#25110;&#19981;&#21487;&#34892;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#34394;&#25311;&#33410;&#28857;&#35843;&#25972;&#65288;VNT&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22270;&#21464;&#25442;&#22120;&#20316;&#20026;&#32534;&#30721;&#22120;&#65292;&#24182;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#27880;&#20837;&#34394;&#25311;&#33410;&#28857;&#20316;&#20026;&#36719;&#25552;&#31034;&#65292;&#21487;&#36890;&#36807;&#23569;&#37327;&#26679;&#26412;&#26631;&#31614;&#22312;&#26032;&#31867;&#21035;&#20013;&#36827;&#34892;&#20248;&#21270;&#65292;&#20197;&#35843;&#21046;&#27599;&#20010;&#29305;&#23450;&#30340;FSNC&#20219;&#21153;&#30340;&#33410;&#28857;&#23884;&#20837;&#12290; VNT&#30340;&#19968;&#39033;&#29420;&#29305;&#29305;&#28857;&#26159;&#65292;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#22270;&#30340;&#20266;&#25552;&#31034;&#28436;&#21270;&#65288;GPPE&#65289;&#27169;&#22359;&#65292;VNT-GPPE&#21487;&#20197;&#22788;&#29702;&#22522;&#31867;&#21035;&#20013;&#26377;&#31232;&#30095;&#26631;&#31614;&#30340;&#24773;&#20917;&#12290;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#20998;&#31867;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#22810;&#20010;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot Node Classification (FSNC) is a challenge in graph representation learning where only a few labeled nodes per class are available for training. To tackle this issue, meta-learning has been proposed to transfer structural knowledge from base classes with abundant labels to target novel classes. However, existing solutions become ineffective or inapplicable when base classes have no or limited labeled nodes. To address this challenge, we propose an innovative method dubbed Virtual Node Tuning (VNT). Our approach utilizes a pretrained graph transformer as the encoder and injects virtual nodes as soft prompts in the embedding space, which can be optimized with few-shot labels in novel classes to modulate node embeddings for each specific FSNC task. A unique feature of VNT is that, by incorporating a Graph-based Pseudo Prompt Evolution (GPPE) module, VNT-GPPE can handle scenarios with sparse labels in base classes. Experimental results on four datasets demonstrate the superiority o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;FIM&#65292;&#19968;&#31181;&#20174;&#28857;&#20113;&#25968;&#25454;&#20013;&#35745;&#31639;Fisher&#20449;&#24687;&#24230;&#37327;&#65288;FIM&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36830;&#32493;&#22320;&#23545;&#25968;&#25454;&#36827;&#34892;&#27969;&#24418;&#24314;&#27169;&#65292;&#20174;&#32780;&#25552;&#39640;&#27969;&#24418;&#29305;&#24449;&#30340;&#25551;&#36848;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.06062</link><description>&lt;p&gt;
&#20174;&#28857;&#20113;&#25968;&#25454;&#20013;&#23398;&#20064;Fisher&#20449;&#24687;&#30697;&#38453;&#30340;&#31070;&#32463;FIM
&lt;/p&gt;
&lt;p&gt;
Neural FIM for learning Fisher Information Metrics from point cloud data. (arXiv:2306.06062v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;FIM&#65292;&#19968;&#31181;&#20174;&#28857;&#20113;&#25968;&#25454;&#20013;&#35745;&#31639;Fisher&#20449;&#24687;&#24230;&#37327;&#65288;FIM&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36830;&#32493;&#22320;&#23545;&#25968;&#25454;&#36827;&#34892;&#27969;&#24418;&#24314;&#27169;&#65292;&#20174;&#32780;&#25552;&#39640;&#27969;&#24418;&#29305;&#24449;&#30340;&#25551;&#36848;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#25968;&#25454;&#25193;&#25955;&#23884;&#20837;&#22312;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#38543;&#22788;&#21487;&#35265;&#65292;&#24182;&#19988;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#25581;&#31034;&#25968;&#25454;&#28508;&#22312;&#20869;&#22312;&#20960;&#20309;&#30340;&#21487;&#34892;&#25216;&#26415;&#65292;&#20294;&#30001;&#20110;&#20854;&#31163;&#25955;&#24615;&#65292;&#25193;&#25955;&#23884;&#20837;&#22266;&#26377;&#30340;&#23616;&#38480;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31070;&#32463;FIM&#65292;&#19968;&#31181;&#20174;&#28857;&#20113;&#25968;&#25454;&#20013;&#35745;&#31639;Fisher&#20449;&#24687;&#24230;&#37327;&#65288;FIM&#65289;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#23545;&#25968;&#25454;&#36827;&#34892;&#36830;&#32493;&#27969;&#24418;&#24314;&#27169;&#12290;&#31070;&#32463;FIM&#20174;&#31163;&#25955;&#30340;&#28857;&#20113;&#25968;&#25454;&#20013;&#21019;&#24314;&#21487;&#25193;&#23637;&#30340;&#24230;&#37327;&#31354;&#38388;&#65292;&#22240;&#27492;&#20174;&#24230;&#37327;&#20013;&#33719;&#21462;&#30340;&#20449;&#24687;&#21487;&#20197;&#21578;&#35785;&#25105;&#20204;&#27969;&#24418;&#30340;&#29305;&#24449;&#65292;&#22914;&#20307;&#31215;&#21644;&#27979;&#22320;&#32447;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31070;&#32463;FIM&#22312;&#36873;&#25321;PHATE&#21487;&#35270;&#21270;&#26041;&#27861;&#30340;&#21442;&#25968;&#20197;&#21450;&#22312;&#29609;&#20855;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;IPSC&#37325;&#32534;&#31243;&#21644;PBMCs&#21333;&#32454;&#32990;&#25968;&#25454;&#38598;&#30340;&#20998;&#25903;&#28857;&#21644;&#32858;&#31867;&#20013;&#24515;&#23884;&#20837;&#20013;&#33719;&#21462;&#20449;&#24687;&#26041;&#38754;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although data diffusion embeddings are ubiquitous in unsupervised learning and have proven to be a viable technique for uncovering the underlying intrinsic geometry of data, diffusion embeddings are inherently limited due to their discrete nature. To this end, we propose neural FIM, a method for computing the Fisher information metric (FIM) from point cloud data - allowing for a continuous manifold model for the data. Neural FIM creates an extensible metric space from discrete point cloud data such that information from the metric can inform us of manifold characteristics such as volume and geodesics. We demonstrate Neural FIM's utility in selecting parameters for the PHATE visualization method as well as its ability to obtain information pertaining to local volume illuminating branching points and cluster centers embeddings of a toy dataset and two single-cell datasets of IPSC reprogramming and PBMCs (immune cells).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;K-means&#36827;&#34892;&#38750;&#27954;&#22899;&#24615;&#22270;&#20687;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#35782;&#21035;&#19981;&#21516;&#30340;&#38754;&#37096;&#32676;&#38598;&#21644;&#21457;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.06061</link><description>&lt;p&gt;
&#21033;&#29992;PCA&#21644;K-means&#23545;&#38750;&#27954;&#21457;&#22411;&#25968;&#25454;&#38598;&#36827;&#34892;&#32858;&#31867;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
clustering an african hairstyle dataset using pca and k-means. (arXiv:2306.06061v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;K-means&#36827;&#34892;&#38750;&#27954;&#22899;&#24615;&#22270;&#20687;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#35782;&#21035;&#19981;&#21516;&#30340;&#38754;&#37096;&#32676;&#38598;&#21644;&#21457;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#21270;&#36716;&#22411;&#24182;&#27809;&#26377;&#22312;&#26500;&#24314;&#38750;&#27954;&#38754;&#37096;&#24418;&#29366;&#20998;&#31867;&#22120;&#19978;&#24471;&#21040;&#34920;&#29616;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#29992;K-means&#23545;&#38750;&#27954;&#22899;&#24615;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#12290;&#38750;&#27954;&#22899;&#24615;&#22312;&#36873;&#25321;&#21512;&#36866;&#30340;&#21457;&#22411;&#26102;&#65292;&#20250;&#20381;&#36182;&#20110;&#32654;&#20029;&#30340;&#26631;&#20934;&#24314;&#35758;&#65292;&#20010;&#20154;&#20559;&#22909;&#25110;&#26368;&#26032;&#30340;&#21457;&#22411;&#36235;&#21183;&#12290;&#25991;&#31456;&#20351;&#29992;Haarcascade&#36827;&#34892;&#22522;&#20110;&#29305;&#24449;&#35757;&#32451;&#65292;&#24212;&#29992;K-means&#32858;&#31867;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#65292;&#20197;&#35782;&#21035;&#28508;&#22312;&#30340;&#38754;&#37096;&#32676;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The adoption of digital transformation was not expressed in building an African face shape classifier. In this paper, an approach is presented that uses k-means to classify African women images. African women rely on beauty standards recommendations, personal preference, or the newest trends in hairstyles to decide on the appropriate hairstyle for them. In this paper, an approach is presented that uses K-means clustering to classify African women's images. In order to identify potential facial clusters, Haarcascade is used for feature-based training, and K-means clustering is applied for image classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#24494;&#35843;&#23545;&#23569;&#26679;&#26412;&#19979;&#28216;&#20219;&#21153;&#30340;&#22806;&#20998;&#24067;&#26816;&#27979;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#36866;&#24403;&#36873;&#25321;&#22806;&#20998;&#24067;&#20998;&#25968;&#23545;&#20110;CLIP-based &#24494;&#35843;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#22823;&#27010;&#24565;&#21305;&#37197;&#65288;MCM&#65289;&#20998;&#25968;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.06048</link><description>&lt;p&gt;
&#24494;&#35843;&#23545;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22806;&#20998;&#24067;&#26816;&#27979;&#30340;&#24433;&#21709;&#26159;&#24590;&#26679;&#30340;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Does Fine-Tuning Impact Out-of-Distribution Detection for Vision-Language Models?. (arXiv:2306.06048v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#24494;&#35843;&#23545;&#23569;&#26679;&#26412;&#19979;&#28216;&#20219;&#21153;&#30340;&#22806;&#20998;&#24067;&#26816;&#27979;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#36866;&#24403;&#36873;&#25321;&#22806;&#20998;&#24067;&#20998;&#25968;&#23545;&#20110;CLIP-based &#24494;&#35843;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#22823;&#27010;&#24565;&#21305;&#37197;&#65288;MCM&#65289;&#20998;&#25968;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;CLIP&#65292;&#22312;&#22806;&#20998;&#24067;&#26816;&#27979;&#21644;&#27867;&#21270;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#38646;&#26679;&#26412;&#20869;&#20998;&#24067;&#20934;&#30830;&#24615;&#24448;&#24448;&#22312;&#19979;&#28216;&#25968;&#25454;&#38598;&#20013;&#21463;&#21040;&#38480;&#21046;&#12290;&#26368;&#36817;&#30340;&#22522;&#20110;CLIP&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#22914;&#25552;&#31034;&#23398;&#20064;&#65292;&#24050;&#32463;&#22312;&#23384;&#22312;&#22806;&#20998;&#24067;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25913;&#36827;&#20102;&#20869;&#20998;&#24067;&#20998;&#31867;&#21644;&#22806;&#20998;&#24067;&#27867;&#21270;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#23545;&#20110;&#27809;&#26377;&#22806;&#20998;&#24067;&#26631;&#31614;&#30340;&#35821;&#20041;&#36716;&#31227;&#26159;&#21542;&#21487;&#38752;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#26088;&#22312;&#23545;&#24494;&#35843;&#23545;&#20110;&#23569;&#26679;&#26412;&#19979;&#28216;&#20219;&#21153;&#30340;&#22806;&#20998;&#24067;&#26816;&#27979;&#30340;&#24433;&#21709;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#12290;&#36890;&#36807;&#23558;&#22806;&#20998;&#24067;&#26816;&#27979;&#26694;&#26550;&#21270;&#20026;&#22810;&#27169;&#24335;&#27010;&#24565;&#21305;&#37197;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#24494;&#35843;&#26041;&#27861;&#21644;&#21508;&#31181;&#22806;&#20998;&#24067;&#20998;&#25968;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36873;&#25321;&#36866;&#24403;&#30340;&#22806;&#20998;&#24067;&#20998;&#25968;&#23545;&#20110;&#22522;&#20110;CLIP&#30340;&#24494;&#35843;&#33267;&#20851;&#37325;&#35201;&#12290;&#29305;&#21035;&#26159;&#65292;&#26368;&#22823;&#27010;&#24565;&#21305;&#37197;&#65288;MCM&#65289;&#20998;&#25968;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent large vision-language models such as CLIP have shown remarkable out-of-distribution (OOD) detection and generalization performance. However, their zero-shot in-distribution (ID) accuracy is often limited for downstream datasets. Recent CLIP-based fine-tuning methods such as prompt learning have demonstrated significant improvements in ID classification and OOD generalization where OOD labels are available. Nonetheless, it remains unclear whether the model is reliable to semantic shifts without OOD labels. In this paper, we aim to bridge the gap and present a comprehensive study to understand how fine-tuning impact OOD detection for few-shot downstream tasks. By framing OOD detection as multi-modal concept matching, we establish a connection between fine-tuning methods and various OOD scores. Our results suggest that a proper choice of OOD scores is essential for CLIP-based fine-tuning. In particular, the maximum concept matching (MCM) score provides a promising solution consiste
&lt;/p&gt;</description></item><item><title>DYGR&#26159;&#19968;&#31181;&#29992;&#20110;&#20851;&#31995;&#25512;&#26029;&#30340;&#21160;&#24577;&#22270;&#20808;&#39564;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#39640;&#24230;&#38750;&#23616;&#37096;&#30340;&#22810;&#39033;&#24335;&#28388;&#27874;&#22120;&#20013;&#26500;&#36896;&#24615;&#22320;&#21033;&#29992;&#35823;&#24046;&#25918;&#22823;&#26469;&#29983;&#25104;&#29992;&#20110;&#22270;&#24418;&#23398;&#20064;&#30340;&#33391;&#22909;&#26799;&#24230;&#65292;&#24182;&#33021;&#22815;&#21516;&#26102;&#36866;&#29992;&#20110;&#20855;&#26377;&#20849;&#20139;&#22270;&#25299;&#25169;&#30340;&#8220;&#27973;&#23618;&#8221;&#19968;&#27493;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.06041</link><description>&lt;p&gt;
&#21160;&#24577;&#22270;&#37051;&#20808;&#39564;&#29992;&#20110;&#20851;&#31995;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
A Dynamical Graph Prior for Relational Inference. (arXiv:2306.06041v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06041
&lt;/p&gt;
&lt;p&gt;
DYGR&#26159;&#19968;&#31181;&#29992;&#20110;&#20851;&#31995;&#25512;&#26029;&#30340;&#21160;&#24577;&#22270;&#20808;&#39564;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#39640;&#24230;&#38750;&#23616;&#37096;&#30340;&#22810;&#39033;&#24335;&#28388;&#27874;&#22120;&#20013;&#26500;&#36896;&#24615;&#22320;&#21033;&#29992;&#35823;&#24046;&#25918;&#22823;&#26469;&#29983;&#25104;&#29992;&#20110;&#22270;&#24418;&#23398;&#20064;&#30340;&#33391;&#22909;&#26799;&#24230;&#65292;&#24182;&#33021;&#22815;&#21516;&#26102;&#36866;&#29992;&#20110;&#20855;&#26377;&#20849;&#20139;&#22270;&#25299;&#25169;&#30340;&#8220;&#27973;&#23618;&#8221;&#19968;&#27493;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25512;&#26029;&#26088;&#22312;&#20174;&#35266;&#23519;&#21040;&#30340;&#21160;&#24577;&#31995;&#32479;&#20013;&#35782;&#21035;&#37096;&#20214;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26159;&#22312;&#21487;&#23398;&#20064;&#30340;&#22270;&#19978;&#25311;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476; (GNN) &#26469;&#36827;&#34892;&#20851;&#31995;&#25512;&#26029;&#12290;&#23427;&#20204;&#20351;&#29992;&#19968;&#27493;&#28040;&#24687;&#20256;&#36882; GNN--&#30452;&#35266;&#19978;&#26469;&#35828;&#26159;&#27491;&#30830;&#30340;&#36873;&#25321;&#65292;&#22240;&#20026;&#22810;&#27493;&#25110;&#35889; GNN &#30340;&#38750;&#23616;&#37096;&#24615;&#21487;&#33021;&#20250;&#28151;&#28102;&#30452;&#25509;&#21644;&#38388;&#25509;&#30456;&#20114;&#20316;&#29992;&#12290;&#20294;&#26159;&#8220;&#26377;&#25928;&#8221;&#30340;&#20132;&#20114;&#22270;&#21462;&#20915;&#20110;&#37319;&#26679;&#36895;&#29575;&#65292;&#24456;&#23569;&#23616;&#38480;&#20110;&#30452;&#25509;&#37051;&#23621;&#65292;&#23548;&#33268;&#19968;&#20010;&#27493;&#39588;&#27169;&#22411;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#21160;&#24577;&#22270;&#20808;&#39564;&#8221;(DYGR)&#26469;&#36827;&#34892;&#20851;&#31995;&#25512;&#26029;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#20808;&#39564;&#30340;&#21407;&#22240;&#26159;&#65292;&#19982;&#24050;&#26377;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#23427;&#22312;&#39640;&#24230;&#38750;&#23616;&#37096;&#30340;&#22810;&#39033;&#24335;&#28388;&#27874;&#22120;&#20013;&#26500;&#36896;&#24615;&#22320;&#21033;&#29992;&#35823;&#24046;&#25918;&#22823;&#26469;&#29983;&#25104;&#29992;&#20110;&#22270;&#24418;&#23398;&#20064;&#30340;&#33391;&#22909;&#26799;&#24230;&#12290;&#20026;&#20102;&#22788;&#29702;&#38750;&#21807;&#19968;&#24615;&#65292;DYGR &#21516;&#26102;&#36866;&#29992;&#20110;&#20855;&#26377;&#20849;&#20139;&#22270;&#25299;&#25169;&#30340;&#8220;&#27973;&#23618;&#8221;&#19968;&#27493;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126; DYGR &#33021;&#22815;&#37325;&#26032;&#26500;&#24314;&#20132;&#20114;&#32467;&#26500;&#65292;&#21516;&#26102;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relational inference aims to identify interactions between parts of a dynamical system from the observed dynamics. Current state-of-the-art methods fit a graph neural network (GNN) on a learnable graph to the dynamics. They use one-step message-passing GNNs -- intuitively the right choice since non-locality of multi-step or spectral GNNs may confuse direct and indirect interactions. But the \textit{effective} interaction graph depends on the sampling rate and it is rarely localized to direct neighbors, leading to local minima for the one-step model. In this work, we propose a \textit{dynamical graph prior} (DYGR) for relational inference. The reason we call it a prior is that, contrary to established practice, it constructively uses error amplification in high-degree non-local polynomial filters to generate good gradients for graph learning. To deal with non-uniqueness, DYGR simultaneously fits a ``shallow'' one-step model with shared graph topology. Experiments show that DYGR reconstr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Transformer&#32593;&#32476;&#26469;&#37325;&#24314;&#38050;&#29748;&#28436;&#22863;&#20013;&#20154;&#31867;&#34920;&#29616;&#21147;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#36716;&#24405;&#20048;&#35889;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#25972;&#21512;&#38050;&#29748;&#23478;&#36523;&#20221;&#20197;&#25511;&#21046;&#37319;&#26679;&#36807;&#31243;&#65292;&#24182;&#25104;&#21151;&#29983;&#25104;&#20102;&#39640;&#24230;&#31867;&#20154;&#30340;&#38050;&#29748;&#34920;&#28436;&#12290;</title><link>http://arxiv.org/abs/2306.06040</link><description>&lt;p&gt;
&#21033;&#29992;Transformer&#32593;&#32476;&#37325;&#24314;&#38050;&#29748;&#28436;&#22863;&#20013;&#20154;&#31867;&#34920;&#29616;&#21147;
&lt;/p&gt;
&lt;p&gt;
Reconstructing Human Expressiveness in Piano Performances with a Transformer Network. (arXiv:2306.06040v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Transformer&#32593;&#32476;&#26469;&#37325;&#24314;&#38050;&#29748;&#28436;&#22863;&#20013;&#20154;&#31867;&#34920;&#29616;&#21147;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#36716;&#24405;&#20048;&#35889;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#25972;&#21512;&#38050;&#29748;&#23478;&#36523;&#20221;&#20197;&#25511;&#21046;&#37319;&#26679;&#36807;&#31243;&#65292;&#24182;&#25104;&#21151;&#29983;&#25104;&#20102;&#39640;&#24230;&#31867;&#20154;&#30340;&#38050;&#29748;&#34920;&#28436;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#35745;&#31639;&#26041;&#27861;&#25429;&#25417;&#20154;&#31867;&#38899;&#20048;&#28436;&#22863;&#20013;&#22797;&#26434;&#24494;&#22937;&#30340;&#34920;&#29616;&#21147;&#21464;&#21270;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#23618;&#21452;&#21521;Transformer&#32534;&#30721;&#22120;&#37325;&#24314;&#38050;&#29748;&#28436;&#22863;&#20013;&#30340;&#20154;&#31867;&#34920;&#29616;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#22823;&#37327;&#20934;&#30830;&#25429;&#25417;&#21644;&#24471;&#20998;&#23545;&#40784;&#30340;&#28436;&#22863;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#25105;&#20204;&#20351;&#29992;&#20174;&#29616;&#26377;&#36716;&#24405;&#27169;&#22411;&#33719;&#21462;&#30340;&#36716;&#24405;&#20048;&#35889;&#26469;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#25972;&#21512;&#20102;&#38050;&#29748;&#23478;&#36523;&#20221;&#20197;&#25511;&#21046;&#37319;&#26679;&#36807;&#31243;&#65292;&#24182;&#25506;&#35752;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#27169;&#25311;&#19981;&#21516;&#38050;&#29748;&#23478;&#34920;&#29616;&#21147;&#21464;&#21270;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#29983;&#25104;&#30340;&#34920;&#29616;&#21147;&#28436;&#22863;&#30340;&#32479;&#35745;&#20998;&#26512;&#21644;&#21548;&#21147;&#27979;&#35797;&#23545;&#31995;&#32479;&#36827;&#34892;&#35780;&#20272;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20174;&#36716;&#24405;&#30340;&#20048;&#35889;&#20013;&#29983;&#25104;&#31867;&#20154;&#38050;&#29748;&#28436;&#22863;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#65292;&#21516;&#26102;&#20805;&#20998;&#21644;&#19968;&#33268;&#22320;&#37325;&#24314;&#20102;&#20154;&#31867;&#34920;&#29616;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Capturing intricate and subtle variations in human expressiveness in music performance using computational approaches is challenging. In this paper, we propose a novel approach for reconstructing human expressiveness in piano performance with a multi-layer bi-directional Transformer encoder. To address the needs for large amounts of accurately captured and score-aligned performance data in training neural networks, we use transcribed scores obtained from an existing transcription model to train our model. We integrate pianist identities to control the sampling process and explore the ability of our system to model variations in expressiveness for different pianists. The system is evaluated through statistical analysis of generated expressive performances and a listening test. Overall, the results suggest that our method achieves state-of-the-art in generating human-like piano performances from transcribed scores, while fully and consistently reconstructing human expressiveness poses fu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;RANS-PINN&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;2&#26041;&#31243;&#28065;&#31896;&#24230;&#27169;&#22411;&#65292;&#21487;&#39044;&#27979;&#39640;&#38647;&#35834;&#25968;&#28237;&#27969;&#27969;&#21160;&#20013;&#30340;&#27969;&#22330;&#65292;&#20174;&#32780;&#25552;&#39640;&#27969;&#20307;&#21160;&#21147;&#23398;&#27169;&#25311;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.06034</link><description>&lt;p&gt;
&#22522;&#20110;RANS-PINN&#30340;&#27169;&#25311;&#20195;&#29702;&#39044;&#27979;&#28237;&#27969;&#27969;&#21160;
&lt;/p&gt;
&lt;p&gt;
RANS-PINN based Simulation Surrogates for Predicting Turbulent Flows. (arXiv:2306.06034v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;RANS-PINN&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;2&#26041;&#31243;&#28065;&#31896;&#24230;&#27169;&#22411;&#65292;&#21487;&#39044;&#27979;&#39640;&#38647;&#35834;&#25968;&#28237;&#27969;&#27969;&#21160;&#20013;&#30340;&#27969;&#22330;&#65292;&#20174;&#32780;&#25552;&#39640;&#27969;&#20307;&#21160;&#21147;&#23398;&#27169;&#25311;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#30693;&#35782;&#24341;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#20026;&#24314;&#31435;&#30001;&#24494;&#20998;&#26041;&#31243;&#25511;&#21046;&#30340;&#21160;&#24577;&#31995;&#32479;&#30340;&#20195;&#29702;&#27169;&#22411;&#25552;&#20379;&#20102;&#26694;&#26550;&#12290; PINN&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#20250;&#36890;&#36807;&#25439;&#22833;&#20989;&#25968;&#20013;&#30340;&#29289;&#29702;&#22522;&#30784;&#27491;&#21017;&#21270;&#39033;&#26469;&#22686;&#24378;&#27867;&#21270;&#24615;&#33021;&#12290;&#30001;&#20110;&#27169;&#25311;&#30001;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#25511;&#21046;&#30340;&#21160;&#24577;&#21487;&#33021;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#65292;PINN&#24050;&#32463;&#22312;&#23398;&#20064;&#30001;Navier-Stokes&#26041;&#31243;&#25511;&#21046;&#30340;&#28082;&#20307;&#27969;&#21160;&#38382;&#39064;&#30340;&#21442;&#25968;&#20195;&#29702;&#26041;&#38754;&#24191;&#21463;&#27426;&#36814;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;RANS-PINN&#65292;&#19968;&#31181;&#20462;&#25913;&#21518;&#30340;PINN&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#39640;&#38647;&#35834;&#25968;&#28237;&#27969;&#27969;&#21160;&#20013;&#30340;&#27969;&#22330;&#65288;&#21363;&#36895;&#24230;&#21644;&#21387;&#21147;&#65289;&#12290;&#20026;&#20102;&#32771;&#34385;&#28237;&#27969;&#24341;&#20837;&#30340;&#39069;&#22806;&#22797;&#26434;&#24615;&#65292;RANS-PINN&#37319;&#29992;&#22522;&#20110;&#38647;&#35834;&#24179;&#22343;Navier-Stokes&#65288;RANS&#65289;&#30340;2&#26041;&#31243;&#28065;&#31896;&#24230;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#30830;&#20445;&#21508;&#20010;&#32452;&#25104;&#37096;&#20998;&#30340;&#26377;&#25928;&#21021;&#22987;&#21270;&#21644;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINNs) provide a framework to build surrogate models for dynamical systems governed by differential equations. During the learning process, PINNs incorporate a physics-based regularization term within the loss function to enhance generalization performance. Since simulating dynamics controlled by partial differential equations (PDEs) can be computationally expensive, PINNs have gained popularity in learning parametric surrogates for fluid flow problems governed by Navier-Stokes equations. In this work, we introduce RANS-PINN, a modified PINN framework, to predict flow fields (i.e., velocity and pressure) in high Reynolds number turbulent flow regime. To account for the additional complexity introduced by turbulence, RANS-PINN employs a 2-equation eddy viscosity model based on a Reynolds-averaged Navier-Stokes (RANS) formulation. Furthermore, we adopt a novel training approach that ensures effective initialization and balance among the various component
&lt;/p&gt;</description></item><item><title>FinGPT&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#21487;&#35775;&#38382;&#21644;&#36879;&#26126;&#30340;&#36164;&#28304;&#26469;&#24320;&#21457;&#37329;&#34701;LLMs&#65292;&#20854;&#37325;&#35201;&#24615;&#22312;&#20110;&#33258;&#21160;&#25968;&#25454;&#31579;&#36873;&#31649;&#36947;&#21644;&#36731;&#37327;&#32423;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2306.06031</link><description>&lt;p&gt;
FinGPT&#65306;&#24320;&#28304;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FinGPT: Open-Source Financial Large Language Models. (arXiv:2306.06031v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06031
&lt;/p&gt;
&lt;p&gt;
FinGPT&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#21487;&#35775;&#38382;&#21644;&#36879;&#26126;&#30340;&#36164;&#28304;&#26469;&#24320;&#21457;&#37329;&#34701;LLMs&#65292;&#20854;&#37325;&#35201;&#24615;&#22312;&#20110;&#33258;&#21160;&#25968;&#25454;&#31579;&#36873;&#31649;&#36947;&#21644;&#36731;&#37327;&#32423;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#22312;&#21508;&#20010;&#39046;&#22495;&#38761;&#26032;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#28508;&#21147;&#65292;&#24341;&#36215;&#20102;&#37329;&#34701;&#39046;&#22495;&#30340;&#27987;&#21402;&#20852;&#36259;&#12290;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#37329;&#34701;&#25968;&#25454;&#26159;&#37329;&#34701;LLMs&#65288;FinLLMs&#65289;&#30340;&#31532;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#37329;&#34701;&#39046;&#22495;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;FinGPT&#12290;&#19982;&#19987;&#26377;&#27169;&#22411;&#19981;&#21516;&#65292;FinGPT&#37319;&#29992;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#21487;&#35775;&#38382;&#21644;&#36879;&#26126;&#30340;&#36164;&#28304;&#26469;&#24320;&#21457;&#20182;&#20204;&#30340;&#37329;&#34701;LLMs&#12290;&#25105;&#20204;&#24378;&#35843;&#33258;&#21160;&#25968;&#25454;&#31579;&#36873;&#31649;&#36947;&#21644;&#36731;&#37327;&#32423;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#22312;&#24314;&#31435;FinGPT&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20960;&#20010;&#28508;&#22312;&#30340;&#24212;&#29992;&#20316;&#20026;&#29992;&#25143;&#30340;&#22522;&#30784;&#65292;&#22914;&#26426;&#22120;&#39038;&#38382;&#12289;&#31639;&#27861;&#20132;&#26131;&#21644;&#35770; &#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown the potential of revolutionizing natural language processing tasks in diverse domains, sparking great interest in finance. Accessing high-quality financial data is the first challenge for financial LLMs (FinLLMs). While proprietary models like BloombergGPT have taken advantage of their unique data accumulation, such privileged access calls for an open-source alternative to democratize Internet-scale financial data.  In this paper, we present an open-source large language model, FinGPT, for the finance sector. Unlike proprietary models, FinGPT takes a data-centric approach, providing researchers and practitioners with accessible and transparent resources to develop their FinLLMs. We highlight the importance of an automatic data curation pipeline and the lightweight low-rank adaptation technique in building FinGPT. Furthermore, we showcase several potential applications as stepping stones for users, such as robo-advising, algorithmic trading, and l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#35299;&#37322;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;CounTS&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#21453;&#20107;&#23454;&#21644;&#21487;&#25805;&#20316;&#30340;&#35299;&#37322;&#65292;&#36866;&#29992;&#20110;&#20851;&#38190;&#39046;&#22495;&#22914;&#21307;&#30103;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#20026;&#21487;&#35299;&#37322;&#24615;&#24314;&#27169;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2306.06024</link><description>&lt;p&gt;
&#21487;&#33258;&#25105;&#35299;&#37322;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#19982;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Self-Interpretable Time Series Prediction with Counterfactual Explanations. (arXiv:2306.06024v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#35299;&#37322;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;CounTS&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#21453;&#20107;&#23454;&#21644;&#21487;&#25805;&#20316;&#30340;&#35299;&#37322;&#65292;&#36866;&#29992;&#20110;&#20851;&#38190;&#39046;&#22495;&#22914;&#21307;&#30103;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#20026;&#21487;&#35299;&#37322;&#24615;&#24314;&#27169;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#23545;&#20110;&#20687;&#21307;&#30103;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21516;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24605;&#36335;&#65292;&#26088;&#22312;&#24320;&#21457;&#20986;&#19968;&#31181;&#33258;&#25105;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#34987;&#31216;&#20026;Counterfactual Time Series&#65288;CounTS&#65289;&#65292;&#35813;&#27169;&#22411;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#29983;&#25104;&#21453;&#20107;&#23454;&#21644;&#21487;&#25805;&#20316;&#30340;&#35299;&#37322;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24418;&#24335;&#21270;&#20102;&#26102;&#38388;&#24207;&#21015;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#38382;&#39064;&#65292;&#24314;&#31435;&#20102;&#30456;&#24212;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#26102;&#38388;&#24207;&#21015;&#32465;&#26550;&#12289;&#34892;&#21160;&#21644;&#39044;&#27979;&#21453;&#20107;&#23454;&#25512;&#29702;&#33021;&#21147;&#30340;&#21464;&#20998;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#33258;&#25105;&#35299;&#37322;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#26356;&#22909;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#24403;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretable time series prediction is crucial for safety-critical areas such as healthcare and autonomous driving. Most existing methods focus on interpreting predictions by assigning important scores to segments of time series. In this paper, we take a different and more challenging route and aim at developing a self-interpretable model, dubbed Counterfactual Time Series (CounTS), which generates counterfactual and actionable explanations for time series predictions. Specifically, we formalize the problem of time series counterfactual explanations, establish associated evaluation protocols, and propose a variational Bayesian deep learning model equipped with counterfactual inference capability of time series abduction, action, and prediction. Compared with state-of-the-art baselines, our self-interpretable model can generate better counterfactual explanations while maintaining comparable prediction accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#22788;&#29702;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#30340;&#20998;&#24067;&#24335;&#20849;&#35782;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26368;&#23567;&#21270;&#27599;&#20010;&#26102;&#38388;&#27493;&#19981;&#36873;&#25321;&#26368;&#20248;&#33218;&#25152;&#20135;&#29983;&#30340;&#26399;&#26395;&#24635;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2306.05998</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#20998;&#24067;&#24335;&#20849;&#35782;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Distributed Consensus Algorithm for Decision-Making in Multi-agent Multi-armed Bandit. (arXiv:2306.05998v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#22788;&#29702;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#30340;&#20998;&#24067;&#24335;&#20849;&#35782;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26368;&#23567;&#21270;&#27599;&#20010;&#26102;&#38388;&#27493;&#19981;&#36873;&#25321;&#26368;&#20248;&#33218;&#25152;&#20135;&#29983;&#30340;&#26399;&#26395;&#24635;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21160;&#24577;&#29615;&#22659;&#20013;&#32467;&#26500;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#12290;&#22270;&#24418;&#21453;&#26144;&#20102;&#20195;&#29702;&#20043;&#38388;&#30340;&#20449;&#24687;&#20849;&#20139;&#32467;&#26500;&#65292;&#33218;&#30340;&#22870;&#21169;&#20998;&#24067;&#26159;&#20855;&#26377;&#20960;&#20010;&#26410;&#30693;&#21464;&#21270;&#28857;&#30340;&#20998;&#27573;&#23450;&#24120;&#20998;&#24067;&#12290;&#20195;&#29702;&#38754;&#20020;&#30456;&#21516;&#30340;&#20998;&#27573;&#23450;&#24120;MAB&#38382;&#39064;&#12290;&#30446;&#26631;&#26159;&#20026;&#20195;&#29702;&#24320;&#21457;&#19968;&#20010;&#20915;&#31574;&#31574;&#30053;&#65292;&#20351;&#24471;&#21518;&#24724;&#26368;&#23567;&#65292;&#21363;&#27599;&#20010;&#26102;&#38388;&#27493;&#19981;&#36873;&#25321;&#26368;&#20248;&#33218;&#25152;&#20135;&#29983;&#30340;&#26399;&#26395;&#24635;&#25439;&#22833;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#20855;&#26377;&#36125;&#21494;&#26031;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979;&#30340;&#37325;&#26032;&#21551;&#21160;&#21327;&#20316;UCB&#31639;&#27861;(RBO-Coop-UCB)&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#39640;&#25928;&#30340;&#22810;&#26234;&#33021;&#20307;UCB&#31639;&#27861;&#20316;&#20026;&#26680;&#24515;&#65292;&#24182;&#25552;&#21319;&#20102;&#21512;&#20316;&#20915;&#31574;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;RBO-Coop-UCB&#30340;&#39044;&#26399;&#22242;&#20307;&#21518;&#24724;&#19978;&#30028;&#20026;$\mathcal{O}(KNM\log T + K\sqrt{MT\log T})$&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a structured multi-agent multi-armed bandit (MAMAB) problem in a dynamic environment. A graph reflects the information-sharing structure among agents, and the arms' reward distributions are piecewise-stationary with several unknown change points. The agents face the identical piecewise-stationary MAB problem. The goal is to develop a decision-making policy for the agents that minimizes the regret, which is the expected total loss of not playing the optimal arm at each time step. Our proposed solution, Restarted Bayesian Online Change Point Detection in Cooperative Upper Confidence Bound Algorithm (RBO-Coop-UCB), involves an efficient multi-agent UCB algorithm as its core enhanced with a Bayesian change point detector. We also develop a simple restart decision cooperation that improves decision-making. Theoretically, we establish that the expected group regret of RBO-Coop-UCB is upper bounded by $\mathcal{O}(KNM\log T + K\sqrt{MT\log T})$, where K is the number of agents, M is 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24490;&#29615;Q-learning&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#19979;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36817;&#20284;&#20449;&#24687;&#29366;&#24577;&#30340;&#21464;&#20307;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#34920;&#29616;&#19978;&#20248;&#20110;&#24378;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2306.05991</link><description>&lt;p&gt;
&#22522;&#20110;&#36817;&#20284;&#20449;&#24687;&#29366;&#24577;&#30340;&#24490;&#29615;Q-learning&#25910;&#25947;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Approximate information state based convergence analysis of recurrent Q-learning. (arXiv:2306.05991v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24490;&#29615;Q-learning&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#19979;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36817;&#20284;&#20449;&#24687;&#29366;&#24577;&#30340;&#21464;&#20307;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#34920;&#29616;&#19978;&#20248;&#20110;&#24378;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26377;&#24456;&#22810;&#38024;&#23545;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25991;&#29486;&#65292;&#20294;&#20173;&#32570;&#20047;&#23436;&#25972;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#24773;&#20917;&#19979;&#65292;&#20195;&#29702;&#21487;&#29992;&#30340;&#25968;&#25454;&#21382;&#21490;&#20250;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32780;&#22686;&#21152;&#65292;&#22240;&#27492;&#22823;&#22810;&#25968;&#23454;&#29992;&#31639;&#27861;&#35201;&#20040;&#23558;&#21382;&#21490;&#25130;&#26029;&#21040;&#26377;&#38480;&#30340;&#31383;&#21475;&#65292;&#35201;&#20040;&#20351;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21387;&#32553;&#65292;&#23548;&#33268;&#20195;&#29702;&#29366;&#24577;&#19981;&#26159;&#39532;&#23572;&#21487;&#22827;&#30340;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#23613;&#31649;&#32570;&#20047;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#65292;&#24490;&#29615;Q-learning&#65288;RQL&#65289;&#22312;&#34920;&#26684;&#35774;&#32622;&#20013;&#20063;&#20250;&#25910;&#25947;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#35777;&#26126;&#20102;&#25910;&#25947;&#26497;&#38480;&#30340;&#36136;&#37327;&#21462;&#20915;&#20110;&#34920;&#31034;&#24418;&#24335;&#30340;&#36136;&#37327;&#65292;&#36825;&#26159;&#20351;&#29992;&#36817;&#20284;&#20449;&#24687;&#29366;&#24577;&#65288;AIS&#65289;&#26469;&#37327;&#21270;&#30340;&#12290;&#22522;&#20110;&#36825;&#31181;&#36817;&#20284;&#35823;&#24046;&#30340;&#34920;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;AIS&#25439;&#22833;&#30340;RQL&#21464;&#20307;&#12290;&#19982;&#19981;&#20351;&#29992;AIS&#25439;&#22833;&#30340;RQL&#24378;&#22522;&#32447;&#30456;&#27604;&#65292;&#36825;&#31181;&#21464;&#20307;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In spite of the large literature on reinforcement learning (RL) algorithms for partially observable Markov decision processes (POMDPs), a complete theoretical understanding is still lacking. In a partially observable setting, the history of data available to the agent increases over time so most practical algorithms either truncate the history to a finite window or compress it using a recurrent neural network leading to an agent state that is non-Markovian. In this paper, it is shown that in spite of the lack of the Markov property, recurrent Q-learning (RQL) converges in the tabular setting. Moreover, it is shown that the quality of the converged limit depends on the quality of the representation which is quantified in terms of what is known as an approximate information state (AIS). Based on this characterization of the approximation error, a variant of RQL with AIS losses is presented. This variant performs better than a strong baseline for RQL that does not use AIS losses. It is de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QBSD&#30340;&#23454;&#26102;&#39044;&#27979;&#26041;&#27861;&#65292;&#20197;&#22312;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#20013;&#21462;&#24471;&#26368;&#20339;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2306.05989</link><description>&lt;p&gt;
&#22522;&#20110;&#22235;&#20998;&#20301;&#25968;&#30340;&#23395;&#33410;&#24615;&#20998;&#35299;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#21644;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Quartile-Based Seasonality Decomposition for Time Series Forecasting and Anomaly Detection. (arXiv:2306.05989v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QBSD&#30340;&#23454;&#26102;&#39044;&#27979;&#26041;&#27861;&#65292;&#20197;&#22312;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#20013;&#21462;&#24471;&#26368;&#20339;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#20449;&#39046;&#22495;&#65292;&#21450;&#26102;&#26816;&#27979;&#24322;&#24120;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#36825;&#26377;&#21161;&#20110;&#35782;&#21035;&#21644;&#34920;&#24449;&#19981;&#35268;&#21017;&#27169;&#24335;&#12289;&#24322;&#24120;&#34892;&#20026;&#21644;&#32593;&#32476;&#24322;&#24120;&#65292;&#20174;&#32780;&#25552;&#39640;&#26381;&#21153;&#36136;&#37327;&#21644;&#25805;&#20316;&#25928;&#29575;&#12290;&#31934;&#30830;&#22320;&#39044;&#27979;&#21644;&#28040;&#38500;&#21487;&#39044;&#27979;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#24335;&#26159;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#22235;&#20998;&#20301;&#25968;&#30340;&#23395;&#33410;&#24615;&#20998;&#35299;&#65288;QBSD&#65289;&#30340;&#23454;&#26102;&#39044;&#27979;&#26041;&#27861;&#65292;&#20197;&#22312;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#39044;&#27979;&#20934;&#30830;&#29575;&#20043;&#38388;&#21462;&#24471;&#26368;&#20339;&#24179;&#34913;&#12290;&#26412;&#25991;&#27604;&#36739;&#20102;QBSD&#19982;&#29616;&#26377;&#39044;&#27979;&#26041;&#27861;&#30340;&#24615;&#33021;&#21450;&#20854;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The timely detection of anomalies is essential in the telecom domain as it facilitates the identification and characterization of irregular patterns, abnormal behaviors, and network anomalies, contributing to enhanced service quality and operational efficiency. Precisely forecasting and eliminating predictable time series patterns constitutes a vital component of time series anomaly detection. While the state-of-the-art methods aim to maximize forecasting accuracy, the computational performance takes a hit. In a system composed of a large number of time series variables, e.g., cell Key Performance Indicators (KPIs), the time and space complexity of the forecasting employed is of crucial importance. Quartile-Based Seasonality Decomposition (QBSD) is a live forecasting method proposed in this paper to make an optimal trade-off between computational complexity and forecasting accuracy. This paper compares the performance of QBSD to the state-of-the-art forecasting methods and their applic
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#20195;&#29702;&#24066;&#22330;&#35746;&#21333;&#30340;&#34920;&#31034;&#12290;&#36827;&#19968;&#27493;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;K&#22343;&#20540;&#32858;&#31867;&#31639;&#27861;&#23545;&#20195;&#29702;&#35746;&#21333;&#30340;&#23398;&#20064;&#34920;&#31034;&#21521;&#37327;&#36827;&#34892;&#32858;&#31867;&#65292;&#20197;&#30830;&#23450;&#27599;&#20010;&#31751;&#20013;&#30340;&#19981;&#21516;&#34892;&#20026;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.05987</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#20195;&#29702;&#24066;&#22330;&#35746;&#21333;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Agent market orders representation through a contrastive learning approach. (arXiv:2306.05987v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05987
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#20195;&#29702;&#24066;&#22330;&#35746;&#21333;&#30340;&#34920;&#31034;&#12290;&#36827;&#19968;&#27493;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;K&#22343;&#20540;&#32858;&#31867;&#31639;&#27861;&#23545;&#20195;&#29702;&#35746;&#21333;&#30340;&#23398;&#20064;&#34920;&#31034;&#21521;&#37327;&#36827;&#34892;&#32858;&#31867;&#65292;&#20197;&#30830;&#23450;&#27599;&#20010;&#31751;&#20013;&#30340;&#19981;&#21516;&#34892;&#20026;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35775;&#38382;Euronext&#30340;CAC40&#25968;&#25454;&#20013;&#30340;&#26631;&#35760;&#35746;&#21333;&#65292;&#20998;&#26512;&#20195;&#29702;&#22312;&#24066;&#22330;&#20013;&#26681;&#25454;&#20854;&#19979;&#36798;&#30340;&#35746;&#21333;&#30340;&#34892;&#20026;&#12290;&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#19977;&#20803;&#32452;&#25439;&#22833;&#26469;&#26377;&#25928;&#22320;&#23398;&#20064;&#20195;&#29702;&#24066;&#22330;&#35746;&#21333;&#30340;&#34920;&#31034;&#12290;&#36890;&#36807;&#33719;&#21462;&#36825;&#20010;&#23398;&#20064;&#34920;&#31034;&#65292;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#21464;&#24471;&#21487;&#34892;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;K&#22343;&#20540;&#32858;&#31867;&#31639;&#27861;&#23545;&#20195;&#29702;&#35746;&#21333;&#30340;&#23398;&#20064;&#34920;&#31034;&#21521;&#37327;&#36827;&#34892;&#32858;&#31867;&#65292;&#20197;&#30830;&#23450;&#27599;&#20010;&#31751;&#20013;&#30340;&#19981;&#21516;&#34892;&#20026;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the access to the labeled orders on the CAC40 data from Euronext, we are able to analyse agents' behaviours in the market based on their placed orders. In this study, we construct a self-supervised learning model using triplet loss to effectively learn the representation of agent market orders. By acquiring this learned representation, various downstream tasks become feasible. In this work, we utilise the K-means clustering algorithm on the learned representation vectors of agent orders to identify distinct behaviour types within each cluster.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#33258;&#23450;&#20041;&#28151;&#21512;&#33410;&#28857; Forney &#26679;&#24335;&#30340;&#22240;&#23376;&#22270;&#28040;&#24687;&#20256;&#36882;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#33258;&#21160;&#21270;&#36125;&#21494;&#26031;&#27169;&#22411;&#24179;&#22343;&#12289;&#36873;&#25321;&#21644;&#32452;&#21512;&#65292;&#24182;&#32553;&#30701;&#20102;&#27169;&#22411;&#35774;&#35745;&#21608;&#26399;&#12290;</title><link>http://arxiv.org/abs/2306.05965</link><description>&lt;p&gt;
&#22312;&#22240;&#23376;&#22270;&#20013;&#33258;&#21160;&#36827;&#34892;&#27169;&#22411;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Automating Model Comparison in Factor Graphs. (arXiv:2306.05965v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#33258;&#23450;&#20041;&#28151;&#21512;&#33410;&#28857; Forney &#26679;&#24335;&#30340;&#22240;&#23376;&#22270;&#28040;&#24687;&#20256;&#36882;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#33258;&#21160;&#21270;&#36125;&#21494;&#26031;&#27169;&#22411;&#24179;&#22343;&#12289;&#36873;&#25321;&#21644;&#32452;&#21512;&#65292;&#24182;&#32553;&#30701;&#20102;&#27169;&#22411;&#35774;&#35745;&#21608;&#26399;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#29486;&#20013;&#65292;&#36125;&#21494;&#26031;&#29366;&#24577;&#21644;&#21442;&#25968;&#20272;&#35745;&#24050;&#32463;&#34987;&#26377;&#25928;&#33258;&#21160;&#21270;&#65292;&#20294;&#23545;&#20110;&#27169;&#22411;&#27604;&#36739;&#23578;&#26410;&#22914;&#27492;&#65292;&#22240;&#27492;&#20173;&#38656;&#35201;&#23481;&#26131;&#20986;&#38169;&#21644;&#32791;&#26102;&#30340;&#25163;&#21160;&#25512;&#23548;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#27604;&#36739;&#32463;&#24120;&#34987;&#24573;&#35270;&#21644;&#24573;&#30053;&#65292;&#23613;&#31649;&#23427;&#24456;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;Forney&#26679;&#24335;&#30340;&#22240;&#23376;&#22270;&#19978;&#20351;&#29992;&#33258;&#23450;&#20041;&#28151;&#21512;&#33410;&#28857;&#19978;&#30340;&#28040;&#24687;&#20256;&#36882;&#26469;&#39640;&#25928;&#22320;&#33258;&#21160;&#21270;&#36125;&#21494;&#26031;&#27169;&#22411;&#24179;&#22343;&#12289;&#36873;&#25321;&#21644;&#32452;&#21512;&#12290;&#36827;&#32780;&#21487;&#20351;&#29992;&#32553;&#25918;&#22240;&#23376;&#21516;&#26102;&#25191;&#34892;&#21442;&#25968;&#21644;&#29366;&#24577;&#25512;&#26029;&#20197;&#21450;&#27169;&#22411;&#27604;&#36739;&#12290;&#36825;&#31181;&#26041;&#27861;&#32553;&#30701;&#20102;&#27169;&#22411;&#35774;&#35745;&#21608;&#26399;&#65292;&#21516;&#26102;&#20801;&#35768;&#31616;&#21333;&#22320;&#25193;&#23637;&#21040;&#20998;&#23618;&#21644;&#26102;&#38388;&#27169;&#22411;&#20808;&#39564;&#65292;&#20197;&#36866;&#24212;&#24314;&#27169;&#22797;&#26434;&#30340;&#26102;&#21464;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian state and parameter estimation have been automated effectively in the literature, however, this has not yet been the case for model comparison, which therefore still requires error-prone and time-consuming manual derivations. As a result, model comparison is often overlooked and ignored, despite its importance. This paper efficiently automates Bayesian model averaging, selection, and combination by message passing on a Forney-style factor graph with a custom mixture node. Parameter and state inference, and model comparison can then be executed simultaneously using message passing with scale factors. This approach shortens the model design cycle and allows for the straightforward extension to hierarchical and temporal model priors to accommodate for modeling complicated time-varying processes.
&lt;/p&gt;</description></item><item><title>DDLP&#31639;&#27861;&#20351;&#29992;&#28145;&#24230;&#28508;&#22312;&#31890;&#23376;(DLP)&#34920;&#31034;&#27861;&#23454;&#29616;&#26080;&#30417;&#30563;&#29289;&#20307;&#20013;&#24515;&#35270;&#39057;&#39044;&#27979;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#31639;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#20351;&#24471;&#21487;&#20197;&#36827;&#34892;&#8220;&#20551;&#35774;&#8221;&#29983;&#25104;&#65292;&#32780;DLP&#30340;&#32039;&#20945;&#32467;&#26500;&#20351;&#24471;&#25928;&#29575;&#39640;&#24182;&#21487;&#20197;&#36827;&#34892;&#22522;&#20110;&#25193;&#25955;&#30340;&#26080;&#26465;&#20214;&#35270;&#39057;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2306.05957</link><description>&lt;p&gt;
DDLP&#65306;&#22522;&#20110;&#28145;&#24230;&#21160;&#24577;&#28508;&#22312;&#31890;&#23376;&#30340;&#26080;&#30417;&#30563;&#29289;&#20307;&#20013;&#24515;&#35270;&#39057;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
DDLP: Unsupervised Object-Centric Video Prediction with Deep Dynamic Latent Particles. (arXiv:2306.05957v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05957
&lt;/p&gt;
&lt;p&gt;
DDLP&#31639;&#27861;&#20351;&#29992;&#28145;&#24230;&#28508;&#22312;&#31890;&#23376;(DLP)&#34920;&#31034;&#27861;&#23454;&#29616;&#26080;&#30417;&#30563;&#29289;&#20307;&#20013;&#24515;&#35270;&#39057;&#39044;&#27979;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#31639;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#20351;&#24471;&#21487;&#20197;&#36827;&#34892;&#8220;&#20551;&#35774;&#8221;&#29983;&#25104;&#65292;&#32780;DLP&#30340;&#32039;&#20945;&#32467;&#26500;&#20351;&#24471;&#25928;&#29575;&#39640;&#24182;&#21487;&#20197;&#36827;&#34892;&#22522;&#20110;&#25193;&#25955;&#30340;&#26080;&#26465;&#20214;&#35270;&#39057;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#28508;&#22312;&#31890;&#23376;&#65288;DLP&#65289;&#34920;&#31034;&#30340;&#26032;&#22411;&#29289;&#20307;&#20013;&#24515;&#35270;&#39057;&#39044;&#27979;&#31639;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#27133;&#25110;&#34917;&#19969;&#30340;&#34920;&#31034;&#30456;&#27604;&#65292;DLP&#20351;&#29992;&#19968;&#32452;&#20851;&#38190;&#28857;&#27169;&#25311;&#22330;&#26223;&#65292;&#23398;&#20064;&#21442;&#25968;&#29992;&#20110;&#23646;&#24615;&#20363;&#22914;&#20301;&#32622;&#21644;&#22823;&#23567;&#65292;&#24182;&#19988;&#26082;&#39640;&#25928;&#21448;&#21487;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#8212;&#8212;&#28145;&#24230;&#21160;&#24577;&#28508;&#22312;&#31890;&#23376;(DDLP)&#65292;&#22312;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#29289;&#20307;&#20013;&#24515;&#35270;&#39057;&#39044;&#27979;&#32467;&#26524;&#12290;DDLP&#30340;&#21487;&#35299;&#37322;&#24615;&#20351;&#25105;&#20204;&#33021;&#22815;&#25191;&#34892;&#8220;&#20551;&#35774;&#8221;&#29983;&#25104;&#8212;&#8212;&#39044;&#27979;&#26356;&#25913;&#21021;&#22987;&#24103;&#20013;&#23545;&#35937;&#23646;&#24615;&#30340;&#32467;&#26524;&#65292;&#32780;DLP&#30340;&#32039;&#20945;&#32467;&#26500;&#20351;&#24471;&#25928;&#29575;&#39640;&#24182;&#21487;&#20197;&#36827;&#34892;&#22522;&#20110;&#25193;&#25955;&#30340;&#26080;&#26465;&#20214;&#35270;&#39057;&#29983;&#25104;&#12290;&#35270;&#39057;&#12289;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#22312;&#27492;&#38142;&#25509;&#25214;&#21040;&#65306;https://taldatech.github.io/ddlp-web
&lt;/p&gt;
&lt;p&gt;
We propose a new object-centric video prediction algorithm based on the deep latent particle (DLP) representation. In comparison to existing slot- or patch-based representations, DLPs model the scene using a set of keypoints with learned parameters for properties such as position and size, and are both efficient and interpretable. Our method, deep dynamic latent particles (DDLP), yields state-of-the-art object-centric video prediction results on several challenging datasets. The interpretable nature of DDLP allows us to perform ``what-if'' generation -- predict the consequence of changing properties of objects in the initial frames, and DLP's compact structure enables efficient diffusion-based unconditional video generation. Videos, code and pre-trained models are available: https://taldatech.github.io/ddlp-web
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36335;&#24452;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#32858;&#21512;&#20174;&#33410;&#28857;&#20986;&#21457;&#30340;&#36335;&#24452;&#26469;&#26356;&#26032;&#33410;&#28857;&#34920;&#31034;&#65292;&#30456;&#27604;&#26631;&#20934;&#22270;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26356;&#24378;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#33021;&#22815;&#21306;&#20998;&#19968;&#20123;1-WL&#26080;&#27861;&#21306;&#20998;&#30340;&#38750;&#21516;&#26500;&#22270;&#23545;&#12290;</title><link>http://arxiv.org/abs/2306.05955</link><description>&lt;p&gt;
&#36335;&#24452;&#31070;&#32463;&#32593;&#32476;&#65306;&#34920;&#36798;&#33021;&#21147;&#24378;&#19988;&#31934;&#30830;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Path Neural Networks: Expressive and Accurate Graph Neural Networks. (arXiv:2306.05955v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36335;&#24452;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#32858;&#21512;&#20174;&#33410;&#28857;&#20986;&#21457;&#30340;&#36335;&#24452;&#26469;&#26356;&#26032;&#33410;&#28857;&#34920;&#31034;&#65292;&#30456;&#27604;&#26631;&#20934;&#22270;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26356;&#24378;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#33021;&#22815;&#21306;&#20998;&#19968;&#20123;1-WL&#26080;&#27861;&#21306;&#20998;&#30340;&#38750;&#21516;&#26500;&#22270;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36817;&#26399;&#25104;&#20026;&#20102;&#23398;&#20064;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#26631;&#20934;&#26041;&#27861;&#12290;&#26089;&#20808;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#28508;&#21147;&#65292;&#20063;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#38480;&#21046;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#26631;&#20934;&#30340;GNN&#22312;&#34920;&#36798;&#33021;&#21147;&#19978;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22312;&#21306;&#20998;&#38750;&#21516;&#26500;&#22270;&#26041;&#38754;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#19981;&#36229;&#36807;&#19968;&#32500;Weisfeiler-Leman&#65288;1-WL&#65289;&#31639;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#36335;&#24452;&#31070;&#32463;&#32593;&#32476;&#65288;PathNNs&#65289;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#32858;&#21512;&#20174;&#33410;&#28857;&#20986;&#21457;&#30340;&#36335;&#24452;&#26469;&#26356;&#26032;&#33410;&#28857;&#34920;&#31034;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;PathNN&#27169;&#22411;&#21464;&#20307;&#65292;&#20197;&#32858;&#21512;&#21333;&#20010;&#26368;&#30701;&#36335;&#24452;&#12289;&#25152;&#26377;&#26368;&#30701;&#36335;&#24452;&#21644;&#25152;&#26377;&#38271;&#24230;&#19981;&#36229;&#36807;K&#30340;&#31616;&#21333;&#36335;&#24452;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20854;&#20013;&#20004;&#20010;&#21464;&#20307;&#22312;&#34920;&#36798;&#33021;&#21147;&#19978;&#27604;1-WL&#31639;&#27861;&#20005;&#35880;&#65292;&#21516;&#26102;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;PathNNs&#21487;&#20197;&#21306;&#20998;&#19968;&#20123;1-WL&#26080;&#27861;&#21306;&#20998;&#30340;&#38750;&#21516;&#26500;&#22270;&#23545;&#65292;&#32780;&#25105;&#20204;&#26368;&#34920;&#36798;&#20016;&#23500;&#30340;PathNN&#21464;&#20307;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have recently become the standard approach for learning with graph-structured data. Prior work has shed light into their potential, but also their limitations. Unfortunately, it was shown that standard GNNs are limited in their expressive power. These models are no more powerful than the 1-dimensional Weisfeiler-Leman (1-WL) algorithm in terms of distinguishing non-isomorphic graphs. In this paper, we propose Path Neural Networks (PathNNs), a model that updates node representations by aggregating paths emanating from nodes. We derive three different variants of the PathNN model that aggregate single shortest paths, all shortest paths and all simple paths of length up to K. We prove that two of these variants are strictly more powerful than the 1-WL algorithm, and we experimentally validate our theoretical results. We find that PathNNs can distinguish pairs of non-isomorphic graphs that are indistinguishable by 1-WL, while our most expressive PathNN variant 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20154;&#31867;&#35270;&#35273;&#27880;&#24847;&#21147;&#27169;&#22411;&#25913;&#21892;&#20154;&#26426;&#22270;&#20687;&#20998;&#26512;&#31995;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#20811;&#26381;&#38024;&#23545;&#20154;&#26426;&#20132;&#20114;&#24212;&#29992;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2306.05952</link><description>&lt;p&gt;
&#20811;&#26381;&#38024;&#23545;&#20154;&#26426;&#20132;&#20114;&#24212;&#29992;&#30340;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Overcoming Adversarial Attacks for Human-in-the-Loop Applications. (arXiv:2306.05952v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05952
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20154;&#31867;&#35270;&#35273;&#27880;&#24847;&#21147;&#27169;&#22411;&#25913;&#21892;&#20154;&#26426;&#22270;&#20687;&#20998;&#26512;&#31995;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#20811;&#26381;&#38024;&#23545;&#20154;&#26426;&#20132;&#20114;&#24212;&#29992;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21253;&#21547;&#20154;&#31867;&#20998;&#26512;&#21487;&#33021;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#38887;&#24615;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#65292;&#22312;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30456;&#23545;&#36739;&#23569;&#34987;&#25506;&#32034;&#12290;&#31070;&#32463;&#32593;&#32476;&#35270;&#35273;&#35299;&#37322;&#22270;&#32463;&#24120;&#23481;&#26131;&#36973;&#21463;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#20026;&#20102;&#35753;&#22270;&#20687;&#20998;&#26512;&#32773;&#35780;&#20272;&#32473;&#23450;&#27169;&#22411;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#36873;&#25321;&#31283;&#20581;&#30340;&#35299;&#37322;&#21487;&#35270;&#21270;&#12290;&#36825;&#20123;&#22240;&#32032;&#26497;&#22823;&#22320;&#24433;&#21709;&#20102;&#20154;&#26426;&#20132;&#20114;&#65288;HITL&#65289;&#35780;&#20272;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#23545;&#25239;&#24615;&#22270;&#20687;&#65292;&#21253;&#25324;&#35299;&#37322;&#22270;&#21644;&#40065;&#26834;&#24615;&#27979;&#37327;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#20154;&#31867;&#35270;&#35273;&#27880;&#24847;&#21147;&#27169;&#22411;&#21487;&#20197;&#25913;&#21892;&#20154;&#26426;&#22270;&#20687;&#20998;&#26512;&#31995;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#38754;&#20020;&#30340;&#25361;&#25112;&#26159;&#65292;&#22312;&#36825;&#31181;&#23545;&#25239;&#24615;&#29615;&#22659;&#19979;&#65292;&#22914;&#20309;&#20351;HITL&#35780;&#20272;&#26356;&#21152;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;
Including human analysis has the potential to positively affect the robustness of Deep Neural Networks and is relatively unexplored in the Adversarial Machine Learning literature. Neural network visual explanation maps have been shown to be prone to adversarial attacks. Further research is needed in order to select robust visualizations of explanations for the image analyst to evaluate a given model. These factors greatly impact Human-In-The-Loop (HITL) evaluation tools due to their reliance on adversarial images, including explanation maps and measurements of robustness. We believe models of human visual attention may improve interpretability and robustness of human-machine imagery analysis systems. Our challenge remains, how can HITL evaluation be robust in this adversarial landscape?
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;RidgeGAN&#27169;&#22411;&#30340;&#20132;&#36890;&#25351;&#25968;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22478;&#24066;&#21270;&#24102;&#26469;&#30340;&#20132;&#36890;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#31934;&#24230;&#19978;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#22238;&#24402;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.05951</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#21512;RidgeGAN&#27169;&#22411;&#30340;&#21360;&#24230;&#23567;&#20013;&#22478;&#24066;&#20132;&#36890;&#25351;&#25968;&#39044;&#27979;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Prediction of Transportation Index for Urban Patterns in Small and Medium-sized Indian Cities using Hybrid RidgeGAN Model. (arXiv:2306.05951v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;RidgeGAN&#27169;&#22411;&#30340;&#20132;&#36890;&#25351;&#25968;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22478;&#24066;&#21270;&#24102;&#26469;&#30340;&#20132;&#36890;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#31934;&#24230;&#19978;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#22238;&#24402;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21360;&#24230;&#31561;&#22823;&#22810;&#25968;&#21457;&#23637;&#20013;&#22269;&#23478;&#30340;&#24555;&#36895;&#22478;&#24066;&#21270;&#36235;&#21183;&#24102;&#26469;&#20102;&#22823;&#37327;&#31038;&#20250;&#38382;&#39064;&#65292;&#22914;&#32511;&#22320;&#20007;&#22833;&#12289;&#29615;&#22659;&#20581;&#24247;&#36864;&#21270;&#12289;&#28165;&#27905;&#27700;&#36164;&#28304;&#30701;&#32570;&#12289;&#31354;&#27668;&#27745;&#26579;&#12289;&#20132;&#36890;&#25317;&#22581;&#31561;&#12290;&#36890;&#36807;&#20132;&#36890;&#25351;&#25968;&#36827;&#34892;&#20132;&#36890;&#32593;&#32476;&#24314;&#27169;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#20102;&#35299;&#22478;&#24066;&#20132;&#36890;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#39044;&#27979;&#20132;&#36890;&#25351;&#25968;&#21183;&#22312;&#24517;&#34892;&#65292;&#20197;&#20419;&#36827;&#22478;&#24066;&#35268;&#21010;&#21644;&#20132;&#36890;&#31649;&#29702;&#30340;&#21487;&#25345;&#32493;&#24615;&#21457;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28151;&#21512;RidgeGAN&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#21360;&#24230;&#23567;&#20013;&#22478;&#24066;&#30340;&#20132;&#36890;&#25351;&#25968;&#12290;&#20854;&#20013;&#65292;Ridge&#22238;&#24402;&#29992;&#20110;&#22788;&#29702;&#33258;&#21464;&#37327;&#20043;&#38388;&#30340;&#22810;&#37325;&#20849;&#32447;&#24615;&#65292;GAN&#26550;&#26500;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#23545;&#26469;&#33258;&#21360;&#24230;&#22478;&#24066;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#32467;&#35770;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#22238;&#24402;&#27169;&#22411;&#30456;&#27604;&#65292;&#28151;&#21512;RidgeGAN&#27169;&#22411;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid urbanization trend in most developing countries including India is creating a plethora of civic concerns such as loss of green space, degradation of environmental health, clean water availability, air pollution, traffic congestion leading to delays in vehicular transportation, etc. Transportation and network modeling through transportation indices have been widely used to understand transportation problems in the recent past. This necessitates predicting transportation indices to facilitate sustainable urban planning and traffic management. Recent advancements in deep learning research, in particular, Generative Adversarial Networks (GANs), and their modifications in spatial data analysis such as CityGAN, Conditional GAN, and MetroGAN have enabled urban planners to simulate hyper-realistic urban patterns. These synthetic urban universes mimic global urban patterns and evaluating their landscape structures through spatial pattern analysis can aid in comprehending landscape dyn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#27169;&#22411;&#65292;&#20854;&#20013;&#22788;&#26041;&#24615;&#31995;&#25968;&#20195;&#26367;&#20102;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30446;&#26631;&#65292;&#21487;&#29992;&#20110;&#30830;&#23450;&#26368;&#22823;&#21270;&#19978;&#19979;&#25991;&#20915;&#31574;&#36136;&#37327;&#21644;&#21442;&#32771;&#20915;&#31574;&#20197;&#21450;&#20391;&#38754;&#20449;&#24687;&#22788;&#26041;&#33021;&#21147;&#30340;&#26368;&#20339;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.05937</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#25968;&#25454;&#39537;&#21160;&#22788;&#26041;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Robust Data-driven Prescriptiveness Optimization. (arXiv:2306.05937v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#27169;&#22411;&#65292;&#20854;&#20013;&#22788;&#26041;&#24615;&#31995;&#25968;&#20195;&#26367;&#20102;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30446;&#26631;&#65292;&#21487;&#29992;&#20110;&#30830;&#23450;&#26368;&#22823;&#21270;&#19978;&#19979;&#25991;&#20915;&#31574;&#36136;&#37327;&#21644;&#21442;&#32771;&#20915;&#31574;&#20197;&#21450;&#20391;&#38754;&#20449;&#24687;&#22788;&#26041;&#33021;&#21147;&#30340;&#26368;&#20339;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24040;&#37327;&#30340;&#25968;&#25454;&#20419;&#36827;&#20102;&#21508;&#31181;&#20248;&#21270;&#25216;&#26415;&#30340;&#20986;&#29616;&#65292;&#26088;&#22312;&#21033;&#29992;&#29616;&#26377;&#30340;&#20391;&#38754;&#20449;&#24687;&#25552;&#20379;&#26356;&#20855;&#39044;&#21028;&#24615;&#30340;&#20915;&#31574;&#12290;&#24191;&#27867;&#30340;&#26041;&#27861;&#21644;&#24212;&#29992;&#32972;&#26223;&#20419;&#36827;&#20102;&#35774;&#35745;&#19968;&#31181;&#31216;&#20026;&#22788;&#26041;&#24615;&#31995;&#25968;&#30340;&#36890;&#29992;&#26080;&#21333;&#20301;&#24615;&#33021;&#25351;&#26631;&#30340;&#20135;&#29983;&#12290;&#35813;&#31995;&#25968;&#26088;&#22312;&#37327;&#21270;&#19978;&#19979;&#25991;&#20915;&#31574;&#30340;&#36136;&#37327;&#19982;&#21442;&#32771;&#20915;&#31574;&#20197;&#21450;&#20391;&#38754;&#20449;&#24687;&#30340;&#22788;&#26041;&#33021;&#21147;&#12290;&#20026;&#20102;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#24773;&#20917;&#19979;&#30830;&#23450;&#26368;&#22823;&#21270;&#21069;&#32773;&#30340;&#31574;&#30053;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#40065;&#26834;&#30340;&#19978;&#19979;&#25991;&#20248;&#21270;&#27169;&#22411;&#65292;&#20854;&#20013;&#22788;&#26041;&#24615;&#31995;&#25968;&#20195;&#26367;&#20102;&#32463;&#20856;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#20998;&#27861;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#27169;&#22411;&#65292;&#24403;&#20998;&#24067;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#20855;&#26377;&#36866;&#24403;&#30340;&#23884;&#22871;&#24418;&#24335;&#21644;&#22810;&#38754;&#20307;&#32467;&#26500;&#26102;&#65292;&#35813;&#31639;&#27861;&#20381;&#36182;&#20110;&#35299;&#20915;&#19968;&#31995;&#21015;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#30701;&#35821;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The abundance of data has led to the emergence of a variety of optimization techniques that attempt to leverage available side information to provide more anticipative decisions. The wide range of methods and contexts of application have motivated the design of a universal unitless measure of performance known as the coefficient of prescriptiveness. This coefficient was designed to quantify both the quality of contextual decisions compared to a reference one and the prescriptive power of side information. To identify policies that maximize the former in a data-driven context, this paper introduces a distributionally robust contextual optimization model where the coefficient of prescriptiveness substitutes for the classical empirical risk minimization objective. We present a bisection algorithm to solve this model, which relies on solving a series of linear programs when the distributional ambiguity set has an appropriate nested form and polyhedral structure. Studying a contextual short
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#26469;&#33258;&#19981;&#21516;&#35821;&#35328;&#32452;&#12289;&#32972;&#26223;&#21508;&#24322;&#30340;&#19968;&#30334;&#22810;&#21517;&#35828;&#35805;&#32773;&#35757;&#32451;&#20102;&#19968;&#31181;&#35821;&#38899;&#24212;&#28608;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#28151;&#21512;&#29305;&#24449;&#20013;&#28155;&#21152;&#20010;&#20307;&#24046;&#24322;&#24615;&#35782;&#21035;&#25216;&#26415;&#65292;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05915</link><description>&lt;p&gt;
&#20197;&#35828;&#35805;&#20154;&#23884;&#20837;&#20316;&#20026;&#35821;&#38899;&#24212;&#28608;&#26816;&#27979;&#30340;&#20010;&#24615;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Speaker Embeddings as Individuality Proxy for Voice Stress Detection. (arXiv:2306.05915v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05915
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#26469;&#33258;&#19981;&#21516;&#35821;&#35328;&#32452;&#12289;&#32972;&#26223;&#21508;&#24322;&#30340;&#19968;&#30334;&#22810;&#21517;&#35828;&#35805;&#32773;&#35757;&#32451;&#20102;&#19968;&#31181;&#35821;&#38899;&#24212;&#28608;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#28151;&#21512;&#29305;&#24449;&#20013;&#28155;&#21152;&#20010;&#20307;&#24046;&#24322;&#24615;&#35782;&#21035;&#25216;&#26415;&#65292;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35828;&#35805;&#20154;&#30340;&#24515;&#29702;&#29366;&#24577;&#35843;&#33410;&#35821;&#38899;&#65292;&#22240;&#27492;&#35748;&#30693;&#25110;&#36523;&#20307;&#36127;&#33655;&#24341;&#36215;&#30340;&#24212;&#28608;&#21487;&#20197;&#22312;&#35821;&#38899;&#20013;&#26816;&#27979;&#21040;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36229;&#36807;100&#21517;&#26469;&#33258;9&#20010;&#35821;&#35328;&#32452;&#21644;5&#31181;&#19981;&#21516;&#31867;&#22411;&#24212;&#28608;&#30340;&#35828;&#35805;&#20154;&#36827;&#34892;&#35757;&#32451;&#30340;&#35821;&#38899;&#24212;&#28608;&#26816;&#27979;&#30340;&#35774;&#35745;&#21644;&#24320;&#21457;&#12290;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;&#35828;&#35805;&#20154;&#23884;&#20837;&#21040;&#28151;&#21512;BYOL-S&#29305;&#24449;&#20013;&#26469;&#35299;&#20915;&#35821;&#38899;&#24212;&#28608;&#20998;&#26512;&#20013;&#30340;&#20010;&#20307;&#24046;&#24322;&#12290;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#22768;&#38899;&#24212;&#28608;&#26816;&#27979;&#24615;&#33021;&#65292;&#21482;&#38656;3-5&#31186;&#30340;&#36755;&#20837;&#38899;&#39057;&#38271;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the mental states of the speaker modulate speech, stress introduced by cognitive or physical loads could be detected in the voice. The existing voice stress detection benchmark has shown that the audio embeddings extracted from the Hybrid BYOL-S self-supervised model perform well. However, the benchmark only evaluates performance separately on each dataset, but does not evaluate performance across the different types of stress and different languages. Moreover, previous studies found strong individual differences in stress susceptibility. This paper presents the design and development of voice stress detection, trained on more than 100 speakers from 9 language groups and five different types of stress. We address individual variabilities in voice stress analysis by adding speaker embeddings to the hybrid BYOL-S features. The proposed method significantly improves voice stress detection performance with an input audio length of only 3-5 seconds.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#22823;&#22411;&#12289;&#21487;&#25193;&#23637;&#30340;&#20108;&#32500;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#24320;&#21457;&#21508;&#31181;&#22270;&#20687;&#37325;&#24314;&#20219;&#21153;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;X&#23556;&#32447;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#25968;&#25454;&#38598;&#21294;&#20047;&#30340;&#29616;&#29366;&#12290;</title><link>http://arxiv.org/abs/2306.05907</link><description>&lt;p&gt;
2DeteCT -- &#19968;&#20010;&#22823;&#22411;&#30340;&#21487;&#25193;&#23637;&#12289;&#21487;&#35757;&#32451;&#30340;&#20108;&#32500;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
2DeteCT -- A large 2D expandable, trainable, experimental Computed Tomography dataset for machine learning. (arXiv:2306.05907v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05907
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#22823;&#22411;&#12289;&#21487;&#25193;&#23637;&#30340;&#20108;&#32500;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#24320;&#21457;&#21508;&#31181;&#22270;&#20687;&#37325;&#24314;&#20219;&#21153;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;X&#23556;&#32447;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#25968;&#25454;&#38598;&#21294;&#20047;&#30340;&#29616;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35745;&#31639;&#25104;&#20687;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#24320;&#21457;&#29992;&#20110;&#22270;&#20687;&#37325;&#24314;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#19978;&#65292;&#36825;&#38656;&#35201;&#21253;&#21547;&#27979;&#37327;&#25968;&#25454;&#21644;&#22320;&#38754;&#30495;&#23454;&#22270;&#20687;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;X&#23556;&#32447;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#30340;&#36866;&#24403;&#23454;&#39564;&#25968;&#25454;&#38598;&#24456;&#23569;&#65292;&#26041;&#27861;&#36890;&#24120;&#22312;&#27169;&#25311;&#25968;&#25454;&#19978;&#24320;&#21457;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#12289;&#24320;&#25918;&#30340;&#20108;&#32500;&#25159;&#24418;&#26463;CT&#25968;&#25454;&#38598;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#36866;&#29992;&#20110;&#24320;&#21457;&#21508;&#31181;&#22270;&#20687;&#37325;&#24314;&#20219;&#21153;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#20026;&#20102;&#33719;&#21462;&#27492;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#12289;&#21322;&#33258;&#21160;&#30340;&#25195;&#25551;&#31243;&#24207;&#65292;&#21033;&#29992;&#39640;&#24230;&#28789;&#27963;&#30340;&#23454;&#39564;&#23460;X&#23556;&#32447;CT&#35013;&#32622;&#12290;&#25195;&#25551;&#20102;&#19968;&#20010;&#20855;&#26377;&#39640;&#33258;&#28982;&#21464;&#24322;&#24615;&#30340;&#26679;&#26412;&#28151;&#21512;&#29289;&#65292;&#20854;&#24418;&#29366;&#21644;&#23494;&#24230;&#22312;&#27599;&#20010;&#20999;&#29255;&#19978;&#37117;&#26377;&#25152;&#19981;&#21516;&#65288;&#24635;&#20849;5000&#20010;&#20999;&#29255;&#65289;&#65292;&#20855;&#26377;&#39640;&#35282;&#20998;&#36776;&#29575;&#21644;&#31354;&#38388;&#20998;&#36776;&#29575;&#65292;&#24182;&#20855;&#26377;&#19977;&#31181;&#19981;&#21516;&#30340;&#20809;&#26463;&#29305;&#24449;&#65306;&#39640;&#36924;&#30495;&#24230;&#12289;&#20302;&#21058;&#37327;&#21644;&#20809;&#26463;&#30828;&#21270;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;750&#20010;&#20998;&#24067;&#22806;&#30340;&#26679;&#26412;&#65292;&#36825;&#20123;&#26679;&#26412;&#21487;&#29992;&#20110;&#27979;&#35797;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research in computational imaging largely focuses on developing machine learning (ML) techniques for image reconstruction, which requires large-scale training datasets consisting of measurement data and ground-truth images. However, suitable experimental datasets for X-ray Computed Tomography (CT) are scarce, and methods are often developed and evaluated only on simulated data. We fill this gap by providing the community with a versatile, open 2D fan-beam CT dataset suitable for developing ML techniques for a range of image reconstruction tasks. To acquire it, we designed a sophisticated, semi-automatic scan procedure that utilizes a highly-flexible laboratory X-ray CT setup. A diverse mix of samples with high natural variability in shape and density was scanned slice-by-slice (5000 slices in total) with high angular and spatial resolution and three different beam characteristics: A high-fidelity, a low-dose and a beam-hardening-inflicted mode. In addition, 750 out-of-distributi
&lt;/p&gt;</description></item><item><title>TreeDQN&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#26356;&#21152;&#25928;&#29575;&#30340;&#20998;&#25903;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#20943;&#23569;&#20102;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#20135;&#29983;&#26356;&#23567;&#30340;&#23376;&#20219;&#21153;&#26641;&#12290;</title><link>http://arxiv.org/abs/2306.05905</link><description>&lt;p&gt;
TreeDQN: &#23398;&#20064;&#22914;&#20309;&#26368;&#23567;&#21270;&#20998;&#25903;&#23450;&#30028;&#26641;
&lt;/p&gt;
&lt;p&gt;
TreeDQN: Learning to minimize Branch-and-Bound tree. (arXiv:2306.05905v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05905
&lt;/p&gt;
&lt;p&gt;
TreeDQN&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#26356;&#21152;&#25928;&#29575;&#30340;&#20998;&#25903;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#20943;&#23569;&#20102;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#20135;&#29983;&#26356;&#23567;&#30340;&#23376;&#20219;&#21153;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#38656;&#35201;&#36890;&#36807;&#20840;&#38754;&#25628;&#32034;&#25165;&#33021;&#25214;&#21040;&#26368;&#20248;&#35299;&#12290;&#20998;&#25903;&#23450;&#30028;&#26159;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#30340;&#26041;&#20415;&#26041;&#27861;&#12290;&#20998;&#25903;&#23450;&#30028;&#27714;&#35299;&#22120;&#23558;&#20219;&#21153;&#20998;&#25104;&#20004;&#20010;&#37096;&#20998;&#65292;&#23558;&#25972;&#25968;&#21464;&#37327;&#30340;&#22495;&#20998;&#25104;&#20004;&#20010;&#37096;&#20998;&#65292;&#28982;&#21518;&#36882;&#24402;&#22320;&#35299;&#20915;&#23427;&#20204;&#65292;&#20135;&#29983;&#19968;&#20010;&#23884;&#22871;&#23376;&#20219;&#21153;&#26641;&#12290;&#27714;&#35299;&#22120;&#30340;&#25928;&#29575;&#21462;&#20915;&#20110;&#29992;&#20110;&#36873;&#25321;&#20998;&#35010;&#21464;&#37327;&#30340;&#20998;&#25903;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#20998;&#25903;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#25105;&#20204;&#23558;&#21464;&#37327;&#36873;&#25321;&#20219;&#21153;&#35270;&#20026;&#26641;&#24418;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#35777;&#26126;&#20102;&#36866;&#29992;&#20110;&#26641;&#24418;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#36125;&#23572;&#26364;&#31639;&#23376;&#24179;&#22343;&#25910;&#32553;&#65292;&#24182;&#25552;&#20986;&#20102;&#20462;&#25913;&#21518;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#23398;&#20064;&#30446;&#26631;&#12290;&#19982;&#20043;&#21069;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#20195;&#29702;&#38656;&#35201;&#26356;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#20135;&#29983;&#26356;&#23567;&#30340;&#23376;&#20219;&#21153;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combinatorial optimization problems require an exhaustive search to find the optimal solution. A convenient approach to solving combinatorial optimization tasks in the form of Mixed Integer Linear Programs is Branch-and-Bound. Branch-and-Bound solver splits a task into two parts dividing the domain of an integer variable, then it solves them recursively, producing a tree of nested sub-tasks. The efficiency of the solver depends on the branchning heuristic used to select a variable for splitting. In the present work, we propose a reinforcement learning method that can efficiently learn the branching heuristic. We view the variable selection task as a tree Markov Decision Process, prove that the Bellman operator adapted for the tree Markov Decision Process is contracting in mean, and propose a modified learning objective for the reinforcement learning agent. Our agent requires less training data and produces smaller trees compared to previous reinforcement learning methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;C(NN)FD&#65292;&#29992;&#20110;&#23454;&#26102;&#39044;&#27979;&#29123;&#27668;&#36718;&#26426;&#20013;&#36724;&#21521;&#21387;&#32553;&#26426;&#21046;&#36896;&#21644;&#32452;&#35013;&#21464;&#21270;&#23545;&#25972;&#20307;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#35813;&#26694;&#26550;&#21487;&#36807;&#28388;&#25481;CFD&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#30456;&#20851;&#37096;&#20998;&#65292;&#22240;&#27492;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#19988;&#23454;&#26102;&#31934;&#24230;&#21487;&#19982;CFD&#22522;&#20934;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2306.05889</link><description>&lt;p&gt;
C(NN)FD - &#19968;&#31181;&#29992;&#20110;&#28065;&#36718;&#26426;&#26800;CFD&#20998;&#26512;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
C(NN)FD -- a deep learning framework for turbomachinery CFD analysis. (arXiv:2306.05889v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;C(NN)FD&#65292;&#29992;&#20110;&#23454;&#26102;&#39044;&#27979;&#29123;&#27668;&#36718;&#26426;&#20013;&#36724;&#21521;&#21387;&#32553;&#26426;&#21046;&#36896;&#21644;&#32452;&#35013;&#21464;&#21270;&#23545;&#25972;&#20307;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#35813;&#26694;&#26550;&#21487;&#36807;&#28388;&#25481;CFD&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#30456;&#20851;&#37096;&#20998;&#65292;&#22240;&#27492;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#19988;&#23454;&#26102;&#31934;&#24230;&#21487;&#19982;CFD&#22522;&#20934;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#22312;&#19981;&#21516;&#24037;&#19994;&#39046;&#22495;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#23545;&#20110;&#35832;&#22914;CFD&#65288;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#65289;&#30340;&#29289;&#29702;&#27169;&#25311;&#30340;&#24212;&#29992;&#20165;&#38480;&#20110;&#24037;&#19994;&#30456;&#20851;&#24615;&#36739;&#23567;&#30340;&#31616;&#21333;&#27979;&#35797;&#26696;&#20363;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#24320;&#21457;&#65292;&#29992;&#20110;&#23454;&#26102;&#39044;&#27979;&#29123;&#27668;&#36718;&#26426;&#20013;&#36724;&#21521;&#21387;&#32553;&#26426;&#21046;&#36896;&#21644;&#32452;&#35013;&#21464;&#21270;&#23545;&#25972;&#20307;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#37325;&#28857;&#20851;&#27880;&#20110;&#21494;&#29255;&#38388;&#38553;&#30340;&#21464;&#21270;&#12290;&#25928;&#29575;&#30340;&#25955;&#24067;&#21487;&#20197;&#26174;&#33879;&#22686;&#21152;$CO_2$&#25490;&#25918;&#37327;&#65292;&#22240;&#27492;&#20855;&#26377;&#37325;&#35201;&#30340;&#24037;&#19994;&#21644;&#29615;&#22659;&#24847;&#20041;&#12290;&#25152;&#25552;&#20986;&#30340;C(NN)FD&#26550;&#26500;&#23454;&#26102;&#31934;&#24230;&#21487;&#19982;CFD&#22522;&#20934;&#30456;&#23218;&#32654;&#12290;&#39044;&#27979;&#27969;&#22330;&#24182;&#20351;&#29992;&#20854;&#35745;&#31639;&#30456;&#24212;&#30340;&#25972;&#20307;&#24615;&#33021;&#20351;&#35813;&#26041;&#27861;&#20855;&#26377;&#26222;&#36866;&#24615;&#65292;&#32780;&#20165;&#36807;&#28388;CFD&#35299;&#20915;&#26041;&#26696;&#30340;&#30456;&#20851;&#37096;&#20998;&#20351;&#35813;&#26041;&#27861;&#21487;&#25193;&#23637;&#21040;&#23454;&#38469;&#24212;&#29992;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning methods have seen a wide range of successful applications across different industries. Up until now, applications to physical simulations such as CFD (Computational Fluid Dynamics), have been limited to simple test-cases of minor industrial relevance. This paper demonstrates the development of a novel deep learning framework for real-time predictions of the impact of manufacturing and build variations on the overall performance of axial compressors in gas turbines, with a focus on tip clearance variations. The associated scatter in efficiency can significantly increase the $CO_2$ emissions, thus being of great industrial and environmental relevance. The proposed \textit{C(NN)FD} architecture achieves in real-time accuracy comparable to the CFD benchmark. Predicting the flow field and using it to calculate the corresponding overall performance renders the methodology generalisable, while filtering only relevant parts of the CFD solution makes the methodology scalable to in
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;INR&#30340;&#26102;&#38388;&#24207;&#21015;&#36830;&#32493;&#24314;&#27169;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12289;&#19981;&#35268;&#21017;&#37319;&#26679;&#21644;&#22810;&#20256;&#24863;&#22120;&#19981;&#23545;&#20934;&#35266;&#27979;&#31561;&#37325;&#22797;&#24314;&#27169;&#38382;&#39064;&#65292;&#24182;&#22312;&#39044;&#27979;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.05880</link><description>&lt;p&gt;
&#22522;&#20110;Implicit Neural Representations&#30340;&#26102;&#38388;&#24207;&#21015;&#36830;&#32493;&#24314;&#27169;&#29992;&#20110;&#25554;&#20540;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Time Series Continuous Modeling for Imputation and Forecasting with Implicit Neural Representations. (arXiv:2306.05880v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05880
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;INR&#30340;&#26102;&#38388;&#24207;&#21015;&#36830;&#32493;&#24314;&#27169;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12289;&#19981;&#35268;&#21017;&#37319;&#26679;&#21644;&#22810;&#20256;&#24863;&#22120;&#19981;&#23545;&#20934;&#35266;&#27979;&#31561;&#37325;&#22797;&#24314;&#27169;&#38382;&#39064;&#65292;&#24182;&#22312;&#39044;&#27979;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#24050;&#34987;&#24191;&#27867;&#25506;&#32034;&#65292;&#20294;&#22312;&#38754;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#26102;&#20173;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#21033;&#29992;Implicit Neural Representations (INR)&#12290;&#35813;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#30340;&#36830;&#32493;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#33258;&#28982;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12289;&#22788;&#29702;&#19981;&#35268;&#21017;&#37319;&#26679;&#25110;&#26469;&#33258;&#22810;&#20010;&#20256;&#24863;&#22120;&#30340;&#19981;&#23545;&#20934;&#35266;&#27979;&#31561;&#37325;&#22797;&#24314;&#27169;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#26465;&#20214;&#35843;&#21046;INR&#21442;&#25968;&#24182;&#21033;&#29992;&#20803;&#23398;&#20064;&#25216;&#26415;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#27169;&#22411;&#27867;&#21270;&#21040;&#26410;&#35265;&#26679;&#26412;&#21644;&#26102;&#38388;&#31383;&#21475;&#31227;&#20301;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#39044;&#27979;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#39046;&#20808;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#22788;&#29702;&#35768;&#22810;&#31454;&#20105;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#30340;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#26041;&#38754;&#23637;&#29616;&#20102;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although widely explored, time series modeling continues to encounter significant challenges when confronted with real-world data. We propose a novel modeling approach leveraging Implicit Neural Representations (INR). This approach enables us to effectively capture the continuous aspect of time series and provides a natural solution to recurring modeling issues such as handling missing data, dealing with irregular sampling, or unaligned observations from multiple sensors. By introducing conditional modulation of INR parameters and leveraging meta-learning techniques, we address the issue of generalization to both unseen samples and time window shifts. Through extensive experimentation, our model demonstrates state-of-the-art performance in forecasting and imputation tasks, while exhibiting flexibility in handling a wide range of challenging scenarios that competing models cannot.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#22810;&#39046;&#22495;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;FedWon&#65292;&#36890;&#36807;&#28040;&#38500;&#35268;&#33539;&#21270;&#27493;&#39588;&#26469;&#26377;&#25928;&#22320;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2306.05879</link><description>&lt;p&gt;
&#22810;&#39046;&#22495;&#32852;&#37030;&#23398;&#20064;&#26159;&#21542;&#31163;&#19981;&#24320;&#26631;&#20934;&#21270;?
&lt;/p&gt;
&lt;p&gt;
Is Normalization Indispensable for Multi-domain Federated Learning?. (arXiv:2306.05879v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#22810;&#39046;&#22495;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;FedWon&#65292;&#36890;&#36807;&#28040;&#38500;&#35268;&#33539;&#21270;&#27493;&#39588;&#26469;&#26377;&#25928;&#22320;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#20998;&#25955;&#22312;&#23458;&#25143;&#31471;&#19978;&#30340;&#21327;&#20316;&#24335;&#20869;&#37096;&#35757;&#32451;&#22686;&#24378;&#20102;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;&#23398;&#20064;&#38754;&#20020;&#35832;&#22810;&#25361;&#25112;&#65292;&#20854;&#20013;&#21253;&#25324;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#65288;non-i.i.d&#65289;&#23548;&#33268;&#30340;&#28508;&#22312;&#24615;&#33021;&#19979;&#38477;&#21644;&#25910;&#25947;&#21463;&#38459;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35299;&#20915;&#20102;&#19968;&#20010;&#20851;&#38190;&#20294;&#24120;&#24120;&#34987;&#24573;&#35270;&#30340;&#38382;&#39064;&#8212;&#8212;&#22810;&#39046;&#22495;&#32852;&#37030;&#23398;&#20064;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23458;&#25143;&#31471;&#25968;&#25454;&#26469;&#28304;&#20110;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#20998;&#24067;&#30340;&#21508;&#31181;&#39046;&#22495;&#65292;&#32780;&#19981;&#26159;&#26631;&#31614;&#20998;&#24067;&#12290;&#20026;&#20102;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#22810;&#39046;&#22495;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#31216;&#20026;&#19981;&#20351;&#29992;&#35268;&#33539;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FedWon&#65289;&#12290;FedWon&#20174;&#19968;&#20010;&#35266;&#23519;&#20986;&#21457;&#65292;&#21363;&#25209;&#37327;&#24402;&#19968;&#21270;&#65288;BN&#65289;&#22312;&#26377;&#25928;&#22320;&#24314;&#27169;&#22810;&#20010;&#39046;&#22495;&#30340;&#32479;&#35745;&#20449;&#24687;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#32780;&#26367;&#20195;&#35268;&#33539;&#21270;&#25216;&#26415;&#20855;&#26377;&#33258;&#36523;&#30340;&#23616;&#38480;&#24615;&#12290;FedWon&#36890;&#36807;&#28040;&#38500;&#35268;&#33539;&#21270;&#27493;&#39588;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enhances data privacy with collaborative in-situ training on decentralized clients. Nevertheless, FL encounters challenges due to non-independent and identically distributed (non-i.i.d) data, leading to potential performance degradation and hindered convergence. While prior studies predominantly addressed the issue of skewed label distribution, our research addresses a crucial yet frequently overlooked problem known as multi-domain FL. In this scenario, clients' data originate from diverse domains with distinct feature distributions, as opposed to label distributions. To address the multi-domain problem in FL, we propose a novel method called Federated learning Without normalizations (FedWon). FedWon draws inspiration from the observation that batch normalization (BN) faces challenges in effectively modeling the statistics of multiple domains, while alternative normalization techniques possess their own limitations. In order to address these issues, FedWon elimi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#27979;&#38750;&#40065;&#26834;&#26041;&#21521;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23616;&#22495;&#20108;&#27425;&#36924;&#36817;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#24182;&#20026;&#23433;&#20840;&#35266;&#27979;&#21644;&#23545;&#25239;&#24615;&#35266;&#27979;&#20043;&#38388;&#30340;&#22522;&#26412;&#25130;&#27490;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#12290;&#24182;&#19988;&#35813;&#26041;&#27861;&#26159;&#38750;&#24120;&#26377;&#25928;&#30340;&#65292;&#33021;&#25104;&#21151;&#26816;&#27979;&#21040;&#23545;&#25239;&#24615;&#26041;&#21521;&#24182;&#20570;&#20986;&#40065;&#26834;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2306.05873</link><description>&lt;p&gt;
&#26816;&#27979;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23545;&#25239;&#24615;&#26041;&#21521;&#20197;&#20570;&#20986;&#40065;&#26834;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Detecting Adversarial Directions in Deep Reinforcement Learning to Make Robust Decisions. (arXiv:2306.05873v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05873
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#27979;&#38750;&#40065;&#26834;&#26041;&#21521;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23616;&#22495;&#20108;&#27425;&#36924;&#36817;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#24182;&#20026;&#23433;&#20840;&#35266;&#27979;&#21644;&#23545;&#25239;&#24615;&#35266;&#27979;&#20043;&#38388;&#30340;&#22522;&#26412;&#25130;&#27490;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#12290;&#24182;&#19988;&#35813;&#26041;&#27861;&#26159;&#38750;&#24120;&#26377;&#25928;&#30340;&#65292;&#33021;&#25104;&#21151;&#26816;&#27979;&#21040;&#23545;&#25239;&#24615;&#26041;&#21521;&#24182;&#20570;&#20986;&#40065;&#26834;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#29616;&#22312;&#21487;&#20197;&#22312;&#20855;&#26377;&#39640;&#24230;&#22797;&#26434;&#29366;&#24577;&#34920;&#31034;&#30340;MDPs&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22797;&#26434;&#24230;&#30340;&#22686;&#21152;&#20197;&#21450;&#35266;&#27979;&#31354;&#38388;&#32500;&#24230;&#30340;&#22686;&#21152;&#37117;&#24102;&#26469;&#20102;&#26131;&#21463;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#27874;&#21160;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#31574;&#30053;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#28145;&#24230;&#31070;&#32463;&#31574;&#30053;&#25439;&#22833;&#30340;&#23616;&#22495;&#20108;&#27425;&#36924;&#36817;&#26469;&#26816;&#27979;&#36825;&#20123;&#38750;&#40065;&#26834;&#26041;&#21521;&#30340;&#23384;&#22312;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#23433;&#20840;&#35266;&#27979;&#21644;&#23545;&#25239;&#24615;&#35266;&#27979;&#20043;&#38388;&#30340;&#22522;&#26412;&#25130;&#27490;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#20855;&#26377;&#35745;&#31639;&#19978;&#30340;&#25928;&#29575;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;&#29983;&#25104;&#26368;&#22351;&#24773;&#20917;&#26041;&#21521;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;Arcade Learning Environment&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#22810;&#31181;&#19981;&#21516;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#25216;&#26415;&#12290;&#26368;&#26174;&#30528;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#38750;&#24120;&#26377;&#25928;&#30340;&#65292;&#33021;&#22815;&#25104;&#21151;&#26816;&#27979;&#21040;&#23545;&#25239;&#24615;&#26041;&#21521;&#24182;&#20570;&#20986;&#30456;&#24212;&#30340;&#40065;&#26834;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning in MDPs with highly complex state representations is currently possible due to multiple advancements in reinforcement learning algorithm design. However, this incline in complexity, and furthermore the increase in the dimensions of the observation came at the cost of volatility that can be taken advantage of via adversarial attacks (i.e. moving along worst-case directions in the observation space). To solve this policy instability problem we propose a novel method to detect the presence of these non-robust directions via local quadratic approximation of the deep neural policy loss. Our method provides a theoretical basis for the fundamental cut-off between safe observations and adversarial observations. Furthermore, our technique is computationally efficient, and does not depend on the methods used to produce the worst-case directions. We conduct extensive experiments in the Arcade Learning Environment with several different adversarial attack techniques. Most significantly, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#39044;&#27979;&#26041;&#27861;&#21152;&#36895;M-&#20984;&#20989;&#25968;&#26497;&#23567;&#21270;&#30340;&#26694;&#26550;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#20998;&#23618;&#20984;&#26497;&#23567;&#21270;&#31561;&#31163;&#25955;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#36739;&#22909;&#22320;&#25913;&#21892;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.05865</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#27979;&#21152;&#36895;&#20998;&#31163;&#20984;&#20989;&#25968;&#26497;&#23567;&#21270;&#65306;M-&#20984;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Faster Discrete Convex Function Minimization with Predictions: The M-Convex Case. (arXiv:2306.05865v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#39044;&#27979;&#26041;&#27861;&#21152;&#36895;M-&#20984;&#20989;&#25968;&#26497;&#23567;&#21270;&#30340;&#26694;&#26550;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#20998;&#23618;&#20984;&#26497;&#23567;&#21270;&#31561;&#31163;&#25955;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#36739;&#22909;&#22320;&#25913;&#21892;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#32773;&#23558;&#26426;&#22120;&#23398;&#20064;&#30340;&#39044;&#27979;&#26041;&#27861;&#24212;&#29992;&#20110;&#20248;&#21270;&#31639;&#27861;&#30340;&#21152;&#36895;&#12290;Sakaue&#21644;Oki&#65288;NeurIPS 2022&#65289;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#27979;&#26041;&#27861;&#21551;&#21160;L-&#20984;&#20989;&#25968;&#26497;&#23567;&#21270;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#21508;&#31181;&#31163;&#25955;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#39044;&#27979;&#26041;&#27861;&#21152;&#36895;M-&#20984;&#20989;&#25968;&#26497;&#23567;&#21270;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#34917;&#20805;&#20197;&#21069;&#30340;&#30740;&#31350;&#65292;&#25193;&#23637;&#33021;&#20174;&#39044;&#27979;&#20013;&#21463;&#30410;&#30340;&#31163;&#25955;&#20248;&#21270;&#31639;&#27861;&#33539;&#22260;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#37325;&#35201;&#23376;&#31867;&#8212;&#8212;&#20998;&#23618;&#20984;&#26497;&#23567;&#21270;&#20013;&#29305;&#21035;&#26377;&#25928;&#65292;&#35813;&#23376;&#31867;&#20986;&#29616;&#22312;&#35768;&#22810;&#36816;&#31609;&#23398;&#24212;&#29992;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#39044;&#27979;&#25913;&#21892;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#26368;&#24046;&#24773;&#20917;&#19979;&#30340;&#32467;&#26524;&#65292;&#29978;&#33267;&#26377;&#21487;&#33021;&#36229;&#36807;&#26368;&#20302;&#19979;&#30028;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen a growing interest in accelerating optimization algorithms with machine-learned predictions. Sakaue and Oki (NeurIPS 2022) have developed a general framework that warm-starts the L-convex function minimization method with predictions, revealing the idea's usefulness for various discrete optimization problems. In this paper, we present a framework for using predictions to accelerate M-convex function minimization, thus complementing previous research and extending the range of discrete optimization algorithms that can benefit from predictions. Our framework is particularly effective for an important subclass called laminar convex minimization, which appears in many operations research applications. Our methods can improve time complexity bounds upon the best worst-case results by using predictions and even have potential to go beyond a lower-bound result.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#35774;&#23450;&#65292;&#25506;&#35752;&#20102;&#36890;&#20449;&#27425;&#25968;&#23545;&#27867;&#21270;&#35823;&#24046;&#30340;&#24433;&#21709;&#65292;&#24182;&#24314;&#31435;&#20102;PAC-Bayes&#21644;&#29575;&#22833;&#30495;&#29702;&#35770;&#38480;&#21046;&#65292;&#36825;&#20123;&#38480;&#21046;&#23545;&#24191;&#27867;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#23398;&#20064;&#31639;&#27861;&#36866;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.05862</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65306;&#20943;&#23569;&#36890;&#20449;&#27425;&#25968;&#65281;
&lt;/p&gt;
&lt;p&gt;
Federated Learning You May Communicate Less Often!. (arXiv:2306.05862v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#35774;&#23450;&#65292;&#25506;&#35752;&#20102;&#36890;&#20449;&#27425;&#25968;&#23545;&#27867;&#21270;&#35823;&#24046;&#30340;&#24433;&#21709;&#65292;&#24182;&#24314;&#31435;&#20102;PAC-Bayes&#21644;&#29575;&#22833;&#30495;&#29702;&#35770;&#38480;&#21046;&#65292;&#36825;&#20123;&#38480;&#21046;&#23545;&#24191;&#27867;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#23398;&#20064;&#31639;&#27861;&#36866;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#32852;&#37030;&#23398;&#20064;(Federated Learning, FL)&#27169;&#22411;&#22312;&#19968;&#33324;&#24615;&#30340;&#35774;&#32622;&#19979;&#30340;&#27867;&#21270;&#35823;&#24046;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23458;&#25143;&#31471;&#21644;&#21442;&#25968;&#26381;&#21153;&#22120;&#20043;&#38388;&#36890;&#20449;&#27425;&#25968;&#30340;&#27867;&#21270;&#35823;&#24046;&#28436;&#21464;&#65292;&#21363;&#23458;&#25143;&#31471;&#35745;&#31639;&#30340;&#26412;&#22320;&#27169;&#22411;&#22312;&#21442;&#25968;&#26381;&#21153;&#22120;&#19978;&#21512;&#24182;&#30340;&#39057;&#29575;&#23545;&#27867;&#21270;&#35823;&#24046;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;PAC-Bayes&#21644;&#29575;&#22833;&#30495;&#29702;&#35770;&#23545;&#27867;&#21270;&#35823;&#24046;&#30340;&#38480;&#21046;&#65292;&#26126;&#30830;&#32771;&#34385;&#36890;&#20449;&#27425;&#25968;&#23545;&#35823;&#24046;&#30340;&#24433;&#21709;&#65292;&#21478;&#22806;&#36824;&#32771;&#34385;&#20102;&#21442;&#19982;&#35774;&#22791;&#25968;&#37327;K&#21644;&#20010;&#20154;&#25968;&#25454;&#38598;&#22823;&#23567;n&#23545;&#35823;&#24046;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#38480;&#21046;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#23398;&#20064;&#31639;&#27861;&#65292;&#20284;&#20046;&#26159;FL&#35774;&#32622;&#20013;&#39318;&#27425;&#20986;&#29616;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#38480;&#21046;&#24212;&#29992;&#20110;FL&#31867;&#22411;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;(FSVM)&#65307;&#25105;&#20204;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#25512;&#23548;&#20102;&#26356;&#26126;&#30830;&#30340;&#27867;&#21270;&#35823;&#24046;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the generalization error of statistical learning models in a Federated Learning (FL) setting. Specifically, we study the evolution of the generalization error with the number of communication rounds between the clients and the parameter server, i.e., the effect on the generalization error of how often the local models as computed by the clients are aggregated at the parameter server. We establish PAC-Bayes and rate-distortion theoretic bounds on the generalization error that account explicitly for the effect of the number of rounds, say $ R \in \mathbb{N}$, in addition to the number of participating devices $K$ and individual datasets size $n$. The bounds, which apply in their generality for a large class of loss functions and learning algorithms, appear to be the first of their kind for the FL setting. Furthermore, we apply our bounds to FL-type Support Vector Machines (FSVM); and we derive (more) explicit bounds on the generalization error in this case. In particular, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22855;&#30340;&#22312;&#32447;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#36817;&#20284;&#23545;&#25239;&#26680;&#24182;&#20351;&#29992;&#26631;&#20934;&#38750;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#23398;&#20064;&#40065;&#26834;&#31574;&#30053;&#65292;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#22522;&#30784;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20043;&#19978;&#65292;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#39640;&#32500;&#24230;&#22495;&#12290;</title><link>http://arxiv.org/abs/2306.05859</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#26680;&#36817;&#20284;&#23454;&#29616;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Reinforcement Learning via Adversarial Kernel Approximation. (arXiv:2306.05859v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05859
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22855;&#30340;&#22312;&#32447;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#36817;&#20284;&#23545;&#25239;&#26680;&#24182;&#20351;&#29992;&#26631;&#20934;&#38750;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#23398;&#20064;&#40065;&#26834;&#31574;&#30053;&#65292;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#22522;&#30784;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20043;&#19978;&#65292;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#39640;&#32500;&#24230;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#36716;&#31227;&#26680;&#21457;&#29983;&#25200;&#21160;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#24207;&#21015;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#22312;&#20855;&#26377;&#39640;&#32500;&#24230;&#22495;&#30340;&#29616;&#23454;&#22312;&#32447;&#29615;&#22659;&#20013;&#65292;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24182;&#19981;&#23481;&#26131;&#25193;&#23637;&#12290;&#36890;&#36807;&#34920;&#24449;&#40065;&#26834;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#23545;&#25239;&#26680;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36817;&#20284;&#23545;&#25239;&#26680;&#24182;&#20351;&#29992;&#26631;&#20934;&#65288;&#38750;&#40065;&#26834;&#65289;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#23398;&#20064;&#19968;&#20010;&#40065;&#26834;&#24615;&#26041;&#38024;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#22522;&#30784;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20043;&#19978;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#39640;&#32500;&#24230;&#22495;&#30340;&#36731;&#26494;&#25193;&#23637;&#12290;&#22312;&#32463;&#20856;&#25511;&#21046;&#20219;&#21153;&#12289;MinAtar&#21644;DeepMind&#25511;&#21046;&#22871;&#20214;&#20013;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust Markov Decision Processes (RMDPs) provide a framework for sequential decision-making that is robust to perturbations on the transition kernel. However, robust reinforcement learning (RL) approaches in RMDPs do not scale well to realistic online settings with high-dimensional domains. By characterizing the adversarial kernel in RMDPs, we propose a novel approach for online robust RL that approximates the adversarial kernel and uses a standard (non-robust) RL algorithm to learn a robust policy. Notably, our approach can be applied on top of any underlying RL algorithm, enabling easy scaling to high-dimensional domains. Experiments in classic control tasks, MinAtar and DeepMind Control Suite demonstrate the effectiveness and the applicability of our method.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#29289;&#20307;&#20449;&#24687;&#34701;&#20837;&#22522;&#20110;&#39592;&#26550;&#30340;&#20154;&#31867;&#34892;&#20026;&#35782;&#21035;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#27492;&#26469;&#25552;&#39640;&#21327;&#20316;&#26426;&#22120;&#20154;&#22312;&#24037;&#19994;&#21046;&#36896;&#20013;&#30340;&#20154;&#26426;&#21327;&#20316;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.05844</link><description>&lt;p&gt;
&#22312;&#35013;&#37197;&#20219;&#21153;&#20013;&#65292;&#29289;&#20307;&#20449;&#24687;&#22914;&#20309;&#25552;&#39640;&#22522;&#20110;&#39592;&#26550;&#30340;&#20154;&#31867;&#34892;&#20026;&#35782;&#21035;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
How Object Information Improves Skeleton-based Human Action Recognition in Assembly Tasks. (arXiv:2306.05844v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#29289;&#20307;&#20449;&#24687;&#34701;&#20837;&#22522;&#20110;&#39592;&#26550;&#30340;&#20154;&#31867;&#34892;&#20026;&#35782;&#21035;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#27492;&#26469;&#25552;&#39640;&#21327;&#20316;&#26426;&#22120;&#20154;&#22312;&#24037;&#19994;&#21046;&#36896;&#20013;&#30340;&#20154;&#26426;&#21327;&#20316;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21327;&#20316;&#26426;&#22120;&#20154; (cobots) &#22312;&#24037;&#19994;&#21046;&#36896;&#20013;&#30340;&#24212;&#29992;&#19981;&#26029;&#22686;&#38271;&#65292;&#26377;&#25928;&#30340;&#20154;&#26426;&#21327;&#20316;&#38656;&#35201;&#20154;&#31867;&#34892;&#20026;&#35782;&#21035;&#33021;&#21147;&#30340;&#25903;&#25345;&#12290;&#39592;&#26550;&#20998;&#26512;&#26041;&#27861;&#22240;&#20854;&#26356;&#22909;&#22320;&#36866;&#24212;&#19981;&#21516;&#20154;&#21644;&#29615;&#22659;&#32780;&#32463;&#24120;&#34987;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#20165;&#22788;&#29702;&#39592;&#26550;&#26102;&#65292;&#20154;&#19982;&#29289;&#20307;&#30340;&#20132;&#20114;&#20449;&#24687;&#23558;&#20002;&#22833;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#29289;&#20307;&#20449;&#24687;&#34701;&#20837;&#22522;&#20110;&#39592;&#26550;&#30340;&#20154;&#31867;&#34892;&#20026;&#35782;&#21035;&#30340;&#26032;&#26041;&#27861;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#29289;&#20307;&#20013;&#24515;&#35270;&#20026;&#36827;&#19968;&#27493;&#30340;&#39592;&#26550;&#20851;&#33410;&#65292;&#22686;&#24378;&#20102;&#20004;&#31181;&#20808;&#36827;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20351;&#29992;&#20102;&#19968;&#20010;&#20808;&#36827;&#30340;&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#23545;&#39592;&#26550;&#20851;&#33410;&#19982;&#29289;&#20307;&#30340;&#39044;&#27979;&#36827;&#34892;&#20102;&#32452;&#21512;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22823;&#22823;&#25913;&#21892;&#20102;&#36825;&#20123;&#20808;&#36827;&#26041;&#27861;&#22312;&#35013;&#37197;&#25968;&#25454;&#38598;IKEA ASM&#19978;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#20026;&#22312;&#24037;&#19994;&#21046;&#36896;&#20013;&#23454;&#29616;&#26377;&#25928;&#30340;&#20154;&#26426;&#21327;&#20316;&#20197;&#21450;&#39592;&#26550;&#26816;&#27979;&#28155;&#21152;&#29289;&#20307;&#20449;&#24687;&#30340;&#22909;&#22788;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the use of collaborative robots (cobots) in industrial manufacturing continues to grow, human action recognition for effective human-robot collaboration becomes increasingly important. This ability is crucial for cobots to act autonomously and assist in assembly tasks. Recently, skeleton-based approaches are often used as they tend to generalize better to different people and environments. However, when processing skeletons alone, information about the objects a human interacts with is lost. Therefore, we present a novel approach of integrating object information into skeleton-based action recognition. We enhance two state-of-the-art methods by treating object centers as further skeleton joints. Our experiments on the assembly dataset IKEA ASM show that our approach improves the performance of these state-of-the-art methods to a large extent when combining skeleton joints with objects predicted by a state-of-the-art instance segmentation model. Our research sheds light on the benefi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;cSOBER&#65292;&#19968;&#31181;&#22788;&#29702;&#22810;&#26679;&#21270;&#32422;&#26463;&#26465;&#20214;&#12289;&#31163;&#25955;&#21644;&#28151;&#21512;&#31354;&#38388;&#12289;&#26410;&#30693;&#32422;&#26463;&#20197;&#21450;&#26597;&#35810;&#25298;&#32477;&#38382;&#39064;&#30340;&#39046;&#22495;&#26080;&#20851;&#22411;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.05843</link><description>&lt;p&gt;
&#26080;&#39046;&#22495;&#20559;&#35265;&#25209;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#31215;&#20998;&#22788;&#29702;&#22810;&#31181;&#32422;&#26463;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
Domain-Agnostic Batch Bayesian Optimization with Diverse Constraints via Bayesian Quadrature. (arXiv:2306.05843v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;cSOBER&#65292;&#19968;&#31181;&#22788;&#29702;&#22810;&#26679;&#21270;&#32422;&#26463;&#26465;&#20214;&#12289;&#31163;&#25955;&#21644;&#28151;&#21512;&#31354;&#38388;&#12289;&#26410;&#30693;&#32422;&#26463;&#20197;&#21450;&#26597;&#35810;&#25298;&#32477;&#38382;&#39064;&#30340;&#39046;&#22495;&#26080;&#20851;&#22411;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#30340;&#20248;&#21270;&#38382;&#39064;&#36890;&#24120;&#20855;&#26377;&#22810;&#26679;&#30340;&#32422;&#26463;&#26465;&#20214;&#12289;&#31163;&#25955;&#21644;&#28151;&#21512;&#31354;&#38388;&#12289;&#39640;&#24230;&#21487;&#24182;&#34892;&#21270;&#31561;&#29305;&#28857;&#12290;&#21516;&#26102;&#65292;&#24403;&#23384;&#22312;&#26410;&#30693;&#32422;&#26463;&#26102;&#65292;&#20363;&#22914;&#22312;&#33647;&#29289;&#21457;&#29616;&#21644;&#21160;&#29289;&#23454;&#39564;&#23433;&#20840;&#24615;&#31561;&#39046;&#22495;&#65292;&#24517;&#39035;&#30830;&#31435;&#26410;&#30693;&#32422;&#26463;&#20043;&#21518;&#25165;&#33021;&#26597;&#35810;&#30446;&#26631;&#20989;&#25968;&#12290;&#29616;&#26377;&#24037;&#20316;&#36890;&#24120;&#20165;&#38024;&#23545;&#19978;&#36848;&#26576;&#20123;&#29305;&#24449;&#32780;&#24182;&#38750;&#32508;&#21512;&#32771;&#34385;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;cSOBER&#65292;&#19968;&#31181;&#22522;&#20110;SOBER&#31639;&#27861;&#30340;&#39046;&#22495;&#26080;&#20851;&#22411;&#35880;&#24910;&#24182;&#34892;&#20027;&#21160;&#37319;&#26679;&#22120;&#65292;&#32771;&#34385;&#21040;&#20102;&#26410;&#30693;&#32422;&#26463;&#24773;&#20917;&#19979;&#30340;&#38598;&#25104;&#35823;&#24046;&#30340;&#24433;&#21709;&#24182;&#25552;&#20986;&#20102;&#22788;&#29702;&#26041;&#27861;&#65292;&#22788;&#29702;&#22810;&#31181;&#32422;&#26463;&#26465;&#20214;&#21644;&#26410;&#30693;&#32422;&#26463;&#26597;&#35810;&#25298;&#32477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world optimisation problems often feature complex combinations of (1) diverse constraints, (2) discrete and mixed spaces, and are (3) highly parallelisable. (4) There are also cases where the objective function cannot be queried if unknown constraints are not satisfied, e.g. in drug discovery, safety on animal experiments (unknown constraints) must be established before human clinical trials (querying objective function) may proceed. However, most existing works target each of the above three problems in isolation and do not consider (4) unknown constraints with query rejection. For problems with diverse constraints and/or unconventional input spaces, it is difficult to apply these techniques as they are often mutually incompatible. We propose cSOBER, a domain-agnostic prudent parallel active sampler for Bayesian optimisation, based on SOBER of Adachi et al. (2023). We consider infeasibility under unknown constraints as a type of integration error that we can estimate. We propose 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21516;&#24577;&#26144;&#23556;&#30340;&#26032;&#22411;&#38543;&#26426;&#22270;&#23884;&#20837;&#26041;&#27861;&#65292;&#22312;&#26399;&#26395;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35745;&#31639;&#65292;&#33021;&#22312;&#26399;&#26395;&#20013;&#21306;&#20998;&#25152;&#26377;&#38750;&#21516;&#26500;&#22270;&#65292;&#20855;&#26377;&#39640;&#25928;&#30340;&#26367;&#20195;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.05838</link><description>&lt;p&gt;
&#22522;&#20110;&#21516;&#24577;&#26144;&#23556;&#30340;&#26399;&#26395;&#23436;&#20840;&#22270;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Expectation-Complete Graph Representations with Homomorphisms. (arXiv:2306.05838v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05838
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21516;&#24577;&#26144;&#23556;&#30340;&#26032;&#22411;&#38543;&#26426;&#22270;&#23884;&#20837;&#26041;&#27861;&#65292;&#22312;&#26399;&#26395;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35745;&#31639;&#65292;&#33021;&#22312;&#26399;&#26395;&#20013;&#21306;&#20998;&#25152;&#26377;&#38750;&#21516;&#26500;&#22270;&#65292;&#20855;&#26377;&#39640;&#25928;&#30340;&#26367;&#20195;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#22270;&#23884;&#20837;&#65292;&#21487;&#20197;&#22312;&#26399;&#26395;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35745;&#31639;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#26399;&#26395;&#20013;&#21306;&#20998;&#25152;&#26377;&#38750;&#21516;&#26500;&#22270;&#12290;&#20197;&#21069;&#30340;&#22270;&#23884;&#20837;&#20855;&#26377;&#26377;&#38480;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#19988;&#35201;&#20040;&#19981;&#33021;&#21306;&#20998;&#25152;&#26377;&#22270;&#65292;&#35201;&#20040;&#19981;&#33021;&#26377;&#25928;&#22320;&#35745;&#31639;&#27599;&#20010;&#22270;&#12290;&#20026;&#20102;&#33021;&#22815;&#22312;&#22270;&#19978;&#36817;&#20284;&#20219;&#24847;&#20989;&#25968;&#65292;&#25105;&#20204;&#23545;&#20855;&#26377;&#36882;&#22686;&#36164;&#28304;&#30340;&#39640;&#25928;&#26367;&#20195;&#26041;&#26696;&#24863;&#20852;&#36259;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110; Lov\'asz &#23545;&#36890;&#36807;&#21516;&#24577;&#35745;&#25968;&#30340;&#26080;&#38480;&#32500;&#21521;&#37327;&#30340;&#22270;&#21516;&#26500;&#30340;&#34920;&#24449;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#26174;&#31034;&#20102;&#22312;&#20960;&#20010;&#22522;&#20934;&#22270;&#23398;&#20064;&#20219;&#21153;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate novel random graph embeddings that can be computed in expected polynomial time and that are able to distinguish all non-isomorphic graphs in expectation. Previous graph embeddings have limited expressiveness and either cannot distinguish all graphs or cannot be computed efficiently for every graph. To be able to approximate arbitrary functions on graphs, we are interested in efficient alternatives that become arbitrarily expressive with increasing resources. Our approach is based on Lov\'asz' characterisation of graph isomorphism through an infinite dimensional vector of homomorphism counts. Our empirical evaluation shows competitive results on several benchmark graph learning tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65288;Corr2Cause&#65289;&#65292;&#29992;&#20110;&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22240;&#26524;&#25512;&#26029;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#24456;&#24046;&#12290;</title><link>http://arxiv.org/abs/2306.05836</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#20174;&#30456;&#20851;&#24615;&#20013;&#25512;&#26029;&#20986;&#22240;&#26524;&#20851;&#31995;?
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Infer Causation from Correlation?. (arXiv:2306.05836v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65288;Corr2Cause&#65289;&#65292;&#29992;&#20110;&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22240;&#26524;&#25512;&#26029;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#24456;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#26159;&#20154;&#31867;&#26234;&#24935;&#30340;&#26631;&#24535;&#20043;&#19968;&#12290;&#34429;&#28982;CausalNLP&#39046;&#22495;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;NLP&#20013;&#29616;&#26377;&#30340;&#22240;&#26524;&#25512;&#26029;&#25968;&#25454;&#38598;&#20027;&#35201;&#20381;&#36182;&#20110;&#20174;&#32463;&#39564;&#30693;&#35782;&#65288;&#20363;&#22914;&#24120;&#35782;&#30693;&#35782;&#65289;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#32431;&#22240;&#26524;&#25512;&#26029;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;Corr2Cause&#65292;&#23427;&#37319;&#29992;&#19968;&#32452;&#30456;&#20851;&#35821;&#21477;&#24182;&#30830;&#23450;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;400K&#20010;&#26679;&#26412;&#65292;&#25105;&#20204;&#22312;&#20854;&#20013;&#35780;&#20272;&#20102;17&#20010;&#29616;&#26377;&#30340;LLMs&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;LLMs&#22312;&#22240;&#26524;&#25512;&#26029;&#25216;&#33021;&#26041;&#38754;&#30340;&#19968;&#20010;&#20851;&#38190;&#32570;&#38519;&#65292;&#24182;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20960;&#20046;&#25509;&#36817;&#38543;&#26426;&#12290;&#24403;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#24494;&#35843;&#23558;LLMs&#37325;&#26032;&#29992;&#20110;&#36825;&#31181;&#25216;&#33021;&#26102;&#65292;&#36825;&#31181;&#32570;&#38519;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#24471;&#21040;&#20102;&#32531;&#35299;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#22833;&#36133;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal inference is one of the hallmarks of human intelligence. While the field of CausalNLP has attracted much interest in the recent years, existing causal inference datasets in NLP primarily rely on discovering causality from empirical knowledge (e.g., commonsense knowledge). In this work, we propose the first benchmark dataset to test the pure causal inference skills of large language models (LLMs). Specifically, we formulate a novel task Corr2Cause, which takes a set of correlational statements and determines the causal relationship between the variables. We curate a large-scale dataset of more than 400K samples, on which we evaluate seventeen existing LLMs. Through our experiments, we identify a key shortcoming of LLMs in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task. This shortcoming is somewhat mitigated when we try to re-purpose LLMs for this skill via finetuning, but we find that these models still fa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#20598;&#21270;&#25193;&#23637;&#30340;&#26680;&#20027;&#25104;&#20998;&#20998;&#26512;&#26041;&#27861;&#65292;&#20351;KPCA&#33258;&#28982;&#22320;&#25193;&#23637;&#21040;&#22810;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#23548;&#33268;&#36991;&#20813;&#35745;&#31639;&#26684;&#25289;&#22982;&#30697;&#38453;&#30340;&#26114;&#36149;SVD&#30340;&#39640;&#25928;&#26799;&#24230;&#31639;&#27861;&#12290;&#35813;&#26041;&#27861;&#36824;&#33021;&#22312;&#21516;&#19968;&#26694;&#26550;&#20869;&#20419;&#36827;&#31283;&#20581;&#24615;&#21644;&#31232;&#30095;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.05815</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#20598;&#21270;&#25193;&#23637;&#26680;&#20027;&#25104;&#20998;&#20998;&#26512;&#65306;&#31232;&#30095;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#24555;&#36895;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Extending Kernel PCA through Dualization: Sparsity, Robustness and Fast Algorithms. (arXiv:2306.05815v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#20598;&#21270;&#25193;&#23637;&#30340;&#26680;&#20027;&#25104;&#20998;&#20998;&#26512;&#26041;&#27861;&#65292;&#20351;KPCA&#33258;&#28982;&#22320;&#25193;&#23637;&#21040;&#22810;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#23548;&#33268;&#36991;&#20813;&#35745;&#31639;&#26684;&#25289;&#22982;&#30697;&#38453;&#30340;&#26114;&#36149;SVD&#30340;&#39640;&#25928;&#26799;&#24230;&#31639;&#27861;&#12290;&#35813;&#26041;&#27861;&#36824;&#33021;&#22312;&#21516;&#19968;&#26694;&#26550;&#20869;&#20419;&#36827;&#31283;&#20581;&#24615;&#21644;&#31232;&#30095;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23545;&#20598;&#21270;&#20984;&#24046;&#20989;&#25968;&#65292;&#37325;&#26032;&#23457;&#35270;&#26680;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;KPCA&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;KPCA&#33258;&#28982;&#22320;&#25193;&#23637;&#21040;&#22810;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#23548;&#33268;&#36991;&#20813;&#35745;&#31639;&#26684;&#25289;&#22982;&#30697;&#38453;&#30340;&#26114;&#36149;SVD&#30340;&#39640;&#25928;&#26799;&#24230;&#31639;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#32771;&#34385;&#21487;&#20197;&#20889;&#25104;Moreau&#21253;&#32476;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#28436;&#31034;&#22914;&#20309;&#22312;&#21516;&#19968;&#26694;&#26550;&#20869;&#20419;&#36827;&#31283;&#20581;&#24615;&#21644;&#31232;&#30095;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#26174;&#31034;&#20986;KPCA&#35757;&#32451;&#26102;&#38388;&#26174;&#33879;&#21152;&#36895;&#65292;&#32780;&#19988;&#22312;&#31283;&#20581;&#24615;&#21644;&#31232;&#30095;&#24615;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of this paper is to revisit Kernel Principal Component Analysis (KPCA) through dualization of a difference of convex functions. This allows to naturally extend KPCA to multiple objective functions and leads to efficient gradient-based algorithms avoiding the expensive SVD of the Gram matrix. Particularly, we consider objective functions that can be written as Moreau envelopes, demonstrating how to promote robustness and sparsity within the same framework. The proposed method is evaluated on synthetic and real-world benchmarks, showing significant speedup in KPCA training time as well as highlighting the benefits in terms of robustness and sparsity.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20808;&#39564;&#30693;&#35782;&#30340;&#28145;&#24230;&#33258;&#32534;&#30721;&#26694;&#26550;PAAE&#65292;&#23558;&#29983;&#29289;&#36890;&#36335;&#20449;&#24687;&#34701;&#20837;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#25552;&#39640;&#23545;&#30142;&#30149;&#30340;&#39044;&#27979;&#24314;&#27169;&#21644;&#20010;&#24615;&#21270;&#27835;&#30103;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.05813</link><description>&lt;p&gt;
&#22522;&#20110;&#36890;&#36335;&#27963;&#24615;&#33258;&#32534;&#30721;&#22120;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#34701;&#20837;&#20808;&#39564;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Incorporating Prior Knowledge in Deep Learning Models via Pathway Activity Autoencoders. (arXiv:2306.05813v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20808;&#39564;&#30693;&#35782;&#30340;&#28145;&#24230;&#33258;&#32534;&#30721;&#26694;&#26550;PAAE&#65292;&#23558;&#29983;&#29289;&#36890;&#36335;&#20449;&#24687;&#34701;&#20837;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#25552;&#39640;&#23545;&#30142;&#30149;&#30340;&#39044;&#27979;&#24314;&#27169;&#21644;&#20010;&#24615;&#21270;&#27835;&#30103;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#39640;&#36890;&#37327;&#20998;&#23376;&#20998;&#26512;&#25216;&#26415;&#65288;&#20363;&#22914;&#36716;&#24405;&#32452;&#23398;&#65289;&#30340;&#35745;&#31639;&#20998;&#26512;&#21462;&#24471;&#30340;&#36827;&#23637;&#65292;&#26041;&#27861;&#30340;&#31616;&#21333;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#19982;&#26041;&#27861;&#30340;&#22797;&#26434;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#36739;&#20302;&#20043;&#38388;&#23384;&#22312;&#19968;&#31181;&#20108;&#20998;&#27861;&#12290;&#27492;&#22806;&#65292;&#24456;&#23569;&#26377;&#26041;&#27861;&#35797;&#22270;&#23558;&#35299;&#37322;&#24615;&#36716;&#21270;&#20026;&#29983;&#29289;&#23398;&#30456;&#20851;&#26415;&#35821;&#65292;&#20363;&#22914;&#24050;&#30693;&#30340;&#36890;&#36335;&#32423;&#32852;&#12290;&#29983;&#29289;&#23398;&#36890;&#36335;&#21453;&#26144;&#20449;&#21495;&#20107;&#20214;&#25110;&#20195;&#35874;&#36716;&#21270;&#12290;&#36890;&#36807;&#30830;&#23450;&#21738;&#20123;&#36890;&#36335;&#28041;&#21450;&#30142;&#30149;&#24182;&#23558;&#27492;&#31867;&#36890;&#36335;&#25968;&#25454;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#32435;&#20837;&#21487;&#33021;&#22686;&#24378;&#39044;&#27979;&#24314;&#27169;&#21644;&#20010;&#24615;&#21270;&#35786;&#26029;&#27835;&#30103;&#21644;&#39044;&#38450;&#30142;&#30149;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivation: Despite advances in the computational analysis of high-throughput molecular profiling assays (e.g. transcriptomics), a dichotomy exists between methods that are simple and interpretable, and ones that are complex but with lower degree of interpretability. Furthermore, very few methods deal with trying to translate interpretability in biologically relevant terms, such as known pathway cascades. Biological pathways reflecting signalling events or metabolic conversions are Small improvements or modifications of existing algorithms will generally not be suitable, unless novel biological results have been predicted and verified. Determining which pathways are implicated in disease and incorporating such pathway data as prior knowledge may enhance predictive modelling and personalised strategies for diagnosis, treatment and prevention of disease.  Results: We propose a novel prior-knowledge-based deep auto-encoding framework, PAAE, together with its accompanying generative varian
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#23558;HRTF&#19978;&#37319;&#26679;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#20351;&#24102;HRTF&#30340;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#21644;&#22686;&#24378;&#29616;&#23454;&#65288;AR&#65289;&#29615;&#22659;&#26356;&#21152;&#36924;&#30495;&#12290;</title><link>http://arxiv.org/abs/2306.05812</link><description>&lt;p&gt;
&#20351;&#29992;gnomonic equiangular&#25237;&#24433;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#36827;&#34892;HRTF&#19978;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
HRTF upsampling with a generative adversarial network using a gnomonic equiangular projection. (arXiv:2306.05812v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#23558;HRTF&#19978;&#37319;&#26679;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#20351;&#24102;HRTF&#30340;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#21644;&#22686;&#24378;&#29616;&#23454;&#65288;AR&#65289;&#29615;&#22659;&#26356;&#21152;&#36924;&#30495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#30340;&#22836;&#37096;&#30456;&#20851;&#36716;&#31227;&#20989;&#25968;&#65288;HRTF&#65289;&#23545;&#20110;&#21019;&#24314;&#36924;&#30495;&#30340;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#21644;&#22686;&#24378;&#29616;&#23454;&#65288;AR&#65289;&#29615;&#22659;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22768;&#23398;&#27979;&#37327;&#39640;&#36136;&#37327;&#30340;HRTF&#38656;&#35201;&#26114;&#36149;&#30340;&#35774;&#22791;&#21644;&#22768;&#23398;&#23454;&#39564;&#23460;&#35774;&#32622;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#24182;&#20351;&#27979;&#37327;&#26356;&#21152;&#39640;&#25928;&#65292;&#36807;&#21435;&#24050;&#32463;&#21033;&#29992;&#20102;HRTF&#19978;&#37319;&#26679;&#65292;&#20854;&#20013;&#20174;&#20302;&#20998;&#36776;&#29575;&#30340;HRTF&#21019;&#24314;&#39640;&#20998;&#36776;&#29575;&#30340;HRTF&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#24212;&#29992;&#20110;HRTF&#19978;&#37319;&#26679;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;HRTF&#25968;&#25454;&#36716;&#25442;&#20026;&#19982;&#21367;&#31215;&#36229;&#20998;&#36776;&#29575;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;SRGAN&#65289;&#26041;&#20415;&#20351;&#29992;&#12290;&#36825;&#31181;&#26032;&#26041;&#27861;&#19982;&#20004;&#20010;&#22522;&#32447;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65306;&#37325;&#24515;&#19978;&#37319;&#26679;&#21644;HRTF&#36873;&#25321;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20351;&#29992;&#24863;&#30693;&#27169;&#22411;&#26102;&#65292;&#20174;log-spectral&#22833;&#30495;&#65288;LSD&#65289;&#21644;&#23450;&#20301;&#24615;&#33021;&#26041;&#38754;&#22343;&#20248;&#20110;&#20004;&#20010;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
An individualised head-related transfer function (HRTF) is essential for creating realistic virtual reality (VR) and augmented reality (AR) environments. However, acoustically measuring high-quality HRTFs requires expensive equipment and an acoustic lab setting. To overcome these limitations and to make this measurement more efficient HRTF upsampling has been exploited in the past where a high-resolution HRTF is created from a low-resolution one. This paper demonstrates how generative adversarial networks (GANs) can be applied to HRTF upsampling. We propose a novel approach that transforms the HRTF data for convenient use with a convolutional super-resolution generative adversarial network (SRGAN). This new approach is benchmarked against two baselines: barycentric upsampling and a HRTF selection approach. Experimental results show that the proposed method outperforms both baselines in terms of log-spectral distortion (LSD) and localisation performance using perceptual models when the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Shapley&#20540;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#31216;&#20043;&#20026;Shapley Values for Explaining Reinforcement Learning (SVERL)&#65292;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20855;&#26377;&#21305;&#37197;&#24182;&#34917;&#20805;&#20154;&#31867;&#30452;&#35273;&#30340;&#26377;&#24847;&#20041;&#35299;&#37322;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.05810</link><description>&lt;p&gt;
&#20351;&#29992;Shapley Values&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Explaining Reinforcement Learning with Shapley Values. (arXiv:2306.05810v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05810
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Shapley&#20540;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#31216;&#20043;&#20026;Shapley Values for Explaining Reinforcement Learning (SVERL)&#65292;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20855;&#26377;&#21305;&#37197;&#24182;&#34917;&#20805;&#20154;&#31867;&#30452;&#35273;&#30340;&#26377;&#24847;&#20041;&#35299;&#37322;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35201;&#24819;&#24191;&#27867;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#65292;&#29992;&#25143;&#24517;&#39035;&#29702;&#35299;&#24182;&#20449;&#20219;&#23427;&#20204;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;Shapley&#20540;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#35813;&#20998;&#26512;&#36981;&#24490;&#21338;&#24328;&#35770;&#30340;&#21407;&#21017;&#65292;&#29992;&#20110;&#30830;&#23450;&#20010;&#20307;&#29609;&#23478;&#23545;&#21512;&#20316;&#21338;&#24328;&#32467;&#26524;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;Shapley Values for Explaining Reinforcement Learning (SVERL)&#30340;&#19968;&#33324;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#20197;&#21069;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20351;&#29992;Shapley&#20540;&#30340;&#23616;&#38480;&#24615;&#65292;&#28982;&#21518;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20351;&#29992;Shapley&#20540;&#35299;&#37322;&#20195;&#29702;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#65292;SVERL&#25552;&#20379;&#20102;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#65292;&#21305;&#37197;&#24182;&#34917;&#20805;&#20102;&#20154;&#31867;&#30340;&#30452;&#35266;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
For reinforcement learning systems to be widely adopted, their users must understand and trust them. We present a theoretical analysis of explaining reinforcement learning using Shapley values, following a principled approach from game theory for identifying the contribution of individual players to the outcome of a cooperative game. We call this general framework Shapley Values for Explaining Reinforcement Learning (SVERL). Our analysis exposes the limitations of earlier uses of Shapley values in reinforcement learning. We then develop an approach that uses Shapley values to explain agent performance. In a variety of domains, SVERL produces meaningful explanations that match and supplement human intuition.
&lt;/p&gt;</description></item><item><title>RankFormer&#26159;&#19968;&#20010;&#21487;&#20197;&#21033;&#29992;&#21015;&#34920;&#26631;&#31614;&#36827;&#34892;&#21015;&#34920;&#23398;&#20064;&#25490;&#24207;&#30340;&#26550;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;LTR&#26041;&#27861;&#19978;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.05808</link><description>&lt;p&gt;
RankFormer&#65306;&#20351;&#29992;&#21015;&#34920;&#26631;&#31614;&#30340;&#21015;&#34920;&#23398;&#20064;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
RankFormer: Listwise Learning-to-Rank Using Listwide Labels. (arXiv:2306.05808v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05808
&lt;/p&gt;
&lt;p&gt;
RankFormer&#26159;&#19968;&#20010;&#21487;&#20197;&#21033;&#29992;&#21015;&#34920;&#26631;&#31614;&#36827;&#34892;&#21015;&#34920;&#23398;&#20064;&#25490;&#24207;&#30340;&#26550;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;LTR&#26041;&#27861;&#19978;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#24212;&#29992;&#31243;&#24207;&#24120;&#24120;&#20351;&#29992;&#25490;&#24207;&#27169;&#22411;&#23558;&#26368;&#30456;&#20851;&#30340;&#32467;&#26524;&#25490;&#22312;&#21069;&#38754;&#65292;&#20197;&#21576;&#29616;&#32473;&#29992;&#25143;&#26377;&#38480;&#30340;&#36873;&#25321;&#12290;&#36890;&#24120;&#20551;&#23450;&#20174;&#29992;&#25143;&#33719;&#24471;&#30340;&#21453;&#39304;&#21482;&#21453;&#26144;&#20102;&#39033;&#30446;&#25928;&#29992;&#30340;&#30456;&#23545;&#35780;&#20215;&#65292;&#20363;&#22914;&#29992;&#25143;&#21333;&#20987;&#39033;&#30446;&#21482;&#26263;&#31034;&#23427;&#27604;&#22312;&#21516;&#19968;&#25490;&#24207;&#21015;&#34920;&#20013;&#26410;&#21333;&#20987;&#30340;&#39033;&#30446;&#26356;&#22909;&#12290;&#22240;&#27492;&#65292;&#23398;&#20064;&#25490;&#24207;&#65288;LTR&#65289;&#20013;&#20248;&#21270;&#30340;&#30446;&#26631;&#24448;&#24448;&#26159;&#25104;&#23545;&#25110;&#25353;&#21015;&#34920;&#25490;&#24207;&#12290;&#28982;&#32780;&#65292;&#21482;&#30475;&#21040;&#30456;&#23545;&#21453;&#39304;&#65292;&#25105;&#20204;&#24573;&#35270;&#20102;&#29992;&#25143;&#23545;&#21015;&#34920;&#25972;&#20307;&#36136;&#37327;&#30340;&#32477;&#23545;&#21453;&#39304;&#65292;&#20363;&#22914;&#24403;&#36873;&#25321;&#20013;&#27809;&#26377;&#39033;&#30446;&#34987;&#21333;&#20987;&#26102;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#20102;&#26631;&#20934;&#30340;LTR&#33539;&#24335;&#65292;&#24182;&#35770;&#36848;&#20102;&#20174;&#36825;&#31181;&#21015;&#34920;&#32423;&#20449;&#21495;&#20013;&#23398;&#20064;&#30340;&#22909;&#22788;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RankFormer&#20316;&#20026;&#19968;&#20010;&#24102;&#26377;Transformer&#26680;&#24515;&#30340;&#26550;&#26500;&#65292;&#21487;&#20197;&#20849;&#21516;&#20248;&#21270;&#26032;&#30340;&#21015;&#34920;&#35780;&#20272;&#30446;&#26631;&#21644;&#20256;&#32479;&#30340;&#25353;&#21015;&#34920;LTR&#30446;&#26631;&#12290;&#25105;&#20204;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#27169;&#25311;&#38544;&#24335;&#21453;&#39304;&#65292;&#24182;&#35266;&#23519;&#21040;RankFormer&#27604;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;LTR&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#21033;&#29992;&#21015;&#34920;&#26631;&#31614;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Web applications where users are presented with a limited selection of items have long employed ranking models to put the most relevant results first. Any feedback received from users is typically assumed to reflect a relative judgement on the utility of items, e.g. a user clicking on an item only implies it is better than items not clicked in the same ranked list. Hence, the objectives optimized in Learning-to-Rank (LTR) tend to be pairwise or listwise.  Yet, by only viewing feedback as relative, we neglect the user's absolute feedback on the list's overall quality, e.g. when no items in the selection are clicked. We thus reconsider the standard LTR paradigm and argue the benefits of learning from this listwide signal. To this end, we propose the RankFormer as an architecture that, with a Transformer at its core, can jointly optimize a novel listwide assessment objective and a traditional listwise LTR objective.  We simulate implicit feedback on public datasets and observe that the Ra
&lt;/p&gt;</description></item><item><title>DynaBench&#26159;&#19968;&#20010;&#26032;&#30340;&#27169;&#25311;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30452;&#25509;&#20174;&#20302;&#20998;&#36776;&#29575;&#30340;&#31232;&#30095;&#25968;&#25454;&#20013;&#23398;&#20064;&#21160;&#21147;&#31995;&#32479;&#65292;&#35780;&#20272;&#20102;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05805</link><description>&lt;p&gt;
DynaBench: &#20174;&#20302;&#20998;&#36776;&#29575;&#25968;&#25454;&#20013;&#23398;&#20064;&#21160;&#21147;&#31995;&#32479;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
DynaBench: A benchmark dataset for learning dynamical systems from low-resolution data. (arXiv:2306.05805v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05805
&lt;/p&gt;
&lt;p&gt;
DynaBench&#26159;&#19968;&#20010;&#26032;&#30340;&#27169;&#25311;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30452;&#25509;&#20174;&#20302;&#20998;&#36776;&#29575;&#30340;&#31232;&#30095;&#25968;&#25454;&#20013;&#23398;&#20064;&#21160;&#21147;&#31995;&#32479;&#65292;&#35780;&#20272;&#20102;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#29289;&#29702;&#31995;&#32479;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#39640;&#20998;&#36776;&#29575;&#32593;&#26684;&#32467;&#26500;&#27979;&#37327;&#12290;&#20294;&#26159;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#36825;&#31181;&#31995;&#32479;&#30340;&#30693;&#35782;&#65288;&#20363;&#22914;&#22825;&#27668;&#25968;&#25454;&#65289;&#20381;&#36182;&#20110;&#31232;&#30095;&#20998;&#24067;&#30340;&#27979;&#37327;&#31449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27169;&#25311;&#22522;&#20934;&#25968;&#25454;&#38598;DynaBench&#65292;&#29992;&#20110;&#30452;&#25509;&#20174;&#31232;&#30095;&#25955;&#24067;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#21160;&#21147;&#31995;&#32479;&#65292;&#19981;&#38656;&#35201;&#20808;&#21069;&#20102;&#35299;&#35813;&#26041;&#31243;&#24335;&#12290;&#35813;&#25968;&#25454;&#38598;&#19987;&#27880;&#20110;&#39044;&#27979;&#21160;&#21147;&#31995;&#32479;&#30340;&#28436;&#21464;&#65292;&#20351;&#29992;&#20302;&#20998;&#36776;&#29575;&#12289;&#38750;&#32467;&#26500;&#21270;&#30340;&#27979;&#37327;&#26041;&#24335;&#12290;&#25105;&#20204;&#27169;&#25311;&#20102;&#20845;&#20010;&#19981;&#21516;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#28085;&#30422;&#20102;&#25991;&#29486;&#20013;&#24120;&#29992;&#30340;&#21508;&#31181;&#29289;&#29702;&#31995;&#32479;&#65292;&#24182;&#35780;&#20272;&#20102;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21253;&#25324;&#20256;&#32479;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#28857;&#20113;&#22788;&#29702;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#31995;&#32479;&#30340;&#28436;&#21464;&#12290;&#35813;&#22522;&#20934;&#25968;&#25454;&#38598;&#21487;&#20197;&#26399;&#26395;&#25104;&#20026;&#19968;&#31181;&#24320;&#31665;&#21363;&#29992;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous work on learning physical systems from data has focused on high-resolution grid-structured measurements. However, real-world knowledge of such systems (e.g. weather data) relies on sparsely scattered measuring stations. In this paper, we introduce a novel simulated benchmark dataset, DynaBench, for learning dynamical systems directly from sparsely scattered data without prior knowledge of the equations. The dataset focuses on predicting the evolution of a dynamical system from low-resolution, unstructured measurements. We simulate six different partial differential equations covering a variety of physical systems commonly used in the literature and evaluate several machine learning models, including traditional graph neural networks and point cloud processing models, with the task of predicting the evolution of the system. The proposed benchmark dataset is expected to advance the state of art as an out-of-the-box easy-to-use tool for evaluating models in a setting where only u
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#20027;&#39064;&#24314;&#27169;&#21644;&#24773;&#24863;&#20998;&#26512;&#30456;&#32467;&#21512;&#65292;&#24314;&#31435;&#20102;&#20851;&#20110;&#21152;&#23494;&#36135;&#24065;&#30340;&#22810;&#20010;&#21465;&#36848;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#21465;&#36848;&#19982;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#20043;&#38388;&#23384;&#22312;&#24378;&#22823;&#30340;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.05803</link><description>&lt;p&gt;
&#24773;&#24863;&#21644;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Causality between Sentiment and Cryptocurrency Prices. (arXiv:2306.05803v1 [q-fin.CP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#20027;&#39064;&#24314;&#27169;&#21644;&#24773;&#24863;&#20998;&#26512;&#30456;&#32467;&#21512;&#65292;&#24314;&#31435;&#20102;&#20851;&#20110;&#21152;&#23494;&#36135;&#24065;&#30340;&#22810;&#20010;&#21465;&#36848;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#21465;&#36848;&#19982;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#20043;&#38388;&#23384;&#22312;&#24378;&#22823;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#24494;&#21338;&#24179;&#21488;&#65288;Twitter&#65289;&#20256;&#36882;&#30340;&#21465;&#36848;&#19982;&#21152;&#23494;&#36164;&#20135;&#20215;&#20540;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37319;&#29992;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#25216;&#26415;&#65292;&#23558;&#30701;&#25991;&#26412;&#30340;&#20027;&#39064;&#24314;&#27169;&#19982;&#24773;&#24863;&#20998;&#26512;&#30456;&#32467;&#21512;&#65292;&#24314;&#31435;&#20102;&#20851;&#20110;&#21152;&#23494;&#36135;&#24065;&#30340;&#21465;&#36848;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#20174;Twitter&#30340;&#22823;&#35268;&#27169;&#21644;&#22024;&#26434;&#25991;&#26412;&#25968;&#25454;&#20013;&#21457;&#29616;&#28508;&#22312;&#20027;&#39064;&#65292;&#28982;&#21518;&#25105;&#20204;&#25581;&#31034;&#20102;4-5&#20010;&#19982;&#21152;&#23494;&#36135;&#24065;&#30456;&#20851;&#30340;&#21465;&#36848;&#65292;&#21253;&#25324;&#19982;&#21152;&#23494;&#36135;&#24065;&#30456;&#20851;&#30340;&#37329;&#34701;&#25237;&#36164;&#12289;&#25216;&#26415;&#36827;&#27493;&#12289;&#37329;&#34701;&#21644;&#25919;&#27835;&#30417;&#31649;&#12289;&#21152;&#23494;&#36164;&#20135;&#21644;&#23186;&#20307;&#25253;&#36947;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#25105;&#20204;&#30340;&#21465;&#36848;&#19982;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#20043;&#38388;&#23384;&#22312;&#24378;&#22823;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#26368;&#26032;&#30340;&#32463;&#27982;&#23398;&#21019;&#26032;&#8212;&#8212;&#21465;&#20107;&#32463;&#27982;&#23398;&#19982;&#20027;&#39064;&#24314;&#27169;&#21644;&#24773;&#24863;&#20998;&#26512;&#30456;&#32467;&#21512;&#30340;&#26032;&#39046;&#22495;&#32852;&#31995;&#36215;&#26469;&#65292;&#20197;&#20851;&#32852;&#28040;&#36153;&#32773;&#34892;&#20026;&#21644;&#21465;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates the relationship between narratives conveyed through microblogging platforms, namely Twitter, and the value of crypto assets. Our study provides a unique technique to build narratives about cryptocurrency by combining topic modelling of short texts with sentiment analysis. First, we used an unsupervised machine learning algorithm to discover the latent topics within the massive and noisy textual data from Twitter, and then we revealed 4-5 cryptocurrency-related narratives, including financial investment, technological advancement related to crypto, financial and political regulations, crypto assets, and media coverage. In a number of situations, we noticed a strong link between our narratives and crypto prices. Our work connects the most recent innovation in economics, Narrative Economics, to a new area of study that combines topic modelling and sentiment analysis to relate consumer behaviour to narratives.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;G-Enum&#30452;&#26041;&#22270;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;MDL&#21407;&#21017;&#26500;&#24314;&#30452;&#26041;&#22270;&#65292;&#22312;&#38750;&#24120;&#20934;&#30830;&#30340;&#24773;&#20917;&#19979;,&#26080;&#38656;&#20219;&#20309;&#29992;&#25143;&#21442;&#25968;&#65292;&#24182;&#22312;&#24322;&#24120;&#20540;&#25110;&#37325;&#23614;&#20998;&#24067;&#24773;&#20917;&#19979;&#25552;&#20986;&#19968;&#31181;&#21452;&#23618;&#21551;&#21457;&#24335;&#26041;&#27861;</title><link>http://arxiv.org/abs/2306.05786</link><description>&lt;p&gt;
&#22788;&#29702;&#24322;&#24120;&#20540;&#21644;&#37325;&#23614;&#20998;&#24067;&#30340;&#21452;&#23618;&#30452;&#26041;&#22270;
&lt;/p&gt;
&lt;p&gt;
Two-level histograms for dealing with outliers and heavy tail distributions. (arXiv:2306.05786v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;G-Enum&#30452;&#26041;&#22270;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;MDL&#21407;&#21017;&#26500;&#24314;&#30452;&#26041;&#22270;&#65292;&#22312;&#38750;&#24120;&#20934;&#30830;&#30340;&#24773;&#20917;&#19979;,&#26080;&#38656;&#20219;&#20309;&#29992;&#25143;&#21442;&#25968;&#65292;&#24182;&#22312;&#24322;&#24120;&#20540;&#25110;&#37325;&#23614;&#20998;&#24067;&#24773;&#20917;&#19979;&#25552;&#20986;&#19968;&#31181;&#21452;&#23618;&#21551;&#21457;&#24335;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#26041;&#22270;&#26159;&#22312;&#25506;&#32034;&#24615;&#20998;&#26512;&#20013;&#29992;&#20110;&#24635;&#32467;&#19968;&#20803;&#20998;&#24067;&#30340;&#26368;&#27969;&#34892;&#26041;&#27861;&#20043;&#19968;&#12290;&#23588;&#20854;&#26159;&#19981;&#35268;&#21017;&#30340;&#30452;&#26041;&#22270;&#26159;&#33391;&#22909;&#30340;&#38750;&#21442;&#25968;&#23494;&#24230;&#20272;&#35745;&#37327;&#65292;&#20854;&#20165;&#38656;&#35201;&#26497;&#23569;&#30340;&#21442;&#25968;&#65306;&#39057;&#29575;&#21644;&#24102;&#26377;&#38271;&#24230;&#30340;&#31665;&#25968;&#12290;&#33267;&#20170;&#24050;&#32463;&#26377;&#35768;&#22810;&#26041;&#27861;&#34987;&#25552;&#20986;&#29992;&#20110;&#25512;&#26029;&#36825;&#20123;&#21442;&#25968;&#65292;&#26377;&#20123;&#37319;&#29992;&#23545;&#22522;&#30784;&#25968;&#25454;&#20998;&#24067;&#30340;&#27169;&#22411;&#20551;&#35774;&#65292;&#26377;&#20123;&#21017;&#29992;&#29305;&#23450;&#30340;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#12290;&#26412;&#25991;&#20851;&#27880;&#30340;&#26159;G-Enum&#30452;&#26041;&#22270;&#26041;&#27861;&#65292;&#20854;&#37319;&#29992;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#65288;MDL&#65289;&#27861;&#24314;&#31435;&#30452;&#26041;&#22270;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#29992;&#25143;&#21442;&#25968;&#65292;&#24182;&#19988;&#22312;&#20934;&#30830;&#24615;&#12289;&#31616;&#27905;&#24615;&#21644;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#35813;&#26041;&#27861;&#22312;&#24322;&#24120;&#20540;&#25110;&#37325;&#23614;&#20998;&#24067;&#24773;&#20917;&#19979;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#22788;&#29702;&#36825;&#20123;&#24773;&#20917;&#12290; &#31532;&#19968;&#23618;&#21033;&#29992;&#25968;&#25454;&#30340;&#23545;&#25968;&#21464;&#25442;&#23558;&#25968;&#25454;&#38598;&#20998;&#20026;&#25968;&#25454;&#23376;&#38598;&#21015;&#34920;
&lt;/p&gt;
&lt;p&gt;
Histograms are among the most popular methods used in exploratory analysis to summarize univariate distributions. In particular, irregular histograms are good non-parametric density estimators that require very few parameters: the number of bins with their lengths and frequencies. Many approaches have been proposed in the literature to infer these parameters, either assuming hypotheses about the underlying data distributions or exploiting a model selection approach. In this paper, we focus on the G-Enum histogram method, which exploits the Minimum Description Length (MDL) principle to build histograms without any user parameter and achieves state-of-the art performance w.r.t accuracy; parsimony and computation time. We investigate on the limits of this method in the case of outliers or heavy-tailed distributions. We suggest a two-level heuristic to deal with such cases. The first level exploits a logarithmic transformation of the data to split the data set into a list of data subsets w
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#25216;&#26415;&#65292;&#36890;&#36807;&#20248;&#21270;&#27169;&#22411;&#30340;&#28014;&#28857;&#36816;&#31639;&#37327;&#25110;&#35774;&#22791;&#24310;&#36831;&#26469;&#33258;&#21160;&#35774;&#32622;&#21387;&#32553;&#36229;&#21442;&#25968;&#12290;&#31639;&#27861;&#36895;&#24230;&#24555;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#22810;&#31181;&#24120;&#29992;&#21387;&#32553;&#26041;&#27861;&#19968;&#36215;&#20351;&#29992;&#65292;&#22914;&#21098;&#26525;&#12289;&#20302;&#31209;&#20998;&#35299;&#21644;&#37327;&#21270;&#12290;&#22312;GLUE&#24494;&#35843;&#20219;&#21153;&#30340;BERT&#21387;&#32553;&#20013;&#65292;FLOPs&#21487;&#38477;&#20302;50&#65285;&#65292;&#19988;&#24615;&#33021;&#20165;&#19979;&#38477;1&#65285;&#65307;&#32780;&#22312;ImageNet-1K&#19978;&#23545;MobileNetV3&#36827;&#34892;&#21387;&#32553;&#65292;&#21017;&#21487;&#20197;&#20351;FLOPs&#38477;&#20302;15&#65285;&#65292;&#21516;&#26102;&#24615;&#33021;&#22522;&#26412;&#19981;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2306.05785</link><description>&lt;p&gt;
&#36890;&#36807;$\frac{\ell_1}{\ell_2}$&#27491;&#21017;&#21270;&#24310;&#36831;&#20195;&#29702;&#36827;&#34892;&#31471;&#21040;&#31471;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
End-to-End Neural Network Compression via $\frac{\ell_1}{\ell_2}$ Regularized Latency Surrogates. (arXiv:2306.05785v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05785
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#25216;&#26415;&#65292;&#36890;&#36807;&#20248;&#21270;&#27169;&#22411;&#30340;&#28014;&#28857;&#36816;&#31639;&#37327;&#25110;&#35774;&#22791;&#24310;&#36831;&#26469;&#33258;&#21160;&#35774;&#32622;&#21387;&#32553;&#36229;&#21442;&#25968;&#12290;&#31639;&#27861;&#36895;&#24230;&#24555;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#22810;&#31181;&#24120;&#29992;&#21387;&#32553;&#26041;&#27861;&#19968;&#36215;&#20351;&#29992;&#65292;&#22914;&#21098;&#26525;&#12289;&#20302;&#31209;&#20998;&#35299;&#21644;&#37327;&#21270;&#12290;&#22312;GLUE&#24494;&#35843;&#20219;&#21153;&#30340;BERT&#21387;&#32553;&#20013;&#65292;FLOPs&#21487;&#38477;&#20302;50&#65285;&#65292;&#19988;&#24615;&#33021;&#20165;&#19979;&#38477;1&#65285;&#65307;&#32780;&#22312;ImageNet-1K&#19978;&#23545;MobileNetV3&#36827;&#34892;&#21387;&#32553;&#65292;&#21017;&#21487;&#20197;&#20351;FLOPs&#38477;&#20302;15&#65285;&#65292;&#21516;&#26102;&#24615;&#33021;&#22522;&#26412;&#19981;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;&#21387;&#32553;&#36890;&#24120;&#38656;&#35201;&#25163;&#21160;&#25110;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#35774;&#32622;&#27599;&#20010;&#23618;&#30340;&#21387;&#32553;&#36229;&#21442;&#25968;&#65288;&#20363;&#22914;&#65292;&#35201;&#21098;&#26525;&#30340;&#36890;&#36947;&#25968;&#65292;&#37327;&#21270;&#30340;&#20301;&#23485;&#65289;&#65292;&#36825;&#21487;&#33021;&#20250;&#32791;&#36153;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#25216;&#26415;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;$\frac{\ell_1}{\ell_2}$&#24310;&#36831;&#20195;&#29702;&#20248;&#21270;&#27169;&#22411;&#30340;&#28014;&#28857;&#36816;&#31639;&#37327;&#65288;FLOPs&#65289;&#25110;&#35774;&#22791;&#24310;&#36831;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#38750;&#24120;&#28789;&#27963;&#65292;&#21487;&#20197;&#19982;&#35768;&#22810;&#24120;&#29992;&#30340;&#21387;&#32553;&#26041;&#27861;&#65288;&#21253;&#25324;&#21098;&#26525;&#12289;&#20302;&#31209;&#20998;&#35299;&#21644;&#37327;&#21270;&#65289;&#19968;&#36215;&#20351;&#29992;&#12290;&#20851;&#38190;&#26159;&#65292;&#23427;&#38750;&#24120;&#24555;&#36895;&#65292;&#20960;&#20046;&#19982;&#21333;&#27169;&#22411;&#35757;&#32451;&#30456;&#21516;&#30340;&#26102;&#38388;&#65292;&#36825;&#27604;&#26631;&#20934;NAS&#26041;&#27861;&#21487;&#33410;&#30465;&#22823;&#37327;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#22312;GLUE&#24494;&#35843;&#20219;&#21153;&#30340;BERT&#21387;&#32553;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#20351;FLOPs&#38477;&#20302;50&#65285;&#65292;&#20165;&#25439;&#22833;1&#65285;&#30340;&#24615;&#33021;&#12290;&#32780;&#22312;ImageNet-1K&#19978;&#23545;MobileNetV3&#36827;&#34892;&#21387;&#32553;&#65292;&#21017;&#21487;&#20197;&#20351;FLOPs&#38477;&#20302;15&#65285;&#65292;&#21516;&#26102;&#24615;&#33021;&#22522;&#26412;&#19981;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network (NN) compression via techniques such as pruning, quantization requires setting compression hyperparameters (e.g., number of channels to be pruned, bitwidths for quantization) for each layer either manually or via neural architecture search (NAS) which can be computationally expensive. We address this problem by providing an end-to-end technique that optimizes for model's Floating Point Operations (FLOPs) or for on-device latency via a novel $\frac{\ell_1}{\ell_2}$ latency surrogate. Our algorithm is versatile and can be used with many popular compression methods including pruning, low-rank factorization, and quantization. Crucially, it is fast and runs in almost the same amount of time as single model training; which is a significant training speed-up over standard NAS methods. For BERT compression on GLUE fine-tuning tasks, we achieve $50\%$ reduction in FLOPs with only $1\%$ drop in performance. For compressing MobileNetV3 on ImageNet-1K, we achieve $15\%$ reduction in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39640;&#20809;&#35889;&#25104;&#20687;&#30340;&#22696;&#27700;&#20998;&#26512;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#22696;&#27700;&#32858;&#31867;&#21644;&#21306;&#20998;&#19981;&#21516;&#30340;&#22696;&#27700;&#65292;&#20197;&#27492;&#26469;&#30830;&#23450;&#25991;&#26723;&#20013;&#20351;&#29992;&#30340;&#19981;&#21516;&#22696;&#27700;&#30340;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.05784</link><description>&lt;p&gt;
&#25968;&#37327;&#21270;&#22696;&#27700;&#20998;&#26512;&#65306;&#22522;&#20110;&#39640;&#20809;&#35889;&#25104;&#20687;&#20272;&#35745;&#25991;&#20214;&#20013;&#30340;&#22696;&#27700;&#25968;&#37327;
&lt;/p&gt;
&lt;p&gt;
Quantitative Ink Analysis: Estimating the Number of Inks in Documents through Hyperspectral Imaging. (arXiv:2306.05784v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39640;&#20809;&#35889;&#25104;&#20687;&#30340;&#22696;&#27700;&#20998;&#26512;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#22696;&#27700;&#32858;&#31867;&#21644;&#21306;&#20998;&#19981;&#21516;&#30340;&#22696;&#27700;&#65292;&#20197;&#27492;&#26469;&#30830;&#23450;&#25991;&#26723;&#20013;&#20351;&#29992;&#30340;&#19981;&#21516;&#22696;&#27700;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#20214;&#21462;&#35777;&#39046;&#22495;&#65292;&#22696;&#27700;&#20998;&#26512;&#22312;&#30830;&#23450;&#27861;&#24459;&#21644;&#21382;&#21490;&#25991;&#20214;&#30340;&#30495;&#23454;&#24615;&#20197;&#21450;&#26816;&#27979;&#20266;&#36896;&#26041;&#38754;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20165;&#20973;&#30446;&#35270;&#26816;&#26597;&#26080;&#27861;&#21306;&#20998;&#22806;&#35266;&#30456;&#20284;&#30340;&#22696;&#27700;&#65292;&#24517;&#39035;&#37319;&#29992;&#20808;&#36827;&#30340;&#31185;&#23398;&#25216;&#26415;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#20809;&#35889;&#25104;&#20687;&#30340;&#22696;&#27700;&#20998;&#26512;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#25968;&#30334;&#20010;&#29421;&#31364;&#30340;&#20809;&#35889;&#24102;&#20013;&#26816;&#26597;&#25991;&#20214;&#65292;&#25581;&#31034;&#38544;&#34255;&#30340;&#32454;&#33410;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#30830;&#23450;&#25991;&#26723;&#20013;&#20351;&#29992;&#30340;&#19981;&#21516;&#22696;&#27700;&#30340;&#25968;&#37327;&#12290;&#37319;&#29992;&#20102;&#19977;&#31181;&#32858;&#31867;&#31639;&#27861;&#65292;&#21363;k-means&#65292;Agglomerative&#21644;c-means&#65292;&#20197;&#20272;&#35745;&#23384;&#22312;&#30340;&#22696;&#27700;&#25968;&#37327;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#25968;&#25454;&#25552;&#21462;&#65292;&#22696;&#27700;&#20687;&#32032;&#20998;&#21106;&#21644;&#22696;&#27700;&#25968;&#30830;&#23450;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#25216;&#26415;&#22312;&#35782;&#21035;&#22696;&#27700;&#32858;&#31867;&#21644;&#21306;&#20998;&#19981;&#21516;&#22696;&#27700;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;&#39640;&#20809;&#35889;&#31435;&#26041;&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;......&#65288;&#26410;&#23436;&#24453;&#32493;&#65289;
&lt;/p&gt;
&lt;p&gt;
In the field of document forensics, ink analysis plays a crucial role in determining the authenticity of legal and historic documents and detecting forgery. Visual examination alone is insufficient for distinguishing visually similar inks, necessitating the use of advanced scientific techniques. This paper proposes an ink analysis technique based on hyperspectral imaging, which enables the examination of documents in hundreds of narrowly spaced spectral bands, revealing hidden details. The main objective of this study is to identify the number of distinct inks used in a document. Three clustering algorithms, namely k-means, Agglomerative, and c-means, are employed to estimate the number of inks present. The methodology involves data extraction, ink pixel segmentation, and ink number determination. The results demonstrate the effectiveness of the proposed technique in identifying ink clusters and distinguishing between different inks. The analysis of a hyperspectral cube dataset reveals
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22240;&#26524;&#22270;&#21457;&#29616;&#30340;&#36866;&#24212;&#24615;&#22797;&#26434;&#24230;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;r-adaptive&#31639;&#27861;&#22312;&#26368;&#23567;&#21270;&#24635;&#24178;&#39044;&#27425;&#25968;&#30340;&#21516;&#26102;&#27491;&#30830;&#23398;&#20064;&#20986;&#22240;&#26524;&#22270;&#12290;</title><link>http://arxiv.org/abs/2306.05781</link><description>&lt;p&gt;
&#22240;&#26524;&#22270;&#21457;&#29616;&#30340;&#36866;&#24212;&#24615;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Adaptivity Complexity for Causal Graph Discovery. (arXiv:2306.05781v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05781
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22240;&#26524;&#22270;&#21457;&#29616;&#30340;&#36866;&#24212;&#24615;&#22797;&#26434;&#24230;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;r-adaptive&#31639;&#27861;&#22312;&#26368;&#23567;&#21270;&#24635;&#24178;&#39044;&#27425;&#25968;&#30340;&#21516;&#26102;&#27491;&#30830;&#23398;&#20064;&#20986;&#22240;&#26524;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#21457;&#29616;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#20219;&#21153;&#26159;&#35774;&#35745;&#19968;&#20010;&#24178;&#39044;&#31574;&#30053;&#65292;&#22312;&#26368;&#23567;&#21270;&#25191;&#34892;&#24178;&#39044;&#30340;&#25968;&#37327;&#30340;&#21516;&#26102;&#23398;&#20064;&#21253;&#21547;n&#20010;&#33410;&#28857;&#30340;&#22240;&#26524;&#22270;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#20110;&#24635;&#20849;$r$&#20010;&#39034;&#24207;&#22238;&#21512;&#65292;&#31639;&#27861;&#35774;&#35745;&#24072;&#22914;&#20309;&#22312;&#26368;&#23567;&#21270;&#24635;&#24178;&#39044;&#27425;&#25968;&#30340;&#21516;&#26102;&#24674;&#22797;&#22240;&#26524;&#22270;&#30340;$r$&#36866;&#24212;&#24615;&#38382;&#39064;&#12290;&#23545;&#20110;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;$r$-adaptive&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;$O(n^2 2^r)$&#30340;&#24635;&#24178;&#39044;&#27425;&#25968;&#19979;&#65292;&#20197;&#39640;&#27010;&#29575;&#27491;&#30830;&#22320;&#23398;&#20064;$n$&#20010;&#33410;&#28857;&#30340;&#22240;&#26524;&#22270;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20010;&#30028;&#38480;&#26159;&#26368;&#20339;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal discovery from interventional data is an important problem, where the task is to design an interventional strategy that learns the hidden ground truth causal graph $G(V,E)$ on $|V| = n$ nodes while minimizing the number of performed interventions. Most prior interventional strategies broadly fall into two categories: non-adaptive and adaptive. Non-adaptive strategies decide on a single fixed set of interventions to be performed while adaptive strategies can decide on which nodes to intervene on sequentially based on past interventions. While adaptive algorithms may use exponentially fewer interventions than their non-adaptive counterparts, there are practical concerns that constrain the amount of adaptivity allowed. Motivated by this trade-off, we study the problem of $r$-adaptivity, where the algorithm designer recovers the causal graph under a total of $r$ sequential rounds whilst trying to minimize the total number of interventions. For this problem, we provide a $r$-adaptive
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Transformer&#30340;STRAFE&#27169;&#22411;&#65292;&#29992;&#20110;&#24930;&#24615;&#32958;&#33039;&#30142;&#30149;&#24694;&#21270;&#26102;&#38388;&#30340;&#29983;&#23384;&#20998;&#26512;&#39044;&#27979;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#34920;&#26126;&#20854;&#22312;&#39044;&#27979;&#29305;&#23450;&#20107;&#20214;&#30340;&#30830;&#20999;&#26102;&#38388;&#26041;&#38754;&#20855;&#26377;&#36739;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05779</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#24930;&#24615;&#32958;&#33039;&#30142;&#30149;&#24694;&#21270;&#26102;&#38388;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Transformer-based Time-to-Event Prediction for Chronic Kidney Disease Deterioration. (arXiv:2306.05779v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Transformer&#30340;STRAFE&#27169;&#22411;&#65292;&#29992;&#20110;&#24930;&#24615;&#32958;&#33039;&#30142;&#30149;&#24694;&#21270;&#26102;&#38388;&#30340;&#29983;&#23384;&#20998;&#26512;&#39044;&#27979;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#34920;&#26126;&#20854;&#22312;&#39044;&#27979;&#29305;&#23450;&#20107;&#20214;&#30340;&#30830;&#20999;&#26102;&#38388;&#26041;&#38754;&#20855;&#26377;&#36739;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#23588;&#20854;&#26159;Transformer&#27169;&#22411;&#65292;&#24050;&#22312;&#22686;&#24378;&#32437;&#21521;&#20581;&#24247;&#35760;&#24405;&#30340;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#22266;&#23450;&#26102;&#38388;&#39118;&#38505;&#39044;&#27979;&#19978;&#65292;&#20294;&#26102;&#38388;&#21040;&#36798;&#20107;&#20214;&#39044;&#27979;&#65288;&#20063;&#31216;&#20026;&#29983;&#23384;&#20998;&#26512;&#65289;&#24448;&#24448;&#26356;&#36866;&#21512;&#20110;&#20020;&#24202;&#22330;&#26223;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#21517;&#20026;STRAFE&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#36890;&#29992;&#29983;&#23384;&#20998;&#26512;&#20307;&#31995;&#32467;&#26500;&#65292;&#29992;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#12290;&#20351;&#29992;&#36229;&#36807;130,000&#20154;&#30340;&#30495;&#23454;&#19990;&#30028;&#32034;&#36180;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;STRAFE&#30340;&#24615;&#33021;&#65292;&#36825;&#20123;&#20154;&#24739;&#26377;3&#26399;&#24930;&#24615;&#32958;&#33039;&#30142;&#30149;&#65288;CKD&#65289;&#65292;&#21457;&#29616;&#20854;&#22312;&#39044;&#27979;&#21040;&#36798;5&#26399;&#30340;&#24694;&#21270;&#30340;&#30830;&#20999;&#26102;&#38388;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26102;&#38388;&#21040;&#36798;&#20107;&#20214;&#39044;&#27979;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;STRAFE&#22312;&#39044;&#27979;&#22266;&#23450;&#26102;&#38388;&#39118;&#38505;&#26041;&#38754;&#20063;&#20248;&#20110;&#20108;&#20803;&#32467;&#26524;&#31639;&#27861;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#20854;&#33021;&#22815;&#23545;&#34987;&#23457;&#26597;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;STRAFE&#30340;&#39044;&#27979;&#21487;&#20197;&#25552;&#39640;&#38451;&#24615;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep-learning techniques, particularly the transformer model, have shown great potential in enhancing the prediction performance of longitudinal health records. While previous methods have mainly focused on fixed-time risk prediction, time-to-event prediction (also known as survival analysis) is often more appropriate for clinical scenarios. Here, we present a novel deep-learning architecture we named STRAFE, a generalizable survival analysis transformer-based architecture for electronic health records. The performance of STRAFE was evaluated using a real-world claim dataset of over 130,000 individuals with stage 3 chronic kidney disease (CKD) and was found to outperform other time-to-event prediction algorithms in predicting the exact time of deterioration to stage 5. Additionally, STRAFE was found to outperform binary outcome algorithms in predicting fixed-time risk, possibly due to its ability to train on censored data. We show that STRAFE predictions can improve the positive predic
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#20351;&#29992;&#26435;&#37325;&#37325;&#26032;&#26144;&#23556;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;(VQC)&#30340;&#35757;&#32451;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;VQC&#35757;&#32451;&#26041;&#27861;&#26080;&#27861;&#20805;&#20998;&#32771;&#34385;&#37327;&#23376;&#35745;&#31639;&#20013;&#21487;&#35757;&#32451;&#21442;&#25968;(&#25110;&#26435;&#37325;)&#30340;&#38382;&#39064;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19971;&#31181;&#19981;&#21516;&#30340;&#26435;&#37325;&#37325;&#26032;&#26144;&#23556;&#20989;&#25968;&#26469;&#35780;&#20272;&#23427;&#20204;&#30340;&#23454;&#29616;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.05776</link><description>&lt;p&gt;
&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#30340;&#26435;&#37325;&#37325;&#26032;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Weight Re-Mapping for Variational Quantum Algorithms. (arXiv:2306.05776v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05776
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#26435;&#37325;&#37325;&#26032;&#26144;&#23556;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;(VQC)&#30340;&#35757;&#32451;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;VQC&#35757;&#32451;&#26041;&#27861;&#26080;&#27861;&#20805;&#20998;&#32771;&#34385;&#37327;&#23376;&#35745;&#31639;&#20013;&#21487;&#35757;&#32451;&#21442;&#25968;(&#25110;&#26435;&#37325;)&#30340;&#38382;&#39064;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19971;&#31181;&#19981;&#21516;&#30340;&#26435;&#37325;&#37325;&#26032;&#26144;&#23556;&#20989;&#25968;&#26469;&#35780;&#20272;&#23427;&#20204;&#30340;&#23454;&#29616;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#24191;&#27867;AI&#20219;&#21153;&#20013;&#30340;&#21331;&#36234;&#25104;&#21151;&#21551;&#21457;&#65292;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;(VQCs)&#26368;&#36817;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#26041;&#38754;&#20986;&#29616;&#20102;&#19968;&#27874;&#39640;&#28526;&#12290;VQC&#25152;&#23637;&#31034;&#20986;&#30340;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#22914;&#25913;&#36827;&#30340;&#27867;&#21270;&#21644;&#20943;&#23569;&#30340;&#21442;&#25968;&#35757;&#32451;&#35201;&#27714;&#65292;&#24402;&#21151;&#20110;&#37327;&#23376;&#35745;&#31639;&#30340;&#24378;&#22823;&#31639;&#27861;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;VQC&#35757;&#32451;&#26041;&#27861;&#27809;&#26377;&#20805;&#20998;&#32771;&#34385;&#21040;&#21487;&#35757;&#32451;&#21442;&#25968;(&#25110;&#26435;&#37325;)&#36890;&#24120;&#34987;&#29992;&#20316;&#26059;&#36716;&#38376;&#20013;&#30340;&#35282;&#24230;&#36825;&#19968;&#20107;&#23454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;Kolle&#31561;&#20154;&#24341;&#20837;&#30340;VQC&#26435;&#37325;&#37325;&#26032;&#26144;&#23556;&#27010;&#24565;&#12290;&#36825;&#31181;&#26041;&#27861;&#26126;&#30830;&#22320;&#23558;&#26435;&#37325;&#26144;&#23556;&#21040;&#38271;&#24230;&#20026;$2\pi$&#30340;&#21306;&#38388;&#65292;&#38236;&#20687;&#24120;&#35268;&#26426;&#22120;&#23398;&#20064;&#20013;&#24050;&#34987;&#35777;&#26126;&#22312;&#20247;&#22810;&#22330;&#26223;&#20013;&#38750;&#24120;&#26377;&#30410;&#30340;&#25968;&#25454;&#37325;&#26032;&#32553;&#25918;&#25216;&#26415;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19971;&#31181;&#19981;&#21516;&#30340;&#26435;&#37325;&#37325;&#26032;&#26144;&#23556;&#20989;&#25968;&#26469;&#35780;&#20272;&#23427;&#20204;&#30340;&#23454;&#29616;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the remarkable success of artificial neural networks across a broad spectrum of AI tasks, variational quantum circuits (VQCs) have recently seen an upsurge in quantum machine learning applications. The promising outcomes shown by VQCs, such as improved generalization and reduced parameter training requirements, are attributed to the robust algorithmic capabilities of quantum computing. However, the current gradient-based training approaches for VQCs do not adequately accommodate the fact that trainable parameters (or weights) are typically used as angles in rotational gates. To address this, we extend the concept of weight re-mapping for VQCs, as introduced by K\"olle et al. (2023). This approach unambiguously maps the weights to an interval of length $2\pi$, mirroring data rescaling techniques in conventional machine learning that have proven to be highly beneficial in numerous scenarios. In our study, we employ seven distinct weight re-mapping functions to assess their im
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#26435;&#37325;&#20923;&#32467;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#20840;&#36830;&#25509;&#23618;&#20013;&#30340;&#29305;&#23450;&#31070;&#32463;&#20803;&#23545;&#29305;&#23450;EEG&#20219;&#21153;&#30340;&#20915;&#31574;&#36807;&#31243;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#25552;&#39640;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#30340;&#24615;&#33021;&#65292;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.05775</link><description>&lt;p&gt;
&#26435;&#37325;&#20923;&#32467;&#65306;&#19968;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#22312;&#33041;&#30005;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Weight Freezing: A Regularization Approach for Fully Connected Layers with an Application in EEG Classification. (arXiv:2306.05775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#26435;&#37325;&#20923;&#32467;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#20840;&#36830;&#25509;&#23618;&#20013;&#30340;&#29305;&#23450;&#31070;&#32463;&#20803;&#23545;&#29305;&#23450;EEG&#20219;&#21153;&#30340;&#20915;&#31574;&#36807;&#31243;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#25552;&#39640;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#30340;&#24615;&#33021;&#65292;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33041;&#30005;&#35299;&#30721;&#39046;&#22495;&#65292;&#25552;&#39640;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#30340;&#24615;&#33021;&#20855;&#26377;&#37325;&#35201;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#26435;&#37325;&#20923;&#32467;&#8221;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;ANN&#27491;&#21017;&#21270;&#21644;&#31070;&#32463;&#31185;&#23398;&#20808;&#39564;&#30693;&#35782;&#30340;&#21407;&#21017;&#12290;&#26435;&#37325;&#20923;&#32467;&#30340;&#27010;&#24565;&#22260;&#32469;&#30528;&#36890;&#36807;&#20923;&#32467;&#20840;&#36830;&#25509;&#23618;&#20013;&#30340;&#29305;&#23450;&#26435;&#37325;&#26469;&#20943;&#23569;&#26576;&#20123;&#31070;&#32463;&#20803;&#23545;&#29305;&#23450;EEG&#20219;&#21153;&#30340;&#20915;&#31574;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;&#36825;&#26159;&#36890;&#36807;&#20351;&#29992;&#25513;&#30721;&#30697;&#38453;&#21644;&#38408;&#20540;&#26469;&#30830;&#23450;&#22312;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#38656;&#35201;&#20923;&#32467;&#30340;&#26435;&#37325;&#27604;&#20363;&#26469;&#23454;&#29616;&#30340;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23558;&#34987;&#25513;&#34109;&#30340;&#26435;&#37325;&#35774;&#32622;&#20026;&#38646;&#65292;&#26435;&#37325;&#20923;&#32467;&#19981;&#20165;&#21487;&#20197;&#22312;&#20855;&#26377;&#20840;&#36830;&#25509;&#23618;&#20998;&#31867;&#22120;&#30340;&#32593;&#32476;&#20013;&#23454;&#29616;&#31232;&#30095;&#36830;&#25509;&#65292;&#36824;&#21487;&#20197;&#20316;&#20026;&#20840;&#36830;&#25509;&#23618;&#30340;&#26377;&#25928;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of EEG decoding, enhancing the performance of artificial neural networks (ANNs) carries significant potential. This study introduces a novel approach, termed "weight freezing", that is anchored on the principles of ANN regularization and neuroscience prior knowledge. The concept of weight freezing revolves around the idea of reducing certain neurons' influence on the decision-making process for a specific EEG task by freezing specific weights in the fully connected layer during the backpropagation process. This is actualized through the use of a mask matrix and a threshold to determine the proportion of weights to be frozen during backpropagation. Moreover, by setting the masked weights to zero, weight freezing can not only realize sparse connections in networks with a fully connected layer as the classifier but also function as an efficacious regularization method for fully connected layers. Through experiments involving three distinct ANN architectures and three widely r
&lt;/p&gt;</description></item><item><title>&#33258;&#23450;&#36827;&#24230;&#32477;&#23545;&#23398;&#20064;&#20316;&#20026;&#35838;&#31243;&#23398;&#20064;&#30340;&#35268;&#21017;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#33258;&#23450;&#36827;&#24230;&#65288;&#28145;&#24230;&#65289;&#23398;&#20064;&#30340;&#26032;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;&#33258;&#23450;&#36827;&#24230;&#32477;&#23545;&#23398;&#20064;&#36827;&#23637;&#65288;SPALP&#65289;&#65292;&#35299;&#20915;&#20102;&#32477;&#23545;&#23398;&#20064;&#36827;&#23637;&#65288;ALP&#65289;&#22312;&#37325;&#22797;&#24050;&#23398;&#20064;&#30340;&#34892;&#20026;&#19978;&#28010;&#36153;&#35745;&#31639;&#36164;&#28304;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.05769</link><description>&lt;p&gt;
&#33258;&#23450;&#36827;&#24230;&#32477;&#23545;&#23398;&#20064;&#20316;&#20026;&#35838;&#31243;&#23398;&#20064;&#30340;&#35268;&#21017;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Self-Paced Absolute Learning Progress as a Regularized Approach to Curriculum Learning. (arXiv:2306.05769v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05769
&lt;/p&gt;
&lt;p&gt;
&#33258;&#23450;&#36827;&#24230;&#32477;&#23545;&#23398;&#20064;&#20316;&#20026;&#35838;&#31243;&#23398;&#20064;&#30340;&#35268;&#21017;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#33258;&#23450;&#36827;&#24230;&#65288;&#28145;&#24230;&#65289;&#23398;&#20064;&#30340;&#26032;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;&#33258;&#23450;&#36827;&#24230;&#32477;&#23545;&#23398;&#20064;&#36827;&#23637;&#65288;SPALP&#65289;&#65292;&#35299;&#20915;&#20102;&#32477;&#23545;&#23398;&#20064;&#36827;&#23637;&#65288;ALP&#65289;&#22312;&#37325;&#22797;&#24050;&#23398;&#20064;&#30340;&#34892;&#20026;&#19978;&#28010;&#36153;&#35745;&#31639;&#36164;&#28304;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#29992;&#24615;&#21463;&#21040;&#20854;&#25152;&#38656;&#30340;&#22823;&#37327;&#35745;&#31639;&#26102;&#38388;&#30340;&#38480;&#21046;&#12290;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#23450;&#20041;&#24110;&#21161;&#26234;&#33021;&#20307;&#36935;&#21040;&#20219;&#21153;&#30340;&#26377;&#29992;&#39034;&#24207;&#65288;&#21363;&#20174;&#31616;&#21333;&#21040;&#38590;&#65289;&#65292;&#21152;&#36895;&#23398;&#20064;&#12290;&#22522;&#20110;&#32477;&#23545;&#23398;&#20064;&#36827;&#23637;&#65288;ALP&#65289;&#30340;&#35838;&#31243;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#24050;&#34987;&#35777;&#26126;&#25104;&#21151;&#65292;&#20294;&#28010;&#36153;&#35745;&#31639;&#36164;&#28304;&#22312;&#26032;&#20219;&#21153;&#20013;&#37325;&#22797;&#24050;&#23398;&#20064;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#33258;&#23450;&#36827;&#24230;&#65288;&#28145;&#24230;&#65289;&#23398;&#20064;&#30340;&#26032;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;&#33258;&#23450;&#36827;&#24230;&#32477;&#23545;&#23398;&#20064;&#36827;&#23637;&#65288;SPALP&#65289;&#65292;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25928;&#26524;&#19982;&#21407;&#22987;ALP&#30456;&#24403;&#65292;&#24182;&#19988;&#22312;&#20854;&#20013;&#20004;&#31181;&#24773;&#20917;&#19979;&#26356;&#24555;&#22320;&#36798;&#21040;&#20102;ALP&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#36824;&#38416;&#36848;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;SPALP&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The usability of Reinforcement Learning is restricted by the large computation times it requires. Curriculum Reinforcement Learning speeds up learning by defining a helpful order in which an agent encounters tasks, i.e. from simple to hard. Curricula based on Absolute Learning Progress (ALP) have proven successful in different environments, but waste computation on repeating already learned behaviour in new tasks. We solve this problem by introducing a new regularization method based on Self-Paced (Deep) Learning, called Self-Paced Absolute Learning Progress (SPALP). We evaluate our method in three different environments. Our method achieves performance comparable to original ALP in all cases, and reaches it quicker than ALP in two of them. We illustrate possibilities to further improve the efficiency and performance of SPALP.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#20844;&#24179;&#30340;&#21327;&#20316;&#23398;&#20064;&#28608;&#21169;&#35774;&#35745;&#65292;&#36991;&#20813;&#20102;&#8220;&#23500;&#32773;&#36234;&#23500;&#8221;&#30340;&#29616;&#35937;&#65292;&#24182;&#20026;&#36739;&#23569;&#36164;&#28304;&#30340;&#33410;&#28857;&#25552;&#20379;&#20102;&#38271;&#26399;&#24179;&#31561;&#30340;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2306.05764</link><description>&lt;p&gt;
&#20844;&#24179;&#20294;&#28176;&#36817;&#30456;&#31561;&#30340;&#21327;&#20316;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fair yet Asymptotically Equal Collaborative Learning. (arXiv:2306.05764v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#20844;&#24179;&#30340;&#21327;&#20316;&#23398;&#20064;&#28608;&#21169;&#35774;&#35745;&#65292;&#36991;&#20813;&#20102;&#8220;&#23500;&#32773;&#36234;&#23500;&#8221;&#30340;&#29616;&#35937;&#65292;&#24182;&#20026;&#36739;&#23569;&#36164;&#28304;&#30340;&#33410;&#28857;&#25552;&#20379;&#20102;&#38271;&#26399;&#24179;&#31561;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27969;&#25968;&#25454;&#30340;&#21327;&#20316;&#23398;&#20064;&#20013;&#65292;&#33410;&#28857;&#65288;&#20363;&#22914;&#32452;&#32455;&#65289;&#36890;&#36807;&#20849;&#20139;&#20174;&#20854;&#26368;&#26032;&#27969;&#25968;&#25454;&#35745;&#31639;&#20986;&#30340;&#26368;&#26032;&#27169;&#22411;&#26356;&#26032;&#26469;&#20849;&#21516;&#25345;&#32493;&#23398;&#20064;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#12290;&#20026;&#20102;&#26356;&#26377;&#36164;&#28304;&#30340;&#33410;&#28857;&#24895;&#24847;&#20849;&#20139;&#20854;&#27169;&#22411;&#26356;&#26032;&#65292;&#20182;&#20204;&#38656;&#35201;&#24471;&#21040;&#20844;&#24179;&#30340;&#28608;&#21169;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#28608;&#21169;&#35774;&#35745;&#65292;&#20445;&#35777;&#20844;&#24179;&#65292;&#20351;&#33410;&#28857;&#33719;&#24471;&#19982;&#20854;&#36129;&#29486;&#30456;&#31216;&#30340;&#22870;&#21169;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#25506;&#32034;-&#21033;&#29992;&#30340;&#24418;&#24335;&#20272;&#35745;&#33410;&#28857;&#30340;&#36129;&#29486;&#65288;&#21363;&#25506;&#32034;&#65289;&#65292;&#23454;&#29616;&#20102;&#29702;&#35770;&#19978;&#20445;&#35777;&#30340;&#20844;&#27491;&#28608;&#21169;&#65288;&#21363;&#21033;&#29992;&#65289;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#29616;&#26377;&#30340;&#20445;&#35777;&#20844;&#24179;&#30340;&#26041;&#27861;&#20013;&#20986;&#29616;&#20102;&#8220;&#23500;&#32773;&#36234;&#23500;&#8221;&#30340;&#29616;&#35937;&#65292;&#36825;&#38459;&#30861;&#20102;&#36164;&#28304;&#36739;&#23569;&#30340;&#33410;&#28857;&#30340;&#21442;&#19982;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21478;&#22806;&#20445;&#25345;&#28176;&#36817;&#24179;&#31561;&#65292;&#21363;&#36739;&#23569;&#36164;&#28304;&#30340;&#33410;&#28857;&#26368;&#32456;&#23454;&#29616;&#19982;&#36739;&#26377;&#36164;&#28304;&#30340;&#8220;&#23500;&#8221;&#33410;&#28857;&#30456;&#31561;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In collaborative learning with streaming data, nodes (e.g., organizations) jointly and continuously learn a machine learning (ML) model by sharing the latest model updates computed from their latest streaming data. For the more resourceful nodes to be willing to share their model updates, they need to be fairly incentivized. This paper explores an incentive design that guarantees fairness so that nodes receive rewards commensurate to their contributions. Our approach leverages an explore-then-exploit formulation to estimate the nodes' contributions (i.e., exploration) for realizing our theoretically guaranteed fair incentives (i.e., exploitation). However, we observe a "rich get richer" phenomenon arising from the existing approaches to guarantee fairness and it discourages the participation of the less resourceful nodes. To remedy this, we additionally preserve asymptotic equality, i.e., less resourceful nodes achieve equal performance eventually to the more resourceful/"rich" nodes. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20998;&#20301;&#25968;&#22238;&#24402;&#30340;&#21453;&#20107;&#23454;&#25512;&#26029;&#26041;&#27861;&#65292;&#26088;&#22312;&#29992;&#20110;&#32570;&#20047;&#22240;&#26524;&#27169;&#22411;&#21644;&#30452;&#25509;&#26465;&#20214;&#20998;&#24067;&#20272;&#35745;&#30340;&#24773;&#20917;&#65292;&#24182;&#33021;&#25552;&#20379;&#20272;&#35745;&#32467;&#26524;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#27867;&#21270;&#35823;&#24046;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2306.05751</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#20301;&#25968;&#22238;&#24402;&#25512;&#36827;&#21453;&#20107;&#23454;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Advancing Counterfactual Inference through Quantile Regression. (arXiv:2306.05751v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20998;&#20301;&#25968;&#22238;&#24402;&#30340;&#21453;&#20107;&#23454;&#25512;&#26029;&#26041;&#27861;&#65292;&#26088;&#22312;&#29992;&#20110;&#32570;&#20047;&#22240;&#26524;&#27169;&#22411;&#21644;&#30452;&#25509;&#26465;&#20214;&#20998;&#24067;&#20272;&#35745;&#30340;&#24773;&#20917;&#65292;&#24182;&#33021;&#25552;&#20379;&#20272;&#35745;&#32467;&#26524;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#27867;&#21270;&#35823;&#24046;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#23545;&#21453;&#20107;&#23454;&#8220;&#20551;&#35774;&#8221;&#38382;&#39064;&#30340;&#33021;&#21147;&#23545;&#20110;&#29702;&#35299;&#21644;&#21033;&#29992;&#22240;&#26524;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#21453;&#20107;&#23454;&#25512;&#26029;&#36890;&#24120;&#20551;&#23450;&#23384;&#22312;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#26679;&#30340;&#22240;&#26524;&#27169;&#22411;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#65292;&#29978;&#33267;&#19981;&#21487;&#36776;&#35782;&#30340;&#12290;&#26412;&#25991;&#26088;&#22312;&#22522;&#20110;&#65288;&#23398;&#20064;&#21040;&#30340;&#65289;&#23450;&#24615;&#22240;&#26524;&#32467;&#26500;&#21644;&#35266;&#27979;&#25968;&#25454;&#65292;&#19981;&#38656;&#35201;&#32473;&#23450;&#22240;&#26524;&#27169;&#22411;&#29978;&#33267;&#19981;&#38656;&#35201;&#30452;&#25509;&#20272;&#35745;&#26465;&#20214;&#20998;&#24067;&#65292;&#23601;&#33021;&#36827;&#34892;&#21487;&#38752;&#30340;&#21453;&#20107;&#23454;&#25512;&#26029;&#12290;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23558;&#21453;&#20107;&#23454;&#25512;&#29702;&#37325;&#26032;&#36716;&#21270;&#20026;&#19968;&#20010;&#25193;&#23637;&#20998;&#20301;&#25968;&#22238;&#24402;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#32479;&#35745;&#19978;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#36827;&#19968;&#27493;&#20351;&#24471;&#20272;&#35745;&#30340;&#21453;&#20107;&#23454;&#32467;&#26524;&#23545;&#26410;&#35265;&#25968;&#25454;&#20855;&#26377;&#19968;&#23450;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#27867;&#21270;&#35823;&#24046;&#30340;&#19978;&#30028;&#12290;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#24378;&#28872;&#25903;&#25345;&#25105;&#20204;&#30340;&#29702;&#35770;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
The capacity to address counterfactual "what if" inquiries is crucial for understanding and making use of causal influences. Traditional counterfactual inference usually assumes a structural causal model is available. However, in practice, such a causal model is often unknown and may not be identifiable. This paper aims to perform reliable counterfactual inference based on the (learned) qualitative causal structure and observational data, without a given causal model or even directly estimating conditional distributions. We re-cast counterfactual reasoning as an extended quantile regression problem using neural networks. The approach is statistically more efficient than existing ones, and further makes it possible to develop the generalization ability of the estimated counterfactual outcome to unseen data and provide an upper bound on the generalization error. Experiment results on multiple datasets strongly support our theoretical claims.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#32422;&#26463;&#32534;&#31243;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#36890;&#36807;&#36890;&#29992;CP&#32534;&#30721;&#12289;&#32422;&#26463;&#26465;&#20214;&#21644;&#21453;&#26144;JSSP&#26368;&#20248;&#26631;&#20934;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#21487;&#20026;&#22823;&#22411;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#20135;&#29983;&#26368;&#20248;&#25110;&#36817;&#20284;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.05747</link><description>&lt;p&gt;
&#22522;&#20110;&#32422;&#26463;&#32534;&#31243;&#30340;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#30340;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An End-to-End Reinforcement Learning Approach for Job-Shop Scheduling Problems Based on Constraint Programming. (arXiv:2306.05747v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#32422;&#26463;&#32534;&#31243;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#36890;&#36807;&#36890;&#29992;CP&#32534;&#30721;&#12289;&#32422;&#26463;&#26465;&#20214;&#21644;&#21453;&#26144;JSSP&#26368;&#20248;&#26631;&#20934;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#21487;&#20026;&#22823;&#22411;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#20135;&#29983;&#26368;&#20248;&#25110;&#36817;&#20284;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32422;&#26463;&#32534;&#31243;(CP)&#26159;&#19968;&#31181;&#22768;&#26126;&#24335;&#32534;&#31243;&#33539;&#20363;&#65292;&#21487;&#29992;&#20110;&#24314;&#27169;&#21644;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#22914;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;(JSSP)&#12290;&#34429;&#28982;CP&#27714;&#35299;&#22120;&#33021;&#22815;&#25214;&#21040;&#23567;&#23454;&#20363;&#30340;&#26368;&#20248;&#25110;&#36817;&#20284;&#26368;&#20248;&#35299;&#65292;&#20294;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#22823;&#23454;&#20363;&#65292;&#21363;&#38656;&#35201;&#38271;&#26102;&#38388;&#35745;&#31639;&#25110;&#20135;&#29983;&#20302;&#36136;&#37327;&#35299;&#12290;&#22240;&#27492;&#65292;&#23454;&#38469;&#35843;&#24230;&#24212;&#29992;&#32463;&#24120;&#37319;&#29992;&#24555;&#36895;&#25163;&#24037;&#21046;&#20316;&#30340;&#22522;&#20110;&#20248;&#20808;&#32423;&#30340;&#35843;&#24230;&#21551;&#21457;&#24335;&#26041;&#27861;&#20197;&#25214;&#21040;&#33391;&#22909;&#30340;&#21021;&#22987;&#35299;&#65292;&#24182;&#20351;&#29992;&#20248;&#21270;&#26041;&#27861;&#23545;&#20854;&#36827;&#34892;&#20248;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#36890;&#36807;CP&#21644;&#24378;&#21270;&#23398;&#20064;(RL)&#26469;&#35299;&#20915;&#35843;&#24230;&#38382;&#39064;&#12290;&#19982;&#20808;&#21069;&#38024;&#23545;&#29305;&#23450;&#38382;&#39064;&#23450;&#21046;&#30340;RL&#26041;&#27861;&#19981;&#21516;&#65292;&#21253;&#25324;&#36807;&#31243;&#27169;&#25311;&#31639;&#27861;&#12289;&#22797;&#26434;&#30340;&#29305;&#24449;&#24037;&#31243;&#25110;&#25163;&#24037;&#21046;&#20316;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#25105;&#20204;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#35757;&#32451;&#31639;&#27861;&#20165;&#38656;&#35201;&#26576;&#20123;&#35843;&#24230;&#38382;&#39064;&#30340;&#36890;&#29992;CP&#32534;&#30721;&#12289;&#19968;&#23567;&#32452;&#38382;&#39064;&#29305;&#23450;&#32422;&#26463;&#26465;&#20214;&#21644;&#21453;&#26144;JSSP&#26368;&#20248;&#26631;&#20934;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#23545;&#20960;&#20010;JSSP&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;RL&#21644;CP&#26041;&#27861;&#31454;&#20105;&#65292;&#24182;&#19988;&#36890;&#24120;&#21487;&#20197;&#22312;&#24456;&#30701;&#26102;&#38388;&#20869;&#20026;&#22823;&#22411;&#23454;&#20363;&#20135;&#29983;&#26368;&#20248;&#25110;&#36817;&#20284;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Constraint Programming (CP) is a declarative programming paradigm that allows for modeling and solving combinatorial optimization problems, such as the Job-Shop Scheduling Problem (JSSP). While CP solvers manage to find optimal or near-optimal solutions for small instances, they do not scale well to large ones, i.e., they require long computation times or yield low-quality solutions. Therefore, real-world scheduling applications often resort to fast, handcrafted, priority-based dispatching heuristics to find a good initial solution and then refine it using optimization methods.  This paper proposes a novel end-to-end approach to solving scheduling problems by means of CP and Reinforcement Learning (RL). In contrast to previous RL methods, tailored for a given problem by including procedural simulation algorithms, complex feature engineering, or handcrafted reward functions, our neural-network architecture and training algorithm merely require a generic CP encoding of some scheduling pr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;3D-DenseUNet&#30340;&#26032;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#33258;&#36866;&#24212;&#20840;&#23616;&#32858;&#21512;&#22359;&#21644;&#33258;&#27880;&#24847;&#27169;&#22359;&#65292;&#20197;&#35299;&#20915;&#21322;&#30417;&#30563;&#25216;&#26415;&#20013;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#38382;&#39064;&#65292;&#21516;&#26102;&#36890;&#36807;&#35757;&#32451;&#20004;&#20010;&#29420;&#31435;&#30340;&#35757;&#32451;&#22120;&#22788;&#29702;&#19981;&#21516;&#30340;&#32452;&#32455;&#29305;&#24615;&#65292;&#22312;&#23156;&#20799;&#33041;&#37096;&#20998;&#26512;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.05745</link><description>&lt;p&gt;
&#20004;&#20010;&#29420;&#31435;&#35757;&#32451;&#22120;&#26159;&#26356;&#22909;&#30340;&#27169;&#33539;
&lt;/p&gt;
&lt;p&gt;
Two Independent Teachers are Better Role Model. (arXiv:2306.05745v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05745
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;3D-DenseUNet&#30340;&#26032;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#33258;&#36866;&#24212;&#20840;&#23616;&#32858;&#21512;&#22359;&#21644;&#33258;&#27880;&#24847;&#27169;&#22359;&#65292;&#20197;&#35299;&#20915;&#21322;&#30417;&#30563;&#25216;&#26415;&#20013;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#38382;&#39064;&#65292;&#21516;&#26102;&#36890;&#36807;&#35757;&#32451;&#20004;&#20010;&#29420;&#31435;&#30340;&#35757;&#32451;&#22120;&#22788;&#29702;&#19981;&#21516;&#30340;&#32452;&#32455;&#29305;&#24615;&#65292;&#22312;&#23156;&#20799;&#33041;&#37096;&#20998;&#26512;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#23156;&#20799;&#33041;&#37096;&#20998;&#26512;&#20013;&#24341;&#36215;&#20102;&#24040;&#22823;&#30340;&#20851;&#27880;&#12290;&#36825;&#20123;&#27169;&#22411;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#21322;&#30417;&#30563;&#25216;&#26415;&#65288;&#20363;&#22914;&#65292;&#26102;&#38388;&#38598;&#25104;&#65292;&#24179;&#22343;&#25945;&#24072;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20381;&#36182;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#65292;&#29992;&#22534;&#21472;&#30340;&#23616;&#37096;&#36816;&#31639;&#31526;&#26469;&#25910;&#38598;&#36828;&#31243;&#20449;&#24687;&#65292;&#32780;&#23616;&#37096;&#36816;&#31639;&#31526;&#38480;&#21046;&#20102;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;MRI&#25968;&#25454;&#21253;&#21547;&#19981;&#21516;&#30340;&#32452;&#32455;&#29305;&#24615;&#65288;TPs&#65289;&#65292;&#20363;&#22914;T1&#21644;T2&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#26159;&#65292;&#23427;&#20204;&#23558;&#20004;&#31181;&#25968;&#25454;&#37117;&#20316;&#20026;&#36755;&#20837;&#29992;&#20110;&#20998;&#21106;&#36807;&#31243;&#65292;&#21363;&#27169;&#22411;&#19968;&#27425;&#35757;&#32451;&#20110;&#25968;&#25454;&#38598;&#65292;&#25512;&#26029;&#36807;&#31243;&#20013;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#31354;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#21517;&#20026;3D-DenseUNet&#30340;&#26032;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35299;&#20915;&#19978;&#36848;&#38480;&#21046;&#65292;&#20854;&#20316;&#20026;&#33258;&#36866;&#24212;&#20840;&#23616;&#32858;&#21512;&#22359;&#22312;&#19979;&#37319;&#26679;&#20013;&#24037;&#20316;&#20197;&#35299;&#20915;&#31354;&#38388;&#20449;&#24687;&#20002;&#22833;&#30340;&#38382;&#39064;&#12290;&#33258;&#27880;&#24847;&#27169;&#22359;&#23558;&#19979;&#37319;&#26679;&#23618;&#19982;&#19978;&#37319;&#26679;&#23618;&#36830;&#25509;&#22312;&#19968;&#36215;&#65292;&#20197;&#25910;&#38598;&#36828;&#31243;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#36890;&#36807;&#35757;&#32451;&#20004;&#20010;&#29420;&#31435;&#30340;&#35757;&#32451;&#22120;&#22788;&#29702;&#19981;&#21516;&#30340;&#32452;&#32455;&#29305;&#24615;&#65292;&#24182;&#21462;&#24471;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent deep learning models have attracted substantial attention in infant brain analysis. These models have performed state-of-the-art performance, such as semi-supervised techniques (e.g., Temporal Ensembling, mean teacher). However, these models depend on an encoder-decoder structure with stacked local operators to gather long-range information, and the local operators limit the efficiency and effectiveness. Besides, the $MRI$ data contain different tissue properties ($TPs$) such as $T1$ and $T2$. One major limitation of these models is that they use both data as inputs to the segment process, i.e., the models are trained on the dataset once, and it requires much computational and memory requirements during inference. In this work, we address the above limitations by designing a new deep-learning model, called 3D-DenseUNet, which works as adaptable global aggregation blocks in down-sampling to solve the issue of spatial information loss. The self-attention module connects the down-s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#36827;&#34892;&#26641;&#24418;&#31995;&#32479;&#25506;&#32034;&#21644;&#25512;&#26029;&#65292;&#29992;&#20110;&#26377;&#26681;&#21644;&#26080;&#26681;&#26641;&#65292;&#20248;&#20110;&#24403;&#21069;&#26368;&#20339;&#26041;&#27861;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#25928;&#26524;&#65292;&#21487;&#29992;&#20110;&#21152;&#36895;&#29983;&#21629;&#31185;&#23398;&#30340;&#26032;&#36827;&#21270;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.05739</link><description>&lt;p&gt;
&#36291;&#36801;&#20110;&#26641;&#31354;&#38388;&#65306;&#36830;&#32493;&#30340;&#26641;&#24418;&#31995;&#32479;&#25512;&#26029;&#26041;&#27861;&#29992;&#20110;&#26377;&#26681;&#21644;&#26080;&#26681;&#26641;
&lt;/p&gt;
&lt;p&gt;
Leaping through tree space: continuous phylogenetic inference for rooted and unrooted trees. (arXiv:2306.05739v1 [q-bio.PE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#36827;&#34892;&#26641;&#24418;&#31995;&#32479;&#25506;&#32034;&#21644;&#25512;&#26029;&#65292;&#29992;&#20110;&#26377;&#26681;&#21644;&#26080;&#26681;&#26641;&#65292;&#20248;&#20110;&#24403;&#21069;&#26368;&#20339;&#26041;&#27861;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#25928;&#26524;&#65292;&#21487;&#29992;&#20110;&#21152;&#36895;&#29983;&#21629;&#31185;&#23398;&#30340;&#26032;&#36827;&#21270;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#36827;&#21270;&#31995;&#32479;&#23398;&#29616;&#22312;&#26159;&#29983;&#21629;&#31185;&#23398;&#20013;&#30340;&#19968;&#20010;&#22522;&#30784;&#65292;&#21487;&#20197;&#38416;&#26126;&#29983;&#21629;&#26089;&#26399;&#25903;&#31995;&#21644;&#20256;&#26579;&#30149;&#30340;&#36215;&#28304;&#21644;&#20256;&#25773;&#12290;&#28982;&#32780;&#65292;&#20174;&#21487;&#33021;&#30340;&#26641;&#30340;&#24191;&#38420;&#31354;&#38388;&#20013;&#25214;&#21040;&#21512;&#36866;&#30340;&#31995;&#32479;&#26641;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#27425;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#36827;&#34892;&#20102;&#26641;&#24418;&#31995;&#32479;&#25506;&#32034;&#21644;&#25512;&#26029;&#65292;&#20351;&#26799;&#24230;&#35745;&#31639;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#31181;&#36830;&#32493;&#30340;&#25918;&#26494;&#26041;&#24335;&#20801;&#35768;&#22312;&#26377;&#26681;&#21644;&#26080;&#26681;&#26641;&#20013;&#36328;&#36234;&#26641;&#31354;&#38388;&#65292;&#19988;&#19981;&#26131;&#25910;&#25947;&#21040;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#24403;&#21069;&#26368;&#20339;&#30340;&#26080;&#26681;&#26641;&#25512;&#26029;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#27169;&#25311;&#20013;&#20934;&#30830;&#22320;&#25512;&#26029;&#20986;&#26641;&#21644;&#26641;&#26681;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#25968;&#25454;&#20013;&#20063;&#24456;&#26377;&#25928;&#65292;&#25105;&#20204;&#22312;&#39052;&#21475;&#21160;&#29289;&#30340;&#31995;&#32479;&#21457;&#32946;&#20013;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#20107;&#23454;&#19978;&#65292;&#20165;&#20855;&#26377;&#36229;&#25351;&#25968;&#20449;&#21495;&#30340;&#23569;&#25968;&#22522;&#22240;&#36890;&#24120;&#36275;&#20197;&#20998;&#36776;&#33034;&#26894;&#21160;&#29289;&#30340;&#20027;&#35201;&#35889;&#31995;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#24076;&#26395;&#21152;&#36895;&#21457;&#29616;&#29983;&#21629;&#31185;&#23398;&#20013;&#30340;&#26032;&#36827;&#21270;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Phylogenetics is now fundamental in life sciences, providing insights into the earliest branches of life and the origins and spread of epidemics. However, finding suitable phylogenies from the vast space of possible trees remains challenging. To address this problem, for the first time, we perform both tree exploration and inference in a continuous space where the computation of gradients is possible. This continuous relaxation allows for major leaps across tree space in both rooted and unrooted trees, and is less susceptible to convergence to local minima. Our approach outperforms the current best methods for inference on unrooted trees and, in simulation, accurately infers the tree and root in ultrametric cases. The approach is effective in cases of empirical data with negligible amounts of data, which we demonstrate on the phylogeny of jawed vertebrates. Indeed, only a few genes with an ultrametric signal were generally sufficient for resolving the major lineages of vertebrate. With
&lt;/p&gt;</description></item><item><title>DP-HyPO &#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#12289;&#38544;&#31169;&#20445;&#25252;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#26694;&#26550;&#65292;&#37319;&#29992;&#26368;&#20808;&#36827;&#30340;&#24046;&#20998;&#38544;&#31169;&#39640;&#26031;&#36807;&#31243;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#37327;&#36523;&#23450;&#21046;&#30340;&#20248;&#21270;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20445;&#35777;&#38544;&#31169;&#21644;&#25928;&#29575;&#65292;&#22312;&#20445;&#25345;&#24378;&#22823;&#38544;&#31169;&#20445;&#35777;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#19982;&#38750;&#31169;&#26377;&#33258;&#36866;&#24212;&#20248;&#21270;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05734</link><description>&lt;p&gt;
DP-HyPO: &#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#31169;&#26377;&#36229;&#21442;&#25968;&#20248;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DP-HyPO: An Adaptive Private Hyperparameter Optimization Framework. (arXiv:2306.05734v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05734
&lt;/p&gt;
&lt;p&gt;
DP-HyPO &#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#12289;&#38544;&#31169;&#20445;&#25252;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#26694;&#26550;&#65292;&#37319;&#29992;&#26368;&#20808;&#36827;&#30340;&#24046;&#20998;&#38544;&#31169;&#39640;&#26031;&#36807;&#31243;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#37327;&#36523;&#23450;&#21046;&#30340;&#20248;&#21270;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20445;&#35777;&#38544;&#31169;&#21644;&#25928;&#29575;&#65292;&#22312;&#20445;&#25345;&#24378;&#22823;&#38544;&#31169;&#20445;&#35777;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#19982;&#38750;&#31169;&#26377;&#33258;&#36866;&#24212;&#20248;&#21270;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#26159;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#24191;&#27867;&#35748;&#21487;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#31169;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#65292;&#35768;&#22810;&#20174;&#19994;&#32773;&#32463;&#24120;&#24573;&#35270;&#19982;&#36229;&#21442;&#25968;&#20248;&#21270;&#30456;&#20851;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#36825;&#21487;&#33021;&#20250;&#26292;&#38706;&#26377;&#20851;&#24213;&#23618;&#25968;&#25454;&#38598;&#30340;&#25935;&#24863;&#20449;&#24687;&#12290;&#30446;&#21069;&#65292;&#21807;&#19968;&#29616;&#26377;&#30340;&#20801;&#35768;&#38544;&#31169;&#20445;&#25252;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#26041;&#27861;&#26159;&#38543;&#26426;&#36873;&#25321;&#19968;&#20123;&#36229;&#21442;&#25968;&#36827;&#34892;&#22810;&#27425;&#36816;&#34892;&#65292;&#24182;&#26368;&#32456;&#25253;&#21578;&#26368;&#20339;&#36229;&#21442;&#25968;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22312;&#38750;&#31169;&#26377;&#35774;&#32622;&#20013;&#65292;&#20174;&#19994;&#32773;&#36890;&#24120;&#20351;&#29992;&#8220;&#33258;&#36866;&#24212;&#8221;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#22914;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#20248;&#21270;&#65292;&#35813;&#20248;&#21270;&#26041;&#27861;&#22522;&#20110;&#20808;&#21069;&#36755;&#20986;&#25910;&#38598;&#30340;&#20449;&#24687;&#36873;&#25321;&#19979;&#19968;&#20010;&#20505;&#36873;&#39033;&#12290;&#36825;&#31181;&#31169;&#26377;&#21644;&#38750;&#31169;&#26377;&#36229;&#21442;&#25968;&#20248;&#21270;&#20043;&#38388;&#30340;&#24040;&#22823;&#24046;&#24322;&#20984;&#26174;&#20986;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DP-HyPO&#65292;&#19968;&#31181;&#25552;&#20379;&#33258;&#36866;&#24212;&#21644;&#38544;&#31169;&#20445;&#25252;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#24320;&#21019;&#24615;&#26694;&#26550;&#12290;DP-HyPO&#37319;&#29992;&#26368;&#20808;&#36827;&#30340;&#24046;&#20998;&#38544;&#31169;&#39640;&#26031;&#36807;&#31243;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#37327;&#36523;&#23450;&#21046;&#30340;&#20248;&#21270;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20445;&#35777;&#38544;&#31169;&#21644;&#25928;&#29575;&#65292;&#24182;&#19988;&#22312;&#20445;&#25345;&#24378;&#22823;&#38544;&#31169;&#20445;&#35777;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#19982;&#38750;&#31169;&#26377;&#33258;&#36866;&#24212;&#20248;&#21270;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter optimization, also known as hyperparameter tuning, is a widely recognized technique for improving model performance. Regrettably, when training private ML models, many practitioners often overlook the privacy risks associated with hyperparameter optimization, which could potentially expose sensitive information about the underlying dataset. Currently, the sole existing approach to allow privacy-preserving hyperparameter optimization is to uniformly and randomly select hyperparameters for a number of runs, subsequently reporting the best-performing hyperparameter. In contrast, in non-private settings, practitioners commonly utilize "adaptive" hyperparameter optimization methods such as Gaussian process-based optimization, which select the next candidate based on information gathered from previous outputs. This substantial contrast between private and non-private hyperparameter optimization underscores a critical concern. In our paper, we introduce DP-HyPO, a pioneering fr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#22686;&#21152;&#37325;&#25918;&#32531;&#23384;&#20013;&#25968;&#25454;&#36807;&#28193;&#30340;&#22810;&#26679;&#24615;&#21487;&#20197;&#25552;&#39640;&#38646;-shot&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#33021;&#36890;&#36807;&#25552;&#39640;&#28508;&#22312;&#34920;&#31034;&#30340;&#27867;&#21270;&#24615;&#33021;&#26469;&#23454;&#29616;&#36825;&#31181;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2306.05727</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#22810;&#26679;&#24615;&#37325;&#25918;&#22312;&#27867;&#21270;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Role of Diverse Replay for Generalisation in Reinforcement Learning. (arXiv:2306.05727v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#22686;&#21152;&#37325;&#25918;&#32531;&#23384;&#20013;&#25968;&#25454;&#36807;&#28193;&#30340;&#22810;&#26679;&#24615;&#21487;&#20197;&#25552;&#39640;&#38646;-shot&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#33021;&#36890;&#36807;&#25552;&#39640;&#28508;&#22312;&#34920;&#31034;&#30340;&#27867;&#21270;&#24615;&#33021;&#26469;&#23454;&#29616;&#36825;&#31181;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#25506;&#32034;&#31574;&#30053;&#21644;&#37325;&#25918;&#32531;&#23384;&#26159;&#20854;&#35768;&#22810;&#31639;&#27861;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#36825;&#20123;&#31574;&#30053;&#35268;&#23450;&#20102;&#35201;&#25910;&#38598;&#21644;&#35757;&#32451;&#21738;&#20123;&#29615;&#22659;&#25968;&#25454;&#65292;&#24182;&#22312;&#24378;&#21270;&#23398;&#20064;&#25991;&#29486;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#36825;&#20123;&#32452;&#20214;&#22312;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#27867;&#21270;&#26102;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#20551;&#35774;&#65306;&#20174;&#35757;&#32451;&#29615;&#22659;&#20013;&#25910;&#38598;&#21644;&#35757;&#32451;&#26356;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#23558;&#25552;&#39640;&#21040;&#26032;&#29615;&#22659;/&#20219;&#21153;&#30340;&#38646;-shot&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#23398;&#25512;&#23548;&#21644;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#65292;&#22686;&#21152;&#37325;&#25918;&#32531;&#23384;&#20013;&#36807;&#28193;&#30340;&#22810;&#26679;&#24615;&#21487;&#20197;&#25552;&#39640;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#8220;&#21487;&#36798;&#21040;&#8221;&#30340;&#29366;&#24577;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#36825;&#31181;&#31574;&#30053;&#23545;&#20110;&#31867;&#20284;&#20294;&#8220;&#19981;&#21487;&#36798;&#8221;&#29366;&#24577;&#30340;&#27867;&#21270;&#24615;&#33021;&#20063;&#26377;&#25152;&#25552;&#39640;&#65292;&#24182;&#19988;&#21487;&#33021;&#26159;&#30001;&#20110;&#28508;&#22312;&#34920;&#31034;&#30340;&#27867;&#21270;&#24615;&#33021;&#24471;&#21040;&#20102;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
In reinforcement learning (RL), key components of many algorithms are the exploration strategy and replay buffer. These strategies regulate what environment data is collected and trained on and have been extensively studied in the RL literature. In this paper, we investigate the impact of these components in the context of generalisation in multi-task RL. We investigate the hypothesis that collecting and training on more diverse data from the training environment will improve zero-shot generalisation to new environments/tasks. We motivate mathematically and show empirically that generalisation to states that are "reachable" during training is improved by increasing the diversity of transitions in the replay buffer. Furthermore, we show empirically that this same strategy also shows improvement for generalisation to similar but "unreachable" states and could be due to improved generalisation of latent representations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26679;&#26412;&#20869;&#31574;&#30053;&#36845;&#20195;&#30340;&#31639;&#27861;&#26469;&#22686;&#24378;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#34892;&#20026;&#35268;&#21017;&#26041;&#27861;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.05726</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#20869;&#25919;&#31574;&#36845;&#20195;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
In-Sample Policy Iteration for Offline Reinforcement Learning. (arXiv:2306.05726v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26679;&#26412;&#20869;&#31574;&#30053;&#36845;&#20195;&#30340;&#31639;&#27861;&#26469;&#22686;&#24378;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#34892;&#20026;&#35268;&#21017;&#26041;&#27861;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#21033;&#29992;&#20197;&#21069;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#26469;&#25512;&#23548;&#20986;&#26377;&#25928;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;&#20026;&#20102;&#35299;&#20915;&#30001;&#20110;&#25968;&#25454;&#35206;&#30422;&#19981;&#36275;&#32780;&#23548;&#33268;&#30340;&#38169;&#35823;&#65292;&#34892;&#20026;&#35268;&#21017;&#26041;&#27861;&#20248;&#21270;&#25511;&#21046;&#31574;&#30053;&#30340;&#21516;&#26102;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20559;&#31163;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#30340;&#35823;&#24046;&#12290;&#28982;&#32780;&#65292;&#24403;&#31163;&#32447;&#25968;&#25454;&#38598;&#30001;&#27425;&#20248;&#31574;&#30053;&#25910;&#38598;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#32463;&#24120;&#34920;&#29616;&#20986;&#19981;&#20339;&#30340;&#23454;&#38469;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26679;&#26412;&#20869;&#31574;&#30053;&#36845;&#20195;&#30340;&#26032;&#31639;&#27861;&#65292;&#23427;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#26174;&#33879;&#22686;&#24378;&#20102;&#34892;&#20026;&#35268;&#21017;&#26041;&#27861;&#12290;&#26680;&#24515;&#35265;&#35299;&#26159;&#36890;&#36807;&#19981;&#26029;&#25913;&#36827;&#29992;&#20110;&#34892;&#20026;&#35268;&#21017;&#30340;&#31574;&#30053;&#65292;&#26679;&#26412;&#20869;&#25919;&#31574;&#36845;&#20195;&#36880;&#28176;&#25913;&#36827;&#33258;&#36523;&#65292;&#21516;&#26102;&#38544;&#24335;&#36991;&#20813;&#26597;&#35810;&#26679;&#26412;&#22806;&#30340;&#34892;&#21160;&#65292;&#20197;&#36991;&#20813;&#28798;&#38590;&#24615;&#30340;&#23398;&#20064;&#22833;&#36133;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#39564;&#35777;&#20102;&#20854;&#23398;&#20064;&#20165;&#21033;&#29992;&#25968;&#25454;&#38598;&#20013;&#33391;&#22909;&#35206;&#30422;&#30340;&#34892;&#21160;&#23398;&#20064;&#26679;&#26412;&#20869;&#26368;&#20248;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#22235;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20013;&#33021;&#22815;&#26174;&#33879;&#25913;&#36827;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) seeks to derive an effective control policy from previously collected data. To circumvent errors due to inadequate data coverage, behavior-regularized methods optimize the control policy while concurrently minimizing deviation from the data collection policy. Nevertheless, these methods often exhibit subpar practical performance, particularly when the offline dataset is collected by sub-optimal policies. In this paper, we propose a novel algorithm employing in-sample policy iteration that substantially enhances behavior-regularized methods in offline RL. The core insight is that by continuously refining the policy used for behavior regularization, in-sample policy iteration gradually improves itself while implicitly avoids querying out-of-sample actions to avert catastrophic learning failures. Our theoretical analysis verifies its ability to learn the in-sample optimal policy, exclusively utilizing actions well-covered by the dataset. Moreover, we pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;Shapley&#20540;&#35299;&#37322;&#19981;&#30830;&#23450;&#24615;&#39044;&#27979;&#65292;&#21487;&#20197;&#37327;&#21270;&#27599;&#20010;&#29305;&#24449;&#23545;&#20010;&#21035;&#27169;&#22411;&#36755;&#20986;&#26465;&#20214;&#29109;&#30340;&#36129;&#29486;&#65292;&#36866;&#29992;&#20110;&#21327;&#21464;&#37327;&#36716;&#31227;&#26816;&#27979;&#12289;&#20027;&#21160;&#23398;&#20064;&#12289;&#29305;&#24449;&#36873;&#25321;&#21644;&#27963;&#21160;&#29305;&#24449;&#20215;&#20540;&#35780;&#20272;&#31561;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2306.05724</link><description>&lt;p&gt;
&#29992;&#20449;&#24687;&#29702;&#35770;&#30340;Shapley&#20540;&#35299;&#37322;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Explaining Predictive Uncertainty with Information Theoretic Shapley Values. (arXiv:2306.05724v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;Shapley&#20540;&#35299;&#37322;&#19981;&#30830;&#23450;&#24615;&#39044;&#27979;&#65292;&#21487;&#20197;&#37327;&#21270;&#27599;&#20010;&#29305;&#24449;&#23545;&#20010;&#21035;&#27169;&#22411;&#36755;&#20986;&#26465;&#20214;&#29109;&#30340;&#36129;&#29486;&#65292;&#36866;&#29992;&#20110;&#21327;&#21464;&#37327;&#36716;&#31227;&#26816;&#27979;&#12289;&#20027;&#21160;&#23398;&#20064;&#12289;&#29305;&#24449;&#36873;&#25321;&#21644;&#27963;&#21160;&#29305;&#24449;&#20215;&#20540;&#35780;&#20272;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#22823;&#37327;&#26041;&#27861;&#26469;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#22797;&#26434;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#35299;&#37322;&#27169;&#22411;&#36755;&#20986;&#30340;$\textit{&#19981;&#30830;&#23450;&#24615;}$&#21364;&#21463;&#21040;&#20102;&#30456;&#23545;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#23558;&#24191;&#27867;&#20351;&#29992;&#30340;Shapley&#20540;&#26694;&#26550;&#29992;&#20110;&#35299;&#37322;&#21508;&#31181;&#31867;&#22411;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#37327;&#21270;&#27599;&#20010;&#29305;&#24449;&#23545;&#20010;&#21035;&#27169;&#22411;&#36755;&#20986;&#26465;&#20214;&#29109;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20462;&#25913;&#29305;&#24449;&#20989;&#25968;&#30340;&#21338;&#24328;&#65292;&#24182;&#21457;&#29616;&#20102;&#30001;&#27492;&#20135;&#29983;&#30340;Shapley&#20540;&#19982;&#20449;&#24687;&#35770;&#21644;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#20013;&#30340;&#22522;&#26412;&#37327;&#20043;&#38388;&#30340;&#28145;&#21051;&#32852;&#31995;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#26377;&#35777;&#26126;&#20445;&#35777;&#30340;&#26377;&#38480;&#26679;&#26412;&#35823;&#24046;&#29575;&#25511;&#21046;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#22312;&#30495;&#23454;&#21644;&#27169;&#25311;&#25968;&#25454;&#30340;&#19968;&#31995;&#21015;&#23454;&#39564;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#21327;&#21464;&#37327;&#36716;&#31227;&#26816;&#27979;&#12289;&#20027;&#21160;&#23398;&#20064;&#12289;&#29305;&#24449;&#36873;&#25321;&#21644;&#27963;&#21160;&#29305;&#24449;&#20215;&#20540;&#35780;&#20272;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers in explainable artificial intelligence have developed numerous methods for helping users understand the predictions of complex supervised learning models. By contrast, explaining the $\textit{uncertainty}$ of model outputs has received relatively little attention. We adapt the popular Shapley value framework to explain various types of predictive uncertainty, quantifying each feature's contribution to the conditional entropy of individual model outputs. We consider games with modified characteristic functions and find deep connections between the resulting Shapley values and fundamental quantities from information theory and conditional independence testing. We outline inference procedures for finite sample error rate control with provable guarantees, and implement an efficient algorithm that performs well in a range of experiments on real and simulated data. Our method has applications to covariate shift detection, active learning, feature selection, and active feature-val
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#21033;&#29992;&#23494;&#24230;&#20989;&#25968;&#30340;&#38750;&#32447;&#24615;&#21464;&#25442;&#23545;&#23725;&#30340;&#20272;&#35745;&#65292;&#24182;&#21457;&#29616;&#20854;&#21487;&#20197;&#25913;&#36827;&#23545;&#20999;&#31354;&#38388;&#30340;&#20272;&#35745;&#20197;&#21450;&#22312;&#36817;&#20284;&#24213;&#23618;&#30495;&#23454;&#27969;&#24418;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#27969;&#24418;&#25311;&#21512;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.05722</link><description>&lt;p&gt;
&#21033;&#29992;&#23494;&#24230;&#20989;&#25968;&#30340;&#38750;&#32447;&#24615;&#21464;&#25442;&#20272;&#35745;&#23725;
&lt;/p&gt;
&lt;p&gt;
Estimation of Ridge Using Nonlinear Transformation on Density Function. (arXiv:2306.05722v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#21033;&#29992;&#23494;&#24230;&#20989;&#25968;&#30340;&#38750;&#32447;&#24615;&#21464;&#25442;&#23545;&#23725;&#30340;&#20272;&#35745;&#65292;&#24182;&#21457;&#29616;&#20854;&#21487;&#20197;&#25913;&#36827;&#23545;&#20999;&#31354;&#38388;&#30340;&#20272;&#35745;&#20197;&#21450;&#22312;&#36817;&#20284;&#24213;&#23618;&#30495;&#23454;&#27969;&#24418;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#27969;&#24418;&#25311;&#21512;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23725;&#22312;&#20934;&#30830;&#36817;&#20284;&#27969;&#24418;&#30340;&#22522;&#30784;&#32467;&#26500;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#20985;&#38750;&#32447;&#24615;&#21464;&#25442;&#24212;&#29992;&#20110;&#23494;&#24230;&#20989;&#25968;&#20197;&#25506;&#32034;&#23725;&#30340;&#21464;&#21270;&#12290;&#36890;&#36807;&#23545;Hessian&#30697;&#38453;&#30340;&#25512;&#23548;&#21644;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#38750;&#32447;&#24615;&#21464;&#25442;&#20135;&#29983;&#20102;Hessian&#30697;&#38453;&#30340;&#31209;&#19968;&#20462;&#25913;&#12290;&#21033;&#29992;&#29305;&#24449;&#20540;&#38382;&#39064;&#30340;&#21464;&#20998;&#24615;&#36136;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#30456;&#24212;&#23725;&#20043;&#38388;&#30340;&#20559;&#24207;&#21253;&#21547;&#20851;&#31995;&#12290;&#25105;&#20204;&#30452;&#35266;&#22320;&#21457;&#29616;&#65292;&#36890;&#36807;Hessian&#30697;&#38453;&#30340;&#31209;&#19968;&#20462;&#25913;&#65292;&#21464;&#25442;&#21487;&#20197;&#23548;&#33268;&#23545;&#20999;&#31354;&#38388;&#30340;&#20272;&#35745;&#25913;&#36827;&#12290;&#20026;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#25968;&#20540;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#19982;&#20854;&#20182;&#27969;&#24418;&#25311;&#21512;&#31639;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#21464;&#25442;&#26041;&#27861;&#24471;&#21040;&#30340;&#23725;&#22312;&#36817;&#20284;&#24213;&#23618;&#30495;&#23454;&#27969;&#24418;&#26041;&#38754;&#26356;&#21152;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ridges play a vital role in accurately approximating the underlying structure of manifolds. In this paper, we explore the ridge's variation by applying a concave nonlinear transformation to the density function. Through the derivation of the Hessian matrix, we observe that nonlinear transformations yield a rank-one modification of the Hessian matrix. Leveraging the variational properties of eigenvalue problems, we establish a partial order inclusion relationship among the corresponding ridges. We intuitively discover that the transformation can lead to improved estimation of the tangent space via rank-one modification of the Hessian matrix. To validate our theories, we conduct extensive numerical experiments on synthetic and real-world datasets that demonstrate the superiority of the ridges obtained from our transformed approach in approximating the underlying truth manifold compared to other manifold fitting algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#20869;&#37096;&#26159;&#21542;&#21019;&#24314;&#21644;&#20351;&#29992;&#31616;&#21333;&#22330;&#26223;&#20960;&#20309;&#20869;&#37096;&#34920;&#31034;&#65292;&#20351;&#29992;&#32447;&#24615;&#25506;&#38024;&#21457;&#29616;LDM&#20869;&#37096;&#28608;&#27963;&#25552;&#20379;&#20102;&#20851;&#20110;3D&#28145;&#24230;&#25968;&#25454;&#21644;&#31361;&#20986;&#23545;&#35937;/&#32972;&#26223;&#20998;&#31163;&#30340;&#32447;&#24615;&#34920;&#31034;&#65292;&#22312;&#22270;&#20687;&#21512;&#25104;&#20013;&#20855;&#26377;&#22240;&#26524;&#20316;&#29992;, &#21487;&#29992;&#20110;LDM&#36755;&#20986;&#30340;&#31616;&#21333;&#39640;&#32423;&#32534;&#36753;&#12290;</title><link>http://arxiv.org/abs/2306.05720</link><description>&lt;p&gt;
&#36229;&#36234;&#34920;&#38754;&#32479;&#35745;&#23398;&#65306;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#22330;&#26223;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Beyond Surface Statistics: Scene Representations in a Latent Diffusion Model. (arXiv:2306.05720v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#20869;&#37096;&#26159;&#21542;&#21019;&#24314;&#21644;&#20351;&#29992;&#31616;&#21333;&#22330;&#26223;&#20960;&#20309;&#20869;&#37096;&#34920;&#31034;&#65292;&#20351;&#29992;&#32447;&#24615;&#25506;&#38024;&#21457;&#29616;LDM&#20869;&#37096;&#28608;&#27963;&#25552;&#20379;&#20102;&#20851;&#20110;3D&#28145;&#24230;&#25968;&#25454;&#21644;&#31361;&#20986;&#23545;&#35937;/&#32972;&#26223;&#20998;&#31163;&#30340;&#32447;&#24615;&#34920;&#31034;&#65292;&#22312;&#22270;&#20687;&#21512;&#25104;&#20013;&#20855;&#26377;&#22240;&#26524;&#20316;&#29992;, &#21487;&#29992;&#20110;LDM&#36755;&#20986;&#30340;&#31616;&#21333;&#39640;&#32423;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65288;LDM&#65289;&#23637;&#29616;&#20102;&#20135;&#29983;&#36924;&#30495;&#22270;&#20687;&#30340;&#24778;&#20154;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#20869;&#22312;&#26426;&#21046;&#20173;&#28982;&#31070;&#31192;&#12290;&#21363;&#20351;&#22312;&#27809;&#26377;&#26174;&#24335;&#28145;&#24230;&#20449;&#24687;&#30340;&#22270;&#20687;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#23427;&#20204;&#36890;&#24120;&#20063;&#20250;&#36755;&#20986;&#19968;&#33268;&#30340;3D&#22330;&#26223;&#22270;&#29255;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#20010;&#22522;&#26412;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#65306;LDM&#26159;&#21542;&#21019;&#24314;&#24182;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#22330;&#26223;&#20960;&#20309;&#20869;&#37096;&#34920;&#31034;&#65311;&#20351;&#29992;&#32447;&#24615;&#25506;&#38024;&#65292;&#25105;&#20204;&#21457;&#29616;LDM&#30340;&#20869;&#37096;&#28608;&#27963;&#32534;&#30721;&#20102;&#32447;&#24615;&#34920;&#31034;&#65292;&#26082;&#21253;&#25324;3D&#28145;&#24230;&#25968;&#25454;&#65292;&#21448;&#21253;&#25324;&#31361;&#20986;&#23545;&#35937;/&#32972;&#26223;&#21306;&#21035;&#12290;&#36825;&#20123;&#34920;&#31034;&#20284;&#20046;&#22312;&#21435;&#22122;&#36807;&#31243;&#30340;&#26089;&#26399;&#23601;&#20986;&#29616;&#20102;&#8212;&#8212;&#22312;&#20154;&#31867;&#33021;&#36731;&#26131;&#29702;&#35299;&#22024;&#26434;&#30340;&#22270;&#20687;&#20043;&#21069;&#12290;&#24178;&#39044;&#24615;&#23454;&#39564;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#36825;&#20123;&#34920;&#31034;&#22312;&#22270;&#20687;&#21512;&#25104;&#20013;&#25198;&#28436;&#22240;&#26524;&#20316;&#29992;&#65292;&#24182;&#19988;&#21487;&#33021;&#29992;&#20110;LDM&#36755;&#20986;&#30340;&#31616;&#21333;&#39640;&#32423;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latent diffusion models (LDMs) exhibit an impressive ability to produce realistic images, yet the inner workings of these models remain mysterious. Even when trained purely on images without explicit depth information, they typically output coherent pictures of 3D scenes. In this work, we investigate a basic interpretability question: does an LDM create and use an internal representation of simple scene geometry? Using linear probes, we find evidence that the internal activations of the LDM encode linear representations of both 3D depth data and a salient-object / background distinction. These representations appear surprisingly early in the denoising process$-$well before a human can easily make sense of the noisy images. Intervention experiments further indicate these representations play a causal role in image synthesis, and may be used for simple high-level editing of an LDM's output.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32447;&#24615;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#38899;&#21512;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#23454;&#29616;&#24555;&#36895;&#25512;&#29702;&#21644;&#39640;&#26679;&#26412;&#36136;&#37327;</title><link>http://arxiv.org/abs/2306.05708</link><description>&lt;p&gt;
&#32447;&#24615;&#25193;&#25955;&#25552;&#21319;&#20102;&#24555;&#36895;&#39640;&#36136;&#37327;&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Boosting Fast and High-Quality Speech Synthesis with Linear Diffusion. (arXiv:2306.05708v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32447;&#24615;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#38899;&#21512;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#23454;&#29616;&#24555;&#36895;&#25512;&#29702;&#21644;&#39640;&#26679;&#26412;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#21508;&#31181;&#29983;&#25104;&#20219;&#21153;&#65292;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#24050;&#32463;&#23637;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24930;&#25512;&#29702;&#36895;&#24230;&#20351;&#24471;&#23427;&#20204;&#22312;&#35821;&#38899;&#21512;&#25104;&#20013;&#19981;&#23454;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#30340;&#32447;&#24615;&#25193;&#25955;&#27169;&#22411;&#65288;LinDiff&#65289;&#65292;&#20197;&#21516;&#26102;&#23454;&#29616;&#24555;&#36895;&#25512;&#29702;&#21644;&#39640;&#26679;&#26412;&#36136;&#37327;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#23454;&#29616;&#23545;&#22122;&#22768;&#35821;&#38899;&#30340;&#26377;&#25928;&#20840;&#23616;&#24314;&#27169;&#65292;LinDiff&#37319;&#29992;&#20102;&#22522;&#20110;&#20998;&#21306;&#30340;&#22788;&#29702;&#26041;&#27861;&#65292;&#23558;&#36755;&#20837;&#20449;&#21495;&#21010;&#20998;&#20026;&#23567;&#34917;&#19969;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising Diffusion Probabilistic Models have shown extraordinary ability on various generative tasks. However, their slow inference speed renders them impractical in speech synthesis. This paper proposes a linear diffusion model (LinDiff) based on an ordinary differential equation to simultaneously reach fast inference and high sample quality. Firstly, we employ linear interpolation between the target and noise to design a diffusion sequence for training, while previously the diffusion path that links the noise and target is a curved segment. When decreasing the number of sampling steps (i.e., the number of line segments used to fit the path), the ease of fitting straight lines compared to curves allows us to generate higher quality samples from a random noise with fewer iterations. Secondly, to reduce computational complexity and achieve effective global modeling of noisy speech, LinDiff employs a patch-based processing approach that partitions the input signal into small patches. Th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861; FedInit&#65292;&#36890;&#36807;&#38454;&#27573;&#24615;&#25918;&#26494;&#21021;&#22987;&#21270;&#26469;&#32531;&#35299;&#8220;&#23458;&#25143;&#26426;&#28418;&#31227;&#8221;&#38382;&#39064;&#65292;&#20174;&#32780;&#22686;&#24378;&#26412;&#22320;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#35299;&#37322;&#26412;&#22320;&#19981;&#19968;&#33268;&#24615;&#30340;&#34892;&#20026;&#65292;&#24182;&#24378;&#35843;&#20102;&#21021;&#22987;&#21270;&#36807;&#31243;&#22312;&#20248;&#21270;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.05706</link><description>&lt;p&gt;
&#36890;&#36807;&#38454;&#27573;&#24615;&#25918;&#26494;&#21021;&#22987;&#21270;&#29702;&#35299;&#32852;&#37030;&#23398;&#20064;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Understanding How Consistency Works in Federated Learning via Stage-wise Relaxed Initialization. (arXiv:2306.05706v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861; FedInit&#65292;&#36890;&#36807;&#38454;&#27573;&#24615;&#25918;&#26494;&#21021;&#22987;&#21270;&#26469;&#32531;&#35299;&#8220;&#23458;&#25143;&#26426;&#28418;&#31227;&#8221;&#38382;&#39064;&#65292;&#20174;&#32780;&#22686;&#24378;&#26412;&#22320;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#35299;&#37322;&#26412;&#22320;&#19981;&#19968;&#33268;&#24615;&#30340;&#34892;&#20026;&#65292;&#24182;&#24378;&#35843;&#20102;&#21021;&#22987;&#21270;&#36807;&#31243;&#22312;&#20248;&#21270;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#33539;&#24335;&#65292;&#36890;&#36807;&#38454;&#27573;&#24615;&#30340;&#26412;&#22320;&#35757;&#32451;&#36807;&#31243;&#21327;&#35843;&#22823;&#37327;&#26412;&#22320;&#23458;&#25143;&#31471;&#65292;&#23545;&#24322;&#26500;&#25968;&#25454;&#38598;&#36827;&#34892;&#20840;&#23616;&#27169;&#22411;&#21327;&#21516;&#35757;&#32451;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#26263;&#31034;&#20102;&#32852;&#37030;&#23398;&#20064;&#38754;&#20020;&#8220;&#23458;&#25143;&#26426;&#28418;&#31227;&#8221;&#38382;&#39064;&#30340;&#22256;&#25200;&#65292;&#36825;&#26159;&#30001;&#20110;&#26412;&#22320;&#23458;&#25143;&#31471;&#30340;&#19981;&#19968;&#33268;&#24615;&#26368;&#20248;&#35299;&#25152;&#23548;&#33268;&#30340;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#20173;&#32570;&#20047;&#23454;&#36136;&#24615;&#30340;&#29702;&#35770;&#20998;&#26512;&#26469;&#35299;&#37322;&#36825;&#31181;&#26412;&#22320;&#19981;&#19968;&#33268;&#24615;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#32531;&#35299;&#8220;&#23458;&#25143;&#26426;&#28418;&#31227;&#8221;&#30340;&#36127;&#38754;&#24433;&#21709;&#24182;&#25506;&#32034;&#20854;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23454;&#36136;&#65292;&#26412;&#25991;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861; FedInit&#65292;&#35813;&#31639;&#27861;&#20801;&#35768;&#22312;&#27599;&#20010;&#26412;&#22320;&#35757;&#32451;&#38454;&#27573;&#24320;&#22987;&#26102;&#20351;&#29992;&#20010;&#24615;&#21270;&#30340;&#25918;&#26494;&#21021;&#22987;&#21270;&#29366;&#24577;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FedInit&#36890;&#36807;&#26397;&#30528;&#26368;&#26032;&#26412;&#22320;&#29366;&#24577;&#30340;&#21453;&#21521;&#31227;&#21160;&#65292;&#23558;&#26412;&#22320;&#29366;&#24577;&#20174;&#24403;&#21069;&#20840;&#23616;&#29366;&#24577;&#20013;&#31227;&#24320;&#20197;&#36827;&#34892;&#25918;&#26494;&#30340;&#21021;&#22987;&#21270;&#12290;&#36825;&#31181;&#25918;&#26494;&#21021;&#22987;&#21270;&#26377;&#21161;&#20110;&#20462;&#27491;&#26412;&#22320;&#20998;&#27495;&#24182;&#22686;&#24378;&#26412;&#22320;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#20197;&#29702;&#35299;&#32852;&#37030;&#23398;&#20064;&#20013;&#26412;&#22320;&#19981;&#19968;&#33268;&#24615;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#38454;&#27573;&#24615;&#25918;&#26494;&#21021;&#22987;&#21270;&#21487;&#20197;&#32531;&#35299;&#26412;&#22320;&#19981;&#19968;&#33268;&#24615;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#24378;&#35843;&#21021;&#22987;&#21270;&#36807;&#31243;&#22312;&#20248;&#21270;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a distributed paradigm that coordinates massive local clients to collaboratively train a global model via stage-wise local training processes on the heterogeneous dataset. Previous works have implicitly studied that FL suffers from the ``client-drift'' problem, which is caused by the inconsistent optimum across local clients. However, till now it still lacks solid theoretical analysis to explain the impact of this local inconsistency. To alleviate the negative impact of the ``client drift'' and explore its substance in FL, in this paper, we first design an efficient FL algorithm \textit{FedInit}, which allows employing the personalized relaxed initialization state at the beginning of each local training stage. Specifically, \textit{FedInit} initializes the local state by moving away from the current global state towards the reverse direction of the latest local state. This relaxed initialization helps to revise the local divergence and enhance the local consi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Q-learning&#31639;&#27861;&#24212;&#29992;&#20110;&#20004;&#20154;&#38646;&#21644;Markov&#21338;&#24328;&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#20999;&#25442;&#31995;&#32479;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#31616;&#21333;&#21644;&#28145;&#20837;&#30340;&#25910;&#25947;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2306.05700</link><description>&lt;p&gt;
&#20004;&#20154;&#38646;&#21644;Markov&#21338;&#24328;&#30340;&#26497;&#23567;&#26497;&#22823;Q-learning&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#65306;&#20999;&#25442;&#31995;&#32479;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Finite-Time Analysis of Minimax Q-Learning for Two-Player Zero-Sum Markov Games: Switching System Approach. (arXiv:2306.05700v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Q-learning&#31639;&#27861;&#24212;&#29992;&#20110;&#20004;&#20154;&#38646;&#21644;Markov&#21338;&#24328;&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#20999;&#25442;&#31995;&#32479;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#31616;&#21333;&#21644;&#28145;&#20837;&#30340;&#25910;&#25947;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;Q-learning&#31639;&#27861;&#24212;&#29992;&#20110;&#20004;&#20154;&#38646;&#21644;Markov&#21338;&#24328;&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#12290;&#25105;&#20204;&#38024;&#23545;&#26497;&#23567;&#26497;&#22823;Q-learning&#31639;&#27861;&#20197;&#21450;&#30456;&#24212;&#30340;&#20215;&#20540;&#36845;&#20195;&#26041;&#27861;&#36827;&#34892;&#20102;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#12290;&#20026;&#20102;&#22686;&#24378;&#23545;&#20215;&#20540;&#36845;&#20195;&#21644;Q-learning&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26497;&#23567;&#26497;&#22823;Q-learning&#30340;&#20999;&#25442;&#31995;&#32479;&#27169;&#22411;&#21644;&#30456;&#24212;&#30340;&#20215;&#20540;&#36845;&#20195;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#23545;&#26497;&#23567;&#26497;&#22823;Q-learning&#30340;&#36827;&#19968;&#27493;&#27934;&#23519;&#65292;&#24182;&#26377;&#21161;&#20110;&#26356;&#31616;&#21333;&#21644;&#28145;&#20837;&#30340;&#25910;&#25947;&#20998;&#26512;&#12290;&#25105;&#20204;&#39044;&#35745;&#36825;&#20123;&#39069;&#22806;&#30340;&#27934;&#23519;&#21147;&#26377;&#28508;&#21147;&#25581;&#31034;&#25511;&#21046;&#29702;&#35770;&#21644;&#24378;&#21270;&#23398;&#20064;&#31038;&#21306;&#27010;&#24565;&#20043;&#38388;&#30340;&#26032;&#32852;&#31995;&#65292;&#24182;&#20419;&#36827;&#23427;&#20204;&#20043;&#38388;&#30340;&#21512;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of this paper is to investigate the finite-time analysis of a Q-learning algorithm applied to two-player zero-sum Markov games. Specifically, we establish a finite-time analysis of both the minimax Q-learning algorithm and the corresponding value iteration method. To enhance the analysis of both value iteration and Q-learning, we employ the switching system model of minimax Q-learning and the associated value iteration. This approach provides further insights into minimax Q-learning and facilitates a more straightforward and insightful convergence analysis. We anticipate that the introduction of these additional insights has the potential to uncover novel connections and foster collaboration between concepts in the fields of control theory and reinforcement learning communities.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;JABBERWOCK&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#36890;&#36807;JavaScript&#25910;&#38598;&#24182;&#29983;&#25104;WebAssembly&#25968;&#25454;&#38598;&#65292;&#20197;&#29992;&#20110;&#24694;&#24847;&#32593;&#31449;&#26816;&#27979;&#65292;&#24182;&#35777;&#26126;&#20854;&#33021;&#22815;&#22312;4.5&#31186;&#20869;&#29983;&#25104;&#19968;&#20010;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.05698</link><description>&lt;p&gt;
JABBERWOCK&#65306;WebAssembly&#25968;&#25454;&#38598;&#29983;&#25104;&#24037;&#20855;&#21450;&#20854;&#22312;&#24694;&#24847;&#32593;&#31449;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
JABBERWOCK: A Tool for WebAssembly Dataset Generation and Its Application to Malicious Website Detection. (arXiv:2306.05698v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;JABBERWOCK&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#36890;&#36807;JavaScript&#25910;&#38598;&#24182;&#29983;&#25104;WebAssembly&#25968;&#25454;&#38598;&#65292;&#20197;&#29992;&#20110;&#24694;&#24847;&#32593;&#31449;&#26816;&#27979;&#65292;&#24182;&#35777;&#26126;&#20854;&#33021;&#22815;&#22312;4.5&#31186;&#20869;&#29983;&#25104;&#19968;&#20010;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24120;&#34987;&#29992;&#20110;&#24694;&#24847;&#32593;&#31449;&#30340;&#26816;&#27979;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36824;&#27809;&#26377;&#25506;&#32034;&#21033;&#29992;WebAssembly&#20316;&#20026;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#21407;&#22240;&#26159;&#26679;&#26412;&#25968;&#37327;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;JABBERWOCK&#65288;&#22522;&#20110;JavaScript&#21644;WebAssembly&#20248;&#21270;&#25171;&#21253;&#30340;&#20108;&#36827;&#21046;&#32534;&#30721;&#24037;&#20855;&#65289;&#65292;&#29992;&#20110;&#36890;&#36807;JavaScript&#20266;&#36896;&#29983;&#25104;WebAssembly&#25968;&#25454;&#38598;&#12290;JABBERWOCK&#20027;&#35201;&#33258;&#21160;&#25910;&#38598;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;JavaScript&#20195;&#30721;&#65292;&#23558;&#20854;&#36716;&#25442;&#25104;WebAssembly&#65292;&#28982;&#21518;&#23558;&#36755;&#20986;WebAssembly&#30340;&#21521;&#37327;&#20316;&#20026;&#24694;&#24847;&#32593;&#31449;&#26816;&#27979;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#36824;&#23545;JABBERWOCK&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#29983;&#25104;&#30340;&#22788;&#29702;&#26102;&#38388;&#65292;&#29983;&#25104;&#26679;&#26412;&#19982;&#20174;&#20114;&#32852;&#32593;&#25910;&#38598;&#30340;&#23454;&#38469;WebAssembly&#26679;&#26412;&#30340;&#27604;&#36739;&#65292;&#20197;&#21450;&#22312;&#24694;&#24847;&#32593;&#31449;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#20851;&#20110;&#22788;&#29702;&#26102;&#38388;&#65292;&#25105;&#20204;&#26174;&#31034;JABBERWOCK&#21487;&#20197;&#22312;&#20219;&#24847;&#26679;&#26412;&#25968;&#37327;&#19979;&#27599;4.5&#31186;&#26500;&#24314;&#19968;&#20010;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning is often used for malicious website detection, but an approach incorporating WebAssembly as a feature has not been explored due to a limited number of samples, to the best of our knowledge. In this paper, we propose JABBERWOCK (JAvascript-Based Binary EncodeR by WebAssembly Optimization paCKer), a tool to generate WebAssembly datasets in a pseudo fashion via JavaScript. Loosely speaking, JABBERWOCK automatically gathers JavaScript code in the real world, convert them into WebAssembly, and then outputs vectors of the WebAssembly as samples for malicious website detection. We also conduct experimental evaluations of JABBERWOCK in terms of the processing time for dataset generation, comparison of the generated samples with actual WebAssembly samples gathered from the Internet, and an application for malicious website detection. Regarding the processing time, we show that JABBERWOCK can construct a dataset in 4.5 seconds per sample for any number of samples. Next, comparin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;&#32676;&#21367;&#31215;&#21040;&#39057;&#29575;&#22495;&#65292;&#24182;&#35774;&#35745;&#20102;&#20855;&#26377;&#26059;&#36716;&#12289;&#24179;&#31227;&#21644;&#38236;&#20687;&#31561;&#21464;&#24615;&#36136;&#30340;&#20613;&#37324;&#21494;&#23618;&#30340;G-FNO&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#22312;&#19981;&#21516;&#23545;&#31216;&#24615;&#27700;&#24179;&#30340;&#35774;&#32622;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.05697</link><description>&lt;p&gt;
&#22522;&#20110;&#32676;&#31561;&#21464;&#24615;&#30340;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Group Equivariant Fourier Neural Operators for Partial Differential Equations. (arXiv:2306.05697v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05697
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;&#32676;&#21367;&#31215;&#21040;&#39057;&#29575;&#22495;&#65292;&#24182;&#35774;&#35745;&#20102;&#20855;&#26377;&#26059;&#36716;&#12289;&#24179;&#31227;&#21644;&#38236;&#20687;&#31561;&#21464;&#24615;&#36136;&#30340;&#20613;&#37324;&#21494;&#23618;&#30340;G-FNO&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#22312;&#19981;&#21516;&#23545;&#31216;&#24615;&#27700;&#24179;&#30340;&#35774;&#32622;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#20351;&#29992;&#22312;&#39057;&#29575;&#22495;&#19979;&#25805;&#20316;&#30340;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;FNOs&#65289;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#12290;&#30001;&#20110;&#29289;&#29702;&#23450;&#24459;&#19981;&#20381;&#36182;&#20110;&#29992;&#20110;&#25551;&#36848;&#23427;&#20204;&#30340;&#22352;&#26631;&#31995;, &#22240;&#27492;&#23558;&#36825;&#20123;&#23545;&#31216;&#24615;&#32534;&#30721;&#36827;&#31070;&#32463;&#31639;&#23376;&#26550;&#26500;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#26356;&#23481;&#26131;&#30340;&#23398;&#20064;&#26159;&#26377;&#30410;&#30340;&#12290;&#23613;&#31649;&#20351;&#29992;&#32676;&#35770;&#32534;&#30721;&#29289;&#29702;&#22495;&#20869;&#30340;&#23545;&#31216;&#24615;&#24050;&#32463;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22914;&#20309;&#22312;&#39057;&#29575;&#22495;&#20869;&#25429;&#25417;&#23545;&#31216;&#24615;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#25991;&#23558;&#32676;&#21367;&#31215;&#25193;&#23637;&#21040;&#39057;&#29575;&#22495;&#65292;&#24182;&#35774;&#35745;&#20855;&#26377;&#26059;&#36716;&#12289;&#24179;&#31227;&#21644;&#38236;&#20687;&#31561;&#21464;&#24615;&#36136;&#30340;&#20613;&#37324;&#21494;&#23618;&#65292;&#21033;&#29992;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#31561;&#21464;&#24615;&#36136;&#12290;&#20135;&#29983;&#30340;G-FNO&#26550;&#26500;&#36328;&#36755;&#20837;&#20998;&#36776;&#29575;&#20855;&#26377;&#33391;&#22909;&#30340;&#25512;&#24191;&#25928;&#26524;&#65292;&#24182;&#22312;&#20855;&#26377;&#19981;&#21516;&#23545;&#31216;&#24615;&#27700;&#24179;&#30340;&#35774;&#32622;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20316;&#20026;AIRS&#24211;&#30340;&#19968;&#37096;&#20998;&#20844;&#24320;&#22312;Github&#65288;https://github.com/divelab/AIRS&#65289;&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider solving partial differential equations (PDEs) with Fourier neural operators (FNOs), which operate in the frequency domain. Since the laws of physics do not depend on the coordinate system used to describe them, it is desirable to encode such symmetries in the neural operator architecture for better performance and easier learning. While encoding symmetries in the physical domain using group theory has been studied extensively, how to capture symmetries in the frequency domain is under-explored. In this work, we extend group convolutions to the frequency domain and design Fourier layers that are equivariant to rotations, translations, and reflections by leveraging the equivariance property of the Fourier transform. The resulting $G$-FNO architecture generalizes well across input resolutions and performs well in settings with varying levels of symmetry. Our code is publicly available as part of the AIRS library (https://github.com/divelab/AIRS).
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#23567;&#37327;&#23376;&#24577;&#19978;&#30340;&#21487;&#35299;&#37322;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#19982;&#28508;&#22312;&#32467;&#26500;&#30456;&#32852;&#31995;&#30340;&#21487;&#29702;&#35299;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.05694</link><description>&lt;p&gt;
&#23567;&#37327;&#23376;&#24577;&#30340;&#21487;&#35299;&#37322;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Explainable Representation Learning of Small Quantum States. (arXiv:2306.05694v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05694
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#23567;&#37327;&#23376;&#24577;&#19978;&#30340;&#21487;&#35299;&#37322;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#19982;&#28508;&#22312;&#32467;&#26500;&#30456;&#32852;&#31995;&#30340;&#21487;&#29702;&#35299;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#22312;&#27809;&#26377;&#26126;&#30830;&#20154;&#31867;&#25351;&#23548;&#25110;&#29305;&#24449;&#24037;&#31243;&#30340;&#24773;&#20917;&#19979;&#24314;&#31435;&#36215;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#20869;&#37096;&#34920;&#31034;&#12290;&#36890;&#36807;&#36825;&#31181;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#21487;&#20197;&#36827;&#19968;&#27493;&#20102;&#35299;&#21040;&#20851;&#20110;&#22788;&#29702;&#20219;&#21153;&#25152;&#38656;&#30340;&#25968;&#25454;&#29305;&#24449;&#20449;&#24687;&#12290;&#22312;&#37327;&#23376;&#29289;&#29702;&#23398;&#30340;&#32972;&#26223;&#19979;&#65292;&#35757;&#32451;&#27169;&#22411;&#20197;&#25551;&#36848;&#37327;&#23376;&#24577;&#21448;&#26410;&#32463;&#20154;&#31867;&#24178;&#39044;&#22320;&#23398;&#20064;&#20449;&#24687;&#26159;&#33719;&#24471;&#28145;&#20837;&#20102;&#35299;&#26426;&#22120;&#22914;&#20309;&#21576;&#29616;&#22797;&#26434;&#37327;&#23376;&#24577;&#30340;&#37325;&#35201;&#36884;&#24452;&#12290;&#35299;&#37322;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#23558;&#20026;&#38750;&#24179;&#20961;&#30340;&#37327;&#23376;&#31995;&#32479;&#21450;&#20854;&#26377;&#25928;&#34920;&#31034;&#24102;&#26469;&#26032;&#30340;&#35270;&#35282;&#12290;&#25105;&#20204;&#38024;&#23545;&#30001;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#29983;&#25104;&#30340;&#20004;&#37327;&#23376;&#27604;&#29305;&#23494;&#24230;&#30697;&#38453;&#35757;&#32451;&#20102;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#35745;&#31639;&#23454;&#39564;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#35813;&#27169;&#22411;&#25152;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#20197;&#21450;&#20854;&#20869;&#37096;&#23545;&#25968;&#25454;&#20449;&#24687;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#27169;&#22411;&#23398;&#20064;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#26041;&#24335;&#65292;&#24182;&#23558;&#37327;&#23376;&#24577;&#19982;&#20854;&#28508;&#22312;&#32467;&#26500;&#30456;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised machine learning models build an internal representation of their training data without the need for explicit human guidance or feature engineering. This learned representation provides insights into which features of the data are relevant for the task at hand. In the context of quantum physics, training models to describe quantum states without human intervention offers a promising approach to gaining insight into how machines represent complex quantum states. The ability to interpret the learned representation may offer a new perspective on non-trivial features of quantum systems and their efficient representation. We train a generative model on two-qubit density matrices generated by a parameterized quantum circuit. In a series of computational experiments, we investigate the learned representation of the model and its internal understanding of the data. We observe that the model learns an interpretable representation which relates the quantum states to their underlying
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Transformer&#21644;&#20840;&#23616;&#20196;&#29260;&#20849;&#20139;&#30340;&#36731;&#37327;&#32423;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#38024;&#23545;&#23884;&#20837;&#24335;&#35774;&#22791;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.05682</link><description>&lt;p&gt;
&#36890;&#36807;&#20196;&#29260;&#20849;&#20139;Transformer&#23454;&#29616;&#36731;&#37327;&#32423;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Lightweight Monocular Depth Estimation via Token-Sharing Transformer. (arXiv:2306.05682v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05682
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Transformer&#21644;&#20840;&#23616;&#20196;&#29260;&#20849;&#20139;&#30340;&#36731;&#37327;&#32423;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#38024;&#23545;&#23884;&#20837;&#24335;&#35774;&#22791;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20272;&#35745;&#26159;&#21508;&#31181;&#26426;&#22120;&#20154;&#31995;&#32479;&#21644;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#26159;&#19968;&#31181;&#29702;&#24819;&#30340;&#36873;&#25321;&#65292;&#28982;&#32780;&#22823;&#22810;&#25968;&#26041;&#27861;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#32780;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Token-Sharing Transformer&#20307;&#31995;&#32467;&#26500;&#26469;&#20248;&#21270;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#20013;&#30340;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Depth estimation is an important task in various robotics systems and applications. In mobile robotics systems, monocular depth estimation is desirable since a single RGB camera can be deployable at a low cost and compact size. Due to its significant and growing needs, many lightweight monocular depth estimation networks have been proposed for mobile robotics systems. While most lightweight monocular depth estimation methods have been developed using convolution neural networks, the Transformer has been gradually utilized in monocular depth estimation recently. However, massive parameters and large computational costs in the Transformer disturb the deployment to embedded devices. In this paper, we present a Token-Sharing Transformer (TST), an architecture using the Transformer for monocular depth estimation, optimized especially in embedded devices. The proposed TST utilizes global token sharing, which enables the model to obtain an accurate depth prediction with high throughput in emb
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;MPC&#12289;MSC&#20197;&#21450;DE&#32467;&#21512;&#21040;&#22270;&#20687;&#30697;&#38453;&#20013;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;EEG&#30340;&#24773;&#32490;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#30740;&#31350;&#20026;&#33041;&#26426;&#25509;&#21475;&#21644;&#24247;&#22797;&#21307;&#23398;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.05680</link><description>&lt;p&gt;
&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#22522;&#20110;&#33041;&#30005;&#22270;&#26816;&#27979;&#24773;&#32490;
&lt;/p&gt;
&lt;p&gt;
Emotion Detection from EEG using Transfer Learning. (arXiv:2306.05680v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;MPC&#12289;MSC&#20197;&#21450;DE&#32467;&#21512;&#21040;&#22270;&#20687;&#30697;&#38453;&#20013;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;EEG&#30340;&#24773;&#32490;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#30740;&#31350;&#20026;&#33041;&#26426;&#25509;&#21475;&#21644;&#24247;&#22797;&#21307;&#23398;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#36827;&#34892;&#24773;&#32490;&#26816;&#27979;&#26159;&#33041;&#26426;&#25509;&#21475;&#39046;&#22495;&#20013;&#38750;&#24120;&#20851;&#38190;&#30340;&#39046;&#22495;&#65292;&#24182;&#22312;&#24247;&#22797;&#21644;&#21307;&#23398;&#39046;&#22495;&#26377;&#24456;&#26377;&#20215;&#20540;&#30340;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#20102;&#36801;&#31227;&#23398;&#20064;&#26469;&#20811;&#26381;&#22522;&#20110;EEG&#24773;&#32490;&#26816;&#27979;&#30340;&#25968;&#25454;&#26377;&#38480;&#24615;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#22522;&#26412;&#27169;&#22411;&#26159;Resnet50&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#22522;&#20110;EEG&#30340;&#24773;&#32490;&#26816;&#27979;&#20013;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#32452;&#21512;&#12290;&#27169;&#22411;&#30340;&#36755;&#20837;&#24418;&#24335;&#26159;&#19968;&#20010;&#22270;&#20687;&#30697;&#38453;&#65292;&#20854;&#20013;&#19978;&#19977;&#35282;&#30697;&#38453;&#21644;&#19979;&#19977;&#35282;&#30697;&#38453;&#20998;&#21035;&#21253;&#25324;&#24179;&#22343;&#30456;&#20301;&#30456;&#24178;&#65288;MPC&#65289;&#21644;&#24133;&#24230;&#24179;&#26041;&#30456;&#24178;&#65288;MSC&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#24046;&#20998;&#29109;&#65288;DE&#65289;&#33719;&#24471;&#30340;&#29305;&#24449;&#32467;&#21512;&#21040;&#23545;&#35282;&#32447;&#20013;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#35813;&#25216;&#26415;&#65292;&#20197;&#21069;&#23545;&#24773;&#32490;&#20998;&#31867;&#20960;&#20046;&#27809;&#26377;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#26159;SEED EEG&#65288;62&#36890;&#36947;EEG&#65289;&#65292;&#21253;&#25324;&#19977;&#20010;&#31867;&#21035;&#65288;&#31215;&#26497;&#65292;&#20013;&#24615;&#21644;&#28040;&#26497;&#65289;&#12290;&#25105;&#20204;&#35745;&#31639;&#20102;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#24773;&#32490;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#27604;&#36739;&#20102;&#20854;&#23427;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The detection of emotions using an Electroencephalogram (EEG) is a crucial area in brain-computer interfaces and has valuable applications in fields such as rehabilitation and medicine. In this study, we employed transfer learning to overcome the challenge of limited data availability in EEG-based emotion detection. The base model used in this study was Resnet50. Additionally, we employed a novel feature combination in EEG-based emotion detection. The input to the model was in the form of an image matrix, which comprised Mean Phase Coherence (MPC) and Magnitude Squared Coherence (MSC) in the upper-triangular and lower-triangular matrices, respectively. We further improved the technique by incorporating features obtained from the Differential Entropy (DE) into the diagonal, which previously held little to no useful information for classifying emotions. The dataset used in this study, SEED EEG (62 channel EEG), comprises three classes (Positive, Neutral, and Negative). We calculated both
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22522;&#20110;&#31070;&#32463;&#20999;&#21521;&#26680;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#20197;&#20943;&#23569;&#36229;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#31243;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#21482;&#38656;&#35201;&#20351;&#29992;&#19968;&#31181;&#36741;&#21161;&#32593;&#32476;&#23601;&#21487;&#20197;&#28040;&#38500;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.05674</link><description>&lt;p&gt;
&#38754;&#21521;&#36229;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#20943;&#23569;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Uncertainty Quantification and Reduction for Over-Parameterized Neural Networks. (arXiv:2306.05674v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22522;&#20110;&#31070;&#32463;&#20999;&#21521;&#26680;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#20197;&#20943;&#23569;&#36229;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#31243;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#21482;&#38656;&#35201;&#20351;&#29992;&#19968;&#31181;&#36741;&#21161;&#32593;&#32476;&#23601;&#21487;&#20197;&#28040;&#38500;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#35780;&#20272;&#21644;&#25913;&#36827;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#19981;&#30830;&#23450;&#24615;&#19981;&#20165;&#26469;&#33258;&#25968;&#25454;&#65292;&#36824;&#26469;&#33258;&#35757;&#32451;&#36807;&#31243;&#20013;&#27880;&#20837;&#30340;&#22823;&#37327;&#22122;&#22768;&#21644;&#20559;&#24046;&#12290;&#36825;&#20123;&#22122;&#22768;&#21644;&#20559;&#24046;&#22952;&#30861;&#20102;&#32479;&#35745;&#20445;&#35777;&#30340;&#23454;&#29616;&#65292;&#24182;&#19988;&#30001;&#20110;&#38656;&#35201;&#37325;&#22797;&#30340;&#32593;&#32476;&#37325;&#26032;&#35757;&#32451;&#65292;&#23545;UQ&#25552;&#20986;&#20102;&#35745;&#31639;&#25361;&#25112;&#12290;&#22522;&#20110;&#26368;&#36817;&#30340;&#31070;&#32463;&#20999;&#21521;&#26680;&#29702;&#35770;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#20855;&#26377;&#32479;&#35745;&#20445;&#35777;&#30340;&#26041;&#26696;&#65292;&#20197;&#36890;&#36807;&#38750;&#24120;&#20302;&#30340;&#35745;&#31639;&#37327;&#37327;&#21270;&#21644;&#20943;&#23569;&#36229;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36807;&#31243;&#19981;&#30830;&#23450;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#25105;&#20204;&#31216;&#20026;&#36807;&#31243;&#22122;&#22768;&#26657;&#27491;&#65288;PNC&#65289;&#39044;&#27979;&#22120;&#65292;&#36890;&#36807;&#21482;&#20351;&#29992;&#19968;&#31181;&#36866;&#24403;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#36741;&#21161;&#32593;&#32476;&#26469;&#28040;&#38500;&#36807;&#31243;&#19981;&#30830;&#23450;&#24615;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#28145;&#23618;&#38598;&#25104;&#20013;&#30340;&#35768;&#22810;&#37325;&#26032;&#35757;&#32451;&#30340;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;PNC&#39044;&#27979;&#22120;&#19982;&#25152;&#25552;&#20986;&#30340;&#20808;&#39564;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#25152;&#38656;&#30340;&#32593;&#32476;&#27491;&#21017;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty quantification (UQ) is important for reliability assessment and enhancement of machine learning models. In deep learning, uncertainties arise not only from data, but also from the training procedure that often injects substantial noises and biases. These hinder the attainment of statistical guarantees and, moreover, impose computational challenges on UQ due to the need for repeated network retraining. Building upon the recent neural tangent kernel theory, we create statistically guaranteed schemes to principally \emph{quantify}, and \emph{remove}, the procedural uncertainty of over-parameterized neural networks with very low computation effort. In particular, our approach, based on what we call a procedural-noise-correcting (PNC) predictor, removes the procedural uncertainty by using only \emph{one} auxiliary network that is trained on a suitably labeled data set, instead of many retrained networks employed in deep ensembles. Moreover, by combining our PNC predictor with su
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#29289;&#29702;&#27169;&#25311;&#21644;&#29615;&#22659;&#24863;&#30693;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#22312;&#39640;&#24230;&#32422;&#26463;&#30340;&#29615;&#22659;&#20013;&#65292;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#30340;&#20840;&#36523;&#23039;&#21183;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#20132;&#20114;&#21160;&#20316;&#12290;</title><link>http://arxiv.org/abs/2306.05666</link><description>&lt;p&gt;
QuestEnvSim&#65306;&#22522;&#20110;&#29615;&#22659;&#24863;&#30693;&#30340;&#31232;&#30095;&#20256;&#24863;&#22120;&#27169;&#25311;&#36816;&#21160;&#36861;&#36394;
&lt;/p&gt;
&lt;p&gt;
QuestEnvSim: Environment-Aware Simulated Motion Tracking from Sparse Sensors. (arXiv:2306.05666v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#29289;&#29702;&#27169;&#25311;&#21644;&#29615;&#22659;&#24863;&#30693;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#22312;&#39640;&#24230;&#32422;&#26463;&#30340;&#29615;&#22659;&#20013;&#65292;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#30340;&#20840;&#36523;&#23039;&#21183;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#20132;&#20114;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#20351;&#29992;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#22797;&#21046;&#29992;&#25143;&#30340;&#23039;&#21183;&#23545;&#20110;&#35768;&#22810;AR/VR&#24212;&#29992;&#31243;&#24207;&#38750;&#24120;&#37325;&#35201;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#36816;&#21160;&#36319;&#36394;&#26041;&#27861;&#22312;&#29615;&#22659;&#20132;&#20114;&#26041;&#38754;&#37117;&#36991;&#20813;&#20102;&#38500;&#33050;-&#22320;&#38754;&#25509;&#35302;&#22806;&#30340;&#20854;&#20182;&#22240;&#32032;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#22797;&#26434;&#30340;&#21160;&#24577;&#21644;&#30828;&#32422;&#26463;&#12290;&#20294;&#26159;&#65292;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#20154;&#20204;&#32463;&#24120;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#65292;&#20363;&#22914;&#22352;&#22312;&#27801;&#21457;&#19978;&#25110;&#20506;&#38752;&#22312;&#26700;&#23376;&#19978;&#12290;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#39640;&#24230;&#32422;&#26463;&#30340;&#29615;&#22659;&#20013;&#65292;&#21482;&#35201;&#23558;&#22836;&#25140;&#24335;&#26174;&#31034;&#22120;&#21644;&#25511;&#21046;&#22120;&#23039;&#21183;&#19982;&#29289;&#29702;&#27169;&#25311;&#21644;&#29615;&#22659;&#35266;&#23519;&#30456;&#32467;&#21512;&#65292;&#21363;&#21487;&#29983;&#25104;&#36924;&#30495;&#30340;&#20840;&#36523;&#23039;&#21183;&#12290;&#29289;&#29702;&#27169;&#25311;&#33258;&#21160;&#23454;&#26045;&#36924;&#30495;&#23039;&#21183;&#25152;&#38656;&#30340;&#21508;&#31181;&#32422;&#26463;&#26465;&#20214;&#65292;&#32780;&#19981;&#20687;&#35768;&#22810;&#36816;&#21160;&#23398;&#26041;&#27861;&#20013;&#37027;&#26679;&#25163;&#21160;&#25351;&#23450;&#23427;&#20204;&#12290;&#36825;&#20123;&#30828;&#32422;&#26463;&#26465;&#20214;&#20351;&#25105;&#20204;&#33021;&#22815;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#20132;&#20114;&#21160;&#20316;&#65292;&#32780;&#19981;&#20250;&#20986;&#29616;&#20856;&#22411;&#30340;&#31359;&#36879;&#25110;&#25509;&#35302;&#28369;&#21160;&#31561;&#38382;&#39064;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#19977;&#20010;&#29305;&#24615;&#65292;&#29615;&#22659;&#34920;&#24449;&#65292;&#25509;&#35302;&#22870;&#21169;&#21644;&#22330;&#26223;&#38543;&#26426;&#21270;&#65292;&#36825;&#20123;&#29305;&#24615;&#26377;&#21161;&#20110;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25104;&#21151;&#65292;&#24182;&#23637;&#31034;&#20102;&#21508;&#31181;&#23450;&#24615;&#21644;&#23450;&#37327;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Replicating a user's pose from only wearable sensors is important for many AR/VR applications. Most existing methods for motion tracking avoid environment interaction apart from foot-floor contact due to their complex dynamics and hard constraints. However, in daily life people regularly interact with their environment, e.g. by sitting on a couch or leaning on a desk. Using Reinforcement Learning, we show that headset and controller pose, if combined with physics simulation and environment observations can generate realistic full-body poses even in highly constrained environments. The physics simulation automatically enforces the various constraints necessary for realistic poses, instead of manually specifying them as in many kinematic approaches. These hard constraints allow us to achieve high-quality interaction motions without typical artifacts such as penetration or contact sliding. We discuss three features, the environment representation, the contact reward and scene randomizatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#30446;&#26631;&#36319;&#36394;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#21033;&#29992;&#22522;&#20110;&#35823;&#24046;&#21453;&#39304;&#30340;&#21387;&#32553;&#26041;&#26696;&#26469;&#35299;&#20915;&#20195;&#29702;&#30340;&#36890;&#20449;&#38480;&#21046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.05655</link><description>&lt;p&gt;
&#36890;&#20449;&#25928;&#29575;&#20248;&#21270;&#38382;&#39064;&#30340;&#20998;&#24067;&#24335;&#38646;&#38454;&#22312;&#32447;&#20248;&#21270;&#65306;&#31639;&#27861;&#12289;&#29702;&#35770;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Communication-Efficient Zeroth-Order Distributed Online Optimization: Algorithm, Theory, and Applications. (arXiv:2306.05655v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#30446;&#26631;&#36319;&#36394;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#21033;&#29992;&#22522;&#20110;&#35823;&#24046;&#21453;&#39304;&#30340;&#21387;&#32553;&#26041;&#26696;&#26469;&#35299;&#20915;&#20195;&#29702;&#30340;&#36890;&#20449;&#38480;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#20110;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#65292;&#38024;&#23545;&#30446;&#26631;&#36319;&#36394;&#30340;&#22810;&#26234;&#33021;&#20307;&#38646;&#38454;&#22312;&#32447;&#20248;&#21270;&#38382;&#39064;&#12290;&#20195;&#29702;&#21482;&#33021;&#24863;&#30693;&#33258;&#24049;&#19982;&#30446;&#26631;&#30340;&#24403;&#21069;&#36317;&#31163;&#65292;&#24182;&#21147;&#27714;&#20445;&#25345;&#24444;&#27492;&#20043;&#38388;&#30340;&#26368;&#23567;&#23433;&#20840;&#36317;&#31163;&#65292;&#20197;&#36991;&#20813;&#30896;&#25758;&#12290;&#20195;&#29702;&#20043;&#38388;&#30340;&#21327;&#35843;&#21644;&#30896;&#25758;&#39044;&#38450;&#20449;&#24687;&#30340;&#20256;&#25773;&#30001;&#20013;&#22830;&#26381;&#21153;&#22120;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#33539;&#24335;&#36827;&#34892;&#31649;&#29702;&#12290;&#25152;&#25552;&#20986;&#30340;&#20844;&#24335;&#23548;&#33268;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#22312;&#32447;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#23454;&#20363;&#65292;&#35813;&#38382;&#39064;&#36890;&#36807;&#19968;&#32452;&#21463;&#36890;&#20449;&#38480;&#21046;&#30340;&#20195;&#29702;&#35299;&#20915;&#12290;&#20026;&#20102;&#22788;&#29702;&#20195;&#29702;&#30340;&#36890;&#20449;&#38480;&#21046;&#65292;&#21033;&#29992;&#22522;&#20110;&#35823;&#24046;&#21453;&#39304;&#30340;&#21387;&#32553;&#26041;&#26696;&#36827;&#34892;&#20195;&#29702;&#21040;&#26381;&#21153;&#22120;&#30340;&#36890;&#20449;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#20998;&#24067;&#24335;&#22312;&#32447;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#19968;&#33324;&#31867;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#38750;&#28176;&#36817;&#25910;&#25947;&#36895;&#29575;&#65292;&#34920;&#26126;&#20027;&#23548;&#39033;&#29420;&#31435;&#20110;&#21387;&#32553;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on a multi-agent zeroth-order online optimization problem in a federated learning setting for target tracking. The agents only sense their current distances to their targets and aim to maintain a minimum safe distance from each other to prevent collisions. The coordination among the agents and dissemination of collision-prevention information is managed by a central server using the federated learning paradigm. The proposed formulation leads to an instance of distributed online nonconvex optimization problem that is solved via a group of communication-constrained agents. To deal with the communication limitations of the agents, an error feedback-based compression scheme is utilized for agent-to-server communication. The proposed algorithm is analyzed theoretically for the general class of distributed online nonconvex optimization problems. We provide non-asymptotic convergence rates that show the dominant term is independent of the characteristics of the compression 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#32531;&#35299;&#38544;&#31169;-&#20248;&#21270;&#21046;&#32422;&#20851;&#31995;&#30340;&#38160;&#21270;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.05651</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#38160;&#21270;&#24863;&#30693;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Sharpness-Aware Training. (arXiv:2306.05651v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#32531;&#35299;&#38544;&#31169;-&#20248;&#21270;&#21046;&#32422;&#20851;&#31995;&#30340;&#38160;&#21270;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#36827;&#34892;&#35757;&#32451;&#20250;&#23548;&#33268;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#31169;&#26377;&#23398;&#20064;&#30340;&#20960;&#20309;&#23646;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38160;&#21270;&#24863;&#30693;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#38544;&#31169;-&#20248;&#21270;&#30340;&#21046;&#32422;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training deep learning models with differential privacy (DP) results in a degradation of performance. The training dynamics of models with DP show a significant difference from standard training, whereas understanding the geometric properties of private learning remains largely unexplored. In this paper, we investigate sharpness, a key factor in achieving better generalization, in private learning. We show that flat minima can help reduce the negative effects of per-example gradient clipping and the addition of Gaussian noise. We then verify the effectiveness of Sharpness-Aware Minimization (SAM) for seeking flat minima in private learning. However, we also discover that SAM is detrimental to the privacy budget and computational time due to its two-step optimization. Thus, we propose a new sharpness-aware training method that mitigates the privacy-optimization trade-off. Our experimental results demonstrate that the proposed method improves the performance of deep learning models with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;CVXPY&#20197;&#29992;&#25143;&#21451;&#22909;&#30340;&#26041;&#24335;&#33258;&#21160;&#21270;&#40065;&#26834;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#23545;&#20598;&#21270;&#36807;&#31243;&#65292;&#20351;&#24471;&#29992;&#25143;&#21487;&#20197;&#26041;&#20415;&#22320;&#35299;&#20915;&#21508;&#31181;&#22238;&#24402;&#21644;&#20998;&#31867;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.05649</link><description>&lt;p&gt;
&#20351;&#29992;CVXPY&#35268;&#23450;&#21644;&#35299;&#20915;&#40065;&#26834;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Specifying and Solving Robust Empirical Risk Minimization Problems Using CVXPY. (arXiv:2306.05649v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;CVXPY&#20197;&#29992;&#25143;&#21451;&#22909;&#30340;&#26041;&#24335;&#33258;&#21160;&#21270;&#40065;&#26834;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#23545;&#20598;&#21270;&#36807;&#31243;&#65292;&#20351;&#24471;&#29992;&#25143;&#21487;&#20197;&#26041;&#20415;&#22320;&#35299;&#20915;&#21508;&#31181;&#22238;&#24402;&#21644;&#20998;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#40065;&#26834;&#24615;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#65292;&#20854;&#20013;&#27169;&#22411;&#21442;&#25968;&#34987;&#36873;&#20026;&#20351;&#24471;&#27599;&#20010;&#25968;&#25454;&#28857;&#22312;&#32473;&#23450;&#30340;&#20984;&#19981;&#30830;&#23450;&#24615;&#38598;&#20869;&#21464;&#21270;&#26102;&#26368;&#23567;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#32463;&#39564;&#25439;&#22833;&#12290;&#22312;&#19968;&#20123;&#31616;&#21333;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#34920;&#36798;&#20026;&#35299;&#26512;&#24418;&#24335;&#12290;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#23545;&#20598;&#21270;&#20351;&#38382;&#39064;&#21464;&#24471;&#21487;&#34892;&#65292;&#36825;&#23558;&#19968;&#20010;min-max&#38382;&#39064;&#36716;&#25442;&#20026;&#19968;&#20010;min-min&#38382;&#39064;&#12290;&#23545;&#20598;&#21270;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#65292;&#24456;&#28902;&#29712;&#20063;&#23481;&#26131;&#20986;&#29616;&#38169;&#35823;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;CVXPY&#20197;&#29992;&#25143;&#21451;&#22909;&#30340;&#26041;&#24335;&#33258;&#21160;&#21270;&#36825;&#20010;&#23545;&#20598;&#21270;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#20174;&#19968;&#20010;&#19968;&#33324;&#30340;&#20984;&#25439;&#22833;&#31867;&#20013;&#25429;&#25417;&#35768;&#22810;&#26631;&#20934;&#30340;&#22238;&#24402;&#21644;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#19988;&#29992;&#25143;&#21487;&#20197;&#36731;&#26494;&#22320;&#25351;&#23450;&#20219;&#20309;&#21487;&#20197;&#29992;&#32426;&#24459;&#21270;&#20984;&#35268;&#21010;&#65288;DCP&#65289;&#32422;&#26463;&#34920;&#31034;&#30340;&#22797;&#26434;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider robust empirical risk minimization (ERM), where model parameters are chosen to minimize the worst-case empirical loss when each data point varies over a given convex uncertainty set. In some simple cases, such problems can be expressed in an analytical form. In general the problem can be made tractable via dualization, which turns a min-max problem into a min-min problem. Dualization requires expertise and is tedious and error-prone. We demonstrate how CVXPY can be used to automate this dualization procedure in a user-friendly manner. Our framework allows practitioners to specify and solve robust ERM problems with a general class of convex losses, capturing many standard regression and classification problems. Users can easily specify any complex uncertainty set that is representable via disciplined convex programming (DCP) constraints.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#19981;&#21516;&#25968;&#25454;&#38598;&#21512;&#24182;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#26356;&#20026;&#26174;&#33879;&#65292;&#22240;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#30340;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#20351;&#24471;&#21512;&#24182;&#26356;&#21152;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25968;&#25454;&#38598;&#21387;&#32553;&#21019;&#24314;&#30340;&#21387;&#32553;&#25968;&#25454;&#38598;&#21487;&#20197;&#20316;&#20026;&#21407;&#25968;&#25454;&#38598;&#30340;&#26367;&#20195;&#21697;&#12290;</title><link>http://arxiv.org/abs/2306.05641</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#32622;&#25442;&#23545;&#19981;&#21516;&#25968;&#25454;&#38598;&#38388;&#27169;&#22411;&#21512;&#24182;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Revisiting Permutation Symmetry for Merging Models between Different Datasets. (arXiv:2306.05641v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#19981;&#21516;&#25968;&#25454;&#38598;&#21512;&#24182;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#26356;&#20026;&#26174;&#33879;&#65292;&#22240;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#30340;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#20351;&#24471;&#21512;&#24182;&#26356;&#21152;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25968;&#25454;&#38598;&#21387;&#32553;&#21019;&#24314;&#30340;&#21387;&#32553;&#25968;&#25454;&#38598;&#21487;&#20197;&#20316;&#20026;&#21407;&#25968;&#25454;&#38598;&#30340;&#26367;&#20195;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#21512;&#24182;&#26159;&#19968;&#31181;&#36890;&#36807;&#32452;&#21512;&#19981;&#21516;&#35757;&#32451;&#27169;&#22411;&#30340;&#26435;&#37325;&#26469;&#21019;&#24314;&#26032;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#27169;&#22411;&#21512;&#24182;&#23545;&#20110;&#19981;&#21516;&#38543;&#26426;&#25968;&#35757;&#32451;&#27169;&#22411;&#30340;&#21333;&#19968;&#25968;&#25454;&#38598;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#26159;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#36827;&#34892;&#27169;&#22411;&#21512;&#24182;&#21364;&#24456;&#22256;&#38590;&#12290;&#23558;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#21512;&#24182;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#65292;&#20294;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#25506;&#35752;&#20102;&#19981;&#21516;&#25968;&#25454;&#38598;&#38388;&#21512;&#24182;&#27169;&#22411;&#30340;&#29305;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#38543;&#30528;&#25968;&#25454;&#38598;&#30340;&#24046;&#24322;&#36234;&#22823;&#65292;&#21512;&#24182;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#24471;&#26356;&#20026;&#26174;&#33879;&#65292;&#32780;&#27599;&#20010;&#25968;&#25454;&#38598;&#30340;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#20351;&#24471;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#27169;&#22411;&#21512;&#24182;&#26356;&#21152;&#22256;&#38590;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#21512;&#24182;&#30340;&#27169;&#22411;&#38656;&#35201;&#25968;&#25454;&#38598;&#25165;&#33021;&#23454;&#29616;&#39640;&#31934;&#24230;&#21512;&#24182;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#24403;&#21512;&#24182;&#27169;&#22411;&#26102;&#65292;&#36890;&#36807;&#25968;&#25454;&#38598;&#21387;&#32553;&#21019;&#24314;&#30340;&#21387;&#32553;&#25968;&#25454;&#38598;&#21487;&#20197;&#20316;&#20026;&#21407;&#25968;&#25454;&#38598;&#30340;&#26367;&#20195;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model merging is a new approach to creating a new model by combining the weights of different trained models. Previous studies report that model merging works well for models trained on a single dataset with different random seeds, while model merging between different datasets is difficult. Merging knowledge from different datasets has practical significance, but it has not been well investigated. In this paper, we investigate the properties of merging models between different datasets. Through theoretical and empirical analyses, we find that the accuracy of the merged model decreases more significantly as the datasets diverge more and that the different loss landscapes for each dataset make model merging between different datasets difficult. We also show that merged models require datasets for merging in order to achieve a high accuracy. Furthermore, we show that condensed datasets created by dataset condensation can be used as substitutes for the original datasets when merging model
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;URL&#26694;&#26550;&#65292;&#36890;&#36807;&#21435;&#30456;&#20851;&#28508;&#31354;&#38388;&#20013;&#30340;&#29305;&#24449;&#26469;&#22240;&#26524;&#24615;&#22320;&#39044;&#27979;&#26410;&#26469;&#29366;&#24577;&#24182;&#22686;&#21152;&#28508;&#31354;&#38388;&#30340;&#32500;&#24230;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#23398;&#20064;&#39044;&#27979;&#34920;&#31034;&#32780;&#19981;&#20250;&#23849;&#28291;&#65292;&#26174;&#30528;&#25552;&#39640;&#20102;&#22522;&#20110;Atari 100k&#22522;&#20934;&#30340;&#26368;&#26032;URL&#26041;&#27861;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.05637</link><description>&lt;p&gt;
&#20851;&#20110;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#29305;&#24449;&#21435;&#30456;&#20851;&#24615;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Importance of Feature Decorrelation for Unsupervised Representation Learning in Reinforcement Learning. (arXiv:2306.05637v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05637
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;URL&#26694;&#26550;&#65292;&#36890;&#36807;&#21435;&#30456;&#20851;&#28508;&#31354;&#38388;&#20013;&#30340;&#29305;&#24449;&#26469;&#22240;&#26524;&#24615;&#22320;&#39044;&#27979;&#26410;&#26469;&#29366;&#24577;&#24182;&#22686;&#21152;&#28508;&#31354;&#38388;&#30340;&#32500;&#24230;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#23398;&#20064;&#39044;&#27979;&#34920;&#31034;&#32780;&#19981;&#20250;&#23849;&#28291;&#65292;&#26174;&#30528;&#25552;&#39640;&#20102;&#22522;&#20110;Atari 100k&#22522;&#20934;&#30340;&#26368;&#26032;URL&#26041;&#27861;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65288;URL&#65289;&#36890;&#36807;&#20174;&#22823;&#22411;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#22522;&#26412;&#21407;&#21017;&#26159;&#36890;&#36807;&#22312;&#28508;&#31354;&#38388;&#20013;&#39044;&#27979;&#26410;&#26469;&#29366;&#24577;&#26469;&#23398;&#20064;&#26102;&#38388;&#19978;&#30340;&#39044;&#27979;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#34920;&#31034;&#23849;&#28291;&#65292;&#20854;&#20013;&#28508;&#34920;&#24449;&#30340;&#23376;&#31354;&#38388;&#23849;&#28291;&#20026;&#20302;&#32500;&#27969;&#24418;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;URL&#26694;&#26550;&#65292;&#36890;&#36807;&#21435;&#30456;&#20851;&#28508;&#31354;&#38388;&#20013;&#30340;&#29305;&#24449;&#26469;&#22240;&#26524;&#24615;&#22320;&#39044;&#27979;&#26410;&#26469;&#29366;&#24577;&#24182;&#22686;&#21152;&#28508;&#31354;&#38388;&#30340;&#32500;&#24230;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#32463;&#39564;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#26377;&#25928;&#22320;&#23398;&#20064;&#39044;&#27979;&#34920;&#31034;&#32780;&#19981;&#20250;&#23849;&#28291;&#65292;&#20174;&#32780;&#26174;&#30528;&#25552;&#39640;&#20102;&#22522;&#20110;Atari 100k&#22522;&#20934;&#30340;&#26368;&#26032;URL&#26041;&#27861;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#20195;&#30721;&#21487;&#22312; https://github.com/dojeon-ai/SimTPR &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, unsupervised representation learning (URL) has improved the sample efficiency of Reinforcement Learning (RL) by pretraining a model from a large unlabeled dataset. The underlying principle of these methods is to learn temporally predictive representations by predicting future states in the latent space. However, an important challenge of this approach is the representational collapse, where the subspace of the latent representations collapses into a low-dimensional manifold. To address this issue, we propose a novel URL framework that causally predicts future states while increasing the dimension of the latent manifold by decorrelating the features in the latent space. Through extensive empirical studies, we demonstrate that our framework effectively learns predictive representations without collapse, which significantly improves the sample efficiency of state-of-the-art URL methods on the Atari 100k benchmark. The code is available at https://github.com/dojeon-ai/SimTPR.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#21487;&#38752;&#24615;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#37319;&#26679;&#19968;&#32452;&#39069;&#22806;&#30340;&#21487;&#38752;&#30693;&#35782;&#28857;&#65292;&#23454;&#29616;&#20102;GNN&#21521;MLP&#30340;&#21487;&#38752;&#33976;&#39311;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#33976;&#39311;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.05628</link><description>&lt;p&gt;
&#37327;&#21270;GNN&#20013;&#30340;&#30693;&#35782;&#20197;&#23454;&#29616;&#21487;&#38752;&#30340;&#33976;&#39311;&#21040;MLP
&lt;/p&gt;
&lt;p&gt;
Quantifying the Knowledge in GNNs for Reliable Distillation into MLPs. (arXiv:2306.05628v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05628
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#21487;&#38752;&#24615;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#37319;&#26679;&#19968;&#32452;&#39069;&#22806;&#30340;&#21487;&#38752;&#30693;&#35782;&#28857;&#65292;&#23454;&#29616;&#20102;GNN&#21521;MLP&#30340;&#21487;&#38752;&#33976;&#39311;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#33976;&#39311;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#24357;&#21512;&#26377;&#25299;&#25169;&#24863;&#30693;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#25512;&#29702;&#39640;&#25928;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;GLNN&#25552;&#35758;&#20174;&#32463;&#36807;&#20805;&#20998;&#35757;&#32451;&#30340;Teacher GNN&#20013;&#33976;&#39311;&#20986;&#30693;&#35782;&#21040;Student MLP&#20013;&#12290;&#26412;&#25991;&#19981;&#21516;&#20110;&#20854;&#20182;&#24037;&#20316;&#25506;&#32034;&#20102;GNN&#20013;&#19981;&#21516;&#30693;&#35782;&#28857;&#65288;&#33410;&#28857;&#65289;&#30340;&#21487;&#38752;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#33976;&#39311;&#36807;&#31243;&#20013;&#25198;&#28436;&#30340;&#35282;&#33394;&#26041;&#38754;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#35745;&#31639;&#20449;&#24687;&#29109;&#23545;&#22122;&#22768;&#25200;&#21160;&#30340;&#19981;&#21464;&#24615;&#26469;&#37327;&#21270;GNN&#20013;&#30340;&#30693;&#35782;&#21487;&#38752;&#24615;&#65292;&#20174;&#20013;&#25105;&#20204;&#35266;&#23519;&#21040;&#19981;&#21516;&#30340;&#30693;&#35782;&#28857;&#65306;&#65288;1&#65289;&#26174;&#31034;&#20986;&#19981;&#21516;&#30340;&#33976;&#39311;&#36895;&#24230;&#65288;&#26102;&#38388;&#24615;&#65289;&#65307;&#65288;2&#65289;&#22312;&#22270;&#20013;&#20998;&#24067;&#19981;&#22343;&#21248;&#65288;&#31354;&#38388;&#26041;&#38754;&#65289;&#12290;&#20026;&#20102;&#23454;&#29616;&#21487;&#38752;&#33976;&#39311;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#30693;&#35782;&#21551;&#21457;&#30340;&#21487;&#38752;&#33976;&#39311;&#65288;KRD&#65289;&#65292;&#23427;&#22522;&#20110;&#23545;&#33410;&#28857;&#25104;&#20026;&#20449;&#24687;&#20016;&#23500;&#12289;&#21487;&#38752;&#30340;&#30693;&#35782;&#28857;&#30340;&#27010;&#29575;&#24314;&#27169;&#65292;&#20174;&#20013;&#37319;&#26679;&#19968;&#32452;&#39069;&#22806;&#30340;&#21487;&#38752;&#30693;&#35782;&#28857;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#33976;&#39311;&#12290;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;KRD&#30456;&#23545;&#20110;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To bridge the gaps between topology-aware Graph Neural Networks (GNNs) and inference-efficient Multi-Layer Perceptron (MLPs), GLNN proposes to distill knowledge from a well-trained teacher GNN into a student MLP. Despite their great progress, comparatively little work has been done to explore the reliability of different knowledge points (nodes) in GNNs, especially their roles played during distillation. In this paper, we first quantify the knowledge reliability in GNN by measuring the invariance of their information entropy to noise perturbations, from which we observe that different knowledge points (1) show different distillation speeds (temporally); (2) are differentially distributed in the graph (spatially). To achieve reliable distillation, we propose an effective approach, namely Knowledge-inspired Reliable Distillation (KRD), that models the probability of each node being an informative and reliable knowledge point, based on which we sample a set of additional reliable knowledg
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; QSeed &#30340;&#31181;&#23376;&#21512;&#25104;&#31639;&#27861;&#65292;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#26174;&#33879;&#21152;&#36895;&#21512;&#25104;&#31639;&#27861;&#65292;&#38024;&#23545;&#26680;&#24515;&#37327;&#23376;&#31639;&#27861;&#32452;&#20214;&#65292;&#22914; Shor &#22240;&#24335;&#20998;&#35299;&#31639;&#27861;&#20013;&#30340; 64 &#27604;&#29305;&#27169;&#24130;&#30005;&#36335;&#65292;QSeed &#33021;&#22312;&#20445;&#25345;&#20302;&#38376;&#35745;&#25968;&#30340;&#21516;&#26102;&#23558;&#21512;&#25104;&#26102;&#38388;&#21152;&#36895; $3.7\times$&#12290;</title><link>http://arxiv.org/abs/2306.05622</link><description>&lt;p&gt;
&#29992;&#26426;&#22120;&#23398;&#20064;&#25552;&#39640;&#37327;&#23376;&#30005;&#36335;&#21512;&#25104;&#30340;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Improving Quantum Circuit Synthesis with Machine Learning. (arXiv:2306.05622v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; QSeed &#30340;&#31181;&#23376;&#21512;&#25104;&#31639;&#27861;&#65292;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#26174;&#33879;&#21152;&#36895;&#21512;&#25104;&#31639;&#27861;&#65292;&#38024;&#23545;&#26680;&#24515;&#37327;&#23376;&#31639;&#27861;&#32452;&#20214;&#65292;&#22914; Shor &#22240;&#24335;&#20998;&#35299;&#31639;&#27861;&#20013;&#30340; 64 &#27604;&#29305;&#27169;&#24130;&#30005;&#36335;&#65292;QSeed &#33021;&#22312;&#20445;&#25345;&#20302;&#38376;&#35745;&#25968;&#30340;&#21516;&#26102;&#23558;&#21512;&#25104;&#26102;&#38388;&#21152;&#36895; $3.7\times$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22024;&#26434;&#20013;&#31561;&#35268;&#27169;&#37327;&#23376; (NISQ) &#26102;&#20195;&#65292;&#25214;&#21040;&#26368;&#23567;&#21270;&#26114;&#36149;&#21644;&#23481;&#26131;&#20986;&#38169;&#30340;&#22810;&#27604;&#29305;&#38376;&#25968;&#37327;&#30340;&#37327;&#23376;&#31639;&#27861;&#23454;&#29616;&#23545;&#20110;&#30830;&#20445;&#35745;&#31639;&#20135;&#29983;&#26377;&#24847;&#20041;&#30340;&#36755;&#20986;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#36171;&#33539;&#21512;&#25104;&#26159;&#25214;&#21040;&#23454;&#29616;&#30446;&#26631;&#37193;&#30697;&#38453;&#30340;&#37327;&#23376;&#30005;&#36335;&#30340;&#36807;&#31243;&#65292;&#22312;&#24456;&#22810;&#24773;&#20917;&#19979;&#33021;&#22815;&#26368;&#20248;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#33258;&#24213;&#21521;&#19978;&#30340;&#36171;&#33539;&#21512;&#25104;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#21463;&#25351;&#25968;&#22686;&#38271;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#36171;&#33539;&#25968;&#25454;&#38598;&#65292;&#20351;&#21512;&#25104;&#31639;&#27861;&#24471;&#21040;&#26174;&#33879;&#30340;&#21152;&#36895;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102; QSeed&#65292;&#19968;&#31181;&#31181;&#23376;&#21512;&#25104;&#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#23398;&#20064;&#30340;&#27169;&#22411;&#24555;&#36895;&#25552;&#20986;&#36164;&#28304;&#26377;&#25928;&#30340;&#30005;&#36335;&#23454;&#29616;&#12290;QSeed &#20445;&#25345;&#20302;&#38376;&#35745;&#25968;&#65292;&#24182;&#38024;&#23545; Shor &#22240;&#24335;&#20998;&#35299;&#31639;&#27861;&#20013;&#30340; 64 &#27604;&#29305;&#27169;&#24130;&#30005;&#36335;&#26680;&#24515;&#32452;&#20214;&#65292;&#27604;&#29616;&#26377;&#25216;&#26415;&#22312;&#21512;&#25104;&#26102;&#38388;&#26041;&#38754;&#25552;&#20379;&#20102; $3.7\times$ &#30340;&#21152;&#36895;&#12290;QSeed &#30340;&#24615;&#33021;&#20248;&#20110;&#24403;&#21069;&#20844;&#35748;&#30340;&#21512;&#25104;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the Noisy Intermediate Scale Quantum (NISQ) era, finding implementations of quantum algorithms that minimize the number of expensive and error prone multi-qubit gates is vital to ensure computations produce meaningful outputs. Unitary synthesis, the process of finding a quantum circuit that implements some target unitary matrix, is able to solve this problem optimally in many cases. However, current bottom-up unitary synthesis algorithms are limited by their exponentially growing run times. We show how applying machine learning to unitary datasets permits drastic speedups for synthesis algorithms. This paper presents QSeed, a seeded synthesis algorithm that employs a learned model to quickly propose resource efficient circuit implementations of unitaries. QSeed maintains low gate counts and offers a speedup of $3.7\times$ in synthesis time over the state of the art for a 64 qubit modular exponentiation circuit, a core component in Shor's factoring algorithm. QSeed's performance impr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#21512;&#20316;&#23398;&#20064;&#20013;&#25968;&#25454;&#22810;&#26679;&#24615;&#19982;&#20195;&#34920;&#24615;&#30340;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21338;&#24328;&#26426;&#21046;&#20197;&#40723;&#21169;&#25968;&#25454;&#36129;&#29486;&#32773;&#25552;&#20379;&#20195;&#34920;&#20840;&#23616;&#20154;&#21475;&#30340;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2306.05592</link><description>&lt;p&gt;
&#21512;&#20316;&#23398;&#20064;&#20013;&#22810;&#26679;&#21270;&#25968;&#25454;&#36129;&#29486;&#30340;&#35780;&#20272;&#19982;&#28608;&#21169;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Evaluating and Incentivizing Diverse Data Contributions in Collaborative Learning. (arXiv:2306.05592v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#21512;&#20316;&#23398;&#20064;&#20013;&#25968;&#25454;&#22810;&#26679;&#24615;&#19982;&#20195;&#34920;&#24615;&#30340;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21338;&#24328;&#26426;&#21046;&#20197;&#40723;&#21169;&#25968;&#25454;&#36129;&#29486;&#32773;&#25552;&#20379;&#20195;&#34920;&#20840;&#23616;&#20154;&#21475;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#34920;&#29616;&#33391;&#22909;&#65292;&#25317;&#26377;&#22810;&#26679;&#21270;&#21644;&#26377;&#20195;&#34920;&#24615;&#30340;&#25968;&#25454;&#38598;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#36129;&#29486;&#32773;&#21487;&#33021;&#21482;&#20851;&#24515;&#29305;&#23450;&#20154;&#32676;&#23376;&#38598;&#30340;&#34920;&#29616;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#21453;&#26144;&#26356;&#24191;&#27867;&#20154;&#21475;&#30340;&#22810;&#26679;&#24615;&#12290;&#36825;&#23548;&#33268;&#20102;PRINCIPAL&#65288;FL&#24179;&#21488;&#35774;&#35745;&#32773;&#65289;&#20851;&#24515;&#20840;&#23616;&#34920;&#29616;&#19982;AGENTS&#65288;&#25968;&#25454;&#25910;&#38598;&#32773;&#65289;&#20851;&#24515;&#23616;&#37096;&#34920;&#29616;&#20043;&#38388;&#30340;&#32039;&#24352;&#20851;&#31995;&#12290;&#26412;&#30740;&#31350;&#23558;&#36825;&#31181;&#32039;&#24352;&#20851;&#31995;&#24418;&#24335;&#21270;&#20026;PRINCIPAL&#19982;&#22810;&#20010;AGENTS&#20043;&#38388;&#30340;&#21338;&#24328;&#65292;&#24182;&#38024;&#23545;&#32447;&#24615;&#35797;&#39564;&#35774;&#35745;&#38382;&#39064;&#26469;&#30740;&#31350;&#23427;&#20204;&#30340;&#20132;&#20114;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29992;&#20110;&#37327;&#21270;&#25968;&#25454;&#22810;&#26679;&#24615;&#30340;&#32479;&#35745;&#26631;&#20934;&#20197;&#21450;&#25152;&#36873;&#25321;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#23545;&#32467;&#26524;&#24179;&#34913;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#28857;&#26469;&#35774;&#35745;&#31616;&#21333;&#30340;&#26368;&#20248;&#32852;&#37030;&#23398;&#20064;&#26426;&#21046;&#65292;&#20197;&#40723;&#21169;&#25968;&#25454;&#25910;&#38598;&#32773;&#36129;&#29486;&#20195;&#34920;&#20840;&#23616;&#20154;&#21475;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
For a federated learning model to perform well, it is crucial to have a diverse and representative dataset. However, the data contributors may only be concerned with the performance on a specific subset of the population, which may not reflect the diversity of the wider population. This creates a tension between the principal (the FL platform designer) who cares about global performance and the agents (the data collectors) who care about local performance. In this work, we formulate this tension as a game between the principal and multiple agents, and focus on the linear experiment design problem to formally study their interaction. We show that the statistical criterion used to quantify the diversity of the data, as well as the choice of the federated learning algorithm used, has a significant effect on the resulting equilibrium. We leverage this to design simple optimal federated learning mechanisms that encourage data collectors to contribute data representative of the global popula
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#36890;&#36947;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#39044;&#27979;&#27969;&#24863;A&#30149;&#27602;&#23487;&#20027;&#21644;&#25239;&#21407;&#20122;&#22411;&#30340;&#26041;&#27861;&#65292;&#25968;&#25454;&#26174;&#31034;&#22810;&#36890;&#36947;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.05587</link><description>&lt;p&gt;
MC-NN&#65306;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#22810;&#36890;&#36947;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#27969;&#24863;&#30149;&#27602;&#23487;&#20027;&#21644;&#25239;&#21407;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
MC-NN: An End-to-End Multi-Channel Neural Network Approach for Predicting Influenza A Virus Hosts and Antigenic Types. (arXiv:2306.05587v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05587
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#36890;&#36947;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#39044;&#27979;&#27969;&#24863;A&#30149;&#27602;&#23487;&#20027;&#21644;&#25239;&#21407;&#20122;&#22411;&#30340;&#26041;&#27861;&#65292;&#25968;&#25454;&#26174;&#31034;&#22810;&#36890;&#36947;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24863;&#23545;&#20844;&#20849;&#21355;&#29983;&#26500;&#25104;&#37325;&#22823;&#23041;&#32961;&#65292;&#29305;&#21035;&#26159;&#23545;&#32769;&#24180;&#20154;&#12289;&#20799;&#31461;&#21644;&#24739;&#26377;&#28508;&#22312;&#30142;&#30149;&#30340;&#20154;&#26469;&#35828;&#26356;&#20026;&#20005;&#37325;&#12290;&#20005;&#37325;&#30149;&#20917;&#30340;&#21457;&#29983;&#65292;&#22914;&#32954;&#28814;&#65292;&#20984;&#26174;&#20102;&#39044;&#38450;&#27969;&#24863;&#20256;&#25773;&#30340;&#37325;&#35201;&#24615;&#12290;&#20934;&#30830;&#32780;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#39044;&#27979;&#27969;&#24863;A&#30149;&#27602;&#30340;&#23487;&#20027;&#21644;&#25239;&#21407;&#20122;&#22411;&#23545;&#20110;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#22320;&#21306;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#36890;&#36947;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#34880;&#20957;&#32032;&#21644;&#31070;&#32463;&#27688;&#37240;&#37238;&#34507;&#30333;&#24207;&#21015;&#39044;&#27979;&#27969;&#24863;A&#30149;&#27602;&#30340;&#23487;&#20027;&#21644;&#25239;&#21407;&#20122;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;&#22312;&#19968;&#20010;&#23436;&#25972;&#34507;&#30333;&#36136;&#24207;&#21015;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#24182;&#22312;&#21508;&#31181;&#23436;&#25972;&#21644;&#19981;&#23436;&#25972;&#24207;&#21015;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#22810;&#36890;&#36947;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#26469;&#33258;&#23436;&#25972;&#21644;&#37096;&#20998;&#34507;&#30333;&#36136;&#24207;&#21015;&#30340;&#27969;&#24863;A&#30149;&#27602;&#30340;&#23487;&#20027;&#21644;&#25239;&#21407;&#20122;&#22411;&#20855;&#26377;&#28508;&#21147;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Influenza poses a significant threat to public health, particularly among the elderly, young children, and people with underlying dis-eases. The manifestation of severe conditions, such as pneumonia, highlights the importance of preventing the spread of influenza. An accurate and cost-effective prediction of the host and antigenic sub-types of influenza A viruses is essential to addressing this issue, particularly in resource-constrained regions. In this study, we propose a multi-channel neural network model to predict the host and antigenic subtypes of influenza A viruses from hemagglutinin and neuraminidase protein sequences. Our model was trained on a comprehensive data set of complete protein sequences and evaluated on various test data sets of complete and incomplete sequences. The results demonstrate the potential and practicality of using multi-channel neural networks in predicting the host and antigenic subtypes of influenza A viruses from both full and partial protein sequence
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;SE&#65288;3&#65289;&#31561;&#21464;&#32467;&#26500;&#21644;&#38750;&#30417;&#30563;&#35757;&#32451;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#21018;&#24615;&#20998;&#21106;&#21644;&#36816;&#21160;&#20272;&#35745;&#65292;&#19981;&#38656;&#35201;&#31867;&#21035;&#20449;&#24687;&#19988;&#20855;&#26377;&#26497;&#39640;&#30340;&#27169;&#22411;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.05584</link><description>&lt;p&gt;
&#22810;&#20307;SE&#65288;3&#65289;&#31561;&#21464;&#24615;&#29992;&#20110;&#26080;&#30417;&#30563;&#30340;&#21018;&#20307;&#20998;&#21106;&#21644;&#36816;&#21160;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Multi-body SE(3) Equivariance for Unsupervised Rigid Segmentation and Motion Estimation. (arXiv:2306.05584v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05584
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;SE&#65288;3&#65289;&#31561;&#21464;&#32467;&#26500;&#21644;&#38750;&#30417;&#30563;&#35757;&#32451;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#21018;&#24615;&#20998;&#21106;&#21644;&#36816;&#21160;&#20272;&#35745;&#65292;&#19981;&#38656;&#35201;&#31867;&#21035;&#20449;&#24687;&#19988;&#20855;&#26377;&#26497;&#39640;&#30340;&#27169;&#22411;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#21018;&#20307;&#20998;&#21106;&#21644;&#36816;&#21160;&#20272;&#35745;&#30340;&#30495;&#27491;&#36890;&#29992;&#26041;&#27861;&#23545;&#20110;&#29702;&#35299;&#20851;&#33410;&#29289;&#20307;&#21644;&#31227;&#21160;&#22330;&#26223;&#30340;&#19977;&#32500;&#24433;&#20687;&#33267;&#20851;&#37325;&#35201;&#12290;&#37492;&#20110;&#20998;&#21106;&#21644;&#36816;&#21160;&#20272;&#35745;&#20043;&#38388;&#23494;&#20999;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;SE&#65288;3&#65289;&#31561;&#21464;&#20307;&#31995;&#32467;&#26500;&#21644;&#22521;&#35757;&#31574;&#30053;&#65292;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20307;&#31995;&#32467;&#26500;&#21253;&#25324;&#20004;&#20010;&#36731;&#37327;&#32423;&#21644;&#30456;&#20114;&#36830;&#25509;&#30340;&#22836;&#37096;&#65292;&#20351;&#29992;&#28857;&#32423;&#19981;&#21464;&#29305;&#24449;&#21644;&#26469;&#33258;SE&#65288;3&#65289;&#31561;&#21464;&#29305;&#24449;&#30340;&#36816;&#21160;&#20272;&#35745;&#26469;&#39044;&#27979;&#20998;&#21106;&#25513;&#27169;&#65292;&#32780;&#19981;&#38656;&#35201;&#31867;&#21035;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#32479;&#19968;&#22521;&#35757;&#31574;&#30053;&#21487;&#20197;&#22312;&#32447;&#25191;&#34892;&#65292;&#36890;&#36807;&#21033;&#29992;&#22330;&#26223;&#27969;&#65292;&#20998;&#21106;&#25513;&#27169;&#21644;&#21018;&#24615;&#21464;&#25442;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#26469;&#21516;&#26102;&#20248;&#21270;&#20004;&#20010;&#39044;&#27979;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27169;&#22411;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#22343;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#65292;&#21482;&#26377;0.25M&#21442;&#25968;&#21644;0.92G FLOPs&#12290;
&lt;/p&gt;
&lt;p&gt;
A truly generalizable approach to rigid segmentation and motion estimation is fundamental to 3D understanding of articulated objects and moving scenes. In view of the tightly coupled relationship between segmentation and motion estimates, we present an SE(3) equivariant architecture and a training strategy to tackle this task in an unsupervised manner. Our architecture comprises two lightweight and inter-connected heads that predict segmentation masks using point-level invariant features and motion estimates from SE(3) equivariant features without the prerequisites of category information. Our unified training strategy can be performed online while jointly optimizing the two predictions by exploiting the interrelations among scene flow, segmentation mask, and rigid transformations. We show experiments on four datasets as evidence of the superiority of our method both in terms of model performance and computational efficiency with only 0.25M parameters and 0.92G FLOPs. To the best of ou
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#38024;&#23545;&#36229;&#21442;&#25968;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;SGLD&#20449;&#24687;&#20934;&#21017;&#65292;&#36890;&#36807;KL&#20449;&#24687;&#21644;KL&#25955;&#24230;&#32602;&#39033;&#26469;&#36861;&#36394;&#21452;&#19979;&#38477;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2306.05583</link><description>&lt;p&gt;
&#22522;&#20110;SGLD&#30340;&#20449;&#24687;&#20934;&#21017;&#19982;&#36229;&#21442;&#25968;&#21270;&#27169;&#22411;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
SGLD-Based Information Criteria and the Over-Parameterized Regime. (arXiv:2306.05583v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#38024;&#23545;&#36229;&#21442;&#25968;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;SGLD&#20449;&#24687;&#20934;&#21017;&#65292;&#36890;&#36807;KL&#20449;&#24687;&#21644;KL&#25955;&#24230;&#32602;&#39033;&#26469;&#36861;&#36394;&#21452;&#19979;&#38477;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#21452;&#19992;&#38517;&#8221;&#26159;&#25351;&#36807;&#24230;&#21442;&#25968;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#25554;&#20540;&#38408;&#20540;&#20043;&#22806;&#30340;&#24847;&#22806;&#27979;&#35797;&#25439;&#22833;&#19979;&#38477;&#65292;&#36825;&#19981;&#26159;&#30001;&#20110;&#26631;&#20934;&#28176;&#36827;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#23548;&#33268;&#32463;&#20856;&#24418;&#24335;&#30340;&#20449;&#24687;&#20934;&#21017;&#26080;&#27861;&#39044;&#27979;&#12290;&#25105;&#20204;&#20351;&#29992;&#20449;&#24687;&#39118;&#38505;&#26368;&#23567;&#21270;&#26694;&#26550;&#26356;&#26032;&#36825;&#20123;&#20998;&#26512;&#65292;&#24182;&#20026;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;Langevin&#21160;&#21147;&#23398;&#65288;SGLD&#65289;&#23398;&#20064;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;Akaike&#20449;&#24687;&#20934;&#21017;&#65288;AIC&#65289;&#21644;&#36125;&#21494;&#26031;&#20449;&#24687;&#20934;&#21017;&#65288;BIC&#65289;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;SGLD&#30340;AIC&#21644;BIC&#32602;&#39033;&#23545;&#24212;&#29305;&#23450;&#30340;&#20449;&#24687;&#24230;&#37327;&#65292;&#21363;&#23545;&#31216;&#30340;KL&#20449;&#24687;&#21644;KL&#25955;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#34920;&#24449;&#22823;&#37327;&#21442;&#25968;&#27169;&#22411;&#30340;SGLD-BIC&#25193;&#23637;&#20102;&#27492;&#20449;&#24687;&#29702;&#35770;&#20998;&#26512;&#65292;&#20854;&#20013;&#21442;&#25968;&#25968;$p$&#21644;&#26679;&#26412;&#25968;$n$&#36235;&#20110;&#26080;&#31351;&#22823;&#65292;$p/n$&#22266;&#23450;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25913;&#36827;&#30340;SGLD-BIC&#21487;&#20197;&#36319;&#36394;&#21452;&#19979;&#38477;&#26354;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Double-descent refers to the unexpected drop in test loss of a learning algorithm beyond an interpolating threshold with over-parameterization, which is not predicted by information criteria in their classical forms due to the limitations in the standard asymptotic approach. We update these analyses using the information risk minimization framework and provide Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) for models learned by stochastic gradient Langevin dynamics (SGLD). Notably, the AIC and BIC penalty terms for SGLD correspond to specific information measures, i.e., symmetrized KL information and KL divergence. We extend this information-theoretic analysis to over-parameterized models by characterizing the SGLD-based BIC for the random feature model in the regime where the number of parameters $p$ and the number of samples $n$ tend to infinity, with $p/n$ fixed. Our experiments demonstrate that the refined SGLD-based BIC can track the double-descent cur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#36890;&#36807;&#31283;&#20581;&#27169;&#25311;&#29983;&#25104;&#38543;&#26426;&#22270;&#24182;&#23558;&#21152;&#26435;&#25216;&#26415;&#32467;&#21512;UCB&#31639;&#27861;&#65292;&#20197;&#21327;&#20316;&#26041;&#24335;&#20943;&#23567;&#25972;&#20010;&#31995;&#32479;&#30340;&#24635;&#36951;&#25022;&#12290;</title><link>http://arxiv.org/abs/2306.05579</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#38543;&#26426;&#20998;&#24067;&#30340;&#24322;&#26500;&#22870;&#21169;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Decentralized Randomly Distributed Multi-agent Multi-armed Bandit with Heterogeneous Rewards. (arXiv:2306.05579v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#36890;&#36807;&#31283;&#20581;&#27169;&#25311;&#29983;&#25104;&#38543;&#26426;&#22270;&#24182;&#23558;&#21152;&#26435;&#25216;&#26415;&#32467;&#21512;UCB&#31639;&#27861;&#65292;&#20197;&#21327;&#20316;&#26041;&#24335;&#20943;&#23567;&#25972;&#20010;&#31995;&#32479;&#30340;&#24635;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#22810;&#20010;&#23458;&#25143;&#31471;&#36890;&#36807;&#30001;&#29615;&#22659;&#25552;&#20379;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#38543;&#26426;&#22270;&#36827;&#34892;&#36830;&#25509;&#12290;&#27599;&#20010;&#33218;&#30340;&#22870;&#21169;&#20998;&#24067;&#22240;&#23458;&#25143;&#32780;&#24322;&#65292;&#24182;&#19988;&#22870;&#21169;&#26159;&#26681;&#25454;&#21253;&#25324;&#20122;&#25351;&#25968;&#21644;&#20122;&#39640;&#26031;&#20998;&#24067;&#22312;&#20869;&#30340;&#20998;&#24067;&#65292;&#30001;&#29615;&#22659;&#29420;&#31435;&#22320;&#38543;&#26102;&#38388;&#29983;&#25104;&#30340;&#12290;&#27599;&#20010;&#23458;&#25143;&#31471;&#37117;&#20250;&#25289;&#21160;&#19968;&#20010;&#33218;&#65292;&#24182;&#26681;&#25454;&#30001;&#29615;&#22659;&#25552;&#20379;&#30340;&#22270;&#19982;&#37051;&#23621;&#36827;&#34892;&#36890;&#20449;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#21327;&#20316;&#26469;&#20943;&#23567;&#25972;&#20010;&#31995;&#32479;&#30340;&#24635;&#36951;&#25022;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#39318;&#20808;&#25552;&#20379;&#20102;&#20351;&#29992;&#24555;&#36895;&#28151;&#21512;&#39532;&#23572;&#21487;&#22827;&#38142;&#25110;&#38543;&#26426;&#22270;&#27169;&#22411;&#29983;&#25104;&#38543;&#26426;&#22270;&#30340;&#31283;&#20581;&#20223;&#30495;&#26041;&#27861;&#65292;&#28982;&#21518;&#23558;&#22522;&#20110;&#24179;&#22343;&#19968;&#33268;&#24615;&#26041;&#27861;&#21644;&#26032;&#25552;&#20986;&#30340;&#21152;&#26435;&#25216;&#26415;&#20197;&#21450;&#19978;&#32622;&#20449;&#38480;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;UCB&#31867;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#32771;&#34385;&#21040;&#20102;&#22270;&#24418;&#20013;&#30340;&#38543;&#26426;&#24615;&#65292;&#28040;&#38500;&#20102;&#38480;&#21046;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a decentralized multi-agent multi-armed bandit problem in which multiple clients are connected by time dependent random graphs provided by an environment. The reward distributions of each arm vary across clients and rewards are generated independently over time by an environment based on distributions that include both sub-exponential and sub-gaussian distributions. Each client pulls an arm and communicates with neighbors based on the graph provided by the environment. The goal is to minimize the overall regret of the entire system through collaborations. To this end, we introduce a novel algorithmic framework, which first provides robust simulation methods for generating random graphs using rapidly mixing Markov chains or the random graph model, and then combines an averaging-based consensus approach with a newly proposed weighting technique and the upper confidence bound to deliver a UCB-type solution. Our algorithms account for the randomness in the graphs, removing the con
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#26234;&#33021;&#22478;&#24066;&#33021;&#28304;&#31649;&#29702;&#30340;&#29289;&#32852;&#32593;&#26694;&#26550;&#65292;&#32467;&#21512;&#26234;&#33021;&#20998;&#26512;&#21644;&#22810;&#32452;&#20214;&#30340;&#26550;&#26500;&#65292;&#30740;&#31350;&#20102;&#22522;&#20110;&#26234;&#33021;&#26426;&#21046;&#30340;&#26234;&#33021;&#33021;&#28304;&#31649;&#29702;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#26399;&#33410;&#33021;&#21644;&#20248;&#21270;&#31649;&#29702;&#12290;</title><link>http://arxiv.org/abs/2306.05567</link><description>&lt;p&gt;
&#26234;&#33021;&#20998;&#26512;&#65292;&#22312;&#29289;&#32852;&#32593;&#26694;&#26550;&#19979;&#30340;&#26234;&#33021;&#22478;&#24066;&#33021;&#28304;&#31649;&#29702;&#65306;&#22797;&#26434;&#32593;&#32476;&#21644;&#31995;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Intelligent Energy Management with IoT Framework in Smart Cities Using Intelligent Analysis: An Application of Machine Learning Methods for Complex Networks and Systems. (arXiv:2306.05567v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#26234;&#33021;&#22478;&#24066;&#33021;&#28304;&#31649;&#29702;&#30340;&#29289;&#32852;&#32593;&#26694;&#26550;&#65292;&#32467;&#21512;&#26234;&#33021;&#20998;&#26512;&#21644;&#22810;&#32452;&#20214;&#30340;&#26550;&#26500;&#65292;&#30740;&#31350;&#20102;&#22522;&#20110;&#26234;&#33021;&#26426;&#21046;&#30340;&#26234;&#33021;&#33021;&#28304;&#31649;&#29702;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#26399;&#33410;&#33021;&#21644;&#20248;&#21270;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#24314;&#31569;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#22522;&#20110;&#29289;&#32852;&#32593;&#30340;&#26080;&#32447;&#20256;&#24863;&#31995;&#32479;&#26469;&#38477;&#20302;&#33021;&#28304;&#28040;&#32791;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#24320;&#21457;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20110;&#29289;&#32852;&#32593;&#30340;&#26234;&#33021;&#22478;&#24066;&#33021;&#28304;&#31649;&#29702;&#26694;&#26550;&#65292;&#34701;&#21512;&#20102;&#22810;&#20010;&#29289;&#32852;&#32593;&#26550;&#26500;&#21644;&#26694;&#26550;&#30340;&#32452;&#20214;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#26234;&#33021;&#20998;&#26512;&#65292;&#19981;&#20165;&#25910;&#38598;&#21644;&#23384;&#20648;&#20449;&#24687;&#65292;&#32780;&#19988;&#36824;&#26159;&#20854;&#20182;&#20225;&#19994;&#24320;&#21457;&#24212;&#29992;&#30340;&#24179;&#21488;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#22522;&#20110;&#26234;&#33021;&#26426;&#21046;&#30340;&#26234;&#33021;&#33021;&#28304;&#31649;&#29702;&#35299;&#20915;&#26041;&#26696;&#12290;&#33021;&#28304;&#36164;&#28304;&#30340;&#28040;&#32791;&#21644;&#38656;&#27714;&#22686;&#21152;&#23548;&#33268;&#20102;&#33410;&#33021;&#19982;&#20248;&#21270;&#31649;&#29702;&#30340;&#38656;&#27714;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Smart buildings are increasingly using Internet of Things (IoT)-based wireless sensing systems to reduce their energy consumption and environmental impact. As a result of their compact size and ability to sense, measure, and compute all electrical properties, Internet of Things devices have become increasingly important in our society. A major contribution of this study is the development of a comprehensive IoT-based framework for smart city energy management, incorporating multiple components of IoT architecture and framework. An IoT framework for intelligent energy management applications that employ intelligent analysis is an essential system component that collects and stores information. Additionally, it serves as a platform for the development of applications by other companies. Furthermore, we have studied intelligent energy management solutions based on intelligent mechanisms. The depletion of energy resources and the increase in energy demand have led to an increase in energy 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#33258;&#36866;&#24212;&#30340;&#36125;&#21494;&#26031;&#28388;&#27874;&#33539;&#24335;&#26469;&#35299;&#20915;&#29616;&#26377;&#27010;&#29575;ODE&#27714;&#35299;&#22120;&#26080;&#27861;&#35299;&#20915;&#30340;&#28145;&#23618;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#38382;&#39064;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#27010;&#29575;ODE&#27714;&#35299;&#22120;&#26356;&#31934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2306.05566</link><description>&lt;p&gt;
&#25968;&#25454;&#33258;&#36866;&#24212;&#27010;&#29575;&#20284;&#28982;&#36924;&#36817;&#24120;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Data-Adaptive Probabilistic Likelihood Approximation for Ordinary Differential Equations. (arXiv:2306.05566v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#33258;&#36866;&#24212;&#30340;&#36125;&#21494;&#26031;&#28388;&#27874;&#33539;&#24335;&#26469;&#35299;&#20915;&#29616;&#26377;&#27010;&#29575;ODE&#27714;&#35299;&#22120;&#26080;&#27861;&#35299;&#20915;&#30340;&#28145;&#23618;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#38382;&#39064;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#27010;&#29575;ODE&#27714;&#35299;&#22120;&#26356;&#31934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODEs&#65289;&#30340;&#21442;&#25968;&#25512;&#26029;&#22312;&#35768;&#22810;&#31185;&#23398;&#24212;&#29992;&#20013;&#20855;&#26377;&#22522;&#26412;&#37325;&#35201;&#24615;&#12290;&#34429;&#28982;ODE&#35299;&#36890;&#24120;&#30001;&#30830;&#23450;&#24615;&#31639;&#27861;&#36817;&#20284;&#65292;&#20294;&#26377;&#20851;&#27010;&#29575;&#27714;&#35299;&#22120;&#30340;&#26032;&#30740;&#31350;&#34920;&#26126;&#65292;&#23427;&#20204;&#36890;&#36807;&#26356;&#22909;&#22320;&#32771;&#34385;&#25968;&#23383;&#35823;&#24046;&#20135;&#29983;&#26356;&#21487;&#38752;&#30340;&#21442;&#25968;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;ODE&#31995;&#32479;&#23545;&#20854;&#21442;&#25968;&#20540;&#38750;&#24120;&#25935;&#24863;&#12290;&#36825;&#22312;&#20284;&#28982;&#20989;&#25968;&#20013;&#20135;&#29983;&#20102;&#28145;&#23618;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#8212;&#8212;&#29616;&#26377;&#30340;&#27010;&#29575;&#27714;&#35299;&#22120;&#23578;&#26410;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#29992;&#20110;&#27010;&#29575;ODE&#35299;&#30340;&#36125;&#21494;&#26031;&#28388;&#27874;&#33539;&#24335;&#65292;&#36890;&#36807;&#25968;&#25454;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#22024;&#26434;&#30340;ODE&#35266;&#23519;&#32467;&#26524;&#65292;&#21487;&#20197;&#26174;&#30528;&#38477;&#20302;&#21442;&#25968;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#20855;&#26377;&#37096;&#20998;&#26410;&#35266;&#27979;&#20998;&#37327;&#21644;&#20219;&#24847;&#38750;&#39640;&#26031;&#22122;&#22768;&#30340;ODEs&#12290;&#20960;&#20010;&#20363;&#23376;&#34920;&#26126;&#65292;&#23427;&#27604;&#29616;&#26377;&#30340;&#27010;&#29575;ODE&#27714;&#35299;&#22120;&#26356;&#31934;&#30830;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#27604;&#31934;&#30830;ODE&#20284;&#28982;&#20989;&#25968;&#26356;&#31934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter inference for ordinary differential equations (ODEs) is of fundamental importance in many scientific applications. While ODE solutions are typically approximated by deterministic algorithms, new research on probabilistic solvers indicates that they produce more reliable parameter estimates by better accounting for numerical errors. However, many ODE systems are highly sensitive to their parameter values. This produces deep local minima in the likelihood function -- a problem which existing probabilistic solvers have yet to resolve. Here, we show that a Bayesian filtering paradigm for probabilistic ODE solution can dramatically reduce sensitivity to parameters by learning from the noisy ODE observations in a data-adaptive manner. Our method is applicable to ODEs with partially unobserved components and with arbitrary non-Gaussian noise. Several examples demonstrate that it is more accurate than existing probabilistic ODE solvers, and even in some cases than the exact ODE likel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;GNN&#22312;&#27979;&#35797;&#26102;&#33410;&#28857;&#30340;&#26412;&#22320;&#21516;&#36136;&#24615;&#27700;&#24179;&#19982;&#20854;&#22270;&#30340;&#20840;&#23616;&#21516;&#36136;&#24615;&#27700;&#24179;&#20559;&#31163;&#26102;&#30340;&#24615;&#33021;&#65292;&#24182;&#20171;&#32461;&#19968;&#31181;&#26032;&#21442;&#25968;&#29992;&#20110;&#25511;&#21046;&#21516;&#36136;&#24615;&#65292;&#22312;&#29983;&#25104;&#30340;&#22270;&#20013;&#31995;&#32479;&#22320;&#30740;&#31350;&#26412;&#22320;&#21516;&#36136;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.05557</link><description>&lt;p&gt;
&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#26412;&#22320;&#21516;&#36136;&#24615;&#27700;&#24179;&#30340;&#24615;&#33021;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
On Performance Discrepancies Across Local Homophily Levels in Graph Neural Networks. (arXiv:2306.05557v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;GNN&#22312;&#27979;&#35797;&#26102;&#33410;&#28857;&#30340;&#26412;&#22320;&#21516;&#36136;&#24615;&#27700;&#24179;&#19982;&#20854;&#22270;&#30340;&#20840;&#23616;&#21516;&#36136;&#24615;&#27700;&#24179;&#20559;&#31163;&#26102;&#30340;&#24615;&#33021;&#65292;&#24182;&#20171;&#32461;&#19968;&#31181;&#26032;&#21442;&#25968;&#29992;&#20110;&#25511;&#21046;&#21516;&#36136;&#24615;&#65292;&#22312;&#29983;&#25104;&#30340;&#22270;&#20013;&#31995;&#32479;&#22320;&#30740;&#31350;&#26412;&#22320;&#21516;&#36136;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GNN&#30340;&#30740;&#31350;&#24378;&#35843;&#39640;&#21516;&#36136;&#24615;&#65288;&#21363;&#30456;&#20284;&#31867;&#33410;&#28857;&#30456;&#20114;&#36830;&#25509;&#30340;&#20542;&#21521;&#65289;&#19982;&#33410;&#28857;&#20998;&#31867;&#30340;&#24378;&#39044;&#27979;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#36825;&#31181;&#20851;&#31995;&#26356;&#21152;&#24494;&#22937;&#65292;&#35777;&#26126;&#21363;&#20351;&#31616;&#21333;&#30340;GNN&#20063;&#21487;&#20197;&#22312;&#26576;&#20123;&#24322;&#36136;&#24615;&#29615;&#22659;&#20013;&#23398;&#20064;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#20123;&#21457;&#29616;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#20102;&#20808;&#21069;&#20316;&#21697;&#20013;&#30340;&#20551;&#35774;&#65292;&#24182;&#30830;&#23450;&#25968;&#25454;&#38598;&#32463;&#24120;&#34987;&#35270;&#20026;&#22312;&#33410;&#28857;&#38388;&#20855;&#26377;&#24658;&#23450;&#30340;&#21516;&#36136;&#24615;&#27700;&#24179;&#12290;&#20026;&#20102;&#26356;&#25509;&#36817;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#22320;&#30740;&#31350;&#20102;GNN&#22312;&#27979;&#35797;&#26102;&#33410;&#28857;&#30340;&#26412;&#22320;&#21516;&#36136;&#24615;&#27700;&#24179;&#19982;&#20854;&#22270;&#30340;&#20840;&#23616;&#21516;&#36136;&#24615;&#27700;&#24179;&#20559;&#31163;&#26102;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#24110;&#21161;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#22312;&#21516;&#36136;&#24615;&#20998;&#26512;&#20013;&#24120;&#29992;&#30340;&#20248;&#20808;&#38468;&#21152;&#27169;&#22411;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#21442;&#25968;&#65292;&#20197;&#25511;&#21046;&#29983;&#25104;&#30340;&#22270;&#20013;&#30340;&#26412;&#22320;&#21516;&#36136;&#24615;&#27700;&#24179;&#65292;&#20174;&#32780;&#23454;&#29616;&#31995;&#32479;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25506;&#31350;&#26412;&#22320;&#21516;&#36136;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research on GNNs has highlighted a relationship between high homophily (i.e., the tendency for nodes of a similar class to connect) and strong predictive performance in node classification. However, recent research has found the relationship to be more nuanced, demonstrating that even simple GNNs can learn in certain heterophilous settings. To bridge the gap between these findings, we revisit the assumptions made in previous works and identify that datasets are often treated as having a constant homophily level across nodes. To align closer to real-world datasets, we theoretically and empirically study the performance of GNNs when the local homophily level of a node deviates at test-time from the global homophily level of its graph. To aid our theoretical analysis, we introduce a new parameter to the preferential attachment model commonly used in homophily analysis to enable the control of local homophily levels in generated graphs, enabling a systematic empirical study on how local ho
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24773;&#24863;&#21644;&#24773;&#32490;&#24341;&#23548;&#30340;&#25913;&#20889;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#32454;&#31890;&#24230;&#30340;&#24773;&#24863;&#35843;&#33410;&#65292;&#21487;&#29992;&#20110;&#22810;&#31181;&#22330;&#26223;&#65292;&#36890;&#36807;&#28155;&#21152;&#32454;&#31890;&#24230;&#24773;&#24863;&#26631;&#31614;&#21487;&#20197;&#25552;&#39640;&#32467;&#26524;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.05556</link><description>&lt;p&gt;
&#24773;&#24863;&#21644;&#24773;&#32490;&#24341;&#23548;&#30340;&#25913;&#20889;
&lt;/p&gt;
&lt;p&gt;
Emotion and Sentiment Guided Paraphrasing. (arXiv:2306.05556v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24773;&#24863;&#21644;&#24773;&#32490;&#24341;&#23548;&#30340;&#25913;&#20889;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#32454;&#31890;&#24230;&#30340;&#24773;&#24863;&#35843;&#33410;&#65292;&#21487;&#29992;&#20110;&#22810;&#31181;&#22330;&#26223;&#65292;&#36890;&#36807;&#28155;&#21152;&#32454;&#31890;&#24230;&#24773;&#24863;&#26631;&#31614;&#21487;&#20197;&#25552;&#39640;&#32467;&#26524;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25913;&#20889;&#29983;&#25104;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24120;&#35265;&#19988;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#23427;&#21487;&#20197;&#23545;&#25991;&#26412;&#36827;&#34892;&#20462;&#25913;&#21516;&#26102;&#20445;&#30041;&#21407;&#26412;&#30340;&#24847;&#24605;&#12290;&#24773;&#32490;&#25913;&#20889;&#21487;&#29992;&#20110;&#22810;&#31181;&#22330;&#26223;&#65292;&#21253;&#25324;&#35843;&#33410;&#22312;&#32447;&#23545;&#35805;&#21644;&#38450;&#27490;&#32593;&#32476;&#27450;&#20940;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#32454;&#31890;&#24230;&#24773;&#32490;&#25913;&#20889;&#65292;&#21487;&#20197;&#22312;&#20445;&#30041;&#21407;&#25991;&#24847;&#20041;&#30340;&#21516;&#26102;&#65292;&#24179;&#28369;&#22320;&#25913;&#21464;&#20854;&#24773;&#24863;&#24378;&#24230;&#65292;&#20174;&#32780;&#23454;&#29616;&#32454;&#31890;&#24230;&#30340;&#24773;&#24863;&#35843;&#33410;&#12290;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;&#32454;&#31890;&#24230;&#24773;&#24863;&#26631;&#31614;&#23545;&#20960;&#20010;&#24120;&#29992;&#30340;&#25913;&#20889;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#37325;&#24314;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26377;&#26465;&#20214;&#30340;&#25991;&#26412;&#29983;&#25104;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24773;&#24863;&#21644;&#24773;&#32490;&#24341;&#23548;&#30340;&#25913;&#20889;&#26694;&#26550;&#12290;&#23545;&#35843;&#25972;&#21518;&#30340;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#23558;&#32454;&#31890;&#24230;&#24773;&#24863;&#26631;&#31614;&#21253;&#21547;&#22312;&#25913;&#20889;&#35757;&#32451;&#20013;&#21487;&#20197;&#25552;&#39640;&#32467;&#26524;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Paraphrase generation, a.k.a. paraphrasing, is a common and important task in natural language processing. Emotional paraphrasing, which changes the emotion embodied in a piece of text while preserving its meaning, has many potential applications, including moderating online dialogues and preventing cyberbullying. We introduce a new task of fine-grained emotional paraphrasing along emotion gradients, that is, altering the emotional intensities of the paraphrases in fine-grained settings following smooth variations in affective dimensions while preserving the meaning of the original text. We reconstruct several widely used paraphrasing datasets by augmenting the input and target texts with their fine-grained emotion labels. Then, we propose a framework for emotion and sentiment guided paraphrasing by leveraging pre-trained language models for conditioned text generation. Extensive evaluation of the fine-tuned models suggests that including fine-grained emotion labels in the paraphrase t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#22810;&#23380;&#26448;&#26009;&#20013;&#30340;&#36870;&#27969;&#33258;&#21457;&#28183;&#36879;&#36807;&#31243;&#36827;&#34892;&#20102;&#26089;&#26399;&#21644;&#26202;&#26399;&#30340;&#27169;&#25311;&#21644;&#39044;&#27979;&#65292;&#24182;&#20351;&#29992;&#25913;&#21464;&#21464;&#37327;&#25216;&#26415;&#26469;&#25913;&#36827;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05554</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#22312;&#36870;&#27969;&#33258;&#21457;&#28183;&#36879;&#20013;&#30340;&#24212;&#29992;&#21644;&#39044;&#27979;&#65306;&#26089;&#26399;&#21644;&#26202;&#26399;&#30340;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Simulation and Prediction of Countercurrent Spontaneous Imbibition at Early and Late Times Using Physics-Informed Neural Networks. (arXiv:2306.05554v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#22810;&#23380;&#26448;&#26009;&#20013;&#30340;&#36870;&#27969;&#33258;&#21457;&#28183;&#36879;&#36807;&#31243;&#36827;&#34892;&#20102;&#26089;&#26399;&#21644;&#26202;&#26399;&#30340;&#27169;&#25311;&#21644;&#39044;&#27979;&#65292;&#24182;&#20351;&#29992;&#25913;&#21464;&#21464;&#37327;&#25216;&#26415;&#26469;&#25913;&#36827;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#27969;&#33258;&#21457;&#28183;&#36879;&#65288;COUCSI&#65289;&#26159;&#19968;&#31181;&#22810;&#23380;&#26448;&#26009;&#20013;&#30340;&#36807;&#31243;&#65292;&#20854;&#20013;&#28070;&#28287;&#30456;&#21462;&#20195;&#20102;&#38750;&#28070;&#28287;&#30456;&#30340;&#20301;&#32622;&#12290;&#26412;&#25991;&#39318;&#27425;&#25506;&#35752;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#22312;&#35299;&#20915;&#26089;&#26399;&#65288;ET&#65289;&#21644;&#26202;&#26399;&#65288;LT&#65289;COUCSI&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#25913;&#21464;&#21464;&#37327;&#25216;&#26415;&#20197;&#25913;&#36827;PINNs&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#25913;&#21464;&#33258;&#21464;&#37327;&#23558;COUCSI&#38382;&#39064;&#20998;&#21035;&#29992;XT-&#65292;XY-&#21644;Z-&#19977;&#31181;&#31561;&#25928;&#24418;&#24335;&#36827;&#34892;&#25551;&#36848;&#65306;&#31532;&#19968;&#20010;&#25551;&#36848;&#20102;&#39281;&#21644;&#24230;&#20316;&#20026;&#35268;&#33539;&#21270;&#20301;&#32622;X&#21644;&#26102;&#38388;T&#30340;&#20989;&#25968;;&#31532;&#20108;&#20010;&#25551;&#36848;&#20102;X&#21644;Y=T^0.5&#20316;&#20026;&#20989;&#25968;&#30340;&#39281;&#21644;&#24230;;&#31532;&#19977;&#20010;&#20316;&#20026;Z=X/T^0.5&#30340;&#21807;&#19968;&#20989;&#25968;&#65288;&#20165;&#22312;ET&#19979;&#26377;&#25928;&#65289;&#12290;&#35813;PINN&#27169;&#22411;&#20351;&#29992;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#65292;&#24182;&#22522;&#20110;&#26368;&#23567;&#21270;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#65292;&#21253;&#25324;&#29289;&#29702;&#20449;&#24687;&#20002;&#22833;&#39033;&#21644;&#19982;&#21021;&#22987;&#36793;&#30028;&#26465;&#20214;&#30456;&#23545;&#24212;&#30340;&#39033;&#12290;&#27809;&#26377;&#21512;&#25104;&#25110;&#23454;&#39564;&#25968;&#25454;&#34987;&#35843;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Countercurrent spontaneous imbibition (COUCSI) is a process in porous materials in which a wetting phase displaces non-wetting phase. In this work, we investigate for the first time the application of Physics-Informed Neural Networks (PINNs) in solving the 1D COUCSI problem in both early (ET) and late (LT) times. Also novel, we examine the Change-of-Variables technique for improving the performance of PINNs. We formulated the COUCSI problem in three equivalent forms by changing the independent variables: XT-, XY-, and Z-formulations. The first describes saturation as function of normalized position X and time T; the second as function of X and Y=T^0.5; and the third as a sole function of Z=X/T^0.5 (valid only at ET). The PINN model was generated using a feed-forward neural network and trained based on minimizing a weighted loss function, including the physics-informed loss term and terms corresponding to the initial and boundary conditions. No synthetical or experimental data were invo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32622;&#25442;&#31561;&#21464;&#39592;&#24178;&#21644;&#32622;&#25442;&#19981;&#21464;&#20840;&#23616;&#27744;&#21270;&#22312;&#28857;&#20113;&#20998;&#31867;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25581;&#31034;&#20102;&#20351;&#29992;&#22797;&#26434;&#27744;&#21270;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31616;&#21333;&#39592;&#24178;&#30340;&#24615;&#33021;&#65292;&#20294;&#21363;&#20351;&#26159;&#22797;&#26434;&#30340;&#39592;&#24178;&#20063;&#21487;&#20197;&#21463;&#30410;&#20110;&#26356;&#22797;&#26434;&#30340;&#12289;&#26126;&#30830;&#32534;&#30721;&#32622;&#25442;&#19981;&#21464;&#24615;&#30340;&#27744;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#32622;&#25442;&#19981;&#21464;&#27744;&#21270;&#26159;&#33719;&#24471;&#26368;&#20808;&#36827;&#32467;&#26524;&#30340;&#20851;&#38190;&#12290;</title><link>http://arxiv.org/abs/2306.05553</link><description>&lt;p&gt;
&#31561;&#21464;&#23618;&#19982;&#19981;&#21464;&#23618;&#30340;&#23545;&#27604;&#65306;&#28857;&#20113;&#20998;&#31867;&#20013;&#39592;&#24178;&#32593;&#32476;&#21644;&#27744;&#21270;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Equivariant vs. Invariant Layers: A Comparison of Backbone and Pooling for Point Cloud Classification. (arXiv:2306.05553v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32622;&#25442;&#31561;&#21464;&#39592;&#24178;&#21644;&#32622;&#25442;&#19981;&#21464;&#20840;&#23616;&#27744;&#21270;&#22312;&#28857;&#20113;&#20998;&#31867;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25581;&#31034;&#20102;&#20351;&#29992;&#22797;&#26434;&#27744;&#21270;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31616;&#21333;&#39592;&#24178;&#30340;&#24615;&#33021;&#65292;&#20294;&#21363;&#20351;&#26159;&#22797;&#26434;&#30340;&#39592;&#24178;&#20063;&#21487;&#20197;&#21463;&#30410;&#20110;&#26356;&#22797;&#26434;&#30340;&#12289;&#26126;&#30830;&#32534;&#30721;&#32622;&#25442;&#19981;&#21464;&#24615;&#30340;&#27744;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#32622;&#25442;&#19981;&#21464;&#27744;&#21270;&#26159;&#33719;&#24471;&#26368;&#20808;&#36827;&#32467;&#26524;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#28857;&#20113;&#31561;&#38598;&#21512;&#32467;&#26500;&#25968;&#25454;&#24050;&#21463;&#21040;&#23398;&#26415;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#36890;&#36807;&#25972;&#21512;&#32622;&#25442;&#23545;&#31216;&#24615;&#65292;&#20026;&#35774;&#35745;&#26377;&#25928;&#30340;&#28857;&#20113;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#34013;&#26412;&#12290;&#25105;&#20204;&#24863;&#20852;&#36259;&#30340;&#26159;&#32622;&#25442;&#19981;&#21464;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#30001;&#32622;&#25442;&#31561;&#21464;&#39592;&#24178;&#12289;&#32622;&#25442;&#19981;&#21464;&#20840;&#23616;&#27744;&#21270;&#21644;&#22238;&#24402;/&#20998;&#31867;&#22836;&#32452;&#25104;&#12290;&#23613;&#31649;&#29616;&#26377;&#25991;&#29486;&#20391;&#37325;&#20110;&#25913;&#21892;&#32622;&#25442;&#31561;&#21464;&#39592;&#24178;&#65292;&#20294;&#20840;&#23616;&#27744;&#21270;&#30340;&#24433;&#21709;&#24448;&#24448;&#34987;&#24573;&#35270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32622;&#25442;&#31561;&#21464;&#39592;&#24178;&#21644;&#32622;&#25442;&#19981;&#21464;&#20840;&#23616;&#27744;&#21270;&#22312;&#19977;&#20010;&#22522;&#20934;&#28857;&#20113;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65306;1&#65289;&#35832;&#22914;&#22522;&#20110;&#20256;&#36755;&#25110;&#27880;&#24847;&#21147;&#30340;&#22797;&#26434;&#27744;&#21270;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31616;&#21333;&#39592;&#24178;&#30340;&#24615;&#33021;&#65292;&#20294;&#23545;&#20110;&#26356;&#22797;&#26434;&#30340;&#39592;&#24178;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#25910;&#30410;&#20250;&#20943;&#24369;&#12290;2&#65289;&#29978;&#33267;&#22797;&#26434;&#30340;&#39592;&#24178;&#20063;&#21487;&#20197;&#21463;&#30410;&#20110;&#26356;&#22797;&#26434;&#30340;&#27744;&#21270;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#26126;&#30830;&#22320;&#32534;&#30721;&#32622;&#25442;&#19981;&#21464;&#24615;&#12290;3&#65289;&#20351;&#29992;&#32622;&#25442;&#19981;&#21464;&#27744;&#21270;&#23545;&#20110;&#22312;&#28857;&#20113;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from set-structured data, such as point clouds, has gained significant attention from the community. Geometric deep learning provides a blueprint for designing effective set neural networks by incorporating permutation symmetry. Of our interest are permutation invariant networks, which are composed of a permutation equivariant backbone, permutation invariant global pooling, and regression/classification head. While existing literature has focused on improving permutation equivariant backbones, the impact of global pooling is often overlooked. In this paper, we examine the interplay between permutation equivariant backbones and permutation invariant global pooling on three benchmark point cloud classification datasets. Our findings reveal that: 1) complex pooling methods, such as transport-based or attention-based poolings, can significantly boost the performance of simple backbones, but the benefits diminish for more complex backbones, 2) even complex backbones can benefit fro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#33521;&#35821;&#39044;&#35757;&#32451;&#30340;Masked Language Models&#65288;MLMs&#65289;&#20197;&#21450;&#23427;&#20204;&#30340;&#19979;&#28216;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#23545;&#32654;&#22269;93&#20010;&#31038;&#20250;&#27745;&#21517;&#21270;&#32676;&#20307;&#30340;&#20559;&#35265;&#65292;&#20026;&#20102;&#35780;&#20272;93&#20010;&#27745;&#21517;&#21270;&#26465;&#20214;&#30340;&#20559;&#35265;&#23384;&#22312;&#65292;&#25105;&#20204;&#23545;&#23427;&#20204;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#25214;&#21040;&#20102;&#20559;&#35265;&#23384;&#22312;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.05550</link><description>&lt;p&gt;
&#23545;&#24102;&#8220;&#26631;&#35760;&#35821;&#35328;&#27169;&#22411;&#8221;&#20013;93&#20010;&#21463;&#27495;&#35270;&#32676;&#20307;&#30340;&#20559;&#35265;&#21450;&#20854;&#23545;&#19979;&#28216;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Bias Against 93 Stigmatized Groups in Masked Language Models and Downstream Sentiment Classification Tasks. (arXiv:2306.05550v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#33521;&#35821;&#39044;&#35757;&#32451;&#30340;Masked Language Models&#65288;MLMs&#65289;&#20197;&#21450;&#23427;&#20204;&#30340;&#19979;&#28216;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#23545;&#32654;&#22269;93&#20010;&#31038;&#20250;&#27745;&#21517;&#21270;&#32676;&#20307;&#30340;&#20559;&#35265;&#65292;&#20026;&#20102;&#35780;&#20272;93&#20010;&#27745;&#21517;&#21270;&#26465;&#20214;&#30340;&#20559;&#35265;&#23384;&#22312;&#65292;&#25105;&#20204;&#23545;&#23427;&#20204;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#25214;&#21040;&#20102;&#20559;&#35265;&#23384;&#22312;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#24555;&#36895;&#37096;&#32626;&#38656;&#35201;&#23545;&#36825;&#20123;&#27169;&#22411;&#22266;&#26377;&#30340;&#20559;&#35265;&#21644;&#39118;&#38505;&#36827;&#34892;&#24443;&#24213;&#30340;&#35843;&#26597;&#65292;&#20197;&#20102;&#35299;&#23427;&#20204;&#23545;&#20010;&#20154;&#21644;&#31038;&#20250;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22823;&#35268;&#27169;&#31038;&#20250;&#27745;&#21517;&#21270;&#30340;&#20559;&#35265;&#36827;&#34892;&#30740;&#31350;&#65292;&#25193;&#23637;&#20102;&#24050;&#26377;&#24037;&#20316;&#23545;&#20559;&#35265;&#35780;&#20272;&#30340;&#28966;&#28857;&#12290;&#23427;&#20851;&#27880;&#32654;&#22269;93&#20010;&#31038;&#20250;&#27745;&#21517;&#21270;&#32676;&#20307;&#65292;&#21253;&#25324;&#19982;&#30142;&#30149;&#12289;&#27531;&#30142;&#12289;&#33647;&#29289;&#20351;&#29992;&#12289;&#24515;&#29702;&#30142;&#30149;&#12289;&#23447;&#25945;&#12289;&#24615;&#21462;&#21521;&#12289;&#31038;&#20250;&#32463;&#27982;&#22320;&#20301;&#21644;&#20854;&#20182;&#30456;&#20851;&#22240;&#32032;&#26377;&#20851;&#30340;&#19968;&#31995;&#21015;&#24773;&#20917;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#33521;&#35821;&#39044;&#35757;&#32451;&#30340;Masked Language Models&#65288;MLMs&#65289;&#20197;&#21450;&#23427;&#20204;&#30340;&#19979;&#28216;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#23545;&#36825;&#20123;&#32676;&#20307;&#30340;&#20559;&#35265;&#12290;&#20026;&#20102;&#35780;&#20272;93&#20010;&#27745;&#21517;&#21270;&#26465;&#20214;&#30340;&#20559;&#35265;&#23384;&#22312;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;29&#20010;&#38750;&#27745;&#21517;&#21270;&#26465;&#20214;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#12290;&#22522;&#20110;&#31038;&#20250;&#25490;&#26021;&#30340;&#24515;&#29702;&#23398;&#23610;&#24230;-&#31038;&#20250;&#36317;&#31163;&#37327;&#34920;&#65292;&#25105;&#20204;&#23545;&#20845;&#20010;MLMs&#36827;&#34892;&#20102;&#25552;&#31034;&#65306;RoBERTa-base&#12289;RoBERTa-large&#12289;XLNet-large&#12289;BERTweet-base&#12289;BERTweet-la&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid deployment of artificial intelligence (AI) models demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society. This study extends the focus of bias evaluation in extant work by examining bias against social stigmas on a large scale. It focuses on 93 stigmatized groups in the United States, including a wide range of conditions related to disease, disability, drug use, mental illness, religion, sexuality, socioeconomic status, and other relevant factors. We investigate bias against these groups in English pre-trained Masked Language Models (MLMs) and their downstream sentiment classification tasks. To evaluate the presence of bias against 93 stigmatized conditions, we identify 29 non-stigmatized conditions to conduct a comparative analysis. Building upon a psychology scale of social rejection, the Social Distance Scale, we prompt six MLMs: RoBERTa-base, RoBERTa-large, XLNet-large, BERTweet-base, BERTweet-la
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#21033;&#29992;AI&#24037;&#20855;&#36827;&#34892;&#25511;&#21046;&#24037;&#31243;&#30340;&#26041;&#27861;&#12290;&#20854;&#20013;&#65292;&#33258;&#21160;&#24494;&#20998;&#26159;&#26680;&#24515;&#24037;&#20855;&#20043;&#19968;&#65292;&#21487;&#29992;&#20110;&#23616;&#37096;&#31283;&#23450;&#24615;&#20998;&#26512;&#21644;&#29366;&#24577;&#20272;&#35745;&#31561;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#36824;&#25506;&#35752;&#20102;&#23558;&#24494;&#20998;&#20195;&#25968;&#26041;&#31243;&#36716;&#25442;&#20026;&#24120;&#24494;&#20998;&#26041;&#31243;&#36827;&#34892;&#25511;&#21046;&#35774;&#35745;&#20197;&#21450;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20840;&#23616;&#21442;&#25968;&#21270;&#31561;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.05545</link><description>&lt;p&gt;
AI&#22686;&#24378;&#30340;&#25511;&#21046;&#24037;&#31243;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AI Enhanced Control Engineering Methods. (arXiv:2306.05545v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#21033;&#29992;AI&#24037;&#20855;&#36827;&#34892;&#25511;&#21046;&#24037;&#31243;&#30340;&#26041;&#27861;&#12290;&#20854;&#20013;&#65292;&#33258;&#21160;&#24494;&#20998;&#26159;&#26680;&#24515;&#24037;&#20855;&#20043;&#19968;&#65292;&#21487;&#29992;&#20110;&#23616;&#37096;&#31283;&#23450;&#24615;&#20998;&#26512;&#21644;&#29366;&#24577;&#20272;&#35745;&#31561;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#36824;&#25506;&#35752;&#20102;&#23558;&#24494;&#20998;&#20195;&#25968;&#26041;&#31243;&#36716;&#25442;&#20026;&#24120;&#24494;&#20998;&#26041;&#31243;&#36827;&#34892;&#25511;&#21046;&#35774;&#35745;&#20197;&#21450;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20840;&#23616;&#21442;&#25968;&#21270;&#31561;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#27491;&#22312;&#20960;&#20046;&#25152;&#26377;&#24037;&#31243;&#39046;&#22495;&#20013;&#21464;&#24471;&#26222;&#36941;&#12290;&#25511;&#21046;&#24037;&#31243;&#20063;&#19981;&#33021;&#36867;&#36991;&#36825;&#19968;&#36235;&#21183;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;&#25511;&#21046;&#24212;&#29992;&#20013;&#20351;&#29992;AI&#24037;&#20855;&#12290;&#25105;&#20204;&#25152;&#20851;&#27880;&#30340;&#26680;&#24515;&#24037;&#20855;&#26159;&#33258;&#21160;&#24494;&#20998;&#12290;&#20854;&#20004;&#20010;&#30452;&#25509;&#24212;&#29992;&#21253;&#25324;&#21033;&#29992;Kalman&#28388;&#27874;&#22120;&#23558;&#31995;&#32479;&#21160;&#21147;&#23398;&#32447;&#24615;&#21270;&#20197;&#36827;&#34892;&#23616;&#37096;&#31283;&#23450;&#24615;&#20998;&#26512;&#25110;&#29366;&#24577;&#20272;&#35745;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#20854;&#20182;&#29992;&#36884;&#65292;&#20363;&#22914;&#23558;&#24494;&#20998;&#20195;&#25968;&#26041;&#31243;&#36716;&#25442;&#20026;&#24120;&#24494;&#20998;&#26041;&#31243;&#36827;&#34892;&#25511;&#21046;&#35774;&#35745;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#22312;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#24212;&#29992;&#20013;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20026;&#29366;&#24577;&#21521;&#37327;&#21644;&#25511;&#21046;&#36755;&#20837;&#36827;&#34892;&#20840;&#23616;&#21442;&#25968;&#21270;&#30340;&#29992;&#36884;&#12290;&#23545;&#20110;&#27599;&#20010;&#32771;&#34385;&#30340;&#29992;&#20363;&#65292;&#25105;&#20204;&#37117;&#32473;&#20986;&#20102;&#20363;&#23376;&#21644;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI and machine learning based approaches are becoming ubiquitous in almost all engineering fields. Control engineering cannot escape this trend. In this paper, we explore how AI tools can be useful in control applications. The core tool we focus on is automatic differentiation. Two immediate applications are linearization of system dynamics for local stability analysis or for state estimation using Kalman filters. We also explore other usages such as conversion of differential algebraic equations to ordinary differential equations for control design. In addition, we explore the use of machine learning models for global parameterizations of state vectors and control inputs in model predictive control applications. For each considered use case, we give examples and results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BOOT&#30340;&#25216;&#26415;&#65292;&#23427;&#20351;&#29992;&#26377;&#25928;&#30340;&#26080;&#25968;&#25454;&#33976;&#39311;&#31639;&#27861;&#26469;&#35757;&#32451;&#19968;&#20010;&#26102;&#38388;&#26465;&#20214;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#39044;&#35757;&#32451;&#30340;&#24046;&#20998;&#25193;&#25955;&#27169;&#22411;&#32769;&#24072;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25512;&#29702;&#26102;&#38388;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2306.05544</link><description>&lt;p&gt;
BOOT: &#26080;&#38656;&#25968;&#25454;&#30340;&#24046;&#20998;&#25193;&#25955;&#27169;&#22411;&#33976;&#39311;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
BOOT: Data-free Distillation of Denoising Diffusion Models with Bootstrapping. (arXiv:2306.05544v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BOOT&#30340;&#25216;&#26415;&#65292;&#23427;&#20351;&#29992;&#26377;&#25928;&#30340;&#26080;&#25968;&#25454;&#33976;&#39311;&#31639;&#27861;&#26469;&#35757;&#32451;&#19968;&#20010;&#26102;&#38388;&#26465;&#20214;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#39044;&#35757;&#32451;&#30340;&#24046;&#20998;&#25193;&#25955;&#27169;&#22411;&#32769;&#24072;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25512;&#29702;&#26102;&#38388;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#22810;&#26679;&#30340;&#22270;&#20687;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36845;&#20195;&#21435;&#22122;&#30340;&#32536;&#25925;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#24120;&#24120;&#21463;&#21040;&#32531;&#24930;&#30340;&#29983;&#25104;&#36895;&#24230;&#30340;&#24433;&#21709;&#12290;&#30693;&#35782;&#33976;&#39311;&#26368;&#36817;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#21487;&#34892;&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#19981;&#26174;&#33879;&#38477;&#20302;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#23558;&#25512;&#29702;&#27493;&#39588;&#20943;&#23569;&#21040;&#19968;&#20010;&#25110;&#20960;&#20010;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#33976;&#39311;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#22312;&#32769;&#24072;&#27169;&#22411;&#20013;&#29983;&#25104;&#22823;&#37327;&#30340;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#35201;&#20040;&#38656;&#35201;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#26114;&#36149;&#30340;&#22312;&#32447;&#23398;&#20064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;BOOT&#65292;&#36890;&#36807;&#26377;&#25928;&#30340;&#26080;&#25968;&#25454;&#33976;&#39311;&#31639;&#27861;&#20811;&#26381;&#20102;&#36825;&#20123;&#38480;&#21046;&#12290;&#26680;&#24515;&#24605;&#24819;&#26159;&#23398;&#20064;&#19968;&#20010;&#26102;&#38388;&#26465;&#20214;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#20219;&#20309;&#26102;&#38388;&#27493;&#38271;&#39044;&#27979;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#24046;&#20998;&#25193;&#25955;&#27169;&#22411;&#32769;&#24072;&#30340;&#36755;&#20986;&#12290;&#36825;&#31181;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20004;&#20010;&#36830;&#32493;&#37319;&#26679;&#27493;&#39588;&#30340;&#33258;&#21161;&#27861;&#36827;&#34892;&#26377;&#25928;&#30340;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20219;&#20309;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#25110;&#35745;&#31639;&#36164;&#28304;&#12290;&#22312;&#26679;&#26412;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#65292;&#25105;&#20204;&#22312;&#20960;&#20010;&#25193;&#25955;&#27169;&#22411;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25512;&#29702;&#26102;&#38388;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have demonstrated excellent potential for generating diverse images. However, their performance often suffers from slow generation due to iterative denoising. Knowledge distillation has been recently proposed as a remedy that can reduce the number of inference steps to one or a few without significant quality degradation. However, existing distillation methods either require significant amounts of offline computation for generating synthetic training data from the teacher model or need to perform expensive online learning with the help of real data. In this work, we present a novel technique called BOOT, that overcomes these limitations with an efficient data-free distillation algorithm. The core idea is to learn a time-conditioned model that predicts the output of a pre-trained diffusion model teacher given any time step. Such a model can be efficiently trained based on bootstrapping from two consecutive sampled steps. Furthermore, our method can be easily adapted to 
&lt;/p&gt;</description></item><item><title>&#25919;&#27835;&#36777;&#35770;&#12289;&#28436;&#35762;&#21644;&#35775;&#35848;&#20013;&#30340;&#20540;&#24471;&#26680;&#23454;&#30340;&#35770;&#26029;&#21487;&#20197;&#20351;&#29992;&#38899;&#39057;&#25968;&#25454;&#36827;&#34892;&#26816;&#27979;&#21644;&#30830;&#35748;&#65292;&#36825;&#21487;&#24110;&#21161;&#20027;&#25345;&#20154;&#12289;&#35760;&#32773;&#21644;&#20107;&#23454;&#26680;&#26597;&#32452;&#32455;&#36827;&#34892;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2306.05535</link><description>&lt;p&gt;
&#20351;&#29992;&#38899;&#39057;&#25968;&#25454;&#26816;&#27979;&#25919;&#27835;&#36777;&#35770;&#12289;&#28436;&#35762;&#21644;&#35775;&#35848;&#20013;&#20540;&#24471;&#26680;&#23454;&#30340;&#35770;&#26029;
&lt;/p&gt;
&lt;p&gt;
Detecting Check-Worthy Claims in Political Debates, Speeches, and Interviews Using Audio Data. (arXiv:2306.05535v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05535
&lt;/p&gt;
&lt;p&gt;
&#25919;&#27835;&#36777;&#35770;&#12289;&#28436;&#35762;&#21644;&#35775;&#35848;&#20013;&#30340;&#20540;&#24471;&#26680;&#23454;&#30340;&#35770;&#26029;&#21487;&#20197;&#20351;&#29992;&#38899;&#39057;&#25968;&#25454;&#36827;&#34892;&#26816;&#27979;&#21644;&#30830;&#35748;&#65292;&#36825;&#21487;&#24110;&#21161;&#20027;&#25345;&#20154;&#12289;&#35760;&#32773;&#21644;&#20107;&#23454;&#26680;&#26597;&#32452;&#32455;&#36827;&#34892;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#30340;&#19968;&#22823;&#37096;&#20998;&#22242;&#32467;&#22312;&#30456;&#21516;&#30340;&#24895;&#26223;&#21644;&#24605;&#24819;&#21608;&#22260;&#65292;&#20855;&#26377;&#24040;&#22823;&#30340;&#33021;&#37327;&#12290;&#36825;&#27491;&#26159;&#25919;&#27835;&#20154;&#29289;&#24076;&#26395;&#20026;&#20182;&#20204;&#30340;&#20107;&#19994;&#25152;&#32047;&#31215;&#30340;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#65292;&#20182;&#20204;&#26377;&#26102;&#20250;&#20351;&#29992;&#25197;&#26354;&#25110;&#38544;&#34255;&#30495;&#30456;&#30340;&#25163;&#27573;&#65292;&#26080;&#35770;&#26159;&#26080;&#24847;&#30340;&#36824;&#26159;&#26377;&#24847;&#30340;&#65292;&#36825;&#20026;&#38169;&#35823;&#20449;&#24687;&#21644;&#35823;&#23548;&#24320;&#20102;&#22823;&#38376;&#12290;&#33258;&#21160;&#26816;&#27979;&#20540;&#24471;&#26680;&#23454;&#30340;&#35770;&#26029;&#30340;&#24037;&#20855;&#23558;&#23545;&#36777;&#35770;&#20027;&#25345;&#20154;&#12289;&#35760;&#32773;&#21644;&#20107;&#23454;&#26680;&#26597;&#32452;&#32455;&#26377;&#24456;&#22823;&#24110;&#21161;&#12290;&#34429;&#28982;&#20197;&#21069;&#20851;&#20110;&#26816;&#27979;&#20540;&#24471;&#26680;&#23454;&#30340;&#35770;&#26029;&#30340;&#24037;&#20316;&#37325;&#28857;&#26159;&#25991;&#26412;&#65292;&#20294;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#38899;&#39057;&#20449;&#21495;&#20316;&#20026;&#39069;&#22806;&#20449;&#24687;&#28304;&#30340;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65288;&#33521;&#35821;&#25991;&#26412;&#21644;&#38899;&#39057;&#65289;&#65292;&#21253;&#21547;48&#23567;&#26102;&#30340;&#28436;&#35762;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22810;&#20010;&#28436;&#35762;&#32773;&#30340;&#24773;&#20917;&#19979;&#65292;&#38899;&#39057;&#27169;&#24577;&#19982;&#25991;&#26412;&#32467;&#21512;&#20351;&#29992;&#27604;&#20165;&#20351;&#29992;&#25991;&#26412;&#20855;&#26377;&#25913;&#36827;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#21333;&#22768;&#36947;&#38899;&#39057;&#27169;&#22411;&#21487;&#20197;&#32988;&#36807;&#21333;&#22768;&#36947;&#25991;&#26412;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
A large portion of society united around the same vision and ideas carries enormous energy. That is precisely what political figures would like to accumulate for their cause. With this goal in mind, they can sometimes resort to distorting or hiding the truth, unintentionally or on purpose, which opens the door for misinformation and disinformation. Tools for automatic detection of check-worthy claims would be of great help to moderators of debates, journalists, and fact-checking organizations. While previous work on detecting check-worthy claims has focused on text, here we explore the utility of the audio signal as an additional information source. We create a new multimodal dataset (text and audio in English) containing 48 hours of speech. Our evaluation results show that the audio modality together with text yields improvements over text alone in the case of multiple speakers. Moreover, an audio-only model could outperform a text-only one for a single speaker.
&lt;/p&gt;</description></item><item><title>PeFLL&#26159;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#23884;&#20837;&#32593;&#32476;&#21644;&#36229;&#32593;&#32476;&#65292;PeFLL&#33021;&#22815;&#23398;&#20064;&#36755;&#20986;&#29305;&#23450;&#20110;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#20854;&#23427;&#26032;&#20986;&#29616;&#30340;&#23458;&#25143;&#31471;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.05515</link><description>&lt;p&gt;
&#19968;&#31181;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#32456;&#36523;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PeFLL: A Lifelong Learning Approach to Personalized Federated Learning. (arXiv:2306.05515v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05515
&lt;/p&gt;
&lt;p&gt;
PeFLL&#26159;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#23884;&#20837;&#32593;&#32476;&#21644;&#36229;&#32593;&#32476;&#65292;PeFLL&#33021;&#22815;&#23398;&#20064;&#36755;&#20986;&#29305;&#23450;&#20110;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#20854;&#23427;&#26032;&#20986;&#29616;&#30340;&#23458;&#25143;&#31471;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;pFL&#65289;&#24050;&#25104;&#20026;&#24212;&#23545;&#21442;&#19982;&#23458;&#25143;&#31471;&#25968;&#25454;&#20998;&#24067;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#25361;&#25112;&#30340;&#24120;&#29992;&#26041;&#27861;&#12290;pFL&#19981;&#26159;&#23398;&#20064;&#21333;&#20010;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#26159;&#26088;&#22312;&#23398;&#20064;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#20010;&#20307;&#27169;&#22411;&#65292;&#21516;&#26102;&#20173;&#28982;&#21033;&#29992;&#20854;&#20182;&#23458;&#25143;&#31471;&#21487;&#29992;&#30340;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PeFLL&#65292;&#36825;&#26159;&#19968;&#31181;&#26681;&#26893;&#20110;&#32456;&#36523;&#23398;&#20064;&#30340;&#26032;&#22411;pFL&#26041;&#27861;&#65292;&#19981;&#20165;&#22312;&#35757;&#32451;&#38454;&#27573;&#23384;&#22312;&#30340;&#23458;&#25143;&#31471;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#32780;&#19988;&#22312;&#26410;&#26469;&#21487;&#33021;&#20986;&#29616;&#30340;&#23458;&#25143;&#31471;&#19978;&#20063;&#34920;&#29616;&#33391;&#22909;&#12290;PeFLL&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#23884;&#20837;&#32593;&#32476;&#21644;&#36229;&#32593;&#32476;&#26469;&#23398;&#20064;&#36755;&#20986;&#29305;&#23450;&#20110;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#12290;&#23884;&#20837;&#32593;&#32476;&#23398;&#20064;&#20197;&#19968;&#31181;&#21453;&#26144;&#23427;&#20204;&#20043;&#38388;&#30456;&#20284;&#24615;&#30340;&#28508;&#22312;&#25551;&#36848;&#31526;&#31354;&#38388;&#20013;&#34920;&#31034;&#23458;&#25143;&#31471;&#12290;&#36229;&#32593;&#32476;&#23398;&#20064;&#20174;&#36825;&#20010;&#28508;&#22312;&#31354;&#38388;&#21040;&#21487;&#33021;&#30340;&#23458;&#25143;&#27169;&#22411;&#31354;&#38388;&#30340;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;PeFLL&#20135;&#29983;&#20102;&#26356;&#39640;&#20934;&#30830;&#29575;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized federated learning (pFL) has emerged as a popular approach to dealing with the challenge of statistical heterogeneity between the data distributions of the participating clients. Instead of learning a single global model, pFL aims to learn an individual model for each client while still making use of the data available at other clients. In this work, we present PeFLL, a new pFL approach rooted in lifelong learning that performs well not only on clients present during its training phase, but also on any that may emerge in the future. PeFLL learns to output client specific models by jointly training an embedding network and a hypernetwork. The embedding network learns to represent clients in a latent descriptor space in a way that reflects their similarity to each other. The hypernetwork learns a mapping from this latent space to the space of possible client models. We demonstrate experimentally that PeFLL produces models of superior accuracy compared to previous methods, es
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;MRI&#29305;&#24449;&#21644;&#22238;&#24402;&#27169;&#22411;&#30340;&#20581;&#24247;&#22823;&#33041;&#24180;&#40836;&#40065;&#26834;&#20272;&#35745;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#21644;&#20102;&#35299;&#34928;&#32769;&#26399;&#38388;&#24418;&#24577;&#23398;&#21464;&#21270;&#65292;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#20026;3.02&#23681;&#65292;&#30456;&#20851;&#31995;&#25968;&#20026;0.96&#65292;&#19988;&#20855;&#26377;&#39640;&#24230;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.05514</link><description>&lt;p&gt;
&#22522;&#20110;&#22238;&#24402;&#27169;&#22411;&#21644;MRI&#29305;&#24449;&#30340;&#20581;&#24247;&#22823;&#33041;&#24180;&#40836;&#40065;&#26834;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust Brain Age Estimation via Regression Models and MRI-derived Features. (arXiv:2306.05514v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;MRI&#29305;&#24449;&#21644;&#22238;&#24402;&#27169;&#22411;&#30340;&#20581;&#24247;&#22823;&#33041;&#24180;&#40836;&#40065;&#26834;&#20272;&#35745;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#21644;&#20102;&#35299;&#34928;&#32769;&#26399;&#38388;&#24418;&#24577;&#23398;&#21464;&#21270;&#65292;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#20026;3.02&#23681;&#65292;&#30456;&#20851;&#31995;&#25968;&#20026;0.96&#65292;&#19988;&#20855;&#26377;&#39640;&#24230;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#23398;&#22823;&#33041;&#24180;&#40836;&#30340;&#30830;&#23450;&#26159;&#35780;&#20272;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#21644;&#20102;&#35299;&#34928;&#32769;&#26399;&#38388;&#24418;&#24577;&#23398;&#21464;&#21270;&#30340;&#37325;&#35201;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#25552;&#20986;&#20102;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#36890;&#36807;&#20581;&#24247;&#25511;&#21046;&#32773;&#30340;&#30913;&#20849;&#25391;&#25104;&#20687;(MRI)&#26469;&#20272;&#35745;&#33041;&#40836;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36873;&#25321;&#36866;&#24403;&#30340;MRI&#29305;&#24449;&#21644;MRI&#33719;&#21462;&#25104;&#26412;&#39640;&#26114;&#65292;&#24320;&#21457;&#24378;&#22823;&#30340;&#33041;&#40836;&#20272;&#27979;(BAE)&#26694;&#26550;&#19968;&#30452;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#20351;&#29992;OpenBHB&#25968;&#25454;&#38598;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;BAE&#26694;&#26550;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#26032;&#30340;&#22810;&#31449;&#28857;&#21644;&#20844;&#24320;&#21487;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#26469;&#33258;6&#33267;86&#23681;&#20043;&#38388;3965&#21517;&#20581;&#24247;&#21463;&#35797;&#32773;&#30340;T1&#21152;&#26435;(T1-w)&#33041;MRI&#25195;&#25551;&#30340;&#21306;&#22495;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#38598;&#25104;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;MRI&#21306;&#22495;&#29305;&#24449;&#21644;&#19981;&#21516;&#30340;&#22238;&#24402;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#33041;&#40836;&#20272;&#35745;&#65292;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;(MAE)&#20026;3.02&#23681;&#65292;&#39044;&#27979;&#21644;&#23454;&#38469;&#24180;&#40836;&#20043;&#38388;&#30340;&#30456;&#20851;&#31995;&#25968;(r)&#20026;0.96&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#22806;&#37096;&#39564;&#35777;&#20197;&#21450;&#23545;&#19981;&#21516;MRI&#37319;&#38598;&#21327;&#35758;&#30340;&#27010;&#25324;&#26041;&#38754;&#39640;&#24230;&#40065;&#26834;&#65292;&#39044;&#35745;&#22312;&#20020;&#24202;&#21644;&#30740;&#31350;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The determination of biological brain age is a crucial biomarker in the assessment of neurological disorders and understanding of the morphological changes that occur during aging. Various machine learning models have been proposed for estimating brain age through Magnetic Resonance Imaging (MRI) of healthy controls. However, developing a robust brain age estimation (BAE) framework has been challenging due to the selection of appropriate MRI-derived features and the high cost of MRI acquisition. In this study, we present a novel BAE framework using the Open Big Healthy Brain (OpenBHB) dataset, which is a new multi-site and publicly available benchmark dataset that includes region-wise feature metrics derived from T1-weighted (T1-w) brain MRI scans of 3965 healthy controls aged between 6 to 86 years. Our approach integrates three different MRI-derived region-wise features and different regression models, resulting in a highly accurate brain age estimation with a Mean Absolute Error (MAE
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#23569;&#25968;&#26063;&#35028;&#20559;&#35265;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23631;&#34109;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#25552;&#31034;&#20013;&#21333;&#35789;&#24471;&#20998;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#21487;&#20197;&#35782;&#21035;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#23384;&#22312;&#30340;&#31038;&#20250;&#21051;&#26495;&#21360;&#35937;&#12290;</title><link>http://arxiv.org/abs/2306.05500</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#20998;&#26512;&#20559;&#35265;&#30340;&#21333;&#35789;&#32423;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Word-Level Explanations for Analyzing Bias in Text-to-Image Models. (arXiv:2306.05500v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#23569;&#25968;&#26063;&#35028;&#20559;&#35265;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23631;&#34109;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#25552;&#31034;&#20013;&#21333;&#35789;&#24471;&#20998;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#21487;&#20197;&#35782;&#21035;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#23384;&#22312;&#30340;&#31038;&#20250;&#21051;&#26495;&#21360;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#25509;&#25910;&#19968;&#21477;&#35805;&#65288;&#21363;&#25552;&#31034;&#65289;&#24182;&#29983;&#25104;&#19982;&#35813;&#36755;&#20837;&#25552;&#31034;&#30456;&#20851;&#32852;&#30340;&#22270;&#20687;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#22522;&#20110;&#31181;&#26063;&#21644;&#24615;&#21035;&#32780;&#20559;&#34962;&#23569;&#25968;&#26063;&#35028;&#30340;&#22270;&#20687;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36755;&#20837;&#25552;&#31034;&#20013;&#21738;&#20010;&#21333;&#35789;&#23548;&#33268;&#29983;&#25104;&#22270;&#20687;&#20986;&#29616;&#20559;&#35265;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#35745;&#31639;&#25552;&#31034;&#20013;&#27599;&#20010;&#21333;&#35789;&#24471;&#20998;&#30340;&#26041;&#27861;&#65307;&#36825;&#20123;&#24471;&#20998;&#20195;&#34920;&#20854;&#22312;&#27169;&#22411;&#36755;&#20986;&#20559;&#24046;&#20013;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36981;&#24490;&#8220;&#21024;&#38500;&#35299;&#37322;&#8221;&#30340;&#21407;&#21017;&#65292;&#21033;&#29992;&#23631;&#34109;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#24433;&#21709;&#24471;&#20998;&#12290;&#25105;&#20204;&#22312;&#31283;&#23450;&#25193;&#25955;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#35782;&#21035;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#22797;&#21046;&#31038;&#20250;&#21051;&#26495;&#21360;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image models take a sentence (i.e., prompt) and generate images associated with this input prompt. These models have created award wining-art, videos, and even synthetic datasets. However, text-to-image (T2I) models can generate images that underrepresent minorities based on race and sex. This paper investigates which word in the input prompt is responsible for bias in generated images. We introduce a method for computing scores for each word in the prompt; these scores represent its influence on biases in the model's output. Our method follows the principle of \emph{explaining by removing}, leveraging masked language models to calculate the influence scores. We perform experiments on Stable Diffusion to demonstrate that our method identifies the replication of societal stereotypes in generated images.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22122;&#22768;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#26032;&#25216;&#26415;&#65306;&#21253;&#25324;&#36755;&#20986;&#20559;&#32622;&#12290;</title><link>http://arxiv.org/abs/2306.05497</link><description>&lt;p&gt;
&#37325;&#26032;&#35780;&#20272;&#25439;&#22833;&#20989;&#25968;&#65306;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Reevaluating Loss Functions: Enhancing Robustness to Label Noise in Deep Learning Models. (arXiv:2306.05497v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22122;&#22768;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#26032;&#25216;&#26415;&#65306;&#21253;&#25324;&#36755;&#20986;&#20559;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#20013;&#38590;&#20813;&#20250;&#20986;&#29616;&#38169;&#35823;&#30340;&#26631;&#31614;&#65292;&#36825;&#32473;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#24102;&#26469;&#20102;&#26497;&#22823;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#24456;&#23481;&#26131;&#36866;&#24212;&#36825;&#20123;&#38169;&#35823;&#30340;&#26631;&#31614;&#12290;&#21482;&#26377;&#20351;&#29992;&#19981;&#21463;&#22122;&#22768;&#24178;&#25200;&#30340;&#40065;&#26834;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#25165;&#33021;&#33719;&#24471;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#21019;&#24314;&#22122;&#22768;&#40065;&#26834;&#27169;&#22411;&#30340;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#24335;&#26159;&#20351;&#29992;&#22122;&#22768;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#25968;&#37327;&#20247;&#22810;&#65292;&#23427;&#20204;&#36890;&#24120;&#20276;&#38543;&#30528;&#36229;&#21442;&#25968;&#65292;&#32780;&#19988;&#21487;&#33021;&#23398;&#20064;&#36895;&#24230;&#27604;&#24191;&#27867;&#20351;&#29992;&#20294;&#23545;&#22122;&#22768;&#25935;&#24863;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#35201;&#24930;&#12290;&#36890;&#36807;&#21551;&#21457;&#24335;&#32771;&#34385;&#21644;&#24191;&#27867;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21738;&#20123;&#24773;&#20917;&#19979;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#36866;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#36873;&#25321;&#21512;&#36866;&#30340;&#25439;&#22833;&#30340;&#24314;&#35758;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#26469;&#22686;&#24378;&#24102;&#26377;&#26377;&#30028;&#25439;&#22833;&#20989;&#25968;&#30340;&#23398;&#20064;&#65306;&#21253;&#25324;&#36755;&#20986;&#20559;&#32622;&#65292;&#21363;&#30053;&#24494;&#22686;&#21152;&#19982;&#27491;&#30830;&#26631;&#31614;&#30456;&#23545;&#24212;&#30340;&#31070;&#32463;&#20803;&#39044;&#28608;&#27963;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#25216;&#26415;&#22312;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#31867;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large annotated datasets inevitably contain incorrect labels, which poses a major challenge for the training of deep neural networks as they easily fit the labels. Only when training with a robust model that is not easily distracted by the noise, a good generalization performance can be achieved. A simple yet effective way to create a noise robust model is to use a noise robust loss function. However, the number of proposed loss functions is large, they often come with hyperparameters, and may learn slower than the widely used but noise sensitive Cross Entropy loss. By heuristic considerations and extensive numerical experiments, we study in which situations the proposed loss functions are applicable and give suggestions on how to choose an appropriate loss. Additionally, we propose a novel technique to enhance learning with bounded loss functions: the inclusion of an output bias, i.e. a slight increase in the neuron pre-activation corresponding to the correct label. Surprisingly, we f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20256;&#32479;&#35270;&#35273;&#21464;&#25442;&#22120;&#21644;&#20840;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#30740;&#31350;&#21457;&#29616;&#27880;&#24847;&#36890;&#36947;&#22788;&#29702;&#35774;&#35745;&#23545;&#20110;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#24456;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2306.05495</link><description>&lt;p&gt;
&#35270;&#35273;&#21464;&#25442;&#22120;&#19982;&#20840;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#32508;&#21512;&#20998;&#26512;&#65306;&#38656;&#35201;&#27880;&#24847;&#36890;&#36947;&#22788;&#29702;&#35774;&#35745;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Attentional Channel Processing Design Required? Comprehensive Analysis Of Robustness Between Vision Transformers And Fully Attentional Networks. (arXiv:2306.05495v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20256;&#32479;&#35270;&#35273;&#21464;&#25442;&#22120;&#21644;&#20840;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#30740;&#31350;&#21457;&#29616;&#27880;&#24847;&#36890;&#36947;&#22788;&#29702;&#35774;&#35745;&#23545;&#20110;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#24456;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#26631;&#20934;CNN&#27169;&#22411;&#21644;&#35270;&#35273;&#21464;&#25442;&#22120;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#40065;&#26834;&#24615;&#27979;&#35797;&#65292;&#20294;&#26159;&#32570;&#20047;&#20256;&#32479;&#35270;&#35273;&#21464;&#25442;&#22120;&#19981;&#24102;&#39069;&#22806;&#27880;&#24847;&#36890;&#36947;&#35774;&#35745;&#19982;&#26368;&#26032;&#30340;&#20840;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#32508;&#21512;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20351;&#29992;ImageNet&#25968;&#25454;&#38598;&#27604;&#36739;&#20840;&#33258;&#27880;&#24847;&#32593;&#32476;(FAN)&#27169;&#22411;&#19982;&#20256;&#32479;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#20197;&#20102;&#35299;&#27880;&#24847;&#36890;&#36947;&#22788;&#29702;&#35774;&#35745;&#30340;&#20316;&#29992;&#65292;&#24182;&#20351;&#29992;&#30333;&#30418;&#25915;&#20987;&#21644;&#40657;&#30418;&#25915;&#20987;&#30740;&#31350;&#23427;&#20204;&#20043;&#38388;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The robustness testing has been performed for standard CNN models and Vision Transformers, however there is a lack of comprehensive study between the robustness of traditional Vision Transformers without an extra attentional channel design and the latest fully attentional network(FAN) models. So in this paper, we use the ImageNet dataset to compare the robustness of fully attentional network(FAN) models with traditional Vision Transformers to understand the role of an attentional channel processing design using white box attacks and also study the transferability between the same using black box attacks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20110;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;(NIDS)&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#21516;&#26102;&#25506;&#31350;&#20102;&#25345;&#32493;&#20877;&#35757;&#32451;&#23545;NIDS&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#21363;&#20351;&#27809;&#26377;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#25345;&#32493;&#20877;&#35757;&#32451;&#20063;&#21487;&#20197;&#20943;&#23569;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.05494</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#23545;&#25239;&#24615;&#28431;&#27934;&#25915;&#20987;&#30340;&#23454;&#29992;&#24615;&#27979;&#35797;&#65306;&#21160;&#24577;&#23398;&#20064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Adversarial Evasion Attacks Practicality in Networks: Testing the Impact of Dynamic Learning. (arXiv:2306.05494v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20110;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;(NIDS)&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#21516;&#26102;&#25506;&#31350;&#20102;&#25345;&#32493;&#20877;&#35757;&#32451;&#23545;NIDS&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#21363;&#20351;&#27809;&#26377;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#25345;&#32493;&#20877;&#35757;&#32451;&#20063;&#21487;&#20197;&#20943;&#23569;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;(NIDS)&#20013;&#65292;&#30001;&#20110;&#20854;&#33258;&#21160;&#21270;&#30340;&#29305;&#24615;&#21644;&#22312;&#22788;&#29702;&#21644;&#20998;&#31867;&#22823;&#37327;&#25968;&#25454;&#19978;&#30340;&#39640;&#31934;&#24230;&#12290;&#20294;&#26426;&#22120;&#23398;&#20064;&#23384;&#22312;&#32570;&#38519;&#65292;&#20854;&#20013;&#26368;&#22823;&#30340;&#38382;&#39064;&#20043;&#19968;&#26159;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#20854;&#30446;&#30340;&#26159;&#20351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20135;&#29983;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#29420;&#29305;&#30340;&#36129;&#29486;&#65306;&#23545;&#25239;&#24615;&#25915;&#20987;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;NIDS&#23454;&#29992;&#24615;&#38382;&#39064;&#30340;&#20998;&#31867;&#21644;&#23545;&#25345;&#32493;&#35757;&#32451;&#23545;NIDS&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21363;&#20351;&#27809;&#26377;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#25345;&#32493;&#20877;&#35757;&#32451;&#20063;&#21487;&#20197;&#20943;&#23569;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#23545;&#25239;&#24615;&#25915;&#20987;&#21487;&#33021;&#20250;&#21361;&#21450;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;NIDS&#65292;&#20294;&#25345;&#32493;&#20877;&#35757;&#32451;&#21487;&#24102;&#26469;&#19968;&#23450;&#30340;&#32531;&#35299;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) has become ubiquitous, and its deployment in Network Intrusion Detection Systems (NIDS) is inevitable due to its automated nature and high accuracy in processing and classifying large volumes of data. However, ML has been found to have several flaws, on top of them are adversarial attacks, which aim to trick ML models into producing faulty predictions. While most adversarial attack research focuses on computer vision datasets, recent studies have explored the practicality of such attacks against ML-based network security entities, especially NIDS.  This paper presents two distinct contributions: a taxonomy of practicality issues associated with adversarial attacks against ML-based NIDS and an investigation of the impact of continuous training on adversarial attacks against NIDS. Our experiments indicate that continuous re-training, even without adversarial training, can reduce the effect of adversarial attacks. While adversarial attacks can harm ML-based NIDSs, ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24320;&#25918;&#35789;&#27719;&#29289;&#20307;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#20998;&#31867;&#22120;&#65292;&#33021;&#22815;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#26816;&#27979;&#36229;&#20986;&#35757;&#32451;&#33539;&#30068;&#30340;&#30446;&#26631;&#65292;&#24182;&#20248;&#20110;&#20256;&#32479;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.05493</link><description>&lt;p&gt;
&#24320;&#25918;&#35789;&#27719;&#29289;&#20307;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Multi-Modal Classifiers for Open-Vocabulary Object Detection. (arXiv:2306.05493v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24320;&#25918;&#35789;&#27719;&#29289;&#20307;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#20998;&#31867;&#22120;&#65292;&#33021;&#22815;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#26816;&#27979;&#36229;&#20986;&#35757;&#32451;&#33539;&#30068;&#30340;&#30446;&#26631;&#65292;&#24182;&#20248;&#20110;&#20256;&#32479;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30340;&#30446;&#26631;&#22312;&#20110;&#23454;&#29616;&#24320;&#25918;&#35789;&#27719;&#29289;&#20307;&#26816;&#27979;&#65288;OVOD&#65289;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#35821;&#35328;&#25551;&#36848;&#12289;&#22270;&#20687;&#23454;&#20363;&#25110;&#20004;&#32773;&#30340;&#32452;&#21512;&#65292;&#25351;&#23450;&#26032;&#31867;&#21035;&#30340;&#19977;&#31181;&#26041;&#24335;&#65292;&#24182;&#26500;&#24314;&#20102;&#22522;&#20110;&#25991;&#26412;&#21644;&#22522;&#20110;&#35270;&#35273;&#30340;&#24378;&#22823;&#20998;&#31867;&#22120;&#65292;&#26368;&#32456;&#25552;&#20986;&#19968;&#31181;&#34701;&#21512;&#20102;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#20998;&#31867;&#22120;&#12290;&#22312;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of this paper is open-vocabulary object detection (OVOD) $\unicode{x2013}$ building a model that can detect objects beyond the set of categories seen at training, thus enabling the user to specify categories of interest at inference without the need for model retraining. We adopt a standard two-stage object detector architecture, and explore three ways for specifying novel categories: via language descriptions, via image exemplars, or via a combination of the two. We make three contributions: first, we prompt a large language model (LLM) to generate informative language descriptions for object classes, and construct powerful text-based classifiers; second, we employ a visual aggregator on image exemplars that can ingest any number of images as input, forming vision-based classifiers; and third, we provide a simple method to fuse information from language descriptions and image exemplars, yielding a multi-modal classifier. When evaluating on the challenging LVIS open-vocabulary
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#28201;&#24230;&#25351;&#25968;&#27979;&#24230;&#20026;&#22522;&#30784;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#8212;&#8212;$t$-AdaBoost&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;$t\in [0,1)$&#26102;&#21487;&#20197;&#20445;&#25345;AdaBoost&#30340;&#25351;&#25968;&#25910;&#25947;&#36895;&#29575;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.05487</link><description>&lt;p&gt;
&#24102;&#28201;&#24230;&#25351;&#25968;&#27979;&#24230;&#30340;&#22686;&#24378;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Boosting with Tempered Exponential Measures. (arXiv:2306.05487v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#28201;&#24230;&#25351;&#25968;&#27979;&#24230;&#20026;&#22522;&#30784;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#8212;&#8212;$t$-AdaBoost&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;$t\in [0,1)$&#26102;&#21487;&#20197;&#20445;&#25345;AdaBoost&#30340;&#25351;&#25968;&#25910;&#25947;&#36895;&#29575;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#8212;&#8212;$t$-AdaBoost&#65292;&#23427;&#26159;AdaBoost&#31639;&#27861;&#30340;&#25512;&#24191;&#12290;&#25105;&#20204;&#20351;&#29992;&#20197;&#28201;&#24230;&#21442;&#25968;$t$&#20026;&#32034;&#24341;&#30340;&#28201;&#24230;&#25351;&#25968;&#27979;&#24230;&#65288;TEM&#65289;&#23545;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#24314;&#27169;&#24182;&#35777;&#26126;&#24403;$t\in [0,1)$&#26102;&#65292;&#35813;&#31639;&#27861;&#20445;&#25345;&#20102;AdaBoost&#30340;&#25351;&#25968;&#25910;&#25947;&#36895;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#19968;&#20123;&#23454;&#39564;&#65292;&#23558;&#26412;&#31639;&#27861;&#19982;AdaBoost&#31639;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#23637;&#31034;&#20102;$t$-AdaBoost&#31639;&#27861;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most popular ML algorithms, AdaBoost, can be derived from the dual of a relative entropy minimization problem subject to the fact that the positive weights on the examples sum to one. Essentially, harder examples receive higher probabilities. We generalize this setup to the recently introduced {\it tempered exponential measure}s (TEMs) where normalization is enforced on a specific power of the measure and not the measure itself. TEMs are indexed by a parameter $t$ and generalize exponential families ($t=1$). Our algorithm, $t$-AdaBoost, recovers AdaBoost~as a special case ($t=1$). We show that $t$-AdaBoost retains AdaBoost's celebrated exponential convergence rate when $t\in [0,1)$ while allowing a slight improvement of the rate's hidden constant compared to $t=1$. $t$-AdaBoost partially computes on a generalization of classical arithmetic over the reals and brings notable properties like guaranteed bounded leveraging coefficients for $t\in [0,1)$. From the loss that $t$-Ada
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#29305;&#23450;&#30340;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#21644;&#26679;&#26412;&#37327;&#19978;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#26041;&#27861;&#65292;&#20363;&#22914;&#22312;&#23450;&#21521;&#33829;&#38144;&#20219;&#21153;&#19978;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#26469;&#36798;&#21040;&#19982;RCT&#30456;&#21516;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05484</link><description>&lt;p&gt;
&#20219;&#21153;&#29305;&#23450;&#30340;&#23454;&#39564;&#35774;&#35745;&#29992;&#20110;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Task-specific experimental design for treatment effect estimation. (arXiv:2306.05484v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#29305;&#23450;&#30340;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#21644;&#26679;&#26412;&#37327;&#19978;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#26041;&#27861;&#65292;&#20363;&#22914;&#22312;&#23450;&#21521;&#33829;&#38144;&#20219;&#21153;&#19978;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#26469;&#36798;&#21040;&#19982;RCT&#30456;&#21516;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#22240;&#26524;&#20851;&#31995;&#24212;&#35813;&#26159;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#23454;&#29616;&#30495;&#27491;&#24433;&#21709;&#30340;&#26680;&#24515;&#35201;&#27714;&#12290;&#30001;&#20110;&#21453;&#20107;&#23454;&#30340;&#20869;&#22312;&#19981;&#21487;&#35266;&#27979;&#24615;&#65292;&#22823;&#22411;&#38543;&#26426;&#35797;&#39564;&#65288;RCT&#65289;&#26159;&#22240;&#26524;&#25512;&#26029;&#30340;&#26631;&#20934;&#12290;&#20294;&#22823;&#22411;&#23454;&#39564;&#36890;&#24120;&#20195;&#20215;&#39640;&#26114;&#65292;&#32780;&#38543;&#26426;&#21270;&#26412;&#36523;&#20063;&#20855;&#26377;&#25104;&#26412;&#65292;&#20363;&#22914;&#22312;&#35797;&#39564;&#30340;&#26102;&#20505;&#36827;&#34892;&#27425;&#20248;&#20915;&#31574;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#26356;&#21152;&#26679;&#26412;&#39640;&#25928;&#30340;&#26367;&#20195;&#21697;&#65292;&#20294;&#36825;&#20123;&#26367;&#20195;&#21697;&#19981;&#33021;&#36866;&#24212;&#23547;&#27714;&#22240;&#26524;&#25928;&#24212;&#30340;&#19979;&#28216;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20219;&#21153;&#29305;&#23450;&#30340;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#24182;&#25512;&#23548;&#20986;&#36866;&#29992;&#20110;&#29305;&#23450;&#19979;&#28216;&#24212;&#29992;&#30340;&#37319;&#26679;&#31574;&#30053;&#12290;&#22312;&#19968;&#31995;&#21015;&#37325;&#35201;&#20219;&#21153;&#12289;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#21644;&#26679;&#26412;&#37327;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#65292;&#20363;&#22914;&#22312;&#23450;&#21521;&#33829;&#38144;&#20219;&#21153;&#19978;&#38656;&#35201;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#26356;&#23569;&#25968;&#25454;&#26469;&#19982;RCT&#30340;&#24615;&#33021;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding causality should be a core requirement of any attempt to build real impact through AI. Due to the inherent unobservability of counterfactuals, large randomised trials (RCTs) are the standard for causal inference. But large experiments are generically expensive, and randomisation carries its own costs, e.g. when suboptimal decisions are trialed. Recent work has proposed more sample-efficient alternatives to RCTs, but these are not adaptable to the downstream application for which the causal effect is sought. In this work, we develop a task-specific approach to experimental design and derive sampling strategies customised to particular downstream applications. Across a range of important tasks, real-world datasets, and sample sizes, our method outperforms other benchmarks, e.g. requiring an order-of-magnitude less data to match RCT performance on targeted marketing tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21208;&#25506;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EDE&#30340;&#22522;&#20110;&#20540;&#30340;&#31639;&#27861;&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#22312;Procgen&#21644;Crafter&#31561;&#29615;&#22659;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#32467;&#26524;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#23454;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.05483</link><description>&lt;p&gt;
&#20851;&#20110;&#21208;&#25506;&#22312;&#24378;&#21270;&#23398;&#20064;&#27867;&#21270;&#20013;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Importance of Exploration for Generalization in Reinforcement Learning. (arXiv:2306.05483v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21208;&#25506;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EDE&#30340;&#22522;&#20110;&#20540;&#30340;&#31639;&#27861;&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#22312;Procgen&#21644;Crafter&#31561;&#29615;&#22659;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#32467;&#26524;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#23454;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#27867;&#21270;&#33021;&#21147;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#34920;&#31034;&#23398;&#20064;&#65292;&#32780;&#24573;&#30053;&#20102;&#21208;&#25506;&#31561;&#24378;&#21270;&#23398;&#20064;&#29305;&#26377;&#30340;&#26041;&#38754;&#12290;&#25105;&#20204;&#20551;&#35774;&#20195;&#29702;&#30340;&#21208;&#25506;&#31574;&#30053;&#22312;&#20854;&#27867;&#21270;&#21040;&#26032;&#29615;&#22659;&#30340;&#33021;&#21147;&#20013;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#22312;&#19968;&#20010;&#22522;&#20110;&#34920;&#26684;&#30340;&#24773;&#22659;MDP&#20013;&#23637;&#31034;&#20102;&#21208;&#25506;&#19981;&#20165;&#26377;&#21161;&#20110;&#22312;&#35757;&#32451;&#29615;&#22659;&#20013;&#26377;&#25928;&#22320;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#65292;&#32780;&#19988;&#26377;&#21161;&#20110;&#33719;&#21462;&#30693;&#35782;&#20197;&#20415;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#36827;&#34892;&#20915;&#31574;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EDE&#65306;&#36890;&#36807;&#20998;&#24067;&#24335;&#38598;&#21512;&#23454;&#29616;&#21208;&#25506;&#65292;&#36825;&#26159;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;Q&#20540;&#20998;&#24067;&#30340;&#38598;&#21512;&#40723;&#21169;&#24320;&#21457;&#20855;&#26377;&#39640;&#20808;&#39564;&#19981;&#30830;&#23450;&#24615;&#30340;&#29366;&#24577;&#30340;&#21208;&#25506;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#31532;&#19968;&#20010;&#22312;&#39640;&#32500;&#35266;&#23519;&#20013;&#33719;&#24471;Procgen&#21644;Crafter&#20013;&#27867;&#21270;&#24615;&#26368;&#22909;&#30340;&#20540;&#26041;&#27861;&#12290;&#25552;&#20379;&#20102;&#24320;&#28304;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing approaches for improving generalization in deep reinforcement learning (RL) have mostly focused on representation learning, neglecting RL-specific aspects such as exploration. We hypothesize that the agent's exploration strategy plays a key role in its ability to generalize to new environments. Through a series of experiments in a tabular contextual MDP, we show that exploration is helpful not only for efficiently finding the optimal policy for the training environments but also for acquiring knowledge that helps decision making in unseen environments. Based on these observations, we propose EDE: Exploration via Distributional Ensemble, a method that encourages exploration of states with high epistemic uncertainty through an ensemble of Q-value distributions. Our algorithm is the first value-based approach to achieve state-of-the-art on both Procgen and Crafter, two benchmarks for generalization in RL with high-dimensional observations. The open-sourced implementation can be f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#20219;&#20309;&#38271;&#24230;&#30340;&#35266;&#23519;&#65292;&#24182;&#23454;&#29616;&#20102;&#22312;&#39640;&#36895;&#20844;&#36335;&#21512;&#24182;&#24773;&#22659;&#20013;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2306.05478</link><description>&lt;p&gt;
&#38024;&#23545;&#39640;&#36895;&#20844;&#36335;&#21512;&#27969;&#22330;&#26223;&#30340;&#36816;&#21160;&#35268;&#21010;&#65292;&#22522;&#20110;&#21487;&#21464;&#38271;&#24230;&#35266;&#23519;&#30340;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Trajectory Prediction with Observations of Variable-Length for Motion Planning in Highway Merging scenarios. (arXiv:2306.05478v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#20219;&#20309;&#38271;&#24230;&#30340;&#35266;&#23519;&#65292;&#24182;&#23454;&#29616;&#20102;&#22312;&#39640;&#36895;&#20844;&#36335;&#21512;&#24182;&#24773;&#22659;&#20013;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21160;&#24577;&#39550;&#39542;&#22330;&#26223;&#65288;&#22914;&#39640;&#36895;&#20844;&#36335;&#21512;&#27969;&#65289;&#20013;&#65292;&#20934;&#30830;&#39044;&#27979;&#38468;&#36817;&#36710;&#36742;&#30340;&#36712;&#36857;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#23433;&#20840;&#36816;&#21160;&#35268;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#26080;&#27861;&#23545;&#36710;&#36742;&#36827;&#34892;&#39044;&#27979;&#65292;&#38500;&#38750;&#35266;&#23519;&#20102;&#22266;&#23450;&#26102;&#38271;&#30340;&#20004;&#31186;&#25110;&#20197;&#19978;&#12290;&#36825;&#23548;&#33268;&#36710;&#36742;&#23545;&#20110;&#36827;&#20837;&#20854;&#24863;&#30693;&#33539;&#22260;&#30340;&#36710;&#36742;&#26080;&#27861;&#24555;&#36895;&#21453;&#24212;&#65292;&#20174;&#32780;&#20135;&#29983;&#23433;&#20840;&#38544;&#24739;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#29305;&#21035;&#35757;&#32451;&#20197;&#22788;&#29702;&#35266;&#23519;&#38271;&#24230;&#22823;&#20110;&#19968;&#24103;&#30340;&#20219;&#20309;&#24773;&#20917;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#22823;&#22411;&#39640;&#36895;&#20844;&#36335;&#36712;&#36857;&#25968;&#25454;&#38598;&#65288;highD&#21644;exiD&#65289;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;exiD&#25968;&#25454;&#38598;&#20013;&#30340;&#24191;&#27867;&#21512;&#24182;&#22330;&#26223;&#30740;&#31350;&#20102;&#25152;&#25552;&#20986;&#30340;&#39044;&#27979;&#26041;&#27861;&#23545;&#36816;&#21160;&#35268;&#21010;&#21644;&#25511;&#21046;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#37319;&#29992;&#36825;&#26679;&#19968;&#20010;&#22823;&#22411;&#39640;&#36895;&#20844;&#36335;&#21512;&#27969;&#25968;&#25454;&#38598;&#36827;&#34892;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate trajectory prediction of nearby vehicles is crucial for the safe motion planning of automated vehicles in dynamic driving scenarios such as highway merging. Existing methods cannot initiate prediction for a vehicle unless observed for a fixed duration of two or more seconds. This prevents a fast reaction by the ego vehicle to vehicles that enter its perception range, thus creating safety concerns. Therefore, this paper proposes a novel transformer-based trajectory prediction approach, specifically trained to handle any observation length larger than one frame. We perform a comprehensive evaluation of the proposed method using two large-scale highway trajectory datasets, namely the highD and exiD. In addition, we study the impact of the proposed prediction approach on motion planning and control tasks using extensive merging scenarios from the exiD dataset. To the best of our knowledge, this marks the first instance where such a large-scale highway merging dataset has been empl
&lt;/p&gt;</description></item><item><title>&#24739;&#26377;&#35821;&#35328;&#38556;&#30861;&#30340;&#20154;&#22312;&#29616;&#26377;&#28040;&#36153;&#31867;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20013;&#34920;&#29616;&#19981;&#20339;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20010;&#24615;&#21270;&#30701;&#35821;&#35782;&#21035;&#31995;&#32479;&#29992;&#20110;&#36866;&#24212;&#20854;&#38750;&#20856;&#22411;&#30340;&#35821;&#38899;&#27169;&#24335;&#65292;&#20854;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#19978;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.05446</link><description>&lt;p&gt;
&#24739;&#35821;&#35328;&#38556;&#30861;&#32773;&#30340;&#28508;&#22312;&#30701;&#35821;&#21305;&#37197;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Latent Phrase Matching for Dysarthric Speech. (arXiv:2306.05446v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05446
&lt;/p&gt;
&lt;p&gt;
&#24739;&#26377;&#35821;&#35328;&#38556;&#30861;&#30340;&#20154;&#22312;&#29616;&#26377;&#28040;&#36153;&#31867;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20013;&#34920;&#29616;&#19981;&#20339;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20010;&#24615;&#21270;&#30701;&#35821;&#35782;&#21035;&#31995;&#32479;&#29992;&#20110;&#36866;&#24212;&#20854;&#38750;&#20856;&#22411;&#30340;&#35821;&#38899;&#27169;&#24335;&#65292;&#20854;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#19978;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24456;&#22810;&#28040;&#36153;&#31867;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#19981;&#33021;&#20026;&#35821;&#35328;&#38556;&#30861;&#24739;&#32773;&#25552;&#20379;&#33391;&#22909;&#30340;&#20307;&#39564;&#21644;&#35782;&#21035;&#25928;&#26524;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#35821;&#35328;&#38556;&#30861;&#24773;&#20917;&#26356;&#20026;&#20005;&#37325;&#30340;&#20154;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#20010;&#24615;&#21270;&#35821;&#38899;&#27169;&#22411;&#30340;&#25506;&#32034;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#36866;&#24212;&#38750;&#20856;&#22411;&#30340;&#35821;&#38899;&#27169;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31034;&#20363;&#30340;&#20010;&#24615;&#21270;&#30701;&#35821;&#35782;&#21035;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;&#23569;&#37327;&#30340;&#35821;&#38899;&#36827;&#34892;&#35757;&#32451;&#65292;&#19981;&#20381;&#36182;&#20110;&#20256;&#32479;&#21457;&#38899;&#35789;&#20856;&#65292;&#19981;&#21463;&#19981;&#21516;&#35821;&#35328;&#24433;&#21709;&#65292;&#19988;&#22312;&#21508;&#31181;&#35821;&#38899;&#38556;&#30861;&#24773;&#20917;&#19979;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;32&#21517;&#24739;&#26377;&#35328;&#35821;&#22256;&#38590;&#30340;&#20154;&#21592;&#30340;&#20869;&#37096;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#26041;&#27861;&#19981;&#21463;&#20005;&#37325;&#31243;&#24230;&#24433;&#21709;&#65292;&#19982;&#21830;&#19994;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30456;&#27604;&#65292;&#21484;&#22238;&#29575;&#25552;&#39640;&#20102;60%&#12290;&#22312;&#20844;&#20849;EasyCall&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#31934;&#24230;&#25552;&#39640;&#20102;30.5%&#12290;&#24403;&#35757;&#32451;50&#20010;&#29420;&#29305;&#30701;&#35821;&#26102;&#65292;&#24615;&#33021;&#38543;&#30701;&#35821;&#25968;&#37327;&#22686;&#21152;&#32780;&#19979;&#38477;&#65292;&#20294;&#22987;&#32456;&#20248;&#20110;ASR&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many consumer speech recognition systems are not tuned for people with speech disabilities, resulting in poor recognition and user experience, especially for severe speech differences. Recent studies have emphasized interest in personalized speech models from people with atypical speech patterns. We propose a query-by-example-based personalized phrase recognition system that is trained using small amounts of speech, is language agnostic, does not assume a traditional pronunciation lexicon, and generalizes well across speech difference severities. On an internal dataset collected from 32 people with dysarthria, this approach works regardless of severity and shows a 60% improvement in recall relative to a commercial speech recognition system. On the public EasyCall dataset of dysarthric speech, our approach improves accuracy by 30.5%. Performance degrades as the number of phrases increases, but consistently outperforms ASR systems when trained with 50 unique phrases.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;DiG&#65292;&#36890;&#36807;&#23545;&#20998;&#23376;&#31995;&#32479;&#36827;&#34892;&#25551;&#36848;&#31526;&#30340;&#26465;&#20214;&#65292;&#33021;&#22815;&#39044;&#27979;&#20998;&#23376;&#31995;&#32479;&#30340;&#24179;&#34913;&#20998;&#24067;&#65292;&#20174;&#32780;&#25552;&#20379;&#39640;&#25928;&#29983;&#25104;&#22810;&#26679;&#26500;&#22411;&#21644;&#29366;&#24577;&#23494;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.05445</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#20998;&#23376;&#31995;&#32479;&#30340;&#24179;&#34913;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Towards Predicting Equilibrium Distributions for Molecular Systems with Deep Learning. (arXiv:2306.05445v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;DiG&#65292;&#36890;&#36807;&#23545;&#20998;&#23376;&#31995;&#32479;&#36827;&#34892;&#25551;&#36848;&#31526;&#30340;&#26465;&#20214;&#65292;&#33021;&#22815;&#39044;&#27979;&#20998;&#23376;&#31995;&#32479;&#30340;&#24179;&#34913;&#20998;&#24067;&#65292;&#20174;&#32780;&#25552;&#20379;&#39640;&#25928;&#29983;&#25104;&#22810;&#26679;&#26500;&#22411;&#21644;&#29366;&#24577;&#23494;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#27493;&#22823;&#22823;&#25552;&#39640;&#20102;&#20998;&#23376;&#32467;&#26500;&#39044;&#27979;&#30340;&#31934;&#24230;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#26469;&#35828;&#37325;&#35201;&#30340;&#23439;&#35266;&#35266;&#23519;&#32467;&#26524;&#19981;&#20165;&#20165;&#26159;&#19968;&#20010;&#20998;&#23376;&#32467;&#26500;&#30340;&#20989;&#25968;&#65292;&#32780;&#26159;&#30001;&#32467;&#26500;&#30340;&#24179;&#34913;&#20998;&#24067;&#20915;&#23450;&#30340;&#12290;&#33719;&#24471;&#36825;&#20123;&#20998;&#24067;&#30340;&#20256;&#32479;&#26041;&#27861;&#65292;&#22914;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#65292;&#35745;&#31639;&#20195;&#20215;&#26114;&#36149;&#19988;&#24120;&#24120;&#38590;&#20197;&#22788;&#29702;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;Distributional Graphormer&#65288;DiG&#65289;&#65292;&#20197;&#39044;&#27979;&#20998;&#23376;&#31995;&#32479;&#30340;&#24179;&#34913;&#20998;&#24067;&#12290;&#21463;&#28909;&#21147;&#23398;&#20013;&#30340;&#36864;&#28779;&#36807;&#31243;&#21551;&#21457;&#65292;DiG&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23558;&#19968;&#20010;&#31616;&#21333;&#30340;&#20998;&#24067;&#36716;&#21270;&#20026;&#24179;&#34913;&#20998;&#24067;&#65292;&#26465;&#20214;&#26159;&#23545;&#20998;&#23376;&#31995;&#32479;&#30340;&#25551;&#36848;&#31526;&#65292;&#22914;&#21270;&#23398;&#22270;&#25110;&#34507;&#30333;&#36136;&#24207;&#21015;&#12290;&#36825;&#20010;&#26694;&#26550;&#20351;&#24471;&#21508;&#31181;&#26500;&#22411;&#30340;&#39640;&#25928;&#29983;&#25104;&#25104;&#20026;&#21487;&#33021;&#65292;&#24182;&#25552;&#20379;&#29366;&#24577;&#23494;&#24230;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#21508;&#31181;&#20998;&#23376;&#31995;&#32479;&#30340;&#28909;&#21147;&#23398;&#21644;&#21160;&#21147;&#23398;&#24615;&#36136;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in deep learning have greatly improved structure prediction of molecules. However, many macroscopic observations that are important for real-world applications are not functions of a single molecular structure, but rather determined from the equilibrium distribution of structures. Traditional methods for obtaining these distributions, such as molecular dynamics simulation, are computationally expensive and often intractable. In this paper, we introduce a novel deep learning framework, called Distributional Graphormer (DiG), in an attempt to predict the equilibrium distribution of molecular systems. Inspired by the annealing process in thermodynamics, DiG employs deep neural networks to transform a simple distribution towards the equilibrium distribution, conditioned on a descriptor of a molecular system, such as a chemical graph or a protein sequence. This framework enables efficient generation of diverse conformations and provides estimations of state densities. We demonstrat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#32858;&#31867;&#26041;&#27861;&#65288;CLC&#65289;&#65292;&#23427;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#30452;&#25509;&#23398;&#20064;&#32858;&#31867;&#20998;&#37197;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05439</link><description>&lt;p&gt;
CLC: &#22522;&#20110;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#30340;&#32858;&#31867;&#20998;&#37197;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CLC: Cluster Assignment via Contrastive Representation Learning. (arXiv:2306.05439v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#32858;&#31867;&#26041;&#27861;&#65288;CLC&#65289;&#65292;&#23427;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#30452;&#25509;&#23398;&#20064;&#32858;&#31867;&#20998;&#37197;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#26159;&#19968;&#39033;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#23558;&#26679;&#26412;&#20998;&#32452;&#65292;&#32780;&#19981;&#38656;&#35201;&#25163;&#21160;&#27880;&#37322;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#23545;&#33258;&#30417;&#30563;&#23398;&#20064;&#24471;&#21040;&#30340;&#29305;&#24449;&#34920;&#31034;&#36827;&#34892;&#32858;&#31867;&#65292;&#22312;&#23567;&#22411;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21253;&#21547;&#22823;&#37327;&#32858;&#31867;&#30340;&#25968;&#25454;&#38598;&#65292;&#22914;ImageNet&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#20173;&#28982;&#26080;&#27861;&#23454;&#29616;&#39640;&#32858;&#31867;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#32858;&#31867;&#26041;&#27861;&#65288;CLC&#65289;&#65292;&#23427;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#30452;&#25509;&#23398;&#20064;&#32858;&#31867;&#20998;&#37197;&#12290;&#25105;&#20204;&#23558;&#34920;&#31034;&#20998;&#35299;&#20026;&#20004;&#37096;&#20998;&#65306;&#19968;&#37096;&#20998;&#23545;&#31867;&#21035;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#37319;&#29992;&#31561;&#20998;&#32422;&#26463;&#65292;&#21478;&#19968;&#37096;&#20998;&#25429;&#25417;&#23454;&#20363;&#22240;&#32032;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#25439;&#22833;&#65292;&#20351;&#29992;&#34920;&#31034;&#30340;&#20004;&#20010;&#37096;&#20998;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#25152;&#25552;&#20986;&#30340;&#23545;&#27604;&#25439;&#22833;&#65292;&#24182;&#25581;&#31034;&#20102;CLC&#22312;&#23398;&#20064;&#32858;&#31867;&#20998;&#37197;&#26102;&#20026;&#36127;&#26679;&#26412;&#35774;&#32622;&#19981;&#21516;&#30340;&#26435;&#37325;&#12290;&#36827;&#19968;&#27493;&#30340;&#26799;&#24230;&#20998;&#26512;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;CLC&#26102;&#65292;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clustering remains an important and challenging task of grouping samples into clusters without manual annotations. Recent works have achieved excellent results on small datasets by performing clustering on feature representations learned from self-supervised learning. However, for datasets with a large number of clusters, such as ImageNet, current methods still can not achieve high clustering performance. In this paper, we propose Contrastive Learning-based Clustering (CLC), which uses contrastive learning to directly learn cluster assignment. We decompose the representation into two parts: one encodes the categorical information under an equipartition constraint, and the other captures the instance-wise factors. We propose a contrastive loss using both parts of the representation. We theoretically analyze the proposed contrastive loss and reveal that CLC sets different weights for the negative samples while learning cluster assignments. Further gradient analysis shows that the larger 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36712;&#36857;&#30340;&#20154;&#32676;&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#31867;&#40657;&#30418;&#20248;&#21270;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#23569;&#37327;&#20989;&#25968;&#35780;&#20272;&#19979;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.05438</link><description>&lt;p&gt;
DynamoRep: &#22522;&#20110;&#36712;&#36857;&#30340;&#20154;&#32676;&#23398;&#20064;&#21160;&#21147;&#23398;&#29992;&#20110;&#40657;&#30418;&#20248;&#21270;&#38382;&#39064;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
DynamoRep: Trajectory-Based Population Dynamics for Classification of Black-box Optimization Problems. (arXiv:2306.05438v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36712;&#36857;&#30340;&#20154;&#32676;&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#31867;&#40657;&#30418;&#20248;&#21270;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#23569;&#37327;&#20989;&#25968;&#35780;&#20272;&#19979;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20110;&#20248;&#21270;&#31639;&#27861;&#30340;&#20998;&#26512;&#38656;&#35201;&#20351;&#29992;&#25968;&#20540;&#29305;&#24449;&#26469;&#34920;&#31034;&#20248;&#21270;&#38382;&#39064;&#65292;&#36825;&#20123;&#29305;&#24449;&#21487;&#29992;&#20316;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20837;&#65292;&#35813;&#27169;&#22411;&#34987;&#35757;&#32451;&#29992;&#20110;&#36873;&#25321;&#25110;&#37197;&#32622;&#36866;&#21512;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#29305;&#23450;&#38382;&#39064;&#12290;&#22312;&#32431;&#40657;&#30418;&#20248;&#21270;&#20013;&#65292;&#26377;&#20851;&#38382;&#39064;&#23454;&#20363;&#30340;&#20449;&#24687;&#21482;&#33021;&#36890;&#36807;&#20989;&#25968;&#35780;&#20272;&#33719;&#21462;&#65292;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#19987;&#38376;&#29992;&#20110;&#29305;&#24449;&#25552;&#21462;&#30340;&#21151;&#33021;&#35780;&#20272;&#65292;&#20363;&#22914;&#20351;&#29992;&#38543;&#26426;&#25277;&#26679;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#20004;&#20010;&#20851;&#38190;&#32570;&#28857;&#65306;&#65288;1&#65289;&#23427;&#20943;&#23569;&#20102;&#23454;&#38469;&#20248;&#21270;&#38454;&#27573;&#30340;&#39044;&#31639;&#65292;&#65288;2&#65289;&#23427;&#24573;&#30053;&#20102;&#21487;&#20197;&#20174;&#38382;&#39064;&#27714;&#35299;&#22120;&#20132;&#20114;&#20013;&#33719;&#24471;&#30340;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31616;&#21333;&#25551;&#36848;&#32479;&#35745;&#25551;&#36848;&#20248;&#21270;&#31639;&#27861;&#36712;&#36857;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#29983;&#25104;&#30340;&#29305;&#24449;&#29992;&#20110;&#20174;Black Box Optimization Benchmarking&#65288;BBOB&#65289;&#22871;&#20214;&#20013;&#20998;&#31867;&#38382;&#39064;&#31867;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#38656;&#35201;&#26126;&#26174;&#36739;&#23569;&#30340;&#21151;&#33021;&#35780;&#20272;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of machine learning (ML) models to the analysis of optimization algorithms requires the representation of optimization problems using numerical features. These features can be used as input for ML models that are trained to select or to configure a suitable algorithm for the problem at hand. Since in pure black-box optimization information about the problem instance can only be obtained through function evaluation, a common approach is to dedicate some function evaluations for feature extraction, e.g., using random sampling. This approach has two key downsides: (1) It reduces the budget left for the actual optimization phase, and (2) it neglects valuable information that could be obtained from a problem-solver interaction.  In this paper, we propose a feature extraction method that describes the trajectories of optimization algorithms using simple descriptive statistics. We evaluate the generated features for the task of classifying problem classes from the Black Box Op
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#27493;&#22810;&#35270;&#35282;&#32858;&#31867;&#19982;&#22810;&#26679;&#24615;&#34920;&#24449;&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#35270;&#35282;&#23398;&#20064;&#21644;k-means&#32858;&#31867;&#34701;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#20013;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#21508;&#31181;&#26631;&#20934;&#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#38598;&#19978;&#37117;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.05437</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#32858;&#31867;&#26041;&#27861;&#65306;&#19968;&#27493;&#22810;&#35270;&#35282;&#32858;&#31867;&#19982;&#22810;&#26679;&#24615;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
One-step Multi-view Clustering with Diverse Representation. (arXiv:2306.05437v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#27493;&#22810;&#35270;&#35282;&#32858;&#31867;&#19982;&#22810;&#26679;&#24615;&#34920;&#24449;&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#35270;&#35282;&#23398;&#20064;&#21644;k-means&#32858;&#31867;&#34701;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#20013;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#21508;&#31181;&#26631;&#20934;&#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#38598;&#19978;&#37117;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#32858;&#31867;&#22240;&#20854;&#33021;&#22815;&#21033;&#29992;&#19981;&#21516;&#35270;&#35282;&#30340;&#20449;&#24687;&#26469;&#25552;&#39640;&#25928;&#26524;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#27493;&#22810;&#35270;&#35282;&#32858;&#31867;&#19982;&#22810;&#26679;&#24615;&#34920;&#24449;&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#35270;&#35282;&#23398;&#20064;&#21644;k-means&#32858;&#31867;&#34701;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#26631;&#20934;&#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25928;&#26524;&#21644;&#25928;&#29575;&#26041;&#38754;&#37117;&#20248;&#20110;&#29616;&#26377;&#30340;&#21516;&#31867;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-view clustering has attracted broad attention due to its capacity to utilize consistent and complementary information among views. Although tremendous progress has been made recently, most existing methods undergo high complexity, preventing them from being applied to large-scale tasks. Multi-view clustering via matrix factorization is a representative to address this issue. However, most of them map the data matrices into a fixed dimension, which limits the expressiveness of the model. Moreover, a range of methods suffer from a two-step process, i.e., multimodal learning and the subsequent $k$-means, inevitably causing a sub-optimal clustering result. In light of this, we propose a one-step multi-view clustering with diverse representation method, which incorporates multi-view learning and $k$-means into a unified framework. Specifically, we first project original data matrices into various latent spaces to attain comprehensive information and auto-weight them in a self-supervis
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36808;&#21521;&#31471;&#21040;&#31471;&#35821;&#38899;&#21040;&#25991;&#26412;&#25688;&#35201;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#25216;&#26415;&#21487;&#20197;&#29992;&#20110;&#36807;&#28388;&#21644;&#36319;&#36394;&#27599;&#22825;&#19978;&#20256;&#22312;&#32447;&#30340;&#24191;&#25773;&#26032;&#38395;&#65292;&#24182;&#33021;&#22815;&#20135;&#29983;&#20016;&#23500;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#20294;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#26469;&#25552;&#39640;&#20854;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.05432</link><description>&lt;p&gt;
&#36808;&#21521;&#31471;&#21040;&#31471;&#35821;&#38899;&#21040;&#25991;&#26412;&#25688;&#35201;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Towards End-to-end Speech-to-text Summarization. (arXiv:2306.05432v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05432
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36808;&#21521;&#31471;&#21040;&#31471;&#35821;&#38899;&#21040;&#25991;&#26412;&#25688;&#35201;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#25216;&#26415;&#21487;&#20197;&#29992;&#20110;&#36807;&#28388;&#21644;&#36319;&#36394;&#27599;&#22825;&#19978;&#20256;&#22312;&#32447;&#30340;&#24191;&#25773;&#26032;&#38395;&#65292;&#24182;&#33021;&#22815;&#20135;&#29983;&#20016;&#23500;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#20294;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#26469;&#25552;&#39640;&#20854;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#21040;&#25991;&#26412;&#65288;S2T&#65289;&#25688;&#35201;&#26159;&#19968;&#31181;&#33410;&#30465;&#26102;&#38388;&#30340;&#25216;&#26415;&#65292;&#21487;&#29992;&#20110;&#36807;&#28388;&#21644;&#36319;&#36394;&#27599;&#22825;&#19978;&#20256;&#22312;&#32447;&#30340;&#24191;&#25773;&#26032;&#38395;&#12290;&#26368;&#36817;&#28145;&#24230;&#23398;&#20064;&#20013;&#20986;&#29616;&#20102;&#20855;&#26377;&#20986;&#33394;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24341;&#36215;&#20102;&#23545;&#20135;&#29983;&#31616;&#27905;&#25991;&#26723;&#29256;&#26412;&#30340;&#25688;&#35201;&#31995;&#32479;&#30340;&#30740;&#31350;&#37325;&#28857;&#65292;&#20063;&#31216;&#20026;&#25277;&#35937;&#25688;&#35201;&#12290;&#31471;&#21040;&#31471;&#65288;E2E&#65289;&#24314;&#27169;&#30340;S2T&#25277;&#35937;&#25688;&#35201;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#23427;&#25552;&#20379;&#20102;&#20135;&#29983;&#20016;&#23500;&#28508;&#22312;&#34920;&#31034;&#30340;&#21487;&#33021;&#24615;&#65292;&#36825;&#20123;&#34920;&#31034;&#21033;&#29992;&#20102;&#38750;&#35821;&#35328;&#21644;&#22768;&#23398;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#20165;&#20351;&#29992;&#32423;&#32852;&#31995;&#32479;&#20013;&#33258;&#21160;&#29983;&#25104;&#30340;&#36716;&#24405;&#25991;&#26412;&#30340;&#35821;&#35328;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#20851;&#20110;&#36825;&#39033;&#20219;&#21153;&#30340;E2E&#24314;&#27169;&#24456;&#23569;&#25506;&#32034;&#19981;&#21516;&#30340;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#24191;&#25773;&#26032;&#38395;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39046;&#22495;&#65292;&#27599;&#22825;&#21521;&#29992;&#25143;&#21576;&#29616;&#22823;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#37319;&#29992;&#32423;&#32852;&#32763;&#35793;&#26041;&#27861;&#21644;&#31471;&#21040;&#31471;&#26041;&#27861;&#24314;&#27169;S2T&#25688;&#35201;&#65292;&#24182;&#22312;&#24191;&#25773;&#26032;&#38395;&#30340;&#26032;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;E2E&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#28508;&#21147;&#65292;&#20294;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#26469;&#25552;&#39640;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech-to-text (S2T) summarization is a time-saving technique for filtering and keeping up with the broadcast news uploaded online on a daily basis. The rise of large language models from deep learning with impressive text generation capabilities has placed the research focus on summarization systems that produce paraphrased compact versions of the document content, also known as abstractive summaries. End-to-end (E2E) modelling of S2T abstractive summarization is a promising approach that offers the possibility of generating rich latent representations that leverage non-verbal and acoustic information, as opposed to the use of only linguistic information from automatically generated transcripts in cascade systems. However, the few literature on E2E modelling of this task fails on exploring different domains, namely broadcast news, which is challenging domain where large and diversified volumes of data are presented to the user every day. We model S2T summarization both with a cascade 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26230;&#20307;&#29305;&#24322;&#24615;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#35774;&#35745;&#20102;&#19968;&#31181;&#20114;&#26021;&#25513;&#30721;&#31574;&#30053;&#65292;&#32531;&#35299;&#20102;&#26230;&#20307;&#29289;&#24615;&#39044;&#27979;&#20013;&#23384;&#22312;&#30340;&#26631;&#35760;&#21463;&#38480;&#38382;&#39064;&#65292;&#24182;&#22686;&#24378;&#20102;&#34920;&#31034;&#23398;&#20064;&#12290;&#21516;&#26102;&#65292;&#35813;&#26694;&#26550;&#36824;&#32771;&#34385;&#20102;&#26230;&#20307;&#32467;&#26500;&#20013;&#30340;&#21608;&#26399;&#24615;&#19981;&#21464;&#24615;&#65292;&#24320;&#21457;&#20102;&#21608;&#26399;&#24615;&#19981;&#21464;&#30340;&#22810;&#22270;&#27169;&#22359;&#21644;&#21608;&#26399;&#29305;&#24615;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2306.05344</link><description>&lt;p&gt;
&#27700;&#26230;&#26448;&#26009;&#24615;&#33021;&#39044;&#27979;&#30340;&#26230;&#20307;&#29305;&#24322;&#24615;&#39044;&#35757;&#32451;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Crystal-Specific Pre-Training Framework for Crystal Material Property Prediction. (arXiv:2306.05344v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26230;&#20307;&#29305;&#24322;&#24615;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#35774;&#35745;&#20102;&#19968;&#31181;&#20114;&#26021;&#25513;&#30721;&#31574;&#30053;&#65292;&#32531;&#35299;&#20102;&#26230;&#20307;&#29289;&#24615;&#39044;&#27979;&#20013;&#23384;&#22312;&#30340;&#26631;&#35760;&#21463;&#38480;&#38382;&#39064;&#65292;&#24182;&#22686;&#24378;&#20102;&#34920;&#31034;&#23398;&#20064;&#12290;&#21516;&#26102;&#65292;&#35813;&#26694;&#26550;&#36824;&#32771;&#34385;&#20102;&#26230;&#20307;&#32467;&#26500;&#20013;&#30340;&#21608;&#26399;&#24615;&#19981;&#21464;&#24615;&#65292;&#24320;&#21457;&#20102;&#21608;&#26399;&#24615;&#19981;&#21464;&#30340;&#22810;&#22270;&#27169;&#22359;&#21644;&#21608;&#26399;&#29305;&#24615;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#26230;&#29289;&#24615;&#39044;&#27979;&#26159;&#24320;&#21457;&#26032;&#26448;&#26009;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#21152;&#36895;&#26230;&#20307;&#30340;&#30740;&#31350;&#65292;&#38656;&#35201;&#35299;&#20915;&#20004;&#20010;&#25216;&#26415;&#38590;&#39064;&#65306;&#20854;&#19968;&#65292;&#26631;&#35760;&#26230;&#20307;&#29289;&#24615;&#26412;&#36136;&#19978;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#38656;&#35201;&#36827;&#34892;&#26114;&#36149;&#32791;&#26102;&#30340;&#29289;&#29702;&#27169;&#25311;&#25110;&#23454;&#39564;&#65307;&#20854;&#20108;&#65292;&#26230;&#20307;&#36981;&#24490;&#29305;&#23450;&#30340;&#37327;&#23376;&#21270;&#23398;&#21407;&#29702;&#65292;&#21363;&#21608;&#26399;&#24615;&#19981;&#21464;&#24615;&#65292;&#24448;&#24448;&#26080;&#27861;&#34987;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25429;&#25417;&#21040;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26230;&#20307;&#29305;&#24322;&#24615;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#26230;&#20307;&#34920;&#31034;&#12290;&#26694;&#26550;&#35774;&#35745;&#20102;&#19968;&#31181;&#20114;&#26021;&#25513;&#30721;&#31574;&#30053;&#65292;&#22686;&#24378;&#20102;&#34920;&#31034;&#23398;&#20064;&#65292;&#32531;&#35299;&#20102;&#26230;&#20307;&#29289;&#24615;&#39044;&#27979;&#20013;&#23384;&#22312;&#30340;&#26631;&#35760;&#21463;&#38480;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#26230;&#20307;&#32467;&#26500;&#20013;&#30340;&#21608;&#26399;&#24615;&#19981;&#21464;&#24615;&#65292;&#24320;&#21457;&#20102;&#21608;&#26399;&#24615;&#19981;&#21464;&#30340;&#22810;&#22270;&#27169;&#22359;&#21644;&#21608;&#26399;&#29305;&#24615;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Crystal property prediction is a crucial aspect of developing novel materials. However, there are two technical challenges to be addressed for speeding up the investigation of crystals. First, labeling crystal properties is intrinsically difficult due to the high cost and time involved in physical simulations or lab experiments. Second, crystals adhere to a specific quantum chemical principle known as periodic invariance, which is often not captured by existing machine learning methods. To overcome these challenges, we propose the crystal-specific pre-training framework for learning crystal representations with self-supervision. The framework designs a mutex mask strategy for enhancing representation learning so as to alleviate the limited labels available for crystal property prediction. Moreover, we take into account the specific periodic invariance in crystal structures by developing a periodic invariance multi-graph module and periodic attribute learning within our framework. This 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#30340;&#32852;&#37030;&#32447;&#24615;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#20026;CDP&#25552;&#20986;&#20102;&#36817;&#20046;&#26368;&#20248;&#30340;&#32852;&#37030;&#31639;&#27861;\robin&#65292;&#22312;LDP&#19979;&#35777;&#26126;&#20102;&#23398;&#20064;&#24517;&#39035;&#25215;&#21463;&#33267;&#23569;&#19968;&#20010;&#36951;&#25022;&#33192;&#32960;&#22240;&#23376;&#12290;</title><link>http://arxiv.org/abs/2306.05275</link><description>&lt;p&gt;
&#24102;&#26377;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#30340;&#32852;&#37030;&#32447;&#24615;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Linear Contextual Bandits with User-level Differential Privacy. (arXiv:2306.05275v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#30340;&#32852;&#37030;&#32447;&#24615;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#20026;CDP&#25552;&#20986;&#20102;&#36817;&#20046;&#26368;&#20248;&#30340;&#32852;&#37030;&#31639;&#27861;\robin&#65292;&#22312;LDP&#19979;&#35777;&#26126;&#20102;&#23398;&#20064;&#24517;&#39035;&#25215;&#21463;&#33267;&#23569;&#19968;&#20010;&#36951;&#25022;&#33192;&#32960;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#27010;&#24565;&#19979;&#30340;&#32852;&#37030;&#32447;&#24615;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#36866;&#24212;&#39034;&#24207;&#20915;&#31574;&#35774;&#32622;&#20013;DP&#30340;&#21508;&#31181;&#23450;&#20041;&#12290;&#28982;&#21518;&#22312;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#27491;&#24335;&#24341;&#20837;&#20102;&#29992;&#25143;&#32423;&#20013;&#24515;DP&#21644;&#26412;&#22320;DP&#65292;&#24182;&#30740;&#31350;&#20102;&#32852;&#37030;&#32447;&#24615;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#20013;&#23398;&#20064;&#36951;&#25022;&#21644;&#30456;&#24212;DP&#20445;&#35777;&#20043;&#38388;&#30340;&#22522;&#26412;&#26435;&#34913;&#12290;&#23545;&#20110;CDP&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;\robin&#30340;&#32852;&#37030;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#25512;&#23548;&#22312;&#28385;&#36275;&#29992;&#25143;&#32423;DP&#26102;&#30340;&#20960;&#20046;&#21305;&#37197;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#36951;&#25022;&#30028;&#65292;&#35777;&#26126;&#20854;&#22312;&#23458;&#25143;&#31471;&#25968;&#37327;$M$&#21644;&#38544;&#31169;&#39044;&#31639;$\varepsilon$&#26041;&#38754;&#26159;&#36817;&#20046;&#26368;&#20248;&#30340;&#12290;&#23545;&#20110;LDP&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#20960;&#20010;&#19979;&#30028;&#65292;&#34920;&#26126;&#22312;&#29992;&#25143;&#32423;$(\varepsilon,\delta)$-LDP&#19979;&#23398;&#20064;&#24517;&#39035;&#33267;&#23569;&#25215;&#21463;&#19968;&#20010;&#36951;&#25022;&#33192;&#32960;&#22240;&#23376;&#33267;&#23569;&#20026;{$\min\{1/\varepsilon,M\}$&#25110;$\min\{1/\sqrt{\varepsilon},\sq
&lt;/p&gt;
&lt;p&gt;
This paper studies federated linear contextual bandits under the notion of user-level differential privacy (DP). We first introduce a unified federated bandits framework that can accommodate various definitions of DP in the sequential decision-making setting. We then formally introduce user-level central DP (CDP) and local DP (LDP) in the federated bandits framework, and investigate the fundamental trade-offs between the learning regrets and the corresponding DP guarantees in a federated linear contextual bandits model. For CDP, we propose a federated algorithm termed as \robin and show that it is near-optimal in terms of the number of clients $M$ and the privacy budget $\varepsilon$ by deriving nearly-matching upper and lower regret bounds when user-level DP is satisfied. For LDP, we obtain several lower bounds, indicating that learning under user-level $(\varepsilon,\delta)$-LDP must suffer a regret blow-up factor at least {$\min\{1/\varepsilon,M\}$ or $\min\{1/\sqrt{\varepsilon},\sq
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#32858;&#31867;&#27969;&#31243;&#65292;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24378;&#22823;&#29305;&#24449;&#34920;&#31034;&#65292;&#22312;&#35268;&#27169;&#19978;&#26377;&#25928;&#22320;&#23545;&#22270;&#20687;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#36895;&#29575;&#38477;&#20302;&#30446;&#26631;&#21644; CLIP &#30340;&#22270;&#20687;-&#25991;&#26412;&#32465;&#23450;&#65292;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#32858;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#33258;&#26631;&#35760;&#31639;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.05272</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36895;&#29575;&#38477;&#20302;&#21407;&#21017;&#36827;&#34892;&#22270;&#20687;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Image Clustering via the Principle of Rate Reduction in the Age of Pretrained Models. (arXiv:2306.05272v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#32858;&#31867;&#27969;&#31243;&#65292;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24378;&#22823;&#29305;&#24449;&#34920;&#31034;&#65292;&#22312;&#35268;&#27169;&#19978;&#26377;&#25928;&#22320;&#23545;&#22270;&#20687;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#36895;&#29575;&#38477;&#20302;&#30446;&#26631;&#21644; CLIP &#30340;&#22270;&#20687;-&#25991;&#26412;&#32465;&#23450;&#65292;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#32858;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#33258;&#26631;&#35760;&#31639;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20986;&#29616;&#24050;&#32463;&#22312;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#24102;&#26469;&#20102;&#33539;&#24335;&#36716;&#21464;&#65292;&#20294;&#26159;&#32858;&#31867;&#26410;&#26631;&#35760;&#30340;&#22270;&#20687;&#20316;&#20026;&#19968;&#31181;&#22522;&#26412;&#21644;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#20173;&#28982;&#32570;&#20047;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#32858;&#31867;&#27969;&#31243;&#65292;&#21033;&#29992; CLIP &#31561;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24378;&#22823;&#29305;&#24449;&#34920;&#31034;&#65292;&#22312;&#35268;&#27169;&#19978;&#26377;&#25928;&#22320;&#23545;&#22270;&#20687;&#36827;&#34892;&#32858;&#31867;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#39044;&#35757;&#32451;&#29305;&#24449;&#36890;&#36807;&#36827;&#19968;&#27493;&#20248;&#21270;&#36895;&#29575;&#38477;&#20302;&#30446;&#26631;&#65292;&#26356;&#20855;&#26377;&#32467;&#26500;&#24615;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#29305;&#24449;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#32858;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#20363;&#22914;&#20174; ImageNet-1k &#30340; 57&#65285;&#25552;&#39640;&#21040; 66&#65285;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21033;&#29992; CLIP &#30340;&#22270;&#20687;-&#25991;&#26412;&#32465;&#23450;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26032;&#30340;&#32858;&#31867;&#26041;&#27861;&#22914;&#20309;&#23548;&#33268;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33258;&#26631;&#35760;&#31639;&#27861;&#65292;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#26410;&#26631;&#35760;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914; MS-COCO &#21644; LAION-Aesthetics&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of large pre-trained models has brought about a paradigm shift in both visual representation learning and natural language processing. However, clustering unlabeled images, as a fundamental and classic machine learning problem, still lacks effective solution, particularly for large-scale datasets. In this paper, we propose a novel image clustering pipeline that leverages the powerful feature representation of large pre-trained models such as CLIP and cluster images effectively and efficiently at scale. We show that the pre-trained features are significantly more structured by further optimizing the rate reduction objective. The resulting features may significantly improve the clustering accuracy, e.g., from 57\% to 66\% on ImageNet-1k. Furthermore, by leveraging CLIP's image-text binding, we show how the new clustering method leads to a simple yet effective self-labeling algorithm that successfully works on unlabeled large datasets such as MS-COCO and LAION-Aesthetics. We wi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#36716;&#31227;&#23398;&#20064;&#22312;&#19981;&#21516;&#30456;&#20284;&#24230;&#26465;&#20214;&#19979;&#30340;&#34920;&#29616;&#65292;&#20351;&#29992;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20998;&#26512;&#20102;&#20004;&#31181;&#21442;&#25968;&#20256;&#36882;&#36873;&#39033;&#30340;&#27604;&#36739;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#27169;&#22411;&#35823;&#24046;&#30340;&#29305;&#24449;&#26469;&#26816;&#39564;&#27867;&#21270;&#24615;&#33021;&#22914;&#20309;&#38543;&#30528;&#21442;&#25968;&#25968;&#37327;&#30340;&#21464;&#21270;&#32780;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.04901</link><description>&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#33021;&#65306;&#36807;&#21442;&#25968;&#21270;&#21644;&#27424;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Generalization Performance of Transfer Learning: Overparameterized and Underparameterized Regimes. (arXiv:2306.04901v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04901
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#36716;&#31227;&#23398;&#20064;&#22312;&#19981;&#21516;&#30456;&#20284;&#24230;&#26465;&#20214;&#19979;&#30340;&#34920;&#29616;&#65292;&#20351;&#29992;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20998;&#26512;&#20102;&#20004;&#31181;&#21442;&#25968;&#20256;&#36882;&#36873;&#39033;&#30340;&#27604;&#36739;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#27169;&#22411;&#35823;&#24046;&#30340;&#29305;&#24449;&#26469;&#26816;&#39564;&#27867;&#21270;&#24615;&#33021;&#22914;&#20309;&#38543;&#30528;&#21442;&#25968;&#25968;&#37327;&#30340;&#21464;&#21270;&#32780;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#26159;&#19968;&#31181;&#20351;&#29992;&#28304;&#20219;&#21153;&#20013;&#33719;&#24471;&#30340;&#30693;&#35782;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#30446;&#26631;&#20219;&#21153;&#65292;&#20197;&#23454;&#29616;&#25913;&#21892;&#24615;&#33021;&#21644;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#30340;&#26377;&#29992;&#25216;&#26415;&#12290;&#35780;&#20272;&#36716;&#31227;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#20381;&#36182;&#20110;&#29702;&#35299;&#28304;&#20219;&#21153;&#21644;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#20219;&#21153;&#36890;&#24120;&#34920;&#29616;&#20986;&#37096;&#20998;&#30456;&#20284;&#24615;&#65292;&#20854;&#20013;&#26576;&#20123;&#26041;&#38754;&#30456;&#20284;&#32780;&#21478;&#19968;&#20123;&#26041;&#38754;&#21017;&#19981;&#21516;&#25110;&#26080;&#20851;&#12290;&#20026;&#20102;&#30740;&#31350;&#37096;&#20998;&#30456;&#20284;&#24615;&#23545;&#36716;&#31227;&#23398;&#20064;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20855;&#26377;&#20004;&#32452;&#19981;&#21516;&#29305;&#24449;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65306;&#22312;&#20219;&#21153;&#38388;&#20849;&#20139;&#30340;&#20844;&#20849;&#37096;&#20998;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#37096;&#20998;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#36716;&#31227;&#23398;&#20064;&#65292;&#28085;&#30422;&#20102;&#20004;&#20010;&#21442;&#25968;&#20256;&#36882;&#36873;&#39033;&#12290;&#36890;&#36807;&#24314;&#31435;&#23398;&#20064;&#27169;&#22411;&#35823;&#24046;&#30340;&#29702;&#35770;&#29305;&#24449;&#65292;&#25105;&#20204;&#27604;&#36739;&#36825;&#20123;&#36716;&#31227;&#23398;&#20064;&#36873;&#39033;&#65292;&#29305;&#21035;&#20851;&#27880;&#27867;&#21270;&#24615;&#33021;&#22914;&#20309;&#38543;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#30340;&#21464;&#21270;&#32780;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning is a useful technique for achieving improved performance and reducing training costs by leveraging the knowledge gained from source tasks and applying it to target tasks. Assessing the effectiveness of transfer learning relies on understanding the similarity between the ground truth of the source and target tasks. In real-world applications, tasks often exhibit partial similarity, where certain aspects are similar while others are different or irrelevant. To investigate the impact of partial similarity on transfer learning performance, we focus on a linear regression model with two distinct sets of features: a common part shared across tasks and a task-specific part. Our study explores various types of transfer learning, encompassing two options for parameter transfer. By establishing a theoretical characterization on the error of the learned model, we compare these transfer learning options, particularly examining how generalization performance changes with the numbe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#21435;&#22122;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#20174;&#32593;&#32476;&#30340;&#35282;&#24230;&#29983;&#25104;&#22478;&#24066;&#20840;&#22495;&#30340;&#36215;&#28857;&#19982;&#32456;&#28857;&#30697;&#38453;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#21306;&#22495;&#23618;&#38754;&#30340;&#22478;&#24066;&#29305;&#24449;&#26469;&#35774;&#35745;&#20986;&#22270;&#21435;&#22122;&#25193;&#25955;&#26041;&#27861;&#30340;&#26465;&#20214;&#27010;&#29575;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2306.04873</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#21435;&#22122;&#25193;&#25955;&#30340;&#22478;&#24066;&#20840;&#22495;&#36215;&#28857;&#19982;&#32456;&#28857;&#30697;&#38453;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
City-wide Origin-Destination Matrix Generation via Graph Denoising Diffusion. (arXiv:2306.04873v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#21435;&#22122;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#20174;&#32593;&#32476;&#30340;&#35282;&#24230;&#29983;&#25104;&#22478;&#24066;&#20840;&#22495;&#30340;&#36215;&#28857;&#19982;&#32456;&#28857;&#30697;&#38453;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#21306;&#22495;&#23618;&#38754;&#30340;&#22478;&#24066;&#29305;&#24449;&#26469;&#35774;&#35745;&#20986;&#22270;&#21435;&#22122;&#25193;&#25955;&#26041;&#27861;&#30340;&#26465;&#20214;&#27010;&#29575;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36215;&#28857;&#32456;&#28857;&#30697;&#38453;&#20272;&#35745;&#20102;&#21306;&#22495;&#20043;&#38388;&#30340;&#20986;&#34892;&#20154;&#25968;&#65292;&#21363;&#22478;&#24066;&#20013;&#30340;&#20154;&#27969;&#37327;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#22478;&#24066;&#35268;&#21010;&#12289;&#20132;&#36890;&#31561;&#39046;&#22495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#21435;&#22122;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#20174;&#32593;&#32476;&#30340;&#35282;&#24230;&#29983;&#25104;&#22478;&#24066;&#20840;&#22495;&#30340;&#36215;&#28857;&#19982;&#32456;&#28857;&#30697;&#38453;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#21306;&#22495;&#23618;&#38754;&#30340;&#22478;&#24066;&#29305;&#24449;&#26469;&#35774;&#35745;&#20986;&#22270;&#21435;&#22122;&#25193;&#25955;&#26041;&#27861;&#30340;&#26465;&#20214;&#27010;&#29575;&#20998;&#24067;&#12290;&#20026;&#20102;&#20811;&#26381;&#28085;&#30422;&#25968;&#21315;&#20010;&#21306;&#22495;&#30340;&#22478;&#24066;&#20840;&#22495;&#36215;&#28857;&#21644;&#32456;&#28857;&#30697;&#38453;&#30340;&#23398;&#20064;&#38590;&#24230;&#65292;&#25105;&#20204;&#23558;&#20854;&#20998;&#35299;&#25104;&#20102;&#33509;&#24178;&#23567;&#22359;&#12289;&#29420;&#31435;&#30340;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Origin-Destination~(OD) matrix provides an estimation of number of individuals traveling between regions, i.e., mobility flow in the city, which is widely-used in urban planning, transportation, etc. Given various city characteristics of urban regions, generating the city-wide OD matrix without using historical flow information has become increasingly appealing to both researchers and practitioners. However, existing works are limited in independent generation of each element, i.e., flow, in OD matrix, overlooking the element relations within the matrix that can be well formulated as a network. In this paper, we instead propose to generate the city-wide OD matrix from the network perspective, and design a graph denoising diffusion method to learn the conditional joint probability distribution of all elements in the OD matrix given city characteristics at region level. To overcome the learning difficulty of the city-wide OD matrix covering over thousands of regions, we decompose the
&lt;/p&gt;</description></item><item><title>SLCE&#26159;&#29992;&#32447;&#24615;&#21464;&#25442;&#23558;&#28857;&#37325;&#26500;&#20026;&#31867;&#21035;&#36136;&#24515;&#24182;&#20351;&#29992;$\ell_1$&#33539;&#25968;&#24809;&#32602;&#20174;&#36755;&#20837;&#25968;&#25454;&#20013;&#28388;&#38500;&#19981;&#24517;&#35201;&#29305;&#24449;&#30340;&#29305;&#24449;&#36873;&#25321;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20026;&#22810;&#31867;&#25968;&#25454;&#20351;&#29992;&#21333;&#20010;&#27169;&#22411;&#65292;&#20855;&#26377;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#26174;&#30528;&#36739;&#23569;&#30340;&#29305;&#24449;&#25968;&#12290;</title><link>http://arxiv.org/abs/2306.04824</link><description>&lt;p&gt;
&#31232;&#30095;&#32447;&#24615;&#36136;&#24515;&#32534;&#30721;&#22120;&#65306;&#19968;&#31181;&#29305;&#24449;&#36873;&#25321;&#30340;&#20984;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sparse Linear Centroid-Encoder: A Convex Method for Feature Selection. (arXiv:2306.04824v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04824
&lt;/p&gt;
&lt;p&gt;
SLCE&#26159;&#29992;&#32447;&#24615;&#21464;&#25442;&#23558;&#28857;&#37325;&#26500;&#20026;&#31867;&#21035;&#36136;&#24515;&#24182;&#20351;&#29992;$\ell_1$&#33539;&#25968;&#24809;&#32602;&#20174;&#36755;&#20837;&#25968;&#25454;&#20013;&#28388;&#38500;&#19981;&#24517;&#35201;&#29305;&#24449;&#30340;&#29305;&#24449;&#36873;&#25321;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20026;&#22810;&#31867;&#25968;&#25454;&#20351;&#29992;&#21333;&#20010;&#27169;&#22411;&#65292;&#20855;&#26377;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#26174;&#30528;&#36739;&#23569;&#30340;&#29305;&#24449;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#36873;&#25321;&#25216;&#26415;&#65292;&#31216;&#20026;&#31232;&#30095;&#32447;&#24615;&#36136;&#24515;&#32534;&#30721;&#22120;&#65288;SLCE&#65289;&#12290;&#35813;&#31639;&#27861;&#20351;&#29992;&#19968;&#20010;&#32447;&#24615;&#21464;&#25442;&#23558;&#19968;&#20010;&#28857;&#37325;&#26500;&#20026;&#20854;&#31867;&#21035;&#30340;&#36136;&#24515;&#65292;&#24182;&#21516;&#26102;&#20351;&#29992;$\ell_1$&#33539;&#25968;&#24809;&#32602;&#20174;&#36755;&#20837;&#25968;&#25454;&#20013;&#28388;&#38500;&#19981;&#24517;&#35201;&#30340;&#29305;&#24449;&#12290;&#20248;&#21270;&#38382;&#39064;&#30340;&#21407;&#22987;&#20844;&#24335;&#26159;&#38750;&#20984;&#30340;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;&#27861;&#65292;&#20854;&#20013;&#27599;&#19968;&#27493;&#37117;&#26159;&#20984;&#30340;&#12290;&#22312;&#31532;&#19968;&#27493;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#32447;&#24615;&#36136;&#24515;&#32534;&#30721;&#22120;&#65292;&#23427;&#26159;&#19968;&#20010;&#30697;&#38453;$A$&#19978;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#31532;&#20108;&#27493;&#20013;&#65292;&#25105;&#20204;&#21482;&#22312;&#23545;&#35282;&#32447;&#30697;&#38453;$B$&#19978;&#25628;&#32034;&#31232;&#30095;&#35299;&#65292;&#21516;&#26102;&#20445;&#25345;$A$&#19981;&#21464;&#12290;&#19982;&#20854;&#20182;&#32447;&#24615;&#26041;&#27861;&#65288;&#20363;&#22914;&#31232;&#30095;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;Lasso&#65289;&#19981;&#21516;&#65292;&#31232;&#30095;&#32447;&#24615;&#36136;&#24515;&#32534;&#30721;&#22120;&#23545;&#20110;&#22810;&#31867;&#25968;&#25454;&#20351;&#29992;&#21333;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#23545;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#24182;&#34920;&#26126;&#23427;&#20419;&#36827;&#20102;&#21508;&#31181;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#39640;&#32500;&#29983;&#29289;&#25968;&#25454;&#65289;&#19978;&#30340;&#31232;&#30095;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SLCE&#22312;&#20998;&#31867;&#20934;&#30830;&#29575;&#26041;&#38754;&#20855;&#26377;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#26174;&#30528;&#36739;&#23569;&#30340;&#29305;&#24449;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel feature selection technique, Sparse Linear Centroid-Encoder (SLCE). The algorithm uses a linear transformation to reconstruct a point as its class centroid and, at the same time, uses the $\ell_1$-norm penalty to filter out unnecessary features from the input data. The original formulation of the optimization problem is nonconvex, but we propose a two-step approach, where each step is convex. In the first step, we solve the linear Centroid-Encoder, a convex optimization problem over a matrix $A$. In the second step, we only search for a sparse solution over a diagonal matrix $B$ while keeping $A$ fixed. Unlike other linear methods, e.g., Sparse Support Vector Machines and Lasso, Sparse Linear Centroid-Encoder uses a single model for multi-class data. We present an in-depth empirical analysis of the proposed model and show that it promotes sparsity on various data sets, including high-dimensional biological data. Our experimental results show that SLCE has a performan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#26435;&#37325;&#23545;&#31216;&#30340;&#26377;&#30417;&#30563;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#29289;&#21512;&#29702;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#30456;&#20851;&#20449;&#24687;&#26368;&#22823;&#21270;&#22312;&#23618;&#28608;&#27963;&#20043;&#38388;&#25551;&#36848;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20449;&#21495;&#20256;&#25773;&#12290;&#36890;&#36807;&#22352;&#26631;&#19979;&#38477;&#20248;&#21270;&#30456;&#24212;&#30340;&#30446;&#26631;&#21644;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#20135;&#29983;&#19968;&#20010;&#26356;&#29983;&#29289;&#30495;&#23454;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2306.04810</link><description>&lt;p&gt;
&#30456;&#20851;&#20449;&#24687;&#26368;&#22823;&#21270;&#65306;&#19968;&#31181;&#26080;&#38656;&#26435;&#37325;&#23545;&#31216;&#30340;&#26377;&#30417;&#30563;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#29289;&#21512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Correlative Information Maximization: A Biologically Plausible Approach to Supervised Deep Neural Networks without Weight Symmetry. (arXiv:2306.04810v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#26435;&#37325;&#23545;&#31216;&#30340;&#26377;&#30417;&#30563;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#29289;&#21512;&#29702;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#30456;&#20851;&#20449;&#24687;&#26368;&#22823;&#21270;&#22312;&#23618;&#28608;&#27963;&#20043;&#38388;&#25551;&#36848;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20449;&#21495;&#20256;&#25773;&#12290;&#36890;&#36807;&#22352;&#26631;&#19979;&#38477;&#20248;&#21270;&#30456;&#24212;&#30340;&#30446;&#26631;&#21644;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#20135;&#29983;&#19968;&#20010;&#26356;&#29983;&#29289;&#30495;&#23454;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#22312;&#35757;&#32451;&#22823;&#35268;&#27169;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#20854;&#29983;&#29289;&#21512;&#29702;&#24615;&#21463;&#21040;&#20105;&#35758;&#65292;&#29616;&#22312;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#65292;&#21363;&#22823;&#33041;&#26159;&#21542;&#37319;&#29992;&#31867;&#20284;&#20110;&#23427;&#30340;&#30417;&#30563;&#23398;&#20064;&#26426;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#23618;&#28608;&#27963;&#20043;&#38388;&#36827;&#34892;&#30456;&#20851;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#26367;&#20195;&#35268;&#33539;&#26041;&#27861;&#65292;&#20197;&#25551;&#36848;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#20013;&#20449;&#21495;&#22312;&#21069;&#21521;&#21644;&#21518;&#21521;&#26041;&#21521;&#19978;&#20256;&#25773;&#30340;&#26426;&#21046;&#12290;&#36825;&#31181;&#26032;&#26694;&#26550;&#35299;&#20915;&#20102;&#26377;&#20851;&#20256;&#32479;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#29983;&#29289;&#21512;&#29702;&#24615;&#30340;&#35768;&#22810;&#38382;&#39064;&#12290;&#30456;&#24212;&#30446;&#26631;&#30340;&#22352;&#26631;&#19979;&#38477;&#20248;&#21270;&#65292;&#19982;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#20989;&#25968;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#20135;&#29983;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#27169;&#25311;&#19968;&#31181;&#20855;&#26377;&#26641;&#31361;&#22788;&#29702;&#21644;&#20391;&#25233;&#21046;&#31070;&#32463;&#20803;&#30340;&#26356;&#29983;&#29289;&#30495;&#23454;&#30340;&#22810;&#23460;&#37329;&#23383;&#22612;&#24418;&#31070;&#32463;&#20803;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
The backpropagation algorithm has experienced remarkable success in training large-scale artificial neural networks, however, its biological-plausibility is disputed, and it remains an open question whether the brain employs supervised learning mechanisms akin to it. Here, we propose correlative information maximization between layer activations as an alternative normative approach to describe the signal propagation in biological neural networks in both forward and backward directions. This new framework addresses many concerns about the biological-plausibility of conventional artificial neural networks and the backpropagation algorithm. The coordinate descent-based optimization of the corresponding objective, combined with the mean square error loss function for fitting labeled supervision data, gives rise to a neural network structure that emulates a more biologically realistic network of multi-compartment pyramidal neurons with dendritic processing and lateral inhibitory neurons. Fu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#31232;&#30095;&#33258;&#36866;&#24212;&#29942;&#39048;&#36136;&#24515;&#32534;&#30721;&#22120;&#30340;&#29305;&#24449;&#36873;&#25321;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#25552;&#21462;&#26377;&#21306;&#21035;&#30340;&#29305;&#24449;&#32452;&#20197;&#21450;&#22312;&#37325;&#26500;&#31867;&#21035;&#36136;&#24515;&#30340;&#21516;&#26102;&#20943;&#23569;&#21516;&#31867;&#20998;&#25955;&#24230;&#12289;&#22686;&#21152;&#19981;&#21516;&#31867;&#21035;&#36136;&#24515;&#30340;&#20998;&#31163;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#36807;&#28388;&#36755;&#20837;&#25968;&#25454;&#20013;&#19981;&#24517;&#35201;&#29305;&#24449;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.04795</link><description>&lt;p&gt;
&#20351;&#29992;&#31232;&#30095;&#33258;&#36866;&#24212;&#29942;&#39048;&#36136;&#24515;&#32534;&#30721;&#22120;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Feature Selection using Sparse Adaptive Bottleneck Centroid-Encoder. (arXiv:2306.04795v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04795
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#31232;&#30095;&#33258;&#36866;&#24212;&#29942;&#39048;&#36136;&#24515;&#32534;&#30721;&#22120;&#30340;&#29305;&#24449;&#36873;&#25321;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#25552;&#21462;&#26377;&#21306;&#21035;&#30340;&#29305;&#24449;&#32452;&#20197;&#21450;&#22312;&#37325;&#26500;&#31867;&#21035;&#36136;&#24515;&#30340;&#21516;&#26102;&#20943;&#23569;&#21516;&#31867;&#20998;&#25955;&#24230;&#12289;&#22686;&#21152;&#19981;&#21516;&#31867;&#21035;&#36136;&#24515;&#30340;&#20998;&#31163;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#36807;&#28388;&#36755;&#20837;&#25968;&#25454;&#20013;&#19981;&#24517;&#35201;&#29305;&#24449;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#65292;&#21363;&#31232;&#30095;&#33258;&#36866;&#24212;&#29942;&#39048;&#36136;&#24515;&#32534;&#30721;&#22120;&#65288;SABCE&#65289;&#65292;&#29992;&#20110;&#30830;&#23450;&#33021;&#22815;&#21306;&#20998;&#20004;&#20010;&#25110;&#22810;&#20010;&#31867;&#21035;&#30340;&#29305;&#24449;&#12290;&#35813;&#31639;&#27861;&#26088;&#22312;&#25552;&#21462;&#26377;&#21306;&#21035;&#30340;&#29305;&#24449;&#32452;&#65292;&#24182;&#22312;&#29615;&#22659;&#31354;&#38388;&#20013;&#37325;&#26500;&#31867;&#21035;&#36136;&#24515;&#65292;&#21516;&#26102;&#22312;&#29942;&#39048;&#23618;&#20351;&#29992;&#38468;&#21152;&#24809;&#32602;&#39033;&#26469;&#20943;&#23569;&#21516;&#31867;&#20998;&#25955;&#24230;&#24182;&#22686;&#21152;&#19981;&#21516;&#31867;&#21035;&#36136;&#24515;&#30340;&#20998;&#31163;&#24615;&#12290;&#27169;&#22411;&#20855;&#26377;&#20419;&#36827;&#31232;&#30095;&#24615;&#30340;&#23618;&#65288;SPL&#65289;&#65292;&#19982;&#36755;&#20837;&#23618;&#20855;&#26377;&#19968;&#23545;&#19968;&#30340;&#36830;&#25509;&#12290;&#38500;&#20102;&#20027;&#35201;&#30446;&#26631;&#65292;&#25105;&#20204;&#36824;&#26368;&#23567;&#21270;&#31232;&#30095;&#23618;&#30340;$l_{2,1}$-&#33539;&#25968;&#65292;&#20174;&#32780;&#36807;&#28388;&#25481;&#36755;&#20837;&#25968;&#25454;&#20013;&#30340;&#19981;&#24517;&#35201;&#30340;&#29305;&#24449;&#12290;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#36136;&#24515;&#21644;&#31232;&#30095;&#23618;&#30340;&#26435;&#37325;&#30340;Hadamard&#31215;&#26469;&#26356;&#26032;&#31867;&#21035;&#36136;&#24515;&#65292;&#20174;&#32780;&#24573;&#30053;&#30446;&#26631;&#20013;&#30340;&#26080;&#20851;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#37325;&#24314;&#31867;&#21035;&#36136;&#24515;&#30340;&#20851;&#38190;&#32452;&#20214;&#65292;&#32780;&#19981;&#26159;&#25972;&#20010;&#36136;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel nonlinear model, Sparse Adaptive Bottleneck Centroid-Encoder (SABCE), for determining the features that discriminate between two or more classes. The algorithm aims to extract discriminatory features in groups while reconstructing the class centroids in the ambient space and simultaneously use additional penalty terms in the bottleneck layer to decrease within-class scatter and increase the separation of different class centroids. The model has a sparsity-promoting layer (SPL) with a one-to-one connection to the input layer. Along with the primary objective, we minimize the $l_{2,1}$-norm of the sparse layer, which filters out unnecessary features from input data. During training, we update class centroids by taking the Hadamard product of the centroids and weights of the sparse layer, thus ignoring the irrelevant features from the target. Therefore the proposed method learns to reconstruct the critical components of class centroids rather than the whole centroids.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;DDLearn&#65292;&#36890;&#36807;&#26500;&#24314;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#22312;&#32771;&#34385;&#22810;&#26679;&#21270;&#21644;&#21306;&#20998;&#21270;&#23398;&#20064;&#30340;&#22522;&#30784;&#19978;&#25552;&#39640;&#36890;&#29992;&#20302;&#36164;&#28304;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.04641</link><description>&lt;p&gt;
&#22810;&#26679;&#21270;&#21644;&#21306;&#20998;&#21270;&#34920;&#31034;&#23398;&#20064;&#30340;&#36890;&#29992;&#20302;&#36164;&#28304;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Generalizable Low-Resource Activity Recognition with Diverse and Discriminative Representation Learning. (arXiv:2306.04641v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04641
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;DDLearn&#65292;&#36890;&#36807;&#26500;&#24314;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#22312;&#32771;&#34385;&#22810;&#26679;&#21270;&#21644;&#21306;&#20998;&#21270;&#23398;&#20064;&#30340;&#22522;&#30784;&#19978;&#25552;&#39640;&#36890;&#29992;&#20302;&#36164;&#28304;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#26159;&#19968;&#39033;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#65292;&#37325;&#28857;&#26159;&#20174;&#20154;&#31867;&#20256;&#24863;&#22120;&#35835;&#25968;&#20013;&#35782;&#21035;&#21160;&#20316;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#20805;&#36275;&#30340;&#25968;&#25454;&#26159;&#35757;&#32451;&#36890;&#29992;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;&#30340;&#20851;&#38190;&#38590;&#28857;&#65292;&#36825;&#26377;&#21161;&#20110;&#22312;&#32447;&#32593;&#32476;&#24212;&#29992;&#31243;&#24207;&#30340;&#23450;&#21046;&#21644;&#20248;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;DDLearn&#65292;&#36890;&#36807;&#26500;&#24314;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#21516;&#26102;&#32771;&#34385;&#24046;&#24322;&#24615;&#21644;&#27495;&#35270;&#24615;&#23398;&#20064;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#36890;&#29992;&#20302;&#36164;&#28304;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human activity recognition (HAR) is a time series classification task that focuses on identifying the motion patterns from human sensor readings. Adequate data is essential but a major bottleneck for training a generalizable HAR model, which assists customization and optimization of online web applications. However, it is costly in time and economy to collect large-scale labeled data in reality, i.e., the low-resource challenge. Meanwhile, data collected from different persons have distribution shifts due to different living habits, body shapes, age groups, etc. The low-resource and distribution shift challenges are detrimental to HAR when applying the trained model to new unseen subjects. In this paper, we propose a novel approach called Diverse and Discriminative representation Learning (DDLearn) for generalizable low-resource HAR. DDLearn simultaneously considers diversity and discrimination learning. With the constructed self-supervised learning task, DDLearn enlarges the data dive
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#22312;&#28151;&#21512;&#20854;&#20182;&#25991;&#26412;&#26469;&#28304;&#26102;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2306.04634</link><description>&lt;p&gt;
&#35770;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Reliability of Watermarks for Large Language Models. (arXiv:2306.04634v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#22312;&#28151;&#21512;&#20854;&#20182;&#25991;&#26412;&#26469;&#28304;&#26102;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#24320;&#22987;&#24212;&#29992;&#20110;&#26085;&#24120;&#20351;&#29992;&#65292;&#24182;&#26377;&#33021;&#21147;&#22312;&#26410;&#26469;&#30340;&#21313;&#24180;&#20869;&#20135;&#29983;&#22823;&#37327;&#30340;&#25991;&#26412;&#12290;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#21487;&#33021;&#20250;&#21462;&#20195;&#20114;&#32852;&#32593;&#19978;&#30340;&#20154;&#31867;&#20889;&#20316;&#25991;&#26412;&#65292;&#24182;&#26377;&#21487;&#33021;&#34987;&#29992;&#20110;&#24694;&#24847;&#30446;&#30340;&#65292;&#22914;&#38035;&#40060;&#25915;&#20987;&#21644;&#31038;&#20132;&#23186;&#20307;&#26426;&#22120;&#20154;&#12290;&#27700;&#21360;&#26159;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#21487;&#26816;&#27979;&#21644;&#21487;&#35760;&#24405;&#65292;&#26469;&#38477;&#20302;&#36825;&#20123;&#20260;&#23475;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65306;&#22312;&#29616;&#23454;&#20013;&#28151;&#21512;&#20102;&#20854;&#20182;&#30340;&#25991;&#26412;&#26469;&#28304;&#65292;&#34987;&#20154;&#31867;&#20889;&#20316;&#32773;&#25110;&#20854;&#20182;&#35821;&#35328;&#27169;&#22411;&#25913;&#20889;&#65292;&#34987;&#29992;&#20110;&#31038;&#20132;&#21644;&#25216;&#26415;&#39046;&#22495;&#30340;&#21508;&#31181;&#24212;&#29992;&#26102;&#65292;&#27700;&#21360;&#22312;&#23454;&#38469;&#35774;&#32622;&#20013;&#30340;&#21487;&#38752;&#24615;&#22914;&#20309;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19981;&#21516;&#30340;&#26816;&#27979;&#26041;&#26696;&#65292;&#37327;&#21270;&#20102;&#23427;&#20204;&#26816;&#27979;&#27700;&#21360;&#30340;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#22312;&#27599;&#20010;&#24773;&#20917;&#19979;&#38656;&#35201;&#35266;&#23519;&#22810;&#23569;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#25165;&#33021;&#21487;&#38752;&#22320;&#26816;&#27979;&#27700;&#21360;&#12290;&#25105;&#20204;&#29305;&#21035;&#24378;&#35843;&#20102;&#24403;&#27700;&#21360;&#19982;&#20854;&#20182;&#25991;&#26412;&#26469;&#28304;&#28151;&#21512;&#26102;&#27700;&#21360;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#27700;&#21360;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are now deployed to everyday use and positioned to produce large quantities of text in the coming decade. Machine-generated text may displace human-written text on the internet and has the potential to be used for malicious purposes, such as spearphishing attacks and social media bots. Watermarking is a simple and effective strategy for mitigating such harms by enabling the detection and documentation of LLM-generated text. Yet, a crucial question remains: How reliable is watermarking in realistic settings in the wild? There, watermarked text might be mixed with other text sources, paraphrased by human writers or other language models, and used for applications in a broad number of domains, both social and technical. In this paper, we explore different detection schemes, quantify their power at detecting watermarks, and determine how much machine-generated text needs to be observed in each scenario to reliably detect the watermark. We especially highlight o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#39564;&#35777;&#26041;&#27861;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#20197;&#39564;&#35777;&#23433;&#20840;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#24182;&#20195;&#34920;&#20102;&#36890;&#29992;&#12289;&#21487;&#20280;&#32553;&#21644;&#21487;&#39564;&#35777;&#30340;&#25511;&#21046;&#31995;&#32479;&#35774;&#35745;&#26694;&#26550;&#30340;&#31532;&#19968;&#27493;&#12290;</title><link>http://arxiv.org/abs/2306.04026</link><description>&lt;p&gt;
&#20215;&#20540;&#20989;&#25968;&#21363;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#65306;&#20351;&#29992;&#25511;&#21046;&#29702;&#35770;&#39564;&#35777;&#23398;&#20064;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Your Value Function is a Control Barrier Function: Verification of Learned Policies using Control Theory. (arXiv:2306.04026v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#39564;&#35777;&#26041;&#27861;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#20197;&#39564;&#35777;&#23433;&#20840;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#24182;&#20195;&#34920;&#20102;&#36890;&#29992;&#12289;&#21487;&#20280;&#32553;&#21644;&#21487;&#39564;&#35777;&#30340;&#25511;&#21046;&#31995;&#32479;&#35774;&#35745;&#26694;&#26550;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#20855;&#26377;&#39640;&#24230;&#30340;&#36890;&#29992;&#24615;&#21644;&#21487;&#20280;&#32553;&#24615;&#65292;&#20294;&#39564;&#35777;&#31574;&#30053;&#34892;&#20026;&#30340;&#38590;&#24230;&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#31243;&#24207;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#25511;&#21046;&#29702;&#35770;&#20013;&#20351;&#29992;&#30340;&#39564;&#35777;&#26041;&#27861;&#24212;&#29992;&#20110;&#23398;&#20064;&#30340;&#20215;&#20540;&#20989;&#25968;&#12290;&#36890;&#36807;&#20998;&#26512;&#23433;&#20840;&#32500;&#25252;&#30340;&#31616;&#21333;&#20219;&#21153;&#32467;&#26500;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#23558;&#20540;&#20989;&#25968;&#19982;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#30456;&#32852;&#31995;&#30340;&#21407;&#22987;&#23450;&#29702;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#20197;&#39564;&#35777;&#23433;&#20840;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#21892;&#23398;&#20064;&#30340;&#23454;&#38469;&#23454;&#26045;&#32454;&#33410;&#12290;&#38500;&#20102;&#25552;&#20986;&#35777;&#20070;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#22806;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;RL&#31574;&#30053;&#35299;&#38145;&#20102;&#20016;&#23500;&#30340;&#25511;&#21046;&#29702;&#35770;&#39564;&#35777;&#26041;&#27861;&#65292;&#24182;&#20195;&#34920;&#20102;&#36890;&#29992;&#12289;&#21487;&#20280;&#32553;&#21644;&#21487;&#39564;&#35777;&#30340;&#25511;&#21046;&#31995;&#32479;&#35774;&#35745;&#26694;&#26550;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although RL is highly general and scalable, the difficulty of verifying policy behaviours poses challenges for safety-critical applications. To remedy this, we propose to apply verification methods used in control theory to learned value functions. By analyzing a simple task structure for safety preservation, we derive original theorems linking value functions to control barrier functions. Inspired by this, we propose novel metrics for verification of value functions in safe control tasks, and practical implementation details that improve learning. Besides proposing a novel method for certificate learning, our work unlocks a wealth of verification methods in control theory for RL policies, and represents a first step towards a framework for general, scalable, and verifiable design of control systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#35805;&#36136;&#37327;&#27880;&#37322;&#24037;&#20316;&#27969;&#31243;&#31216;&#20026;DQA&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#21644;&#21487;&#25512;&#24191;&#22320;&#35780;&#20272;&#23545;&#35805;&#36136;&#37327;&#65292;&#23588;&#20854;&#26159;&#36890;&#36807;&#19968;&#20123;&#23458;&#35266;&#23545;&#35805;&#23646;&#24615;&#30340;&#21028;&#26029;&#12290;</title><link>http://arxiv.org/abs/2306.03984</link><description>&lt;p&gt;
&#38754;&#21521;&#20219;&#21153;&#22411;&#23545;&#35805;&#30340;&#26356;&#20934;&#30830;&#21644;&#21487;&#25512;&#24191;&#30340;&#35780;&#20272;&#25351;&#26631;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Toward More Accurate and Generalizable Evaluation Metrics for Task-Oriented Dialogs. (arXiv:2306.03984v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#35805;&#36136;&#37327;&#27880;&#37322;&#24037;&#20316;&#27969;&#31243;&#31216;&#20026;DQA&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#21644;&#21487;&#25512;&#24191;&#22320;&#35780;&#20272;&#23545;&#35805;&#36136;&#37327;&#65292;&#23588;&#20854;&#26159;&#36890;&#36807;&#19968;&#20123;&#23458;&#35266;&#23545;&#35805;&#23646;&#24615;&#30340;&#21028;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20132;&#20114;&#36136;&#37327;&#23545;&#20110;&#25913;&#36827;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#23545;&#35805;&#36136;&#37327;&#20272;&#35745;&#26041;&#27861;&#35201;&#20040;&#20391;&#37325;&#20110;&#35780;&#20272;&#21333;&#20010;&#23545;&#35805;&#36718;&#27425;&#30340;&#36136;&#37327;&#65292;&#35201;&#20040;&#20174;&#32456;&#31471;&#29992;&#25143;&#31435;&#21363;&#22312;&#20132;&#20114;&#20043;&#21518;&#25910;&#38598;&#23545;&#35805;&#32423;&#21035;&#30340;&#36136;&#37327;&#27979;&#37327;&#25968;&#25454;&#12290;&#19982;&#36825;&#20123;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#35805;&#32423;&#21035;&#27880;&#37322;&#24037;&#20316;&#27969;&#31243;&#31216;&#20026;&#23545;&#35805;&#36136;&#37327;&#27880;&#37322;&#65288;DQA&#65289;&#12290;DQA&#19987;&#23478;&#27880;&#37322;&#21592;&#35780;&#20272;&#25972;&#20010;&#23545;&#35805;&#30340;&#36136;&#37327;&#65292;&#24182;&#26631;&#35760;&#23545;&#35805;&#30340;&#30446;&#26631;&#23436;&#25104;&#21644;&#29992;&#25143;&#24773;&#24863;&#31561;&#23646;&#24615;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#65306;&#65288;i&#65289;&#23613;&#31649;&#23545;&#35805;&#36136;&#37327;&#19981;&#33021;&#23436;&#20840;&#20998;&#35299;&#25104;&#23545;&#35805;&#32423;&#21035;&#23646;&#24615;&#65292;&#20294;&#26576;&#20123;&#23458;&#35266;&#23545;&#35805;&#23646;&#24615;&#19982;&#23545;&#35805;&#36136;&#37327;&#30340;&#21028;&#26029;&#20043;&#38388;&#23384;&#22312;&#30528;&#24378;&#20851;&#31995;&#65307;&#65288;ii&#65289;&#23545;&#20110;&#23545;&#35805;&#32423;&#21035;&#36136;&#37327;&#20272;&#35745;&#20219;&#21153;&#65292;&#19968;&#20010;&#22312;&#23545;&#35805;&#32423;&#21035;&#27880;&#37322;&#19978;&#35757;&#32451;&#30340;&#30417;&#30563;&#27169;&#22411;&#20248;&#20110;&#20165;&#22522;&#20110;&#32858;&#21512;&#36718;&#27425;&#32423;&#21035;&#29305;&#24449;&#30340;&#26041;&#27861;&#65307;&#20197;&#21450;&#65288;iii&#65289;&#20351;&#29992;DQA&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#33021;&#22815;&#24471;&#21040;&#26356;&#20934;&#30830;&#21644;&#21487;&#25512;&#24191;&#30340;&#23545;&#35805;&#36136;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measurement of interaction quality is a critical task for the improvement of spoken dialog systems. Existing approaches to dialog quality estimation either focus on evaluating the quality of individual turns, or collect dialog-level quality measurements from end users immediately following an interaction. In contrast to these approaches, we introduce a new dialog-level annotation workflow called Dialog Quality Annotation (DQA). DQA expert annotators evaluate the quality of dialogs as a whole, and also label dialogs for attributes such as goal completion and user sentiment. In this contribution, we show that: (i) while dialog quality cannot be completely decomposed into dialog-level attributes, there is a strong relationship between some objective dialog attributes and judgments of dialog quality; (ii) for the task of dialog-level quality estimation, a supervised model trained on dialog-level annotations outperforms methods based purely on aggregating turn-level features; and (iii) the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#27169;&#22411;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#22810;&#21464;&#37327;&#19981;&#23545;&#40784;&#31232;&#30095;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#20854;&#20013;&#19968;&#31181;&#26159;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;SERT&#65292;&#21478;&#19968;&#31181;&#26159;&#25552;&#20379;&#21487;&#35299;&#37322;&#32467;&#26524;&#30340;&#31616;&#21333;&#27169;&#22411;SST-ANN</title><link>http://arxiv.org/abs/2306.03042</link><description>&lt;p&gt;
SERT: &#22522;&#20110;Transformer&#30340;&#32570;&#22833;&#31354;&#38388;-&#26102;&#38388;&#20256;&#24863;&#22120;&#25968;&#25454;&#27169;&#22411;&#29992;&#20110;&#29615;&#22659;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
SERT: A Transfomer Based Model for Spatio-Temporal Sensor Data with Missing Values for Environmental Monitoring. (arXiv:2306.03042v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03042
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#27169;&#22411;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#22810;&#21464;&#37327;&#19981;&#23545;&#40784;&#31232;&#30095;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#20854;&#20013;&#19968;&#31181;&#26159;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;SERT&#65292;&#21478;&#19968;&#31181;&#26159;&#25552;&#20379;&#21487;&#35299;&#37322;&#32467;&#26524;&#30340;&#31616;&#21333;&#27169;&#22411;SST-ANN
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29615;&#22659;&#30417;&#27979;&#23545;&#20110;&#25105;&#20204;&#29702;&#35299;&#27668;&#20505;&#21464;&#21270;&#12289;&#29983;&#29289;&#22810;&#26679;&#24615;&#20007;&#22833;&#21644;&#27745;&#26579;&#31561;&#26041;&#38754;&#20855;&#26377;&#20851;&#38190;&#20316;&#29992;&#12290;&#26469;&#33258;&#20256;&#24863;&#22120;&#21644;&#21355;&#26143;&#31561;&#25968;&#25454;&#28304;&#30340;&#22823;&#35268;&#27169;&#31354;&#38388;-&#26102;&#38388;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#20351;&#25105;&#20204;&#33021;&#22815;&#24320;&#21457;&#22797;&#26434;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#20102;&#35299;&#20027;&#35201;&#30340;&#39537;&#21160;&#22240;&#32032;&#12290;&#20294;&#30001;&#20110;&#35774;&#22791;&#38382;&#39064;&#25110;&#32500;&#25252;&#38382;&#39064;&#65292;&#20174;&#20256;&#24863;&#22120;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#24448;&#24448;&#21253;&#21547;&#32570;&#22833;&#20540;&#12290;&#32570;&#22833;&#20540;&#24456;&#23569;&#21516;&#26102;&#21457;&#29983;&#65292;&#23548;&#33268;&#25968;&#25454;&#26159;&#22810;&#21464;&#37327;&#19981;&#23545;&#40784;&#30340;&#31232;&#30095;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#26102;&#33258;&#28982;&#22320;&#25191;&#34892;&#22810;&#21464;&#37327;&#31354;&#38388;-&#26102;&#38388;&#39044;&#27979;&#65292;&#32780;&#19981;&#38656;&#35201;&#25554;&#34917;&#12290;&#31532;&#19968;&#20010;&#27169;&#22411;&#26159;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;SERT&#65288;&#26469;&#33258;Transformer&#30340;&#31354;&#38388;-&#26102;&#38388;&#32534;&#30721;&#34920;&#31034;&#65289;&#12290;&#31532;&#20108;&#20010;&#27169;&#22411;&#26159;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#27169;&#22411;&#65292;&#34987;&#21629;&#21517;&#20026;SST-ANN&#65288;&#31232;&#30095;&#31354;&#38388;-&#26102;&#38388;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65289;&#65292;&#33021;&#22815;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;...
&lt;/p&gt;
&lt;p&gt;
Environmental monitoring is crucial to our understanding of climate change, biodiversity loss and pollution. The availability of large-scale spatio-temporal data from sources such as sensors and satellites allows us to develop sophisticated models for forecasting and understanding key drivers. However, the data collected from sensors often contain missing values due to faulty equipment or maintenance issues. The missing values rarely occur simultaneously leading to data that are multivariate misaligned sparse time series. We propose two models that are capable of performing multivariate spatio-temporal forecasting while handling missing data naturally without the need for imputation. The first model is a transformer-based model, which we name SERT (Spatio-temporal Encoder Representations from Transformers). The second is a simpler model named SST-ANN (Sparse Spatio-Temporal Artificial Neural Network) which is capable of providing interpretable results. We conduct extensive experiments 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#30693;&#35782;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#39044;&#35757;&#32451;&#21644;&#33258;&#32534;&#30721;&#22120;&#32467;&#26500;&#65292;&#21516;&#26102;&#21033;&#29992;&#22522;&#20110;&#30456;&#20851;&#24615;&#20248;&#21270;&#27169;&#22411;&#26500;&#24314;&#22810;&#23618;&#32593;&#32476;&#21644;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#26469;&#25552;&#39640;&#39118;&#36895;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#24314;&#27169;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.01986</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#30456;&#20851;&#24615;&#20248;&#21270;&#30340;&#39118;&#36895;&#39044;&#27979;&#28145;&#24230;&#23398;&#20064;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Novel Correlation-optimized Deep Learning Method for Wind Speed Forecast. (arXiv:2306.01986v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01986
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#30693;&#35782;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#39044;&#35757;&#32451;&#21644;&#33258;&#32534;&#30721;&#22120;&#32467;&#26500;&#65292;&#21516;&#26102;&#21033;&#29992;&#22522;&#20110;&#30456;&#20851;&#24615;&#20248;&#21270;&#27169;&#22411;&#26500;&#24314;&#22810;&#23618;&#32593;&#32476;&#21644;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#26469;&#25552;&#39640;&#39118;&#36895;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#24314;&#27169;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39118;&#21147;&#21457;&#30005;&#35013;&#26426;&#29575;&#30340;&#22686;&#21152;&#65292;&#32473;&#20840;&#29699;&#33021;&#28304;&#31995;&#32479;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#30830;&#20445;&#30005;&#21147;&#31995;&#32479;&#30340;&#21487;&#38752;&#36816;&#34892;&#65292;&#38656;&#35201;&#20934;&#30830;&#39044;&#27979;&#39118;&#21147;&#28065;&#36718;&#26426;&#30340;&#39118;&#36895;&#21644;&#21151;&#29575;&#12290;&#30446;&#21069;&#65292;&#28145;&#24230;&#23398;&#20064;&#34987;&#36880;&#28176;&#24212;&#29992;&#20110;&#39118;&#36895;&#39044;&#27979;&#65292;&#20294;&#30001;&#20110;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#21644;&#30828;&#20214;&#38480;&#21046;&#31561;&#38382;&#39064;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20173;&#23384;&#22312;&#23604;&#23596;&#20043;&#22788;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#28145;&#24230;&#30693;&#35782;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#28151;&#21512;&#20102;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#33258;&#32534;&#30721;&#22120;&#32467;&#26500;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#30693;&#35782;&#23398;&#20064;&#26694;&#26550;&#30340;&#25968;&#25454;&#34920;&#29616;&#21644;&#24314;&#27169;&#12290;&#20026;&#20102;&#24418;&#25104;&#30693;&#35782;&#21644;&#23545;&#24212;&#21560;&#25910;&#22120;&#65292;&#21407;&#22987;&#25968;&#25454;&#32463;&#30001;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#20248;&#21270;&#27169;&#22411;&#39044;&#22788;&#29702;&#65292;&#26500;&#24314;&#22810;&#23618;&#32593;&#32476;&#65288;&#30693;&#35782;&#65289;&#65292;&#34987;&#24207;&#21015;&#21040;&#24207;&#21015;&#65288;Seq2Seq&#65289;&#27169;&#22411;&#21560;&#25910;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing installation rate of wind power poses great challenges to the global power system. In order to ensure the reliable operation of the power system, it is necessary to accurately forecast the wind speed and power of the wind turbines. At present, deep learning is progressively applied to the wind speed prediction. Nevertheless, the recent deep learning methods still reflect the embarrassment for practical applications due to model interpretability and hardware limitation. To this end, a novel deep knowledge-based learning method is proposed in this paper. The proposed method hybridizes pre-training method and auto-encoder structure to improve data representation and modeling of the deep knowledge-based learning framework. In order to form knowledge and corresponding absorbers, the original data is preprocessed by an optimization model based on correlation to construct multi-layer networks (knowledge) which are absorbed by sequence to sequence (Seq2Seq) models. Specifically,
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;BBF&#22522;&#20110;&#20215;&#20540;&#20989;&#25968;&#30340;RL&#20195;&#29702;&#65292;&#22312;Atari 100K&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#36229;&#20154;&#31867;&#34920;&#29616;&#65292;&#20855;&#26377;&#20154;&#31867;&#25928;&#29575;&#65292;&#25552;&#20986;&#20102;&#22312;&#26679;&#26412;&#39640;&#25928;RL&#30740;&#31350;&#30340;ALE&#20013;&#26356;&#26032;&#30446;&#26631;&#30340;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19452</link><description>&lt;p&gt;
&#26356;&#22823;&#12289;&#26356;&#22909;&#12289;&#26356;&#24555;&#65306;&#20855;&#26377;&#20154;&#31867;&#25928;&#29575;&#30340;&#20154;&#31867;&#32423;Atari&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Bigger, Better, Faster: Human-level Atari with human-level efficiency. (arXiv:2305.19452v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19452
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;BBF&#22522;&#20110;&#20215;&#20540;&#20989;&#25968;&#30340;RL&#20195;&#29702;&#65292;&#22312;Atari 100K&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#36229;&#20154;&#31867;&#34920;&#29616;&#65292;&#20855;&#26377;&#20154;&#31867;&#25928;&#29575;&#65292;&#25552;&#20986;&#20102;&#22312;&#26679;&#26412;&#39640;&#25928;RL&#30740;&#31350;&#30340;ALE&#20013;&#26356;&#26032;&#30446;&#26631;&#30340;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;BBF&#30340;&#22522;&#20110;&#20215;&#20540;&#20989;&#25968;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#26469;&#23454;&#29616;Atari 100K&#22522;&#20934;&#27979;&#35797;&#30340;&#36229;&#20154;&#31867;&#34920;&#29616;&#12290;BBF&#20381;&#38752;&#31070;&#32463;&#32593;&#32476;&#30340;&#20215;&#20540;&#20272;&#35745;&#25193;&#23637;&#20197;&#21450;&#20854;&#20182;&#35774;&#35745;&#36873;&#25321;&#65292;&#22312;&#36981;&#24490;&#26679;&#26412;&#39640;&#25928;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#23454;&#29616;&#36825;&#31181;&#25193;&#23637;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#35774;&#35745;&#36873;&#25321;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#20998;&#26512;&#24182;&#20026;&#26410;&#26469;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26356;&#26032;ALE&#19978;&#26679;&#26412;&#39640;&#25928;&#30340;RL&#30740;&#31350;&#30446;&#26631;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#20844;&#24320;&#22312;https://github.com/google-research/google-research/tree/master/bigger_better_faster&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a value-based RL agent, which we call BBF, that achieves super-human performance in the Atari 100K benchmark. BBF relies on scaling the neural networks used for value estimation, as well as a number of other design choices that enable this scaling in a sample-efficient manner. We conduct extensive analyses of these design choices and provide insights for future work. We end with a discussion about updating the goalposts for sample-efficient RL research on the ALE. We make our code and data publicly available at https://github.com/google-research/google-research/tree/master/bigger_better_faster.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#28789;&#27963;&#30340;PFN&#20316;&#20026;BO&#20195;&#29702;&#24314;&#27169;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20801;&#35768;&#36827;&#19968;&#27493;&#20449;&#24687;&#32435;&#20837;&#20197;&#36827;&#34892;&#38750;&#36828;&#35270;BO&#12290;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#38382;&#39064;&#19978;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.17535</link><description>&lt;p&gt;
PFN&#26159;&#36866;&#29992;&#20110;&#23454;&#38469;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#28789;&#27963;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
PFNs Are Flexible Models for Real-World Bayesian Optimization. (arXiv:2305.17535v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#28789;&#27963;&#30340;PFN&#20316;&#20026;BO&#20195;&#29702;&#24314;&#27169;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20801;&#35768;&#36827;&#19968;&#27493;&#20449;&#24687;&#32435;&#20837;&#20197;&#36827;&#34892;&#38750;&#36828;&#35270;BO&#12290;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#38382;&#39064;&#19978;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#20808;&#39564;&#25968;&#25454;&#25311;&#21512;&#32593;&#32476;(PFNs)&#20316;&#20026;&#36125;&#21494;&#26031;&#20248;&#21270;(BO)&#30340;&#28789;&#27963;&#20195;&#29702;&#12290;PFN&#26159;&#19968;&#31181;&#31070;&#32463;&#36807;&#31243;&#65292;&#34987;&#35757;&#32451;&#29992;&#20110;&#36817;&#20284;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;(PPD)&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#21487;&#26377;&#25928;&#37319;&#26679;&#30340;&#20808;&#39564;&#20998;&#24067;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#31181;&#28789;&#27963;&#24615;&#26469;&#36827;&#34892;BO&#30340;&#20195;&#29702;&#24314;&#27169;&#12290;&#25105;&#20204;&#20351;&#29992;PFN&#26469;&#27169;&#25311;&#19968;&#20010;&#26420;&#32032;&#39640;&#26031;&#36807;&#31243;(GP)&#65292;&#19968;&#20010;&#20808;&#36827;&#30340;GP&#21644;&#19968;&#20010;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;(BNN)&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#36827;&#19968;&#27493;&#30340;&#20449;&#24687;&#32435;&#20837;&#20808;&#39564;&#65292;&#20363;&#22914;&#20801;&#35768;&#26377;&#20851;&#26368;&#20248;&#20301;&#32622;&#30340;&#25552;&#31034;(&#29992;&#25143;&#20808;&#39564;)&#65292;&#24573;&#30053;&#19981;&#30456;&#20851;&#30340;&#32500;&#24230;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#33719;&#21462;&#20989;&#25968;&#26469;&#25191;&#34892;&#38750;&#36828;&#35270;BO&#12290;&#36825;&#20123;&#25193;&#23637;&#30340;&#28789;&#27963;&#24615;&#20026;&#20351;&#29992;PFN&#36827;&#34892;BO&#24320;&#36767;&#20102;&#24191;&#38420;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#22312;&#20154;&#24037;&#39640;&#26031;&#36807;&#31243;&#26679;&#26412;&#21644;&#19977;&#20010;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#27979;&#35797;&#24179;&#21488;&#19978;&#23637;&#31034;&#20102;PFN&#23545;BO&#30340;&#26377;&#29992;&#24615;&#65306;HPO-B&#12289;Bayesmark&#21644;PD1&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we use Prior-data Fitted Networks (PFNs) as a flexible surrogate for Bayesian Optimization (BO). PFNs are neural processes that are trained to approximate the posterior predictive distribution (PPD) for any prior distribution that can be efficiently sampled from. We describe how this flexibility can be exploited for surrogate modeling in BO. We use PFNs to mimic a naive Gaussian process (GP), an advanced GP, and a Bayesian Neural Network (BNN). In addition, we show how to incorporate further information into the prior, such as allowing hints about the position of optima (user priors), ignoring irrelevant dimensions, and performing non-myopic BO by learning the acquisition function. The flexibility underlying these extensions opens up vast possibilities for using PFNs for BO. We demonstrate the usefulness of PFNs for BO in a large-scale evaluation on artificial GP samples and three different hyperparameter optimization testbeds: HPO-B, Bayesmark, and PD1. We publish code 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#35843;&#26041;&#27861;&#26469;&#25552;&#39640;&#32858;&#21512;&#26354;&#32447;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#24503;&#22269;&#26085;&#21069;&#30005;&#21147;&#31454;&#25293;&#24066;&#22330;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.16255</link><description>&lt;p&gt;
&#23618;&#27425;&#39044;&#27979;&#32858;&#21512;&#26354;&#32447;&#24182;&#24212;&#29992;&#20110;&#26085;&#21069;&#30005;&#21147;&#31454;&#25293;
&lt;/p&gt;
&lt;p&gt;
Hierarchical forecasting for aggregated curves with an application to day-ahead electricity price auctions. (arXiv:2305.16255v1 [stat.AP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#35843;&#26041;&#27861;&#26469;&#25552;&#39640;&#32858;&#21512;&#26354;&#32447;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#24503;&#22269;&#26085;&#21069;&#30005;&#21147;&#31454;&#25293;&#24066;&#22330;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#21512;&#26354;&#32447;&#22312;&#32463;&#27982;&#21644;&#37329;&#34701;&#20013;&#24456;&#24120;&#35265;&#65292;&#26368;&#31361;&#20986;&#30340;&#20363;&#23376;&#26159;&#20379;&#32473;&#21644;&#38656;&#27714;&#26354;&#32447;&#12290;&#25105;&#20204;&#21457;&#29616;&#25152;&#26377;&#32858;&#21512;&#26354;&#32447;&#37117;&#20855;&#26377;&#20869;&#22312;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#22240;&#27492;&#21487;&#20197;&#20351;&#29992;&#23618;&#27425;&#21327;&#35843;&#26041;&#27861;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#32858;&#21512;&#26354;&#32447;&#22914;&#20309;&#26500;&#24314;&#25110;&#35299;&#26500;&#30340;&#28145;&#20837;&#29702;&#35770;&#65292;&#24182;&#24471;&#20986;&#36825;&#20123;&#26041;&#27861;&#22312;&#24369;&#20551;&#35774;&#19979;&#26159;&#31561;&#25928;&#30340;&#32467;&#35770;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#22810;&#31181;&#32858;&#21512;&#26354;&#32447;&#30340;&#21327;&#35843;&#26041;&#27861;&#65292;&#21253;&#25324;&#24050;&#32463;&#24314;&#31435;&#30340;&#33258;&#19979;&#32780;&#19978;&#12289;&#33258;&#19978;&#32780;&#19979;&#21644;&#32447;&#24615;&#26368;&#20248;&#21327;&#35843;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#21327;&#35843;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#32858;&#21512;-&#19979;&#8221;&#65292;&#20854;&#22797;&#26434;&#24230;&#31867;&#20284;&#20110;&#33258;&#19979;&#32780;&#19978;&#21644;&#33258;&#19978;&#32780;&#19979;&#26041;&#27861;&#65292;&#20294;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#24448;&#24448;&#25552;&#20379;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#23545;&#24503;&#22269;&#26085;&#21069;&#30005;&#21147;&#31454;&#25293;&#24066;&#22330;&#36827;&#34892;&#20102;&#23454;&#35777;&#39044;&#27979;&#30740;&#31350;&#65292;&#39044;&#27979;&#20102;&#38656;&#27714;&#21644;&#20379;&#32473;&#26354;&#32447;&#65292;&#24182;&#23545;&#20854;&#22343;&#34913;&#24615;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aggregated curves are common structures in economics and finance, and the most prominent examples are supply and demand curves. In this study, we exploit the fact that all aggregated curves have an intrinsic hierarchical structure, and thus hierarchical reconciliation methods can be used to improve the forecast accuracy. We provide an in-depth theory on how aggregated curves can be constructed or deconstructed, and conclude that these methods are equivalent under weak assumptions. We consider multiple reconciliation methods for aggregated curves, including previously established bottom-up, top-down, and linear optimal reconciliation approaches. We also present a new benchmark reconciliation method called 'aggregated-down' with similar complexity to bottom-up and top-down approaches, but it tends to provide better accuracy in this setup. We conducted an empirical forecasting study on the German day-ahead power auction market by predicting the demand and supply curves, where their equili
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31163;&#32447;&#20559;&#22909;&#24341;&#23548;&#31574;&#30053;&#20248;&#21270;(OPPO)&#33539;&#20363;&#65292;&#23427;&#28040;&#38500;&#20102;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#20013;&#22870;&#21169;&#20989;&#25968;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2305.16217</link><description>&lt;p&gt;
&#36229;&#36234;&#22870;&#21169;&#65306;&#31163;&#32447;&#20559;&#22909;&#24341;&#23548;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Beyond Reward: Offline Preference-guided Policy Optimization. (arXiv:2305.16217v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31163;&#32447;&#20559;&#22909;&#24341;&#23548;&#31574;&#30053;&#20248;&#21270;(OPPO)&#33539;&#20363;&#65292;&#23427;&#28040;&#38500;&#20102;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#20013;&#22870;&#21169;&#20989;&#25968;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#31163;&#32447;&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;(PbRL)&#65292;&#23427;&#26159;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#19981;&#38656;&#35201;&#22312;&#32447;&#20132;&#20114;&#25110;&#25351;&#23450;&#22870;&#21169;&#20989;&#25968;&#12290;&#30456;&#21453;&#65292;&#20195;&#29702;&#34987;&#25552;&#20379;&#20102;&#22266;&#23450;&#30340;&#31163;&#32447;&#36712;&#36857;&#21644;&#20154;&#31867;&#20559;&#22909;&#20043;&#38388;&#30340;&#20449;&#24687;&#26469;&#20998;&#21035;&#25552;&#21462;&#21160;&#24577;&#21644;&#20219;&#21153;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#20449;&#24687;&#29942;&#39048;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31163;&#32447;&#20559;&#22909;&#24341;&#23548;&#31574;&#30053;&#20248;&#21270;(OPPO)&#33539;&#20363;&#65292;&#23427;&#22312;&#19968;&#20010;&#27493;&#39588;&#20013;&#23545;&#31163;&#32447;&#36712;&#36857;&#21644;&#20559;&#22909;&#36827;&#34892;&#24314;&#27169;&#65292;&#28040;&#38500;&#20102;&#20998;&#21035;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study focuses on the topic of offline preference-based reinforcement learning (PbRL), a variant of conventional reinforcement learning that dispenses with the need for online interaction or specification of reward functions. Instead, the agent is provided with fixed offline trajectories and human preferences between pairs of trajectories to extract the dynamics and task information, respectively. Since the dynamics and task information are orthogonal, a naive approach would involve using preference-based reward learning followed by an off-the-shelf offline RL algorithm. However, this requires the separate learning of a scalar reward function, which is assumed to be an information bottleneck of the learning process. To address this issue, we propose the offline preference-guided policy optimization (OPPO) paradigm, which models offline trajectories and preferences in a one-step process, eliminating the need for separately learning a reward function. OPPO achieves this by introducin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#39318;&#20010;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#22522;&#20110;&#27169;&#25311;&#30340;&#25512;&#35770;&#65288;&#22914;ABC&#21644;NPE&#65289;&#20013;&#30001;&#20110;&#27169;&#22411;&#38169;&#35823;&#24341;&#36215;&#30340;&#19981;&#21487;&#38752;&#25512;&#35770;&#12290;&#36890;&#36807;&#32422;&#26463;&#32479;&#35745;&#37327;&#30340;&#36873;&#25321;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#24809;&#32602;&#19982;&#25968;&#25454;&#21644;&#27169;&#22411;&#20043;&#38388;&#19981;&#21305;&#37197;&#30340;&#32479;&#35745;&#37327;&#26469;&#38450;&#27490;&#19981;&#21487;&#38752;&#25512;&#35770;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#26412;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15871</link><description>&lt;p&gt;
&#23398;&#20064;&#40065;&#26834;&#32479;&#35745;&#29992;&#20110;&#27169;&#22411;&#38169;&#35823;&#24773;&#20917;&#19979;&#30340;&#22522;&#20110;&#27169;&#25311;&#25512;&#35770;
&lt;/p&gt;
&lt;p&gt;
Learning Robust Statistics for Simulation-based Inference under Model Misspecification. (arXiv:2305.15871v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#39318;&#20010;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#22522;&#20110;&#27169;&#25311;&#30340;&#25512;&#35770;&#65288;&#22914;ABC&#21644;NPE&#65289;&#20013;&#30001;&#20110;&#27169;&#22411;&#38169;&#35823;&#24341;&#36215;&#30340;&#19981;&#21487;&#38752;&#25512;&#35770;&#12290;&#36890;&#36807;&#32422;&#26463;&#32479;&#35745;&#37327;&#30340;&#36873;&#25321;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#24809;&#32602;&#19982;&#25968;&#25454;&#21644;&#27169;&#22411;&#20043;&#38388;&#19981;&#21305;&#37197;&#30340;&#32479;&#35745;&#37327;&#26469;&#38450;&#27490;&#19981;&#21487;&#38752;&#25512;&#35770;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#26412;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#25311;&#30340;&#25512;&#35770;&#26041;&#27861;&#65288;&#22914;&#36817;&#20284;&#36125;&#21494;&#26031;&#35745;&#31639;&#65288;ABC&#65289;&#65292;&#21512;&#25104;&#20284;&#28982;&#24615;&#21644;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#65288;NPE&#65289;&#65289;&#20381;&#36182;&#20110;&#27169;&#25311;&#32479;&#35745;&#37327;&#20197;&#25512;&#26029;&#38590;&#20197;&#35745;&#31639;&#30340;&#20284;&#28982;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#24050;&#30693;&#36825;&#31181;&#26041;&#27861;&#22312;&#27169;&#22411;&#38169;&#35823;&#24773;&#20917;&#19979;&#20250;&#20135;&#29983;&#19981;&#21487;&#20449;&#21644;&#35823;&#23548;&#24615;&#30340;&#25512;&#35770;&#32467;&#26524;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36890;&#29992;&#26041;&#27861;&#26469;&#22788;&#29702;&#36328;&#19981;&#21516;&#31867;&#21035;&#30340;SBI&#26041;&#27861;&#30340;&#27169;&#22411;&#38169;&#35823;&#24773;&#20917;&#12290;&#21033;&#29992;&#32479;&#35745;&#37327;&#30340;&#36873;&#25321;&#30830;&#23450;SBI&#20013;&#30340;&#35823;&#24046;&#31243;&#24230;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27491;&#21017;&#21270;&#25439;&#22833;&#20989;&#25968;&#65292;&#24809;&#32602;&#37027;&#20123;&#22686;&#21152;&#25968;&#25454;&#21644;&#27169;&#22411;&#20043;&#38388;&#19981;&#21305;&#37197;&#30340;&#32479;&#35745;&#37327;&#12290;&#20197;NPE&#21644;ABC&#20026;&#24212;&#29992;&#26696;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20154;&#24037;&#38169;&#35823;&#35268;&#33539;&#21270;&#30340;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#26469;&#33258;&#26080;&#32447;&#30005;&#20256;&#25773;&#39046;&#22495;&#30340;&#23454;&#38469;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulation-based inference (SBI) methods such as approximate Bayesian computation (ABC), synthetic likelihood, and neural posterior estimation (NPE) rely on simulating statistics to infer parameters of intractable likelihood models. However, such methods are known to yield untrustworthy and misleading inference outcomes under model misspecification, thus hindering their widespread applicability. In this work, we propose the first general approach to handle model misspecification that works across different classes of SBI methods. Leveraging the fact that the choice of statistics determines the degree of misspecification in SBI, we introduce a regularized loss function that penalises those statistics that increase the mismatch between the data and the model. Taking NPE and ABC as use cases, we demonstrate the superior performance of our method on high-dimensional time-series models that are artificially misspecified. We also apply our method to real data from the field of radio propagat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#26435;&#38160;&#24230;&#24418;&#24335;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861; WSAM&#65292;&#29992;&#20110;&#25913;&#36827; Sharpness-Aware Minimization (SAM) &#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#25913;&#36827;&#25110;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.15817</link><description>&lt;p&gt;
&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#65306;&#23558;&#38160;&#24230;&#20316;&#20026;&#27491;&#21017;&#21270;&#39033;&#30340;&#21152;&#26435;&#24418;&#24335;
&lt;/p&gt;
&lt;p&gt;
Sharpness-Aware Minimization Revisited: Weighted Sharpness as a Regularization Term. (arXiv:2305.15817v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#26435;&#38160;&#24230;&#24418;&#24335;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861; WSAM&#65292;&#29992;&#20110;&#25913;&#36827; Sharpness-Aware Minimization (SAM) &#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#25913;&#36827;&#25110;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#19982;&#26368;&#23567;&#20540;&#30340;&#24179;&#22374;&#24230;&#23494;&#20999;&#30456;&#20851;&#65292;&#23548;&#33268;&#21457;&#23637;&#20102;Sharpness-Aware Minimization (SAM)&#26469;&#23547;&#25214;&#26356;&#24179;&#22374;&#30340;&#26368;&#23567;&#20540;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102; SAM &#30340;&#25439;&#22833;&#20989;&#25968;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#20026;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026; WSAM&#65292;&#36890;&#36807;&#23558;&#38160;&#24230;&#20316;&#20026;&#27491;&#21017;&#21270;&#39033;&#36827;&#34892;&#25913;&#36827;&#12290;&#25105;&#20204;&#36890;&#36807;PAC&#21644;Bayes-PAC&#25216;&#26415;&#30340;&#32467;&#21512;&#35777;&#26126;&#20102;&#23427;&#30340;&#27867;&#21270;&#36793;&#30028;&#65292;&#24182;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#23427;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982; vanilla optimizer&#12289;SAM &#21450;&#20854;&#21464;&#20307;&#30456;&#27604;&#65292;WSAM &#21462;&#24471;&#20102;&#25913;&#21892;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#25110;&#32773;&#33267;&#23569;&#26159;&#38750;&#24120;&#26377;&#31454;&#20105;&#21147;&#30340;&#12290;&#20195;&#30721;&#21487;&#20174; https://github.com/intelligent-machine-learning/dlrover/tree/master/atorch/atorch/optimizers &#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) generalization is known to be closely related to the flatness of minima, leading to the development of Sharpness-Aware Minimization (SAM) for seeking flatter minima and better generalization. In this paper, we revisit the loss of SAM and propose a more general method, called WSAM, by incorporating sharpness as a regularization term. We prove its generalization bound through the combination of PAC and Bayes-PAC techniques, and evaluate its performance on various public datasets. The results demonstrate that WSAM achieves improved generalization, or is at least highly competitive, compared to the vanilla optimizer, SAM and its variants. The code is available at https://github.com/intelligent-machine-learning/dlrover/tree/master/atorch/atorch/optimizers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#28436;&#31034;&#30340;&#33258;&#20027;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;IBC&#65289;&#65292;&#36890;&#36807;&#36741;&#21161;&#20195;&#29702;&#21644;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#21452;&#21521;&#30446;&#26631;&#35838;&#31243;&#65292;&#33021;&#22815;&#22312;&#26080;&#38656;&#20808;&#21069;&#25968;&#25454;&#20381;&#36182;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20174;&#38750;&#21608;&#26399;&#24615;&#20132;&#20114;&#20013;&#23398;&#20064;&#65292;&#24182;&#22312;&#31232;&#30095;&#20219;&#21153;&#30456;&#20851;&#20132;&#20114;&#30340;&#29615;&#22659;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.09943</link><description>&lt;p&gt;
&#26080;&#38656;&#28436;&#31034;&#30340;&#33258;&#20027;&#22686;&#24378;&#23398;&#20064;&#65306;&#38544;&#24335;&#21452;&#21521;&#35838;&#31243;&#27861;
&lt;/p&gt;
&lt;p&gt;
Demonstration-free Autonomous Reinforcement Learning via Implicit and Bidirectional Curriculum. (arXiv:2305.09943v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#28436;&#31034;&#30340;&#33258;&#20027;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;IBC&#65289;&#65292;&#36890;&#36807;&#36741;&#21161;&#20195;&#29702;&#21644;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#21452;&#21521;&#30446;&#26631;&#35838;&#31243;&#65292;&#33021;&#22815;&#22312;&#26080;&#38656;&#20808;&#21069;&#25968;&#25454;&#20381;&#36182;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20174;&#38750;&#21608;&#26399;&#24615;&#20132;&#20114;&#20013;&#23398;&#20064;&#65292;&#24182;&#22312;&#31232;&#30095;&#20219;&#21153;&#30456;&#20851;&#20132;&#20114;&#30340;&#29615;&#22659;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#24378;&#21270;&#23398;&#20064;&#22312;&#20165;&#36890;&#36807;&#19982;&#29615;&#22659;&#20132;&#20114;&#26469;&#33719;&#24471;&#22797;&#26434;&#25216;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23427;&#20551;&#35774;&#22312;&#27599;&#20010;&#21608;&#26399;&#32467;&#26463;&#26102;&#37117;&#21487;&#20197;&#36731;&#26131;&#22320;&#22238;&#21040;&#21021;&#22987;&#29366;&#24577;&#12290;&#36825;&#31181;&#20551;&#35774;&#22952;&#30861;&#20102;&#20855;&#36523;&#20195;&#29702;&#30340;&#33258;&#20027;&#23398;&#20064;&#65292;&#22240;&#20026;&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#36827;&#34892;&#37325;&#32622;&#38656;&#35201;&#32791;&#36153;&#26102;&#38388;&#21644;&#32321;&#29712;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#33021;&#22815;&#20174;&#38750;&#21608;&#26399;&#24615;&#20132;&#20114;&#20013;&#23398;&#20064;&#30340;&#33258;&#20027;&#24378;&#21270;&#23398;&#20064;&#65288;ARL&#65289;&#26041;&#27861;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;ARL&#26041;&#27861;&#21463;&#21040;&#20854;&#23545;&#20808;&#21069;&#25968;&#25454;&#30340;&#20381;&#36182;&#30340;&#38480;&#21046;&#65292;&#26080;&#27861;&#22312;&#20219;&#21153;&#30456;&#20851;&#20132;&#20114;&#31232;&#30095;&#30340;&#29615;&#22659;&#20013;&#23398;&#20064;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38544;&#24335;&#21644;&#21452;&#21521;&#35838;&#31243;&#30340;&#26080;&#28436;&#31034;ARL&#31639;&#27861;&#65288;IBC&#65289;&#12290;&#36890;&#36807;&#36741;&#21161;&#20195;&#29702;&#20197;&#21450;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#21452;&#21521;&#30446;&#26631;&#35838;&#31243;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#65292;&#29978;&#33267;&#27604;&#21033;&#29992;&#28436;&#31034;&#30340;&#26041;&#27861;&#36824;&#35201;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
While reinforcement learning (RL) has achieved great success in acquiring complex skills solely from environmental interactions, it assumes that resets to the initial state are readily available at the end of each episode. Such an assumption hinders the autonomous learning of embodied agents due to the time-consuming and cumbersome workarounds for resetting in the physical world. Hence, there has been a growing interest in autonomous RL (ARL) methods that are capable of learning from non-episodic interactions. However, existing works on ARL are limited by their reliance on prior data and are unable to learn in environments where task-relevant interactions are sparse. In contrast, we propose a demonstration-free ARL algorithm via Implicit and Bi-directional Curriculum (IBC). With an auxiliary agent that is conditionally activated upon learning progress and a bidirectional goal curriculum based on optimal transport, our method outperforms previous methods, even the ones that leverage dem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#21152;&#26435;&#30340;&#26368;&#23567;&#26497;&#22823;&#39118;&#38505;&#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#36991;&#20813;&#21327;&#21464;&#37327;&#28418;&#31227;&#23545;&#30417;&#30563;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.08637</link><description>&lt;p&gt;
&#20026;&#21327;&#21464;&#37327;&#28418;&#31227;&#33258;&#36866;&#24212;&#24341;&#20837;&#21452;&#37325;&#21152;&#26435;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Double-Weighting for Covariate Shift Adaptation. (arXiv:2305.08637v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08637
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#21152;&#26435;&#30340;&#26368;&#23567;&#26497;&#22823;&#39118;&#38505;&#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#36991;&#20813;&#21327;&#21464;&#37327;&#28418;&#31227;&#23545;&#30417;&#30563;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#23398;&#20064;&#20013;&#24120;&#24120;&#21463;&#21040;&#21327;&#21464;&#37327;&#28418;&#31227;&#24433;&#21709;&#65292;&#21363;&#35757;&#32451;&#26679;&#26412;&#21644;&#27979;&#35797;&#26679;&#26412;&#30340;&#23454;&#20363;&#36793;&#32536;&#20998;&#24067;&#19981;&#21516;&#20294;&#26631;&#31614;&#26465;&#20214;&#30456;&#21516;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#27604;&#29575;p_te&#65288;x&#65289;/p_tr&#65288;x&#65289;&#23545;&#35757;&#32451;&#26679;&#26412;&#36827;&#34892;&#21152;&#26435;&#65288;&#37325;&#26032;&#21152;&#26435;&#26041;&#27861;&#65289;&#65292;&#25110;&#32773;&#20351;&#29992;&#27604;&#29575;p_tr&#65288;x&#65289;/p_te&#65288;x&#65289;&#23545;&#27979;&#35797;&#26679;&#26412;&#36827;&#34892;&#21152;&#26435;&#65288;&#40065;&#26834;&#26041;&#27861;&#65289;&#26469;&#35299;&#20915;&#36825;&#31181;&#21327;&#21464;&#37327;&#28418;&#31227;&#12290;&#28982;&#32780;&#65292;&#22312;&#25903;&#25345;&#19981;&#21305;&#37197;&#25110;&#19978;&#36848;&#27604;&#29575;&#21462;&#22823;&#20540;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#21487;&#33021;&#24456;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#26497;&#22823;&#39118;&#38505;&#20998;&#31867;(MRC)&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35757;&#32451;&#26679;&#26412;&#21644;&#27979;&#35797;&#26679;&#26412;&#36827;&#34892;&#21152;&#26435;&#26469;&#36991;&#20813;&#36825;&#31181;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#26377;&#25928;&#30340;&#25216;&#26415;&#26469;&#33719;&#24471;&#20004;&#32452;&#21152;&#26435;&#65292;&#24182;&#25512;&#24191;&#20102;&#20256;&#32479;&#30340;&#26680;&#22343;&#20540;&#21305;&#37197;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26469;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised learning is often affected by a covariate shift in which the marginal distributions of instances (covariates $x$) of training and testing samples $\mathrm{p}_\text{tr}(x)$ and $\mathrm{p}_\text{te}(x)$ are different but the label conditionals coincide. Existing approaches address such covariate shift by either using the ratio $\mathrm{p}_\text{te}(x)/\mathrm{p}_\text{tr}(x)$ to weight training samples (reweighting methods) or using the ratio $\mathrm{p}_\text{tr}(x)/\mathrm{p}_\text{te}(x)$ to weight testing samples (robust methods). However, the performance of such approaches can be poor under support mismatch or when the above ratios take large values. We propose a minimax risk classification (MRC) approach for covariate shift adaptation that avoids such limitations by weighting both training and testing samples. In addition, we develop effective techniques that obtain both sets of weights and generalize the conventional kernel mean matching method. We provide novel genera
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#19977;&#32500;&#26080;&#30417;&#30563;&#21644;(&#28145;&#24230;)&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22686;&#26448;&#21046;&#36896;&#20214;&#23380;&#38553;&#29575;&#26816;&#27979;&#30340;&#20307;&#32032;&#32423;&#20998;&#31867;&#65292;&#24471;&#20986;&#20351;&#29992;&#20307;&#32032;&#32423;&#20998;&#31867;&#30340;&#19977;&#32500;DL&#27169;&#22411;&#22312;AM&#38646;&#20214;&#30340;&#23380;&#38553;&#29575;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#30340;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2305.07894</link><description>&lt;p&gt;
&#21033;&#29992;&#19977;&#32500;&#26080;&#30417;&#30563;&#21644;(&#28145;&#24230;)&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22686;&#26448;&#21046;&#36896;&#20214;&#23380;&#38553;&#29575;&#26816;&#27979;&#30340;&#20307;&#32032;&#32423;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Voxel-wise classification for porosity investigation of additive manufactured parts with 3D unsupervised and (deeply) supervised neural networks. (arXiv:2305.07894v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#19977;&#32500;&#26080;&#30417;&#30563;&#21644;(&#28145;&#24230;)&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22686;&#26448;&#21046;&#36896;&#20214;&#23380;&#38553;&#29575;&#26816;&#27979;&#30340;&#20307;&#32032;&#32423;&#20998;&#31867;&#65292;&#24471;&#20986;&#20351;&#29992;&#20307;&#32032;&#32423;&#20998;&#31867;&#30340;&#19977;&#32500;DL&#27169;&#22411;&#22312;AM&#38646;&#20214;&#30340;&#23380;&#38553;&#29575;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#26448;&#21046;&#36896;&#24050;&#25104;&#20026;&#19968;&#31181;&#29983;&#20135;&#36827;&#31243;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#25968;&#23383;&#27169;&#22411;&#20013;&#29983;&#20135;&#26679;&#26412;&#12290;&#20026;&#20102;&#30830;&#20445;&#22312;&#25972;&#20010;&#21046;&#36896;&#25209;&#27425;&#20013;&#30340;&#25152;&#26377;&#20135;&#21697;&#26679;&#26412;&#37117;&#31526;&#21512;&#36136;&#37327;&#26631;&#20934;&#65292;&#36890;&#24120;&#20351;&#29992;X&#23556;&#32447;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;X-CT&#65289;&#19982;&#33258;&#21160;&#21270;&#24322;&#24120;&#26816;&#27979;&#30456;&#32467;&#21512;&#12290;&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#26368;&#36817;&#30340;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#65292;&#23545;&#26469;&#33258;X-CT&#22270;&#20687;&#30340;AM&#26679;&#21697;&#20013;&#30340;&#23380;&#38553;&#20998;&#26512;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#20197;&#25509;&#21463;3D&#36755;&#20837;&#25968;&#25454;&#36827;&#34892;&#20307;&#32032;&#32423;&#20998;&#31867;&#65292;&#20174;&#32780;&#38477;&#20302;&#35745;&#31639;&#35201;&#27714;&#65292;&#25552;&#39640;&#25928;&#29575;&#21644;&#36890;&#29992;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#20307;&#32032;&#32423;&#20998;&#31867;&#30340;&#19977;&#32500;DL&#27169;&#22411;&#22312;AM&#38646;&#20214;&#30340;&#23380;&#38553;&#29575;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Additive Manufacturing (AM) has emerged as a manufacturing process that allows the direct production of samples from digital models. To ensure that quality standards are met in all manufactured samples of a batch, X-ray computed tomography (X-CT) is often used combined with automated anomaly detection. For the latter, deep learning (DL) anomaly detection techniques are increasingly, as they can be trained to be robust to the material being analysed and resilient towards poor image quality. Unfortunately, most recent and popular DL models have been developed for 2D image processing, thereby disregarding valuable volumetric information.  This study revisits recent supervised (UNet, UNet++, UNet 3+, MSS-UNet) and unsupervised (VAE, ceVAE, gmVAE, vqVAE) DL models for porosity analysis of AM samples from X-CT images and extends them to accept 3D input data with a 3D-patch pipeline for lower computational requirements, improved efficiency and generalisability. The supervised models were trai
&lt;/p&gt;</description></item><item><title>COPP-Net&#26159;&#19968;&#31181;&#21033;&#29992;&#21152;&#26435;&#34917;&#19969;&#36136;&#37327;&#39044;&#27979;&#36827;&#34892;&#23616;&#37096;&#30456;&#20851;&#24615;&#20998;&#26512;&#30340;&#26080;&#21442;&#32771;&#28857;&#20113;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;NR-PCQA&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.07829</link><description>&lt;p&gt;
&#22522;&#20110;&#21152;&#26435;&#34917;&#19969;&#36136;&#37327;&#39044;&#27979;&#30340;&#26080;&#21442;&#32771;&#28857;&#20113;&#36136;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
No-Reference Point Cloud Quality Assessment via Weighted Patch Quality Prediction. (arXiv:2305.07829v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07829
&lt;/p&gt;
&lt;p&gt;
COPP-Net&#26159;&#19968;&#31181;&#21033;&#29992;&#21152;&#26435;&#34917;&#19969;&#36136;&#37327;&#39044;&#27979;&#36827;&#34892;&#23616;&#37096;&#30456;&#20851;&#24615;&#20998;&#26512;&#30340;&#26080;&#21442;&#32771;&#28857;&#20113;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;NR-PCQA&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;&#28857;&#20113;&#30340;&#19977;&#32500;&#35270;&#35273;&#24212;&#29992;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#28857;&#20113;&#36136;&#37327;&#35780;&#20272;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#23616;&#37096;&#21306;&#22495;&#30456;&#20851;&#24615;&#20998;&#26512;&#33021;&#21147;&#30340;&#26080;&#21442;&#32771;&#28857;&#20113;&#36136;&#37327;&#35780;&#20272;&#65288;NR-PCQA&#65289;&#26041;&#27861;&#65292;&#31216;&#20026;COPP-Net&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#28857;&#20113;&#20998;&#20026;&#34917;&#19969;&#65292;&#20026;&#27599;&#20010;&#34917;&#19969;&#29983;&#25104;&#32441;&#29702;&#21644;&#32467;&#26500;&#29305;&#24449;&#65292;&#24182;&#23558;&#23427;&#20204;&#34701;&#21512;&#25104;&#34917;&#19969;&#29305;&#24449;&#26469;&#39044;&#27979;&#34917;&#19969;&#36136;&#37327;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25910;&#38598;&#19968;&#20010;&#28857;&#20113;&#20013;&#25152;&#26377;&#34917;&#19969;&#30340;&#29305;&#24449;&#36827;&#34892;&#30456;&#20851;&#24615;&#20998;&#26512;&#65292;&#20197;&#33719;&#24471;&#30456;&#20851;&#24615;&#26435;&#37325;&#12290;&#26368;&#21518;&#65292;&#39044;&#27979;&#30340;&#36136;&#37327;&#21644;&#25152;&#26377;&#34917;&#19969;&#30340;&#30456;&#20851;&#26435;&#37325;&#29992;&#20110;&#25512;&#23548;&#26368;&#32456;&#36136;&#37327;&#24471;&#20998;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;NR-PCQA&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of 3D vision applications based on point clouds, point cloud quality assessment(PCQA) is becoming an important research topic. However, the prior PCQA methods ignore the effect of local quality variance across different areas of the point cloud. To take an advantage of the quality distribution imbalance, we propose a no-reference point cloud quality assessment (NR-PCQA) method with local area correlation analysis capability, denoted as COPP-Net. More specifically, we split a point cloud into patches, generate texture and structure features for each patch, and fuse them into patch features to predict patch quality. Then, we gather the features of all the patches of a point cloud for correlation analysis, to obtain the correlation weights. Finally, the predicted qualities and correlation weights for all the patches are used to derive the final quality score. Experimental results show that our method outperforms the state-of-the-art benchmark NR-PCQA methods. Th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#30417;&#30563;&#23398;&#20064;&#20013;&#27491;&#30830;&#25439;&#22833;&#20989;&#25968;&#30340;&#27010;&#24565;&#65292;&#20854;&#36890;&#36807;&#27010;&#29575;&#27979;&#24230;&#30340;&#26465;&#20214;&#27491;&#21017;&#27010;&#29575;&#27979;&#24230;&#35299;&#20915;&#32447;&#24615;&#31639;&#23376;&#26041;&#31243;&#30340;&#38382;&#39064;&#24471;&#21040;&#23450;&#20041;&#65292;&#36866;&#29992;&#20110;&#21487;&#27979;&#31354;&#38388;&#30340;&#36755;&#20837;&#31354;&#38388;&#21644;&#26631;&#31614;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.06348</link><description>&lt;p&gt;
&#24102;&#27010;&#29575;&#24577;&#23556;&#21644;&#26680;&#24179;&#22343;&#23884;&#20837;&#30340;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Supervised learning with probabilistic morphisms and kernel mean embeddings. (arXiv:2305.06348v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#30417;&#30563;&#23398;&#20064;&#20013;&#27491;&#30830;&#25439;&#22833;&#20989;&#25968;&#30340;&#27010;&#24565;&#65292;&#20854;&#36890;&#36807;&#27010;&#29575;&#27979;&#24230;&#30340;&#26465;&#20214;&#27491;&#21017;&#27010;&#29575;&#27979;&#24230;&#35299;&#20915;&#32447;&#24615;&#31639;&#23376;&#26041;&#31243;&#30340;&#38382;&#39064;&#24471;&#21040;&#23450;&#20041;&#65292;&#36866;&#29992;&#20110;&#21487;&#27979;&#31354;&#38388;&#30340;&#36755;&#20837;&#31354;&#38388;&#21644;&#26631;&#31614;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30417;&#30563;&#23398;&#20064;&#30340;&#29983;&#25104;&#27169;&#22411;&#20013;&#27491;&#30830;&#25439;&#22833;&#20989;&#25968;&#30340;&#27010;&#24565;&#65292;&#36866;&#29992;&#20110;&#21487;&#27979;&#31354;&#38388;&#30340;&#36755;&#20837;&#31354;&#38388;X&#21644;&#26631;&#31614;&#31354;&#38388;Y&#12290; &#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#27491;&#30830;&#25439;&#22833;&#20989;&#25968;&#24517;&#39035;&#27491;&#30830;&#22320;&#24230;&#37327;&#21487;&#33021;&#39044;&#27979;&#22120;&#30340;&#20551;&#35774;&#31354;&#38388;H&#20013;&#30340;&#20803;&#32032;&#19982;&#30417;&#31649;&#36816;&#31639;&#31526;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#32780;&#30417;&#31649;&#36816;&#31639;&#31526;&#21487;&#33021;&#19981;&#23646;&#20110;H&#12290; &#20026;&#20102;&#23450;&#20041;&#27491;&#30830;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#27010;&#29575;&#27979;&#24230;&#956;&#22312;&#25237;&#24433;&#928;X&#65306;X&#215;Y&#8594;X&#30456;&#23545;&#20110;&#27010;&#29575;&#27979;&#24230;&#956;&#119883;&#215;&#119884;&#30340;&#26465;&#20214;&#27491;&#21017;&#27010;&#29575;&#27979;&#24230;&#956;Y| X&#30340;&#29305;&#27530;&#24615;&#36136;&#30340;&#34920;&#24449;&#26041;&#27861;&#65292;&#20316;&#20026;&#32447;&#24615;&#31639;&#23376;&#26041;&#31243;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290; &#22914;&#26524;Y&#26159;&#19968;&#20010;&#20855;&#26377;Borel &#963;-&#20195;&#25968; BY&#30340;&#21487;&#20998;&#30340;&#21487;&#24230;&#37327;&#21270;&#25299;&#25169;&#31354;&#38388;&#65292;&#21017;&#25552;&#20986;&#20102;&#20851;&#20110;&#27010;&#29575;&#27979;&#24230;&#956;&#30456;&#23545;&#20110;&#25237;&#24433;&#928;X&#30340;&#26465;&#20214;&#27491;&#21017;&#27010;&#29575;&#27979;&#24230;&#956;Y| X&#30340;&#21478;&#19968;&#31181;&#29305;&#27530;&#24615;&#36136;&#30340;&#34920;&#24449;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper I propose a concept of a correct loss function in a generative model of supervised learning for an input space $\mathcal{X}$ and a label space $\mathcal{Y}$, which are measurable spaces. A correct loss function in a generative model of supervised learning must correctly measure the discrepancy between elements of a hypothesis space $\mathcal{H}$ of possible predictors and the supervisor operator, which may not belong to $\mathcal{H}$. To define correct loss functions, I propose a characterization of a regular conditional probability measure $\mu_{\mathcal{Y}|\mathcal{X}}$ for a probability measure $\mu$ on $\mathcal{X} \times \mathcal{Y}$ relative to the projection $\Pi_{\mathcal{X}}: \mathcal{X}\times\mathcal{Y}\to \mathcal{X}$ as a solution of a linear operator equation. If $\mathcal{Y}$ is a separable metrizable topological space with the Borel $\sigma$-algebra $ \mathcal{B} (\mathcal{Y})$, I propose another characterization of a regular conditional probability measure
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEGA&#30340;&#38170;&#28857;&#35270;&#22270;&#29992;&#20110;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#65292;&#35813;&#35270;&#22270;&#36890;&#36807;&#32467;&#26500;&#29109;&#24341;&#23548;&#20197;&#20445;&#25345;&#36755;&#20837;&#22270;&#30340;&#22522;&#26412;&#20449;&#24687;&#65292;&#36798;&#21040;&#20102;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.04501</link><description>&lt;p&gt;
SEGA&#65306;&#29992;&#20110;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#30340;&#32467;&#26500;&#29109;&#24341;&#23548;&#30340;&#38170;&#28857;&#35270;&#22270;
&lt;/p&gt;
&lt;p&gt;
SEGA: Structural Entropy Guided Anchor View for Graph Contrastive Learning. (arXiv:2305.04501v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEGA&#30340;&#38170;&#28857;&#35270;&#22270;&#29992;&#20110;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#65292;&#35813;&#35270;&#22270;&#36890;&#36807;&#32467;&#26500;&#29109;&#24341;&#23548;&#20197;&#20445;&#25345;&#36755;&#20837;&#22270;&#30340;&#22522;&#26412;&#20449;&#24687;&#65292;&#36798;&#21040;&#20102;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#27604;&#23398;&#20064;&#20013;&#65292;&#8220;&#35270;&#22270;&#8221;&#30340;&#36873;&#25321;&#25511;&#21046;&#30528;&#34920;&#31034;&#25429;&#33719;&#30340;&#20449;&#24687;&#24182;&#24433;&#21709;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20027;&#35201;&#30340;&#22270;&#20687;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#38543;&#26426;&#25439;&#22351;&#25110;&#23398;&#20064;&#26469;&#20135;&#29983;&#35270;&#22270;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20851;&#38190;&#20449;&#24687;&#30340;&#20002;&#22833;&#21644;&#35821;&#20041;&#20449;&#24687;&#30340;&#21464;&#21270;&#12290;&#23545;&#20110;&#23545;&#27604;&#23398;&#20064;&#26469;&#35828;&#65292;&#20445;&#25345;&#36755;&#20837;&#22270;&#24418;&#30340;&#22522;&#26412;&#20449;&#24687;&#30340;&#38170;&#28857;&#35270;&#22270;&#24456;&#23569;&#21463;&#21040;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#22522;&#20110;&#22270;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#36825;&#20010;&#38170;&#28857;&#35270;&#22270;&#30340;&#23450;&#20041;&#65307;&#25442;&#21477;&#35805;&#35828;&#65292;&#8220;&#20855;&#26377;&#36755;&#20837;&#22270;&#30340;&#22522;&#26412;&#20449;&#24687;&#30340;&#38170;&#28857;&#35270;&#22270;&#24212;&#35813;&#20855;&#26377;&#26368;&#23567;&#30340;&#32467;&#26500;&#19981;&#30830;&#23450;&#24615;&#8221;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#32467;&#26500;&#29109;&#23454;&#29616;&#20102;&#35813;&#38170;&#28857;&#35270;&#22270;&#65292;&#31216;&#20026;SEGA&#65292;&#29992;&#20110;&#22270;&#23545;&#27604;&#23398;&#20064;&#12290;&#25105;&#20204;&#23545;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#39564;&#35777;&#65292;&#21253;&#25324;&#22312;&#26080;&#30417;&#30563;&#12289;&#21322;&#30417;&#30563;&#21644;&#30417;&#30563;&#19979;&#30340;&#22270;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
In contrastive learning, the choice of ``view'' controls the information that the representation captures and influences the performance of the model. However, leading graph contrastive learning methods generally produce views via random corruption or learning, which could lead to the loss of essential information and alteration of semantic information. An anchor view that maintains the essential information of input graphs for contrastive learning has been hardly investigated. In this paper, based on the theory of graph information bottleneck, we deduce the definition of this anchor view; put differently, \textit{the anchor view with essential information of input graph is supposed to have the minimal structural uncertainty}. Furthermore, guided by structural entropy, we implement the anchor view, termed \textbf{SEGA}, for graph contrastive learning. We extensively validate the proposed anchor view on various benchmarks regarding graph classification under unsupervised, semi-supervise
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;Bootstrap&#23398;&#20064;&#29992;&#20110;&#36828;&#31243;&#30417;&#30563;&#25968;&#25454;&#32852;&#21512;&#25277;&#21462;&#12290;&#36890;&#36807;&#25506;&#32034;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#21644;&#33258;&#25105;&#38598;&#25104;&#27491;&#21017;&#21270;&#22120;&#65292;&#20351;&#24471;&#27169;&#22411;&#22312;&#26089;&#26399;&#24555;&#36895;&#25910;&#25947;&#24182;&#19988;&#32531;&#35299;&#20102;&#22122;&#22768;&#26631;&#31614;&#20135;&#29983;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#20004;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#23454;&#20307;&#23545;&#25277;&#21462;&#21644;&#20851;&#31995;&#25277;&#21462;&#26041;&#38754;&#30340;F1&#24471;&#20998;&#20998;&#21035;&#25552;&#39640;&#20102;4.43%&#21644;4.92%&#12290;</title><link>http://arxiv.org/abs/2305.03827</link><description>&lt;p&gt;
&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;Bootstrap&#23398;&#20064;&#29992;&#20110;&#36828;&#31243;&#30417;&#30563;&#25968;&#25454;&#32852;&#21512;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Bootstrap Learning for Joint Extraction on Distantly-Supervised Data. (arXiv:2305.03827v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;Bootstrap&#23398;&#20064;&#29992;&#20110;&#36828;&#31243;&#30417;&#30563;&#25968;&#25454;&#32852;&#21512;&#25277;&#21462;&#12290;&#36890;&#36807;&#25506;&#32034;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#21644;&#33258;&#25105;&#38598;&#25104;&#27491;&#21017;&#21270;&#22120;&#65292;&#20351;&#24471;&#27169;&#22411;&#22312;&#26089;&#26399;&#24555;&#36895;&#25910;&#25947;&#24182;&#19988;&#32531;&#35299;&#20102;&#22122;&#22768;&#26631;&#31614;&#20135;&#29983;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#20004;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#23454;&#20307;&#23545;&#25277;&#21462;&#21644;&#20851;&#31995;&#25277;&#21462;&#26041;&#38754;&#30340;F1&#24471;&#20998;&#20998;&#21035;&#25552;&#39640;&#20102;4.43%&#21644;4.92%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22788;&#29702;&#24102;&#26377;&#27169;&#31946;&#25110;&#22122;&#22768;&#26631;&#31614;&#30340;&#36828;&#31243;&#30417;&#30563;&#25968;&#25454;&#26102;&#65292;&#32852;&#21512;&#25277;&#21462;&#23454;&#20307;&#23545;&#21450;&#20854;&#20851;&#31995;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;Bootstrap&#23398;&#20064;&#65292;&#20854;&#21160;&#26426;&#26159;&#26681;&#25454;&#30452;&#35273;&#65292;&#19968;&#20010;&#23454;&#20363;&#30340;&#19981;&#30830;&#23450;&#24615;&#36234;&#39640;&#65292;&#27169;&#22411;&#32622;&#20449;&#24230;&#19982;&#30495;&#23454;&#26631;&#31614;&#19981;&#19968;&#33268;&#30340;&#21487;&#33021;&#24615;&#23601;&#36234;&#22823;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25506;&#32034;&#23454;&#20363;&#32423;&#21035;&#30340;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#65292;&#21019;&#24314;&#19968;&#20010;&#39640;&#32622;&#20449;&#30340;&#21021;&#22987;&#26679;&#20363;&#38598;&#12290;&#36825;&#26679;&#30340;&#23376;&#38598;&#29992;&#20110;&#36807;&#28388;&#22122;&#22768;&#23454;&#20363;&#65292;&#24182;&#26377;&#21161;&#20110;&#27169;&#22411;&#22312;&#26089;&#26399;&#24555;&#36895;&#25910;&#25947;&#12290;&#22312;Bootstrap&#23398;&#20064;&#26399;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#33258;&#25105;&#38598;&#25104;&#20316;&#20026;&#27491;&#21017;&#21270;&#22120;&#65292;&#20197;&#20943;&#36731;&#22122;&#22768;&#26631;&#31614;&#20135;&#29983;&#30340;&#27169;&#22411;&#38388;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23450;&#20041;&#32852;&#21512;&#26631;&#35760;&#27010;&#29575;&#30340;&#27010;&#29575;&#26041;&#24046;&#65292;&#20197;&#20272;&#35745;&#20869;&#37096;&#27169;&#22411;&#21442;&#25968;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#29992;&#20110;&#36873;&#25321;&#21644;&#24314;&#31435;&#26032;&#30340;&#21487;&#38752;&#35757;&#32451;&#23454;&#20363;&#36827;&#34892;&#19979;&#19968;&#27425;&#36845;&#20195;&#12290;&#20004;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#22312;&#23454;&#20307;&#23545;&#25277;&#21462;&#21644;&#20851;&#31995;&#25277;&#21462;&#26041;&#38754;&#30340;F1&#24471;&#20998;&#20998;&#21035;&#25552;&#39640;&#20102;4.43%&#21644;4.92%&#12290;
&lt;/p&gt;
&lt;p&gt;
Jointly extracting entity pairs and their relations is challenging when working on distantly-supervised data with ambiguous or noisy labels. To mitigate such impact, we propose uncertainty-aware bootstrap learning, which is motivated by the intuition that the higher uncertainty of an instance, the more likely the model confidence is inconsistent with the ground truths. Specifically, we first explore instance-level data uncertainty to create an initial high-confident examples. Such subset serves as filtering noisy instances and facilitating the model to converge fast at the early stage. During bootstrap learning, we propose self-ensembling as a regularizer to alleviate inter-model uncertainty produced by noisy labels. We further define probability variance of joint tagging probabilities to estimate inner-model parametric uncertainty, which is used to select and build up new reliable training instances for the next iteration. Experimental results on two large datasets reveal that our app
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;pFedGate&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#39640;&#25928;&#30340;&#26041;&#24335;&#23398;&#20064;&#31232;&#30095;&#30340;&#26412;&#22320;&#27169;&#22411;&#65292;&#20351;&#24471;&#23458;&#25143;&#31471;&#21487;&#20197;&#21457;&#25381;&#20854;&#27169;&#22411;&#23481;&#37327;&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.02776</link><description>&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#27169;&#22411;&#33258;&#36866;&#24212;&#23454;&#29616;&#39640;&#25928;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Personalized Federated Learning via Sparse Model-Adaptation. (arXiv:2305.02776v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02776
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;pFedGate&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#39640;&#25928;&#30340;&#26041;&#24335;&#23398;&#20064;&#31232;&#30095;&#30340;&#26412;&#22320;&#27169;&#22411;&#65292;&#20351;&#24471;&#23458;&#25143;&#31471;&#21487;&#20197;&#21457;&#25381;&#20854;&#27169;&#22411;&#23481;&#37327;&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26088;&#22312;&#20026;&#22810;&#20010;&#23458;&#25143;&#31471;&#22521;&#35757;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#19981;&#20849;&#20139;&#20854;&#31169;&#26377;&#25968;&#25454;&#12290;&#30001;&#20110;&#23458;&#25143;&#31471;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#30340;&#24322;&#26500;&#24615;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20010;&#24615;&#21270;FL&#65292;&#36890;&#36807;&#36741;&#21161;&#20840;&#23616;&#27169;&#22411;&#23398;&#20064;&#24182;&#37096;&#32626;&#19981;&#21516;&#30340;&#23616;&#37096;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#23458;&#25143;&#31471;&#21487;&#33021;&#22312;&#35745;&#31639;&#21644;&#36890;&#20449;&#36164;&#28304;&#26041;&#38754;&#23384;&#22312;&#24322;&#36136;&#24615;&#12290;&#20010;&#24615;&#21270;&#27169;&#22411;&#30340;&#23481;&#37327;&#21644;&#25928;&#29575;&#21463;&#21040;&#26368;&#20302;&#36164;&#28304;&#23458;&#25143;&#31471;&#30340;&#38480;&#21046;&#65292;&#23548;&#33268;&#20010;&#24615;&#21270;FL&#30340;&#24615;&#33021;&#19981;&#20339;&#21644;&#23454;&#29992;&#24615;&#26377;&#38480;&#12290;&#20026;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;pFedGate&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#39640;&#25928;&#22320;&#23398;&#20064;&#31232;&#30095;&#30340;&#26412;&#22320;&#27169;&#22411;&#26469;&#23454;&#29616;&#39640;&#25928;&#20010;&#24615;&#21270;FL&#12290;&#36890;&#36807;&#36731;&#37327;&#32423;&#21487;&#35757;&#32451;&#30340;&#38376;&#25511;&#23618;&#65292;pFedGate&#33021;&#22815;&#20135;&#29983;&#19981;&#21516;&#30340;&#31232;&#30095;&#27169;&#22411;&#65292;&#20174;&#32780;&#21457;&#25381;&#23458;&#25143;&#31471;&#22312;&#27169;&#22411;&#23481;&#37327;&#26041;&#38754;&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#32771;&#34385;&#21040;&#24322;&#26500;&#25968;&#25454;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) aims to train machine learning models for multiple clients without sharing their own private data. Due to the heterogeneity of clients' local data distribution, recent studies explore the personalized FL that learns and deploys distinct local models with the help of auxiliary global models. However, the clients can be heterogeneous in terms of not only local data distribution, but also their computation and communication resources. The capacity and efficiency of personalized models are restricted by the lowest-resource clients, leading to sub-optimal performance and limited practicality of personalized FL. To overcome these challenges, we propose a novel approach named pFedGate for efficient personalized FL by adaptively and efficiently learning sparse local models. With a lightweight trainable gating layer, pFedGate enables clients to reach their full potential in model capacity by generating different sparse models accounting for both the heterogeneous data di
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#36866;&#24212;&#26410;&#30693;&#20998;&#24067;&#28418;&#31227;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#30456;&#23545;&#20110;&#24403;&#21069;&#20998;&#24067;&#22312;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#19968;&#20010;&#20989;&#25968;&#26063;&#65292;&#19988;&#35823;&#24046;&#20960;&#20046;&#19982;&#39044;&#20808;&#30693;&#36947;&#28418;&#31227;&#22823;&#23567;&#30340;&#23398;&#20064;&#31639;&#27861;&#30456;&#21516;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#35813;&#31639;&#27861;&#36866;&#24212;&#25968;&#25454;&#65292;&#22240;&#27492;&#21487;&#20197;&#20445;&#35777;&#27604;&#20381;&#36182;&#20110;&#28418;&#31227;&#23485;&#26494;&#38480;&#21046;&#30340;&#31639;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.02252</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#24212;&#26410;&#30693;&#20998;&#24067;&#28418;&#31227;&#30340;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Adaptive Algorithm for Learning with Unknown Distribution Drift. (arXiv:2305.02252v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02252
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#36866;&#24212;&#26410;&#30693;&#20998;&#24067;&#28418;&#31227;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#30456;&#23545;&#20110;&#24403;&#21069;&#20998;&#24067;&#22312;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#19968;&#20010;&#20989;&#25968;&#26063;&#65292;&#19988;&#35823;&#24046;&#20960;&#20046;&#19982;&#39044;&#20808;&#30693;&#36947;&#28418;&#31227;&#22823;&#23567;&#30340;&#23398;&#20064;&#31639;&#27861;&#30456;&#21516;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#35813;&#31639;&#27861;&#36866;&#24212;&#25968;&#25454;&#65292;&#22240;&#27492;&#21487;&#20197;&#20445;&#35777;&#27604;&#20381;&#36182;&#20110;&#28418;&#31227;&#23485;&#26494;&#38480;&#21046;&#30340;&#31639;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#21644;&#20998;&#26512;&#20102;&#19968;&#31181;&#23398;&#20064;&#26410;&#30693;&#20998;&#24067;&#28418;&#31227;&#30340;&#36890;&#29992;&#25216;&#26415;&#12290;&#32473;&#23450;&#19968;&#20010;&#20174;&#28418;&#31227;&#20998;&#24067;&#30340;&#26368;&#21518;$T$&#27493;&#20013;&#29420;&#31435;&#35266;&#27979;&#21040;&#30340;&#24207;&#21015;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;$T$&#26102;&#21051;&#19981;&#21152;&#21306;&#20998;&#22320;&#23398;&#20064;&#19968;&#20010;&#20989;&#25968;&#26063;&#65292;&#30456;&#23545;&#20110;&#24403;&#21069;&#20998;&#24067;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#19981;&#38656;&#35201;&#20851;&#20110;&#28418;&#31227;&#22823;&#23567;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#30456;&#21453;&#65292;&#35813;&#31639;&#27861;&#36866;&#24212;&#26679;&#26412;&#25968;&#25454;&#12290;&#22312;&#19981;&#26126;&#30830;&#20272;&#35745;&#28418;&#31227;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#31639;&#27861;&#23398;&#20064;&#30340;&#20989;&#25968;&#26063;&#30340;&#35823;&#24046;&#20960;&#20046;&#19982;&#39044;&#20808;&#30693;&#36947;&#28418;&#31227;&#22823;&#23567;&#30340;&#23398;&#20064;&#31639;&#27861;&#30456;&#21516;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#25105;&#20204;&#30340;&#31639;&#27861;&#36866;&#24212;&#25968;&#25454;&#65292;&#23427;&#21487;&#20197;&#20445;&#35777;&#27604;&#20381;&#36182;&#20110;&#28418;&#31227;&#23485;&#26494;&#38480;&#21046;&#30340;&#31639;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop and analyze a general technique for learning with an unknown distribution drift. Given a sequence of independent observations from the last $T$ steps of a drifting distribution, our algorithm agnostically learns a family of functions with respect to the current distribution at time $T$. Unlike previous work, our technique does not require prior knowledge about the magnitude of the drift. Instead, the algorithm adapts to the sample data. Without explicitly estimating the drift, the algorithm learns a family of functions with almost the same error as a learning algorithm that knows the magnitude of the drift in advance. Furthermore, since our algorithm adapts to the data, it can guarantee a better learning error than an algorithm that relies on loose bounds on the drift.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;GPT-3.5&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#65292;&#23454;&#29616;&#22312;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.01555</link><description>&lt;p&gt;
&#22914;&#20309;&#21457;&#25381;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#33021;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?. (arXiv:2305.01555v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;GPT-3.5&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#65292;&#23454;&#29616;&#22312;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#25193;&#23637;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#24191;&#27867;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#20294;&#26159;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#36824;&#27809;&#26377;&#24471;&#21040;&#20840;&#38754;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#35814;&#32454;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#20351;&#29992;GPT-3.5&#36827;&#34892;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#30340;&#22522;&#26412;&#26041;&#27861;&#8212;&#8212;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#25968;&#25454;&#29983;&#25104;&#12290;&#20026;&#20102;&#22686;&#24378;&#23569;&#26679;&#26412;&#24615;&#33021;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#19982;&#20197;&#21069;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#32780;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#25512;&#21160;&#20197;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#22312;&#22235;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#23569;&#26679;&#26412;&#32467;&#26524;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#21487;&#20197;&#28608;&#21457;&#26410;&#26469;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#33021;&#21147;&#30340;&#30740;&#31350;&#12290;&#20195;&#30721;&#21487;&#20197;&#22312; \url{https://github.com/zjunlp/DeepKE/tree/main/example/llm} &#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scaling language models have revolutionized widespread NLP tasks, yet little comprehensively explored few-shot relation extraction with large language models. In this paper, we investigate principal methodologies, in-context learning and data generation, for few-shot relation extraction via GPT-3.5 through exhaustive experiments. To enhance few-shot performance, we further propose task-related instructions and schema-constrained data generation. We observe that in-context learning can achieve performance on par with previous prompt learning approaches, and data generation with the large language model can boost previous solutions to obtain new state-of-the-art few-shot results on four widely-studied relation extraction datasets. We hope our work can inspire future research for the capabilities of large language models in few-shot relation extraction. Code is available in \url{https://github.com/zjunlp/DeepKE/tree/main/example/llm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25910;&#38598;&#21644;&#35780;&#20272;&#22810;&#26041;&#23545;&#35805;&#24773;&#20917;&#65292;&#25506;&#35752;&#20102;&#27169;&#22411;&#22312;&#32676;&#20307;&#23545;&#35805;&#20013;&#38656;&#35201;&#20855;&#22791;&#30340;&#25216;&#33021;&#65292;&#21457;&#29616;&#26032;&#25968;&#25454;&#38598;MultiLIGHT&#21487;&#20197;&#22312;&#36825;&#20010;&#39046;&#22495;&#24102;&#26469;&#26174;&#30528;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2304.13835</link><description>&lt;p&gt;
&#22810;&#26041;&#32842;&#22825;&#65306;&#20154;&#31867;&#21644;&#27169;&#22411;&#20013;&#30340;&#32676;&#32842;&#23545;&#35805;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models. (arXiv:2304.13835v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25910;&#38598;&#21644;&#35780;&#20272;&#22810;&#26041;&#23545;&#35805;&#24773;&#20917;&#65292;&#25506;&#35752;&#20102;&#27169;&#22411;&#22312;&#32676;&#20307;&#23545;&#35805;&#20013;&#38656;&#35201;&#20855;&#22791;&#30340;&#25216;&#33021;&#65292;&#21457;&#29616;&#26032;&#25968;&#25454;&#38598;MultiLIGHT&#21487;&#20197;&#22312;&#36825;&#20010;&#39046;&#22495;&#24102;&#26469;&#26174;&#30528;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#23545;&#35805;&#30740;&#31350;&#20027;&#35201;&#30740;&#31350;&#25104;&#23545;&#65288;&#21452;&#26041;&#65289;&#23545;&#35805;&#65292;&#24182;&#27809;&#26377;&#28041;&#21450;&#21040;&#22810;&#20110;&#20004;&#20010;&#20154;&#22312;&#19968;&#36215;&#23545;&#35805;&#30340;&#26085;&#24120;&#24773;&#26223;&#12290;&#26412;&#25991;&#20351;&#29992;LIGHT&#29615;&#22659;&#26500;&#24314;&#25509;&#22320;&#23545;&#35805;&#26469;&#25910;&#38598;&#21644;&#35780;&#20272;&#22810;&#26041;&#23545;&#35805;&#24773;&#20917;&#12290;&#25105;&#20204;&#23545;&#27604;&#22312;&#26032;&#25968;&#25454;&#38598;MultiLIGHT&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#29616;&#26377;&#30340;&#25104;&#23545;&#35757;&#32451;&#30340;&#23545;&#35805;&#27169;&#22411;&#20197;&#21450;&#24102;&#26377;&#23569;&#37327;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#23558;&#20844;&#24320;&#21457;&#24067;MultiLIGHT&#25968;&#25454;&#38598;&#65292;&#36825;&#23558;&#26377;&#21161;&#20110;&#22312;&#32676;&#20307;&#35774;&#32622;&#20013;&#24102;&#26469;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current dialogue research primarily studies pairwise (two-party) conversations, and does not address the everyday setting where more than two speakers converse together. In this work, we both collect and evaluate multi-party conversations to study this more general case. We use the LIGHT environment to construct grounded conversations, where each participant has an assigned character to role-play. We thus evaluate the ability of language models to act as one or more characters in such conversations. Models require two skills that pairwise-trained models appear to lack: (1) being able to decide when to talk; (2) producing coherent utterances grounded on multiple characters. We compare models trained on our new dataset to existing pairwise-trained dialogue models, as well as large language models with few-shot prompting. We find that our new dataset, MultiLIGHT, which we will publicly release, can help bring significant improvements in the group setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#38543;&#26426;&#22806;&#25512;&#25216;&#26415;&#65292;&#29992;&#20110;&#38477;&#20302;&#26465;&#20214;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#20559;&#24046;&#65292;&#24182;&#35777;&#26126;&#22312;&#38750;&#20984;&#20809;&#28369;&#30446;&#26631;&#20989;&#25968;&#20013;&#65292;&#23558;&#22806;&#25512;&#19982;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.10613</link><description>&lt;p&gt;
&#28040;&#38500;&#26465;&#20214;&#38543;&#26426;&#20248;&#21270;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Debiasing Conditional Stochastic Optimization. (arXiv:2304.10613v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#38543;&#26426;&#22806;&#25512;&#25216;&#26415;&#65292;&#29992;&#20110;&#38477;&#20302;&#26465;&#20214;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#20559;&#24046;&#65292;&#24182;&#35777;&#26126;&#22312;&#38750;&#20984;&#20809;&#28369;&#30446;&#26631;&#20989;&#25968;&#20013;&#65292;&#23558;&#22806;&#25512;&#19982;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35206;&#30422;&#20102;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#65292;&#21253;&#25324;&#25237;&#36164;&#32452;&#21512;&#36873;&#25321;&#12289;&#24378;&#21270;&#23398;&#20064;&#12289;&#40065;&#26834;&#23398;&#20064;&#12289;&#22240;&#26524;&#25512;&#26029;&#31561;&#30340;&#26465;&#20214;&#38543;&#26426;&#20248;&#21270;&#65288;CSO&#65289;&#38382;&#39064;&#12290;&#30001;&#20110;&#20854;&#23884;&#22871;&#32467;&#26500;&#65292;CSO&#30446;&#26631;&#30340;&#26679;&#26412;&#24179;&#22343;&#26799;&#24230;&#23384;&#22312;&#20559;&#24046;&#65292;&#22240;&#27492;&#38656;&#35201;&#36739;&#39640;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#25165;&#33021;&#36798;&#21040;&#25910;&#25947;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#38477;&#20302;&#20559;&#24046;&#30340;&#36890;&#29992;&#38543;&#26426;&#22806;&#25512;&#25216;&#26415;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#38750;&#20984;&#20809;&#28369;&#30446;&#26631;&#20989;&#25968;&#20013;&#65292;&#23558;&#36825;&#31181;&#22806;&#25512;&#19982;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#36798;&#21040;&#27604;&#29616;&#26377;&#30028;&#38480;&#26356;&#22909;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#29992;&#20110;&#26377;&#38480;&#21644;&#21464;&#37327;&#30340;CSO&#30340;&#26032;&#31639;&#27861;&#65292;&#20063;&#26174;&#33879;&#25913;&#36827;&#20102;&#29616;&#26377;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#30340;&#21435;&#20559;&#25216;&#26415;&#20063;&#21487;&#33021;&#26159;&#36866;&#29992;&#20110;&#20854;&#20182;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#30340;&#26377;&#36259;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the conditional stochastic optimization (CSO) problem which covers a variety of applications including portfolio selection, reinforcement learning, robust learning, causal inference, etc. The sample-averaged gradient of the CSO objective is biased due to its nested structure and therefore requires a high sample complexity to reach convergence. We introduce a general stochastic extrapolation technique that effectively reduces the bias. We show that for nonconvex smooth objectives, combining this extrapolation with variance reduction techniques can achieve a significantly better sample complexity than existing bounds. We also develop new algorithms for the finite-sum variant of CSO that also significantly improve upon existing results. Finally, we believe that our debiasing technique could be an interesting tool applicable to other stochastic optimization problems too.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21453;&#23556;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#21453;&#23556;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#25200;&#21160;&#35780;&#20998;&#20989;&#25968;&#65292;&#23558;&#25968;&#25454;&#32422;&#26463;&#21407;&#21017;&#24615;&#22320;&#25972;&#21512;&#21040;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#20197;&#21462;&#20195;&#20043;&#21069;&#37319;&#29992;&#30340;&#23548;&#33268;&#19981;&#33258;&#28982;&#26679;&#26412;&#30340;&#38408;&#20540;&#22788;&#29702;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.04740</link><description>&lt;p&gt;
&#21453;&#23556;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Reflected Diffusion Models. (arXiv:2304.04740v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04740
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21453;&#23556;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#21453;&#23556;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#25200;&#21160;&#35780;&#20998;&#20989;&#25968;&#65292;&#23558;&#25968;&#25454;&#32422;&#26463;&#21407;&#21017;&#24615;&#22320;&#25972;&#21512;&#21040;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#20197;&#21462;&#20195;&#20043;&#21069;&#37319;&#29992;&#30340;&#23548;&#33268;&#19981;&#33258;&#28982;&#26679;&#26412;&#30340;&#38408;&#20540;&#22788;&#29702;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#23558;&#25968;&#25454;&#26144;&#23556;&#21040;&#22122;&#22768;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#21453;&#21521;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22797;&#26434;&#20219;&#21153;&#65292;&#25968;&#20540;&#35823;&#24046;&#21487;&#20197;&#32047;&#31215;&#24182;&#23548;&#33268;&#39640;&#24230;&#19981;&#33258;&#28982;&#30340;&#26679;&#26412;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#36890;&#36807;&#38408;&#20540;&#22788;&#29702;&#26469;&#32531;&#35299;&#28418;&#31227;&#65292;&#27599;&#27425;&#25193;&#25955;&#27493;&#39588;&#21518;&#25237;&#24433;&#21040;&#33258;&#28982;&#25968;&#25454;&#22495;&#65288;&#20363;&#22914;&#22270;&#20687;&#30340;&#20687;&#32032;&#31354;&#38388;&#65289;&#65292;&#20294;&#36825;&#23548;&#33268;&#35757;&#32451;&#21644;&#29983;&#25104;&#36807;&#31243;&#20043;&#38388;&#23384;&#22312;&#19981;&#21305;&#37197;&#12290;&#20026;&#20102;&#20197;&#19968;&#31181;&#21407;&#21017;&#24615;&#30340;&#26041;&#24335;&#21512;&#24182;&#25968;&#25454;&#32422;&#26463;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21453;&#23556;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21453;&#21521;&#28436;&#21270;&#22312;&#25968;&#25454;&#25903;&#25345;&#30340;&#21453;&#23556;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#19978;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#19968;&#33324;&#21270;&#30340;&#20998;&#25968;&#21305;&#37197;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#25200;&#21160;&#35780;&#20998;&#20989;&#25968;&#65292;&#24182;&#25193;&#23637;&#20102;&#26631;&#20934;&#25193;&#25955;&#27169;&#22411;&#30340;&#20851;&#38190;&#32452;&#20214;&#65292;&#21253;&#25324;&#25193;&#25955;&#24341;&#23548;&#12289;&#22522;&#20110;&#20284;&#28982;&#30340;&#35757;&#32451;&#21644;ODE&#37319;&#26679;&#12290;&#25105;&#20204;&#36824;&#24357;&#21512;&#20102;&#38408;&#20540;&#22788;&#29702;&#30340;&#29702;&#35770;&#24046;&#36317;:&#36825;&#26679;&#30340;&#26041;&#26696;&#21482;&#26159;&#21453;&#23556;SDE&#30340;&#31163;&#25955;&#21270;&#12290;&#22312;&#26631;&#20934;&#22270;&#20687;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;
&lt;/p&gt;
&lt;p&gt;
Score-based diffusion models learn to reverse a stochastic differential equation that maps data to noise. However, for complex tasks, numerical error can compound and result in highly unnatural samples. Previous work mitigates this drift with thresholding, which projects to the natural data domain (such as pixel space for images) after each diffusion step, but this leads to a mismatch between the training and generative processes. To incorporate data constraints in a principled manner, we present Reflected Diffusion Models, which instead reverse a reflected stochastic differential equation evolving on the support of the data. Our approach learns the perturbed score function through a generalize score matching loss and extends key components of standard diffusion models including diffusion guidance, likelihood-based training, and ODE sampling. We also bridge the theoretical gap with thresholding: such schemes are just discretizations of reflected SDEs. On standard image benchmarks, our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MGTBench&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#20013;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#24191;&#27867;&#35780;&#20272;&#21644;&#20844;&#24320;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;MGT&#26816;&#27979;&#35780;&#20272;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#27604;&#36739;&#19981;&#21516;&#26816;&#27979;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.14822</link><description>&lt;p&gt;
MGTBench&#65306;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
MGTBench: Benchmarking Machine-Generated Text Detection. (arXiv:2303.14822v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MGTBench&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#20013;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#24191;&#27867;&#35780;&#20272;&#21644;&#20844;&#24320;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;MGT&#26816;&#27979;&#35780;&#20272;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#27604;&#36739;&#19981;&#21516;&#26816;&#27979;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#22810;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20102;&#38761;&#21629;&#24615;&#30340;&#21147;&#37327;&#65292;&#20363;&#22914;&#25991;&#26412;&#20998;&#31867;&#65292;&#24773;&#24863;&#20998;&#26512;&#65292;&#35821;&#35328;&#32763;&#35793;&#21644;&#38382;&#31572;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#65288;MGT&#65289;&#38543;&#30528;LLM&#21464;&#24471;&#36234;&#26469;&#36234;&#20808;&#36827;&#21644;&#26222;&#36941;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#31867;&#20284;&#20110;&#20154;&#31867;&#20889;&#20316;&#30340;&#35821;&#35328;&#65292;&#24456;&#38590;&#19982;&#20154;&#31867;&#20889;&#30340;&#25991;&#26412;&#21306;&#20998;&#24320;&#26469;&#65292;&#36825;&#24341;&#21457;&#20102;&#20851;&#20110;&#30495;&#23454;&#24615;&#65292;&#38382;&#36131;&#21644;&#28508;&#22312;&#20559;&#35265;&#30340;&#25285;&#24551;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MGT&#26816;&#27979;&#26041;&#27861;&#26159;&#22312;&#19981;&#21516;&#30340;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#65292;&#25968;&#25454;&#38598;&#21644;&#23454;&#39564;&#35774;&#32622;&#19979;&#36827;&#34892;&#35780;&#20272;&#30340;&#65292;&#23548;&#33268;&#32570;&#20047;&#36328;&#19981;&#21516;&#26041;&#27861;&#23398;&#30340;&#20840;&#38754;&#35780;&#20272;&#26694;&#26550;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#21517;&#20026;MGTBench&#30340;MGT&#26816;&#27979;&#22522;&#20934;&#26694;&#26550;&#26469;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#12290;&#23545;&#30001;ChatGPT&#29983;&#25104;&#30340;&#31572;&#26696;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#35813;&#27169;&#22411;&#26159;&#20013;&#22269;&#26368;&#20855;&#20195;&#34920;&#24615;&#21644;&#26368;&#24378;&#22823;&#30340;LLM&#27169;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;MGTBench&#25552;&#20379;&#20102;&#20844;&#24179;&#21644;&#20840;&#38754;&#30340;MGT&#26816;&#27979;&#35780;&#20272;&#65292;&#24182;&#20351;&#30740;&#31350;&#20154;&#21592;&#27604;&#36739;&#20102;&#19981;&#21516;&#26816;&#27979;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays large language models (LLMs) have shown revolutionary power in a variety of natural language processing (NLP) tasks such as text classification, sentiment analysis, language translation, and question-answering. In this way, detecting machine-generated texts (MGTs) is becoming increasingly important as LLMs become more advanced and prevalent. These models can generate human-like language that can be difficult to distinguish from text written by a human, which raises concerns about authenticity, accountability, and potential bias. However, existing detection methods against MGTs are evaluated under different model architectures, datasets, and experimental settings, resulting in a lack of a comprehensive evaluation framework across different methodologies  In this paper, we fill this gap by proposing the first benchmark framework for MGT detection, named MGTBench. Extensive evaluations on public datasets with curated answers generated by ChatGPT (the most representative and power
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24179;&#28369;&#65288;&#24378;&#65289;&#20984;&#26377;&#38480;&#21644;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#26080;&#26367;&#25442;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#25910;&#25947;&#19979;&#30028;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#27604;&#29616;&#26377;&#19979;&#30028;&#26356;&#32039;&#30340;$k$&#20381;&#36182;&#24615;&#30340;&#19979;&#30028;&#65292;&#24182;&#36890;&#36807;&#22312;&#24378;&#20984;&#21644;&#20984;&#24773;&#20917;&#19979;&#23436;&#20840;&#28040;&#38500;&#21152;&#26435;&#24179;&#22343;&#36845;&#20195;&#30340;&#19978;&#19979;&#30028;&#24046;&#36317;&#65292;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.07160</link><description>&lt;p&gt;
&#25913;&#36827; Shuffling SGD &#30340;&#25910;&#25947;&#19979;&#30028;&#65306;&#38543;&#26426;&#32622;&#25442;&#21450;&#20854;&#20182;
&lt;/p&gt;
&lt;p&gt;
Tighter Lower Bounds for Shuffling SGD: Random Permutations and Beyond. (arXiv:2303.07160v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24179;&#28369;&#65288;&#24378;&#65289;&#20984;&#26377;&#38480;&#21644;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#26080;&#26367;&#25442;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#25910;&#25947;&#19979;&#30028;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#27604;&#29616;&#26377;&#19979;&#30028;&#26356;&#32039;&#30340;$k$&#20381;&#36182;&#24615;&#30340;&#19979;&#30028;&#65292;&#24182;&#36890;&#36807;&#22312;&#24378;&#20984;&#21644;&#20984;&#24773;&#20917;&#19979;&#23436;&#20840;&#28040;&#38500;&#21152;&#26435;&#24179;&#22343;&#36845;&#20195;&#30340;&#19978;&#19979;&#30028;&#24046;&#36317;&#65292;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24179;&#28369;&#65288;&#24378;&#65289;&#20984;&#26377;&#38480;&#21644;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#26080;&#26367;&#25442;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#25910;&#25947;&#19979;&#30028;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#32467;&#26524;&#20391;&#37325;&#20110;&#26368;&#32456;&#36845;&#20195;&#19979;&#30028;&#65292;&#36825;&#20123;&#19979;&#30028;&#26159;&#20851;&#20110;&#32452;&#20214;&#25968;$n$&#21644;&#36845;&#20195;&#27425;&#25968;$K$&#30340;&#65292;&#25105;&#20204;&#23547;&#27714;&#21508;&#31181;&#24102;&#26435;&#37325;&#24179;&#22343;&#36845;&#20195;&#30340;&#19979;&#30028;&#65292;&#36825;&#20123;&#19979;&#30028;&#22312;&#21253;&#25324;&#26465;&#20214;&#25968;$k$&#22312;&#20869;&#30340;&#25152;&#26377;&#22240;&#32032;&#20013;&#37117;&#26159;&#32039;&#30340;&#12290;&#23545;&#20110;&#24102;&#38543;&#26426;&#27927;&#29260;&#30340;SGD&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#27604;&#29616;&#26377;&#19979;&#30028;&#26356;&#32039;&#30340;$k$&#20381;&#36182;&#24615;&#30340;&#19979;&#30028;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#39318;&#20010;&#22312;&#24378;&#20984;&#21644;&#20984;&#24773;&#20917;&#19979;&#23436;&#20840;&#28040;&#38500;&#21152;&#26435;&#24179;&#22343;&#36845;&#20195;&#30340;&#19978;&#19979;&#30028;&#24046;&#36317;&#30340;&#12290;&#25105;&#20204;&#36824;&#20026;&#20219;&#24847;&#22522;&#20110;&#32622;&#25442;&#30340;SGD&#35777;&#26126;&#20102;&#21152;&#26435;&#24179;&#22343;&#36845;&#20195;&#30340;&#19979;&#30028;&#65292;&#36825;&#36866;&#29992;&#20110;&#25152;&#26377;&#23567;&#24515;&#36873;&#25321;&#26368;&#20339;&#32622;&#25442;&#30340;&#21464;&#20307;&#12290;&#25105;&#20204;&#30340;&#19979;&#30028;&#22312;$n$&#21644;$k$&#22240;&#32032;&#26041;&#38754;&#37117;&#20248;&#20110;&#29616;&#26377;&#19979;&#30028;&#65292;&#20174;&#32780;&#19982;&#26368;&#36817;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#19978;&#30028;&#30456;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study convergence lower bounds of without-replacement stochastic gradient descent (SGD) for solving smooth (strongly-)convex finite-sum minimization problems. Unlike most existing results focusing on final iterate lower bounds in terms of the number of components $n$ and the number of epochs $K$, we seek bounds for arbitrary weighted average iterates that are tight in all factors including the condition number $\kappa$. For SGD with Random Reshuffling, we present lower bounds that have tighter $\kappa$ dependencies than existing bounds. Our results are the first to perfectly close the gap between lower and upper bounds for weighted average iterates in both strongly-convex and convex cases. We also prove weighted average iterate lower bounds for arbitrary permutation-based SGD, which apply to all variants that carefully choose the best permutation. Our bounds improve the existing bounds in factors of $n$ and $\kappa$ and thereby match the upper bounds shown for a recently proposed al
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20219;&#21153;&#30340;&#36890;&#29992;&#26694;&#26550;&#26469;&#35299;&#20915;Abstraction and Reasoning Corpus&#65288;ARC&#65289;&#38382;&#39064;&#65292;&#34429;&#28982;&#36824;&#27809;&#26377;&#22312;ARC&#19978;&#20987;&#36133;&#26368;&#20808;&#36827;&#30340;DSL&#27169;&#22411;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#35299;&#20915;&#20808;&#21069;&#26410;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.04091</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#23454;&#29616;&#35270;&#35273;&#25277;&#35937;&#21644;&#25512;&#29702;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Visual Abstraction and Reasoning through Language. (arXiv:2303.04091v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20219;&#21153;&#30340;&#36890;&#29992;&#26694;&#26550;&#26469;&#35299;&#20915;Abstraction and Reasoning Corpus&#65288;ARC&#65289;&#38382;&#39064;&#65292;&#34429;&#28982;&#36824;&#27809;&#26377;&#22312;ARC&#19978;&#20987;&#36133;&#26368;&#20808;&#36827;&#30340;DSL&#27169;&#22411;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#35299;&#20915;&#20808;&#21069;&#26410;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#22312;&#23616;&#38480;&#24212;&#29992;&#20013;&#24050;&#32463;&#36798;&#21040;&#20102;&#20154;&#31867;&#29978;&#33267;&#36229;&#36234;&#20154;&#31867;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#38590;&#20197;&#23637;&#29616;&#26356;&#24191;&#27867;&#21644;&#26356;&#28789;&#27963;&#30340;&#26234;&#33021;&#12290;Abstraction and Reasoning Corpus&#65288;ARC&#65289;&#26088;&#22312;&#35780;&#20272;AI&#31995;&#32479;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#30446;&#21069;&#22823;&#22810;&#25968;&#26041;&#27861;&#20381;&#36182;&#20110;&#31934;&#24515;&#35774;&#35745;&#30340;&#29305;&#23450;&#39046;&#22495;&#35821;&#35328;&#65288;DSL&#65289;&#65292;&#29992;&#20110;&#26292;&#21147;&#35299;&#20915;ARC&#20013;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20219;&#21153;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#36890;&#29992;&#26694;&#26550;&#26469;&#35299;&#20915;ARC&#38382;&#39064;&#12290;&#34429;&#28982;&#36824;&#27809;&#26377;&#22312;ARC&#19978;&#20987;&#36133;&#26368;&#20808;&#36827;&#30340;DSL&#27169;&#22411;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#35299;&#20915;&#20808;&#21069;&#26410;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Artificial Intelligence (AI) models have achieved human or even superhuman performance in narrowly defined applications, they still struggle to show signs of broader and more flexible intelligence. The Abstraction and Reasoning Corpus (ARC), introduced by Fran\c{c}ois Chollet, aims to assess how close AI systems are to human-like cognitive abilities. Most current approaches rely on carefully handcrafted domain-specific languages (DSLs), which are used to brute-force solutions to the tasks present in ARC. In this work, we propose a general framework for solving ARC based on natural language descriptions of the tasks. While not yet beating state-of-the-art DSL models on ARC, we demonstrate the immense potential of our approach hinted at by the ability to solve previously unsolved tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21152;&#26435;&#32858;&#21512;&#26041;&#27861;&#12290;&#20316;&#32773;&#21457;&#29616;&#26435;&#37325;&#24635;&#21644;&#21487;&#33021;&#23567;&#20110;1&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;&#20316;&#32773;&#25506;&#32034;&#20102;&#26368;&#20248;&#32553;&#23567;&#22240;&#23376;&#22914;&#20309;&#21463;&#21040;&#23458;&#25143;&#31471;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#26412;&#22320;&#21608;&#26399;&#30340;&#24433;&#21709;&#65292;&#24182;&#20351;&#29992;&#23458;&#25143;&#31471;&#30456;&#24178;&#24615;&#30740;&#31350;&#20102;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#30456;&#23545;&#32858;&#21512;&#26435;&#37325;&#20197;&#25551;&#32472;&#23458;&#25143;&#31471;&#30340;&#37325;&#35201;&#24615;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65288;FLLAW&#65289;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#21487;&#23398;&#20064;&#32858;&#21512;&#26435;&#37325;&#21644;&#20840;&#23616;&#26435;&#37325;&#32553;&#23567;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2302.10911</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21152;&#26435;&#32858;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Revisiting Weighted Aggregation in Federated Learning with Neural Networks. (arXiv:2302.10911v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10911
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21152;&#26435;&#32858;&#21512;&#26041;&#27861;&#12290;&#20316;&#32773;&#21457;&#29616;&#26435;&#37325;&#24635;&#21644;&#21487;&#33021;&#23567;&#20110;1&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;&#20316;&#32773;&#25506;&#32034;&#20102;&#26368;&#20248;&#32553;&#23567;&#22240;&#23376;&#22914;&#20309;&#21463;&#21040;&#23458;&#25143;&#31471;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#26412;&#22320;&#21608;&#26399;&#30340;&#24433;&#21709;&#65292;&#24182;&#20351;&#29992;&#23458;&#25143;&#31471;&#30456;&#24178;&#24615;&#30740;&#31350;&#20102;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#30456;&#23545;&#32858;&#21512;&#26435;&#37325;&#20197;&#25551;&#32472;&#23458;&#25143;&#31471;&#30340;&#37325;&#35201;&#24615;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65288;FLLAW&#65289;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#21487;&#23398;&#20064;&#32858;&#21512;&#26435;&#37325;&#21644;&#20840;&#23616;&#26435;&#37325;&#32553;&#23567;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#65292;&#23545;&#26412;&#22320;&#27169;&#22411;&#36827;&#34892;&#21152;&#26435;&#32858;&#21512;&#20197;&#29983;&#25104;&#20840;&#23616;&#27169;&#22411;&#65292;&#32858;&#21512;&#26435;&#37325;&#34987;&#26631;&#20934;&#21270;&#65288;&#26435;&#37325;&#21644;&#20026;1&#65289;&#24182;&#19982;&#26412;&#22320;&#25968;&#25454;&#22823;&#23567;&#25104;&#27604;&#20363;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#21152;&#26435;&#32858;&#21512;&#36807;&#31243;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;FL&#30340;&#35757;&#32451;&#21160;&#21147;&#23398;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#29616;&#26435;&#37325;&#24635;&#21644;&#21487;&#33021;&#23567;&#20110;1&#65292;&#23548;&#33268;&#20840;&#23616;&#26435;&#37325;&#32553;&#23567;&#25928;&#24212;&#65288;&#31867;&#20284;&#20110;&#26435;&#37325;&#34928;&#20943;&#65289;&#24182;&#25913;&#21892;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#26368;&#20248;&#32553;&#23567;&#22240;&#23376;&#22914;&#20309;&#21463;&#21040;&#23458;&#25143;&#31471;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#26412;&#22320;&#21608;&#26399;&#30340;&#24433;&#21709;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#30456;&#23545;&#32858;&#21512;&#26435;&#37325;&#20197;&#25551;&#32472;&#23458;&#25143;&#31471;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#23458;&#25143;&#31471;&#30456;&#24178;&#24615;&#26469;&#30740;&#31350;&#23398;&#20064;&#21160;&#24577;&#65292;&#24182;&#21457;&#29616;&#23384;&#22312;&#19968;&#20010;&#20851;&#38190;&#28857;&#12290;&#22312;&#36827;&#20837;&#20020;&#30028;&#28857;&#20043;&#21069;&#65292;&#30456;&#24178;&#24615;&#26356;&#39640;&#30340;&#23458;&#25143;&#31471;&#22312;&#27867;&#21270;&#20013;&#21457;&#25381;&#20102;&#26356;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22522;&#20110;&#19978;&#36848;&#27934;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#20855;&#26377;&#21487;&#23398;&#20064;&#32858;&#21512;&#26435;&#37325;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FLLAW&#65289;&#65292;&#23427;&#20801;&#35768;&#20840;&#23616;&#26435;&#37325;&#32553;&#23567;&#25928;&#24212;&#21644;&#21487;&#23398;&#20064;&#32858;&#21512;&#26435;&#37325;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FLLAW&#22312;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12289;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26356;&#22909;&#30340;&#25239;&#25968;&#25454;&#24322;&#36136;&#24615;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In federated learning (FL), weighted aggregation of local models is conducted to generate a global model, and the aggregation weights are normalized (the sum of weights is 1) and proportional to the local data sizes. In this paper, we revisit the weighted aggregation process and gain new insights into the training dynamics of FL. First, we find that the sum of weights can be smaller than 1, causing global weight shrinking effect (analogous to weight decay) and improving generalization. We explore how the optimal shrinking factor is affected by clients' data heterogeneity and local epochs. Second, we dive into the relative aggregation weights among clients to depict the clients' importance. We develop client coherence to study the learning dynamics and find a critical point that exists. Before entering the critical point, more coherent clients play more essential roles in generalization. Based on the above insights, we propose an effective method for Federated Learning with Learnable Ag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22240;&#26524;&#31574;&#30053;&#20998;&#31867;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#30495;&#23454;&#22240;&#26524;&#20316;&#29992;&#19979;&#24179;&#34913;&#31574;&#30053;&#24615;&#34892;&#20026;&#21644;&#20998;&#24067;&#36716;&#21464;&#65292;&#23454;&#29616;&#23545;&#20110;&#29305;&#24449;&#20462;&#25913;&#30340;&#40065;&#26834;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2302.06280</link><description>&lt;p&gt;
&#22240;&#26524;&#31574;&#30053;&#20998;&#31867;&#65306;&#20004;&#31181;&#36716;&#21464;&#20043;&#26053;
&lt;/p&gt;
&lt;p&gt;
Causal Strategic Classification: A Tale of Two Shifts. (arXiv:2302.06280v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22240;&#26524;&#31574;&#30053;&#20998;&#31867;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#30495;&#23454;&#22240;&#26524;&#20316;&#29992;&#19979;&#24179;&#34913;&#31574;&#30053;&#24615;&#34892;&#20026;&#21644;&#20998;&#24067;&#36716;&#21464;&#65292;&#23454;&#29616;&#23545;&#20110;&#29305;&#24449;&#20462;&#25913;&#30340;&#40065;&#26834;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#29992;&#25143;&#21487;&#20197;&#20174;&#26576;&#20123;&#39044;&#27979;&#32467;&#26524;&#20013;&#33719;&#30410;&#26102;&#65292;&#20182;&#20204;&#21487;&#33021;&#20250;&#20542;&#21521;&#20110;&#36890;&#36807;&#31574;&#30053;&#24615;&#20462;&#25913;&#20854;&#29305;&#24449;&#26469;&#23454;&#29616;&#36825;&#20123;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#31574;&#30053;&#20998;&#31867;&#30340;&#30446;&#26631;&#26159;&#35757;&#32451;&#20855;&#26377;&#23545;&#36825;&#31181;&#34892;&#20026;&#31283;&#20581;&#24615;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26694;&#26550;&#20551;&#35774;&#26356;&#25913;&#29305;&#24449;&#19981;&#20250;&#26356;&#25913;&#23454;&#38469;&#32467;&#26524;&#65292;&#36825;&#25551;&#32472;&#20102;&#29992;&#25143;&#8220;&#25805;&#32437;&#8221;&#31995;&#32479;&#30340;&#24773;&#24418;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#28040;&#38500;&#20102;&#36825;&#31181;&#20551;&#35774;&#65292;&#24182;&#30740;&#31350;&#20102;&#30495;&#23454;&#32467;&#26524;&#20250;&#21457;&#29983;&#21464;&#21270;&#30340;&#22240;&#26524;&#31574;&#30053;&#35774;&#32622;&#20013;&#30340;&#23398;&#20064;&#12290;&#20197;&#20934;&#30830;&#24615;&#20026;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31574;&#30053;&#24615;&#34892;&#20026;&#21644;&#22240;&#26524;&#20316;&#29992;&#22312;&#20004;&#31181;&#20114;&#34917;&#30340;&#20998;&#24067;&#36716;&#21464;&#24418;&#24335;&#19979;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#34920;&#24449;&#20102;&#36825;&#20123;&#21464;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#36825;&#20004;&#20010;&#21147;&#37327;&#20043;&#38388;&#24179;&#34913;&#24182;&#20801;&#35768;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;&#22312;&#21512;&#25104;&#21644;&#21322;&#21512;&#25104;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
When users can benefit from certain predictive outcomes, they may be prone to act to achieve those outcome, e.g., by strategically modifying their features. The goal in strategic classification is therefore to train predictive models that are robust to such behavior. However, the conventional framework assumes that changing features does not change actual outcomes, which depicts users as "gaming" the system. Here we remove this assumption, and study learning in a causal strategic setting where true outcomes do change. Focusing on accuracy as our primary objective, we show how strategic behavior and causal effects underlie two complementing forms of distribution shift. We characterize these shifts, and propose a learning algorithm that balances between these two forces and over time, and permits end-to-end training. Experiments on synthetic and semi-synthetic data demonstrate the utility of our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#36880;&#27493;&#36829;&#35268;&#32422;&#26463;&#30340;&#26032;&#22411;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;SUCBVI&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#26368;&#20248;&#24615;&#33021;&#65292;&#21516;&#26102;&#30740;&#31350;&#20102;&#20855;&#26377;&#36880;&#27493;&#36829;&#35268;&#32422;&#26463;&#30340;&#23433;&#20840;&#26080;&#22870;&#25506;&#32034;&#38382;&#39064;&#24182;&#35774;&#35745;&#20102;&#31639;&#27861;SRF-UCRL&#12290;</title><link>http://arxiv.org/abs/2302.06064</link><description>&lt;p&gt;
&#20855;&#26377;&#36880;&#27493;&#36829;&#35268;&#32422;&#26463;&#30340;&#21487;&#35777;&#26126;&#23433;&#20840;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Provably Safe Reinforcement Learning with Step-wise Violation Constraints. (arXiv:2302.06064v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#36880;&#27493;&#36829;&#35268;&#32422;&#26463;&#30340;&#26032;&#22411;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;SUCBVI&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#26368;&#20248;&#24615;&#33021;&#65292;&#21516;&#26102;&#30740;&#31350;&#20102;&#20855;&#26377;&#36880;&#27493;&#36829;&#35268;&#32422;&#26463;&#30340;&#23433;&#20840;&#26080;&#22870;&#25506;&#32034;&#38382;&#39064;&#24182;&#35774;&#35745;&#20102;&#31639;&#27861;SRF-UCRL&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20855;&#26377;&#36880;&#27493;&#36829;&#35268;&#32422;&#26463;&#30340;&#26032;&#22411;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#38382;&#39064;&#19981;&#21516;&#20110;&#29616;&#26377;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#32771;&#34385;&#26356;&#20005;&#26684;&#30340;&#36880;&#27493;&#36829;&#35268;&#32422;&#26463;&#65292;&#24182;&#19988;&#19981;&#20551;&#23450;&#23384;&#22312;&#23433;&#20840;&#21160;&#20316;&#65292;&#20351;&#24471;&#25105;&#20204;&#30340;&#34920;&#36848;&#26356;&#36866;&#21512;&#38656;&#35201;&#22312;&#25152;&#26377;&#20915;&#31574;&#27493;&#39588;&#20013;&#30830;&#20445;&#23433;&#20840;&#19988;&#21487;&#33021;&#19981;&#24635;&#26159;&#20855;&#26377;&#23433;&#20840;&#21160;&#20316;&#30340;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#65292;&#20363;&#22914;&#26426;&#22120;&#20154;&#25511;&#21046;&#21644;&#33258;&#21160;&#39550;&#39542;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;SUCBVI&#65292;&#35813;&#31639;&#27861;&#20445;&#35777;$\tilde{O}(\sqrt{ST})$&#30340;&#36880;&#27493;&#36829;&#35268;&#21644;$\tilde{O}(\sqrt{H^3SAT})$&#30340;&#21518;&#24724;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19979;&#30028;&#65292;&#20197;&#39564;&#35777;&#23545;&#20110;$S$&#21644;$T$&#30340;&#36829;&#35268;&#21644;&#21518;&#24724;&#24615;&#33021;&#30340;&#26368;&#20248;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#19968;&#31181;&#20855;&#26377;&#36880;&#27493;&#36829;&#35268;&#32422;&#26463;&#30340;&#26032;&#22411;&#23433;&#20840;&#26080;&#22870;&#25506;&#32034;&#38382;&#39064;&#12290;&#23545;&#20110;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;$(\varepsilon,\delta)$-PAC&#31639;&#27861;SRF-UCRL&#65292;&#20854;&#23454;&#29616;&#20102;&#20960;&#20046;&#26368;&#20808;&#36827;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;$\tilde{O}((\frac{nHS^2A}{\epsilon})^{\frac{1}{3}}(SAT)^{\frac{2}{3}})$&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate a novel safe reinforcement learning problem with step-wise violation constraints. Our problem differs from existing works in that we consider stricter step-wise violation constraints and do not assume the existence of safe actions, making our formulation more suitable for safety-critical applications which need to ensure safety in all decision steps and may not always possess safe actions, e.g., robot control and autonomous driving. We propose a novel algorithm SUCBVI, which guarantees $\widetilde{O}(\sqrt{ST})$ step-wise violation and $\widetilde{O}(\sqrt{H^3SAT})$ regret. Lower bounds are provided to validate the optimality in both violation and regret performance with respect to $S$ and $T$. Moreover, we further study a novel safe reward-free exploration problem with step-wise violation constraints. For this problem, we design an $(\varepsilon,\delta)$-PAC algorithm SRF-UCRL, which achieves nearly state-of-the-art sample complexity $\widetilde{O}((\frac
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24179;&#28369;&#38750;&#20984;ERM&#30340;&#24046;&#20998;&#38544;&#31169;&#20248;&#21270;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#37319;&#29992;&#32447;&#24615;&#25628;&#32034;&#12289;&#23567;&#25209;&#37327;&#35757;&#32451;&#21644;&#20004;&#38454;&#27573;&#31574;&#30053;&#31561;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#31639;&#27861;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2302.04972</link><description>&lt;p&gt;
&#24179;&#28369;&#38750;&#20984;ERM&#30340;&#24046;&#20998;&#38544;&#31169;&#20248;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Optimization for Smooth Nonconvex ERM. (arXiv:2302.04972v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24179;&#28369;&#38750;&#20984;ERM&#30340;&#24046;&#20998;&#38544;&#31169;&#20248;&#21270;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#37319;&#29992;&#32447;&#24615;&#25628;&#32034;&#12289;&#23567;&#25209;&#37327;&#35757;&#32451;&#21644;&#20004;&#38454;&#27573;&#31574;&#30053;&#31561;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#31639;&#27861;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#24046;&#20998;&#38544;&#31169;&#20248;&#21270;&#31639;&#27861;&#65292;&#27839;&#30528;&#26399;&#26395;&#19979;&#38477;&#26041;&#21521;&#23547;&#25214;&#38750;&#20984;ERM&#30340;&#20108;&#38454;&#36817;&#20284;&#35299;&#12290;&#25105;&#20204;&#20351;&#29992;&#32447;&#24615;&#25628;&#32034;&#65292;&#23567;&#25209;&#37327;&#35757;&#32451;&#21644;&#20004;&#38454;&#27573;&#31574;&#30053;&#26469;&#25552;&#39640;&#31639;&#27861;&#30340;&#36895;&#24230;&#21644;&#23454;&#29992;&#24615;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop simple differentially private optimization algorithms that move along directions of (expected) descent to find an approximate second-order solution for nonconvex ERM. We use line search, mini-batching, and a two-phase strategy to improve the speed and practicality of the algorithm. Numerical experiments demonstrate the effectiveness of these approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25191;&#34892;&#25512;&#33616;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28608;&#21169;&#20869;&#23481;&#21019;&#20316;&#32773;&#21019;&#24314;&#22810;&#26679;&#24615;&#20869;&#23481;&#65292;&#20197;&#23454;&#29616;&#25512;&#33616;&#31995;&#32479;&#30340;&#22810;&#26679;&#24615;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#25512;&#33616;&#31995;&#32479;&#30340;&#34920;&#28436;&#24615;&#36136;&#21644;&#19968;&#31181;&#26032;&#22411;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#39044;&#27979;&#20869;&#23481;&#30340;&#31574;&#30053;&#21464;&#21270;&#65292;&#24182;&#23545;&#20869;&#23481;&#30340;&#21516;&#36136;&#24615;&#36827;&#34892;&#24809;&#32602;&#12290;</title><link>http://arxiv.org/abs/2302.04336</link><description>&lt;p&gt;
&#25191;&#34892;&#25512;&#33616;&#65306;&#36890;&#36807;&#31574;&#30053;&#28608;&#21169;&#23454;&#29616;&#22810;&#26679;&#24615;&#20869;&#23481;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Performative Recommendation: Diversifying Content via Strategic Incentives. (arXiv:2302.04336v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25191;&#34892;&#25512;&#33616;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28608;&#21169;&#20869;&#23481;&#21019;&#20316;&#32773;&#21019;&#24314;&#22810;&#26679;&#24615;&#20869;&#23481;&#65292;&#20197;&#23454;&#29616;&#25512;&#33616;&#31995;&#32479;&#30340;&#22810;&#26679;&#24615;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#25512;&#33616;&#31995;&#32479;&#30340;&#34920;&#28436;&#24615;&#36136;&#21644;&#19968;&#31181;&#26032;&#22411;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#39044;&#27979;&#20869;&#23481;&#30340;&#31574;&#30053;&#21464;&#21270;&#65292;&#24182;&#23545;&#20869;&#23481;&#30340;&#21516;&#36136;&#24615;&#36827;&#34892;&#24809;&#32602;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#21521;&#29992;&#25143;&#25512;&#33616;&#30456;&#20851;&#32852;&#30340;&#20869;&#23481;&#65292;&#20294;&#26159;&#20248;&#21270;&#25512;&#33616;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#24448;&#24448;&#23548;&#33268;&#25512;&#33616;&#30340;&#20869;&#23481;&#32570;&#20047;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20256;&#32479;&#30340;&#26041;&#27861;&#65292;&#22914;&#37325;&#26032;&#25490;&#24207;&#25512;&#33616;&#65292;&#21487;&#20197;&#36890;&#36807;&#21576;&#29616;&#26356;&#22810;&#26679;&#30340;&#39033;&#30446;&#26469;&#25552;&#39640;&#22810;&#26679;&#24615;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#20026;&#20102;&#20419;&#36827;&#22810;&#26679;&#24615;&#30340;&#20135;&#29983;&#21644;&#24310;&#32493;&#65292;&#31995;&#32479;&#24517;&#39035;&#40723;&#21169;&#20869;&#23481;&#21019;&#36896;&#32773;&#21019;&#36896;&#22810;&#26679;&#24615;&#20869;&#23481;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#25512;&#33616;&#31995;&#32479;&#30340;&#34920;&#28436;&#24615;&#36136;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#23398;&#20064;&#26469;&#28608;&#21169;&#20869;&#23481;&#21019;&#20316;&#32773;&#21019;&#24314;&#22810;&#26679;&#24615;&#20869;&#23481;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#31181;&#26032;&#22411;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#39044;&#27979;&#20869;&#23481;&#30340;&#31574;&#30053;&#21464;&#21270;&#65292;&#24182;&#23545;&#20869;&#23481;&#30340;&#21516;&#36136;&#24615;&#36827;&#34892;&#24809;&#32602;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20998;&#26512;&#21644;&#23454;&#35777;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#21487;&#20197;&#28608;&#21169;&#22810;&#26679;&#24615;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#21322;&#21512;&#25104;&#25968;&#25454;&#19978;&#23454;&#39564;&#24615;&#22320;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The primary goal in recommendation is to suggest relevant content to users, but optimizing for accuracy often results in recommendations that lack diversity. To remedy this, conventional approaches such as re-ranking improve diversity by presenting more diverse items. Here we argue that to promote inherent and prolonged diversity, the system must encourage its creation. Towards this, we harness the performative nature of recommendation, and show how learning can incentivize strategic content creators to create diverse content. Our approach relies on a novel form of regularization that anticipates strategic changes to content, and penalizes for content homogeneity. We provide analytic and empirical results that demonstrate when and how diversity can be incentivized, and experimentally demonstrate the utility of our approach on synthetic and semi-synthetic data.
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#35299;&#20915;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;DFedSAM &#21644; DFedSAM-MGS&#65292;&#20998;&#21035;&#36890;&#36807;&#37319;&#29992;&#26799;&#24230;&#25200;&#21160;&#21644;&#22810;&#27425;&#20843;&#21350;&#27493;&#39588;&#26469;&#29983;&#25104;&#26412;&#22320;&#24179;&#22374;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102; DFL &#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.04083</link><description>&lt;p&gt;
&#25552;&#39640;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#30340;&#27169;&#22411;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving the Model Consistency of Decentralized Federated Learning. (arXiv:2302.04083v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04083
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;DFedSAM &#21644; DFedSAM-MGS&#65292;&#20998;&#21035;&#36890;&#36807;&#37319;&#29992;&#26799;&#24230;&#25200;&#21160;&#21644;&#22810;&#27425;&#20843;&#21350;&#27493;&#39588;&#26469;&#29983;&#25104;&#26412;&#22320;&#24179;&#22374;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102; DFL &#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#38544;&#31169;&#27844;&#28431;&#21644;&#36890;&#20449;&#36127;&#25285;&#65292;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#25918;&#24323;&#20102;&#20013;&#22830;&#26381;&#21153;&#22120;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#21482;&#19982;&#20854;&#22312;&#20998;&#25955;&#24335;&#36890;&#20449;&#32593;&#32476;&#20013;&#30340;&#37051;&#23621;&#36890;&#20449;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;DFL&#22312;&#26412;&#22320;&#23458;&#25143;&#31471;&#20043;&#38388;&#23384;&#22312;&#39640;&#24230;&#19981;&#19968;&#33268;&#24615;&#65292;&#36825;&#23548;&#33268;&#20102;&#20005;&#37325;&#30340;&#20998;&#24067;&#36716;&#31227;&#21644;&#36739;&#20302;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#24322;&#26500;&#25968;&#25454;&#25110;&#31232;&#30095;&#36890;&#20449;&#25299;&#25169;&#19978;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#21517;&#20026;DFedSAM&#21644;DFedSAM-MGS&#30340;DFL&#31639;&#27861;&#26469;&#25913;&#21892;DFL&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DFedSAM&#21033;&#29992;&#26799;&#24230;&#25200;&#21160;&#36890;&#36807;Sharpness Aware Minimization&#65288;SAM&#65289;&#26469;&#29983;&#25104;&#26412;&#22320;&#24179;&#22374;&#27169;&#22411;&#65292;SAM&#25628;&#32034;&#20855;&#26377;&#32479;&#19968;&#20302;&#25439;&#22833;&#20540;&#30340;&#27169;&#22411;&#12290;DFedSAM-MGS&#36890;&#36807;&#37319;&#29992;&#22810;&#27425;&#20843;&#21350;&#27493;&#39588;&#65288;MGS&#65289;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;DFedSAM&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#27169;&#22411;&#19968;&#33268;&#24615;&#65292;&#21152;&#36895;&#26412;&#22320;&#24179;&#22374;&#27169;&#22411;&#30340;&#32858;&#21512;&#65292;&#24182;&#26356;&#22909;&#22320;&#24179;&#34913;&#36890;&#20449;&#22797;&#26434;&#24230;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
To mitigate the privacy leakages and communication burdens of Federated Learning (FL), decentralized FL (DFL) discards the central server and each client only communicates with its neighbors in a decentralized communication network. However, existing DFL suffers from high inconsistency among local clients, which results in severe distribution shift and inferior performance compared with centralized FL (CFL), especially on heterogeneous data or sparse communication topology. To alleviate this issue, we propose two DFL algorithms named DFedSAM and DFedSAM-MGS to improve the performance of DFL. Specifically, DFedSAM leverages gradient perturbation to generate local flat models via Sharpness Aware Minimization (SAM), which searches for models with uniformly low loss values. DFedSAM-MGS further boosts DFedSAM by adopting Multiple Gossip Steps (MGS) for better model consistency, which accelerates the aggregation of local flat models and better balances communication complexity and generaliza
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SLaM&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#30693;&#35782;&#33976;&#39311;&#20013;&#25945;&#24072;&#20266;&#26631;&#31614;&#22122;&#22768;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;SLaM&#20351;&#29992;&#23398;&#29983;-&#26631;&#31614;&#28151;&#21512;&#30340;&#26041;&#24335;&#26469;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#26631;&#31614;&#65292;&#24182;&#22312;&#22810;&#20010;&#26631;&#20934;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.03806</link><description>&lt;p&gt;
SLaM: &#29992;&#26410;&#26631;&#27880;&#26679;&#26412;&#30340;&#23398;&#29983;-&#26631;&#31614;&#28151;&#21512;&#36827;&#34892;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
SLaM: Student-Label Mixing for Distillation with Unlabeled Examples. (arXiv:2302.03806v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03806
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SLaM&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#30693;&#35782;&#33976;&#39311;&#20013;&#25945;&#24072;&#20266;&#26631;&#31614;&#22122;&#22768;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;SLaM&#20351;&#29992;&#23398;&#29983;-&#26631;&#31614;&#28151;&#21512;&#30340;&#26041;&#24335;&#26469;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#26631;&#31614;&#65292;&#24182;&#22312;&#22810;&#20010;&#26631;&#20934;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#26631;&#27880;&#26679;&#26412;&#30340;&#30693;&#35782;&#33976;&#39311;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#29992;&#20110;&#22312;&#26631;&#35760;&#25968;&#25454;&#26377;&#38480;&#20294;&#21487;&#20197;&#35775;&#38382;&#22823;&#37327;&#26410;&#26631;&#27880;&#25968;&#25454;&#30340;&#24212;&#29992;&#20013;&#29983;&#25104;&#32039;&#20945;&#12289;&#36731;&#37327;&#32423;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#65292;&#19968;&#20010;&#22823;&#30340;&#25945;&#24072;&#27169;&#22411;&#20026;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#29983;&#25104;&#8220;&#36719;&#8221;&#20266;&#26631;&#31614;&#65292;&#28982;&#21518;&#29992;&#20110;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#12290;&#23613;&#31649;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#37117;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#32570;&#28857;&#26159;&#25945;&#24072;&#30340;&#20266;&#26631;&#31614;&#32463;&#24120;&#26159;&#24102;&#26377;&#22122;&#22768;&#30340;&#65292;&#20174;&#32780;&#23548;&#33268;&#23398;&#29983;&#24615;&#33021;&#21463;&#21040;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23398;&#29983;-&#26631;&#31614;&#28151;&#21512;&#65288;SLaM&#65289;&#30340;&#26410;&#26631;&#27880;&#31034;&#20363;&#30693;&#35782;&#33976;&#39311;&#30340;&#21407;&#21017;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#35780;&#20272;&#20960;&#20010;&#26631;&#20934;&#22522;&#20934;&#26102;&#22987;&#32456;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;SLaM&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#65307;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#25913;&#36827;&#20102;&#21322;&#31354;&#38388;&#23398;&#20064;&#30340;&#26368;&#20339;&#24050;&#30693;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation with unlabeled examples is a powerful training paradigm for generating compact and lightweight student models in applications where the amount of labeled data is limited but one has access to a large pool of unlabeled data. In this setting, a large teacher model generates ``soft'' pseudo-labels for the unlabeled dataset which are then used for training the student model. Despite its success in a wide variety of applications, a shortcoming of this approach is that the teacher's pseudo-labels are often noisy, leading to impaired student performance. In this paper, we present a principled method for knowledge distillation with unlabeled examples that we call Student-Label Mixing (SLaM) and we show that it consistently improves over prior approaches by evaluating it on several standard benchmarks. Finally, we show that SLaM comes with theoretical guarantees; along the way we give an algorithm improving the best-known sample complexity for learning halfspaces with mar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21463;&#38480;&#29627;&#23572;&#20857;&#26364;&#26426;&#65288;RBM&#65289;&#30340;&#23398;&#20064;&#21160;&#24577;&#26500;&#24314;&#20851;&#31995;&#25968;&#25454;&#26641;&#30340;&#26080;&#30417;&#30563;&#23618;&#27425;&#32858;&#31867;&#26041;&#27861;&#65292;&#24182;&#22312;&#25968;&#23383;&#22270;&#20687;&#12289;&#20154;&#31867;&#22522;&#22240;&#32452;&#31361;&#21464;&#21644;&#21516;&#28304;&#34507;&#30333;&#36136;&#23478;&#26063;&#31561;&#19981;&#21516;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#33021;&#22815;&#33258;&#21160;&#35782;&#21035;&#25968;&#25454;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2302.01851</link><description>&lt;p&gt;
&#21033;&#29992;RBM&#30340;&#23398;&#20064;&#21160;&#24577;&#36827;&#34892;&#26080;&#30417;&#30563;&#23618;&#27425;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Unsupervised hierarchical clustering using the learning dynamics of RBMs. (arXiv:2302.01851v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21463;&#38480;&#29627;&#23572;&#20857;&#26364;&#26426;&#65288;RBM&#65289;&#30340;&#23398;&#20064;&#21160;&#24577;&#26500;&#24314;&#20851;&#31995;&#25968;&#25454;&#26641;&#30340;&#26080;&#30417;&#30563;&#23618;&#27425;&#32858;&#31867;&#26041;&#27861;&#65292;&#24182;&#22312;&#25968;&#23383;&#22270;&#20687;&#12289;&#20154;&#31867;&#22522;&#22240;&#32452;&#31361;&#21464;&#21644;&#21516;&#28304;&#34507;&#30333;&#36136;&#23478;&#26063;&#31561;&#19981;&#21516;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#33021;&#22815;&#33258;&#21160;&#35782;&#21035;&#25968;&#25454;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#25968;&#25454;&#38598;&#24448;&#24448;&#26159;&#22797;&#26434;&#30340;&#65292;&#20855;&#26377;&#19968;&#23450;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#19981;&#21516;&#25277;&#35937;&#32423;&#21035;&#19978;&#30340;&#25968;&#25454;&#32452;&#21644;&#23376;&#32452;&#20849;&#20139;&#20849;&#21516;&#29305;&#24449;&#12290;&#29702;&#35299;&#21644;&#25581;&#31034;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#38544;&#34255;&#32467;&#26500;&#26159;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#65292;&#20855;&#26377;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21463;&#38480;&#29627;&#23572;&#20857;&#26364;&#26426;&#65288;RBM&#65289;&#30340;&#23398;&#20064;&#21160;&#24577;&#26500;&#24314;&#20851;&#31995;&#25968;&#25454;&#26641;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#24179;&#22343;&#22330;&#26041;&#27861;&#65292;&#30001;Plefka&#25193;&#23637;&#23548;&#20986;&#65292;&#24182;&#22312;&#26080;&#24207;&#31995;&#32479;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#24320;&#21457;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#26131;&#20110;&#29702;&#35299;&#12290;&#25105;&#20204;&#22312;&#20154;&#24037;&#21019;&#24314;&#30340;&#23618;&#27425;&#25968;&#25454;&#38598;&#21644;&#19977;&#20010;&#19981;&#21516;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#65288;&#25968;&#23383;&#22270;&#20687;&#65292;&#20154;&#31867;&#22522;&#22240;&#32452;&#31361;&#21464;&#21644;&#21516;&#28304;&#34507;&#30333;&#36136;&#23478;&#26063;&#65289;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#33258;&#21160;&#35782;&#21035;&#25968;&#25454;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#36825;&#22312;&#21516;&#28304;&#34507;&#30333;&#36136;&#30340;&#30740;&#31350;&#20013;&#21487;&#33021;&#20250;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Datasets in the real world are often complex and to some degree hierarchical, with groups and sub-groups of data sharing common characteristics at different levels of abstraction. Understanding and uncovering the hidden structure of these datasets is an important task that has many practical applications. To address this challenge, we present a new and general method for building relational data trees by exploiting the learning dynamics of the Restricted Boltzmann Machine (RBM). Our method is based on the mean-field approach, derived from the Plefka expansion, and developed in the context of disordered systems. It is designed to be easily interpretable. We tested our method in an artificially created hierarchical dataset and on three different real-world datasets (images of digits, mutations in the human genome, and a homologous family of proteins). The method is able to automatically identify the hierarchical structure of the data. This could be useful in the study of homologous prote
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;GAN&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;MonoFlow&#65292;&#36890;&#36807;Wasserstein&#26799;&#24230;&#27969;&#33719;&#24471;&#29702;&#35770;&#27934;&#35265;&#21644;&#31639;&#27861;&#21551;&#31034;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#23494;&#24230;&#27604;&#20363;&#30340;&#21333;&#35843;&#36882;&#22686;&#26144;&#23556;&#37325;&#26032;&#32553;&#25918;&#31890;&#23376;&#28436;&#21270;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#37492;&#21035;&#22120;&#33719;&#24471;MonoFlow&#30340;&#21521;&#37327;&#22330;&#65292;&#21033;&#29992;&#30456;&#24212;&#30340;&#21521;&#37327;&#22330;&#36827;&#34892;&#31890;&#23376;&#27969;&#30340;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2302.01075</link><description>&lt;p&gt;
MonoFlow: &#20174;Wasserstein&#26799;&#24230;&#27969;&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;Divergence GANs
&lt;/p&gt;
&lt;p&gt;
MonoFlow: Rethinking Divergence GANs via the Perspective of Wasserstein Gradient Flows. (arXiv:2302.01075v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;GAN&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;MonoFlow&#65292;&#36890;&#36807;Wasserstein&#26799;&#24230;&#27969;&#33719;&#24471;&#29702;&#35770;&#27934;&#35265;&#21644;&#31639;&#27861;&#21551;&#31034;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#23494;&#24230;&#27604;&#20363;&#30340;&#21333;&#35843;&#36882;&#22686;&#26144;&#23556;&#37325;&#26032;&#32553;&#25918;&#31890;&#23376;&#28436;&#21270;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#37492;&#21035;&#22120;&#33719;&#24471;MonoFlow&#30340;&#21521;&#37327;&#22330;&#65292;&#21033;&#29992;&#30456;&#24212;&#30340;&#21521;&#37327;&#22330;&#36827;&#34892;&#31890;&#23376;&#27969;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#23545;&#25239;&#35757;&#32451;&#26159;&#36890;&#36807;&#21028;&#21035;&#22120;&#26469;&#20272;&#35745;&#31163;&#25955;&#24230;&#65292;&#29983;&#25104;&#22120;&#23398;&#20064;&#26368;&#23567;&#21270;&#36825;&#20010;&#31163;&#25955;&#24230;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#23613;&#31649;&#35768;&#22810;GANs&#21464;&#20307;&#37117;&#26159;&#25353;&#29031;&#36825;&#20010;&#33539;&#20363;&#24320;&#21457;&#30340;&#65292;&#20294;&#24403;&#21069;GANs&#30340;&#29702;&#35770;&#29702;&#35299;&#21644;&#23454;&#38469;&#31639;&#27861;&#26159;&#19981;&#19968;&#33268;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#23637;&#31034;&#20102;&#26679;&#26412;&#31354;&#38388;&#20869;&#31890;&#23376;&#28436;&#21270;&#30340;Wasserstein&#26799;&#24230;&#27969;&#26469;&#33719;&#24471;GANs&#30340;&#29702;&#35770;&#27934;&#35265;&#21644;&#31639;&#27861;&#21551;&#31034;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;MonoFlow&#65306;&#31890;&#23376;&#28436;&#21270;&#36890;&#36807;&#23494;&#24230;&#27604;&#20363;&#30340;&#21333;&#35843;&#36882;&#22686;&#26144;&#23556;&#36827;&#34892;&#37325;&#26032;&#32553;&#25918;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#19979;&#65292;&#23545;&#25239;&#24615;&#35757;&#32451;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#20010;&#36807;&#31243;&#65292;&#39318;&#20808;&#36890;&#36807;&#35757;&#32451;&#37492;&#21035;&#22120;&#33719;&#24471;MonoFlow&#30340;&#21521;&#37327;&#22330;&#65292;&#28982;&#21518;&#29983;&#25104;&#22120;&#23398;&#20064;&#30001;&#30456;&#24212;&#21521;&#37327;&#22330;&#25152;&#23450;&#20041;&#30340;&#31890;&#23376;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;
The conventional understanding of adversarial training in generative adversarial networks (GANs) is that the discriminator is trained to estimate a divergence, and the generator learns to minimize this divergence. We argue that despite the fact that many variants of GANs were developed following this paradigm, the current theoretical understanding of GANs and their practical algorithms are inconsistent. In this paper, we leverage Wasserstein gradient flows which characterize the evolution of particles in the sample space, to gain theoretical insights and algorithmic inspiration of GANs. We introduce a unified generative modeling framework - MonoFlow: the particle evolution is rescaled via a monotonically increasing mapping of the log density ratio. Under our framework, adversarial training can be viewed as a procedure first obtaining MonoFlow's vector field via training the discriminator and the generator learns to draw the particle flow defined by the corresponding vector field. We al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;MPNN&#19982;&#22270;&#36716;&#25442;&#22120;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#24182;&#35777;&#26126;&#20102;&#24102;&#26377;&#34394;&#25311;&#33410;&#28857;&#30340;MPNN&#21487;&#20197;&#20219;&#24847;&#36924;&#36817;GT&#30340;&#33258;&#27880;&#24847;&#21147;&#23618;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#38416;&#26126;&#20102;&#20004;&#31181;&#22270;&#23398;&#20064;&#33539;&#24335;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#33021;&#21147;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2301.11956</link><description>&lt;p&gt;
MPNN&#19982;&#22270;&#36716;&#25442;&#22120;&#20043;&#38388;&#30340;&#36830;&#25509;
&lt;/p&gt;
&lt;p&gt;
On the Connection Between MPNN and Graph Transformer. (arXiv:2301.11956v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;MPNN&#19982;&#22270;&#36716;&#25442;&#22120;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#24182;&#35777;&#26126;&#20102;&#24102;&#26377;&#34394;&#25311;&#33410;&#28857;&#30340;MPNN&#21487;&#20197;&#20219;&#24847;&#36924;&#36817;GT&#30340;&#33258;&#27880;&#24847;&#21147;&#23618;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#38416;&#26126;&#20102;&#20004;&#31181;&#22270;&#23398;&#20064;&#33539;&#24335;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#33021;&#21147;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20986;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#23398;&#20064;&#31639;&#27861;&#33539;&#20363;&#8212;&#8212;&#22270;&#36716;&#25442;&#22120;&#65288;GT&#65289;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#20043;&#21069;&#27969;&#34892;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#20301;&#32622;&#23884;&#20837;&#65292;GT&#21487;&#20197;&#20219;&#24847;&#36924;&#36817;MPNN&#65292;&#36825;&#24847;&#21619;&#30528;GT&#33267;&#23569;&#19982;MPNN&#19968;&#26679;&#24378;&#22823;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21453;&#21521;&#36830;&#25509;&#65292;&#24182;&#23637;&#31034;&#20102;&#24102;&#26377;&#34394;&#25311;&#33410;&#28857;&#65288;VN&#65289;&#30340;MPNN&#36275;&#22815;&#24378;&#22823;&#65292;&#21487;&#20197;&#20219;&#24847;&#36924;&#36817;GT&#30340;&#33258;&#27880;&#24847;&#21147;&#23618;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#22914;&#26524;&#32771;&#34385;&#19968;&#31181;&#32447;&#24615;&#21464;&#25442;&#22120;&#8212;&#8212;&#25152;&#35859;&#30340;&#34920;&#29616;&#32773;/&#32447;&#24615;&#21464;&#25442;&#22120;&#65292;&#21017;&#20855;&#26377;O&#65288;1&#65289;&#28145;&#24230;&#21644;O&#65288;1&#65289;&#23485;&#24230;&#30340;MPNN + VN&#21487;&#20197;&#36924;&#36817;&#34920;&#29616;&#32773;/&#32447;&#24615;&#21464;&#25442;&#22120;&#30340;&#33258;&#25105;&#27880;&#24847;&#23618;&#12290;&#25509;&#19979;&#26469;&#65292;&#36890;&#36807;MPNN + VN&#19982;DeepSets&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20855;&#26377;O(n^d)&#23485;&#24230;&#21644;O(1)&#28145;&#24230;&#30340;MPNN + VN&#21487;&#20197;&#36924;&#36817;GT&#30340;&#20219;&#20309;&#23618;&#65292;&#20854;&#20013;n&#26159;&#22270;&#20013;&#30340;&#33410;&#28857;&#25968;&#65292;d&#26159;&#22270;&#30340;&#30452;&#24452;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#38416;&#26126;&#20102;&#20004;&#31181;&#22270;&#23398;&#20064;&#33539;&#24335;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#33021;&#21147;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Transformer (GT) recently has emerged as a new paradigm of graph learning algorithms, outperforming the previously popular Message Passing Neural Network (MPNN) on multiple benchmarks. Previous work (Kim et al., 2022) shows that with proper position embedding, GT can approximate MPNN arbitrarily well, implying that GT is at least as powerful as MPNN. In this paper, we study the inverse connection and show that MPNN with virtual node (VN), a commonly used heuristic with little theoretical understanding, is powerful enough to arbitrarily approximate the self-attention layer of GT.  In particular, we first show that if we consider one type of linear transformer, the so-called Performer/Linear Transformer (Choromanski et al., 2020; Katharopoulos et al., 2020), then MPNN + VN with only O(1) depth and O(1) width can approximate a self-attention layer in Performer/Linear Transformer. Next, via a connection between MPNN + VN and DeepSets, we prove the MPNN + VN with O(n^d) width and O(1)
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;AI&#26694;&#26550;ExplainableFold&#65292;&#29992;&#20110;&#23545;AlphaFold&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#36827;&#34892;&#35299;&#37322;&#12290;&#25552;&#20379;&#20102;&#25509;&#36817;&#23454;&#39564;&#30340;&#23545;&#27688;&#22522;&#37240;&#23545;3D&#34507;&#30333;&#36136;&#32467;&#26500;&#24433;&#21709;&#30340;&#29702;&#35299;&#65292;&#26377;&#28508;&#21147;&#20419;&#36827;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#26356;&#28145;&#20837;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2301.11765</link><description>&lt;p&gt;
ExplainableFold&#65306;&#21033;&#29992;&#21487;&#35299;&#37322;AI&#29702;&#35299;AlphaFold&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
ExplainableFold: Understanding AlphaFold Prediction with Explainable AI. (arXiv:2301.11765v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;AI&#26694;&#26550;ExplainableFold&#65292;&#29992;&#20110;&#23545;AlphaFold&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#36827;&#34892;&#35299;&#37322;&#12290;&#25552;&#20379;&#20102;&#25509;&#36817;&#23454;&#39564;&#30340;&#23545;&#27688;&#22522;&#37240;&#23545;3D&#34507;&#30333;&#36136;&#32467;&#26500;&#24433;&#21709;&#30340;&#29702;&#35299;&#65292;&#26377;&#28508;&#21147;&#20419;&#36827;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#26356;&#28145;&#20837;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ExplainableFold&#65292;&#19968;&#31181;&#29992;&#20110;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;AI&#26694;&#26550;&#12290;&#23613;&#31649;&#20687;AlphaFold&#36825;&#26679;&#30340;&#22522;&#20110;AI&#30340;&#26041;&#27861;&#22312;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#40657;&#30418;&#29305;&#24615;&#65292;&#20854;&#39044;&#27979;&#30340;&#22522;&#26412;&#21407;&#22240;&#20173;&#19981;&#28165;&#26970;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#29983;&#29289;&#23398;&#21407;&#29702;&#21551;&#21457;&#30340;&#21453;&#20107;&#23454;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#20174;&#32780;&#23454;&#29616;&#19968;&#20010;&#24178;&#23454;&#39564;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ExplainableFold&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;AlphaFold&#39044;&#27979;&#35299;&#37322;&#65292;&#25552;&#20379;&#25509;&#36817;&#23454;&#39564;&#30340;&#23545;&#27688;&#22522;&#37240;&#23545;3D&#34507;&#30333;&#36136;&#32467;&#26500;&#24433;&#21709;&#30340;&#29702;&#35299;&#12290;&#36825;&#19968;&#26694;&#26550;&#26377;&#28508;&#21147;&#20419;&#36827;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#26356;&#28145;&#20837;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents ExplainableFold, an explainable AI framework for protein structure prediction. Despite the success of AI-based methods such as AlphaFold in this field, the underlying reasons for their predictions remain unclear due to the black-box nature of deep learning models. To address this, we propose a counterfactual learning framework inspired by biological principles to generate counterfactual explanations for protein structure prediction, enabling a dry-lab experimentation approach. Our experimental results demonstrate the ability of ExplainableFold to generate high-quality explanations for AlphaFold's predictions, providing near-experimental understanding of the effects of amino acids on 3D protein structure. This framework has the potential to facilitate a deeper understanding of protein structures.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36873;&#39033;&#26041;&#27861;&#65292;&#20197;&#22788;&#29702;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#24615;&#21160;&#20316;&#36873;&#25321;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#26368;&#36817;&#30340;&#32467;&#26524;&#26469;&#25299;&#23637;&#33267;&#21487;&#20280;&#32553;&#24615;&#26356;&#22909;&#30340;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2301.11181</link><description>&lt;p&gt;
&#22522;&#20110;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#28145;&#24230;&#36873;&#39033;&#26041;&#27861;&#29992;&#20110;&#26102;&#38388;&#25193;&#23637;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Deep Laplacian-based Options for Temporally-Extended Exploration. (arXiv:2301.11181v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36873;&#39033;&#26041;&#27861;&#65292;&#20197;&#22788;&#29702;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#24615;&#21160;&#20316;&#36873;&#25321;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#26368;&#36817;&#30340;&#32467;&#26524;&#26469;&#25299;&#23637;&#33267;&#21487;&#20280;&#32553;&#24615;&#26356;&#22909;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36873;&#25321;&#20855;&#26377;&#20016;&#23500;&#32463;&#39564;&#27969;&#30340;&#25506;&#32034;&#24615;&#21160;&#20316;&#20197;&#36827;&#34892;&#26356;&#22909;&#23398;&#20064;&#26159;&#19968;&#39033;&#22522;&#26412;&#25361;&#25112;&#12290;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#26681;&#25454;&#29305;&#23450;&#30340;&#31574;&#30053;&#22312;&#19968;&#27573;&#26102;&#38388;&#20869;&#36873;&#25321;&#21160;&#20316;&#65292;&#20063;&#31216;&#20026;&#36873;&#39033;&#12290;&#26368;&#36817;&#19968;&#31181;&#29992;&#20110;&#33719;&#21462;&#27492;&#31867;&#25506;&#32034;&#36873;&#39033;&#30340;&#24037;&#20316;&#22522;&#20110;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#26412;&#24449;&#20989;&#25968;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#30452;&#21040;&#29616;&#22312;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#22270;&#34920;&#39046;&#22495;&#20013;&#22823;&#22810;&#34987;&#38480;&#21046;&#22312;&#20197;&#19979;&#26041;&#38754;&#65306;(1)&#22270;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#24050;&#32473;&#20986;&#25110;&#21487;&#20197;&#23436;&#20840;&#20272;&#35745;&#65292;(2)&#22312;&#35813;&#30697;&#38453;&#19978;&#36827;&#34892;&#29305;&#24449;&#20998;&#35299;&#26159;&#21487;&#35745;&#31639;&#30340;&#65292;(3)&#20540;&#20989;&#25968;&#21487;&#20197;&#34987;&#23436;&#20840;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#19968;&#20010;&#21333;&#29420;&#30340;&#36873;&#39033;&#21457;&#29616;&#38454;&#27573;&#12290;&#36825;&#20123;&#20551;&#35774;&#22312;&#26681;&#26412;&#19978;&#26159;&#19981;&#21487;&#25193;&#23637;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#26368;&#36817;&#30452;&#25509;&#36924;&#36817;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#26412;&#24449;&#20989;&#25968;&#30340;&#32467;&#26524;&#65292;&#30495;&#27491;&#23454;&#29616;&#36873;&#39033;&#26041;&#27861;&#30340;&#21487;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Selecting exploratory actions that generate a rich stream of experience for better learning is a fundamental challenge in reinforcement learning (RL). An approach to tackle this problem consists in selecting actions according to specific policies for an extended period of time, also known as options. A recent line of work to derive such exploratory options builds upon the eigenfunctions of the graph Laplacian. Importantly, until now these methods have been mostly limited to tabular domains where (1) the graph Laplacian matrix was either given or could be fully estimated, (2) performing eigendecomposition on this matrix was computationally tractable, and (3) value functions could be learned exactly. Additionally, these methods required a separate option discovery phase. These assumptions are fundamentally not scalable. In this paper we address these limitations and show how recent results for directly approximating the eigenfunctions of the Laplacian can be leveraged to truly scale up o
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;&#23376;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;TWI-QRNN&#65292;&#23427;&#36890;&#36807;&#37327;&#23376;-&#32463;&#20856;&#33258;&#36866;&#24212;&#38376;&#25511;&#26426;&#21046;&#23454;&#29616;&#26102;&#38388;&#25197;&#26354;&#19981;&#21464;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.08173</link><description>&lt;p&gt;
&#36890;&#36807;&#37327;&#23376;-&#32463;&#20856;&#33258;&#36866;&#24212;&#38376;&#25511;&#23454;&#29616;&#26102;&#38388;&#25197;&#26354;&#19981;&#21464;&#37327;&#30340;&#37327;&#23376;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time-Warping Invariant Quantum Recurrent Neural Networks via Quantum-Classical Adaptive Gating. (arXiv:2301.08173v3 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08173
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;&#23376;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;TWI-QRNN&#65292;&#23427;&#36890;&#36807;&#37327;&#23376;-&#32463;&#20856;&#33258;&#36866;&#24212;&#38376;&#25511;&#26426;&#21046;&#23454;&#29616;&#26102;&#38388;&#25197;&#26354;&#19981;&#21464;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#20856;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#36890;&#36807;&#33258;&#36866;&#24212;&#38376;&#25511;&#22312;&#26102;&#38388;&#25968;&#25454;&#22788;&#29702;&#20013;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#20419;&#36827;&#20102;&#20445;&#30041;&#36807;&#21435;&#20449;&#24687;&#20197;&#39044;&#27979;&#26410;&#26469;&#30340;&#26426;&#21046;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20445;&#25345;&#23545;&#26102;&#38388;&#25197;&#26354;&#36716;&#25442;&#19981;&#21464;&#30340;&#26426;&#21046;&#12290;&#26412;&#25991;&#22522;&#20110;&#37327;&#23376;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;QRNNs&#65289;&#65292;&#19968;&#31181;&#20855;&#26377;&#37327;&#23376;&#20869;&#23384;&#30340;&#21160;&#24577;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#25968;&#25454;&#22788;&#29702;&#37327;&#23376;&#27169;&#22411;&#65292;&#21487;&#20197;&#20445;&#25345;&#23545;&#36755;&#20837;-&#36755;&#20986;&#24207;&#21015;&#30340;&#26102;&#38388;&#25197;&#26354;&#36716;&#25442;&#19981;&#21464;&#24615;&#12290;&#36825;&#20010;&#27169;&#22411;&#34987;&#31216;&#20026;&#26102;&#38388;&#25197;&#26354;&#19981;&#21464;QRNN&#65288;TWI-QRNN&#65289;&#65292;&#36890;&#36807;&#19968;&#20010;&#37327;&#23376;-&#32463;&#20856;&#33258;&#36866;&#24212;&#38376;&#25511;&#26426;&#21046;&#22686;&#24378;&#20102;QRNN&#65292;&#35813;&#26426;&#21046;&#21487;&#20197;&#26681;&#25454;&#36755;&#20837;&#24207;&#21015;&#30340;&#36807;&#21435;&#26679;&#26412;&#36890;&#36807;&#32463;&#20856;&#36882;&#24402;&#27169;&#22411;&#20915;&#23450;&#26159;&#21542;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#24212;&#29992;&#21442;&#25968;&#21270;&#30340;&#37193;&#21464;&#25442;&#12290; TWI-QRNN&#27169;&#22411;&#31867;&#20174;&#22522;&#26412;&#21407;&#29702;&#20013;&#25512;&#23548;&#20986;&#26469;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#26102;&#38388;&#25197;&#26354;&#36716;&#25442;&#19981;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive gating plays a key role in temporal data processing via classical recurrent neural networks (RNN), as it facilitates retention of past information necessary to predict the future, providing a mechanism that preserves invariance to time warping transformations. This paper builds on quantum recurrent neural networks (QRNNs), a dynamic model with quantum memory, to introduce a novel class of temporal data processing quantum models that preserve invariance to time-warping transformations of the (classical) input-output sequences. The model, referred to as time warping-invariant QRNN (TWI-QRNN), augments a QRNN with a quantum-classical adaptive gating mechanism that chooses whether to apply a parameterized unitary transformation at each time step as a function of the past samples of the input sequence via a classical recurrent model. The TWI-QRNN model class is derived from first principles, and its capacity to successfully implement time-warping transformations is experimentally d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;LQR&#25511;&#21046;&#22120;&#65292;&#22312;&#20960;&#20046;&#24517;&#28982;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377; $\tilde{ \mathcal{O}}(\sqrt{T})$ &#21518;&#24724;&#19978;&#38480;&#35777;&#26126;&#65292;&#19988;&#20855;&#26377;&#26029;&#30005;&#26426;&#21046;&#20445;&#35777;&#23433;&#20840;&#24182;&#23545;&#24615;&#33021;&#24433;&#21709;&#24456;&#23567;&#12290;</title><link>http://arxiv.org/abs/2301.05537</link><description>&lt;p&gt;
&#33258;&#36866;&#24212; LQR &#31639;&#27861;&#30340;&#36817;&#20046;&#24517;&#28982; $\sqrt{T}$ &#21518;&#24724;&#19978;&#38480;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Almost Surely $\sqrt{T}$ Regret Bound for Adaptive LQR. (arXiv:2301.05537v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;LQR&#25511;&#21046;&#22120;&#65292;&#22312;&#20960;&#20046;&#24517;&#28982;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377; $\tilde{ \mathcal{O}}(\sqrt{T})$ &#21518;&#24724;&#19978;&#38480;&#35777;&#26126;&#65292;&#19988;&#20855;&#26377;&#26029;&#30005;&#26426;&#21046;&#20445;&#35777;&#23433;&#20840;&#24182;&#23545;&#24615;&#33021;&#24433;&#21709;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#26410;&#30693;&#31995;&#32479;&#21442;&#25968;&#30340;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#38382;&#39064;(LQR)&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#26159;&#33267;&#20170;&#20173;&#19981;&#28165;&#26970;&#26159;&#21542;&#33021;&#20960;&#20046;&#24517;&#28982;&#22320;&#36798;&#21040; $\tilde{ \mathcal{O}}(\sqrt{T})$ &#30340;&#21518;&#24724;&#19978;&#38480;&#65292;&#32780;&#26412;&#25991;&#21017;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;LQR&#25511;&#21046;&#22120;&#65292;&#22312;&#20960;&#20046;&#24517;&#28982;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377; $\tilde{ \mathcal{O}}(\sqrt{T})$ &#21518;&#24724;&#19978;&#38480;&#30340;&#35777;&#26126;&#12290;&#35813;&#25511;&#21046;&#22120;&#20855;&#26377;&#26029;&#30005;&#26426;&#21046;&#65292;&#21487;&#20197;&#32469;&#36807;&#28508;&#22312;&#30340;&#23433;&#20840;&#38544;&#24739;&#24182;&#30830;&#20445;&#31995;&#32479;&#21442;&#25968;&#20272;&#35745;&#30340;&#25910;&#25947;&#24615;&#65292;&#20294;&#34987;&#35777;&#26126;&#21482;&#20250;&#26377;&#26377;&#38480;&#27425;&#35302;&#21457;&#65292;&#24182;&#23545;&#25511;&#21046;&#22120;&#30340;&#28176;&#36817;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;&#36890;&#36807;&#22312;&#30000;&#32435;&#35199;&#20234;&#22763;&#26364;(Tennessee Eastman)&#24037;&#33402;&#20013;&#36827;&#34892;&#20223;&#30495;&#39564;&#35777;&#20102;&#35813;&#25511;&#21046;&#22120;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Linear-Quadratic Regulation (LQR) problem with unknown system parameters has been widely studied, but it has remained unclear whether $\tilde{ \mathcal{O}}(\sqrt{T})$ regret, which is the best known dependence on time, can be achieved almost surely. In this paper, we propose an adaptive LQR controller with almost surely $\tilde{ \mathcal{O}}(\sqrt{T})$ regret upper bound. The controller features a circuit-breaking mechanism, which circumvents potential safety breach and guarantees the convergence of the system parameter estimate, but is shown to be triggered only finitely often and hence has negligible effect on the asymptotic performance of the controller. The proposed controller is also validated via simulation on Tennessee Eastman Process~(TEP), a commonly used industrial process example.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#20256;&#32479;&#38543;&#26426;&#25928;&#29992;&#27169;&#22411;&#22312;&#24314;&#27169;&#20986;&#34892;&#26041;&#24335;&#36873;&#25321;&#26041;&#38754;&#30340;&#24615;&#33021;&#21644;&#29305;&#28857;&#65292;&#24182;&#30830;&#23450;&#21738;&#20123;&#27169;&#22411;&#26368;&#36866;&#21512;&#20110;&#24314;&#27169;&#65292;&#20986;&#34892;&#34892;&#20026;&#21487;&#35299;&#37322;&#24615;&#21644;&#35299;&#37322;&#24615;&#12289;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#25968;&#25454;&#25928;&#29575;&#31561;&#22240;&#32032;&#38656;&#35201;&#36827;&#34892;&#25972;&#20307;&#32771;&#34385;&#12290;</title><link>http://arxiv.org/abs/2301.04404</link><description>&lt;p&gt;
&#23545;&#20986;&#34892;&#26041;&#24335;&#36873;&#25321;&#24314;&#27169;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#39044;&#27979;&#21644;&#34892;&#20026;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A prediction and behavioural analysis of machine learning methods for modelling travel mode choice. (arXiv:2301.04404v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#20256;&#32479;&#38543;&#26426;&#25928;&#29992;&#27169;&#22411;&#22312;&#24314;&#27169;&#20986;&#34892;&#26041;&#24335;&#36873;&#25321;&#26041;&#38754;&#30340;&#24615;&#33021;&#21644;&#29305;&#28857;&#65292;&#24182;&#30830;&#23450;&#21738;&#20123;&#27169;&#22411;&#26368;&#36866;&#21512;&#20110;&#24314;&#27169;&#65292;&#20986;&#34892;&#34892;&#20026;&#21487;&#35299;&#37322;&#24615;&#21644;&#35299;&#37322;&#24615;&#12289;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#25968;&#25454;&#25928;&#29575;&#31561;&#22240;&#32032;&#38656;&#35201;&#36827;&#34892;&#25972;&#20307;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20986;&#34892;&#26041;&#24335;&#36873;&#25321;&#38382;&#39064;&#65292;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#20986;&#29616;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#38382;&#39064;&#65306;&#21738;&#20123;&#27169;&#22411;&#36866;&#29992;&#20110;&#21738;&#20123;&#24212;&#29992;&#65311;&#36825;&#20010;&#38382;&#39064;&#30340;&#31572;&#26696;&#19981;&#20165;&#20165;&#22312;&#20110;&#31616;&#21333;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#36824;&#28041;&#21450;&#34892;&#20026;&#21487;&#35299;&#37322;&#24615;&#21644;&#35299;&#37322;&#24615;&#12289;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#25968;&#25454;&#25928;&#29575;&#31561;&#22810;&#26041;&#38754;&#30340;&#24179;&#34913;&#12290;&#35768;&#22810;&#30740;&#31350;&#35797;&#22270;&#27604;&#36739;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#19982;&#20256;&#32479;&#38543;&#26426;&#25928;&#29992;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#36890;&#24120;&#21482;&#20998;&#26512;&#31163;&#25955;&#39044;&#27979;&#24615;&#33021;&#65292;&#24573;&#30053;&#20854;&#20182;&#24433;&#21709;&#27169;&#22411;&#36873;&#25321;&#30340;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#30740;&#31350;&#21463;&#25216;&#26415;&#38480;&#21046;&#30340;&#24433;&#21709;&#65292;&#20363;&#22914;&#20351;&#29992;&#19981;&#24688;&#24403;&#30340;&#39564;&#35777;&#26041;&#26696;&#12289;&#20998;&#23618;&#25968;&#25454;&#30340;&#38169;&#35823;&#25277;&#26679;&#12289;&#32570;&#20047;&#22806;&#37096;&#39564;&#35777;&#20197;&#21450;&#20165;&#20351;&#29992;&#31163;&#25955;&#24230;&#37327;&#31561;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#30340;&#34892;&#20026;&#20998;&#26512;&#65292;&#26088;&#22312;&#27604;&#36739;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#34920;&#29616;&#21644;&#29305;&#28857;&#65292;&#20197;&#20415;&#30830;&#23450;&#26368;&#36866;&#21512;&#24314;&#27169;&#20986;&#34892;&#36873;&#25321;&#30340;&#27169;&#22411;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of a variety of Machine Learning (ML) approaches for travel mode choice prediction poses an interesting question to transport modellers: which models should be used for which applications? The answer to this question goes beyond simple predictive performance, and is instead a balance of many factors, including behavioural interpretability and explainability, computational complexity, and data efficiency. There is a growing body of research which attempts to compare the predictive performance of different ML classifiers with classical random utility models. However, existing studies typically analyse only the disaggregate predictive performance, ignoring other aspects affecting model choice. Furthermore, many studies are affected by technical limitations, such as the use of inappropriate validation schemes, incorrect sampling for hierarchical data, lack of external validation, and the exclusive use of discrete metrics. We address these limitations by conducting a systemati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22343;&#22330;&#21338;&#24328;&#20013;&#20351;&#29992;&#31574;&#30053;&#38236;&#38754;&#19978;&#21319;&#23454;&#29616;&#39640;&#25928;&#29420;&#31435;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#20154;&#21475;&#29983;&#25104;&#27169;&#22411;&#65292;&#19988;&#21482;&#38656;&#35201;&#32422;$\widetilde{\mathcal{O}}(\varepsilon^{-2})$&#20010;&#26679;&#26412;&#21363;&#21487;&#36798;&#21040;&#32435;&#20160;&#22343;&#34913;&#12290;</title><link>http://arxiv.org/abs/2212.14449</link><description>&lt;p&gt;
&#12298;&#22312;&#22343;&#22330;&#21338;&#24328;&#20013;&#20351;&#29992;&#31574;&#30053;&#38236;&#38754;&#19978;&#21319;&#23454;&#29616;&#39640;&#25928;&#29420;&#31435;&#23398;&#20064;&#12299;
&lt;/p&gt;
&lt;p&gt;
Policy Mirror Ascent for Efficient and Independent Learning in Mean Field Games. (arXiv:2212.14449v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22343;&#22330;&#21338;&#24328;&#20013;&#20351;&#29992;&#31574;&#30053;&#38236;&#38754;&#19978;&#21319;&#23454;&#29616;&#39640;&#25928;&#29420;&#31435;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#20154;&#21475;&#29983;&#25104;&#27169;&#22411;&#65292;&#19988;&#21482;&#38656;&#35201;&#32422;$\widetilde{\mathcal{O}}(\varepsilon^{-2})$&#20010;&#26679;&#26412;&#21363;&#21487;&#36798;&#21040;&#32435;&#20160;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22343;&#22330;&#21338;&#24328;&#34987;&#29992;&#20316;&#33719;&#24471;&#23545;&#31216;&#21644;&#21311;&#21517;&#30340;N&#20154;&#21338;&#24328;&#30340;&#36817;&#20284;&#32435;&#20160;&#22343;&#34913;&#30340;&#29702;&#35770;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29702;&#35770;&#32467;&#26524;&#21482;&#36866;&#29992;&#20110;"&#20154;&#21475;&#29983;&#25104;&#27169;&#22411;"&#30340;&#21464;&#21270;&#65292;&#35813;&#27169;&#22411;&#20801;&#35768;&#23398;&#20064;&#31639;&#27861;&#36890;&#36807;&#20154;&#21475;&#20998;&#24067;&#36827;&#34892;&#20219;&#24847;&#20462;&#25913;&#12290;&#27492;&#22806;&#65292;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#20351;&#29992;&#20855;&#26377;&#20154;&#21475;&#23646;&#24615;&#30340;&#25277;&#35937;&#27169;&#25311;&#22120;&#65292;&#32780;&#19981;&#26159;N&#20154;&#21338;&#24328;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;N&#20010;&#20195;&#29702;&#36816;&#34892;&#31574;&#30053;&#38236;&#38754;&#19978;&#21319;&#26159;&#22914;&#20309;&#22312;&#19981;&#20351;&#29992;&#20154;&#21475;&#29983;&#25104;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#21333;&#20010;&#26679;&#26412;&#36712;&#36857;&#20013;&#25910;&#25947;&#20110;&#35268;&#21017;&#21338;&#24328;&#30340;&#32435;&#20160;&#22343;&#34913;&#65292;&#21482;&#38656;&#35201;&#22823;&#32422;$\widetilde{\mathcal{O}}(\varepsilon^{-2})$&#30340;&#26679;&#26412;&#65292;&#30001;&#20110;&#22343;&#22330;&#30340;&#32536;&#25925;&#65292;&#35823;&#24046;&#20026;$\mathcal{O}(\frac{1}{\sqrt{N}})$&#12290;&#30456;&#36739;&#20110;&#25991;&#29486;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#21487;&#20197;&#20351;&#29992;&#31574;&#30053;&#38236;&#38754;&#19978;&#21319;&#26144;&#23556;&#26469;&#26500;&#24314;&#19968;&#20010;&#22865;&#32422;&#31639;&#23376;&#65292;&#32780;&#19981;&#26159;&#19982;&#26368;&#20339;&#21709;&#24212;&#26144;&#23556;&#19968;&#36215;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mean-field games have been used as a theoretical tool to obtain an approximate Nash equilibrium for symmetric and anonymous $N$-player games. However, limiting applicability, existing theoretical results assume variations of a "population generative model", which allows arbitrary modifications of the population distribution by the learning algorithm. Moreover, learning algorithms typically work on abstract simulators with population instead of the $N$-player game. Instead, we show that $N$ agents running policy mirror ascent converge to the Nash equilibrium of the regularized game within $\widetilde{\mathcal{O}}(\varepsilon^{-2})$ samples from a single sample trajectory without a population generative model, up to a standard $\mathcal{O}(\frac{1}{\sqrt{N}})$ error due to the mean field. Taking a divergent approach from the literature, instead of working with the best-response map we first show that a policy mirror ascent map can be used to construct a contractive operator having the Na
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#24179;&#28369;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#27861; (DSGDA)&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#38750;&#20984;-&#38750;&#20985;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#65292;&#24182;&#19988;&#33021;&#22815;&#20840;&#23616;&#25910;&#25947;&#24182;&#28040;&#38500;&#26497;&#38480;&#29615;&#12290;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#65292;DSGDA &#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#36798;&#21040;&#20102;&#25991;&#29486;&#20013;&#21333;&#24490;&#29615;&#31639;&#27861;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.12978</link><description>&lt;p&gt;
&#21452;&#37325;&#24179;&#28369;GDA&#65306;&#29992;&#20110;&#38750;&#20984;-&#38750;&#20985;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#30340;&#20840;&#23616;&#25910;&#25947;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Doubly Smoothed GDA: Global Convergent Algorithm for Constrained Nonconvex-Nonconcave Minimax Optimization. (arXiv:2212.12978v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#24179;&#28369;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#27861; (DSGDA)&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#38750;&#20984;-&#38750;&#20985;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#65292;&#24182;&#19988;&#33021;&#22815;&#20840;&#23616;&#25910;&#25947;&#24182;&#28040;&#38500;&#26497;&#38480;&#29615;&#12290;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#65292;DSGDA &#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#36798;&#21040;&#20102;&#25991;&#29486;&#20013;&#21333;&#24490;&#29615;&#31639;&#27861;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#20984;-&#38750;&#20985;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#31639;&#27861;&#19981;&#33021;&#20445;&#35777;&#20840;&#23616;&#25910;&#25947;&#65292;&#29978;&#33267;&#20250;&#36973;&#21463;&#26497;&#38480;&#29615;&#30340;&#22256;&#25200;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;&#24490;&#29615;&#31639;&#27861;&#65292;&#31216;&#20026;&#21452;&#37325;&#24179;&#28369;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#27861; (DSGDA)&#65292;&#23427;&#33021;&#22815;&#33258;&#28982;&#22320;&#24179;&#34913;&#21407;&#22987;&#19982;&#23545;&#20598;&#26356;&#26032;&#65292;&#24182;&#19988;&#23558;&#26497;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38750;&#20984;-&#38750;&#20985;&#20363;&#23376;&#20013;&#30340;&#26497;&#38480;&#29615;&#28040;&#38500;&#65292;&#21253;&#25324; Forsaken&#65292;Bilinearly-coupled minimax&#65292;Sixth-order polynomial &#21644; PolarGame&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#65292;&#22312;&#19968;&#20010;&#21333;&#20391;&#30340; $\theta\in(0,1)$ Kurdyka-\L{}ojasiewicz&#26465;&#20214;&#65288;&#25110;&#20984;&#21407;&#22987;/&#20985;&#23545;&#20598;&#20989;&#25968;&#65289;&#19979;&#65292;DSGDA &#21487;&#20197;&#25214;&#21040;&#19968;&#20010;&#28216;&#25103;&#24179;&#34913;&#28857;&#65292;&#24182;&#19988;&#20855;&#26377;&#36845;&#20195;&#22797;&#26434;&#24230; $\mathcal{O}(\epsilon^{-2\max\{2\theta,1\}})$&#65288;&#25110; $\mathcal{O}(\epsilon^{-4})$&#65289;&#65292;&#36825;&#20123;&#19982;&#25991;&#29486;&#20013;&#21333;&#24490;&#29615;&#31639;&#27861;&#30340;&#26368;&#20339;&#32467;&#26524;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nonconvex-nonconcave minimax optimization has received intense attention over the last decade due to its broad applications in machine learning. Unfortunately, most existing algorithms cannot be guaranteed to converge globally and even suffer from limit cycles. To address this issue, we propose a novel single-loop algorithm called doubly smoothed gradient descent ascent method (DSGDA), which naturally balances the primal and dual updates. The proposed DSGDA can get rid of limit cycles in various challenging nonconvex-nonconcave examples in the literature, including Forsaken, Bilinearly-coupled minimax, Sixth-order polynomial, and PolarGame. We further show that under an one-sided Kurdyka-\L{}ojasiewicz condition with exponent $\theta\in(0,1)$ (resp. convex primal/concave dual function), DSGDA can find a game-stationary point with an iteration complexity of $\mathcal{O}(\epsilon^{-2\max\{2\theta,1\}})$ (resp. $\mathcal{O}(\epsilon^{-4})$). These match the best results for single-loop al
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#27169;&#22411;&#12289;&#28145;&#24230;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#24033;&#36923;&#29615;&#22659;&#20013;&#22810;&#20010;&#33258;&#20027;&#36710;&#36742;&#21327;&#21516;&#25191;&#34892;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36824;&#32771;&#34385;&#20102;&#33021;&#37327;&#28040;&#32791;&#21644;&#23481;&#38169;&#31561;&#22240;&#32032;&#65292;&#20197;&#30830;&#20445;&#31995;&#32479;&#36830;&#32493;&#33258;&#36866;&#24212;&#22320;&#36816;&#34892;&#12290;</title><link>http://arxiv.org/abs/2212.08230</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#26234;&#33021;&#20307;&#24033;&#36923;&#38382;&#39064;&#30340;&#33021;&#37327;&#24863;&#30693;&#21644;&#23481;&#38169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Energy-aware and Fault-tolerant Deep Reinforcement Learning based approach for Multi-agent Patrolling Problems. (arXiv:2212.08230v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#27169;&#22411;&#12289;&#28145;&#24230;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#24033;&#36923;&#29615;&#22659;&#20013;&#22810;&#20010;&#33258;&#20027;&#36710;&#36742;&#21327;&#21516;&#25191;&#34892;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36824;&#32771;&#34385;&#20102;&#33021;&#37327;&#28040;&#32791;&#21644;&#23481;&#38169;&#31561;&#22240;&#32032;&#65292;&#20197;&#30830;&#20445;&#31995;&#32479;&#36830;&#32493;&#33258;&#36866;&#24212;&#22320;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#36710;&#36742;&#36866;&#29992;&#20110;&#36830;&#32493;&#21306;&#22495;&#24033;&#36923;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20247;&#22810;&#21407;&#22240;&#65292;&#23547;&#25214;&#26368;&#20248;&#24033;&#36923;&#31574;&#30053;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#39318;&#20808;&#65292;&#24033;&#36923;&#29615;&#22659;&#36890;&#24120;&#36739;&#20026;&#22797;&#26434;&#65292;&#21487;&#33021;&#20250;&#21253;&#21547;&#35832;&#22914;&#39118;&#25110;&#26223;&#35266;&#31561;&#26410;&#30693;&#29615;&#22659;&#22240;&#32032;&#12290;&#20854;&#27425;&#65292;&#33258;&#20027;&#36710;&#36742;&#21487;&#33021;&#20250;&#20986;&#29616;&#25925;&#38556;&#25110;&#30828;&#20214;&#38480;&#21046;&#65292;&#20363;&#22914;&#30005;&#27744;&#23551;&#21629;&#26377;&#38480;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#24033;&#36923;&#22823;&#21306;&#22495;&#36890;&#24120;&#38656;&#35201;&#22810;&#20010;&#26234;&#33021;&#20307;&#21327;&#21516;&#25191;&#34892;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#27169;&#22411;&#12289;&#28145;&#24230;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#22312;&#35813;&#26041;&#27861;&#20013;&#65292;&#26234;&#33021;&#20307;&#34987;&#35757;&#32451;&#22312;&#20855;&#26377;&#21508;&#31181;&#26410;&#30693;&#21160;&#24577;&#21644;&#22240;&#32032;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#24033;&#36923;&#12290;&#23427;&#20204;&#21487;&#20197;&#33258;&#21160;&#20805;&#30005;&#20197;&#25903;&#25345;&#36830;&#32493;&#38598;&#20307;&#24033;&#36923;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#30340;&#21516;&#26500;&#22810;&#26234;&#33021;&#20307;&#26550;&#26500;&#65292;&#20854;&#20013;&#25152;&#26377;&#24033;&#36923;&#26234;&#33021;&#20307;&#22522;&#20110;&#20854;&#26412;&#22320;&#35266;&#23519;&#21644;&#20849;&#20139;&#36890;&#20449;&#22312;&#26412;&#22320;&#25191;&#34892;&#30456;&#21516;&#30340;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#32771;&#34385;&#20102;&#33021;&#37327;&#28040;&#32791;&#21644;&#23481;&#38169;&#31561;&#22240;&#32032;&#65292;&#20197;&#30830;&#20445;&#31995;&#32479;&#36830;&#32493;&#33258;&#36866;&#24212;&#22320;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous vehicles are suited for continuous area patrolling problems. However, finding an optimal patrolling strategy can be challenging for many reasons. Firstly, patrolling environments are often complex and can include unknown environmental factors, such as wind or landscape. Secondly, autonomous vehicles can have failures or hardware constraints, such as limited battery life. Importantly, patrolling large areas often requires multiple agents that need to collectively coordinate their actions. In this work, we consider these limitations and propose an approach based on model-free, deep multi-agent reinforcement learning. In this approach, the agents are trained to patrol an environment with various unknown dynamics and factors. They can automatically recharge themselves to support continuous collective patrolling. A distributed homogeneous multi-agent architecture is proposed, where all patrolling agents execute identical policies locally based on their local observations and shar
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Huber&#33021;&#37327;&#37327;&#21270;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#30446;&#26631;&#27010;&#29575;&#23450;&#24459;&#30340;&#26368;&#20339;&#36924;&#36817;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#21407;&#27979;&#24230;&#19982;&#37327;&#21270;&#29256;&#26412;&#20043;&#38388;&#30340;&#32479;&#35745;&#36317;&#31163;&#26469;&#23454;&#29616;&#12290;&#35813;&#31639;&#27861;&#24050;&#22312;&#22810;&#32500;&#39640;&#26031;&#28151;&#21512;&#29289;&#12289;&#32500;&#32435;&#31354;&#38388;&#39764;&#26041;&#31561;&#20960;&#20010;&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2212.08162</link><description>&lt;p&gt;
Huber&#33021;&#37327;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Huber-energy measure quantization. (arXiv:2212.08162v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08162
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Huber&#33021;&#37327;&#37327;&#21270;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#30446;&#26631;&#27010;&#29575;&#23450;&#24459;&#30340;&#26368;&#20339;&#36924;&#36817;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#21407;&#27979;&#24230;&#19982;&#37327;&#21270;&#29256;&#26412;&#20043;&#38388;&#30340;&#32479;&#35745;&#36317;&#31163;&#26469;&#23454;&#29616;&#12290;&#35813;&#31639;&#27861;&#24050;&#22312;&#22810;&#32500;&#39640;&#26031;&#28151;&#21512;&#29289;&#12289;&#32500;&#32435;&#31354;&#38388;&#39764;&#26041;&#31561;&#20960;&#20010;&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#27979;&#37327;&#37327;&#21270;&#36807;&#31243;&#65292;&#21363;&#19968;&#31181;&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;$Q$&#20010;&#29380;&#25289;&#20811;&#20989;&#25968;&#30340;&#24635;&#21644;&#65288;$Q$&#20026;&#37327;&#21270;&#21442;&#25968;&#65289;&#65292;&#25214;&#21040;&#30446;&#26631;&#27010;&#29575;&#23450;&#24459;&#65288;&#26356;&#19968;&#33324;&#22320;&#65292;&#20026;&#26377;&#38480;&#21464;&#24046;&#27979;&#24230;&#65289;&#30340;&#26368;&#20339;&#36924;&#36817;&#12290;&#35813;&#36807;&#31243;&#36890;&#36807;&#23558;&#21407;&#27979;&#24230;&#19982;&#20854;&#37327;&#21270;&#29256;&#26412;&#20043;&#38388;&#30340;&#32479;&#35745;&#36317;&#31163;&#26368;&#23567;&#21270;&#26469;&#23454;&#29616;&#65307;&#35813;&#36317;&#31163;&#22522;&#20110;&#36127;&#23450;&#26680;&#26500;&#24314;&#65292;&#24182;&#19988;&#22914;&#26524;&#24517;&#35201;&#65292;&#21487;&#20197;&#23454;&#26102;&#35745;&#31639;&#24182;&#36755;&#20837;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#65288;&#22914;SGD&#65292;Adam&#31561;&#65289;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#26368;&#20248;&#27979;&#37327;&#37327;&#21270;&#22120;&#30340;&#23384;&#22312;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#24182;&#30830;&#23450;&#20102;&#38656;&#35201;&#20445;&#35777;&#21512;&#36866;&#34892;&#20026;&#30340;&#26680;&#23646;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26368;&#20339;&#32447;&#24615;&#26080;&#20559;&#65288;BLUE&#65289;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#24179;&#26041;&#32479;&#35745;&#36317;&#31163;&#65292;&#24182;&#23558;&#23427;&#20204;&#29992;&#20110;&#26080;&#20559;&#31243;&#24207;HEMQ&#20013;&#65292;&#20197;&#25214;&#21040;&#26368;&#20339;&#37327;&#21270;&#12290;&#25105;&#20204;&#22312;&#22810;&#32500;&#39640;&#26031;&#28151;&#21512;&#29289;&#12289;&#32500;&#32435;&#31354;&#38388;&#39764;&#26041;&#31561;&#20960;&#20010;&#25968;&#25454;&#24211;&#19978;&#27979;&#35797;&#20102;HEMQ
&lt;/p&gt;
&lt;p&gt;
We describe a measure quantization procedure i.e., an algorithm which finds the best approximation of a target probability law (and more generally signed finite variation measure) by a sum of $Q$ Dirac masses ($Q$ being the quantization parameter). The procedure is implemented by minimizing the statistical distance between the original measure and its quantized version; the distance is built from a negative definite kernel and, if necessary, can be computed on the fly and feed to a stochastic optimization algorithm (such as SGD, Adam, ...). We investigate theoretically the fundamental questions of existence of the optimal measure quantizer and identify what are the required kernel properties that guarantee suitable behavior. We propose two best linear unbiased (BLUE) estimators for the squared statistical distance and use them in an unbiased procedure, called HEMQ, to find the optimal quantization. We test HEMQ on several databases: multi-dimensional Gaussian mixtures, Wiener space cub
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#21518;&#38376;&#25915;&#20987;&#26694;&#26550; BadDiffusion&#65292;&#20854;&#21487;&#20197;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#23454;&#29616;&#26893;&#20837;&#21518;&#38376;&#65292;&#23548;&#33268;&#27169;&#22411;&#22312;&#27491;&#24120;&#25968;&#25454;&#36755;&#20837;&#26102;&#20381;&#28982;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#25509;&#25910;&#21040;&#35302;&#21457;&#20449;&#21495;&#26102;&#20135;&#29983;&#35823;&#23548;&#24615;&#36755;&#20986;&#12290;&#35813;&#25915;&#20987;&#21487;&#33021;&#23545;&#24314;&#31435;&#22312;&#26377;&#38382;&#39064;&#30340;&#27169;&#22411;&#20043;&#19978;&#30340;&#19979;&#28216;&#20219;&#21153;&#21644;&#24212;&#29992;&#36896;&#25104;&#20005;&#37325;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2212.05400</link><description>&lt;p&gt;
&#22914;&#20309;&#21518;&#38376;&#25193;&#25955;&#27169;&#22411;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to Backdoor Diffusion Models?. (arXiv:2212.05400v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05400
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#21518;&#38376;&#25915;&#20987;&#26694;&#26550; BadDiffusion&#65292;&#20854;&#21487;&#20197;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#23454;&#29616;&#26893;&#20837;&#21518;&#38376;&#65292;&#23548;&#33268;&#27169;&#22411;&#22312;&#27491;&#24120;&#25968;&#25454;&#36755;&#20837;&#26102;&#20381;&#28982;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#25509;&#25910;&#21040;&#35302;&#21457;&#20449;&#21495;&#26102;&#20135;&#29983;&#35823;&#23548;&#24615;&#36755;&#20986;&#12290;&#35813;&#25915;&#20987;&#21487;&#33021;&#23545;&#24314;&#31435;&#22312;&#26377;&#38382;&#39064;&#30340;&#27169;&#22411;&#20043;&#19978;&#30340;&#19979;&#28216;&#20219;&#21153;&#21644;&#24212;&#29992;&#36896;&#25104;&#20005;&#37325;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20854;&#35757;&#32451;&#21407;&#29702;&#26159;&#36890;&#36807;&#36880;&#27493;&#28155;&#21152;&#22122;&#22768;&#21644;&#21435;&#22122;&#23398;&#20064;&#27491;&#21521;&#21644;&#21453;&#21521;&#30340;&#25193;&#25955;&#36807;&#31243;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#20102;&#35299;&#20854;&#38480;&#21046;&#21644;&#28508;&#22312;&#39118;&#38505;&#65292;&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#23545;&#21518;&#38376;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BadDiffusion&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#25915;&#20987;&#26694;&#26550;&#65292;&#23427;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#24037;&#31243;&#21270;&#20102;&#21463;&#25439;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#36827;&#34892;&#21518;&#38376;&#26893;&#20837;&#12290;&#22312;&#25512;&#29702;&#38454;&#27573;&#65292;&#21518;&#38376;&#25193;&#25955;&#27169;&#22411;&#23558;&#20687;&#26222;&#36890;&#25968;&#25454;&#36755;&#20837;&#30340;&#26410;&#31713;&#25913;&#29983;&#25104;&#22120;&#19968;&#26679;&#36816;&#34892;&#65292;&#21516;&#26102;&#22312;&#25509;&#25910;&#21040;&#26893;&#20837;&#30340;&#35302;&#21457;&#20449;&#21495;&#21518;&#65292;&#20266;&#36896;&#20986;&#19968;&#20123;&#34987;&#22351;&#28436;&#21592;&#35774;&#35745;&#30340;&#30446;&#26631;&#32467;&#26524;&#12290;&#36825;&#31181;&#37325;&#22823;&#39118;&#38505;&#21487;&#33021;&#23545;&#24314;&#31435;&#22312;&#26377;&#38382;&#39064;&#30340;&#27169;&#22411;&#20043;&#19978;&#30340;&#19979;&#28216;&#20219;&#21153;&#21644;&#24212;&#29992;&#36896;&#25104;&#20005;&#37325;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#21518;&#38376;&#25915;&#20987;&#35774;&#32622;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126; BadDiffusion &#21487;&#20197;&#31283;&#23450;&#22320;&#20266;&#36896;&#29305;&#23450;&#30340;&#30446;&#26631;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are state-of-the-art deep learning empowered generative models that are trained based on the principle of learning forward and reverse diffusion processes via progressive noise-addition and denoising. To gain a better understanding of the limitations and potential risks, this paper presents the first study on the robustness of diffusion models against backdoor attacks. Specifically, we propose BadDiffusion, a novel attack framework that engineers compromised diffusion processes during model training for backdoor implantation. At the inference stage, the backdoored diffusion model will behave just like an untampered generator for regular data inputs, while falsely generating some targeted outcome designed by the bad actor upon receiving the implanted trigger signal. Such a critical risk can be dreadful for downstream tasks and applications built upon the problematic model. Our extensive experiments on various backdoor attack settings show that BadDiffusion can consisten
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#25968;&#25454;&#24322;&#26500;&#24615;&#22914;&#20309;&#24433;&#21709;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23618;&#20043;&#38388;&#30340;&#26799;&#24230;&#26356;&#26032;&#65292;&#21457;&#29616;&#34429;&#28982;&#29305;&#24449;&#25552;&#21462;&#23618;&#33021;&#34987;&#26377;&#25928;&#23398;&#20064;&#65292;&#20294;&#23458;&#25143;&#31471;&#26368;&#32456;&#20998;&#31867;&#23618;&#30340;&#22823;&#37327;&#24046;&#24322;&#38459;&#30861;&#20102;&#35813;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#20165;&#23545;&#26368;&#32456;&#23618;&#30340;&#26041;&#24046;&#36827;&#34892;&#38477;&#20302;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20854;&#21487;&#20197;&#22312;&#30456;&#20284;&#25110;&#26356;&#20302;&#30340;&#36890;&#20449;&#25104;&#26412;&#19979;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.02191</link><description>&lt;p&gt;
&#35770;&#24322;&#26500;&#25968;&#25454;&#20013;&#37096;&#20998;&#26041;&#24046;&#38477;&#20302;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the effectiveness of partial variance reduction in federated learning with heterogeneous data. (arXiv:2212.02191v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#25968;&#25454;&#24322;&#26500;&#24615;&#22914;&#20309;&#24433;&#21709;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23618;&#20043;&#38388;&#30340;&#26799;&#24230;&#26356;&#26032;&#65292;&#21457;&#29616;&#34429;&#28982;&#29305;&#24449;&#25552;&#21462;&#23618;&#33021;&#34987;&#26377;&#25928;&#23398;&#20064;&#65292;&#20294;&#23458;&#25143;&#31471;&#26368;&#32456;&#20998;&#31867;&#23618;&#30340;&#22823;&#37327;&#24046;&#24322;&#38459;&#30861;&#20102;&#35813;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#20165;&#23545;&#26368;&#32456;&#23618;&#30340;&#26041;&#24046;&#36827;&#34892;&#38477;&#20302;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20854;&#21487;&#20197;&#22312;&#30456;&#20284;&#25110;&#26356;&#20302;&#30340;&#36890;&#20449;&#25104;&#26412;&#19979;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23458;&#25143;&#31471;&#25968;&#25454;&#30340;&#24322;&#26500;&#24615;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#20043;&#21069;&#30340;&#26041;&#27861;&#36890;&#36807;&#35843;&#25972;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#27169;&#22411;&#25110;&#20351;&#29992;&#25511;&#21046;&#21464;&#37327;&#26469;&#32416;&#27491;&#23458;&#25143;&#31471;&#27169;&#22411;&#28418;&#31227;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#33021;&#22312;&#20984;&#25110;&#31616;&#21333;&#30340;&#38750;&#20984;&#38382;&#39064;&#20013;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#65292;&#20294;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31561;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#27169;&#22411;&#20013;&#24615;&#33021;&#36739;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#37325;&#26032;&#23457;&#26597;&#20102;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;FedAvg&#31639;&#27861;&#65292;&#20197;&#20102;&#35299;&#25968;&#25454;&#24322;&#26500;&#24615;&#22914;&#20309;&#24433;&#21709;&#31070;&#32463;&#32593;&#32476;&#23618;&#20043;&#38388;&#30340;&#26799;&#24230;&#26356;&#26032;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#34429;&#28982;FedAvg&#26377;&#25928;&#22320;&#23398;&#20064;&#20102;&#29305;&#24449;&#25552;&#21462;&#23618;&#65292;&#20294;&#23458;&#25143;&#31471;&#26368;&#32456;&#20998;&#31867;&#23618;&#30340;&#22823;&#37327;&#24046;&#24322;&#20250;&#38459;&#30861;&#24615;&#33021;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20165;&#23545;&#26368;&#32456;&#23618;&#30340;&#26041;&#24046;&#36827;&#34892;&#38477;&#20302;&#20197;&#32416;&#27491;&#27169;&#22411;&#28418;&#31227;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#26679;&#20570;&#21487;&#20197;&#22312;&#30456;&#20284;&#25110;&#26356;&#20302;&#30340;&#36890;&#20449;&#25104;&#26412;&#19979;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#35777;&#26126;&#8230;
&lt;/p&gt;
&lt;p&gt;
Data heterogeneity across clients is a key challenge in federated learning. Prior works address this by either aligning client and server models or using control variates to correct client model drift. Although these methods achieve fast convergence in convex or simple non-convex problems, the performance in over-parameterized models such as deep neural networks is lacking. In this paper, we first revisit the widely used FedAvg algorithm in a deep neural network to understand how data heterogeneity influences the gradient updates across the neural network layers. We observe that while the feature extraction layers are learned efficiently by FedAvg, the substantial diversity of the final classification layers across clients impedes the performance. Motivated by this, we propose to correct model drift by variance reduction only on the final layers. We demonstrate that this significantly outperforms existing benchmarks at a similar or lower communication cost. We furthermore provide proof
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;DRL&#21435;&#28151;&#28102;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#25110;&#37325;&#26032;&#37319;&#26679;&#31163;&#32447;&#25968;&#25454;&#38598;&#26469;&#35843;&#25972;&#19981;&#21516;&#37319;&#26679;&#23545;&#25439;&#22833;&#20989;&#25968;&#30340;&#24433;&#21709;&#65292;&#20197;&#30830;&#20445;&#20854;&#26080;&#20559;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;DRL&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.15355</link><description>&lt;p&gt;
&#21033;&#29992;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#22240;&#26524;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Causal Deep Reinforcement Learning Using Observational Data. (arXiv:2211.15355v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;DRL&#21435;&#28151;&#28102;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#25110;&#37325;&#26032;&#37319;&#26679;&#31163;&#32447;&#25968;&#25454;&#38598;&#26469;&#35843;&#25972;&#19981;&#21516;&#37319;&#26679;&#23545;&#25439;&#22833;&#20989;&#25968;&#30340;&#24433;&#21709;&#65292;&#20197;&#30830;&#20445;&#20854;&#26080;&#20559;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;DRL&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#38656;&#35201;&#25910;&#38598;&#24178;&#39044;&#25968;&#25454;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#26377;&#26102;&#26114;&#36149;&#29978;&#33267;&#19981;&#36947;&#24503;&#65292;&#27604;&#22914;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#21307;&#30103;&#39046;&#22495;&#12290;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#21033;&#29992;&#29616;&#23454;&#19990;&#30028;&#20013;&#24191;&#27867;&#21487;&#29992;&#30340;&#35266;&#27979;&#25968;&#25454;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#22914;&#26524;&#20135;&#29983;&#25968;&#25454;&#30340;&#34892;&#20026;&#31574;&#30053;&#21462;&#20915;&#20110;&#26410;&#35266;&#23519;&#21040;&#30340;&#38543;&#26426;&#21464;&#37327;&#65288;&#21363;&#28151;&#28102;&#22240;&#32032;&#65289;&#65292;&#37027;&#20040;&#35266;&#27979;&#25968;&#25454;&#21487;&#33021;&#20250;&#35823;&#23548;&#23398;&#20064;&#26234;&#33021;&#20307;&#20135;&#29983;&#19981;&#24819;&#35201;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;DRL&#21435;&#28151;&#28102;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#39318;&#20808;&#20351;&#29992;&#22240;&#26524;&#25512;&#26029;&#25216;&#26415;&#35745;&#31639;&#19981;&#21516;&#37319;&#26679;&#30340;&#37325;&#35201;&#31243;&#24230;&#65292;&#28982;&#21518;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#25110;&#37325;&#26032;&#37319;&#26679;&#31163;&#32447;&#25968;&#25454;&#38598;&#26469;&#35843;&#25972;&#19981;&#21516;&#37319;&#26679;&#23545;&#25439;&#22833;&#20989;&#25968;&#30340;&#24433;&#21709;&#65292;&#20197;&#30830;&#20445;&#20854;&#26080;&#20559;&#12290;&#36825;&#20123;&#21435;&#28151;&#28102;&#30340;&#26041;&#27861;&#21487;&#20197;&#28789;&#27963;&#22320;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;DRL&#31639;&#27861;&#65288;&#22914;&#36719;&#28436;&#21592;-&#35780;&#35770;&#23478;&#21644;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#23398;&#20064;&#22240;&#26524;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#20154;&#24037;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20855;&#26377;&#28151;&#28102;&#22240;&#32032;&#30340;&#22522;&#20934;&#27979;&#35797;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#34920;&#26126;&#23427;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#25552;&#39640;DRL&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (DRL) requires the collection of interventional data, which is sometimes expensive and even unethical in the real world, such as in the autonomous driving and the medical field. Offline reinforcement learning promises to alleviate this issue by exploiting the vast amount of observational data available in the real world. However, observational data may mislead the learning agent to undesirable outcomes if the behavior policy that generates the data depends on unobserved random variables (i.e., confounders). In this paper, we propose two deconfounding methods in DRL to address this problem. The methods first calculate the importance degree of different samples based on the causal inference technique, and then adjust the impact of different samples on the loss function by reweighting or resampling the offline dataset to ensure its unbiasedness. These deconfounding methods can be flexibly combined with existing model-free DRL algorithms such as soft actor-criti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#20855;&#26377;&#22810;&#27425;&#36890;&#36807;&#25968;&#25454;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26426;&#22120;&#23398;&#20064;&#25552;&#20986;&#20102;&#26032;&#30340;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#65292;&#22823;&#22823;&#25913;&#21892;&#20102;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#35745;&#31639;&#20043;&#38388;&#30340;&#25240;&#34935;&#38382;&#39064;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30697;&#38453;&#20998;&#35299;&#25193;&#23637;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20613;&#37324;&#21494;&#21464;&#25442;&#26426;&#21046;&#65292;&#21516;&#26102;&#21462;&#24471;&#20102;&#20248;&#20110;&#20197;&#24448;&#26041;&#27861;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2211.06530</link><description>&lt;p&gt;
&#38024;&#23545;&#31169;&#26377;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#26102;&#26399;&#30697;&#38453;&#20998;&#35299;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Multi-Epoch Matrix Factorization Mechanisms for Private Machine Learning. (arXiv:2211.06530v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#20855;&#26377;&#22810;&#27425;&#36890;&#36807;&#25968;&#25454;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26426;&#22120;&#23398;&#20064;&#25552;&#20986;&#20102;&#26032;&#30340;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#65292;&#22823;&#22823;&#25913;&#21892;&#20102;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#35745;&#31639;&#20043;&#38388;&#30340;&#25240;&#34935;&#38382;&#39064;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30697;&#38453;&#20998;&#35299;&#25193;&#23637;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20613;&#37324;&#21494;&#21464;&#25442;&#26426;&#21046;&#65292;&#21516;&#26102;&#21462;&#24471;&#20102;&#20248;&#20110;&#20197;&#24448;&#26041;&#27861;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#26426;&#21046;&#65292;&#29992;&#20110;&#20855;&#26377;&#22810;&#27425;&#36890;&#36807;&#65288;&#26102;&#26399;&#65289;&#25968;&#25454;&#38598;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#65292;&#22823;&#22823;&#25913;&#21892;&#20102;&#21487;&#23454;&#29616;&#30340;&#38544;&#31169;-&#25928;&#29992;-&#35745;&#31639;&#25240;&#34935;&#12290;&#25105;&#20204;&#24418;&#24335;&#21270;&#20102;&#38024;&#23545;&#20855;&#26377;&#22810;&#27425;&#21442;&#19982;&#30340;&#33258;&#36866;&#24212;&#27969;&#30340;DP&#26426;&#21046;&#30340;&#38382;&#39064;&#65292;&#24182;&#23558;&#22312;&#32447;&#30697;&#38453;&#20998;&#35299;DP&#26426;&#21046;&#30340;&#38750;&#24179;&#20961;&#25193;&#23637;&#24341;&#20837;&#21040;&#25105;&#20204;&#30340;&#35774;&#32622;&#20013;&#12290;&#36825;&#21253;&#25324;&#24314;&#31435;&#28789;&#25935;&#24230;&#35745;&#31639;&#30340;&#24517;&#35201;&#29702;&#35770;&#21644;&#20248;&#21270;&#30697;&#38453;&#30340;&#39640;&#25928;&#35745;&#31639;&#12290;&#23545;&#20110;&#19968;&#20123;&#24212;&#29992;&#31243;&#24207;&#65292;&#20363;&#22914;$&gt;\!\! 10,000$ SGD&#27493;&#39588;&#65292;&#24212;&#29992;&#36825;&#20123;&#26368;&#20339;&#25216;&#26415;&#20250;&#21464;&#24471;&#35745;&#31639;&#26114;&#36149;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#39640;&#25928;&#26426;&#21046;&#65292;&#21482;&#26377;&#36731;&#24494;&#30340;&#25928;&#29992;&#25439;&#22833;&#12290;&#23545;&#20110;&#22270;&#20687;&#20998;&#31867;&#30340;&#31034;&#20363;&#32423;DP&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#29992;&#25143;&#32423;DP&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#32463;&#39564;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#22312;&#25152;&#26377;&#20808;&#21069;&#26041;&#27861;&#20013;&#22343;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#65292;&#21253;&#25324;&#24191;&#27867;&#20351;&#29992;&#30340;DP-SGD&#12290;&#34429;&#28982;&#25105;&#20204;&#30340;&#20027;&#35201;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
We introduce new differentially private (DP) mechanisms for gradient-based machine learning (ML) with multiple passes (epochs) over a dataset, substantially improving the achievable privacy-utility-computation tradeoffs. We formalize the problem of DP mechanisms for adaptive streams with multiple participations and introduce a non-trivial extension of online matrix factorization DP mechanisms to our setting. This includes establishing the necessary theory for sensitivity calculations and efficient computation of optimal matrices. For some applications like $&gt;\!\! 10,000$ SGD steps, applying these optimal techniques becomes computationally expensive. We thus design an efficient Fourier-transform-based mechanism with only a minor utility loss. Extensive empirical evaluation on both example-level DP for image classification and user-level DP for language modeling demonstrate substantial improvements over all previous methods, including the widely-used DP-SGD . Though our primary applicati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#33258;&#21160;&#29983;&#25104;&#26032;&#30340;&#31163;&#32447;&#26816;&#27979;&#26041;&#27861;&#30340;&#26041;&#24335;&#65292;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#33258;&#21160;&#21464;&#28857;&#26816;&#27979;&#65292;&#20854;&#24615;&#33021;&#21487;&#19982;&#26631;&#20934;CUSUM&#20998;&#31867;&#22120;&#24615;&#33021;&#31454;&#20105;&#12290;</title><link>http://arxiv.org/abs/2211.03860</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#33258;&#21160;&#21464;&#28857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Automatic Change-Point Detection in Time Series via Deep Learning. (arXiv:2211.03860v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#33258;&#21160;&#29983;&#25104;&#26032;&#30340;&#31163;&#32447;&#26816;&#27979;&#26041;&#27861;&#30340;&#26041;&#24335;&#65292;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#33258;&#21160;&#21464;&#28857;&#26816;&#27979;&#65292;&#20854;&#24615;&#33021;&#21487;&#19982;&#26631;&#20934;CUSUM&#20998;&#31867;&#22120;&#24615;&#33021;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20013;&#30340;&#21464;&#28857;&#26816;&#27979;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#33258;&#21160;&#29983;&#25104;&#26032;&#30340;&#31163;&#32447;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#37327;&#21270;&#27492;&#26041;&#27861;&#30340;&#35823;&#24046;&#29575;&#21450;&#20854;&#19982;&#35757;&#32451;&#25968;&#25454;&#37327;&#30340;&#20851;&#31995;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20854;&#24615;&#33021;&#20063;&#21487;&#19982;&#29992;&#20110;&#26816;&#27979;&#20013;&#21464;&#21270;&#30340;&#26631;&#20934;CUSUM&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting change-points in data is challenging because of the range of possible types of change and types of behaviour of data when there is no change. Statistically efficient methods for detecting a change will depend on both of these features, and it can be difficult for a practitioner to develop an appropriate detection method for their application of interest. We show how to automatically generate new offline detection methods based on training a neural network. Our approach is motivated by many existing tests for the presence of a change-point being representable by a simple neural network, and thus a neural network trained with sufficient data should have performance at least as good as these methods. We present theory that quantifies the error rate for such an approach, and how it depends on the amount of training data. Empirical results show that, even with limited training data, its performance is competitive with the standard CUSUM-based classifier for detecting a change in m
&lt;/p&gt;</description></item><item><title>L-GreCo&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#36866;&#24212;&#31639;&#27861;&#30340;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#21487;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#27169;&#22411;&#23618;&#20043;&#38388;&#30340;&#21387;&#32553;&#31243;&#24230;&#65292;&#25552;&#39640;&#20102;&#24635;&#20307;&#21387;&#32553;&#29575;&#65292;&#22823;&#24133;&#21152;&#36895;&#65292;&#19981;&#25439;&#22833;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2210.17357</link><description>&lt;p&gt;
L-GreCo&#65306;&#23618;&#38388;&#33258;&#36866;&#24212;&#26799;&#24230;&#21387;&#32553;&#21487;&#29992;&#20110;&#39640;&#25928;&#21644;&#31934;&#30830;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
L-GreCo: Layerwise-Adaptive Gradient Compression for Efficient and Accurate Deep Learning. (arXiv:2210.17357v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17357
&lt;/p&gt;
&lt;p&gt;
L-GreCo&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#36866;&#24212;&#31639;&#27861;&#30340;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#21487;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#27169;&#22411;&#23618;&#20043;&#38388;&#30340;&#21387;&#32553;&#31243;&#24230;&#65292;&#25552;&#39640;&#20102;&#24635;&#20307;&#21387;&#32553;&#29575;&#65292;&#22823;&#24133;&#21152;&#36895;&#65292;&#19981;&#25439;&#22833;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#25968;&#25454;&#24182;&#34892;&#20998;&#24067;&#24335;&#35757;&#32451;&#24471;&#21040;&#24191;&#27867;&#37319;&#29992;&#65292;&#20294;&#20173;&#21487;&#33021;&#23384;&#22312;&#36890;&#20449;&#29942;&#39048;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;&#21253;&#25324;&#37327;&#21270;&#12289;&#31232;&#30095;&#21270;&#21644;&#20302;&#31209;&#36924;&#36817;&#22312;&#20869;&#30340;&#25972;&#20010;&#21387;&#32553;&#26426;&#21046;&#31995;&#21015;&#65292;&#20854;&#20013;&#19968;&#20123;&#27491;&#22312;&#24471;&#21040;&#26174;&#30528;&#30340;&#23454;&#38469;&#37319;&#29992;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20960;&#20046;&#25152;&#26377;&#24050;&#30693;&#21387;&#32553;&#26041;&#26696;&#22343;&#22312;DNN&#23618;&#19978;&#22343;&#21248;&#24212;&#29992;&#21387;&#32553;&#65292;&#23613;&#31649;&#22312;&#21442;&#25968;&#35745;&#25968;&#21644;&#23545;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#26041;&#38754;&#65292;&#23618;&#26159;&#24322;&#26500;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#27169;&#22411;&#23618;&#20043;&#38388;&#30340;&#21387;&#32553;&#31243;&#24230;&#65292;&#25552;&#39640;&#20102;&#24635;&#20307;&#21387;&#32553;&#29575;&#65292;&#21516;&#26102;&#23548;&#33268;&#20102;&#22823;&#24133;&#21152;&#36895;&#65292;&#32780;&#19981;&#20250;&#25439;&#22833;&#31934;&#24230;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21483;&#20570;L-GreCo&#65292;&#22522;&#20110;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#36873;&#25321;&#20026;&#27169;&#22411;&#23618;&#36873;&#25321;&#26368;&#20339;&#21387;&#32553;&#21442;&#25968;&#65292;&#20197;&#20445;&#35777;&#26368;&#20339;&#21387;&#32553;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-parallel distributed training of deep neural networks (DNN) has gained very widespread adoption, but can still experience communication bottlenecks. To address this issue, entire families of compression mechanisms have been developed, including quantization, sparsification, and low-rank approximation, some of which are seeing significant practical adoption. Despite this progress, almost all known compression schemes apply compression uniformly across DNN layers, although layers are heterogeneous in terms of parameter count and their impact on model accuracy. In this work, we provide a general framework for adapting the degree of compression across the model's layers dynamically during training, improving the overall compression, while leading to substantial speedups, without sacrificing accuracy. Our framework, called L-GreCo, is based on an adaptive algorithm, which automatically picks the optimal compression parameters for model layers guaranteeing the best compression ratio whi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#22810;&#38454;&#27573;&#20272;&#35745;&#26041;&#27861;&#26469;&#20272;&#35745;&#39640;&#26031;&#22270;&#27169;&#22411;&#20013;&#30340;&#65288;&#23545;&#35282;&#32447;&#21344;&#20248;&#30340;&#65289;M&#30697;&#38453;&#65292;&#21516;&#26102;&#22522;&#20110;&#26799;&#24230;&#25237;&#24433;&#27861;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#27491;&#21017;&#21270;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#31934;&#24230;&#30697;&#38453;&#20272;&#35745;&#21644;&#22270;&#36793;&#32536;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.15471</link><description>&lt;p&gt;
&#20840;&#27491;&#21322;&#23450;&#19979;&#22270;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Adaptive Estimation of Graphical Models under Total Positivity. (arXiv:2210.15471v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#22810;&#38454;&#27573;&#20272;&#35745;&#26041;&#27861;&#26469;&#20272;&#35745;&#39640;&#26031;&#22270;&#27169;&#22411;&#20013;&#30340;&#65288;&#23545;&#35282;&#32447;&#21344;&#20248;&#30340;&#65289;M&#30697;&#38453;&#65292;&#21516;&#26102;&#22522;&#20110;&#26799;&#24230;&#25237;&#24433;&#27861;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#27491;&#21017;&#21270;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#31934;&#24230;&#30697;&#38453;&#20272;&#35745;&#21644;&#22270;&#36793;&#32536;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#39640;&#26031;&#22270;&#27169;&#22411;&#20013;&#23558;&#65288;&#23545;&#35282;&#32447;&#21344;&#20248;&#30340;&#65289;M&#30697;&#38453;&#20316;&#20026;&#31934;&#24230;&#30697;&#38453;&#36827;&#34892;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#27169;&#22411;&#23637;&#29616;&#20102;&#26377;&#36259;&#30340;&#24615;&#36136;&#65292;&#20363;&#22914;&#20165;&#20351;&#29992;&#20004;&#20010;&#35266;&#27979;&#20540;&#23601;&#33021;&#33719;&#24471;M&#30697;&#38453;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745; \citep{lauritzen2019maximum,slawski2015estimation}&#65292;&#20197;&#21450;&#23545;&#35282;&#32447;&#21344;&#20248;&#30340;M&#30697;&#38453;&#20165;&#29992;&#19968;&#20010;&#35266;&#27979;&#20540;&#23601;&#33021;&#33719;&#24471;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745; \citep{truell2021maximum}&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#22810;&#38454;&#27573;&#20272;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#27599;&#20010;&#38454;&#27573;&#35299;&#20915;&#21152;&#26435;&#30340;$\ell_1$-&#27491;&#21017;&#38382;&#39064;&#26469;&#20248;&#21270;&#20272;&#35745;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;&#26799;&#24230;&#25237;&#24433;&#27861;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#27491;&#21017;&#21270;&#38382;&#39064;&#65292;&#32467;&#21512;&#19981;&#21516;&#30340;&#25237;&#24433;&#26469;&#22788;&#29702;M&#30697;&#38453;&#21644;&#23545;&#35282;&#32447;&#21344;&#20248;&#30340;M&#30697;&#38453;&#30340;&#32422;&#26463;&#12290;&#25552;&#20379;&#20102;&#23545;&#20272;&#35745;&#35823;&#24046;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31934;&#24230;&#30697;&#38453;&#20272;&#35745;&#21644;&#22270;&#36793;&#32536;&#35782;&#21035;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#36825;&#19968;&#28857;&#22312;&#21512;&#25104;&#21644;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#24471;&#21040;&#20102;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of estimating (diagonally dominant) M-matrices as precision matrices in Gaussian graphical models. These models exhibit intriguing properties, such as the existence of the maximum likelihood estimator with merely two observations for M-matrices \citep{lauritzen2019maximum,slawski2015estimation} and even one observation for diagonally dominant M-matrices \citep{truell2021maximum}. We propose an adaptive multiple-stage estimation method that refines the estimate by solving a weighted $\ell_1$-regularized problem at each stage. Furthermore, we develop a unified framework based on the gradient projection method to solve the regularized problem, incorporating distinct projections to handle the constraints of M-matrices and diagonally dominant M-matrices. A theoretical analysis of the estimation error is provided. Our method outperforms state-of-the-art methods in precision matrix estimation and graph edge identification, as evidenced by synthetic and financial time-s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20195;&#30721;&#25512;&#33616;&#31995;&#32479;GitHub Copilot&#65292;&#21457;&#29616;&#20102;&#31243;&#24207;&#21592;&#19982;&#35813;&#31995;&#32479;&#20132;&#20114;&#30340;&#19968;&#20123;&#24120;&#35265;&#27963;&#21160;&#65292;&#25581;&#31034;&#20102;&#20854;&#20302;&#25928;&#24615;&#21644;&#26102;&#38388;&#25104;&#26412;&#65292;&#20174;&#32780;&#20026;&#25913;&#36827;&#30028;&#38754;&#35774;&#35745;&#21644;&#24230;&#37327;&#26041;&#27861;&#25552;&#20379;&#20102;&#21160;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.14306</link><description>&lt;p&gt;
&#35835;&#25026;&#20195;&#30721;&#32972;&#21518;&#65306;&#27169;&#25311;AI&#36741;&#21161;&#32534;&#31243;&#20013;&#30340;&#29992;&#25143;&#34892;&#20026;&#21644;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
Reading Between the Lines: Modeling User Behavior and Costs in AI-Assisted Programming. (arXiv:2210.14306v3 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14306
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20195;&#30721;&#25512;&#33616;&#31995;&#32479;GitHub Copilot&#65292;&#21457;&#29616;&#20102;&#31243;&#24207;&#21592;&#19982;&#35813;&#31995;&#32479;&#20132;&#20114;&#30340;&#19968;&#20123;&#24120;&#35265;&#27963;&#21160;&#65292;&#25581;&#31034;&#20102;&#20854;&#20302;&#25928;&#24615;&#21644;&#26102;&#38388;&#25104;&#26412;&#65292;&#20174;&#32780;&#20026;&#25913;&#36827;&#30028;&#38754;&#35774;&#35745;&#21644;&#24230;&#37327;&#26041;&#27861;&#25552;&#20379;&#20102;&#21160;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#25512;&#33616;&#31995;&#32479;&#65292;&#22914;Copilot&#21644;CodeWhisperer&#65292;&#36890;&#36807;&#33258;&#21160;&#24314;&#35758;&#21644;&#33258;&#21160;&#23436;&#25104;&#20195;&#30721;&#65292;&#26377;&#28508;&#21147;&#25552;&#39640;&#31243;&#24207;&#21592;&#30340;&#29983;&#20135;&#29575;&#12290;&#28982;&#32780;&#65292;&#35201;&#20805;&#20998;&#21457;&#25381;&#23427;&#20204;&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#24517;&#39035;&#20102;&#35299;&#31243;&#24207;&#21592;&#22914;&#20309;&#19982;&#36825;&#20123;&#31995;&#32479;&#20132;&#20114;&#65292;&#24182;&#30830;&#23450;&#25913;&#36827;&#20132;&#20114;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#21462;&#24471;&#36827;&#23637;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27599;&#22825;&#30001;&#25968;&#30334;&#19975;&#31243;&#24207;&#21592;&#20351;&#29992;&#30340;&#20195;&#30721;&#25512;&#33616;&#31995;&#32479;GitHub Copilot&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#24120;&#35265;&#31243;&#24207;&#21592;&#27963;&#21160;&#30340;&#20998;&#31867;&#31995;&#32479;CUPS&#65292;&#20197;&#20415;&#27169;&#25311;&#29992;&#25143;&#19982;Copilot&#30340;&#20132;&#20114;&#12290;&#25105;&#20204;&#23545;21&#21517;&#23436;&#25104;&#32534;&#30721;&#20219;&#21153;&#24182;&#22238;&#39038;&#24615;&#22320;&#20351;&#29992;CUPS&#26631;&#35760;&#20854;&#20250;&#35805;&#30340;&#31243;&#24207;&#21592;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;CUPS&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#20102;&#35299;&#31243;&#24207;&#21592;&#22914;&#20309;&#19982;&#20195;&#30721;&#25512;&#33616;&#31995;&#32479;&#20132;&#20114;&#65292;&#25581;&#31034;&#20102;&#25928;&#29575;&#20302;&#19979;&#21644;&#26102;&#38388;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#27934;&#35265;&#25581;&#31034;&#20102;&#31243;&#24207;&#21592;&#22914;&#20309;&#19982;Copilot&#20132;&#20114;&#65292;&#24182;&#28608;&#21457;&#20102;&#26032;&#30340;&#30028;&#38754;&#35774;&#35745;&#21644;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code-recommendation systems, such as Copilot and CodeWhisperer, have the potential to improve programmer productivity by suggesting and auto-completing code. However, to fully realize their potential, we must understand how programmers interact with these systems and identify ways to improve that interaction. To make progress, we studied GitHub Copilot, a code-recommendation system used by millions of programmers daily. We developed CUPS, a taxonomy of common programmer activities when interacting with Copilot. Our study of 21 programmers, who completed coding tasks and retrospectively labeled their sessions with CUPS, showed that CUPS can help us understand how programmers interact with code-recommendation systems, revealing inefficiencies and time costs. Our insights reveal how programmers interact with Copilot and motivate new interface designs and metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#20316;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#26102;&#38388;&#36793;&#32536;&#30340;&#24555;&#29031;&#20013;&#23398;&#20064;&#21160;&#21147;&#23398;&#65292;&#26080;&#38656;&#23436;&#25972;&#26679;&#26412;&#36712;&#36857;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#19968;&#31181;&#34913;&#37327;&#21160;&#20316;&#24207;&#21015;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#27169;&#25311;&#25972;&#20010;&#20010;&#20307;&#36712;&#36857;&#65292;&#24182;&#22312;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#21644;&#37327;&#23376;&#30005;&#36335;&#21160;&#21147;&#23398;&#31995;&#32479;&#19978;&#24471;&#21040;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2210.06662</link><description>&lt;p&gt;
&#21160;&#20316;&#21305;&#37197;&#65306;&#20174;&#26679;&#26412;&#20013;&#23398;&#20064;&#38543;&#26426;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Action Matching: Learning Stochastic Dynamics from Samples. (arXiv:2210.06662v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#20316;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#26102;&#38388;&#36793;&#32536;&#30340;&#24555;&#29031;&#20013;&#23398;&#20064;&#21160;&#21147;&#23398;&#65292;&#26080;&#38656;&#23436;&#25972;&#26679;&#26412;&#36712;&#36857;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#19968;&#31181;&#34913;&#37327;&#21160;&#20316;&#24207;&#21015;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#27169;&#25311;&#25972;&#20010;&#20010;&#20307;&#36712;&#36857;&#65292;&#24182;&#22312;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#21644;&#37327;&#23376;&#30005;&#36335;&#21160;&#21147;&#23398;&#31995;&#32479;&#19978;&#24471;&#21040;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#26102;&#38388;&#36793;&#32536;&#30340;&#24555;&#29031;&#20013;&#23398;&#20064;&#31995;&#32479;&#30340;&#36830;&#32493;&#21160;&#21147;&#23398;&#26159;&#33258;&#28982;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#20986;&#29616;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#37327;&#23376;&#31995;&#32479;&#12289;&#21333;&#32454;&#32990;&#29983;&#29289;&#25968;&#25454;&#21644;&#29983;&#25104;&#24314;&#27169;&#12290;&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;&#21482;&#33021;&#35775;&#38382;&#22312;&#26102;&#38388;&#19978;&#19981;&#30456;&#20851;&#30340;&#27178;&#25130;&#38754;&#26679;&#26412;&#65292;&#32780;&#19981;&#26159;&#23436;&#25972;&#30340;&#26679;&#26412;&#36712;&#36857;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#35266;&#23519;&#30340;&#31995;&#32479;&#65292;&#25105;&#20204;&#24076;&#26395;&#23398;&#20064;&#19968;&#20010;&#27169;&#22411;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#26102;&#38388;&#19978;&#20256;&#25773;&#26679;&#26412;&#65292;&#20174;&#32780;&#27169;&#25311;&#25972;&#20010;&#20010;&#20307;&#36712;&#36857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#20316;&#21305;&#37197;&#26041;&#27861;&#65292;&#20351;&#29992;&#20165;&#26469;&#33258;&#20854;&#26102;&#38388;&#28436;&#21270;&#30340;&#29420;&#31435;&#26679;&#26412;&#26469;&#23398;&#20064;&#20016;&#23500;&#30340;&#21160;&#21147;&#23398;&#23478;&#26063;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#20010;&#23481;&#26131;&#22788;&#29702;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#23427;&#19981;&#20381;&#36182;&#20110;&#20851;&#20110;&#24213;&#23618;&#21160;&#21147;&#23398;&#30340;&#26174;&#24335;&#20551;&#35774;&#65292;&#20063;&#19981;&#38656;&#35201;&#36890;&#36807;&#24494;&#20998;&#26041;&#31243;&#25110;&#26368;&#20248;&#20256;&#36755;&#35299;&#31639;&#22120;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#12290;&#21463;&#21040;&#26368;&#20248;&#20256;&#36755;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#20064;&#19968;&#31181;&#25439;&#22833;&#20989;&#25968;&#65292;&#23427;&#34913;&#37327;&#19968;&#31995;&#21015;&#21160;&#20316;&#23558;&#31995;&#32479;&#20174;&#19968;&#20010;&#26102;&#38388;&#28857;&#39537;&#21160;&#21040;&#19979;&#19968;&#20010;&#26102;&#38388;&#28857;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#21160;&#21147;&#23398;&#31995;&#32479;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#65292;&#21253;&#25324;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#21644;&#37327;&#23376;&#30005;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning the continuous dynamics of a system from snapshots of its temporal marginals is a problem which appears throughout natural sciences and machine learning, including in quantum systems, single-cell biological data, and generative modeling. In these settings, we assume access to cross-sectional samples that are uncorrelated over time, rather than full trajectories of samples. In order to better understand the systems under observation, we would like to learn a model of the underlying process that allows us to propagate samples in time and thereby simulate entire individual trajectories. In this work, we propose Action Matching, a method for learning a rich family of dynamics using only independent samples from its time evolution. We derive a tractable training objective, which does not rely on explicit assumptions about the underlying dynamics and does not require back-propagation through differential equations or optimal transport solvers. Inspired by connections with optimal tr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#33976;&#39311;&#20316;&#20026;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;&#38454;&#27573;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;Vision Transformer&#22312;&#30446;&#26631;&#26410;&#26631;&#27880;&#25968;&#25454;&#38598;&#19978;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#39033;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.02871</link><description>&lt;p&gt;
&#33258;&#33976;&#39311;&#22312;Transformer&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Self-Distillation for Further Pre-training of Transformers. (arXiv:2210.02871v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#33976;&#39311;&#20316;&#20026;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;&#38454;&#27573;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;Vision Transformer&#22312;&#30446;&#26631;&#26410;&#26631;&#27880;&#25968;&#25454;&#38598;&#19978;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#39033;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#37327;&#26080;&#26631;&#27880;&#25968;&#25454;&#19978;&#23545;Transformer&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#24182;&#22312;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#25104;&#21151;&#30340;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20043;&#38388;&#23384;&#22312;&#22823;&#30340;&#25968;&#25454;&#39046;&#22495;&#19978;&#30340;&#24046;&#24322;&#65292;&#21017;&#30452;&#25509;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#33021;&#26159;&#27425;&#20248;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21069;&#20154;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#21363;&#22312;&#30446;&#26631;&#26410;&#26631;&#27880;&#25968;&#25454;&#38598;&#19978;&#32487;&#32493;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20294;&#25152;&#26377;&#36825;&#20123;&#26041;&#27861;&#37117;&#20165;&#20851;&#27880;&#20110;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#23545;&#30446;&#26631;&#26410;&#26631;&#27880;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#26102;&#65292;Vision Transformer&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#33976;&#39311;&#20316;&#20026;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;&#38454;&#27573;&#30340;&#27491;&#21017;&#21270;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#30446;&#26631;&#26410;&#26631;&#27880;&#25968;&#25454;&#38598;&#19978;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;&#21021;&#22987;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#28982;&#21518;&#23558;&#20174;&#36827;&#19968;&#27493;&#35757;&#32451;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#21021;&#22987;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#12290;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#35270;&#35273;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#33258;&#33976;&#39311;&#26041;&#27861;&#22987;&#32456;&#25552;&#39640;&#20102;&#21021;&#22987;&#39044;&#35757;&#32451;Vision Transformer&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#20808;&#21069;&#30340;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training a large transformer model on a massive amount of unlabeled data and fine-tuning it on labeled datasets for diverse downstream tasks has proven to be a successful strategy, for a variety of vision and natural language processing tasks. However, direct fine-tuning of the pre-trained model may be suboptimal if there exist large discrepancies across data domains for pre-training and fine-tuning. To tackle this issue, several previous studies have proposed further pre-training strategies, where we continue to pre-train the model on the target unlabeled dataset before fine-tuning. However, all of them solely focus on language models and we empirically find that a Vision Transformer is vulnerable to overfitting as we continue to pretrain the model on target unlabeled data. In order to tackle this limitation, we propose self-distillation as a regularization for a further pre-training stage. Specifically, we first further pre-train the initial pre-trained model on the target unlabe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#30340;&#21160;&#24577;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23558;&#26435;&#37325;&#19982;&#36755;&#20837;&#26679;&#26412;&#36827;&#34892;&#26465;&#20214;&#65292;&#23398;&#20064;&#21305;&#37197;&#24494;&#35843;&#30340;&#22522;&#30784;&#27169;&#22411;&#22312;&#36755;&#20837;&#21644;&#26631;&#31614;&#19978;&#33719;&#24471;&#30340;&#26435;&#37325;&#65292;&#21487;&#20197;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;3D&#37325;&#24314;&#12289;&#34920;&#26684;&#25968;&#25454;&#12289;&#35821;&#38899;&#20998;&#31163;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2210.00471</link><description>&lt;p&gt;
OCD&#65306;&#20351;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#36807;&#24230;&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
OCD: Learning to Overfit with Conditional Diffusion Models. (arXiv:2210.00471v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#30340;&#21160;&#24577;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23558;&#26435;&#37325;&#19982;&#36755;&#20837;&#26679;&#26412;&#36827;&#34892;&#26465;&#20214;&#65292;&#23398;&#20064;&#21305;&#37197;&#24494;&#35843;&#30340;&#22522;&#30784;&#27169;&#22411;&#22312;&#36755;&#20837;&#21644;&#26631;&#31614;&#19978;&#33719;&#24471;&#30340;&#26435;&#37325;&#65292;&#21487;&#20197;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;3D&#37325;&#24314;&#12289;&#34920;&#26684;&#25968;&#25454;&#12289;&#35821;&#38899;&#20998;&#31163;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#24577;&#27169;&#22411;&#65292;&#20854;&#20013;&#26435;&#37325;&#22522;&#20110;&#36755;&#20837;&#26679;&#26412;x&#36827;&#34892;&#26465;&#20214;&#65292;&#23398;&#20064;&#21305;&#37197;&#36890;&#36807;&#23545;x&#21450;&#20854;&#26631;&#31614;y&#36827;&#34892;&#24494;&#35843;&#25152;&#33719;&#24471;&#30340;&#26435;&#37325;&#12290;&#35813;&#36755;&#20837;&#26679;&#26412;&#19982;&#32593;&#32476;&#26435;&#37325;&#20043;&#38388;&#30340;&#26144;&#23556;&#36890;&#36807;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#36817;&#20284;&#12290;&#25105;&#20204;&#20351;&#29992;&#30340;&#25193;&#25955;&#27169;&#22411;&#19987;&#27880;&#20110;&#20462;&#25913;&#22522;&#30784;&#27169;&#22411;&#30340;&#19968;&#20010;&#21333;&#23618;&#65292;&#24182;&#22522;&#20110;&#35813;&#23618;&#30340;&#36755;&#20837;&#12289;&#28608;&#27963;&#20197;&#21450;&#36755;&#20986;&#36827;&#34892;&#26465;&#20214;&#12290;&#30001;&#20110;&#25193;&#25955;&#27169;&#22411;&#20855;&#26377;&#38543;&#26426;&#24615;&#36136;&#65292;&#22810;&#27425;&#21021;&#22987;&#21270;&#20250;&#29983;&#25104;&#19981;&#21516;&#30340;&#32593;&#32476;&#65292;&#24418;&#25104;&#19968;&#20010;&#38598;&#21512;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#19977;&#32500;&#37325;&#24314;&#12289;&#34920;&#26684;&#25968;&#25454;&#12289;&#35821;&#38899;&#20998;&#31163;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#20301;&#20110; https://github.com/ShaharLutatiPersonal/OCD&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a dynamic model in which the weights are conditioned on an input sample x and are learned to match those that would be obtained by finetuning a base model on x and its label y. This mapping between an input sample and network weights is approximated by a denoising diffusion model. The diffusion model we employ focuses on modifying a single layer of the base model and is conditioned on the input, activations, and output of this layer. Since the diffusion model is stochastic in nature, multiple initializations generate different networks, forming an ensemble, which leads to further improvements. Our experiments demonstrate the wide applicability of the method for image classification, 3D reconstruction, tabular data, speech separation, and natural language processing. Our code is available at https://github.com/ShaharLutatiPersonal/OCD
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#26500;&#29615;&#22659;&#19979;&#20998;&#24067;&#24335;&#32858;&#31867;&#23398;&#20064;&#30340;&#36890;&#20449;&#25928;&#29575;&#39640;&#30340;&#19968;&#27425;&#24615;&#26041;&#27861;&#26063;&#65292;&#36890;&#36807;&#23616;&#37096;&#35745;&#31639;&#21644;&#32858;&#31867;&#32858;&#21512;&#27493;&#39588;&#65292;&#22312;&#27599;&#20010;&#29992;&#25143;&#22788;&#23398;&#20064;&#20986;&#30495;&#23454;&#27169;&#22411;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#23398;&#20064;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2209.10866</link><description>&lt;p&gt;
&#24322;&#26500;&#29615;&#22659;&#19979;&#20998;&#24067;&#24335;&#32858;&#31867;&#23398;&#20064;&#30340;&#19968;&#27425;&#24615;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A One-shot Framework for Distributed Clustered Learning in Heterogeneous Environments. (arXiv:2209.10866v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#26500;&#29615;&#22659;&#19979;&#20998;&#24067;&#24335;&#32858;&#31867;&#23398;&#20064;&#30340;&#36890;&#20449;&#25928;&#29575;&#39640;&#30340;&#19968;&#27425;&#24615;&#26041;&#27861;&#26063;&#65292;&#36890;&#36807;&#23616;&#37096;&#35745;&#31639;&#21644;&#32858;&#31867;&#32858;&#21512;&#27493;&#39588;&#65292;&#22312;&#27599;&#20010;&#29992;&#25143;&#22788;&#23398;&#20064;&#20986;&#30495;&#23454;&#27169;&#22411;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#23398;&#20064;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#20449;&#25928;&#29575;&#39640;&#30340;&#26041;&#27861;&#26063;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#29992;&#25143;&#20174;$K$&#20010;&#19981;&#21516;&#20998;&#24067;&#20013;&#33719;&#21462;&#25968;&#25454;&#30340;&#24322;&#26500;&#29615;&#22659;&#20013;&#36827;&#34892;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#35774;&#32622;&#20013;&#65292;&#29992;&#25143;&#30340;&#20998;&#32452;&#65288;&#22522;&#20110;&#20182;&#20204;&#37319;&#26679;&#30340;&#25968;&#25454;&#20998;&#24067;&#65289;&#20197;&#21450;&#20998;&#24067;&#30340;&#32479;&#35745;&#23646;&#24615;&#26159;&#20808;&#39564;&#26410;&#30693;&#30340;&#12290;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#22522;&#20110;&#38598;&#21512;&#21487;&#25509;&#21463;&#30340;&#32858;&#31867;&#31639;&#27861;$\mathcal{C}$&#21442;&#25968;&#21270;&#30340;&#19968;&#27425;&#24615;&#20998;&#24067;&#24335;&#32858;&#31867;&#23398;&#20064;&#26041;&#27861;&#65288;ODCL-$\mathcal{C}$&#65289;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#27599;&#20010;&#29992;&#25143;&#22788;&#23398;&#20064;&#30495;&#23454;&#27169;&#22411;&#12290;&#21487;&#25509;&#21463;&#30340;&#32858;&#31867;&#26041;&#27861;&#21253;&#25324;$K$&#22343;&#20540;&#65288;KM&#65289;&#21644;&#20984;&#32858;&#31867;&#65288;CC&#65289;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#25152;&#25552;&#20986;&#30340;&#21508;&#31181;&#19968;&#27425;&#24615;&#26041;&#27861;&#65292;&#22914;ODCL-KM&#21644;ODCL-CC&#12290;&#25152;&#25552;&#20986;&#30340;&#19968;&#27425;&#24615;&#26041;&#27861;&#65292;&#22522;&#20110;&#29992;&#25143;&#30340;&#26412;&#22320;&#35745;&#31639;&#21644;&#26381;&#21153;&#22120;&#19978;&#22522;&#20110;&#32858;&#31867;&#30340;&#32858;&#21512;&#27493;&#39588;&#65292;&#34987;&#35777;&#26126;&#33021;&#22815;&#25552;&#20379;&#24378;&#22823;&#30340;&#23398;&#20064;&#20445;&#35777;&#12290;&#29305;&#21035;&#26159;&#65292;&#23545;&#20110;&#24378;&#20984;&#38382;&#39064;&#65292;
&lt;/p&gt;
&lt;p&gt;
The paper proposes a family of communication efficient methods for distributed learning in heterogeneous environments in which users obtain data from one of $K$ different distributions. In the proposed setup, the grouping of users (based on the data distributions they sample), as well as the underlying statistical properties of the distributions, are apriori unknown. A family of One-shot Distributed Clustered Learning methods (ODCL-$\mathcal{C}$) is proposed, parametrized by the set of admissible clustering algorithms $\mathcal{C}$, with the objective of learning the true model at each user. The admissible clustering methods include $K$-means (KM) and convex clustering (CC), giving rise to various one-shot methods within the proposed family, such as ODCL-KM and ODCL-CC. The proposed one-shot approach, based on local computations at the users and a clustering based aggregation step at the server is shown to provide strong learning guarantees. In particular, for strongly convex problems 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#35299;&#32544;&#35270;&#35282;&#22788;&#29702;&#35270;&#39057;&#39046;&#22495;&#26080;&#30417;&#30563;&#33258;&#36866;&#24212;&#38382;&#39064;&#65292;&#36890;&#36807;&#36880;&#27493;&#35299;&#32544;&#38745;&#24577;&#21644;&#21160;&#24577;&#20449;&#24687;&#24182;&#20351;&#29992;&#22810;&#31181;&#32422;&#26463;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#31227;&#38500;&#31354;&#38388;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#21644;&#20943;&#23569;&#26102;&#38388;&#39046;&#22495;&#24046;&#24322;&#65292;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.07365</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#35270;&#39057;&#39046;&#22495;&#33258;&#36866;&#24212;&#21160;&#20316;&#35782;&#21035;&#65306;&#19968;&#20010;&#35299;&#32544;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Video Domain Adaptation for Action Recognition: A Disentanglement Perspective. (arXiv:2208.07365v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#35299;&#32544;&#35270;&#35282;&#22788;&#29702;&#35270;&#39057;&#39046;&#22495;&#26080;&#30417;&#30563;&#33258;&#36866;&#24212;&#38382;&#39064;&#65292;&#36890;&#36807;&#36880;&#27493;&#35299;&#32544;&#38745;&#24577;&#21644;&#21160;&#24577;&#20449;&#24687;&#24182;&#20351;&#29992;&#22810;&#31181;&#32422;&#26463;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#31227;&#38500;&#31354;&#38388;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#21644;&#20943;&#23569;&#26102;&#38388;&#39046;&#22495;&#24046;&#24322;&#65292;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#35270;&#39057;&#39046;&#22495;&#33258;&#36866;&#24212;&#26159;&#19968;&#39033;&#23454;&#36341;&#24615;&#32780;&#21448;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#39318;&#27425;&#20174;&#35299;&#32544;&#35270;&#35282;&#20837;&#25163;&#22788;&#29702;&#35813;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#24605;&#36335;&#26159;&#36890;&#36807;&#35299;&#32544;&#26469;&#20998;&#21035;&#22788;&#29702;&#31354;&#38388;&#21644;&#26102;&#38388;&#39046;&#22495;&#30340;&#24046;&#24322;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20174;&#21253;&#21547;&#38745;&#24577;&#20449;&#24687;&#30340;&#19968;&#32452;&#28508;&#22312;&#22240;&#32032;&#21644;&#21253;&#21547;&#21160;&#24577;&#20449;&#24687;&#30340;&#21478;&#19968;&#32452;&#28508;&#22312;&#22240;&#32032;&#20013;&#29983;&#25104;&#36328;&#39046;&#22495;&#35270;&#39057;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36716;&#31227;&#26102;&#24207;VAE&#65288;TranSVAE&#65289;&#26694;&#26550;&#26469;&#24314;&#27169;&#36825;&#31181;&#29983;&#25104;&#36807;&#31243;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#36827;&#34892;&#33258;&#36866;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#32422;&#26463;&#28508;&#22312;&#22240;&#32032;&#30340;&#30446;&#26631;&#12290;&#36890;&#36807;&#36825;&#20123;&#32422;&#26463;&#65292;&#38745;&#24577;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#30340;&#35299;&#32544;&#21487;&#20197;&#36731;&#26494;&#31227;&#38500;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#23398;&#20064;&#20174;&#24103;&#21644;&#35270;&#39057;&#23618;&#38754;&#36827;&#19968;&#27493;&#20943;&#23569;&#20102;&#26102;&#38388;&#24046;&#24322;&#12290;&#22312;UCF-HMDB&#12289;Jester&#21644;Epic-Kitchens&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#39564;&#35777;&#20102;TranSVAE&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised video domain adaptation is a practical yet challenging task. In this work, for the first time, we tackle it from a disentanglement view. Our key idea is to handle the spatial and temporal domain divergence separately through disentanglement. Specifically, we consider the generation of cross-domain videos from two sets of latent factors, one encoding the static information and another encoding the dynamic information. A Transfer Sequential VAE (TranSVAE) framework is then developed to model such generation. To better serve for adaptation, we propose several objectives to constrain the latent factors. With these constraints, the spatial divergence can be readily removed by disentangling the static domain-specific information out, and the temporal divergence is further reduced from both frame- and video-levels through adversarial learning. Extensive experiments on the UCF-HMDB, Jester, and Epic-Kitchens datasets verify the effectiveness and superiority of TranSVAE compared wi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#25935;&#24863;&#23646;&#24615;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#21644;&#25511;&#21046;&#24179;&#31561;&#26426;&#20250;&#30340;&#36829;&#35268;&#65292;&#20854;&#36890;&#36807;&#19968;&#31181;&#21518;&#22788;&#29702;&#26657;&#27491;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#20855;&#26377;&#20445;&#35777;&#30340;EOD&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2207.12497</link><description>&lt;p&gt;
&#36890;&#36807;&#25935;&#24863;&#23646;&#24615;&#39044;&#27979;&#22120;&#20272;&#35745;&#21644;&#25511;&#21046;&#24179;&#31561;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
Estimating and Controlling for Equalized Odds via Sensitive Attribute Predictors. (arXiv:2207.12497v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.12497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#25935;&#24863;&#23646;&#24615;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#21644;&#25511;&#21046;&#24179;&#31561;&#26426;&#20250;&#30340;&#36829;&#35268;&#65292;&#20854;&#36890;&#36807;&#19968;&#31181;&#21518;&#22788;&#29702;&#26657;&#27491;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#20855;&#26377;&#20445;&#35777;&#30340;EOD&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29616;&#23454;&#20013;&#39640;&#39118;&#38505;&#20915;&#31574;&#22330;&#26223;&#20013;&#30340;&#20351;&#29992;&#19981;&#26029;&#22686;&#38271;&#65292;&#25105;&#20204;&#33021;&#22815;&#23457;&#35745;&#21644;&#25511;&#21046;&#36825;&#20123;&#27169;&#22411;&#21521;&#26576;&#20123;&#32676;&#20307;&#34920;&#29616;&#20986;&#30340;&#20219;&#20309;&#28508;&#22312;&#20844;&#24179;&#24615;&#36829;&#35268;&#38750;&#24120;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#33258;&#28982;&#38656;&#35201;&#35775;&#38382;&#25935;&#24863;&#23646;&#24615;&#65292;&#20363;&#22914;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#12289;&#24615;&#21035;&#25110;&#20854;&#20182;&#21487;&#33021;&#20915;&#23450;&#32452;&#25104;&#21592;&#36523;&#20221;&#30340;&#25935;&#24863;&#29305;&#24449;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#20449;&#24687;&#36890;&#24120;&#19981;&#21487;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20844;&#24179;&#24615;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#8220;&#24179;&#31561;&#26426;&#20250;&#8221;&#65288;EOD&#65289;&#23450;&#20041;&#12290;&#22312;&#27809;&#26377;&#25935;&#24863;&#23646;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#39318;&#20808;&#20026;&#39044;&#27979;&#22120;&#30340;EOD&#36829;&#35268;&#25552;&#20379;&#20102;&#32039;&#23494;&#19988;&#21487;&#35745;&#31639;&#30340;&#19978;&#30028;&#12290;&#36825;&#20123;&#36793;&#30028;&#27491;&#22909;&#21453;&#26144;&#20102;&#26368;&#22351;&#30340;EOD&#36829;&#35268;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#29992;&#19968;&#31181;&#26032;&#30340;&#21518;&#22788;&#29702;&#26657;&#27491;&#26041;&#27861;&#26377;&#20445;&#35777;&#22320;&#25511;&#21046;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;EOD&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#20102;&#30452;&#25509;&#38024;&#23545;&#39044;&#27979;&#30340;&#25935;&#24863;&#23646;&#24615;&#25511;&#21046;EOD&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the use of machine learning models in real world high-stakes decision settings continues to grow, it is highly important that we are able to audit and control for any potential fairness violations these models may exhibit towards certain groups. To do so, one naturally requires access to sensitive attributes, such as demographics, gender, or other potentially sensitive features that determine group membership. Unfortunately, in many settings, this information is often unavailable. In this work we study the well known \emph{equalized odds} (EOD) definition of fairness. In a setting without sensitive attributes, we first provide tight and computable upper bounds for the EOD violation of a predictor. These bounds precisely reflect the worst possible EOD violation. Second, we demonstrate how one can provably control the worst-case EOD by a new post-processing correction method. Our results characterize when directly controlling for EOD with respect to the predicted sensitive attributes 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#35745;&#31639;&#22270;&#36716;&#25442;&#22120;&#65288;CGT&#65289;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#36890;&#36807;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#26041;&#24335;&#23398;&#20064;&#21644;&#22797;&#21046;&#30495;&#23454;&#19990;&#30028;&#22270;&#30340;&#20998;&#24067;&#65292;&#20174;&#32780;&#29983;&#25104;&#26377;&#20195;&#34920;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#22270;&#65292;&#35753;GNNs&#22312;&#20854;&#19978;&#23637;&#31034;&#19982;&#28304;&#22270;&#30456;&#20284;&#30340;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2207.04396</link><description>&lt;p&gt;
&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Graph Generative Model for Benchmarking Graph Neural Networks. (arXiv:2207.04396v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.04396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#35745;&#31639;&#22270;&#36716;&#25442;&#22120;&#65288;CGT&#65289;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#36890;&#36807;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#26041;&#24335;&#23398;&#20064;&#21644;&#22797;&#21046;&#30495;&#23454;&#19990;&#30028;&#22270;&#30340;&#20998;&#24067;&#65292;&#20174;&#32780;&#29983;&#25104;&#26377;&#20195;&#34920;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#22270;&#65292;&#35753;GNNs&#22312;&#20854;&#19978;&#23637;&#31034;&#19982;&#28304;&#22270;&#30456;&#20284;&#30340;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#39046;&#22495;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#23545;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#26032;&#30340;GNN&#27169;&#22411;&#20197;&#35299;&#20915;&#23454;&#38469;&#38382;&#39064;&#65292;&#38656;&#35201;&#20351;&#29992;&#22823;&#37327;&#30340;&#30495;&#23454;&#19990;&#30028;&#22270;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#26469;&#33258;&#20110;&#22312;&#32447;&#12289;&#39640;&#24230;&#38480;&#21046;&#38544;&#31169;&#30340;&#29983;&#24577;&#31995;&#32479;&#65292;&#36825;&#20351;&#24471;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30740;&#31350;&#21644;&#24320;&#21457;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#35745;&#31639;&#22270;&#36716;&#25442;&#22120;&#65288;CGT&#65289;&#65292;&#35813;&#27169;&#22411;&#21487;&#36890;&#36807;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#26041;&#24335;&#23398;&#20064;&#21644;&#22797;&#21046;&#30495;&#23454;&#19990;&#30028;&#22270;&#30340;&#20998;&#24067;&#65292;&#20174;&#32780;&#29983;&#25104;&#26377;&#20195;&#34920;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#22270;&#65292;&#35753;GNNs&#22312;&#20854;&#19978;&#23637;&#31034;&#19982;&#28304;&#22270;&#30456;&#20284;&#30340;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the field of Graph Neural Networks (GNN) continues to grow, it experiences a corresponding increase in the need for large, real-world datasets to train and test new GNN models on challenging, realistic problems. Unfortunately, such graph datasets are often generated from online, highly privacy-restricted ecosystems, which makes research and development on these datasets hard, if not impossible. This greatly reduces the amount of benchmark graphs available to researchers, causing the field to rely only on a handful of publicly-available datasets. To address this problem, we introduce a novel graph generative model, Computation Graph Transformer (CGT) that learns and reproduces the distribution of real-world graphs in a privacy-controlled way. More specifically, CGT (1) generates effective benchmark graphs on which GNNs show similar task performance as on the source graphs, (2) scales to process large-scale graphs, (3) incorporates off-the-shelf privacy modules to guarantee end-user p
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#19968;&#31181;&#36890;&#29992;&#25216;&#24039;&#8212;&#8212;&#36890;&#36807;&#31227;&#38500;&#32593;&#32476;&#23618;&#26469;&#25913;&#21892;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#36825;&#31181;&#25216;&#24039;&#33021;&#22815;&#22863;&#25928;&#12290;</title><link>http://arxiv.org/abs/2206.13378</link><description>&lt;p&gt;
&#26029;&#22836;&#21488;&#27491;&#21017;&#21270;&#65306;&#20026;&#20160;&#20040;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#38656;&#35201;&#31227;&#38500;&#32593;&#32476;&#23618;&#20197;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Guillotine Regularization: Why removing layers is needed to improve generalization in Self-Supervised Learning. (arXiv:2206.13378v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.13378
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#19968;&#31181;&#36890;&#29992;&#25216;&#24039;&#8212;&#8212;&#36890;&#36807;&#31227;&#38500;&#32593;&#32476;&#23618;&#26469;&#25913;&#21892;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#36825;&#31181;&#25216;&#24039;&#33021;&#22815;&#22863;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#20986;&#29616;&#20102;&#19968;&#31181;&#20986;&#20154;&#24847;&#26009;&#30340;&#25216;&#26415;&#65292;&#21363;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DN&#65289;&#65292;&#24182;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20351;&#29992;&#36825;&#20010;&#32593;&#32476;&#65292;&#20294;&#23558;&#20854;&#26368;&#21518;&#20960;&#20010;&#25237;&#24433;&#23618;&#23436;&#20840;&#21435;&#38500;&#12290;&#23545;&#20110;&#22312;ImageNet&#19978;&#23637;&#29616;&#31454;&#20105;&#24615;&#33021;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36825;&#20010;&#21435;&#38500;&#25237;&#24433;&#22120;&#30340;&#25216;&#24039;&#23454;&#38469;&#19978;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#36825;&#26679;&#21487;&#20197;&#33719;&#24471;&#36229;&#36807;30&#20010;&#30334;&#20998;&#28857;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#25216;&#24039;&#20284;&#20046;&#19982;&#22312;&#33258;&#30417;&#30563;&#35757;&#32451;&#20013;&#26174;&#24335;&#24378;&#21046;&#25191;&#34892;&#19981;&#21464;&#24615;&#30340;&#26368;&#21518;&#19968;&#20010;&#25237;&#24433;&#23618;&#19981;&#31526;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;Guillotine Regularization&#65288;GR&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#24050;&#29992;&#20110;&#25913;&#21892;&#36716;&#31227;&#23398;&#20064;&#24773;&#20917;&#19979;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20854;&#25104;&#21151;&#32972;&#21518;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
One unexpected technique that emerged in recent years consists in training a Deep Network (DN) with a Self-Supervised Learning (SSL) method, and using this network on downstream tasks but with its last few projector layers entirely removed. This trick of throwing away the projector is actually critical for SSL methods to display competitive performances on ImageNet for which more than 30 percentage points can be gained that way. This is a little vexing, as one would hope that the network layer at which invariance is explicitly enforced by the SSL criterion during training (the last projector layer) should be the one to use for best generalization performance downstream. But it seems not to be, and this study sheds some light on why. This trick, which we name Guillotine Regularization (GR), is in fact a generically applicable method that has been used to improve generalization performance in transfer learning scenarios. In this work, we identify the underlying reasons behind its success
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MetaGL&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20813;&#35780;&#20272;&#30340;&#24773;&#20917;&#19979;&#36873;&#25321;&#26368;&#20339;&#30340;&#22270;&#23398;&#20064;&#27169;&#22411;&#21644;&#36229;&#21442;&#25968;&#65292;&#24182;&#19988;&#36890;&#36807;&#24341;&#20837;&#20803;&#22270;&#29305;&#24449;&#26469;&#39044;&#27979;&#21508;&#31181;&#22270;&#23398;&#20064;&#27169;&#22411;&#22312;&#26032;&#22270;&#19978;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;&#20973;&#32463;&#39564;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2206.09280</link><description>&lt;p&gt;
MetaGL: &#21033;&#29992;&#20803;&#23398;&#20064;&#23545;&#22270;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20813;&#35780;&#20272;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
MetaGL: Evaluation-Free Selection of Graph Learning Models via Meta-Learning. (arXiv:2206.09280v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MetaGL&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20813;&#35780;&#20272;&#30340;&#24773;&#20917;&#19979;&#36873;&#25321;&#26368;&#20339;&#30340;&#22270;&#23398;&#20064;&#27169;&#22411;&#21644;&#36229;&#21442;&#25968;&#65292;&#24182;&#19988;&#36890;&#36807;&#24341;&#20837;&#20803;&#22270;&#29305;&#24449;&#26469;&#39044;&#27979;&#21508;&#31181;&#22270;&#23398;&#20064;&#27169;&#22411;&#22312;&#26032;&#22270;&#19978;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;&#20973;&#32463;&#39564;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#20010;&#22270;&#23398;&#20064;&#20219;&#21153;&#65292;&#20363;&#22914;&#22312;&#26032;&#22270;&#20013;&#30340;&#38142;&#25509;&#39044;&#27979;&#65292;&#22914;&#20309;&#22312;&#19981;&#35757;&#32451;&#25110;&#35780;&#20272;&#26032;&#22270;&#19978;&#30340;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#36873;&#25321;&#26368;&#20339;&#26041;&#27861;&#20197;&#21450;&#20854;&#36229;&#21442;&#25968;&#65288;&#32479;&#31216;&#20026;&#27169;&#22411;&#65289;?&#22270;&#23398;&#20064;&#27169;&#22411;&#30340;&#36873;&#25321;&#22823;&#22810;&#26159;&#20973;&#32463;&#39564;&#30340;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#23558;&#27969;&#34892;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#26032;&#25968;&#25454;&#38598;&#65292;&#20294;&#36825;&#36890;&#24120;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;&#26032;&#22270;&#19978;&#31995;&#32479;&#22320;&#27604;&#36739;&#27169;&#22411;&#24456;&#24555;&#23601;&#21464;&#24471;&#36807;&#20110;&#26114;&#36149;&#65292;&#29978;&#33267;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;MetaGL&#65292;&#29992;&#20110;&#20813;&#35780;&#20272;&#30340;&#22270;&#23398;&#20064;&#27169;&#22411;&#36873;&#25321;&#65292;&#21033;&#29992;&#29616;&#26377;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#22270;&#25968;&#25454;&#38598;&#19978;&#30340;&#20808;&#21069;&#34920;&#29616;&#65292;&#33258;&#21160;&#36873;&#25321;&#36866;&#29992;&#20110;&#26032;&#22270;&#30340;&#26377;&#25928;&#27169;&#22411;&#65292;&#26080;&#38656;&#23545;&#26032;&#22270;&#19978;&#30340;&#20219;&#20309;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#25110;&#35780;&#20272;&#12290;&#20026;&#20102;&#37327;&#21270;&#21508;&#31181;&#31867;&#22411;&#22270;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19987;&#38376;&#30340;&#20803;&#22270;&#29305;&#24449;&#65292;&#25429;&#25417;&#22270;&#30340;&#32467;&#26500;&#29305;&#24449;&#12290;&#28982;&#21518;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#36825;&#20123;&#20803;&#22270;&#29305;&#24449;&#39044;&#27979;&#21508;&#31181;&#22270;&#23398;&#20064;&#27169;&#22411;&#22312;&#26032;&#22270;&#19978;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#22312;&#26032;&#22270;&#19978;&#35757;&#32451;&#25110;&#35780;&#20272;&#20219;&#20309;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;MetaGL&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#22823;&#22823;&#20248;&#20110;&#20973;&#32463;&#39564;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a graph learning task, such as link prediction, on a new graph, how can we select the best method as well as its hyperparameters (collectively called a model) without having to train or evaluate any model on the new graph? Model selection for graph learning has been largely ad hoc. A typical approach has been to apply popular methods to new datasets, but this is often suboptimal. On the other hand, systematically comparing models on the new graph quickly becomes too costly, or even impractical. In this work, we develop the first meta-learning approach for evaluation-free graph learning model selection, called MetaGL, which utilizes the prior performances of existing methods on various benchmark graph datasets to automatically select an effective model for the new graph, without any model training or evaluations. To quantify similarities across a wide variety of graphs, we introduce specialized meta-graph features that capture the structural characteristics of a graph. Then we des
&lt;/p&gt;</description></item><item><title>BridgeTower&#25552;&#20986;&#20102;&#22810;&#20010;&#26725;&#25509;&#23618;&#24314;&#31435;&#20102;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;&#39030;&#23618;&#19982;&#36328;&#27169;&#32534;&#30721;&#22120;&#30340;&#36830;&#25509;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#33258;&#19979;&#32780;&#19978;&#36328;&#27169;&#24577;&#23545;&#40784;&#21644;&#34701;&#21512;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.08657</link><description>&lt;p&gt;
BridgeTower&#65306;&#22312;&#35270;&#35273;&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#20013;&#24314;&#31435;&#32534;&#30721;&#22120;&#20043;&#38388;&#30340;&#26725;&#26753;
&lt;/p&gt;
&lt;p&gt;
BridgeTower: Building Bridges Between Encoders in Vision-Language Representation Learning. (arXiv:2206.08657v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08657
&lt;/p&gt;
&lt;p&gt;
BridgeTower&#25552;&#20986;&#20102;&#22810;&#20010;&#26725;&#25509;&#23618;&#24314;&#31435;&#20102;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;&#39030;&#23618;&#19982;&#36328;&#27169;&#32534;&#30721;&#22120;&#30340;&#36830;&#25509;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#33258;&#19979;&#32780;&#19978;&#36328;&#27169;&#24577;&#23545;&#40784;&#21644;&#34701;&#21512;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#25317;&#26377;&#21452;&#22612;&#26550;&#26500;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24041;&#22266;&#20102;&#35270;&#35273;&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#30340;&#22320;&#20301;&#12290;&#24403;&#21069;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#35201;&#20040;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#65292;&#23398;&#20064;&#20174;&#28145;&#24230;&#36328;&#27169;&#32534;&#30721;&#22120;&#20013;&#25552;&#21462;&#12289;&#23545;&#40784;&#21644;&#34701;&#21512;&#20004;&#31181;&#27169;&#24577;&#30340;&#34920;&#31034;&#65292;&#35201;&#20040;&#23558;&#28145;&#24230;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;&#26368;&#21518;&#19968;&#23618;&#21333;&#27169;&#24577;&#34920;&#31034;&#39304;&#36865;&#21040;&#39030;&#37096;&#36328;&#27169;&#32534;&#30721;&#22120;&#20013;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#21487;&#33021;&#38480;&#21046;&#35270;&#35273;&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#24182;&#38480;&#21046;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;BridgeTower&#65292;&#23427;&#24341;&#20837;&#20102;&#22810;&#20010;&#26725;&#25509;&#23618;&#65292;&#24314;&#31435;&#20102;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;&#39030;&#23618;&#19982;&#36328;&#27169;&#32534;&#30721;&#22120;&#30340;&#27599;&#19968;&#23618;&#20043;&#38388;&#30340;&#36830;&#25509;&#12290;&#36825;&#20351;&#24471;&#26469;&#33258;&#39044;&#35757;&#32451;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;&#19981;&#21516;&#35821;&#20041;&#32423;&#21035;&#30340;&#35270;&#35273;&#21644;&#25991;&#26412;&#34920;&#31034;&#22312;&#36328;&#27169;&#32534;&#30721;&#22120;&#20013;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#33258;&#19979;&#32780;&#19978;&#36328;&#27169;&#24577;&#23545;&#40784;&#21644;&#34701;&#21512;&#12290;&#36890;&#36807;&#20165;&#20351;&#29992;400&#19975;&#24352;&#22270;&#20687;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;BridgeTower&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-Language (VL) models with the Two-Tower architecture have dominated visual-language representation learning in recent years. Current VL models either use lightweight uni-modal encoders and learn to extract, align and fuse both modalities simultaneously in a deep cross-modal encoder, or feed the last-layer uni-modal representations from the deep pre-trained uni-modal encoders into the top cross-modal encoder. Both approaches potentially restrict vision-language representation learning and limit model performance. In this paper, we propose BridgeTower, which introduces multiple bridge layers that build a connection between the top layers of uni-modal encoders and each layer of the cross-modal encoder. This enables effective bottom-up cross-modal alignment and fusion between visual and textual representations of different semantic levels of pre-trained uni-modal encoders in the cross-modal encoder. Pre-trained with only 4M images, BridgeTower achieves state-of-the-art performance o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#38548;&#31163;&#26862;&#26519;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23558;&#21407;&#22987;&#25968;&#25454;&#26144;&#23556;&#21040;&#38543;&#26426;&#34920;&#31034;&#38598;&#21512;&#20013;&#65292;&#36890;&#36807;&#38543;&#26426;&#36724;&#24182;&#34892;&#20999;&#21106;&#26469;&#25191;&#34892;&#25968;&#25454;&#20998;&#21306;&#65292;&#20197;&#35299;&#20915;&#23396;&#31435;&#26862;&#26519;&#31639;&#27861;&#19981;&#33021;&#25104;&#21151;&#26816;&#27979;&#39640;&#32500;/&#38750;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#31354;&#38388;&#20013;&#30340;&#38590;&#20197;&#38548;&#31163;&#30340;&#22256;&#38590;&#24322;&#24120;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.06602</link><description>&lt;p&gt;
&#28145;&#24230;&#38548;&#31163;&#26862;&#26519;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Isolation Forest for Anomaly Detection. (arXiv:2206.06602v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#38548;&#31163;&#26862;&#26519;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23558;&#21407;&#22987;&#25968;&#25454;&#26144;&#23556;&#21040;&#38543;&#26426;&#34920;&#31034;&#38598;&#21512;&#20013;&#65292;&#36890;&#36807;&#38543;&#26426;&#36724;&#24182;&#34892;&#20999;&#21106;&#26469;&#25191;&#34892;&#25968;&#25454;&#20998;&#21306;&#65292;&#20197;&#35299;&#20915;&#23396;&#31435;&#26862;&#26519;&#31639;&#27861;&#19981;&#33021;&#25104;&#21151;&#26816;&#27979;&#39640;&#32500;/&#38750;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#31354;&#38388;&#20013;&#30340;&#38590;&#20197;&#38548;&#31163;&#30340;&#22256;&#38590;&#24322;&#24120;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23396;&#31435;&#26862;&#26519;&#65288;iForest&#65289;&#30001;&#20110;&#22312;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#26222;&#36866;&#26377;&#25928;&#24615;&#21644;&#24378;&#22823;&#30340;&#21487;&#25193;&#23637;&#24615;&#32780;&#36880;&#28176;&#25104;&#20026;&#21487;&#33021;&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#24322;&#24120;&#26816;&#27979;&#22120;&#12290;&#28982;&#32780;&#65292;&#23427;&#30340;&#32447;&#24615;&#36724;&#24182;&#34892;&#38548;&#31163;&#26041;&#27861;&#32463;&#24120;&#23548;&#33268;&#65288;i&#65289;&#26816;&#27979;&#38590;&#20197;&#22312;&#39640;&#32500;/&#38750;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#31354;&#38388;&#20013;&#38548;&#31163;&#30340;&#22256;&#38590;&#24322;&#24120;&#30340;&#22833;&#36133;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#33261;&#21517;&#26157;&#30528;&#30340;&#31639;&#27861;&#20559;&#24046;&#65292;&#23558;&#39044;&#26399;&#36739;&#20302;&#30340;&#24322;&#24120;&#24471;&#20998;&#20998;&#37197;&#32473;&#24037;&#20214;&#21306;&#22495;&#12290;&#36825;&#20123;&#38382;&#39064;&#23548;&#33268;&#39640;&#35823;&#25253;&#29575;&#12290;&#24341;&#20837;&#20102;&#20960;&#20010;iForest&#25193;&#23637;&#65292;&#20294;&#23427;&#20204;&#26412;&#36136;&#19978;&#20173;&#28982;&#20351;&#29992;&#27973;&#23618;&#30340;&#32447;&#24615;&#25968;&#25454;&#20998;&#21306;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#38548;&#31163;&#30495;&#27491;&#24322;&#24120;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#38548;&#31163;&#26862;&#26519;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#34920;&#31034;&#26041;&#26696;&#65292;&#21033;&#29992;&#38543;&#24847;&#21021;&#22987;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#23558;&#21407;&#22987;&#25968;&#25454;&#26144;&#23556;&#21040;&#38543;&#26426;&#34920;&#31034;&#38598;&#21512;&#20013;&#65292;&#38543;&#21518;&#24212;&#29992;&#38543;&#26426;&#36724;&#24182;&#34892;&#20999;&#21106;&#26469;&#25191;&#34892;&#25968;&#25454;&#20998;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;
Isolation forest (iForest) has been emerging as arguably the most popular anomaly detector in recent years due to its general effectiveness across different benchmarks and strong scalability. Nevertheless, its linear axis-parallel isolation method often leads to (i) failure in detecting hard anomalies that are difficult to isolate in high-dimensional/non-linear-separable data space, and (ii) notorious algorithmic bias that assigns unexpectedly lower anomaly scores to artefact regions. These issues contribute to high false negative errors. Several iForest extensions are introduced, but they essentially still employ shallow, linear data partition, restricting their power in isolating true anomalies. Therefore, this paper proposes deep isolation forest. We introduce a new representation scheme that utilises casually initialised neural networks to map original data into random representation ensembles, where random axis-parallel cuts are subsequently applied to perform the data partition. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31867;&#20855;&#26377;&#28508;&#22312;&#20302;&#31209;&#32467;&#26500;&#30340;MDP&#38382;&#39064;&#65292;&#20854;&#20013;&#30456;&#20851;&#30340;&#26368;&#20248;$Q^*$&#20989;&#25968;&#26159;&#20302;&#31209;&#30340;&#65292;&#21033;&#29992;&#31639;&#27861;&#21487;&#20197;&#22312;&#23567;&#20110;&#22810;&#39033;&#24335;&#22686;&#38271;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20869;&#65292;&#26377;&#25928;&#22320;&#23398;&#20064;&#20102;&#19968;&#20010;&#36817;&#20284;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2206.03569</link><description>&lt;p&gt;
&#20855;&#26377;&#28508;&#22312;&#20302;&#31209;&#32467;&#26500;&#30340;&#26679;&#26412;&#26377;&#25928;&#24378;&#21270;&#23398;&#20064;&#20811;&#26381;&#38271;&#26102;&#24046;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Overcoming the Long Horizon Barrier for Sample-Efficient Reinforcement Learning with Latent Low-Rank Structure. (arXiv:2206.03569v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31867;&#20855;&#26377;&#28508;&#22312;&#20302;&#31209;&#32467;&#26500;&#30340;MDP&#38382;&#39064;&#65292;&#20854;&#20013;&#30456;&#20851;&#30340;&#26368;&#20248;$Q^*$&#20989;&#25968;&#26159;&#20302;&#31209;&#30340;&#65292;&#21033;&#29992;&#31639;&#27861;&#21487;&#20197;&#22312;&#23567;&#20110;&#22810;&#39033;&#24335;&#22686;&#38271;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20869;&#65292;&#26377;&#25928;&#22320;&#23398;&#20064;&#20102;&#19968;&#20010;&#36817;&#20284;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#19982;&#38382;&#39064;&#35268;&#27169;&#30456;&#20851;&#30340;&#32553;&#25918;&#24615;&#33021;&#24046;&#65292;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#23454;&#29992;&#24615;&#21463;&#21040;&#38480;&#21046;&#65292;&#23398;&#20064;&#949;-&#26368;&#20248;&#31574;&#30053;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026; $\tilde{\Omega}\left(|S||A|H^3 / \epsilon^2\right)$&#65292;&#20854;&#20013;$S$&#26159;&#29366;&#24577;&#31354;&#38388;&#65292;$A$&#26159;&#21160;&#20316;&#31354;&#38388;&#65292;$H$&#26159;&#26102;&#38388;&#27178;&#36328;&#30340;&#22810;&#20010;&#29366;&#24577;&#12290; &#25105;&#20204;&#32771;&#34385;&#19968;&#31867;&#20855;&#26377;&#28508;&#22312;&#20302;&#31209;&#32467;&#26500;&#30340;MDP&#65292;&#20854;&#20013;&#30456;&#20851;&#30340;&#26368;&#20248; $Q^*$ &#20989;&#25968;&#26159;&#20302;&#31209;&#30340;&#65292;&#28508;&#22312;&#30340;&#29305;&#24449;&#26159;&#26410;&#30693;&#30340;&#12290;&#34429;&#28982;&#30001;&#20110;&#20302;&#31209;&#32467;&#26500;&#65292;&#25105;&#20204;&#24076;&#26395;&#33021;&#22815;&#22312; $|S|$ &#21644; $|A|$ &#19978;&#23454;&#29616;&#32447;&#24615;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#20294;&#22914;&#26524;&#38500;&#20102; $Q^*$ &#30340;&#20302;&#31209;&#24615;&#20043;&#22806;&#27809;&#26377;&#26045;&#21152;&#36827;&#19968;&#27493;&#30340;&#20551;&#35774;&#65292;&#21017;&#22312;&#20165;&#20351;&#29992;&#26469;&#33258;&#26465;&#30446;&#23376;&#38598;&#30340;&#35266;&#23519;&#26469;&#20272;&#35745; $Q$ &#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#23384;&#22312;&#26368;&#22351;&#30340;&#23454;&#20363;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#35201;&#23398;&#20064;&#25509;&#36817;&#26368;&#20248;&#31574;&#30053;&#65292;&#24517;&#39035;&#25215;&#25285;&#38543;&#30528;&#26102;&#38388;&#35270;&#37326; $H$ &#25351;&#25968;&#22686;&#38271;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#38543;&#21518;&#23637;&#31034;&#65292;&#22312;&#26356;&#24378;&#30340;&#20302;&#31209;&#32467;&#26500;&#20551;&#35774;&#19979;&#65292;&#20551;&#35774;&#26377;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#21033;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#65292;&#25105;&#20204;&#22312;&#23567;&#20110;&#22810;&#39033;&#24335;&#22686;&#38271;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20869;&#65292;&#21363; $\tilde O\left(|S|^{3/2}/\epsilon\right)$&#65292;&#26377;&#25928;&#22320;&#23398;&#20064;&#20102;&#19968;&#20010;&#36817;&#20284;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
The practicality of reinforcement learning algorithms has been limited due to poor scaling with respect to the problem size, as the sample complexity of learning an $\epsilon$-optimal policy is $\tilde{\Omega}\left(|S||A|H^3 / \epsilon^2\right)$ over worst case instances of an MDP with state space $S$, action space $A$, and horizon $H$. We consider a class of MDPs for which the associated optimal $Q^*$ function is low rank, where the latent features are unknown. While one would hope to achieve linear sample complexity in $|S|$ and $|A|$ due to the low rank structure, we show that without imposing further assumptions beyond low rank of $Q^*$, if one is constrained to estimate the $Q$ function using only observations from a subset of entries, there is a worst case instance in which one must incur a sample complexity exponential in the horizon $H$ to learn a near optimal policy. We subsequently show that under stronger low rank structural assumptions, given access to a generative model, L
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#39640;&#38454;&#20276;&#38543;&#24494;&#20998;&#30340;&#31070;&#32463;ODE&#26694;&#26550;PNODE&#65292;&#36890;&#36807;&#31163;&#25955;&#20276;&#38543;&#26102;&#38388;&#31215;&#20998;&#22120;&#21644;&#20808;&#36827;&#30340;&#26816;&#26597;&#28857;&#31574;&#30053;&#23454;&#29616;&#20869;&#23384;&#39640;&#25928;&#21644;&#26799;&#24230;&#35745;&#31639;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;PyTorch&#21644;PE&#30340;&#24320;&#28304;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2206.01298</link><description>&lt;p&gt;
&#22522;&#20110;&#39640;&#38454;&#20276;&#38543;&#24494;&#20998;&#30340;&#20869;&#23384;&#39640;&#25928;&#31070;&#32463;ODE&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A memory-efficient neural ODE framework based on high-level adjoint differentiation. (arXiv:2206.01298v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.01298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#39640;&#38454;&#20276;&#38543;&#24494;&#20998;&#30340;&#31070;&#32463;ODE&#26694;&#26550;PNODE&#65292;&#36890;&#36807;&#31163;&#25955;&#20276;&#38543;&#26102;&#38388;&#31215;&#20998;&#22120;&#21644;&#20808;&#36827;&#30340;&#26816;&#26597;&#28857;&#31574;&#30053;&#23454;&#29616;&#20869;&#23384;&#39640;&#25928;&#21644;&#26799;&#24230;&#35745;&#31639;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;PyTorch&#21644;PE&#30340;&#24320;&#28304;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;(&#31070;&#32463;ODE)&#20316;&#20026;&#19968;&#31181;&#23558;&#21160;&#24577;&#31995;&#32479;&#21644;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;&#32593;&#32476;&#26550;&#26500;&#24050;&#32463;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#21407;&#22987;&#31070;&#32463;ODE&#20013;&#30340;&#36830;&#32493;&#20276;&#38543;&#26041;&#27861;&#27714;&#35299;&#30340;&#26799;&#24230;&#24182;&#19981;&#20855;&#26377;&#21453;&#21521;&#31934;&#24230;&#12290;&#20854;&#20182;&#26041;&#27861;&#30001;&#20110;&#28145;&#24230;&#35745;&#31639;&#22270;&#30340;&#38656;&#27714;&#36807;&#39640;&#25110;&#26102;&#38388;&#31215;&#20998;&#26041;&#26696;&#30340;&#36873;&#25321;&#21463;&#38480;&#65292;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#12290;&#20026;&#20102;&#22312;&#19981;&#24433;&#21709;&#20869;&#23384;&#25928;&#29575;&#21644;&#28789;&#27963;&#24615;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#31934;&#30830;&#26799;&#24230;&#35745;&#31639;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;ODE&#26694;&#26550;PNODE&#65292;&#22522;&#20110;&#39640;&#32423;&#31163;&#25955;&#20276;&#38543;&#31639;&#27861;&#24494;&#20998;&#12290;&#36890;&#36807;&#21033;&#29992;&#31163;&#25955;&#20276;&#38543;&#26102;&#38388;&#31215;&#20998;&#22120;&#21644;&#20026;&#36825;&#20123;&#31215;&#20998;&#22120;&#37327;&#36523;&#23450;&#21046;&#30340;&#39640;&#32423;&#26816;&#26597;&#28857;&#31574;&#30053;&#65292;PNODE&#21487;&#22312;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#20043;&#38388;&#25552;&#20379;&#24179;&#34913;&#65292;&#21516;&#26102;&#35745;&#31639;&#26799;&#24230;&#26102;&#20445;&#25345;&#19968;&#33268;&#21644;&#20934;&#30830;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;PyTorch&#21644;PE&#30340;&#24320;&#28304;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural ordinary differential equations (neural ODEs) have emerged as a novel network architecture that bridges dynamical systems and deep learning. However, the gradient obtained with the continuous adjoint method in the vanilla neural ODE is not reverse-accurate. Other approaches suffer either from an excessive memory requirement due to deep computational graphs or from limited choices for the time integration scheme, hampering their application to large-scale complex dynamical systems. To achieve accurate gradients without compromising memory efficiency and flexibility, we present a new neural ODE framework, PNODE, based on high-level discrete adjoint algorithmic differentiation. By leveraging discrete adjoint time integrators and advanced checkpointing strategies tailored for these integrators, PNODE can provide a balance between memory and computational costs, while computing the gradients consistently and accurately. We provide an open-source implementation based on PyTorch and PE
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#32447;&#24615;&#25237;&#24433;&#26041;&#27861;&#26469;&#20998;&#26512;&#27969;&#34892;&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#30340;&#23616;&#37096;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#25506;&#32034;&#39044;&#27979;&#21464;&#37327;&#20043;&#38388;&#30340;&#20132;&#20114;&#22914;&#20309;&#24433;&#21709;&#21464;&#37327;&#37325;&#35201;&#24615;&#20272;&#35745;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#38750;&#24120;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2205.05359</link><description>&lt;p&gt;
&#20351;&#29992;&#21160;&#24577;&#32447;&#24615;&#25237;&#24433;&#26041;&#27861;&#25506;&#32034;&#38750;&#32447;&#24615;&#27169;&#22411;&#30340;&#23616;&#37096;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Exploring Local Explanations of Nonlinear Models Using Animated Linear Projections. (arXiv:2205.05359v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.05359
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#32447;&#24615;&#25237;&#24433;&#26041;&#27861;&#26469;&#20998;&#26512;&#27969;&#34892;&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#30340;&#23616;&#37096;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#25506;&#32034;&#39044;&#27979;&#21464;&#37327;&#20043;&#38388;&#30340;&#20132;&#20114;&#22914;&#20309;&#24433;&#21709;&#21464;&#37327;&#37325;&#35201;&#24615;&#20272;&#35745;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#26085;&#30410;&#22686;&#24378;&#65292;&#20294;&#19982;&#21442;&#25968;&#32479;&#35745;&#27169;&#22411;&#30456;&#27604;&#65292;&#20854;&#22797;&#26434;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#19979;&#38477;&#12290;&#36825;&#31181;&#25240;&#34935;&#23548;&#33268;&#20102;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#20986;&#29616;&#65292;&#25552;&#20379;&#20102;&#35832;&#22914;&#23616;&#37096;&#35299;&#37322;&#65288;LE&#65289;&#21644;&#23616;&#37096;&#21464;&#37327;&#24402;&#22240;&#65288;LVA&#65289;&#20043;&#31867;&#30340;&#26041;&#27861;&#65292;&#20197;&#25581;&#31034;&#27169;&#22411;&#22914;&#20309;&#20351;&#29992;&#39044;&#27979;&#21464;&#37327;&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;LVA&#36890;&#24120;&#19981;&#33021;&#26377;&#25928;&#22788;&#29702;&#39044;&#27979;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#20026;&#20102;&#29702;&#35299;&#39044;&#27979;&#21464;&#37327;&#20043;&#38388;&#30340;&#20132;&#20114;&#22914;&#20309;&#24433;&#21709;&#21464;&#37327;&#37325;&#35201;&#24615;&#20272;&#35745;&#65292;&#21487;&#20197;&#23558;LVA&#36716;&#25442;&#20026;&#32447;&#24615;&#25237;&#24433;&#65292;&#24182;&#20351;&#29992;&#24452;&#21521;&#28216;&#35272;&#12290;&#36825;&#23545;&#20110;&#23398;&#20064;&#27169;&#22411;&#22914;&#20309;&#29359;&#38169;&#65292;&#25110;&#24322;&#24120;&#20540;&#30340;&#24433;&#21709;&#65292;&#25110;&#35266;&#27979;&#20540;&#30340;&#32858;&#31867;&#20063;&#38750;&#24120;&#26377;&#29992;&#12290;&#26412;&#25991;&#20351;&#29992;&#21508;&#31181;&#27969;&#34892;&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#65288;&#21253;&#25324;&#38543;&#26426;&#26862;&#26519;&#21644;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#31034;&#20363;&#26469;&#35828;&#26126;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increased predictive power of machine learning models comes at the cost of increased complexity and loss of interpretability, particularly in comparison to parametric statistical models. This trade-off has led to the emergence of eXplainable AI (XAI) which provides methods, such as local explanations (LEs) and local variable attributions (LVAs), to shed light on how a model use predictors to arrive at a prediction. These provide a point estimate of the linear variable importance in the vicinity of a single observation. However, LVAs tend not to effectively handle association between predictors. To understand how the interaction between predictors affects the variable importance estimate, we can convert LVAs into linear projections and use the radial tour. This is also useful for learning how a model has made a mistake, or the effect of outliers, or the clustering of observations. The approach is illustrated with examples from categorical (penguin species, chocolate types) and quant
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24369;&#30417;&#30563;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#65292;&#19981;&#20165;&#36873;&#25321;&#35201;&#27880;&#37322;&#30340;&#35266;&#27979;&#32467;&#26524;&#65292;&#36824;&#36873;&#25321;&#35201;&#33719;&#24471;&#30340;&#27880;&#37322;&#31934;&#24230;&#65292;&#24182;&#22312;&#39640;&#26031;&#36807;&#31243;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2204.08335</link><description>&lt;p&gt;
&#22522;&#20110;&#24369;&#30417;&#30563;&#30340;&#39640;&#26031;&#36807;&#31243;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active Learning with Weak Supervision for Gaussian Processes. (arXiv:2204.08335v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.08335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24369;&#30417;&#30563;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#65292;&#19981;&#20165;&#36873;&#25321;&#35201;&#27880;&#37322;&#30340;&#35266;&#27979;&#32467;&#26524;&#65292;&#36824;&#36873;&#25321;&#35201;&#33719;&#24471;&#30340;&#27880;&#37322;&#31934;&#24230;&#65292;&#24182;&#22312;&#39640;&#26031;&#36807;&#31243;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#30417;&#30563;&#23398;&#20064;&#65292;&#36827;&#34892;&#25968;&#25454;&#27880;&#37322;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#25104;&#26412;&#12290;&#24403;&#27880;&#37322;&#39044;&#31639;&#26377;&#38480;&#26102;&#65292;&#21487;&#20197;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#26469;&#36873;&#25321;&#21644;&#27880;&#37322;&#37027;&#20123;&#21487;&#33021;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#33719;&#24471;&#26368;&#22823;&#25910;&#30410;&#30340;&#35266;&#27979;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#65292;&#38500;&#20102;&#36873;&#25321;&#35201;&#27880;&#37322;&#30340;&#35266;&#27979;&#32467;&#26524;&#22806;&#65292;&#36824;&#36873;&#25321;&#35201;&#33719;&#24471;&#30340;&#27880;&#37322;&#31934;&#24230;&#12290;&#20551;&#23450;&#20855;&#26377;&#20302;&#31934;&#24230;&#30340;&#27880;&#37322;&#27604;&#20855;&#26377;&#39640;&#31934;&#24230;&#30340;&#27880;&#37322;&#26356;&#20415;&#23452;&#65292;&#36825;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#22312;&#30456;&#21516;&#30340;&#27880;&#37322;&#39044;&#31639;&#19979;&#25506;&#32034;&#36755;&#20837;&#31354;&#38388;&#30340;&#26356;&#22823;&#37096;&#20998;&#12290;&#25105;&#20204;&#22312;&#20808;&#21069;&#38024;&#23545;&#39640;&#26031;&#36807;&#31243;&#25552;&#20986;&#30340;BALD&#30446;&#26631;&#22522;&#30784;&#19978;&#26500;&#24314;&#20102;&#25105;&#20204;&#30340;&#33719;&#21462;&#20989;&#25968;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#33021;&#22815;&#35843;&#25972;&#20027;&#21160;&#23398;&#20064;&#24490;&#29615;&#20013;&#30340;&#27880;&#37322;&#31934;&#24230;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Annotating data for supervised learning can be costly. When the annotation budget is limited, active learning can be used to select and annotate those observations that are likely to give the most gain in model performance. We propose an active learning algorithm that, in addition to selecting which observation to annotate, selects the precision of the annotation that is acquired. Assuming that annotations with low precision are cheaper to obtain, this allows the model to explore a larger part of the input space, with the same annotation budget. We build our acquisition function on the previously proposed BALD objective for Gaussian Processes, and empirically demonstrate the gains of being able to adjust the annotation precision in the active learning loop.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26080;&#24724;&#22312;&#32447;&#20915;&#31574;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;&#38654;&#35745;&#31639;&#32593;&#32476;&#20013;&#20219;&#21153;&#20998;&#37197;&#30340;&#35774;&#35745;&#12290;&#35813;&#21338;&#24328;&#20855;&#26377;&#21807;&#19968;&#30340;&#32435;&#20160;&#22343;&#34913;&#28857;&#65292;&#21487;&#20197;&#20351;&#29992;&#26080;&#24724;&#23398;&#20064;&#31574;&#30053;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2203.14572</link><description>&lt;p&gt;
&#38654;&#35745;&#31639;&#20013;&#30340;&#20998;&#24067;&#24335;&#20219;&#21153;&#31649;&#29702;&#65306;&#19968;&#20010;&#31038;&#20132;&#20985;&#20984;&#36172;&#21338;&#21338;&#24328;
&lt;/p&gt;
&lt;p&gt;
Distributed Task Management in Fog Computing: A Socially Concave Bandit Game. (arXiv:2203.14572v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.14572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26080;&#24724;&#22312;&#32447;&#20915;&#31574;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;&#38654;&#35745;&#31639;&#32593;&#32476;&#20013;&#20219;&#21153;&#20998;&#37197;&#30340;&#35774;&#35745;&#12290;&#35813;&#21338;&#24328;&#20855;&#26377;&#21807;&#19968;&#30340;&#32435;&#20160;&#22343;&#34913;&#28857;&#65292;&#21487;&#20197;&#20351;&#29992;&#26080;&#24724;&#23398;&#20064;&#31574;&#30053;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38654;&#35745;&#31639;&#21033;&#29992;&#32593;&#32476;&#36793;&#32536;&#30340;&#20219;&#21153;&#21368;&#36733;&#21151;&#33021;&#25552;&#39640;&#25928;&#29575;&#24182;&#24555;&#36895;&#21709;&#24212;&#24212;&#29992;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38654;&#33410;&#28857;&#30340;&#24322;&#26500;&#24615;&#21644;&#31995;&#32479;&#21160;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#38654;&#35745;&#31639;&#32593;&#32476;&#20013;&#20219;&#21153;&#20998;&#37197;&#31574;&#30053;&#30340;&#35774;&#35745;&#20173;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#23558;&#20998;&#24067;&#24335;&#20219;&#21153;&#20998;&#37197;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#20855;&#26377;&#36172;&#21338;&#21453;&#39304;&#30340;&#31038;&#20132;&#20985;&#20984;&#21338;&#24328;&#65292;&#24182;&#35777;&#26126;&#35813;&#21338;&#24328;&#20855;&#26377;&#21807;&#19968;&#30340;&#32435;&#20160;&#22343;&#34913;&#28857;&#65292;&#21487;&#20197;&#20351;&#29992;&#26080;&#24724;&#23398;&#20064;&#31574;&#30053;&#65288;&#23545;&#25968;&#20943;&#23567;&#30340;&#36951;&#25022;&#65289;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#26080;&#24724;&#30340;&#22312;&#32447;&#20915;&#31574;&#31574;&#30053;&#12290;&#19968;&#31181;&#31574;&#30053;&#26159;&#20855;&#26377;&#21160;&#37327;&#30340;&#36172;&#21338;&#26799;&#24230;&#19978;&#21319;&#65292;&#26159;&#20855;&#26377;&#36172;&#21338;&#21453;&#39304;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#31639;&#27861;&#12290;&#21478;&#19968;&#31181;&#31574;&#30053;&#26159;&#21033;&#26222;&#24076;&#33576;&#36172;&#24466;&#26377;&#21021;&#22987;&#21270;&#31639;&#27861;&#65292;&#26159;EXP3&#22810;&#33218;&#36172;&#21338;&#31639;&#27861;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#20004;&#31181;&#31574;&#30053;&#30340;&#36951;&#25022;&#30028;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#25910;&#25947;&#29305;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#25152;&#25552;&#20986;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fog computing leverages the task offloading capabilities at the network's edge to improve efficiency and enable swift responses to application demands. However, the design of task allocation strategies in a fog computing network is still challenging because of the heterogeneity of fog nodes and uncertainties in system dynamics. We formulate the distributed task allocation problem as a social-concave game with bandit feedback and show that the game has a unique Nash equilibrium, which is implementable using no-regret learning strategies (regret with sublinear growth). We then develop two no-regret online decision-making strategies. One strategy, namely bandit gradient ascent with momentum, is an online convex optimization algorithm with bandit feedback. The other strategy, Lipschitz bandit with initialization, is an EXP3 multi-armed bandit algorithm. We establish regret bounds for both strategies and analyze their convergence characteristics. Moreover, we compare the proposed strategies
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#27969;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#26694;&#26550;EmotionNAS&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65292;&#23558;&#20004;&#31181;&#27969;&#65288;&#21363;&#25163;&#24037;&#29305;&#24449;&#21644;&#28145;&#24230;&#29305;&#24449;&#65289;&#20316;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#20449;&#24687;&#34917;&#20805;&#27169;&#22359;&#23454;&#29616;&#27969;&#20043;&#38388;&#30340;&#20449;&#24687;&#25972;&#21512;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21019;&#19979;&#26032;&#30340;&#26368;&#20248;&#35760;&#24405;&#12290;</title><link>http://arxiv.org/abs/2203.13617</link><description>&lt;p&gt;
EmotionNAS: &#38754;&#21521;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#21452;&#27969;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
EmotionNAS: Two-stream Neural Architecture Search for Speech Emotion Recognition. (arXiv:2203.13617v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.13617
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#27969;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#26694;&#26550;EmotionNAS&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65292;&#23558;&#20004;&#31181;&#27969;&#65288;&#21363;&#25163;&#24037;&#29305;&#24449;&#21644;&#28145;&#24230;&#29305;&#24449;&#65289;&#20316;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#20449;&#24687;&#34917;&#20805;&#27169;&#22359;&#23454;&#29616;&#27969;&#20043;&#38388;&#30340;&#20449;&#24687;&#25972;&#21512;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21019;&#19979;&#26032;&#30340;&#26368;&#20248;&#35760;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#26159;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#37325;&#35201;&#30740;&#31350;&#35838;&#39064;&#65292;&#29616;&#26377;&#24037;&#20316;&#20027;&#35201;&#20381;&#36182;&#20110;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#26469;&#35774;&#35745;&#27169;&#22411;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#25104;&#21151;&#29575;&#39640;&#65292;&#20294;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#24448;&#24448;&#38656;&#35201;&#19981;&#21516;&#30340;&#32467;&#26500;&#21644;&#36229;&#21442;&#25968;&#65292;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#25628;&#32034;&#26368;&#20248;&#27169;&#22411;&#30340;&#36807;&#31243;&#32791;&#36153;&#26102;&#38388;&#21644;&#31934;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#27969;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#8220;EmotionNAS&#8221;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#20004;&#31181;&#27969;&#65288;&#21363;&#25163;&#24037;&#29305;&#24449;&#21644;&#28145;&#24230;&#29305;&#24449;&#65289;&#20316;&#20026;&#36755;&#20837;&#65292;&#28982;&#21518;&#36827;&#34892;NAS&#20197;&#25628;&#32034;&#27599;&#20010;&#27969;&#30340;&#26368;&#20339;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#39640;&#25928;&#30340;&#20449;&#24687;&#34917;&#20805;&#27169;&#22359;&#25972;&#21512;&#19981;&#21516;&#27969;&#20013;&#30340;&#20114;&#34917;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#25163;&#21160;&#35774;&#35745;&#21644;&#22522;&#20110;NAS&#30340;&#27169;&#22411;&#65292;&#24182;&#21019;&#19979;&#20102;&#26032;&#30340;&#26368;&#20248;&#35760;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech emotion recognition (SER) is an important research topic in human-computer interaction. Existing works mainly rely on human expertise to design models. Despite their success, different datasets often require distinct structures and hyperparameters. Searching for an optimal model for each dataset is time-consuming and labor-intensive. To address this problem, we propose a two-stream neural architecture search (NAS) based framework, called \enquote{EmotionNAS}. Specifically, we take two-stream features (i.e., handcrafted and deep features) as the inputs, followed by NAS to search for the optimal structure for each stream. Furthermore, we incorporate complementary information in different streams through an efficient information supplement module. Experimental results demonstrate that our method outperforms existing manually-designed and NAS-based models, setting the new state-of-the-art record.
&lt;/p&gt;</description></item><item><title>L0Learn&#26159;&#19968;&#20010;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#25968;&#30334;&#19975;&#29305;&#24449;&#38382;&#39064;&#30340;&#31232;&#30095;&#23398;&#20064;&#36719;&#20214;&#21253;&#65292;&#20855;&#26377;&#21487;&#25193;&#23637;&#30340;&#36817;&#20284;&#31639;&#27861;&#21644;&#29992;&#25143;&#21451;&#22909;&#30340;R&#21644;Python&#25509;&#21475;&#12290;</title><link>http://arxiv.org/abs/2202.04820</link><description>&lt;p&gt;
L0Learn: &#20351;&#29992;L0&#27491;&#21017;&#21270;&#30340;&#31232;&#30095;&#23398;&#20064;&#30340;&#21487;&#25193;&#23637;&#36719;&#20214;&#21253;
&lt;/p&gt;
&lt;p&gt;
L0Learn: A Scalable Package for Sparse Learning using L0 Regularization. (arXiv:2202.04820v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.04820
&lt;/p&gt;
&lt;p&gt;
L0Learn&#26159;&#19968;&#20010;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#25968;&#30334;&#19975;&#29305;&#24449;&#38382;&#39064;&#30340;&#31232;&#30095;&#23398;&#20064;&#36719;&#20214;&#21253;&#65292;&#20855;&#26377;&#21487;&#25193;&#23637;&#30340;&#36817;&#20284;&#31639;&#27861;&#21644;&#29992;&#25143;&#21451;&#22909;&#30340;R&#21644;Python&#25509;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;L0Learn&#65306;&#19968;&#20010;&#20351;&#29992; $\ell_0$ &#27491;&#21017;&#21270;&#36827;&#34892;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#21644;&#20998;&#31867;&#30340;&#24320;&#28304;&#36719;&#20214;&#21253;&#12290;L0Learn&#23454;&#29616;&#20102;&#22522;&#20110;&#22352;&#26631;&#19979;&#38477;&#21644;&#26412;&#22320;&#32452;&#21512;&#20248;&#21270;&#30340;&#21487;&#25193;&#23637;&#30340;&#36817;&#20284;&#31639;&#27861;&#12290;&#35813;&#36719;&#20214;&#21253;&#20351;&#29992;C ++&#26500;&#24314;&#65292;&#24182;&#20855;&#26377;&#29992;&#25143;&#21451;&#22909;&#30340;R&#21644;Python&#25509;&#21475;&#12290;L0Learn&#33021;&#22815;&#35299;&#20915;&#20855;&#26377;&#25968;&#30334;&#19975;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#22312;&#19982;&#26368;&#20808;&#36827;&#30340;&#31232;&#30095;&#23398;&#20064;&#36719;&#20214;&#21253;&#30456;&#27604;&#33719;&#24471;&#26377;&#31454;&#20105;&#21147;&#30340;&#36816;&#34892;&#26102;&#21644;&#32479;&#35745;&#24615;&#33021;&#12290;L0Learn&#22312;CRAN&#21644;GitHub&#19978;&#37117;&#21487;&#29992;&#65288;https://cran.r-project.org/package=L0Learn&#21644;https://github.com/hazimehh/L0Learn&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present L0Learn: an open-source package for sparse linear regression and classification using $\ell_0$ regularization. L0Learn implements scalable, approximate algorithms, based on coordinate descent and local combinatorial optimization. The package is built using C++ and has user-friendly R and Python interfaces. L0Learn can address problems with millions of features, achieving competitive run times and statistical performance with state-of-the-art sparse learning packages. L0Learn is available on both CRAN and GitHub (https://cran.r-project.org/package=L0Learn and https://github.com/hazimehh/L0Learn).
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38024;&#23545;&#39640;&#32500;&#30697;&#38453;&#25968;&#25454;&#30340;&#29305;&#24449;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#20351;&#29992;&#21152;&#26435;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#24046;&#24322;&#20316;&#20026;&#19981;&#30456;&#20284;&#24230;&#27979;&#37327;&#30340;&#23618;&#27425;&#32858;&#31867;&#31639;&#27861;&#65292;&#29702;&#35770;&#19978;&#23454;&#29616;&#20102;&#32858;&#31867;&#19968;&#33268;&#24615;&#65292;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#31034;&#20363;&#20013;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2112.12909</link><description>&lt;p&gt;
&#38024;&#23545;&#39640;&#32500;&#30697;&#38453;&#25968;&#25454;&#30340;&#26368;&#20248;&#21464;&#37327;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Optimal Variable Clustering for High-Dimensional Matrix Valued Data. (arXiv:2112.12909v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.12909
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38024;&#23545;&#39640;&#32500;&#30697;&#38453;&#25968;&#25454;&#30340;&#29305;&#24449;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#20351;&#29992;&#21152;&#26435;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#24046;&#24322;&#20316;&#20026;&#19981;&#30456;&#20284;&#24230;&#27979;&#37327;&#30340;&#23618;&#27425;&#32858;&#31867;&#31639;&#27861;&#65292;&#29702;&#35770;&#19978;&#23454;&#29616;&#20102;&#32858;&#31867;&#19968;&#33268;&#24615;&#65292;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#31034;&#20363;&#20013;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30697;&#38453;&#20540;&#25968;&#25454;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#26085;&#30410;&#26222;&#21450;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#36825;&#31181;&#31867;&#22411;&#25968;&#25454;&#30340;&#32858;&#31867;&#26041;&#27861;&#26159;&#38024;&#23545;&#24179;&#22343;&#27169;&#22411;&#35774;&#35745;&#30340;&#65292;&#19981;&#32771;&#34385;&#29305;&#24449;&#30340;&#20381;&#36182;&#32467;&#26500;&#65292;&#32780;&#35813;&#32467;&#26500;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#23588;&#20026;&#37325;&#35201;&#12290;&#20026;&#20102;&#20174;&#20381;&#36182;&#32467;&#26500;&#20013;&#25552;&#21462;&#20449;&#24687;&#36827;&#34892;&#32858;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;&#29305;&#24449;&#25490;&#21015;&#25104;&#30697;&#38453;&#24418;&#24335;&#65292;&#24182;&#20351;&#29992;&#19968;&#20123;&#26410;&#30693;&#30340;&#25104;&#21592;&#30697;&#38453;&#34920;&#31034;&#34892;&#21644;&#21015;&#30340;&#32858;&#31867;&#12290;&#22312;&#27492;&#27169;&#22411;&#19979;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31867;&#20351;&#29992;&#21152;&#26435;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#24046;&#24322;&#20316;&#20026;&#19981;&#30456;&#20284;&#24230;&#27979;&#37327;&#30340;&#23618;&#27425;&#32858;&#31867;&#31639;&#27861;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#23454;&#29616;&#32858;&#31867;&#19968;&#33268;&#24615;&#12290;&#34429;&#28982;&#36825;&#31181;&#19968;&#33268;&#24615;&#32467;&#26524;&#36866;&#29992;&#20110;&#25105;&#20204;&#30340;&#31639;&#27861;&#21644;&#24191;&#27867;&#30340;&#21152;&#26435;&#21327;&#26041;&#24046;&#30697;&#38453;&#31867;&#21035;&#65292;&#20294;&#36825;&#20010;&#32467;&#26524;&#30340;&#26465;&#20214;&#20381;&#36182;&#20110;&#21327;&#26041;&#24046;&#20989;&#25968;&#21644;&#21152;&#26435;&#26426;&#21046;&#30340;&#36873;&#25321;&#12290;&#36890;&#36807;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#31034;&#20363;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#32858;&#31867;&#24615;&#33021;&#21644;&#29305;&#24449;&#36873;&#25321;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Matrix valued data has become increasingly prevalent in many applications. Most of the existing clustering methods for this type of data are tailored to the mean model and do not account for the dependence structure of the features, which can be very informative, especially in high-dimensional settings. To extract the information from the dependence structure for clustering, we propose a new latent variable model for the features arranged in matrix form, with some unknown membership matrices representing the clusters for the rows and columns. Under this model, we further propose a class of hierarchical clustering algorithms using the difference of a weighted covariance matrix as the dissimilarity measure. Theoretically, we show that under mild conditions, our algorithm attains clustering consistency in the high-dimensional setting. While this consistency result holds for our algorithm with a broad class of weighted covariance matrices, the conditions for this result depend on the choic
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#33410;&#28857;&#32593;&#32476;&#21450;&#20854;&#20043;&#38388;&#30340;&#27969;&#20316;&#20026;&#22270;&#20687;&#36827;&#34892;&#22788;&#29702;&#65292;&#24182;&#21033;&#29992;&#22270;&#20687;&#21387;&#32553;&#25216;&#26415;&#21644;&#22320;&#29702;&#31614;&#21517;&#23398;&#20064;&#32593;&#32476;&#32467;&#26500;&#20197;&#25512;&#33616;&#26410;&#26469;&#30340;&#32593;&#32476;&#36830;&#36890;&#24615;&#12290;&#27492;&#22806;&#65292;&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#32479;&#35745;&#25688;&#35201;&#30340;&#32593;&#32476;&#20449;&#24687;&#21644;&#29992;&#25143;&#20915;&#31574;&#26469;&#24378;&#21270;&#20195;&#29702;&#27010;&#29575;&#20915;&#31574;&#30340;&#36125;&#21494;&#26031;&#24378;&#21270;&#31639;&#27861;&#65292;&#24182;&#25506;&#31350;&#20102;&#21033;&#29992;&#21387;&#32553;&#30452;&#25509;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2112.03419</link><description>&lt;p&gt;
&#21033;&#29992;&#22270;&#20687;&#21464;&#25442;&#23398;&#20064;&#32593;&#32476;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Using Image Transformations to Learn Network Structure. (arXiv:2112.03419v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.03419
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#33410;&#28857;&#32593;&#32476;&#21450;&#20854;&#20043;&#38388;&#30340;&#27969;&#20316;&#20026;&#22270;&#20687;&#36827;&#34892;&#22788;&#29702;&#65292;&#24182;&#21033;&#29992;&#22270;&#20687;&#21387;&#32553;&#25216;&#26415;&#21644;&#22320;&#29702;&#31614;&#21517;&#23398;&#20064;&#32593;&#32476;&#32467;&#26500;&#20197;&#25512;&#33616;&#26410;&#26469;&#30340;&#32593;&#32476;&#36830;&#36890;&#24615;&#12290;&#27492;&#22806;&#65292;&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#32479;&#35745;&#25688;&#35201;&#30340;&#32593;&#32476;&#20449;&#24687;&#21644;&#29992;&#25143;&#20915;&#31574;&#26469;&#24378;&#21270;&#20195;&#29702;&#27010;&#29575;&#20915;&#31574;&#30340;&#36125;&#21494;&#26031;&#24378;&#21270;&#31639;&#27861;&#65292;&#24182;&#25506;&#31350;&#20102;&#21033;&#29992;&#21387;&#32553;&#30452;&#25509;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23398;&#20064;&#20219;&#21153;&#38656;&#35201;&#35266;&#23519;&#19968;&#31995;&#21015;&#22270;&#20687;&#24182;&#20570;&#20986;&#20915;&#31574;&#12290;&#22312;&#35774;&#35745;&#21644;&#35268;&#21010;&#33410;&#28857;&#20043;&#38388;&#30340;&#36135;&#36816;&#31665;&#30340;&#36816;&#36755;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#33410;&#28857;&#32593;&#32476;&#21450;&#20854;&#20043;&#38388;&#30340;&#27969;&#20316;&#20026;&#22270;&#20687;&#22788;&#29702;&#12290;&#36825;&#20123;&#22270;&#20687;&#20855;&#26377;&#26377;&#29992;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#21487;&#20197;&#36827;&#34892;&#32479;&#35745;&#25688;&#35201;&#12290;&#21033;&#29992;&#22270;&#20687;&#21387;&#32553;&#25216;&#26415;&#65292;&#23558;&#22270;&#20687;&#21387;&#32553;&#25104;&#21253;&#21547;&#21487;&#35299;&#37322;&#22320;&#29702;&#20449;&#24687;&#30340;&#25968;&#23383;&#38598;&#21512;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20316;&#22320;&#29702;&#31614;&#21517;&#12290;&#21033;&#29992;&#22320;&#29702;&#31614;&#21517;&#65292;&#25105;&#20204;&#23398;&#20064;&#32593;&#32476;&#32467;&#26500;&#65292;&#20197;&#25512;&#33616;&#26410;&#26469;&#30340;&#32593;&#32476;&#36830;&#36890;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#24378;&#21270;&#31639;&#27861;&#65292;&#21033;&#29992;&#32479;&#35745;&#25688;&#35201;&#30340;&#32593;&#32476;&#20449;&#24687;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#21644;&#29992;&#25143;&#20915;&#31574;&#26469;&#24378;&#21270;&#20195;&#29702;&#30340;&#27010;&#29575;&#20915;&#31574;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#31616;&#21333;&#20219;&#21153;&#20013;&#22914;&#20309;&#30452;&#25509;&#20351;&#29992;&#21387;&#32553;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65292;&#19981;&#38656;&#35201;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many learning tasks require observing a sequence of images and making a decision. In a transportation problem of designing and planning for shipping boxes between nodes, we show how to treat the network of nodes and the flows between them as images. These images have useful structural information that can be statistically summarized. Using image compression techniques, we reduce an image down to a set of numbers that contain interpretable geographic information that we call geographic signatures. Using geographic signatures, we learn network structure that can be utilized to recommend future network connectivity. We develop a Bayesian reinforcement algorithm that takes advantage of statistically summarized network information as priors and user-decisions to reinforce an agent's probabilistic decision. Additionally, we show how reinforcement learning can be used with compression directly without interpretation in simple tasks.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#23384;&#22312;&#22810;&#31181;&#25915;&#20987;&#28431;&#27934;&#65292;&#21253;&#25324;&#27169;&#22411;&#21453;&#28436;&#12289;&#25104;&#21592;&#25512;&#26029;&#12289;&#25968;&#25454;&#27745;&#26579;&#12289;&#21518;&#38376;&#31561;&#65292;&#36825;&#20123;&#38382;&#39064;&#21487;&#33021;&#23545;&#29616;&#23454;&#19990;&#30028;&#30340;AI&#24212;&#29992;&#20135;&#29983;&#24433;&#21709;&#65292;&#38656;&#35201;&#23547;&#27714;&#35299;&#20915;&#26041;&#26696;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2110.15444</link><description>&lt;p&gt;
&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#23384;&#22312;&#30340;10&#20010;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
10 Security and Privacy Problems in Large Foundation Models. (arXiv:2110.15444v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.15444
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#23384;&#22312;&#22810;&#31181;&#25915;&#20987;&#28431;&#27934;&#65292;&#21253;&#25324;&#27169;&#22411;&#21453;&#28436;&#12289;&#25104;&#21592;&#25512;&#26029;&#12289;&#25968;&#25454;&#27745;&#26579;&#12289;&#21518;&#38376;&#31561;&#65292;&#36825;&#20123;&#38382;&#39064;&#21487;&#33021;&#23545;&#29616;&#23454;&#19990;&#30028;&#30340;AI&#24212;&#29992;&#20135;&#29983;&#24433;&#21709;&#65292;&#38656;&#35201;&#23547;&#27714;&#35299;&#20915;&#26041;&#26696;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65288;&#22914;GPT&#12289;CLIP&#21644;DINO&#65289;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#36827;&#23637;&#65292;&#34987;&#26222;&#36941;&#35748;&#20026;&#26159;&#36890;&#29992;AI&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#29305;&#21035;&#26159;&#65292;&#37319;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#26469;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#65292;&#20351;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#23601;&#20687;AI&#29983;&#24577;&#31995;&#32479;&#30340;&#8220;&#25805;&#20316;&#31995;&#32479;&#8221;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#39044;&#35757;&#32451;&#19968;&#20010;&#26356;&#22909;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#25913;&#21892;&#20854;&#22312;&#38750;&#23545;&#25239;&#35774;&#32622;&#19979;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32780;&#22312;&#23545;&#25239;&#35774;&#32622;&#19979;&#65292;&#20854;&#23433;&#20840;&#21644;&#38544;&#31169;&#21017;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#23433;&#20840;&#25110;&#38544;&#31169;&#38382;&#39064;&#20250;&#23548;&#33268;AI&#29983;&#24577;&#31995;&#32479;&#30340;&#21333;&#19968;&#25925;&#38556;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models--such as GPT, CLIP, and DINO--have achieved revolutionary progress in the past several years and are commonly believed to be a promising approach for general-purpose AI. In particular, self-supervised learning is adopted to pre-train a foundation model using a large amount of unlabeled data. A pre-trained foundation model is like an ``operating system'' of the AI ecosystem. Specifically, a foundation model can be used as a feature extractor for many downstream tasks with little or no labeled training data. Existing studies on foundation models mainly focused on pre-training a better foundation model to improve its performance on downstream tasks in non-adversarial settings, leaving its security and privacy in adversarial settings largely unexplored. A security or privacy issue of a pre-trained foundation model leads to a single point of failure for the AI ecosystem. In this book chapter, we discuss 10 basic security and privacy problems for the pre-trained foundation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#33218;&#32769;&#34382;&#26426;&#20013;&#21518;&#24724;&#26368;&#23567;&#21270;&#21644;&#26368;&#20339;&#33218;&#35782;&#21035;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;,&#35774;&#35745;&#20102;BoBW-lil'UCB $(\gamma)$&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#27809;&#26377;&#31639;&#27861;&#33021;&#21516;&#26102;&#20026;RM&#21644;BAI&#30446;&#26631;&#34920;&#29616;&#26368;&#20339;&#65292;BoBW-lil'UCB $(\gamma)$&#21487;&#22312;&#19981;&#21516;&#30340;$\gamma$&#20540;&#19979;&#23454;&#29616;RM&#25110;BAI&#30340;&#26368;&#20248;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2110.08627</link><description>&lt;p&gt;
&#22312;&#22810;&#33218;&#32769;&#34382;&#26426;&#20013;&#23454;&#29616;&#21518;&#24724;&#26368;&#23567;&#21270;&#21644;&#26368;&#20339;&#33218;&#35782;&#21035;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;
&lt;/p&gt;
&lt;p&gt;
Achieving the Pareto Frontier of Regret Minimization and Best Arm Identification in Multi-Armed Bandits. (arXiv:2110.08627v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.08627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#33218;&#32769;&#34382;&#26426;&#20013;&#21518;&#24724;&#26368;&#23567;&#21270;&#21644;&#26368;&#20339;&#33218;&#35782;&#21035;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;,&#35774;&#35745;&#20102;BoBW-lil'UCB $(\gamma)$&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#27809;&#26377;&#31639;&#27861;&#33021;&#21516;&#26102;&#20026;RM&#21644;BAI&#30446;&#26631;&#34920;&#29616;&#26368;&#20339;&#65292;BoBW-lil'UCB $(\gamma)$&#21487;&#22312;&#19981;&#21516;&#30340;$\gamma$&#20540;&#19979;&#23454;&#29616;RM&#25110;BAI&#30340;&#26368;&#20248;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#33218;&#32769;&#34382;&#26426;&#20013;&#20004;&#20010;&#20856;&#22411;&#30446;&#26631;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#65292;&#21363;&#22266;&#23450;&#26102;&#38388;&#20869;&#30340;&#21518;&#24724;&#26368;&#23567;&#21270;&#21644;&#26368;&#20339;&#33218;&#35782;&#21035;&#12290;&#24179;&#34913;&#25506;&#32034;&#21644;&#21033;&#29992;&#23545;&#20110;&#21518;&#24724;&#26368;&#23567;&#21270;&#21644;&#26368;&#20339;&#33218;&#35782;&#21035;&#37117;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#23545;&#20110;&#21518;&#32773;&#26469;&#35828;&#65292;&#25506;&#32034;&#26356;&#20851;&#38190;&#12290;&#26412;&#25991;&#35774;&#35745;&#21644;&#20998;&#26512;&#20102;BoBW-lil'UCB $(\gamma)$&#31639;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#22522;&#20110;BAI&#22833;&#36133;&#27010;&#29575;&#30340;&#21487;&#36798;&#21040;&#36951;&#25022;&#19979;&#38480;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;(i)&#27809;&#26377;&#31639;&#27861;&#33021;&#21516;&#26102;&#20026;RM&#21644;BAI&#30446;&#26631;&#34920;&#29616;&#26368;&#20339;&#65292;(ii)BoBW-lil'UCB $(\gamma)$&#21487;&#22312;&#19981;&#21516;&#30340;$\gamma$&#20540;&#19979;&#23454;&#29616;RM&#25110;BAI&#30340;&#26368;&#20248;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#23637;&#31034;&#20808;&#21069;&#20316;&#21697;&#20013;&#30340;&#24120;&#25968;&#22914;&#20309;&#20381;&#36182;&#26576;&#20123;&#38590;&#24230;&#21442;&#25968;&#65292;&#26356;&#31934;&#30830;&#22320;&#38416;&#26126;&#20102;&#27492;&#31867;&#31639;&#27861;&#20013;&#30340;&#26435;&#34913;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;BoBW-lil'UCB&#20248;&#20110;&#20854;&#26368;&#25509;&#36817;&#30340;&#31454;&#20105;&#32773;UCB$_\alpha$&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the Pareto frontier of two archetypal objectives in multi-armed bandits, namely, regret minimization (RM) and best arm identification (BAI) with a fixed horizon. It is folklore that the balance between exploitation and exploration is crucial for both RM and BAI, but exploration is more critical in achieving the optimal performance for the latter objective. To this end, we design and analyze the BoBW-lil'UCB$(\gamma)$ algorithm. Complementarily, by establishing lower bounds on the regret achievable by any algorithm with a given BAI failure probability, we show that (i) no algorithm can simultaneously perform optimally for both the RM and BAI objectives, and (ii) BoBW-lil'UCB$(\gamma)$ achieves order-wise optimal performance for RM or BAI under different values of $\gamma$. Our work elucidates the trade-off more precisely by showing how the constants in previous works depend on certain hardness parameters. Finally, we show that BoBW-lil'UCB outperforms a close competitor UCB$_\a
&lt;/p&gt;</description></item><item><title>&#35768;&#22810;&#30740;&#31350;&#23581;&#35797;&#37325;&#26032;&#21046;&#23450;&#21363;&#24109;&#26597;&#35810;&#20197;&#25903;&#25345;&#24320;&#21457;&#20154;&#21592;&#36827;&#34892;&#20195;&#30721;&#25628;&#32034;&#65292;&#26412;&#25991;&#38024;&#23545;70&#20010;&#20027;&#35201;&#26597;&#35810;&#20877;&#21046;&#23450;&#30740;&#31350;&#36827;&#34892;&#20102;&#32454;&#33268;&#31579;&#36873;&#21644;&#28145;&#20837;&#30340;&#23450;&#24615;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#20843;&#31181;&#20027;&#35201;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2108.09646</link><description>&lt;p&gt;
&#33258;&#21160;&#26597;&#35810;&#20877;&#21046;&#23450;&#22312;&#28304;&#20195;&#30721;&#25628;&#32034;&#20013;&#30340;&#31995;&#32479;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Systematic Review of Automated Query Reformulations in Source Code Search. (arXiv:2108.09646v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.09646
&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30740;&#31350;&#23581;&#35797;&#37325;&#26032;&#21046;&#23450;&#21363;&#24109;&#26597;&#35810;&#20197;&#25903;&#25345;&#24320;&#21457;&#20154;&#21592;&#36827;&#34892;&#20195;&#30721;&#25628;&#32034;&#65292;&#26412;&#25991;&#38024;&#23545;70&#20010;&#20027;&#35201;&#26597;&#35810;&#20877;&#21046;&#23450;&#30740;&#31350;&#36827;&#34892;&#20102;&#32454;&#33268;&#31579;&#36873;&#21644;&#28145;&#20837;&#30340;&#23450;&#24615;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#20843;&#31181;&#20027;&#35201;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20462;&#22797;&#36719;&#20214;&#28431;&#27934;&#21644;&#28155;&#21152;&#26032;&#21151;&#33021;&#26159;&#20027;&#35201;&#30340;&#32500;&#25252;&#20219;&#21153;&#20043;&#20108;&#12290;&#36825;&#20123;&#28431;&#27934;&#21644;&#21151;&#33021;&#20197;&#26356;&#25913;&#35831;&#27714;&#30340;&#24418;&#24335;&#25253;&#21578;&#12290;&#24320;&#21457;&#20154;&#21592;&#20250;&#20174;&#36825;&#20123;&#35831;&#27714;&#20013;&#36873;&#25321;&#19968;&#20123;&#20851;&#38190;&#35789;&#20316;&#20026;&#21363;&#24109;&#26597;&#35810;&#65292;&#28982;&#21518;&#20351;&#29992;&#25628;&#32034;&#24341;&#25806;&#25191;&#34892;&#26597;&#35810;&#65292;&#26597;&#25214;&#38656;&#35201;&#26356;&#25913;&#30340;&#36719;&#20214;&#20195;&#30721;&#30340;&#30830;&#20999;&#20301;&#32622;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#32463;&#39564;&#20016;&#23500;&#30340;&#24320;&#21457;&#20154;&#21592;&#36890;&#24120;&#20063;&#26080;&#27861;&#36873;&#25321;&#36866;&#24403;&#30340;&#26597;&#35810;&#65292;&#36825;&#23548;&#33268;&#22312;&#20195;&#30721;&#25628;&#32034;&#26399;&#38388;&#36827;&#34892;&#26114;&#36149;&#30340;&#35797;&#38169;&#12290;&#22810;&#24180;&#26469;&#65292;&#35768;&#22810;&#30740;&#31350;&#23581;&#35797;&#37325;&#26032;&#21046;&#23450;&#24320;&#21457;&#20154;&#21592;&#30340;&#21363;&#24109;&#26597;&#35810;&#20197;&#25903;&#25345;&#20182;&#20204;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#23545;70&#20010;&#20027;&#35201;&#26597;&#35810;&#20877;&#21046;&#23450;&#30740;&#31350;&#36827;&#34892;&#32454;&#33268;&#31579;&#36873;&#65288;&#20174;2,970&#20010;&#20505;&#36873;&#30740;&#31350;&#20013;&#36873;&#25321;&#65289;&#65292;&#36827;&#34892;&#28145;&#20837;&#30340;&#23450;&#24615;&#20998;&#26512;&#65288;&#22914;&#22522;&#30784;&#29702;&#35770;&#65289;&#65292;&#24182;&#22238;&#31572;&#19971;&#20010;&#30740;&#31350;&#38382;&#39064;&#24182;&#25552;&#20986;&#20027;&#35201;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#24050;&#37319;&#29992;&#20102;&#20843;&#31181;&#20027;&#35201;&#26041;&#27861;&#65288;&#22914;&#35789;&#39033;&#21152;&#26435;&#65292;&#35789;&#39033;&#20849;&#29616;&#20998;&#26512;&#65292;&#35789;&#24211;&#26597;&#25214;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fixing software bugs and adding new features are two of the major maintenance tasks. Software bugs and features are reported as change requests. Developers consult these requests and often choose a few keywords from them as an ad hoc query. Then they execute the query with a search engine to find the exact locations within software code that need to be changed. Unfortunately, even experienced developers often fail to choose appropriate queries, which leads to costly trials and errors during a code search. Over the years, many studies attempt to reformulate the ad hoc queries from developers to support them. In this systematic literature review, we carefully select 70 primary studies on query reformulations from 2,970 candidate studies, perform an in-depth qualitative analysis (e.g., Grounded Theory), and then answer seven research questions with major findings. First, to date, eight major methodologies (e.g., term weighting, term co-occurrence analysis, thesaurus lookup) have been adop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#38544;&#31169;&#22312;&#32447;&#38543;&#26426;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#21253;&#25324;&#24046;&#20998;&#38544;&#31169;&#30340;&#38543;&#26426;&#36172;&#21338;&#26426;&#31639;&#27861;&#21644;&#31169;&#26377;&#38543;&#26426;&#22312;&#32447;&#23398;&#20064;&#30340;&#23436;&#20840;&#20449;&#24687;&#29256;&#26412;&#12290;&#20854;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#38543;&#26102;&#21487;&#29992;&#30340;&#22522;&#20110;UCB&#30340;&#31639;&#27861;&#36798;&#21040;&#20102;&#26368;&#20248;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2102.07929</link><description>&lt;p&gt;
&#38544;&#31169;&#22312;&#32447;&#38543;&#26426;&#23398;&#20064;&#30340;&#36817;&#20284;&#26368;&#20248;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Near-Optimal Algorithms for Private Online Learning in a Stochastic Environment. (arXiv:2102.07929v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.07929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#38544;&#31169;&#22312;&#32447;&#38543;&#26426;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#21253;&#25324;&#24046;&#20998;&#38544;&#31169;&#30340;&#38543;&#26426;&#36172;&#21338;&#26426;&#31639;&#27861;&#21644;&#31169;&#26377;&#38543;&#26426;&#22312;&#32447;&#23398;&#20064;&#30340;&#23436;&#20840;&#20449;&#24687;&#29256;&#26412;&#12290;&#20854;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#38543;&#26102;&#21487;&#29992;&#30340;&#22522;&#20110;UCB&#30340;&#31639;&#27861;&#36798;&#21040;&#20102;&#26368;&#20248;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#31169;&#26377;&#38543;&#26426;&#22312;&#32447;&#23398;&#20064;&#30340;&#21464;&#20307;&#12290;&#31532;&#19968;&#31181;&#21464;&#20307;&#26159;&#24046;&#20998;&#38544;&#31169;&#30340;&#38543;&#26426;&#36172;&#21338;&#26426;&#31639;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26102;&#21487;&#29992;&#30340;&#22522;&#20110;UCB&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#36798;&#21040;&#26368;&#20248;&#24615;&#33021;&#12290;&#31532;&#20108;&#31181;&#21464;&#20307;&#26159;&#31169;&#26377;&#38543;&#26426;&#22312;&#32447;&#23398;&#20064;&#30340;&#23436;&#20840;&#20449;&#24687;&#29256;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#21487;&#21516;&#26102;&#33719;&#24471;&#38544;&#31169;&#21644;&#24615;&#33021;&#30340;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider two variants of private stochastic online learning. The first variant is differentially private stochastic bandits. Previously, Sajed and Sheffet (2019) devised the DP Successive Elimination (DP-SE) algorithm that achieves the optimal $ O \biggl(\sum\limits_{1\le j \le K: \Delta_j &gt;0} \frac{ \log T}{ \Delta_j} + \frac{ K\log T}{\epsilon} \biggr)$ problem-dependent regret bound, where $K$ is the number of arms, $\Delta_j$ is the mean reward gap of arm $j$, $T$ is the time horizon, and $\epsilon$ is the required privacy parameter. However, like other elimination style algorithms, it is not an anytime algorithm. Until now, it was not known whether UCB-based algorithms could achieve this optimal regret bound. We present an anytime, UCB-based algorithm that achieves optimality. Our experiments show that the UCB-based algorithm is competitive with DP-SE. The second variant is the full information version of private stochastic online learning. Specifically, for the problem of deci
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26041;&#27861;&#26469;&#35299;&#20915;&#27491;&#20132;&#32676;&#23376;&#32676;&#21516;&#27493;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24191;&#20041;&#24130;&#26041;&#27861;&#30340;&#36845;&#20195;&#20462;&#27491;&#21644;&#24688;&#24403;&#30340;&#21021;&#22987;&#27493;&#39588;&#21487;&#20197;&#22312;&#26576;&#20123;&#20551;&#35774;&#26465;&#20214;&#19979;&#33719;&#24471;&#36739;&#24378;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20256;&#24863;&#22120;&#32593;&#32476;&#31561;&#39046;&#22495;&#26377;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2009.07514</link><description>&lt;p&gt;
&#27491;&#20132;&#32676;&#23376;&#32676;&#21516;&#27493;&#38382;&#39064;&#30340;&#32479;&#19968;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Unified Approach to Synchronization Problems over Subgroups of the Orthogonal Group. (arXiv:2009.07514v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.07514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26041;&#27861;&#26469;&#35299;&#20915;&#27491;&#20132;&#32676;&#23376;&#32676;&#21516;&#27493;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24191;&#20041;&#24130;&#26041;&#27861;&#30340;&#36845;&#20195;&#20462;&#27491;&#21644;&#24688;&#24403;&#30340;&#21021;&#22987;&#27493;&#39588;&#21487;&#20197;&#22312;&#26576;&#20123;&#20551;&#35774;&#26465;&#20214;&#19979;&#33719;&#24471;&#36739;&#24378;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20256;&#24863;&#22120;&#32593;&#32476;&#31561;&#39046;&#22495;&#26377;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#27493;&#38382;&#39064;&#26088;&#22312;&#20272;&#35745;&#19968;&#32452;&#32676;&#20803;&#32032;$G^*_1,\dots,G^*_n\in\mathcal{G}$,&#22522;&#20110;&#24418;&#22914;$G^*_i{G^*_j}^{-1}$&#30340;&#25152;&#26377;&#25104;&#23545;&#27604;&#29575;&#23376;&#38598;&#30340;&#22024;&#26434;&#35266;&#27979;&#12290;&#26412;&#25991;&#32771;&#34385;&#32676;&#20026;&#27491;&#20132;&#32676;&#30340;&#21516;&#27493;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#30001;&#36866;&#24403;&#30340;&#21021;&#22987;&#21270;&#27493;&#39588;&#21644;&#22522;&#20110;&#24191;&#20041;&#24130;&#27861;&#30340;&#36845;&#20195;&#32454;&#21270;&#27493;&#39588;&#32452;&#25104;&#65292;&#24182;&#34920;&#26126;&#23427;&#22312;&#23545;&#32676;&#12289;&#27979;&#37327;&#22270;&#12289;&#22122;&#22768;&#21644;&#21021;&#22987;&#21270;&#30340;&#26576;&#20123;&#20551;&#35774;&#19979;&#20855;&#26377;&#24378;&#22823;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of synchronization over a group $\mathcal{G}$ aims to estimate a collection of group elements $G^*_1, \dots, G^*_n \in \mathcal{G}$ based on noisy observations of a subset of all pairwise ratios of the form $G^*_i {G^*_j}^{-1}$. Such a problem has gained much attention recently and finds many applications across a wide range of scientific and engineering areas. In this paper, we consider the class of synchronization problems in which the group is a closed subgroup of the orthogonal group. This class covers many group synchronization problems that arise in practice. Our contribution is fivefold. First, we propose a unified approach for solving this class of group synchronization problems, which consists of a suitable initialization step and an iterative refinement step based on the generalized power method, and show that it enjoys a strong theoretical guarantee on the estimation error under certain assumptions on the group, measurement graph, noise, and initialization. Secon
&lt;/p&gt;</description></item></channel></rss>