<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;&#32467;&#26500;&#21270;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09518</link><description>&lt;p&gt;
&#20154;&#31867;&#35838;&#31243;&#25351;&#23548;&#19979;&#30340;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning with Human Curriculum. (arXiv:2310.09518v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;&#32467;&#26500;&#21270;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#30340;&#20027;&#27969;&#33539;&#24335;&#26159;&#38543;&#26426;&#27927;&#29260;&#35757;&#32451;&#26368;&#22823;&#22810;&#26679;&#21270;&#25351;&#20196;-&#21709;&#24212;&#23545;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#24403;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;ChatGPT&#21644;GPT-4&#20013;&#24212;&#29992;&#32467;&#26500;&#21270;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#19982;&#20197;&#24448;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#25351;&#20196;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#27169;&#25311;&#20102;&#20154;&#31867;&#25945;&#32946;&#30340;&#28176;&#36827;&#24615;&#21644;&#26377;&#32452;&#32455;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#25968;&#25454;&#38598;&#19982;&#25945;&#32946;&#26694;&#26550;&#23545;&#40784;&#26469;&#31574;&#21010;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#20026;&#27599;&#20010;&#26679;&#26412;&#21253;&#25324;&#20027;&#39064;&#21644;&#35748;&#30693;&#20005;&#35880;&#31243;&#24230;&#31561;&#20803;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#20174;&#20013;&#23398;&#21040;&#30740;&#31350;&#29983;&#38454;&#27573;&#30340;&#20840;&#38754;&#32454;&#31890;&#24230;&#20027;&#39064;&#65292;&#27599;&#20010;&#20027;&#39064;&#37117;&#26377;&#21508;&#31181;&#38382;&#39064;&#65292;&#20197;&#21033;&#29992;&#24067;&#40065;&#22982;&#30340;&#35748;&#30693;&#20998;&#32423;&#27861;&#25552;&#39640;&#27010;&#24565;&#28145;&#24230;&#65292;&#35813;&#20998;&#32423;&#27861;&#29992;&#20110;&#21306;&#20998;&#27599;&#20010;&#27010;&#24565;&#30340;&#19981;&#21516;&#20154;&#31867;&#35748;&#30693;&#27700;&#24179;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dominant paradigm for instruction tuning is the random-shuffled training of maximally diverse instruction-response pairs. This paper explores the potential benefits of applying a structured cognitive learning approach to instruction tuning in contemporary large language models like ChatGPT and GPT-4. Unlike the previous conventional randomized instruction dataset, we propose a highly structured synthetic dataset that mimics the progressive and organized nature of human education. We curate our dataset by aligning it with educational frameworks, incorporating meta information including its topic and cognitive rigor level for each sample. Our dataset covers comprehensive fine-grained topics spanning diverse educational stages (from middle school to graduate school) with various questions for each topic to enhance conceptual depth using Bloom's taxonomy-a classification framework distinguishing various levels of human cognition for each concept. The results demonstrate that this cogni
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GNN&#26550;&#26500;&#65292;&#36890;&#36807;&#36127;&#37319;&#26679;&#35825;&#23548;&#20102;&#27491;&#36793;&#21644;&#36127;&#36793;&#30340;&#27491;&#21521;&#20256;&#36882;&#65292;&#20197;&#26356;&#21152;&#28789;&#27963;&#32780;&#31283;&#23450;&#22320;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.09516</link><description>&lt;p&gt;
&#36890;&#36807;&#36127;&#37319;&#26679;&#35825;&#23548;&#30340;GNN&#23618;&#36827;&#34892;&#39640;&#25928;&#30340;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Efficient Link Prediction via GNN Layers Induced by Negative Sampling. (arXiv:2310.09516v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GNN&#26550;&#26500;&#65292;&#36890;&#36807;&#36127;&#37319;&#26679;&#35825;&#23548;&#20102;&#27491;&#36793;&#21644;&#36127;&#36793;&#30340;&#27491;&#21521;&#20256;&#36882;&#65292;&#20197;&#26356;&#21152;&#28789;&#27963;&#32780;&#31283;&#23450;&#22320;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#25509;&#39044;&#27979;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#21487;&#20197;&#22823;&#33268;&#20998;&#20026;&#20004;&#22823;&#31867;&#12290;&#31532;&#19968;&#31867;&#26159;&#22522;&#20110;&#33410;&#28857;&#30340;&#32467;&#26500;&#65292;&#20026;&#27599;&#20010;&#33410;&#28857;&#39044;&#20808;&#35745;&#31639;&#20010;&#20307;&#23884;&#20837;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#35299;&#30721;&#22120;&#36827;&#34892;&#32452;&#21512;&#20197;&#36827;&#34892;&#39044;&#27979;&#12290;&#23613;&#31649;&#22312;&#25512;&#29702;&#26102;&#38750;&#24120;&#39640;&#25928;&#65288;&#22240;&#20026;&#33410;&#28857;&#23884;&#20837;&#21482;&#35745;&#31639;&#19968;&#27425;&#24182;&#21453;&#22797;&#37325;&#29992;&#65289;&#65292;&#20294;&#27169;&#22411;&#34920;&#36798;&#33021;&#21147;&#26377;&#38480;&#65292;&#23548;&#33268;&#26080;&#27861;&#21306;&#20998;&#23545;&#20505;&#36873;&#36793;&#26377;&#36129;&#29486;&#30340;&#21516;&#26500;&#33410;&#28857;&#65292;&#20174;&#32780;&#24433;&#21709;&#20934;&#30830;&#24615;&#12290;&#19982;&#20043;&#30456;&#21453;&#65292;&#31532;&#20108;&#31867;&#26041;&#27861;&#21017;&#20381;&#36182;&#20110;&#24418;&#25104;&#38024;&#23545;&#27599;&#20010;&#36793;&#30340;&#23376;&#22270;&#23884;&#20837;&#65292;&#20197;&#20016;&#23500;&#20004;&#20004;&#20851;&#31995;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#28040;&#38500;&#21516;&#26500;&#33410;&#28857;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#20195;&#20215;&#26159;&#22686;&#21152;&#20102;&#27169;&#22411;&#22797;&#26434;&#24230;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#26435;&#34913;&#36825;&#20010;&#21462;&#33293;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GNN&#26550;&#26500;&#65292;&#20854;&#20013;&#30340;&#27491;&#21521;&#20256;&#36882;&#26126;&#30830;&#20381;&#36182;&#20110;&#27491;&#36793;&#65288;&#36890;&#24120;&#24773;&#20917;&#19979;&#65289;&#21644;&#36127;&#36793;&#65288;&#25105;&#20204;&#26041;&#27861;&#30340;&#29420;&#29305;&#20043;&#22788;&#65289;&#65292;&#20197;&#25552;&#20379;&#26356;&#28789;&#27963;&#20294;&#20173;&#31283;&#23450;&#30340;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) for link prediction can loosely be divided into two broad categories. First, \emph{node-wise} architectures pre-compute individual embeddings for each node that are later combined by a simple decoder to make predictions. While extremely efficient at inference time (since node embeddings are only computed once and repeatedly reused), model expressiveness is limited such that isomorphic nodes contributing to candidate edges may not be distinguishable, compromising accuracy. In contrast, \emph{edge-wise} methods rely on the formation of edge-specific subgraph embeddings to enrich the representation of pair-wise relationships, disambiguating isomorphic nodes to improve accuracy, but with the cost of increased model complexity. To better navigate this trade-off, we propose a novel GNN architecture whereby the \emph{forward pass} explicitly depends on \emph{both} positive (as is typical) and negative (unique to our approach) edges to inform more flexible, yet sti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24191;&#20041;&#38750;&#21512;&#20316;&#21338;&#24328;&#30340;&#22312;&#32447;&#21442;&#25968;&#20272;&#35745;&#38382;&#39064;&#65292;&#20351;&#29992;&#35266;&#27979;&#21040;&#30340;&#25968;&#25454;&#35782;&#21035;&#26410;&#30693;&#21442;&#25968;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#21442;&#25968;&#35782;&#21035;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.09511</link><description>&lt;p&gt;
&#24191;&#20041;&#38750;&#21512;&#20316;&#21338;&#24328;&#30340;&#22312;&#32447;&#21442;&#25968;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Online Parameter Identification of Generalized Non-cooperative Game. (arXiv:2310.09511v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24191;&#20041;&#38750;&#21512;&#20316;&#21338;&#24328;&#30340;&#22312;&#32447;&#21442;&#25968;&#20272;&#35745;&#38382;&#39064;&#65292;&#20351;&#29992;&#35266;&#27979;&#21040;&#30340;&#25968;&#25454;&#35782;&#21035;&#26410;&#30693;&#21442;&#25968;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#21442;&#25968;&#35782;&#21035;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24191;&#20041;&#38750;&#21512;&#20316;&#21338;&#24328;&#30340;&#21442;&#25968;&#20272;&#35745;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#20010;&#29609;&#23478;&#30340;&#25104;&#26412;&#20989;&#25968;&#21463;&#21487;&#35266;&#27979;&#20449;&#21495;&#21644;&#19968;&#20123;&#26410;&#30693;&#21442;&#25968;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#26576;&#20123;&#21487;&#35266;&#27979;&#20449;&#21495;&#19978;&#30340;&#24179;&#34913;&#21487;&#20197;&#34987;&#24102;&#26377;&#22122;&#22768;&#30340;&#35266;&#27979;&#21040;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#35266;&#27979;&#25968;&#25454;&#35782;&#21035;&#26410;&#30693;&#21442;&#25968;&#12290;&#20551;&#35774;&#21487;&#35266;&#27979;&#20449;&#21495;&#21644;&#30456;&#24212;&#30340;&#24102;&#26377;&#22122;&#22768;&#30340;&#24179;&#34913;&#26159;&#25353;&#39034;&#24207;&#33719;&#24471;&#30340;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#21442;&#25968;&#35782;&#21035;&#38382;&#39064;&#26500;&#36896;&#20026;&#22312;&#32447;&#20248;&#21270;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#21442;&#25968;&#35782;&#21035;&#31639;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26500;&#36896;&#20102;&#19968;&#20010;&#24179;&#34913;&#20102;&#20445;&#23432;&#24615;&#21644;&#27491;&#30830;&#24615;&#30340;&#27491;&#21017;&#21270;&#25439;&#22833;&#20989;&#25968;&#65292;&#20854;&#20013;&#20445;&#23432;&#24615;&#39033;&#30830;&#20445;&#26032;&#20272;&#35745;&#20540;&#19982;&#24403;&#21069;&#20272;&#35745;&#20540;&#30340;&#20559;&#24046;&#19981;&#22823;&#65292;&#32780;&#27491;&#30830;&#24615;&#39033;&#30001;Karush-Kuhn-Tucker&#26465;&#20214;&#25429;&#25417;&#12290;&#28982;&#21518;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#29609;&#23478;&#30340;&#25104;&#26412;&#20989;&#25968;...
&lt;/p&gt;
&lt;p&gt;
This work studies the parameter identification problem of a generalized non-cooperative game, where each player's cost function is influenced by an observable signal and some unknown parameters. We consider the scenario where equilibrium of the game at some observable signals can be observed with noises, whereas our goal is to identify the unknown parameters with the observed data. Assuming that the observable signals and the corresponding noise-corrupted equilibriums are acquired sequentially, we construct this parameter identification problem as online optimization and introduce a novel online parameter identification algorithm. To be specific, we construct a regularized loss function that balances conservativeness and correctiveness, where the conservativeness term ensures that the new estimates do not deviate significantly from the current estimates, while the correctiveness term is captured by the Karush-Kuhn-Tucker conditions. We then prove that when the players' cost functions a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39537;&#21160;MAC&#21327;&#35758;&#20998;&#31867;&#65292;&#21253;&#25324;&#19977;&#20010;&#32423;&#21035;&#65306;&#38754;&#21521;&#20219;&#21153;&#30340;&#31070;&#32463;&#21327;&#35758;&#65292;&#31070;&#32463;&#32593;&#32476;&#23548;&#21521;&#30340;&#31526;&#21495;&#21327;&#35758;&#21644;&#38754;&#21521;&#35821;&#35328;&#30340;&#35821;&#20041;&#21327;&#35758;&#12290;&#35813;&#20998;&#31867;&#26088;&#22312;&#25506;&#32034;&#27599;&#20010;&#32423;&#21035;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#24182;&#28145;&#20837;&#30740;&#31350;&#23427;&#20204;&#30340;&#22522;&#30784;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2310.09506</link><description>&lt;p&gt;
&#38754;&#21521;6G&#30340;&#35821;&#20041;&#36890;&#20449;&#21327;&#35758;&#65306;&#20174;&#21327;&#35758;&#23398;&#20064;&#21040;&#38754;&#21521;&#35821;&#35328;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Semantic Communication Protocols for 6G: From Protocol Learning to Language-Oriented Approaches. (arXiv:2310.09506v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39537;&#21160;MAC&#21327;&#35758;&#20998;&#31867;&#65292;&#21253;&#25324;&#19977;&#20010;&#32423;&#21035;&#65306;&#38754;&#21521;&#20219;&#21153;&#30340;&#31070;&#32463;&#21327;&#35758;&#65292;&#31070;&#32463;&#32593;&#32476;&#23548;&#21521;&#30340;&#31526;&#21495;&#21327;&#35758;&#21644;&#38754;&#21521;&#35821;&#35328;&#30340;&#35821;&#20041;&#21327;&#35758;&#12290;&#35813;&#20998;&#31867;&#26088;&#22312;&#25506;&#32034;&#27599;&#20010;&#32423;&#21035;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#24182;&#28145;&#20837;&#30740;&#31350;&#23427;&#20204;&#30340;&#22522;&#30784;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35745;&#21363;&#23558;&#21040;&#26469;&#30340;6G&#31995;&#32479;&#23558;&#38754;&#20020;&#21508;&#31181;&#38750;&#38745;&#24577;&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;&#36825;&#32473;&#20256;&#32479;&#30340;&#23186;&#20171;&#35775;&#38382;&#25511;&#21046;&#65288;MAC&#65289;&#21327;&#35758;&#24102;&#26469;&#20102;&#22256;&#25200;&#65292;&#22240;&#20026;MAC&#21327;&#35758;&#26159;&#38745;&#24577;&#21644;&#39044;&#23450;&#20041;&#30340;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#26368;&#36817;&#20986;&#29616;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;MAC&#21327;&#35758;&#65292;&#21487;&#20197;&#26681;&#25454;&#29305;&#23450;&#20219;&#21153;&#23450;&#21046;&#20449;&#20196;&#28040;&#24687;&#12290;&#26412;&#25991;&#23558;&#36825;&#20123;&#25968;&#25454;&#39537;&#21160;&#30340;MAC&#21327;&#35758;&#20998;&#20026;&#19977;&#20010;&#32423;&#21035;&#36827;&#34892;&#20102;&#26032;&#39062;&#30340;&#20998;&#31867;&#65306;&#31532;1&#32423;MAC&#65306;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;MADRL&#65289;&#26500;&#24314;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#31070;&#32463;&#21327;&#35758;&#65307;&#31532;2&#32423;MAC&#65306;&#36890;&#36807;&#23558;&#31532;1&#32423;MAC&#30340;&#36755;&#20986;&#36716;&#25442;&#20026;&#26174;&#24335;&#31526;&#21495;&#24320;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#23548;&#21521;&#30340;&#31526;&#21495;&#21327;&#35758;&#65307;&#31532;3&#32423;MAC&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#38754;&#21521;&#35821;&#35328;&#30340;&#35821;&#20041;&#21327;&#35758;&#12290;&#36890;&#36807;&#36825;&#31181;&#20998;&#31867;&#65292;&#25105;&#20204;&#26088;&#22312;&#28145;&#20837;&#25506;&#35752;&#27599;&#20010;&#32423;&#21035;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#24182;&#25506;&#32034;&#23427;&#20204;&#30340;&#22522;&#30784;&#25216;&#26415;&#12290;&#20174;&#20449;&#24687;&#35770;&#21644;&#30456;&#20851;&#21407;&#29702;&#20197;&#21450;&#36873;&#23450;&#30340;&#26696;&#20363;&#20013;&#27762;&#21462;&#32463;&#39564;&#65292;
&lt;/p&gt;
&lt;p&gt;
The forthcoming 6G systems are expected to address a wide range of non-stationary tasks. This poses challenges to traditional medium access control (MAC) protocols that are static and predefined. In response, data-driven MAC protocols have recently emerged, offering ability to tailor their signaling messages for specific tasks. This article presents a novel categorization of these data-driven MAC protocols into three levels: Level 1 MAC. task-oriented neural protocols constructed using multi-agent deep reinforcement learning (MADRL); Level 2 MAC. neural network-oriented symbolic protocols developed by converting Level 1 MAC outputs into explicit symbols; and Level 3 MAC. language-oriented semantic protocols harnessing large language models (LLMs) and generative models. With this categorization, we aim to explore the opportunities and challenges of each level by delving into their foundational techniques. Drawing from information theory and associated principles as well as selected case
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22768;&#23398;&#22522;&#30784;&#27169;&#22411;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24320;&#25918;&#19990;&#30028;&#25968;&#25454;&#36716;&#25442;&#20013;&#30340;&#20998;&#24067;&#21464;&#21270;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22122;&#22768;&#36739;&#22823;&#30340;&#35821;&#38899;&#24103;&#21253;&#21547;&#37325;&#35201;&#30340;&#35821;&#20041;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2310.09505</link><description>&lt;p&gt;
&#22312;&#24320;&#25918;&#19990;&#30028;&#36716;&#25442;&#20013;&#25512;&#36827;&#22768;&#23398;&#22522;&#30784;&#27169;&#22411;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Advancing Test-Time Adaptation for Acoustic Foundation Models in Open-World Shifts. (arXiv:2310.09505v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22768;&#23398;&#22522;&#30784;&#27169;&#22411;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24320;&#25918;&#19990;&#30028;&#25968;&#25454;&#36716;&#25442;&#20013;&#30340;&#20998;&#24067;&#21464;&#21270;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22122;&#22768;&#36739;&#22823;&#30340;&#35821;&#38899;&#24103;&#21253;&#21547;&#37325;&#35201;&#30340;&#35821;&#20041;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;TTA&#65289;&#26159;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#35299;&#20915;&#20998;&#24067;&#36716;&#25442;&#38382;&#39064;&#30340;&#20851;&#38190;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#22768;&#23398;&#27169;&#22411;&#22312;&#27979;&#35797;&#26102;&#38388;&#30340;&#35821;&#38899;&#20998;&#24067;&#36716;&#25442;&#20013;&#38754;&#20020;&#30456;&#20284;&#30340;&#25361;&#25112;&#65292;&#20294;&#38024;&#23545;&#22768;&#23398;&#24314;&#27169;&#22312;&#24320;&#25918;&#19990;&#30028;&#25968;&#25454;&#36716;&#25442;&#29615;&#22659;&#19979;&#30340;TTA&#25216;&#26415;&#20173;&#28982;&#24456;&#23569;&#35265;&#12290;&#32771;&#34385;&#21040;&#22768;&#23398;&#22522;&#30784;&#27169;&#22411;&#30340;&#29305;&#28857;&#65306;1&#65289;&#23427;&#20204;&#20027;&#35201;&#26159;&#22522;&#20110;&#20855;&#26377;&#23618;&#24402;&#19968;&#21270;&#30340;&#21464;&#21387;&#22120;&#26550;&#26500;&#26500;&#24314;&#30340;&#65307;2&#65289;&#23427;&#20204;&#20197;&#19968;&#31181;&#38750;&#38745;&#24577;&#30340;&#26041;&#24335;&#22788;&#29702;&#38271;&#24230;&#19981;&#21516;&#30340;&#27979;&#35797;&#26102;&#38388;&#35821;&#38899;&#25968;&#25454;&#12290;&#36825;&#20123;&#22240;&#32032;&#20351;&#24471;&#22312;&#35270;&#35273;&#32858;&#28966;&#30340;TTA&#26041;&#27861;&#30340;&#30452;&#25509;&#24212;&#29992;&#21464;&#24471;&#19981;&#21487;&#34892;&#65292;&#36825;&#20123;&#26041;&#27861;&#22823;&#22810;&#20381;&#36182;&#20110;&#25209;&#24402;&#19968;&#21270;&#24182;&#20551;&#35774;&#29420;&#31435;&#26679;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#38754;&#20020;&#24320;&#25918;&#19990;&#30028;&#25968;&#25454;&#36716;&#25442;&#30340;&#39044;&#35757;&#32451;&#22768;&#23398;&#27169;&#22411;&#30340;TTA&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22122;&#22768;&#36739;&#22823;&#12289;&#29109;&#36739;&#39640;&#30340;&#35821;&#38899;&#24103;&#36890;&#24120;&#24102;&#26377;&#20851;&#38190;&#30340;&#35821;&#20041;&#20869;&#23481;&#12290;&#20256;&#32479;&#30340;&#35270;&#35273;TTA&#26041;&#27861;&#30340;&#30452;&#25509;&#24212;&#29992;&#22312;&#22768;&#23398;&#24314;&#27169;&#20013;&#24182;&#19981;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Test-Time Adaptation (TTA) is a critical paradigm for tackling distribution shifts during inference, especially in visual recognition tasks. However, while acoustic models face similar challenges due to distribution shifts in test-time speech, TTA techniques specifically designed for acoustic modeling in the context of open-world data shifts remain scarce. This gap is further exacerbated when considering the unique characteristics of acoustic foundation models: 1) they are primarily built on transformer architectures with layer normalization and 2) they deal with test-time speech data of varying lengths in a non-stationary manner. These aspects make the direct application of vision-focused TTA methods, which are mostly reliant on batch normalization and assume independent samples, infeasible. In this paper, we delve into TTA for pre-trained acoustic models facing open-world data shifts. We find that noisy, high-entropy speech frames, often non-silent, carry key semantic content. Tradit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#22270;&#20687;&#21160;&#24577;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#28508;&#22312;&#21160;&#24577;&#20272;&#35745;&#22270;&#20687;&#28436;&#21464;&#30340;&#20013;&#38388;&#38454;&#27573;&#65292;&#20174;&#32780;&#23454;&#29616;&#35299;&#37322;&#24615;&#65292;&#24182;&#20445;&#30041;&#19982;&#22270;&#20687;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#36981;&#24490;&#29289;&#29702;&#27169;&#22411;&#30340;&#28508;&#22312;&#21464;&#37327;&#65292;&#30830;&#20445;&#20102;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#22320;&#29699;&#31185;&#23398;&#22270;&#20687;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#20854;&#40065;&#26834;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.09495</link><description>&lt;p&gt;
&#36890;&#36807;&#29289;&#29702;&#28508;&#22312;&#31354;&#38388;&#23398;&#20064;&#22270;&#20687;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Learning In-between Imagery Dynamics via Physical Latent Spaces. (arXiv:2310.09495v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#22270;&#20687;&#21160;&#24577;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#28508;&#22312;&#21160;&#24577;&#20272;&#35745;&#22270;&#20687;&#28436;&#21464;&#30340;&#20013;&#38388;&#38454;&#27573;&#65292;&#20174;&#32780;&#23454;&#29616;&#35299;&#37322;&#24615;&#65292;&#24182;&#20445;&#30041;&#19982;&#22270;&#20687;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#36981;&#24490;&#29289;&#29702;&#27169;&#22411;&#30340;&#28508;&#22312;&#21464;&#37327;&#65292;&#30830;&#20445;&#20102;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#22320;&#29699;&#31185;&#23398;&#22270;&#20687;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#20854;&#40065;&#26834;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#23398;&#20064;&#22312;&#36830;&#32493;&#26102;&#38388;&#27493;&#39588;&#20013;&#35266;&#23519;&#21040;&#30340;&#20004;&#20010;&#22270;&#20687;&#20043;&#38388;&#30340;&#24213;&#23618;&#21160;&#24577;&#12290;&#22270;&#20687;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21644;&#32570;&#20047;&#26102;&#38388;&#20449;&#24687;&#23548;&#33268;&#22312;&#25429;&#25417;&#29420;&#29305;&#30340;&#28436;&#21464;&#27169;&#24335;&#26102;&#23384;&#22312;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19987;&#27880;&#20110;&#20272;&#35745;&#22270;&#20687;&#28436;&#21464;&#30340;&#20013;&#38388;&#38454;&#27573;&#65292;&#36890;&#36807;&#28508;&#22312;&#21160;&#24577;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#19982;&#22270;&#20687;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#23558;&#36981;&#24490;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#30340;&#29289;&#29702;&#27169;&#22411;&#34920;&#36798;&#30340;&#28508;&#22312;&#21464;&#37327;&#32435;&#20837;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30830;&#20445;&#20102;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#24212;&#30340;&#22270;&#20687;&#21160;&#24577;&#30340;&#27934;&#23519;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#20351;&#29992;&#22320;&#29699;&#31185;&#23398;&#22270;&#20687;&#25968;&#25454;&#30340;&#25968;&#20540;&#27979;&#35797;&#35777;&#26126;&#20102;&#25105;&#20204;&#23398;&#20064;&#26694;&#26550;&#30340;&#31283;&#20581;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a framework designed to learn the underlying dynamics between two images observed at consecutive time steps. The complex nature of image data and the lack of temporal information pose significant challenges in capturing the unique evolving patterns. Our proposed method focuses on estimating the intermediary stages of image evolution, allowing for interpretability through latent dynamics while preserving spatial correlations with the image. By incorporating a latent variable that follows a physical model expressed in partial differential equations (PDEs), our approach ensures the interpretability of the learned model and provides insight into corresponding image dynamics. We demonstrate the robustness and effectiveness of our learning framework through a series of numerical tests using geoscientific imagery data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;ARM&#65292;&#19968;&#31181;&#22810;&#21464;&#37327;&#30340;&#26102;&#38388;-&#19978;&#19979;&#25991;&#33258;&#36866;&#24212;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;ARM&#36890;&#36807;&#37319;&#29992;&#33258;&#36866;&#24212;&#21333;&#21464;&#37327;&#25928;&#24212;&#23398;&#20064;&#12289;&#38543;&#26426;&#20002;&#24323;&#35757;&#32451;&#31574;&#30053;&#21644;&#22810;&#26680;&#23616;&#37096;&#24179;&#28369;&#65292;&#33021;&#26356;&#22909;&#22320;&#22788;&#29702;&#26102;&#38388;&#27169;&#24335;&#21644;&#23398;&#20064;&#31995;&#21015;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;ARM&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#32780;&#35745;&#31639;&#25104;&#26412;&#30456;&#23545;&#36739;&#20302;&#12290;</title><link>http://arxiv.org/abs/2310.09488</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#36866;&#24212;&#26102;&#38388;-&#19978;&#19979;&#25991;&#23398;&#20064;&#20248;&#21270;&#22810;&#21464;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
ARM: Refining Multivariate Forecasting with Adaptive Temporal-Contextual Learning. (arXiv:2310.09488v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;ARM&#65292;&#19968;&#31181;&#22810;&#21464;&#37327;&#30340;&#26102;&#38388;-&#19978;&#19979;&#25991;&#33258;&#36866;&#24212;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;ARM&#36890;&#36807;&#37319;&#29992;&#33258;&#36866;&#24212;&#21333;&#21464;&#37327;&#25928;&#24212;&#23398;&#20064;&#12289;&#38543;&#26426;&#20002;&#24323;&#35757;&#32451;&#31574;&#30053;&#21644;&#22810;&#26680;&#23616;&#37096;&#24179;&#28369;&#65292;&#33021;&#26356;&#22909;&#22320;&#22788;&#29702;&#26102;&#38388;&#27169;&#24335;&#21644;&#23398;&#20064;&#31995;&#21015;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;ARM&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#32780;&#35745;&#31639;&#25104;&#26412;&#30456;&#23545;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;LTSF&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#37117;&#24456;&#37325;&#35201;&#65292;&#20294;&#22312;&#22788;&#29702;&#22797;&#26434;&#30340;&#26102;&#38388;-&#19978;&#19979;&#25991;&#20851;&#31995;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#30001;&#20110;&#22810;&#21464;&#37327;&#36755;&#20837;&#27169;&#22411;&#34920;&#29616;&#19981;&#22914;&#26368;&#36817;&#30340;&#19968;&#20123;&#21333;&#21464;&#37327;&#27169;&#22411;&#65292;&#25105;&#20204;&#35748;&#20026;&#38382;&#39064;&#22312;&#20110;&#29616;&#26377;&#30340;&#22810;&#21464;&#37327;LTSF&#21464;&#21387;&#22120;&#27169;&#22411;&#26080;&#27861;&#39640;&#25928;&#22320;&#24314;&#27169;&#31995;&#21015;&#20043;&#38388;&#30340;&#20851;&#31995;&#65306;&#24448;&#24448;&#19981;&#33021;&#27491;&#30830;&#22320;&#25429;&#25417;&#21040;&#31995;&#21015;&#20043;&#38388;&#30340;&#29305;&#24449;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ARM&#65306;&#19968;&#31181;&#22810;&#21464;&#37327;&#30340;&#26102;&#38388;-&#19978;&#19979;&#25991;&#33258;&#36866;&#24212;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#26159;&#19987;&#38376;&#20026;&#22810;&#21464;&#37327;LTSF&#24314;&#27169;&#32780;&#35774;&#35745;&#30340;&#22686;&#24378;&#22411;&#26550;&#26500;&#12290;ARM&#37319;&#29992;&#33258;&#36866;&#24212;&#21333;&#21464;&#37327;&#25928;&#24212;&#23398;&#20064;&#65288;AUEL&#65289;&#12289;&#38543;&#26426;&#20002;&#24323;&#65288;RD&#65289;&#35757;&#32451;&#31574;&#30053;&#21644;&#22810;&#26680;&#23616;&#37096;&#24179;&#28369;&#65288;MKLS&#65289;&#26469;&#26356;&#22909;&#22320;&#22788;&#29702;&#21333;&#20010;&#31995;&#21015;&#30340;&#26102;&#38388;&#27169;&#24335;&#24182;&#27491;&#30830;&#23398;&#20064;&#31995;&#21015;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;ARM&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#32780;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#24182;&#27809;&#26377;&#26174;&#33879;&#22686;&#21152;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-term time series forecasting (LTSF) is important for various domains but is confronted by challenges in handling the complex temporal-contextual relationships. As multivariate input models underperforming some recent univariate counterparts, we posit that the issue lies in the inefficiency of existing multivariate LTSF Transformers to model series-wise relationships: the characteristic differences between series are often captured incorrectly. To address this, we introduce ARM: a multivariate temporal-contextual adaptive learning method, which is an enhanced architecture specifically designed for multivariate LTSF modelling. ARM employs Adaptive Univariate Effect Learning (AUEL), Random Dropping (RD) training strategy, and Multi-kernel Local Smoothing (MKLS), to better handle individual series temporal patterns and correctly learn inter-series dependencies. ARM demonstrates superior performance on multiple benchmarks without significantly increasing computational costs compared to
&lt;/p&gt;</description></item><item><title>Mirage&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#33976;&#39311;&#31639;&#27861;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#26356;&#23567;&#30340;&#21512;&#25104;&#35757;&#32451;&#38598;&#65292;&#20197;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#23454;&#29616;&#22270;&#20998;&#31867;&#65292;&#24182;&#20811;&#26381;&#20102;&#29616;&#26377;&#31639;&#27861;&#23545;&#23436;&#25972;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#24615;&#21644;&#23545;&#24314;&#27169;&#27969;&#31243;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.09486</link><description>&lt;p&gt;
Mirage: &#22270;&#20998;&#31867;&#30340;&#27169;&#22411;&#26080;&#20851;&#22270;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Mirage: Model-Agnostic Graph Distillation for Graph Classification. (arXiv:2310.09486v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09486
&lt;/p&gt;
&lt;p&gt;
Mirage&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#33976;&#39311;&#31639;&#27861;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#26356;&#23567;&#30340;&#21512;&#25104;&#35757;&#32451;&#38598;&#65292;&#20197;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#23454;&#29616;&#22270;&#20998;&#31867;&#65292;&#24182;&#20811;&#26381;&#20102;&#29616;&#26377;&#31639;&#27861;&#23545;&#23436;&#25972;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#24615;&#21644;&#23545;&#24314;&#27169;&#27969;&#31243;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GNNs&#21644;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19968;&#26679;&#65292;&#23545;&#25968;&#25454;&#21644;&#35745;&#31639;&#38656;&#27714;&#37327;&#24456;&#22823;&#12290;&#24613;&#38656;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#25193;&#23637;GNN&#30340;&#35757;&#32451;&#65292;&#20197;&#20415;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#20351;&#29992;&#23427;&#20204;&#12290;&#22270;&#33976;&#39311;&#26159;&#20026;&#27492;&#30446;&#30340;&#32780;&#21162;&#21147;&#65292;&#26088;&#22312;&#20174;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#26500;&#24314;&#19968;&#20010;&#26356;&#23567;&#30340;&#21512;&#25104;&#35757;&#32451;&#38598;&#65292;&#32780;&#19981;&#20250;&#26174;&#33879;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#34429;&#28982;&#21021;&#27493;&#24037;&#20316;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#22522;&#20110;&#20004;&#20010;&#20851;&#38190;&#35266;&#23519;&#65306;(1)&#29616;&#26377;&#30340;&#22270;&#33976;&#39311;&#31639;&#27861;&#26412;&#36523;&#20381;&#36182;&#20110;&#20351;&#29992;&#23436;&#25972;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#23601;&#30772;&#22351;&#20102;&#22270;&#33976;&#39311;&#30340;&#21069;&#25552;&#12290;(2)&#33976;&#39311;&#36807;&#31243;&#23545;&#30446;&#26631;GNN&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#20855;&#26377;&#29305;&#24322;&#24615;&#65292;&#22240;&#27492;&#23545;&#24314;&#27169;&#27969;&#31243;&#30340;&#21464;&#21270;&#19981;&#20855;&#22791;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#21517;&#20026;Mirage&#30340;&#22270;&#20998;&#31867;&#33976;&#39311;&#31639;&#27861;&#26469;&#36991;&#20813;&#36825;&#20123;&#38480;&#21046;&#12290;Mirage&#24314;&#31435;&#22312;&#19968;&#20010;&#27934;&#23519;&#30340;&#22522;&#30784;&#19978;&#65292;&#21363;&#19968;&#20010;&#28040;&#24687;&#20256;&#36882;&#30340;GNN&#23558;&#36755;&#20837;&#22270;&#20998;&#35299;&#20026;&#35745;&#31639;&#30340;&#22810;&#37325;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
GNNs, like other deep learning models, are data and computation hungry. There is a pressing need to scale training of GNNs on large datasets to enable their usage on low-resource environments. Graph distillation is an effort in that direction with the aim to construct a smaller synthetic training set from the original training data without significantly compromising model performance. While initial efforts are promising, this work is motivated by two key observations: (1) Existing graph distillation algorithms themselves rely on training with the full dataset, which undermines the very premise of graph distillation. (2) The distillation process is specific to the target GNN architecture and hyper-parameters and thus not robust to changes in the modeling pipeline. We circumvent these limitations by designing a distillation algorithm called Mirage for graph classification. Mirage is built on the insight that a message-passing GNN decomposes the input graph into a multiset of computation 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36125;&#21494;&#26031;&#23725;&#22238;&#24402;&#22312;&#30149;&#27602;&#20005;&#37325;&#31243;&#24230;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#36739;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#22312;&#25968;&#25454;&#32452;&#32455;&#26041;&#38754;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#65292;&#20005;&#37325;&#31243;&#24230;&#25351;&#25968;&#26377;&#21161;&#20110;&#33719;&#24471;&#24739;&#32773;&#25252;&#29702;&#38656;&#27714;&#30340;&#24191;&#27867;&#27010;&#36848;&#12290;</title><link>http://arxiv.org/abs/2310.09485</link><description>&lt;p&gt;
&#24212;&#29992;&#36125;&#21494;&#26031;&#23725;&#22238;&#24402;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#30149;&#27602;&#20005;&#37325;&#31243;&#24230;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Applying Bayesian Ridge Regression AI Modeling in Virus Severity Prediction. (arXiv:2310.09485v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36125;&#21494;&#26031;&#23725;&#22238;&#24402;&#22312;&#30149;&#27602;&#20005;&#37325;&#31243;&#24230;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#36739;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#22312;&#25968;&#25454;&#32452;&#32455;&#26041;&#38754;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#65292;&#20005;&#37325;&#31243;&#24230;&#25351;&#25968;&#26377;&#21161;&#20110;&#33719;&#24471;&#24739;&#32773;&#25252;&#29702;&#38656;&#27714;&#30340;&#24191;&#27867;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#26159;&#37325;&#22609;&#21307;&#30103;&#31995;&#32479;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#22312;&#21307;&#30103;&#39046;&#22495;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#25968;&#25454;&#22788;&#29702;&#33021;&#21147;&#26080;&#20215;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#21644;&#24555;&#36895;&#30340;&#35786;&#26029;&#65292;&#20174;&#32780;&#20943;&#36731;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#30340;&#24037;&#20316;&#36127;&#25285;&#12290;&#22240;&#27492;&#65292;&#20154;&#24037;&#26234;&#33021;&#22312;&#21508;&#20010;&#34892;&#19994;&#37117;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#31616;&#21270;&#20102;&#22797;&#26434;&#30340;&#20219;&#21153;&#21644;&#27169;&#24335;&#35782;&#21035;&#65292;&#36825;&#20123;&#23545;&#20154;&#31867;&#25110;&#20256;&#32479;&#30340;&#35745;&#31639;&#26426;&#31639;&#27861;&#26469;&#35828;&#26412;&#26469;&#26159;&#38590;&#20197;&#25215;&#21463;&#30340;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#36125;&#21494;&#26031;&#23725;&#22238;&#24402;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#20197;&#23558;&#23574;&#31471;&#30149;&#27602;&#20998;&#26512;&#24102;&#32473;&#20840;&#29699;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#12290;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#35780;&#20272;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#25968;&#25454;&#32452;&#32455;&#26041;&#38754;&#36824;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#20005;&#37325;&#31243;&#24230;&#25351;&#25968;&#20316;&#20026;&#19968;&#31181;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#33719;&#24471;&#20851;&#20110;&#24739;&#32773;&#25252;&#29702;&#38656;&#27714;&#30340;&#24191;&#27867;&#27010;&#36848;&#65292;&#31526;&#21512;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#23545;&#26356;&#24191;&#27867;&#20998;&#31867;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) is a powerful tool for reshaping healthcare systems. In healthcare, AI is invaluable for its capacity to manage vast amounts of data, which can lead to more accurate and speedy diagnoses, ultimately easing the workload on healthcare professionals. As a result, AI has proven itself to be a power tool across various industries, simplifying complex tasks and pattern recognition that would otherwise be overwhelming for humans or traditional computer algorithms. In this paper, we review the strengths and weaknesses of Bayesian Ridge Regression, an AI model that can be used to bring cutting edge virus analysis to healthcare professionals around the world. The model's accuracy assessment revealed promising results, with room for improvement primarily related to data organization. In addition, the severity index serves as a valuable tool to gain a broad overview of patient care needs, aligning with healthcare professionals' preference for broader categorizations.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#38754;&#21521;&#20154;&#33080;&#21464;&#24418;&#30340;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#30740;&#31350;&#20102;&#37319;&#26679;&#31639;&#27861;&#12289;&#36870;&#21521;DDIM&#27714;&#35299;&#22120;&#21644;&#37096;&#20998;&#37319;&#26679;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.09484</link><description>&lt;p&gt;
&#25506;&#32034;&#38754;&#21521;&#20154;&#33080;&#21464;&#24418;&#30340;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#30340;&#35774;&#35745;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Exploring the Design Space of Diffusion Autoencoders for Face Morphing. (arXiv:2310.09484v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09484
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#38754;&#21521;&#20154;&#33080;&#21464;&#24418;&#30340;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#30740;&#31350;&#20102;&#37319;&#26679;&#31639;&#27861;&#12289;&#36870;&#21521;DDIM&#27714;&#35299;&#22120;&#21644;&#37096;&#20998;&#37319;&#26679;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#21019;&#24314;&#30340;&#20154;&#33080;&#21464;&#24418;&#26159;&#19968;&#31181;&#26368;&#36817;&#30340;&#21019;&#26032;&#65292;&#32780;&#36825;&#31181;&#26041;&#27861;&#30340;&#35774;&#35745;&#31354;&#38388;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#35774;&#35745;&#31354;&#38388;&#30340;&#19977;&#20010;&#26041;&#38754;&#65292;&#21363;1&#65289;&#37319;&#26679;&#31639;&#27861;&#65292;2&#65289;&#36870;&#21521;DDIM&#27714;&#35299;&#22120;&#65292;&#20197;&#21450;3&#65289;&#36890;&#36807;&#28155;&#21152;&#23569;&#37327;&#22122;&#22768;&#36827;&#34892;&#37096;&#20998;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face morphs created by Diffusion Autoencoders are a recent innovation and the design space of such an approach has not been well explored. We explore three axes of the design space, i.e., 1) sampling algorithms, 2) the reverse DDIM solver, and 3) partial sampling through small amounts of added noise.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#35782;&#21035;&#21644;&#20998;&#31867;&#20154;&#31867;&#38754;&#37096;&#34920;&#24773;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#35777;&#26126;&#27169;&#22411;&#22312;&#20998;&#31867;&#19977;&#31181;&#24773;&#32490;&#31867;&#21035;&#26102;&#34920;&#29616;&#20986;&#20102;&#20248;&#20110;&#38543;&#26426;&#29468;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.09473</link><description>&lt;p&gt;
&#33021;&#21542;&#20934;&#30830;&#20998;&#31867;&#20154;&#31867;&#24773;&#32490;&#65311;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#30340;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can CNNs Accurately Classify Human Emotions? A Deep-Learning Facial Expression Recognition Study. (arXiv:2310.09473v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09473
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#35782;&#21035;&#21644;&#20998;&#31867;&#20154;&#31867;&#38754;&#37096;&#34920;&#24773;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#35777;&#26126;&#27169;&#22411;&#22312;&#20998;&#31867;&#19977;&#31181;&#24773;&#32490;&#31867;&#21035;&#26102;&#34920;&#29616;&#20986;&#20102;&#20248;&#20110;&#38543;&#26426;&#29468;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#32490;&#20154;&#24037;&#26234;&#33021;&#24403;&#21069;&#26159;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#26368;&#21463;&#26399;&#24453;&#30340;&#21457;&#23637;&#20043;&#19968;&#12290;&#22914;&#26524;&#25104;&#21151;&#65292;&#36825;&#20123;&#20154;&#24037;&#26234;&#33021;&#23558;&#34987;&#24402;&#31867;&#20026;&#26368;&#22797;&#26434;&#12289;&#26368;&#26234;&#33021;&#30340;&#38750;&#20154;&#31867;&#23454;&#20307;&#65292;&#22240;&#20026;&#23427;&#20204;&#23558;&#20855;&#22791;&#24863;&#30693;&#33021;&#21147;&#65292;&#36825;&#26159;&#21306;&#20998;&#29983;&#29289;&#20154;&#31867;&#21644;&#26426;&#26800;&#26426;&#22120;&#20154;&#30340;&#20027;&#35201;&#22240;&#32032;&#20043;&#19968;&#12290;&#20026;&#20102;&#34987;&#24402;&#31867;&#20026;&#8220;&#26377;&#24773;&#24863;&#8221;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#23427;&#20204;&#24212;&#35813;&#33021;&#22815;&#19982;&#20182;&#20154;&#20135;&#29983;&#20849;&#24773;&#24182;&#20998;&#31867;&#20182;&#20204;&#30340;&#24773;&#32490;&#65292;&#22240;&#20026;&#27809;&#26377;&#36825;&#20123;&#33021;&#21147;&#65292;&#23427;&#20204;&#26080;&#27861;&#27491;&#24120;&#19982;&#20154;&#31867;&#20114;&#21160;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#35782;&#21035;&#21644;&#20998;&#31867;&#20154;&#31867;&#38754;&#37096;&#34920;&#24773;&#65288;&#31215;&#26497;&#12289;&#20013;&#24615;&#12289;&#28040;&#26497;&#65289;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;Python&#32534;&#20889;&#20102;CNN&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#33437;&#21152;&#21733;&#38754;&#37096;&#25968;&#25454;&#24211;&#30340;&#39044;&#22788;&#29702;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#27169;&#22411;&#25925;&#24847;&#35774;&#35745;&#24471;&#36739;&#31616;&#21333;&#65292;&#20197;&#36827;&#19968;&#27493;&#30740;&#31350;&#20854;&#33021;&#21147;&#12290;&#25105;&#20204;&#20551;&#35774;&#35813;&#27169;&#22411;&#22312;&#20998;&#31867;&#36755;&#20837;&#25968;&#25454;&#30340;&#27599;&#20010;&#24773;&#24863;&#31867;&#21035;&#26102;&#23558;&#27604;&#38543;&#26426;&#29468;&#27979;&#65288;33.3%&#65289;&#34920;&#29616;&#26356;&#22909;&#12290;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#36890;&#36807;n&#36827;&#34892;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotional Artificial Intelligences are currently one of the most anticipated developments of AI. If successful, these AIs will be classified as one of the most complex, intelligent nonhuman entities as they will possess sentience, the primary factor that distinguishes living humans and mechanical machines. For AIs to be classified as "emotional," they should be able to empathize with others and classify their emotions because without such abilities they cannot normally interact with humans. This study investigates the CNN model's ability to recognize and classify human facial expressions (positive, neutral, negative). The CNN model made for this study is programmed in Python and trained with preprocessed data from the Chicago Face Database. The model is intentionally designed with less complexity to further investigate its ability. We hypothesized that the model will perform better than chance (33.3%) in classifying each emotion class of input data. The model accuracy was tested with n
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#38543;&#26426;&#22522;&#20934;&#27979;&#35797;&#27604;&#36739;&#20102;&#23616;&#37096;&#38646;&#38454;&#20248;&#21270;&#22120;&#22312;&#37327;&#23376;&#20248;&#21270;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#65292;&#20026;&#26410;&#26469;&#25913;&#36827;&#36825;&#20123;&#20248;&#21270;&#22120;&#25552;&#20379;&#20102;&#19968;&#20123;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.09468</link><description>&lt;p&gt;
&#29992;&#20110;&#21464;&#20998;&#37327;&#23376;&#31995;&#32479;&#30340;&#23616;&#37096;&#38646;&#38454;&#20248;&#21270;&#22120;&#30340;&#38543;&#26426;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Randomized Benchmarking of Local Zeroth-Order Optimizers for Variational Quantum Systems. (arXiv:2310.09468v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#38543;&#26426;&#22522;&#20934;&#27979;&#35797;&#27604;&#36739;&#20102;&#23616;&#37096;&#38646;&#38454;&#20248;&#21270;&#22120;&#22312;&#37327;&#23376;&#20248;&#21270;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#65292;&#20026;&#26410;&#26469;&#25913;&#36827;&#36825;&#20123;&#20248;&#21270;&#22120;&#25552;&#20379;&#20102;&#19968;&#20123;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37327;&#23376;&#20449;&#24687;&#39046;&#22495;&#20013;&#65292;&#32463;&#20856;&#20248;&#21270;&#22120;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20174;&#20248;&#21270;&#29289;&#29702;&#35774;&#22791;&#30340;&#23454;&#39564;&#32773;&#21040;&#25506;&#32034;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#30340;&#29702;&#35770;&#23478;&#65292;&#37327;&#23376;&#20449;&#24687;&#30340;&#35768;&#22810;&#26041;&#38754;&#37117;&#38656;&#35201;&#20351;&#29992;&#32463;&#20856;&#20248;&#21270;&#22120;&#12290;&#22240;&#27492;&#65292;&#26377;&#35768;&#22810;&#35770;&#25991;&#23545;&#19981;&#21516;&#20248;&#21270;&#22120;&#22312;&#29305;&#23450;&#37327;&#23376;&#20248;&#21270;&#20219;&#21153;&#21644;&#21442;&#25968;&#21270;&#31639;&#27861;&#36873;&#25321;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25506;&#32034;&#26032;&#31639;&#27861;&#25110;&#29289;&#29702;&#35774;&#22791;&#30340;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#65292;&#36825;&#20123;&#30740;&#31350;&#30340;&#35265;&#35299;&#19981;&#19968;&#23450;&#36866;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#37096;&#20998;&#38543;&#26426;&#21270;&#30340;&#20219;&#21153;&#26469;&#27604;&#36739;&#32463;&#20856;&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#65292;&#20197;&#26356;&#24191;&#27867;&#22320;&#25277;&#26679;&#37327;&#23376;&#20248;&#21270;&#38382;&#39064;&#30340;&#31354;&#38388;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#23616;&#37096;&#38646;&#38454;&#20248;&#21270;&#22120;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#37327;&#23376;&#31995;&#32479;&#19978;&#36890;&#24120;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#21644;&#26597;&#35810;&#25928;&#29575;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#20123;&#23454;&#39564;&#30340;&#35265;&#35299;&#65292;&#21487;&#20197;&#24110;&#21161;&#28608;&#21457;&#26410;&#26469;&#30340;&#24037;&#20316;&#26469;&#25913;&#36827;&#36825;&#20123;&#20248;&#21270;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of quantum information, classical optimizers play an important role. From experimentalists optimizing their physical devices to theorists exploring variational quantum algorithms, many aspects of quantum information require the use of a classical optimizer. For this reason, there are many papers that benchmark the effectiveness of different optimizers for specific quantum optimization tasks and choices of parameterized algorithms. However, for researchers exploring new algorithms or physical devices, the insights from these studies don't necessarily translate. To address this concern, we compare the performance of classical optimizers across a series of partially-randomized tasks to more broadly sample the space of quantum optimization problems. We focus on local zeroth-order optimizers due to their generally favorable performance and query-efficiency on quantum systems. We discuss insights from these experiments that can help motivate future works to improve these optimiz
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#20132;&#26131;&#31995;&#32479;&#26694;&#26550;CausalReinforceNet&#65292;&#36890;&#36807;&#22240;&#26524;&#20998;&#26512;&#22686;&#24378;&#20102;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#33021;&#21147;&#65292;&#20197;&#25552;&#39640;&#23545;&#21152;&#23494;&#36135;&#24065;&#24066;&#22330;&#30340;&#20132;&#26131;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.09462</link><description>&lt;p&gt;
&#19968;&#20010;&#36171;&#20104;&#22240;&#26524;&#20998;&#26512;&#33021;&#21147;&#30340;&#22686;&#24378;&#23398;&#20064;&#26234;&#33021;&#20195;&#29702;&#26694;&#26550;&#65306;&#22686;&#24378;&#33258;&#21160;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;
&lt;/p&gt;
&lt;p&gt;
A Framework for Empowering Reinforcement Learning Agents with Causal Analysis: Enhancing Automated Cryptocurrency Trading. (arXiv:2310.09462v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#20132;&#26131;&#31995;&#32479;&#26694;&#26550;CausalReinforceNet&#65292;&#36890;&#36807;&#22240;&#26524;&#20998;&#26512;&#22686;&#24378;&#20102;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#33021;&#21147;&#65292;&#20197;&#25552;&#39640;&#23545;&#21152;&#23494;&#36135;&#24065;&#24066;&#22330;&#30340;&#20132;&#26131;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#20132;&#26131;&#26041;&#27861;&#21462;&#24471;&#20102;&#19968;&#23450;&#36827;&#23637;&#65292;&#20294;&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#21152;&#23494;&#36135;&#24065;&#24066;&#22330;&#20013;&#24320;&#21457;&#30408;&#21033;&#30340;&#33258;&#21160;&#20132;&#26131;&#31995;&#32479;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#20132;&#26131;&#31995;&#32479;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#38024;&#23545;&#20116;&#31181;&#28909;&#38376;&#30340;&#26367;&#20195;&#21152;&#23494;&#36135;&#24065;&#65288;&#21363;&#27604;&#29305;&#24065;&#20197;&#22806;&#30340;&#21152;&#23494;&#36135;&#24065;&#65289;&#65306;&#24065;&#23433;&#24065;&#12289;&#20197;&#22826;&#22346;&#12289;&#33713;&#29305;&#24065;&#12289;&#29790;&#27874;&#24065;&#21644;&#27888;&#36798;&#24065;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CausalReinforceNet&#65292;&#19968;&#20010;&#34987;&#26500;&#24314;&#20026;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#30340;&#26694;&#26550;&#12290;&#20316;&#20026;&#20132;&#26131;&#31995;&#32479;&#30340;&#22522;&#30784;&#26550;&#26500;&#65292;CausalReinforceNet&#26694;&#26550;&#36890;&#36807;&#22240;&#26524;&#20998;&#26512;&#22686;&#24378;&#20102;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#33021;&#21147;&#12290;&#22312;&#35813;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#22312;&#29305;&#24449;&#24037;&#31243;&#36807;&#31243;&#20013;&#20351;&#29992;&#36125;&#21494;&#26031;&#32593;&#32476;&#26469;&#35782;&#21035;&#24433;&#21709;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#21464;&#21160;&#30340;&#20855;&#26377;&#22240;&#26524;&#20851;&#31995;&#30340;&#26368;&#30456;&#20851;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#21160;&#24577;&#36125;&#21494;&#26031;&#32593;&#32476;&#23558;&#27010;&#29575;&#24615;&#20215;&#26684;&#26041;&#21521;&#20449;&#21495;&#32435;&#20837;&#26694;&#26550;&#20013;&#65292;&#20197;&#22686;&#24378;&#20132;&#26131;&#31995;&#32479;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite advances in artificial intelligence-enhanced trading methods, developing a profitable automated trading system remains challenging in the rapidly evolving cryptocurrency market. This study aims to address these challenges by developing a reinforcement learning-based automated trading system for five popular altcoins~(cryptocurrencies other than Bitcoin): Binance Coin, Ethereum, Litecoin, Ripple, and Tether. To this end, we present CausalReinforceNet, a framework framed as a decision support system. Designed as the foundational architecture of the trading system, the CausalReinforceNet framework enhances the capabilities of the reinforcement learning agent through causal analysis. Within this framework, we use Bayesian networks in the feature engineering process to identify the most relevant features with causal relationships that influence cryptocurrency price movements. Additionally, we incorporate probabilistic price direction signals from dynamic Bayesian networks to enhance
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LgTS&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;LLM&#30340;&#35268;&#21010;&#33021;&#21147;&#65292;&#20026;&#26080;&#27861;&#35775;&#38382;&#29615;&#22659;&#36716;&#25442;&#21160;&#21147;&#23398;&#30340;RL&#20195;&#29702;&#25552;&#20379;&#23376;&#30446;&#26631;&#30340;&#22270;&#24418;&#34920;&#31034;&#12290;&#36825;&#20010;&#26041;&#27861;&#21487;&#20197;&#25945;&#23548;RL&#20195;&#29702;&#23398;&#20064;&#19968;&#32452;&#25104;&#21151;&#30340;&#31574;&#30053;&#26469;&#36798;&#21040;&#30446;&#26631;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2310.09454</link><description>&lt;p&gt;
LgTS&#65306;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#23376;&#30446;&#26631;&#36827;&#34892;&#21160;&#24577;&#20219;&#21153;&#37319;&#26679;&#65292;&#29992;&#20110;&#22686;&#24378;&#23398;&#20064;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
LgTS: Dynamic Task Sampling using LLM-generated sub-goals for Reinforcement Learning Agents. (arXiv:2310.09454v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LgTS&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;LLM&#30340;&#35268;&#21010;&#33021;&#21147;&#65292;&#20026;&#26080;&#27861;&#35775;&#38382;&#29615;&#22659;&#36716;&#25442;&#21160;&#21147;&#23398;&#30340;RL&#20195;&#29702;&#25552;&#20379;&#23376;&#30446;&#26631;&#30340;&#22270;&#24418;&#34920;&#31034;&#12290;&#36825;&#20010;&#26041;&#27861;&#21487;&#20197;&#25945;&#23548;RL&#20195;&#29702;&#23398;&#20064;&#19968;&#32452;&#25104;&#21151;&#30340;&#31574;&#30053;&#26469;&#36798;&#21040;&#30446;&#26631;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#26426;&#22120;&#20154;&#21644;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#38656;&#35201;&#39640;&#32423;&#35268;&#21010;&#30340;&#38382;&#39064;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#21033;&#29992;LLM&#36827;&#34892;&#27492;&#31867;&#35268;&#21010;&#20219;&#21153;&#30340;&#25216;&#26415;&#23384;&#22312;&#19968;&#20123;&#20851;&#38190;&#20551;&#35774;&#65292;&#27604;&#22914;&#38656;&#35201;&#35775;&#38382;&#20801;&#35768;&#24494;&#35843;&#30340;&#25968;&#25454;&#38598;&#65292;&#38656;&#35201;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#20449;&#24687;&#20165;&#21521;LLM&#25552;&#20379;&#30456;&#20851;&#19988;&#24517;&#35201;&#30340;&#20449;&#24687;&#65292;&#20197;&#21450;&#24517;&#39035;&#37319;&#29992;&#30830;&#23450;&#24615;&#26041;&#27861;&#25191;&#34892;LLM&#30340;&#21709;&#24212;&#65292;&#20363;&#22914;&#20351;&#29992;&#29616;&#26377;&#31574;&#30053;&#25110;&#35745;&#21010;&#25805;&#20316;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LgTS&#65288;LLM&#24341;&#23548;&#30340;&#24072;&#29983;&#23398;&#20064;&#65289;&#36825;&#19968;&#20840;&#26032;&#30340;&#26041;&#27861;&#65292;&#23427;&#25506;&#32034;&#20102;LLM&#30340;&#35268;&#21010;&#33021;&#21147;&#65292;&#20026;&#26080;&#27861;&#35775;&#38382;&#29615;&#22659;&#36716;&#25442;&#21160;&#21147;&#23398;&#30340;&#22686;&#24378;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#25552;&#20379;&#20102;&#23376;&#30446;&#26631;&#30340;&#22270;&#24418;&#34920;&#31034;&#12290;RL&#20195;&#29702;&#21033;&#29992;&#24072;&#29983;&#23398;&#20064;&#31639;&#27861;&#23398;&#20064;&#19968;&#32452;&#25104;&#21151;&#36798;&#21040;&#30446;&#26631;&#29366;&#24577;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in reasoning abilities of Large Language Models (LLM) has promoted their usage in problems that require high-level planning for robots and artificial agents. However, current techniques that utilize LLMs for such planning tasks make certain key assumptions such as, access to datasets that permit finetuning, meticulously engineered prompts that only provide relevant and essential information to the LLM, and most importantly, a deterministic approach to allow execution of the LLM responses either in the form of existing policies or plan operators. In this work, we propose LgTS (LLM-guided Teacher-Student learning), a novel approach that explores the planning abilities of LLMs to provide a graphical representation of the sub-goals to a reinforcement learning (RL) agent that does not have access to the transition dynamics of the environment. The RL agent uses Teacher-Student learning algorithm to learn a set of successful policies for reaching the goal state from the st
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SimPLE&#30340;&#31616;&#21333;&#30340;&#26080;&#20195;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21452;&#21521;&#30456;&#20284;&#24615;&#23398;&#20064;(PSL)&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#26080;&#38656;&#39069;&#22806;&#30340;&#35268;&#33539;&#21270;&#21644;&#36793;&#36317;&#35774;&#32622;&#65292;&#24182;&#22312;&#24320;&#25918;&#38598;&#35782;&#21035;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.09449</link><description>&lt;p&gt;
&#21452;&#21521;&#30456;&#20284;&#24615;&#23398;&#20064;(SimPLE)&#26159;&#31616;&#21333;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pairwise Similarity Learning is SimPLE. (arXiv:2310.09449v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SimPLE&#30340;&#31616;&#21333;&#30340;&#26080;&#20195;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21452;&#21521;&#30456;&#20284;&#24615;&#23398;&#20064;(PSL)&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#26080;&#38656;&#39069;&#22806;&#30340;&#35268;&#33539;&#21270;&#21644;&#36793;&#36317;&#35774;&#32622;&#65292;&#24182;&#22312;&#24320;&#25918;&#38598;&#35782;&#21035;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#20110;&#19968;&#31181;&#26222;&#36941;&#32780;&#37325;&#35201;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#21363;&#21452;&#21521;&#30456;&#20284;&#24615;&#23398;&#20064;(PSL)&#12290;PSL&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#37325;&#35201;&#24212;&#29992;&#65292;&#22914;&#24320;&#25918;&#24335;&#20154;&#33080;&#35782;&#21035;&#12289;&#35828;&#35805;&#20154;&#39564;&#35777;&#12289;&#22270;&#20687;&#26816;&#32034;&#21644;&#20154;&#29289;&#20877;&#35782;&#21035;&#12290;PSL&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#20010;&#21452;&#21521;&#30456;&#20284;&#24615;&#20989;&#25968;&#65292;&#20026;&#27491;&#26679;&#26412;&#23545;&#65288;&#21363;&#20855;&#26377;&#30456;&#21516;&#26631;&#31614;&#30340;&#26679;&#26412;&#23545;&#65289;&#36171;&#20104;&#26356;&#39640;&#30340;&#30456;&#20284;&#24615;&#24471;&#20998;&#65292;&#32780;&#23545;&#20110;&#36127;&#26679;&#26412;&#23545;&#65288;&#21363;&#20855;&#26377;&#19981;&#21516;&#26631;&#31614;&#30340;&#26679;&#26412;&#23545;&#65289;&#36171;&#20104;&#36739;&#20302;&#30340;&#30456;&#20284;&#24615;&#24471;&#20998;&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;PSL&#30340;&#19968;&#20010;&#20851;&#38190;&#35201;&#27714;&#65292;&#28982;&#21518;&#35752;&#35770;&#20102;&#29616;&#26377;&#26041;&#27861;&#22914;&#20309;&#23454;&#29616;&#36825;&#20010;&#35201;&#27714;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20196;&#20154;&#24778;&#35766;&#30340;&#31616;&#21333;&#30340;&#26080;&#20195;&#29702;&#26041;&#27861;&#8212;&#8212;SimPLE&#65292;&#23427;&#26082;&#19981;&#38656;&#35201;&#29305;&#24449;/&#20195;&#29702;&#35268;&#33539;&#21270;&#65292;&#20063;&#19981;&#38656;&#35201;&#35282;&#24230;&#36793;&#36317;&#65292;&#20294;&#22312;&#24320;&#25918;&#38598;&#35782;&#21035;&#20013;&#33021;&#22815;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#19977;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;PSL&#20219;&#21153;&#65306;&#24320;&#25918;&#24335;&#20154;&#33080;&#35782;&#21035;&#12289;&#22270;&#20687;&#26816;&#32034;&#21644;&#35828;&#35805;&#20154;&#39564;&#35777;&#12290;&#22522;&#20110;&#22823;&#35268;&#27169;&#22522;&#20934;&#30340;&#20840;&#38754;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we focus on a general yet important learning problem, pairwise similarity learning (PSL). PSL subsumes a wide range of important applications, such as open-set face recognition, speaker verification, image retrieval and person re-identification. The goal of PSL is to learn a pairwise similarity function assigning a higher similarity score to positive pairs (i.e., a pair of samples with the same label) than to negative pairs (i.e., a pair of samples with different label). We start by identifying a key desideratum for PSL, and then discuss how existing methods can achieve this desideratum. We then propose a surprisingly simple proxy-free method, called SimPLE, which requires neither feature/proxy normalization nor angular margin and yet is able to generalize well in open-set recognition. We apply the proposed method to three challenging PSL tasks: open-set face recognition, image retrieval and speaker verification. Comprehensive experimental results on large-scale benchmar
&lt;/p&gt;</description></item><item><title>G10&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;GPU&#20869;&#23384;&#21644;&#23384;&#20648;&#26550;&#26500;&#65292;&#36890;&#36807;&#26234;&#33021;&#24352;&#37327;&#36801;&#31227;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#25193;&#23637;GPU&#20869;&#23384;&#23481;&#37327;&#24182;&#28385;&#36275;&#28145;&#24230;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#30340;&#21487;&#25193;&#23637;&#24615;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.09443</link><description>&lt;p&gt;
G10&#65306;&#36890;&#36807;&#26234;&#33021;&#24352;&#37327;&#36801;&#31227;&#23454;&#29616;&#39640;&#25928;&#30340;&#32479;&#19968;GPU&#20869;&#23384;&#21644;&#23384;&#20648;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
G10: Enabling An Efficient Unified GPU Memory and Storage Architecture with Smart Tensor Migrations. (arXiv:2310.09443v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09443
&lt;/p&gt;
&lt;p&gt;
G10&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;GPU&#20869;&#23384;&#21644;&#23384;&#20648;&#26550;&#26500;&#65292;&#36890;&#36807;&#26234;&#33021;&#24352;&#37327;&#36801;&#31227;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#25193;&#23637;GPU&#20869;&#23384;&#23481;&#37327;&#24182;&#28385;&#36275;&#28145;&#24230;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#30340;&#21487;&#25193;&#23637;&#24615;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#31361;&#30772;GPU&#20869;&#23384;&#22681;&#65292;&#20197;&#25193;&#23637;&#28145;&#24230;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#35268;&#27169;&#65292;&#36817;&#26399;&#25552;&#20986;&#20102;&#21508;&#31181;&#26550;&#26500;&#21644;&#31995;&#32479;&#25216;&#26415;&#12290;&#23427;&#20204;&#30340;&#20856;&#22411;&#26041;&#27861;&#21253;&#25324;&#20351;&#29992;&#38378;&#23384;&#25193;&#23637;&#20869;&#23384;&#21644;&#30452;&#25509;&#23384;&#20648;&#35775;&#38382;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#20173;&#28982;&#23384;&#22312;&#24615;&#33021;&#19981;&#20339;&#20197;&#21450;&#24341;&#20837;GPU&#20869;&#23384;&#31649;&#29702;&#22797;&#26434;&#24615;&#30340;&#38382;&#39064;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#28385;&#36275;&#24403;&#20170;&#28145;&#24230;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#30340;&#21487;&#25193;&#23637;&#24615;&#35201;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;G10&#30340;&#32479;&#19968;GPU&#20869;&#23384;&#21644;&#23384;&#20648;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#21463;&#21040;&#28145;&#24230;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#30340;&#24352;&#37327;&#34892;&#20026;&#39640;&#24230;&#21487;&#39044;&#27979;&#30340;&#20107;&#23454;&#25512;&#21160;&#12290;G10&#23558;&#20027;&#26426;&#20869;&#23384;&#12289;GPU&#20869;&#23384;&#21644;&#38378;&#23384;&#20869;&#23384;&#38598;&#25104;&#21040;&#32479;&#19968;&#30340;&#20869;&#23384;&#31354;&#38388;&#20013;&#65292;&#20197;&#25193;&#23637;GPU&#20869;&#23384;&#23481;&#37327;&#24182;&#23454;&#29616;&#36879;&#26126;&#30340;&#25968;&#25454;&#36801;&#31227;&#12290;&#22522;&#20110;&#36825;&#31181;&#32479;&#19968;&#30340;GPU&#20869;&#23384;&#21644;&#23384;&#20648;&#26550;&#26500;&#65292;G10&#21033;&#29992;&#32534;&#35793;&#22120;&#25216;&#26415;&#23545;&#28145;&#24230;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#20013;&#30340;&#24352;&#37327;&#34892;&#20026;&#36827;&#34892;&#29305;&#24449;&#21270;&#12290;&#22240;&#27492;&#65292;&#23427;&#21487;&#20197;&#25552;&#21069;&#35843;&#24230;&#25968;&#25454;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
To break the GPU memory wall for scaling deep learning workloads, a variety of architecture and system techniques have been proposed recently. Their typical approaches include memory extension with flash memory and direct storage access. However, these techniques still suffer from suboptimal performance and introduce complexity to the GPU memory management, making them hard to meet the scalability requirement of deep learning workloads today. In this paper, we present a unified GPU memory and storage architecture named G10 driven by the fact that the tensor behaviors of deep learning workloads are highly predictable. G10 integrates the host memory, GPU memory, and flash memory into a unified memory space, to scale the GPU memory capacity while enabling transparent data migrations. Based on this unified GPU memory and storage architecture, G10 utilizes compiler techniques to characterize the tensor behaviors in deep learning workloads. Therefore, it can schedule data migrations in advan
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#22238;&#24402;&#21644;&#20998;&#31867;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#21457;&#29616;&#22238;&#24402;&#27169;&#22411;&#38656;&#35201;&#26356;&#22810;&#35745;&#31639;&#24037;&#20316;&#25165;&#33021;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#19988;&#23545;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#24615;&#21644;&#21551;&#21457;&#24335;&#36873;&#25321;&#26356;&#20026;&#25935;&#24863;&#12290;</title><link>http://arxiv.org/abs/2310.09440</link><description>&lt;p&gt;
&#30446;&#26631;&#21464;&#37327;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Target Variable Engineering. (arXiv:2310.09440v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#22238;&#24402;&#21644;&#20998;&#31867;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#21457;&#29616;&#22238;&#24402;&#27169;&#22411;&#38656;&#35201;&#26356;&#22810;&#35745;&#31639;&#24037;&#20316;&#25165;&#33021;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#19988;&#23545;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#24615;&#21644;&#21551;&#21457;&#24335;&#36873;&#25321;&#26356;&#20026;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#21464;&#37327;&#30340;&#35774;&#35745;&#22914;&#20309;&#24433;&#21709;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#30340;&#24615;&#33021;&#65311;&#26412;&#30740;&#31350;&#30340;&#23454;&#39564;&#36890;&#36807;&#23558;&#25968;&#20540;&#30446;&#26631;&#19982;&#38408;&#20540;&#36827;&#34892;&#27604;&#36739;&#26469;&#36827;&#34892;&#20108;&#20803;&#21270;&#22788;&#29702;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#38024;&#23545;&#25968;&#20540;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#30340;&#22238;&#24402;&#27169;&#22411;&#19982;&#38024;&#23545;&#20854;&#20108;&#20803;&#21270;&#29256;&#26412;&#36827;&#34892;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#38543;&#26426;&#36229;&#21442;&#25968;&#20248;&#21270;&#25628;&#32034;&#30340;&#27599;&#20010;&#28857;&#19978;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#20102;&#35299;&#35745;&#31639;&#36164;&#28304;&#39044;&#31639;&#23545;&#20004;&#32773;&#20043;&#38388;&#26435;&#34913;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22238;&#24402;&#27169;&#22411;&#38656;&#35201;&#26356;&#22810;&#30340;&#35745;&#31639;&#24037;&#20316;&#25165;&#33021;&#25910;&#25947;&#21040;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#19988;&#23545;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#24615;&#21644;&#21551;&#21457;&#24335;&#36873;&#25321;&#26356;&#20026;&#25935;&#24863;&#12290;&#34429;&#28982;&#20998;&#31867;&#20063;&#21487;&#20197;&#36890;&#36807;&#31995;&#32479;&#21270;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#27169;&#22411;&#36873;&#25321;&#36827;&#34892;&#25913;&#36827;&#65292;&#20294;&#25913;&#36827;&#31243;&#24230;&#36828;&#19981;&#21450;&#22238;&#24402;&#12290;&#26412;&#30740;&#31350;&#26159;&#23545;&#22238;&#24402;&#21644;&#20998;&#31867;&#36827;&#34892;&#31995;&#32479;&#27604;&#36739;&#30340;&#31532;&#19968;&#39033;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
How does the formulation of a target variable affect performance within the ML pipeline? The experiments in this study examine numeric targets that have been binarized by comparing against a threshold. We compare the predictive performance of regression models trained to predict the numeric targets vs. classifiers trained to predict their binarized counterparts. Specifically, we make this comparison at every point of a randomized hyperparameter optimization search to understand the effect of computational resource budget on the tradeoff between the two. We find that regression requires significantly more computational effort to converge upon the optimal performance, and is more sensitive to both randomness and heuristic choices in the training process. Although classification can and does benefit from systematic hyperparameter tuning and model selection, the improvements are much less than for regression. This work comprises the first systematic comparison of regression and classificat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21457;&#29616;&#27599;&#20010;&#20219;&#21153;&#30340;&#23376;&#32593;&#32476;&#21644;&#36719;&#25513;&#34109;&#26426;&#21046;&#26469;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#20419;&#36827;&#30693;&#35782;&#20256;&#36882;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#37117;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.09436</link><description>&lt;p&gt;
&#28151;&#21512;&#20219;&#21153;&#30340;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#23376;&#32593;&#32476;&#21457;&#29616;&#21644;&#36719;&#25513;&#34109;
&lt;/p&gt;
&lt;p&gt;
Sub-network Discovery and Soft-masking for Continual Learning of Mixed Tasks. (arXiv:2310.09436v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09436
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21457;&#29616;&#27599;&#20010;&#20219;&#21153;&#30340;&#23376;&#32593;&#32476;&#21644;&#36719;&#25513;&#34109;&#26426;&#21046;&#26469;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#20419;&#36827;&#30693;&#35782;&#20256;&#36882;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#37117;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#26377;&#20004;&#20010;&#20027;&#35201;&#30446;&#26631;: &#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#20419;&#36827;&#30693;&#35782;&#20256;&#36882;&#12290;&#29616;&#26377;&#25991;&#29486;&#20027;&#35201;&#20851;&#27880;&#20110;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#19968;&#20123;&#24037;&#20316;&#20063;&#38024;&#23545;&#20219;&#21153;&#30456;&#20284;&#26102;&#30340;&#30693;&#35782;&#20256;&#36882;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#21482;&#26377;&#19968;&#31181;&#26041;&#27861;&#25552;&#20986;&#20102;&#36830;&#32493;&#23398;&#20064;&#28151;&#21512;&#20219;&#21153;&#24207;&#21015;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#20173;&#28982;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;/&#25110;&#26377;&#38480;&#30340;&#30693;&#35782;&#20256;&#36882;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#26469;&#23454;&#29616;&#20004;&#32773;&#12290;&#23427;&#36890;&#36807;&#21457;&#29616;&#27599;&#20010;&#20219;&#21153;&#30340;&#23376;&#32593;&#32476;&#26469;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36719;&#25513;&#34109;&#26426;&#21046;&#26469;&#20445;&#30041;&#20808;&#21069;&#30340;&#30693;&#35782;&#65292;&#24182;&#20351;&#26032;&#20219;&#21153;&#33021;&#22815;&#20511;&#21161;&#36807;&#21435;&#30340;&#30693;&#35782;&#23454;&#29616;&#30693;&#35782;&#20256;&#36882;&#12290;&#20351;&#29992;&#20998;&#31867;&#12289;&#29983;&#25104;&#12289;&#20449;&#24687;&#25552;&#21462;&#21450;&#20854;&#28151;&#21512; (&#21363;&#24322;&#26500;&#20219;&#21153;) &#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#24378;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) has two main objectives: preventing catastrophic forgetting (CF) and encouraging knowledge transfer (KT). The existing literature mainly focused on overcoming CF. Some work has also been done on KT when the tasks are similar. To our knowledge, only one method has been proposed to learn a sequence of mixed tasks. However, these techniques still suffer from CF and/or limited KT. This paper proposes a new CL method to achieve both. It overcomes CF by isolating the knowledge of each task via discovering a subnetwork for it. A soft-masking mechanism is also proposed to preserve the previous knowledge and to enable the new task to leverage the past knowledge to achieve KT. Experiments using classification, generation, information extraction, and their mixture (i.e., heterogeneous tasks) show that the proposed method consistently outperforms strong baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;LSTM-RNN&#23398;&#20064;&#38750;&#32447;&#24615;&#31215;&#20998;&#31639;&#23376;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#27714;&#35299;&#31215;&#20998;&#24494;&#20998;&#26041;&#31243;&#12290;&#20351;&#29992;LSTM-RNN&#34920;&#31034;&#38750;&#32447;&#24615;&#31215;&#20998;&#31639;&#23376;&#21487;&#23558;&#38750;&#32447;&#24615;&#31215;&#20998;&#24494;&#20998;&#26041;&#31243;&#36716;&#21270;&#20026;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65292;&#20174;&#32780;&#25552;&#39640;&#27714;&#35299;&#25928;&#29575;&#12290;&#36890;&#36807;&#27169;&#22411;&#38382;&#39064;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#31283;&#23450;&#24615;&#65292;&#24182;&#39564;&#35777;&#20102;&#23398;&#20064;&#21040;&#30340;&#31215;&#20998;&#31639;&#23376;&#30340;&#26222;&#36866;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.09434</link><description>&lt;p&gt;
&#36890;&#36807;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#38750;&#32447;&#24615;&#31215;&#20998;&#31639;&#23376;&#21450;&#20854;&#22312;&#27714;&#35299;&#31215;&#20998;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning nonlinear integral operators via Recurrent Neural Networks and its application in solving Integro-Differential Equations. (arXiv:2310.09434v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;LSTM-RNN&#23398;&#20064;&#38750;&#32447;&#24615;&#31215;&#20998;&#31639;&#23376;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#27714;&#35299;&#31215;&#20998;&#24494;&#20998;&#26041;&#31243;&#12290;&#20351;&#29992;LSTM-RNN&#34920;&#31034;&#38750;&#32447;&#24615;&#31215;&#20998;&#31639;&#23376;&#21487;&#23558;&#38750;&#32447;&#24615;&#31215;&#20998;&#24494;&#20998;&#26041;&#31243;&#36716;&#21270;&#20026;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65292;&#20174;&#32780;&#25552;&#39640;&#27714;&#35299;&#25928;&#29575;&#12290;&#36890;&#36807;&#27169;&#22411;&#38382;&#39064;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#31283;&#23450;&#24615;&#65292;&#24182;&#39564;&#35777;&#20102;&#23398;&#20064;&#21040;&#30340;&#31215;&#20998;&#31639;&#23376;&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;LSTM-RNN&#65288;&#38271;&#30701;&#26399;&#35760;&#24518;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65289;&#26469;&#23398;&#20064;&#21644;&#34920;&#31034;&#20986;&#29616;&#22312;&#38750;&#32447;&#24615;&#31215;&#20998;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#38750;&#32447;&#24615;&#31215;&#20998;&#31639;&#23376;&#12290;&#38750;&#32447;&#24615;&#31215;&#20998;&#31639;&#23376;&#30340;LSTM-RNN&#34920;&#31034;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#23558;&#38750;&#32447;&#24615;&#31215;&#20998;&#24494;&#20998;&#26041;&#31243;&#30340;&#31995;&#32479;&#36716;&#21270;&#20026;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#30340;&#31995;&#32479;&#65292;&#20174;&#32780;&#21487;&#20197;&#20351;&#29992;&#24456;&#22810;&#39640;&#25928;&#30340;&#27714;&#35299;&#22120;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#22312;&#31215;&#20998;&#24494;&#20998;&#26041;&#31243;&#20013;&#20351;&#29992;LSTM-RNN&#34920;&#31034;&#30340;&#38750;&#32447;&#24615;&#31215;&#20998;&#31639;&#23376;&#19981;&#38656;&#35201;&#22312;&#27599;&#20010;&#25968;&#20540;&#26102;&#38388;&#28436;&#21270;&#27493;&#39588;&#20013;&#36827;&#34892;&#25968;&#20540;&#31215;&#20998;&#65292;LSTM-RNN&#22522;&#20110;&#30340;&#31215;&#20998;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#30340;&#24635;&#26102;&#38388;&#25104;&#26412;&#21487;&#20197;&#20174;$O(n_T^2)$&#38477;&#20302;&#21040;$O(n_T)$&#65292;&#20854;&#20013;$n_T$&#20026;&#38656;&#35201;&#35745;&#31639;&#30340;&#27493;&#38271;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#27169;&#22411;&#38382;&#39064;&#23637;&#31034;&#20102;&#36825;&#31181;LSTM-RNN&#22522;&#20110;&#30340;&#31215;&#20998;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#30340;&#25928;&#29575;&#21644;&#31283;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#31215;&#20998;&#24494;&#20998;&#26041;&#31243;&#28436;&#31034;&#20102;&#23398;&#20064;&#21040;&#30340;&#31215;&#20998;&#31639;&#23376;&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose using LSTM-RNNs (Long Short-Term Memory-Recurrent Neural Networks) to learn and represent nonlinear integral operators that appear in nonlinear integro-differential equations (IDEs). The LSTM-RNN representation of the nonlinear integral operator allows us to turn a system of nonlinear integro-differential equations into a system of ordinary differential equations for which many efficient solvers are available. Furthermore, because the use of LSTM-RNN representation of the nonlinear integral operator in an IDE eliminates the need to perform a numerical integration in each numerical time evolution step, the overall temporal cost of the LSTM-RNN-based IDE solver can be reduced to $O(n_T)$ from $O(n_T^2)$ if a $n_T$-step trajectory is to be computed. We illustrate the efficiency and robustness of this LSTM-RNN-based numerical IDE solver with a model problem. Additionally, we highlight the generalizability of the learned integral operator by applying it to IDEs dri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24494;&#29615;&#35856;&#25391;&#22120;&#20013;&#33108;&#20307;&#38750;&#32447;&#24615;&#21644;&#32447;&#24615;&#25439;&#32791;&#23545;&#24211;&#20177;&#35745;&#31639;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#20102;&#19977;&#20010;&#21306;&#22495;&#65292;&#20854;&#20013;&#19968;&#20010;&#21306;&#22495;&#22312;&#20302;&#36755;&#20837;&#21151;&#29575;&#21644;&#33410;&#28857;&#25968;&#19979;&#20855;&#26377;&#38750;&#24120;&#20302;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#35823;&#24046;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#25913;&#36827;&#26102;&#38388;&#24310;&#36831;&#24211;&#20177;&#35745;&#31639;&#30340;&#39044;&#27979;&#24615;&#33021;&#26377;&#30528;&#37325;&#35201;&#30340;&#35774;&#35745;&#21644;&#20248;&#21270;&#25351;&#23548;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.09433</link><description>&lt;p&gt;
&#22522;&#20110;&#30789;&#24494;&#29615;&#30340;&#24211;&#20177;&#35745;&#31639;&#20013;&#33108;&#20307;&#38750;&#32447;&#24615;&#21644;&#32447;&#24615;&#25439;&#32791;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Effects of cavity nonlinearities and linear losses on silicon microring-based reservoir computing. (arXiv:2310.09433v1 [physics.optics])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24494;&#29615;&#35856;&#25391;&#22120;&#20013;&#33108;&#20307;&#38750;&#32447;&#24615;&#21644;&#32447;&#24615;&#25439;&#32791;&#23545;&#24211;&#20177;&#35745;&#31639;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#20102;&#19977;&#20010;&#21306;&#22495;&#65292;&#20854;&#20013;&#19968;&#20010;&#21306;&#22495;&#22312;&#20302;&#36755;&#20837;&#21151;&#29575;&#21644;&#33410;&#28857;&#25968;&#19979;&#20855;&#26377;&#38750;&#24120;&#20302;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#35823;&#24046;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#25913;&#36827;&#26102;&#38388;&#24310;&#36831;&#24211;&#20177;&#35745;&#31639;&#30340;&#39044;&#27979;&#24615;&#33021;&#26377;&#30528;&#37325;&#35201;&#30340;&#35774;&#35745;&#21644;&#20248;&#21270;&#25351;&#23548;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#29615;&#35856;&#25391;&#22120;&#26159;&#26102;&#38388;&#24310;&#36831;&#20809;&#23376;&#24211;&#20177;&#35745;&#31639;&#30340;&#26377;&#24076;&#26395;&#30340;&#22120;&#20214;&#65292;&#20294;&#26159;&#24494;&#29615;&#35856;&#25391;&#22120;&#20013;&#19981;&#21516;&#29289;&#29702;&#25928;&#24212;&#23545;&#24211;&#20177;&#35745;&#31639;&#24615;&#33021;&#30340;&#24433;&#21709;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#20998;&#26512;&#32447;&#24615;&#25439;&#32791;&#12289;&#28909;&#20809;&#21644;&#33258;&#30001;&#36733;&#27969;&#23376;&#25928;&#24212;&#26494;&#24347;&#26102;&#38388;&#23545;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153; NARMA-10 &#30340;&#39044;&#27979;&#35823;&#24046;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23384;&#22312;&#19977;&#20010;&#21306;&#22495;&#65292;&#30001;&#36755;&#20837;&#21151;&#29575;&#21644;&#20809;&#28304;&#19982;&#24494;&#29615;&#35856;&#25391;&#20043;&#38388;&#30340;&#39057;&#29575;&#22833;&#35856;&#23450;&#20041;&#65292;&#36825;&#20123;&#21306;&#22495;&#25581;&#31034;&#20102;&#33108;&#20307;&#20174;&#32447;&#24615;&#21040;&#38750;&#32447;&#24615;&#29366;&#24577;&#30340;&#36716;&#21464;&#12290;&#20854;&#20013;&#19968;&#20010;&#21306;&#22495;&#22312;&#30456;&#23545;&#36739;&#20302;&#30340;&#36755;&#20837;&#21151;&#29575;&#21644;&#33410;&#28857;&#25968;&#19979;&#25552;&#20379;&#20102;&#38750;&#24120;&#20302;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#35823;&#24046;&#65292;&#32780;&#20854;&#20182;&#21306;&#22495;&#35201;&#20040;&#32570;&#20047;&#38750;&#32447;&#24615;&#65292;&#35201;&#20040;&#21464;&#24471;&#19981;&#31283;&#23450;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#24494;&#29615;&#35856;&#25391;&#22120;&#30340;&#35774;&#35745;&#21644;&#20248;&#21270;&#20854;&#29289;&#29702;&#29305;&#24615;&#20197;&#25552;&#39640;&#26102;&#38388;&#24310;&#36831;&#24211;&#20177;&#35745;&#31639;&#30340;&#39044;&#27979;&#24615;&#33021;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Microring resonators (MRRs) are promising devices for time-delay photonic reservoir computing, but the impact of the different physical effects taking place in the MRRs on the reservoir computing performance is yet to be fully understood. We numerically analyze the impact of linear losses as well as thermo-optic and free-carrier effects relaxation times on the prediction error of the time-series task NARMA-10. We demonstrate the existence of three regions, defined by the input power and the frequency detuning between the optical source and the microring resonance, that reveal the cavity transition from linear to nonlinear regimes. One of these regions offers very low error in time-series prediction under relatively low input power and number of nodes while the other regions either lack nonlinearity or become unstable. This study provides insight into the design of the MRR and the optimization of its physical properties for improving the prediction performance of time-delay reservoir co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20851;&#38190;&#35789;&#39537;&#21160;&#30340;&#21477;&#23376;&#36873;&#25321;&#31574;&#30053;&#26469;&#22686;&#24378;&#22522;&#20110;BERT&#30340;&#35270;&#35273;&#38382;&#31572;&#12290;&#36890;&#36807;&#21033;&#29992;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#25216;&#26415;&#24494;&#35843;BERT&#27169;&#22411;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#24102;&#26377;&#25935;&#24863;&#20851;&#38190;&#35789;&#30340;&#21477;&#23376;&#65292;&#26412;&#26041;&#27861;&#33021;&#22815;&#22312;&#25991;&#26723;&#20013;&#35782;&#21035;&#20986;&#22238;&#31572;&#38382;&#39064;&#30340;&#30456;&#20851;&#20803;&#32032;&#65292;&#24182;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09432</link><description>&lt;p&gt;
&#36890;&#36807;&#20851;&#38190;&#35789;&#39537;&#21160;&#30340;&#21477;&#23376;&#36873;&#25321;&#22686;&#24378;&#22522;&#20110;BERT&#30340;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Enhancing BERT-Based Visual Question Answering through Keyword-Driven Sentence Selection. (arXiv:2310.09432v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20851;&#38190;&#35789;&#39537;&#21160;&#30340;&#21477;&#23376;&#36873;&#25321;&#31574;&#30053;&#26469;&#22686;&#24378;&#22522;&#20110;BERT&#30340;&#35270;&#35273;&#38382;&#31572;&#12290;&#36890;&#36807;&#21033;&#29992;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#25216;&#26415;&#24494;&#35843;BERT&#27169;&#22411;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#24102;&#26377;&#25935;&#24863;&#20851;&#38190;&#35789;&#30340;&#21477;&#23376;&#65292;&#26412;&#26041;&#27861;&#33021;&#22815;&#22312;&#25991;&#26723;&#20013;&#35782;&#21035;&#20986;&#22238;&#31572;&#38382;&#39064;&#30340;&#30456;&#20851;&#20803;&#32032;&#65292;&#24182;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#26723;&#30340;&#35270;&#35273;&#38382;&#31572;&#31454;&#36187;&#35299;&#20915;&#20102;&#22312;&#22810;&#39029;&#25991;&#26723;&#20013;&#33258;&#21160;&#26816;&#27979;&#20803;&#32032;&#20043;&#38388;&#30340;&#29238;&#23376;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;&#30446;&#26631;&#26159;&#35782;&#21035;&#22238;&#31572;&#33258;&#28982;&#35821;&#35328;&#25552;&#20986;&#30340;&#29305;&#23450;&#38382;&#39064;&#30340;&#25991;&#26723;&#20803;&#32032;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;PoliTo&#38024;&#23545;&#27492;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#25506;&#32034;&#20102;&#19968;&#31181;&#20165;&#20351;&#29992;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#19968;&#31181;&#29305;&#21046;&#30340;&#25277;&#26679;&#31574;&#30053;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#25216;&#26415;&#26469;&#24494;&#35843;BERT&#27169;&#22411;&#65292;&#37325;&#28857;&#20851;&#27880;&#21253;&#21547;&#25935;&#24863;&#20851;&#38190;&#35789;&#19988;&#19982;&#38382;&#39064;&#20013;&#20986;&#29616;&#30340;&#20851;&#38190;&#35789;&#30456;&#21516;&#30340;&#21477;&#23376;&#65292;&#20363;&#22914;&#23545;&#34920;&#26684;&#25110;&#22270;&#20687;&#30340;&#24341;&#29992;&#12290;&#30001;&#20110;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#33021;&#22815;&#19982;&#22522;&#32447;&#30456;&#27604;&#21462;&#24471;&#36739;&#39640;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#23545;&#27492;&#20219;&#21153;&#30340;&#31215;&#26497;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Document-based Visual Question Answering competition addresses the automatic detection of parent-child relationships between elements in multi-page documents. The goal is to identify the document elements that answer a specific question posed in natural language. This paper describes the PoliTo's approach to addressing this task, in particular, our best solution explores a text-only approach, leveraging an ad hoc sampling strategy. Specifically, our approach leverages the Masked Language Modeling technique to fine-tune a BERT model, focusing on sentences containing sensitive keywords that also occur in the questions, such as references to tables or images. Thanks to the effectiveness of this approach, we are able to achieve high performance compared to baselines, demonstrating how our solution contributes positively to this task.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#20248;&#21270;&#29983;&#20135;&#29615;&#22659;&#20013;&#31454;&#26631;&#31574;&#30053;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20248;&#21270;&#20219;&#20309;&#21487;&#24494;&#20998;&#30340;&#22522;&#30784;&#31574;&#30053;&#65292;&#21482;&#38656;&#35201;&#20351;&#29992;&#22522;&#30784;&#31574;&#30053;&#29983;&#25104;&#30340;&#25968;&#25454;&#12290;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#20195;&#29702;&#26550;&#26500;&#65292;&#23558;&#22522;&#30784;&#31574;&#30053;&#19982;&#24378;&#21270;&#23398;&#20064;&#27169;&#22359;&#30456;&#32467;&#21512;&#12290;</title><link>http://arxiv.org/abs/2310.09426</link><description>&lt;p&gt;
&#32447;&#19979;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#20248;&#21270;&#29983;&#20135;&#31454;&#26631;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Offline Reinforcement Learning for Optimizing Production Bidding Policies. (arXiv:2310.09426v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09426
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#20248;&#21270;&#29983;&#20135;&#29615;&#22659;&#20013;&#31454;&#26631;&#31574;&#30053;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20248;&#21270;&#20219;&#20309;&#21487;&#24494;&#20998;&#30340;&#22522;&#30784;&#31574;&#30053;&#65292;&#21482;&#38656;&#35201;&#20351;&#29992;&#22522;&#30784;&#31574;&#30053;&#29983;&#25104;&#30340;&#25968;&#25454;&#12290;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#20195;&#29702;&#26550;&#26500;&#65292;&#23558;&#22522;&#30784;&#31574;&#30053;&#19982;&#24378;&#21270;&#23398;&#20064;&#27169;&#22359;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#24191;&#21578;&#24066;&#22330;&#27599;&#31186;&#36827;&#34892;&#25968;&#21315;&#27425;&#25293;&#21334;&#65292;&#23545;&#20110;&#24076;&#26395;&#22312;&#39044;&#31639;&#38480;&#21046;&#19979;&#20248;&#21270;&#25903;&#20986;&#30340;&#24191;&#21578;&#21830;&#26469;&#35828;&#65292;&#36825;&#26159;&#19968;&#20010;&#33392;&#24040;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#24191;&#21578;&#24179;&#21488;&#36890;&#24120;&#20026;&#20182;&#20204;&#30340;&#23458;&#25143;&#25552;&#20379;&#33258;&#21160;&#21270;&#20195;&#29702;&#20154;&#65292;&#20195;&#34920;&#20182;&#20204;&#23454;&#26102;&#22823;&#35268;&#27169;&#31454;&#26631;&#12290;&#30001;&#20110;&#36825;&#20123;&#20195;&#29702;&#20154;&#30001;&#24179;&#21488;&#25317;&#26377;&#20294;&#20351;&#29992;&#24191;&#21578;&#21830;&#30340;&#36164;&#37329;&#36827;&#34892;&#25805;&#20316;&#65292;&#22240;&#27492;&#22312;&#24179;&#34913;&#20195;&#29702;&#20154;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#19982;&#20248;&#21270;&#24615;&#33021;&#26041;&#38754;&#23384;&#22312;&#30528;&#24378;&#28872;&#30340;&#23454;&#38469;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20174;&#30495;&#23454;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#20248;&#21270;&#29983;&#20135;&#29615;&#22659;&#20013;&#30340;&#31454;&#26631;&#31574;&#30053;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#20248;&#21270;&#20219;&#20309;&#21487;&#24494;&#20998;&#30340;&#22522;&#30784;&#31574;&#30053;&#65288;&#23454;&#38469;&#19978;&#26159;&#22522;&#20110;&#24191;&#21578;&#21830;&#33021;&#22815;&#36731;&#26494;&#29702;&#35299;&#30340;&#21407;&#21017;&#30340;&#21551;&#21457;&#24335;&#31574;&#30053;&#65289;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#30001;&#22522;&#30784;&#31574;&#30053;&#26412;&#36523;&#29983;&#25104;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#20351;&#29992;&#28151;&#21512;&#20195;&#29702;&#26550;&#26500;&#65292;&#23558;&#20219;&#24847;&#22522;&#30784;&#31574;&#30053;&#19982;&#24378;&#21270;&#23398;&#20064;&#27169;&#22359;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The online advertising market, with its thousands of auctions run per second, presents a daunting challenge for advertisers who wish to optimize their spend under a budget constraint. Thus, advertising platforms typically provide automated agents to their customers, which act on their behalf to bid for impression opportunities in real time at scale. Because these proxy agents are owned by the platform but use advertiser funds to operate, there is a strong practical need to balance reliability and explainability of the agent with optimizing power. We propose a generalizable approach to optimizing bidding policies in production environments by learning from real data using offline reinforcement learning. This approach can be used to optimize any differentiable base policy (practically, a heuristic policy based on principles which the advertiser can easily understand), and only requires data generated by the base policy itself. We use a hybrid agent architecture that combines arbitrary ba
&lt;/p&gt;</description></item><item><title>ZeroSwap &#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#30340; DeFi &#24066;&#22330;&#20570;&#24066;&#26041;&#26696;&#65292;&#22312;&#20445;&#25345;&#24066;&#22330;&#20570;&#24066;&#21830;&#38646;&#21033;&#28070;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#36866;&#24212;&#20132;&#26131;&#32773;&#34892;&#20026;&#26469;&#35299;&#20915;&#20102;&#27969;&#21160;&#24615;&#25552;&#20379;&#32773;&#36973;&#21463;&#22871;&#21033;&#25439;&#22833;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.09413</link><description>&lt;p&gt;
ZeroSwap: &#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340; DeFi &#20013;&#30340;&#26368;&#20248;&#24066;&#22330;&#20570;&#24066;
&lt;/p&gt;
&lt;p&gt;
ZeroSwap: Data-driven Optimal Market Making in DeFi. (arXiv:2310.09413v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09413
&lt;/p&gt;
&lt;p&gt;
ZeroSwap &#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#30340; DeFi &#24066;&#22330;&#20570;&#24066;&#26041;&#26696;&#65292;&#22312;&#20445;&#25345;&#24066;&#22330;&#20570;&#24066;&#21830;&#38646;&#21033;&#28070;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#36866;&#24212;&#20132;&#26131;&#32773;&#34892;&#20026;&#26469;&#35299;&#20915;&#20102;&#27969;&#21160;&#24615;&#25552;&#20379;&#32773;&#36973;&#21463;&#22871;&#21033;&#25439;&#22833;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20570;&#24066;&#21830; (AMMs) &#26159;&#21435;&#20013;&#24515;&#21270;&#37329;&#34701;&#20013;&#21305;&#37197;&#27969;&#21160;&#24615;&#20379;&#32473;&#21644;&#38656;&#27714;&#30340;&#20027;&#35201;&#20013;&#24515;&#12290;&#23427;&#20204;&#30340;&#21151;&#33021;&#20027;&#35201;&#20381;&#36182;&#20110;&#27969;&#21160;&#24615;&#25552;&#20379;&#32773; (LPs) &#23558;&#20854;&#36164;&#20135;&#25237;&#36164;&#20110;&#27969;&#21160;&#24615;&#27744;&#12290;&#28982;&#32780;&#65292;&#27744;&#20013;&#36164;&#20135;&#20132;&#26131;&#30340;&#20215;&#26684;&#36890;&#24120;&#27604;&#38598;&#20013;&#21270;&#21644;&#26356;&#27969;&#21160;&#30340;&#20132;&#26131;&#25152;&#20215;&#26684;&#24310;&#36831;&#26356;&#22810;&#12290;&#36825;&#23548;&#33268;&#27969;&#21160;&#24615;&#25552;&#20379;&#32773;&#36973;&#21463;&#22871;&#21033;&#25439;&#22833;&#12290;&#25105;&#20204;&#36890;&#36807;&#37319;&#29992; Glosten &#21644; Milgrom &#30340;&#32463;&#20856;&#24066;&#22330;&#24494;&#35266;&#32467;&#26500;&#27169;&#22411;&#65292;&#23558;&#24066;&#22330;&#20215;&#26684;&#36866;&#24212;&#20110;&#20132;&#26131;&#32773;&#34892;&#20026;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26368;&#20248;&#36125;&#21494;&#26031;&#21644;&#31532;&#19968;&#20010;&#26080;&#27169;&#22411;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#26469;&#26368;&#20248;&#22320;&#36319;&#36394;&#36164;&#20135;&#30340;&#22806;&#37096;&#20215;&#26684;&#12290;&#25105;&#20204;&#20351;&#29992;&#30340;&#26368;&#20248;&#24615;&#27010;&#24565;&#22312;&#24066;&#22330;&#20570;&#24066;&#21830;&#30340;&#20215;&#26684;&#19978;&#24378;&#21046;&#25191;&#34892;&#20102;&#38646;&#21033;&#28070;&#26465;&#20214;&#65292;&#22240;&#27492;&#21462;&#21517;&#20026; ZeroSwap&#12290;&#36825;&#30830;&#20445;&#20102;&#24066;&#22330;&#20570;&#24066;&#21830;&#22312;&#25439;&#22833;&#30693;&#24773;&#20132;&#26131;&#32773;&#30340;&#21516;&#26102;&#20174;&#22122;&#22768;&#20132;&#26131;&#32773;&#37027;&#37324;&#33719;&#24471;&#21033;&#28070;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated Market Makers (AMMs) are major centers of matching liquidity supply and demand in Decentralized Finance. Their functioning relies primarily on the presence of liquidity providers (LPs) incentivized to invest their assets into a liquidity pool. However, the prices at which a pooled asset is traded is often more stale than the prices on centralized and more liquid exchanges. This leads to the LPs suffering losses to arbitrage. This problem is addressed by adapting market prices to trader behavior, captured via the classical market microstructure model of Glosten and Milgrom. In this paper, we propose the first optimal Bayesian and the first model-free data-driven algorithm to optimally track the external price of the asset. The notion of optimality that we use enforces a zero-profit condition on the prices of the market maker, hence the name ZeroSwap. This ensures that the market maker balances losses to informed traders with profits from noise traders. The key property of our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#28151;&#21512;&#24378;&#21270;&#23398;&#20064;&#30340;&#27893;&#31449;&#35843;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#25552;&#39640;&#23454;&#26102;&#25511;&#21046;&#30340;&#21516;&#26102;&#38477;&#20302;&#33021;&#28304;&#28040;&#32791;&#21644;&#36816;&#33829;&#25104;&#26412;&#12290;&#20256;&#32479;&#30340;&#20248;&#21270;&#25216;&#26415;&#30001;&#20110;&#32570;&#20047;&#25910;&#25947;&#24615;&#20445;&#35777;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#24378;&#21270;&#23398;&#20064;&#33021;&#22815;&#36866;&#24212;&#19981;&#30830;&#23450;&#24615;&#24182;&#23454;&#29616;&#23454;&#26102;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#30340;&#27700;&#21147;&#31995;&#32479;&#27169;&#25311;&#27169;&#22411;&#23545;&#20110;&#23454;&#26045;&#24378;&#21270;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#21069;&#30340;&#24212;&#29992;&#21463;&#21040;&#27169;&#25311;&#35757;&#32451;&#25968;&#25454;&#35823;&#24046;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.09412</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#21512;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#23454;&#38469;&#27700;&#21147;&#31995;&#32479;&#20013;&#27893;&#31449;&#30340;&#21487;&#25345;&#32493;&#24615;
&lt;/p&gt;
&lt;p&gt;
Hybrid Reinforcement Learning for Optimizing Pump Sustainability in Real-World Water Distribution Networks. (arXiv:2310.09412v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#28151;&#21512;&#24378;&#21270;&#23398;&#20064;&#30340;&#27893;&#31449;&#35843;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#25552;&#39640;&#23454;&#26102;&#25511;&#21046;&#30340;&#21516;&#26102;&#38477;&#20302;&#33021;&#28304;&#28040;&#32791;&#21644;&#36816;&#33829;&#25104;&#26412;&#12290;&#20256;&#32479;&#30340;&#20248;&#21270;&#25216;&#26415;&#30001;&#20110;&#32570;&#20047;&#25910;&#25947;&#24615;&#20445;&#35777;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#24378;&#21270;&#23398;&#20064;&#33021;&#22815;&#36866;&#24212;&#19981;&#30830;&#23450;&#24615;&#24182;&#23454;&#29616;&#23454;&#26102;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#30340;&#27700;&#21147;&#31995;&#32479;&#27169;&#25311;&#27169;&#22411;&#23545;&#20110;&#23454;&#26045;&#24378;&#21270;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#21069;&#30340;&#24212;&#29992;&#21463;&#21040;&#27169;&#25311;&#35757;&#32451;&#25968;&#25454;&#35823;&#24046;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#23454;&#38469;&#27700;&#21147;&#31995;&#32479;&#20013;&#27893;&#31449;&#35843;&#24230;&#20248;&#21270;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#26088;&#22312;&#25552;&#39640;&#23454;&#26102;&#25511;&#21046;&#30340;&#21516;&#26102;&#36981;&#23432;&#29289;&#29702;&#36816;&#34892;&#32422;&#26463;&#65292;&#20943;&#23569;&#33021;&#28304;&#28040;&#32791;&#21644;&#36816;&#33829;&#25104;&#26412;&#12290;&#20256;&#32479;&#30340;&#20248;&#21270;&#25216;&#26415;&#65292;&#22914;&#22522;&#20110;&#36827;&#21270;&#21644;&#36951;&#20256;&#31639;&#27861;&#65292;&#24448;&#24448;&#30001;&#20110;&#32570;&#20047;&#25910;&#25947;&#24615;&#20445;&#35777;&#32780;&#34920;&#29616;&#19981;&#20339;&#12290;&#30456;&#21453;&#65292;&#24378;&#21270;&#23398;&#20064;&#22312;&#36866;&#24212;&#19981;&#30830;&#23450;&#24615;&#21644;&#20943;&#23569;&#25512;&#26029;&#26102;&#38388;&#26041;&#38754;&#20855;&#26377;&#31361;&#20986;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#23454;&#26102;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#23454;&#26045;&#24378;&#21270;&#23398;&#20064;&#20381;&#36182;&#20110;&#20934;&#30830;&#30340;&#27700;&#21147;&#31995;&#32479;&#27169;&#25311;&#27169;&#22411;&#65292;&#20197;&#21069;&#30340;&#24212;&#29992;&#21463;&#21040;&#27169;&#25311;&#35757;&#32451;&#25968;&#25454;&#35823;&#24046;&#30340;&#38480;&#21046;&#12290;&#36825;&#20123;&#35823;&#24046;&#21487;&#33021;&#23548;&#33268;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#23398;&#20064;&#21040;&#35823;&#23548;&#24615;&#30340;&#27169;&#24335;&#21644;&#34892;&#20026;&#65292;&#24182;&#25512;&#33616;&#27425;&#20248;&#30340;&#36816;&#34892;&#31574;&#30053;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#8220;&#28151;&#21512;&#24378;&#21270;&#23398;&#20064;&#8221;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article addresses the pump-scheduling optimization problem to enhance real-time control of real-world water distribution networks (WDNs). Our primary objectives are to adhere to physical operational constraints while reducing energy consumption and operational costs. Traditional optimization techniques, such as evolution-based and genetic algorithms, often fall short due to their lack of convergence guarantees. Conversely, reinforcement learning (RL) stands out for its adaptability to uncertainties and reduced inference time, enabling real-time responsiveness. However, the effective implementation of RL is contingent on building accurate simulation models for WDNs, and prior applications have been limited by errors in simulation training data. These errors can potentially cause the RL agent to learn misleading patterns and actions and recommend suboptimal operational strategies. To overcome these challenges, we present an improved "hybrid RL" methodology. This method integrates th
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#24182;&#19988;&#22312;&#25991;&#26412;&#25688;&#35201;&#39046;&#22495;&#20063;&#34920;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.09411</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#23398;&#20064;&#35843;&#26597;&#25991;&#26412;&#25688;&#35201;&#30340;&#29616;&#29366;&#65306;&#19968;&#39033;&#32508;&#36848;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Surveying the Landscape of Text Summarization with Deep Learning: A Comprehensive Review. (arXiv:2310.09411v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09411
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#24182;&#19988;&#22312;&#25991;&#26412;&#25688;&#35201;&#39046;&#22495;&#20063;&#34920;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#36890;&#36807;&#20351;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#35821;&#35328;&#25968;&#25454;&#30340;&#22797;&#26434;&#34920;&#31034;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#65292;&#20174;&#32780;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#30340;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#19982;&#20256;&#32479;&#30340;NLP&#26041;&#27861;&#30456;&#21453;&#65292;&#28145;&#24230;&#23398;&#20064;NLP&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#22823;&#37327;&#25968;&#25454;&#26469;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#20854;&#33021;&#22815;&#23398;&#20064;&#35821;&#35328;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#21644;&#20851;&#31995;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#23398;&#20064;&#35821;&#35328;&#25968;&#25454;&#30340;&#23618;&#27425;&#34920;&#31034;&#65292;&#22788;&#29702;&#21487;&#21464;&#38271;&#24230;&#30340;&#36755;&#20837;&#24207;&#21015;&#65292;&#24182;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#36825;&#20351;&#23427;&#20204;&#38750;&#24120;&#36866;&#21512;NLP&#24212;&#29992;&#12290;&#30001;&#20110;&#25991;&#26412;&#25968;&#25454;&#30340;&#25351;&#25968;&#22686;&#38271;&#21644;&#23545;&#31616;&#27905;&#12289;&#36830;&#36143;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#25688;&#35201;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#65292;&#25991;&#26412;&#25688;&#35201;&#24050;&#25104;&#20026;NLP&#39046;&#22495;&#30340;&#19968;&#20010;&#20851;&#38190;&#30740;&#31350;&#39046;&#22495;&#12290;&#23558;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#25991;&#26412;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep learning has revolutionized natural language processing (NLP) by enabling the development of models that can learn complex representations of language data, leading to significant improvements in performance across a wide range of NLP tasks. Deep learning models for NLP typically use large amounts of data to train deep neural networks, allowing them to learn the patterns and relationships in language data. This is in contrast to traditional NLP approaches, which rely on hand-engineered features and rules to perform NLP tasks. The ability of deep neural networks to learn hierarchical representations of language data, handle variable-length input sequences, and perform well on large datasets makes them well-suited for NLP applications. Driven by the exponential growth of textual data and the increasing demand for condensed, coherent, and informative summaries, text summarization has been a critical research area in the field of NLP. Applying deep learning to text su
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19987;&#23478;&#27169;&#22411;&#30340;&#21487;&#35782;&#21035;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#24403;&#28508;&#21464;&#37327;&#22343;&#21248;&#20998;&#24067;&#26102;&#65292;&#20351;&#29992;&#19982;&#21442;&#25968;&#25968;&#37327;&#30456;&#31561;&#30340;&#21487;&#35266;&#27979;&#37327;&#21487;&#20197;&#23436;&#20840;&#35782;&#21035;&#27169;&#22411;&#65307;&#32780;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#21487;&#35782;&#21035;&#24615;&#20173;&#28982;&#25104;&#31435;&#65292;&#20294;&#38656;&#35201;&#26356;&#22810;&#30340;&#35266;&#27979;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2310.09397</link><description>&lt;p&gt;
&#19987;&#23478;&#27169;&#22411;&#30340;&#21487;&#35782;&#21035;&#24615;
&lt;/p&gt;
&lt;p&gt;
Identifiability of Product of Experts Models. (arXiv:2310.09397v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19987;&#23478;&#27169;&#22411;&#30340;&#21487;&#35782;&#21035;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#24403;&#28508;&#21464;&#37327;&#22343;&#21248;&#20998;&#24067;&#26102;&#65292;&#20351;&#29992;&#19982;&#21442;&#25968;&#25968;&#37327;&#30456;&#31561;&#30340;&#21487;&#35266;&#27979;&#37327;&#21487;&#20197;&#23436;&#20840;&#35782;&#21035;&#27169;&#22411;&#65307;&#32780;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#21487;&#35782;&#21035;&#24615;&#20173;&#28982;&#25104;&#31435;&#65292;&#20294;&#38656;&#35201;&#26356;&#22810;&#30340;&#35266;&#27979;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19987;&#23478;&#27169;&#22411;&#26159;&#20998;&#23618;&#32593;&#32476;&#65292;&#27599;&#20010;&#33410;&#28857;&#30340;&#20540;&#26159;&#20854;&#36755;&#20837;&#20540;&#65288;&#21487;&#33021;&#21462;&#21453;&#65289;&#30340;&#20056;&#31215;&#12290;&#36825;&#20123;&#27169;&#22411;&#34987;&#24341;&#20837;&#20026;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#29983;&#25104;&#28385;&#36275;&#35768;&#22810;&#20302;&#32500;&#32422;&#26463;&#30340;&#39640;&#32500;&#25968;&#25454; - &#20174;&#32780;&#20801;&#35768;&#27599;&#20010;&#19987;&#23478;&#25191;&#34892;&#31616;&#21333;&#30340;&#20219;&#21153;&#12290;&#19987;&#23478;&#27169;&#22411;&#22312;&#23398;&#20064;&#20013;&#26377;&#21508;&#31181;&#24212;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20855;&#26377;&#20108;&#36827;&#21046;&#28508;&#21464;&#37327;&#23618;&#21644;&#26465;&#20214;&#29420;&#31435;&#20110;&#28508;&#21464;&#37327;&#30340;&#20108;&#36827;&#21046;&#21487;&#35266;&#27979;&#23618;&#30340;&#19987;&#23478;&#27169;&#22411;&#30340;&#21487;&#35782;&#21035;&#24615;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#26368;&#20248;&#19978;&#30028;&#34920;&#26126;&#65292;&#35782;&#21035;&#35813;&#27169;&#22411;&#25152;&#38656;&#30340;&#21487;&#35266;&#27979;&#37327;&#25968;&#30446;&#38543;&#21442;&#25968;&#25968;&#37327;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#25105;&#20204;&#35777;&#26126;&#65306;&#65288;a&#65289;&#24403;&#28508;&#21464;&#37327;&#22343;&#21248;&#20998;&#24067;&#26102;&#65292;&#27169;&#22411;&#21487;&#36890;&#36807;&#19982;&#21442;&#25968;&#25968;&#37327;&#30456;&#31561;&#30340;&#21487;&#35266;&#27979;&#37327;&#26469;&#35782;&#21035;&#65288;&#20174;&#32780;&#36798;&#21040;&#26368;&#20339;&#21487;&#35782;&#21035;&#24615;&#65289;&#12290;&#65288;b&#65289;&#22312;&#26356;&#19968;&#33324;&#30340;&#24773;&#20917;&#19979;&#65292;&#24403;&#28508;&#21464;&#37327;&#26381;&#20174;&#20219;&#24847;&#20998;&#24067;&#26102;&#65292;&#27169;&#22411;&#30340;&#21487;&#35782;&#21035;&#24615;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
Product of experts (PoE) are layered networks in which the value at each node is an AND (or product) of the values (possibly negated) at its inputs. These were introduced as a neural network architecture that can efficiently learn to generate high-dimensional data which satisfy many low-dimensional constraints -- thereby allowing each individual expert to perform a simple task. PoEs have found a variety of applications in learning.  We study the problem of identifiability of a product of experts model having a layer of binary latent variables, and a layer of binary observables that are iid conditional on the latents. The previous best upper bound on the number of observables needed to identify the model was exponential in the number of parameters. We show: (a) When the latents are uniformly distributed, the model is identifiable with a number of observables equal to the number of parameters (and hence best possible). (b) In the more general case of arbitrarily distributed latents, the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#21106;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#20010;&#31070;&#32463;&#25910;&#21457;&#22120;&#20043;&#38388;&#23545;&#40784;&#35821;&#20041;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#23616;&#37096;&#24494;&#35843;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25511;&#21046;&#35745;&#31639;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2310.09394</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#21106;&#23398;&#20064;&#30340;&#35821;&#20041;&#23545;&#40784;&#29992;&#20110;&#24377;&#24615;&#22810;&#29992;&#25143;&#35821;&#20041;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Semantics Alignment via Split Learning for Resilient Multi-User Semantic Communication. (arXiv:2310.09394v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#21106;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#20010;&#31070;&#32463;&#25910;&#21457;&#22120;&#20043;&#38388;&#23545;&#40784;&#35821;&#20041;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#23616;&#37096;&#24494;&#35843;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25511;&#21046;&#35745;&#31639;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#35821;&#20041;&#36890;&#20449;&#30340;&#30740;&#31350;&#24120;&#24120;&#20381;&#36182;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25910;&#21457;&#22120;&#65292;&#22914;&#28145;&#24230;&#32852;&#21512;&#28304;&#21644;&#20449;&#36947;&#32534;&#30721;&#65288;DeepJSCC&#65289;&#12290;&#19982;&#20256;&#32479;&#30340;&#25910;&#21457;&#22120;&#19981;&#21516;&#65292;&#36825;&#20123;&#31070;&#32463;&#25910;&#21457;&#22120;&#21487;&#20197;&#20351;&#29992;&#23454;&#38469;&#30340;&#28304;&#25968;&#25454;&#21644;&#20449;&#36947;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#20854;&#33021;&#22815;&#25552;&#21462;&#21644;&#20256;&#36882;&#35821;&#20041;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#31070;&#32463;&#25910;&#21457;&#22120;&#22266;&#26377;&#22320;&#20559;&#21521;&#20110;&#29305;&#23450;&#30340;&#28304;&#25968;&#25454;&#21644;&#20449;&#36947;&#65292;&#20351;&#24471;&#19981;&#21516;&#30340;&#25910;&#21457;&#22120;&#24456;&#38590;&#29702;&#35299;&#39044;&#26399;&#30340;&#35821;&#20041;&#65292;&#23588;&#20854;&#26159;&#22312;&#21021;&#22987;&#36935;&#21040;&#26102;&#12290;&#20026;&#20102;&#22312;&#22810;&#20010;&#31070;&#32463;&#25910;&#21457;&#22120;&#20043;&#38388;&#23545;&#40784;&#35821;&#20041;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#20998;&#21106;&#23398;&#20064;&#65288;SL&#65289;&#21644;&#23616;&#37096;&#31070;&#32463;&#32593;&#32476;&#24494;&#35843;&#25216;&#26415;&#12290;&#22312;&#36825;&#31181;&#31216;&#20026;&#20855;&#26377;&#23618;&#20923;&#32467;&#30340;SL&#65288;SLF&#65289;&#30340;&#26041;&#27861;&#20013;&#65292;&#27599;&#20010;&#32534;&#30721;&#22120;&#19979;&#36733;&#19968;&#20010;&#19981;&#23545;&#40784;&#30340;&#35299;&#30721;&#22120;&#65292;&#24182;&#22312;&#26412;&#22320;&#24494;&#35843;&#36825;&#20123;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#19968;&#37096;&#20998;&#12290;&#36890;&#36807;&#35843;&#25972;&#36825;&#20010;&#27604;&#20363;&#65292;SLF&#21487;&#20197;&#25511;&#21046;&#35745;&#31639;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;&#20223;&#30495;&#32467;&#26524;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies on semantic communication commonly rely on neural network (NN) based transceivers such as deep joint source and channel coding (DeepJSCC). Unlike traditional transceivers, these neural transceivers are trainable using actual source data and channels, enabling them to extract and communicate semantics. On the flip side, each neural transceiver is inherently biased towards specific source data and channels, making different transceivers difficult to understand intended semantics, particularly upon their initial encounter. To align semantics over multiple neural transceivers, we propose a distributed learning based solution, which leverages split learning (SL) and partial NN fine-tuning techniques. In this method, referred to as SL with layer freezing (SLF), each encoder downloads a misaligned decoder, and locally fine-tunes a fraction of these encoder-decoder NN layers. By adjusting this fraction, SLF controls computing and communication costs. Simulation results confirm t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;U-Nets&#65292;&#36890;&#36807;3D&#38647;&#36798;&#21453;&#23556;&#29575;&#65292;&#25104;&#21151;&#22320;&#20272;&#35745;&#20102;&#26368;&#22823;&#22402;&#30452;&#36895;&#24230;&#21450;&#20854;&#38754;&#31215;&#33539;&#22260;&#65292;&#24182;&#37319;&#29992;Sinh-arcsinh-normal&#65288;SHASH&#65289;&#20998;&#24067;&#21442;&#25968;&#22238;&#24402;&#25216;&#26415;&#36827;&#34892;&#20102;&#30830;&#23450;&#24615;&#21644;&#27010;&#29575;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.09392</link><description>&lt;p&gt;
&#20174;&#38647;&#36798;&#20013;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20272;&#35745;&#26368;&#22823;&#22402;&#30452;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Estimation of Maximum Vertical Velocity from Radar. (arXiv:2310.09392v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09392
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;U-Nets&#65292;&#36890;&#36807;3D&#38647;&#36798;&#21453;&#23556;&#29575;&#65292;&#25104;&#21151;&#22320;&#20272;&#35745;&#20102;&#26368;&#22823;&#22402;&#30452;&#36895;&#24230;&#21450;&#20854;&#38754;&#31215;&#33539;&#22260;&#65292;&#24182;&#37319;&#29992;Sinh-arcsinh-normal&#65288;SHASH&#65289;&#20998;&#24067;&#21442;&#25968;&#22238;&#24402;&#25216;&#26415;&#36827;&#34892;&#20102;&#30830;&#23450;&#24615;&#21644;&#27010;&#29575;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26159;&#20005;&#37325;&#22825;&#27668;&#28798;&#23475;&#30340;&#28304;&#22836;&#65292;&#20294;&#23545;&#24555;&#36895;&#19978;&#21319;&#27668;&#27969;&#65288;&#21363;&#19978;&#21319;&#27668;&#27969;&#65289;&#30340;&#37327;&#21270;&#20173;&#26080;&#27861;&#29992;&#20110;&#25805;&#20316;&#39044;&#27979;&#12290;&#20687;&#21355;&#26143;&#22270;&#20687;&#20013;&#30340;&#36879;&#39030;&#21306;&#22495;&#36825;&#26679;&#30340;&#19978;&#21319;&#27668;&#27969;&#20195;&#29702;&#29289;&#24050;&#34987;&#19982;&#20005;&#37325;&#22825;&#27668;&#28798;&#23475;&#32852;&#31995;&#36215;&#26469;&#65292;&#20294;&#21482;&#19982;&#24635;&#20307;&#39118;&#26292;&#19978;&#21319;&#27668;&#27969;&#30340;&#19968;&#23567;&#37096;&#20998;&#26377;&#20851;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21363;U-Nets&#65292;&#26159;&#21542;&#33021;&#22815;&#20165;&#21033;&#29992;&#19977;&#32500;&#65288;3D&#65289;&#26684;&#32593;&#38647;&#36798;&#21453;&#23556;&#29575;&#65292;&#31934;&#30830;&#22320;&#25552;&#21462;&#26368;&#22823;&#22402;&#30452;&#36895;&#24230;&#21450;&#20854;&#38754;&#31215;&#33539;&#22260;&#12290;&#35813;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20351;&#29992;&#27169;&#25311;&#38647;&#36798;&#21453;&#23556;&#29575;&#21644;&#22402;&#30452;&#36895;&#24230;&#35757;&#32451;&#20110;&#22269;&#23478;&#20005;&#37325;&#39118;&#26292;&#23454;&#39564;&#23460;&#30340;&#39044;&#27979;&#39044;&#35686;&#31995;&#32479;&#65288;WoFS&#65289;&#12290;&#37319;&#29992;Sinh-arcsinh-normal&#65288;SHASH&#65289;&#20998;&#24067;&#30340;&#21442;&#25968;&#22238;&#24402;&#25216;&#26415;&#26469;&#36866;&#24212;UNets&#65292;&#20801;&#35768;&#23545;&#26368;&#22823;&#22402;&#30452;&#36895;&#24230;&#36827;&#34892;&#30830;&#23450;&#24615;&#21644;&#27010;&#29575;&#39044;&#27979;&#12290;&#32463;&#36807;&#36229;&#21442;&#25968;&#25628;&#32034;&#21518;&#65292;&#36873;&#20986;&#20102;&#26368;&#20339;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite being the source region of severe weather hazards, the quantification of the fast current of upward moving air (i.e., updraft) remains unavailable for operational forecasting. Updraft proxies, like overshooting top area from satellite images, have been linked to severe weather hazards but only relate to a limited portion of the total storm updraft. This study investigates if a machine learning model, namely U-Nets, can skillfully retrieve maximum vertical velocity and its areal extent from 3-dimensional (3D) gridded radar reflectivity alone. The machine learning model is trained using simulated radar reflectivity and vertical velocity from the National Severe Storm Laboratory's convection permitting Warn on Forecast System (WoFS). A parametric regression technique using the Sinh-arcsinh-normal (SHASH) distribution is adapted to run with UNets, allowing for both deterministic and probabilistic predictions of maximum vertical velocity. The best models after hyperparameter search 
&lt;/p&gt;</description></item><item><title>CORN&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23558;&#20840;&#21442;&#32771;&#21644;&#38750;&#21442;&#32771;&#38899;&#39057;&#24230;&#37327;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#23581;&#35797;&#22312;&#35757;&#32451;&#26102;&#21516;&#26102;&#35757;&#32451;&#36825;&#20004;&#31181;&#27169;&#22411;&#12290;CORN FR&#27169;&#24335;&#21516;&#26102;&#20855;&#22791;&#20840;&#21442;&#32771;&#21644;&#38750;&#21442;&#32771;&#24230;&#37327;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09388</link><description>&lt;p&gt;
CORN: &#20840;&#21442;&#32771;&#21644;&#38750;&#21442;&#32771;&#38899;&#39057;&#24230;&#37327;&#30340;&#20849;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CORN: Co-Trained Full-Reference And No-Reference Audio Metrics. (arXiv:2310.09388v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09388
&lt;/p&gt;
&lt;p&gt;
CORN&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23558;&#20840;&#21442;&#32771;&#21644;&#38750;&#21442;&#32771;&#38899;&#39057;&#24230;&#37327;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#23581;&#35797;&#22312;&#35757;&#32451;&#26102;&#21516;&#26102;&#35757;&#32451;&#36825;&#20004;&#31181;&#27169;&#22411;&#12290;CORN FR&#27169;&#24335;&#21516;&#26102;&#20855;&#22791;&#20840;&#21442;&#32771;&#21644;&#38750;&#21442;&#32771;&#24230;&#37327;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24863;&#30693;&#35780;&#20272;&#26159;&#21508;&#31181;&#38899;&#39057;&#22788;&#29702;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#26041;&#38754;&#12290;&#20840;&#21442;&#32771;&#65288;FR&#65289;&#25110;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#24230;&#37327;&#20381;&#36182;&#20110;&#39640;&#36136;&#37327;&#30340;&#21442;&#32771;&#24405;&#38899;&#65292;&#23558;&#20854;&#19982;&#24405;&#38899;&#30340;&#20302;&#36136;&#37327;&#25110;&#25439;&#22351;&#29256;&#26412;&#36827;&#34892;&#27604;&#36739;&#20197;&#36827;&#34892;&#35780;&#20272;&#12290;&#30456;&#21453;&#65292;&#38750;&#21442;&#32771;&#65288;NR&#65289;&#24230;&#37327;&#35780;&#20272;&#24405;&#38899;&#32780;&#19981;&#20381;&#36182;&#21442;&#32771;&#12290;FR&#21644;NR&#20004;&#31181;&#26041;&#27861;&#30456;&#23545;&#20110;&#24444;&#27492;&#37117;&#20855;&#26377;&#20248;&#21183;&#21644;&#32570;&#28857;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#31216;&#20026;CORN&#65292;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#21516;&#26102;&#35757;&#32451;FR&#21644;NR&#27169;&#22411;&#12290;&#35757;&#32451;&#23436;&#25104;&#21518;&#65292;&#21487;&#20197;&#29420;&#31435;&#24212;&#29992;&#36825;&#20123;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#39044;&#27979;&#20960;&#20010;&#24120;&#35265;&#30340;&#23458;&#35266;&#24230;&#37327;&#25351;&#26631;&#20197;&#21450;&#22312;&#20004;&#31181;&#19981;&#21516;&#26550;&#26500;&#19978;&#36827;&#34892;&#35780;&#20272;CORN&#12290;&#20351;&#29992;CORN&#35757;&#32451;&#30340;NR&#27169;&#22411;&#22312;&#35757;&#32451;&#26399;&#38388;&#21487;&#20197;&#35775;&#38382;&#21442;&#32771;&#24405;&#38899;&#65292;&#22240;&#27492;&#21487;&#20197;&#39044;&#26399;&#65292;&#23427;&#22987;&#32456;&#20248;&#20110;&#29420;&#31435;&#35757;&#32451;&#30340;&#22522;&#32447;NR&#27169;&#22411;&#12290;&#26356;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26159;CORN FR&#27169;&#24335;&#21487;&#20197;&#21516;&#26102;&#25552;&#20379;&#20840;&#21442;&#32771;&#21644;&#38750;&#21442;&#32771;&#24230;&#37327;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Perceptual evaluation constitutes a crucial aspect of various audio-processing tasks. Full reference (FR) or similarity-based metrics rely on high-quality reference recordings, to which lower-quality or corrupted versions of the recording may be compared for evaluation. In contrast, no-reference (NR) metrics evaluate a recording without relying on a reference. Both the FR and NR approaches exhibit advantages and drawbacks relative to each other. In this paper, we present a novel framework called CORN that amalgamates these dual approaches, concurrently training both FR and NR models together. After training, the models can be applied independently. We evaluate CORN by predicting several common objective metrics and across two different architectures. The NR model trained using CORN has access to a reference recording during training, and thus, as one would expect, it consistently outperforms baseline NR models trained independently. Perhaps even more remarkable is that the CORN FR mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LL-VQ-VAE&#30340;&#23398;&#20064;&#21487;&#23398;&#20064;&#30340;&#26684;&#23376;&#21521;&#37327;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#26367;&#25442;&#21521;&#37327;&#37327;&#21270;&#23618;&#26469;&#23454;&#29616;&#39640;&#25928;&#34920;&#31034;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#30456;&#21516;&#35757;&#32451;&#26465;&#20214;&#19979;&#20855;&#26377;&#26356;&#20302;&#30340;&#37325;&#26500;&#35823;&#24046;&#65292;&#35757;&#32451;&#26102;&#38388;&#26356;&#30701;&#65292;&#21442;&#25968;&#25968;&#37327;&#24658;&#23450;&#12290;</title><link>http://arxiv.org/abs/2310.09382</link><description>&lt;p&gt;
LL-VQ-VAE: &#23398;&#20064;&#21487;&#23398;&#20064;&#30340;&#26684;&#23376;&#21521;&#37327;&#37327;&#21270;&#20197;&#25552;&#39640;&#34920;&#31034;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
LL-VQ-VAE: Learnable Lattice Vector-Quantization For Efficient Representations. (arXiv:2310.09382v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LL-VQ-VAE&#30340;&#23398;&#20064;&#21487;&#23398;&#20064;&#30340;&#26684;&#23376;&#21521;&#37327;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#26367;&#25442;&#21521;&#37327;&#37327;&#21270;&#23618;&#26469;&#23454;&#29616;&#39640;&#25928;&#34920;&#31034;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#30456;&#21516;&#35757;&#32451;&#26465;&#20214;&#19979;&#20855;&#26377;&#26356;&#20302;&#30340;&#37325;&#26500;&#35823;&#24046;&#65292;&#35757;&#32451;&#26102;&#38388;&#26356;&#30701;&#65292;&#21442;&#25968;&#25968;&#37327;&#24658;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21487;&#23398;&#20064;&#30340;&#26684;&#23376;&#21521;&#37327;&#37327;&#21270;&#26041;&#27861;(LL-VQ-VAE)&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#23398;&#20064;&#31163;&#25955;&#34920;&#31034;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;VQ-VAE&#20013;&#30340;&#21521;&#37327;&#37327;&#21270;&#23618;&#26367;&#25442;&#20026;&#22522;&#20110;&#26684;&#23376;&#30340;&#31163;&#25955;&#21270;&#26041;&#27861;&#12290;&#21487;&#23398;&#20064;&#30340;&#26684;&#23376;&#23545;&#25152;&#26377;&#31163;&#25955;&#23884;&#20837;&#26045;&#21152;&#19968;&#31181;&#32467;&#26500;&#65292;&#38450;&#27490;&#30721;&#26412;&#23849;&#28291;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#30721;&#26412;&#21033;&#29992;&#29575;&#12290;&#19982;VQ-VAE&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30456;&#21516;&#30340;&#35757;&#32451;&#26465;&#20214;&#19979;&#24471;&#21040;&#20102;&#26356;&#20302;&#30340;&#37325;&#26500;&#35823;&#24046;&#65292;&#35757;&#32451;&#26102;&#38388;&#20165;&#20026;&#19968;&#23567;&#37096;&#20998;&#65292;&#24182;&#19988;&#20855;&#26377;&#24658;&#23450;&#25968;&#37327;&#30340;&#21442;&#25968;&#65288;&#31561;&#20110;&#23884;&#20837;&#32500;&#24230;D&#65289;&#65292;&#20351;&#20854;&#25104;&#20026;&#19968;&#31181;&#38750;&#24120;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;FFHQ-1024&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#36825;&#20123;&#32467;&#26524;&#65292;&#24182;&#21253;&#25324;&#20102;FashionMNIST&#21644;Celeb-A&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we introduce learnable lattice vector quantization and demonstrate its effectiveness for learning discrete representations. Our method, termed LL-VQ-VAE, replaces the vector quantization layer in VQ-VAE with lattice-based discretization. The learnable lattice imposes a structure over all discrete embeddings, acting as a deterrent against codebook collapse, leading to high codebook utilization. Compared to VQ-VAE, our method obtains lower reconstruction errors under the same training conditions, trains in a fraction of the time, and with a constant number of parameters (equal to the embedding dimension $D$), making it a very scalable approach. We demonstrate these results on the FFHQ-1024 dataset and include FashionMNIST and Celeb-A.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#25506;&#35752;&#20102;&#20943;&#23569;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#24615;&#21035;&#23646;&#24615;&#20559;&#35265;&#23545;&#24037;&#36164;&#39044;&#27979;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#31038;&#20250;&#20013;&#23454;&#26045;&#28151;&#21512;&#27169;&#22411;&#30340;&#24517;&#35201;&#24615;&#65292;&#20197;&#23454;&#29616;&#20844;&#27491;&#21644;&#21253;&#23481;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.09373</link><description>&lt;p&gt;
&#35782;&#21035;&#24182;&#30740;&#31350;&#25104;&#20154;&#25968;&#25454;&#38598;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Identifying and examining machine learning biases on Adult dataset. (arXiv:2310.09373v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09373
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#25506;&#35752;&#20102;&#20943;&#23569;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#24615;&#21035;&#23646;&#24615;&#20559;&#35265;&#23545;&#24037;&#36164;&#39044;&#27979;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#31038;&#20250;&#20013;&#23454;&#26045;&#28151;&#21512;&#27169;&#22411;&#30340;&#24517;&#35201;&#24615;&#65292;&#20197;&#23454;&#29616;&#20844;&#27491;&#21644;&#21253;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#28145;&#20837;&#25506;&#35752;&#20102;&#20943;&#23569;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#20005;&#26684;&#30340;&#26041;&#27861;&#20840;&#38754;&#35780;&#20272;&#20102;&#21508;&#20010;&#20998;&#31867;&#21464;&#37327;&#19978;&#30340;&#20559;&#35265;&#65292;&#26368;&#32456;&#25581;&#31034;&#20102;&#26126;&#26174;&#30340;&#24615;&#21035;&#23646;&#24615;&#20559;&#35265;&#12290;&#23454;&#35777;&#35777;&#25454;&#25581;&#31034;&#20102;&#26174;&#33879;&#30340;&#22522;&#20110;&#24615;&#21035;&#30340;&#24037;&#36164;&#39044;&#27979;&#24046;&#36317;&#65306;&#22312;&#23558;&#24615;&#21035;&#23646;&#24615;&#25913;&#20026;&#22899;&#24615;&#26102;&#65292;&#30007;&#24615;&#30340;&#39044;&#27979;&#24037;&#36164;&#20174;&#21021;&#22987;&#30340;902.91&#32654;&#20803;&#22823;&#24133;&#38477;&#33267;774.31&#32654;&#20803;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;Kullback-Leibler&#25955;&#24230;&#24471;&#20998;&#25351;&#20986;&#20102;&#24615;&#21035;&#20559;&#35265;&#65292;&#20540;&#36229;&#36807;0.13&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#20013;&#12290;&#37319;&#29992;&#38598;&#25104;&#23398;&#20064;&#26377;&#21161;&#20110;&#36861;&#27714;&#20844;&#24179;&#21644;&#36879;&#26126;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#22534;&#21472;&#27169;&#22411;&#19982;&#21508;&#20010;&#21333;&#29420;&#27169;&#22411;&#19968;&#33268;&#65292;&#30830;&#35748;&#20102;&#27169;&#22411;&#20559;&#35265;&#30340;&#24377;&#24615;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;&#36947;&#24503;&#32771;&#34385;&#65292;&#24182;&#20027;&#24352;&#22312;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#31038;&#20250;&#20013;&#23454;&#26045;&#28151;&#21512;&#27169;&#22411;&#65292;&#20197;&#36861;&#27714;&#20844;&#27491;&#21644;&#21253;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research delves into the reduction of machine learning model bias through Ensemble Learning. Our rigorous methodology comprehensively assesses bias across various categorical variables, ultimately revealing a pronounced gender attribute bias. The empirical evidence unveils a substantial gender-based wage prediction disparity: wages predicted for males, initially at \$902.91, significantly decrease to \$774.31 when the gender attribute is alternated to females. Notably, Kullback-Leibler divergence scores point to gender bias, with values exceeding 0.13, predominantly within tree-based models. Employing Ensemble Learning elucidates the quest for fairness and transparency. Intriguingly, our findings reveal that the stacked model aligns with individual models, confirming the resilience of model bias. This study underscores ethical considerations and advocates the implementation of hybrid models for a data-driven society marked by impartiality and inclusivity.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#33021;&#22815;&#21457;&#20986;&#22768;&#38899;&#30340;&#27874;&#26031;&#35821;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#29992;&#20110;&#25351;&#23548;&#29992;&#25143;&#36827;&#34892;&#22522;&#20110;&#20381;&#24651;&#29702;&#35770;&#30340;&#33258;&#25105;&#20381;&#24651;&#25216;&#26415;&#12290;&#36890;&#36807;&#20351;&#29992;&#35268;&#21017;&#21644;&#20998;&#31867;&#27169;&#22359;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#21487;&#20197;&#29702;&#35299;&#29992;&#25143;&#36755;&#20837;&#24182;&#25512;&#33616;&#36866;&#24403;&#30340;&#33258;&#25105;&#20381;&#24651;&#32451;&#20064;&#12290;&#35813;&#30740;&#31350;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#20934;&#30830;&#29575;&#36229;&#36807;92%&#30340;&#24773;&#24863;&#20998;&#26512;&#27169;&#22359;&#65292;&#20197;&#35782;&#21035;&#29992;&#25143;&#24773;&#24863;&#12290;&#36825;&#39033;&#24037;&#20316;&#26377;&#21161;&#20110;&#22312;&#21518;&#30123;&#24773;&#26102;&#20195;&#25552;&#20379;&#25968;&#23383;&#24515;&#29702;&#30103;&#27861;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.09362</link><description>&lt;p&gt;
&#20174;&#35789;&#35821;&#21644;&#32451;&#20064;&#21040;&#20581;&#24247;&#65306;&#29992;&#20110;&#33258;&#25105;&#20381;&#24651;&#25216;&#26415;&#30340;&#27874;&#26031;&#35821;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
From Words and Exercises to Wellness: Farsi Chatbot for Self-Attachment Technique. (arXiv:2310.09362v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09362
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#33021;&#22815;&#21457;&#20986;&#22768;&#38899;&#30340;&#27874;&#26031;&#35821;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#29992;&#20110;&#25351;&#23548;&#29992;&#25143;&#36827;&#34892;&#22522;&#20110;&#20381;&#24651;&#29702;&#35770;&#30340;&#33258;&#25105;&#20381;&#24651;&#25216;&#26415;&#12290;&#36890;&#36807;&#20351;&#29992;&#35268;&#21017;&#21644;&#20998;&#31867;&#27169;&#22359;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#21487;&#20197;&#29702;&#35299;&#29992;&#25143;&#36755;&#20837;&#24182;&#25512;&#33616;&#36866;&#24403;&#30340;&#33258;&#25105;&#20381;&#24651;&#32451;&#20064;&#12290;&#35813;&#30740;&#31350;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#20934;&#30830;&#29575;&#36229;&#36807;92%&#30340;&#24773;&#24863;&#20998;&#26512;&#27169;&#22359;&#65292;&#20197;&#35782;&#21035;&#29992;&#25143;&#24773;&#24863;&#12290;&#36825;&#39033;&#24037;&#20316;&#26377;&#21161;&#20110;&#22312;&#21518;&#30123;&#24773;&#26102;&#20195;&#25552;&#20379;&#25968;&#23383;&#24515;&#29702;&#30103;&#27861;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21518;&#30123;&#24773;&#26102;&#20195;&#65292;&#31038;&#20132;&#23396;&#31435;&#21644;&#25233;&#37057;&#28966;&#34385;&#30151;&#30340;&#24739;&#30149;&#29575;&#25856;&#21319;&#30340;&#32972;&#26223;&#19979;&#65292;&#22522;&#20110;&#25968;&#23383;&#24515;&#29702;&#30103;&#27861;&#30340;&#23545;&#35805;&#20195;&#29702;&#30456;&#23545;&#20110;&#20256;&#32479;&#30103;&#27861;&#20250;&#21457;&#25381;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#33021;&#22815;&#21457;&#20986;&#22768;&#38899;&#30340;&#27874;&#26031;&#35821;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#25351;&#23548;&#29992;&#25143;&#36827;&#34892;&#33258;&#25105;&#20381;&#24651;(Self-Attachment, SAT)&#25216;&#26415;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#20381;&#24651;&#29702;&#35770;&#30340;&#26032;&#22411;&#12289;&#33258;&#25105;&#31649;&#29702;&#12289;&#20840;&#38754;&#30340;&#24515;&#29702;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#20351;&#29992;&#19968;&#31995;&#21015;&#22522;&#20110;&#35268;&#21017;&#21644;&#20998;&#31867;&#30340;&#27169;&#22359;&#26469;&#29702;&#35299;&#29992;&#25143;&#22312;&#23545;&#35805;&#20013;&#30340;&#36755;&#20837;&#65292;&#24182;&#30456;&#24212;&#22320;&#23548;&#33322;&#23545;&#35805;&#27969;&#31243;&#22270;&#65292;&#26681;&#25454;&#29992;&#25143;&#30340;&#24773;&#24863;&#21644;&#24515;&#29702;&#29366;&#24577;&#25512;&#33616;&#36866;&#24403;&#30340;SAT&#32451;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#36229;&#36807;6,000&#27425;&#35805;&#35821;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24773;&#24863;&#20998;&#26512;&#27169;&#22359;&#65292;&#21487;&#20197;&#23558;&#29992;&#25143;&#30340;&#24773;&#24863;&#20998;&#20026;12&#20010;&#31867;&#21035;&#65292;&#20934;&#30830;&#29575;&#36229;&#36807;92%&#12290;&#20026;&#20102;&#20445;&#25345;&#23545;&#35805;&#30340;&#26032;&#39062;&#21644;&#21560;&#24341;&#21147;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#22238;&#31572;&#26159;&#20174;&#22823;&#37327;&#35805;&#35821;&#25968;&#25454;&#38598;&#20013;&#26816;&#32034;&#24471;&#21040;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the wake of the post-pandemic era, marked by social isolation and surging rates of depression and anxiety, conversational agents based on digital psychotherapy can play an influential role compared to traditional therapy sessions. In this work, we develop a voice-capable chatbot in Farsi to guide users through Self-Attachment (SAT), a novel, self-administered, holistic psychological technique based on attachment theory. Our chatbot uses a dynamic array of rule-based and classification-based modules to comprehend user input throughout the conversation and navigates a dialogue flowchart accordingly, recommending appropriate SAT exercises that depend on the user's emotional and mental state. In particular, we collect a dataset of over 6,000 utterances and develop a novel sentiment-analysis module that classifies user sentiment into 12 classes, with accuracy above 92%. To keep the conversation novel and engaging, the chatbot's responses are retrieved from a large dataset of utterances c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#24182;&#22238;&#31572;&#20102;&#19977;&#20010;&#38382;&#39064;&#65306;&#25105;&#20204;&#20026;&#20160;&#20040;&#20851;&#24515;&#40065;&#26834;&#24615;&#30740;&#31350;&#65311;&#25105;&#20204;&#20026;&#20160;&#20040;&#20851;&#24515;$\ell_p$&#26377;&#30028;&#30340;&#23041;&#32961;&#27169;&#22411;&#65311;&#25105;&#20204;&#20026;&#20160;&#20040;&#20851;&#24515;&#35748;&#35777;&#32780;&#19981;&#26159;&#32463;&#39564;&#24615;&#38450;&#24481;&#65311;</title><link>http://arxiv.org/abs/2310.09361</link><description>&lt;p&gt;
&#26159;&#21542;&#20173;&#28982;&#20540;&#24471;&#23545;$\ell_p$&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#35748;&#35777;?
&lt;/p&gt;
&lt;p&gt;
Is Certifying $\ell_p$ Robustness Still Worthwhile?. (arXiv:2310.09361v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#24182;&#22238;&#31572;&#20102;&#19977;&#20010;&#38382;&#39064;&#65306;&#25105;&#20204;&#20026;&#20160;&#20040;&#20851;&#24515;&#40065;&#26834;&#24615;&#30740;&#31350;&#65311;&#25105;&#20204;&#20026;&#20160;&#20040;&#20851;&#24515;$\ell_p$&#26377;&#30028;&#30340;&#23041;&#32961;&#27169;&#22411;&#65311;&#25105;&#20204;&#20026;&#20160;&#20040;&#20851;&#24515;&#35748;&#35777;&#32780;&#19981;&#26159;&#32463;&#39564;&#24615;&#38450;&#24481;&#65311;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#21508;&#31181;&#25915;&#20987;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#26222;&#21450;&#24615;&#65292;&#20197;&#21450;&#26088;&#22312;&#38450;&#33539;&#27492;&#31867;&#25915;&#20987;&#25152;&#24102;&#26469;&#30340;&#23433;&#20840;&#28431;&#27934;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;&#26412;&#25991;&#20027;&#35201;&#20851;&#27880;&#25552;&#20379;&#23545;$\ell_p$&#26377;&#30028;&#25915;&#20987;&#25552;&#20379;&#21487;&#35777;&#26126;&#20445;&#35777;&#30340;&#38450;&#24481;&#12290;&#35748;&#35777;&#38450;&#24481;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#23558;&#40065;&#26834;&#24615;&#35748;&#35777;&#20174;&#29609;&#20855;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#25193;&#23637;&#21040;&#20687;ImageNet&#20998;&#31867;&#36825;&#26679;&#30340;&#22823;&#35268;&#27169;&#38382;&#39064;&#12290;&#34429;&#28982;&#36825;&#26080;&#30097;&#26159;&#19968;&#20010;&#26377;&#36259;&#30340;&#23398;&#26415;&#38382;&#39064;&#65292;&#20294;&#38543;&#30528;&#39046;&#22495;&#30340;&#25104;&#29087;&#65292;&#20854;&#22312;&#23454;&#36341;&#20013;&#30340;&#24433;&#21709;&#23578;&#19981;&#28165;&#26970;&#65292;&#22240;&#27492;&#25105;&#20204;&#21457;&#29616;&#37325;&#26032;&#23457;&#35270;&#32487;&#32493;&#36827;&#34892;&#36825;&#19968;&#30740;&#31350;&#32447;&#36335;&#30340;&#21160;&#26426;&#26159;&#26377;&#24847;&#20041;&#30340;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#35299;&#20915;&#20102;&#19977;&#20010;&#23618;&#27425;&#30340;&#38382;&#39064;&#65306;(1)&#25105;&#20204;&#20026;&#20160;&#20040;&#20851;&#24515;&#40065;&#26834;&#24615;&#30740;&#31350;&#65311;(2)&#25105;&#20204;&#20026;&#20160;&#20040;&#20851;&#24515;$\ell_p$&#26377;&#30028;&#30340;&#23041;&#32961;&#27169;&#22411;&#65311;(3)&#25105;&#20204;&#20026;&#20160;&#20040;&#20851;&#24515;&#35748;&#35777;&#32780;&#19981;&#26159;&#32463;&#39564;&#24615;&#38450;&#24481;&#65311;&#31616;&#32780;&#35328;&#20043;&#65292;&#25105;&#20204;&#25345;&#20197;&#19979;&#31435;&#22330;&#65306;
&lt;/p&gt;
&lt;p&gt;
Over the years, researchers have developed myriad attacks that exploit the ubiquity of adversarial examples, as well as defenses that aim to guard against the security vulnerabilities posed by such attacks. Of particular interest to this paper are defenses that provide provable guarantees against the class of $\ell_p$-bounded attacks. Certified defenses have made significant progress, taking robustness certification from toy models and datasets to large-scale problems like ImageNet classification. While this is undoubtedly an interesting academic problem, as the field has matured, its impact in practice remains unclear, thus we find it useful to revisit the motivation for continuing this line of research. There are three layers to this inquiry, which we address in this paper: (1) why do we care about robustness research? (2) why do we care about the $\ell_p$-bounded threat model? And (3) why do we care about certification as opposed to empirical defenses? In brief, we take the position
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39564;&#35777;&#20855;&#26377;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#21069;&#39304;&#31070;&#32463;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#30340;&#23433;&#20840;&#24615;&#30340;&#26032;&#31934;&#30830;&#26465;&#20214;&#21644;&#31639;&#27861;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#23433;&#20840;&#39564;&#35777;&#26041;&#27861;&#20013;ReLU&#20989;&#25968;&#30340;&#19981;&#21487;&#24494;&#24615;&#36136;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.09360</link><description>&lt;p&gt;
ReLU&#31070;&#32463;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#30340;&#31934;&#30830;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Exact Verification of ReLU Neural Control Barrier Functions. (arXiv:2310.09360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39564;&#35777;&#20855;&#26377;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#21069;&#39304;&#31070;&#32463;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#30340;&#23433;&#20840;&#24615;&#30340;&#26032;&#31934;&#30830;&#26465;&#20214;&#21644;&#31639;&#27861;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#23433;&#20840;&#39564;&#35777;&#26041;&#27861;&#20013;ReLU&#20989;&#25968;&#30340;&#19981;&#21487;&#24494;&#24615;&#36136;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;(CBFs)&#26159;&#19968;&#31181;&#29992;&#20110;&#38750;&#32447;&#24615;&#31995;&#32479;&#23433;&#20840;&#25511;&#21046;&#30340;&#24120;&#29992;&#26041;&#27861;&#12290;&#22312;&#22522;&#20110;CBF&#30340;&#25511;&#21046;&#20013;&#65292;&#31995;&#32479;&#30340;&#26399;&#26395;&#23433;&#20840;&#24615;&#36136;&#34987;&#26144;&#23556;&#21040;CBF&#30340;&#38750;&#36127;&#24615;&#65292;&#25511;&#21046;&#36755;&#20837;&#34987;&#36873;&#25321;&#20197;&#30830;&#20445;CBF&#22987;&#32456;&#20445;&#25345;&#38750;&#36127;&#12290;&#26368;&#36817;&#65292;&#23558;CBFs&#34920;&#31034;&#20026;&#31070;&#32463;&#32593;&#32476;(&#31070;&#32463;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#65292;&#25110;NCBFs)&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26222;&#36866;&#34920;&#31034;&#33021;&#21147;&#32780;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#39564;&#35777;&#23398;&#20064;&#21040;&#30340;CBF&#26159;&#21542;&#33021;&#20445;&#35777;&#23433;&#20840;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#21069;&#39304;NCBFs&#30340;&#31934;&#30830;&#26465;&#20214;&#21644;&#31639;&#27861;&#26469;&#39564;&#35777;&#23433;&#20840;&#24615;&#12290;&#20854;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#65292;&#30001;&#20110;ReLU&#20989;&#25968;&#30340;&#20998;&#27573;&#32447;&#24615;&#24615;&#36136;&#65292;&#22312;&#26576;&#20123;&#28857;&#22788;NCBF&#23558;&#26159;&#19981;&#21487;&#24494;&#30340;&#65292;&#22240;&#27492;&#26080;&#25928;&#22320;&#20351;&#20256;&#32479;&#30340;&#23433;&#20840;&#39564;&#35777;&#26041;&#27861;&#26080;&#27861;&#36866;&#29992;&#65292;&#36825;&#20123;&#26041;&#27861;&#20551;&#35774;&#19968;&#20010;&#20809;&#28369;&#30340;&#23631;&#38556;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;g&#20989;&#25968;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Control Barrier Functions (CBFs) are a popular approach for safe control of nonlinear systems. In CBF-based control, the desired safety properties of the system are mapped to nonnegativity of a CBF, and the control input is chosen to ensure that the CBF remains nonnegative for all time. Recently, machine learning methods that represent CBFs as neural networks (neural control barrier functions, or NCBFs) have shown great promise due to the universal representability of neural networks. However, verifying that a learned CBF guarantees safety remains a challenging research problem. This paper presents novel exact conditions and algorithms for verifying safety of feedforward NCBFs with ReLU activation functions. The key challenge in doing so is that, due to the piecewise linearity of the ReLU function, the NCBF will be nondifferentiable at certain points, thus invalidating traditional safety verification methods that assume a smooth barrier function. We resolve this issue by leveraging a g
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21442;&#25968;&#21270;&#30340;&#24378;&#30423;&#31639;&#27861;&#21644;&#24773;&#22659;&#21270;&#30340;&#24378;&#30423;&#31639;&#27861;&#22312;&#30495;&#23454;&#22870;&#21169;&#19982;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#35823;&#24046;&#30340;&#24773;&#20917;&#19979;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#25214;&#21040;&#20102;&#20381;&#36182;&#20110;&#38382;&#39064;&#23454;&#20363;&#21644;&#27169;&#22411;&#31867;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#20351;&#24471;&#32463;&#20856;&#31639;&#27861;&#22914;&#949;-&#36138;&#24515;&#21644;LinUCB&#33021;&#22815;&#22312;&#26102;&#38388;&#33539;&#22260;&#20869;&#20445;&#25345;&#27425;&#32447;&#24615;&#30340;&#36951;&#25022;&#20445;&#38556;&#12290;</title><link>http://arxiv.org/abs/2310.09358</link><description>&lt;p&gt;
&#20309;&#26102;&#25165;&#33021;&#20351;&#21095;&#26412;&#22312;&#38169;&#35823;&#35268;&#33539;&#19979;&#20445;&#25345;&#31283;&#23450;? (arXiv:2310.09358v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
When are Bandits Robust to Misspecification?. (arXiv:2310.09358v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09358
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21442;&#25968;&#21270;&#30340;&#24378;&#30423;&#31639;&#27861;&#21644;&#24773;&#22659;&#21270;&#30340;&#24378;&#30423;&#31639;&#27861;&#22312;&#30495;&#23454;&#22870;&#21169;&#19982;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#35823;&#24046;&#30340;&#24773;&#20917;&#19979;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#25214;&#21040;&#20102;&#20381;&#36182;&#20110;&#38382;&#39064;&#23454;&#20363;&#21644;&#27169;&#22411;&#31867;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#20351;&#24471;&#32463;&#20856;&#31639;&#27861;&#22914;&#949;-&#36138;&#24515;&#21644;LinUCB&#33021;&#22815;&#22312;&#26102;&#38388;&#33539;&#22260;&#20869;&#20445;&#25345;&#27425;&#32447;&#24615;&#30340;&#36951;&#25022;&#20445;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#29305;&#24449;&#20026;&#22522;&#30784;&#30340;&#22870;&#21169;&#27169;&#22411;&#24191;&#27867;&#24212;&#29992;&#20110;&#20915;&#31574;&#38382;&#39064;&#65292;&#22914;&#24378;&#30423;&#31639;&#27861;&#21644;&#24773;&#22659;&#21270;&#30340;&#24378;&#30423;&#31639;&#27861;&#12290;&#36890;&#24120;&#30340;&#20551;&#35774;&#26159;&#21487;&#34892;&#24615;&#65292;&#21363;&#34892;&#20026;&#30340;&#30495;&#23454;&#22870;&#21169;&#23436;&#20840;&#30001;&#26576;&#20010;&#21442;&#25968;&#21270;&#27169;&#22411;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#30495;&#23454;&#22870;&#21169;&#19982;&#27169;&#22411;&#31867;&#20043;&#38388;&#23384;&#22312;&#65288;&#21487;&#33021;&#26174;&#33879;&#65289;&#30340;&#35823;&#24046;&#30340;&#24773;&#20917;&#12290;&#23545;&#20110;&#21442;&#25968;&#21270;&#30340;&#24378;&#30423;&#21644;&#24773;&#22659;&#21270;&#30340;&#24378;&#30423;&#65292;&#25105;&#20204;&#35782;&#21035;&#20986;&#20381;&#36182;&#38382;&#39064;&#23454;&#20363;&#21644;&#27169;&#22411;&#31867;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#20351;&#24471;&#32463;&#20856;&#31639;&#27861;&#22914;&#949;-&#36138;&#24515;&#21644;LinUCB&#22312;&#21363;&#20351;&#22870;&#21169;&#23384;&#22312;&#20005;&#37325;&#35823;&#24046;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#22815;&#22312;&#26102;&#38388;&#33539;&#22260;&#20869;&#20445;&#35777;&#27425;&#32447;&#24615;&#65288;&#27425;&#20110;&#26102;&#38388;&#33539;&#22260;&#65289;&#30340;&#36951;&#25022;&#20445;&#38556;&#12290;&#36825;&#19982;&#29616;&#26377;&#30340;&#38024;&#23545;&#38169;&#35823;&#35268;&#33539;&#30340;&#26368;&#22351;&#24773;&#20917;&#32467;&#26524;&#24418;&#25104;&#23545;&#27604;&#65292;&#21518;&#32773;&#26174;&#31034;&#36951;&#25022;&#36793;&#30028;&#38543;&#26102;&#38388;&#25104;&#32447;&#24615;&#27604;&#20363;&#22686;&#38271;&#65292;&#24182;&#19988;&#35828;&#26126;&#23384;&#22312;&#19968;&#20010;&#30456;&#24403;&#22823;&#30340;&#24378;&#30423;&#38382;&#39064;&#23454;&#20363;&#38598;&#21512;&#22312;&#38169;&#35823;&#35268;&#33539;&#19979;&#20173;&#28982;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parametric feature-based reward models are widely employed by algorithms for decision making settings such as bandits and contextual bandits. The typical assumption under which they are analysed is realizability, i.e., that the true rewards of actions are perfectly explained by some parametric model in the class. We are, however, interested in the situation where the true rewards are (potentially significantly) misspecified with respect to the model class. For parameterized bandits and contextual bandits, we identify sufficient conditions, depending on the problem instance and model class, under which classic algorithms such as $\epsilon$-greedy and LinUCB enjoy sublinear (in the time horizon) regret guarantees under even grossly misspecified rewards. This is in contrast to existing worst-case results for misspecified bandits which show regret bounds that scale linearly with time, and shows that there can be a nontrivially large set of bandit instances that are robust to misspecificati
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#26041;&#27861;&#30340;&#22686;&#37327;&#29983;&#25104;&#33945;&#29305;&#21345;&#27931;(IGMC)&#26041;&#27861;&#65292;&#29992;&#20110;&#27979;&#37327;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36890;&#36807;&#36845;&#20195;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#24182;&#23558;&#20854;&#36755;&#20986;&#28155;&#21152;&#21040;&#25968;&#25454;&#38598;&#20013;&#65292;IGMC&#33021;&#35745;&#31639;&#38543;&#26426;&#21464;&#37327;&#26399;&#26395;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#19988;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#65292;&#24182;&#22312;MNIST&#25968;&#23383;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.09338</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#26041;&#27861;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification using Generative Approach. (arXiv:2310.09338v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09338
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#26041;&#27861;&#30340;&#22686;&#37327;&#29983;&#25104;&#33945;&#29305;&#21345;&#27931;(IGMC)&#26041;&#27861;&#65292;&#29992;&#20110;&#27979;&#37327;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36890;&#36807;&#36845;&#20195;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#24182;&#23558;&#20854;&#36755;&#20986;&#28155;&#21152;&#21040;&#25968;&#25454;&#38598;&#20013;&#65292;IGMC&#33021;&#35745;&#31639;&#38543;&#26426;&#21464;&#37327;&#26399;&#26395;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#19988;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#65292;&#24182;&#22312;MNIST&#25968;&#23383;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#37327;&#29983;&#25104;Monte Carlo (IGMC) &#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#26041;&#27861;&#26469;&#27979;&#37327;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;IGMC&#36890;&#36807;&#36845;&#20195;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#36755;&#20986;&#28155;&#21152;&#21040;&#25968;&#25454;&#38598;&#20013;&#65292;&#20197;&#35745;&#31639;&#38543;&#26426;&#21464;&#37327;&#26399;&#26395;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;IGMC&#30456;&#23545;&#20110;&#26679;&#26412;&#22823;&#23567;&#21644;&#25277;&#26679;&#28145;&#24230;&#30340;&#25910;&#25947;&#36895;&#24230;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#30001;&#20110;&#20854;&#19982;&#28145;&#24230;&#29983;&#25104;&#26041;&#27861;&#30340;&#20860;&#23481;&#24615;&#65292;IGMC&#36866;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;MNIST&#25968;&#23383;&#20998;&#31867;&#20219;&#21153;&#19978;&#23545;IGMC&#30340;&#34892;&#20026;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the Incremental Generative Monte Carlo (IGMC) method, designed to measure uncertainty in deep neural networks using deep generative approaches. IGMC iteratively trains generative models, adding their output to the dataset, to compute the posterior distribution of the expectation of a random variable. We provide a theoretical guarantee of the convergence rate of IGMC relative to the sample size and sampling depth. Due to its compatibility with deep generative approaches, IGMC is adaptable to both neural network classification and regression tasks. We empirically study the behavior of IGMC on the MNIST digit classification task.
&lt;/p&gt;</description></item><item><title>&#32452;&#21512;&#33021;&#21147;&#20197;&#20056;&#27861;&#26041;&#24335;&#20986;&#29616;&#65306;&#30740;&#31350;&#20102;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#22312;&#21512;&#25104;&#20219;&#21153;&#20013;&#30340;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#31181;&#33021;&#21147;&#21463;&#21040;&#24213;&#23618;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#32467;&#26500;&#24433;&#21709;&#65292;&#19988;&#27169;&#22411;&#22312;&#23398;&#20064;&#21040;&#26356;&#39640;&#32423;&#30340;&#32452;&#21512;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2310.09336</link><description>&lt;p&gt;
&#32452;&#21512;&#33021;&#21147;&#20197;&#20056;&#27861;&#26041;&#24335;&#20986;&#29616;&#65306;&#22312;&#21512;&#25104;&#20219;&#21153;&#20013;&#25506;&#32034;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Compositional Abilities Emerge Multiplicatively: Exploring Diffusion Models on a Synthetic Task. (arXiv:2310.09336v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09336
&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#33021;&#21147;&#20197;&#20056;&#27861;&#26041;&#24335;&#20986;&#29616;&#65306;&#30740;&#31350;&#20102;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#22312;&#21512;&#25104;&#20219;&#21153;&#20013;&#30340;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#31181;&#33021;&#21147;&#21463;&#21040;&#24213;&#23618;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#32467;&#26500;&#24433;&#21709;&#65292;&#19988;&#27169;&#22411;&#22312;&#23398;&#20064;&#21040;&#26356;&#39640;&#32423;&#30340;&#32452;&#21512;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#29983;&#25104;&#27169;&#22411;&#23637;&#31034;&#20986;&#20102;&#20135;&#29983;&#26497;&#20026;&#36924;&#30495;&#25968;&#25454;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#33258;&#28982;&#32452;&#21512;&#24615;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#38752;&#20351;&#29992;&#38656;&#35201;&#23637;&#31034;&#20986;&#33021;&#22815;&#32452;&#21512;&#26032;&#30340;&#27010;&#24565;&#38598;&#21512;&#20197;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#26410;&#35265;&#30340;&#36755;&#20986;&#30340;&#33021;&#21147;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26368;&#36817;&#30340;&#25193;&#25955;&#27169;&#22411;&#30830;&#23454;&#34920;&#29616;&#20986;&#20102;&#26377;&#36259;&#30340;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#20063;&#20250;&#20986;&#29616;&#26080;&#27861;&#39044;&#27979;&#30340;&#22833;&#36133;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#26377;&#25511;&#21046;&#24615;&#30340;&#30740;&#31350;&#65292;&#20197;&#20102;&#35299;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#21464;&#21270;&#20102;&#35757;&#32451;&#25968;&#25454;&#30340;&#19981;&#21516;&#23646;&#24615;&#24182;&#27979;&#37327;&#20102;&#27169;&#22411;&#29983;&#25104;&#36234;&#30028;&#26679;&#26412;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65306;&#65288;i&#65289;&#20174;&#19968;&#20010;&#27010;&#24565;&#29983;&#25104;&#26679;&#26412;&#30340;&#33021;&#21147;&#21644;&#23558;&#23427;&#20204;&#32452;&#21512;&#36215;&#26469;&#30340;&#33021;&#21147;&#30340;&#20986;&#29616;&#39034;&#24207;&#21463;&#21040;&#20102;&#24213;&#23618;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#32467;&#26500;&#30340;&#24433;&#21709;&#65307;&#65288;ii&#65289;&#22312;&#32452;&#21512;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#34920;&#26126;&#27169;&#22411;&#22312;&#23398;&#20064;&#21040;&#26356;&#39640;&#32423;&#30340;&#32452;&#21512;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern generative models exhibit unprecedented capabilities to generate extremely realistic data. However, given the inherent compositionality of the real world, reliable use of these models in practical applications requires that they exhibit the capability to compose a novel set of concepts to generate outputs not seen in the training data set. Prior work demonstrates that recent diffusion models do exhibit intriguing compositional generalization abilities, but also fail unpredictably. Motivated by this, we perform a controlled study for understanding compositional generalization in conditional diffusion models in a synthetic setting, varying different attributes of the training data and measuring the model's ability to generate samples out-of-distribution. Our results show: (i) the order in which the ability to generate samples from a concept and compose them emerges is governed by the structure of the underlying data-generating process; (ii) performance on compositional tasks exhib
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#38543;&#26426;Metropolis-Hastings&#31639;&#27861;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;&#36890;&#36807;&#24341;&#20837;&#31616;&#21333;&#30340;&#20462;&#27491;&#39033;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36991;&#20813;&#35745;&#31639;&#25104;&#26412;&#19978;&#30340;&#25439;&#22833;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#38750;&#21442;&#25968;&#22238;&#24402;&#24773;&#26223;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22238;&#24402;&#30340;&#25968;&#20540;&#23454;&#20363;&#26469;&#35777;&#26126;&#20102;&#20854;&#22312;&#37319;&#26679;&#21644;&#21487;&#20449;&#21306;&#38388;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.09335</link><description>&lt;p&gt;
&#38024;&#23545;&#38543;&#26426;Metropolis-Hastings&#31639;&#27861;&#30340;&#32479;&#35745;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Statistical guarantees for stochastic Metropolis-Hastings. (arXiv:2310.09335v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09335
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#38543;&#26426;Metropolis-Hastings&#31639;&#27861;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;&#36890;&#36807;&#24341;&#20837;&#31616;&#21333;&#30340;&#20462;&#27491;&#39033;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36991;&#20813;&#35745;&#31639;&#25104;&#26412;&#19978;&#30340;&#25439;&#22833;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#38750;&#21442;&#25968;&#22238;&#24402;&#24773;&#26223;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22238;&#24402;&#30340;&#25968;&#20540;&#23454;&#20363;&#26469;&#35777;&#26126;&#20102;&#20854;&#22312;&#37319;&#26679;&#21644;&#21487;&#20449;&#21306;&#38388;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Metropolis-Hastings&#27493;&#39588;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22522;&#20110;&#26799;&#24230;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#20013;&#12290;&#36890;&#36807;&#23545;&#25209;&#27425;&#35745;&#31639;&#25509;&#21463;&#27010;&#29575;&#65292;&#38543;&#26426;Metropolis-Hastings&#27493;&#39588;&#33410;&#30465;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#20294;&#38477;&#20302;&#20102;&#26377;&#25928;&#26679;&#26412;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#31616;&#21333;&#30340;&#20462;&#27491;&#39033;&#21487;&#20197;&#36991;&#20813;&#36825;&#20010;&#38556;&#30861;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#26524;&#22312;&#38750;&#21442;&#25968;&#22238;&#24402;&#35774;&#32622;&#20013;&#24212;&#29992;&#25913;&#36827;&#30340;&#38543;&#26426;Metropolis-Hastings&#26041;&#27861;&#20174;Gibbs&#21518;&#39564;&#20998;&#24067;&#20013;&#37319;&#26679;&#65292;&#21017;&#38142;&#30340;&#32467;&#26524;&#31283;&#24577;&#20998;&#24067;&#30340;&#32479;&#35745;&#23646;&#24615;&#12290;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22238;&#24402;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;PAC-Bayes&#39044;&#35328;&#19981;&#31561;&#24335;&#65292;&#23427;&#25552;&#20379;&#20102;&#26368;&#20248;&#30340;&#25910;&#32553;&#36895;&#29575;&#65292;&#24182;&#20998;&#26512;&#20102;&#32467;&#26524;&#21487;&#20449;&#21306;&#38388;&#30340;&#30452;&#24452;&#21644;&#39640;&#32622;&#20449;&#27010;&#29575;&#12290;&#36890;&#36807;&#22312;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#20013;&#30340;&#25968;&#20540;&#23454;&#20363;&#65292;&#25105;&#20204;&#35828;&#26126;&#20102;&#38543;&#26426;Metropolis-Hastings&#31639;&#27861;&#30340;&#21487;&#20449;&#21306;&#38388;&#21644;&#25910;&#32553;&#36895;&#29575;&#30830;&#23454;&#34920;&#29616;&#20986;&#31867;&#20284;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Metropolis-Hastings step is widely used for gradient-based Markov chain Monte Carlo methods in uncertainty quantification. By calculating acceptance probabilities on batches, a stochastic Metropolis-Hastings step saves computational costs, but reduces the effective sample size. We show that this obstacle can be avoided by a simple correction term. We study statistical properties of the resulting stationary distribution of the chain if the corrected stochastic Metropolis-Hastings approach is applied to sample from a Gibbs posterior distribution in a nonparametric regression setting. Focusing on deep neural network regression, we prove a PAC-Bayes oracle inequality which yields optimal contraction rates and we analyze the diameter and show high coverage probability of the resulting credible sets. With a numerical example in a high-dimensional parameter space, we illustrate that credible sets and contraction rates of the stochastic Metropolis-Hastings algorithm indeed behave similar to 
&lt;/p&gt;</description></item><item><title>&#26412;&#27425;&#35843;&#26597;&#24635;&#32467;&#20102;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#22312;&#26234;&#33021;&#21046;&#36896;&#21644;&#20135;&#19994;4.0&#32972;&#26223;&#19979;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#31361;&#20986;&#20102;&#20854;&#22312;&#24037;&#19994;&#29983;&#20135;&#21644;&#21046;&#36896;&#39046;&#22495;&#30340;&#20851;&#38190;&#20248;&#21183;&#21644;&#25361;&#25112;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#20805;&#20998;&#21033;&#29992;&#30340;TDA&#26041;&#27861;&#21644;&#24050;&#35782;&#21035;&#30340;&#24212;&#29992;&#31867;&#22411;&#65292;&#20197;&#25512;&#21160;&#26356;&#22810;&#30340;&#30456;&#20851;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.09319</link><description>&lt;p&gt;
&#26234;&#33021;&#21046;&#36896;&#36807;&#31243;&#20013;&#30340;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;--&#20851;&#20110;&#29616;&#29366;&#30340;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Topological Data Analysis in smart manufacturing processes -- A survey on the state of the art. (arXiv:2310.09319v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#27425;&#35843;&#26597;&#24635;&#32467;&#20102;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#22312;&#26234;&#33021;&#21046;&#36896;&#21644;&#20135;&#19994;4.0&#32972;&#26223;&#19979;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#31361;&#20986;&#20102;&#20854;&#22312;&#24037;&#19994;&#29983;&#20135;&#21644;&#21046;&#36896;&#39046;&#22495;&#30340;&#20851;&#38190;&#20248;&#21183;&#21644;&#25361;&#25112;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#20805;&#20998;&#21033;&#29992;&#30340;TDA&#26041;&#27861;&#21644;&#24050;&#35782;&#21035;&#30340;&#24212;&#29992;&#31867;&#22411;&#65292;&#20197;&#25512;&#21160;&#26356;&#22810;&#30340;&#30456;&#20851;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#65288;TDA&#65289;&#26159;&#19968;&#31181;&#20351;&#29992;&#25299;&#25169;&#23398;&#25216;&#26415;&#23545;&#22797;&#26434;&#30340;&#22810;&#32500;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#30340;&#25968;&#23398;&#26041;&#27861;&#65292;&#24050;&#32463;&#22312;&#21307;&#23398;&#12289;&#26448;&#26009;&#31185;&#23398;&#12289;&#29983;&#29289;&#23398;&#31561;&#22810;&#20010;&#39046;&#22495;&#34987;&#24191;&#27867;&#32780;&#25104;&#21151;&#22320;&#24212;&#29992;&#12290;&#26412;&#35843;&#26597;&#24635;&#32467;&#20102;TDA&#22312;&#21478;&#19968;&#20010;&#24212;&#29992;&#39046;&#22495;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65306;&#24037;&#19994;&#21046;&#36896;&#21644;&#20135;&#19994;4.0&#32972;&#26223;&#19979;&#30340;&#29983;&#20135;&#12290;&#25105;&#20204;&#23545;&#24037;&#19994;&#29983;&#20135;&#21644;&#21046;&#36896;&#39046;&#22495;&#20013;TDA&#24212;&#29992;&#36827;&#34892;&#20102;&#20005;&#35880;&#21487;&#37325;&#22797;&#30340;&#25991;&#29486;&#25628;&#32034;&#12290;&#36890;&#36807;&#23545;&#32467;&#26524;&#36827;&#34892;&#32858;&#31867;&#21644;&#20998;&#26512;&#65292;&#22522;&#20110;&#20854;&#22312;&#21046;&#36896;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;&#39046;&#22495;&#21644;&#36755;&#20837;&#25968;&#25454;&#31867;&#22411;&#36827;&#34892;&#35770;&#36848;&#12290;&#25105;&#20204;&#31361;&#20986;&#20102;TDA&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#20851;&#38190;&#20248;&#21183;&#21450;&#20854;&#24037;&#20855;&#65292;&#24182;&#25551;&#36848;&#20102;&#23427;&#30340;&#25361;&#25112;&#20197;&#21450;&#26410;&#26469;&#30340;&#28508;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#65288;&#29305;&#23450;&#39046;&#22495;&#30340;&#65289;&#24037;&#19994;&#20013;&#26410;&#20805;&#20998;&#21033;&#29992;&#30340;TDA&#26041;&#27861;&#21644;&#24050;&#35782;&#21035;&#30340;&#24212;&#29992;&#31867;&#22411;&#65292;&#26088;&#22312;&#20419;&#36827;&#26356;&#22810;&#30340;&#30740;&#31350;&#22312;&#24403;&#21069;&#39046;&#22495;&#20013;&#30340;&#24320;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topological Data Analysis (TDA) is a mathematical method using techniques from topology for the analysis of complex, multi-dimensional data that has been widely and successfully applied in several fields such as medicine, material science, biology, and others. This survey summarizes the state of the art of TDA in yet another application area: industrial manufacturing and production in the context of Industry 4.0. We perform a rigorous and reproducible literature search of applications of TDA on the setting of industrial production and manufacturing. The resulting works are clustered and analyzed based on their application area within the manufacturing process and their input data type. We highlight the key benefits of TDA and their tools in this area and describe its challenges, as well as future potential. Finally, we discuss which TDA methods are underutilized in (the specific area of) industry and the identified types of application, with the goal of prompting more research in this 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#25968;&#25454;&#21644;&#21487;&#35270;&#21270;&#35774;&#35745;&#25506;&#38024;&#26469;&#30740;&#31350;&#39046;&#22495;&#19987;&#23478;&#22914;&#20309;&#36890;&#36807;&#35821;&#20041;&#20132;&#20114;&#26356;&#26032;&#20998;&#31867;&#27169;&#22411;&#65292;&#21457;&#29616;&#35768;&#22810;&#35821;&#20041;&#20132;&#20114;&#30340;&#30446;&#26631;&#26159;&#22686;&#24378;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#35843;&#25972;&#27169;&#22411;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.09314</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#21644;&#21487;&#35270;&#21270;&#35774;&#35745;&#25506;&#38024;&#20174;&#29992;&#25143;&#37027;&#37324;&#24341;&#20986;&#27169;&#22411;&#35843;&#25972;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Eliciting Model Steering Interactions from Users via Data and Visual Design Probes. (arXiv:2310.09314v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#25968;&#25454;&#21644;&#21487;&#35270;&#21270;&#35774;&#35745;&#25506;&#38024;&#26469;&#30740;&#31350;&#39046;&#22495;&#19987;&#23478;&#22914;&#20309;&#36890;&#36807;&#35821;&#20041;&#20132;&#20114;&#26356;&#26032;&#20998;&#31867;&#27169;&#22411;&#65292;&#21457;&#29616;&#35768;&#22810;&#35821;&#20041;&#20132;&#20114;&#30340;&#30446;&#26631;&#26159;&#22686;&#24378;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#35843;&#25972;&#27169;&#22411;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#19987;&#23478;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#33258;&#21160;&#21270;&#25968;&#25454;&#31185;&#23398;&#24037;&#20855;&#23558;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#32435;&#20837;&#24037;&#20316;&#20013;&#65292;&#20294;&#22312;&#36825;&#20123;&#27169;&#22411;&#38169;&#35823;&#26102;&#24456;&#38590;&#36827;&#34892;&#8220;&#35843;&#35797;&#8221;&#12290;&#23545;&#20110;&#36825;&#20123;&#19987;&#23478;&#26469;&#35828;&#65292;&#35821;&#20041;&#20132;&#20114;&#21487;&#20197;&#25552;&#20379;&#19968;&#31181;&#21487;&#35775;&#38382;&#30340;&#36884;&#24452;&#65292;&#20197;&#24341;&#23548;&#21644;&#23436;&#21892;ML&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#28145;&#20837;&#20854;&#25216;&#26415;&#32454;&#33410;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#25968;&#25454;&#21644;&#21487;&#35270;&#21270;&#35774;&#35745;&#25506;&#38024;&#36827;&#34892;&#35843;&#26597;&#30740;&#31350;&#65292;&#20197;&#26816;&#26597;&#20855;&#26377;&#21508;&#31181;ML&#19987;&#19994;&#30693;&#35782;&#30340;&#19987;&#23478;&#22914;&#20309;&#20351;&#29992;&#35821;&#20041;&#20132;&#20114;&#26469;&#26356;&#26032;&#31616;&#21333;&#30340;&#20998;&#31867;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#35774;&#35745;&#25506;&#38024;&#19982;20&#20010;&#21442;&#19982;&#32773;&#36827;&#34892;&#20132;&#20114;&#24335;&#23545;&#35805;&#65292;&#24182;&#23558;&#20854;&#20132;&#20114;&#34892;&#20026;&#32534;&#30721;&#20026;&#19968;&#32452;&#30446;&#26631;-&#20132;&#20114;&#23545;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#35768;&#22810;&#35821;&#20041;&#20132;&#20114;&#30340;&#30446;&#26631;&#24182;&#19981;&#30452;&#25509;&#26144;&#23556;&#21040;ML&#27169;&#22411;&#21442;&#25968;&#65292;&#32780;&#26159;&#26088;&#22312;&#22686;&#24378;&#27169;&#22411;&#29992;&#20110;&#35757;&#32451;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#21442;&#19982;&#32773;&#23545;&#20110;&#19982;ML&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#29369;&#35947;&#30340;&#21407;&#22240;&#65292;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
Domain experts increasingly use automated data science tools to incorporate machine learning (ML) models in their work but struggle to "debug" these models when they are incorrect. For these experts, semantic interactions can provide an accessible avenue to guide and refine ML models without having to programmatically dive into its technical details. In this research, we conduct an elicitation study using data and visual design probes to examine if and how experts with a spectrum of ML expertise use semantic interactions to update a simple classification model. We use our design probes to facilitate an interactive dialogue with 20 participants and codify their interactions as a set of target-interaction pairs. Interestingly, our findings revealed that many targets of semantic interactions do not directly map to ML model parameters, but instead aim to augment the data a model uses for training. We also identify reasons that participants would hesitate to interact with ML models, includi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23383;&#23402;&#29983;&#36741;&#21161;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#32593;&#32476;&#20999;&#29255;&#20837;&#22330;&#25511;&#21046;&#20013;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#21021;&#22987;&#19981;&#31283;&#23450;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.09299</link><description>&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#36741;&#21161;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#32593;&#32476;&#20999;&#29255;&#20837;&#22330;&#25511;&#21046;&#30340;&#22312;&#32447;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Digital Twin Assisted Deep Reinforcement Learning for Online Optimization of Network Slicing Admission Control. (arXiv:2310.09299v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09299
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23383;&#23402;&#29983;&#36741;&#21161;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#32593;&#32476;&#20999;&#29255;&#20837;&#22330;&#25511;&#21046;&#20013;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#21021;&#22987;&#19981;&#31283;&#23450;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
5G&#21450;&#20197;&#19978;&#32593;&#32476;&#20013;&#22810;&#26679;&#21270;&#30340;&#32593;&#32476;&#26381;&#21153;&#30340;&#26222;&#21450;&#23548;&#33268;&#20102;&#32593;&#32476;&#20999;&#29255;&#25216;&#26415;&#30340;&#20986;&#29616;&#12290;&#22312;&#20854;&#20013;&#65292;&#20837;&#22330;&#25511;&#21046;&#36890;&#36807;&#36873;&#25321;&#24615;&#25509;&#21463;&#26381;&#21153;&#35831;&#27714;&#26469;&#23454;&#29616;&#29305;&#23450;&#30340;&#20248;&#21270;&#30446;&#26631;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23613;&#31649;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#22312;&#35768;&#22810;&#20837;&#22330;&#25511;&#21046;&#26041;&#27861;&#20013;&#36215;&#30528;&#22522;&#30784;&#21644;&#28789;&#27963;&#24615;&#30340;&#20316;&#29992;&#65292;&#20294;DRL&#27169;&#22411;&#30340;&#21021;&#22987;&#19981;&#31283;&#23450;&#24615;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#32593;&#32476;&#20013;&#30340;&#23454;&#38469;&#37096;&#32626;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23383;&#23402;&#29983;(DT)&#36741;&#21161;&#30340;DRL&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#20837;&#22330;&#20915;&#31574;&#36807;&#31243;&#24418;&#24335;&#21270;&#20026;&#21322;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#38543;&#21518;&#31616;&#21270;&#20026;&#31561;&#20215;&#30340;&#31163;&#25955;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#20197;&#20415;&#23454;&#26045;DRL&#26041;&#27861;&#12290;DT&#26159;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#24314;&#31435;&#30340;&#65292;&#24182;&#29992;&#20110;&#36741;&#21161;DRL&#27169;&#22411;&#30340;&#35757;&#32451;&#38454;&#27573;&#12290;&#24191;&#27867;&#30340;&#27169;&#25311;&#34920;&#26126;&#65292;DT&#20316;&#20026;&#19968;&#31181;&#36741;&#21161;&#25163;&#27573;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;DRL&#30340;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of diverse network services in 5G and beyond networks has led to the emergence of network slicing technologies. Among these, admission control plays a crucial role in achieving specific optimization goals through the selective acceptance of service requests. Although Deep Reinforcement Learning (DRL) forms the foundation in many admission control approaches for its effectiveness and flexibility, the initial instability of DRL models hinders their practical deployment in real-world networks. In this work, we propose a digital twin (DT) assisted DRL solution to address this issue. Specifically, we first formulate the admission decision-making process as a semi-Markov decision process, which is subsequently simplified into an equivalent discrete-time Markov decision process to facilitate the implementation of DRL methods. The DT is established through supervised learning and employed to assist the training phase of the DRL model. Extensive simulations show that the DT-as
&lt;/p&gt;</description></item><item><title>ByteStack-ID&#26159;&#19968;&#31181;&#22522;&#20110;&#28784;&#24230;&#22270;&#20687;&#21644;&#36127;&#36733;&#23383;&#33410;&#39057;&#29575;&#30340;&#38598;&#25104;&#22534;&#21472;&#27169;&#22411;&#65292;&#29992;&#20110;&#25968;&#25454;&#21253;&#32423;&#20837;&#20405;&#26816;&#27979;&#12290;&#23427;&#33021;&#36805;&#36895;&#20934;&#30830;&#22320;&#35782;&#21035;&#32593;&#32476;&#27969;&#37327;&#20013;&#30340;&#21508;&#31181;&#25915;&#20987;&#31867;&#22411;&#65292;&#24182;&#19982;&#20256;&#32479;&#26041;&#27861;&#26377;&#25152;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2310.09298</link><description>&lt;p&gt;
ByteStack-ID: &#22522;&#20110;&#28784;&#24230;&#22270;&#20687;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#30340;&#38598;&#25104;&#22534;&#21472;&#27169;&#22411;&#65292;&#21033;&#29992;&#36127;&#36733;&#23383;&#33410;&#39057;&#29575;
&lt;/p&gt;
&lt;p&gt;
ByteStack-ID: Integrated Stacked Model Leveraging Payload Byte Frequency for Grayscale Image-based Network Intrusion Detection. (arXiv:2310.09298v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09298
&lt;/p&gt;
&lt;p&gt;
ByteStack-ID&#26159;&#19968;&#31181;&#22522;&#20110;&#28784;&#24230;&#22270;&#20687;&#21644;&#36127;&#36733;&#23383;&#33410;&#39057;&#29575;&#30340;&#38598;&#25104;&#22534;&#21472;&#27169;&#22411;&#65292;&#29992;&#20110;&#25968;&#25454;&#21253;&#32423;&#20837;&#20405;&#26816;&#27979;&#12290;&#23427;&#33021;&#36805;&#36895;&#20934;&#30830;&#22320;&#35782;&#21035;&#32593;&#32476;&#27969;&#37327;&#20013;&#30340;&#21508;&#31181;&#25915;&#20987;&#31867;&#22411;&#65292;&#24182;&#19982;&#20256;&#32479;&#26041;&#27861;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#21457;&#23637;&#30340;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#65292;&#36805;&#36895;&#20934;&#30830;&#22320;&#35782;&#21035;&#32593;&#32476;&#27969;&#37327;&#20013;&#30340;&#21508;&#31181;&#25915;&#20987;&#31867;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;"ByteStack-ID"&#65292;&#19968;&#31181;&#19987;&#20026;&#25968;&#25454;&#21253;&#32423;&#20837;&#20405;&#26816;&#27979;&#32780;&#35774;&#35745;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;ByteStack-ID&#26680;&#24515;&#26159;&#21033;&#29992;&#20174;&#36127;&#36733;&#25968;&#25454;&#30340;&#39057;&#29575;&#20998;&#24067;&#29983;&#25104;&#30340;&#28784;&#24230;&#22270;&#20687;&#65292;&#36825;&#26159;&#19968;&#31181;&#31361;&#30772;&#24615;&#30340;&#25216;&#26415;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#35782;&#21035;&#22797;&#26434;&#25968;&#25454;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23436;&#20840;&#22522;&#20110;&#25968;&#25454;&#21253;&#32423;&#20449;&#24687;&#65292;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#27969;&#37327;&#25968;&#25454;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;NIDS&#65289;&#26377;&#25152;&#19981;&#21516;&#12290;&#22312;&#22522;&#26412;&#22534;&#21472;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;ByteStack-ID&#19982;&#20256;&#32479;&#30340;&#22534;&#21472;&#26041;&#27861;&#19981;&#21516;&#12290;&#23427;&#23558;&#38468;&#21152;&#30340;&#20803;&#23398;&#20064;&#22120;&#23618;&#26080;&#32541;&#38598;&#25104;&#21040;&#36830;&#25509;&#30340;&#22522;&#30784;&#23398;&#20064;&#22120;&#20013;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#39640;&#24230;&#20248;&#21270;&#30340;&#32479;&#19968;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the ever-evolving realm of network security, the swift and accurate identification of diverse attack classes within network traffic is of paramount importance. This paper introduces "ByteStack-ID," a pioneering approach tailored for packet-level intrusion detection. At its core, ByteStack-ID leverages grayscale images generated from the frequency distributions of payload data, a groundbreaking technique that greatly enhances the model's ability to discern intricate data patterns. Notably, our approach is exclusively grounded in packet-level information, a departure from conventional Network Intrusion Detection Systems (NIDS) that predominantly rely on flow-based data. While building upon the fundamental concept of stacking methodology, ByteStack-ID diverges from traditional stacking approaches. It seamlessly integrates additional meta learner layers into the concatenated base learners, creating a highly optimized, unified model. Empirical results unequivocally confirm the outstandin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#20154;&#31867;&#35760;&#24518;&#26426;&#21046;&#21551;&#21457;&#30340;&#31070;&#32463;&#27169;&#22359;&#65292;&#27169;&#25311;&#20154;&#31867;&#21644;&#26426;&#22120;&#22914;&#20309;&#23545;&#24403;&#21069;&#36755;&#20837;&#36827;&#34892;&#20851;&#32852;&#25512;&#29702;&#21644;&#38382;&#31572;&#65292;&#24182;&#23558;&#20854;&#19982;&#36807;&#21435;&#30340;&#35760;&#24518;&#32467;&#21512;&#36215;&#26469;&#12290;&#36890;&#36807;&#24863;&#30693;&#12289;&#35760;&#24518;&#21644;&#25512;&#29702;&#32452;&#20214;&#65292;&#35813;&#27169;&#22359;&#23454;&#29616;&#20102;&#24863;&#30693;&#26356;&#26032;&#12289;&#35760;&#24518;&#34701;&#21512;&#21644;&#20449;&#24687;&#26816;&#32034;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09297</link><description>&lt;p&gt;
&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#35748;&#30693;&#65306;&#21463;&#20154;&#31867;&#35760;&#24518;&#26426;&#21046;&#21551;&#21457;&#30340;&#25512;&#29702;&#31070;&#32463;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Understanding AI Cognition: A Neural Module for Inference Inspired by Human Memory Mechanisms. (arXiv:2310.09297v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09297
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#20154;&#31867;&#35760;&#24518;&#26426;&#21046;&#21551;&#21457;&#30340;&#31070;&#32463;&#27169;&#22359;&#65292;&#27169;&#25311;&#20154;&#31867;&#21644;&#26426;&#22120;&#22914;&#20309;&#23545;&#24403;&#21069;&#36755;&#20837;&#36827;&#34892;&#20851;&#32852;&#25512;&#29702;&#21644;&#38382;&#31572;&#65292;&#24182;&#23558;&#20854;&#19982;&#36807;&#21435;&#30340;&#35760;&#24518;&#32467;&#21512;&#36215;&#26469;&#12290;&#36890;&#36807;&#24863;&#30693;&#12289;&#35760;&#24518;&#21644;&#25512;&#29702;&#32452;&#20214;&#65292;&#35813;&#27169;&#22359;&#23454;&#29616;&#20102;&#24863;&#30693;&#26356;&#26032;&#12289;&#35760;&#24518;&#34701;&#21512;&#21644;&#20449;&#24687;&#26816;&#32034;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21644;&#26426;&#22120;&#22914;&#20309;&#23558;&#24403;&#21069;&#30340;&#36755;&#20837;&#19982;&#36807;&#21435;&#30340;&#35760;&#24518;&#32467;&#21512;&#36215;&#26469;&#65292;&#36827;&#34892;&#20851;&#32852;&#25512;&#29702;&#21644;&#38382;&#31572;&#65292;&#24182;&#23558;&#24863;&#30693;&#21040;&#30340;&#20449;&#24687;&#32622;&#20110;&#19978;&#19979;&#25991;&#20013;&#65292;&#36825;&#26159;&#35748;&#30693;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35868;&#39064;&#12290;&#21463;&#21040;&#20154;&#33041;&#35760;&#24518;&#31995;&#32479;&#21644;&#35748;&#30693;&#32467;&#26500;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#24863;&#30693;&#12289;&#35760;&#24518;&#21644;&#25512;&#29702;&#32452;&#20214;&#30340;PMI&#26694;&#26550;&#12290;&#29305;&#21035;&#22320;&#65292;&#35760;&#24518;&#27169;&#22359;&#21253;&#25324;&#24037;&#20316;&#35760;&#24518;&#21644;&#38271;&#26399;&#35760;&#24518;&#65292;&#20854;&#20013;&#21518;&#32773;&#20855;&#26377;&#26356;&#39640;&#38454;&#30340;&#32467;&#26500;&#26469;&#20445;&#30041;&#26356;&#22810;&#30340;&#32047;&#31215;&#30693;&#35782;&#21644;&#32463;&#39564;&#12290;&#36890;&#36807;&#21487;&#21306;&#20998;&#30340;&#31454;&#20105;&#20889;&#20837;&#35775;&#38382;&#65292;&#24403;&#21069;&#30340;&#24863;&#30693;&#26356;&#26032;&#24037;&#20316;&#35760;&#24518;&#65292;&#20043;&#21518;&#36890;&#36807;&#22806;&#31215;&#20851;&#32852;&#19982;&#38271;&#26399;&#35760;&#24518;&#34701;&#21512;&#65292;&#36991;&#20813;&#20869;&#23384;&#28322;&#20986;&#24182;&#26368;&#23567;&#21270;&#20449;&#24687;&#20914;&#31361;&#12290;&#22312;&#25512;&#29702;&#27169;&#22359;&#20013;&#65292;&#30456;&#20851;&#20449;&#24687;&#20174;&#20004;&#20010;&#21333;&#29420;&#30340;&#35760;&#24518;&#28304;&#26816;&#32034;&#24182;&#32467;&#21512;&#65292;&#20197;&#33719;&#24471;&#26356;&#20840;&#38754;&#21644;&#31934;&#30830;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
How humans and machines make sense of current inputs for relation reasoning and question-answering while putting the perceived information into context of our past memories, has been a challenging conundrum in cognitive science and artificial intelligence. Inspired by human brain's memory system and cognitive architectures, we propose a PMI framework that consists of perception, memory and inference components. Notably, the memory module comprises working and long-term memory, with the latter endowed with a higher-order structure to retain more accumulated knowledge and experiences. Through a differentiable competitive write access, current perceptions update working memory, which is later merged with long-term memory via outer product associations, averting memory overflow and minimizing information conflicts. In the inference module, relevant information is retrieved from two separate memory origins and associatively integrated to attain a more comprehensive and precise interpretatio
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#29983;&#25104;&#29109;&#31070;&#32463;&#26368;&#20248;&#20256;&#36755;&#22312;&#27979;&#24230;&#21040;&#27979;&#24230;&#26144;&#23556;&#20013;&#30340;&#24212;&#29992;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#38750;&#24179;&#26041;&#27431;&#27663;&#36317;&#31163;&#25104;&#26412;&#12289;&#30830;&#23450;&#24615;&#33945;&#26684;&#26144;&#23556;&#12289;&#26144;&#23556;&#36328;&#19981;&#21487;&#27604;&#36739;&#31354;&#38388;&#21644;&#36136;&#37327;&#23432;&#24658;&#32422;&#26463;&#31561;&#23454;&#38469;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.09254</link><description>&lt;p&gt;
&#29983;&#25104;&#29109;&#31070;&#32463;&#26368;&#20248;&#20256;&#36755;&#22312;&#31354;&#38388;&#20869;&#22806;&#26144;&#23556;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Generative Entropic Neural Optimal Transport To Map Within and Across Spaces. (arXiv:2310.09254v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09254
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#29983;&#25104;&#29109;&#31070;&#32463;&#26368;&#20248;&#20256;&#36755;&#22312;&#27979;&#24230;&#21040;&#27979;&#24230;&#26144;&#23556;&#20013;&#30340;&#24212;&#29992;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#38750;&#24179;&#26041;&#27431;&#27663;&#36317;&#31163;&#25104;&#26412;&#12289;&#30830;&#23450;&#24615;&#33945;&#26684;&#26144;&#23556;&#12289;&#26144;&#23556;&#36328;&#19981;&#21487;&#27604;&#36739;&#31354;&#38388;&#21644;&#36136;&#37327;&#23432;&#24658;&#32422;&#26463;&#31561;&#23454;&#38469;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#27979;&#24230;&#21040;&#27979;&#24230;&#30340;&#26144;&#23556;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#23588;&#20854;&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#21344;&#25454;&#37325;&#35201;&#22320;&#20301;&#12290;&#36817;&#24180;&#26469;&#65292;&#21463;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#21551;&#21457;&#30340;&#25216;&#26415;&#19981;&#26029;&#28044;&#29616;&#12290;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36825;&#20123;&#26041;&#27861;&#32479;&#31216;&#20026;"&#31070;&#32463;&#26368;&#20248;&#20256;&#36755;"&#65292;&#23558;&#26368;&#20248;&#20256;&#36755;&#20316;&#20026;&#24402;&#32435;&#20559;&#22909;&#65306;&#36825;&#20123;&#26144;&#23556;&#24212;&#35813;&#38024;&#23545;&#32473;&#23450;&#30340;&#25104;&#26412;&#20989;&#25968;&#26159;&#26368;&#20248;&#30340;&#65292;&#33021;&#20197;&#33410;&#32422;&#30340;&#26041;&#24335;&#65288;&#36890;&#36807;&#26368;&#23567;&#21270;&#20301;&#31227;&#65289;&#22312;&#31354;&#38388;&#20869;&#25110;&#31354;&#38388;&#38388;&#31227;&#21160;&#28857;&#12290;&#36825;&#19968;&#21407;&#21017;&#22312;&#30452;&#35266;&#19978;&#26159;&#21512;&#29702;&#30340;&#65292;&#20294;&#24448;&#24448;&#38754;&#20020;&#20960;&#20010;&#23454;&#38469;&#25361;&#25112;&#65292;&#38656;&#35201;&#35843;&#25972;&#26368;&#20248;&#20256;&#36755;&#24037;&#20855;&#31665;&#65306;&#22788;&#29702;&#20854;&#20182;&#38750;&#24179;&#26041;&#27431;&#27663;&#36317;&#31163;&#25104;&#26412;&#30340;&#25361;&#25112;&#65292;&#30830;&#23450;&#24615;&#29366;&#20917;&#19979;&#30340;&#33945;&#26684;&#26144;&#23556;&#20844;&#24335;&#20250;&#38480;&#21046;&#28789;&#27963;&#24615;&#65292;&#26144;&#23556;&#22312;&#19981;&#21487;&#27604;&#36739;&#30340;&#31354;&#38388;&#20013;&#20250;&#24102;&#26469;&#22810;&#20010;&#25361;&#25112;&#65292;&#26368;&#20248;&#20256;&#36755;&#22266;&#26377;&#30340;&#36136;&#37327;&#23432;&#24658;&#32422;&#26463;&#21487;&#33021;&#23545;&#24322;&#24120;&#25968;&#25454;&#32473;&#20104;&#36807;&#22810;&#30340;&#37325;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning measure-to-measure mappings is a crucial task in machine learning, featured prominently in generative modeling. Recent years have witnessed a surge of techniques that draw inspiration from optimal transport (OT) theory. Combined with neural network models, these methods collectively known as \textit{Neural OT} use optimal transport as an inductive bias: such mappings should be optimal w.r.t. a given cost function, in the sense that they are able to move points in a thrifty way, within (by minimizing displacements) or across spaces (by being isometric). This principle, while intuitive, is often confronted with several practical challenges that require adapting the OT toolbox: cost functions other than the squared-Euclidean cost can be challenging to handle, the deterministic formulation of Monge maps leaves little flexibility, mapping across incomparable spaces raises multiple challenges, while the mass conservation constraint inherent to OT can provide too much credit to outli
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36801;&#31227;&#23398;&#20064;&#20013;&#25968;&#25454;&#38598;&#20462;&#21098;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#25968;&#25454;&#38598;&#20462;&#21098;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#35266;&#28857;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#36801;&#31227;&#23398;&#20064;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#26631;&#31614;&#26144;&#23556;&#21644;&#29305;&#24449;&#26144;&#23556;&#36825;&#20004;&#31181;&#26032;&#30340;&#25968;&#25454;&#38598;&#20462;&#21098;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.08782</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#39537;&#21160;&#29983;&#20135;&#21147;&#65306;&#22686;&#24378;&#36801;&#31227;&#23398;&#20064;&#30340;&#39640;&#25928;&#25968;&#25454;&#38598;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
Selectivity Drives Productivity: Efficient Dataset Pruning for Enhanced Transfer Learning. (arXiv:2310.08782v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36801;&#31227;&#23398;&#20064;&#20013;&#25968;&#25454;&#38598;&#20462;&#21098;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#25968;&#25454;&#38598;&#20462;&#21098;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#35266;&#28857;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#36801;&#31227;&#23398;&#20064;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#26631;&#31614;&#26144;&#23556;&#21644;&#29305;&#24449;&#26144;&#23556;&#36825;&#20004;&#31181;&#26032;&#30340;&#25968;&#25454;&#38598;&#20462;&#21098;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25968;&#25454;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#20294;&#21516;&#26102;&#20063;&#20250;&#24102;&#26469;&#24040;&#22823;&#30340;&#35745;&#31639;&#21644;&#22522;&#30784;&#35774;&#26045;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#38598;&#20462;&#21098;&#65288;DP&#65289;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#20986;&#29616;&#65292;&#36890;&#36807;&#35782;&#21035;&#21644;&#21024;&#38500;&#20887;&#20313;&#30340;&#35757;&#32451;&#26679;&#26412;&#26469;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#35299;&#20915;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;DP&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#22312;&#19979;&#28216;&#30446;&#26631;&#20219;&#21153;&#20013;&#25552;&#39640;&#39044;&#35757;&#32451;&#25928;&#29575;&#21644;&#23436;&#25972;&#24494;&#35843;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20462;&#21098;&#28304;&#25968;&#25454;&#38598;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36801;&#31227;&#23398;&#20064;&#30340;DP&#38382;&#39064;&#20173;&#28982;&#26410;&#35299;&#20915;&#65292;&#22240;&#20026;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#23558;DP&#21644;&#36801;&#31227;&#23398;&#20064;&#35270;&#20026;&#29420;&#31435;&#30340;&#38382;&#39064;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35270;&#35282;&#65292;&#23558;DP&#19982;&#36801;&#31227;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;DP&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#36801;&#31227;&#23398;&#20064;&#33539;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;DP&#26041;&#27861;&#65292;&#21363;&#26631;&#31614;&#26144;&#23556;&#21644;&#29305;&#24449;&#26144;&#23556;&#65292;&#29992;&#20110;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Massive data is often considered essential for deep learning applications, but it also incurs significant computational and infrastructural costs. Therefore, dataset pruning (DP) has emerged as an effective way to improve data efficiency by identifying and removing redundant training samples without sacrificing performance. In this work, we aim to address the problem of DP for transfer learning, i.e., how to prune a source dataset for improved pretraining efficiency and lossless finetuning accuracy on downstream target tasks. To our best knowledge, the problem of DP for transfer learning remains open, as previous studies have primarily addressed DP and transfer learning as separate problems. By contrast, we establish a unified viewpoint to integrate DP with transfer learning and find that existing DP methods are not suitable for the transfer learning paradigm. We then propose two new DP methods, label mapping and feature mapping, for supervised and self-supervised pretraining settings 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;TD3&#31639;&#27861;&#30340;&#21333;&#26234;&#33021;&#20307;&#26041;&#27861;&#65292;&#22312;CARLA&#27169;&#25311;&#24179;&#21488;&#20013;&#23637;&#31034;&#20102;&#22312;&#22797;&#26434;T&#22411;&#36335;&#21475;&#23548;&#33322;&#20013;&#31283;&#23450;&#25910;&#25947;&#21644;&#25913;&#36827;&#23433;&#20840;&#24615;&#33021;&#65292;&#24182;&#22312;&#34892;&#31243;&#24310;&#35823;&#12289;&#30896;&#25758;&#20943;&#23569;&#21644;&#24635;&#20307;&#25104;&#26412;&#31561;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.08595</link><description>&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#20132;&#21449;&#36335;&#21475;&#23548;&#33322;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Autonomous Vehicle Intersection Navigation. (arXiv:2310.08595v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;TD3&#31639;&#27861;&#30340;&#21333;&#26234;&#33021;&#20307;&#26041;&#27861;&#65292;&#22312;CARLA&#27169;&#25311;&#24179;&#21488;&#20013;&#23637;&#31034;&#20102;&#22312;&#22797;&#26434;T&#22411;&#36335;&#21475;&#23548;&#33322;&#20013;&#31283;&#23450;&#25910;&#25947;&#21644;&#25913;&#36827;&#23433;&#20840;&#24615;&#33021;&#65292;&#24182;&#22312;&#34892;&#31243;&#24310;&#35823;&#12289;&#30896;&#25758;&#20943;&#23569;&#21644;&#24635;&#20307;&#25104;&#26412;&#31561;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#23494;&#38598;&#20132;&#36890;&#22330;&#26223;&#20013;&#65292;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#22312;&#22797;&#26434;T&#22411;&#36335;&#21475;&#23548;&#33322;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#23454;&#26102;&#22320;&#20351;AVs&#20570;&#20986;&#23433;&#20840;&#39640;&#25928;&#30340;&#20915;&#31574;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;&#21452;&#24310;&#36831;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;TD3&#65289;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#20302;&#25104;&#26412;&#12289;&#21333;&#26234;&#33021;&#20307;&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;T&#22411;&#36335;&#21475;&#19978;&#30340;&#39640;&#25928;&#23433;&#20840;&#23548;&#33322;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#25105;&#20204;&#22312;CARLA&#27169;&#25311;&#24179;&#21488;&#19978;&#23545;&#25105;&#20204;&#30340;TD3&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#65292;&#35813;&#26041;&#27861;&#21576;&#29616;&#20986;&#31283;&#23450;&#30340;&#25910;&#25947;&#24615;&#21644;&#25913;&#36827;&#30340;&#23433;&#20840;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#20132;&#36890;&#23494;&#24230;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;AV&#33021;&#22815;&#26377;&#25928;&#22320;&#23548;&#33322;T&#22411;&#36335;&#21475;&#65292;&#22312;&#34892;&#31243;&#24310;&#35823;&#12289;&#30896;&#25758;&#20943;&#23569;&#21644;&#24635;&#20307;&#25104;&#26412;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#23545;&#24378;&#21270;&#23398;&#20064;&#22312;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#39046;&#22495;&#30340;&#24212;&#29992;&#36129;&#29486;&#20102;&#26032;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore the challenges associated with navigating complex T-intersections in dense traffic scenarios for autonomous vehicles (AVs). Reinforcement learning algorithms have emerged as a promising approach to address these challenges by enabling AVs to make safe and efficient decisions in real-time. Here, we address the problem of efficiently and safely navigating T-intersections using a lower-cost, single-agent approach based on the Twin Delayed Deep Deterministic Policy Gradient (TD3) reinforcement learning algorithm. We show that our TD3-based method, when trained and tested in the CARLA simulation platform, demonstrates stable convergence and improved safety performance in various traffic densities. Our results reveal that the proposed approach enables the AV to effectively navigate T-intersections, outperforming previous methods in terms of travel delays, collision minimization, and overall cost. This study contributes to the growing body of knowledge on reinforceme
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23545;&#35270;&#35273;&#25968;&#25454;&#31867;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#21457;&#29616;&#34429;&#28982;&#22312;&#26576;&#20123;&#26679;&#24335;&#21270;&#25968;&#25454;&#31867;&#22411;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#22522;&#26412;&#25805;&#20316;&#24341;&#36215;&#30340;&#31616;&#21333;&#25968;&#25454;&#31867;&#22411;&#19978;&#34920;&#29616;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2310.08577</link><description>&lt;p&gt;
&#35270;&#35273;&#25968;&#25454;&#31867;&#22411;&#29702;&#35299;&#24182;&#38750;&#28304;&#33258;&#25193;&#23637;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Visual Data-Type Understanding does not emerge from Scaling Vision-Language Models. (arXiv:2310.08577v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23545;&#35270;&#35273;&#25968;&#25454;&#31867;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#21457;&#29616;&#34429;&#28982;&#22312;&#26576;&#20123;&#26679;&#24335;&#21270;&#25968;&#25454;&#31867;&#22411;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#22522;&#26412;&#25805;&#20316;&#24341;&#36215;&#30340;&#31616;&#21333;&#25968;&#25454;&#31867;&#22411;&#19978;&#34920;&#29616;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#21457;&#23637;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#35270;&#35273;&#35821;&#20041;&#20869;&#23481;&#35782;&#21035;&#25928;&#26524;&#65292;&#21253;&#25324;&#20986;&#33394;&#30340;&#22797;&#21512;&#22270;&#20687;&#29702;&#35299;&#23454;&#20363;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#35270;&#35273;&#25968;&#25454;&#31867;&#22411;&#35782;&#21035;&#65292;&#36825;&#26159;&#19968;&#39033;&#22522;&#26412;&#30340;&#24863;&#30693;&#25216;&#33021;&#65292;&#23545;&#25968;&#25454;&#25972;&#29702;&#65288;&#20363;&#22914;&#20174;&#22823;&#22411;&#25968;&#25454;&#38598;&#20013;&#21435;&#38500;&#22122;&#22768;&#25968;&#25454;&#65292;&#39046;&#22495;&#29305;&#23450;&#30340;&#26816;&#32034;&#65289;&#21644;&#33258;&#20027;&#35270;&#35273;&#65288;&#20363;&#22914;&#21306;&#20998;&#19981;&#21516;&#30340;&#22825;&#27668;&#21464;&#21270;&#21644;&#30456;&#26426;&#38236;&#22836;&#27745;&#26579;&#65289;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#32463;&#36807;27&#31181;&#35270;&#35273;&#25968;&#25454;&#31867;&#22411;&#30340;&#21160;&#29289;&#22270;&#20687;&#30340;&#20462;&#25913;&#65292;&#28085;&#30422;&#20102;&#22235;&#20010;&#24191;&#27867;&#30340;&#31867;&#21035;&#12290;&#23545;39&#20010;&#21442;&#25968;&#33539;&#22260;&#20174;100M&#21040;80B&#30340;VLMs&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#38646;&#26679;&#26412;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#20102;&#19968;&#20010;&#32454;&#33268;&#30340;&#24615;&#33021;&#26223;&#35266;&#12290;&#34429;&#28982;VLMs&#22312;&#35782;&#21035;&#26576;&#20123;&#26679;&#24335;&#21270;&#30340;&#25968;&#25454;&#31867;&#22411;&#65288;&#20363;&#22914;&#21345;&#36890;&#21644;&#33609;&#22270;&#65289;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#22522;&#26412;&#25805;&#20316;&#65288;&#20363;&#22914;&#22270;&#20687;&#26059;&#36716;&#25110;&#28155;&#21152;&#22122;&#22768;&#65289;&#24341;&#36215;&#30340;&#31616;&#21333;&#25968;&#25454;&#31867;&#22411;&#19978;&#34920;&#29616;&#20986;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in the development of vision-language models (VLMs) are yielding remarkable success in recognizing visual semantic content, including impressive instances of compositional image understanding. Here, we introduce the novel task of Visual Data-Type Identification, a basic perceptual skill with implications for data curation (e.g., noisy data-removal from large datasets, domain-specific retrieval) and autonomous vision (e.g., distinguishing changing weather conditions from camera lens staining). We develop two datasets consisting of animal images altered across a diverse set of 27 visual data-types, spanning four broad categories. An extensive zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a nuanced performance landscape. While VLMs are reasonably good at identifying certain stylistic \textit{data-types}, such as cartoons and sketches, they struggle with simpler data-types arising from basic manipulations like image rotations or additive noise.
&lt;/p&gt;</description></item><item><title>&#24322;&#26500;&#36801;&#31227;&#23398;&#20064;&#36866;&#29992;&#20110;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#12289;&#25968;&#25454;&#20998;&#24067;&#21644;&#26631;&#31614;&#31354;&#38388;&#30340;&#24773;&#20917;&#65292;&#36890;&#36807;&#22788;&#29702;&#36825;&#20123;&#24046;&#24322;&#26469;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.08459</link><description>&lt;p&gt;
&#24322;&#26500;&#36801;&#31227;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Heterogeneous Transfer Learning. (arXiv:2310.08459v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08459
&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#36801;&#31227;&#23398;&#20064;&#36866;&#29992;&#20110;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#12289;&#25968;&#25454;&#20998;&#24067;&#21644;&#26631;&#31614;&#31354;&#38388;&#30340;&#24773;&#20917;&#65292;&#36890;&#36807;&#22788;&#29702;&#36825;&#20123;&#24046;&#24322;&#26469;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36801;&#31227;&#23398;&#20064;&#30340;&#24212;&#29992;&#22312;&#24456;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#23427;&#21033;&#29992;&#28304;&#39046;&#22495;&#30340;&#30693;&#35782;&#26469;&#22686;&#24378;&#30446;&#26631;&#39046;&#22495;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20854;&#25104;&#21151;&#30340;&#20851;&#38190;&#22312;&#20110;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#20043;&#38388;&#30340;&#20849;&#20139;&#30693;&#35782;&#65292;&#36825;&#26159;&#22823;&#22810;&#25968;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#30340;&#21069;&#25552;&#26465;&#20214;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#20004;&#20010;&#39046;&#22495;&#20855;&#26377;&#30456;&#21516;&#30340;&#29305;&#24449;&#31354;&#38388;&#21644;&#26631;&#31614;&#31354;&#38388;&#65292;&#21363;&#21516;&#36136;&#36801;&#31227;&#23398;&#20064;&#65292;&#20294;&#36825;&#24182;&#19981;&#24635;&#26159;&#29616;&#23454;&#21512;&#29702;&#30340;&#20551;&#35774;&#12290;&#36890;&#24120;&#65292;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#22312;&#29305;&#24449;&#31354;&#38388;&#12289;&#25968;&#25454;&#20998;&#24067;&#21644;&#26631;&#31614;&#31354;&#38388;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#36825;&#20351;&#24471;&#33719;&#21462;&#20855;&#26377;&#19982;&#30446;&#26631;&#39046;&#22495;&#30456;&#21516;&#29305;&#24449;&#21644;&#26631;&#31614;&#31354;&#38388;&#30340;&#28304;&#39046;&#22495;&#25968;&#25454;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#25110;&#26114;&#36149;&#12290;&#23545;&#36825;&#20123;&#24046;&#24322;&#36827;&#34892;&#38543;&#24847;&#30340;&#28040;&#38500;&#24182;&#19981;&#24635;&#26159;&#21487;&#34892;&#25110;&#26368;&#20248;&#30340;&#12290;&#22240;&#27492;&#65292;&#24322;&#26500;&#36801;&#31227;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#24212;&#23545;&#36825;&#31181;&#24046;&#24322;&#30340;&#26041;&#27861;&#24050;&#32463;&#23853;&#38706;&#22836;&#35282;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of transfer learning, an approach utilizing knowledge from a source domain to enhance model performance in a target domain, has seen a tremendous rise in recent years, underpinning many real-world scenarios. The key to its success lies in the shared common knowledge between the domains, a prerequisite in most transfer learning methodologies. These methods typically presuppose identical feature spaces and label spaces in both domains, known as homogeneous transfer learning, which, however, is not always a practical assumption. Oftentimes, the source and target domains vary in feature spaces, data distributions, and label spaces, making it challenging or costly to secure source domain data with identical feature and label spaces as the target domain. Arbitrary elimination of these differences is not always feasible or optimal. Thus, heterogeneous transfer learning, acknowledging and dealing with such disparities, has emerged as a promising approach for a variety of tasks.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PAIR&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#21482;&#33021;&#40657;&#30418;&#35775;&#38382;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#30772;&#35299;&#65292;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#12290;&#23454;&#35777;&#34920;&#26126;&#65292;PAIR&#36890;&#24120;&#21482;&#38656;&#35201;&#23569;&#20110;&#20108;&#21313;&#20010;&#26597;&#35810;&#26469;&#29983;&#25104;&#30772;&#35299;&#65292;&#26159;&#29616;&#26377;&#31639;&#27861;&#30340;&#25968;&#20010;&#25968;&#37327;&#32423;&#26356;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.08419</link><description>&lt;p&gt;
&#22312;&#20108;&#21313;&#20010;&#26597;&#35810;&#20013;&#30772;&#35299;&#40657;&#30418;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jailbreaking Black Box Large Language Models in Twenty Queries. (arXiv:2310.08419v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08419
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PAIR&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#21482;&#33021;&#40657;&#30418;&#35775;&#38382;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#30772;&#35299;&#65292;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#12290;&#23454;&#35777;&#34920;&#26126;&#65292;PAIR&#36890;&#24120;&#21482;&#38656;&#35201;&#23569;&#20110;&#20108;&#21313;&#20010;&#26597;&#35810;&#26469;&#29983;&#25104;&#30772;&#35299;&#65292;&#26159;&#29616;&#26377;&#31639;&#27861;&#30340;&#25968;&#20010;&#25968;&#37327;&#32423;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#30340;&#19968;&#33268;&#24615;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#31867;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#30772;&#35299;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#36843;&#20351;LLMs&#36229;&#36234;&#20854;&#23433;&#20840;&#38450;&#25252;&#25514;&#26045;&#12290;&#22240;&#27492;&#65292;&#35782;&#21035;&#36825;&#20123;&#28431;&#27934;&#23545;&#20110;&#29702;&#35299;&#22266;&#26377;&#24369;&#28857;&#24182;&#38450;&#27490;&#26410;&#26469;&#30340;&#19981;&#24403;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Prompt Automatic Iterative Refinement&#65288;PAIR&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20165;&#36890;&#36807;&#23545;LLM&#36827;&#34892;&#40657;&#30418;&#35775;&#38382;&#30340;&#31639;&#27861;&#29983;&#25104;&#35821;&#20041;&#30772;&#35299;&#12290;PAIR&#21463;&#21040;&#31038;&#20250;&#24037;&#31243;&#25915;&#20987;&#30340;&#21551;&#21457;&#65292;&#20351;&#29992;&#25915;&#20987;&#32773;LLM&#33258;&#21160;&#29983;&#25104;&#38024;&#23545;&#30446;&#26631;LLM&#30340;&#30772;&#35299;&#65292;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#12290;&#25915;&#20987;&#32773;LLM&#36890;&#36807;&#36845;&#20195;&#26597;&#35810;&#30446;&#26631;LLM&#26469;&#26356;&#26032;&#21644;&#25913;&#36827;&#20505;&#36873;&#30772;&#35299;&#12290;&#22312;&#23454;&#35777;&#19978;&#65292;PAIR&#36890;&#24120;&#21482;&#38656;&#35201;&#23569;&#20110;&#20108;&#21313;&#20010;&#26597;&#35810;&#26469;&#29983;&#25104;&#30772;&#35299;&#65292;&#36825;&#27604;&#29616;&#26377;&#31639;&#27861;&#39640;&#25928;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;PAIR&#36824;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#30772;&#35299;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is growing interest in ensuring that large language models (LLMs) align with human values. However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse. To this end, we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM. PAIR -- which is inspired by social engineering attacks -- uses an attacker LLM to automatically generate jailbreaks for a separate targeted LLM without human intervention. In this way, the attacker LLM iteratively queries the target LLM to update and refine a candidate jailbreak. Empirically, PAIR often requires fewer than twenty queries to produce a jailbreak, which is orders of magnitude more efficient than existing algorithms. PAIR also achieves competitive jailbrea
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#22522;&#20110;&#24605;&#32500;&#38142;&#30340;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#20801;&#35768;&#20351;&#29992;&#20013;&#38388;&#29983;&#25104;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;Transformer&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#32447;&#24615;&#25968;&#37327;&#30340;&#35299;&#30721;&#27493;&#39588;&#22312;&#26631;&#20934;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#22686;&#21152;&#20102;&#26126;&#26174;&#30340;&#26032;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.07923</link><description>&lt;p&gt;
&#22522;&#20110;&#24605;&#32500;&#38142;&#30340;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
The Expresssive Power of Transformers with Chain of Thought. (arXiv:2310.07923v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#22522;&#20110;&#24605;&#32500;&#38142;&#30340;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#20801;&#35768;&#20351;&#29992;&#20013;&#38388;&#29983;&#25104;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;Transformer&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#32447;&#24615;&#25968;&#37327;&#30340;&#35299;&#30721;&#27493;&#39588;&#22312;&#26631;&#20934;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#22686;&#21152;&#20102;&#26126;&#26174;&#30340;&#26032;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#29702;&#35770;&#30740;&#31350;&#21457;&#29616;&#20102;&#19968;&#20123;&#20986;&#20154;&#24847;&#26009;&#22320;&#31616;&#21333;&#30340;&#25512;&#29702;&#38382;&#39064;&#65292;&#20363;&#22914;&#26816;&#26597;&#22270;&#20013;&#26159;&#21542;&#23384;&#22312;&#36830;&#25509;&#30340;&#20004;&#20010;&#33410;&#28857;&#65292;&#25110;&#27169;&#25311;&#26377;&#38480;&#29366;&#24577;&#26426;&#65292;&#36825;&#20123;&#38382;&#39064;&#34987;&#35777;&#26126;&#26080;&#27861;&#30001;&#31435;&#21363;&#35835;&#21462;&#36755;&#20837;&#21518;&#22238;&#31572;&#30340;&#26631;&#20934;Transformer&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#36890;&#36807;&#20801;&#35768;Transformer&#20351;&#29992;&#8220;&#24605;&#32500;&#38142;&#8221;&#25110;&#8220;&#33609;&#31295;&#32440;&#8221;&#65292;&#21363;&#22312;&#22238;&#31572;&#20043;&#21069;&#29983;&#25104;&#24182;&#20381;&#36182;&#19968;&#31995;&#21015;&#20013;&#38388;token&#65292;&#21487;&#20197;&#25913;&#21892;&#20854;&#25512;&#29702;&#33021;&#21147;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#38382;&#65306;&#36825;&#31181;&#20013;&#38388;&#29983;&#25104;&#26159;&#21542;&#20174;&#26681;&#26412;&#19978;&#25193;&#23637;&#20102;&#20165;&#26377;&#35299;&#30721;&#22120;&#30340;Transformer&#30340;&#35745;&#31639;&#33021;&#21147;&#65311;&#25105;&#20204;&#34920;&#26126;&#31572;&#26696;&#26159;&#32943;&#23450;&#30340;&#65292;&#20294;&#22686;&#21152;&#30340;&#31243;&#24230;&#20851;&#38190;&#21462;&#20915;&#20110;&#20013;&#38388;&#29983;&#25104;&#30340;&#25968;&#37327;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;&#30456;&#23545;&#20110;&#36755;&#20837;&#38271;&#24230;&#26469;&#35828;&#65292;&#20855;&#26377;&#23545;&#25968;&#32423;&#35299;&#30721;&#27493;&#39588;&#30340;Transformer&#35299;&#30721;&#22120;&#20165;&#30053;&#24494;&#25512;&#21160;&#20102;&#26631;&#20934;Transformer&#30340;&#26497;&#38480;&#65292;&#32780;&#32447;&#24615;&#25968;&#37327;&#30340;&#35299;&#30721;&#27493;&#39588;&#21017;&#22686;&#21152;&#20102;&#26126;&#26174;&#30340;&#26032;&#33021;&#21147;&#65288;&#22312;&#26631;&#20934;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent theoretical work has identified surprisingly simple reasoning problems, such as checking if two nodes in a graph are connected or simulating finite-state machines, that are provably unsolvable by standard transformers that answer immediately after reading their input. However, in practice, transformers' reasoning can be improved by allowing them to use a "chain of thought" or "scratchpad", i.e., generate and condition on a sequence of intermediate tokens before answering. Motivated by this, we ask: Does such intermediate generation fundamentally extend the computational power of a decoder-only transformer? We show that the answer is yes, but the amount of increase depends crucially on the amount of intermediate generation. For instance, we find that transformer decoders with a logarithmic number of decoding steps (w.r.t. the input length) push the limits of standard transformers only slightly, while a linear number of decoding steps adds a clear new ability (under standard compl
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#24230;&#37327;&#26041;&#27861;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#29305;&#24449;&#31232;&#30095;&#24615;&#30340;&#25104;&#21151;&#65292;&#24182;&#27979;&#35797;&#20102;&#32447;&#24615;&#24615;&#21644;&#31232;&#30095;&#24615;&#20551;&#35774;&#30340;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#30340;&#28608;&#27963;&#21487;&#20197;&#20934;&#30830;&#22320;&#24314;&#27169;&#20026;&#29305;&#24449;&#30340;&#31232;&#30095;&#32447;&#24615;&#32452;&#21512;&#65292;&#24182;&#19988;&#22312;&#31532;&#19968;&#23618;&#21644;&#26368;&#21518;&#19968;&#23618;&#20013;&#21576;&#26368;&#31232;&#30095;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2310.07837</link><description>&lt;p&gt;
&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#34913;&#37327;&#29305;&#24449;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
Measuring Feature Sparsity in Language Models. (arXiv:2310.07837v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07837
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#24230;&#37327;&#26041;&#27861;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#29305;&#24449;&#31232;&#30095;&#24615;&#30340;&#25104;&#21151;&#65292;&#24182;&#27979;&#35797;&#20102;&#32447;&#24615;&#24615;&#21644;&#31232;&#30095;&#24615;&#20551;&#35774;&#30340;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#30340;&#28608;&#27963;&#21487;&#20197;&#20934;&#30830;&#22320;&#24314;&#27169;&#20026;&#29305;&#24449;&#30340;&#31232;&#30095;&#32447;&#24615;&#32452;&#21512;&#65292;&#24182;&#19988;&#22312;&#31532;&#19968;&#23618;&#21644;&#26368;&#21518;&#19968;&#23618;&#20013;&#21576;&#26368;&#31232;&#30095;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#65292;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28608;&#27963;&#21487;&#20197;&#34987;&#24314;&#27169;&#20026;&#19982;&#36755;&#20837;&#25991;&#26412;&#29305;&#24449;&#23545;&#24212;&#30340;&#21521;&#37327;&#30340;&#31232;&#30095;&#32447;&#24615;&#32452;&#21512;&#12290;&#22312;&#36825;&#20010;&#20551;&#35774;&#19979;&#65292;&#36825;&#20123;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#31232;&#30095;&#32534;&#30721;&#37325;&#26500;&#29305;&#24449;&#26041;&#21521;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#24230;&#37327;&#26041;&#27861;&#26469;&#35780;&#20272;&#36825;&#20123;&#31232;&#30095;&#32534;&#30721;&#25216;&#26415;&#30340;&#25104;&#21151;&#65292;&#24182;&#27979;&#35797;&#32447;&#24615;&#24615;&#21644;&#31232;&#30095;&#24615;&#20551;&#35774;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#24230;&#37327;&#26041;&#27861;&#21487;&#20197;&#39044;&#27979;&#21512;&#25104;&#31232;&#30095;&#32447;&#24615;&#28608;&#27963;&#30340;&#31232;&#30095;&#31243;&#24230;&#65292;&#24182;&#33021;&#22815;&#21306;&#20998;&#31232;&#30095;&#32447;&#24615;&#25968;&#25454;&#21644;&#20854;&#20182;&#20960;&#31181;&#20998;&#24067;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#24230;&#37327;&#26041;&#27861;&#26469;&#34913;&#37327;&#20960;&#20010;&#35821;&#35328;&#27169;&#22411;&#30340;&#31232;&#30095;&#31243;&#24230;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#23545;&#29031;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#35821;&#35328;&#27169;&#22411;&#30340;&#28608;&#27963;&#21487;&#20197;&#20934;&#30830;&#22320;&#24314;&#27169;&#20026;&#29305;&#24449;&#30340;&#31232;&#30095;&#32447;&#24615;&#32452;&#21512;&#65292;&#24182;&#19988;&#22312;&#31532;&#19968;&#23618;&#21644;&#26368;&#21518;&#19968;&#23618;&#20013;&#21576;&#26368;&#31232;&#30095;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have proposed that activations in language models can be modelled as sparse linear combinations of vectors corresponding to features of input text. Under this assumption, these works aimed to reconstruct feature directions using sparse coding. We develop metrics to assess the success of these sparse coding techniques and test the validity of the linearity and sparsity assumptions. We show our metrics can predict the level of sparsity on synthetic sparse linear activations, and can distinguish between sparse linear data and several other distributions. We use our metrics to measure levels of sparsity in several language models. We find evidence that language model activations can be accurately modelled by sparse linear combinations of features, significantly more so than control datasets. We also show that model activations appear to be sparsest in the first and final layers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#31283;&#24577;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#19982;&#31934;&#24515;&#35774;&#35745;&#30340;&#25506;&#32034;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#20013;&#20248;&#20808;&#25910;&#38598;&#25345;&#20037;&#20215;&#20540;&#20449;&#24687;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07786</link><description>&lt;p&gt;
&#38750;&#31283;&#24577;&#29615;&#22659;&#19979;&#22522;&#20110;&#31070;&#32463;&#39044;&#27979;&#38598;&#25104;&#25277;&#26679;&#30340;&#24773;&#22659;&#36172;&#21338;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Non-Stationary Contextual Bandit Learning via Neural Predictive Ensemble Sampling. (arXiv:2310.07786v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#31283;&#24577;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#19982;&#31934;&#24515;&#35774;&#35745;&#30340;&#25506;&#32034;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#20013;&#20248;&#20808;&#25910;&#38598;&#25345;&#20037;&#20215;&#20540;&#20449;&#24687;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#19990;&#30028;&#20013;&#30340;&#24773;&#22659;&#36172;&#21338;&#24212;&#29992;&#24120;&#24120;&#22240;&#23395;&#33410;&#24615;&#12289;&#20598;&#28982;&#24615;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#31038;&#20132;&#36235;&#21183;&#32780;&#21576;&#38750;&#31283;&#24577;&#12290;&#23613;&#31649;&#25991;&#29486;&#20013;&#24050;&#25552;&#20986;&#20102;&#35768;&#22810;&#38750;&#31283;&#24577;&#24773;&#22659;&#36172;&#21338;&#23398;&#20064;&#31639;&#27861;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#23545;&#25345;&#20037;&#20215;&#20540;&#20449;&#24687;&#30340;&#20248;&#20808;&#32771;&#34385;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#25506;&#32034;&#26102;&#36807;&#24230;&#65292;&#25110;&#32773;&#35774;&#35745;&#26041;&#24335;&#38590;&#20197;&#22312;&#20855;&#26377;&#39640;&#32500;&#29992;&#25143;&#29305;&#23450;&#29305;&#24449;&#21644;&#22823;&#35268;&#27169;&#21160;&#20316;&#38598;&#30340;&#29616;&#20195;&#24212;&#29992;&#20013;&#25193;&#23637;&#65292;&#25110;&#32773;&#20004;&#32773;&#37117;&#26377;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#31283;&#24577;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;&#65292;&#23427;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#23427;&#23558;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#19982;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#25506;&#32034;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#20013;&#25112;&#30053;&#24615;&#22320;&#20248;&#20808;&#25910;&#38598;&#20855;&#26377;&#26368;&#25345;&#20037;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#36890;&#36807;&#22312;&#23637;&#31034;&#26126;&#26174;&#38750;&#31283;&#24577;&#30340;&#20004;&#20010;&#23454;&#38469;&#25512;&#33616;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#32988;&#36807;&#29616;&#26377;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world applications of contextual bandits often exhibit non-stationarity due to seasonality, serendipity, and evolving social trends. While a number of non-stationary contextual bandit learning algorithms have been proposed in the literature, they excessively explore due to a lack of prioritization for information of enduring value, or are designed in ways that do not scale in modern applications with high-dimensional user-specific features and large action set, or both. In this paper, we introduce a novel non-stationary contextual bandit algorithm that addresses these concerns. It combines a scalable, deep-neural-network-based architecture with a carefully designed exploration mechanism that strategically prioritizes collecting information with the most lasting value in a non-stationary environment. Through empirical evaluations on two real-world recommendation datasets, which exhibit pronounced non-stationarity, we demonstrate that our approach significantly outperforms the state
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#30340;&#22270;&#20687;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#65292;&#25972;&#21512;&#20102;Siamese&#32593;&#32476;&#21644;Grad-CAM&#65292;&#33021;&#22815;&#25552;&#20379;&#30456;&#20284;&#24615;&#20998;&#25968;&#20197;&#21450;&#35270;&#35273;&#19978;&#30340;&#20107;&#23454;&#21644;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#26377;&#28508;&#21147;&#25552;&#39640;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.07678</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22270;&#20687;&#30456;&#20284;&#24615;:&#25972;&#21512;Siamese&#32593;&#32476;&#21644;Grad-CAM
&lt;/p&gt;
&lt;p&gt;
Explainable Image Similarity: Integrating Siamese Networks and Grad-CAM. (arXiv:2310.07678v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#30340;&#22270;&#20687;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#65292;&#25972;&#21512;&#20102;Siamese&#32593;&#32476;&#21644;Grad-CAM&#65292;&#33021;&#22815;&#25552;&#20379;&#30456;&#20284;&#24615;&#20998;&#25968;&#20197;&#21450;&#35270;&#35273;&#19978;&#30340;&#20107;&#23454;&#21644;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#26377;&#28508;&#21147;&#25552;&#39640;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22270;&#20687;&#24212;&#29992;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#26222;&#21450;&#65292;&#20934;&#30830;&#19988;&#21487;&#35299;&#37322;&#30340;&#22270;&#20687;&#30456;&#20284;&#24615;&#24230;&#37327;&#30340;&#38656;&#27714;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#12290;&#29616;&#26377;&#30340;&#22270;&#20687;&#30456;&#20284;&#24615;&#27169;&#22411;&#24120;&#24120;&#32570;&#20047;&#36879;&#26126;&#24230;&#65292;&#38590;&#20197;&#29702;&#35299;&#20026;&#20309;&#35748;&#20026;&#20004;&#20010;&#22270;&#20687;&#30456;&#20284;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#30340;&#22270;&#20687;&#30456;&#20284;&#24615;&#30340;&#27010;&#24565;&#65292;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#31181;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#20379;&#30456;&#20284;&#24615;&#20998;&#25968;&#20197;&#21450;&#35270;&#35273;&#19978;&#30340;&#20107;&#23454;&#21644;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;Siamese&#32593;&#32476;&#21644;Grad-CAM&#25972;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#22270;&#20687;&#30456;&#20284;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#30340;&#28508;&#22312;&#21033;&#30410;&#21644;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#25152;&#25552;&#20986;&#26694;&#26550;&#25552;&#20379;&#30340;&#20107;&#23454;&#21644;&#21453;&#20107;&#23454;&#35299;&#37322;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35752;&#35770;&#65292;&#20197;&#36741;&#21161;&#20915;&#31574;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#28508;&#21147;&#25552;&#39640;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the proliferation of image-based applications in various domains, the need for accurate and interpretable image similarity measures has become increasingly critical. Existing image similarity models often lack transparency, making it challenging to understand the reasons why two images are considered similar. In this paper, we propose the concept of explainable image similarity, where the goal is the development of an approach, which is capable of providing similarity scores along with visual factual and counterfactual explanations. Along this line, we present a new framework, which integrates Siamese Networks and Grad-CAM for providing explainable image similarity and discuss the potential benefits and challenges of adopting this approach. In addition, we provide a comprehensive discussion about factual and counterfactual explanations provided by the proposed framework for assisting decision making. The proposed approach has the potential to enhance the interpretability, trustwor
&lt;/p&gt;</description></item><item><title>&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#65292;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#40657;&#30418;&#23376;&#30340;&#20915;&#31574;&#36807;&#31243;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#65292;&#23458;&#35266;&#35780;&#20272;&#20102;&#19977;&#31181;&#39046;&#20808;&#30340;&#35299;&#37322;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#37117;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.07534</link><description>&lt;p&gt;
XAI&#26041;&#27861;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Human-Centered Evaluation of XAI Methods. (arXiv:2310.07534v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07534
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#65292;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#40657;&#30418;&#23376;&#30340;&#20915;&#31574;&#36807;&#31243;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#65292;&#23458;&#35266;&#35780;&#20272;&#20102;&#19977;&#31181;&#39046;&#20808;&#30340;&#35299;&#37322;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#37117;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#21457;&#23637;&#30340;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#65292;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#26159;&#35299;&#26512;&#28145;&#24230;&#23398;&#20064;&#20013;&#25152;&#35859;&#30340;&#8220;&#40657;&#30418;&#23376;&#8221;&#20013;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#36817;&#24180;&#26469;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#26041;&#27861;&#65292;&#19987;&#38376;&#29992;&#20110;&#35299;&#37322;&#21508;&#31181;&#20219;&#21153;&#30340;&#20915;&#31574;&#12290;&#29305;&#21035;&#26159;&#22312;&#22270;&#20687;&#20998;&#31867;&#31561;&#20219;&#21153;&#20013;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20250;&#35782;&#21035;&#24182;&#24378;&#35843;&#23545;&#20998;&#31867;&#22120;&#39044;&#27979;&#24433;&#21709;&#26368;&#22823;&#30340;&#20851;&#38190;&#20687;&#32032;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36825;&#31181;&#26041;&#27861;&#19982;&#20154;&#31867;&#34892;&#20026;&#30456;&#20284;&#65306;&#24403;&#25105;&#20204;&#34987;&#35201;&#27714;&#35299;&#37322;&#20998;&#31867;&#22270;&#20687;&#30340;&#29702;&#30001;&#26102;&#65292;&#25105;&#20204;&#36890;&#24120;&#20250;&#25351;&#20986;&#26368;&#26174;&#33879;&#30340;&#29305;&#24449;&#25110;&#26041;&#38754;&#12290;&#21033;&#29992;&#36825;&#31181;&#31867;&#20284;&#24615;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#35797;&#22270;&#23458;&#35266;&#22320;&#35780;&#20272;&#19977;&#31181;&#39046;&#20808;&#30340;&#35299;&#37322;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#65306;&#65288;1&#65289;&#20856;&#22411;&#23616;&#37096;&#32593;&#32476;&#12289;&#65288;2&#65289;&#36974;&#25377;&#21644;&#65288;3&#65289;&#23618;&#27425;&#30456;&#20851;&#20256;&#25773;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#25152;&#31361;&#20986;&#30340;&#21306;&#22495;&#21487;&#33021;&#24046;&#24322;&#24456;&#22823;&#65292;&#20294;&#23427;&#20204;&#37117;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the ever-evolving field of Artificial Intelligence, a critical challenge has been to decipher the decision-making processes within the so-called "black boxes" in deep learning. Over recent years, a plethora of methods have emerged, dedicated to explaining decisions across diverse tasks. Particularly in tasks like image classification, these methods typically identify and emphasize the pivotal pixels that most influence a classifier's prediction. Interestingly, this approach mirrors human behavior: when asked to explain our rationale for classifying an image, we often point to the most salient features or aspects. Capitalizing on this parallel, our research embarked on a user-centric study. We sought to objectively measure the interpretability of three leading explanation methods: (1) Prototypical Part Network, (2) Occlusion, and (3) Layer-wise Relevance Propagation. Intriguingly, our results highlight that while the regions spotlighted by these methods can vary widely, they all offe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25903;&#30340;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#65292;&#29992;&#20110;&#39044;&#27979;&#24052;&#40654;&#38654;&#38718;&#30340;&#21457;&#29983;&#12290;&#35813;&#32593;&#32476;&#21033;&#29992;&#27668;&#35937;&#22270;&#30340;&#19981;&#21516;&#29305;&#24449;&#31354;&#38388;&#23610;&#24230;&#26469;&#35757;&#32451;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#24180;&#26399;&#30340;&#27668;&#35937;&#21464;&#37327;&#21644;&#22320;&#38754;&#33021;&#35265;&#24230;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21644;&#39564;&#35777;&#12290;&#36825;&#20123;&#26032;&#26550;&#26500;&#25552;&#39640;&#20102;&#32593;&#32476;&#24615;&#33021;&#65292;&#22312;&#26410;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#30450;&#39044;&#27979;&#65292;&#20855;&#26377;&#21512;&#29702;&#30340;&#39044;&#27979;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.07437</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#20998;&#25903;&#30340;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#20351;&#29992;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#31354;&#38388;&#23610;&#24230;&#30340;&#27668;&#35937;&#22270;&#39044;&#27979;&#24052;&#40654;&#38654;&#38718;&#30340;&#21457;&#29983;
&lt;/p&gt;
&lt;p&gt;
A Branched Deep Convolutional Network for Forecasting the Occurrence of Hazes in Paris using Meteorological Maps with Different Characteristic Spatial Scales. (arXiv:2310.07437v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07437
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25903;&#30340;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#65292;&#29992;&#20110;&#39044;&#27979;&#24052;&#40654;&#38654;&#38718;&#30340;&#21457;&#29983;&#12290;&#35813;&#32593;&#32476;&#21033;&#29992;&#27668;&#35937;&#22270;&#30340;&#19981;&#21516;&#29305;&#24449;&#31354;&#38388;&#23610;&#24230;&#26469;&#35757;&#32451;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#24180;&#26399;&#30340;&#27668;&#35937;&#21464;&#37327;&#21644;&#22320;&#38754;&#33021;&#35265;&#24230;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21644;&#39564;&#35777;&#12290;&#36825;&#20123;&#26032;&#26550;&#26500;&#25552;&#39640;&#20102;&#32593;&#32476;&#24615;&#33021;&#65292;&#22312;&#26410;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#30450;&#39044;&#27979;&#65292;&#20855;&#26377;&#21512;&#29702;&#30340;&#39044;&#27979;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#24179;&#21488;&#65292;&#29992;&#20110;&#39044;&#27979;&#20302;&#33021;&#35265;&#24230;&#20107;&#20214;&#25110;&#38654;&#38718;&#30340;&#21457;&#29983;&#12290;&#23427;&#36890;&#36807;&#20351;&#29992;&#22810;&#24180;&#26399;&#30340;&#21508;&#31181;&#27668;&#35937;&#21644;&#27700;&#25991;&#21464;&#37327;&#30340;&#21306;&#22495;&#26085;&#24120;&#22270;&#20316;&#20026;&#36755;&#20837;&#29305;&#24449;&#65292;&#24182;&#23558;&#22320;&#38754;&#33021;&#35265;&#24230;&#35266;&#27979;&#20316;&#20026;&#30446;&#26631;&#26469;&#36827;&#34892;&#35757;&#32451;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#20445;&#30041;&#19981;&#21516;&#36755;&#20837;&#29305;&#24449;&#30340;&#29305;&#24449;&#31354;&#38388;&#20449;&#24687;&#36827;&#34892;&#35757;&#32451;&#65292;&#26368;&#36817;&#24320;&#21457;&#20102;&#20004;&#31181;&#38024;&#23545;&#24052;&#40654;&#38654;&#38718;&#30340;&#20998;&#25903;&#26550;&#26500;&#12290;&#36825;&#20123;&#26032;&#26550;&#26500;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#22312;&#39564;&#35777;&#21644;&#30450;&#39044;&#27979;&#35780;&#20272;&#20013;&#20135;&#29983;&#20102;&#21512;&#29702;&#30340;&#20998;&#25968;&#65292;&#20351;&#29992;&#20102;2021&#24180;&#21644;2022&#24180;&#30340;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#27809;&#26377;&#22312;&#35757;&#32451;&#21644;&#39564;&#35777;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
A deep learning platform has been developed to forecast the occurrence of the low visibility events or hazes. It is trained by using multi-decadal daily regional maps of various meteorological and hydrological variables as input features and surface visibility observations as the targets. To better preserve the characteristic spatial information of different input features for training, two branched architectures have recently been developed for the case of Paris hazes. These new architectures have improved the performance of the network, producing reasonable scores in both validation and a blind forecasting evaluation using the data of 2021 and 2022 that have not been used in the training and validation.
&lt;/p&gt;</description></item><item><title>&#22312;gelu-4l&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#20869;&#23384;&#31649;&#29702;&#23545;&#20110;transformer&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#35828;&#26126;&#20102;Direct Logit Attribution&#25216;&#26415;&#30340;&#19981;&#20934;&#30830;&#20043;&#22788;&#12290;</title><link>http://arxiv.org/abs/2310.07325</link><description>&lt;p&gt;
&#30452;&#25509;&#36923;&#36753;&#23646;&#24615;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65306;gelu-4l&#20013;&#30340;&#20869;&#23384;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
An Adversarial Example for Direct Logit Attribution: Memory Management in gelu-4l. (arXiv:2310.07325v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07325
&lt;/p&gt;
&lt;p&gt;
&#22312;gelu-4l&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#20869;&#23384;&#31649;&#29702;&#23545;&#20110;transformer&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#35828;&#26126;&#20102;Direct Logit Attribution&#25216;&#26415;&#30340;&#19981;&#20934;&#30830;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;4&#23618;transformer&#20013;&#20869;&#23384;&#31649;&#29702;&#30340;&#20855;&#20307;&#35777;&#25454;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#21069;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#32452;&#20214;&#19968;&#33268;&#22320;&#31227;&#38500;&#21069;&#38754;&#32452;&#20214;&#30340;&#36755;&#20986;&#65292;&#36825;&#26159;&#19968;&#31181;&#28165;&#29702;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35299;&#37322;&#24615;&#25216;&#26415;Direct Logit Attribution&#25552;&#20379;&#20102;&#35823;&#23548;&#24615;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26126;&#30830;&#30340;&#20363;&#23376;&#65292;&#35777;&#26126;&#36825;&#31181;&#25216;&#26415;&#26159;&#19981;&#20934;&#30830;&#30340;&#65292;&#22240;&#20026;&#23427;&#27809;&#26377;&#32771;&#34385;&#21040;&#28165;&#29702;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide concrete evidence for memory management in a 4-layer transformer. Specifically, we identify clean-up behavior, in which model components consistently remove the output of preceeding components during a forward pass. Our findings suggest that the interpretability technique Direct Logit Attribution provides misleading results. We show explicit examples where this technique is inaccurate, as it does not account for clean-up behavior.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20174;&#24050;&#26377;&#30340;&#27169;&#24577;&#29983;&#25104;&#32570;&#22833;&#30340;MRI&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#22312;BraTS&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.07250</link><description>&lt;p&gt;
&#22312;BraTS&#25968;&#25454;&#38598;&#20013;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20174;&#24050;&#26377;&#30340;&#27169;&#24577;&#29983;&#25104;&#32570;&#22833;&#30340;MRI&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Synthesizing Missing MRI Sequences from Available Modalities using Generative Adversarial Networks in BraTS Dataset. (arXiv:2310.07250v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20174;&#24050;&#26377;&#30340;&#27169;&#24577;&#29983;&#25104;&#32570;&#22833;&#30340;MRI&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#22312;BraTS&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#24230;&#20405;&#34989;&#24615;&#21644;&#33268;&#21629;&#30340;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#33041;&#30284;&#12290;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#30001;&#20110;&#20854;&#26080;&#21019;&#21644;&#26080;&#36752;&#23556;&#24615;&#36136;&#65292;&#22312;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#24739;&#32773;&#30340;&#35786;&#26029;&#12289;&#27835;&#30103;&#35745;&#21010;&#21644;&#38543;&#35775;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22269;&#38469;&#33041;&#32959;&#30244;&#20998;&#21106;&#65288;BraTS&#65289;&#25361;&#25112;&#20026;&#21033;&#29992;&#22235;&#31181;&#32467;&#26500;&#24615;MRI&#25195;&#25551;&#65288;T1&#12289;T1Gd&#12289;T2&#12289;T2-FLAIR&#65289;&#20934;&#30830;&#39640;&#25928;&#22320;&#20998;&#21106;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#20122;&#21306;&#22495;&#25552;&#20379;&#20102;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#22235;&#20010;MRI&#24207;&#21015;&#19981;&#24635;&#26159;&#21487;&#29992;&#30340;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21487;&#20197;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#21512;&#25104;&#32570;&#22833;&#30340;MRI&#24207;&#21015;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23454;&#29616;&#24182;&#21033;&#29992;&#24320;&#28304;&#30340;GAN&#26041;&#27861;&#65292;&#20197;&#20219;&#19977;&#20010;MRI&#24207;&#21015;&#20316;&#20026;&#36755;&#20837;&#29983;&#25104;&#32570;&#22833;&#30340;&#31532;&#22235;&#20010;&#32467;&#26500;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36129;&#29486;&#32473;&#20102;&#31038;&#21306;&#39537;&#21160;&#30340;&#36890;&#29992;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65288;GaNDLF&#65289;&#65292;&#24182;&#22312;&#21512;&#25104;&#32570;&#22833;&#30340;MRI&#24207;&#21015;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Glioblastoma is a highly aggressive and lethal form of brain cancer. Magnetic resonance imaging (MRI) plays a significant role in the diagnosis, treatment planning, and follow-up of glioblastoma patients due to its non-invasive and radiation-free nature. The International Brain Tumor Segmentation (BraTS) challenge has contributed to generating numerous AI algorithms to accurately and efficiently segment glioblastoma sub-compartments using four structural (T1, T1Gd, T2, T2-FLAIR) MRI scans. However, these four MRI sequences may not always be available. To address this issue, Generative Adversarial Networks (GANs) can be used to synthesize the missing MRI sequences. In this paper, we implement and utilize an open-source GAN approach that takes any three MRI sequences as input to generate the missing fourth structural sequence. Our proposed approach is contributed to the community-driven generally nuanced deep learning framework (GaNDLF) and demonstrates promising results in synthesizing 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#38750;&#31574;&#30053;&#35780;&#20272;&#65288;OPEHF&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#20934;&#30830;&#35780;&#20272;&#20154;&#31867;&#21453;&#39304;&#20449;&#21495;&#12290;&#36825;&#20010;&#26694;&#26550;&#35299;&#20915;&#20102;&#29616;&#26377;OPE&#26041;&#27861;&#22312;&#20272;&#35745;&#20154;&#31867;&#21453;&#39304;&#20449;&#21495;&#19978;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2310.07123</link><description>&lt;p&gt;
&#29992;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#38750;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Off-Policy Evaluation for Human Feedback. (arXiv:2310.07123v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#38750;&#31574;&#30053;&#35780;&#20272;&#65288;OPEHF&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#20934;&#30830;&#35780;&#20272;&#20154;&#31867;&#21453;&#39304;&#20449;&#21495;&#12290;&#36825;&#20010;&#26694;&#26550;&#35299;&#20915;&#20102;&#29616;&#26377;OPE&#26041;&#27861;&#22312;&#20272;&#35745;&#20154;&#31867;&#21453;&#39304;&#20449;&#21495;&#19978;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#23545;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#31163;&#32447;&#35757;&#32451;&#21644;&#35780;&#20272;&#20043;&#38388;&#30340;&#24046;&#36317;&#30340;&#32553;&#23567;&#38750;&#24120;&#37325;&#35201;&#65292;&#23427;&#36890;&#36807;&#20165;&#20351;&#29992;&#31163;&#32447;&#36712;&#36857;&#20272;&#35745;&#30446;&#26631;&#65288;&#35780;&#20272;&#65289;&#31574;&#30053;&#30340;&#24615;&#33021;&#21644;/&#25110;&#25490;&#21517;&#12290;&#23427;&#21487;&#20197;&#25552;&#39640;&#25968;&#25454;&#25910;&#38598;&#21644;&#31574;&#30053;&#27979;&#35797;&#36807;&#31243;&#30340;&#23433;&#20840;&#24615;&#21644;&#25928;&#29575;&#65292;&#22312;&#22312;&#32447;&#37096;&#32626;&#25104;&#26412;&#36739;&#39640;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#20013;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;OPE&#26041;&#27861;&#22312;&#20272;&#35745;&#20154;&#31867;&#21453;&#39304;&#65288;HF&#65289;&#20449;&#21495;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#22240;&#20026;HF&#21487;&#33021;&#20250;&#21463;&#21040;&#22810;&#20010;&#28508;&#22312;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#32780;&#19988;&#21482;&#26159;&#31232;&#30095;&#21487;&#29992;&#30340;&#65307;&#32780;&#19981;&#21516;&#20110;&#20195;&#29702;&#23450;&#20041;&#30340;&#29615;&#22659;&#22870;&#21169;&#65288;&#29992;&#20110;&#31574;&#30053;&#20248;&#21270;&#65289;&#65292;&#29615;&#22659;&#22870;&#21169;&#36890;&#24120;&#26159;&#22312;&#21442;&#25968;&#20989;&#25968;&#25110;&#20998;&#24067;&#19978;&#20915;&#23450;&#30340;&#12290;&#22240;&#27492;&#65292;&#30001;&#20110;HF&#20449;&#21495;&#30340;&#24615;&#36136;&#65292;&#20934;&#30830;&#22320;&#25512;&#26029;OPE&#20272;&#35745;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;HF&#30340;OPE&#26694;&#26550;&#65292;&#23427;&#37325;&#26032;&#20351;&#29992;&#29616;&#26377;&#30340;OPE&#26041;&#27861;&#65292;&#20197;&#20934;&#30830;&#35780;&#20272;HF&#20449;&#21495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26041;&#27861;&#26469;
&lt;/p&gt;
&lt;p&gt;
Off-policy evaluation (OPE) is important for closing the gap between offline training and evaluation of reinforcement learning (RL), by estimating performance and/or rank of target (evaluation) policies using offline trajectories only. It can improve the safety and efficiency of data collection and policy testing procedures in situations where online deployments are expensive, such as healthcare. However, existing OPE methods fall short in estimating human feedback (HF) signals, as HF may be conditioned over multiple underlying factors and is only sparsely available; as opposed to the agent-defined environmental rewards (used in policy optimization), which are usually determined over parametric functions or distributions. Consequently, the nature of HF signals makes extrapolating accurate OPE estimations to be challenging. To resolve this, we introduce an OPE for HF (OPEHF) framework that revives existing OPE methods in order to accurately evaluate the HF signals. Specifically, we deve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#30340;&#20223;&#30495;&#20195;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26102;&#38388;&#25193;&#23637;&#21644;&#27880;&#37322;&#36923;&#36753;&#65292;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#21487;&#25193;&#23637;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#39532;&#23572;&#21487;&#22827;&#20551;&#35774;&#31561;&#38382;&#39064;&#12290;&#19982;&#39640;&#20445;&#30495;&#24230;&#30340;&#27169;&#25311;&#22120;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#19977;&#20010;&#25968;&#37327;&#32423;&#30340;&#21516;&#26102;&#20445;&#25345;&#20102;&#31574;&#30053;&#36136;&#37327;&#65292;&#24182;&#19988;&#33021;&#22815;&#24314;&#27169;&#21644;&#21033;&#29992;&#38750;&#39532;&#23572;&#21487;&#22827;&#21160;&#21147;&#23398;&#21644;&#21363;&#26102;&#21160;&#20316;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#20195;&#29702;&#34892;&#20026;&#32467;&#26524;&#30165;&#36857;&#12290;</title><link>http://arxiv.org/abs/2310.06835</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#35821;&#20041;&#38750;&#39532;&#23572;&#21487;&#22827;&#20223;&#30495;&#20195;&#29702;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Scalable Semantic Non-Markovian Simulation Proxy for Reinforcement Learning. (arXiv:2310.06835v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#30340;&#20223;&#30495;&#20195;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26102;&#38388;&#25193;&#23637;&#21644;&#27880;&#37322;&#36923;&#36753;&#65292;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#21487;&#25193;&#23637;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#39532;&#23572;&#21487;&#22827;&#20551;&#35774;&#31561;&#38382;&#39064;&#12290;&#19982;&#39640;&#20445;&#30495;&#24230;&#30340;&#27169;&#25311;&#22120;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#19977;&#20010;&#25968;&#37327;&#32423;&#30340;&#21516;&#26102;&#20445;&#25345;&#20102;&#31574;&#30053;&#36136;&#37327;&#65292;&#24182;&#19988;&#33021;&#22815;&#24314;&#27169;&#21644;&#21033;&#29992;&#38750;&#39532;&#23572;&#21487;&#22827;&#21160;&#21147;&#23398;&#21644;&#21363;&#26102;&#21160;&#20316;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#20195;&#29702;&#34892;&#20026;&#32467;&#26524;&#30165;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#36827;&#23637;&#22312;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#24456;&#22810;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#21487;&#25193;&#23637;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#39532;&#23572;&#21487;&#22827;&#20551;&#35774;&#31561;&#38382;&#39064;&#38480;&#21046;&#20102;&#20854;&#22312;&#26576;&#20123;&#39046;&#22495;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#32570;&#28857;&#22823;&#22810;&#26469;&#33258;&#20110;&#27169;&#25311;&#22120;&#32780;&#19981;&#26159;RL&#35757;&#32451;&#31639;&#27861;&#26412;&#36523;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#27880;&#36923;&#36753;&#30340;&#26102;&#38388;&#25193;&#23637;&#30340;&#35821;&#20041;&#20195;&#29702;&#26469;&#36827;&#34892;&#20223;&#30495;&#12290;&#19982;&#20004;&#20010;&#39640;&#20445;&#30495;&#24230;&#30340;&#27169;&#25311;&#22120;&#30456;&#27604;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19977;&#20010;&#25968;&#37327;&#32423;&#30340;&#21152;&#36895;&#65292;&#24182;&#19988;&#20445;&#25345;&#20102;&#23398;&#20064;&#30340;&#31574;&#30053;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#33021;&#22815;&#24314;&#27169;&#21644;&#21033;&#29992;&#38750;&#39532;&#23572;&#21487;&#22827;&#21160;&#21147;&#23398;&#21644;&#21363;&#26102;&#21160;&#20316;&#65292;&#24182;&#25552;&#20379;&#20102;&#33021;&#22815;&#35299;&#37322;&#20195;&#29702;&#34892;&#20026;&#32467;&#26524;&#30340;&#30165;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in reinforcement learning (RL) have shown much promise across a variety of applications. However, issues such as scalability, explainability, and Markovian assumptions limit its applicability in certain domains. We observe that many of these shortcomings emanate from the simulator as opposed to the RL training algorithms themselves. As such, we propose a semantic proxy for simulation based on a temporal extension to annotated logic. In comparison with two high-fidelity simulators, we show up to three orders of magnitude speed-up while preserving the quality of policy learned. In addition, we show the ability to model and leverage non-Markovian dynamics and instantaneous actions while providing an explainable trace describing the outcomes of the agent actions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#36716;&#21270;&#20026;&#23567;&#22411;&#21512;&#25104;&#26679;&#26412;&#65292;&#20197;&#25903;&#25345;&#39640;&#25928;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;&#27169;&#22411;&#23545;&#21512;&#25104;&#26679;&#26412;&#30340;&#34920;&#31034;&#21644;&#21487;&#23398;&#20064;&#30446;&#26631;&#29305;&#24449;&#34920;&#31034;&#20043;&#38388;&#30340;&#22343;&#26041;&#35823;&#24046;&#65292;&#35299;&#20915;&#20102;&#21512;&#25104;&#26679;&#26412;&#26799;&#24230;&#20559;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.06511</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#25968;&#25454;&#38598;&#33976;&#39311;&#29992;&#20110;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Dataset Distillation for Transfer Learning. (arXiv:2310.06511v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#36716;&#21270;&#20026;&#23567;&#22411;&#21512;&#25104;&#26679;&#26412;&#65292;&#20197;&#25903;&#25345;&#39640;&#25928;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;&#27169;&#22411;&#23545;&#21512;&#25104;&#26679;&#26412;&#30340;&#34920;&#31034;&#21644;&#21487;&#23398;&#20064;&#30446;&#26631;&#29305;&#24449;&#34920;&#31034;&#20043;&#38388;&#30340;&#22343;&#26041;&#35823;&#24046;&#65292;&#35299;&#20915;&#20102;&#21512;&#25104;&#26679;&#26412;&#26799;&#24230;&#20559;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#22312;&#23558;&#22823;&#22411;&#25968;&#25454;&#38598;&#36716;&#21270;&#20026;&#23569;&#37327;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#26679;&#26412;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24182;&#19981;&#34987;&#35774;&#35745;&#29992;&#20110;&#20135;&#29983;&#19968;&#20010;&#36866;&#29992;&#20110;&#20419;&#36827;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#33976;&#39311;&#25968;&#25454;&#38598;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#33976;&#39311;&#20026;&#19968;&#32452;&#23567;&#22411;&#21512;&#25104;&#26679;&#26412;&#20197;&#29992;&#20110;&#39640;&#25928;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#30340;&#26032;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#22312;&#26420;&#32032;&#21452;&#23618;&#20248;&#21270;&#20013;&#65292;&#21512;&#25104;&#26679;&#26412;&#30456;&#23545;&#20110;&#33258;&#30417;&#30563;&#30446;&#26631;&#30340;&#26799;&#24230;&#26159;&#8220;&#26377;&#20559;&#8221;&#30340;&#65292;&#36825;&#26159;&#30001;&#20110;&#25968;&#25454;&#22686;&#24378;&#25110;&#36974;&#34109;&#24341;&#36215;&#30340;&#38543;&#26426;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#23567;&#21270;&#27169;&#22411;&#23545;&#21512;&#25104;&#26679;&#26412;&#30340;&#34920;&#31034;&#21644;&#30456;&#24212;&#30340;&#21487;&#23398;&#20064;&#30446;&#26631;&#29305;&#24449;&#34920;&#31034;&#20043;&#38388;&#30340;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#20316;&#20026;&#20869;&#37096;&#30446;&#26631;&#65292;&#36825;&#19981;&#24341;&#20837;&#20219;&#20309;&#38543;&#26426;&#24615;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21160;&#26426;&#26159;&#36890;&#36807;&#25552;&#20986;&#30340;&#20869;&#37096;&#20248;&#21270;&#33719;&#24471;&#30340;&#27169;&#22411;&#21487;&#20197;&#27169;&#20223;...
&lt;/p&gt;
&lt;p&gt;
Dataset distillation methods have achieved remarkable success in distilling a large dataset into a small set of representative samples. However, they are not designed to produce a distilled dataset that can be effectively used for facilitating self-supervised pre-training. To this end, we propose a novel problem of distilling an unlabeled dataset into a set of small synthetic samples for efficient self-supervised learning (SSL). We first prove that a gradient of synthetic samples with respect to a SSL objective in naive bilevel optimization is \textit{biased} due to the randomness originating from data augmentations or masking. To address this issue, we propose to minimize the mean squared error (MSE) between a model's representations of the synthetic examples and their corresponding learnable target feature representations for the inner objective, which does not introduce any randomness. Our primary motivation is that the model obtained by the proposed inner optimization can mimic the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20462;&#27491;&#20998;&#31867;&#27169;&#22411;&#30340;&#35268;&#21017;&#25366;&#25496;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#19981;&#20934;&#30830;&#23376;&#38598;&#21644;&#23545;&#20854;&#36827;&#34892;&#20462;&#27491;&#30340;&#35268;&#21017;&#21015;&#34920;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06446</link><description>&lt;p&gt;
&#29992;&#20110;&#20462;&#27491;&#20998;&#31867;&#27169;&#22411;&#30340;&#35268;&#21017;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Rule Mining for Correcting Classification Models. (arXiv:2310.06446v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20462;&#27491;&#20998;&#31867;&#27169;&#22411;&#30340;&#35268;&#21017;&#25366;&#25496;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#19981;&#20934;&#30830;&#23376;&#38598;&#21644;&#23545;&#20854;&#36827;&#34892;&#20462;&#27491;&#30340;&#35268;&#21017;&#21015;&#34920;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#19981;&#26029;&#26356;&#26032;&#25110;&#20462;&#27491;&#65292;&#20197;&#30830;&#20445;&#39044;&#27979;&#20934;&#30830;&#24615;&#22987;&#32456;&#20445;&#25345;&#39640;&#27700;&#24179;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#27169;&#22411;&#20462;&#27491;&#21487;&#33021;&#25913;&#21464;&#39044;&#27979;&#32467;&#26524;&#30340;&#22330;&#26223;&#65292;&#20363;&#22914;&#27169;&#22411;&#26159;&#22797;&#26434;&#31995;&#32479;&#25110;&#36719;&#20214;&#30340;&#19968;&#37096;&#20998;&#12290;&#22312;&#36825;&#31181;&#22330;&#26223;&#20013;&#65292;&#24320;&#21457;&#20154;&#21592;&#24076;&#26395;&#33021;&#22815;&#25511;&#21046;&#20462;&#27491;&#30340;&#35268;&#33539;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#24320;&#21457;&#20154;&#21592;&#38656;&#35201;&#20102;&#35299;&#21738;&#20123;&#36755;&#20837;&#30340;&#23376;&#38598;&#20250;&#23548;&#33268;&#27169;&#22411;&#30340;&#39044;&#27979;&#19981;&#20934;&#30830;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20462;&#27491;&#35268;&#21017;&#25366;&#25496;&#26041;&#27861;&#65292;&#20197;&#33719;&#21462;&#25551;&#36848;&#19981;&#20934;&#30830;&#23376;&#38598;&#21644;&#22914;&#20309;&#36827;&#34892;&#20462;&#27491;&#30340;&#20840;&#38754;&#35268;&#21017;&#21015;&#34920;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20462;&#27491;&#35268;&#21017;&#25366;&#25496;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#32467;&#21512;&#20102;&#39057;&#32321;&#39033;&#38598;&#25366;&#25496;&#21644;&#29420;&#29305;&#30340;&#20462;&#27491;&#35268;&#21017;&#20462;&#21098;&#25216;&#26415;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#35813;&#31639;&#27861;&#25214;&#21040;&#20102;&#21508;&#31181;&#35268;&#21017;&#65292;&#26377;&#21161;&#20110;&#25910;&#38598;&#19981;&#20805;&#20998;&#23398;&#20064;&#30340;&#25968;&#25454;&#65292;&#30452;&#25509;&#20462;&#27491;&#27169;&#22411;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models need to be continually updated or corrected to ensure that the prediction accuracy remains consistently high. In this study, we consider scenarios where developers should be careful to change the prediction results by the model correction, such as when the model is part of a complex system or software. In such scenarios, the developers want to control the specification of the corrections. To achieve this, the developers need to understand which subpopulations of the inputs get inaccurate predictions by the model. Therefore, we propose correction rule mining to acquire a comprehensive list of rules that describe inaccurate subpopulations and how to correct them. We also develop an efficient correction rule mining algorithm that is a combination of frequent itemset mining and a unique pruning technique for correction rules. We observed that the proposed algorithm found various rules which help to collect data insufficiently learned, directly correct model outputs,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DiffuSeq-v2&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#31163;&#25955;&#21644;&#36830;&#32493;&#25991;&#26412;&#31354;&#38388;&#36830;&#25509;&#36215;&#26469;&#23454;&#29616;&#20102;Seq2Seq&#25193;&#25955;&#27169;&#22411;&#30340;&#21152;&#36895;&#12290;&#22312;&#35757;&#32451;&#20013;&#24341;&#20837;&#36719;&#21560;&#25910;&#24577;&#65292;&#25552;&#39640;&#31163;&#25955;&#31361;&#21464;&#37325;&#26500;&#33021;&#21147;&#65307;&#22312;&#37319;&#26679;&#38454;&#27573;&#20351;&#29992;ODE&#27714;&#35299;&#22120;&#21152;&#24555;&#37319;&#26679;&#36895;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35757;&#32451;&#25910;&#25947;&#36895;&#24230;&#25552;&#39640;4&#20493;&#65292;&#29983;&#25104;&#26679;&#26412;&#36895;&#24230;&#25552;&#39640;800&#20493;&#65292;&#26356;&#36866;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.05793</link><description>&lt;p&gt;
DiffuSeq-v2&#65306;&#23558;&#31163;&#25955;&#21644;&#36830;&#32493;&#25991;&#26412;&#31354;&#38388;&#36830;&#25509;&#36215;&#26469;&#20197;&#21152;&#36895;Seq2Seq&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DiffuSeq-v2: Bridging Discrete and Continuous Text Spaces for Accelerated Seq2Seq Diffusion Models. (arXiv:2310.05793v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DiffuSeq-v2&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#31163;&#25955;&#21644;&#36830;&#32493;&#25991;&#26412;&#31354;&#38388;&#36830;&#25509;&#36215;&#26469;&#23454;&#29616;&#20102;Seq2Seq&#25193;&#25955;&#27169;&#22411;&#30340;&#21152;&#36895;&#12290;&#22312;&#35757;&#32451;&#20013;&#24341;&#20837;&#36719;&#21560;&#25910;&#24577;&#65292;&#25552;&#39640;&#31163;&#25955;&#31361;&#21464;&#37325;&#26500;&#33021;&#21147;&#65307;&#22312;&#37319;&#26679;&#38454;&#27573;&#20351;&#29992;ODE&#27714;&#35299;&#22120;&#21152;&#24555;&#37319;&#26679;&#36895;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35757;&#32451;&#25910;&#25947;&#36895;&#24230;&#25552;&#39640;4&#20493;&#65292;&#29983;&#25104;&#26679;&#26412;&#36895;&#24230;&#25552;&#39640;800&#20493;&#65292;&#26356;&#36866;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#24207;&#21015;&#26041;&#38754;&#26159;&#24456;&#26377;&#28508;&#21147;&#30340;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#26159;&#21033;&#29992;&#36830;&#32493;&#30340;&#25193;&#25955;&#31354;&#38388;&#34920;&#31034;&#31163;&#25955;&#25991;&#26412;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20135;&#29983;&#20102;&#22823;&#37327;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#23548;&#33268;&#37319;&#26679;&#36895;&#24230;&#21464;&#24930;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#36719;&#21560;&#25910;&#24577;&#65292;&#24110;&#21161;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#22522;&#20110;&#24213;&#23618;&#39640;&#26031;&#31354;&#38388;&#30340;&#31163;&#25955;&#31361;&#21464;&#37325;&#26500;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#24674;&#22797;&#26465;&#20214;&#20449;&#21495;&#30340;&#33021;&#21147;&#12290;&#22312;&#37319;&#26679;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#36830;&#32493;&#31354;&#38388;ODE&#27714;&#35299;&#22120;&#26469;&#21152;&#24555;&#37319;&#26679;&#36807;&#31243;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#23558;&#35757;&#32451;&#25910;&#25947;&#36895;&#24230;&#25552;&#39640;&#20102;4&#20493;&#65292;&#24182;&#20197;800&#20493;&#30340;&#36895;&#24230;&#29983;&#25104;&#30456;&#36817;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#20351;&#20854;&#26356;&#25509;&#36817;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have gained prominence in generating high-quality sequences of text. Nevertheless, current approaches predominantly represent discrete text within a continuous diffusion space, which incurs substantial computational overhead during training and results in slower sampling speeds. In this paper, we introduce a soft absorbing state that facilitates the diffusion model in learning to reconstruct discrete mutations based on the underlying Gaussian space, thereby enhancing its capacity to recover conditional signals. During the sampling phase, we employ state-of-the-art ODE solvers within the continuous space to expedite the sampling process. Comprehensive experimental evaluations reveal that our proposed method effectively accelerates the training convergence by 4x and generates samples of similar quality 800x faster, rendering it significantly closer to practical application. \footnote{The code is released at \url{https://github.com/Shark-NLP/DiffuSeq}
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#26469;&#34701;&#21512;&#22522;&#20110;Transformer&#30340;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23545;&#40784;&#21508;&#31181;&#26550;&#26500;&#32452;&#20214;&#24182;&#20801;&#35768;&#19981;&#21516;&#22823;&#23567;&#30340;&#27169;&#22411;&#30340;&#34701;&#21512;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#21387;&#32553;Transformer&#30340;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2310.05719</link><description>&lt;p&gt;
&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#22120;&#21512;&#24182;Transformer
&lt;/p&gt;
&lt;p&gt;
Transformer Fusion with Optimal Transport. (arXiv:2310.05719v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#26469;&#34701;&#21512;&#22522;&#20110;Transformer&#30340;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23545;&#40784;&#21508;&#31181;&#26550;&#26500;&#32452;&#20214;&#24182;&#20801;&#35768;&#19981;&#21516;&#22823;&#23567;&#30340;&#27169;&#22411;&#30340;&#34701;&#21512;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#21387;&#32553;Transformer&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34701;&#21512;&#26159;&#19968;&#31181;&#23558;&#22810;&#20010;&#29420;&#31435;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#21512;&#24182;&#20197;&#32467;&#21512;&#23427;&#20204;&#30340;&#33021;&#21147;&#30340;&#25216;&#26415;&#12290;&#36807;&#21435;&#30340;&#23581;&#35797;&#20165;&#38480;&#20110;&#20840;&#36830;&#25509;&#12289;&#21367;&#31215;&#21644;&#27531;&#24046;&#32593;&#32476;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26368;&#20248;&#36755;&#36816;&#26469;&#34701;&#21512;&#20004;&#20010;&#25110;&#22810;&#20010;&#22522;&#20110;Transformer&#30340;&#32593;&#32476;&#65292;&#20197;&#65288;&#36719;&#65289;&#23545;&#40784;&#21508;&#31181;&#26550;&#26500;&#32452;&#20214;&#12290;&#25105;&#20204;&#35814;&#32454;&#25551;&#36848;&#20102;&#19968;&#31181;&#23618;&#23545;&#40784;&#30340;&#25277;&#35937;&#26041;&#27861;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#20219;&#24847;&#26550;&#26500;&#65292;&#20363;&#22914;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#12289;&#23618;&#24402;&#19968;&#21270;&#21644;&#27531;&#24046;&#36830;&#25509;&#12290;&#25105;&#20204;&#36890;&#36807;&#21508;&#31181;&#28040;&#34701;&#30740;&#31350;&#35752;&#35770;&#20102;&#22914;&#20309;&#22788;&#29702;&#36825;&#20123;&#26550;&#26500;&#32452;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#19981;&#21516;&#22823;&#23567;&#30340;&#27169;&#22411;&#36827;&#34892;&#34701;&#21512;&#65288;&#24322;&#26500;&#34701;&#21512;&#65289;&#65292;&#20026;Transformer&#30340;&#21387;&#32553;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;Vision Transformer&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20197;&#21450;&#33258;&#28982;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Fusion is a technique for merging multiple independently-trained neural networks in order to combine their capabilities. Past attempts have been restricted to the case of fully-connected, convolutional, and residual networks. In this paper, we present a systematic approach for fusing two or more transformer-based networks exploiting Optimal Transport to (soft-)align the various architectural components. We flesh out an abstraction for layer alignment, that can generalize to arbitrary architectures -- in principle -and we apply this to the key ingredients of Transformers such as multi-head self-attention, layer-normalization, and residual connections, and we discuss how to handle them via various ablation studies. Furthermore, our method allows the fusion of models of different sizes (heterogeneous fusion), providing a new and efficient way for compression of Transformers. The proposed approach is evaluated on both image classification tasks via Vision Transformer and natural language
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#23558;&#20445;&#30041;&#26426;&#21046;&#25972;&#21512;&#21040;&#32467;&#30452;&#32928;&#24687;&#32905;&#20998;&#21106;&#20013;&#65292;&#35299;&#20915;&#20102;&#35270;&#35273;&#21464;&#25442;&#22120;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#23454;&#26102;&#30142;&#30149;&#26816;&#27979;&#20013;&#30340;&#20869;&#23384;&#21644;&#24182;&#34892;&#24615;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.05446</link><description>&lt;p&gt;
RetSeg: &#22522;&#20110;&#20445;&#30041;&#26426;&#21046;&#30340;&#32467;&#30452;&#32928;&#24687;&#32905;&#20998;&#21106;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
RetSeg: Retention-based Colorectal Polyps Segmentation Network. (arXiv:2310.05446v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#23558;&#20445;&#30041;&#26426;&#21046;&#25972;&#21512;&#21040;&#32467;&#30452;&#32928;&#24687;&#32905;&#20998;&#21106;&#20013;&#65292;&#35299;&#20915;&#20102;&#35270;&#35273;&#21464;&#25442;&#22120;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#23454;&#26102;&#30142;&#30149;&#26816;&#27979;&#20013;&#30340;&#20869;&#23384;&#21644;&#24182;&#34892;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViTs&#65289;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#19982;&#20256;&#32479;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#30456;&#27604;&#65292;&#22312;&#24687;&#32905;&#20998;&#31867;&#12289;&#26816;&#27979;&#21644;&#20998;&#21106;&#31561;&#20851;&#38190;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#26356;&#39640;&#30340;&#25928;&#33021;&#12290;&#36890;&#36807;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#32858;&#28966;&#20110;&#29305;&#23450;&#22270;&#20687;&#21306;&#22495;&#65292;ViTs&#22312;&#22788;&#29702;&#35270;&#35273;&#25968;&#25454;&#26102;&#34920;&#29616;&#20986;&#19978;&#19979;&#25991;&#24863;&#30693;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#22797;&#26434;&#21307;&#23398;&#22270;&#20687;&#26102;&#23454;&#29616;&#20102;&#24378;&#22823;&#19988;&#31934;&#30830;&#30340;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#21464;&#25442;&#22120;&#20013;&#22266;&#26377;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#36866;&#24212;&#20102;&#19981;&#21516;&#30340;&#36755;&#20837;&#23610;&#23544;&#21644;&#20998;&#36776;&#29575;&#65292;&#20026;&#20256;&#32479;&#30340;CNNs&#25152;&#19981;&#20855;&#22791;&#30340;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#28789;&#27963;&#24615;&#12290;&#28982;&#32780;&#65292;&#21464;&#25442;&#22120;&#30001;&#20110;&#33258;&#27880;&#24847;&#26426;&#21046;&#32780;&#38754;&#20020;&#30528;&#36807;&#22810;&#30340;&#20869;&#23384;&#20351;&#29992;&#21644;&#26377;&#38480;&#30340;&#35757;&#32451;&#24182;&#34892;&#24615;&#31561;&#25361;&#25112;&#65292;&#20174;&#32780;&#20351;&#20854;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#23454;&#26102;&#30142;&#30149;&#26816;&#27979;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25506;&#31350;&#23558;&#26368;&#36817;&#24341;&#20837;&#30340;&#20445;&#30041;&#26426;&#21046;&#25972;&#21512;&#21040;&#24687;&#32905;&#20998;&#21106;&#20013;&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers (ViTs) have revolutionized medical imaging analysis, showcasing superior efficacy compared to conventional Convolutional Neural Networks (CNNs) in vital tasks such as polyp classification, detection, and segmentation. Leveraging attention mechanisms to focus on specific image regions, ViTs exhibit contextual awareness in processing visual data, culminating in robust and precise predictions, even for intricate medical images. Moreover, the inherent self-attention mechanism in Transformers accommodates varying input sizes and resolutions, granting an unprecedented flexibility absent in traditional CNNs. However, Transformers grapple with challenges like excessive memory usage and limited training parallelism due to self-attention, rendering them impractical for real-time disease detection on resource-constrained devices. In this study, we address these hurdles by investigating the integration of the recently introduced retention mechanism into polyp segmentation, intr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LaSyn&#30340;&#25991;&#26412;&#25968;&#25454;&#21033;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#25968;&#25454;&#36716;&#25442;&#20026;&#20013;&#38388;&#28508;&#21464;&#34920;&#31034;&#26469;&#22686;&#24378;&#31471;&#21040;&#31471;&#35821;&#38899;&#22788;&#29702;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#30340;&#35821;&#38899;&#35782;&#21035;&#21644;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#20013;&#65292;LaSyn&#30456;&#23545;&#35789;&#38169;&#35823;&#29575;&#20943;&#23569;&#20102;22.3%&#65292;&#32477;&#23545;&#24847;&#22270;&#20998;&#31867;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;4.1%&#12290;</title><link>http://arxiv.org/abs/2310.05374</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#25991;&#26412;&#25968;&#25454;&#21512;&#25104;&#25552;&#39640;&#31471;&#21040;&#31471;&#35821;&#38899;&#22788;&#29702;&#30340;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Improving End-to-End Speech Processing by Efficient Text Data Utilization with Latent Synthesis. (arXiv:2310.05374v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LaSyn&#30340;&#25991;&#26412;&#25968;&#25454;&#21033;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#25968;&#25454;&#36716;&#25442;&#20026;&#20013;&#38388;&#28508;&#21464;&#34920;&#31034;&#26469;&#22686;&#24378;&#31471;&#21040;&#31471;&#35821;&#38899;&#22788;&#29702;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#30340;&#35821;&#38899;&#35782;&#21035;&#21644;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#20013;&#65292;LaSyn&#30456;&#23545;&#35789;&#38169;&#35823;&#29575;&#20943;&#23569;&#20102;22.3%&#65292;&#32477;&#23545;&#24847;&#22270;&#20998;&#31867;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;4.1%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#65292;&#22521;&#35757;&#39640;&#24615;&#33021;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#22788;&#29702;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#30340;&#35821;&#38899;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#19982;&#25991;&#26412;&#25968;&#25454;&#30456;&#27604;&#65292;&#26631;&#35760;&#30340;&#35821;&#38899;&#25968;&#25454;&#36890;&#24120;&#26356;&#21152;&#31232;&#32570;&#21644;&#26114;&#36149;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LaSyn&#30340;&#26377;&#25928;&#30340;&#25991;&#26412;&#25968;&#25454;&#21033;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;&#35821;&#38899;&#22788;&#29702;&#27169;&#22411;&#12290;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#28508;&#21464;&#21512;&#25104;&#22120;&#23558;&#25991;&#26412;&#25968;&#25454;&#36716;&#25442;&#20026;&#39044;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;&#30340;&#20013;&#38388;&#28508;&#21464;&#34920;&#31034;&#12290;&#36825;&#20123;&#20266;&#22768;&#23398;&#34920;&#31034;&#29992;&#20110;&#22686;&#24378;&#27169;&#22411;&#35757;&#32451;&#30340;&#22768;&#23398;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#20302;&#36164;&#28304;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#21475;&#35821;&#29702;&#35299;&#65288;SLU&#65289;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;LaSyn&#12290;&#23545;&#20110;ASR&#65292;LaSyn&#25913;&#36827;&#20102;&#22312;LibriSpeech train-clean-100&#19978;&#35757;&#32451;&#30340;E2E&#22522;&#32447;&#65292;&#22312;&#19981;&#21516;&#30340;&#27979;&#35797;&#38598;&#19978;&#30456;&#23545;&#35789;&#38169;&#35823;&#29575;&#20943;&#23569;&#20102;22.3%&#12290;&#23545;&#20110;SLU&#65292;LaSyn&#25913;&#36827;&#20102;&#25105;&#20204;&#30340;E2E&#22522;&#32447;&#65292;&#32477;&#23545;&#24847;&#22270;&#20998;&#31867;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;4.1%&#12290;
&lt;/p&gt;
&lt;p&gt;
Training a high performance end-to-end speech (E2E) processing model requires an enormous amount of labeled speech data, especially in the era of data-centric artificial intelligence. However, labeled speech data are usually scarcer and more expensive for collection, compared to textual data. We propose Latent Synthesis (LaSyn), an efficient textual data utilization framework for E2E speech processing models. We train a latent synthesizer to convert textual data into an intermediate latent representation of a pre-trained speech model. These pseudo acoustic representations of textual data augment acoustic data for model training. We evaluate LaSyn on low-resource automatic speech recognition (ASR) and spoken language understanding (SLU) tasks. For ASR, LaSyn improves an E2E baseline trained on LibriSpeech train-clean-100, with relative word error rate reductions over 22.3% on different test sets. For SLU, LaSyn improves our E2E baseline by absolute 4.1% for intent classification accurac
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#31070;&#32463;&#23849;&#28291;&#27010;&#24565;&#25193;&#23637;&#21040;&#31867;&#21035;&#25968;&#36828;&#22823;&#20110;&#29305;&#24449;&#31354;&#38388;&#32500;&#24230;&#30340;&#24773;&#20917;&#65292;&#24182;&#23637;&#31034;&#20102;&#24191;&#20041;&#31070;&#32463;&#23849;&#28291;&#29616;&#35937;&#30340;&#26368;&#23567;&#36793;&#30028;&#20540;&#34987;&#26368;&#22823;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.05351</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#31867;&#21035;&#19979;&#30340;&#24191;&#20041;&#31070;&#32463;&#23849;&#28291;
&lt;/p&gt;
&lt;p&gt;
Generalized Neural Collapse for a Large Number of Classes. (arXiv:2310.05351v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#31070;&#32463;&#23849;&#28291;&#27010;&#24565;&#25193;&#23637;&#21040;&#31867;&#21035;&#25968;&#36828;&#22823;&#20110;&#29305;&#24449;&#31354;&#38388;&#32500;&#24230;&#30340;&#24773;&#20917;&#65292;&#24182;&#23637;&#31034;&#20102;&#24191;&#20041;&#31070;&#32463;&#23849;&#28291;&#29616;&#35937;&#30340;&#26368;&#23567;&#36793;&#30028;&#20540;&#34987;&#26368;&#22823;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#23849;&#28291;&#25552;&#20379;&#20102;&#28145;&#24230;&#20998;&#31867;&#27169;&#22411;&#20013;&#23398;&#20064;&#30340;&#26368;&#21518;&#19968;&#23618;&#34920;&#31034;&#65288;&#21363;&#29305;&#24449;&#65289;&#21644;&#20998;&#31867;&#22120;&#26435;&#37325;&#30340;&#20248;&#38597;&#25968;&#23398;&#25551;&#36848;&#12290;&#36825;&#31181;&#32467;&#26524;&#19981;&#20165;&#25552;&#20379;&#20102;&#27934;&#23519;&#21147;&#65292;&#36824;&#28608;&#21457;&#20102;&#25913;&#36827;&#23454;&#38469;&#28145;&#24230;&#27169;&#22411;&#30340;&#26032;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20851;&#20110;&#31070;&#32463;&#23849;&#28291;&#30340;&#29616;&#26377;&#32463;&#39564;&#21644;&#29702;&#35770;&#30740;&#31350;&#37117;&#38598;&#20013;&#20110;&#31867;&#21035;&#25968;&#30456;&#23545;&#20110;&#29305;&#24449;&#31354;&#38388;&#32500;&#24230;&#36739;&#23567;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#23558;&#31070;&#32463;&#23849;&#28291;&#25193;&#23637;&#21040;&#31867;&#21035;&#25968;&#36828;&#22823;&#20110;&#29305;&#24449;&#31354;&#38388;&#32500;&#24230;&#30340;&#24773;&#20917;&#65292;&#36825;&#22312;&#35821;&#35328;&#27169;&#22411;&#12289;&#26816;&#32034;&#31995;&#32479;&#21644;&#20154;&#33080;&#35782;&#21035;&#24212;&#29992;&#20013;&#24191;&#27867;&#20986;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29305;&#24449;&#21644;&#20998;&#31867;&#22120;&#23637;&#29616;&#20986;&#20102;&#24191;&#20041;&#31070;&#32463;&#23849;&#28291;&#29616;&#35937;&#65292;&#20854;&#20013;&#26368;&#23567;&#30340;&#19968;&#23545;&#20854;&#20182;&#31867;&#21035;&#38388;&#36793;&#30028;&#20540;&#34987;&#26368;&#22823;&#21270;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#20197;&#39564;&#35777;&#23454;&#38469;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#24191;&#20041;&#31070;&#32463;&#23849;&#28291;&#30340;&#21457;&#29983;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#30740;&#31350;&#65292;&#20197;&#34920;&#26126;&#8230;.
&lt;/p&gt;
&lt;p&gt;
Neural collapse provides an elegant mathematical characterization of learned last layer representations (a.k.a. features) and classifier weights in deep classification models. Such results not only provide insights but also motivate new techniques for improving practical deep models. However, most of the existing empirical and theoretical studies in neural collapse focus on the case that the number of classes is small relative to the dimension of the feature space. This paper extends neural collapse to cases where the number of classes are much larger than the dimension of feature space, which broadly occur for language models, retrieval systems, and face recognition applications. We show that the features and classifier exhibit a generalized neural collapse phenomenon, where the minimum one-vs-rest margins is maximized.We provide empirical study to verify the occurrence of generalized neural collapse in practical deep neural networks. Moreover, we provide theoretical study to show tha
&lt;/p&gt;</description></item><item><title>&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#23433;&#20840;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#20113;&#22522;&#30784;&#35774;&#26045;&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#38544;&#31169;&#23433;&#20840;&#21644;&#32463;&#27982;&#26377;&#25928;&#30340;&#36890;&#20449;&#12290;&#23427;&#36890;&#36807;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#24182;&#23558;&#32467;&#26524;&#19978;&#20256;&#21040;&#20113;&#31471;&#36827;&#34892;&#25972;&#20307;&#27169;&#22411;&#32858;&#21512;&#65292;&#36991;&#20813;&#20102;&#30452;&#25509;&#26292;&#38706;&#21407;&#22987;&#25968;&#25454;&#65292;&#20855;&#26377;&#39640;&#25928;&#12289;&#21487;&#20280;&#32553;&#21644;&#20445;&#25252;&#38544;&#31169;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.05269</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65306;&#26368;&#26032;&#36827;&#23637;&#21644;&#24212;&#29992;&#30340;&#21069;&#27839;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Federated Learning: A Cutting-Edge Survey of the Latest Advancements and Applications. (arXiv:2310.05269v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05269
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#23433;&#20840;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#20113;&#22522;&#30784;&#35774;&#26045;&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#38544;&#31169;&#23433;&#20840;&#21644;&#32463;&#27982;&#26377;&#25928;&#30340;&#36890;&#20449;&#12290;&#23427;&#36890;&#36807;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#24182;&#23558;&#32467;&#26524;&#19978;&#20256;&#21040;&#20113;&#31471;&#36827;&#34892;&#25972;&#20307;&#27169;&#22411;&#32858;&#21512;&#65292;&#36991;&#20813;&#20102;&#30452;&#25509;&#26292;&#38706;&#21407;&#22987;&#25968;&#25454;&#65292;&#20855;&#26377;&#39640;&#25928;&#12289;&#21487;&#20280;&#32553;&#21644;&#20445;&#25252;&#38544;&#31169;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#23458;&#25143;&#21644;&#20027;&#26426;&#36830;&#25509;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31995;&#32479;&#39046;&#22495;&#65292;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#23433;&#20840;&#20998;&#24067;&#24335;ML&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#38544;&#31169;&#23433;&#20840;&#24615;&#12290;FL&#36890;&#36807;&#20351;&#29992;&#21306;&#22359;&#38142;&#25216;&#26415;&#23558;ML&#27169;&#22411;&#20256;&#36755;&#21040;&#36793;&#32536;&#26381;&#21153;&#22120;&#19978;&#65292;&#26377;&#25928;&#22320;&#23558;&#20113;&#22522;&#30784;&#35774;&#26045;&#19982;&#20013;&#24515;&#21270;&#21644;&#20998;&#25955;&#21270;&#31995;&#32479;&#30340;&#27969;&#31243;&#22788;&#29702;&#21644;&#25968;&#25454;&#23384;&#20648;&#38656;&#27714;&#30456;&#32467;&#21512;&#65292;&#27880;&#37325;&#21487;&#20280;&#32553;&#24615;&#12289;&#38544;&#31169;&#32771;&#34385;&#21644;&#32463;&#27982;&#39640;&#25928;&#30340;&#36890;&#20449;&#12290;&#22312;&#24403;&#21069;&#30340;FL&#23454;&#29616;&#20013;&#65292;&#25968;&#25454;&#25152;&#26377;&#32773;&#22312;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#65292;&#28982;&#21518;&#20197;&#26435;&#37325;&#12289;&#26799;&#24230;&#21644;&#21442;&#25968;&#30340;&#24418;&#24335;&#23558;&#32467;&#26524;&#19978;&#20256;&#21040;&#20113;&#31471;&#36827;&#34892;&#25972;&#20307;&#27169;&#22411;&#32858;&#21512;&#12290;&#36825;&#31181;&#21019;&#26032;&#28040;&#38500;&#20102;&#19982;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#23458;&#25143;&#21644;&#21442;&#19982;&#32773;&#30452;&#25509;&#19982;&#20113;&#20013;&#24515;&#36890;&#20449;&#21407;&#22987;&#21644;&#28508;&#22312;&#26426;&#23494;&#25968;&#25454;&#30340;&#24517;&#35201;&#24615;&#12290;&#36825;&#19981;&#20165;&#38477;&#20302;&#20102;&#19982;&#19982;&#20256;&#32479;&#20113;&#20013;&#24515;&#36890;&#20449;&#30456;&#20851;&#30340;&#36153;&#29992;&#65292;&#36824;&#20445;&#25252;&#20102;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of machine learning (ML) systems featuring client-host connections, the enhancement of privacy security can be effectively achieved through federated learning (FL) as a secure distributed ML methodology. FL effectively integrates cloud infrastructure to transfer ML models onto edge servers using blockchain technology. Through this mechanism, it guarantees the streamlined processing and data storage requirements of both centralized and decentralized systems, with an emphasis on scalability, privacy considerations, and cost-effective communication. In current FL implementations, data owners locally train their models, and subsequently upload the outcomes in the form of weights, gradients, and parameters to the cloud for overall model aggregation. This innovation obviates the necessity of engaging Internet of Things (IoT) clients and participants to communicate raw and potentially confidential data directly with a cloud center. This not only reduces the costs associated with 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#24863;&#30693;&#32852;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;HJCL&#65289;&#65292;&#29992;&#20110;&#23618;&#27425;&#21270;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#23454;&#20363;&#32423;&#21644;&#26631;&#31614;&#32423;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#65292;&#20197;&#21450;&#31934;&#24515;&#26500;&#24314;&#25209;&#27425;&#26469;&#22788;&#29702;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#22312;HMTC&#20013;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.05128</link><description>&lt;p&gt;
&#23454;&#20363;&#21644;&#26631;&#31614;: &#38024;&#23545;&#23618;&#27425;&#21270;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#30340;&#23618;&#27425;&#24863;&#30693;&#32852;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Instances and Labels: Hierarchy-aware Joint Supervised Contrastive Learning for Hierarchical Multi-Label Text Classification. (arXiv:2310.05128v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05128
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#24863;&#30693;&#32852;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;HJCL&#65289;&#65292;&#29992;&#20110;&#23618;&#27425;&#21270;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#23454;&#20363;&#32423;&#21644;&#26631;&#31614;&#32423;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#65292;&#20197;&#21450;&#31934;&#24515;&#26500;&#24314;&#25209;&#27425;&#26469;&#22788;&#29702;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#22312;HMTC&#20013;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#27425;&#21270;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#65288;HMTC&#65289;&#26088;&#22312;&#21033;&#29992;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#36827;&#34892;&#22810;&#26631;&#31614;&#20998;&#31867;&#12290;&#36817;&#26399;&#20851;&#20110;HMTC&#30340;&#26041;&#27861;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#22312;&#29983;&#25104;&#30340;&#26679;&#26412;&#19978;&#20197;&#21322;&#30417;&#30563;&#30340;&#26041;&#24335;&#23558;&#25991;&#26412;&#21644;&#26631;&#31614;&#23884;&#20837;&#25509;&#36817;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#23545;&#36755;&#20986;&#31354;&#38388;&#26045;&#21152;&#36807;&#24230;&#32422;&#26463;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#26679;&#26412;&#30340;&#29983;&#25104;&#24448;&#24448;&#24341;&#20837;&#22122;&#22768;&#65292;&#22240;&#20026;&#23427;&#24573;&#30053;&#20102;&#21516;&#19968;&#25209;&#27425;&#20013;&#30456;&#20284;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#26041;&#27861;&#26159;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65292;&#20294;&#30001;&#20110;&#20854;&#22797;&#26434;&#30340;&#32467;&#26500;&#21270;&#26631;&#31614;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;$\textbf{HJCL}$&#30340;&#23618;&#27425;&#24863;&#30693;&#32852;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22635;&#34917;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#21644;HMTC&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#23454;&#20363;&#32423;&#21644;&#26631;&#31614;&#32423;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#20180;&#32454;&#26500;&#36896;&#25209;&#27425;&#26469;&#28385;&#36275;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical multi-label text classification (HMTC) aims at utilizing a label hierarchy in multi-label classification. Recent approaches to HMTC deal with the problem of imposing an over-constrained premise on the output space by using contrastive learning on generated samples in a semi-supervised manner to bring text and label embeddings closer. However, the generation of samples tends to introduce noise as it ignores the correlation between similar samples in the same batch. One solution to this issue is supervised contrastive learning, but it remains an underexplored topic in HMTC due to its complex structured labels. To overcome this challenge, we propose $\textbf{HJCL}$, a $\textbf{H}$ierarchy-aware $\textbf{J}$oint Supervised $\textbf{C}$ontrastive $\textbf{L}$earning method that bridges the gap between supervised contrastive learning and HMTC. Specifically, we employ both instance-wise and label-wise contrastive learning techniques and carefully construct batches to fulfill the 
&lt;/p&gt;</description></item><item><title>EMOFM&#26159;&#19968;&#20010;&#38598;&#25104;MLP&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#29305;&#24449;&#30340;&#28151;&#21512;&#22120;&#23454;&#29616;&#20102;&#23383;&#27573;&#21644;&#31867;&#22411;&#29305;&#24449;&#30340;&#34701;&#21512;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#28857;&#20987;&#29575;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.04482</link><description>&lt;p&gt;
EMOFM: &#24102;&#26377;&#22522;&#20110;&#29305;&#24449;&#30340;&#28151;&#21512;&#22120;&#30340;&#38598;&#25104;MLP&#27169;&#22411;&#29992;&#20110;&#28857;&#20987;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
EMOFM: Ensemble MLP mOdel with Feature-based Mixers for Click-Through Rate Prediction. (arXiv:2310.04482v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04482
&lt;/p&gt;
&lt;p&gt;
EMOFM&#26159;&#19968;&#20010;&#38598;&#25104;MLP&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#29305;&#24449;&#30340;&#28151;&#21512;&#22120;&#23454;&#29616;&#20102;&#23383;&#27573;&#21644;&#31867;&#22411;&#29305;&#24449;&#30340;&#34701;&#21512;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#28857;&#20987;&#29575;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36712;&#36947;&#19968;&#30340;CTI&#31454;&#36187;&#20851;&#27880;&#30340;&#26159;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;&#25968;&#30334;&#19975;&#26465;&#35760;&#24405;&#65292;&#27599;&#26465;&#35760;&#24405;&#20013;&#30340;&#27599;&#20010;&#23383;&#27573;&#29305;&#24449;&#37117;&#30001;&#21704;&#24076;&#25972;&#25968;&#32452;&#25104;&#65292;&#20197;&#20445;&#25252;&#38544;&#31169;&#12290;&#23545;&#20110;&#36825;&#20010;&#20219;&#21153;&#65292;&#32593;&#32476;&#26041;&#27861;&#30340;&#20851;&#38190;&#21487;&#33021;&#26159;&#25353;&#31867;&#22411;&#25552;&#21462;&#29305;&#24449;&#21644;&#36328;&#19981;&#21516;&#23383;&#27573;&#36827;&#34892;&#20449;&#24687;&#34701;&#21512;&#12290;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#33021;&#22815;&#25552;&#21462;&#23383;&#27573;&#29305;&#24449;&#65292;&#20294;&#26080;&#27861;&#26377;&#25928;&#22320;&#34701;&#21512;&#29305;&#24449;&#12290;&#21463;&#21040;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#33258;&#28982;&#34701;&#21512;&#29305;&#24615;&#21644;&#22522;&#20110;Transformer&#32467;&#26500;&#30340;&#39640;&#25928;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#25554;&#20214;&#28151;&#21512;&#22120;&#29992;&#20110;&#23383;&#27573;/&#31867;&#22411;&#29305;&#24449;&#34701;&#21512;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#23383;&#27573;&#21644;&#31867;&#22411;&#28151;&#21512;&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#21363;EMOFM&#65288;&#24102;&#26377;&#22522;&#20110;&#29305;&#24449;&#30340;&#28151;&#21512;&#22120;&#30340;&#38598;&#25104;MLP&#27169;&#22411;&#65289;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23545;&#35813;&#27169;&#22411;&#22312;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21487;&#35270;&#21270;&#20102;&#20248;&#21270;&#36807;&#31243;&#24182;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;EMOFM&#20248;&#20110;&#27604;&#36739;&#22522;&#32447;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Track one of CTI competition is on click-through rate (CTR) prediction. The dataset contains millions of records and each field-wise feature in a record consists of hashed integers for privacy. For this task, the keys of network-based methods might be type-wise feature extraction and information fusion across different fields. Multi-layer perceptrons (MLPs) are able to extract field feature, but could not efficiently fuse features. Motivated by the natural fusion characteristic of cross attention and the efficiency of transformer-based structures, we propose simple plug-in mixers for field/type-wise feature fusion, and thus construct an field&amp;type-wise ensemble model, namely EMOFM (Ensemble MLP mOdel with Feature-based Mixers). In the experiments, the proposed model is evaluated on the dataset, the optimization process is visualized and ablation studies are explored. It is shown that EMOFM outperforms compared baselines. In the end, we discuss on future work. WARNING: The comparison mi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#29992;&#20110;&#23454;&#26102;&#39044;&#27979;&#22810;&#32423;&#36724;&#21521;&#21387;&#32553;&#26426;&#22312;&#29123;&#27668;&#36718;&#26426;&#20013;&#23574;&#38388;&#38553;&#21464;&#21270;&#23545;&#27668;&#21160;&#24615;&#33021;&#24433;&#21709;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#19982;CFD&#22522;&#20934;&#30456;&#23218;&#32654;&#30340;&#23454;&#26102;&#20934;&#30830;&#24615;&#65292;&#26041;&#20415;&#38598;&#25104;&#21040;&#29123;&#27668;&#36718;&#26426;&#30340;&#21046;&#36896;&#21644;&#26500;&#24314;&#36807;&#31243;&#20013;&#36827;&#34892;&#24615;&#33021;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.04264</link><description>&lt;p&gt;
C(NN)FD -- &#22810;&#32423;&#36724;&#21521;&#21387;&#32553;&#26426;&#27668;&#21160;&#24615;&#33021;&#20013;&#23574;&#38388;&#38553;&#21464;&#21270;&#30340;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
C(NN)FD -- deep learning predictions of tip clearance variations on multi-stage axial compressors aerodynamic performance. (arXiv:2310.04264v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#29992;&#20110;&#23454;&#26102;&#39044;&#27979;&#22810;&#32423;&#36724;&#21521;&#21387;&#32553;&#26426;&#22312;&#29123;&#27668;&#36718;&#26426;&#20013;&#23574;&#38388;&#38553;&#21464;&#21270;&#23545;&#27668;&#21160;&#24615;&#33021;&#24433;&#21709;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#19982;CFD&#22522;&#20934;&#30456;&#23218;&#32654;&#30340;&#23454;&#26102;&#20934;&#30830;&#24615;&#65292;&#26041;&#20415;&#38598;&#25104;&#21040;&#29123;&#27668;&#36718;&#26426;&#30340;&#21046;&#36896;&#21644;&#26500;&#24314;&#36807;&#31243;&#20013;&#36827;&#34892;&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36804;&#20170;&#20026;&#27490;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#35832;&#22914;CFD&#65288;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#65289;&#31561;&#29289;&#29702;&#27169;&#25311;&#22312;&#24037;&#19994;&#19978;&#30340;&#37325;&#35201;&#24615;&#26377;&#38480;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#32423;&#36724;&#21521;&#21387;&#32553;&#26426;&#22312;&#29123;&#27668;&#36718;&#26426;&#20013;&#23574;&#38388;&#38553;&#21464;&#21270;&#23545;&#27668;&#21160;&#24615;&#33021;&#30340;&#23454;&#26102;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#24320;&#21457;&#21644;&#24212;&#29992;&#12290;&#25152;&#25552;&#20986;&#30340;C(NN)FD&#26550;&#26500;&#32463;&#35777;&#26126;&#21487;&#25193;&#23637;&#33267;&#24037;&#19994;&#24212;&#29992;&#65292;&#24182;&#36798;&#21040;&#19982;CFD&#22522;&#20934;&#30456;&#23218;&#32654;&#30340;&#23454;&#26102;&#20934;&#30830;&#24615;&#12290;&#37096;&#32626;&#30340;&#27169;&#22411;&#21487;&#36731;&#26494;&#38598;&#25104;&#21040;&#29123;&#27668;&#36718;&#26426;&#30340;&#21046;&#36896;&#21644;&#26500;&#24314;&#36807;&#31243;&#20013;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#20998;&#26512;&#35780;&#20272;&#24615;&#33021;&#24433;&#21709;&#24182;&#28508;&#22312;&#20943;&#23569;&#26114;&#36149;&#29289;&#29702;&#27979;&#35797;&#35201;&#27714;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Application of deep learning methods to physical simulations such as CFD (Computational Fluid Dynamics), have been so far of limited industrial relevance. This paper demonstrates the development and application of a deep learning framework for real-time predictions of the impact of tip clearance variations on the aerodynamic performance of multi-stage axial compressors in gas turbines. The proposed C(NN)FD architecture is proven to be scalable to industrial applications, and achieves in real-time accuracy comparable to the CFD benchmark. The deployed model, is readily integrated within the manufacturing and build process of gas turbines, thus providing the opportunity to analytically assess the impact on performance and potentially reduce requirements for expensive physical tests.
&lt;/p&gt;</description></item><item><title>PyDCM&#26159;&#19968;&#20010;&#29992;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#30340;&#21487;&#23450;&#21046;&#30340;&#25968;&#25454;&#20013;&#24515;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#23450;&#20041;&#37197;&#32622;&#21644;&#21521;&#37327;&#21270;&#30340;&#28909;&#35745;&#31639;&#65292;&#23454;&#29616;&#20102;&#23545;&#25968;&#25454;&#20013;&#24515;&#30340;&#20248;&#21270;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.03906</link><description>&lt;p&gt;
&#29992;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#30340;PyDCM&#65306;&#20026;&#21487;&#25345;&#32493;&#24615;&#23450;&#21046;&#25968;&#25454;&#20013;&#24515;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PyDCM: Custom Data Center Models with Reinforcement Learning for Sustainability. (arXiv:2310.03906v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03906
&lt;/p&gt;
&lt;p&gt;
PyDCM&#26159;&#19968;&#20010;&#29992;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#30340;&#21487;&#23450;&#21046;&#30340;&#25968;&#25454;&#20013;&#24515;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#23450;&#20041;&#37197;&#32622;&#21644;&#21521;&#37327;&#21270;&#30340;&#28909;&#35745;&#31639;&#65292;&#23454;&#29616;&#20102;&#23545;&#25968;&#25454;&#20013;&#24515;&#30340;&#20248;&#21270;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#23545;&#21487;&#25345;&#32493;&#24615;&#21644;&#20943;&#23569;&#30899;&#25490;&#25918;&#30340;&#24378;&#35843;&#26085;&#30410;&#22686;&#21152;&#65292;&#20419;&#20351;&#25919;&#24220;&#21644;&#20225;&#19994;&#37325;&#26032;&#24605;&#32771;&#25968;&#25454;&#20013;&#24515;&#30340;&#35774;&#35745;&#21644;&#36816;&#33829;&#26041;&#27861;&#12290;&#37492;&#20110;&#25968;&#25454;&#20013;&#24515;&#30340;&#39640;&#33021;&#32791;&#21644;&#25351;&#25968;&#32423;&#35745;&#31639;&#24037;&#20316;&#37327;&#65292;&#20248;&#21270;&#33021;&#32791;&#29305;&#21035;&#26159;&#22312;&#20919;&#21364;&#21644;IT&#33021;&#28304;&#20351;&#29992;&#26041;&#38754;&#65292;&#25968;&#25454;&#20013;&#24515;&#26159;&#20248;&#21270;&#30005;&#21147;&#28040;&#32791;&#30340;&#29702;&#24819;&#20505;&#36873;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#32570;&#20047;&#21487;&#37197;&#32622;&#21644;&#21487;&#25193;&#23637;&#30340;&#28909;&#25968;&#25454;&#20013;&#24515;&#27169;&#22411;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31649;&#36947;&#12290;&#25968;&#25454;&#20013;&#24515;&#30001;&#22810;&#20010;IT&#32452;&#20214;&#32452;&#25104;&#65292;&#20854;&#20960;&#20309;&#37197;&#32622;&#21644;&#25955;&#28909;&#20351;&#24471;&#28909;&#24314;&#27169;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PyDCM&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;Python&#23454;&#29616;&#30340;&#21487;&#23450;&#21046;&#30340;&#25968;&#25454;&#20013;&#24515;&#27169;&#22411;&#65292;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#33258;&#23450;&#20041;&#30340;&#26381;&#21153;&#22120;&#35268;&#26684;&#21644;IT&#26426;&#26588;&#30340;&#20960;&#20309;&#24067;&#32622;&#21019;&#24314;&#29420;&#29305;&#30340;&#37197;&#32622;&#12290;&#20351;&#29992;&#21521;&#37327;&#21270;&#30340;&#28909;&#35745;&#31639;&#20351;&#24471;PyDCM&#27604;&#24403;&#21069;&#26041;&#27861;&#24555;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#65288;30&#20493;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing global emphasis on sustainability and reducing carbon emissions is pushing governments and corporations to rethink their approach to data center design and operation. Given their high energy consumption and exponentially large computational workloads, data centers are prime candidates for optimizing power consumption, especially in areas such as cooling and IT energy usage. A significant challenge in this pursuit is the lack of a configurable and scalable thermal data center model that offers an end-to-end pipeline. Data centers consist of multiple IT components whose geometric configuration and heat dissipation make thermal modeling difficult. This paper presents PyDCM, a customizable Data Center Model implemented in Python, that allows users to create unique configurations of IT equipment with custom server specifications and geometric arrangements of IT cabinets. The use of vectorized thermal calculations makes PyDCM orders of magnitude faster (30 times) than current 
&lt;/p&gt;</description></item><item><title>Swin-Tempo&#26159;&#19968;&#20010;&#21019;&#26032;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;Swin Transformer-Enhanced UNet&#23558;CT&#25195;&#25551;&#20013;&#30340;&#32954;&#32467;&#33410;&#26816;&#27979;&#20316;&#20026;&#35270;&#39057;&#24207;&#21015;&#36827;&#34892;&#26102;&#38388;&#24863;&#30693;&#12290;&#23427;&#20811;&#26381;&#20102;&#29616;&#26377;&#32593;&#32476;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#32954;&#32467;&#33410;&#26816;&#27979;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.03365</link><description>&lt;p&gt;
Swin-Tempo: &#20351;&#29992;Swin Transformer-Enhanced UNet&#23558;CT&#25195;&#25551;&#20013;&#30340;&#32954;&#32467;&#33410;&#26816;&#27979;&#20316;&#20026;&#35270;&#39057;&#24207;&#21015;&#36827;&#34892;&#26102;&#38388;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Swin-Tempo: Temporal-Aware Lung Nodule Detection in CT Scans as Video Sequences Using Swin Transformer-Enhanced UNet. (arXiv:2310.03365v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03365
&lt;/p&gt;
&lt;p&gt;
Swin-Tempo&#26159;&#19968;&#20010;&#21019;&#26032;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;Swin Transformer-Enhanced UNet&#23558;CT&#25195;&#25551;&#20013;&#30340;&#32954;&#32467;&#33410;&#26816;&#27979;&#20316;&#20026;&#35270;&#39057;&#24207;&#21015;&#36827;&#34892;&#26102;&#38388;&#24863;&#30693;&#12290;&#23427;&#20811;&#26381;&#20102;&#29616;&#26377;&#32593;&#32476;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#32954;&#32467;&#33410;&#26816;&#27979;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32954;&#30284;&#20855;&#26377;&#26497;&#39640;&#30340;&#33268;&#27515;&#29575;&#65292;&#26089;&#26399;&#26816;&#27979;&#23545;&#20110;&#38450;&#27835;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25918;&#23556;&#31185;&#21307;&#29983;&#32780;&#35328;&#65292;&#35782;&#21035;&#32954;&#32467;&#33410;&#23384;&#22312;&#37325;&#22823;&#25361;&#25112;&#65292;&#20182;&#20204;&#24448;&#24448;&#20381;&#36182;&#33258;&#24049;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#32463;&#39564;&#26469;&#36827;&#34892;&#20934;&#30830;&#30340;&#35786;&#26029;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#31995;&#32479;&#24050;&#32463;&#20986;&#29616;&#65292;&#24110;&#21161;&#21307;&#29983;&#20174;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#22270;&#20687;&#20013;&#35782;&#21035;&#32954;&#32467;&#33410;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#32593;&#32476;&#24448;&#24448;&#23384;&#22312;&#35745;&#31639;&#22797;&#26434;&#24615;&#38382;&#39064;&#65292;&#23548;&#33268;&#35823;&#25253;&#21644;&#28431;&#25253;&#29575;&#36739;&#39640;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#35270;&#35273;Transformer&#30340;&#20248;&#21183;&#12290;&#21463;&#35270;&#39057;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;3D CT&#22270;&#20687;&#35270;&#20026;&#19968;&#20010;&#35270;&#39057;&#65292;&#23558;&#27599;&#20010;&#20999;&#29255;&#35270;&#20026;&#24103;&#65292;&#23558;&#32954;&#32467;&#33410;&#35270;&#20026;&#30446;&#26631;&#65292;&#23454;&#29616;&#19968;&#20010;&#26102;&#24207;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20811;&#26381;&#30828;&#20214;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lung cancer is highly lethal, emphasizing the critical need for early detection. However, identifying lung nodules poses significant challenges for radiologists, who rely heavily on their expertise and experience for accurate diagnosis. To address this issue, computer-aided diagnosis systems based on machine learning techniques have emerged to assist doctors in identifying lung nodules from computed tomography (CT) scans. Unfortunately, existing networks in this domain often suffer from computational complexity, leading to high rates of false negatives and false positives, limiting their effectiveness. To address these challenges, we present an innovative model that harnesses the strengths of both convolutional neural networks and vision transformers. Inspired by object detection in videos, we treat each 3D CT image as a video, individual slices as frames, and lung nodules as objects, enabling a time-series application. The primary objective of our work is to overcome hardware limitati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26597;&#35810;&#39640;&#25928;&#23545;&#25239;HTML&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#31713;&#25913;&#26469;&#20462;&#25913;&#38035;&#40060;&#32593;&#39029;&#30340;HTML&#20195;&#30721;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#24694;&#24847;&#24615;&#21644;&#35270;&#35273;&#22806;&#35266;&#19981;&#21464;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#23558;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#38450;&#38035;&#40060;&#32593;&#39029;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#25703;&#27585;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;30&#20010;&#26597;&#35810;&#12290;</title><link>http://arxiv.org/abs/2310.03166</link><description>&lt;p&gt;
&#30772;&#22351;&#21040;&#24213;&#65306;&#23545;&#26426;&#22120;&#23398;&#20064;&#38450;&#38035;&#40060;&#32593;&#39029;&#26816;&#27979;&#22120;&#30340;&#26597;&#35810;&#39640;&#25928;&#23545;&#25239;HTML&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Raze to the Ground: Query-Efficient Adversarial HTML Attacks on Machine-Learning Phishing Webpage Detectors. (arXiv:2310.03166v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26597;&#35810;&#39640;&#25928;&#23545;&#25239;HTML&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#31713;&#25913;&#26469;&#20462;&#25913;&#38035;&#40060;&#32593;&#39029;&#30340;HTML&#20195;&#30721;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#24694;&#24847;&#24615;&#21644;&#35270;&#35273;&#22806;&#35266;&#19981;&#21464;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#23558;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#38450;&#38035;&#40060;&#32593;&#39029;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#25703;&#27585;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;30&#20010;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#38450;&#38035;&#40060;&#32593;&#39029;&#26816;&#27979;&#22120;&#65288;ML-PWD&#65289;&#24050;&#34987;&#35777;&#26126;&#23481;&#26131;&#21463;&#21040;&#36755;&#20837;&#32593;&#39029;HTML&#20195;&#30721;&#30340;&#23545;&#25239;&#24615;&#31713;&#25913;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#25552;&#20986;&#30340;&#25915;&#20987;&#30001;&#20110;&#32570;&#20047;&#20248;&#21270;&#25152;&#37319;&#29992;&#30340;&#31713;&#25913;&#30340;&#20351;&#29992;&#20197;&#21450;&#20165;&#20851;&#27880;HTML&#20195;&#30721;&#30340;&#29305;&#23450;&#20803;&#32032;&#32780;&#34920;&#29616;&#20986;&#26377;&#38480;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#39318;&#20808;&#35774;&#35745;&#19968;&#32452;&#26032;&#39062;&#30340;&#32454;&#31890;&#24230;&#31713;&#25913;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#36825;&#20123;&#31713;&#25913;&#20801;&#35768;&#20462;&#25913;&#36755;&#20837;&#38035;&#40060;&#32593;&#39029;&#30340;HTML&#20195;&#30721;&#65292;&#32780;&#26080;&#38656; compromiser&#20854;&#24694;&#24847;&#24615;&#21644;&#35270;&#35273;&#22806;&#35266;&#65292;&#21363;&#31713;&#25913;&#22312;&#35774;&#35745;&#19978;&#26159;&#21151;&#33021;&#21644;&#28210;&#26579;&#20445;&#25345;&#19981;&#21464;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#26597;&#35810;&#39640;&#25928;&#30340;&#40657;&#30418;&#20248;&#21270;&#31639;&#27861;&#36873;&#25321;&#38656;&#35201;&#24212;&#29992;&#21738;&#20123;&#31713;&#25913;&#20197;&#32469;&#36807;&#30446;&#26631;&#26816;&#27979;&#22120;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25915;&#20987;&#21482;&#38656;&#35201;30&#20010;&#26597;&#35810;&#21363;&#21487;&#25703;&#27585;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;ML-PWD&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#36739;&#24369;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine-learning phishing webpage detectors (ML-PWD) have been shown to suffer from adversarial manipulations of the HTML code of the input webpage. Nevertheless, the attacks recently proposed have demonstrated limited effectiveness due to their lack of optimizing the usage of the adopted manipulations, and they focus solely on specific elements of the HTML code. In this work, we overcome these limitations by first designing a novel set of fine-grained manipulations which allow to modify the HTML code of the input phishing webpage without compromising its maliciousness and visual appearance, i.e., the manipulations are functionality- and rendering-preserving by design. We then select which manipulations should be applied to bypass the target detector by a query-efficient black-box optimization algorithm. Our experiments show that our attacks are able to raze to the ground the performance of current state-of-the-art ML-PWD using just 30 queries, thus overcoming the weaker attacks develo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#20449;&#29992;&#21345;&#36829;&#32422;&#36827;&#34892;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20449;&#29992;&#21345;&#35780;&#20998;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#27169;&#22411;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2310.02956</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20449;&#29992;&#21345;&#35780;&#20998;&#39044;&#27979;&#65306;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Credit card score prediction using machine learning models: A new dataset. (arXiv:2310.02956v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#20449;&#29992;&#21345;&#36829;&#32422;&#36827;&#34892;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20449;&#29992;&#21345;&#35780;&#20998;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#27169;&#22411;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20449;&#29992;&#21345;&#30340;&#20351;&#29992;&#37327;&#19981;&#26029;&#22686;&#21152;&#65292;&#20026;&#20102;&#26368;&#23567;&#21270;&#28508;&#22312;&#39118;&#38505;&#65292;&#24613;&#38656;&#20449;&#29992;&#21345;&#35780;&#20272;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20449;&#29992;&#21345;&#36829;&#32422;&#39044;&#27979;&#31995;&#32479;&#30340;&#24212;&#29992;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#30740;&#31350;&#22312;&#26032;&#25552;&#20986;&#30340;&#20449;&#29992;&#21345;&#35780;&#20998;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26368;&#20339;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36825;&#20010;&#26032;&#25968;&#25454;&#38598;&#21253;&#25324;&#20449;&#29992;&#21345;&#20132;&#26131;&#21382;&#21490;&#21644;&#23458;&#25143;&#26723;&#26696;&#65292;&#24182;&#20351;&#29992;&#20102;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21253;&#25324;&#36923;&#36753;&#22238;&#24402;&#12289;&#20915;&#31574;&#26641;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#31070;&#32463;&#32593;&#32476;&#12289;XGBoost&#21644;LightGBM&#12290;&#20026;&#20102;&#20934;&#22791;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#29305;&#24449;&#25552;&#21462;&#12289;&#29305;&#24449;&#36873;&#25321;&#21644;&#25968;&#25454;&#24179;&#34913;&#25216;&#26415;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#30495;&#27491;&#38451;&#24615;&#29575;&#26041;&#38754;&#65292;MLP&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#20248;&#20110;&#36923;&#36753;&#22238;&#24402;&#12289;&#20915;&#31574;&#26641;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;LightGBM&#21644;XGBoost&#65292;&#23454;&#29616;&#20102;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of credit cards has recently increased, creating an essential need for credit card assessment methods to minimize potential risks. This study investigates the utilization of machine learning (ML) models for credit card default prediction system. The main goal here is to investigate the best-performing ML model for new proposed credit card scoring dataset. This new dataset includes credit card transaction histories and customer profiles, is proposed and tested using a variety of machine learning algorithms, including logistic regression, decision trees, random forests, multi layer perceptron (MLP) neural network, XGBoost, and LightGBM. To prepare the data for machine learning models, we perform data pre-proccessing, feature extraction, feature selection, and data balancing techniques. Experimental results demonstrate that MLP outperforms logistic regression, decision trees, random forests, LightGBM, and XGBoost in terms of predictive performance in true positive rate, achieving 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#20013;&#38388;&#23618;&#21464;&#25442;&#30340;&#24179;&#28369;&#24615;&#26469;&#26816;&#27979;&#24102;&#22806;&#25968;&#25454;&#30340;&#26041;&#27861;(BLOOD),&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#35775;&#38382;&#26435;&#38480;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;Transformer&#32593;&#32476;&#19978;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.02832</link><description>&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#23618;&#38388;&#21464;&#25442;&#30340;&#24179;&#28369;&#24615;&#36827;&#34892;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Out-of-Distribution Detection by Leveraging Between-Layer Transformation Smoothness. (arXiv:2310.02832v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#20013;&#38388;&#23618;&#21464;&#25442;&#30340;&#24179;&#28369;&#24615;&#26469;&#26816;&#27979;&#24102;&#22806;&#25968;&#25454;&#30340;&#26041;&#27861;(BLOOD),&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#35775;&#38382;&#26435;&#38480;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;Transformer&#32593;&#32476;&#19978;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;&#23545;&#20110;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#22823;&#22810;&#25968;&#24403;&#21069;&#26041;&#27861;&#30001;&#20110;&#38656;&#35201;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#25110;&#32773;&#24178;&#39044;&#35757;&#32451;&#32780;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32593;&#32476;&#20013;&#38388;&#23618;&#30340;&#21464;&#25442;&#24179;&#28369;&#24615;&#26469;&#26816;&#27979;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24102;&#22806;&#25968;&#25454;&#65288;BLOOD&#65289;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#35775;&#38382;&#26435;&#38480;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;BLOOD&#21033;&#29992;&#20869;&#20998;&#24067;&#65288;ID&#65289;&#25968;&#25454;&#30340;&#23618;&#38388;&#34920;&#31034;&#21464;&#25442;&#30456;&#36739;&#20110;&#24102;&#22806;&#25968;&#25454;&#30340;&#21464;&#25442;&#26356;&#24179;&#28369;&#30340;&#20542;&#21521;&#65292;&#36825;&#20063;&#26159;&#25105;&#20204;&#22312;Transformer&#32593;&#32476;&#20013;&#32463;&#39564;&#35777;&#26126;&#30340;&#19968;&#20010;&#29305;&#24615;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;BLOOD&#19982;Transformer&#32593;&#32476;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#36164;&#28304;&#38656;&#27714;&#30456;&#24403;&#30340;&#26041;&#27861;&#19978;&#24615;&#33021;&#26356;&#22909;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#34920;&#26126;&#65292;&#24403;&#23398;&#20064;&#26356;&#31616;&#21333;&#30340;&#20219;&#21153;&#26102;&#65292;&#24102;&#22806;&#25968;&#25454;&#30340;&#21464;&#25442;&#20250;&#20445;&#25345;&#20854;&#21407;&#22987;&#30340;&#38160;&#24230;&#65292;&#32780;&#38160;&#24230;&#20250;&#38543;&#30528;&#20219;&#21153;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective OOD detection is crucial for reliable machine learning models, yet most current methods are limited in practical use due to requirements like access to training data or intervention in training. We present a novel method for detecting OOD data in deep neural networks based on transformation smoothness between intermediate layers of a network (BLOOD), which is applicable to pre-trained models without access to training data. BLOOD utilizes the tendency of between-layer representation transformations of in-distribution (ID) data to be smoother than the corresponding transformations of OOD data, a property that we also demonstrate empirically for Transformer networks. We evaluate BLOOD on several text classification tasks with Transformer networks and demonstrate that it outperforms methods with comparable resource requirements. Our analysis also suggests that when learning simpler tasks, OOD data transformations maintain their original sharpness, whereas sharpness increases wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;3D&#29289;&#29702;&#31995;&#32479;&#30340;&#26494;&#24347;&#20843;&#38754;&#20307;&#32676;&#21367;&#31215;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#22312;&#20445;&#25345;&#25968;&#25454;&#19968;&#33268;&#30340;&#26368;&#39640;&#31561;&#21464;&#24615;&#27700;&#24179;&#30340;&#21516;&#26102;&#65292;&#21457;&#29616;&#29289;&#29702;&#31995;&#32479;&#20013;&#24494;&#22937;&#30340;&#23545;&#31216;&#24615;&#30772;&#32570;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2310.02299</link><description>&lt;p&gt;
3D&#29289;&#29702;&#31995;&#32479;&#20013;&#23398;&#20064;&#23545;&#31216;&#24615;&#30772;&#32570;&#30340;&#26494;&#24347;&#20843;&#38754;&#20307;&#32676;&#21367;&#31215;
&lt;/p&gt;
&lt;p&gt;
Relaxed Octahedral Group Convolution for Learning Symmetry Breaking in 3D Physical Systems. (arXiv:2310.02299v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;3D&#29289;&#29702;&#31995;&#32479;&#30340;&#26494;&#24347;&#20843;&#38754;&#20307;&#32676;&#21367;&#31215;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#22312;&#20445;&#25345;&#25968;&#25454;&#19968;&#33268;&#30340;&#26368;&#39640;&#31561;&#21464;&#24615;&#27700;&#24179;&#30340;&#21516;&#26102;&#65292;&#21457;&#29616;&#29289;&#29702;&#31995;&#32479;&#20013;&#24494;&#22937;&#30340;&#23545;&#31216;&#24615;&#30772;&#32570;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31561;&#20215;&#27169;&#22411;&#21033;&#29992;&#23545;&#31216;&#24615;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#36825;&#20123;&#27169;&#22411;&#20013;&#65292;&#23436;&#32654;&#23545;&#31216;&#24615;&#30340;&#20551;&#35774;&#26377;&#26102;&#21487;&#33021;&#20250;&#38480;&#21046;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#24403;&#25968;&#25454;&#19982;&#36825;&#20123;&#23545;&#31216;&#24615;&#19981;&#23436;&#20840;&#19968;&#33268;&#26102;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#24341;&#20837;&#20102;&#29992;&#20110;&#24314;&#27169;3D&#29289;&#29702;&#31995;&#32479;&#30340;&#26494;&#24347;&#20843;&#38754;&#20307;&#32676;&#21367;&#31215;&#12290;&#36825;&#31181;&#28789;&#27963;&#30340;&#21367;&#31215;&#25216;&#26415;&#33021;&#22815;&#22312;&#20445;&#25345;&#19982;&#25968;&#25454;&#19968;&#33268;&#30340;&#26368;&#39640;&#31561;&#21464;&#24615;&#27700;&#24179;&#30340;&#21516;&#26102;&#65292;&#21457;&#29616;&#29289;&#29702;&#31995;&#32479;&#20013;&#24494;&#22937;&#30340;&#23545;&#31216;&#24615;&#30772;&#32570;&#22240;&#32032;&#12290;&#23454;&#35777;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25581;&#31034;&#30456;&#21464;&#20013;&#30340;&#23545;&#31216;&#24615;&#30772;&#32570;&#22240;&#32032;&#65292;&#36824;&#21487;&#20197;&#22312;&#27969;&#20307;&#36229;&#20998;&#36776;&#29575;&#20219;&#21153;&#20013;&#23454;&#29616;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep equivariant models use symmetries to improve sample efficiency and generalization. However, the assumption of perfect symmetry in many of these models can sometimes be restrictive, especially when the data does not perfectly align with such symmetries. Thus, we introduce relaxed octahedral group convolution for modeling 3D physical systems in this paper. This flexible convolution technique provably allows the model to both maintain the highest level of equivariance that is consistent with data and discover the subtle symmetry-breaking factors in the physical systems. Empirical results validate that our approach can not only provide insights into the symmetry-breaking factors in phase transitions but also achieves superior performance in fluid super-resolution tasks.
&lt;/p&gt;</description></item><item><title>&#33258;&#32534;&#30721;&#22120;&#26159;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#35843;&#25972;&#21442;&#25968;&#23454;&#29616;&#36755;&#20837;&#25968;&#25454;&#21644;&#37325;&#26500;&#36755;&#20986;&#20043;&#38388;&#30340;&#26368;&#23567;&#24046;&#24322;&#65292;&#29992;&#20110;&#35782;&#21035;&#25968;&#25454;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#20869;&#22312;&#32500;&#24230;&#65292;&#24182;&#19988;&#23545;&#20110;&#26576;&#20123;&#25299;&#25169;&#32467;&#26500;&#65292;&#23384;&#22312;&#38590;&#20197;&#25214;&#21040;&#23436;&#32654;&#37325;&#26500;&#32593;&#32476;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.02250</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#33258;&#32534;&#30721;&#22120;&#36215;&#20316;&#29992;&#65311;
&lt;/p&gt;
&lt;p&gt;
Why do autoencoders work?. (arXiv:2310.02250v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02250
&lt;/p&gt;
&lt;p&gt;
&#33258;&#32534;&#30721;&#22120;&#26159;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#35843;&#25972;&#21442;&#25968;&#23454;&#29616;&#36755;&#20837;&#25968;&#25454;&#21644;&#37325;&#26500;&#36755;&#20986;&#20043;&#38388;&#30340;&#26368;&#23567;&#24046;&#24322;&#65292;&#29992;&#20110;&#35782;&#21035;&#25968;&#25454;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#20869;&#22312;&#32500;&#24230;&#65292;&#24182;&#19988;&#23545;&#20110;&#26576;&#20123;&#25299;&#25169;&#32467;&#26500;&#65292;&#23384;&#22312;&#38590;&#20197;&#25214;&#21040;&#23436;&#32654;&#37325;&#26500;&#32593;&#32476;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33258;&#32534;&#30721;&#22120;&#34987;&#24191;&#27867;&#29992;&#20110;&#27169;&#22411;&#21387;&#32553;&#12290;&#23427;&#20204;&#21487;&#20197;&#35782;&#21035;&#25968;&#25454;&#22312;&#36755;&#20837;&#30340;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;R^n&#20013;&#65292;&#20301;&#20110;k&#32500;&#23376;&#38598;K&#20013;&#30340;&#20869;&#22312;&#32500;&#24230;&#12290;&#20854;&#22522;&#26412;&#24605;&#24819;&#26159;&#33719;&#24471;&#19968;&#20010;&#23558;R^n&#26144;&#23556;&#20026;R^k&#30340;&#32534;&#30721;&#23618;&#65288;&#31216;&#20026;&#29942;&#39048;&#23618;&#25110;&#28508;&#21464;&#37327;&#31354;&#38388;&#65289;&#65292;&#20197;&#21450;&#19968;&#20010;&#23558;R^k&#26144;&#23556;&#22238;R^n&#30340;&#35299;&#30721;&#23618;&#65292;&#20351;&#24471;&#22312;&#32452;&#21512;&#36825;&#20004;&#20010;&#26144;&#23556;&#26102;&#21487;&#20197;&#24674;&#22797;&#26469;&#33258;&#38598;&#21512;K&#30340;&#36755;&#20837;&#25968;&#25454;&#12290;&#36825;&#36890;&#36807;&#35843;&#25972;&#32593;&#32476;&#20013;&#30340;&#21442;&#25968;&#65288;&#26435;&#37325;&#65289;&#26469;&#26368;&#23567;&#21270;&#36755;&#20837;&#21644;&#37325;&#26500;&#36755;&#20986;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#23454;&#29616;&#12290;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#65288;&#20855;&#26377;&#36830;&#32493;&#28608;&#27963;&#20989;&#25968;&#65289;&#35745;&#31639;&#36830;&#32493;&#26144;&#23556;&#65292;&#23454;&#29616;&#23436;&#32654;&#37325;&#26500;&#30340;&#32593;&#32476;&#30340;&#23384;&#22312;&#23558;&#24847;&#21619;&#30528;K&#22312;R^k&#20013;&#26159;&#19968;&#20010;k&#32500;&#23376;&#38598;&#30340;&#21516;&#32986;&#65292;&#22240;&#27492;&#26126;&#26174;&#23384;&#22312;&#25299;&#25169;&#38556;&#30861;&#26469;&#23547;&#25214;&#36825;&#26679;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural network autoencoders are routinely used computationally for model reduction. They allow recognizing the intrinsic dimension of data that lie in a $k$-dimensional subset $K$ of an input Euclidean space $\mathbb{R}^n$. The underlying idea is to obtain both an encoding layer that maps $\mathbb{R}^n$ into $\mathbb{R}^k$ (called the bottleneck layer or the space of latent variables) and a decoding layer that maps $\mathbb{R}^k$ back into $\mathbb{R}^n$, in such a way that the input data from the set $K$ is recovered when composing the two maps. This is achieved by adjusting parameters (weights) in the network to minimize the discrepancy between the input and the reconstructed output. Since neural networks (with continuous activation functions) compute continuous maps, the existence of a network that achieves perfect reconstruction would imply that $K$ is homeomorphic to a $k$-dimensional subset of $\mathbb{R}^k$, so clearly there are topological obstructions to finding such a ne
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MIS-AVoiDD&#30340;&#27169;&#24577;&#19981;&#21464;&#21644;&#29305;&#23450;&#34920;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#38899;&#35270;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#12290;&#36890;&#36807;&#20805;&#20998;&#20851;&#27880;&#38899;&#39057;&#21644;&#35270;&#35273;&#25968;&#25454;&#65292;&#24182;&#35299;&#20915;&#38899;&#39057;&#21644;&#35270;&#35273;&#20449;&#21495;&#30340;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#27169;&#24577;&#25805;&#20316;&#26816;&#27979;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02234</link><description>&lt;p&gt;
MIS-AVoiDD:&#38899;&#35270;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#30340;&#27169;&#24577;&#19981;&#21464;&#21644;&#29305;&#23450;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
MIS-AVoiDD: Modality Invariant and Specific Representation for Audio-Visual Deepfake Detection. (arXiv:2310.02234v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MIS-AVoiDD&#30340;&#27169;&#24577;&#19981;&#21464;&#21644;&#29305;&#23450;&#34920;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#38899;&#35270;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#12290;&#36890;&#36807;&#20805;&#20998;&#20851;&#27880;&#38899;&#39057;&#21644;&#35270;&#35273;&#25968;&#25454;&#65292;&#24182;&#35299;&#20915;&#38899;&#39057;&#21644;&#35270;&#35273;&#20449;&#21495;&#30340;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#27169;&#24577;&#25805;&#20316;&#26816;&#27979;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20266;&#36896;&#26159;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#31639;&#27861;&#29983;&#25104;&#30340;&#21512;&#25104;&#23186;&#20307;&#65292;&#23545;&#31038;&#20250;&#21644;&#25919;&#27835;&#26500;&#25104;&#20102;&#20005;&#37325;&#23041;&#32961;&#12290;&#38500;&#20102;&#38754;&#37096;&#25805;&#20316;&#21644;&#21512;&#25104;&#35821;&#38899;&#65292;&#26368;&#36817;&#36824;&#20986;&#29616;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#28145;&#24230;&#20266;&#36896;&#65292;&#20854;&#20013;&#38899;&#39057;&#25110;&#35270;&#35273;&#27169;&#24577;&#34987;&#25805;&#20316;&#12290;&#20026;&#27492;&#65292;&#27491;&#22312;&#30740;&#31350;&#19968;&#31181;&#26032;&#19968;&#20195;&#30340;&#22810;&#27169;&#24577;&#38899;&#35270;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#65292;&#20849;&#21516;&#20851;&#27880;&#38899;&#39057;&#21644;&#35270;&#35273;&#25968;&#25454;&#36827;&#34892;&#22810;&#27169;&#24577;&#25805;&#20316;&#26816;&#27979;&#12290;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#65288;&#38899;&#35270;&#65289;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#36890;&#24120;&#22522;&#20110;&#20174;&#35270;&#39057;&#20013;&#34701;&#21512;&#38899;&#39057;&#21644;&#35270;&#35273;&#27969;&#12290;&#29616;&#26377;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#22810;&#27169;&#24577;&#26816;&#27979;&#22120;&#36890;&#24120;&#20855;&#26377;&#19982;&#21333;&#27169;&#24577;&#38899;&#39057;&#21644;&#35270;&#35273;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25512;&#27979;&#65292;&#38899;&#39057;&#21644;&#35270;&#35273;&#20449;&#21495;&#30340;&#24322;&#36136;&#24615;&#26412;&#36136;&#36896;&#25104;&#20102;&#27169;&#24577;&#20998;&#24067;&#24046;&#36317;&#65292;&#24182;&#23545;&#26377;&#25928;&#34701;&#21512;&#21644;&#39640;&#25928;&#24615;&#33021;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20174;&#34920;&#31034;&#23618;&#38754;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deepfakes are synthetic media generated using deep generative algorithms and have posed a severe societal and political threat. Apart from facial manipulation and synthetic voice, recently, a novel kind of deepfakes has emerged with either audio or visual modalities manipulated. In this regard, a new generation of multimodal audio-visual deepfake detectors is being investigated to collectively focus on audio and visual data for multimodal manipulation detection. Existing multimodal (audio-visual) deepfake detectors are often based on the fusion of the audio and visual streams from the video. Existing studies suggest that these multimodal detectors often obtain equivalent performances with unimodal audio and visual deepfake detectors. We conjecture that the heterogeneous nature of the audio and visual signals creates distributional modality gaps and poses a significant challenge to effective fusion and efficient performance. In this paper, we tackle the problem at the representation lev
&lt;/p&gt;</description></item><item><title>PR-MPNNs&#36890;&#36807;&#27010;&#29575;&#37325;&#36830;&#23398;&#20064;&#21152;&#20837;&#30456;&#20851;&#36793;&#65292;&#24182;&#30465;&#30053;&#23545;&#39044;&#27979;&#20219;&#21153;&#27809;&#26377;&#24110;&#21161;&#30340;&#36793;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02156</link><description>&lt;p&gt;
&#27010;&#29575;&#37325;&#36830;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Probabilistically Rewired Message-Passing Neural Networks. (arXiv:2310.02156v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02156
&lt;/p&gt;
&lt;p&gt;
PR-MPNNs&#36890;&#36807;&#27010;&#29575;&#37325;&#36830;&#23398;&#20064;&#21152;&#20837;&#30456;&#20851;&#36793;&#65292;&#24182;&#30465;&#30053;&#23545;&#39044;&#27979;&#20219;&#21153;&#27809;&#26377;&#24110;&#21161;&#30340;&#36793;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#20316;&#20026;&#22788;&#29702;&#22270;&#32467;&#26500;&#36755;&#20837;&#30340;&#24378;&#22823;&#24037;&#20855;&#32780;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#22266;&#23450;&#30340;&#36755;&#20837;&#22270;&#32467;&#26500;&#19978;&#25805;&#20316;&#65292;&#24573;&#30053;&#20102;&#28508;&#22312;&#30340;&#22122;&#22768;&#21644;&#32570;&#22833;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#30340;&#23616;&#37096;&#32858;&#21512;&#26426;&#21046;&#21487;&#33021;&#23548;&#33268;&#38382;&#39064;&#65292;&#22914;&#36807;&#24230;&#21387;&#32553;&#21644;&#22312;&#25429;&#25417;&#30456;&#20851;&#22270;&#32467;&#26500;&#26041;&#38754;&#30340;&#26377;&#38480;&#34920;&#36798;&#33021;&#21147;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24448;&#24448;&#24573;&#35270;&#20102;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#25512;&#26029;&#19982;&#32473;&#23450;&#39044;&#27979;&#20219;&#21153;&#30456;&#20851;&#30340;&#22270;&#32467;&#26500;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#21033;&#29992;&#20102;&#26368;&#36817;&#22312;&#31934;&#30830;&#21644;&#21487;&#24494;&#20998;&#30340;k-&#23376;&#38598;&#37319;&#26679;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#27010;&#29575;&#37325;&#36830;&#30340;MPNN (PR-MPNN)&#65292;&#23427;&#20204;&#23398;&#20064;&#22312;&#30465;&#30053;&#23545;&#39044;&#27979;&#20219;&#21153;&#27809;&#26377;&#24110;&#21161;&#30340;&#36793;&#30340;&#21516;&#26102;&#28155;&#21152;&#30456;&#20851;&#30340;&#36793;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#39318;&#27425;&#25506;&#32034;&#20102;PR-MPNN&#22914;&#20309;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#19988;&#25105;&#20204;&#30830;&#23450;&#20102;&#30830;&#20999;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Message-passing graph neural networks (MPNNs) emerged as powerful tools for processing graph-structured input. However, they operate on a fixed input graph structure, ignoring potential noise and missing information. Furthermore, their local aggregation mechanism can lead to problems such as over-squashing and limited expressive power in capturing relevant graph structures. Existing solutions to these challenges have primarily relied on heuristic methods, often disregarding the underlying data distribution. Hence, devising principled approaches for learning to infer graph structures relevant to the given prediction task remains an open challenge. In this work, leveraging recent progress in exact and differentiable $k$-subset sampling, we devise probabilistically rewired MPNNs (PR-MPNNs), which learn to add relevant edges while omitting less beneficial ones. For the first time, our theoretical analysis explores how PR-MPNNs enhance expressive power, and we identify precise conditions un
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SpatialRank&#30340;&#26032;&#39062;&#31354;&#38388;&#20107;&#20214;&#25490;&#21517;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#26102;&#31354;&#25968;&#25454;&#30340;NDCG&#20248;&#21270;&#26469;&#35299;&#20915;&#22478;&#24066;&#20107;&#20214;&#25490;&#21517;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.00270</link><description>&lt;p&gt;
SpatialRank: &#22522;&#20110;&#26102;&#31354;&#25968;&#25454;&#30340;&#22478;&#24066;&#20107;&#20214;&#25490;&#21517;&#19982;NDCG&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
SpatialRank: Urban Event Ranking with NDCG Optimization on Spatiotemporal Data. (arXiv:2310.00270v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00270
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SpatialRank&#30340;&#26032;&#39062;&#31354;&#38388;&#20107;&#20214;&#25490;&#21517;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#26102;&#31354;&#25968;&#25454;&#30340;NDCG&#20248;&#21270;&#26469;&#35299;&#20915;&#22478;&#24066;&#20107;&#20214;&#25490;&#21517;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#20107;&#20214;&#25490;&#21517;&#38382;&#39064;&#26088;&#22312;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#65288;&#22914;&#20132;&#36890;&#20107;&#25925;&#21644;&#29359;&#32618;&#20107;&#20214;&#65289;&#30340;&#39118;&#38505;&#26368;&#39640;&#30340;&#21069;k&#20010;&#22320;&#28857;&#12290;&#36825;&#20010;&#38382;&#39064;&#23545;&#20844;&#20849;&#23433;&#20840;&#21644;&#22478;&#24066;&#31649;&#29702;&#38750;&#24120;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22320;&#28857;&#20043;&#38388;&#22797;&#26434;&#32780;&#21160;&#24577;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#65292;&#31354;&#38388;&#20013;&#22478;&#24066;&#20107;&#20214;&#30340;&#19981;&#22343;&#21248;&#20998;&#24067;&#65292;&#20197;&#21450;&#27491;&#30830;&#23545;&#20855;&#26377;&#30456;&#20284;&#29305;&#24449;&#30340;&#38468;&#36817;&#22320;&#28857;&#36827;&#34892;&#25490;&#21517;&#30340;&#22256;&#38590;&#65292;&#36825;&#20010;&#38382;&#39064;&#24456;&#20855;&#25361;&#25112;&#24615;&#12290;&#21069;&#20154;&#30340;&#30740;&#31350;&#20027;&#35201;&#26088;&#22312;&#20934;&#30830;&#39044;&#27979;&#25152;&#26377;&#22320;&#28857;&#30340;&#23454;&#38469;&#39118;&#38505;&#24471;&#20998;&#25110;&#20107;&#20214;&#35745;&#25968;&#12290;&#30001;&#20110;&#39044;&#27979;&#38169;&#35823;&#65292;&#30001;&#27492;&#24471;&#21040;&#30340;&#25490;&#21517;&#36890;&#24120;&#36136;&#37327;&#36739;&#20302;&#12290;&#23398;&#20064;&#25490;&#24207;&#26041;&#27861;&#30452;&#25509;&#20248;&#21270;&#35832;&#22914;&#26631;&#20934;&#21270;&#25240;&#25187;&#32047;&#31215;&#22686;&#30410;&#65288;NDCG&#65289;&#20043;&#31867;&#30340;&#25351;&#26631;&#65292;&#20294;&#19981;&#33021;&#22788;&#29702;&#22320;&#28857;&#20043;&#38388;&#23384;&#22312;&#30340;&#26102;&#31354;&#33258;&#30456;&#20851;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SpatialRank&#30340;&#26032;&#39062;&#31354;&#38388;&#20107;&#20214;&#25490;&#21517;&#26041;&#27861;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of urban event ranking aims at predicting the top-k most risky locations of future events such as traffic accidents and crimes. This problem is of fundamental importance to public safety and urban administration especially when limited resources are available. The problem is, however, challenging due to complex and dynamic spatio-temporal correlations between locations, uneven distribution of urban events in space, and the difficulty to correctly rank nearby locations with similar features. Prior works on event forecasting mostly aim at accurately predicting the actual risk score or counts of events for all the locations. Rankings obtained as such usually have low quality due to prediction errors. Learning-to-rank methods directly optimize measures such as Normalized Discounted Cumulative Gain (NDCG), but cannot handle the spatiotemporal autocorrelation existing among locations. In this paper, we bridge the gap by proposing a novel spatial event ranking approach named Spati
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#22411;&#26694;&#26550;&#65292;&#32467;&#21512;&#20154;&#31867;&#39118;&#38505;&#35748;&#30693;&#26469;&#29983;&#25104;&#23545;&#25163;&#39550;&#39542;&#34892;&#20026;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#26377;&#25928;&#24615;&#21644;&#24369;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.00029</link><description>&lt;p&gt;
&#34701;&#20837;&#20154;&#31867;&#39118;&#38505;&#35748;&#30693;&#30340;&#23545;&#25239;&#39550;&#39542;&#34892;&#20026;&#29983;&#25104;&#25216;&#26415;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Adversarial Driving Behavior Generation Incorporating Human Risk Cognition for Autonomous Vehicle Evaluation. (arXiv:2310.00029v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#22411;&#26694;&#26550;&#65292;&#32467;&#21512;&#20154;&#31867;&#39118;&#38505;&#35748;&#30693;&#26469;&#29983;&#25104;&#23545;&#25163;&#39550;&#39542;&#34892;&#20026;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#26377;&#25928;&#24615;&#21644;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20851;&#27880;&#20110;&#24320;&#21457;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#23545;&#25163;&#39550;&#39542;&#34892;&#20026;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#20197;&#26292;&#38706;&#20986;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#38754;&#23545;&#30340;&#26377;&#25928;&#21644;&#21512;&#29702;&#30340;&#39118;&#38505;&#20107;&#20214;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#19982;&#32047;&#31215;&#21069;&#26223;&#29702;&#35770;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23545;&#25163;&#34892;&#20026;&#65292;&#32047;&#31215;&#21069;&#26223;&#29702;&#35770;&#33021;&#22815;&#34920;&#31034;&#20154;&#31867;&#30340;&#39118;&#38505;&#35748;&#30693;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#25193;&#23637;&#29256;&#26412;&#30340;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#25216;&#26415;&#65292;&#29992;&#20110;&#35757;&#32451;&#23545;&#25163;&#31574;&#30053;&#65292;&#21516;&#26102;&#20445;&#35777;&#20102;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#12290;&#22312;&#39640;&#20445;&#30495;&#30340;&#30828;&#20214;&#22312;&#29615;&#65288;HiL&#65289;&#24179;&#21488;&#19978;&#36827;&#34892;&#20102;&#22522;&#20110;&#24182;&#32447;&#24773;&#26223;&#30340;&#23545;&#27604;&#26696;&#20363;&#30740;&#31350;&#65292;&#32467;&#26524;&#35777;&#26126;&#20102;&#23545;&#25163;&#30340;&#26377;&#25928;&#24615;&#65292;&#21487;&#20197;&#25512;&#26029;&#20986;&#34987;&#27979;&#35797;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous vehicle (AV) evaluation has been the subject of increased interest in recent years both in industry and in academia. This paper focuses on the development of a novel framework for generating adversarial driving behavior of background vehicle interfering against the AV to expose effective and rational risky events. Specifically, the adversarial behavior is learned by a reinforcement learning (RL) approach incorporated with the cumulative prospect theory (CPT) which allows representation of human risk cognition. Then, the extended version of deep deterministic policy gradient (DDPG) technique is proposed for training the adversarial policy while ensuring training stability as the CPT action-value function is leveraged. A comparative case study regarding the cut-in scenario is conducted on a high fidelity Hardware-in-the-Loop (HiL) platform and the results demonstrate the adversarial effectiveness to infer the weakness of the tested AV.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#36873;&#25321;&#27969;&#31243;&#65292;&#21033;&#29992;&#36890;&#29992;&#27169;&#22411;&#22312;&#21333;&#27425;&#25512;&#29702;&#20013;&#36873;&#25321;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#30417;&#30563;&#12290;&#36890;&#36807;&#23450;&#20041;&#21644;&#21033;&#29992;&#35821;&#20041;&#27169;&#24335;&#25552;&#21462;&#24494;&#22937;&#30340;&#23616;&#37096;&#20449;&#24687;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#25152;&#26377;&#25968;&#25454;&#26679;&#26412;&#30340;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2309.17342</link><description>&lt;p&gt;
&#12298;&#38754;&#21521;&#36890;&#29992;&#27169;&#22411;&#30340;&#33258;&#30001;&#25968;&#25454;&#36873;&#25321;&#12299;
&lt;/p&gt;
&lt;p&gt;
Towards Free Data Selection with General-Purpose Models. (arXiv:2309.17342v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#36873;&#25321;&#27969;&#31243;&#65292;&#21033;&#29992;&#36890;&#29992;&#27169;&#22411;&#22312;&#21333;&#27425;&#25512;&#29702;&#20013;&#36873;&#25321;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#30417;&#30563;&#12290;&#36890;&#36807;&#23450;&#20041;&#21644;&#21033;&#29992;&#35821;&#20041;&#27169;&#24335;&#25552;&#21462;&#24494;&#22937;&#30340;&#23616;&#37096;&#20449;&#24687;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#25152;&#26377;&#25968;&#25454;&#26679;&#26412;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#29702;&#24819;&#30340;&#25968;&#25454;&#36873;&#25321;&#31639;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#65292;&#20197;&#26368;&#22823;&#21270;&#26377;&#38480;&#30340;&#27880;&#37322;&#39044;&#31639;&#30340;&#25928;&#29992;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#65288;&#20363;&#22914;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65289;&#36890;&#24120;&#36981;&#24490;&#19968;&#20010;&#32321;&#29712;&#30340;&#27969;&#31243;&#65292;&#21453;&#22797;&#36827;&#34892;&#32791;&#26102;&#30340;&#27169;&#22411;&#35757;&#32451;&#21644;&#25209;&#37327;&#25968;&#25454;&#36873;&#25321;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#36825;&#31181;&#29616;&#29366;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#29420;&#29305;&#30340;&#25968;&#25454;&#36873;&#25321;&#27969;&#31243;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#22312;&#21333;&#27425;&#25512;&#29702;&#20013;&#36873;&#25321;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#30417;&#30563;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30001;&#25968;&#25454;&#36873;&#25321;&#65288;FreeSel&#65289;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#20010;&#26032;&#30340;&#27969;&#31243;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#20174;&#36890;&#29992;&#27169;&#22411;&#30340;&#20013;&#38388;&#29305;&#24449;&#20013;&#25552;&#21462;&#30340;&#35821;&#20041;&#27169;&#24335;&#65292;&#20197;&#25429;&#25417;&#27599;&#20010;&#22270;&#20687;&#20013;&#24494;&#22937;&#30340;&#23616;&#37096;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#36317;&#31163;&#30340;&#37319;&#26679;&#22312;&#32454;&#31890;&#24230;&#30340;&#35821;&#20041;&#27169;&#24335;&#32423;&#21035;&#19978;&#23454;&#29616;&#20102;&#23545;&#25152;&#26377;&#25968;&#25454;&#26679;&#26412;&#30340;&#36873;&#25321;&#12290;FreeSel&#32469;&#36807;&#20102;&#21407;&#26469;&#30340;&#32791;&#26102;&#35757;&#32451;&#21644;&#25209;&#37327;&#25968;&#25454;&#36873;&#25321;&#30340;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
A desirable data selection algorithm can efficiently choose the most informative samples to maximize the utility of limited annotation budgets. However, current approaches, represented by active learning methods, typically follow a cumbersome pipeline that iterates the time-consuming model training and batch data selection repeatedly. In this paper, we challenge this status quo by designing a distinct data selection pipeline that utilizes existing general-purpose models to select data from various datasets with a single-pass inference without the need for additional training or supervision. A novel free data selection (FreeSel) method is proposed following this new pipeline. Specifically, we define semantic patterns extracted from inter-mediate features of the general-purpose model to capture subtle local information in each image. We then enable the selection of all data samples in a single pass through distance-based sampling at the fine-grained semantic pattern level. FreeSel bypass
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#37327;&#21270;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#31995;&#32479;&#21487;&#38752;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#26469;&#35780;&#20272;&#25511;&#21046;&#30340;&#21487;&#38752;&#24615;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2309.16977</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#25511;&#21046;&#31995;&#32479;&#20013;&#30340;&#21487;&#38752;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Reliability Quantification of Deep Reinforcement Learning-based Control. (arXiv:2309.16977v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#37327;&#21270;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#31995;&#32479;&#21487;&#38752;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#26469;&#35780;&#20272;&#25511;&#21046;&#30340;&#21487;&#38752;&#24615;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#22312;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20013;&#30340;&#21487;&#38752;&#24615;&#37327;&#21270;&#26159;&#20154;&#24037;&#26234;&#33021;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#37327;&#21270;DRL&#25511;&#21046;&#21487;&#38752;&#24615;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#24212;&#29992;&#20102;&#19968;&#31181;&#29616;&#26377;&#26041;&#27861;&#8212;&#8212;&#38543;&#26426;&#22122;&#22768;&#25552;&#21462;&#65292;&#20197;&#26126;&#30830;&#38656;&#35201;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#38752;&#24615;&#37327;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#26469;&#37327;&#21270;&#21487;&#38752;&#24615;&#65306;&#21442;&#32771;&#32593;&#32476;&#21644;&#35780;&#20272;&#32593;&#32476;&#12290;&#23427;&#20204;&#20855;&#26377;&#30456;&#21516;&#30340;&#32467;&#26500;&#21644;&#30456;&#21516;&#30340;&#21021;&#22987;&#21442;&#25968;&#12290;&#22312;&#35757;&#32451;&#20043;&#21069;&#65292;&#20004;&#20010;&#32593;&#32476;&#30340;&#36755;&#20986;&#30456;&#21516;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#35780;&#20272;&#32593;&#32476;&#30340;&#21442;&#25968;&#34987;&#26356;&#26032;&#65292;&#20197;&#26368;&#22823;&#21270;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;&#21442;&#32771;&#32593;&#32476;&#21644;&#35780;&#20272;&#32593;&#32476;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#22522;&#20110;&#20004;&#20010;&#32593;&#32476;&#30340;&#36755;&#20986;&#24046;&#24322;&#35780;&#20272;&#29305;&#23450;&#29366;&#24577;&#19979;DRL&#25511;&#21046;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliability quantification of deep reinforcement learning (DRL)-based control is a significant challenge for the practical application of artificial intelligence (AI) in safety-critical systems. This study proposes a method for quantifying the reliability of DRL-based control. First, an existing method, random noise distillation, was applied to the reliability evaluation to clarify the issues to be solved. Second, a novel method for reliability quantification was proposed to solve these issues. The reliability is quantified using two neural networks: reference and evaluator. They have the same structure with the same initial parameters. The outputs of the two networks were the same before training. During training, the evaluator network parameters were updated to maximize the difference between the reference and evaluator networks for trained data. Thus, the reliability of the DRL-based control for a state can be evaluated based on the difference in output between the two networks. The
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ChannelViT&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;ViT&#26550;&#26500;&#30340;&#20462;&#25913;&#21644;&#24341;&#20837;&#20998;&#23618;&#36890;&#36947;&#37319;&#26679;&#25216;&#26415;&#65292;&#22686;&#24378;&#20102;&#23545;&#22810;&#36890;&#36947;&#22270;&#20687;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#26174;&#24494;&#38236;&#21644;&#21355;&#26143;&#25104;&#20687;&#31561;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2309.16108</link><description>&lt;p&gt;
&#39057;&#36947;&#35270;&#35273;Transformer&#65306;&#19968;&#24352;&#22270;&#20540;C x 16 x 16&#20010;&#35789;
&lt;/p&gt;
&lt;p&gt;
Channel Vision Transformers: An Image Is Worth C x 16 x 16 Words. (arXiv:2309.16108v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ChannelViT&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;ViT&#26550;&#26500;&#30340;&#20462;&#25913;&#21644;&#24341;&#20837;&#20998;&#23618;&#36890;&#36947;&#37319;&#26679;&#25216;&#26415;&#65292;&#22686;&#24378;&#20102;&#23545;&#22810;&#36890;&#36947;&#22270;&#20687;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#26174;&#24494;&#38236;&#21644;&#21355;&#26143;&#25104;&#20687;&#31561;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#22312;&#29616;&#20195;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#26576;&#20123;&#22270;&#20687;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#22914;&#26174;&#24494;&#38236;&#21644;&#21355;&#26143;&#25104;&#20687;&#65292;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#65292;&#22270;&#20687;&#36890;&#24120;&#21253;&#21547;&#22810;&#20010;&#36890;&#36947;&#65292;&#27599;&#20010;&#36890;&#36947;&#37117;&#25658;&#24102;&#30528;&#35821;&#20041;&#19978;&#19981;&#21516;&#21644;&#29420;&#31435;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#24517;&#39035;&#23545;&#36755;&#20837;&#36890;&#36947;&#30340;&#31232;&#30095;&#24615;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#65292;&#22312;&#35757;&#32451;&#25110;&#27979;&#35797;&#36807;&#31243;&#20013;&#21487;&#33021;&#27809;&#26377;&#23494;&#38598;&#21487;&#29992;&#30340;&#36890;&#36947;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;ViT&#26550;&#26500;&#30340;&#20462;&#25913;&#65292;&#22686;&#24378;&#20102;&#23545;&#36755;&#20837;&#36890;&#36947;&#20043;&#38388;&#30340;&#25512;&#29702;&#65292;&#24182;&#24341;&#20837;&#20102;&#20998;&#23618;&#36890;&#36947;&#37319;&#26679;(HCS)&#20316;&#20026;&#19968;&#31181;&#38468;&#21152;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#20197;&#30830;&#20445;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#20165;&#20986;&#29616;&#37096;&#20998;&#36890;&#36947;&#26102;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;ChannelViT&#29420;&#31435;&#22320;&#26500;&#24314;&#34917;&#19969;&#20196;&#29260;&#24182;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#36890;&#36947;&#23884;&#20837;&#23558;&#20854;&#28155;&#21152;&#21040;&#34917;&#19969;&#20196;&#29260;&#20013;&#65292;&#31867;&#20284;&#20110;&#20301;&#32622;&#23884;&#20837;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Vision Transformer (ViT) has emerged as a powerful architecture in the realm of modern computer vision. However, its application in certain imaging fields, such as microscopy and satellite imaging, presents unique challenges. In these domains, images often contain multiple channels, each carrying semantically distinct and independent information. Furthermore, the model must demonstrate robustness to sparsity in input channels, as they may not be densely available during training or testing. In this paper, we propose a modification to the ViT architecture that enhances reasoning across the input channels and introduce Hierarchical Channel Sampling (HCS) as an additional regularization technique to ensure robustness when only partial channels are presented during test time. Our proposed model, ChannelViT, constructs patch tokens independently from each input channel and utilizes a learnable channel embedding that is added to the patch tokens, similar to positional embeddings. We evaluate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;NeuralEF&#65292;&#19968;&#20010;&#24555;&#36895;&#30340;&#31070;&#32463;&#36924;&#36817;&#26694;&#26550;&#65292;&#33021;&#22815;&#40065;&#26834;&#22320;&#39044;&#27979;&#39640;&#25928;&#21069;&#27839;&#38382;&#39064;&#30340;&#35299;&#65292;&#21516;&#26102;&#22788;&#29702;&#24322;&#26500;&#32447;&#24615;&#32422;&#26463;&#21644;&#21487;&#21464;&#25968;&#37327;&#30340;&#20248;&#21270;&#36755;&#20837;&#12290;</title><link>http://arxiv.org/abs/2309.15775</link><description>&lt;p&gt;
&#23398;&#20064;&#39640;&#25928;&#21069;&#27839;
&lt;/p&gt;
&lt;p&gt;
Learning the Efficient Frontier. (arXiv:2309.15775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;NeuralEF&#65292;&#19968;&#20010;&#24555;&#36895;&#30340;&#31070;&#32463;&#36924;&#36817;&#26694;&#26550;&#65292;&#33021;&#22815;&#40065;&#26834;&#22320;&#39044;&#27979;&#39640;&#25928;&#21069;&#27839;&#38382;&#39064;&#30340;&#35299;&#65292;&#21516;&#26102;&#22788;&#29702;&#24322;&#26500;&#32447;&#24615;&#32422;&#26463;&#21644;&#21487;&#21464;&#25968;&#37327;&#30340;&#20248;&#21270;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#21069;&#27839;&#65288;EF&#65289;&#26159;&#19968;&#20010;&#22522;&#26412;&#30340;&#36164;&#28304;&#37197;&#32622;&#38382;&#39064;&#65292;&#22312;&#32473;&#23450;&#39118;&#38505;&#27700;&#24179;&#19979;&#23547;&#25214;&#26368;&#20248;&#25237;&#36164;&#32452;&#21512;&#20197;&#26368;&#22823;&#21270;&#25910;&#30410;&#12290;&#20256;&#32479;&#19978;&#65292;&#36890;&#36807;&#35299;&#19968;&#20010;&#20984;&#20248;&#21270;&#38382;&#39064;&#26469;&#25214;&#21040;&#26368;&#20248;&#35299;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;NeuralEF&#65306;&#19968;&#20010;&#24555;&#36895;&#30340;&#31070;&#32463;&#36924;&#36817;&#26694;&#26550;&#65292;&#21487;&#20197;&#40065;&#26834;&#22320;&#39044;&#27979;&#30456;&#23545;&#24322;&#26500;&#32447;&#24615;&#32422;&#26463;&#21644;&#21487;&#21464;&#25968;&#37327;&#30340;&#20248;&#21270;&#36755;&#20837;&#30340;EF&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#23558;&#20248;&#21270;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#24207;&#21015;&#21040;&#24207;&#21015;&#38382;&#39064;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;NeuralEF&#26159;&#21152;&#36895;&#22823;&#35268;&#27169;&#27169;&#25311;&#24182;&#22788;&#29702;&#19981;&#36830;&#32493;&#34892;&#20026;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The efficient frontier (EF) is a fundamental resource allocation problem where one has to find an optimal portfolio maximizing a reward at a given level of risk. This optimal solution is traditionally found by solving a convex optimization problem. In this paper, we introduce NeuralEF: a fast neural approximation framework that robustly forecasts the result of the EF convex optimization problem with respect to heterogeneous linear constraints and variable number of optimization inputs. By reformulating an optimization problem as a sequence to sequence problem, we show that NeuralEF is a viable solution to accelerate large-scale simulation while handling discontinuous behavior.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#22270;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#30340;&#34920;&#31034;&#26469;&#25429;&#25417;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#23454;&#29616;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#30693;&#35782;&#30340;&#26377;&#25928;&#34701;&#21512;&#12290;&#22312;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.15757</link><description>&lt;p&gt;
&#22522;&#20110;&#28508;&#22312;&#22270;&#30340;&#29983;&#29289;&#21307;&#23398;&#34920;&#26684;&#25968;&#25454;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Latent Graph Powered Semi-Supervised Learning on Biomedical Tabular Data. (arXiv:2309.15757v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#22270;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#30340;&#34920;&#31034;&#26469;&#25429;&#25417;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#23454;&#29616;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#30693;&#35782;&#30340;&#26377;&#25928;&#34701;&#21512;&#12290;&#22312;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#29616;&#26377;&#26041;&#27861;&#26410;&#20805;&#20998;&#21033;&#29992;&#65288;&#26377;&#65289;&#26631;&#35760;&#25968;&#25454;&#20043;&#38388;&#30340;&#23454;&#20363;&#38388;&#20851;&#31995;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20379;&#19968;&#31181;&#25512;&#26029;&#25429;&#25417;&#20869;&#22312;&#25968;&#25454;&#20851;&#31995;&#30340;&#28508;&#22312;&#22270;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20419;&#36827;&#20102;&#20449;&#24687;&#22312;&#25972;&#20010;&#22270;&#20013;&#30340;&#26080;&#32541;&#20256;&#25773;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#34701;&#21512;&#20840;&#23616;&#21644;&#23616;&#37096;&#30693;&#35782;&#12290;&#36890;&#36807;&#22312;&#29983;&#29289;&#21307;&#23398;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20854;&#20182;&#24403;&#20195;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#35777;&#26126;&#20102;&#21457;&#29616;&#23454;&#20363;&#38388;&#20851;&#31995;&#20316;&#20026;&#26500;&#24314;&#24378;&#21270;&#21322;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#30340;&#40065;&#26834;&#28508;&#22312;&#22270;&#30340;&#23454;&#38469;&#25163;&#27573;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the domain of semi-supervised learning, the current approaches insufficiently exploit the potential of considering inter-instance relationships among (un)labeled data. In this work, we address this limitation by providing an approach for inferring latent graphs that capture the intrinsic data relationships. By leveraging graph-based representations, our approach facilitates the seamless propagation of information throughout the graph, enabling the effective incorporation of global and local knowledge. Through evaluations on biomedical tabular datasets, we compare the capabilities of our approach to other contemporary methods. Our work demonstrates the significance of inter-instance relationship discovery as practical means for constructing robust latent graphs to enhance semi-supervised learning techniques. Our method achieves state-of-the-art results on three biomedical datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#24320;&#28304;&#22522;&#20934;&#27979;&#35797;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#20462;&#27491;&#65292;&#23454;&#29616;&#20102;&#19982;&#20154;&#31867;&#27700;&#24179;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.15701</link><description>&lt;p&gt;
HyPoradise&#65306;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#24335;&#35821;&#38899;&#35782;&#21035;&#30340;&#24320;&#25918;&#22522;&#20934;&#32447;
&lt;/p&gt;
&lt;p&gt;
HyPoradise: An Open Baseline for Generative Speech Recognition with Large Language Models. (arXiv:2309.15701v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#24320;&#28304;&#22522;&#20934;&#27979;&#35797;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#20462;&#27491;&#65292;&#23454;&#29616;&#20102;&#19982;&#20154;&#31867;&#27700;&#24179;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36827;&#23637;&#20351;&#24471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#22312;&#20960;&#20010;&#20844;&#24320;&#30340;&#24178;&#20928;&#35821;&#38899;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#20154;&#31867;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#22312;&#38754;&#23545;&#36870;&#22659;&#26102;&#20063;&#20250;&#20986;&#29616;&#24615;&#33021;&#19979;&#38477;&#65292;&#22240;&#20026;&#33391;&#22909;&#35757;&#32451;&#30340;&#22768;&#23398;&#27169;&#22411;&#23545;&#20110;&#35821;&#38899;&#39046;&#22495;&#30340;&#21464;&#24322;&#24615;&#24456;&#25935;&#24863;&#65292;&#22914;&#32972;&#26223;&#22122;&#22768;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#24320;&#28304;&#22522;&#20934;&#27979;&#35797;&#65292;&#21033;&#29992;&#22806;&#37096;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#36827;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#20462;&#27491;&#65292;&#20854;&#20013;N&#26368;&#20339;&#35299;&#30721;&#20551;&#35774;&#20026;&#30495;&#23454;&#36716;&#24405;&#39044;&#27979;&#25552;&#20379;&#20102;&#26377;&#20449;&#24687;&#37327;&#30340;&#20803;&#32032;&#12290;&#36825;&#31181;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#35821;&#35328;&#27169;&#22411;&#37325;&#35780;&#20998;&#31574;&#30053;&#19981;&#21516;&#65292;&#21518;&#32773;&#21482;&#33021;&#36873;&#25321;&#19968;&#20010;&#20505;&#36873;&#20551;&#35774;&#20316;&#20026;&#26368;&#32456;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in deep neural networks have allowed automatic speech recognition (ASR) systems to attain human parity on several publicly available clean speech datasets. However, even state-of-the-art ASR systems experience performance degradation when confronted with adverse conditions, as a well-trained acoustic model is sensitive to variations in the speech domain, e.g., background noise. Intuitively, humans address this issue by relying on their linguistic knowledge: the meaning of ambiguous spoken terms is usually inferred from contextual cues thereby reducing the dependency on the auditory system. Inspired by this observation, we introduce the first open-source benchmark to utilize external large language models (LLMs) for ASR error correction, where N-best decoding hypotheses provide informative elements for true transcription prediction. This approach is a paradigm shift from the traditional language model rescoring strategy that can only select one candidate hypothesis as the o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#37327;&#23376;&#21551;&#21457;&#30340;&#20108;&#32500;&#24352;&#37327;&#32593;&#32476;&#39640;&#25928;&#27169;&#25311;IBM&#26368;&#22823;&#30340;&#37327;&#23376;&#22788;&#29702;&#22120;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#24352;&#37327;&#26356;&#26032;&#23454;&#29616;&#21069;&#25152;&#26410;&#26377;&#30340;&#20934;&#30830;&#24230;&#21644;&#26497;&#20302;&#30340;&#35745;&#31639;&#36164;&#28304;&#28040;&#32791;&#65292;&#24182;&#20026;&#26368;&#26032;&#30340;IBM&#37327;&#23376;&#26426;&#22120;&#35774;&#31435;&#20102;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2309.15642</link><description>&lt;p&gt;
IBM&#26368;&#22823;&#37327;&#23376;&#22788;&#29702;&#22120;&#30340;&#24352;&#37327;&#32593;&#32476;&#27169;&#25311;&#30340;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient tensor network simulation of IBM's largest quantum processors. (arXiv:2309.15642v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#37327;&#23376;&#21551;&#21457;&#30340;&#20108;&#32500;&#24352;&#37327;&#32593;&#32476;&#39640;&#25928;&#27169;&#25311;IBM&#26368;&#22823;&#30340;&#37327;&#23376;&#22788;&#29702;&#22120;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#24352;&#37327;&#26356;&#26032;&#23454;&#29616;&#21069;&#25152;&#26410;&#26377;&#30340;&#20934;&#30830;&#24230;&#21644;&#26497;&#20302;&#30340;&#35745;&#31639;&#36164;&#28304;&#28040;&#32791;&#65292;&#24182;&#20026;&#26368;&#26032;&#30340;IBM&#37327;&#23376;&#26426;&#22120;&#35774;&#31435;&#20102;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#37327;&#23376;&#21551;&#21457;&#30340;&#20108;&#32500;&#24352;&#37327;&#32593;&#32476;&#26469;&#39640;&#25928;&#20934;&#30830;&#22320;&#27169;&#25311;IBM&#26368;&#22823;&#30340;&#37327;&#23376;&#22788;&#29702;&#22120;&#65292;&#21363;Eagle&#65288;127&#20010;&#37327;&#23376;&#27604;&#29305;&#65289;&#65292;Osprey&#65288;433&#20010;&#37327;&#23376;&#27604;&#29305;&#65289;&#21644;Condor&#65288;1121&#20010;&#37327;&#23376;&#27604;&#29305;&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#25237;&#24433;&#32416;&#32544;&#23545;&#24577;&#65288;gPEPS&#65289;&#27169;&#25311;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#37327;&#23376;&#22810;&#20307;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#65292;&#20855;&#20307;&#26469;&#35828;&#65292;&#36825;&#26159;IBM&#26368;&#36817;&#22312;Nature 618&#24180;&#31532;500-505&#39029;&#65288;2023&#24180;&#65289;&#19978;&#32771;&#34385;&#30340;&#36386;&#20987;&#26131;&#36763;&#23454;&#39564;-&#25105;&#20204;&#22312;PRB 99, 195105&#65288;2019&#24180;&#65289;&#20013;&#25552;&#20986;&#20102;&#36825;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#35813;&#27169;&#22411;&#65292;&#31616;&#21333;&#30340;&#24352;&#37327;&#26356;&#26032;&#24050;&#32463;&#36275;&#20197;&#20197;&#26497;&#20302;&#30340;&#35745;&#31639;&#36164;&#28304;&#23454;&#29616;&#38750;&#24120;&#22823;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#31934;&#24230;&#12290;&#38500;&#20102;&#27169;&#25311;127&#20010;&#37327;&#23376;&#27604;&#29305;&#30340;&#21407;&#22987;&#23454;&#39564;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#32467;&#26524;&#25193;&#23637;&#21040;433&#20010;&#21644;1121&#20010;&#37327;&#23376;&#27604;&#29305;&#65292;&#20174;&#32780;&#20026;&#26368;&#26032;&#30340;IBM&#37327;&#23376;&#26426;&#22120;&#35774;&#23450;&#20102;&#19968;&#20010;&#22522;&#20934;&#12290;&#25105;&#20204;&#36824;&#25253;&#36947;&#20102;&#26080;&#38480;&#22810;&#20010;&#37327;&#23376;&#27604;&#29305;&#30340;&#20934;&#30830;&#27169;&#25311;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;gPEPS&#26159;&#39640;&#25928;&#27169;&#25311;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#33258;&#28982;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show how quantum-inspired 2d tensor networks can be used to efficiently and accurately simulate the largest quantum processors from IBM, namely Eagle (127 qubits), Osprey (433 qubits) and Condor (1121 qubits). We simulate the dynamics of a complex quantum many-body system -- specifically, the kicked Ising experiment considered recently by IBM in Nature 618, p. 500-505 (2023) -using graph-based Projected Entangled Pair States (gPEPS), which was proposed by some of us in PRB 99, 195105 (2019). Our results show that simple tensor updates are already sufficient to achieve very large unprecedented accuracy with remarkably low computational resources for this model. Apart from simulating the original experiment for 127 qubits, we also extend our results to 433 and 1121 qubits, thus setting a benchmark for the newest IBM quantum machines. We also report accurate simulations for infinitely-many qubits. Our results show that gPEPS are a natural tool to efficiently simulate quantum computer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#20989;&#25968;&#30340;&#21452;&#37325;&#31283;&#20581;&#36817;&#31471;&#22240;&#26524;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#36830;&#32493;&#27835;&#30103;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#27714;&#35299;&#24178;&#25200;&#20989;&#25968;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.12819</link><description>&lt;p&gt;
&#36830;&#32493;&#27835;&#30103;&#30340;&#21452;&#37325;&#31283;&#20581;&#36817;&#31471;&#22240;&#26524;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Doubly Robust Proximal Causal Learning for Continuous Treatments. (arXiv:2309.12819v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#20989;&#25968;&#30340;&#21452;&#37325;&#31283;&#20581;&#36817;&#31471;&#22240;&#26524;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#36830;&#32493;&#27835;&#30103;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#27714;&#35299;&#24178;&#25200;&#20989;&#25968;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#31471;&#22240;&#26524;&#23398;&#20064;&#26159;&#22312;&#23384;&#22312;&#26410;&#27979;&#37327;&#28151;&#28102;&#22240;&#32032;&#19979;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#30340;&#26377;&#24076;&#26395;&#30340;&#26694;&#26550;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#34917;&#20805;&#31283;&#20581;&#65288;DR&#65289;&#20272;&#35745;&#22120;&#34987;&#25512;&#23548;&#20986;&#26469;&#65292;&#24182;&#22312;&#20272;&#35745;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#27169;&#22411;&#20551;&#35774;&#34987;&#36829;&#21453;&#26102;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#24418;&#24335;&#30340;DR&#20272;&#35745;&#22120;&#20165;&#38480;&#20110;&#20108;&#36827;&#21046;&#27835;&#30103;&#65292;&#32780;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#27835;&#30103;&#21487;&#20197;&#26159;&#36830;&#32493;&#30340;&#12290;&#36830;&#32493;&#27835;&#30103;&#30340;&#20027;&#35201;&#38556;&#30861;&#22312;&#20110;&#22312;&#21407;&#22987;DR&#20272;&#35745;&#22120;&#20013;&#23384;&#22312;&#30340;delta&#20989;&#25968;&#65292;&#20351;&#24471;&#22312;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#20013;&#19981;&#21487;&#34892;&#65292;&#24182;&#22312;&#24178;&#25200;&#20989;&#25968;&#20272;&#35745;&#20013;&#24341;&#20837;&#20102;&#27785;&#37325;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#20989;&#25968;&#30340;&#36830;&#32493;&#27835;&#30103;&#30340;DR&#20272;&#35745;&#22120;&#65292;&#21487;&#20197;&#24456;&#22909;&#22320;&#22788;&#29702;&#36830;&#32493;&#27835;&#30103;&#12290;&#37197;&#22791;&#20854;&#24179;&#28369;&#24615;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20854;Oracle&#24418;&#24335;&#26159;&#24433;&#21709;&#20989;&#25968;&#30340;&#19968;&#33268;&#36817;&#20284;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#39640;&#25928;&#35299;&#20915;&#24178;&#25200;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proximal causal learning is a promising framework for identifying the causal effect under the existence of unmeasured confounders. Within this framework, the doubly robust (DR) estimator was derived and has shown its effectiveness in estimation, especially when the model assumption is violated. However, the current form of the DR estimator is restricted to binary treatments, while the treatment can be continuous in many real-world applications. The primary obstacle to continuous treatments resides in the delta function present in the original DR estimator, making it infeasible in causal effect estimation and introducing a heavy computational burden in nuisance function estimation. To address these challenges, we propose a kernel-based DR estimator that can well handle continuous treatments. Equipped with its smoothness, we show that its oracle form is a consistent approximation of the influence function. Further, we propose a new approach to efficiently solve the nuisance functions. We
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Google&#30340;Bard&#22312;&#23545;&#25239;&#22270;&#20687;&#25915;&#20987;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#21457;&#29616;&#23427;&#21487;&#20197;&#34987;&#25915;&#20987;&#20197;&#36755;&#20986;&#38169;&#35823;&#30340;&#22270;&#20687;&#25551;&#36848;&#12290;&#36825;&#19968;&#25915;&#20987;&#36824;&#21487;&#20197;&#23545;&#20854;&#20182;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#24433;&#21709;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;Bard&#30340;&#20004;&#31181;&#38450;&#24481;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.11751</link><description>&lt;p&gt;
Google&#30340;Bard&#22312;&#23545;&#25239;&#22270;&#20687;&#25915;&#20987;&#26041;&#38754;&#26377;&#22810;&#24378;&#22823;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Robust is Google's Bard to Adversarial Image Attacks?. (arXiv:2309.11751v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Google&#30340;Bard&#22312;&#23545;&#25239;&#22270;&#20687;&#25915;&#20987;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#21457;&#29616;&#23427;&#21487;&#20197;&#34987;&#25915;&#20987;&#20197;&#36755;&#20986;&#38169;&#35823;&#30340;&#22270;&#20687;&#25551;&#36848;&#12290;&#36825;&#19968;&#25915;&#20987;&#36824;&#21487;&#20197;&#23545;&#20854;&#20182;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#24433;&#21709;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;Bard&#30340;&#20004;&#31181;&#38450;&#24481;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#25991;&#26412;&#21644;&#20854;&#20182;&#27169;&#24577;&#65288;&#23588;&#20854;&#26159;&#35270;&#35273;&#65289;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#22312;&#21508;&#31181;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35270;&#35273;&#27169;&#22411;&#30340;&#26410;&#35299;&#20915;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#24341;&#20837;&#35270;&#35273;&#36755;&#20837;&#21487;&#33021;&#20351;MLLM&#38754;&#20020;&#26356;&#20005;&#37325;&#30340;&#23433;&#20840;&#39118;&#38505;&#21644;&#23433;&#20840;&#39118;&#38505;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Google&#30340;Bard&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#23427;&#26159;&#19968;&#20010;&#31454;&#20105;&#24615;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#26368;&#36817;&#21457;&#24067;&#20102;&#20854;&#22810;&#27169;&#24577;&#33021;&#21147;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#21830;&#19994;MLLM&#30340;&#28431;&#27934;&#12290;&#36890;&#36807;&#25915;&#20987;&#30333;&#30418;&#23376;&#20195;&#29702;&#35270;&#35273;&#32534;&#30721;&#22120;&#25110;MLLM&#65292;&#29983;&#25104;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#21487;&#20197;&#20351;Bard&#20197;22&#65285;&#30340;&#25104;&#21151;&#29575;&#20165;&#22522;&#20110;&#21487;&#36716;&#31227;&#24615;&#36755;&#20986;&#38169;&#35823;&#30340;&#22270;&#20687;&#25551;&#36848;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#23545;&#25239;&#24615;&#31034;&#20363;&#36824;&#21487;&#20197;&#25915;&#20987;&#20854;&#20182;MLLM&#65292;&#20363;&#22914;&#65292;&#23545;Bing Chat&#30340;&#25104;&#21151;&#25915;&#20987;&#29575;&#20026;26&#65285;&#65292;&#23545;ERNIE bot&#30340;&#25104;&#21151;&#25915;&#20987;&#29575;&#20026;86&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;Bard&#30340;&#20004;&#31181;&#38450;&#24481;&#26426;&#21046;&#65292;&#21253;&#25324;&#38754;&#37096;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Large Language Models (MLLMs) that integrate text and other modalities (especially vision) have achieved unprecedented performance in various multimodal tasks. However, due to the unsolved adversarial robustness problem of vision models, MLLMs can have more severe safety and security risks by introducing the vision inputs. In this work, we study the adversarial robustness of Google's Bard, a competitive chatbot to ChatGPT that released its multimodal capability recently, to better understand the vulnerabilities of commercial MLLMs. By attacking white-box surrogate vision encoders or MLLMs, the generated adversarial examples can mislead Bard to output wrong image descriptions with a 22% success rate based solely on the transferability. We show that the adversarial examples can also attack other MLLMs, e.g., a 26% attack success rate against Bing Chat and a 86% attack success rate against ERNIE bot. Moreover, we identify two defense mechanisms of Bard, including face detection
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20449;&#24687;&#20934;&#21017;&#30340;&#20013;&#38388;&#20840;&#23616;&#27169;&#22411;Clustered FedStack&#12290;&#36890;&#36807;&#23558;&#26412;&#22320;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#39044;&#27979;&#21644;&#36755;&#20986;&#23618;&#26435;&#37325;&#21457;&#36865;&#21040;&#26381;&#21153;&#22120;&#65292;&#26500;&#24314;&#19968;&#20010;&#24378;&#22823;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#26426;&#21046;&#23545;&#26412;&#22320;&#23458;&#25143;&#36827;&#34892;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2309.11044</link><description>&lt;p&gt;
Clustered FedStack&#65306;&#22522;&#20110;&#36125;&#21494;&#26031;&#20449;&#24687;&#20934;&#21017;&#30340;&#20013;&#38388;&#20840;&#23616;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clustered FedStack: Intermediate Global Models with Bayesian Information Criterion. (arXiv:2309.11044v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11044
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20449;&#24687;&#20934;&#21017;&#30340;&#20013;&#38388;&#20840;&#23616;&#27169;&#22411;Clustered FedStack&#12290;&#36890;&#36807;&#23558;&#26412;&#22320;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#39044;&#27979;&#21644;&#36755;&#20986;&#23618;&#26435;&#37325;&#21457;&#36865;&#21040;&#26381;&#21153;&#22120;&#65292;&#26500;&#24314;&#19968;&#20010;&#24378;&#22823;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#26426;&#21046;&#23545;&#26412;&#22320;&#23458;&#25143;&#36827;&#34892;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#30446;&#21069;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#26368;&#21463;&#27426;&#36814;&#30340;&#25216;&#26415;&#20043;&#19968;&#65292;&#22240;&#20854;&#21327;&#20316;&#23398;&#20064;&#21644;&#20445;&#25252;&#23458;&#25143;&#38544;&#31169;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;&#23427;&#38754;&#20020;&#38750;&#29420;&#31435;&#21644;&#38750;&#29420;&#31435;&#20998;&#24067;&#65288;&#38750;IID&#65289;&#20197;&#21450;&#26412;&#22320;&#23458;&#25143;&#20043;&#38388;&#26631;&#31614;&#19981;&#24179;&#34913;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#30740;&#31350;&#22242;&#38431;&#25506;&#32034;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#22914;&#20351;&#29992;&#26412;&#22320;&#27169;&#22411;&#21442;&#25968;&#12289;&#32852;&#37030;&#29983;&#25104;&#23545;&#25239;&#23398;&#20064;&#21644;&#32852;&#37030;&#34920;&#31034;&#23398;&#20064;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24050;&#21457;&#34920;&#30340;Stacked Federated Learning&#65288;FedStack&#65289;&#26694;&#26550;&#30340;&#26032;&#39062;Clustered FedStack&#26694;&#26550;&#12290;&#26412;&#22320;&#23458;&#25143;&#31471;&#23558;&#20854;&#27169;&#22411;&#39044;&#27979;&#21644;&#36755;&#20986;&#23618;&#26435;&#37325;&#21457;&#36865;&#21040;&#26381;&#21153;&#22120;&#65292;&#28982;&#21518;&#26500;&#24314;&#19968;&#20010;&#24378;&#22823;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#36825;&#20010;&#20840;&#23616;&#27169;&#22411;&#20351;&#29992;&#32858;&#31867;&#26426;&#21046;&#22522;&#20110;&#20854;&#36755;&#20986;&#23618;&#26435;&#37325;&#23545;&#26412;&#22320;&#23458;&#25143;&#36827;&#34892;&#32858;&#31867;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19977;&#31181;&#32858;&#31867;&#26426;&#21046;&#65292;&#20998;&#21035;&#26159;K-Means&#12289;Agglomerative&#12289;DBSCAN&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is currently one of the most popular technologies in the field of Artificial Intelligence (AI) due to its collaborative learning and ability to preserve client privacy. However, it faces challenges such as non-identically and non-independently distributed (non-IID) and data with imbalanced labels among local clients. To address these limitations, the research community has explored various approaches such as using local model parameters, federated generative adversarial learning, and federated representation learning. In our study, we propose a novel Clustered FedStack framework based on the previously published Stacked Federated Learning (FedStack) framework. The local clients send their model predictions and output layer weights to a server, which then builds a robust global model. This global model clusters the local clients based on their output layer weights using a clustering mechanism. We adopt three clustering mechanisms, namely K-Means, Agglomerative, a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#22797;&#21046;&#24182;&#37325;&#29616;&#20102;&#19968;&#39033;&#20808;&#21069;&#30340;&#27969;&#37327;&#20998;&#31867;&#30740;&#31350;&#65292;&#35813;&#30740;&#31350;&#20351;&#29992;&#20102;&#23545;&#27604;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;Flowpic&#36755;&#20837;&#34920;&#31034;&#33021;&#22815;&#22312;&#20165;&#26377;100&#20010;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.09733</link><description>&lt;p&gt;
&#22797;&#21046;&#65306;&#22312;&#27969;&#37327;&#20998;&#31867;&#20013;&#20351;&#29992;Flowpic&#36755;&#20837;&#34920;&#31034;&#30340;&#23545;&#27604;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Replication: Contrastive Learning and Data Augmentation in Traffic Classification Using a Flowpic Input Representation. (arXiv:2309.09733v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09733
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#22797;&#21046;&#24182;&#37325;&#29616;&#20102;&#19968;&#39033;&#20808;&#21069;&#30340;&#27969;&#37327;&#20998;&#31867;&#30740;&#31350;&#65292;&#35813;&#30740;&#31350;&#20351;&#29992;&#20102;&#23545;&#27604;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;Flowpic&#36755;&#20837;&#34920;&#31034;&#33021;&#22815;&#22312;&#20165;&#26377;100&#20010;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#37324;&#65292;&#30001;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20852;&#36215;&#65292;&#25105;&#20204;&#35265;&#35777;&#20102;&#23545;&#27969;&#37327;&#20998;&#31867;&#65288;TC&#65289;&#30340;&#37325;&#26032;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#32477;&#22823;&#37096;&#20998; TC &#25991;&#29486;&#32570;&#20047;&#20195;&#30721;&#24037;&#20214;&#12289;&#36328;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#35780;&#20272;&#20197;&#21450;&#19982;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#30340;&#21442;&#32771;&#27604;&#36739;&#12290;&#20854;&#20013;&#65292;IMC22[16]&#30340;&#19968;&#39033;&#26368;&#36817;&#30740;&#31350;&#20540;&#24471;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#37319;&#29992;&#20102;&#26368;&#36817;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65288;&#21363;&#23569;&#26679;&#26412;&#23398;&#20064;&#12289;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#65289;&#65292;&#36825;&#23545;&#20110;&#32593;&#32476;&#26469;&#35828;&#38750;&#24120;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#20174;&#23569;&#37327;&#26679;&#26412;&#20013;&#23398;&#20064;&#24182;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#20043;&#38388;&#20256;&#36755;&#12290;[16]&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#65292;&#22312;UCDAVIS19&#12289;ISCX-VPN&#21644;ISCX-Tor&#25968;&#25454;&#38598;&#19978;&#65292;&#20351;&#29992;&#36825;&#20123;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21482;&#38656;100&#20010;&#36755;&#20837;&#26679;&#26412;&#23601;&#21487;&#20197;&#23454;&#29616;&#38750;&#24120;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#20351;&#29992;&#30340;&#36755;&#20837;&#34920;&#31034;&#21483;&#20570;&#8220;flowpic&#8221;&#65288;&#21363;&#22312;&#26102;&#38388;&#19978;&#28436;&#21464;&#30340;&#25968;&#25454;&#21253;&#22823;&#23567;&#30340;&#20998;&#25351;&#26631;&#30340;&#20108;&#32500;&#30452;&#26041;&#22270;&#65289;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;(i)&#22312;&#30456;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#37325;&#29616;[16]&#30340;&#32467;&#26524;&#65292;&#20197;&#21450;(ii)&#22797;&#21046;&#20854;&#26368;&#26174;&#33879;&#30340;&#26041;&#38754;&#65288;&#21363;&#23545;&#27604;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last years we witnessed a renewed interest toward Traffic Classification (TC) captivated by the rise of Deep Learning (DL). Yet, the vast majority of TC literature lacks code artifacts, performance assessments across datasets and reference comparisons against Machine Learning (ML) methods. Among those works, a recent study from IMC22 [16] is worth of attention since it adopts recent DL methodologies (namely, few-shot learning, self-supervision via contrastive learning and data augmentation) appealing for networking as they enable to learn from a few samples and transfer across datasets. The main result of [16] on the UCDAVIS19, ISCX-VPN and ISCX-Tor datasets is that, with such DL methodologies, 100 input samples are enough to achieve very high accuracy using an input representation called "flowpic" (i.e., a per-flow 2d histograms of the packets size evolution over time). In this paper (i) we reproduce [16] on the same datasets and (ii) we replicate its most salient aspect (the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#26377;&#30028;&#26356;&#26032;&#30340;&#36845;&#20195;&#23398;&#20064;&#31639;&#27861;&#22312;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#19978;&#30340;&#27867;&#21270;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#65292;&#21033;&#29992;&#20102;&#20449;&#24687;&#35770;&#25216;&#26415;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#27169;&#22411;&#32500;&#24230;&#21644;&#35757;&#32451;&#25968;&#25454;&#26679;&#26412;&#25968;&#37327;&#30456;&#31561;&#30340;&#24773;&#20917;&#19979;&#65292;&#30028;&#38480;&#24471;&#21040;&#20102;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2309.05077</link><description>&lt;p&gt;
&#20855;&#26377;&#26377;&#30028;&#26356;&#26032;&#30340;&#36845;&#20195;&#23398;&#20064;&#31639;&#27861;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Generalization error bounds for iterative learning algorithms with bounded updates. (arXiv:2309.05077v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#26377;&#30028;&#26356;&#26032;&#30340;&#36845;&#20195;&#23398;&#20064;&#31639;&#27861;&#22312;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#19978;&#30340;&#27867;&#21270;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#65292;&#21033;&#29992;&#20102;&#20449;&#24687;&#35770;&#25216;&#26415;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#27169;&#22411;&#32500;&#24230;&#21644;&#35757;&#32451;&#25968;&#25454;&#26679;&#26412;&#25968;&#37327;&#30456;&#31561;&#30340;&#24773;&#20917;&#19979;&#65292;&#30028;&#38480;&#24471;&#21040;&#20102;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20855;&#26377;&#26377;&#30028;&#26356;&#26032;&#30340;&#36845;&#20195;&#23398;&#20064;&#31639;&#27861;&#22312;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#19978;&#30340;&#27867;&#21270;&#29305;&#24615;&#65292;&#37319;&#29992;&#20102;&#20449;&#24687;&#35770;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#38024;&#23545;&#20855;&#26377;&#26377;&#30028;&#26356;&#26032;&#30340;&#31639;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#65292;&#36229;&#20986;&#20102;&#20197;&#21069;&#21482;&#20851;&#27880;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#33539;&#22260;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#20004;&#20010;&#20027;&#35201;&#30340;&#21019;&#26032;&#20043;&#22788;&#65306;1&#65289;&#25105;&#20204;&#23558;&#20114;&#20449;&#24687;&#37325;&#26032;&#23450;&#20041;&#20026;&#26356;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65307;2&#65289;&#25105;&#20204;&#19981;&#20351;&#29992;&#20114;&#20449;&#24687;&#30340;&#38142;&#24335;&#27861;&#21017;&#65292;&#32780;&#26159;&#37319;&#29992;&#26041;&#24046;&#20998;&#35299;&#25216;&#26415;&#26469;&#23558;&#20449;&#24687;&#20998;&#35299;&#21040;&#36845;&#20195;&#20013;&#65292;&#20174;&#32780;&#20801;&#35768;&#31616;&#21270;&#30340;&#20195;&#29702;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#20998;&#26512;&#20102;&#25105;&#20204;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#24182;&#22312;&#27169;&#22411;&#32500;&#24230;&#20197;&#19982;&#35757;&#32451;&#25968;&#25454;&#26679;&#26412;&#25968;&#37327;&#30456;&#21516;&#30340;&#36895;&#29575;&#22686;&#21152;&#26102;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#30028;&#38480;&#12290;&#20026;&#20102;&#24357;&#21512;&#29702;&#35770;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20808;&#21069;&#35266;&#23519;&#21040;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the generalization characteristics of iterative learning algorithms with bounded updates for non-convex loss functions, employing information-theoretic techniques. Our key contribution is a novel bound for the generalization error of these algorithms with bounded updates, extending beyond the scope of previous works that only focused on Stochastic Gradient Descent (SGD). Our approach introduces two main novelties: 1) we reformulate the mutual information as the uncertainty of updates, providing a new perspective, and 2) instead of using the chaining rule of mutual information, we employ a variance decomposition technique to decompose information across iterations, allowing for a simpler surrogate process. We analyze our generalization bound under various settings and demonstrate improved bounds when the model dimension increases at the same rate as the number of training data samples. To bridge the gap between theory and practice, we also examine the previously obse
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#30740;&#32508;&#21512;&#23457;&#26597;&#20102;&#22312;&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;&#20013;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#21253;&#25324;&#23545;&#30693;&#35782;&#36319;&#36394;&#12289;&#23398;&#29983;&#19981;&#33391;&#34892;&#20026;&#26816;&#27979;&#12289;&#24615;&#33021;&#39044;&#27979;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#31561;&#20856;&#22411;&#25945;&#32946;&#22330;&#26223;&#30340;&#24212;&#29992;&#12290;&#21516;&#26102;&#25552;&#20379;&#20102;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#22788;&#29702;&#24037;&#20855;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.04761</link><description>&lt;p&gt;
&#22312;&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;&#20013;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#32508;&#21512;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Deep Learning Techniques in Educational Data Mining. (arXiv:2309.04761v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#32508;&#21512;&#23457;&#26597;&#20102;&#22312;&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;&#20013;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#21253;&#25324;&#23545;&#30693;&#35782;&#36319;&#36394;&#12289;&#23398;&#29983;&#19981;&#33391;&#34892;&#20026;&#26816;&#27979;&#12289;&#24615;&#33021;&#39044;&#27979;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#31561;&#20856;&#22411;&#25945;&#32946;&#22330;&#26223;&#30340;&#24212;&#29992;&#12290;&#21516;&#26102;&#25552;&#20379;&#20102;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#22788;&#29702;&#24037;&#20855;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;(EDM)&#20316;&#20026;&#30740;&#31350;&#30340;&#37325;&#35201;&#39046;&#22495;&#65292;&#21033;&#29992;&#35745;&#31639;&#25216;&#26415;&#26469;&#20998;&#26512;&#25945;&#32946;&#25968;&#25454;&#12290;&#38543;&#30528;&#25945;&#32946;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#22686;&#21152;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#35299;&#20915;&#20998;&#26512;&#21644;&#24314;&#27169;&#36825;&#20123;&#25968;&#25454;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;&#26412;&#35843;&#30740;&#26088;&#22312;&#31995;&#32479;&#22320;&#23457;&#26597;&#28145;&#24230;&#23398;&#20064;&#22312;EDM&#39046;&#22495;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#20851;&#20110;EDM&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#31616;&#35201;&#20171;&#32461;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#29616;&#20195;&#25945;&#32946;&#29615;&#22659;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35814;&#32454;&#22238;&#39038;&#20102;&#22312;&#22235;&#20010;&#20856;&#22411;&#25945;&#32946;&#22330;&#26223;&#20013;&#24212;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#21253;&#25324;&#30693;&#35782;&#36319;&#36394;&#12289;&#23398;&#29983;&#19981;&#33391;&#34892;&#20026;&#26816;&#27979;&#12289;&#24615;&#33021;&#39044;&#27979;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;EDM&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#22788;&#29702;&#24037;&#20855;&#30340;&#32508;&#21512;&#27010;&#36848;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;&#35813;&#30740;&#31350;&#39046;&#22495;&#30340;&#26032;&#20852;&#36235;&#21183;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Educational Data Mining (EDM) has emerged as a vital field of research, which harnesses the power of computational techniques to analyze educational data. With the increasing complexity and diversity of educational data, Deep Learning techniques have shown significant advantages in addressing the challenges associated with analyzing and modeling this data. This survey aims to systematically review the state-of-the-art in EDM with Deep Learning. We begin by providing a brief introduction to EDM and Deep Learning, highlighting their relevance in the context of modern education. Next, we present a detailed review of Deep Learning techniques applied in four typical educational scenarios, including knowledge tracing, undesirable student detecting, performance prediction, and personalized recommendation. Furthermore, a comprehensive overview of public datasets and processing tools for EDM is provided. Finally, we point out emerging trends and future directions in this research area.
&lt;/p&gt;</description></item><item><title>RLSynC&#26159;&#19968;&#31181;&#31163;&#32447;-&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#21322;&#27169;&#26495;&#21270;&#36870;&#21521;&#21512;&#25104;&#20013;&#30340;&#21512;&#25104;&#29289;&#34917;&#20840;&#12290;&#23427;&#20351;&#29992;&#22810;&#20010;&#20195;&#29702;&#21516;&#26102;&#23436;&#25104;&#21512;&#25104;&#29289;&#30340;&#34917;&#20840;&#65292;&#24182;&#36890;&#36807;&#27491;&#21521;&#21512;&#25104;&#27169;&#22411;&#35780;&#20272;&#21453;&#24212;&#29289;&#30340;&#21512;&#25104;&#33021;&#21147;&#26469;&#25351;&#23548;&#34892;&#21160;&#25628;&#32034;&#12290;</title><link>http://arxiv.org/abs/2309.02671</link><description>&lt;p&gt;
RLSynC: &#31163;&#32447;-&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#21512;&#25104;&#26041;&#27861;&#30340;&#21512;&#25104;&#29289;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
RLSynC: Offline-Online Reinforcement Learning for Synthon Completion. (arXiv:2309.02671v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02671
&lt;/p&gt;
&lt;p&gt;
RLSynC&#26159;&#19968;&#31181;&#31163;&#32447;-&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#21322;&#27169;&#26495;&#21270;&#36870;&#21521;&#21512;&#25104;&#20013;&#30340;&#21512;&#25104;&#29289;&#34917;&#20840;&#12290;&#23427;&#20351;&#29992;&#22810;&#20010;&#20195;&#29702;&#21516;&#26102;&#23436;&#25104;&#21512;&#25104;&#29289;&#30340;&#34917;&#20840;&#65292;&#24182;&#36890;&#36807;&#27491;&#21521;&#21512;&#25104;&#27169;&#22411;&#35780;&#20272;&#21453;&#24212;&#29289;&#30340;&#21512;&#25104;&#33021;&#21147;&#26469;&#25351;&#23548;&#34892;&#21160;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#21521;&#21512;&#25104;&#26159;&#30830;&#23450;&#33021;&#22815;&#21453;&#24212;&#24418;&#25104;&#25152;&#38656;&#20135;&#29289;&#30340;&#19968;&#32452;&#21453;&#24212;&#29289;&#20998;&#23376;&#30340;&#36807;&#31243;&#12290;&#21322;&#27169;&#26495;&#21270;&#36870;&#21521;&#21512;&#25104;&#26041;&#27861;&#39318;&#20808;&#39044;&#27979;&#20135;&#29289;&#20013;&#30340;&#21453;&#24212;&#20013;&#24515;&#65292;&#28982;&#21518;&#23558;&#29983;&#25104;&#30340;&#21512;&#25104;&#29289;&#37325;&#26032;&#34917;&#20840;&#25104;&#21453;&#24212;&#29289;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#24517;&#35201;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#39640;&#23454;&#29992;&#24615;&#65292;&#20197;&#25351;&#23548;&#21512;&#25104;&#35268;&#21010;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;-&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;RLSynC&#65292;&#29992;&#20110;&#21322;&#27169;&#26495;&#21270;&#26041;&#27861;&#20013;&#30340;&#21512;&#25104;&#29289;&#34917;&#20840;&#12290;RLSynC&#20026;&#27599;&#20010;&#21512;&#25104;&#29289;&#20998;&#37197;&#19968;&#20010;&#20195;&#29702;&#65292;&#25152;&#26377;&#20195;&#29702;&#37117;&#36890;&#36807;&#21516;&#27493;&#36827;&#34892;&#36880;&#27493;&#34892;&#21160;&#65292;&#23436;&#25104;&#21512;&#25104;&#29289;&#30340;&#34917;&#20840;&#12290;RLSynC&#36890;&#36807;&#21516;&#26102;&#36827;&#34892;&#31163;&#32447;&#35757;&#32451;&#21644;&#22312;&#32447;&#20132;&#20114;&#26469;&#23398;&#20064;&#31574;&#30053;&#65292;&#20174;&#32780;&#21487;&#20197;&#25506;&#32034;&#26032;&#30340;&#21453;&#24212;&#31354;&#38388;&#12290;RLSynC&#20351;&#29992;&#27491;&#21521;&#21512;&#25104;&#27169;&#22411;&#26469;&#35780;&#20272;&#39044;&#27979;&#30340;&#21453;&#24212;&#29289;&#22312;&#21512;&#25104;&#20135;&#29289;&#26102;&#30340;&#21487;&#33021;&#24615;&#65292;&#20174;&#32780;&#25351;&#23548;&#34892;&#21160;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrosynthesis is the process of determining the set of reactant molecules that can react to form a desired product. Semi-template-based retrosynthesis methods, which imitate the reverse logic of synthesis reactions, first predict the reaction centers in the products, and then complete the resulting synthons back into reactants. These methods enable necessary interpretability and high practical utility to inform synthesis planning. We develop a new offline-online reinforcement learning method RLSynC for synthon completion in semi-template-based methods. RLSynC assigns one agent to each synthon, all of which complete the synthons by conducting actions step by step in a synchronized fashion. RLSynC learns the policy from both offline training episodes and online interactions which allow RLSynC to explore new reaction spaces. RLSynC uses a forward synthesis model to evaluate the likelihood of the predicted reactants in synthesizing a product, and thus guides the action search. We compare 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32852;&#37030;&#27491;&#20132;&#35757;&#32451;&#65288;FOT&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36830;&#32493;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20840;&#23616;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#23545;&#36807;&#21435;&#25968;&#25454;&#30340;&#19981;&#20999;&#23454;&#38469;&#20551;&#35774;&#21644;&#38544;&#31169;&#21407;&#21017;&#30340;&#36829;&#21453;&#12290;</title><link>http://arxiv.org/abs/2309.01289</link><description>&lt;p&gt;
&#32852;&#37030;&#27491;&#20132;&#35757;&#32451;&#65306;&#20943;&#36731;&#36830;&#32493;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20840;&#23616;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Federated Orthogonal Training: Mitigating Global Catastrophic Forgetting in Continual Federated Learning. (arXiv:2309.01289v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32852;&#37030;&#27491;&#20132;&#35757;&#32451;&#65288;FOT&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36830;&#32493;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20840;&#23616;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#23545;&#36807;&#21435;&#25968;&#25454;&#30340;&#19981;&#20999;&#23454;&#38469;&#20551;&#35774;&#21644;&#38544;&#31169;&#21407;&#21017;&#30340;&#36829;&#21453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#22240;&#20854;&#33021;&#22815;&#23454;&#29616;&#20998;&#25955;&#25968;&#25454;&#19978;&#30340;&#38544;&#31169;&#20445;&#25252;&#35757;&#32451;&#32780;&#21463;&#21040;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#24403;&#21069;&#32852;&#37030;&#23398;&#20064;&#39046;&#22495;&#30340;&#25991;&#29486;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#20219;&#21153;&#23398;&#20064;&#19978;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#65292;&#23458;&#25143;&#31471;&#21487;&#33021;&#20250;&#20986;&#29616;&#26032;&#30340;&#20219;&#21153;&#65292;&#20840;&#23616;&#27169;&#22411;&#24212;&#35813;&#22312;&#19981;&#36951;&#24536;&#20043;&#21069;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#36825;&#20123;&#20219;&#21153;&#12290;&#36825;&#31181;&#30495;&#23454;&#22330;&#26223;&#34987;&#31216;&#20026;&#36830;&#32493;&#32852;&#37030;&#23398;&#20064;&#65288;CFL&#65289;&#12290;CFL&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#20840;&#23616;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21363;&#24403;&#20840;&#23616;&#27169;&#22411;&#22312;&#26032;&#20219;&#21153;&#19978;&#35757;&#32451;&#26102;&#65292;&#20854;&#22312;&#26087;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#36817;&#26399;&#26377;&#19968;&#20123;&#20851;&#20110;CFL&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#35299;&#20915;&#20840;&#23616;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#23545;&#36807;&#21435;&#25968;&#25454;&#26679;&#26412;&#30340;&#21487;&#29992;&#24615;&#20570;&#20986;&#20102;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#65292;&#35201;&#20040;&#36829;&#21453;&#20102;FL&#30340;&#38544;&#31169;&#21407;&#21017;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#32852;&#37030;&#27491;&#20132;&#35757;&#32451;&#65288;FOT&#65289;&#65292;&#20197;&#20811;&#26381;&#36825;&#20123;&#32570;&#28857;&#24182;&#35299;&#20915;&#20840;&#23616;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has gained significant attraction due to its ability to enable privacy-preserving training over decentralized data. Current literature in FL mostly focuses on single-task learning. However, over time, new tasks may appear in the clients and the global model should learn these tasks without forgetting previous tasks. This real-world scenario is known as Continual Federated Learning (CFL). The main challenge of CFL is Global Catastrophic Forgetting, which corresponds to the fact that when the global model is trained on new tasks, its performance on old tasks decreases. There have been a few recent works on CFL to propose methods that aim to address the global catastrophic forgetting problem. However, these works either have unrealistic assumptions on the availability of past data samples or violate the privacy principles of FL. We propose a novel method, Federated Orthogonal Training (FOT), to overcome these drawbacks and address the global catastrophic forgetting
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#36890;&#29992;&#31995;&#32479;&#65292;&#21517;&#20026;&#39034;&#24207;&#28789;&#24039;&#24615;&#65292;&#29992;&#20110;&#35299;&#20915;&#30001;&#19968;&#31995;&#21015;&#20114;&#19981;&#30456;&#21516;&#30340;&#23376;&#20219;&#21153;&#32452;&#25104;&#30340;&#38271;&#31243;&#25805;&#20316;&#38382;&#39064;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#20018;&#32852;&#22810;&#20010;&#29087;&#32451;&#31574;&#30053;&#26469;&#23454;&#29616;&#38271;&#31243;&#20219;&#21153;&#30446;&#26631;&#65292;&#24182;&#20855;&#22791;&#33258;&#20027;&#31574;&#30053;&#20999;&#25442;&#21644;&#32469;&#36807;&#22810;&#20313;&#38454;&#27573;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31995;&#32479;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.00987</link><description>&lt;p&gt;
&#20018;&#32852;&#29087;&#32451;&#25805;&#20316;&#31574;&#30053;&#23454;&#29616;&#38271;&#31243;&#25805;&#20316;&#30340;&#39034;&#24207;&#28789;&#24039;&#24615;
&lt;/p&gt;
&lt;p&gt;
Sequential Dexterity: Chaining Dexterous Policies for Long-Horizon Manipulation. (arXiv:2309.00987v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#36890;&#29992;&#31995;&#32479;&#65292;&#21517;&#20026;&#39034;&#24207;&#28789;&#24039;&#24615;&#65292;&#29992;&#20110;&#35299;&#20915;&#30001;&#19968;&#31995;&#21015;&#20114;&#19981;&#30456;&#21516;&#30340;&#23376;&#20219;&#21153;&#32452;&#25104;&#30340;&#38271;&#31243;&#25805;&#20316;&#38382;&#39064;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#20018;&#32852;&#22810;&#20010;&#29087;&#32451;&#31574;&#30053;&#26469;&#23454;&#29616;&#38271;&#31243;&#20219;&#21153;&#30446;&#26631;&#65292;&#24182;&#20855;&#22791;&#33258;&#20027;&#31574;&#30053;&#20999;&#25442;&#21644;&#32469;&#36807;&#22810;&#20313;&#38454;&#27573;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31995;&#32479;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#25805;&#20316;&#20219;&#21153;&#30001;&#19968;&#31995;&#21015;&#20114;&#19981;&#30456;&#21516;&#30340;&#23376;&#20219;&#21153;&#32452;&#25104;&#12290;&#36825;&#20123;&#38271;&#31243;&#12289;&#22797;&#26434;&#30340;&#20219;&#21153;&#20984;&#26174;&#20102;&#29087;&#32451;&#25163;&#30340;&#28508;&#21147;&#65292;&#23427;&#20204;&#20855;&#26377;&#36866;&#24212;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#22312;&#19981;&#38656;&#35201;&#37325;&#26032;&#25235;&#21462;&#25110;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#26080;&#32541;&#22320;&#22312;&#19981;&#21516;&#21151;&#33021;&#27169;&#24335;&#20043;&#38388;&#36807;&#28193;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29087;&#32451;&#25163;&#30340;&#39640;&#32500;&#34892;&#21160;&#31354;&#38388;&#21644;&#38271;&#31243;&#20219;&#21153;&#30340;&#22797;&#26434;&#32452;&#21512;&#21160;&#21147;&#23398;&#65292;&#20135;&#29983;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#36890;&#29992;&#31995;&#32479;&#8212;&#8212;&#39034;&#24207;&#28789;&#24039;&#24615;&#65292;&#36890;&#36807;&#20018;&#32852;&#22810;&#20010;&#29087;&#32451;&#31574;&#30053;&#26469;&#23454;&#29616;&#38271;&#31243;&#20219;&#21153;&#30446;&#26631;&#12290;&#35813;&#31995;&#32479;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#28176;&#36827;&#35843;&#20248;&#23376;&#31574;&#30053;&#30340;&#36807;&#28193;&#21487;&#34892;&#24615;&#20989;&#25968;&#65292;&#29992;&#20110;&#22686;&#24378;&#20018;&#32852;&#25104;&#21151;&#29575;&#65292;&#24182;&#19988;&#36824;&#33021;&#22815;&#23454;&#29616;&#33258;&#20027;&#31574;&#30053;&#20999;&#25442;&#20197;&#24212;&#23545;&#22833;&#36133;&#21644;&#32469;&#36807;&#22810;&#20313;&#30340;&#38454;&#27573;&#12290;&#23613;&#31649;&#21482;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#35757;&#32451;&#20102;&#20960;&#20010;&#20219;&#21153;&#23545;&#35937;&#65292;&#35813;&#31995;&#32479;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world manipulation tasks consist of a series of subtasks that are significantly different from one another. Such long-horizon, complex tasks highlight the potential of dexterous hands, which possess adaptability and versatility, capable of seamlessly transitioning between different modes of functionality without the need for re-grasping or external tools. However, the challenges arise due to the high-dimensional action space of dexterous hand and complex compositional dynamics of the long-horizon tasks. We present Sequential Dexterity, a general system based on reinforcement learning (RL) that chains multiple dexterous policies for achieving long-horizon task goals. The core of the system is a transition feasibility function that progressively finetunes the sub-policies for enhancing chaining success rate, while also enables autonomous policy-switching for recovery from failures and bypassing redundant stages. Despite being trained only in simulation with a few task objects, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24494;&#20998;&#21338;&#24328;&#12289;&#26368;&#20248;&#25511;&#21046;&#21644;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#33021;&#37327;&#30340;&#28508;&#22312;&#21338;&#24328;&#30340;&#26032;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#24212;&#29992;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21644;&#21487;&#24494;&#20998;&#30340;&#21338;&#24328;&#35770;&#20248;&#21270;&#23618;&#30340;&#32452;&#21512;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.16539</link><description>&lt;p&gt;
&#20851;&#20110;&#22810;&#26234;&#33021;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;&#24494;&#20998;&#21338;&#24328;&#12289;&#26368;&#20248;&#25511;&#21046;&#21644;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
On a Connection between Differential Games, Optimal Control, and Energy-based Models for Multi-Agent Interactions. (arXiv:2308.16539v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24494;&#20998;&#21338;&#24328;&#12289;&#26368;&#20248;&#25511;&#21046;&#21644;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#33021;&#37327;&#30340;&#28508;&#22312;&#21338;&#24328;&#30340;&#26032;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#24212;&#29992;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21644;&#21487;&#24494;&#20998;&#30340;&#21338;&#24328;&#35770;&#20248;&#21270;&#23618;&#30340;&#32452;&#21512;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21338;&#24328;&#35770;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#27169;&#22810;&#26234;&#33021;&#20307;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#65292;&#21338;&#24328;&#35770;&#30340;&#36866;&#29992;&#24615;&#21463;&#21040;&#22810;&#20010;&#25361;&#25112;&#30340;&#38459;&#30861;&#65292;&#27604;&#22914;&#26410;&#30693;&#30340;&#26234;&#33021;&#20307;&#20559;&#22909;&#21644;&#30446;&#26631;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24494;&#20998;&#21338;&#24328;&#12289;&#26368;&#20248;&#25511;&#21046;&#21644;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#29616;&#26377;&#26041;&#27861;&#32479;&#19968;&#21040;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#33021;&#37327;&#30340;&#28508;&#22312;&#21338;&#24328;&#30340;&#24418;&#24335;&#21270;&#20013;&#12290;&#22312;&#36825;&#20010;&#24418;&#24335;&#21270;&#30340;&#22522;&#30784;&#19978;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#24212;&#29992;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#21338;&#24328;&#21442;&#25968;&#25512;&#26029;&#65292;&#24182;&#36890;&#36807;&#21487;&#24494;&#20998;&#30340;&#21338;&#24328;&#35770;&#20248;&#21270;&#23618;&#20316;&#20026;&#24402;&#32435;&#20559;&#22909;&#12290;&#20351;&#29992;&#27169;&#25311;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#34892;&#20154;&#30456;&#20114;&#20316;&#29992;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#33258;&#21160;&#39550;&#39542;&#25968;&#25454;&#30340;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#21338;&#24328;&#35770;&#23618;&#25913;&#21892;&#20102;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#20027;&#24178;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Game theory offers an interpretable mathematical framework for modeling multi-agent interactions. However, its applicability in real-world robotics applications is hindered by several challenges, such as unknown agents' preferences and goals. To address these challenges, we show a connection between differential games, optimal control, and energy-based models and demonstrate how existing approaches can be unified under our proposed Energy-based Potential Game formulation. Building upon this formulation, this work introduces a new end-to-end learning application that combines neural networks for game-parameter inference with a differentiable game-theoretic optimization layer, acting as an inductive bias. The experiments using simulated mobile robot pedestrian interactions and real-world automated driving data provide empirical evidence that the game-theoretic layer improves the predictive performance of various neural network backbones.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DECODE&#30340;&#25193;&#24352;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#26497;&#31471;&#36136;&#37327;&#27604;&#28608;&#21457;&#30340;&#20449;&#21495;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#22312;&#39057;&#22495;&#36827;&#34892;&#24207;&#21015;&#24314;&#27169;&#65292;&#24182;&#32771;&#34385;&#26102;&#38388;&#24310;&#36831;&#24178;&#28041;&#20202;&#20197;&#22788;&#29702;&#22810;&#36890;&#36947;TDI&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.16422</link><description>&lt;p&gt;
DECODE: &#29992;&#20110;&#26816;&#27979;&#26497;&#31471;&#36136;&#37327;&#27604;&#28608;&#21457;&#30340;&#25193;&#24352;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DECODE: DilatEd COnvolutional neural network for Detecting Extreme-mass-ratio inspirals. (arXiv:2308.16422v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16422
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DECODE&#30340;&#25193;&#24352;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#26497;&#31471;&#36136;&#37327;&#27604;&#28608;&#21457;&#30340;&#20449;&#21495;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#22312;&#39057;&#22495;&#36827;&#34892;&#24207;&#21015;&#24314;&#27169;&#65292;&#24182;&#32771;&#34385;&#26102;&#38388;&#24310;&#36831;&#24178;&#28041;&#20202;&#20197;&#22788;&#29702;&#22810;&#36890;&#36947;TDI&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#22797;&#26434;&#30340;&#27874;&#24418;&#12289;&#25345;&#20037;&#30340;&#25345;&#32493;&#26102;&#38388;&#21644;&#20302;&#20449;&#22122;&#27604;&#65292;&#26497;&#31471;&#36136;&#37327;&#27604;&#28608;&#21457;(EMRI)&#30340;&#26816;&#27979;&#26159;&#22797;&#26434;&#30340;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#19982;&#32039;&#20945;&#30340;&#20108;&#36827;&#21046;&#34701;&#21512;&#30456;&#27604;&#26356;&#38590;&#34987;&#35782;&#21035;&#12290;&#34429;&#28982;&#22522;&#20110;&#21305;&#37197;&#28388;&#27874;&#30340;&#25216;&#26415;&#20197;&#20854;&#35745;&#31639;&#35201;&#27714;&#32780;&#38395;&#21517;&#65292;&#20294;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#20027;&#35201;&#22788;&#29702;&#26102;&#22495;&#25968;&#25454;&#65292;&#24182;&#19988;&#36890;&#24120;&#21463;&#21040;&#25968;&#25454;&#25345;&#32493;&#26102;&#38388;&#21644;&#20449;&#22122;&#27604;&#30340;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#24573;&#30053;&#20102;&#26102;&#38388;&#24310;&#36831;&#24178;&#28041;&#20202;(TDI)&#24182;&#22312;&#25506;&#27979;&#22120;&#21709;&#24212;&#35745;&#31639;&#20013;&#24212;&#29992;&#20102;&#38271;&#27874;&#36817;&#20284;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#22788;&#29702;&#28608;&#20809;&#39057;&#29575;&#22122;&#22768;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DECODE&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#39057;&#22495;&#24207;&#21015;&#24314;&#27169;&#20026;&#37325;&#28857;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#29992;&#20110;EMRI&#20449;&#21495;&#26816;&#27979;&#12290;DECODE&#22260;&#32469;&#30528;&#19968;&#20010;&#20197;&#25193;&#24352;&#22240;&#26524;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20026;&#20013;&#24515;&#65292;&#20351;&#29992;&#32771;&#34385;&#21040;TDI-1.5&#25506;&#27979;&#22120;&#21709;&#24212;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#22788;&#29702;&#19968;&#24180;&#30340;&#22810;&#36890;&#36947;TDI&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The detection of Extreme Mass Ratio Inspirals (EMRIs) is intricate due to their complex waveforms, extended duration, and low signal-to-noise ratio (SNR), making them more challenging to be identified compared to compact binary coalescences. While matched filtering-based techniques are known for their computational demands, existing deep learning-based methods primarily handle time-domain data and are often constrained by data duration and SNR. In addition, most existing work ignores time-delay interferometry (TDI) and applies the long-wavelength approximation in detector response calculations, thus limiting their ability to handle laser frequency noise. In this study, we introduce DECODE, an end-to-end model focusing on EMRI signal detection by sequence modeling in the frequency domain. Centered around a dilated causal convolutional neural network, trained on synthetic data considering TDI-1.5 detector response, DECODE can efficiently process a year's worth of multichannel TDI data wi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#27169;&#31946;&#32858;&#31867;&#26377;&#25928;&#24615;&#25351;&#26631;&#65292;&#35813;&#25351;&#26631;&#32771;&#34385;&#20102;&#22312;&#32858;&#31867;&#25968;&#37327;&#36873;&#25321;&#26102;&#21487;&#33021;&#23384;&#22312;&#30340;&#22810;&#20010;&#36873;&#39033;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#19982;&#29616;&#26377;&#25351;&#26631;&#36827;&#34892;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2308.14785</link><description>&lt;p&gt;
&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#27169;&#31946;&#32858;&#31867;&#26377;&#25928;&#24615;&#25351;&#26631;&#19982;&#27425;&#35201;&#36873;&#39033;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
A correlation-based fuzzy cluster validity index with secondary options detector. (arXiv:2308.14785v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#27169;&#31946;&#32858;&#31867;&#26377;&#25928;&#24615;&#25351;&#26631;&#65292;&#35813;&#25351;&#26631;&#32771;&#34385;&#20102;&#22312;&#32858;&#31867;&#25968;&#37327;&#36873;&#25321;&#26102;&#21487;&#33021;&#23384;&#22312;&#30340;&#22810;&#20010;&#36873;&#39033;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#19982;&#29616;&#26377;&#25351;&#26631;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#32858;&#31867;&#20998;&#26512;&#26102;&#65292;&#26368;&#20339;&#32858;&#31867;&#25968;&#37327;&#26159;&#20027;&#35201;&#20851;&#27880;&#28857;&#20043;&#19968;&#12290;&#24050;&#32463;&#24341;&#20837;&#20102;&#22810;&#20010;&#32858;&#31867;&#26377;&#25928;&#24615;&#25351;&#26631;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#26377;&#22810;&#20010;&#36873;&#39033;&#21487;&#20197;&#20316;&#20026;&#26368;&#32456;&#30340;&#32858;&#31867;&#25968;&#37327;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#22312;&#36825;&#20010;&#39046;&#22495;&#24573;&#35270;&#20102;&#36825;&#19968;&#26041;&#38754;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#27169;&#31946;&#32858;&#31867;&#26377;&#25928;&#24615;&#25351;&#26631;&#65292;&#31216;&#20026;Wiroonsri-Preedasawakul&#65288;WP&#65289;&#25351;&#26631;&#12290;&#35813;&#25351;&#26631;&#26681;&#25454;&#19968;&#23545;&#25968;&#25454;&#28857;&#30340;&#23454;&#38469;&#36317;&#31163;&#19982;&#30456;&#24212;&#23545;&#30340;&#35843;&#25972;&#36136;&#24515;&#20043;&#38388;&#30340;&#36317;&#31163;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#23450;&#20041;&#12290;&#25105;&#20204;&#35780;&#20272;&#24182;&#27604;&#36739;&#20102;&#25105;&#20204;&#30340;&#25351;&#26631;&#19982;Xie-Beni&#65292;Pakhira-Bandyopadhyay-Maulik&#65292;Tang&#65292;Wu-Li&#65292;&#24191;&#20041;C&#21644;Kwon2&#31561;&#20960;&#20010;&#29616;&#26377;&#25351;&#26631;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#22235;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65306;&#20154;&#24037;&#25968;&#25454;&#38598;&#65292;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65292;&#24102;&#26377;&#31561;&#32423;&#30340;&#27169;&#25311;&#25968;&#25454;&#38598;&#21644;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#27169;&#31946;c-mea&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The optimal number of clusters is one of the main concerns when applying cluster analysis. Several cluster validity indexes have been introduced to address this problem. However, in some situations, there is more than one option that can be chosen as the final number of clusters. This aspect has been overlooked by most of the existing works in this area. In this study, we introduce a correlation-based fuzzy cluster validity index known as the Wiroonsri-Preedasawakul (WP) index. This index is defined based on the correlation between the actual distance between a pair of data points and the distance between adjusted centroids with respect to that pair. We evaluate and compare the performance of our index with several existing indexes, including Xie-Beni, Pakhira-Bandyopadhyay-Maulik, Tang, Wu-Li, generalized C, and Kwon2. We conduct this evaluation on four types of datasets: artificial datasets, real-world datasets, simulated datasets with ranks, and image datasets, using the fuzzy c-mea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#24211;&#65292;&#21253;&#21547;&#20102;&#36830;&#32493;&#30340;&#21457;&#22768;&#22120;&#23448;&#21644;&#22768;&#23398;&#35821;&#38899;&#25968;&#25454;&#65292;&#29992;&#36229;&#22768;&#22270;&#20687;&#21644;&#22768;&#38899;&#25968;&#25454;&#26469;&#30740;&#31350;&#35328;&#35821;&#29983;&#25104;&#36807;&#31243;&#20013;&#33292;&#22836;&#19978;&#36718;&#24275;&#30340;&#21487;&#35270;&#21270;&#12290;&#36825;&#20010;&#25968;&#25454;&#24211;&#26159;&#30001;&#21733;&#20262;&#27604;&#20122;&#22307;&#22374;&#24503;&#21306;&#30340;17&#21517;&#24180;&#36731;&#34987;&#35797;&#23436;&#25104;&#30340;&#12290;</title><link>http://arxiv.org/abs/2308.13941</link><description>&lt;p&gt;
&#19968;&#20010;&#21253;&#21547;&#22768;&#36947;&#21160;&#24577;&#36229;&#22768;&#22270;&#20687;&#24207;&#21015;&#30340;&#23567;&#22411;&#35789;&#27719;&#25968;&#25454;&#24211;
&lt;/p&gt;
&lt;p&gt;
A small vocabulary database of ultrasound image sequences of vocal tract dynamics. (arXiv:2308.13941v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#24211;&#65292;&#21253;&#21547;&#20102;&#36830;&#32493;&#30340;&#21457;&#22768;&#22120;&#23448;&#21644;&#22768;&#23398;&#35821;&#38899;&#25968;&#25454;&#65292;&#29992;&#36229;&#22768;&#22270;&#20687;&#21644;&#22768;&#38899;&#25968;&#25454;&#26469;&#30740;&#31350;&#35328;&#35821;&#29983;&#25104;&#36807;&#31243;&#20013;&#33292;&#22836;&#19978;&#36718;&#24275;&#30340;&#21487;&#35270;&#21270;&#12290;&#36825;&#20010;&#25968;&#25454;&#24211;&#26159;&#30001;&#21733;&#20262;&#27604;&#20122;&#22307;&#22374;&#24503;&#21306;&#30340;17&#21517;&#24180;&#36731;&#34987;&#35797;&#23436;&#25104;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;&#36830;&#32493;&#30340;&#21457;&#22768;&#22120;&#23448;&#21644;&#22768;&#23398;&#35821;&#38899;&#25968;&#25454;&#30340;&#26032;&#25968;&#25454;&#24211;&#12290;&#21457;&#22768;&#22120;&#23448;&#25968;&#25454;&#23545;&#24212;&#20110;&#22768;&#36947;&#21160;&#24577;&#30340;&#36229;&#22768;&#35270;&#39057;&#65292;&#21487;&#20197;&#22312;&#35328;&#35821;&#29983;&#25104;&#36807;&#31243;&#20013;&#21487;&#35270;&#21270;&#33292;&#22836;&#19978;&#36718;&#24275;&#12290;&#22768;&#23398;&#25968;&#25454;&#30001;&#23450;&#21521;&#24515;&#33039;&#40614;&#20811;&#39118;&#33719;&#21462;&#65292;&#21253;&#25324;30&#20010;&#30701;&#21477;&#23376;&#12290;&#35813;&#25968;&#25454;&#24211;&#21253;&#25324;&#26469;&#33258;&#21733;&#20262;&#27604;&#20122;&#22307;&#22374;&#24503;&#21306;&#30340;17&#21517;&#24180;&#36731;&#34987;&#35797;&#65288;&#30007;&#24615;8&#21517;&#65292;&#22899;&#24615;9&#21517;&#65289;&#65292;&#20182;&#20204;&#25253;&#21578;&#27809;&#26377;&#20219;&#20309;&#35328;&#35821;&#30149;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new database consisting of concurrent articulatory and acoustic speech data. The articulatory data correspond to ultrasound videos of the vocal tract dynamics, which allow the visualization of the tongue upper contour during the speech production process. Acoustic data is composed of 30 short sentences that were acquired by a directional cardioid microphone. This database includes data from 17 young subjects (8 male and 9 female) from the Santander region in Colombia, who reported not having any speech pathology.
&lt;/p&gt;</description></item><item><title>LMSanitator&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#28040;&#38500;Transformer&#27169;&#22411;&#20013;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#21518;&#38376;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;LMSanitator&#36890;&#36807;&#36870;&#36716;&#39044;&#23450;&#20041;&#30340;&#25915;&#20987;&#21521;&#37327;&#32780;&#19981;&#26159;&#35302;&#21457;&#22120;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#33021;&#21644;&#21518;&#38376;&#26816;&#27979;&#31934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.13904</link><description>&lt;p&gt;
LMSanitator: &#38024;&#23545;&#20219;&#21153;&#19981;&#21487;&#30693;&#21518;&#38376;&#30340;Prompt-Tuning&#38450;&#24481;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors. (arXiv:2308.13904v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13904
&lt;/p&gt;
&lt;p&gt;
LMSanitator&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#28040;&#38500;Transformer&#27169;&#22411;&#20013;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#21518;&#38376;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;LMSanitator&#36890;&#36807;&#36870;&#36716;&#39044;&#23450;&#20041;&#30340;&#25915;&#20987;&#21521;&#37327;&#32780;&#19981;&#26159;&#35302;&#21457;&#22120;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#33021;&#21644;&#21518;&#38376;&#26816;&#27979;&#31934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt-Tuning&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24341;&#20154;&#27880;&#30446;&#30340;&#33539;&#24335;&#65292;&#29992;&#20110;&#37096;&#32626;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#24378;&#22823;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#21644;&#39640;&#25928;&#30340;&#22810;&#20219;&#21153;&#26381;&#21153;&#33021;&#21147;&#12290;&#23613;&#31649;&#23427;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#25105;&#20204;&#23454;&#35777;&#34920;&#26126;&#65292;Prompt-Tuning&#23481;&#26131;&#21463;&#21040;&#20219;&#21153;&#19981;&#21487;&#30693;&#21518;&#38376;&#30340;&#25915;&#20987;&#65292;&#36825;&#20123;&#21518;&#38376;&#23384;&#22312;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#65292;&#21487;&#20197;&#24433;&#21709;&#20219;&#24847;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#30446;&#21069;&#30340;&#21518;&#38376;&#26816;&#27979;&#26041;&#27861;&#26080;&#27861;&#38450;&#24481;&#20219;&#21153;&#19981;&#21487;&#30693;&#21518;&#38376;&#65292;&#22240;&#20026;&#23427;&#20204;&#24456;&#38590;&#22312;&#36870;&#36716;&#21518;&#38376;&#35302;&#21457;&#22120;&#26041;&#38754;&#25910;&#25947;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LMSanitator&#65292;&#19968;&#31181;&#22312;Transformer&#27169;&#22411;&#19978;&#26816;&#27979;&#21644;&#21435;&#38500;&#20219;&#21153;&#19981;&#21487;&#30693;&#21518;&#38376;&#30340;&#26032;&#26041;&#27861;&#12290;LMSanitator&#19981;&#30452;&#25509;&#36870;&#36716;&#35302;&#21457;&#22120;&#65292;&#32780;&#26159;&#36870;&#36716;&#39044;&#23450;&#20041;&#30340;&#25915;&#20987;&#21521;&#37327;&#65288;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#36755;&#20837;&#23884;&#20837;&#35302;&#21457;&#22120;&#26102;&#30340;&#36755;&#20986;&#65289;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#33021;&#21644;&#21518;&#38376;&#26816;&#27979;&#31934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt-tuning has emerged as an attractive paradigm for deploying large-scale language models due to its strong downstream task performance and efficient multitask serving ability. Despite its wide adoption, we empirically show that prompt-tuning is vulnerable to downstream task-agnostic backdoors, which reside in the pretrained models and can affect arbitrary downstream tasks. The state-of-the-art backdoor detection approaches cannot defend against task-agnostic backdoors since they hardly converge in reversing the backdoor triggers. To address this issue, we propose LMSanitator, a novel approach for detecting and removing task-agnostic backdoors on Transformer models. Instead of directly inversing the triggers, LMSanitator aims to inverse the predefined attack vectors (pretrained models' output when the input is embedded with triggers) of the task-agnostic backdoors, which achieves much better convergence performance and backdoor detection accuracy. LMSanitator further leverages prom
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#36830;&#32493;&#26102;&#38388;&#19979;&#35266;&#27979;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#24773;&#20917;&#19979;&#65292;Kalman&#28388;&#27874;&#22120;&#30340;&#31995;&#32479;&#35782;&#21035;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#26102;&#38388;Ito&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#26469;&#25512;&#24191;Kalman&#28388;&#27874;&#22120;&#30340;&#23398;&#20064;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#26032;&#39062;&#30340;&#20004;&#28388;&#27874;&#22120;&#30340;&#21518;&#39564;&#35745;&#31639;&#26041;&#27861;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#27966;&#29983;&#33719;&#24471;&#30340;&#35299;&#26512;&#24418;&#24335;&#30340;&#21518;&#39564;&#35745;&#31639;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#20272;&#35745;SDE&#30340;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2308.11933</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#31995;&#32479;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
System Identification for Continuous-time Linear Dynamical Systems. (arXiv:2308.11933v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#36830;&#32493;&#26102;&#38388;&#19979;&#35266;&#27979;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#24773;&#20917;&#19979;&#65292;Kalman&#28388;&#27874;&#22120;&#30340;&#31995;&#32479;&#35782;&#21035;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#26102;&#38388;Ito&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#26469;&#25512;&#24191;Kalman&#28388;&#27874;&#22120;&#30340;&#23398;&#20064;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#26032;&#39062;&#30340;&#20004;&#28388;&#27874;&#22120;&#30340;&#21518;&#39564;&#35745;&#31639;&#26041;&#27861;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#27966;&#29983;&#33719;&#24471;&#30340;&#35299;&#26512;&#24418;&#24335;&#30340;&#21518;&#39564;&#35745;&#31639;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#20272;&#35745;SDE&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Kalman&#28388;&#27874;&#22120;&#30340;&#31995;&#32479;&#35782;&#21035;&#38382;&#39064;&#22312;&#23398;&#20064;&#21160;&#24577;&#31995;&#32479;&#30340;&#22522;&#30784;&#21442;&#25968;&#26102;&#65292;&#36890;&#24120;&#20551;&#35774;&#35266;&#27979;&#20540;&#22312;&#31561;&#38388;&#38548;&#30340;&#26102;&#38388;&#28857;&#37319;&#26679;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#36825;&#20010;&#20551;&#35774;&#26159;&#26377;&#38480;&#21046;&#21644;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26412;&#25991;&#38024;&#23545;&#36830;&#32493;&#31163;&#25955;&#28388;&#27874;&#22120;&#30340;&#31995;&#32479;&#35782;&#21035;&#38382;&#39064;&#65292;&#36890;&#36807;&#27714;&#35299;&#36830;&#32493;&#26102;&#38388;Ito&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#26469;&#25512;&#24191;Kalman&#28388;&#27874;&#22120;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20004;&#28388;&#27874;&#22120;&#65292;&#20855;&#26377;&#36125;&#21494;&#26031;&#27966;&#29983;&#30340;&#35299;&#26512;&#24418;&#24335;&#30340;&#21518;&#39564;&#65292;&#36825;&#26679;&#21487;&#20197;&#24471;&#21040;&#19981;&#38656;&#35201;&#39044;&#20808;&#35745;&#31639;&#30340;&#27491;&#21521;&#20256;&#36882;&#30340;&#35299;&#26512;&#26356;&#26032;&#12290;&#21033;&#29992;&#36825;&#31181;&#35299;&#26512;&#30340;&#39640;&#25928;&#35745;&#31639;&#21518;&#39564;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;EM&#36807;&#31243;&#65292;&#29992;&#20110;&#20272;&#35745;SDE&#30340;&#21442;&#25968;&#65292;&#33258;&#28982;&#22320;&#32435;&#20837;&#20102;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of system identification for the Kalman filter, relying on the expectation-maximization (EM) procedure to learn the underlying parameters of a dynamical system, has largely been studied assuming that observations are sampled at equally-spaced time points. However, in many applications this is a restrictive and unrealistic assumption. This paper addresses system identification for the continuous-discrete filter, with the aim of generalizing learning for the Kalman filter by relying on a solution to a continuous-time It\^o stochastic differential equation (SDE) for the latent state and covariance dynamics. We introduce a novel two-filter, analytical form for the posterior with a Bayesian derivation, which yields analytical updates which do not require the forward-pass to be pre-computed. Using this analytical and efficient computation of the posterior, we provide an EM procedure which estimates the parameters of the SDE, naturally incorporating irregularly sampled measurement
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#35843;&#35797;C&#32534;&#35793;&#22120;&#30340;&#38169;&#35823;&#35299;&#37322;&#25913;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19987;&#23478;&#35780;&#20272;&#35777;&#26126;&#20854;&#22312;&#32534;&#35793;&#26102;&#21644;&#36816;&#34892;&#26102;&#38169;&#35823;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11873</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#35843;&#35797;C&#32534;&#35793;&#22120;&#20013;&#20197;&#29983;&#25104;&#19978;&#19979;&#25991;&#38169;&#35823;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Integrating Large Language Models into the Debugging C Compiler for generating contextual error explanations. (arXiv:2308.11873v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#35843;&#35797;C&#32534;&#35793;&#22120;&#30340;&#38169;&#35823;&#35299;&#37322;&#25913;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19987;&#23478;&#35780;&#20272;&#35777;&#26126;&#20854;&#22312;&#32534;&#35793;&#26102;&#21644;&#36816;&#34892;&#26102;&#38169;&#35823;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25105;&#20204;&#30340;&#35843;&#35797;C&#32534;&#35793;&#22120;&#65288;DCC&#65289;&#20013;&#29983;&#25104;&#22686;&#24378;&#22411;&#32534;&#35793;&#22120;&#38169;&#35823;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#32534;&#35793;&#22120;&#38169;&#35823;&#28040;&#24687;&#23545;&#20110;&#21021;&#23398;&#32773;&#23398;&#20064;&#22914;&#20309;&#32534;&#31243;&#26159;&#19968;&#20010;&#38556;&#30861;&#12290;&#34429;&#28982;&#25105;&#20204;&#26368;&#21021;&#22312;&#20837;&#38376;&#32534;&#31243;&#65288;CS1&#65289;&#20013;&#20351;&#29992;DCC&#24050;&#32463;&#36890;&#36807;&#25552;&#20379;&#24120;&#35265;&#38169;&#35823;&#30340;&#20445;&#25252;&#26426;&#21046;&#21644;&#32763;&#35793;&#36890;&#24120;&#21547;&#20041;&#38544;&#26214;&#30340;&#32534;&#35793;&#22120;&#38169;&#35823;&#28040;&#24687;&#65292;&#26377;&#21161;&#20110;&#25945;&#25480;C&#35821;&#35328;&#32473;&#21021;&#23398;&#32773;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#23558;LLM&#29983;&#25104;&#30340;&#35299;&#37322;&#32435;&#20837;&#36827;&#26469;&#20250;&#36827;&#19968;&#27493;&#22686;&#24378;&#21021;&#23398;&#32773;&#30340;&#23398;&#20064;&#20307;&#39564;&#12290;&#36890;&#36807;&#19987;&#23478;&#35780;&#20272;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;LLM&#29983;&#25104;&#30340;&#32534;&#35793;&#22120;&#38169;&#35823;&#35299;&#37322;&#22312;90%&#30340;&#32534;&#35793;&#26102;&#38169;&#35823;&#21644;75%&#30340;&#36816;&#34892;&#26102;&#38169;&#35823;&#20013;&#22312;&#27010;&#24565;&#19978;&#26159;&#20934;&#30830;&#30340;&#12290;&#27492;&#22806;&#65292;&#26032;&#30340;DCC&#24110;&#21161;&#24037;&#20855;&#24050;&#32463;&#36234;&#26469;&#36234;&#22810;&#34987;&#23398;&#29983;&#20351;&#29992;&#65292;&#27599;&#21608;&#24179;&#22343;&#26377;1047&#27425;&#29420;&#29305;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a method for Large Language Models (LLM) to produce enhanced compiler error explanations, in simple language, within our Debugging C Compiler (DCC). It is well documented that compiler error messages have been known to present a barrier for novices learning how to program. Although our initial use of DCC in introductory programming (CS1) has been instrumental in teaching C to novice programmers by providing safeguards to commonly occurring errors and translating the usually cryptic compiler error messages at both compile- and run-time, we proposed that incorporating LLM-generated explanations would further enhance the learning experience for novice programmers. Through an expert evaluation, we observed that LLM-generated explanations for compiler errors were conceptually accurate in 90% of compile-time errors, and 75% of run-time errors. Additionally, the new DCC-help tool has been increasingly adopted by students, with an average of 1047 unique runs per week, dem
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;&#65306;GPFL&#26159;&#19968;&#31181;&#26032;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#21516;&#26102;&#23398;&#20064;&#20840;&#23616;&#21644;&#20010;&#24615;&#21270;&#30340;&#29305;&#24449;&#20449;&#24687;&#65292;&#22312;&#25928;&#26524;&#12289;&#21487;&#25193;&#23637;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#31283;&#23450;&#24615;&#21644;&#38544;&#31169;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#20943;&#36731;&#20102;&#36807;&#25311;&#21512;&#29616;&#35937;&#65292;&#25552;&#21319;&#20102;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.10279</link><description>&lt;p&gt;
GPFL: &#21516;&#26102;&#23398;&#20064;&#20840;&#23616;&#21644;&#20010;&#24615;&#21270;&#29305;&#24449;&#20449;&#24687;&#20197;&#23454;&#29616;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GPFL: Simultaneously Learning Global and Personalized Feature Information for Personalized Federated Learning. (arXiv:2308.10279v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10279
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;&#65306;GPFL&#26159;&#19968;&#31181;&#26032;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#21516;&#26102;&#23398;&#20064;&#20840;&#23616;&#21644;&#20010;&#24615;&#21270;&#30340;&#29305;&#24449;&#20449;&#24687;&#65292;&#22312;&#25928;&#26524;&#12289;&#21487;&#25193;&#23637;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#31283;&#23450;&#24615;&#21644;&#38544;&#31169;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#20943;&#36731;&#20102;&#36807;&#25311;&#21512;&#29616;&#35937;&#65292;&#25552;&#21319;&#20102;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#22240;&#20854;&#20445;&#25252;&#38544;&#31169;&#21644;&#21327;&#20316;&#23398;&#20064;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#27426;&#36814;&#12290;&#26368;&#36817;&#65292;&#20010;&#24615;&#21270;FL&#65288;pFL&#65289;&#22240;&#20854;&#33021;&#22815;&#35299;&#20915;&#32479;&#35745;&#24322;&#36136;&#24615;&#24182;&#22312;FL&#20013;&#23454;&#29616;&#20010;&#24615;&#21270;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20174;&#29305;&#24449;&#25552;&#21462;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;pFL&#26041;&#27861;&#21482;&#20851;&#27880;&#22312;&#26412;&#22320;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#21462;&#20840;&#23616;&#25110;&#20010;&#24615;&#21270;&#30340;&#29305;&#24449;&#20449;&#24687;&#65292;&#36825;&#26080;&#27861;&#28385;&#36275;pFL&#30340;&#21327;&#20316;&#23398;&#20064;&#21644;&#20010;&#24615;&#21270;&#30446;&#26631;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;pFL&#26041;&#27861;&#65292;&#21517;&#20026;GPFL&#65292;&#29992;&#20110;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#19978;&#21516;&#26102;&#23398;&#20064;&#20840;&#23616;&#21644;&#20010;&#24615;&#21270;&#30340;&#29305;&#24449;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#19977;&#31181;&#32479;&#35745;&#24322;&#36136;&#30340;&#35774;&#32622;&#19979;&#23545;&#20845;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;GPFL&#22312;&#25928;&#26524;&#12289;&#21487;&#25193;&#23637;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#31283;&#23450;&#24615;&#21644;&#38544;&#31169;&#24615;&#26041;&#38754;&#20248;&#20110;&#21313;&#31181;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;&#27492;&#22806;&#65292;GPFL&#20943;&#36731;&#20102;&#36807;&#25311;&#21512;&#29616;&#35937;&#65292;&#24182;&#22312;&#20934;&#30830;&#24230;&#19978;&#36229;&#36807;&#20102;&#22522;&#20934;&#32447;&#26368;&#39640;8.99%&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is popular for its privacy-preserving and collaborative learning capabilities. Recently, personalized FL (pFL) has received attention for its ability to address statistical heterogeneity and achieve personalization in FL. However, from the perspective of feature extraction, most existing pFL methods only focus on extracting global or personalized feature information during local training, which fails to meet the collaborative learning and personalization goals of pFL. To address this, we propose a new pFL method, named GPFL, to simultaneously learn global and personalized feature information on each client. We conduct extensive experiments on six datasets in three statistically heterogeneous settings and show the superiority of GPFL over ten state-of-the-art methods regarding effectiveness, scalability, fairness, stability, and privacy. Besides, GPFL mitigates overfitting and outperforms the baselines by up to 8.99% in accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#22270;&#32534;&#30721;&#35299;&#30721;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#22270;&#20013;&#30340;&#24322;&#24120;&#33410;&#28857;&#12290;&#22312;&#32534;&#30721;&#38454;&#27573;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#26032;&#30340;&#27744;&#21270;&#26426;&#21046;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#33410;&#28857;&#30340;&#24322;&#24120;&#31243;&#24230;&#23545;&#33410;&#28857;&#36827;&#34892;&#25490;&#24207;&#12290;&#27169;&#22411;&#30340;&#27744;&#21270;&#36807;&#31243;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#26356;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.07774</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#22270;&#32534;&#30721;&#35299;&#30721;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A Graph Encoder-Decoder Network for Unsupervised Anomaly Detection. (arXiv:2308.07774v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#22270;&#32534;&#30721;&#35299;&#30721;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#22270;&#20013;&#30340;&#24322;&#24120;&#33410;&#28857;&#12290;&#22312;&#32534;&#30721;&#38454;&#27573;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#26032;&#30340;&#27744;&#21270;&#26426;&#21046;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#33410;&#28857;&#30340;&#24322;&#24120;&#31243;&#24230;&#23545;&#33410;&#28857;&#36827;&#34892;&#25490;&#24207;&#12290;&#27169;&#22411;&#30340;&#27744;&#21270;&#36807;&#31243;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#26356;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20851;&#38190;&#32452;&#20214;&#26159;&#27744;&#21270;&#25805;&#20316;&#65292;&#23427;&#26088;&#22312;&#20943;&#23567;&#22270;&#30340;&#22823;&#23567;&#21516;&#26102;&#20445;&#30041;&#37325;&#35201;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22270;&#27744;&#21270;&#31574;&#30053;&#20381;&#36182;&#20110;&#36890;&#36807;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23618;&#33719;&#24471;&#30340;&#20998;&#37197;&#30697;&#38453;&#65292;&#35813;&#30697;&#38453;&#20855;&#26377;&#21487;&#35757;&#32451;&#30340;&#21442;&#25968;&#65292;&#24448;&#24448;&#23548;&#33268;&#26174;&#33879;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#27744;&#21270;&#36807;&#31243;&#30340;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#22270;&#32534;&#30721;&#35299;&#30721;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#31181;&#24322;&#24120;&#35780;&#20998;&#20989;&#25968;&#23545;&#33410;&#28857;&#36827;&#34892;&#25490;&#24207;&#65292;&#20174;&#32780;&#26816;&#27979;&#20986;&#22270;&#20013;&#30340;&#24322;&#24120;&#33410;&#28857;&#12290;&#22312;&#32534;&#30721;&#38454;&#27573;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#27744;&#21270;&#26426;&#21046;&#65292;&#21629;&#21517;&#20026;LCPool&#65292;&#23427;&#21033;&#29992;&#23616;&#37096;&#32422;&#26463;&#32447;&#24615;&#32534;&#30721;&#36827;&#34892;&#29305;&#24449;&#32534;&#30721;&#65292;&#36890;&#36807;&#27714;&#35299;&#24102;&#26377;&#23616;&#37096;&#27491;&#21017;&#21270;&#39033;&#30340;&#26368;&#23567;&#20108;&#20056;&#20248;&#21270;&#38382;&#39064;&#26469;&#25214;&#21040;&#32858;&#31867;&#20998;&#37197;&#30697;&#38453;&#12290;&#36890;&#36807;&#22312;&#32534;&#30721;&#36807;&#31243;&#20013;&#24378;&#21046;&#25191;&#34892;&#23616;&#37096;&#32422;&#26463;&#65292;LCPool&#34987;&#35774;&#35745;&#25104;&#20813;&#36153;
&lt;/p&gt;
&lt;p&gt;
A key component of many graph neural networks (GNNs) is the pooling operation, which seeks to reduce the size of a graph while preserving important structural information. However, most existing graph pooling strategies rely on an assignment matrix obtained by employing a GNN layer, which is characterized by trainable parameters, often leading to significant computational complexity and a lack of interpretability in the pooling process. In this paper, we propose an unsupervised graph encoder-decoder model to detect abnormal nodes from graphs by learning an anomaly scoring function to rank nodes based on their degree of abnormality. In the encoding stage, we design a novel pooling mechanism, named LCPool, which leverages locality-constrained linear coding for feature encoding to find a cluster assignment matrix by solving a least-squares optimization problem with a locality regularization term. By enforcing locality constraints during the coding process, LCPool is designed to be free fr
&lt;/p&gt;</description></item><item><title>SynJax&#26159;&#19968;&#20010;&#38024;&#23545;JAX&#30340;&#32467;&#26500;&#21270;&#27010;&#29575;&#20998;&#24067;&#24211;&#65292;&#36890;&#36807;&#25552;&#20379;&#39640;&#25928;&#30340;&#21521;&#37327;&#21270;&#23454;&#29616;&#35299;&#20915;&#20102;&#23545;&#20110;&#32467;&#26500;&#21270;&#23545;&#35937;&#30340;&#38590;&#20197;&#23454;&#29616;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.03291</link><description>&lt;p&gt;
SynJax: JAX&#30340;&#32467;&#26500;&#21270;&#27010;&#29575;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
SynJax: Structured Probability Distributions for JAX. (arXiv:2308.03291v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03291
&lt;/p&gt;
&lt;p&gt;
SynJax&#26159;&#19968;&#20010;&#38024;&#23545;JAX&#30340;&#32467;&#26500;&#21270;&#27010;&#29575;&#20998;&#24067;&#24211;&#65292;&#36890;&#36807;&#25552;&#20379;&#39640;&#25928;&#30340;&#21521;&#37327;&#21270;&#23454;&#29616;&#35299;&#20915;&#20102;&#23545;&#20110;&#32467;&#26500;&#21270;&#23545;&#35937;&#30340;&#38590;&#20197;&#23454;&#29616;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#36719;&#20214;&#24211;&#30340;&#21457;&#23637;&#20351;&#24471;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#23427;&#20351;&#29992;&#25143;&#33021;&#22815;&#19987;&#27880;&#20110;&#24314;&#27169;&#65292;&#21516;&#26102;&#35753;&#24211;&#26469;&#22788;&#29702;&#38024;&#23545;&#29616;&#20195;&#30828;&#20214;&#21152;&#36895;&#22120;&#36827;&#34892;&#20248;&#21270;&#25191;&#34892;&#30340;&#32321;&#29712;&#21644;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20165;&#23545;&#29305;&#23450;&#31867;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26377;&#30410;&#65292;&#20363;&#22914;Transformer&#65292;&#20854;&#22522;&#26412;&#25805;&#20316;&#26131;&#20110;&#26144;&#23556;&#21040;&#21521;&#37327;&#21270;&#35745;&#31639;&#12290;&#32780;&#23545;&#20110;&#26174;&#24335;&#32771;&#34385;&#32467;&#26500;&#21270;&#23545;&#35937;&#65288;&#22914;&#26641;&#21644;&#20998;&#21106;&#65289;&#30340;&#27169;&#22411;&#65292;&#24182;&#27809;&#26377;&#21516;&#26679;&#30340;&#21463;&#30410;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#23450;&#21046;&#30340;&#38590;&#20197;&#20197;&#21521;&#37327;&#21270;&#24418;&#24335;&#23454;&#29616;&#30340;&#31639;&#27861;&#12290;SynJax&#36890;&#36807;&#25552;&#20379;&#29992;&#20110;&#32467;&#26500;&#21270;&#20998;&#24067;&#30340;&#25512;&#29702;&#31639;&#27861;&#30340;&#39640;&#25928;&#21521;&#37327;&#21270;&#23454;&#29616;&#26469;&#30452;&#25509;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21253;&#25324;&#23545;&#40784;&#12289;&#26631;&#35760;&#12289;&#20998;&#21106;&#12289;&#32452;&#25104;&#26641;&#21644;&#29983;&#25104;&#26641;&#30340;&#22788;&#29702;&#12290;&#20351;&#29992;SynJax&#65292;&#25105;&#20204;&#21487;&#20197;&#26500;&#24314;&#22823;&#35268;&#27169;&#30340;&#21487;&#24494;&#20998;&#27169;&#22411;&#65292;&#26174;&#24335;&#22320;&#23545;&#25968;&#25454;&#30340;&#32467;&#26500;&#36827;&#34892;&#24314;&#27169;&#12290;&#20195;&#30721;&#21487;&#22312;https://g&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of deep learning software libraries enabled significant progress in the field by allowing users to focus on modeling, while letting the library to take care of the tedious and time-consuming task of optimizing execution for modern hardware accelerators. However, this has benefited only particular types of deep learning models, such as Transformers, whose primitives map easily to the vectorized computation. The models that explicitly account for structured objects, such as trees and segmentations, did not benefit equally because they require custom algorithms that are difficult to implement in a vectorized form.  SynJax directly addresses this problem by providing an efficient vectorized implementation of inference algorithms for structured distributions covering alignment, tagging, segmentation, constituency trees and spanning trees. With SynJax we can build large-scale differentiable models that explicitly model structure in the data. The code is available at https://g
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Encoder-Adaptor-Reconfigurator&#65288;EAR&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#39046;&#22495;&#28418;&#31227;&#19979;&#36827;&#34892;&#39640;&#25928;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#20102;&#22266;&#23450;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#29305;&#24449;&#32534;&#30721;&#22120;&#65292;&#24182;&#22312;&#32534;&#30721;&#22120;&#20043;&#19978;&#35757;&#32451;&#27973;&#23618;&#32593;&#32476;&#26469;&#22788;&#29702;&#26032;&#25968;&#25454;&#12290;&#36890;&#36807;&#32467;&#21512;DNN&#21644;&#36229;&#32500;&#35745;&#31639;&#65288;HDC&#65289;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#26816;&#27979;&#26032;&#25968;&#25454;&#26159;&#21542;&#23646;&#20110;&#20998;&#24067;&#20043;&#22806;&#65288;OOD&#65289;&#65292;&#24182;&#33021;&#22815;&#35782;&#21035;&#20986;... (&#25688;&#35201;&#20869;&#23481;&#30465;&#30053;)</title><link>http://arxiv.org/abs/2308.02084</link><description>&lt;p&gt;
&#22312;&#36793;&#32536;&#31471;&#30340;&#39640;&#25928;&#27169;&#22411;&#36866;&#24212;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Model Adaptation for Continual Learning at the Edge. (arXiv:2308.02084v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02084
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Encoder-Adaptor-Reconfigurator&#65288;EAR&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#39046;&#22495;&#28418;&#31227;&#19979;&#36827;&#34892;&#39640;&#25928;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#20102;&#22266;&#23450;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#29305;&#24449;&#32534;&#30721;&#22120;&#65292;&#24182;&#22312;&#32534;&#30721;&#22120;&#20043;&#19978;&#35757;&#32451;&#27973;&#23618;&#32593;&#32476;&#26469;&#22788;&#29702;&#26032;&#25968;&#25454;&#12290;&#36890;&#36807;&#32467;&#21512;DNN&#21644;&#36229;&#32500;&#35745;&#31639;&#65288;HDC&#65289;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#26816;&#27979;&#26032;&#25968;&#25454;&#26159;&#21542;&#23646;&#20110;&#20998;&#24067;&#20043;&#22806;&#65288;OOD&#65289;&#65292;&#24182;&#33021;&#22815;&#35782;&#21035;&#20986;... (&#25688;&#35201;&#20869;&#23481;&#30465;&#30053;)
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#35757;&#32451;&#21644;&#37096;&#32626;&#36807;&#31243;&#20013;&#20551;&#35774;&#25968;&#25454;&#20998;&#24067;&#26159;&#22266;&#23450;&#21644;&#21305;&#37197;&#30340;&#65292;&#20294;&#36825;&#36890;&#24120;&#26159;&#38169;&#35823;&#30340;&#20551;&#35774;&#12290;&#24403;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37096;&#32626;&#22312;&#30495;&#23454;&#35774;&#22791;&#19978;&#26102;&#65292;&#25968;&#25454;&#20998;&#24067;&#24120;&#24120;&#20250;&#38543;&#26102;&#38388;&#21464;&#21270;&#65292;&#21407;&#22240;&#26159;&#29615;&#22659;&#22240;&#32032;&#12289;&#20256;&#24863;&#22120;&#29305;&#24615;&#21644;&#24863;&#20852;&#36259;&#30340;&#20219;&#21153;&#21457;&#29983;&#20102;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Encoder-Adaptor-Reconfigurator&#65288;EAR&#65289;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#39046;&#22495;&#28418;&#31227;&#19979;&#30340;&#39640;&#25928;&#25345;&#32493;&#23398;&#20064;&#12290;EAR&#26694;&#26550;&#21033;&#29992;&#22266;&#23450;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#29305;&#24449;&#32534;&#30721;&#22120;&#65292;&#24182;&#22312;&#32534;&#30721;&#22120;&#20043;&#19978;&#35757;&#32451;&#27973;&#23618;&#32593;&#32476;&#26469;&#22788;&#29702;&#26032;&#25968;&#25454;&#12290;EAR&#26694;&#26550;&#33021;&#22815;&#36890;&#36807;&#23558;DNN&#19982;&#36229;&#32500;&#35745;&#31639;&#65288;HDC&#65289;&#30456;&#32467;&#21512;&#65292;&#26816;&#27979;&#20986;&#26032;&#25968;&#25454;&#26159;&#21542;&#23646;&#20110;&#20998;&#24067;&#20043;&#22806;&#65288;OOD&#65289;&#65292;&#24182;&#33021;&#22815;&#35782;&#21035;&#20986;
&lt;/p&gt;
&lt;p&gt;
Most machine learning (ML) systems assume stationary and matching data distributions during training and deployment. This is often a false assumption. When ML models are deployed on real devices, data distributions often shift over time due to changes in environmental factors, sensor characteristics, and task-of-interest. While it is possible to have a human-in-the-loop to monitor for distribution shifts and engineer new architectures in response to these shifts, such a setup is not cost-effective. Instead, non-stationary automated ML (AutoML) models are needed. This paper presents the Encoder-Adaptor-Reconfigurator (EAR) framework for efficient continual learning under domain shifts. The EAR framework uses a fixed deep neural network (DNN) feature encoder and trains shallow networks on top of the encoder to handle novel data. The EAR framework is capable of 1) detecting when new data is out-of-distribution (OOD) by combining DNNs with hyperdimensional computing (HDC), 2) identifying l
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20998;&#21306;&#25193;&#25955;&#27169;&#22411;&#65288;CDM&#65289;&#35757;&#32451;&#19981;&#21516;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#22312;&#25512;&#26029;&#26102;&#20219;&#24847;&#32452;&#21512;&#23427;&#20204;&#65292;&#23454;&#29616;&#20102;&#35757;&#32451;&#25968;&#25454;&#20445;&#25252;&#21644;&#36873;&#25321;&#24615;&#36951;&#24536;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#35775;&#38382;&#26435;&#38480;&#25552;&#20379;&#23450;&#21046;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.01937</link><description>&lt;p&gt;
&#20351;&#29992;&#32452;&#21512;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#35757;&#32451;&#25968;&#25454;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Training Data Protection with Compositional Diffusion Models. (arXiv:2308.01937v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01937
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20998;&#21306;&#25193;&#25955;&#27169;&#22411;&#65288;CDM&#65289;&#35757;&#32451;&#19981;&#21516;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#22312;&#25512;&#26029;&#26102;&#20219;&#24847;&#32452;&#21512;&#23427;&#20204;&#65292;&#23454;&#29616;&#20102;&#35757;&#32451;&#25968;&#25454;&#20445;&#25252;&#21644;&#36873;&#25321;&#24615;&#36951;&#24536;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#35775;&#38382;&#26435;&#38480;&#25552;&#20379;&#23450;&#21046;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#21306;&#25193;&#25955;&#27169;&#22411;&#65288;CDM&#65289;&#65292;&#19968;&#31181;&#22312;&#19981;&#21516;&#25968;&#25454;&#28304;&#19978;&#35757;&#32451;&#19981;&#21516;&#25193;&#25955;&#27169;&#22411;&#65288;&#25110;&#25552;&#31034;&#65289;&#24182;&#22312;&#25512;&#26029;&#26102;&#20219;&#24847;&#32452;&#21512;&#23427;&#20204;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#21333;&#29420;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#23396;&#31435;&#29366;&#24577;&#19979;&#12289;&#22312;&#19981;&#21516;&#26102;&#38388;&#12289;&#22312;&#19981;&#21516;&#20998;&#24067;&#21644;&#39046;&#22495;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#21487;&#20197;&#21518;&#32493;&#32452;&#21512;&#20197;&#36798;&#21040;&#19982;&#21516;&#26102;&#35757;&#32451;&#25152;&#26377;&#25968;&#25454;&#30340;&#29702;&#24819;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#27599;&#20010;&#27169;&#22411;&#21482;&#21253;&#21547;&#20854;&#22312;&#35757;&#32451;&#26399;&#38388;&#25509;&#35302;&#21040;&#30340;&#25968;&#25454;&#23376;&#38598;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#31181;&#24418;&#24335;&#30340;&#35757;&#32451;&#25968;&#25454;&#20445;&#25252;&#12290;&#29305;&#21035;&#26159;&#65292;CDM&#26159;&#31532;&#19968;&#31181;&#21487;&#20197;&#23454;&#29616;&#22823;&#35268;&#27169;&#25193;&#25955;&#27169;&#22411;&#30340;&#36873;&#25321;&#24615;&#36951;&#24536;&#21644;&#25345;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#20801;&#35768;&#26681;&#25454;&#29992;&#25143;&#35775;&#38382;&#26435;&#38480;&#25552;&#20379;&#23450;&#21046;&#27169;&#22411;&#12290;CDM&#36824;&#21487;&#20197;&#30830;&#23450;&#29983;&#25104;&#29305;&#23450;&#26679;&#26412;&#30340;&#25968;&#25454;&#23376;&#38598;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Compartmentalized Diffusion Models (CDM), a method to train different diffusion models (or prompts) on distinct data sources and arbitrarily compose them at inference time. The individual models can be trained in isolation, at different times, and on different distributions and domains and can be later composed to achieve performance comparable to a paragon model trained on all data simultaneously. Furthermore, each model only contains information about the subset of the data it was exposed to during training, enabling several forms of training data protection. In particular, CDMs are the first method to enable both selective forgetting and continual learning for large-scale diffusion models, as well as allowing serving customized models based on the user's access rights. CDMs also allow determining the importance of a subset of the data in generating particular samples.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;-ODE&#36827;&#34892;&#32959;&#30244;&#21160;&#21147;&#24314;&#27169;&#21644;&#25972;&#20307;&#29983;&#23384;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#25130;&#26029;&#25968;&#25454;&#20013;&#36827;&#34892;&#26080;&#20559;&#39044;&#27979;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#34701;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#26377;&#21407;&#21017;&#30340;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2308.01362</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#32959;&#30244;&#21160;&#21147;&#24314;&#27169;&#21644;&#20351;&#29992;&#31070;&#32463;-ODE&#36827;&#34892;&#25972;&#20307;&#29983;&#23384;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Explainable Deep Learning for Tumor Dynamic Modeling and Overall Survival Prediction using Neural-ODE. (arXiv:2308.01362v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01362
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;-ODE&#36827;&#34892;&#32959;&#30244;&#21160;&#21147;&#24314;&#27169;&#21644;&#25972;&#20307;&#29983;&#23384;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#25130;&#26029;&#25968;&#25454;&#20013;&#36827;&#34892;&#26080;&#20559;&#39044;&#27979;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#34701;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#26377;&#21407;&#21017;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#32959;&#30244;&#21160;&#21147;&#24314;&#27169;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#25903;&#25345;&#32959;&#30244;&#33647;&#29289;&#30340;&#24320;&#21457;&#65292;&#20294;&#20173;&#28982;&#38656;&#35201;&#22686;&#21152;&#39044;&#27979;&#33021;&#21147;&#65292;&#23454;&#29616;&#20010;&#24615;&#21270;&#27835;&#30103;&#24182;&#25913;&#21892;&#20915;&#31574;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#32959;&#30244;&#21160;&#21147;&#31070;&#32463;-ODE&#65288;TDNODE&#65289;&#20316;&#20026;&#19968;&#31181;&#33647;&#29702;&#23398;&#20449;&#24687;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#20174;&#32437;&#21521;&#32959;&#30244;&#22823;&#23567;&#25968;&#25454;&#20013;&#23454;&#29616;&#27169;&#22411;&#21457;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;TDNODE&#22312;&#20811;&#26381;&#29616;&#26377;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#38480;&#21046;&#19978;&#30340;&#33021;&#21147;&#65292;&#21363;&#33021;&#22815;&#20174;&#25130;&#26029;&#25968;&#25454;&#20013;&#36827;&#34892;&#26080;&#20559;&#39044;&#27979;&#12290;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#35774;&#35745;&#29992;&#20110;&#34920;&#36798;&#20855;&#26377;&#26102;&#38388;&#30340;&#24191;&#20041;&#40784;&#27425;&#24615;&#36825;&#19968;&#22522;&#26412;&#29305;&#24615;&#30340;&#22522;&#30784;&#21160;&#21147;&#23398;&#23450;&#24459;&#12290;&#22240;&#27492;&#65292;&#24314;&#27169;&#24418;&#24335;&#20351;&#24471;&#32534;&#30721;&#22120;&#36755;&#20986;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#21160;&#21147;&#23398;&#36895;&#29575;&#25351;&#26631;&#65292;&#20854;&#20013;&#20498;&#25968;&#26102;&#38388;&#20316;&#20026;&#29289;&#29702;&#21333;&#20301;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29983;&#25104;&#30340;&#25351;&#26631;&#21487;&#20197;&#39640;&#20934;&#30830;&#24230;&#22320;&#29992;&#20110;&#39044;&#27979;&#24739;&#32773;&#30340;&#25972;&#20307;&#29983;&#23384;&#12290;&#25152;&#25552;&#20986;&#30340;&#24314;&#27169;&#24418;&#24335;&#20026;&#34701;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
While tumor dynamic modeling has been widely applied to support the development of oncology drugs, there remains a need to increase predictivity, enable personalized therapy, and improve decision-making. We propose the use of Tumor Dynamic Neural-ODE (TDNODE) as a pharmacology-informed neural network to enable model discovery from longitudinal tumor size data. We show that TDNODE overcomes a key limitation of existing models in its ability to make unbiased predictions from truncated data. The encoder-decoder architecture is designed to express an underlying dynamical law which possesses the fundamental property of generalized homogeneity with respect to time. Thus, the modeling formalism enables the encoder output to be interpreted as kinetic rate metrics, with inverse time as the physical unit. We show that the generated metrics can be used to predict patients' overall survival (OS) with high accuracy. The proposed modeling formalism provides a principled way to integrate multimodal d
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#38646;&#24320;&#22987;&#21457;&#29616;&#38646;-shot&#21487;&#36866;&#24212;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28436;&#21270;&#27169;&#22359;&#21270;&#31574;&#30053;&#26469;&#26500;&#24314;&#20855;&#26377;&#32447;&#24615;&#23492;&#23384;&#22120;&#26426;&#30340;&#25511;&#21046;&#31639;&#27861;&#65292;&#24182;&#26681;&#25454;&#29615;&#22659;&#21464;&#21270;&#21363;&#26102;&#35843;&#25972;&#27169;&#22411;&#21442;&#25968;&#21644;&#25512;&#29702;&#31639;&#27861;&#12290;&#22312;&#36924;&#30495;&#30340;&#20223;&#30495;&#22235;&#36275;&#26426;&#22120;&#20154;&#19978;&#28436;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#21517;&#20026;Cataclysmic Cartpole&#30340;&#38750;&#31283;&#24577;&#25511;&#21046;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2307.16890</link><description>&lt;p&gt;
&#20174;&#38646;&#24320;&#22987;&#21457;&#29616;&#36866;&#24212;&#24615;&#31526;&#21495;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Discovering Adaptable Symbolic Algorithms from Scratch. (arXiv:2307.16890v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16890
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#38646;&#24320;&#22987;&#21457;&#29616;&#38646;-shot&#21487;&#36866;&#24212;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28436;&#21270;&#27169;&#22359;&#21270;&#31574;&#30053;&#26469;&#26500;&#24314;&#20855;&#26377;&#32447;&#24615;&#23492;&#23384;&#22120;&#26426;&#30340;&#25511;&#21046;&#31639;&#27861;&#65292;&#24182;&#26681;&#25454;&#29615;&#22659;&#21464;&#21270;&#21363;&#26102;&#35843;&#25972;&#27169;&#22411;&#21442;&#25968;&#21644;&#25512;&#29702;&#31639;&#27861;&#12290;&#22312;&#36924;&#30495;&#30340;&#20223;&#30495;&#22235;&#36275;&#26426;&#22120;&#20154;&#19978;&#28436;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#21517;&#20026;Cataclysmic Cartpole&#30340;&#38750;&#31283;&#24577;&#25511;&#21046;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#37096;&#32626;&#30340;&#33258;&#20027;&#26426;&#22120;&#20154;&#23558;&#38656;&#35201;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#29615;&#22659;&#21464;&#21270;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoRobotics-Zero&#65288;ARZ&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;AutoML-Zero&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#38646;&#24320;&#22987;&#21457;&#29616;&#38646;-shot&#21487;&#36866;&#24212;&#31574;&#30053;&#12290;&#19982;&#21482;&#20248;&#21270;&#27169;&#22411;&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#36866;&#24212;&#31574;&#30053;&#19981;&#21516;&#65292;ARZ&#21487;&#20197;&#26500;&#24314;&#20855;&#26377;&#32447;&#24615;&#23492;&#23384;&#22120;&#26426;&#30340;&#23436;&#20840;&#34920;&#36798;&#33021;&#21147;&#30340;&#25511;&#21046;&#31639;&#27861;&#12290;&#25105;&#20204;&#28436;&#21270;&#27169;&#22359;&#21270;&#31574;&#30053;&#65292;&#36890;&#36807;&#35843;&#25972;&#27169;&#22411;&#21442;&#25968;&#21644;&#21363;&#26102;&#20462;&#25913;&#25512;&#29702;&#31639;&#27861;&#26469;&#36866;&#24212;&#31361;&#21457;&#29615;&#22659;&#21464;&#21270;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#36924;&#30495;&#30340;&#20223;&#30495;&#22235;&#36275;&#26426;&#22120;&#20154;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28436;&#21270;&#20986;&#36991;&#20813;&#22312;&#21333;&#20010;&#32930;&#20307;&#31361;&#28982;&#26029;&#35010;&#26102;&#25684;&#20498;&#30340;&#23433;&#20840;&#25511;&#21046;&#31574;&#30053;&#12290;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20004;&#31181;&#27969;&#34892;&#30340;&#31070;&#32463;&#32593;&#32476;&#22522;&#20934;&#26041;&#27861;&#37117;&#22833;&#36133;&#20102;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#19968;&#39033;&#21517;&#20026;Cataclysmic Cartpole&#30340;&#26032;&#39062;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38750;&#31283;&#24577;&#25511;&#21046;&#20219;&#21153;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;&#32467;&#26524;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous robots deployed in the real world will need control policies that rapidly adapt to environmental changes. To this end, we propose AutoRobotics-Zero (ARZ), a method based on AutoML-Zero that discovers zero-shot adaptable policies from scratch. In contrast to neural network adaptation policies, where only model parameters are optimized, ARZ can build control algorithms with the full expressive power of a linear register machine. We evolve modular policies that tune their model parameters and alter their inference algorithm on-the-fly to adapt to sudden environmental changes. We demonstrate our method on a realistic simulated quadruped robot, for which we evolve safe control policies that avoid falling when individual limbs suddenly break. This is a challenging task in which two popular neural network baselines fail. Finally, we conduct a detailed analysis of our method on a novel and challenging non-stationary control task dubbed Cataclysmic Cartpole. Results confirm our findi
&lt;/p&gt;</description></item><item><title>Med-HALT&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#21644;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#20943;&#23569;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#21307;&#30103;&#39046;&#22495;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21253;&#25324;&#22810;&#31181;&#21019;&#26032;&#30340;&#27979;&#35797;&#27169;&#24335;&#65292;&#24182;&#35780;&#20272;&#20102;&#39046;&#20808;&#30340;LLMs&#22312;&#24615;&#33021;&#19978;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2307.15343</link><description>&lt;p&gt;
Med-HALT:&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#21307;&#30103;&#39046;&#22495;&#24187;&#35273;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Med-HALT: Medical Domain Hallucination Test for Large Language Models. (arXiv:2307.15343v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15343
&lt;/p&gt;
&lt;p&gt;
Med-HALT&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#21644;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#20943;&#23569;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#21307;&#30103;&#39046;&#22495;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21253;&#25324;&#22810;&#31181;&#21019;&#26032;&#30340;&#27979;&#35797;&#27169;&#24335;&#65292;&#24182;&#35780;&#20272;&#20102;&#39046;&#20808;&#30340;LLMs&#22312;&#24615;&#33021;&#19978;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35770;&#25991;&#20851;&#27880;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#32972;&#26223;&#19979;&#12290;&#24187;&#35273;&#25351;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#20102;&#21512;&#29702;&#20294;&#26410;&#32463;&#39564;&#35777;&#25110;&#38169;&#35823;&#30340;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#23545;&#21307;&#30103;&#24212;&#29992;&#20135;&#29983;&#20005;&#37325;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#21644;&#25968;&#25454;&#38598;&#65292;Med-HALT&#65288;&#21307;&#30103;&#39046;&#22495;&#24187;&#35273;&#27979;&#35797;&#65289;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#21644;&#20943;&#23569;&#24187;&#35273;&#12290;Med-HALT&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#20803;&#21270;&#30340;&#36328;&#22269;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26469;&#33258;&#19981;&#21516;&#22269;&#23478;&#30340;&#21307;&#30103;&#26816;&#26597;&#65292;&#21253;&#25324;&#22810;&#31181;&#21019;&#26032;&#30340;&#27979;&#35797;&#27169;&#24335;&#12290;Med-HALT&#21253;&#25324;&#20004;&#31867;&#27979;&#35797;&#65306;&#25512;&#29702;&#21644;&#22522;&#20110;&#35760;&#24518;&#30340;&#24187;&#35273;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#30340;&#38382;&#39064;&#35299;&#20915;&#21644;&#20449;&#24687;&#26816;&#32034;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35780;&#20272;&#20102;&#25991;&#26412;Davinci&#65292;GPT-3.5&#65292;LlaMa-2&#65292;MPT&#21644;Falcon&#31561;&#39046;&#20808;&#30340;LLMs&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#24615;&#33021;&#19978;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#26377;&#20851;&#25968;&#25454;&#38598;&#30340;&#35814;&#32454;&#35265;&#35299;&#65292;&#20419;&#36827;&#20102;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research paper focuses on the challenges posed by hallucinations in large language models (LLMs), particularly in the context of the medical domain. Hallucination, wherein these models generate plausible yet unverified or incorrect information, can have serious consequences in healthcare applications. We propose a new benchmark and dataset, Med-HALT (Medical Domain Hallucination Test), designed specifically to evaluate and reduce hallucinations. Med-HALT provides a diverse multinational dataset derived from medical examinations across various countries and includes multiple innovative testing modalities. Med-HALT includes two categories of tests reasoning and memory-based hallucination tests, designed to assess LLMs's problem-solving and information retrieval abilities.  Our study evaluated leading LLMs, including Text Davinci, GPT-3.5, LlaMa-2, MPT, and Falcon, revealing significant differences in their performance. The paper provides detailed insights into the dataset, promoting
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;Schr\"odinger&#26725;&#25509;&#31639;&#27861;&#65292;&#24212;&#29992;&#20110;&#33014;&#20307;&#33258;&#32452;&#35013;&#30340;&#26368;&#23567;&#24037;&#20316;&#25511;&#21046;&#38382;&#39064;&#20013;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#33014;&#20307;&#33258;&#32452;&#35013;&#20013;&#30340;&#38750;&#32447;&#24615;&#25511;&#21046;&#31995;&#25968;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#23398;&#20064;&#21644;&#25511;&#21046;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2307.14442</link><description>&lt;p&gt;
&#31070;&#32463;Schr\"odinger&#26725;&#25509;&#37197;&#23545;Sinkhorn&#25439;&#22833;&#65306;&#24212;&#29992;&#20110;&#33014;&#20307;&#33258;&#32452;&#35013;&#30340;&#25968;&#25454;&#39537;&#21160;&#26368;&#23567;&#24037;&#20316;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Neural Schr\"odinger Bridge with Sinkhorn Losses: Application to Data-driven Minimum Effort Control of Colloidal Self-assembly. (arXiv:2307.14442v1 [math.OC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;Schr\"odinger&#26725;&#25509;&#31639;&#27861;&#65292;&#24212;&#29992;&#20110;&#33014;&#20307;&#33258;&#32452;&#35013;&#30340;&#26368;&#23567;&#24037;&#20316;&#25511;&#21046;&#38382;&#39064;&#20013;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#33014;&#20307;&#33258;&#32452;&#35013;&#20013;&#30340;&#38750;&#32447;&#24615;&#25511;&#21046;&#31995;&#25968;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#23398;&#20064;&#21644;&#25511;&#21046;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#33014;&#20307;&#33258;&#32452;&#35013;&#30340;&#26368;&#23567;&#24037;&#20316;&#25511;&#21046;&#21487;&#20197;&#33258;&#28982;&#22320;&#22312;&#27425;&#24207;&#21442;&#25968;&#31354;&#38388;&#20013;&#20197;&#24191;&#20041;Schr\"odinger&#26725;&#25509;&#38382;&#39064;&#30340;&#24418;&#24335;&#34920;&#36798; - &#36825;&#26159;&#19968;&#31867;&#22312;30&#24180;&#20195;&#26411;Erwin Schr\"odinger&#30340;&#20316;&#21697;&#20013;&#36215;&#28304;&#30340;&#22266;&#23450;&#26102;&#22495;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#12290;&#22312;&#36817;&#24180;&#26469;&#65292;&#36825;&#31867;&#38382;&#39064;&#22312;&#25511;&#21046;&#21644;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20013;&#37325;&#26032;&#20852;&#36215;&#20102;&#30740;&#31350;&#27963;&#21160;&#12290;&#19982;&#29616;&#26377;&#25991;&#29486;&#20013;&#20851;&#20110;&#36825;&#31867;&#38382;&#39064;&#30340;&#29702;&#35770;&#21644;&#35745;&#31639;&#26377;&#25152;&#19981;&#21516;&#65292;&#33014;&#20307;&#33258;&#32452;&#35013;&#30340;&#25511;&#21046;&#28418;&#31227;&#21644;&#25193;&#25955;&#31995;&#25968;&#36890;&#24120;&#22312;&#25511;&#21046;&#26041;&#38754;&#26159;&#38750;&#32447;&#24615;&#30340;&#65292;&#19988;&#38590;&#20197;&#20174;&#22522;&#20110;&#29289;&#29702;&#24314;&#27169;&#20013;&#33719;&#24471;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#36825;&#31867;&#24191;&#20041;&#38382;&#39064;&#30340;&#26368;&#20248;&#24615;&#26465;&#20214;&#65292;&#24182;&#23637;&#31034;&#20102;&#23548;&#33268;&#30340;&#26041;&#31243;&#31995;&#32479;&#22312;&#32467;&#26500;&#19978;&#19982;&#29616;&#26377;&#32467;&#26524;&#23436;&#20840;&#19981;&#21516;&#65292;&#26631;&#20934;&#30340;&#35745;&#31639;&#26041;&#27861;&#19981;&#20877;&#36866;&#29992;&#12290;&#20986;&#20110;&#36825;&#20010;&#21160;&#26426;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#23398;&#20064;&#21644;&#25511;&#21046;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that the minimum effort control of colloidal self-assembly can be naturally formulated in the order-parameter space as a generalized Schr\"odinger bridge problem -- a class of fixed-horizon stochastic optimal control problems that originated in the works of Erwin Schr\"odinger in the early 1930s. In recent years, this class of problems has seen a resurgence of research activities in control and machine learning communities. Different from the existing literature on the theory and computation for such problems, the controlled drift and diffusion coefficients for colloidal self-assembly are typically non-affine in control, and are difficult to obtain from physics-based modeling. We deduce the conditions of optimality for such generalized problems, and show that the resulting system of equations is structurally very different from the existing results in a way that standard computational approaches no longer apply. Thus motivated, we propose a data-driven learning and control fram
&lt;/p&gt;</description></item><item><title>QAmplifyNet&#26159;&#19968;&#31181;&#20351;&#29992;&#37327;&#23376;&#21551;&#21457;&#25216;&#26415;&#30340;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#39640;&#25928;&#39044;&#27979;&#20379;&#24212;&#38142;&#32570;&#36135;&#12290;&#23427;&#22312;&#22788;&#29702;&#30701;&#26102;&#38388;&#12289;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.12906</link><description>&lt;p&gt;
QAmplifyNet&#65306;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#25512;&#21160;&#20379;&#24212;&#38142;&#32570;&#36135;&#39044;&#27979;&#30340;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
QAmplifyNet: Pushing the Boundaries of Supply Chain Backorder Prediction Using Interpretable Hybrid Quantum-Classical Neural Network. (arXiv:2307.12906v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12906
&lt;/p&gt;
&lt;p&gt;
QAmplifyNet&#26159;&#19968;&#31181;&#20351;&#29992;&#37327;&#23376;&#21551;&#21457;&#25216;&#26415;&#30340;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#39640;&#25928;&#39044;&#27979;&#20379;&#24212;&#38142;&#32570;&#36135;&#12290;&#23427;&#22312;&#22788;&#29702;&#30701;&#26102;&#38388;&#12289;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20379;&#24212;&#38142;&#31649;&#29702;&#20381;&#36182;&#20110;&#20934;&#30830;&#30340;&#32570;&#36135;&#39044;&#27979;&#65292;&#20197;&#20248;&#21270;&#24211;&#23384;&#25511;&#21046;&#65292;&#38477;&#20302;&#25104;&#26412;&#24182;&#25552;&#39640;&#23458;&#25143;&#28385;&#24847;&#24230;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#22797;&#26434;&#20851;&#31995;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#38480;&#21046;&#20102;&#23454;&#38469;&#25968;&#25454;&#25910;&#38598;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20379;&#24212;&#38142;&#32570;&#36135;&#39044;&#27979;&#26041;&#27861;&#35770;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;QAmplifyNet&#22312;&#19968;&#20010;&#37327;&#23376;-&#32463;&#20856;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;&#20102;&#37327;&#23376;&#21551;&#21457;&#24335;&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#30701;&#26102;&#38388;&#21644;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#26377;&#25928;&#39044;&#27979;&#32570;&#36135;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35780;&#20272;&#26174;&#31034;&#65292;QAmplifyNet&#22312;&#22788;&#29702;&#30701;&#26102;&#38388;&#12289;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#27169;&#22411;&#12289;&#37327;&#23376;&#38598;&#25104;&#12289;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#12290;&#20854;&#22312;&#22788;&#29702;&#30701;&#26102;&#38388;&#12289;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#33021;&#21147;&#20351;&#20854;&#25104;&#20026;&#20379;&#24212;&#38142;&#31649;&#29702;&#30340;&#29702;&#24819;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#22686;&#24378;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supply chain management relies on accurate backorder prediction for optimizing inventory control, reducing costs, and enhancing customer satisfaction. However, traditional machine-learning models struggle with large-scale datasets and complex relationships, hindering real-world data collection. This research introduces a novel methodological framework for supply chain backorder prediction, addressing the challenge of handling large datasets. Our proposed model, QAmplifyNet, employs quantum-inspired techniques within a quantum-classical neural network to predict backorders effectively on short and imbalanced datasets. Experimental evaluations on a benchmark dataset demonstrate QAmplifyNet's superiority over classical models, quantum ensembles, quantum neural networks, and deep reinforcement learning. Its proficiency in handling short, imbalanced datasets makes it an ideal solution for supply chain management. To enhance model interpretability, we use Explainable Artificial Intelligence 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;Temporal Graph Benchmark&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#36890;&#36807;&#25193;&#23637;&#21160;&#24577;&#22270;&#24211;(DyGLib)&#21040;TGB&#65292;&#24182;&#20351;&#29992;&#21313;&#19968;&#31181;&#21160;&#24577;&#22270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#12290;&#23454;&#39564;&#21457;&#29616;&#19981;&#21516;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#24615;&#33021;&#65292;&#19968;&#20123;&#22522;&#32447;&#26041;&#27861;&#30340;&#24615;&#33021;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;DyGLib&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2307.12510</link><description>&lt;p&gt;
Temporal Graph Benchmark&#30340;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Empirical Evaluation of Temporal Graph Benchmark. (arXiv:2307.12510v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;Temporal Graph Benchmark&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#36890;&#36807;&#25193;&#23637;&#21160;&#24577;&#22270;&#24211;(DyGLib)&#21040;TGB&#65292;&#24182;&#20351;&#29992;&#21313;&#19968;&#31181;&#21160;&#24577;&#22270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#12290;&#23454;&#39564;&#21457;&#29616;&#19981;&#21516;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#24615;&#33021;&#65292;&#19968;&#20123;&#22522;&#32447;&#26041;&#27861;&#30340;&#24615;&#33021;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;DyGLib&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#21160;&#24577;&#22270;&#24211;(DyGLib)&#25193;&#23637;&#21040;Temporal Graph Benchmark (TGB)&#65292;&#23545;TGB&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;&#19982;TGB&#30456;&#27604;&#65292;&#25105;&#20204;&#21253;&#25324;&#20102;&#21313;&#19968;&#31181;&#27969;&#34892;&#30340;&#21160;&#24577;&#22270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#26356;&#20840;&#38754;&#30340;&#27604;&#36739;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#65306;&#65288;1&#65289;&#19981;&#21516;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#24615;&#33021;&#65292;&#36825;&#19982;&#20043;&#21069;&#30340;&#35266;&#23519;&#19968;&#33268;&#65307;&#65288;2&#65289;&#20351;&#29992;DyGLib&#26102;&#65292;&#19968;&#20123;&#22522;&#32447;&#26041;&#27861;&#30340;&#24615;&#33021;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#12290;&#26412;&#24037;&#20316;&#26088;&#22312;&#26041;&#20415;&#30740;&#31350;&#20154;&#21592;&#22312;TGB&#19978;&#35780;&#20272;&#21508;&#31181;&#21160;&#24577;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35797;&#22270;&#25552;&#20379;&#21487;&#30452;&#25509;&#21442;&#32771;&#30340;&#32467;&#26524;&#20379;&#21518;&#32493;&#30740;&#31350;&#20351;&#29992;&#12290;&#26412;&#39033;&#30446;&#20013;&#20351;&#29992;&#30340;&#25152;&#26377;&#36164;&#28304;&#22343;&#21487;&#22312;https://github.com/yule-BUAA/DyGLib_TGB&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;&#26412;&#24037;&#20316;&#27491;&#22312;&#36827;&#34892;&#20013;&#65292;&#27426;&#36814;&#31038;&#21306;&#25552;&#20379;&#21453;&#39304;&#20197;&#36827;&#34892;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we conduct an empirical evaluation of Temporal Graph Benchmark (TGB) by extending our Dynamic Graph Library (DyGLib) to TGB. Compared with TGB, we include eleven popular dynamic graph learning methods for more exhaustive comparisons. Through the experiments, we find that (1) different models depict varying performance across various datasets, which is in line with previous observations; (2) the performance of some baselines can be significantly improved over the reported results in TGB when using DyGLib. This work aims to ease the researchers' efforts in evaluating various dynamic graph learning methods on TGB and attempts to offer results that can be directly referenced in the follow-up research. All the used resources in this project are publicly available at https://github.com/yule-BUAA/DyGLib_TGB. This work is in progress, and feedback from the community is welcomed for improvements.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22635;&#34917;&#35770;&#25991;&#39046;&#22495;&#20013;&#30340;&#31354;&#30333;&#65292;&#25552;&#20986;&#20102;11&#20010;&#26032;&#30340;&#38598;&#25104;&#23398;&#20064;&#31639;&#27861;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#65292;&#26088;&#22312;&#25913;&#36827;&#21355;&#26143;&#38477;&#27700;&#20135;&#21697;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.06840</link><description>&lt;p&gt;
&#38598;&#25104;&#23398;&#20064;&#29992;&#20110;&#28151;&#21512;&#26684;&#32593;&#21355;&#26143;&#21644;&#27979;&#37327;&#38477;&#27700;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Ensemble learning for blending gridded satellite and gauge-measured precipitation data. (arXiv:2307.06840v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22635;&#34917;&#35770;&#25991;&#39046;&#22495;&#20013;&#30340;&#31354;&#30333;&#65292;&#25552;&#20986;&#20102;11&#20010;&#26032;&#30340;&#38598;&#25104;&#23398;&#20064;&#31639;&#27861;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#65292;&#26088;&#22312;&#25913;&#36827;&#21355;&#26143;&#38477;&#27700;&#20135;&#21697;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25913;&#21892;&#21355;&#26143;&#38477;&#27700;&#20135;&#21697;&#20934;&#30830;&#24615;&#26041;&#38754;&#65292;&#24120;&#24120;&#20351;&#29992;&#22238;&#24402;&#31639;&#27861;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#22320;&#38754;&#30340;&#27979;&#37327;&#26159;&#22240;&#21464;&#37327;&#65292;&#21355;&#26143;&#25968;&#25454;&#26159;&#39044;&#27979;&#21464;&#37327;&#65292;&#36824;&#26377;&#22320;&#24418;&#22240;&#32032;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21508;&#20010;&#39046;&#22495;&#36234;&#26469;&#36234;&#35748;&#35782;&#21040;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#23558;&#22810;&#20010;&#31639;&#27861;&#32467;&#21512;&#36215;&#26469;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#25991;&#29486;&#20013;&#32570;&#20047;&#36275;&#22815;&#25968;&#37327;&#30340;&#38598;&#25104;&#23398;&#20064;&#31639;&#27861;&#20197;&#25552;&#39640;&#21355;&#26143;&#38477;&#27700;&#20135;&#21697;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#23545;&#23427;&#20204;&#36827;&#34892;&#22823;&#35268;&#27169;&#27604;&#36739;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#25972;&#20010;&#32654;&#22269;&#21644;15&#24180;&#26102;&#38388;&#27573;&#20869;&#36827;&#34892;&#24191;&#27867;&#27604;&#36739;&#65292;&#22635;&#34917;&#20102;&#36825;&#20010;&#29305;&#23450;&#30340;&#31354;&#30333;&#65292;&#25552;&#20986;&#20102;11&#20010;&#26032;&#30340;&#38598;&#25104;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;PERSIANN&#65288;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#36965;&#24863;&#20449;&#24687;&#20272;&#35745;&#38477;&#27700;&#65289;&#21644;IMERG&#65288;&#22810;&#26143;&#32852;&#21512;&#21453;&#28436;&#20272;&#35745;&#38477;&#27700;&#65289;&#30340;&#26376;&#24230;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regression algorithms are regularly used for improving the accuracy of satellite precipitation products. In this context, ground-based measurements are the dependent variable and the satellite data are the predictor variables, together with topography factors. Alongside this, it is increasingly recognised in many fields that combinations of algorithms through ensemble learning can lead to substantial predictive performance improvements. Still, a sufficient number of ensemble learners for improving the accuracy of satellite precipitation products and their large-scale comparison are currently missing from the literature. In this work, we fill this specific gap by proposing 11 new ensemble learners in the field and by extensively comparing them for the entire contiguous United States and for a 15-year period. We use monthly data from the PERSIANN (Precipitation Estimation from Remotely Sensed Information using Artificial Neural Networks) and IMERG (Integrated Multi-satellitE Retrievals f
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#31169;&#26377;&#30340;&#35299;&#32806;&#22270;&#21367;&#31215;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#31890;&#24230;&#25299;&#25169;&#20445;&#25252;&#12290;&#24341;&#20837;&#20102;&#22270;&#24046;&#20998;&#38544;&#31169;&#26694;&#26550;&#65292;&#21487;&#20197;&#30830;&#20445;&#27169;&#22411;&#21442;&#25968;&#21644;&#39044;&#27979;&#30340;&#31169;&#23494;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.06422</link><description>&lt;p&gt;
&#24046;&#20998;&#31169;&#26377;&#30340;&#35299;&#32806;&#22270;&#21367;&#31215;&#29992;&#20110;&#22810;&#31890;&#24230;&#25299;&#25169;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Decoupled Graph Convolutions for Multigranular Topology Protection. (arXiv:2307.06422v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#31169;&#26377;&#30340;&#35299;&#32806;&#22270;&#21367;&#31215;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#31890;&#24230;&#25299;&#25169;&#20445;&#25252;&#12290;&#24341;&#20837;&#20102;&#22270;&#24046;&#20998;&#38544;&#31169;&#26694;&#26550;&#65292;&#21487;&#20197;&#30830;&#20445;&#27169;&#22411;&#21442;&#25968;&#21644;&#39044;&#27979;&#30340;&#31169;&#23494;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#22522;&#20110;&#22270;&#21367;&#31215;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#22312;&#35299;&#20915;&#28041;&#21450;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#23454;&#38469;&#23398;&#20064;&#38382;&#39064;&#26041;&#38754;&#38750;&#24120;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22270;&#23398;&#20064;&#26041;&#27861;&#19981;&#20165;&#36890;&#36807;&#20854;&#27169;&#22411;&#21442;&#25968;&#65292;&#36824;&#36890;&#36807;&#20854;&#27169;&#22411;&#39044;&#27979;&#26292;&#38706;&#20102;&#25935;&#24863;&#30340;&#29992;&#25143;&#20449;&#24687;&#21644;&#20132;&#20114;&#12290;&#22240;&#27492;&#65292;&#20165;&#25552;&#20379;&#27169;&#22411;&#26435;&#37325;&#38544;&#31169;&#30340;&#26631;&#20934;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#25216;&#26415;&#26159;&#19981;&#20805;&#20998;&#30340;&#12290;&#36825;&#23588;&#20854;&#36866;&#29992;&#20110;&#36890;&#36807;&#22270;&#21367;&#31215;&#30452;&#25509;&#21033;&#29992;&#30456;&#37051;&#33410;&#28857;&#23646;&#24615;&#36827;&#34892;&#33410;&#28857;&#39044;&#27979;&#30340;&#24773;&#20917;&#65292;&#36825;&#20250;&#24102;&#26469;&#39069;&#22806;&#30340;&#38544;&#31169;&#27844;&#38706;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22270;&#24046;&#20998;&#38544;&#31169;&#65288;GDP&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#36866;&#29992;&#20110;&#22270;&#23398;&#20064;&#29615;&#22659;&#30340;&#24418;&#24335;&#21270;&#24046;&#20998;&#38544;&#31169;&#26694;&#26550;&#65292;&#21487;&#20197;&#30830;&#20445;&#27169;&#22411;&#21442;&#25968;&#21644;&#39044;&#27979;&#37117;&#26159;&#21487;&#35777;&#26126;&#30340;&#31169;&#26377;&#30340;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#33410;&#28857;&#23646;&#24615;&#21644;&#22270;&#32467;&#26500;&#21487;&#33021;&#23384;&#22312;&#19981;&#21516;&#30340;&#38544;&#31169;&#35201;&#27714;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25918;&#26494;&#30340;&#33410;&#28857;&#23618;&#27425;&#38544;&#31169;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph learning methods, such as Graph Neural Networks (GNNs) based on graph convolutions, are highly successful in solving real-world learning problems involving graph-structured data. However, graph learning methods expose sensitive user information and interactions not only through their model parameters but also through their model predictions. Consequently, standard Differential Privacy (DP) techniques that merely offer model weight privacy are inadequate. This is especially the case for node predictions that leverage neighboring node attributes directly via graph convolutions that create additional risks of privacy leakage. To address this problem, we introduce Graph Differential Privacy (GDP), a new formal DP framework tailored to graph learning settings that ensures both provably private model parameters and predictions. Furthermore, since there may be different privacy requirements for the node attributes and graph structure, we introduce a novel notion of relaxed node-level da
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;&#32534;&#30721;&#25968;&#25454;&#32467;&#26500;&#30340;&#21464;&#20998;&#37327;&#23376;&#22238;&#24402;&#31639;&#27861;&#65292;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#27169;&#22411;&#35299;&#37322;&#24615;&#65292;&#24182;&#33021;&#26377;&#25928;&#22320;&#22788;&#29702;&#20114;&#36830;&#24230;&#36739;&#39640;&#30340;&#37327;&#23376;&#27604;&#29305;&#12290;&#31639;&#27861;&#36890;&#36807;&#21387;&#32553;&#32534;&#30721;&#21644;&#25968;&#23383;-&#27169;&#25311;&#38376;&#25805;&#20316;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#22312;&#22122;&#22768;&#20013;&#23610;&#24230;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#30340;&#36816;&#34892;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.03334</link><description>&lt;p&gt;
&#20855;&#26377;&#32534;&#30721;&#25968;&#25454;&#32467;&#26500;&#30340;&#21464;&#20998;&#37327;&#23376;&#22238;&#24402;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Variational quantum regression algorithm with encoded data structure. (arXiv:2307.03334v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;&#32534;&#30721;&#25968;&#25454;&#32467;&#26500;&#30340;&#21464;&#20998;&#37327;&#23376;&#22238;&#24402;&#31639;&#27861;&#65292;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#27169;&#22411;&#35299;&#37322;&#24615;&#65292;&#24182;&#33021;&#26377;&#25928;&#22320;&#22788;&#29702;&#20114;&#36830;&#24230;&#36739;&#39640;&#30340;&#37327;&#23376;&#27604;&#29305;&#12290;&#31639;&#27861;&#36890;&#36807;&#21387;&#32553;&#32534;&#30721;&#21644;&#25968;&#23383;-&#27169;&#25311;&#38376;&#25805;&#20316;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#22312;&#22122;&#22768;&#20013;&#23610;&#24230;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#30340;&#36816;&#34892;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;(VQAs)&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#20915;&#23454;&#38469;&#38382;&#39064;&#65292;&#22914;&#32452;&#21512;&#20248;&#21270;&#12289;&#37327;&#23376;&#21270;&#23398;&#27169;&#25311;&#12289;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#21644;&#22122;&#22768;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#30340;&#37327;&#23376;&#38169;&#35823;&#32416;&#27491;&#12290;&#23545;&#20110;&#21464;&#20998;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65292;&#23578;&#26410;&#24320;&#21457;&#20986;&#23558;&#27169;&#22411;&#35299;&#37322;&#24615;&#20869;&#23884;&#21040;&#31639;&#27861;&#20013;&#30340;&#21464;&#20998;&#31639;&#27861;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#37327;&#23376;&#22238;&#24402;&#31639;&#27861;&#65292;&#24182;&#30830;&#23450;&#20102;&#21464;&#20998;&#21442;&#25968;&#19982;&#23398;&#20064;&#22238;&#24402;&#31995;&#25968;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#65292;&#21516;&#26102;&#37319;&#29992;&#20102;&#23558;&#25968;&#25454;&#30452;&#25509;&#32534;&#30721;&#20026;&#21453;&#26144;&#32463;&#20856;&#25968;&#25454;&#34920;&#32467;&#26500;&#30340;&#37327;&#23376;&#24133;&#24230;&#30340;&#30005;&#36335;&#12290;&#35813;&#31639;&#27861;&#29305;&#21035;&#36866;&#29992;&#20110;&#20114;&#36830;&#24230;&#36739;&#39640;&#30340;&#37327;&#23376;&#27604;&#29305;&#12290;&#36890;&#36807;&#21387;&#32553;&#32534;&#30721;&#21644;&#25968;&#23383;-&#27169;&#25311;&#38376;&#25805;&#20316;&#65292;&#36816;&#34892;&#26102;&#38388;&#22797;&#26434;&#24230;&#22312;&#25968;&#25454;&#36755;&#20837;&#37327;&#32534;&#30721;&#30340;&#24773;&#20917;&#19979;&#23545;&#25968;&#32423;&#26356;&#26377;&#20248;&#21183;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22122;&#22768;&#20013;&#23610;&#24230;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational quantum algorithms (VQAs) prevail to solve practical problems such as combinatorial optimization, quantum chemistry simulation, quantum machine learning, and quantum error correction on noisy quantum computers. For variational quantum machine learning, a variational algorithm with model interpretability built into the algorithm is yet to be exploited. In this paper, we construct a quantum regression algorithm and identify the direct relation of variational parameters to learned regression coefficients, while employing a circuit that directly encodes the data in quantum amplitudes reflecting the structure of the classical data table. The algorithm is particularly suitable for well-connected qubits. With compressed encoding and digital-analog gate operation, the run time complexity is logarithmically more advantageous than that for digital 2-local gate native hardware with the number of data entries encoded, a decent improvement in noisy intermediate-scale quantum computers a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Caml&#65292;&#19968;&#31181;&#22312;&#20005;&#26684;&#32422;&#26463;&#30340;&#24212;&#29992;&#20013;&#20351;&#29992;&#20803;&#23398;&#20064;&#30340;AutoML&#26041;&#27861;&#12290;Caml&#33021;&#22815;&#33258;&#21160;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#30340;AutoML&#21442;&#25968;&#65292;&#24182;&#32771;&#34385;&#29992;&#25143;&#23450;&#20041;&#30340;&#32422;&#26463;&#65292;&#29983;&#25104;&#28385;&#36275;&#32422;&#26463;&#19988;&#20855;&#26377;&#39640;&#39044;&#27979;&#24615;&#33021;&#30340;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2306.16913</link><description>&lt;p&gt;
&#20005;&#26684;&#32422;&#26463;&#24212;&#29992;&#20013;&#30340;AutoML
&lt;/p&gt;
&lt;p&gt;
AutoML in Heavily Constrained Applications. (arXiv:2306.16913v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Caml&#65292;&#19968;&#31181;&#22312;&#20005;&#26684;&#32422;&#26463;&#30340;&#24212;&#29992;&#20013;&#20351;&#29992;&#20803;&#23398;&#20064;&#30340;AutoML&#26041;&#27861;&#12290;Caml&#33021;&#22815;&#33258;&#21160;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#30340;AutoML&#21442;&#25968;&#65292;&#24182;&#32771;&#34385;&#29992;&#25143;&#23450;&#20041;&#30340;&#32422;&#26463;&#65292;&#29983;&#25104;&#28385;&#36275;&#32422;&#26463;&#19988;&#20855;&#26377;&#39640;&#39044;&#27979;&#24615;&#33021;&#30340;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20248;&#21270;&#29305;&#23450;&#20219;&#21153;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#65292;&#38656;&#35201;&#23545;&#21508;&#31181;&#36229;&#21442;&#25968;&#36827;&#34892;&#20180;&#32454;&#37197;&#32622;&#65292;&#36890;&#24120;&#30001;AutoML&#31995;&#32479;&#25903;&#25345;&#65292;&#35813;&#31995;&#32479;&#20248;&#21270;&#32473;&#23450;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#36229;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#26681;&#25454;AutoML&#31995;&#32479;&#30340;&#20108;&#38454;&#20803;&#37197;&#32622;&#65292;AutoML&#36807;&#31243;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#26377;&#24456;&#22823;&#24046;&#24322;&#12290;&#30446;&#21069;&#30340;AutoML&#31995;&#32479;&#26080;&#27861;&#33258;&#21160;&#36866;&#24212;&#29305;&#23450;&#29992;&#20363;&#30340;&#37197;&#32622;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20063;&#26080;&#27861;&#32534;&#35793;&#29992;&#25143;&#23450;&#20041;&#30340;&#24212;&#29992;&#32422;&#26463;&#65292;&#20197;&#30830;&#20445;&#27969;&#31243;&#21450;&#20854;&#29983;&#25104;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Caml&#65292;&#23427;&#20351;&#29992;&#20803;&#23398;&#20064;&#33258;&#21160;&#36866;&#24212;&#20854;&#33258;&#36523;&#30340;AutoML&#21442;&#25968;&#65292;&#27604;&#22914;&#25628;&#32034;&#31574;&#30053;&#12289;&#39564;&#35777;&#31574;&#30053;&#21644;&#25628;&#32034;&#31354;&#38388;&#65292;&#20197;&#36866;&#24212;&#29305;&#23450;&#30340;&#20219;&#21153;&#12290;Caml&#30340;&#21160;&#24577;AutoML&#31574;&#30053;&#32771;&#34385;&#29992;&#25143;&#23450;&#20041;&#30340;&#32422;&#26463;&#65292;&#24182;&#33719;&#24471;&#20855;&#26377;&#39640;&#39044;&#27979;&#24615;&#33021;&#30340;&#28385;&#36275;&#32422;&#26463;&#30340;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimizing a machine learning pipeline for a task at hand requires careful configuration of various hyperparameters, typically supported by an AutoML system that optimizes the hyperparameters for the given training dataset. Yet, depending on the AutoML system's own second-order meta-configuration, the performance of the AutoML process can vary significantly. Current AutoML systems cannot automatically adapt their own configuration to a specific use case. Further, they cannot compile user-defined application constraints on the effectiveness and efficiency of the pipeline and its generation. In this paper, we propose Caml, which uses meta-learning to automatically adapt its own AutoML parameters, such as the search strategy, the validation strategy, and the search space, for a task at hand. The dynamic AutoML strategy of Caml takes user-defined constraints into account and obtains constraint-satisfying pipelines with high predictive performance.
&lt;/p&gt;</description></item><item><title>CLUE&#20351;&#29992;&#26465;&#20214;&#21487;&#21464;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#19987;&#23478;&#25968;&#25454;&#30340;&#20869;&#22312;&#22870;&#21169;&#65292;&#28040;&#38500;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#20854;&#23427;&#32321;&#37325;&#30340;&#22806;&#22312;&#22870;&#21169;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2306.13412</link><description>&lt;p&gt;
CLUE: &#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26657;&#20934;&#28508;&#22312;&#23548;&#21521;
&lt;/p&gt;
&lt;p&gt;
CLUE: Calibrated Latent Guidance for Offline Reinforcement Learning. (arXiv:2306.13412v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13412
&lt;/p&gt;
&lt;p&gt;
CLUE&#20351;&#29992;&#26465;&#20214;&#21487;&#21464;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#19987;&#23478;&#25968;&#25454;&#30340;&#20869;&#22312;&#22870;&#21169;&#65292;&#28040;&#38500;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#20854;&#23427;&#32321;&#37325;&#30340;&#22806;&#22312;&#22870;&#21169;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#20174;&#39044;&#20808;&#25910;&#38598;&#21644;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#65292;&#28040;&#38500;&#20102;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#32791;&#26102;&#30340;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#12290;&#20294;&#26159;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20173;&#28982;&#38656;&#35201;&#30830;&#23450;&#21644;&#21046;&#23450;&#27599;&#20010;&#25968;&#25454;&#36716;&#25442;&#30340;&#22806;&#22312;&#22870;&#21169;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#32321;&#37325;&#30340;&#24037;&#20316;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CLUE&#65306;&#36890;&#36807;&#20351;&#29992;&#19968;&#20123;&#19987;&#23478;&#25968;&#25454;&#20026;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20869;&#22312;&#22870;&#21169;&#26469;&#28040;&#38500;&#22806;&#22312;&#22870;&#21169;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26465;&#20214;&#21487;&#21464;&#33258;&#32534;&#30721;&#22120;&#26469;&#23398;&#20064;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#65292;&#20351;&#24471;&#20869;&#22312;&#22870;&#21169;&#21487;&#20197;&#30452;&#25509;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36827;&#34892;&#35780;&#20272;&#12290;CLUE&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#36890;&#36807;&#23558;&#19987;&#23478;&#25968;&#25454;&#30340;&#23884;&#20837;&#24378;&#21046;&#36716;&#25442;&#20026;&#26657;&#20934;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#65292;&#20351;&#20869;&#22312;&#22870;&#21169;&#19982;&#19987;&#23478;&#24847;&#22270;&#20445;&#25345;&#19968;&#33268;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#19987;&#23478;&#39537;&#21160;&#30340;&#20869;&#22312;&#22870;&#21169;&#22312;&#22810;&#20010;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) aims to learn an optimal policy from pre-collected and labeled datasets, which eliminates the time-consuming data collection in online RL. However, offline RL still bears a large burden of specifying/handcrafting extrinsic rewards for each transition in the offline data. As a remedy for the labor-intensive labeling, we propose to endow offline RL tasks with a few expert data and utilize the limited expert data to drive intrinsic rewards, thus eliminating the need for extrinsic rewards. To achieve that, we introduce \textbf{C}alibrated \textbf{L}atent g\textbf{U}idanc\textbf{E} (CLUE), which utilizes a conditional variational auto-encoder to learn a latent space such that intrinsic rewards can be directly qualified over the latent space. CLUE's key idea is to align the intrinsic rewards consistent with the expert intention via enforcing the embeddings of expert data to a calibrated contextual representation. We instantiate the expert-driven intrinsic 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#31995;&#32479;&#22238;&#39038;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#22312;&#32467;&#26500;&#33647;&#29289;&#35774;&#35745;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20998;&#21035;&#35752;&#35770;&#20102;&#19981;&#21516;&#20219;&#21153;&#24182;&#25353;&#19981;&#21516;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#32452;&#32455;&#12290;&#35813;&#39046;&#22495;&#30340;&#21069;&#26223;&#30475;&#22909;&#65292;&#20294;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.11768</link><description>&lt;p&gt;
&#22522;&#20110;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#32467;&#26500;&#33647;&#29289;&#35774;&#35745;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Systematic Survey in Geometric Deep Learning for Structure-based Drug Design. (arXiv:2306.11768v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#31995;&#32479;&#22238;&#39038;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#22312;&#32467;&#26500;&#33647;&#29289;&#35774;&#35745;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20998;&#21035;&#35752;&#35770;&#20102;&#19981;&#21516;&#20219;&#21153;&#24182;&#25353;&#19981;&#21516;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#32452;&#32455;&#12290;&#35813;&#39046;&#22495;&#30340;&#21069;&#26223;&#30475;&#22909;&#65292;&#20294;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#33647;&#29289;&#35774;&#35745;&#21033;&#29992;&#34507;&#30333;&#36136;&#30340;&#19977;&#32500;&#20960;&#20309;&#32467;&#26500;&#26469;&#35782;&#21035;&#28508;&#22312;&#30340;&#33647;&#29289;&#20505;&#36873;&#29289;&#65292;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#29289;&#29702;&#21270;&#23398;&#24314;&#27169;&#21644;&#19987;&#23478;&#39046;&#22495;&#30693;&#35782;&#30340;&#20256;&#32479;&#26041;&#27861;&#36153;&#26102;&#36153;&#21147;&#12290;&#36817;&#24180;&#26469;&#65292;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#21487;&#20197;&#22788;&#29702;&#21644;&#25972;&#21512;&#19977;&#32500;&#20960;&#20309;&#25968;&#25454;&#65292;&#21152;&#19978;&#31867;&#20284;AlphaFold&#30340;&#24037;&#20855;&#25552;&#20379;&#20934;&#30830;&#30340;&#34507;&#30333;&#36136;&#19977;&#32500;&#32467;&#26500;&#39044;&#27979;&#65292;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#32467;&#26500;&#33647;&#29289;&#35774;&#35745;&#30340;&#36827;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#22312;&#32467;&#26500;&#33647;&#29289;&#35774;&#35745;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#25105;&#20204;&#20174;&#32467;&#26500;&#33647;&#29289;&#35774;&#35745;&#20013;&#30340;&#20027;&#27969;&#20219;&#21153;&#12289;&#24120;&#29992;&#30340;3D&#34507;&#30333;&#36136;&#34920;&#31034;&#21644;&#39044;&#27979;/&#29983;&#25104;&#27169;&#22411;&#20837;&#25163;&#65292;&#28982;&#21518;&#35814;&#32454;&#20171;&#32461;&#27599;&#20010;&#20219;&#21153;&#30340;&#22238;&#39038;&#65288;&#20363;&#22914;&#32467;&#21512;&#20301;&#28857;&#39044;&#27979;&#12289;&#32467;&#21512;&#26500;&#35937;&#29983;&#25104;&#12289;\emph{de novo} &#20998;&#23376;&#35774;&#35745;&#31561;&#65289;&#65292;&#24182;&#25353;&#19981;&#21516;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#32452;&#32455;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#35813;&#39046;&#22495;&#26410;&#26469;&#30740;&#31350;&#30340;&#25361;&#25112;&#21644;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structure-based drug design (SBDD), which utilizes the three-dimensional geometry of proteins to identify potential drug candidates, is becoming increasingly vital in drug discovery. However, traditional methods based on physiochemical modeling and experts' domain knowledge are time-consuming and laborious. The recent advancements in geometric deep learning, which integrates and processes 3D geometric data, coupled with the availability of accurate protein 3D structure predictions from tools like AlphaFold, have significantly propelled progress in structure-based drug design. In this paper, we systematically review the recent progress of geometric deep learning for structure-based drug design. We start with a brief discussion of the mainstream tasks in structure-based drug design, commonly used 3D protein representations and representative predictive/generative models. Then we delve into detailed reviews for each task (binding site prediction, binding pose generation, \emph{de novo} mo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#40065;&#26834;MDP&#38382;&#39064;&#21644;&#39118;&#38505;&#25935;&#24863;MDP&#38382;&#39064;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#23398;&#20064;&#31639;&#27861;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2306.11626</link><description>&lt;p&gt;
&#27491;&#21017;&#21270;&#40065;&#26834;&#30340;MDPs&#21644;&#39118;&#38505;&#25935;&#24863;&#30340;MDPs&#65306;&#31561;&#20215;&#24615;&#12289;&#31574;&#30053;&#26799;&#24230;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Regularized Robust MDPs and Risk-Sensitive MDPs: Equivalence, Policy Gradient, and Sample Complexity. (arXiv:2306.11626v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#40065;&#26834;MDP&#38382;&#39064;&#21644;&#39118;&#38505;&#25935;&#24863;MDP&#38382;&#39064;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#23398;&#20064;&#31639;&#27861;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20851;&#27880;&#20110;&#27491;&#21017;&#21270;&#40065;&#26834;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#38382;&#39064;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#23427;&#26159;&#40065;&#26834;MDP&#26694;&#26550;&#30340;&#19968;&#20010;&#25193;&#23637;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#39118;&#38505;&#25935;&#24863;MDP&#65292;&#24182;&#24314;&#31435;&#20102;&#39118;&#38505;&#25935;&#24863;MDP&#21644;&#27491;&#21017;&#21270;&#40065;&#26834;MDP&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#12290;&#36825;&#31181;&#31561;&#20215;&#24615;&#20026;&#35299;&#20915;&#27491;&#21017;&#21270;RMDP&#25552;&#20379;&#20102;&#21478;&#19968;&#31181;&#35270;&#35282;&#65292;&#24182;&#19988;&#20351;&#24471;&#35774;&#35745;&#39640;&#25928;&#30340;&#23398;&#20064;&#31639;&#27861;&#25104;&#20026;&#21487;&#33021;&#12290;&#22312;&#36825;&#31181;&#31561;&#20215;&#24615;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25512;&#23548;&#20102;&#27491;&#21017;&#21270;&#40065;&#26834;MDP&#38382;&#39064;&#30340;&#31574;&#30053;&#26799;&#24230;&#23450;&#29702;&#65292;&#24182;&#22312;&#20855;&#26377;&#30452;&#25509;&#21442;&#25968;&#21270;&#30340;&#34920;&#26684;&#35774;&#32622;&#19979;&#35777;&#26126;&#20102;&#31934;&#30830;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#26412;&#30340;&#31163;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#21363;&#40065;&#26834;&#30340;FZI&#36845;&#20195;&#65292;&#29992;&#20110;&#20855;&#26377;KL&#25955;&#24230;&#27491;&#21017;&#21270;&#39033;&#30340;&#29305;&#23450;&#27491;&#21017;&#21270;&#40065;&#26834;MDP&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#31639;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20063;&#24471;&#21040;&#20102;&#25968;&#20540;&#27169;&#25311;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on reinforcement learning for the regularized robust Markov decision process (MDP) problem, an extension of the robust MDP framework. We first introduce the risk-sensitive MDP and establish the equivalence between risk-sensitive MDP and regularized robust MDP. This equivalence offers an alternative perspective for addressing the regularized RMDP and enables the design of efficient learning algorithms. Given this equivalence, we further derive the policy gradient theorem for the regularized robust MDP problem and prove the global convergence of the exact policy gradient method under the tabular setting with direct parameterization. We also propose a sample-based offline learning algorithm, namely the robust fitted-Z iteration (RFZI), for a specific regularized robust MDP problem with a KL-divergence regularization term and analyze the sample complexity of the algorithm. Our results are also supported by numerical simulations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#26080;&#20851;&#30340;&#20114;&#20449;&#24687;&#20272;&#35745;&#22522;&#20934;&#24179;&#21488;&#65292;&#24182;&#35752;&#35770;&#20102;&#32463;&#20856;&#21644;&#31070;&#32463;&#20272;&#35745;&#22120;&#22312;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#12289;&#38271;&#23614;&#20998;&#24067;&#21644;&#39640;&#20114;&#20449;&#24687;&#26102;&#30340;&#26222;&#36866;&#24615;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.11078</link><description>&lt;p&gt;
&#36229;&#36234;&#27491;&#24120;&#65306;&#20851;&#20110;&#20114;&#20449;&#24687;&#20272;&#35745;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Beyond Normal: On the Evaluation of Mutual Information Estimators. (arXiv:2306.11078v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#26080;&#20851;&#30340;&#20114;&#20449;&#24687;&#20272;&#35745;&#22522;&#20934;&#24179;&#21488;&#65292;&#24182;&#35752;&#35770;&#20102;&#32463;&#20856;&#21644;&#31070;&#32463;&#20272;&#35745;&#22120;&#22312;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#12289;&#38271;&#23614;&#20998;&#24067;&#21644;&#39640;&#20114;&#20449;&#24687;&#26102;&#30340;&#26222;&#36866;&#24615;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#20449;&#24687;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#32479;&#35745;&#30456;&#20851;&#24230;&#37327;&#65292;&#24050;&#22312;&#34920;&#31034;&#23398;&#20064;&#12289;&#22240;&#26524;&#24615;&#12289;&#22495;&#27867;&#21270;&#21644;&#35745;&#31639;&#29983;&#29289;&#23398;&#31561;&#39046;&#22495;&#24471;&#21040;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#20114;&#20449;&#24687;&#20272;&#35745;&#36890;&#24120;&#21482;&#22312;&#31616;&#21333;&#30340;&#27010;&#29575;&#20998;&#24067;&#26063;&#31867;&#65288;&#21363;&#22810;&#20803;&#27491;&#24577;&#20998;&#24067;&#21644;&#20855;&#26377;&#19968;&#32500;&#38543;&#26426;&#21464;&#37327;&#30340;&#36873;&#25321;&#20998;&#24067;&#65289;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#26500;&#24314;&#20855;&#26377;&#24050;&#30693;&#22522;&#20934;&#20114;&#20449;&#24687;&#30340;&#21508;&#31181;&#20998;&#24067;&#26063;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#26080;&#20851;&#30340;&#20114;&#20449;&#24687;&#20272;&#35745;&#22522;&#20934;&#24179;&#21488;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#32463;&#20856;&#21644;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#22120;&#22312;&#28041;&#21450;&#39640;&#32500;&#24230;&#12289;&#31232;&#30095;&#30456;&#20114;&#20316;&#29992;&#12289;&#38271;&#23614;&#20998;&#24067;&#21644;&#39640;&#20114;&#20449;&#24687;&#30340;&#24773;&#22659;&#20013;&#30340;&#26222;&#36866;&#24615;&#21644;&#23616;&#38480;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20026;&#20174;&#19994;&#20154;&#21592;&#25552;&#20379;&#20102;&#36873;&#25321;&#36866;&#24403;&#30340;&#20272;&#35745;&#22120;&#20197;&#36866;&#24212;&#25152;&#32771;&#34385;&#38382;&#39064;&#38590;&#24230;&#21644;&#24212;&#29992;&#20272;&#35745;&#20114;&#20449;&#24687;&#26102;&#38656;&#35201;&#32771;&#34385;&#30340;&#38382;&#39064;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mutual information is a general statistical dependency measure which has found applications in representation learning, causality, domain generalization and computational biology. However, mutual information estimators are typically evaluated on simple families of probability distributions, namely multivariate normal distribution and selected distributions with one-dimensional random variables. In this paper, we show how to construct a diverse family of distributions with known ground-truth mutual information and propose a language-independent benchmarking platform for mutual information estimators. We discuss the general applicability and limitations of classical and neural estimators in settings involving high dimensions, sparse interactions, long-tailed distributions, and high mutual information. Finally, we provide guidelines for practitioners on how to select appropriate estimator adapted to the difficulty of problem considered and issues one needs to consider when applying an est
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;Transformer&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#19981;&#21516;&#29305;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20462;&#25913;&#21518;&#30340;&#22522;&#20110;Transformer&#30340;&#36890;&#29992;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;NAR-Former V2&#12290;</title><link>http://arxiv.org/abs/2306.10792</link><description>&lt;p&gt;
NAR-Former V2&#65306;&#37325;&#26032;&#24605;&#32771;Transformer&#29992;&#20110;&#36890;&#29992;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
NAR-Former V2: Rethinking Transformer for Universal Neural Network Representation Learning. (arXiv:2306.10792v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;Transformer&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#19981;&#21516;&#29305;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20462;&#25913;&#21518;&#30340;&#22522;&#20110;Transformer&#30340;&#36890;&#29992;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;NAR-Former V2&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34987;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#65292;&#23545;&#20110;&#24314;&#27169;&#21644;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#26412;&#36523;&#30340;&#34920;&#31034;&#30340;&#38656;&#27714;&#20063;&#36234;&#26469;&#36234;&#22823;&#12290;&#39640;&#25928;&#30340;&#34920;&#31034;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#32593;&#32476;&#30340;&#30446;&#26631;&#23646;&#24615;&#65292;&#32780;&#26080;&#38656;&#23454;&#38469;&#35757;&#32451;&#21644;&#37096;&#32626;&#36807;&#31243;&#65292;&#20174;&#32780;&#20419;&#36827;&#39640;&#25928;&#30340;&#32593;&#32476;&#37096;&#32626;&#21644;&#35774;&#35745;&#12290;&#26368;&#36817;&#65292;&#21463;&#21040;Transformer&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#19968;&#20123;&#22522;&#20110;Transformer&#30340;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#24050;&#34987;&#25552;&#20986;&#65292;&#24182;&#22312;&#22788;&#29702;&#22522;&#20110;&#32454;&#32990;&#32467;&#26500;&#30340;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26041;&#27861;&#20173;&#28982;&#20027;&#23548;&#20102;&#25972;&#20010;&#32593;&#32476;&#30340;&#23398;&#20064;&#34920;&#31034;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;Transformer&#65292;&#24182;&#19982;GNN&#36827;&#34892;&#27604;&#36739;&#65292;&#20998;&#26512;&#23427;&#20204;&#19981;&#21516;&#30340;&#26550;&#26500;&#29305;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20462;&#25913;&#21518;&#30340;&#22522;&#20110;Transformer&#30340;&#36890;&#29992;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;NAR-Former V2&#12290;
&lt;/p&gt;
&lt;p&gt;
As more deep learning models are being applied in real-world applications, there is a growing need for modeling and learning the representations of neural networks themselves. An efficient representation can be used to predict target attributes of networks without the need for actual training and deployment procedures, facilitating efficient network deployment and design. Recently, inspired by the success of Transformer, some Transformer-based representation learning frameworks have been proposed and achieved promising performance in handling cell-structured models. However, graph neural network (GNN) based approaches still dominate the field of learning representation for the entire network. In this paper, we revisit Transformer and compare it with GNN to analyse their different architecture characteristics. We then propose a modified Transformer-based universal neural network representation learning model NAR-Former V2. It can learn efficient representations from both cell-structured
&lt;/p&gt;</description></item><item><title>DIFFender&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20301;&#21644;&#24674;&#22797;&#20004;&#20010;&#38454;&#27573;&#30340;&#25805;&#20316;&#65292;&#21033;&#29992;&#25991;&#26412;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#38450;&#24481;&#23545;&#25239;&#24615;Patch&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#25972;&#20307;&#38450;&#24481;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.09124</link><description>&lt;p&gt;
DIFFender&#65306;&#22522;&#20110;&#25193;&#25955;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#29992;&#20110;&#25269;&#24481;Patch&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
DIFFender: Diffusion-Based Adversarial Defense against Patch Attacks. (arXiv:2306.09124v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09124
&lt;/p&gt;
&lt;p&gt;
DIFFender&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20301;&#21644;&#24674;&#22797;&#20004;&#20010;&#38454;&#27573;&#30340;&#25805;&#20316;&#65292;&#21033;&#29992;&#25991;&#26412;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#38450;&#24481;&#23545;&#25239;&#24615;Patch&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#25972;&#20307;&#38450;&#24481;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#23588;&#20854;&#26159;Patch&#25915;&#20987;&#65292;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#38752;&#24615;&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#12290;&#24320;&#21457;&#21487;&#38752;&#30340;&#38450;&#24481;&#26041;&#27861;&#20197;&#25269;&#24481;Patch&#25915;&#20987;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#24403;&#21069;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#36824;&#19981;&#20196;&#20154;&#28385;&#24847;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DIFFender&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#25991;&#26412;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#38450;&#24481;&#23545;&#25239;&#24615;Patch&#12290;DIFFender&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#38454;&#27573;&#65306;Patch&#23450;&#20301;&#21644;Patch&#24674;&#22797;&#12290;&#22312;&#23450;&#20301;&#38454;&#27573;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#21033;&#29992;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#19968;&#20010;&#26377;&#36259;&#29305;&#24615;&#65292;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#23545;&#25239;&#24615;Patch&#30340;&#20301;&#32622;&#12290;&#22312;&#24674;&#22797;&#38454;&#27573;&#65292;&#25105;&#20204;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#37325;&#24314;&#22270;&#20687;&#20013;&#30340;&#23545;&#25239;&#24615;&#21306;&#22495;&#21516;&#26102;&#20445;&#25345;&#35270;&#35273;&#20869;&#23481;&#30340;&#23436;&#25972;&#24615;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#20004;&#20010;&#38454;&#27573;&#37117;&#21463;&#21040;&#32479;&#19968;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#31934;&#24515;&#24341;&#23548;&#65292;&#22240;&#27492;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#23427;&#20204;&#20043;&#38388;&#30340;&#32039;&#23494;&#30456;&#20114;&#20316;&#29992;&#26469;&#25552;&#39640;&#25972;&#20010;&#38450;&#24481;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks, particularly patch attacks, pose significant threats to the robustness and reliability of deep learning models. Developing reliable defenses against patch attacks is crucial for real-world applications, yet current research in this area is not satisfactory. In this paper, we propose DIFFender, a novel defense method that leverages a text-guided diffusion model to defend against adversarial patches. DIFFender includes two main stages: patch localization and patch restoration. In the localization stage, we find and exploit an intriguing property of the diffusion model to effectively identify the locations of adversarial patches. In the restoration stage, we employ the diffusion model to reconstruct the adversarial regions in the images while preserving the integrity of the visual content. Importantly, these two stages are carefully guided by a unified diffusion model, thus we can utilize the close interaction between them to improve the whole defense performance. Mor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32467;&#26500;&#21270;&#39044;&#27979;PAC-Bayesian&#39118;&#38505;&#30028;&#38480;&#65292;&#23427;&#21487;&#20197;&#38543;&#30528;&#32467;&#26500;&#21270;&#31034;&#20363;&#30340;&#25968;&#37327;&#21644;&#22823;&#23567;&#30340;&#21464;&#21270;&#32780;&#36827;&#34892;&#27867;&#21270;&#65292;&#20026;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#24314;&#31435;&#32467;&#26500;&#21270;&#39044;&#27979;&#30340;&#27867;&#21270;&#30028;&#38480;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#12290;</title><link>http://arxiv.org/abs/2306.09112</link><description>&lt;p&gt;
&#20851;&#20110;&#32467;&#26500;&#39044;&#27979;&#20013;&#30340;&#35748;&#35777;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
On Certified Generalization in Structured Prediction. (arXiv:2306.09112v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09112
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32467;&#26500;&#21270;&#39044;&#27979;PAC-Bayesian&#39118;&#38505;&#30028;&#38480;&#65292;&#23427;&#21487;&#20197;&#38543;&#30528;&#32467;&#26500;&#21270;&#31034;&#20363;&#30340;&#25968;&#37327;&#21644;&#22823;&#23567;&#30340;&#21464;&#21270;&#32780;&#36827;&#34892;&#27867;&#21270;&#65292;&#20026;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#24314;&#31435;&#32467;&#26500;&#21270;&#39044;&#27979;&#30340;&#27867;&#21270;&#30028;&#38480;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32467;&#26500;&#39044;&#27979;&#20013;&#65292;&#30446;&#26631;&#23545;&#35937;&#20855;&#26377;&#20016;&#23500;&#30340;&#20869;&#37096;&#32467;&#26500;&#65292;&#36825;&#31181;&#32467;&#26500;&#26080;&#27861;&#20998;&#35299;&#20026;&#29420;&#31435;&#30340;&#32452;&#20214;&#65292;&#24182;&#36829;&#21453;&#20102;&#24120;&#35265;&#30340;&#29420;&#31435;&#21516;&#20998;&#24067;&#20551;&#35774;&#12290;&#36825;&#19968;&#25361;&#25112;&#22312;&#24212;&#29992;&#31243;&#24207;&#20013;&#34920;&#29616;&#20026;&#25351;&#25968;&#32423;&#30340;&#36755;&#20986;&#31354;&#38388;&#65292;&#22914;&#22270;&#20687;&#20998;&#21106;&#25110;&#22330;&#26223;&#22270;&#29983;&#25104;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32467;&#26500;&#21270;&#39044;&#27979;PAC-Bayesian&#39118;&#38505;&#30028;&#38480;&#65292;&#20854;&#20013;&#27867;&#21270;&#36895;&#29575;&#19981;&#20165;&#38543;&#30528;&#32467;&#26500;&#21270;&#31034;&#20363;&#30340;&#25968;&#37327;&#32780;&#19988;&#36824;&#38543;&#30528;&#23427;&#20204;&#30340;&#22823;&#23567;&#32780;&#21464;&#21270;&#12290;&#22522;&#26412;&#20551;&#35774;&#31526;&#21512;&#29983;&#25104;&#27169;&#22411;&#19978;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#21363;&#25968;&#25454;&#30001;&#20998;&#35299;&#21442;&#32771;&#24230;&#37327;&#30340;Knothe-Rosenblatt&#37325;&#26032;&#25490;&#21015;&#29983;&#25104;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#23558;&#38543;&#26426;&#36755;&#20986;&#21464;&#37327;&#20043;&#38388;&#30340;&#32467;&#26500;&#26174;&#24335;&#22320;&#25552;&#21462;&#21040;Wasserstein&#20381;&#36182;&#30697;&#38453;&#20013;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#21033;&#29992;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#32467;&#26500;&#39044;&#27979;&#36825;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#19979;&#24314;&#31435;&#21028;&#21035;&#24335;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#30028;&#38480;&#36808;&#20986;&#20102;&#21021;&#27493;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
In structured prediction, target objects have rich internal structure which does not factorize into independent components and violates common i.i.d. assumptions. This challenge becomes apparent through the exponentially large output space in applications such as image segmentation or scene graph generation. We present a novel PAC-Bayesian risk bound for structured prediction wherein the rate of generalization scales not only with the number of structured examples but also with their size. The underlying assumption, conforming to ongoing research on generative models, is that data are generated by the Knothe-Rosenblatt rearrangement of a factorizing reference measure. This allows to explicitly distill the structure between random output variables into a Wasserstein dependency matrix. Our work makes a preliminary step towards leveraging powerful generative models to establish generalization bounds for discriminative downstream tasks in the challenging setting of structured prediction.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#33258;&#30001;&#33021;&#21407;&#29702;&#21644;&#20027;&#21160;&#25512;&#29702;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#25512;&#23548;&#20102;&#36866;&#29992;&#20110;&#20219;&#24847;&#22270;&#24418;&#27169;&#22411;&#30340;&#20027;&#21160;&#25512;&#29702;&#29256;&#26412;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#24418;&#35828;&#26126;&#35821;&#35328;&#65288;GSL&#65289;&#26469;&#26126;&#30830;&#35268;&#23450;&#31995;&#32479;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2306.08014</link><description>&lt;p&gt;
&#23454;&#29616;&#21512;&#25104;&#20027;&#21160;&#25512;&#29702;&#20195;&#29702;&#65292;&#31532;&#19968;&#37096;&#20998;&#65306;&#35748;&#35782;&#30446;&#26631;&#21644;&#22270;&#24418;&#35828;&#26126;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Realising Synthetic Active Inference Agents, Part I: Epistemic Objectives and Graphical Specification Language. (arXiv:2306.08014v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#33258;&#30001;&#33021;&#21407;&#29702;&#21644;&#20027;&#21160;&#25512;&#29702;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#25512;&#23548;&#20102;&#36866;&#29992;&#20110;&#20219;&#24847;&#22270;&#24418;&#27169;&#22411;&#30340;&#20027;&#21160;&#25512;&#29702;&#29256;&#26412;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#24418;&#35828;&#26126;&#35821;&#35328;&#65288;GSL&#65289;&#26469;&#26126;&#30830;&#35268;&#23450;&#31995;&#32479;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30001;&#33021;&#21407;&#29702;&#65288;FEP&#65289;&#26159;&#19968;&#31181;&#25551;&#36848;&#31995;&#32479;&#22914;&#20309;&#36890;&#36807;&#26368;&#23567;&#21270;&#33258;&#30001;&#33021;&#27867;&#20989;&#32780;&#33258;&#32452;&#32455;&#25104;&#20855;&#26377;&#36830;&#36143;&#24615;&#12289;&#31283;&#23450;&#32467;&#26500;&#65288;&#26234;&#33021;&#65289;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#20027;&#21160;&#25512;&#29702;&#65288;AIF&#65289;&#26159;FEP&#30340;&#19968;&#20010;&#25512;&#35770;&#65292;&#23427;&#26126;&#30830;&#20102;&#33021;&#22815;&#20026;&#26410;&#26469;&#36827;&#34892;&#35268;&#21010;&#65288;&#20195;&#29702;&#65289;&#30340;&#31995;&#32479;&#26159;&#22914;&#20309;&#36890;&#36807;&#26368;&#23567;&#21270;&#21253;&#21547;&#20449;&#24687;&#23547;&#27714;&#32452;&#20214;&#30340;&#29305;&#23450;&#33258;&#30001;&#33021;&#27867;&#20989;&#26469;&#36816;&#20316;&#30340;&#12290;&#26412;&#25991;&#26159;&#19968;&#20010;&#31995;&#21015;&#20013;&#30340;&#31532;&#19968;&#31687;&#65292;&#25105;&#20204;&#22312;&#33258;&#30001;&#24418;&#24335;&#22240;&#23376;&#22270;&#19978;&#25512;&#23548;&#20102;AIF&#30340;&#21512;&#25104;&#29256;&#26412;&#12290;&#26412;&#25991;&#37325;&#28857;&#25512;&#23548;&#20102;AIF&#25152;&#20351;&#29992;&#30340;&#33258;&#30001;&#33021;&#27867;&#20989;&#30340;&#23616;&#37096;&#29256;&#26412;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#26500;&#36896;&#19968;&#20010;&#36866;&#29992;&#20110;&#20219;&#24847;&#22270;&#24418;&#27169;&#22411;&#24182;&#19982;&#26377;&#20851;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#30340;&#20808;&#21069;&#24037;&#20316;&#25509;&#21475;&#30340;AIF&#29256;&#26412;&#12290;&#32467;&#26524;&#28040;&#24687;&#26159;&#22312;&#25105;&#20204;&#30340;&#20276;&#20387;&#35770;&#25991;&#20013;&#24471;&#20986;&#30340;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#22240;&#23376;&#22270;&#24418;&#24335;&#20013;&#23384;&#22312;&#19968;&#20010;&#32570;&#21475;&#12290;&#34429;&#28982;&#22240;&#23376;&#22270;&#34920;&#36798;&#20102;&#29983;&#25104;&#27169;&#22411;&#65292;&#20294;&#22312;&#25351;&#23450;&#31995;&#32479;&#30446;&#26631;&#26041;&#38754;&#32570;&#20047;&#19968;&#20010;&#22270;&#24418;&#21270;&#35821;&#35328;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22240;&#23376;&#22270;&#25551;&#36848;&#27861;&#30340;&#26032;&#25193;&#23637;&#65292;&#31216;&#20026;&#22270;&#24418;&#35828;&#26126;&#35821;&#35328;&#65288;GSL&#65289;&#65292;&#23427;&#20351;&#31995;&#32479;&#30446;&#26631;&#24471;&#21040;&#26126;&#30830;&#35268;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Free Energy Principle (FEP) is a theoretical framework for describing how (intelligent) systems self-organise into coherent, stable structures by minimising a free energy functional. Active Inference (AIF) is a corollary of the FEP that specifically details how systems that are able to plan for the future (agents) function by minimising particular free energy functionals that incorporate information seeking components. This paper is the first in a series of two where we derive a synthetic version of AIF on free form factor graphs. The present paper focuses on deriving a local version of the free energy functionals used for AIF. This enables us to construct a version of AIF which applies to arbitrary graphical models and interfaces with prior work on message passing algorithms. The resulting messages are derived in our companion paper. We also identify a gap in the graphical notation used for factor graphs. While factor graphs are great at expressing a generative model, they have so
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#31639;&#27861;&#8212;&#8212;GLoRA&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#24191;&#20041;&#25552;&#31034;&#27169;&#22359;&#12289;&#27169;&#22359;&#21270;&#30340;&#36866;&#37197;&#22120;&#23618;&#21644;&#21487;&#25193;&#23637;&#30340;&#32467;&#26500;&#25628;&#32034;&#20855;&#26377;&#20102;&#23545;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#26356;&#39640;&#28789;&#27963;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#24182;&#22312;&#21508;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.07967</link><description>&lt;p&gt;
&#19968;&#36890;&#36866;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#36890;&#29992;LoRA&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning. (arXiv:2306.07967v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#31639;&#27861;&#8212;&#8212;GLoRA&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#24191;&#20041;&#25552;&#31034;&#27169;&#22359;&#12289;&#27169;&#22359;&#21270;&#30340;&#36866;&#37197;&#22120;&#23618;&#21644;&#21487;&#25193;&#23637;&#30340;&#32467;&#26500;&#25628;&#32034;&#20855;&#26377;&#20102;&#23545;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#26356;&#39640;&#28789;&#27963;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#24182;&#22312;&#21508;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#36890;&#29992;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#20219;&#21153;&#31639;&#27861;&#8212;&#8212;&#24191;&#20041;LoRA&#65288;GLoRA&#65289;&#12290;GLoRA &#20351;&#29992;&#24191;&#20041;&#25552;&#31034;&#27169;&#22359;&#26469;&#20248;&#21270;&#39044;&#35757;&#32451;&#27169;&#22411;&#26435;&#37325;&#21644;&#35843;&#25972;&#20013;&#38388;&#28608;&#27963;&#29366;&#24577;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#26356;&#22810;&#30340;&#28789;&#27963;&#24615;&#21644;&#36328;&#24322;&#26500;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;GLoRA &#36890;&#36807;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#12289;&#27169;&#22359;&#21270;&#30340;&#12289;&#23618;&#27425;&#30340;&#32467;&#26500;&#25628;&#32034;&#26469;&#24110;&#21161;&#26377;&#25928;&#30340;&#21442;&#25968;&#35843;&#25972;&#65292;&#23398;&#20064;&#27599;&#20010;&#23618;&#30340;&#36866;&#37197;&#22120;&#65292;&#20174;&#19968;&#20010;&#32479;&#19968;&#30340;&#25968;&#23398;&#20844;&#24335;&#36215;&#28304;&#65292;GLoRA &#20855;&#26377;&#24378;&#22823;&#30340;&#36801;&#31227;&#23398;&#20064;&#12289;&#23569;&#26679;&#26412;&#23398;&#20064;&#21644;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#65292;&#36890;&#36807;&#26435;&#37325;&#21644;&#28608;&#27963;&#29366;&#24577;&#19978;&#30340;&#38468;&#21152;&#32500;&#24230;&#26469;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#32508;&#21512;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#33258;&#28982;&#12289;&#19987;&#19994;&#21644;&#32467;&#26500;&#21270;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;GLoRA &#30340;&#31934;&#24230;&#20248;&#20110;&#25152;&#26377;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#19988;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#35745;&#31639;&#36798;&#21040;&#20102;&#20248;&#36234;&#30340;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26500;&#37325;&#26032;&#35774;&#35745;&#21487;&#20197;&#22823;&#24133;&#20943;&#23569;&#36816;&#31639;&#26102;&#38388;&#21644;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Generalized LoRA (GLoRA), an advanced approach for universal parameter-efficient fine-tuning tasks. Enhancing Low-Rank Adaptation (LoRA), GLoRA employs a generalized prompt module to optimize pre-trained model weights and adjust intermediate activations, providing more flexibility and capability across diverse tasks and datasets. Moreover, GLoRA facilitates efficient parameter adaptation by employing a scalable, modular, layer-wise structure search that learns individual adapter of each layer. Originating from a unified mathematical formulation, GLoRA exhibits strong transfer learning, few-shot learning and domain generalization abilities, as it adjusts to new tasks through additional dimensions on weights and activations. Comprehensive experiments demonstrate that GLoRA outperforms all previous methods in natural, specialized, and structured benchmarks, achieving superior accuracy with fewer parameters and computations on various datasets. Furthermore, our structural re-par
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#23398;&#20064;&#25511;&#21046;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#21033;&#29992;&#20102;&#29289;&#29702;&#30693;&#35782;&#20316;&#20026;&#24402;&#32435;&#20559;&#32622;&#65292;&#24182;&#22312;&#35774;&#35745;&#20013;&#21033;&#29992;&#36317;&#31163;&#24863;&#30693;&#30340;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#65292;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#21516;&#26102;&#21487;&#20197;&#22312;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#65292;&#21487;&#29992;&#20110;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#21644;&#27169;&#22411;&#22522;&#30784;&#22686;&#24378;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2306.06335</link><description>&lt;p&gt;
&#22914;&#20309;&#20174;&#30701;&#30701;&#19977;&#20998;&#38047;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#21644;&#25512;&#24191;&#65306;&#21463;&#38480;&#20110;&#29289;&#29702;&#23398;&#21644;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
How to Learn and Generalize From Three Minutes of Data: Physics-Constrained and Uncertainty-Aware Neural Stochastic Differential Equations. (arXiv:2306.06335v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#23398;&#20064;&#25511;&#21046;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#21033;&#29992;&#20102;&#29289;&#29702;&#30693;&#35782;&#20316;&#20026;&#24402;&#32435;&#20559;&#32622;&#65292;&#24182;&#22312;&#35774;&#35745;&#20013;&#21033;&#29992;&#36317;&#31163;&#24863;&#30693;&#30340;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#65292;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#21516;&#26102;&#21487;&#20197;&#22312;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#65292;&#21487;&#29992;&#20110;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#21644;&#27169;&#22411;&#22522;&#30784;&#22686;&#24378;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#21644;&#31639;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDEs&#65289;&#26469;&#23398;&#20064;&#21463;&#25511;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;&#25105;&#20204;&#26500;&#24314;&#28418;&#31227;&#39033;&#65292;&#21033;&#29992;&#20808;&#39564;&#30340;&#29289;&#29702;&#30693;&#35782;&#20316;&#20026;&#24402;&#32435;&#20559;&#32622;&#65292;&#24182;&#35774;&#35745;&#25193;&#25955;&#39033;&#26469;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#36317;&#31163;&#24863;&#30693;&#20272;&#35745;&#8212;&#8212;&#24403;&#22312;&#25509;&#36817;&#35757;&#32451;&#25968;&#25454;&#38598;&#29366;&#24577;&#30340;&#29366;&#24577;&#19979;&#35780;&#20272;&#26102;&#65292;&#23427;&#21305;&#37197;&#31995;&#32479;&#30340;&#22522;&#26412;&#38543;&#26426;&#24615;&#65292;&#24182;&#19988;&#24403;&#22312;&#35757;&#32451;&#33539;&#22260;&#20043;&#22806;&#30340;&#29366;&#24577;&#19979;&#35780;&#20272;&#26102;&#65292;&#23427;&#20250;&#39044;&#27979;&#39640;&#24230;&#38543;&#26426;&#30340;&#21160;&#24577;&#12290;&#25552;&#20986;&#30340;&#31070;&#32463;SDEs&#21487;&#20197;&#24555;&#36895;&#35780;&#20272;&#65292;&#20197;&#29992;&#20110;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#31639;&#27861;&#65292;&#20063;&#21487;&#20197;&#29992;&#20316;&#27169;&#22411;&#22522;&#30784;&#22686;&#24378;&#23398;&#20064;&#30340;&#27169;&#25311;&#22120;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#22312;&#35206;&#30422;&#29366;&#24577;&#31354;&#38388;&#26377;&#38480;&#21306;&#22495;&#30340;&#23567;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#23427;&#20204;&#20063;&#21487;&#20197;&#22312;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#12290;&#25105;&#20204;&#36890;&#36807;&#25511;&#21046;&#23567;&#22411;&#26080;&#20154;&#26426;&#21644;&#27169;&#25311;&#26426;&#22120;&#33151;&#30340;&#36816;&#21160;&#65288;&#32771;&#34385;&#21040;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#21644;&#20223;&#30495;&#21040;&#29616;&#23454;&#30340;&#24046;&#24322;&#65289;&#20197;&#21450;&#27169;&#25311;&#24314;&#31569;&#30340;&#28909;&#21147;&#23398;&#21160;&#24577;&#65288;&#32771;&#34385;&#21040;&#32570;&#22833;&#25968;&#25454;&#24182;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65289;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a framework and algorithms to learn controlled dynamics models using neural stochastic differential equations (SDEs) -- SDEs whose drift and diffusion terms are both parametrized by neural networks. We construct the drift term to leverage a priori physics knowledge as inductive bias, and we design the diffusion term to represent a distance-aware estimate of the uncertainty in the learned model's predictions -- it matches the system's underlying stochasticity when evaluated on states near those from the training dataset, and it predicts highly stochastic dynamics when evaluated on states beyond the training regime. The proposed neural SDEs can be evaluated quickly enough for use in model predictive control algorithms, or they can be used as simulators for model-based reinforcement learning. Furthermore, they make accurate predictions over long time horizons, even when trained on small datasets that cover limited regions of the state space. We demonstrate these capabilities th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; rewarded soup &#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22810;&#31181;&#20195;&#29702;&#22870;&#21169;&#65292;&#23454;&#29616;&#24494;&#35843;&#26435;&#37325;&#25554;&#20540;&#65292;&#20174;&#32780;&#22312;&#25972;&#20010;&#20559;&#22909;&#31354;&#38388;&#20013;&#23454;&#29616;&#24085;&#32047;&#25176;&#26368;&#20248;&#24191;&#20041;&#21270;&#12290;&#35813;&#26041;&#27861;&#22312;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#19978;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04488</link><description>&lt;p&gt;
&#22522;&#20110;&#32467;&#21512;&#22810;&#26679;&#21270;&#22870;&#21169;&#24494;&#35843;&#26435;&#37325;&#25554;&#20540;&#30340;&#24085;&#32047;&#25176;&#26368;&#20248;&#23545;&#40784;&#30340;&#22870;&#21169;&#27748;
&lt;/p&gt;
&lt;p&gt;
Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards. (arXiv:2306.04488v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; rewarded soup &#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22810;&#31181;&#20195;&#29702;&#22870;&#21169;&#65292;&#23454;&#29616;&#24494;&#35843;&#26435;&#37325;&#25554;&#20540;&#65292;&#20174;&#32780;&#22312;&#25972;&#20010;&#20559;&#22909;&#31354;&#38388;&#20013;&#23454;&#29616;&#24085;&#32047;&#25176;&#26368;&#20248;&#24191;&#20041;&#21270;&#12290;&#35813;&#26041;&#27861;&#22312;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#19978;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#39318;&#20808;&#22312;&#22823;&#37327;&#26080;&#21442;&#32771;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#26377;&#26631;&#27880;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#24378;&#21270;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLHF)&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#20351;&#32593;&#32476;&#19982;&#39044;&#26399;&#30340;&#20351;&#29992;&#30456;&#21305;&#37197;&#12290;&#28982;&#32780;&#65292;&#20195;&#29702;&#22870;&#21169;&#30340;&#32570;&#38519;&#21487;&#33021;&#20250;&#22952;&#30861;&#35757;&#32451;&#65292;&#23548;&#33268;&#27425;&#20248;&#32467;&#26524;&#65307;&#29616;&#23454;&#20219;&#21153;&#21644;&#20154;&#31867;&#24847;&#35265;&#30340;&#22810;&#26679;&#24615;&#21152;&#21095;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#37319;&#29992;&#22810;&#31574;&#30053;&#26041;&#27861;&#26469;&#25317;&#25265;&#22810;&#26679;&#21270;&#22870;&#21169;&#30340;&#24322;&#36136;&#24615;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#19981;&#26159;&#19987;&#27880;&#20110;&#21333;&#19968;&#30340;&#20808;&#39564;&#22870;&#21169;&#65292;&#32780;&#26159;&#22312;&#25972;&#20010;&#20559;&#22909;&#31354;&#38388;&#20013;&#23454;&#29616;&#24085;&#32047;&#25176;&#26368;&#20248;&#24191;&#20041;&#21270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; rewarded soup&#65292;&#39318;&#20808;&#29420;&#31435;&#22320;&#19987;&#38376;&#21270;&#22810;&#20010;&#32593;&#32476;(&#27599;&#20010;&#20195;&#29702;&#22870;&#21169;&#19968;&#20010;)&#65292;&#28982;&#21518;&#22312;&#23427;&#20204;&#30340;&#26435;&#37325;&#20043;&#38388;&#36827;&#34892;&#32447;&#24615;&#25554;&#20540;&#12290;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#26159;&#25104;&#21151;&#30340;&#65292;&#22240;&#20026;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#22810;&#26679;&#21270;&#22870;&#21169;&#26469;&#33258;&#20849;&#20139;&#30340;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#26102;&#65292;&#26435;&#37325;&#20173;&#28982;&#20445;&#25345;&#32447;&#24615;&#36830;&#25509;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models are first pre-trained on vast unsupervised datasets and then fine-tuned on labeled data. Reinforcement learning, notably from human feedback (RLHF), can further align the network with the intended usage. Yet the imperfections in the proxy reward may hinder the training and lead to suboptimal results; the diversity of objectives in real-world tasks and human opinions exacerbate the issue. This paper proposes embracing the heterogeneity of diverse rewards by following a multi-policy strategy. Rather than focusing on a single a priori reward, we aim for Pareto-optimal generalization across the entire space of preferences. To this end, we propose rewarded soup, first specializing multiple networks independently (one for each proxy reward) and then interpolating their weights linearly. This succeeds empirically because we show that the weights remain linearly connected when fine-tuned on diverse rewards from a shared pre-trained initialization. We demonstrate the effective
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#22312;&#22823;&#33041;&#27169;&#22411;&#20013;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#25429;&#33719;&#26102;&#38388;&#24207;&#21015;&#26469;&#36827;&#34892;&#19968;&#31995;&#21015;&#20851;&#20110;&#31070;&#32463;&#20803;&#38598;&#32676;&#30340;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2306.03812</link><description>&lt;p&gt;
&#22312;&#22823;&#33041;&#27169;&#22411;&#20013;&#36827;&#34892;&#24207;&#21015;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Computation with Sequences in a Model of the Brain. (arXiv:2306.03812v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03812
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#22312;&#22823;&#33041;&#27169;&#22411;&#20013;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#25429;&#33719;&#26102;&#38388;&#24207;&#21015;&#26469;&#36827;&#34892;&#19968;&#31995;&#21015;&#20851;&#20110;&#31070;&#32463;&#20803;&#38598;&#32676;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#22312;&#35768;&#22810;&#24212;&#29992;&#19978;&#36229;&#36807;&#20102;&#20154;&#31867;&#27700;&#24179;&#65292;&#20294;&#26159;&#22823;&#33041;&#23398;&#20064;&#33021;&#21147;&#30340;&#26222;&#36941;&#24615;&#12289;&#31283;&#20581;&#24615;&#21644;&#24555;&#36895;&#24615;&#20173;&#28982;&#26080;&#27861;&#21305;&#25932;&#12290;&#35748;&#30693;&#22914;&#20309;&#20135;&#29983;&#20110;&#31070;&#32463;&#27963;&#21160;&#26159;&#31070;&#32463;&#31185;&#23398;&#20013;&#19968;&#20010;&#26680;&#24515;&#30340;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#65292;&#19982;&#26234;&#33021;&#30740;&#31350;&#23494;&#19981;&#21487;&#20998;&#12290;&#22312;Papadimitriou [2020]&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#31070;&#32463;&#27963;&#21160;&#24418;&#24335;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#25968;&#23398;&#35777;&#26126;&#21644;&#27169;&#25311;&#26174;&#31034;&#21487;&#20197;&#21033;&#29992;&#31070;&#32463;&#20803;&#38598;&#32676;&#30340;&#21019;&#24314;&#21644;&#25805;&#32437;&#26469;&#23454;&#29616;&#26576;&#20123;&#31616;&#21333;&#30340;&#35748;&#30693;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26234;&#33021;&#34892;&#20026;&#20381;&#36182;&#20110;&#33021;&#22815;&#35782;&#21035;&#12289;&#23384;&#20648;&#21644;&#25805;&#20316;&#26102;&#38388;&#24207;&#21015;&#30340;&#21050;&#28608;&#65288;&#20363;&#22914;&#35745;&#21010;&#12289;&#35821;&#35328;&#12289;&#23548;&#33322;&#65289;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#21516;&#19968;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#31361;&#35302;&#26435;&#37325;&#21644;&#21487;&#22609;&#24615;&#65292;&#26102;&#38388;&#21487;&#20197;&#33258;&#28982;&#22320;&#20197;&#20248;&#20808;&#39034;&#24207;&#36827;&#34892;&#25429;&#33719;&#65292;&#20174;&#32780;&#21487;&#20197;&#36827;&#34892;&#19968;&#31995;&#21015;&#20851;&#20110;&#31070;&#32463;&#20803;&#38598;&#32676;&#24207;&#21015;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Even as machine learning exceeds human-level performance on many applications, the generality, robustness, and rapidity of the brain's learning capabilities remain unmatched. How cognition arises from neural activity is a central open question in neuroscience, inextricable from the study of intelligence itself. A simple formal model of neural activity was proposed in Papadimitriou [2020] and has been subsequently shown, through both mathematical proofs and simulations, to be capable of implementing certain simple cognitive operations via the creation and manipulation of assemblies of neurons. However, many intelligent behaviors rely on the ability to recognize, store, and manipulate temporal sequences of stimuli (planning, language, navigation, to list a few). Here we show that, in the same model, time can be captured naturally as precedence through synaptic weights and plasticity, and, as a result, a range of computations on sequences of assemblies can be carried out. In particular, r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;GNN&#22312;&#21516;&#26500;&#22270;&#20013;&#30340;&#21516;&#26500;&#33410;&#28857;&#21644;&#24322;&#26500;&#22270;&#20013;&#30340;&#24322;&#26500;&#33410;&#28857;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#32780;&#22312;&#21478;&#19968;&#32452;&#33410;&#28857;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21152;&#26435;&#32858;&#21512;&#25552;&#39640;GNN&#22312;&#19981;&#21516;&#32467;&#26500;&#27169;&#24335;&#33410;&#28857;&#19978;&#30340;&#34920;&#29616;&#65292;&#26377;&#25928;&#35299;&#20915;&#32467;&#26500;&#24046;&#24322;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.01323</link><description>&lt;p&gt;
&#25581;&#31034;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#32467;&#26500;&#24046;&#24322;&#24615;&#65306;&#19968;&#20010;&#23610;&#30721;&#36866;&#29992;&#20110;&#25152;&#26377;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Demystifying Structural Disparity in Graph Neural Networks: Can One Size Fit All?. (arXiv:2306.01323v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;GNN&#22312;&#21516;&#26500;&#22270;&#20013;&#30340;&#21516;&#26500;&#33410;&#28857;&#21644;&#24322;&#26500;&#22270;&#20013;&#30340;&#24322;&#26500;&#33410;&#28857;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#32780;&#22312;&#21478;&#19968;&#32452;&#33410;&#28857;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21152;&#26435;&#32858;&#21512;&#25552;&#39640;GNN&#22312;&#19981;&#21516;&#32467;&#26500;&#27169;&#24335;&#33410;&#28857;&#19978;&#30340;&#34920;&#29616;&#65292;&#26377;&#25928;&#35299;&#20915;&#32467;&#26500;&#24046;&#24322;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#23454;&#35777;&#21644;&#29702;&#35770;&#35777;&#25454;&#65292;&#25903;&#25345;&#23427;&#20204;&#22312;&#25429;&#25417;&#21516;&#26500;&#21644;&#26576;&#20123;&#24322;&#26500;&#22270;&#19978;&#30340;&#32467;&#26500;&#27169;&#24335;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;&#23454;&#38469;&#20013;&#30340;&#21516;&#26500;&#21644;&#24322;&#26500;&#22270;&#37117;&#30001;&#21516;&#26500;&#21644;&#24322;&#26500;&#32467;&#26500;&#27169;&#24335;&#30340;&#28151;&#21512;&#33410;&#28857;&#32452;&#25104;&#65292;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#32467;&#26500;&#24046;&#24322;&#24615;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#19981;&#21516;&#32467;&#26500;&#27169;&#24335;&#19979;&#30340;&#33410;&#28857;&#65288;&#20363;&#22914;&#22312;&#24322;&#26500;&#22270;&#20013;&#30340;&#21516;&#26500;&#33410;&#28857;&#65289;&#22312;GNN&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20998;&#26512;&#20173;&#28982;&#24456;&#26377;&#38480;&#12290;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#65292;GNN&#22312;&#21516;&#26500;&#22270;&#20013;&#30340;&#21516;&#26500;&#33410;&#28857;&#21644;&#24322;&#26500;&#22270;&#20013;&#30340;&#24322;&#26500;&#33410;&#28857;&#19978;&#30340;&#34920;&#29616;&#36890;&#24120;&#26159;&#20986;&#33394;&#30340;&#65292;&#32780;&#22312;&#21478;&#19968;&#32452;&#33410;&#28857;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#34920;&#29616;&#20986;&#24615;&#33021;&#24046;&#24322;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35782;&#21035;&#20102;&#27979;&#35797;&#23637;&#31034;&#19981;&#21516;&#32467;&#26500;&#27169;&#24335;&#33410;&#28857;&#26102;GNN&#30340;&#25928;&#24212;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;GNN&#30340;&#21152;&#26435;&#32858;&#21512;&#20197;&#36866;&#24212;&#24615;&#32467;&#26500;&#24046;&#24322;&#24615;&#30340;&#26032;&#26694;&#26550;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#35299;&#20915;&#32467;&#26500;&#24046;&#24322;&#24615;&#21644;&#25552;&#39640;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies on Graph Neural Networks(GNNs) provide both empirical and theoretical evidence supporting their effectiveness in capturing structural patterns on both homophilic and certain heterophilic graphs. Notably, most real-world homophilic and heterophilic graphs are comprised of a mixture of nodes in both homophilic and heterophilic structural patterns, exhibiting a structural disparity. However, the analysis of GNN performance with respect to nodes exhibiting different structural patterns, e.g., homophilic nodes in heterophilic graphs, remains rather limited. In the present study, we provide evidence that Graph Neural Networks(GNNs) on node classification typically perform admirably on homophilic nodes within homophilic graphs and heterophilic nodes within heterophilic graphs while struggling on the opposite node set, exhibiting a performance disparity. We theoretically and empirically identify effects of GNNs on testing nodes exhibiting distinct structural patterns. We then pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#39318;&#27425;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#36816;&#34892;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#19981;&#21040;2&#31186;&#65292;&#36825;&#26159;&#36890;&#36807;&#24341;&#20837;&#39640;&#25928;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#25913;&#36827;&#27493;&#39588;&#33976;&#39311;&#26469;&#23454;&#29616;&#30340;&#12290;</title><link>http://arxiv.org/abs/2306.00980</link><description>&lt;p&gt;
SnapFusion&#65306;&#31227;&#21160;&#35774;&#22791;&#19978;&#20004;&#31186;&#20869;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds. (arXiv:2306.00980v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#39318;&#27425;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#36816;&#34892;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#19981;&#21040;2&#31186;&#65292;&#36825;&#26159;&#36890;&#36807;&#24341;&#20837;&#39640;&#25928;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#25913;&#36827;&#27493;&#39588;&#33976;&#39311;&#26469;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#20174;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20013;&#21019;&#24314;&#20986;&#24778;&#20154;&#30340;&#22270;&#20687;&#65292;&#19981;&#20122;&#20110;&#19987;&#19994;&#33402;&#26415;&#23478;&#21644;&#25668;&#24433;&#24072;&#30340;&#20316;&#21697;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36739;&#22823;&#65292;&#20855;&#26377;&#22797;&#26434;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#25968;&#21313;&#20010;&#21435;&#22122;&#36845;&#20195;&#65292;&#20351;&#20854;&#35745;&#31639;&#26114;&#36149;&#19988;&#36816;&#34892;&#32531;&#24930;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#39640;&#31471;GPU&#21644;&#22522;&#20110;&#20113;&#30340;&#25512;&#29702;&#26469;&#25353;&#27604;&#20363;&#36816;&#34892;&#25193;&#25955;&#27169;&#22411;&#12290;&#36825;&#26159;&#26114;&#36149;&#30340;&#65292;&#24182;&#19988;&#28041;&#21450;&#38544;&#31169;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#24403;&#29992;&#25143;&#25968;&#25454;&#21457;&#36865;&#21040;&#31532;&#19977;&#26041;&#26102;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#39318;&#27425;&#22312;&#19981;&#21040;2&#31186;&#38047;&#20869;&#35299;&#38145;&#20102;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#36816;&#34892;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#39640;&#25928;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#25913;&#36827;&#27493;&#39588;&#33976;&#39311;&#26469;&#23454;&#29616;&#27492;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image diffusion models can create stunning images from natural language descriptions that rival the work of professional artists and photographers. However, these models are large, with complex network architectures and tens of denoising iterations, making them computationally expensive and slow to run. As a result, high-end GPUs and cloud-based inference are required to run diffusion models at scale. This is costly and has privacy implications, especially when user data is sent to a third party. To overcome these challenges, we present a generic approach that, for the first time, unlocks running text-to-image diffusion models on mobile devices in less than $2$ seconds. We achieve so by introducing efficient network architecture and improving step distillation. Specifically, we propose an efficient UNet by identifying the redundancy of the original model and reducing the computation of the image decoder via data distillation. Further, we enhance the step distillation by explori
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#26032;&#30340;&#35889;&#23884;&#20837;&#26041;&#27861;&#65292;&#19968;&#31181;&#22522;&#20110;&#20989;&#25968;&#20998;&#26512;&#21407;&#29702;&#21644;&#26680;&#26041;&#27861;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#32593;&#32476;&#20248;&#21270;&#25439;&#22833;&#65292;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#21644;&#23454;&#38469;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#26032;&#30340;&#37319;&#26679;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.00742</link><description>&lt;p&gt;
&#22522;&#20110;&#35889;&#23884;&#20837;&#30340;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Going Deeper with Spectral Embeddings. (arXiv:2306.00742v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#26032;&#30340;&#35889;&#23884;&#20837;&#26041;&#27861;&#65292;&#19968;&#31181;&#22522;&#20110;&#20989;&#25968;&#20998;&#26512;&#21407;&#29702;&#21644;&#26680;&#26041;&#27861;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#32593;&#32476;&#20248;&#21270;&#25439;&#22833;&#65292;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#21644;&#23454;&#38469;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#26032;&#30340;&#37319;&#26679;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26377;&#25928;&#22320;&#22788;&#29702;&#28023;&#37327;&#30340;&#25968;&#25454;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#23545;&#20854;&#36827;&#34892;&#34920;&#24449;&#65292;&#31185;&#23398;&#23478;&#20204;&#37319;&#29992;&#34920;&#31034;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#36825;&#20123;&#26041;&#27861;&#19982;&#19968;&#20123;&#24213;&#23618;&#36816;&#31639;&#30340;&#35889;&#20998;&#35299;&#20043;&#38388;&#23637;&#29616;&#20986;&#26126;&#26174;&#30340;&#32852;&#31995;&#12290;&#22312;&#21382;&#21490;&#19978;&#65292;&#26159;&#36890;&#36807;&#22312;&#25968;&#25454;&#30340;&#39030;&#37096;&#26500;&#24314;&#22270;&#24418;&#26469;&#24314;&#31435;&#26126;&#30830;&#30340;&#35889;&#23884;&#20837;&#65292;&#32780;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#26041;&#27861;&#65306;&#19968;&#31181;&#22522;&#20110;&#20989;&#25968;&#20998;&#26512;&#21407;&#29702;&#21644;&#26680;&#26041;&#27861;&#26500;&#24314;&#30340;&#65292;&#36825;&#23558;&#23548;&#33268;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#31639;&#27861;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#32593;&#32476;&#35757;&#32451;&#20197;&#20248;&#21270;&#22522;&#26412;&#21464;&#20998;&#25439;&#22833;&#30340;&#31639;&#27861;&#65292;&#23427;&#20204;&#20135;&#29983;&#20102;&#23454;&#38469;&#26377;&#25928;&#30340;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#31639;&#27861;&#65292;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#34920;&#24449;&#26469;&#22312;&#19968;&#27493;&#20013;&#29983;&#25104;&#26032;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
To make sense of millions of raw data and represent them efficiently, practitioners rely on representation learning. Recently, deep connections have been shown between these approaches and the spectral decompositions of some underlying operators. Historically, explicit spectral embeddings were built from graphs constructed on top of the data. In contrast, we propose two new methods to build spectral embeddings: one based on functional analysis principles and kernel methods, which leads to algorithms with theoretical guarantees, and the other based on deep networks trained to optimize principled variational losses, which yield practically efficient algorithms. Furthermore, we provide a new sampling algorithm that leverages learned representations to generate new samples in a single step.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCSD&#30340;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20195;&#30721;&#24322;&#21619;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#35753;&#32452;&#32455;&#21327;&#20316;&#35757;&#32451;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#23454;&#39564;&#20013;&#20998;&#21035;&#20351;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#30340;&#26816;&#27979;&#25928;&#26524;&#65292;&#24182;&#19988;&#27604;&#38598;&#20013;&#24335;&#21644;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.00038</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20195;&#30721;&#24322;&#21619;&#26816;&#27979;&#26041;&#27861;(FedCSD)
&lt;/p&gt;
&lt;p&gt;
FedCSD: A Federated Learning Based Approach for Code-Smell Detection. (arXiv:2306.00038v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCSD&#30340;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20195;&#30721;&#24322;&#21619;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#35753;&#32452;&#32455;&#21327;&#20316;&#35757;&#32451;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#23454;&#39564;&#20013;&#20998;&#21035;&#20351;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#30340;&#26816;&#27979;&#25928;&#26524;&#65292;&#24182;&#19988;&#27604;&#38598;&#20013;&#24335;&#21644;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCSD&#30340;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20195;&#30721;&#24322;&#21619;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#35753;&#32452;&#32455;&#21327;&#20316;&#35757;&#32451;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#19977;&#20010;&#23454;&#39564;&#26469;&#25903;&#25345;&#36825;&#20123;&#26029;&#35328;&#65292;&#36825;&#20123;&#23454;&#39564;&#21033;&#29992;&#20102;&#19977;&#20010;&#25163;&#21160;&#39564;&#35777;&#30340;&#25968;&#25454;&#38598;&#65292;&#26469;&#26816;&#27979;&#21644;&#30740;&#31350;&#19981;&#21516;&#30340;&#20195;&#30721;&#24322;&#21619;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a Federated Learning Code Smell Detection (FedCSD) approach that allows organizations to collaboratively train federated ML models while preserving their data privacy. These assertions have been supported by three experiments that have significantly leveraged three manually validated datasets aimed at detecting and examining different code smell scenarios. In experiment 1, which was concerned with a centralized training experiment, dataset two achieved the lowest accuracy (92.30%) with fewer smells, while datasets one and three achieved the highest accuracy with a slight difference (98.90% and 99.5%, respectively). This was followed by experiment 2, which was concerned with cross-evaluation, where each ML model was trained using one dataset, which was then evaluated over the other two datasets. Results from this experiment show a significant drop in the model's accuracy (lowest accuracy: 63.80\%) where fewer smells exist in the training dataset, which has a noticeab
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30830;&#23450;&#20102;&#19982;&#30456;&#20851;&#24037;&#20316;&#26377;&#20851;&#30340;&#32972;&#26223;&#34920;&#31034;&#27861;&#65292;&#24182;&#22522;&#20110;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#21644;SHAP&#26041;&#27861;&#35780;&#20272;&#20102;&#22320;&#29702;&#31354;&#38388;&#32972;&#26223;&#20449;&#24687;&#22312;&#20986;&#34892;&#26041;&#24335;&#26816;&#27979;&#20013;&#30340;&#36129;&#29486;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25551;&#36848;&#19982;&#22522;&#30784;&#35774;&#26045;&#32593;&#32476;&#20851;&#31995;&#30340;&#29305;&#24449;&#23545;&#39044;&#27979;&#26377;&#26174;&#30528;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2305.19428</link><description>&lt;p&gt;
&#35780;&#20272;&#22320;&#29702;&#31354;&#38388;&#32972;&#26223;&#20449;&#24687;&#22312;&#20986;&#34892;&#26041;&#24335;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluating geospatial context information for travel mode detection. (arXiv:2305.19428v1 [physics.soc-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30830;&#23450;&#20102;&#19982;&#30456;&#20851;&#24037;&#20316;&#26377;&#20851;&#30340;&#32972;&#26223;&#34920;&#31034;&#27861;&#65292;&#24182;&#22522;&#20110;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#21644;SHAP&#26041;&#27861;&#35780;&#20272;&#20102;&#22320;&#29702;&#31354;&#38388;&#32972;&#26223;&#20449;&#24687;&#22312;&#20986;&#34892;&#26041;&#24335;&#26816;&#27979;&#20013;&#30340;&#36129;&#29486;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25551;&#36848;&#19982;&#22522;&#30784;&#35774;&#26045;&#32593;&#32476;&#20851;&#31995;&#30340;&#29305;&#24449;&#23545;&#39044;&#27979;&#26377;&#26174;&#30528;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#20840;&#29699;&#23450;&#20301;&#21355;&#26143;&#31995;&#32479;&#65288;GNSS&#65289;&#36712;&#36857;&#26816;&#27979;&#20986;&#34892;&#26041;&#24335;&#23545;&#20102;&#35299;&#20010;&#20154;&#20986;&#34892;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#65292;&#26159;&#23454;&#29616;&#21487;&#25345;&#32493;&#20132;&#36890;&#31995;&#32479;&#30340;&#20808;&#20915;&#26465;&#20214;&#12290;&#34429;&#28982;&#30740;&#31350;&#24050;&#32463;&#35748;&#35782;&#21040;&#23558;&#22320;&#29702;&#31354;&#38388;&#32972;&#26223;&#20449;&#24687;&#32435;&#20837;&#20986;&#34892;&#26041;&#24335;&#26816;&#27979;&#27169;&#22411;&#20013;&#30340;&#22909;&#22788;&#65292;&#20294;&#24456;&#23569;&#26377;&#25991;&#31456;&#24635;&#32467;&#20102;&#32972;&#26223;&#24314;&#27169;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#36825;&#20123;&#32972;&#26223;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#36825;&#22952;&#30861;&#20102;&#39640;&#25928;&#27169;&#22411;&#30340;&#24320;&#21457;&#12290;&#26412;&#25991;&#30830;&#23450;&#20102;&#19982;&#30456;&#20851;&#24037;&#20316;&#26377;&#20851;&#30340;&#32972;&#26223;&#34920;&#31034;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#31649;&#36947;&#65292;&#22522;&#20110;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#21644;SHapley Additive exPlanation&#65288;SHAP&#65289;&#26041;&#27861;&#35780;&#20272;&#22320;&#29702;&#31354;&#38388;&#32972;&#26223;&#20449;&#24687;&#22312;&#20986;&#34892;&#26041;&#24335;&#26816;&#27979;&#20013;&#30340;&#36129;&#29486;&#12290;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;GNSS&#36319;&#36394;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#25551;&#36848;&#19982;&#22522;&#30784;&#35774;&#26045;&#32593;&#32476;&#20851;&#31995;&#30340;&#29305;&#24449;&#65292;&#20363;&#22914;&#21040;&#38081;&#36335;&#25110;&#36947;&#36335;&#32593;&#32476;&#30340;&#36317;&#31163;&#65292;&#23545;&#27169;&#22411;&#30340;&#39044;&#27979;&#26377;&#26174;&#30528;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting travel modes from global navigation satellite system (GNSS) trajectories is essential for understanding individual travel behaviour and a prerequisite for achieving sustainable transport systems. While studies have acknowledged the benefits of incorporating geospatial context information into travel mode detection models, few have summarized context modelling approaches and analyzed the significance of these context features, hindering the development of an efficient model. Here, we identify context representations from related work and propose an analytical pipeline to assess the contribution of geospatial context information for travel mode detection based on a random forest model and the SHapley Additive exPlanation (SHAP) method. Through experiments on a large-scale GNSS tracking dataset, we report that features describing relationships with infrastructure networks, such as the distance to the railway or road network, significantly contribute to the model's prediction. Mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#29992;&#20110;&#24191;&#20041;&#22240;&#26524;&#25935;&#24863;&#24615;&#20998;&#26512;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#28789;&#27963;&#30340;&#36793;&#38469;&#25935;&#24863;&#24615;&#27169;&#22411;&#65292;&#25512;&#23548;&#20986;&#21508;&#31181;&#22240;&#26524;&#25928;&#24212;&#30340;&#23574;&#38160;&#30028;&#38480;&#12290;&#35813;&#26694;&#26550;&#36866;&#29992;&#20110;&#22810;&#31181;&#35774;&#32622;&#65292;&#21253;&#25324;&#31163;&#25955;&#12289;&#36830;&#32493;&#21644;&#26102;&#21464;&#30340;&#27835;&#30103;&#12290;</title><link>http://arxiv.org/abs/2305.16988</link><description>&lt;p&gt;
&#24191;&#20041;&#22240;&#26524;&#25935;&#24863;&#24615;&#20998;&#26512;&#30340;&#23574;&#38160;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Sharp Bounds for Generalized Causal Sensitivity Analysis. (arXiv:2305.16988v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#29992;&#20110;&#24191;&#20041;&#22240;&#26524;&#25935;&#24863;&#24615;&#20998;&#26512;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#28789;&#27963;&#30340;&#36793;&#38469;&#25935;&#24863;&#24615;&#27169;&#22411;&#65292;&#25512;&#23548;&#20986;&#21508;&#31181;&#22240;&#26524;&#25928;&#24212;&#30340;&#23574;&#38160;&#30028;&#38480;&#12290;&#35813;&#26694;&#26550;&#36866;&#29992;&#20110;&#22810;&#31181;&#35774;&#32622;&#65292;&#21253;&#25324;&#31163;&#25955;&#12289;&#36830;&#32493;&#21644;&#26102;&#21464;&#30340;&#27835;&#30103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#23519;&#25968;&#25454;&#20013;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#23545;&#20110;&#35768;&#22810;&#23398;&#31185;&#22914;&#21307;&#23398;&#21644;&#32463;&#27982;&#23398;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#25918;&#26494;&#26080;&#28151;&#28102;&#24615;&#20551;&#35774;&#65288;&#22240;&#26524;&#25935;&#24863;&#24615;&#20998;&#26512;&#65289;&#19979;&#30340;&#22240;&#26524;&#25928;&#24212;&#30340;&#23574;&#38160;&#30028;&#38480;&#20173;&#22312;&#30740;&#31350;&#20013;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#20855;&#26377;&#23574;&#38160;&#30028;&#38480;&#30340;&#24037;&#20316;&#20165;&#38480;&#20110;&#30456;&#23545;&#31616;&#21333;&#30340;&#35774;&#32622;&#65288;&#20363;&#22914;&#65292;&#21333;&#19968;&#20108;&#20803;&#27835;&#30103;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#21508;&#31181;&#35774;&#32622;&#19979;&#30340;&#26410;&#35266;&#23519;&#28151;&#28102;&#22240;&#32032;&#19979;&#30340;&#22240;&#26524;&#25935;&#24863;&#24615;&#20998;&#26512;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36793;&#38469;&#25935;&#24863;&#24615;&#27169;&#22411;&#65288;MSM&#65289;&#30340;&#28789;&#27963;&#25512;&#24191;&#65292;&#28982;&#21518;&#25512;&#23548;&#20986;&#22823;&#31867;&#22240;&#26524;&#25928;&#24212;&#30340;&#23574;&#38160;&#30028;&#38480;&#12290;&#36825;&#21253;&#25324;&#65288;&#26465;&#20214;&#65289;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65292;&#20013;&#20171;&#20998;&#26512;&#21644;&#36335;&#24452;&#20998;&#26512;&#30340;&#25928;&#24212;&#65292;&#20197;&#21450;&#20998;&#24067;&#25928;&#24212;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#25935;&#24863;&#24615;&#27169;&#22411;&#36866;&#29992;&#20110;&#31163;&#25955;&#12289;&#36830;&#32493;&#21644;&#26102;&#21464;&#30340;&#27835;&#30103;&#12290;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#26410;&#35266;&#23519;&#28151;&#28102;&#23548;&#33268;&#30340;&#37096;&#20998;&#35782;&#21035;&#38382;&#39064;&#35299;&#37322;&#20026;&#20998;&#24067;&#20559;&#31227;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal inference from observational data is crucial for many disciplines such as medicine and economics. However, sharp bounds for causal effects under relaxations of the unconfoundedness assumption (causal sensitivity analysis) are subject to ongoing research. So far, works with sharp bounds are restricted to fairly simple settings (e.g., a single binary treatment). In this paper, we propose a unified framework for causal sensitivity analysis under unobserved confounding in various settings. For this, we propose a flexible generalization of the marginal sensitivity model (MSM) and then derive sharp bounds for a large class of causal effects. This includes (conditional) average treatment effects, effects for mediation analysis and path analysis, and distributional effects. Furthermore, our sensitivity model is applicable to discrete, continuous, and time-varying treatments. It allows us to interpret the partial identification problem under unobserved confounding as a distribution shift
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;ParaDiGMS&#65292;&#21487;&#20197;&#36890;&#36807;&#24182;&#34892;&#22788;&#29702;&#22810;&#20010;&#27493;&#39588;&#26469;&#21152;&#36895;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#12290;ParaDiGMS&#26159;&#31532;&#19968;&#20010;&#20351;&#35745;&#31639;&#36895;&#24230;&#21644;&#37319;&#26679;&#25928;&#29575;&#23454;&#29616;&#24179;&#34913;&#30340;&#25193;&#25955;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#19982;&#29616;&#26377;&#26041;&#27861;&#20860;&#23481;&#12290;</title><link>http://arxiv.org/abs/2305.16317</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30340;&#24182;&#34892;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Parallel Sampling of Diffusion Models. (arXiv:2305.16317v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;ParaDiGMS&#65292;&#21487;&#20197;&#36890;&#36807;&#24182;&#34892;&#22788;&#29702;&#22810;&#20010;&#27493;&#39588;&#26469;&#21152;&#36895;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#12290;ParaDiGMS&#26159;&#31532;&#19968;&#20010;&#20351;&#35745;&#31639;&#36895;&#24230;&#21644;&#37319;&#26679;&#25928;&#29575;&#23454;&#29616;&#24179;&#34913;&#30340;&#25193;&#25955;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#19982;&#29616;&#26377;&#26041;&#27861;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20294;&#37319;&#26679;&#36895;&#24230;&#32531;&#24930;&#65292;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;1000&#27425;&#39034;&#24207;&#21435;&#22122;&#27493;&#39588;&#25165;&#33021;&#24471;&#21040;&#19968;&#20010;&#26679;&#26412;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#20132;&#25442;&#35745;&#31639;&#26426;&#22788;&#29702;&#36895;&#24230;&#21644;&#37319;&#26679;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#29468;&#27979;&#26410;&#26469;&#30340;&#21435;&#22122;&#27493;&#39588;&#30340;&#35299;&#20915;&#26041;&#26696;&#24182;&#36880;&#27493;&#32454;&#21270;&#33267;&#25910;&#25947;&#30340;Picard&#36845;&#20195;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#21457;&#29616;&#65306;&#23613;&#31649;&#21435;&#22122;&#27493;&#39588;&#26377;&#39034;&#24207;&#24615;&#65292;&#20294;&#20173;&#28982;&#21487;&#20197;&#24182;&#34892;&#37319;&#26679;&#12290;&#22522;&#20110;&#36825;&#19968;&#27934;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ParaDiGMS&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#20197;&#24182;&#34892;&#26041;&#24335;&#21435;&#22122;&#22810;&#20010;&#27493;&#39588;&#21152;&#36895;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#37319;&#26679;&#30340;&#26032;&#26041;&#27861;&#12290;ParaDiGMS&#26159;&#31532;&#19968;&#20010;&#22312;&#35745;&#31639;&#22788;&#29702;&#36895;&#24230;&#21644;&#37319;&#26679;&#25928;&#29575;&#19978;&#23454;&#29616;&#24179;&#34913;&#30340;&#25193;&#25955;&#37319;&#26679;&#26041;&#27861;&#65292;&#29978;&#33267;&#36824;&#20860;&#23481;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are powerful generative models but suffer from slow sampling, often taking 1000 sequential denoising steps for one sample. As a result, considerable efforts have been directed toward reducing the number of denoising steps, but these methods hurt sample quality. Instead of reducing the number of denoising steps (trading quality for speed), in this paper we explore an orthogonal approach: can we run the denoising steps in parallel (trading compute for speed)? In spite of the sequential nature of the denoising steps, we show that surprisingly it is possible to parallelize sampling via Picard iterations, by guessing the solution of future denoising steps and iteratively refining until convergence. With this insight, we present ParaDiGMS, a novel method to accelerate the sampling of pretrained diffusion models by denoising multiple steps in parallel. ParaDiGMS is the first diffusion sampling method that enables trading compute for speed and is even compatible with existing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#24615;&#31034;&#33539;&#25915;&#20987;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;advICL&#65292;&#36890;&#36807;&#25913;&#21464;&#31034;&#33539;&#32780;&#19981;&#25913;&#21464;&#36755;&#20837;&#26469;&#35823;&#23548;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#30528;&#31034;&#33539;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2305.14950</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#31034;&#33539;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Adversarial Demonstration Attacks on Large Language Models. (arXiv:2305.14950v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#24615;&#31034;&#33539;&#25915;&#20987;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;advICL&#65292;&#36890;&#36807;&#25913;&#21464;&#31034;&#33539;&#32780;&#19981;&#25913;&#21464;&#36755;&#20837;&#26469;&#35823;&#23548;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#30528;&#31034;&#33539;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26356;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#22914;ChatGPT&#21644;GPT-4&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;-&#26631;&#31614;&#23545;&#20316;&#20026;&#39044;&#20808;&#26465;&#20214;&#25552;&#31034;&#65292;&#24050;&#32463;&#22312;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#29305;&#23450;&#20219;&#21153;&#26041;&#38754;&#33719;&#24471;&#26174;&#33879;&#30340;&#37325;&#35201;&#24615;&#12290;&#34429;&#28982;&#24341;&#20837;&#31034;&#33539;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;LLMs&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20063;&#21487;&#33021;&#24341;&#20837;&#26032;&#30340;&#23433;&#20840;&#38382;&#39064;&#65306;&#25915;&#20987;&#32773;&#21487;&#20197;&#20165;&#20165;&#25805;&#32437;&#31034;&#33539;&#32780;&#19981;&#25913;&#21464;&#36755;&#20837;&#26469;&#36827;&#34892;&#25915;&#20987;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#23545;&#25239;&#24615;&#30340;&#35282;&#24230;&#35843;&#26597;&#20102;ICL&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#31034;&#33539;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;advICL&#30340;&#26032;&#22411;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#20165;&#20165;&#25913;&#21464;&#31034;&#33539;&#32780;&#19981;&#25913;&#21464;&#36755;&#20837;&#20197;&#35823;&#23548;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#30528;&#31034;&#33539;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#23558;&#20250;&#38477;&#20302;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#31034;&#33539;&#30340;&#22266;&#26377;&#29305;&#24615;&#26159;&#21487;&#20197;&#34987;&#20351;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the emergence of more powerful large language models (LLMs), such as ChatGPT and GPT-4, in-context learning (ICL) has gained significant prominence in leveraging these models for specific tasks by utilizing data-label pairs as precondition prompts. While incorporating demonstrations can greatly enhance the performance of LLMs across various tasks, it may introduce a new security concern: attackers can manipulate only the demonstrations without changing the input to perform an attack. In this paper, we investigate the security concern of ICL from an adversarial perspective, focusing on the impact of demonstrations. We propose a novel attack method named advICL, which aims to manipulate only the demonstration without changing the input to mislead the models. Our results demonstrate that as the number of demonstrations increases, the robustness of in-context learning would decrease. Additionally, we also identify the intrinsic property of the demonstrations is that they can be used (
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#32858;&#21512;&#22836;&#27880;&#24847;&#21147;(Grouped Head Attention)&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#32452;&#32422;&#26463;&#36827;&#34892;&#35757;&#32451;&#65292;&#20026;&#27880;&#24847;&#21147;&#22836;&#36827;&#34892;&#20998;&#32452;&#65292;&#20854;&#20013;&#27599;&#20010;&#32452;&#19987;&#27880;&#20110;&#19968;&#20010;&#37325;&#35201;&#32780;&#29420;&#29305;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;&#27492;&#26041;&#27861;&#21487;&#20197;&#32531;&#35299;MHA&#30340;&#20887;&#20313;&#24615;&#21644;&#36807;&#24230;&#21442;&#25968;&#21270;&#38382;&#39064;&#65292;&#24182;&#23548;&#33268;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;MHA&#65292;&#36827;&#32780;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.14380</link><description>&lt;p&gt;
&#23547;&#25214;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#25903;&#26609;
&lt;/p&gt;
&lt;p&gt;
Finding the Pillars of Strength for Multi-Head Attention. (arXiv:2305.14380v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#32858;&#21512;&#22836;&#27880;&#24847;&#21147;(Grouped Head Attention)&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#32452;&#32422;&#26463;&#36827;&#34892;&#35757;&#32451;&#65292;&#20026;&#27880;&#24847;&#21147;&#22836;&#36827;&#34892;&#20998;&#32452;&#65292;&#20854;&#20013;&#27599;&#20010;&#32452;&#19987;&#27880;&#20110;&#19968;&#20010;&#37325;&#35201;&#32780;&#29420;&#29305;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;&#27492;&#26041;&#27861;&#21487;&#20197;&#32531;&#35299;MHA&#30340;&#20887;&#20313;&#24615;&#21644;&#36807;&#24230;&#21442;&#25968;&#21270;&#38382;&#39064;&#65292;&#24182;&#23548;&#33268;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;MHA&#65292;&#36827;&#32780;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#22810;&#22836;&#27880;&#24847;&#21147;(Multi-Head Attention, MHA)&#30340;&#19968;&#20123;&#38382;&#39064;&#65292;&#20363;&#22914;&#20887;&#20313;&#24615;&#21644;&#36807;&#24230;&#21442;&#25968;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MHA&#30340;&#22836;&#26368;&#21021;&#35774;&#35745;&#20026;&#20174;&#19981;&#21516;&#30340;&#34920;&#24449;&#23376;&#31354;&#38388;&#20013;&#20851;&#27880;&#20449;&#24687;&#65292;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#19968;&#20123;&#27880;&#24847;&#21147;&#22836;&#21487;&#33021;&#23398;&#20064;&#31867;&#20284;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#20462;&#21098;&#26469;&#25552;&#39640;&#25928;&#29575;&#32780;&#19981;&#20250;&#25439;&#23475;&#24615;&#33021;&#12290;&#21463;&#26368;&#23567;&#20887;&#20313;&#29305;&#24449;&#36873;&#25321;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20551;&#35774;&#32858;&#28966;&#20110;&#26368;&#20855;&#20195;&#34920;&#24615;&#21644;&#29420;&#29305;&#24615;&#30340;&#29305;&#24449;&#65292;&#24182;&#26368;&#23567;&#21270;&#36164;&#28304;&#28040;&#32791;&#65292;&#21487;&#20197;&#32531;&#35299;&#19978;&#36848;&#38382;&#39064;&#65292;&#24182;&#23548;&#33268;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;MHA&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32858;&#21512;&#22836;&#27880;&#24847;&#21147;(Grouped Head Attention)&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#32452;&#32422;&#26463;&#36827;&#34892;&#35757;&#32451;&#65292;&#20026;&#27880;&#24847;&#21147;&#22836;&#36827;&#34892;&#20998;&#32452;&#65292;&#20854;&#20013;&#27599;&#20010;&#32452;&#19987;&#27880;&#20110;&#19968;&#20010;&#37325;&#35201;&#32780;&#29420;&#29305;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#25237;&#31080;&#20445;&#30041;&#31243;&#24207;(Voting-to-Stay)&#65292;&#20197;&#21024;&#38500;&#20887;&#20313;&#22836;&#65292;&#20174;&#32780;&#23454;&#29616;&#20855;&#26377;&#26356;&#36731;&#37327;&#32423;&#26435;&#37325;&#30340;&#36716;&#25442;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#30693;&#21517;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#28040;&#34701;&#30740;&#31350;&#25552;&#20379;&#20102;&#25903;&#25345;&#24615;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have revealed some issues of Multi-Head Attention (MHA), e.g., redundancy and over-parameterization. Specifically, the heads of MHA were originally designed to attend to information from different representation subspaces, whereas prior studies found that some attention heads likely learn similar features and can be pruned without harming performance. Inspired by the minimum-redundancy feature selection, we assume that focusing on the most representative and distinctive features with minimum resources can mitigate the above issues and lead to more effective and efficient MHAs. In particular, we propose Grouped Head Attention, trained with a self-supervised group constraint that group attention heads, where each group focuses on an essential but distinctive feature subset. We additionally propose a Voting-to-Stay procedure to remove redundant heads, thus achieving a transformer with lighter weights. Moreover, our method achieves significant performance gains on three well
&lt;/p&gt;</description></item><item><title>VIP5&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#32479;&#19968;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#20010;&#24615;&#21270;&#27169;&#24577;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#30340;&#20849;&#20139;&#26550;&#26500;&#65292;&#25552;&#39640;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.14302</link><description>&lt;p&gt;
VIP5&#65306;&#38754;&#21521;&#25512;&#33616;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
VIP5: Towards Multimodal Foundation Models for Recommendation. (arXiv:2305.14302v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14302
&lt;/p&gt;
&lt;p&gt;
VIP5&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#32479;&#19968;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#20010;&#24615;&#21270;&#27169;&#24577;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#30340;&#20849;&#20139;&#26550;&#26500;&#65292;&#25552;&#39640;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#25512;&#33616;&#31995;&#32479;&#65288;RecSys&#65289;&#26159;&#19977;&#20010;&#37325;&#35201;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#65292;&#23427;&#20204;&#20256;&#32479;&#19978;&#29420;&#31435;&#21457;&#23637;&#65292;&#23548;&#33268;&#20102;&#19981;&#21516;&#30340;&#24314;&#27169;&#21644;&#24037;&#31243;&#26041;&#27861;&#12290;&#36825;&#22952;&#30861;&#20102;&#36825;&#20123;&#39046;&#22495;&#30452;&#25509;&#20174;&#24444;&#27492;&#30340;&#36827;&#23637;&#20013;&#21463;&#30410;&#12290;&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#32479;&#19968;&#19981;&#21516;&#27169;&#24577;&#21644;&#38382;&#39064;&#34920;&#36848;&#30340;&#28508;&#22312;&#36890;&#29992;&#25509;&#21475;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24320;&#21457;&#19968;&#20010;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65288;MFM&#65289;&#65292;&#32771;&#34385;&#20102;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#20010;&#24615;&#21270;&#27169;&#24577;&#65292;&#22312;P5&#25512;&#33616;&#33539;&#24335;&#19979;&#32479;&#19968;&#21508;&#31181;&#27169;&#24577;&#21644;&#25512;&#33616;&#20219;&#21153;&#65292;&#22240;&#27492;&#21629;&#21517;&#20026;VIP5&#65288;Visual P5&#65289;&#65292;&#20197;&#25913;&#36827;&#25512;&#33616;&#21151;&#33021;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#22810;&#27169;&#24577;&#20010;&#24615;&#21270;&#25552;&#31034;&#26469;&#36866;&#24212;&#22810;&#20010;&#27169;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computer Vision (CV), Natural Language Processing (NLP), and Recommender Systems (RecSys) are three prominent AI applications that have traditionally developed independently, resulting in disparate modeling and engineering methodologies. This has impeded the ability for these fields to directly benefit from each other's advancements. With the recent development of foundation models, large language models have emerged as a potential general-purpose interface for unifying different modalities and problem formulations. In light of this, we propose the development of a multimodal foundation model (MFM) considering visual, textual, and personalization modalities under the P5 recommendation paradigm, thus named VIP5 (Visual P5), to unify various modalities and recommendation tasks. This will enable the processing of multiple modalities in a shared architecture for improved recommendations. To achieve this, we introduce multimodal personalized prompts to accommodate multiple modalities under 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CANVI&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#33268;&#21270;&#39044;&#27979;&#22120;&#24182;&#20351;&#29992;&#39044;&#27979;&#25928;&#29575;&#36827;&#34892;&#27604;&#36739;&#65292;&#26469;&#25552;&#20379;&#20855;&#26377;&#20445;&#35777;&#30340;&#21518;&#39564;&#36817;&#20284;&#32467;&#26524;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#24555;&#36895;&#35745;&#31639;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#24182;&#19988;&#23545;&#20110;&#20505;&#36873;&#36817;&#20284;&#22120;&#30340;&#35774;&#35745;&#20915;&#31574;&#26080;&#38656;&#20851;&#27880;&#12290;&#27492;&#22806;&#65292;CANVI&#33021;&#22815;&#22312;&#26080;&#20284;&#28982;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.14275</link><description>&lt;p&gt;
&#20855;&#26377;&#35206;&#30422;&#20445;&#35777;&#30340;&#20998;&#25674;&#21464;&#20998;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Amortized Variational Inference with Coverage Guarantees. (arXiv:2305.14275v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14275
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CANVI&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#33268;&#21270;&#39044;&#27979;&#22120;&#24182;&#20351;&#29992;&#39044;&#27979;&#25928;&#29575;&#36827;&#34892;&#27604;&#36739;&#65292;&#26469;&#25552;&#20379;&#20855;&#26377;&#20445;&#35777;&#30340;&#21518;&#39564;&#36817;&#20284;&#32467;&#26524;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#24555;&#36895;&#35745;&#31639;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#24182;&#19988;&#23545;&#20110;&#20505;&#36873;&#36817;&#20284;&#22120;&#30340;&#35774;&#35745;&#20915;&#31574;&#26080;&#38656;&#20851;&#27880;&#12290;&#27492;&#22806;&#65292;CANVI&#33021;&#22815;&#22312;&#26080;&#20284;&#28982;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25674;&#21464;&#20998;&#25512;&#26029;&#20135;&#29983;&#20102;&#19968;&#20010;&#21518;&#39564;&#36817;&#20284;&#65292;&#21487;&#20197;&#24555;&#36895;&#35745;&#31639;&#32473;&#23450;&#20219;&#20309;&#26032;&#35266;&#27979;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#20123;&#36817;&#20284;&#21518;&#39564;&#30340;&#36136;&#37327;&#65292;&#24456;&#23569;&#26377;&#20445;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CANVI&#30340;&#19968;&#33268;&#21270;&#20998;&#25674;&#31070;&#32463;&#21464;&#20998;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#25193;&#23637;&#12289;&#26131;&#20110;&#23454;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#20445;&#35777;&#30340;&#36793;&#38469;&#35206;&#30422;&#12290;&#32473;&#23450;&#19968;&#31995;&#21015;&#20505;&#36873;&#30340;&#20998;&#25674;&#21518;&#39564;&#36817;&#20284;&#22120;&#65292;CANVI&#22522;&#20110;&#27599;&#20010;&#20505;&#36873;&#26500;&#24314;&#19968;&#33268;&#21270;&#39044;&#27979;&#22120;&#65292;&#20351;&#29992;&#39044;&#27979;&#25928;&#29575;&#36825;&#20010;&#24230;&#37327;&#26631;&#20934;&#27604;&#36739;&#39044;&#27979;&#22120;&#65292;&#24182;&#36820;&#22238;&#26368;&#39640;&#25928;&#30340;&#39044;&#27979;&#22120;&#12290;CANVI&#30830;&#20445;&#25152;&#24471;&#21040;&#30340;&#39044;&#27979;&#22120;&#26500;&#24314;&#30340;&#21306;&#22495;&#20197;&#29992;&#25143;&#25351;&#23450;&#30340;&#27010;&#29575;&#27700;&#24179;&#21253;&#21547;&#30495;&#23454;&#20540;&#12290;CANVI&#23545;&#20505;&#36873;&#36817;&#20284;&#22120;&#30340;&#21046;&#23450;&#20915;&#31574;&#19981;&#20851;&#24515;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#35775;&#38382;&#21069;&#21521;&#27169;&#22411;&#30340;&#26679;&#26412;&#65292;&#21487;&#20197;&#22312;&#26080;&#20284;&#28982;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#39044;&#27979;&#25928;&#29575;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Amortized variational inference produces a posterior approximation that can be rapidly computed given any new observation. Unfortunately, there are few guarantees about the quality of these approximate posteriors. We propose Conformalized Amortized Neural Variational Inference (CANVI), a procedure that is scalable, easily implemented, and provides guaranteed marginal coverage. Given a collection of candidate amortized posterior approximators, CANVI constructs conformalized predictors based on each candidate, compares the predictors using a metric known as predictive efficiency, and returns the most efficient predictor. CANVI ensures that the resulting predictor constructs regions that contain the truth with a user-specified level of probability. CANVI is agnostic to design decisions in formulating the candidate approximators and only requires access to samples from the forward model, permitting its use in likelihood-free settings. We prove lower bounds on the predictive efficiency of t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24230;&#37327;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#32553;&#23567;&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#20013;&#27491;&#24120;&#21644;&#26080;&#22768;&#35821;&#38899;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#21033;&#29992;&#27491;&#24120;&#21644;&#26080;&#22768;&#35821;&#38899;&#20043;&#38388;&#30340;&#20849;&#20139;&#20869;&#23481;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#21644;&#39044;&#27979;Viseme&#36523;&#20221;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26080;&#22768;VSR&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14203</link><description>&lt;p&gt;
&#22522;&#20110;&#24230;&#37327;&#23398;&#20064;&#25552;&#39640;&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#20013;&#27491;&#24120;&#19982;&#26080;&#22768;&#35821;&#38899;&#20043;&#38388;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Improving the Gap in Visual Speech Recognition Between Normal and Silent Speech Based on Metric Learning. (arXiv:2305.14203v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24230;&#37327;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#32553;&#23567;&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#20013;&#27491;&#24120;&#21644;&#26080;&#22768;&#35821;&#38899;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#21033;&#29992;&#27491;&#24120;&#21644;&#26080;&#22768;&#35821;&#38899;&#20043;&#38388;&#30340;&#20849;&#20139;&#20869;&#23481;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#21644;&#39044;&#27979;Viseme&#36523;&#20221;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26080;&#22768;VSR&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#65288;VSR&#65289;&#20013;&#27491;&#24120;&#19982;&#26080;&#22768;&#35821;&#38899;&#20043;&#38388;&#24615;&#33021;&#24046;&#36317;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;VSR&#27169;&#22411;&#22312;&#22788;&#29702;&#26080;&#22768;&#35821;&#38899;&#26102;&#65292;&#30001;&#20110;&#20004;&#32773;&#20043;&#38388;&#30340;&#22068;&#21767;&#36816;&#21160;&#24046;&#24322;&#65292;&#23548;&#33268;&#20934;&#30830;&#24615;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#35299;&#20915;&#26080;&#22768;&#35821;&#38899;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Viseme&#30340;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#27491;&#24120;&#21644;&#26080;&#22768;&#35821;&#38899;&#20043;&#38388;&#30340;&#20849;&#20139;&#23383;&#38754;&#20869;&#23481;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#23558;&#20004;&#31181;&#35821;&#38899;&#31867;&#22411;&#30340;&#36755;&#20837;&#24444;&#27492;&#38752;&#36817;&#65292;&#22914;&#26524;&#23427;&#20204;&#20855;&#26377;&#30456;&#20284;&#30340;Viseme&#34920;&#31034;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;&#39044;&#27979;&#30340;Viseme&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#21644;&#20004;&#31181;&#35821;&#38899;&#31867;&#22411;&#20869;&#37096;&#30340;Kullback-Leibler&#25955;&#24230;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#21644;&#39044;&#27979;Viseme&#36523;&#20221;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#33021;&#25552;&#39640;&#26080;&#22768;VSR&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel metric learning approach to address the performance gap between normal and silent speech in visual speech recognition (VSR). The difference in lip movements between the two poses a challenge for existing VSR models, which exhibit degraded accuracy when applied to silent speech. To solve this issue and tackle the scarcity of training data for silent speech, we propose to leverage the shared literal content between normal and silent speech and present a metric learning approach based on visemes. Specifically, we aim to map the input of two speech types close to each other in a latent space if they have similar viseme representations. By minimizing the Kullback-Leibler divergence of the predicted viseme probability distributions between and within the two speech types, our model effectively learns and predicts viseme identities. Our evaluation demonstrates that our method improves the accuracy of silent VSR, even when limited training data is available.
&lt;/p&gt;</description></item><item><title>ZeroSCROLLS&#26159;&#19968;&#20010;&#29992;&#20110;&#38271;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#38646;Shot&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#20845;&#20010;&#20219;&#21153;&#21644;&#22235;&#20010;&#25968;&#25454;&#38598;&#65292;&#33021;&#22815;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#24403;&#21069;&#65292;GPT-4&#30340;&#24179;&#22343;&#24471;&#20998;&#26368;&#39640;&#65292;&#20294;&#22312;&#32858;&#21512;&#20219;&#21153;&#31561;&#22810;&#20010;&#25361;&#25112;&#19978;&#65292;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.14196</link><description>&lt;p&gt;
ZeroSCROLLS&#65306;&#19968;&#20010;&#29992;&#20110;&#38271;&#25991;&#26412;&#29702;&#35299;&#30340;&#38646;Shot&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding. (arXiv:2305.14196v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14196
&lt;/p&gt;
&lt;p&gt;
ZeroSCROLLS&#26159;&#19968;&#20010;&#29992;&#20110;&#38271;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#38646;Shot&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#20845;&#20010;&#20219;&#21153;&#21644;&#22235;&#20010;&#25968;&#25454;&#38598;&#65292;&#33021;&#22815;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#24403;&#21069;&#65292;GPT-4&#30340;&#24179;&#22343;&#24471;&#20998;&#26368;&#39640;&#65292;&#20294;&#22312;&#32858;&#21512;&#20219;&#21153;&#31561;&#22810;&#20010;&#25361;&#25112;&#19978;&#65292;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; ZeroSCROLLS&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#38271;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#38646;Shot&#22522;&#20934;&#27979;&#35797;&#65292;&#20165;&#21253;&#21547;&#27979;&#35797;&#38598;&#32780;&#27809;&#26377;&#35757;&#32451;&#25110;&#24320;&#21457;&#25968;&#25454;&#12290;&#25105;&#20204;&#20174;SCROLLS&#22522;&#20934;&#27979;&#35797;&#20013;&#36866;&#24212;&#20102;&#20845;&#20010;&#20219;&#21153;&#65292;&#24182;&#28155;&#21152;&#20102;&#22235;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20004;&#20010;&#26032;&#30340;&#20449;&#24687;&#34701;&#21512;&#20219;&#21153;&#65292;&#20363;&#22914;&#32858;&#21512;&#27491;&#38754;&#35780;&#20215;&#30340;&#30334;&#20998;&#27604;&#12290;&#20351;&#29992;ZeroSCROLLS&#65292;&#25105;&#20204;&#23545;&#24320;&#28304;&#21644;&#38381;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;Claude&#20248;&#20110;ChatGPT&#65292;&#24182;&#19988;GPT-4&#33719;&#24471;&#20102;&#26368;&#39640;&#30340;&#24179;&#22343;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#22312;ZeroSCROLLS&#30340;&#22810;&#20010;&#24320;&#25918;&#25361;&#25112;&#26041;&#38754;&#65288;&#20363;&#22914;&#65292;&#32858;&#21512;&#20219;&#21153;&#65289;&#65292;&#36824;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#65292;&#22240;&#20026;&#27169;&#22411;&#24456;&#38590;&#36890;&#36807;&#26420;&#32032;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#30001;&#20110;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#36824;&#22312;&#19981;&#26029;&#26356;&#26032;&#65292;&#25105;&#20204;&#36992;&#35831;&#30740;&#31350;&#20154;&#21592;&#22312;&#23454;&#26102;&#30340;ZeroSCROLLS&#25490;&#34892;&#27036;&#19978;&#35780;&#20272;&#20182;&#20204;&#30340;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce ZeroSCROLLS, a zero-shot benchmark for natural language understanding over long texts, which contains only test sets, without training or development data. We adapt six tasks from the SCROLLS benchmark, and add four new datasets, including two novel information fusing tasks, such as aggregating the percentage of positive reviews. Using ZeroSCROLLS, we conduct a comprehensive evaluation of both open-source and closed large language models, finding that Claude outperforms ChatGPT, and that GPT-4 achieves the highest average score. However, there is still room for improvement on multiple open challenges in ZeroSCROLLS, such as aggregation tasks, where models struggle to pass the naive baseline. As the state of the art is a moving target, we invite researchers to evaluate their ideas on the live ZeroSCROLLS leaderboard
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24605;&#32500;&#38142;&#26465;&#24494;&#35843;&#65288;CoT fine-tuning&#65289;&#26469;&#25552;&#39640;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;CoT Collection&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#36880;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26410;&#35265;&#20219;&#21153;&#21644;4&#20010;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#19978;&#65292;&#36890;&#36807;CoT fine-tuning&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#30340;&#20934;&#30830;&#24230;&#21644;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.14045</link><description>&lt;p&gt;
CoT Collection: &#36890;&#36807;&#24605;&#32500;&#38142;&#26465;&#24494;&#35843;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning. (arXiv:2305.14045v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24605;&#32500;&#38142;&#26465;&#24494;&#35843;&#65288;CoT fine-tuning&#65289;&#26469;&#25552;&#39640;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;CoT Collection&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#36880;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26410;&#35265;&#20219;&#21153;&#21644;4&#20010;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#19978;&#65292;&#36890;&#36807;CoT fine-tuning&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#30340;&#20934;&#30830;&#24230;&#21644;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#30693;&#23545;&#20110;&#23567;&#20110;100&#20159;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#35299;&#20915;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#26102;&#65292;&#20854;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#33021;&#21147;&#19981;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#24605;&#32500;&#38142;&#26465;&#30340;&#32622;&#20449;&#24230;&#35843;&#25972;&#26469;&#36171;&#20104;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#36880;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#35843;&#25972;&#25351;&#20196;&#25968;&#25454;&#38598;&#8212;&#8212;CoT Collection&#65292;&#35813;&#25968;&#25454;&#38598;&#36890;&#36807;&#22686;&#21152;184&#19975;&#20010;&#32622;&#20449;&#24230;&#27880;&#37322;&#21040;1060&#20010;&#20219;&#21153;&#30340;&#29616;&#26377;Flan Collection&#65288;&#21253;&#21547;9&#20010;&#24605;&#32500;&#38142;&#26465;&#20219;&#21153;&#65289;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#26410;&#35265;&#20219;&#21153;&#19978;&#20351;&#29992;CoT Collection&#23545;Flan-T5&#65288;3B&#21644;11B&#65289;&#36827;&#34892;&#24605;&#32500;&#38142;&#26465;&#24494;&#35843;&#65292;&#20351;&#24471;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#24605;&#32500;&#38142;&#26465;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#22312;BIG-Bench-Hard&#65288;BBH&#65289;&#22522;&#20934;&#27979;&#35797;&#19978;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#38646;&#26679;&#26412;&#20219;&#21153;&#20934;&#30830;&#24230;&#26041;&#38754;&#30340;&#24179;&#22343;&#25552;&#21319;&#65306;+4.34%&#65288;Flan-T5 3B&#65289;&#21644;+2.60%&#65288;Flan-T5 11B&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20351;&#29992;CoT Collection&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#20351;&#35821;&#35328;&#27169;&#22411;&#22312;4&#20010;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#19978;&#20855;&#26377;&#26356;&#24378;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) with less than 100B parameters are known to perform poorly on chain-of-thought (CoT) reasoning in contrast to large LMs when solving unseen tasks. In this work, we aim to equip smaller LMs with the step-by-step reasoning capability by instruction tuning with CoT rationales. In order to achieve this goal, we first introduce a new instruction-tuning dataset called the CoT Collection, which augments the existing Flan Collection (including only 9 CoT tasks) with additional 1.84 million rationales across 1,060 tasks. We show that CoT fine-tuning Flan-T5 (3B &amp; 11B) with CoT Collection enables smaller LMs to have better CoT capabilities on unseen tasks. On the BIG-Bench-Hard (BBH) benchmark, we report an average improvement of +4.34% (Flan-T5 3B) and +2.60% (Flan-T5 11B), in terms of zero-shot task accuracy. Furthermore, we show that instruction tuning with CoT Collection allows LMs to possess stronger few-shot learning capabilities on 4 domain-specific tasks, resulting 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#32676;&#19981;&#21464;GAN&#30340;&#32479;&#35745;&#20445;&#35777;&#65292;&#21457;&#29616;&#24403;&#23398;&#20064;&#32676;&#19981;&#21464;&#20998;&#24067;&#26102;&#65292;&#32676;&#19981;&#21464;GAN&#25152;&#38656;&#26679;&#26412;&#25968;&#20250;&#25353;&#32676;&#20307;&#22823;&#23567;&#30340;&#24130;&#27604;&#20363;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2305.13517</link><description>&lt;p&gt;
Group-Invariant GAN&#30340;&#32479;&#35745;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Statistical Guarantees of Group-Invariant GANs. (arXiv:2305.13517v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#32676;&#19981;&#21464;GAN&#30340;&#32479;&#35745;&#20445;&#35777;&#65292;&#21457;&#29616;&#24403;&#23398;&#20064;&#32676;&#19981;&#21464;&#20998;&#24067;&#26102;&#65292;&#32676;&#19981;&#21464;GAN&#25152;&#38656;&#26679;&#26412;&#25968;&#20250;&#25353;&#32676;&#20307;&#22823;&#23567;&#30340;&#24130;&#27604;&#20363;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Group-Invariant&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GAN)&#26159;&#19968;&#31181;GAN&#65292;&#20854;&#20013;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#20855;&#26377;&#30828;&#24615;&#38598;&#22242;&#23545;&#31216;&#24615;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#32593;&#32476;&#33021;&#22815;&#23398;&#20064;&#20855;&#26377;&#26174;&#30528;&#25913;&#36827;&#25968;&#25454;&#25928;&#29575;&#30340;&#38598;&#22242;&#19981;&#21464;&#20998;&#24067;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#20998;&#26512;&#32676;&#19981;&#21464;GAN&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20943;&#23569;&#26469;&#20005;&#26684;&#37327;&#21270;&#36825;&#31181;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#23398;&#20064;&#32676;&#19981;&#21464;&#20998;&#24067;&#26102;&#65292;&#32676;&#19981;&#21464;GAN&#25152;&#38656;&#26679;&#26412;&#25968;&#25353;&#29031;&#32676;&#20307;&#22823;&#23567;&#30340;&#24130;&#27604;&#20363;&#20943;&#23569;&#65292;&#36825;&#20010;&#24130;&#21462;&#20915;&#20110;&#20998;&#24067;&#25903;&#25345;&#30340;&#26412;&#36136;&#32500;&#24230;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#26159;&#39318;&#20010;&#20026;&#32676;&#19981;&#21464;&#29983;&#25104;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;GAN&#25552;&#20379;&#32479;&#35745;&#20272;&#35745;&#30340;&#24037;&#20316;&#65292;&#24182;&#21487;&#20197;&#20026;&#20854;&#20182;&#32676;&#19981;&#21464;&#29983;&#25104;&#27169;&#22411;&#30340;&#30740;&#31350;&#25552;&#20379;&#20511;&#37492;&#12290;
&lt;/p&gt;
&lt;p&gt;
Group-invariant generative adversarial networks (GANs) are a type of GANs in which the generators and discriminators are hardwired with group symmetries. Empirical studies have shown that these networks are capable of learning group-invariant distributions with significantly improved data efficiency. In this study, we aim to rigorously quantify this improvement by analyzing the reduction in sample complexity for group-invariant GANs. Our findings indicate that when learning group-invariant distributions, the number of samples required for group-invariant GANs decreases proportionally with a power of the group size, and this power depends on the intrinsic dimension of the distribution's support. To our knowledge, this work presents the first statistical estimation for group-invariant generative models, specifically for GANs, and it may shed light on the study of other group-invariant generative models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#19968;&#33324;&#22238;&#24402;&#35823;&#24046;&#20551;&#35774;&#30340;&#26080;&#22122;&#22768;&#22238;&#24402;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#20540;&#30340;&#22343;&#26041;&#35823;&#24046;&#65292;&#24182;&#21457;&#29616;&#21253;&#21547;&#22823;&#37327;&#19981;&#37325;&#35201;&#30340;&#21442;&#25968;&#21487;&#20197;&#26377;&#25928;&#22320;&#38477;&#20302;&#20272;&#35745;&#22120;&#30340;&#22343;&#26041;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.12883</link><description>&lt;p&gt;
&#22522;&#20110;&#19968;&#33324;&#22238;&#24402;&#35823;&#24046;&#20551;&#35774;&#26469;&#30740;&#31350;&#26080;&#22122;&#22768;&#22238;&#24402;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#20540;&#30340;&#22343;&#26041;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
The Mean Squared Error of the Ridgeless Least Squares Estimator under General Assumptions on Regression Errors. (arXiv:2305.12883v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12883
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#19968;&#33324;&#22238;&#24402;&#35823;&#24046;&#20551;&#35774;&#30340;&#26080;&#22122;&#22768;&#22238;&#24402;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#20540;&#30340;&#22343;&#26041;&#35823;&#24046;&#65292;&#24182;&#21457;&#29616;&#21253;&#21547;&#22823;&#37327;&#19981;&#37325;&#35201;&#30340;&#21442;&#25968;&#21487;&#20197;&#26377;&#25928;&#22320;&#38477;&#20302;&#20272;&#35745;&#22120;&#30340;&#22343;&#26041;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26368;&#23567;$\ell_2$&#33539;&#25968;&#65288;&#26080;&#23725;&#65289;&#25554;&#20540;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#30340;&#30740;&#31350;&#26041;&#20852;&#26410;&#33406;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20998;&#26512;&#37117;&#23616;&#38480;&#20110;&#31616;&#21333;&#30340;&#22238;&#24402;&#35823;&#24046;&#32467;&#26500;&#65292;&#20551;&#35774;&#35823;&#24046;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#65292;&#20855;&#26377;&#38646;&#22343;&#20540;&#21644;&#30456;&#21516;&#30340;&#26041;&#24046;&#65292;&#19982;&#29305;&#24449;&#21521;&#37327;&#26080;&#20851;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#29702;&#35770;&#20998;&#26512;&#30340;&#20027;&#35201;&#37325;&#28857;&#26159;&#26679;&#26412;&#22806;&#39044;&#27979;&#39118;&#38505;&#12290;&#26412;&#25991;&#36890;&#36807;&#26816;&#26597;&#26080;&#23725;&#25554;&#20540;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#30340;&#22343;&#26041;&#35823;&#24046;&#65292;&#20801;&#35768;&#26356;&#19968;&#33324;&#30340;&#22238;&#24402;&#35823;&#24046;&#20551;&#35774;&#65292;&#25171;&#30772;&#20102;&#29616;&#26377;&#25991;&#29486;&#30340;&#23616;&#38480;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#36890;&#36807;&#25551;&#32472;&#26377;&#38480;&#26679;&#26412;&#20013;&#30340;&#22343;&#26041;&#35823;&#24046;&#26469;&#34920;&#24449;&#22343;&#26041;&#35823;&#24046;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#26679;&#26412;&#37327;&#65292;&#21253;&#21547;&#22823;&#37327;&#19981;&#37325;&#35201;&#30340;&#21442;&#25968;&#21487;&#20197;&#26377;&#25928;&#22320;&#38477;&#20302;&#20272;&#35745;&#22120;&#30340;&#22343;&#26041;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been a significant growth in research focusing on minimum $\ell_2$ norm (ridgeless) interpolation least squares estimators. However, the majority of these analyses have been limited to a simple regression error structure, assuming independent and identically distributed errors with zero mean and common variance, independent of the feature vectors. Additionally, the main focus of these theoretical analyses has been on the out-of-sample prediction risk. This paper breaks away from the existing literature by examining the mean squared error of the ridgeless interpolation least squares estimator, allowing for more general assumptions about the regression errors. Specifically, we investigate the potential benefits of overparameterization by characterizing the mean squared error in a finite sample. Our findings reveal that including a large number of unimportant parameters relative to the sample size can effectively reduce the mean squared error of the estimator. N
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#25193;&#23637;&#24433;&#21709;&#20989;&#25968;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35782;&#21035;&#21644;&#37325;&#26032;&#26631;&#35760;&#26368;&#23567;&#35757;&#32451;&#23376;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#22987;&#32456;&#33021;&#22815;&#25104;&#21151;&#32763;&#36716;&#27979;&#35797;&#32467;&#26524;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#25361;&#25112;&#27169;&#22411;&#39044;&#27979;&#12289;&#35780;&#20272;&#27169;&#22411;&#40065;&#26834;&#24615;&#21644;&#27934;&#23519;&#35757;&#32451;&#38598;&#20559;&#24046;&#31561;&#22810;&#37325;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.12809</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#26631;&#35760;&#26368;&#23567;&#35757;&#32451;&#23376;&#38598;&#26469;&#32763;&#36716;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Relabel Minimal Training Subset to Flip a Prediction. (arXiv:2305.12809v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#25193;&#23637;&#24433;&#21709;&#20989;&#25968;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35782;&#21035;&#21644;&#37325;&#26032;&#26631;&#35760;&#26368;&#23567;&#35757;&#32451;&#23376;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#22987;&#32456;&#33021;&#22815;&#25104;&#21151;&#32763;&#36716;&#27979;&#35797;&#32467;&#26524;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#25361;&#25112;&#27169;&#22411;&#39044;&#27979;&#12289;&#35780;&#20272;&#27169;&#22411;&#40065;&#26834;&#24615;&#21644;&#27934;&#23519;&#35757;&#32451;&#38598;&#20559;&#24046;&#31561;&#22810;&#37325;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Yang&#31561;&#20154;&#21457;&#29616;&#65292;&#20165;&#21024;&#38500;1%&#30340;&#35757;&#32451;&#25968;&#25454;&#23601;&#21487;&#33021;&#23548;&#33268;&#39044;&#27979;&#32467;&#26524;&#32763;&#36716;&#12290;&#37492;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#23384;&#22312;&#22122;&#22768;&#25968;&#25454;&#30340;&#26222;&#36941;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#22312;&#27169;&#22411;&#35757;&#32451;&#20043;&#21069;&#36890;&#36807;&#37325;&#26032;&#26631;&#35760;&#19968;&#20010;&#23567;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#21487;&#21542;&#23548;&#33268;&#27979;&#35797;&#32467;&#26524;&#32763;&#36716;&#65311;&#26412;&#25991;&#21033;&#29992;&#25193;&#23637;&#24433;&#21709;&#20989;&#25968;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35782;&#21035;&#21644;&#37325;&#26032;&#26631;&#35760;&#36825;&#31181;&#23376;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22987;&#32456;&#33021;&#22815;&#20135;&#29983;&#25104;&#21151;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#26426;&#21046;&#26377;&#22810;&#37325;&#20316;&#29992;&#65306;&#65288;1&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#34917;&#20805;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#24674;&#22797;&#21487;&#33021;&#38169;&#35823;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#25361;&#25112;&#27169;&#22411;&#39044;&#27979;&#65307;&#65288;2&#65289;&#35780;&#20272;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#22240;&#20026;&#26412;&#25991;&#21457;&#29616;&#23376;&#38598;&#30340;&#22823;&#23567;&#19982;&#35757;&#32451;&#38598;&#20013;&#22122;&#22768;&#25968;&#25454;&#30340;&#27604;&#20363;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#20851;&#31995;&#65307;&#65288;3&#65289;&#25552;&#20379;&#20102;&#27934;&#23519;&#35757;&#32451;&#38598;&#20559;&#24046;&#30340;&#35265;&#35299;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#20195;&#34920;&#20102;&#23545;&#35782;&#21035;&#26368;&#23567;&#35757;&#32451;&#23376;&#38598;&#38382;&#39064;&#30340;&#31532;&#19968;&#27425;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Yang et al. (2023) discovered that removing a mere 1% of training points can often lead to the flipping of a prediction. Given the prevalence of noisy data in machine learning models, we pose the question: can we also result in the flipping of a test prediction by relabeling a small subset of the training data before the model is trained? In this paper, utilizing the extended influence function, we propose an efficient procedure for identifying and relabeling such a subset, demonstrating consistent success. This mechanism serves multiple purposes: (1) providing a complementary approach to challenge model predictions by recovering potentially mislabeled training points; (2) evaluating model resilience, as our research uncovers a significant relationship between the subset's size and the ratio of noisy data in the training set; and (3) offering insights into bias within the training set. To the best of our knowledge, this work represents the first investigation into the problem of identi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;AI&#20013;&#24120;&#29992;&#30340;&#24037;&#20855;LIME&#22312;&#25991;&#26412;&#25968;&#25454;&#19978;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;XAIFooler&#26469;&#25200;&#21160;&#25991;&#26412;&#36755;&#20837;&#24182;&#25805;&#32437;&#35299;&#37322;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.12351</link><description>&lt;p&gt;
&#20320;&#30340;&#35299;&#37322;&#21487;&#38752;&#21527;&#65311;&#36890;&#36807;&#34701;&#21512;XAI&#21644;&#23545;&#25239;&#25915;&#20987;&#26469;&#25506;&#31350;LIME&#22312;&#35299;&#37322;&#25991;&#26412;&#20998;&#31867;&#22120;&#20013;&#30340;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Are Your Explanations Reliable? Investigating the Stability of LIME in Explaining Text Classifiers by Marrying XAI and Adversarial Attack. (arXiv:2305.12351v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;AI&#20013;&#24120;&#29992;&#30340;&#24037;&#20855;LIME&#22312;&#25991;&#26412;&#25968;&#25454;&#19978;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;XAIFooler&#26469;&#25200;&#21160;&#25991;&#26412;&#36755;&#20837;&#24182;&#25805;&#32437;&#35299;&#37322;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LIME&#24050;&#32463;&#25104;&#20026;&#21487;&#35299;&#37322;AI&#65288;XAI&#65289;&#26694;&#26550;&#20013;&#26368;&#24120;&#34987;&#24341;&#29992;&#30340;&#24037;&#20855;&#20043;&#19968;&#65292;&#22312;&#20851;&#38190;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#38598;&#25104;&#20854;&#20013;&#65292;&#20363;&#22914;&#21307;&#30103;&#20445;&#20581;&#21644;&#37329;&#34701;&#12290;&#28982;&#32780;&#65292;&#23588;&#20854;&#26159;&#22312;&#25991;&#26412;&#25968;&#25454;&#30340;&#32972;&#26223;&#19979;&#65292;&#20854;&#31283;&#23450;&#24615;&#20173;&#28982;&#40092;&#20026;&#20154;&#30693;&#65292;&#36825;&#26159;&#30001;&#20110;&#25991;&#26412;&#31354;&#38388;&#30340;&#29420;&#29305;&#32422;&#26463;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#39318;&#20808;&#35780;&#20272;&#20102;LIME&#22312;&#25991;&#26412;&#25968;&#25454;&#19978;&#22266;&#26377;&#30340;&#19981;&#31283;&#23450;&#24615;&#65292;&#20197;&#24314;&#31435;&#22522;&#20934;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;XAIFooler&#65292;&#20197;&#25200;&#21160;&#25991;&#26412;&#36755;&#20837;&#24182;&#25805;&#32437;&#35299;&#37322;&#65292;&#23558;LIME&#30340;&#31283;&#23450;&#24615;&#20316;&#20026;&#19968;&#20010;&#25991;&#26412;&#25200;&#21160;&#20248;&#21270;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#12290;XAIFooler&#31526;&#21512;&#32422;&#26463;&#26465;&#20214;&#65292;&#20445;&#30041;&#20102;&#25991;&#26412;&#35821;&#20041;&#21644;&#21407;&#22987;&#39044;&#27979;&#65292;&#24182;&#24341;&#20837;&#20102;Rank-biased Overlap&#65288;RBO&#65289;&#20316;&#20026;XAIFooler&#20248;&#21270;&#30340;&#20851;&#38190;&#37096;&#20998;&#65292;&#20197;&#28385;&#36275;&#25152;&#26377;&#35299;&#37322;&#30456;&#20284;&#24230;&#27979;&#37327;&#30340;&#35201;&#27714;&#12290;&#22312;&#30495;&#23454;&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;XAIFool&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
LIME has emerged as one of the most commonly referenced tools in explainable AI (XAI) frameworks that is integrated into critical machine learning applications--e.g., healthcare and finance. However, its stability remains little explored, especially in the context of text data, due to the unique text-space constraints. To address these challenges, in this paper, we first evaluate the inherent instability of LIME on text data to establish a baseline, and then propose a novel algorithm XAIFooler to perturb text inputs and manipulate explanations that casts investigation on the stability of LIME as a text perturbation optimization problem. XAIFooler conforms to the constraints to preserve text semantics and original prediction with small perturbations, and introduces Rank-biased Overlap (RBO) as a key part to guide the optimization of XAIFooler that satisfies all the requirements for explanation similarity measure. Extensive experiments on real-world text datasets demonstrate that XAIFool
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36923;&#36753;&#22238;&#24402;&#24120;&#25968;&#27493;&#38271;&#26799;&#24230;&#19979;&#38477;&#22312;&#31283;&#23450;&#24615;&#36793;&#32536;&#30340;&#25910;&#25947;&#24615;&#21644;&#38544;&#24335;&#20559;&#24046;&#65292;&#35777;&#26126;&#20102;&#36923;&#36753;&#25439;&#22833;&#21487;&#20197;&#36890;&#36807;&#20219;&#20309;&#24120;&#25968;&#27493;&#38271;&#30340;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#26368;&#23567;&#21270;&#65292;&#21516;&#26102;&#20063;&#21457;&#29616;&#20102;&#25351;&#25968;&#25439;&#22833;&#19979;&#30340;&#21457;&#25955;&#24615;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#31283;&#23450;&#24615;&#36793;&#32536;&#19979;&#26799;&#24230;&#19979;&#38477;&#30340;&#19981;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11788</link><description>&lt;p&gt;
&#31283;&#23450;&#24615;&#36793;&#32536;&#22788;&#30340;&#36923;&#36753;&#22238;&#24402;&#26799;&#24230;&#19979;&#38477;&#30340;&#38544;&#24335;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Implicit Bias of Gradient Descent for Logistic Regression at the Edge of Stability. (arXiv:2305.11788v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36923;&#36753;&#22238;&#24402;&#24120;&#25968;&#27493;&#38271;&#26799;&#24230;&#19979;&#38477;&#22312;&#31283;&#23450;&#24615;&#36793;&#32536;&#30340;&#25910;&#25947;&#24615;&#21644;&#38544;&#24335;&#20559;&#24046;&#65292;&#35777;&#26126;&#20102;&#36923;&#36753;&#25439;&#22833;&#21487;&#20197;&#36890;&#36807;&#20219;&#20309;&#24120;&#25968;&#27493;&#38271;&#30340;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#26368;&#23567;&#21270;&#65292;&#21516;&#26102;&#20063;&#21457;&#29616;&#20102;&#25351;&#25968;&#25439;&#22833;&#19979;&#30340;&#21457;&#25955;&#24615;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#31283;&#23450;&#24615;&#36793;&#32536;&#19979;&#26799;&#24230;&#19979;&#38477;&#30340;&#19981;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#20013;&#65292;&#26799;&#24230;&#19979;&#38477; (GD) &#32463;&#24120;&#22312;&#31283;&#23450;&#24615;&#36793;&#32536; (EoS) [Cohen &#31561;&#65292;2021] &#36816;&#34892;&#65292;&#20854;&#20013;&#27493;&#38271;&#34987;&#35774;&#32622;&#20026;&#22823;&#65292;&#23548;&#33268;&#30001; GD &#36845;&#20195;&#24341;&#36215;&#30340;&#38750;&#21333;&#35843;&#25439;&#22833;&#12290;&#26412;&#25991;&#30740;&#31350;&#22312; EoS &#21306;&#22495;&#20869;&#20351;&#29992;&#24120;&#25968;&#27493;&#38271; GD &#36827;&#34892;&#36923;&#36753;&#22238;&#24402;&#30340;&#25910;&#25947;&#24615;&#21644;&#38544;&#24335;&#20559;&#24046;&#65292;&#23545;&#20110;&#32447;&#24615;&#21487;&#20998;&#30340;&#25968;&#25454;&#12290;&#23613;&#31649;&#23384;&#22312;&#23616;&#37096;&#25391;&#33633;&#65292;&#25105;&#20204;&#35777;&#26126;&#36923;&#36753;&#25439;&#22833;&#21487;&#20197;&#36890;&#36807;&#20219;&#20309;&#24120;&#25968;&#27493;&#38271;&#30340; GD &#22312;&#38271;&#26102;&#38388;&#23610;&#24230;&#19978;&#36827;&#34892;&#26368;&#23567;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#20219;&#20309;&#24120;&#25968;&#27493;&#38271;&#19979;&#65292;&#24403;&#25237;&#24433;&#21040;&#26368;&#22823;&#36793;&#38469;&#26041;&#21521; (&#30828;&#36793; SVM &#26041;&#21521;) &#26102;&#65292;GD &#36845;&#20195;&#36235;&#21521;&#20110;&#26080;&#31351;&#22823;&#65292;&#24182;&#22312;&#25237;&#24433;&#21040;&#26368;&#22823;&#36793;&#32536;&#30340;&#27491;&#20132;&#34917;&#31354;&#38388;&#26102;&#65292;&#25910;&#25947;&#20110;&#26368;&#23567;&#21270;&#24378;&#20984;&#21183;&#33021;&#30340;&#22266;&#23450;&#21521;&#37327;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#20063;&#34920;&#26126;&#65292;&#22312; EoS &#21306;&#22495;&#65292;GD &#36845;&#20195;&#21487;&#33021;&#22312;&#25351;&#25968;&#25439;&#22833;&#19979;&#21457;&#29983;&#28798;&#38590;&#24615;&#21457;&#25955;&#65292;&#31361;&#26174;&#20102; EoS &#21306;&#22495;&#20013; GD &#30340;&#19981;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has observed that in machine learning optimization, gradient descent (GD) often operates at the edge of stability (EoS) [Cohen, et al., 2021], where the stepsizes are set to be large, resulting in non-monotonic losses induced by the GD iterates. This paper studies the convergence and implicit bias of constant-stepsize GD for logistic regression on linearly separable data in the EoS regime. Despite the presence of local oscillations, we prove that the logistic loss can be minimized by GD with any constant stepsize over a long time scale. Furthermore, we prove that with any constant stepsize, the GD iterates tend to infinity when projected to a max-margin direction (the hard-margin SVM direction) and converge to a fixed vector that minimizes a strongly convex potential when projected to the orthogonal complement of the max-margin direction. In contrast, we also show that in the EoS regime, GD iterates may diverge catastrophically under the exponential loss, highlighting t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#35757;&#32451;&#12289;&#25110;&#35757;&#32451;&#19987;&#38376;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#26469;&#35835;&#21462;&#21644;&#29983;&#25104;&#20687;&#25991;&#26412;&#19968;&#26679;&#30340;&#22270;&#20687;&#65292;&#24182;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.11490</link><description>&lt;p&gt;
LLM&#33258;&#36523;&#21487;&#35835;&#21462;&#21644;&#29983;&#25104;CXR&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
LLM Itself Can Read and Generate CXR Images. (arXiv:2305.11490v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11490
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#35757;&#32451;&#12289;&#25110;&#35757;&#32451;&#19987;&#38376;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#26469;&#35835;&#21462;&#21644;&#29983;&#25104;&#20687;&#25991;&#26412;&#19968;&#26679;&#30340;&#22270;&#20687;&#65292;&#24182;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20511;&#21161;&#20110;&#36817;&#26399;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26174;&#33879;&#21457;&#23637;&#65292;&#20154;&#20204;&#27491;&#31215;&#26497;&#23581;&#35797;&#23558;LLMs&#30340;&#23454;&#29992;&#24615;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;&#24050;&#32463;&#26377;&#20154;&#23581;&#35797;&#36830;&#25509;&#35821;&#35328;&#21644;&#35270;&#35273;&#20449;&#24687;&#65292;&#24182;&#19988;&#20063;&#22312;&#19981;&#26029;&#23581;&#35797;&#20026;LLMs&#28155;&#21152;&#35270;&#35273;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23581;&#35797;&#21482;&#20351;&#29992;LLMs&#20316;&#20026;&#22270;&#20687;&#35299;&#30721;&#22120;&#65292;&#27809;&#26377;&#23581;&#35797;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#26469;&#29983;&#25104;&#22270;&#20687;&#12290;&#36890;&#36807;&#37319;&#29992;VQ-GAN&#26694;&#26550;&#65292;&#23558;&#22270;&#20687;&#30340;&#28508;&#22312;&#34920;&#31034;&#35270;&#20026;&#19968;&#31181;&#25991;&#26412;&#26631;&#35760;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#65292;&#20197;&#20687;&#25991;&#26412;&#19968;&#26679;&#35835;&#21462;&#21644;&#29983;&#25104;&#22270;&#20687;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#30340;&#35757;&#32451;&#30446;&#26631;&#25110;&#35757;&#32451;&#19987;&#38376;&#30340;&#32593;&#32476;&#65292;&#21516;&#26102;&#20173;&#20445;&#30041;LLM&#30340;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#27492;&#26694;&#26550;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#22240;&#20026;&#36825;&#26159;&#19968;&#20010;&#22797;&#26434;&#20449;&#24687;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20043;&#38388;&#32763;&#35793;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building on the recent remarkable development of large language models (LLMs), active attempts are being made to extend the utility of LLMs to multimodal tasks. There have been previous efforts to link language and visual information, and attempts to add visual capabilities to LLMs are ongoing as well. However, existing attempts use LLMs only as image decoders and no attempt has been made to generate images in the same line as the natural language. By adopting a VQ-GAN framework in which latent representations of images are treated as a kind of text tokens, we present a novel method to fine-tune a pre-trained LLM to read and generate images like text without any structural changes, extra training objectives, or the need for training an ad-hoc network while still preserving the of the instruction-following capability of the LLM. We apply this framework to chest X-ray (CXR) image and report generation tasks as it is a domain in which translation of complex information between visual and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#25299;&#25169;&#8212;&#8212;&#22522;&#30784;$(k+1)$&#22270;&#65292;&#20854;&#20013;&#33410;&#28857;&#22312;&#26377;&#38480;&#30340;&#36845;&#20195;&#27425;&#25968;&#21518;&#33021;&#36798;&#21040;&#30830;&#20999;&#30340;&#20849;&#35782;&#65292;&#20855;&#26377;&#24555;&#36895;&#20849;&#35782;&#29575;&#21644;&#23567;&#30340;&#26368;&#22823;&#24230;&#25968;&#65292;&#20174;&#32780;&#21487;&#20197;&#29992;&#20110;&#20998;&#25955;&#24335;SGD&#12290;</title><link>http://arxiv.org/abs/2305.11420</link><description>&lt;p&gt;
&#36229;&#36234;&#25351;&#25968;&#22270;&#65306;&#26377;&#38480;&#26102;&#38388;&#25910;&#25947;&#30340;&#36890;&#20449;&#25928;&#29575;&#25299;&#25169;&#29992;&#20110;&#20998;&#25955;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Beyond Exponential Graph: Communication-Efficient Topologies for Decentralized Learning via Finite-time Convergence. (arXiv:2305.11420v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#25299;&#25169;&#8212;&#8212;&#22522;&#30784;$(k+1)$&#22270;&#65292;&#20854;&#20013;&#33410;&#28857;&#22312;&#26377;&#38480;&#30340;&#36845;&#20195;&#27425;&#25968;&#21518;&#33021;&#36798;&#21040;&#30830;&#20999;&#30340;&#20849;&#35782;&#65292;&#20855;&#26377;&#24555;&#36895;&#20849;&#35782;&#29575;&#21644;&#23567;&#30340;&#26368;&#22823;&#24230;&#25968;&#65292;&#20174;&#32780;&#21487;&#20197;&#29992;&#20110;&#20998;&#25955;&#24335;SGD&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#20998;&#25955;&#24335;&#23398;&#20064;&#22312;&#24182;&#34892;&#35745;&#31639;&#21644;&#38544;&#31169;&#20445;&#25252;&#20013;&#30340;&#24212;&#29992;&#12290;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#25351;&#20986;&#65292;&#20855;&#26377;&#26356;&#24555;&#20849;&#35782;&#29575;&#65288;&#21363;&#35889;&#38388;&#38553;&#65289;&#30340;&#24213;&#23618;&#32593;&#32476;&#25299;&#25169;&#21487;&#23548;&#33268;&#20998;&#25955;&#24335;&#23398;&#20064;&#30340;&#26356;&#22909;&#25910;&#25947;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#24555;&#36895;&#20849;&#35782;&#29575;&#30340;&#25299;&#25169;&#65292;&#22914;&#25351;&#25968;&#22270;&#65292;&#36890;&#24120;&#20855;&#26377;&#36739;&#22823;&#30340;&#26368;&#22823;&#24230;&#25968;&#65292;&#36825;&#20250;&#23548;&#33268;&#37325;&#35201;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#23547;&#27714;&#26082;&#20855;&#26377;&#24555;&#36895;&#20849;&#35782;&#29575;&#21448;&#20855;&#26377;&#23567;&#30340;&#26368;&#22823;&#24230;&#25968;&#30340;&#25299;&#25169;&#26159;&#37325;&#35201;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#24555;&#36895;&#20849;&#35782;&#29575;&#21644;&#23567;&#26368;&#22823;&#24230;&#30340;&#26032;&#22411;&#25299;&#25169;&#65292;&#31216;&#20026;&#22522;&#30784;$(k+1)$ &#22270;&#12290;&#19982;&#29616;&#26377;&#30340;&#25299;&#25169;&#19981;&#21516;&#65292;&#22522;&#30784;$(k+1)$ &#22270;&#20351;&#25152;&#26377;&#33410;&#28857;&#22312;&#26377;&#38480;&#30340;&#36845;&#20195;&#27425;&#25968;&#21518;&#37117;&#33021;&#36798;&#21040;&#30830;&#20999;&#30340;&#20849;&#35782;&#65292;&#23545;&#20110;&#20219;&#20309;&#33410;&#28857;&#25968;&#21644;&#26368;&#22823;&#24230;k&#37117;&#36866;&#29992;&#12290;&#24471;&#30410;&#20110;&#36825;&#20010;&#26377;&#21033;&#30340;&#23646;&#24615;&#65292;&#22522;&#30784;$(k+1)$ &#22270;&#36171;&#20104;&#20102;&#20998;&#25955;&#24335;SGD
&lt;/p&gt;
&lt;p&gt;
Decentralized learning has recently been attracting increasing attention for its applications in parallel computation and privacy preservation. Many recent studies stated that the underlying network topology with a faster consensus rate (a.k.a. spectral gap) leads to a better convergence rate and accuracy for decentralized learning. However, a topology with a fast consensus rate, e.g., the exponential graph, generally has a large maximum degree, which incurs significant communication costs. Thus, seeking topologies with both a fast consensus rate and small maximum degree is important. In this study, we propose a novel topology combining both a fast consensus rate and small maximum degree called the Base-$(k + 1)$ Graph. Unlike the existing topologies, the Base-$(k + 1)$ Graph enables all nodes to reach the exact consensus after a finite number of iterations for any number of nodes and maximum degree k. Thanks to this favorable property, the Base-$(k + 1)$ Graph endows Decentralized SGD
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#22343;&#22330;&#25511;&#21046;(MFC)&#21644;&#22343;&#22330;&#21338;&#24328;(MFG)&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#20048;&#35266;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#31639;&#27861;&#65292;&#24182;&#20165;&#23545;&#36716;&#31227;&#21160;&#21147;&#23398;&#20855;&#26377;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20551;&#35774;&#65292;&#26368;&#21518;&#24314;&#31435;&#20102;&#19968;&#20010;&#25351;&#25968;&#32423;&#30340;&#19979;&#30028;&#25903;&#25345;MFC&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2305.11283</link><description>&lt;p&gt;
&#20851;&#20110;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#22343;&#22330;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
On the Statistical Efficiency of Mean Field Reinforcement Learning with General Function Approximation. (arXiv:2305.11283v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#22343;&#22330;&#25511;&#21046;(MFC)&#21644;&#22343;&#22330;&#21338;&#24328;(MFG)&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#20048;&#35266;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#31639;&#27861;&#65292;&#24182;&#20165;&#23545;&#36716;&#31227;&#21160;&#21147;&#23398;&#20855;&#26377;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20551;&#35774;&#65292;&#26368;&#21518;&#24314;&#31435;&#20102;&#19968;&#20010;&#25351;&#25968;&#32423;&#30340;&#19979;&#30028;&#25903;&#25345;MFC&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#22343;&#22330;&#25511;&#21046;&#65288;MFC&#65289;&#21644;&#22343;&#22330;&#21338;&#24328;&#65288;MFG&#65289;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#25928;&#29575;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;Mean-Field Model-Based Eluder Dimension (MBED)&#30340;&#26032;&#27010;&#24565;&#65292;&#21253;&#21547;&#20102;&#19968;&#31995;&#21015;&#20016;&#23500;&#30340;&#22343;&#22330;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20048;&#35266;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#36820;&#22238;&#19968;&#20010;$\epsilon$&#20248;&#30340;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;MFC&#25110;$\epsilon$&#32435;&#20160;&#22343;&#34913;&#31574;&#30053;&#36866;&#29992;&#20110;MFG&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#22810;&#39033;&#24335;&#19982;&#30456;&#20851;&#21442;&#25968;&#26080;&#20851;&#65292;&#19982;&#29366;&#24577;&#12289;&#21160;&#20316;&#21644;&#20195;&#29702;&#25968;&#37327;&#26080;&#20851;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#20165;&#23545;&#36716;&#31227;&#21160;&#21147;&#23398;&#20855;&#26377;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20551;&#35774;&#65292;&#36991;&#20813;&#20102;&#20197;&#21069;&#30340;&#24378;&#32467;&#26500;&#20551;&#35774;&#12290;&#26368;&#21518;&#65292;&#22312;tabular&#35774;&#32622;&#19979;&#65292;&#20551;&#35774;&#26377;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#25351;&#25968;&#32423;&#30340;&#19979;&#30028;&#25903;&#25345;MFC&#35774;&#32622;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26679;&#26412;&#39640;&#25928;&#30340;&#27169;&#22411;&#28040;&#38500;&#31639;&#27861;&#20197;&#36924;&#36817;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the statistical efficiency of Reinforcement Learning in Mean-Field Control (MFC) and Mean-Field Game (MFG) with general function approximation. We introduce a new concept called Mean-Field Model-Based Eluder Dimension (MBED), which subsumes a rich family of Mean-Field RL problems. Additionally, we propose algorithms based on Optimistic Maximal Likelihood Estimation, which can return an $\epsilon$-optimal policy for MFC or an $\epsilon$-Nash Equilibrium policy for MFG, with sample complexity polynomial w.r.t. relevant parameters and independent of the number of states, actions and the number of agents. Notably, our results only require a mild assumption of Lipschitz continuity on transition dynamics and avoid strong structural assumptions in previous work. Finally, in the tabular setting, given the access to a generative model, we establish an exponential lower bound for MFC setting, while providing a novel sample-efficient model elimination algorithm to approxim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#26041;&#27861;&#22312;&#30701;&#26102;&#38388;&#20869;&#26631;&#35760;8000&#20010;&#33145;&#37096;CT&#25195;&#25551;&#20013;&#30340;8&#20010;&#22120;&#23448;&#65292;&#24314;&#31435;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#22810;&#22120;&#23448;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2305.09666</link><description>&lt;p&gt;
&#22312;&#19977;&#21608;&#20869;&#20026;8,000&#20010;&#33145;&#37096;CT&#25195;&#25551;&#26631;&#27880;&#22810;&#22120;&#23448;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Annotating 8,000 Abdominal CT Volumes for Multi-Organ Segmentation in Three Weeks. (arXiv:2305.09666v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#26041;&#27861;&#22312;&#30701;&#26102;&#38388;&#20869;&#26631;&#35760;8000&#20010;&#33145;&#37096;CT&#25195;&#25551;&#20013;&#30340;8&#20010;&#22120;&#23448;&#65292;&#24314;&#31435;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#22810;&#22120;&#23448;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#26631;&#27880;&#65292;&#29305;&#21035;&#26159;&#22120;&#23448;&#20998;&#21106;&#65292;&#26159;&#36153;&#26102;&#36153;&#21147;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#21152;&#36895;&#22120;&#23448;&#20998;&#21106;&#30340;&#26631;&#27880;&#36807;&#31243;&#12290;&#25105;&#20204;&#26631;&#27880;&#20102;8,448&#20010;&#33145;&#37096;CT&#25195;&#25551;&#65292;&#26631;&#35760;&#20102;&#33086;&#33039;&#12289;&#32925;&#33039;&#12289;&#32958;&#33039;&#12289;&#32963;&#12289;&#32966;&#22218;&#12289;&#33008;&#33146;&#12289;&#20027;&#21160;&#33033;&#21644;&#19979;&#33108;&#38745;&#33033;&#12290;&#20256;&#32479;&#30340;&#26631;&#27880;&#26041;&#27861;&#38656;&#35201;&#19968;&#20301;&#32463;&#39564;&#20016;&#23500;&#30340;&#26631;&#27880;&#21592;1600&#21608;&#65292;&#32780;&#25105;&#20204;&#30340;&#26631;&#27880;&#26041;&#27861;&#20165;&#29992;&#20102;&#19977;&#21608;&#12290;
&lt;/p&gt;
&lt;p&gt;
Annotating medical images, particularly for organ segmentation, is laborious and time-consuming. For example, annotating an abdominal organ requires an estimated rate of 30-60 minutes per CT volume based on the expertise of an annotator and the size, visibility, and complexity of the organ. Therefore, publicly available datasets for multi-organ segmentation are often limited in data size and organ diversity. This paper proposes a systematic and efficient method to expedite the annotation process for organ segmentation. We have created the largest multi-organ dataset (by far) with the spleen, liver, kidneys, stomach, gallbladder, pancreas, aorta, and IVC annotated in 8,448 CT volumes, equating to 3.2 million slices. The conventional annotation methods would take an experienced annotator up to 1,600 weeks (or roughly 30.8 years) to complete this task. In contrast, our annotation method has accomplished this task in three weeks (based on an 8-hour workday, five days a week) while maintain
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DPMLBench&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#32508;&#21512;&#34913;&#37327;&#21152;&#24378;DP-SGD&#30340;DPML&#31639;&#27861;&#30340;&#23454;&#29992;&#24615;&#21644;&#38450;&#24481;&#33021;&#21147;&#65292;&#22635;&#34917;&#20102;&#27604;&#36739;DPML&#31639;&#27861;&#25913;&#36827;&#34920;&#29616;&#30340;&#31354;&#30333;&#65292;&#25552;&#39640;&#20102;DPML&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.05900</link><description>&lt;p&gt;
DPMLBench&#65306;&#24046;&#20998;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#30340;&#25972;&#20307;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
DPMLBench: Holistic Evaluation of Differentially Private Machine Learning. (arXiv:2305.05900v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DPMLBench&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#32508;&#21512;&#34913;&#37327;&#21152;&#24378;DP-SGD&#30340;DPML&#31639;&#27861;&#30340;&#23454;&#29992;&#24615;&#21644;&#38450;&#24481;&#33021;&#21147;&#65292;&#22635;&#34917;&#20102;&#27604;&#36739;DPML&#31639;&#27861;&#25913;&#36827;&#34920;&#29616;&#30340;&#31354;&#30333;&#65292;&#25552;&#39640;&#20102;DPML&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#20316;&#20026;&#19968;&#31181;&#20005;&#26684;&#30340;&#25968;&#23398;&#23450;&#20041;&#65292;&#37327;&#21270;&#20102;&#38544;&#31169;&#27844;&#38706;&#65292;&#24050;&#25104;&#20026;&#38544;&#31169;&#20445;&#25252;&#30340;&#19968;&#20010;&#24191;&#20026;&#25509;&#21463;&#30340;&#26631;&#20934;&#12290;&#32467;&#21512;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#24046;&#20998;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#65288;DPML&#65289;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#26368;&#32463;&#20856;&#30340;DPML&#31639;&#27861;&#20043;&#19968;&#65292;DP-SGD&#20250;&#36896;&#25104;&#26174;&#33879;&#30340;&#25928;&#29992;&#25439;&#22833;&#65292;&#36825;&#38459;&#30861;&#20102;DPML&#22312;&#23454;&#36341;&#20013;&#30340;&#37096;&#32626;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#35768;&#22810;&#30740;&#31350;&#26368;&#36817;&#25552;&#20986;&#20102;&#22522;&#20110;DP-SGD&#30340;&#25913;&#36827;&#31639;&#27861;&#65292;&#20294;&#26159;&#36825;&#20123;&#30740;&#31350;&#26159;&#23396;&#31435;&#30340;&#65292;&#26080;&#27861;&#20840;&#38754;&#34913;&#37327;&#31639;&#27861;&#20013;&#25552;&#20986;&#30340;&#25913;&#36827;&#30340;&#34920;&#29616;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#36824;&#32570;&#20047;&#20840;&#38754;&#30740;&#31350;&#26469;&#27604;&#36739;&#36825;&#20123;DPML&#31639;&#27861;&#30340;&#25913;&#36827;&#22312;&#25928;&#29992;&#12289;&#38450;&#24481;&#33021;&#21147;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#23545;&#25913;&#36827;&#30340;DPML&#31639;&#27861;&#36827;&#34892;&#32508;&#21512;&#27979;&#37327;&#65292;&#23545;&#23454;&#29992;&#24615;&#21644;&#38450;&#24481;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#24314;&#35774;&#24615;&#21644;&#20840;&#38754;&#24615;&#30340;&#26694;&#26550;DPMLBench&#26469;&#35780;&#20272;DPML&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#24230;&#37327;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#19981;&#21516;&#30340;&#38544;&#31169;&#39044;&#31639;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19979;&#30340;&#23454;&#29992;&#24615;&#21644;&#38450;&#24481;&#33021;&#21147;&#34920;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;DPMLBench&#26694;&#26550;&#20248;&#20110;&#29616;&#26377;&#26694;&#26550;&#65292;&#21516;&#26102;&#36824;&#26174;&#31034;&#20986;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;DPML&#31639;&#27861;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differential privacy (DP), as a rigorous mathematical definition quantifying privacy leakage, has become a well-accepted standard for privacy protection. Combined with powerful machine learning techniques, differentially private machine learning (DPML) is increasingly important. As the most classic DPML algorithm, DP-SGD incurs a significant loss of utility, which hinders DPML's deployment in practice. Many studies have recently proposed improved algorithms based on DP-SGD to mitigate utility loss. However, these studies are isolated and cannot comprehensively measure the performance of improvements proposed in algorithms. More importantly, there is a lack of comprehensive research to compare improvements in these DPML algorithms across utility, defensive capabilities, and generalizability.  We fill this gap by performing a holistic measurement of improved DPML algorithms on utility and defense capability against membership inference attacks (MIAs) on image classification tasks. We fir
&lt;/p&gt;</description></item><item><title>DeepEIK&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#36827;&#34892;AI&#33647;&#29289;&#21457;&#29616;&#65292;&#36890;&#36807;&#29305;&#24449;&#34701;&#21512;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#25552;&#39640;&#27169;&#22411;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.01523</link><description>&lt;p&gt;
&#29992;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#25512;&#21160;AI&#33647;&#29289;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Empowering AI drug discovery with explicit and implicit knowledge. (arXiv:2305.01523v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01523
&lt;/p&gt;
&lt;p&gt;
DeepEIK&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#36827;&#34892;AI&#33647;&#29289;&#21457;&#29616;&#65292;&#36890;&#36807;&#29305;&#24449;&#34701;&#21512;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#25552;&#39640;&#27169;&#22411;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29420;&#31435;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#26174;&#24335;&#30693;&#35782;&#25110;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#30340;&#38544;&#24335;&#30693;&#35782;&#36827;&#34892;AI&#33647;&#29289;&#21457;&#29616;&#30340;&#30740;&#31350;&#36805;&#36895;&#22686;&#38271;&#12290;&#36825;&#20123;&#26041;&#27861;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;AI&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#29420;&#31435;&#22320;&#25972;&#21512;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#20250;&#38459;&#30861;&#23545;&#20998;&#23376;&#30340;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DeepEIK&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#26469;&#36827;&#34892;AI&#33647;&#29289;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivation: Recently, research on independently utilizing either explicit knowledge from knowledge graphs or implicit knowledge from biomedical literature for AI drug discovery has been growing rapidly. These approaches have greatly improved the prediction accuracy of AI models on multiple downstream tasks. However, integrating explicit and implicit knowledge independently hinders their understanding of molecules. Results: We propose DeepEIK, a unified deep learning framework that incorporates both explicit and implicit knowledge for AI drug discovery. We adopt feature fusion to process the multi-modal inputs, and leverage the attention mechanism to denoise the text information. Experiments show that DeepEIK significantly outperforms state-of-the-art methods on crucial tasks in AI drug discovery including drug-target interaction prediction, drug property prediction and protein-protein interaction prediction. Further studies show that benefiting from explicit and implicit knowledge, our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#37327;&#20026;&#22522;&#30784;&#30340;&#20999;&#29255;Wasserstein&#36317;&#31163;&#65292;&#24182;&#23558;&#20854;&#21442;&#25968;&#21270;&#65292;&#20197;&#20811;&#26381;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#22266;&#23450;&#20808;&#39564;&#20998;&#24067;&#32570;&#20047;&#20449;&#24687;&#21644;&#20248;&#21270;&#26368;&#20339;&#20998;&#24067;&#26114;&#36149;&#19981;&#31283;&#23450;&#30340;&#23616;&#38480;&#12290;</title><link>http://arxiv.org/abs/2304.13586</link><description>&lt;p&gt;
&#33021;&#37327;&#20026;&#22522;&#30784;&#30340;&#20999;&#29255;Wasserstein&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
Energy-Based Sliced Wasserstein Distance. (arXiv:2304.13586v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#37327;&#20026;&#22522;&#30784;&#30340;&#20999;&#29255;Wasserstein&#36317;&#31163;&#65292;&#24182;&#23558;&#20854;&#21442;&#25968;&#21270;&#65292;&#20197;&#20811;&#26381;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#22266;&#23450;&#20808;&#39564;&#20998;&#24067;&#32570;&#20047;&#20449;&#24687;&#21644;&#20248;&#21270;&#26368;&#20339;&#20998;&#24067;&#26114;&#36149;&#19981;&#31283;&#23450;&#30340;&#23616;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20999;&#29255;Wasserstein&#65288;SW&#65289;&#36317;&#31163;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#20004;&#20010;&#27010;&#29575;&#27979;&#24230;&#20043;&#38388;&#30340;&#19968;&#31181;&#32479;&#35745;&#26377;&#25928;&#19988;&#35745;&#31639;&#39640;&#25928;&#30340;&#24230;&#37327;&#12290;SW&#36317;&#31163;&#30340;&#19968;&#20010;&#20851;&#38190;&#37096;&#20998;&#26159;&#20999;&#29255;&#20998;&#24067;&#12290;&#30446;&#21069;&#26377;&#20004;&#31181;&#26041;&#27861;&#26469;&#36873;&#25321;&#36825;&#20010;&#20998;&#24067;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#20351;&#29992;&#22266;&#23450;&#30340;&#20808;&#39564;&#20998;&#24067;&#12290;&#31532;&#20108;&#31181;&#26159;&#20248;&#21270;&#24402;&#23646;&#20110;&#21442;&#25968;&#20998;&#24067;&#26063;&#30340;&#26368;&#20339;&#20998;&#24067;&#65292;&#24182;&#19988;&#21487;&#20197;&#26368;&#22823;&#21270;&#26399;&#26395;&#30340;&#36317;&#31163;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#26377;&#23616;&#38480;&#24615;&#12290;&#22266;&#23450;&#30340;&#20808;&#39564;&#20998;&#24067;&#22312;&#31361;&#20986;&#33021;&#22815;&#21306;&#20998;&#20004;&#20010;&#24120;&#35268;&#27010;&#29575;&#27979;&#24230;&#30340;&#25237;&#24433;&#26041;&#21521;&#26041;&#38754;&#32570;&#20047;&#20449;&#24687;&#12290;&#32780;&#20248;&#21270;&#26368;&#20339;&#20998;&#24067;&#36890;&#24120;&#26159;&#26114;&#36149;&#21644;&#19981;&#31283;&#23450;&#30340;&#12290;&#27492;&#22806;&#65292;&#35774;&#35745;&#20505;&#36873;&#20998;&#24067;&#30340;&#21442;&#25968;&#20998;&#24067;&#26063;&#21487;&#33021;&#20250;&#24456;&#23481;&#26131;&#34987;&#38169;&#35823;&#25351;&#23450;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#20999;&#29255;&#20998;&#24067;&#35774;&#35745;&#20026;&#22522;&#20110;&#33021;&#37327;&#30340;&#20998;&#24067;&#65292;&#24182;&#23558;&#20854;&#21442;&#25968;&#21270;&#65292;&#20174;&#32780;&#20351;&#20854;&#26356;&#21152;&#36890;&#29992;&#32780;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;
The sliced Wasserstein (SW) distance has been widely recognized as a statistically effective and computationally efficient metric between two probability measures. A key component of the SW distance is the slicing distribution. There are two existing approaches for choosing this distribution. The first approach is using a fixed prior distribution. The second approach is optimizing for the best distribution which belongs to a parametric family of distributions and can maximize the expected distance. However, both approaches have their limitations. A fixed prior distribution is non-informative in terms of highlighting projecting directions that can discriminate two general probability measures. Doing optimization for the best distribution is often expensive and unstable. Moreover, designing the parametric family of the candidate distribution could be easily misspecified. To address the issues, we propose to design the slicing distribution as an energy-based distribution that is parameter
&lt;/p&gt;</description></item><item><title>QuMoS&#26159;&#19968;&#20010;&#20445;&#25252; QML &#27169;&#22411;&#23433;&#20840;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32463;&#20856;&#21152;&#23494;&#12289;&#37327;&#23376;&#28151;&#28102;&#21644;&#35825;&#39285;&#26679;&#26412;&#31561;&#22810;&#31181;&#25216;&#26415;&#26469;&#20445;&#25252;&#27169;&#22411;&#20813;&#21463;&#31363;&#21462;&#25915;&#20987;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.11511</link><description>&lt;p&gt;
QuMoS: &#20445;&#25252;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23433;&#20840;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
QuMoS: A Framework for Preserving Security of Quantum Machine Learning Model. (arXiv:2304.11511v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11511
&lt;/p&gt;
&lt;p&gt;
QuMoS&#26159;&#19968;&#20010;&#20445;&#25252; QML &#27169;&#22411;&#23433;&#20840;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32463;&#20856;&#21152;&#23494;&#12289;&#37327;&#23376;&#28151;&#28102;&#21644;&#35825;&#39285;&#26679;&#26412;&#31561;&#22810;&#31181;&#25216;&#26415;&#26469;&#20445;&#25252;&#27169;&#22411;&#20813;&#21463;&#31363;&#21462;&#25915;&#20987;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#24615;&#19968;&#30452;&#26159;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#30001;&#20110;&#27169;&#22411;&#35757;&#32451;&#30340;&#39640;&#25104;&#26412;&#65292;&#22914;&#25910;&#38598;&#30456;&#20851;&#26679;&#26412;&#12289;&#26631;&#35760;&#25968;&#25454;&#21644;&#28040;&#32791;&#35745;&#31639;&#36164;&#28304;&#31561;&#65292;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;&#26159;&#26368;&#22522;&#26412;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#32780;&#22312;&#37327;&#23376;&#35745;&#31639;&#20013;&#65292;&#36825;&#26679;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;&#20063;&#23384;&#22312;&#65292;&#29978;&#33267;&#26356;&#21152;&#20005;&#37325;&#65292;&#22240;&#20026;&#20256;&#32479;&#30340;&#21152;&#23494;&#26041;&#27861;&#24456;&#38590;&#30452;&#25509;&#24212;&#29992;&#20110;&#37327;&#23376;&#35745;&#31639;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#30001;&#20110;&#26377;&#38480;&#30340;&#37327;&#23376;&#35745;&#31639;&#36164;&#28304;&#65292;&#36817;&#26399;&#22521;&#35757; QML &#27169;&#22411;&#30340;&#36135;&#24065;&#25104;&#26412;&#29978;&#33267;&#21487;&#33021;&#27604;&#32463;&#20856;&#27169;&#22411;&#26356;&#39640;&#12290;&#22240;&#27492;&#65292;&#19968;&#23478;&#20844;&#21496;&#24320;&#21457;&#30340;&#32463;&#36807;&#33391;&#22909;&#35843;&#25972;&#30340; QML &#27169;&#22411;&#21487;&#20197;&#34987;&#22996;&#27966;&#32473;&#37327;&#23376;&#20113;&#25552;&#20379;&#21830;&#20316;&#20026;&#26381;&#21153;&#65292;&#20379;&#26222;&#36890;&#29992;&#25143;&#20351;&#29992;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22914;&#26524;&#20113;&#25552;&#20379;&#21830;&#21463;&#21040;&#25915;&#20987;&#65292;QML &#27169;&#22411;&#23558;&#27844;&#28431;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363; QuMoS&#65292;&#29992;&#20110;&#20445;&#25252; QML &#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#12290;QuMoS &#21253;&#25324;&#19968;&#31995;&#21015;&#25216;&#26415;&#65292;&#21253;&#25324;&#32463;&#20856;&#21152;&#23494;&#12289;&#37327;&#23376;&#28151;&#28102;&#21644;&#35825;&#39285;&#26679;&#26412;&#65292;&#20197;&#38450;&#27490;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20351;&#29992; PennyLane &#36719;&#20214;&#24211;&#21644; Google Cirq &#21253;&#30340;&#20855;&#20307; QuMoS &#23454;&#29616;&#12290;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#26377;&#25928;&#22320;&#38450;&#27490; QML &#27169;&#22411;&#34987;&#30423;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Security has always been a critical issue in machine learning (ML) applications. Due to the high cost of model training -- such as collecting relevant samples, labeling data, and consuming computing power -model-stealing attack is one of the most fundamental but vitally important issues. When it comes to quantum computing, such a quantum machine learning (QML) model-stealing attack also exists and it is even more severe because the traditional encryption method can hardly be directly applied to quantum computation. On the other hand, due to the limited quantum computing resources, the monetary cost of training QML model can be even higher than classical ones in the near term. Therefore, a well-tuned QML model developed by a company can be delegated to a quantum cloud provider as a service to be used by ordinary users. In this case, the QML model will be leaked if the cloud provider is under attack. To address such a problem, we propose a novel framework, namely QuMoS, to preserve mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#20171;&#32461;&#20102;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#21450;&#20854;&#22312;&#39118;&#33021;&#34892;&#19994;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#20174;&#20135;&#19994;&#35282;&#24230;&#35782;&#21035;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#38656;&#27714;&#21644;&#25361;&#25112;&#65292;&#26368;&#32456;&#25552;&#20379;&#25968;&#23383;&#23402;&#29983;&#21644;&#20854;&#22312;&#39118;&#33021;&#24212;&#29992;&#39046;&#22495;&#26410;&#26469;&#30740;&#31350;&#21644;&#21457;&#23637;&#30340;&#36335;&#32447;&#22270;&#12290;</title><link>http://arxiv.org/abs/2304.11405</link><description>&lt;p&gt;
&#39118;&#33021;&#20013;&#30340;&#25968;&#23383;&#23402;&#29983;&#65306;&#26032;&#20852;&#25216;&#26415;&#21644;&#34892;&#19994;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital Twins in Wind Energy: Emerging Technologies and Industry-Informed Future Directions. (arXiv:2304.11405v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#20171;&#32461;&#20102;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#21450;&#20854;&#22312;&#39118;&#33021;&#34892;&#19994;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#20174;&#20135;&#19994;&#35282;&#24230;&#35782;&#21035;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#38656;&#27714;&#21644;&#25361;&#25112;&#65292;&#26368;&#32456;&#25552;&#20379;&#25968;&#23383;&#23402;&#29983;&#21644;&#20854;&#22312;&#39118;&#33021;&#24212;&#29992;&#39046;&#22495;&#26410;&#26469;&#30740;&#31350;&#21644;&#21457;&#23637;&#30340;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#20171;&#32461;&#20102;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#21450;&#20854;&#33021;&#21147;&#27700;&#24179;&#65292;&#37325;&#28857;&#20851;&#27880;&#20854;&#22312;&#39118;&#33021;&#34892;&#19994;&#20013;&#30340;&#24212;&#29992;&#12290;&#23427;&#22312;0-5&#30340;&#26631;&#24230;&#19978;&#30028;&#23450;&#20102;&#25968;&#23383;&#23402;&#29983;&#21450;&#20854;&#33021;&#21147;&#27700;&#24179;&#30340;&#23450;&#20041;&#65307;0-&#29420;&#31435;&#12289;1-&#25551;&#36848;&#12289;2-&#35786;&#26029;&#12289;3-&#39044;&#27979;&#12289;4-&#35268;&#23450;&#12289;5-&#33258;&#27835;&#12290;&#20174;&#20135;&#19994;&#35282;&#24230;&#65292;&#23427;&#30830;&#23450;&#20102;&#39118;&#33021;&#34892;&#19994;&#30340;&#29616;&#29366;&#21644;&#30740;&#31350;&#38656;&#27714;&#12290;&#26412;&#25991;&#20174;&#30740;&#31350;&#26426;&#26500;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#22871;&#25512;&#33616;&#25514;&#26045;&#65292;&#20197;&#20419;&#36827;&#25216;&#26415;&#30340;&#25509;&#21463;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#22312;&#20110;&#23427;&#32508;&#21512;&#20102;&#24403;&#21069;&#30693;&#35782;&#29366;&#24577;&#24182;&#20174;&#20135;&#19994;&#35282;&#24230;&#35782;&#21035;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#38656;&#27714;&#21644;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#25968;&#23383;&#23402;&#29983;&#21644;&#20854;&#22312;&#39118;&#33021;&#24212;&#29992;&#39046;&#22495;&#26410;&#26469;&#30740;&#31350;&#21644;&#21457;&#23637;&#30340;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents a comprehensive overview of the digital twin technology and its capability levels, with a specific focus on its applications in the wind energy industry. It consolidates the definitions of digital twin and its capability levels on a scale from 0-5; 0-standalone, 1-descriptive, 2-diagnostic, 3-predictive, 4-prescriptive, 5-autonomous. It then, from an industrial perspective, identifies the current state of the art and research needs in the wind energy sector. The article proposes approaches to the identified challenges from the perspective of research institutes and offers a set of recommendations for diverse stakeholders to facilitate the acceptance of the technology. The contribution of this article lies in its synthesis of the current state of knowledge and its identification of future research needs and challenges from an industry perspective, ultimately providing a roadmap for future research and development in the field of digital twin and its applications in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#36924;&#36817;&#29983;&#25104;&#31163;&#25955;&#28508;&#21464;&#37327;&#30340;&#21442;&#25968;&#30340;&#26799;&#24230;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#19968;&#20123;&#25968;&#20540;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20108;&#38454;&#31934;&#24230;&#65292;&#21462;&#24471;&#20102;&#23454;&#39564;&#19978;&#30340;&#25345;&#32493;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2304.08612</link><description>&lt;p&gt;
&#31163;&#25955;&#19982;&#21453;&#21521;&#20256;&#25773;&#30340;&#26725;&#26753;&#65306;&#30452;&#36890;&#27861;&#19982;&#20854;&#23427;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bridging Discrete and Backpropagation: Straight-Through and Beyond. (arXiv:2304.08612v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#36924;&#36817;&#29983;&#25104;&#31163;&#25955;&#28508;&#21464;&#37327;&#30340;&#21442;&#25968;&#30340;&#26799;&#24230;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#19968;&#20123;&#25968;&#20540;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20108;&#38454;&#31934;&#24230;&#65292;&#21462;&#24471;&#20102;&#23454;&#39564;&#19978;&#30340;&#25345;&#32493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#20256;&#25773;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#22522;&#30707;&#65292;&#20294;&#20854;&#20165;&#38480;&#20110;&#35745;&#31639;&#36830;&#32493;&#21464;&#37327;&#30340;&#26799;&#24230;&#65292;&#38480;&#21046;&#20102;&#28041;&#21450;&#31163;&#25955;&#28508;&#21464;&#37327;&#30340;&#38382;&#39064;&#30340;&#30740;&#31350;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#36817;&#20284;&#29983;&#25104;&#31163;&#25955;&#28508;&#21464;&#37327;&#30340;&#21442;&#25968;&#30340;&#26799;&#24230;&#12290;&#25105;&#20204;&#39318;&#20808;&#32771;&#23519;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340; Straight-Through&#65288;ST&#65289;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#20316;&#20026;&#26799;&#24230;&#30340;&#19968;&#38454;&#36817;&#20284;&#20540;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026; ReinMax&#65292;&#23427;&#38598;&#25104;&#20102; Heun's Method&#65292;&#19968;&#31181;&#35299;ODE&#30340;&#20108;&#38454;&#25968;&#20540;&#26041;&#27861;&#65292;&#20197;&#36817;&#20284;&#26799;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#20108;&#38454;&#31934;&#24230;&#65292;&#32780;&#19981;&#38656;&#35201; Hessian &#25110;&#20854;&#20182;&#20108;&#38454;&#23548;&#25968;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#32467;&#26500;&#21270;&#36755;&#20986;&#39044;&#27979;&#21644;&#26080;&#30417;&#30563;&#29983;&#25104;&#24314;&#27169;&#20219;&#21153;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;\ours &#22312;&#29616;&#26377;&#25216;&#26415;&#20013;&#24102;&#26469;&#20102;&#25345;&#32493;&#30340;&#25913;&#36827;&#65292;&#21253;&#25324; ST &#21644; Straight-Through Gum&#12290;
&lt;/p&gt;
&lt;p&gt;
Backpropagation, the cornerstone of deep learning, is limited to computing gradients solely for continuous variables. This limitation hinders various research on problems involving discrete latent variables. To address this issue, we propose a novel approach for approximating the gradient of parameters involved in generating discrete latent variables. First, we examine the widely used Straight-Through (ST) heuristic and demonstrate that it works as a first-order approximation of the gradient. Guided by our findings, we propose a novel method called ReinMax, which integrates Heun's Method, a second-order numerical method for solving ODEs, to approximate the gradient. Our method achieves second-order accuracy without requiring Hessian or other second-order derivatives. We conduct experiments on structured output prediction and unsupervised generative modeling tasks. Our results show that \ours brings consistent improvements over the state of the art, including ST and Straight-Through Gum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#35757;&#32451;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#27169;&#22411;&#30340;&#21453;&#21521;&#26041;&#27861;&#65292;&#21033;&#29992;&#30456;&#23545;&#20934;&#30830;&#24615;&#30340;&#31574;&#30053;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;Polytuplet Loss&#20989;&#25968;&#26469;&#30830;&#20445;&#20248;&#20808;&#23398;&#20064;&#31572;&#26696;&#36873;&#25321;&#30340;&#30456;&#23545;&#27491;&#30830;&#24615;&#65292;&#33719;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#26524;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#19968;&#33324;&#24615;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2304.01046</link><description>&lt;p&gt;
&#8220;Polytuplet Loss: &#35757;&#32451;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#27169;&#22411;&#30340;&#21453;&#21521;&#26041;&#27861;&#8221;
&lt;/p&gt;
&lt;p&gt;
Polytuplet Loss: A Reverse Approach to Training Reading Comprehension and Logical Reasoning Models. (arXiv:2304.01046v1 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#35757;&#32451;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#27169;&#22411;&#30340;&#21453;&#21521;&#26041;&#27861;&#65292;&#21033;&#29992;&#30456;&#23545;&#20934;&#30830;&#24615;&#30340;&#31574;&#30053;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;Polytuplet Loss&#20989;&#25968;&#26469;&#30830;&#20445;&#20248;&#20808;&#23398;&#20064;&#31572;&#26696;&#36873;&#25321;&#30340;&#30456;&#23545;&#27491;&#30830;&#24615;&#65292;&#33719;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#26524;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#19968;&#33324;&#24615;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25972;&#20010;&#23398;&#26657;&#25945;&#32946;&#36807;&#31243;&#20013;&#65292;&#23398;&#29983;&#20204;&#23558;&#21463;&#21040;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#30340;&#32771;&#39564;&#12290;&#23398;&#29983;&#20204;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#31574;&#30053;&#26469;&#23436;&#25104;&#27492;&#31867;&#32771;&#35797;&#65292;&#20854;&#20013;&#26377;&#20123;&#34987;&#35748;&#20026;&#26159;&#36890;&#24120;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#31574;&#30053;&#30340;&#12290;&#36825;&#26679;&#19968;&#31181;&#31574;&#30053;&#28041;&#21450;&#24378;&#35843;&#30456;&#23545;&#20934;&#30830;&#24615;&#32780;&#38750;&#32477;&#23545;&#20934;&#30830;&#24615;&#65292;&#29702;&#35770;&#19978;&#21487;&#20197;&#22312;&#19981;&#23436;&#20840;&#25484;&#25569;&#35299;&#39064;&#25152;&#38656;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#24471;&#20986;&#27491;&#30830;&#31572;&#26696;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#36825;&#31181;&#31574;&#30053;&#26469;&#35757;&#32451;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#20197;&#35299;&#20915;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;ReClor&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#36923;&#36753;&#25512;&#29702;&#25216;&#33021;&#65292;&#20294;&#25105;&#20204;&#19987;&#27880;&#20110;&#19968;&#31181;&#36890;&#29992;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Polytuplet Loss&#20989;&#25968;&#65292;&#26159;&#19977;&#20803;&#32452;&#25439;&#22833;&#20989;&#25968;&#30340;&#25193;&#23637;&#65292;&#20197;&#30830;&#20445;&#20248;&#20808;&#23398;&#20064;&#31572;&#26696;&#36873;&#25321;&#30340;&#30456;&#23545;&#27491;&#30830;&#24615;&#32780;&#38750;&#23398;&#20064;&#32477;&#23545;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Throughout schooling, students are tested on reading comprehension and logical reasoning. Students have developed various strategies for completing such exams, some of which are generally thought to outperform others. One such strategy involves emphasizing relative accuracy over absolute accuracy and can theoretically produce the correct answer without full knowledge of the information required to solve the question. This paper examines the effectiveness of applying such a strategy to train transfer learning models to solve reading comprehension and logical reasoning questions. The models were evaluated on the ReClor dataset, a challenging reading comprehension and logical reasoning benchmark. While previous studies targeted logical reasoning skills, we focus on a general training method and model architecture. We propose the polytuplet loss function, an extension of the triplet loss function, to ensure prioritization of learning the relative correctness of answer choices over learning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;TPMCF&#65292;&#21033;&#29992;&#22810;&#28304;&#29305;&#24449;&#36827;&#34892;QoS&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#24102;&#26377;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#24182;&#20351;&#29992;&#21327;&#20316;&#29305;&#24449;&#25429;&#25417;&#29992;&#25143;&#21644;&#26381;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#25968;&#25454;&#31232;&#30095;&#21644;&#24322;&#24120;&#20540;&#12290;</title><link>http://arxiv.org/abs/2303.18201</link><description>&lt;p&gt;
TPMCF: &#20351;&#29992;&#22810;&#28304;&#21327;&#21516;&#29305;&#24449;&#36827;&#34892;&#26102;&#38388;QoS&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TPMCF: Temporal QoS Prediction using Multi-Source Collaborative Features. (arXiv:2303.18201v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;TPMCF&#65292;&#21033;&#29992;&#22810;&#28304;&#29305;&#24449;&#36827;&#34892;QoS&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#24102;&#26377;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#24182;&#20351;&#29992;&#21327;&#20316;&#29305;&#24449;&#25429;&#25417;&#29992;&#25143;&#21644;&#26381;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#25968;&#25454;&#31232;&#30095;&#21644;&#24322;&#24120;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38543;&#30528;&#26381;&#21153;API&#30340;&#24555;&#36895;&#37096;&#32626;&#65292;&#20010;&#24615;&#21270;&#30340;&#26381;&#21153;&#25512;&#33616;&#22312;&#30005;&#23376;&#21830;&#21153;&#34892;&#19994;&#30340;&#22686;&#38271;&#20013;&#21457;&#25381;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20915;&#23450;&#26381;&#21153;&#24615;&#33021;&#30340;&#26381;&#21153;&#36136;&#37327;(QoS)&#21442;&#25968;&#32463;&#24120;&#34987;&#29992;&#20110;&#25512;&#33616;&#65292;&#20294;&#38543;&#26102;&#38388;&#27874;&#21160;&#12290;&#22240;&#27492;&#65292;QoS&#30340;&#39044;&#27979;&#23545;&#20110;&#22312;&#31561;&#20215;&#26381;&#21153;&#20013;&#35782;&#21035;&#21512;&#36866;&#30340;&#26381;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#20195;&#30340;&#26102;&#38388;QoS&#39044;&#27979;&#26041;&#27861;&#30001;&#20110;&#21508;&#31181;&#38480;&#21046;&#32780;&#24456;&#38590;&#36798;&#21040;&#26399;&#26395;&#30340;&#31934;&#24230;&#65292;&#20363;&#22914;&#26080;&#27861;&#22788;&#29702;&#25968;&#25454;&#31232;&#30095;&#21644;&#24322;&#24120;&#20540;&#20197;&#21450;&#25429;&#33719;&#29992;&#25143;-&#26381;&#21153;&#20132;&#20114;&#20043;&#38388;&#30340;&#39640;&#38454;&#26102;&#38388;&#20851;&#31995;&#12290;&#34429;&#28982;&#26368;&#36817;&#19968;&#20123;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#20307;&#31995;&#32467;&#26500;&#21487;&#20197;&#24314;&#27169;QoS&#25968;&#25454;&#20043;&#38388;&#30340;&#26102;&#38388;&#20851;&#31995;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#20854;&#20182;&#29305;&#24449;&#65288;&#20363;&#22914;&#21327;&#20316;&#29305;&#24449;&#65289;&#26469;&#29702;&#35299;&#29992;&#25143;-&#26381;&#21153;&#20132;&#20114;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#39044;&#27979;&#31934;&#24230;&#20250;&#38477;&#20302;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;TPMCF&#65292;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;TPMCF&#21033;&#29992;&#22810;&#28304;&#29305;&#24449;&#65288;&#21253;&#25324;&#26102;&#38388;&#12289;&#29992;&#25143;&#21644;&#26381;&#21153;&#29305;&#24449;&#65289;&#36827;&#34892;QoS&#39044;&#27979;&#12290;&#20855;&#20307;&#22320;&#65292;&#23427;&#20351;&#29992;&#19968;&#20010;&#24102;&#26377;&#27880;&#24847;&#26426;&#21046;&#30340;&#26032;&#39062;&#32534;&#30721;&#22120;&#35299;&#30721;&#22120;&#26550;&#26500;&#26469;&#21033;&#29992;&#29992;&#25143;-&#26381;&#21153;&#20132;&#20114;&#20043;&#38388;&#30340;&#39640;&#38454;&#26102;&#38388;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#23427;&#20351;&#29992;&#21327;&#20316;&#29305;&#24449;&#26469;&#25429;&#25417;&#29992;&#25143;&#21644;&#26381;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#22788;&#29702;&#25968;&#25454;&#31232;&#30095;&#21644;&#24322;&#24120;&#20540;&#12290;&#23545;&#23454;&#38469;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;TPMCF&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, with the rapid deployment of service APIs, personalized service recommendations have played a paramount role in the growth of the e-commerce industry. Quality-of-Service (QoS) parameters determining the service performance, often used for recommendation, fluctuate over time. Thus, the QoS prediction is essential to identify a suitable service among functionally equivalent services over time. The contemporary temporal QoS prediction methods hardly achieved the desired accuracy due to various limitations, such as the inability to handle data sparsity and outliers and capture higher-order temporal relationships among user-service interactions. Even though some recent recurrent neural-network-based architectures can model temporal relationships among QoS data, prediction accuracy degrades due to the absence of other features (e.g., collaborative features) to comprehend the relationship among the user-service interactions. This paper addresses the above challenges and proposes a s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#23637;&#20102;PCA-Net&#30340;&#36817;&#20284;&#29702;&#35770;&#65292;&#24471;&#20986;&#20102;&#36890;&#29992;&#36924;&#36817;&#32467;&#26524;&#65292;&#24182;&#35782;&#21035;&#20986;&#20102;&#20351;&#29992;PCA-Net&#36827;&#34892;&#39640;&#25928;&#25805;&#20316;&#23398;&#20064;&#30340;&#28508;&#22312;&#38556;&#30861;&#65306;&#36755;&#20986;&#20998;&#24067;&#30340;&#22797;&#26434;&#24615;&#21644;&#31639;&#23376;&#31354;&#38388;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16317</link><description>&lt;p&gt;
PCA-Net&#65306;&#25805;&#20316;&#23398;&#20064;&#30340;&#22797;&#26434;&#24615;&#19978;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
Operator learning with PCA-Net: upper and lower complexity bounds. (arXiv:2303.16317v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#23637;&#20102;PCA-Net&#30340;&#36817;&#20284;&#29702;&#35770;&#65292;&#24471;&#20986;&#20102;&#36890;&#29992;&#36924;&#36817;&#32467;&#26524;&#65292;&#24182;&#35782;&#21035;&#20986;&#20102;&#20351;&#29992;PCA-Net&#36827;&#34892;&#39640;&#25928;&#25805;&#20316;&#23398;&#20064;&#30340;&#28508;&#22312;&#38556;&#30861;&#65306;&#36755;&#20986;&#20998;&#24067;&#30340;&#22797;&#26434;&#24615;&#21644;&#31639;&#23376;&#31354;&#38388;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31639;&#23376;&#22312;&#35745;&#31639;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#22791;&#21463;&#20851;&#27880;&#12290;PCA-Net&#26159;&#19968;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#31070;&#32463;&#31639;&#23376;&#26550;&#26500;&#65292;&#23427;&#23558;&#20027;&#25104;&#20998;&#20998;&#26512;(PCA)&#19982;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#20197;&#36924;&#36817;&#28508;&#22312;&#30340;&#31639;&#23376;&#12290;&#26412;&#25991;&#23545;&#36825;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#36817;&#20284;&#29702;&#35770;&#30340;&#21457;&#23637;&#65292;&#25913;&#36827;&#24182;&#26174;&#30528;&#25193;&#23637;&#20102;&#27492;&#26041;&#21521;&#30340;&#20197;&#21069;&#30340;&#24037;&#20316;&#12290;&#22312;&#23450;&#24615;&#30028;&#38480;&#26041;&#38754;&#65292;&#26412;&#25991;&#24471;&#20986;&#20102;&#26032;&#39062;&#30340;&#36890;&#29992;&#36924;&#36817;&#32467;&#26524;&#65292;&#22312;&#23545;&#28508;&#22312;&#31639;&#23376;&#21644;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#30340;&#26368;&#23567;&#20551;&#35774;&#30340;&#21069;&#25552;&#19979;&#12290;&#22312;&#23450;&#37327;&#38480;&#21046;&#26041;&#38754;&#65292;&#26412;&#25991;&#35782;&#21035;&#20102;&#20351;&#29992;PCA-Net&#36827;&#34892;&#39640;&#25928;&#25805;&#20316;&#23398;&#20064;&#30340;&#20004;&#20010;&#28508;&#22312;&#38556;&#30861;&#65292;&#36890;&#36807;&#23548;&#20986;&#19979;&#30028;&#36827;&#34892;&#20102;&#20005;&#26684;&#35777;&#26126;&#65292;&#31532;&#19968;&#20010;&#38556;&#30861;&#19982;&#36755;&#20986;&#20998;&#24067;&#30340;&#22797;&#26434;&#24615;&#26377;&#20851;&#65292;&#30001;PCA&#29305;&#24449;&#20540;&#30340;&#32531;&#24930;&#34928;&#20943;&#26469;&#34913;&#37327;&#65307;&#21478;&#19968;&#20010;&#38556;&#30861;&#28041;&#21450;&#26080;&#38480;&#32500;&#36755;&#20837;&#21644;&#36755;&#20986;&#31354;&#38388;&#20043;&#38388;&#30340;&#31639;&#23376;&#31354;&#38388;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural operators are gaining attention in computational science and engineering. PCA-Net is a recently proposed neural operator architecture which combines principal component analysis (PCA) with neural networks to approximate an underlying operator. The present work develops approximation theory for this approach, improving and significantly extending previous work in this direction. In terms of qualitative bounds, this paper derives a novel universal approximation result, under minimal assumptions on the underlying operator and the data-generating distribution. In terms of quantitative bounds, two potential obstacles to efficient operator learning with PCA-Net are identified, and made rigorous through the derivation of lower complexity bounds; the first relates to the complexity of the output distribution, measured by a slow decay of the PCA eigenvalues. The other obstacle relates the inherent complexity of the space of operators between infinite-dimensional input and output spaces, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27969;&#24335;&#32437;&#21521;&#25968;&#25454;&#35774;&#32622;&#20013;&#30340;&#21160;&#24577;&#23450;&#20215;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#22522;&#20110;&#20840;&#23616;&#25910;&#32553;&#32467;&#26500;&#21644;PSGD&#26041;&#27861;&#65292;&#24182;&#26126;&#30830;&#22320;&#23558;&#36951;&#25022;&#20316;&#20026;&#26102;&#38388;&#12289;&#27169;&#22411;&#21442;&#25968;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.15652</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#21160;&#24577;&#23450;&#20215;&#65306;&#20840;&#23616;&#25910;&#32553;&#27169;&#22411;&#19979;&#30340;&#26368;&#20248;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Structured Dynamic Pricing: Optimal Regret in a Global Shrinkage Model. (arXiv:2303.15652v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27969;&#24335;&#32437;&#21521;&#25968;&#25454;&#35774;&#32622;&#20013;&#30340;&#21160;&#24577;&#23450;&#20215;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#22522;&#20110;&#20840;&#23616;&#25910;&#32553;&#32467;&#26500;&#21644;PSGD&#26041;&#27861;&#65292;&#24182;&#26126;&#30830;&#22320;&#23558;&#36951;&#25022;&#20316;&#20026;&#26102;&#38388;&#12289;&#27169;&#22411;&#21442;&#25968;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#27969;&#24335;&#32437;&#21521;&#25968;&#25454;&#35774;&#32622;&#20013;&#30340;&#21160;&#24577;&#23450;&#20215;&#31574;&#30053;&#65292;&#30446;&#30340;&#26159;&#26368;&#22823;&#21270;&#22312;&#22823;&#37327;&#23458;&#25143;&#32454;&#20998;&#20013;&#30340;&#32047;&#35745;&#21033;&#28070;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#31181;&#21160;&#24577;&#27010;&#29575;&#27169;&#22411;&#65292;&#20854;&#20013;&#28040;&#36153;&#32773;&#30340;&#20559;&#22909;&#21644;&#20215;&#26684;&#25935;&#24863;&#24230;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#22522;&#20110;&#36825;&#19968;&#20247;&#25152;&#21608;&#30693;&#30340;&#21457;&#29616;&#65292;&#20855;&#26377;&#30456;&#20284;&#29305;&#24449;&#30340;&#28040;&#36153;&#32773;&#20250;&#34920;&#29616;&#20986;&#30456;&#20284;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#32771;&#34385;&#19968;&#31181;&#20840;&#23616;&#25910;&#32553;&#32467;&#26500;&#65292;&#35813;&#32467;&#26500;&#20551;&#23450;&#19981;&#21516;&#32454;&#20998;&#20013;&#30340;&#28040;&#36153;&#32773;&#20559;&#22909;&#21487;&#20197;&#29992;&#31354;&#38388;&#33258;&#22238;&#24402;&#27169;&#22411;&#24456;&#22909;&#22320;&#36817;&#20284;&#12290;&#22312;&#36825;&#26679;&#30340;&#27969;&#24335;&#32437;&#21521;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#36951;&#25022;&#26469;&#34913;&#37327;&#21160;&#24577;&#23450;&#20215;&#31574;&#30053;&#30340;&#24615;&#33021;&#65292;&#36951;&#25022;&#26159;&#19982;&#39044;&#20808;&#30693;&#36947;&#27169;&#22411;&#21442;&#25968;&#24207;&#21015;&#30340;&#21315;&#37324;&#30524;&#30456;&#27604;&#39044;&#26399;&#30340;&#25910;&#20837;&#25439;&#22833;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24809;&#32602;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;PSGD&#65289;&#30340;&#23450;&#20215;&#31574;&#30053;&#65292;&#24182;&#26126;&#30830;&#22320;&#23558;&#20854;&#36951;&#25022;&#20316;&#20026;&#26102;&#38388;&#20989;&#25968;&#12289;&#27169;&#22411;&#21442;&#25968;&#30340;&#26102;&#38388;&#21464;&#21270;&#24615;&#21644;&#28040;&#36153;&#32773;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider dynamic pricing strategies in a streamed longitudinal data set-up where the objective is to maximize, over time, the cumulative profit across a large number of customer segments. We consider a dynamic probit model with the consumers' preferences as well as price sensitivity varying over time. Building on the well-known finding that consumers sharing similar characteristics act in similar ways, we consider a global shrinkage structure, which assumes that the consumers' preferences across the different segments can be well approximated by a spatial autoregressive (SAR) model. In such a streamed longitudinal set-up, we measure the performance of a dynamic pricing policy via regret, which is the expected revenue loss compared to a clairvoyant that knows the sequence of model parameters in advance. We propose a pricing policy based on penalized stochastic gradient descent (PSGD) and explicitly characterize its regret as functions of time, the temporal variability in the model pa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;MAE&#25216;&#26415;&#39044;&#21069;&#32622;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20159;&#32423;&#39044;&#35757;&#32451;&#35268;&#27169;&#65292;&#24182;&#21487;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#25910;&#25947;&#24615;&#21644;&#19979;&#28216;&#36716;&#31227;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.13496</link><description>&lt;p&gt;
MAE&#39044;&#21069;&#32622;&#35757;&#32451;&#23545;&#20110;&#20159;&#32423;&#39044;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
The effectiveness of MAE pre-pretraining for billion-scale pretraining. (arXiv:2303.13496v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;MAE&#25216;&#26415;&#39044;&#21069;&#32622;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20159;&#32423;&#39044;&#35757;&#32451;&#35268;&#27169;&#65292;&#24182;&#21487;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#25910;&#25947;&#24615;&#21644;&#19979;&#28216;&#36716;&#31227;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#29992;&#20110;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#30340;&#26631;&#20934;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;&#27169;&#22411;&#20351;&#29992;&#25968;&#21313;&#20159;&#24352;&#22270;&#20687;&#30340;&#22823;&#35268;&#27169;&#65288;&#24369;&#65289;&#30417;&#30563;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#39044;&#21069;&#32622;&#35757;&#32451;&#38454;&#27573;&#65292;&#23427;&#20351;&#29992;&#20102;&#33258;&#25105;&#30417;&#30563;&#30340;MAE&#25216;&#26415;&#26469;&#21021;&#22987;&#21270;&#27169;&#22411;&#12290;&#34429;&#28982;MAE&#25216;&#26415;&#20165;&#34987;&#35777;&#26126;&#33021;&#22815;&#19982;&#27169;&#22411;&#22823;&#23567;&#30456;&#32553;&#25918;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23427;&#20063;&#21487;&#20197;&#38543;&#25968;&#25454;&#38598;&#22823;&#23567;&#32553;&#25918;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22522;&#20110;MAE&#30340;&#39044;&#21069;&#32622;&#35757;&#32451;&#21487;&#21516;&#26102;&#36866;&#29992;&#20110;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#35268;&#27169;&#12290;&#39044;&#21069;&#32622;&#35757;&#32451;&#22312;&#19968;&#31995;&#21015;&#27169;&#22411;&#35268;&#27169;&#65288;&#21442;&#25968;&#25968;&#30334;&#19975;&#21040;&#25968;&#21313;&#20159;&#65289;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#65288;&#22270;&#20687;&#25968;&#30334;&#19975;&#21040;&#25968;&#21313;&#20159;&#65289;&#19978;&#19968;&#33268;&#25552;&#39640;&#20102;&#27169;&#22411;&#25910;&#25947;&#24615;&#21644;&#19979;&#28216;&#36716;&#31227;&#24615;&#33021;&#65292;&#19988;&#25105;&#20204;&#36824;&#27979;&#37327;&#20102;&#20854;&#22312;10&#20010;&#19981;&#21516;&#30340;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#12289;&#35270;&#39057;&#35782;&#21035;&#21644;&#30446;&#26631;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper revisits the standard pretrain-then-finetune paradigm used in computer vision for visual recognition tasks. Typically, state-of-the-art foundation models are pretrained using large scale (weakly) supervised datasets with billions of images. We introduce an additional pre-pretraining stage that is simple and uses the self-supervised MAE technique to initialize the model. While MAE has only been shown to scale with the size of models, we find that it scales with the size of the training dataset as well. Thus, our MAE-based pre-pretraining scales with both model and data size making it applicable for training foundation models. Pre-pretraining consistently improves both the model convergence and the downstream transfer performance across a range of model scales (millions to billions of parameters), and dataset sizes (millions to billions of images). We measure the effectiveness of pre-pretraining on 10 different visual recognition tasks spanning image classification, video reco
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23450;&#20041;&#20102;&#19968;&#31181;MTC&#20998;&#31867;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;CFG&#27169;&#22411;&#30340;&#25277;&#21462;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;ICL&#33258;&#21160;&#25552;&#21462;&#21644;&#26631;&#20934;&#21270;DUGs&#20013;&#30340;MTC&#65292;&#26377;&#26395;&#36890;&#36807;&#23450;&#20041;&#23433;&#20840;&#30340;&#24739;&#32773;&#27963;&#21160;&#27169;&#24335;&#26469;&#25512;&#36827;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#21307;&#30103;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.09366</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#21307;&#23398;&#26102;&#38388;&#32422;&#26463;&#25277;&#21462;&#33539;&#22260;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Scope of In-Context Learning for the Extraction of Medical Temporal Constraints. (arXiv:2303.09366v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23450;&#20041;&#20102;&#19968;&#31181;MTC&#20998;&#31867;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;CFG&#27169;&#22411;&#30340;&#25277;&#21462;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;ICL&#33258;&#21160;&#25552;&#21462;&#21644;&#26631;&#20934;&#21270;DUGs&#20013;&#30340;MTC&#65292;&#26377;&#26395;&#36890;&#36807;&#23450;&#20041;&#23433;&#20840;&#30340;&#24739;&#32773;&#27963;&#21160;&#27169;&#24335;&#26469;&#25512;&#36827;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#21307;&#30103;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#27835;&#30103;&#36890;&#24120;&#23545;&#24739;&#32773;&#30340;&#26085;&#24120;&#27963;&#21160;&#26045;&#21152;&#26102;&#38388;&#32422;&#26463;&#12290;&#36829;&#21453;&#21307;&#23398;&#26102;&#38388;&#32422;&#26463;&#65288;MTC&#65289;&#20250;&#23548;&#33268;&#32570;&#20047;&#27835;&#30103;&#20381;&#20174;&#24615;&#65292;&#20197;&#21450;&#19981;&#33391;&#30340;&#20581;&#24247;&#32467;&#26524;&#21644;&#22686;&#21152;&#30340;&#21307;&#30103;&#36153;&#29992;&#12290;&#36825;&#20123;MTC&#22312;&#24739;&#32773;&#25945;&#32946;&#26448;&#26009;&#21644;&#20020;&#24202;&#25991;&#26412;&#20013;&#30340;&#33647;&#29289;&#20351;&#29992;&#25351;&#21335;&#65288;DUGs&#65289;&#20013;&#34987;&#21457;&#29616;&#12290;&#36890;&#36807;&#22312;&#35745;&#31639;&#19978;&#34920;&#31034;DUGs&#20013;&#30340;MTC&#65292;&#23558;&#26377;&#21161;&#20110;&#36890;&#36807;&#24110;&#21161;&#23450;&#20041;&#23433;&#20840;&#30340;&#24739;&#32773;&#27963;&#21160;&#27169;&#24335;&#26469;&#25512;&#36827;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#21307;&#30103;&#24212;&#29992;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;DUGs&#20013;&#21457;&#29616;&#30340;MTC&#20998;&#31867;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;&#30340;&#27169;&#22411;&#26469;&#35745;&#31639;&#22320;&#34920;&#31034;MTC&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19977;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20849;&#35745;N = 836&#20010;&#24102;&#26631;&#20934;&#21270;&#30340;MTC&#26631;&#35760;&#30340;DUGs&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#33258;&#21160;&#25552;&#21462;&#21644;&#26631;&#20934;&#21270;DUGs&#20013;&#21457;&#29616;&#30340;MTC&#65292;&#36328;&#25152;&#26377;&#25968;&#25454;&#38598;&#23454;&#29616;&#20102;&#24179;&#22343;F1&#24471;&#20998;0.62&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;ICL&#27169;&#22411;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medications often impose temporal constraints on everyday patient activity. Violations of such medical temporal constraints (MTCs) lead to a lack of treatment adherence, in addition to poor health outcomes and increased healthcare expenses. These MTCs are found in drug usage guidelines (DUGs) in both patient education materials and clinical texts. Computationally representing MTCs in DUGs will advance patient-centric healthcare applications by helping to define safe patient activity patterns. We define a novel taxonomy of MTCs found in DUGs and develop a novel context-free grammar (CFG) based model to computationally represent MTCs from unstructured DUGs. Additionally, we release three new datasets with a combined total of N = 836 DUGs labeled with normalized MTCs. We develop an in-context learning (ICL) solution for automatically extracting and normalizing MTCs found in DUGs, achieving an average F1 score of 0.62 across all datasets. Finally, we rigorously investigate ICL model perfor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#23454;&#38469;&#19978;&#36827;&#34892;&#35299;&#26512;&#20197;&#21450;&#20026;&#20160;&#20040;&#33021;&#25429;&#25417;&#35299;&#26512;&#32467;&#26500;&#65292;&#35777;&#26126;&#20102;&#31867;&#20284;&#20110;BERT&#25110;RoBERTa&#36825;&#26679;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36817;&#20284;&#25191;&#34892;&#33521;&#35821;PCFG&#30340;Inside-Outside&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.08117</link><description>&lt;p&gt;
&#36716;&#25442;&#22120;&#22312;&#39044;&#27979;&#25513;&#30721;&#21333;&#35789;&#26102;&#26159;&#21542;&#35299;&#26512;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Transformers Parse while Predicting the Masked Word?. (arXiv:2303.08117v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#23454;&#38469;&#19978;&#36827;&#34892;&#35299;&#26512;&#20197;&#21450;&#20026;&#20160;&#20040;&#33021;&#25429;&#25417;&#35299;&#26512;&#32467;&#26500;&#65292;&#35777;&#26126;&#20102;&#31867;&#20284;&#20110;BERT&#25110;RoBERTa&#36825;&#26679;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36817;&#20284;&#25191;&#34892;&#33521;&#35821;PCFG&#30340;Inside-Outside&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#20351;&#29992;&#31867;&#20284;&#20110;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#36825;&#26679;&#30340;&#26080;&#30417;&#30563;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#21487;&#20197;&#23545;&#35821;&#35328;&#32467;&#26500;&#36827;&#34892;&#32534;&#30721;&#65292;&#20363;&#22914;&#20381;&#36182;&#20851;&#31995;&#21644;&#32452;&#25104;&#25104;&#20998;&#20998;&#26512;&#26641;&#12290;&#20294;&#26159;&#20154;&#20204;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#23454;&#38469;&#19978;&#36827;&#34892;&#35299;&#26512;&#25110;&#20165;&#36827;&#34892;&#19982;&#35299;&#26512;&#24369;&#30456;&#20851;&#30340;&#19968;&#20123;&#35745;&#31639;&#23384;&#22312;&#30097;&#38382;&#12290;&#26412;&#25991;&#22312;&#29983;&#25104;&#24314;&#27169;&#30340;&#19978;&#19979;&#25991;&#20013;&#19968;&#27493;&#27493;&#22238;&#31572;&#20102;&#19978;&#36848;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;(a)&#26159;&#21542;&#26377;&#21487;&#33021;&#26126;&#30830;&#25551;&#36848;&#20855;&#26377;&#29616;&#23454;&#23884;&#20837;&#32500;&#24230;&#65292;&#22836;&#25968;&#31561;&#30340;&#36716;&#25442;&#22120;&#65292;&#33021;&#22815;&#36827;&#34892;&#35299;&#26512;&#29978;&#33267;&#36817;&#20284;&#35299;&#26512;&#65307;(b)&#39044;&#35757;&#32451;&#27169;&#22411;&#20026;&#20160;&#20040;&#33021;&#22815;&#25429;&#25417;&#35299;&#26512;&#32467;&#26500;&#65311;&#25105;&#20204;&#23637;&#31034;&#20102;&#31867;&#20284;&#20110;BERT&#25110;RoBERTa&#36825;&#26679;&#30340;&#20013;&#31561;&#22823;&#23567;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36817;&#20284;&#25191;&#34892;&#33521;&#35821;PCFG&#65288;Marcus&#31561;&#65292;1993&#65289;&#30340;Inside-Outside&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#65292;&#22312;PCFG&#29983;&#25104;&#35821;&#35328;&#24314;&#27169;&#25439;&#22833;&#19978;&#65292;Inside-Outside&#31639;&#27861;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models have been shown to encode linguistic structures, e.g. dependency and constituency parse trees, in their embeddings while being trained on unsupervised loss functions like masked language modeling. Some doubts have been raised whether the models actually are doing parsing or only some computation weakly correlated with it. We study questions: (a) Is it possible to explicitly describe transformers with realistic embedding dimension, number of heads, etc. that are capable of doing parsing -- or even approximate parsing? (b) Why do pre-trained models capture parsing structure? This paper takes a step toward answering these questions in the context of generative modeling with PCFGs. We show that masked language models like BERT or RoBERTa of moderate sizes can approximately execute the Inside-Outside algorithm for the English PCFG [Marcus et al, 1993]. We also show that the Inside-Outside algorithm is optimal for masked language modeling loss on the PCFG-generate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35782;&#21035;&#32473;&#23450;&#26102;&#38388;&#24207;&#21015;&#21644;&#38750;&#24490;&#29615;&#25688;&#35201;&#22240;&#26524;&#22270;&#20013;&#38598;&#20307;&#24322;&#24120;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#24182;&#36890;&#36807;&#23558;&#38382;&#39064;&#21010;&#20998;&#20026;&#29420;&#31435;&#23376;&#38382;&#39064;&#21644;&#27604;&#36739;&#30452;&#25509;&#25928;&#24212;&#31561;&#26041;&#24335;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;</title><link>http://arxiv.org/abs/2303.04038</link><description>&lt;p&gt;
&#22312;&#32473;&#23450;&#20855;&#26377;&#24490;&#29615;&#30340;&#38750;&#24490;&#29615;&#25688;&#35201;&#22240;&#26524;&#22270;&#30340;&#26102;&#38388;&#24207;&#21015;&#20013;&#65292;&#29992;&#20110;&#38598;&#20307;&#24322;&#24120;&#30340;&#26681;&#26412;&#21407;&#22240;&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Root Cause Identification for Collective Anomalies in Time Series given an Acyclic Summary Causal Graph with Loops. (arXiv:2303.04038v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35782;&#21035;&#32473;&#23450;&#26102;&#38388;&#24207;&#21015;&#21644;&#38750;&#24490;&#29615;&#25688;&#35201;&#22240;&#26524;&#22270;&#20013;&#38598;&#20307;&#24322;&#24120;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#24182;&#36890;&#36807;&#23558;&#38382;&#39064;&#21010;&#20998;&#20026;&#29420;&#31435;&#23376;&#38382;&#39064;&#21644;&#27604;&#36739;&#30452;&#25509;&#25928;&#24212;&#31561;&#26041;&#24335;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#32473;&#23450;&#35266;&#27979;&#26102;&#38388;&#24207;&#21015;&#21644;&#19968;&#20010;&#25277;&#35937;&#34920;&#31034;&#27491;&#24120;&#29366;&#24577;&#19979;&#21160;&#24577;&#31995;&#32479;&#20013;&#22240;&#26524;&#20851;&#31995;&#30340;&#38750;&#24490;&#29615;&#25688;&#35201;&#22240;&#26524;&#22270;&#30340;&#38598;&#20307;&#24322;&#24120;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#26412;&#25991;&#39318;&#20808;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;d-&#20998;&#31163;&#23558;&#30456;&#20851;&#24322;&#24120;&#20998;&#32452;&#65292;&#23558;&#26681;&#26412;&#21407;&#22240;&#35782;&#21035;&#38382;&#39064;&#20998;&#20026;&#22810;&#20010;&#29420;&#31435;&#23376;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#23637;&#31034;&#20102;&#22312;&#27492;&#35774;&#32622;&#19979;&#22914;&#20309;&#30452;&#25509;&#20174;&#22270;&#20013;&#21644;&#24322;&#24120;&#20986;&#29616;&#26102;&#38388;&#20013;&#25214;&#21040;&#19968;&#20123;&#26681;&#26412;&#21407;&#22240;&#12290;&#26368;&#21518;&#65292;&#23427;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#27604;&#36739;&#27491;&#24120;&#21644;&#24322;&#24120;&#29366;&#24577;&#19979;&#30340;&#30452;&#25509;&#24433;&#21709;&#26469;&#25214;&#21040;&#20854;&#20313;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#20026;&#27492;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#35782;&#21035;&#30452;&#25509;&#24433;&#21709;&#30340;&#35843;&#25972;&#38598;&#12290;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an approach for identifying the root causes of collective anomalies given observational time series and an acyclic summary causal graph which depicts an abstraction of causal relations present in a dynamic system at its normal regime. The paper first shows how the problem of root cause identification can be divided into many independent subproblems by grouping related anomalies using d-separation. Further, it shows how, under this setting, some root causes can be found directly from the graph and from the time of appearance of anomalies. Finally, it shows, how the rest of the root causes can be found by comparing direct effects in the normal and in the anomalous regime. To this end, an adjustment set for identifying direct effects is introduced. Extensive experiments conducted on both simulated and real-world datasets demonstrate the effectiveness of the proposed method.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#29615;&#22659;&#36716;&#25442;&#22120;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#24207;&#21015;&#24314;&#27169;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#35757;&#32451;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#22686;&#21152;&#30340;&#38382;&#39064;&#65292;&#24182;&#20943;&#23569;&#29615;&#22659;&#21160;&#21147;&#23398;&#27169;&#22411;&#32047;&#31215;&#35823;&#24046;&#30340;&#24178;&#25200;&#12290;</title><link>http://arxiv.org/abs/2303.03811</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#29615;&#22659;&#36716;&#25442;&#22120;&#21644;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Environment Transformer and Policy Optimization for Model-Based Offline Reinforcement Learning. (arXiv:2303.03811v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03811
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#29615;&#22659;&#36716;&#25442;&#22120;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#24207;&#21015;&#24314;&#27169;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#35757;&#32451;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#22686;&#21152;&#30340;&#38382;&#39064;&#65292;&#24182;&#20943;&#23569;&#29615;&#22659;&#21160;&#21147;&#23398;&#27169;&#22411;&#32047;&#31215;&#35823;&#24046;&#30340;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#65292;&#19982;&#23454;&#38469;&#29615;&#22659;&#20132;&#20114;&#20197;&#33719;&#21462;&#25968;&#25454;&#36890;&#24120;&#26159;&#26114;&#36149;&#19988;&#32791;&#26102;&#30340;&#12290;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#19968;&#26041;&#38754;&#65292;&#23427;&#28040;&#38500;&#20102;&#19982;&#23454;&#38469;&#29615;&#22659;&#30340;&#20132;&#20114;&#35201;&#27714;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#20174;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#36716;&#25442;&#21160;&#21147;&#23398;&#21644;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#29983;&#25104;&#27169;&#25311;&#30340;&#22238;&#21512;&#20197;&#21152;&#36895;&#35757;&#32451;&#12290;&#20197;&#21069;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#37319;&#29992;&#27010;&#29575;&#38598;&#21512;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#26469;&#24314;&#27169;aleatoric&#19981;&#30830;&#23450;&#24615;&#21644;epistemic&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#23548;&#33268;&#20102;&#35757;&#32451;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#30340;&#25351;&#25968;&#22686;&#21152;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#27169;&#25311;&#38271;&#26399;&#22238;&#21512;&#26102;&#23481;&#26131;&#21463;&#21040;&#29615;&#22659;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#32047;&#31215;&#35823;&#24046;&#30340;&#24178;&#25200;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#29615;&#22659;&#36716;&#25442;&#22120;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#24207;&#21015;&#24314;&#27169;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interacting with the actual environment to acquire data is often costly and time-consuming in robotic tasks. Model-based offline reinforcement learning (RL) provides a feasible solution. On the one hand, it eliminates the requirements of interaction with the actual environment. On the other hand, it learns the transition dynamics and reward function from the offline datasets and generates simulated rollouts to accelerate training. Previous model-based offline RL methods adopt probabilistic ensemble neural networks (NN) to model aleatoric uncertainty and epistemic uncertainty. However, this results in an exponential increase in training time and computing resource requirements. Furthermore, these methods are easily disturbed by the accumulative errors of the environment dynamics models when simulating long-term rollouts. To solve the above problems, we propose an uncertainty-aware sequence modeling architecture called Environment Transformer. It models the probability distribution of th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#21160;&#21147;&#31995;&#32479;&#20998;&#26512;&#26041;&#27861;&#65292;&#20351;&#29992;&#36741;&#21161;&#20989;&#25968;&#20316;&#20026;Koopman&#21487;&#35266;&#27979;&#37327;&#65292;&#19981;&#38656;&#35201;&#26126;&#30830;&#30340;&#27169;&#22411;&#21457;&#29616;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#21160;&#21147;&#23398;&#65292;&#20855;&#26377;&#25910;&#25947;&#24615;&#21644;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2303.01483</link><description>&lt;p&gt;
&#36741;&#21161;&#20989;&#25968;&#20316;&#20026;Koopman&#21487;&#35266;&#27979;&#37327;&#65306;&#22522;&#20110;&#25968;&#25454;&#30340;&#21160;&#21147;&#31995;&#32479;&#22810;&#39033;&#24335;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Auxiliary Functions as Koopman Observables: Data-Driven Polynomial Optimization for Dynamical Systems. (arXiv:2303.01483v2 [math.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01483
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#21160;&#21147;&#31995;&#32479;&#20998;&#26512;&#26041;&#27861;&#65292;&#20351;&#29992;&#36741;&#21161;&#20989;&#25968;&#20316;&#20026;Koopman&#21487;&#35266;&#27979;&#37327;&#65292;&#19981;&#38656;&#35201;&#26126;&#30830;&#30340;&#27169;&#22411;&#21457;&#29616;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#21160;&#21147;&#23398;&#65292;&#20855;&#26377;&#25910;&#25947;&#24615;&#21644;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#22522;&#20110;&#25968;&#25454;&#30340;&#21160;&#21147;&#31995;&#32479;&#20998;&#26512;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#26126;&#30830;&#30340;&#27169;&#22411;&#21457;&#29616;&#12290;&#35813;&#26041;&#27861;&#28304;&#20110;&#20174;&#25968;&#25454;&#20013;&#36924;&#36817;Koopman&#31639;&#23376;&#30340;&#25216;&#26415;&#65292;&#24182;&#19988;&#20197;&#19968;&#20010;&#21487;&#20197;&#36890;&#36807;&#25968;&#20540;&#27714;&#35299;&#30340;&#21322;&#23450;&#35268;&#21010;&#38382;&#39064;&#26469;&#23454;&#29616;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#19981;&#20851;&#24515;&#25968;&#25454;&#26159;&#36890;&#36807;&#30830;&#23450;&#24615;&#36824;&#26159;&#38543;&#26426;&#36807;&#31243;&#29983;&#25104;&#30340;&#65292;&#22240;&#27492;&#29992;&#25143;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#35843;&#25972;&#21363;&#21487;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;&#20005;&#26684;&#30340;&#25910;&#25947;&#24615;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#23558;&#25991;&#29486;&#20013;&#31867;&#20284;&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#25193;&#23637;&#21644;&#32479;&#19968;&#12290;&#36890;&#36807;&#23545;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#21160;&#21147;&#23398;&#30340;&#21560;&#24341;&#23376;&#19978;&#21457;&#29616;Lyapunov&#20989;&#25968;&#12289;&#25191;&#34892;&#36941;&#21382;&#20248;&#21270;&#20197;&#21450;&#30028;&#23450;&#26497;&#20540;&#30340;&#31034;&#20363;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a flexible data-driven method for dynamical system analysis that does not require explicit model discovery. The method is rooted in well-established techniques for approximating the Koopman operator from data and is implemented as a semidefinite program that can be solved numerically. Furthermore, the method is agnostic of whether data is generated through a deterministic or stochastic process, so its implementation requires no prior adjustments by the user to accommodate these different scenarios. Rigorous convergence results justify the applicability of the method, while also extending and uniting similar results from across the literature. Examples on discovering Lyapunov functions, performing ergodic optimization, and bounding extrema over attractors for both deterministic and stochastic dynamics exemplify these convergence results and demonstrate the performance of the method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#27169;&#24577;&#24341;&#23548;&#26041;&#27861;&#65292;&#22312;&#38271;&#35270;&#39057;&#20013;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328; grounding &#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#21644;&#20462;&#21098;&#19981;&#21487;&#25551;&#36848;&#30340;&#31383;&#21475;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.13372</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#27169;&#24577;&#24341;&#23548;&#22312;&#38271;&#35270;&#39057;&#20013;&#23450;&#20301;&#26102;&#21051;
&lt;/p&gt;
&lt;p&gt;
Localizing Moments in Long Video Via Multimodal Guidance. (arXiv:2302.13372v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#27169;&#24577;&#24341;&#23548;&#26041;&#27861;&#65292;&#22312;&#38271;&#35270;&#39057;&#20013;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328; grounding &#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#21644;&#20462;&#21098;&#19981;&#21487;&#25551;&#36848;&#30340;&#31383;&#21475;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25512;&#20986;&#20102;&#22823;&#35268;&#27169;&#12289;&#38271;&#26684;&#24335;&#30340;MAD&#21644;Ego4D&#25968;&#25454;&#38598;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#30740;&#31350;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35270;&#39057; grounding &#26041;&#27861;&#22312;&#38271;&#26684;&#24335;&#35774;&#32622;&#20013;&#30340;&#24615;&#33021;&#65292;&#26377;&#36259;&#30340;&#21457;&#29616;&#26159;&#65306;&#24403;&#21069;&#30340; grounding &#26041;&#27861;&#21333;&#29420;&#26080;&#27861;&#22788;&#29702;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#21644;&#35774;&#32622;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#22788;&#29702;&#38271;&#35270;&#39057;&#24207;&#21015;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#38271;&#35270;&#39057;&#33258;&#28982;&#35821;&#35328; grounding &#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#21644;&#20462;&#21098;&#19981;&#21487;&#25551;&#36848;&#30340;&#31383;&#21475;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#24341;&#23548; grounding &#26694;&#26550;&#65292;&#21253;&#25324;&#19968;&#20010;&#24341;&#23548;&#27169;&#22411;&#21644;&#19968;&#20010;&#22522;&#30784; grounding &#27169;&#22411;&#12290;&#24341;&#23548;&#27169;&#22411;&#24378;&#35843;&#21487;&#25551;&#36848;&#30340;&#31383;&#21475;&#65292;&#32780;&#22522;&#30784; grounding &#27169;&#22411;&#20998;&#26512;&#30701;&#26102;&#31383;&#21475;&#65292;&#30830;&#23450;&#21738;&#20123;&#29255;&#27573;&#19982;&#32473;&#23450;&#30340;&#35821;&#35328;&#26597;&#35810;&#20934;&#30830;&#21305;&#37197;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;&#24341;&#23548;&#27169;&#22411;&#30340;&#35774;&#35745;&#65306;Query-Agnostic &#21644; Query-Dependent&#65292;&#20197;&#24179;&#34913;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent introduction of the large-scale, long-form MAD and Ego4D datasets has enabled researchers to investigate the performance of current state-of-the-art methods for video grounding in the long-form setup, with interesting findings: current grounding methods alone fail at tackling this challenging task and setup due to their inability to process long video sequences. In this paper, we propose a method for improving the performance of natural language grounding in long videos by identifying and pruning out non-describable windows. We design a guided grounding framework consisting of a Guidance Model and a base grounding model. The Guidance Model emphasizes describable windows, while the base grounding model analyzes short temporal windows to determine which segments accurately match a given language query. We offer two designs for the Guidance Model: Query-Agnostic and Query-Dependent, which balance efficiency and accuracy. Experiments demonstrate that our proposed method outperfo
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#20107;&#23454;&#22330;&#26223;&#27979;&#35797;&#26694;&#26550;&#65292;&#36890;&#36807;&#27604;&#36739;&#25968;&#25454;&#38598;&#20013;&#31867;&#20284;&#30340;&#20445;&#25252;&#21644;&#38750;&#20445;&#25252;&#23454;&#20363;&#26469;&#26816;&#27979;&#20998;&#31867;&#22120;&#20013;&#30340;&#27495;&#35270;&#65292;&#36890;&#36807;&#27604;&#36739;&#32452;&#38388;&#20915;&#31574;&#32467;&#26524;&#24046;&#24322;&#65292;&#26469;&#21457;&#29616;&#20010;&#20154;&#27495;&#35270;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#26356;&#22909;&#22320;&#23545;&#12300;&#32473;&#23450;&#24046;&#24322;&#30340;&#20844;&#24179;&#21407;&#21017;&#12301;&#36827;&#34892;&#25805;&#20316;&#65292;&#20197;&#25581;&#31034;&#22312;&#20844;&#24179;&#21407;&#21017;&#19979;&#30340;&#27495;&#35270;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2302.11944</link><description>&lt;p&gt;
&#27979;&#35797;&#21453;&#20107;&#23454;&#22330;&#26223;&#65306;&#25581;&#31034;&#22312;&#20844;&#24179;&#21407;&#21017;&#19979;&#30340;&#27495;&#35270;&#24046;&#24322; (arXiv:2302.11944v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Counterfactual Situation Testing: Uncovering Discrimination under Fairness given the Difference. (arXiv:2302.11944v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11944
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#20107;&#23454;&#22330;&#26223;&#27979;&#35797;&#26694;&#26550;&#65292;&#36890;&#36807;&#27604;&#36739;&#25968;&#25454;&#38598;&#20013;&#31867;&#20284;&#30340;&#20445;&#25252;&#21644;&#38750;&#20445;&#25252;&#23454;&#20363;&#26469;&#26816;&#27979;&#20998;&#31867;&#22120;&#20013;&#30340;&#27495;&#35270;&#65292;&#36890;&#36807;&#27604;&#36739;&#32452;&#38388;&#20915;&#31574;&#32467;&#26524;&#24046;&#24322;&#65292;&#26469;&#21457;&#29616;&#20010;&#20154;&#27495;&#35270;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#26356;&#22909;&#22320;&#23545;&#12300;&#32473;&#23450;&#24046;&#24322;&#30340;&#20844;&#24179;&#21407;&#21017;&#12301;&#36827;&#34892;&#25805;&#20316;&#65292;&#20197;&#25581;&#31034;&#22312;&#20844;&#24179;&#21407;&#21017;&#19979;&#30340;&#27495;&#35270;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;&#21453;&#20107;&#23454;&#22330;&#26223;&#27979;&#35797;(CST)&#30340;&#22240;&#26524;&#25968;&#25454;&#25366;&#25496;&#26694;&#26550;&#26469;&#26816;&#27979;&#20998;&#31867;&#22120;&#20013;&#30340;&#27495;&#35270;&#24773;&#20917;&#12290;CST&#26088;&#22312;&#20197;&#21487;&#25805;&#20316;&#19988;&#26377;&#24847;&#20041;&#30340;&#26041;&#24335;&#22238;&#31572;&#19968;&#31181;&#30452;&#35266;&#38382;&#39064;&#65306;&#8220;&#22914;&#26524;&#20010;&#20154;&#25110;&#25237;&#35785;&#20154;&#25152;&#23646;&#30340;&#21463;&#20445;&#25252;&#36523;&#20221;&#19981;&#21516;&#65292;&#27169;&#22411;&#30340;&#32467;&#26524;&#23558;&#20250;&#26159;&#20160;&#20040;&#65311;&#8221;&#23427;&#36890;&#36807;&#21453;&#20107;&#23454;&#25512;&#29702;&#26469;&#23545;&#27861;&#24459;&#22522;&#30784;&#30340;&#24773;&#26223;&#27979;&#35797;&#36827;&#34892;&#25193;&#23637;&#65292;&#20197;&#25805;&#20316;&#8220;&#32473;&#23450;&#24046;&#24322;&#30340;&#20844;&#24179;&#21407;&#21017;&#8221;&#30340;&#27010;&#24565;&#12290;&#23545;&#20110;&#20219;&#20309;&#25237;&#35785;&#20154;&#65292;&#25105;&#20204;&#22312;&#20998;&#31867;&#22120;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#20013;&#25214;&#21040;&#24182;&#27604;&#36739;&#30456;&#20284;&#30340;&#21463;&#20445;&#25252;&#21644;&#38750;&#21463;&#20445;&#25252;&#23454;&#20363;&#65292;&#26500;&#36896;&#25511;&#21046;&#32452;&#21644;&#27979;&#35797;&#32452;&#65292;&#20004;&#32452;&#30340;&#20915;&#31574;&#32467;&#26524;&#24046;&#24322;&#24847;&#21619;&#30528;&#28508;&#22312;&#30340;&#20010;&#20154;&#27495;&#35270;&#12290;&#19982;&#24773;&#22659;&#27979;&#35797;&#19981;&#21516;&#65292;&#24773;&#22659;&#27979;&#35797;&#26159;&#22260;&#32469;&#25237;&#35785;&#20154;&#26500;&#24314;&#20004;&#32452;&#65292;&#25105;&#20204;&#26681;&#25454;&#22240;&#26524;&#30693;&#35782;&#22312;&#25237;&#35785;&#20154;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#27979;&#35797;&#32452;&#12290;&#21453;&#20107;&#23454;&#26088;&#22312;&#21453;&#26144;&#21463;&#20445;&#25252;&#23646;&#24615;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present counterfactual situation testing (CST), a causal data mining framework for detecting discrimination in classifiers. CST aims to answer in an actionable and meaningful way the intuitive question "what would have been the model outcome had the individual, or complainant, been of a different protected status?" It extends the legally-grounded situation testing of Thanh et al. (2011) by operationalizing the notion of fairness given the difference using counterfactual reasoning. For any complainant, we find and compare similar protected and non-protected instances in the dataset used by the classifier to construct a control and test group, where a difference between the decision outcomes of the two groups implies potential individual discrimination. Unlike situation testing, which builds both groups around the complainant, we build the test group on the complainant's counterfactual generated using causal knowledge. The counterfactual is intended to reflect how the protected attrib
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Frozen Pretrained Transformer (FPT) &#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21033;&#29992;&#20174;&#25968;&#21313;&#20159;&#26631;&#35760;&#35757;&#32451;&#20986;&#26469;&#30340;&#35821;&#35328;&#25110; CV &#27169;&#22411;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#25152;&#26377;&#20027;&#35201;&#31867;&#22411;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#36827;&#32780;&#20351;&#20854;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#37117;&#20855;&#22791;&#30528;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.11939</link><description>&lt;p&gt;
&#19968;&#31449;&#24335;&#35299;&#20915;&#26041;&#26696;&#65306;&#21033;&#29992;&#39044;&#35757;&#32451; LM &#36827;&#34892;&#24378;&#22823;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
One Fits All:Power General Time Series Analysis by Pretrained LM. (arXiv:2302.11939v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Frozen Pretrained Transformer (FPT) &#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21033;&#29992;&#20174;&#25968;&#21313;&#20159;&#26631;&#35760;&#35757;&#32451;&#20986;&#26469;&#30340;&#35821;&#35328;&#25110; CV &#27169;&#22411;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#25152;&#26377;&#20027;&#35201;&#31867;&#22411;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#36827;&#32780;&#20351;&#20854;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#37117;&#20855;&#22791;&#30528;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; (NLP) &#21644;&#35745;&#31639;&#26426;&#35270;&#35273; (CV) &#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#22312;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#39046;&#22495;&#21462;&#24471;&#30340;&#36827;&#23637;&#26377;&#38480;&#12290;&#19982; NLP &#21644; CV &#19981;&#21516;&#30340;&#26159;&#65292;&#36825;&#20123;&#39046;&#22495;&#37319;&#29992;&#32479;&#19968;&#27169;&#22411;&#21363;&#21487;&#25191;&#34892;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#32780;&#22312;&#27599;&#20010;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#19987;&#38376;&#35774;&#35745;&#30340;&#26041;&#27861;&#20173;&#28982;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#22914;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#12289;&#39044;&#27979;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#38459;&#30861;&#39044;&#35757;&#32451;&#27169;&#22411;&#21457;&#23637;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#32570;&#20047;&#22823;&#37327;&#29992;&#20110;&#35757;&#32451;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#20174;&#25968;&#21313;&#20159;&#26631;&#35760;&#35757;&#32451;&#20986;&#26469;&#30340;&#35821;&#35328;&#25110; CV &#27169;&#22411;&#65292;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36991;&#20813;&#25913;&#21464;&#39044;&#35757;&#32451;&#35821;&#35328;&#25110;&#22270;&#20687;&#27169;&#22411;&#20013;&#27531;&#24046;&#22359;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#21644;&#21069;&#21521;&#20256;&#36882;&#23618;&#12290;&#36825;&#31181;&#27169;&#22411;&#34987;&#31216;&#20026;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120; (FPT)&#65292;&#36890;&#36807;&#23545;&#28041;&#21450;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#25152;&#26377;&#20027;&#35201;&#31867;&#22411;&#30340;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#12289;&#39044;&#27979;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#31561;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#65292;FPT &#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#37117;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although we have witnessed great success of pre-trained models in natural language processing (NLP) and computer vision (CV), limited progress has been made for general time series analysis. Unlike NLP and CV where a unified model can be used to perform different tasks, specially designed approach still dominates in each time series analysis task such as classification, anomaly detection, forecasting, and few-shot learning. The main challenge that blocks the development of pre-trained model for time series analysis is the lack of a large amount of data for training. In this work, we address this challenge by leveraging language or CV models, pre-trained from billions of tokens, for time series analysis. Specifically, we refrain from altering the self-attention and feedforward layers of the residual blocks in the pre-trained language or image model. This model, known as the Frozen Pretrained Transformer (FPT), is evaluated through fine-tuning on all major types of tasks involving time s
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#27880;&#24847;&#21147;&#35760;&#24518;&#65288;NAM&#65289;&#26159;&#19968;&#31181;&#35760;&#24518;&#26550;&#26500;&#65292;&#36890;&#36807;&#21487;&#24494;&#20998;&#32447;&#24615;&#20195;&#25968;&#25805;&#20316;&#21487;&#34987;&#35835;&#20889;&#65292;&#21487;&#24212;&#29992;&#20110;&#35760;&#24518;&#22686;&#24378;&#22411;&#31070;&#32463;&#32593;&#32476;&#12289;&#23567;&#26679;&#26412;&#23398;&#20064;&#21644;&#39640;&#25928;&#30340;&#38271;&#31243;&#20851;&#27880;&#12290;&#23454;&#39564;&#35777;&#26126;NAM&#22312;&#21508;&#26041;&#38754;&#22343;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.09422</link><description>&lt;p&gt;
&#31070;&#32463;&#27880;&#24847;&#21147;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Neural Attention Memory. (arXiv:2302.09422v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09422
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#27880;&#24847;&#21147;&#35760;&#24518;&#65288;NAM&#65289;&#26159;&#19968;&#31181;&#35760;&#24518;&#26550;&#26500;&#65292;&#36890;&#36807;&#21487;&#24494;&#20998;&#32447;&#24615;&#20195;&#25968;&#25805;&#20316;&#21487;&#34987;&#35835;&#20889;&#65292;&#21487;&#24212;&#29992;&#20110;&#35760;&#24518;&#22686;&#24378;&#22411;&#31070;&#32463;&#32593;&#32476;&#12289;&#23567;&#26679;&#26412;&#23398;&#20064;&#21644;&#39640;&#25928;&#30340;&#38271;&#31243;&#20851;&#27880;&#12290;&#23454;&#39564;&#35777;&#26126;NAM&#22312;&#21508;&#26041;&#38754;&#22343;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#35270;&#35282;&#65292;&#23558;&#20854;&#37325;&#26032;&#35774;&#35745;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#35760;&#24518;&#26550;&#26500;&#65292;&#21363;&#31070;&#32463;&#27880;&#24847;&#21147;&#35760;&#24518;&#65288;NAM&#65289;&#12290;NAM&#26159;&#19968;&#31181;&#21487;&#36890;&#36807;&#21487;&#24494;&#20998;&#32447;&#24615;&#20195;&#25968;&#36816;&#31639;&#36827;&#34892;&#35835;&#20889;&#30340;&#35760;&#24518;&#32467;&#26500;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;NAM&#30340;&#19977;&#20010;&#29992;&#20363;&#65306;&#35760;&#24518;&#22686;&#24378;&#22411;&#31070;&#32463;&#32593;&#32476;&#65288;MANN&#65289;&#12289;&#23567;&#26679;&#26412;&#23398;&#20064;&#21644;&#39640;&#25928;&#30340;&#38271;&#31243;&#20851;&#27880;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#22522;&#20110;NAM&#30340;MANN&#65292;&#20998;&#21035;&#26159;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSAM&#65289;&#21644;NAM&#22270;&#28789;&#26426;&#65288;NAM-TM&#65289;&#65292;&#22312;&#31639;&#27861;&#24615;&#38646;&#26679;&#26412;&#27867;&#21270;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#27604;&#20854;&#20182;&#22522;&#32447;&#65288;&#22914;&#21487;&#24494;&#20998;&#31070;&#32463;&#35745;&#31639;&#26426;&#65289;&#26356;&#22909;&#30340;&#35745;&#31639;&#33021;&#21147;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23558;NAM&#24212;&#29992;&#20110;N-way K-shot&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20854;&#22312;&#20943;&#23569;&#35823;&#25253;&#30340;&#25928;&#26524;&#19978;&#27604;&#22522;&#32447;&#20313;&#24358;&#20998;&#31867;&#22120;&#26356;&#26377;&#25928;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#31181;&#24102;&#26377;NAM&#30340;&#39640;&#25928;Transformer&#65292;&#24182;&#36890;&#36807;&#38271;&#36317;&#31163;&#31454;&#25216;&#20219;&#21153;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#65292;&#34920;&#26126;NAM&#21487;&#20197;&#25104;&#20026;&#32553;&#25918;&#28857;&#31215;&#30340;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel perspective of the attention mechanism by reinventing it as a memory architecture for neural networks, namely Neural Attention Memory (NAM). NAM is a memory structure that is both readable and writable via differentiable linear algebra operations. We explore three use cases of NAM: memory-augmented neural network (MANN), few-shot learning, and efficient long-range attention. First, we design two NAM-based MANNs of Long Short-term Memory (LSAM) and NAM Turing Machine (NAM-TM) that show better computational powers in algorithmic zero-shot generalization tasks compared to other baselines such as differentiable neural computer (DNC). Next, we apply NAM to the N-way K-shot learning task and show that it is more effective at reducing false positives compared to the baseline cosine classifier. Finally, we implement an efficient Transformer with NAM and evaluate it with long-range arena tasks to show that NAM can be an efficient and effective alternative for scaled dot-produ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35266;&#23519;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#34394;&#20551;&#29305;&#24449;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#34394;&#20551;&#29305;&#24449;&#37117;&#26159;&#26377;&#23475;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30456;&#20851;&#30340;&#23450;&#20041;&#26469;&#21306;&#20998;&#34394;&#20551;&#29305;&#24449;&#30340;&#26377;&#23475;&#31243;&#24230;&#65292;&#24182;&#21033;&#29992;&#31034;&#20363;&#22256;&#38590;&#24230;&#26041;&#27861;&#26469;&#37327;&#21270;&#27169;&#22411;&#30340;&#26131;&#23398;&#24615;&#21644;&#35782;&#21035;&#34394;&#20551;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2302.09344</link><description>&lt;p&gt;
&#36229;&#36234;&#20998;&#24067;&#20559;&#31227;&#65306;&#20174;&#35757;&#32451;&#21160;&#24577;&#35270;&#35282;&#35299;&#26512;&#34394;&#20551;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Beyond Distribution Shift: Spurious Features Through the Lens of Training Dynamics. (arXiv:2302.09344v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35266;&#23519;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#34394;&#20551;&#29305;&#24449;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#34394;&#20551;&#29305;&#24449;&#37117;&#26159;&#26377;&#23475;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30456;&#20851;&#30340;&#23450;&#20041;&#26469;&#21306;&#20998;&#34394;&#20551;&#29305;&#24449;&#30340;&#26377;&#23475;&#31243;&#24230;&#65292;&#24182;&#21033;&#29992;&#31034;&#20363;&#22256;&#38590;&#24230;&#26041;&#27861;&#26469;&#37327;&#21270;&#27169;&#22411;&#30340;&#26131;&#23398;&#24615;&#21644;&#35782;&#21035;&#34394;&#20551;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23481;&#26131;&#23398;&#20064;&#21040;&#19982;&#26631;&#31614;&#30456;&#20851;&#20294;&#19982;&#23398;&#20064;&#38382;&#39064;&#26080;&#20851;&#30340;&#34394;&#20551;&#29305;&#24449;&#12290;&#36825;&#20250;&#25439;&#23475;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#24341;&#21457;&#38382;&#39064;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#35266;&#23519;&#35757;&#32451;&#36807;&#31243;&#20013;&#20869;&#37096;&#31070;&#32463;&#20803;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#26356;&#22909;&#22320;&#29702;&#35299;&#34394;&#20551;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20570;&#20986;&#20197;&#19979;&#35266;&#23519;&#65306;(1) &#34429;&#28982;&#20808;&#21069;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#34394;&#20551;&#29305;&#24449;&#23545;DNNs&#27867;&#21270;&#33021;&#21147;&#30340;&#26377;&#23475;&#24433;&#21709;&#65292;&#20294;&#25105;&#20204;&#24378;&#35843;&#24182;&#38750;&#25152;&#26377;&#34394;&#20551;&#29305;&#24449;&#37117;&#26159;&#26377;&#23475;&#30340;&#12290;&#34394;&#20551;&#29305;&#24449;&#30340;&#26377;&#23475;&#31243;&#24230;&#21462;&#20915;&#20110;&#30456;&#36739;&#20110;&#32473;&#23450;&#27169;&#22411;&#30340;&#26680;&#24515;&#29305;&#24449;&#32780;&#35328;&#65292;&#23427;&#20204;&#26356;&#38590;&#36824;&#26159;&#26356;&#23481;&#26131;&#23398;&#20064;&#12290;&#36825;&#20010;&#23450;&#20041;&#26159;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30456;&#20851;&#30340;&#12290; (2) &#25105;&#20204;&#22522;&#20110;&#27492;&#21069;&#25552;&#65292;&#21033;&#29992;&#31034;&#20363;&#22256;&#38590;&#24230;&#26041;&#27861;(&#22914;Prediction Depth)&#26469;&#37327;&#21270;&#27169;&#22411;&#30340;"&#26131;&#23398;&#24615;"&#65292;&#24182;&#35782;&#21035;&#34394;&#20551;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) are prone to learning spurious features that correlate with the label during training but are irrelevant to the learning problem. This hurts model generalization and poses problems when deploying them in safety-critical applications. This paper aims to better understand the effects of spurious features through the lens of the learning dynamics of the internal neurons during the training process. We make the following observations: (1) While previous works highlight the harmful effects of spurious features on the generalization ability of DNNs, we emphasize that not all spurious features are harmful. Spurious features can be "benign" or "harmful" depending on whether they are "harder" or "easier" to learn than the core features for a given model. This definition is model and dataset-dependent. (2) We build upon this premise and use instance difficulty methods (like Prediction Depth (Baldock et al., 2021)) to quantify "easiness" for a given model and to identi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31687;&#28145;&#24230;&#23398;&#20064;&#22312;&#24037;&#31243;&#35774;&#35745;&#20013;&#24230;&#37327;&#26041;&#27861;&#30340;&#32508;&#36848;&#21644;&#25351;&#21335;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#20284;&#28982;&#24615;&#30340;&#32479;&#35745;&#24230;&#37327;&#26041;&#27861;&#22312;&#23545;&#24037;&#31243;&#24212;&#29992;&#30340;&#35201;&#27714;&#19978;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;&#65292;&#22240;&#27492;&#26412;&#25991;&#32534;&#36753;&#20102;&#19968;&#32452;&#20840;&#38754;&#30340;&#26032;&#24230;&#37327;&#26631;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;&#24230;&#37327;&#26631;&#20934;&#30340;&#32570;&#28857;&#65292;&#24182;&#26356;&#22909;&#22320;&#19982;&#24037;&#31243;&#35774;&#35745;&#30340;&#38656;&#27714;&#30456;&#19968;&#33268;&#12290;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#22914;&#20309;&#24212;&#29992;&#20110;&#35780;&#20272;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#24037;&#31243;&#35774;&#35745;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#22312;&#25429;&#25417;&#35774;&#35745;&#30340;&#37325;&#35201;&#32454;&#24494;&#24046;&#21035;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#32479;&#35745;&#24230;&#37327;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2302.02913</link><description>&lt;p&gt;
&#36229;&#36234;&#32479;&#35745;&#30456;&#20284;&#24615;&#65306;&#37325;&#26032;&#24605;&#32771;&#26426;&#22120;&#23398;&#20064;&#22312;&#24037;&#31243;&#35774;&#35745;&#20013;&#30340;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Beyond Statistical Similarity: Rethinking Metrics for Deep Generative Models in Engineering Design. (arXiv:2302.02913v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31687;&#28145;&#24230;&#23398;&#20064;&#22312;&#24037;&#31243;&#35774;&#35745;&#20013;&#24230;&#37327;&#26041;&#27861;&#30340;&#32508;&#36848;&#21644;&#25351;&#21335;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#20284;&#28982;&#24615;&#30340;&#32479;&#35745;&#24230;&#37327;&#26041;&#27861;&#22312;&#23545;&#24037;&#31243;&#24212;&#29992;&#30340;&#35201;&#27714;&#19978;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;&#65292;&#22240;&#27492;&#26412;&#25991;&#32534;&#36753;&#20102;&#19968;&#32452;&#20840;&#38754;&#30340;&#26032;&#24230;&#37327;&#26631;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;&#24230;&#37327;&#26631;&#20934;&#30340;&#32570;&#28857;&#65292;&#24182;&#26356;&#22909;&#22320;&#19982;&#24037;&#31243;&#35774;&#35745;&#30340;&#38656;&#27714;&#30456;&#19968;&#33268;&#12290;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#22914;&#20309;&#24212;&#29992;&#20110;&#35780;&#20272;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#24037;&#31243;&#35774;&#35745;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#22312;&#25429;&#25417;&#35774;&#35745;&#30340;&#37325;&#35201;&#32454;&#24494;&#24046;&#21035;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#32479;&#35745;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#65292;&#25193;&#25955;&#27169;&#22411;&#21644;Transformer&#31561;&#65292;&#22312;&#22270;&#20687;&#21644;&#35821;&#38899;&#21512;&#25104;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#33647;&#29289;&#24320;&#21457;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#24037;&#31243;&#35774;&#35745;&#38382;&#39064;&#20013;&#24212;&#29992;&#36825;&#20123;&#27169;&#22411;&#26102;&#65292;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20256;&#32479;&#30340;&#22522;&#20110;&#20284;&#28982;&#24615;&#30340;&#32479;&#35745;&#24230;&#37327;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;&#24037;&#31243;&#24212;&#29992;&#30340;&#35201;&#27714;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#31687;&#28145;&#24230;&#23398;&#20064;&#22312;&#24037;&#31243;&#35774;&#35745;&#20013;&#30340;&#24230;&#37327;&#25351;&#21335;&#21644;&#32508;&#36848;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#8220;&#32463;&#20856;&#8221;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#65292;&#36825;&#20123;&#26631;&#20934;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#29702;&#35770;&#21644;&#20856;&#22411;&#30340;&#35745;&#31639;&#26426;&#24212;&#29992;&#65292;&#28982;&#21518;&#20351;&#29992;&#26696;&#20363;&#30740;&#31350;&#65292;&#24378;&#35843;&#20102;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#20026;&#20309;&#24456;&#23569;&#33021;&#22815;&#36716;&#21270;&#20026;&#35774;&#35745;&#38382;&#39064;&#20294;&#21448;&#22240;&#32570;&#20047;&#30830;&#31435;&#30340;&#26367;&#20195;&#36873;&#25321;&#32780;&#32463;&#24120;&#20351;&#29992;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#32534;&#36753;&#20102;&#19968;&#32452;&#20840;&#38754;&#30340;&#26032;&#24230;&#37327;&#26631;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;&#24230;&#37327;&#26631;&#20934;&#30340;&#32570;&#28857;&#65292;&#24182;&#26356;&#22909;&#22320;&#19982;&#24037;&#31243;&#35774;&#35745;&#30340;&#38656;&#27714;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#24212;&#29992;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#24037;&#31243;&#35774;&#35745;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25552;&#20986;&#30340;&#24230;&#37327;&#26041;&#27861;&#22312;&#25429;&#25417;&#35774;&#35745;&#30340;&#37325;&#35201;&#32454;&#24494;&#24046;&#21035;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#32479;&#35745;&#24230;&#37327;&#26631;&#20934;&#65292;&#22240;&#27492;&#22312;&#24037;&#31243;&#35774;&#35745;&#24773;&#22659;&#20013;&#20026;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models, such as Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), Diffusion Models, and Transformers, have shown great promise in a variety of applications, including image and speech synthesis, natural language processing, and drug discovery. However, when applied to engineering design problems, evaluating the performance of these models can be challenging, as traditional statistical metrics based on likelihood may not fully capture the requirements of engineering applications. This paper doubles as a review and a practical guide to evaluation metrics for deep generative models (DGMs) in engineering design. We first summarize well-accepted `classic' evaluation metrics for deep generative models grounded in machine learning theory and typical computer science applications. Using case studies, we then highlight why these metrics seldom translate well to design problems but see frequent use due to the lack of established alternatives. Next, we curat
&lt;/p&gt;</description></item><item><title>&#36817;&#26399;&#65292;&#39057;&#29575;&#21464;&#25442;&#65288;FT&#65289;&#22312;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#26412;&#25991;&#31995;&#32479;&#22238;&#39038;&#21644;&#24635;&#32467;&#20102;&#22522;&#20110;FT&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#20248;&#21183;&#12289;&#38480;&#21046;&#20197;&#21450;&#20027;&#35201;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.02173</link><description>&lt;p&gt;
&#22522;&#20110;&#39057;&#29575;&#21464;&#25442;&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Deep Learning based Time Series Analysis with Frequency Transformation. (arXiv:2302.02173v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02173
&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#39057;&#29575;&#21464;&#25442;&#65288;FT&#65289;&#22312;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#26412;&#25991;&#31995;&#32479;&#22238;&#39038;&#21644;&#24635;&#32467;&#20102;&#22522;&#20110;FT&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#20248;&#21183;&#12289;&#38480;&#21046;&#20197;&#21450;&#20027;&#35201;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39057;&#29575;&#21464;&#25442;&#65288;FT&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#32435;&#20837;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#26368;&#26032;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#39057;&#29575;&#21464;&#25442;&#30340;&#20248;&#21183;&#65292;&#22914;&#39640;&#25928;&#24615;&#21644;&#20840;&#23616;&#35270;&#35282;&#65292;&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#21644;&#24212;&#29992;&#20013;&#34987;&#36805;&#36895;&#25506;&#32034;&#21644;&#21033;&#29992;&#65292;&#23637;&#31034;&#20102;&#39057;&#29575;&#21464;&#25442;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#33539;&#24335;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;&#23613;&#31649;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#21463;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#21644;&#30740;&#31350;&#65292;&#20294;&#30446;&#21069;&#36824;&#32570;&#20047;&#23545;&#22522;&#20110;&#39057;&#29575;&#21464;&#25442;&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#31995;&#32479;&#22238;&#39038;&#21644;&#28145;&#20837;&#20998;&#26512;&#12290;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#20026;&#20160;&#20040;&#39057;&#29575;&#21464;&#25442;&#21487;&#20197;&#25552;&#21319;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#25928;&#26524;&#65292;&#20197;&#21450;&#23427;&#22312;&#35813;&#39046;&#22495;&#30340;&#38480;&#21046;&#26159;&#20160;&#20040;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#32508;&#36848;&#65292;&#31995;&#32479;&#35843;&#26597;&#21644;&#24635;&#32467;&#20102;&#22522;&#20110;&#39057;&#29575;&#21464;&#25442;&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20027;&#35201;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, frequency transformation (FT) has been increasingly incorporated into deep learning models to significantly enhance state-of-the-art accuracy and efficiency in time series analysis. The advantages of FT, such as high efficiency and a global view, have been rapidly explored and exploited in various time series tasks and applications, demonstrating the promising potential of FT as a new deep learning paradigm for time series analysis. Despite the growing attention and the proliferation of research in this emerging field, there is currently a lack of a systematic review and in-depth analysis of deep learning-based time series models with FT. It is also unclear why FT can enhance time series analysis and what its limitations in the field are. To address these gaps, we present a comprehensive review that systematically investigates and summarizes the recent research advancements in deep learning-based time series analysis with FT. Specifically, we explore the primary approaches us
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38450;&#24481;&#24615;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#28151;&#28102;&#26469;&#25269;&#24481;&#35745;&#31639;&#26426;&#26550;&#26500;&#20013;&#30340;&#20391;&#20449;&#36947;&#25915;&#20987;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#35774;&#35745;&#12289;&#23454;&#29616;&#12289;&#35757;&#32451;&#21644;&#37096;&#32626;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#38450;&#24481;&#22120;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#20449;&#21495;&#20998;&#26512;&#30340;&#23433;&#20840;&#23041;&#32961;&#12290;</title><link>http://arxiv.org/abs/2302.01474</link><description>&lt;p&gt;
&#38450;&#24481;&#24615;&#26426;&#22120;&#23398;&#20064;&#65306;&#29992;&#23545;&#25239;&#24615;&#28151;&#28102;&#26469;&#25269;&#24481;&#26550;&#26500;&#20391;&#20449;&#36947;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Defensive ML: Defending Architectural Side-channels with Adversarial Obfuscation. (arXiv:2302.01474v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38450;&#24481;&#24615;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#28151;&#28102;&#26469;&#25269;&#24481;&#35745;&#31639;&#26426;&#26550;&#26500;&#20013;&#30340;&#20391;&#20449;&#36947;&#25915;&#20987;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#35774;&#35745;&#12289;&#23454;&#29616;&#12289;&#35757;&#32451;&#21644;&#37096;&#32626;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#38450;&#24481;&#22120;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#20449;&#21495;&#20998;&#26512;&#30340;&#23433;&#20840;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#36827;&#34892;&#20449;&#21495;&#20998;&#26512;&#30340;&#20391;&#20449;&#36947;&#25915;&#20987;&#24050;&#32463;&#25104;&#20026;&#35745;&#31639;&#26426;&#23433;&#20840;&#30340;&#31361;&#20986;&#23041;&#32961;&#65292;&#22240;&#20026;ML&#27169;&#22411;&#24456;&#23481;&#26131;&#25214;&#21040;&#20449;&#21495;&#20013;&#30340;&#27169;&#24335;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#35745;&#31639;&#26426;&#26550;&#26500;&#23618;&#38754;&#19978;&#20351;&#29992;&#23545;&#25239;&#24615;&#26426;&#22120;&#23398;&#20064;&#65288;AML&#65289;&#26041;&#27861;&#26469;&#23545;&#25239;&#24615;&#20391;&#20449;&#36947;&#30340;&#28151;&#28102;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;&#38450;&#24481;&#24615;&#26426;&#22120;&#23398;&#20064;&#65292;&#23558;&#20449;&#21495;&#30340;&#28151;&#28102;&#22120;&#31216;&#20026;&#38450;&#24481;&#22120;&#12290;&#38450;&#24481;&#24615;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#20010;&#24037;&#20316;&#27969;&#31243;&#65292;&#29992;&#20110;&#20026;&#19981;&#21516;&#29615;&#22659;&#35774;&#35745;&#12289;&#23454;&#29616;&#12289;&#35757;&#32451;&#21644;&#37096;&#32626;&#38450;&#24481;&#22120;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26681;&#25454;&#20391;&#20449;&#36947;&#30340;&#29289;&#29702;&#29305;&#24615;&#21644;&#30828;&#20214;&#38480;&#21046;&#35774;&#35745;&#20102;&#19968;&#20010;&#38450;&#24481;&#22120;&#26550;&#26500;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;DefenderGAN&#32467;&#26500;&#26469;&#35757;&#32451;&#38450;&#24481;&#22120;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#38450;&#24481;&#24615;&#26426;&#22120;&#23398;&#20064;&#26469;&#38459;&#27490;&#20004;&#31181;&#20391;&#20449;&#36947;&#25915;&#20987;&#65306;&#19968;&#31181;&#22522;&#20110;&#20869;&#23384;&#20105;&#29992;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#24212;&#29992;&#31243;&#24207;&#21151;&#32791;&#12290;&#21069;&#32773;&#20351;&#29992;&#20855;&#26377;&#32435;&#31186;&#32423;&#21709;&#24212;&#26102;&#38388;&#30340;&#30828;&#20214;&#38450;&#24481;&#22120;&#65292;&#20197;&#21322;&#27491;&#24120;&#24615;&#33021;&#24433;&#21709;&#33719;&#24471;&#36739;&#39640;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Side-channel attacks that use machine learning (ML) for signal analysis have become prominent threats to computer security, as ML models easily find patterns in signals. To address this problem, this paper explores using Adversarial Machine Learning (AML) methods as a defense at the computer architecture layer to obfuscate side channels. We call this approach Defensive ML, and the generator to obfuscate signals, defender. Defensive ML is a workflow to design, implement, train, and deploy defenders for different environments. First, we design a defender architecture given the physical characteristics and hardware constraints of the side-channel. Next, we use our DefenderGAN structure to train the defender. Finally, we apply defensive ML to thwart two side-channel attacks: one based on memory contention and the other on application power. The former uses a hardware defender with ns-level response time that attains a high level of security with half the performance impact of a traditional
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#36890;&#36807;&#21512;&#20316;&#21327;&#21516;&#36827;&#21270;&#25628;&#32034;&#65292;&#30830;&#23450;&#26426;&#22120;&#23398;&#20064;&#33258;&#20027;&#31995;&#32479;&#20013;&#30340;ML&#32452;&#20214;&#30340;&#21361;&#38505;&#36793;&#30028;&#12290;&#36825;&#31181;&#36793;&#30028;&#21487;&#20197;&#29992;&#20110;&#26500;&#24314;&#23433;&#20840;&#30417;&#35270;&#22120;&#65292;&#24182;&#22312;&#36798;&#21040;&#21361;&#38505;&#36793;&#30028;&#26102;&#37319;&#21462;&#39044;&#23450;&#20041;&#30340;&#22238;&#36864;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2301.13807</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#20316;&#21327;&#21516;&#36827;&#21270;&#25628;&#32034;&#30830;&#23450;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#33258;&#20027;&#31995;&#32479;&#30340;&#21361;&#38505;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
Identifying the Hazard Boundary of ML-enabled Autonomous Systems Using Cooperative Co-Evolutionary Search. (arXiv:2301.13807v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#36890;&#36807;&#21512;&#20316;&#21327;&#21516;&#36827;&#21270;&#25628;&#32034;&#65292;&#30830;&#23450;&#26426;&#22120;&#23398;&#20064;&#33258;&#20027;&#31995;&#32479;&#20013;&#30340;ML&#32452;&#20214;&#30340;&#21361;&#38505;&#36793;&#30028;&#12290;&#36825;&#31181;&#36793;&#30028;&#21487;&#20197;&#29992;&#20110;&#26500;&#24314;&#23433;&#20840;&#30417;&#35270;&#22120;&#65292;&#24182;&#22312;&#36798;&#21040;&#21361;&#38505;&#36793;&#30028;&#26102;&#37319;&#21462;&#39044;&#23450;&#20041;&#30340;&#22238;&#36864;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#39537;&#21160;&#30340;&#33258;&#20027;&#31995;&#32479;&#65288;MLAS&#65289;&#20013;&#65292;&#30830;&#23450;ML&#32452;&#20214;&#65288;MLCs&#65289;&#30340;&#21361;&#38505;&#36793;&#30028;&#23545;&#20110;&#20998;&#26512;&#20013;&#30340;MLAS&#33267;&#20851;&#37325;&#35201;&#12290;&#37492;&#20110;&#36825;&#31181;&#36793;&#30028;&#21487;&#20197;&#25429;&#25417;&#21040;&#23548;&#33268;&#21361;&#38505;&#30340;MLC&#34892;&#20026;&#21644;&#31995;&#32479;&#32972;&#26223;&#26465;&#20214;&#65292;&#20363;&#22914;&#65292;&#22312;&#36798;&#21040;&#21361;&#38505;&#36793;&#30028;&#26102;&#65292;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;&#23433;&#20840;&#30417;&#35270;&#22120;&#65292;&#21487;&#20197;&#22312;&#36816;&#34892;&#26102;&#37319;&#21462;&#20219;&#20309;&#39044;&#23450;&#20041;&#30340;&#22238;&#36864;&#26426;&#21046;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;ML&#32452;&#20214;&#30340;&#36825;&#31181;&#21361;&#38505;&#36793;&#30028;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#36825;&#26159;&#30001;&#20110;&#38382;&#39064;&#31354;&#38388;&#23558;&#31995;&#32479;&#29615;&#22659;&#65288;&#21363;&#22330;&#26223;&#65289;&#21644;MLC&#34892;&#20026;&#65288;&#21363;&#36755;&#20837;&#21644;&#36755;&#20986;&#65289;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#36807;&#20110;&#24222;&#22823;&#65292;&#26080;&#27861;&#36890;&#36807;&#20840;&#38754;&#30340;&#25506;&#32034;&#29978;&#33267;&#26159;&#20256;&#32479;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#22914;&#36951;&#20256;&#31639;&#27861;&#26469;&#22788;&#29702;&#12290;&#27492;&#22806;&#65292;&#30830;&#23450;&#20219;&#20309;MLAS&#23433;&#20840;&#36829;&#35268;&#25152;&#38656;&#30340;&#27169;&#25311;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#20351;&#38382;&#39064;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#20197;&#30830;&#23450;&#24615;&#22320;&#32771;&#34385;&#38382;&#39064;&#31354;&#38388;&#20013;&#30340;&#19968;&#20010;&#21306;&#22495;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Machine Learning (ML)-enabled autonomous systems (MLASs), it is essential to identify the hazard boundary of ML Components (MLCs) in the MLAS under analysis. Given that such boundary captures the conditions in terms of MLC behavior and system context that can lead to hazards, it can then be used to, for example, build a safety monitor that can take any predefined fallback mechanisms at runtime when reaching the hazard boundary. However, determining such hazard boundary for an ML component is challenging. This is due to the problem space combining system contexts (i.e., scenarios) and MLC behaviors (i.e., inputs and outputs) being far too large for exhaustive exploration and even to handle using conventional metaheuristics, such as genetic algorithms. Additionally, the high computational cost of simulations required to determine any MLAS safety violations makes the problem even more challenging. Furthermore, it is unrealistic to consider a region in the problem space deterministicall
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21333;&#20809;&#23376;&#37327;&#23376;&#34892;&#36208;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65292;&#20351;&#29992;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#37327;&#23376;&#21270;&#20102;&#39033;&#30446;&#27169;&#25311;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#36229;&#36234;&#32463;&#20856;&#27169;&#22411;&#33021;&#21147;&#30340;&#37327;&#23376;&#24178;&#28041;&#65292;&#20026;&#35299;&#37322;&#24615;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#23454;&#29616;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2301.13669</link><description>&lt;p&gt;
&#36890;&#36807;&#21333;&#20809;&#23376;&#37327;&#23376;&#34892;&#36208;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards interpretable quantum machine learning via single-photon quantum walks. (arXiv:2301.13669v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13669
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21333;&#20809;&#23376;&#37327;&#23376;&#34892;&#36208;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65292;&#20351;&#29992;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#37327;&#23376;&#21270;&#20102;&#39033;&#30446;&#27169;&#25311;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#36229;&#36234;&#32463;&#20856;&#27169;&#22411;&#33021;&#21147;&#30340;&#37327;&#23376;&#24178;&#28041;&#65292;&#20026;&#35299;&#37322;&#24615;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#23454;&#29616;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#34987;&#21442;&#25968;&#21270;&#30340;&#37327;&#23376;&#30005;&#36335;&#21462;&#20195;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#23384;&#22312;&#19968;&#20010;&#26126;&#26174;&#30340;&#38480;&#21046;&#65292;&#21363;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#37327;&#23376;&#39033;&#30446;&#27169;&#25311;&#65288;PS&#65289;&#30340;&#21464;&#20998;&#37327;&#23376;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#23454;&#29616;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;PS&#20013;&#65292;&#20915;&#31574;&#34987;&#24314;&#27169;&#20026;&#25551;&#36848;&#20195;&#29702;&#35760;&#24518;&#30340;&#22270;&#19978;&#30340;&#38543;&#26426;&#28459;&#27493;&#12290;&#20026;&#20102;&#23454;&#29616;&#37327;&#23376;&#21270;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#36890;&#36807;&#21464;&#20998;&#31639;&#27861;&#35757;&#32451;&#30340;&#21487;&#35843;&#35856;Mach-Zehnder&#24178;&#28041;&#20202;&#26230;&#26684;&#20013;&#30340;&#21333;&#20809;&#23376;&#37327;&#23376;&#34892;&#36208;&#12290;&#36890;&#36807;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#30340;&#20363;&#23376;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#37327;&#23376;&#21270;&#30340;PS&#27169;&#22411;&#21487;&#20197;&#21033;&#29992;&#37327;&#23376;&#24178;&#28041;&#26469;&#33719;&#24471;&#36229;&#36234;&#20854;&#32463;&#20856;&#23545;&#24212;&#27169;&#22411;&#33021;&#21147;&#30340;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#37327;&#23376;&#24178;&#28041;&#22312;&#35757;&#32451;&#21644;&#36861;&#36394;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#20316;&#29992;&#65292;&#20026;&#23454;&#29616;&#35299;&#37322;&#24615;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational quantum algorithms represent a promising approach to quantum machine learning where classical neural networks are replaced by parametrized quantum circuits. However, both approaches suffer from a clear limitation, that is a lack of interpretability. Here, we present a variational method to quantize projective simulation (PS), a reinforcement learning model aimed at interpretable artificial intelligence. Decision making in PS is modeled as a random walk on a graph describing the agent's memory. To implement the quantized model, we consider quantum walks of single photons in a lattice of tunable Mach-Zehnder interferometers trained via variational algorithms. Using an example from transfer learning, we show that the quantized PS model can exploit quantum interference to acquire capabilities beyond those of its classical counterpart. Finally, we discuss the role of quantum interference for training and tracing the decision making process, paving the way for realizations of int
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;DiffSTG&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;STGNN&#30340;&#26102;&#31354;&#23398;&#20064;&#33021;&#21147;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23567;STG&#39044;&#27979;&#20013;&#30340;&#25490;&#21517;&#27010;&#29575;&#20998;&#25968;&#21644;&#22343;&#26041;&#26681;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2301.13629</link><description>&lt;p&gt;
DiffSTG: &#24102;&#26377;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#30340;&#27010;&#29575;&#26102;&#31354;&#22270;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
DiffSTG: Probabilistic Spatio-Temporal Graph Forecasting with Denoising Diffusion Models. (arXiv:2301.13629v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;DiffSTG&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;STGNN&#30340;&#26102;&#31354;&#23398;&#20064;&#33021;&#21147;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23567;STG&#39044;&#27979;&#20013;&#30340;&#25490;&#21517;&#27010;&#29575;&#20998;&#25968;&#21644;&#22343;&#26041;&#26681;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;STGNN&#65289;&#24050;&#25104;&#20026;&#26102;&#31354;&#22270;&#65288;STG&#65289;&#39044;&#27979;&#30340;&#20027;&#35201;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26080;&#27861;&#23545;STG&#25968;&#25454;&#20013;&#30340;&#20869;&#22312;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#24314;&#27169;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#23454;&#29992;&#24615;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#20851;&#27880;&#27010;&#29575;STG&#39044;&#27979;&#65292;&#30001;&#20110;&#24314;&#27169;&#19981;&#30830;&#23450;&#24615;&#21644;&#22797;&#26434;&#30340;ST&#20381;&#36182;&#20851;&#31995;&#30340;&#22256;&#38590;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#23558;&#27969;&#34892;&#30340;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#25512;&#24191;&#21040;STG&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DiffSTG&#30340;&#26032;&#30340;&#38750;&#33258;&#22238;&#24402;&#26694;&#26550;&#65292;&#24182;&#22312;&#35813;&#26694;&#26550;&#20013;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;STG&#21435;&#22122;&#32593;&#32476;UGnet&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;STGNN&#30340;&#26102;&#31354;&#23398;&#20064;&#33021;&#21147;&#19982;&#25193;&#25955;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#30456;&#32467;&#21512;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;DiffSTG&#23558;&#25345;&#32493;&#25490;&#21517;&#27010;&#29575;&#20998;&#25968;&#65288;CRPS&#65289;&#38477;&#20302;&#20102;4%-14%&#65292;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#38477;&#20302;&#20102;2%-7%&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatio-temporal graph neural networks (STGNN) have emerged as the dominant model for spatio-temporal graph (STG) forecasting. Despite their success, they fail to model intrinsic uncertainties within STG data, which cripples their practicality in downstream tasks for decision-making. To this end, this paper focuses on probabilistic STG forecasting, which is challenging due to the difficulty in modeling uncertainties and complex ST dependencies. In this study, we present the first attempt to generalize the popular denoising diffusion probabilistic models to STGs, leading to a novel non-autoregressive framework called DiffSTG, along with the first denoising network UGnet for STG in the framework. Our approach combines the spatio-temporal learning capabilities of STGNNs with the uncertainty measurements of diffusion models. Extensive experiments validate that DiffSTG reduces the Continuous Ranked Probability Score (CRPS) by 4%-14%, and Root Mean Squared Error (RMSE) by 2%-7% over existing 
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#33021;&#22815;&#25918;&#22823;&#20915;&#31574;&#36793;&#30028;&#38468;&#36817;&#30340;&#23616;&#37096;&#21306;&#22495;&#65292;&#25913;&#21892;&#25972;&#20010;&#31995;&#32479;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.11375</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#25918;&#22823;&#20915;&#31574;&#36793;&#30028;&#38468;&#36817;&#30340;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
Neural networks learn to magnify areas near decision boundaries. (arXiv:2301.11375v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11375
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#33021;&#22815;&#25918;&#22823;&#20915;&#31574;&#36793;&#30028;&#38468;&#36817;&#30340;&#23616;&#37096;&#21306;&#22495;&#65292;&#25913;&#21892;&#25972;&#20010;&#31995;&#32479;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#35757;&#32451;&#22914;&#20309;&#22609;&#36896;&#31070;&#32463;&#32593;&#32476;&#29305;&#24449;&#22270;&#35825;&#23548;&#30340;&#40654;&#26364;&#20960;&#20309;&#12290;&#22312;&#23485;&#24230;&#20026;&#26080;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#38543;&#26426;&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#36755;&#20837;&#31354;&#38388;&#19978;&#24341;&#23548;&#39640;&#24230;&#23545;&#31216;&#30340;&#24230;&#37327;&#12290;&#35757;&#32451;&#20998;&#31867;&#20219;&#21153;&#30340;&#32593;&#32476;&#20013;&#30340;&#29305;&#24449;&#23398;&#20064;&#25918;&#22823;&#20102;&#27839;&#20915;&#31574;&#36793;&#30028;&#30340;&#23616;&#37096;&#21306;&#22495;&#12290;&#36825;&#20123;&#21464;&#21270;&#19982;&#20808;&#21069;&#25552;&#20986;&#30340;&#29992;&#20110;&#25163;&#21160;&#35843;&#25972;&#26680;&#26041;&#27861;&#20197;&#25913;&#21892;&#27867;&#21270;&#30340;&#20960;&#20309;&#26041;&#27861;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study how training molds the Riemannian geometry induced by neural network feature maps. At infinite width, neural networks with random parameters induce highly symmetric metrics on input space. Feature learning in networks trained to perform classification tasks magnifies local areas along decision boundaries. These changes are consistent with previously proposed geometric approaches for hand-tuning of kernel methods to improve generalization.
&lt;/p&gt;</description></item><item><title>XLM-V&#36890;&#36807;&#20811;&#26381;&#22810;&#35821;&#35328;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35789;&#27719;&#29942;&#39048;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;&#20351;&#29992;&#19968;&#20010;&#19968;&#30334;&#19975;&#26631;&#35760;&#35789;&#27719;&#34920;&#65292;XLM-V&#22312;&#21508;&#39033;&#20219;&#21153;&#20013;&#22343;&#20248;&#20110;XLM-R&#12290;</title><link>http://arxiv.org/abs/2301.10472</link><description>&lt;p&gt;
XLM-V: &#20811;&#26381;&#22810;&#35821;&#35328;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35789;&#27719;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models. (arXiv:2301.10472v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10472
&lt;/p&gt;
&lt;p&gt;
XLM-V&#36890;&#36807;&#20811;&#26381;&#22810;&#35821;&#35328;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35789;&#27719;&#29942;&#39048;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;&#20351;&#29992;&#19968;&#20010;&#19968;&#30334;&#19975;&#26631;&#35760;&#35789;&#27719;&#34920;&#65292;XLM-V&#22312;&#21508;&#39033;&#20219;&#21153;&#20013;&#22343;&#20248;&#20110;XLM-R&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20381;&#36182;&#20110;&#19968;&#20010;&#22312;100&#22810;&#31181;&#35821;&#35328;&#38388;&#20849;&#20139;&#30340;&#21333;&#19968;&#35789;&#27719;&#34920;&#12290;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#21442;&#25968;&#21644;&#28145;&#24230;&#30340;&#22686;&#21152;&#65292;&#35789;&#27719;&#22823;&#23567;&#22522;&#26412;&#20445;&#25345;&#19981;&#21464;&#12290;&#36825;&#31181;"&#35789;&#27719;&#29942;&#39048;"&#38480;&#21046;&#20102;XLM-R&#31561;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#22312;&#35789;&#27719;&#19978;&#30340;&#36328;&#35821;&#35328;&#20849;&#20139;&#65292;&#20026;&#27599;&#31181;&#35821;&#35328;&#20998;&#37197;&#36275;&#22815;&#30340;&#35206;&#30422;&#33021;&#21147;&#65292;&#20174;&#32780;&#25193;&#23637;&#21040;&#38750;&#24120;&#22823;&#30340;&#22810;&#35821;&#35328;&#35789;&#27719;&#34920;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#35789;&#27719;&#36827;&#34892;&#20998;&#35789;&#36890;&#24120;&#27604;XLM-R&#26356;&#35821;&#20041;&#26377;&#24847;&#20041;&#19988;&#26356;&#30701;&#12290;&#21033;&#29992;&#36825;&#20010;&#25913;&#36827;&#30340;&#35789;&#27719;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#20855;&#26377;100&#19975;&#20010;&#26631;&#35760;&#35789;&#27719;&#34920;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;XLM-V&#12290;XLM-V&#22312;&#25105;&#20204;&#27979;&#35797;&#30340;&#27599;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;XLM-R&#65292;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;XNLI&#65289;&#12289;&#38382;&#31572;&#65288;MLQA&#65292;XQuAD&#65292;TyDiQA&#65289;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;WikiAnn&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large multilingual language models typically rely on a single vocabulary shared across 100+ languages. As these models have increased in parameter count and depth, vocabulary size has remained largely unchanged. This \textit{vocabulary bottleneck} limits the representational capabilities of multilingual models like XLM-R. In this paper, we introduce a new approach for scaling to very large multilingual vocabularies by de-emphasizing token sharing between languages with little lexical overlap and assigning vocabulary capacity to achieve sufficient coverage for each individual language. Tokenizations using our vocabulary are typically more semantically meaningful and shorter compared to XLM-R. Leveraging this improved vocabulary, we train XLM-V, a multilingual language model with a one million token vocabulary. XLM-V outperforms XLM-R on every task we tested on ranging from natural language inference (XNLI), question answering (MLQA, XQuAD, TyDiQA), to named entity recognition (WikiAnn).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#26412;&#22320;&#21270;&#19982;&#32534;&#36753;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#23558;&#20107;&#23454;&#26412;&#22320;&#21270;&#21040;&#29305;&#23450;&#27169;&#22411;&#21442;&#25968;&#24182;&#19981;&#33021;&#25552;&#20379;&#32534;&#36753;&#25351;&#23548;&#12290;&#22240;&#26524;&#36861;&#36394;&#26041;&#27861;&#24182;&#19981;&#33021;&#25351;&#23548;&#32534;&#36753;&#21738;&#20010;&#27169;&#22411;&#23618;&#26469;&#35206;&#30422;&#23384;&#20648;&#30340;&#20107;&#23454;&#12290;</title><link>http://arxiv.org/abs/2301.04213</link><description>&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#26412;&#22320;&#21270;&#21644;&#22522;&#20110;&#30693;&#35782;&#32534;&#36753;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#24046;&#24322;&#65292;&#25506;&#35752;&#20102;&#26159;&#21542;&#26412;&#22320;&#21270;&#33021;&#22815;&#25552;&#20379;&#32534;&#36753;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models. (arXiv:2301.04213v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#26412;&#22320;&#21270;&#19982;&#32534;&#36753;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#23558;&#20107;&#23454;&#26412;&#22320;&#21270;&#21040;&#29305;&#23450;&#27169;&#22411;&#21442;&#25968;&#24182;&#19981;&#33021;&#25552;&#20379;&#32534;&#36753;&#25351;&#23548;&#12290;&#22240;&#26524;&#36861;&#36394;&#26041;&#27861;&#24182;&#19981;&#33021;&#25351;&#23548;&#32534;&#36753;&#21738;&#20010;&#27169;&#22411;&#23618;&#26469;&#35206;&#30422;&#23384;&#20648;&#30340;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#23398;&#20064;&#21040;&#22823;&#37327;&#30340;&#20107;&#23454;&#20449;&#24687;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#23558;&#36825;&#20123;&#20449;&#24687;&#23450;&#20301;&#21040;&#27169;&#22411;&#30340;&#29305;&#23450;&#26435;&#37325;&#65292;&#22914;&#20013;&#38388;&#23618;MLP&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#32534;&#36753;&#19981;&#21516;&#20110;&#29616;&#26377;&#26041;&#27861;&#25152;&#24314;&#35758;&#30340;&#23384;&#20648;&#20107;&#23454;&#20301;&#32622;&#30340;&#26435;&#37325;&#65292;&#21487;&#20197;&#25913;&#21464;&#27169;&#22411;&#20013;&#30340;&#20107;&#23454;&#23384;&#20648;&#26041;&#24335;&#12290;&#36825;&#19968;&#21457;&#29616;&#20196;&#20154;&#24847;&#22806;&#65292;&#22240;&#20026;&#25105;&#20204;&#21407;&#26412;&#26399;&#26395;&#23558;&#20107;&#23454;&#26412;&#22320;&#21270;&#21040;&#29305;&#23450;&#30340;&#27169;&#22411;&#21442;&#25968;&#21487;&#20197;&#21578;&#35785;&#25105;&#20204;&#22312;&#27169;&#22411;&#20013;&#22914;&#20309;&#25805;&#32437;&#30693;&#35782;&#65292;&#36825;&#19968;&#20551;&#35774;&#26366;&#28608;&#21457;&#36807;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#30340;&#30740;&#31350;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#34920;&#31034;&#21435;&#22122;&#65288;&#20063;&#31216;&#20026;&#22240;&#26524;&#36861;&#36394;&#65289;&#25152;&#24471;&#20986;&#30340;&#26412;&#22320;&#21270;&#32467;&#35770;&#24182;&#19981;&#33021;&#25552;&#20379;&#20219;&#20309;&#20851;&#20110;&#24212;&#35813;&#22312;&#21738;&#20010;&#27169;&#22411;MLP&#23618;&#36827;&#34892;&#32534;&#36753;&#20197;&#35206;&#30422;&#29616;&#26377;&#23384;&#20648;&#20107;&#23454;&#30340;&#26032;&#20107;&#23454;&#30340;&#35265;&#35299;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#36807;&#21435;&#30340;&#30740;&#31350;&#22914;&#20309;&#20381;&#36182;&#22240;&#26524;&#36861;&#36394;&#26469;&#36873;&#25321;&#38656;&#35201;&#32534;&#36753;&#30340;&#27169;&#22411;&#23618;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20960;&#31181;&#32534;&#36753;&#26041;&#27861;&#30340;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models learn a great quantity of factual information during pretraining, and recent work localizes this information to specific model weights like mid-layer MLP weights. In this paper, we find that we can change how a fact is stored in a model by editing weights that are in a different location than where existing methods suggest that the fact is stored. This is surprising because we would expect that localizing facts to specific model parameters would tell us where to manipulate knowledge in models, and this assumption has motivated past work on model editing methods. Specifically, we show that localization conclusions from representation denoising (also known as Causal Tracing) do not provide any insight into which model MLP layer would be best to edit in order to override an existing stored fact with a new one. This finding raises questions about how past work relies on Causal Tracing to select which model layers to edit. Next, we consider several variants of the editing pr
&lt;/p&gt;</description></item><item><title>&#39532;&#23572;&#21487;&#22827;&#20999;&#29255;Wasserstein&#65288;MSW&#65289;&#36317;&#31163;&#26159;&#19968;&#31181;&#26032;&#30340;SW&#36317;&#31163;&#23478;&#26063;&#65292;&#36890;&#36807;&#22312;&#25237;&#24433;&#26041;&#21521;&#19978;&#26045;&#21152;&#19968;&#38454;&#39532;&#23572;&#21487;&#22827;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#20999;&#29255;Wasserstein&#65288;SW&#65289;&#36317;&#31163;&#20013;&#29420;&#31435;&#25237;&#24433;&#23548;&#33268;&#30340;&#20887;&#20313;&#25237;&#24433;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#65288;found in translation&#65289;</title><link>http://arxiv.org/abs/2301.03749</link><description>&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;&#20999;&#29255;Wasserstein&#36317;&#31163;&#65306;&#36229;&#36234;&#29420;&#31435;&#25237;&#24433;
&lt;/p&gt;
&lt;p&gt;
Markovian Sliced Wasserstein Distances: Beyond Independent Projections. (arXiv:2301.03749v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03749
&lt;/p&gt;
&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;&#20999;&#29255;Wasserstein&#65288;MSW&#65289;&#36317;&#31163;&#26159;&#19968;&#31181;&#26032;&#30340;SW&#36317;&#31163;&#23478;&#26063;&#65292;&#36890;&#36807;&#22312;&#25237;&#24433;&#26041;&#21521;&#19978;&#26045;&#21152;&#19968;&#38454;&#39532;&#23572;&#21487;&#22827;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#20999;&#29255;Wasserstein&#65288;SW&#65289;&#36317;&#31163;&#20013;&#29420;&#31435;&#25237;&#24433;&#23548;&#33268;&#30340;&#20887;&#20313;&#25237;&#24433;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#65288;found in translation&#65289;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20999;&#29255;Wasserstein&#65288;SW&#65289;&#36317;&#31163;&#30001;&#20110;&#29420;&#31435;&#30340;&#22343;&#21248;&#38543;&#26426;&#25237;&#24433;&#26041;&#21521;&#32780;&#23548;&#33268;&#20887;&#20313;&#25237;&#24433;&#12290;&#20026;&#20102;&#37096;&#20998;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#22823;K&#20999;&#29255;Wasserstein&#65288;Max-K-SW&#65289;&#36317;&#31163;&#65288;$K\geq1$&#65289;&#23547;&#27714;&#26368;&#20339;&#30340;&#21306;&#20998;&#27491;&#20132;&#25237;&#24433;&#26041;&#21521;&#12290;&#23613;&#31649;&#33021;&#22815;&#20943;&#23569;&#25237;&#24433;&#25968;&#37327;&#65292;&#20294;Max-K-SW&#30340;&#24230;&#37327;&#24615;&#22312;&#23454;&#36341;&#20013;&#19981;&#33021;&#20445;&#35777;&#65292;&#21407;&#22240;&#26159;&#20248;&#21270;&#30340;&#38750;&#26368;&#20248;&#24615;&#12290;&#27492;&#22806;&#65292;&#27491;&#20132;&#32422;&#26463;&#20063;&#22312;&#35745;&#31639;&#19978;&#26159;&#26114;&#36149;&#30340;&#65292;&#21487;&#33021;&#19981;&#22826;&#26377;&#25928;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;SW&#36317;&#31163;&#23478;&#26063;&#65292;&#31216;&#20026;&#39532;&#23572;&#21487;&#22827;&#20999;&#29255;Wasserstein&#65288;MSW&#65289;&#36317;&#31163;&#65292;&#23427;&#22312;&#25237;&#24433;&#26041;&#21521;&#19978;&#26045;&#21152;&#20102;&#19968;&#38454;&#39532;&#23572;&#21487;&#22827;&#32467;&#26500;&#12290;&#25105;&#20204;&#36890;&#36807;&#25351;&#23450;&#39532;&#23572;&#21487;&#22827;&#32467;&#26500;&#65292;&#21253;&#25324;&#20808;&#39564;&#20998;&#24067;&#12289;&#36716;&#31227;&#20998;&#24067;&#20197;&#21450;&#29123;&#28903;&#21644;&#31232;&#30095;&#21270;&#25216;&#26415;&#65292;&#35752;&#35770;&#20102;MSW&#30340;&#21508;&#31181;&#25104;&#21592;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;MSW&#30340;&#29702;&#35770;&#24615;&#36136;&#65292;&#21253;&#25324;&#25299;&#25169;&#24615;&#36136;&#65288;found in translation&#65289;
&lt;/p&gt;
&lt;p&gt;
Sliced Wasserstein (SW) distance suffers from redundant projections due to independent uniform random projecting directions. To partially overcome the issue, max K sliced Wasserstein (Max-K-SW) distance ($K\geq 1$), seeks the best discriminative orthogonal projecting directions. Despite being able to reduce the number of projections, the metricity of Max-K-SW cannot be guaranteed in practice due to the non-optimality of the optimization. Moreover, the orthogonality constraint is also computationally expensive and might not be effective. To address the problem, we introduce a new family of SW distances, named Markovian sliced Wasserstein (MSW) distance, which imposes a first-order Markov structure on projecting directions. We discuss various members of MSW by specifying the Markov structure including the prior distribution, the transition distribution, and the burning and thinning technique. Moreover, we investigate the theoretical properties of MSW including topological properties (met
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#26041;&#21521;&#23548;&#25968;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#24120;&#29992;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;Mixup&#65292;&#21457;&#29616;&#23427;&#38544;&#21547;&#22320;&#23545;&#25152;&#26377;&#38454;&#25968;&#30340;&#26080;&#38480;&#22810;&#20010;&#26041;&#21521;&#23548;&#25968;&#36827;&#34892;&#20102;&#27491;&#21017;&#21270;&#12290;&#22522;&#20110;&#36825;&#19968;&#24605;&#36335;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#25913;&#36827;&#29256;&#30340;Mixup&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#22312;&#21508;&#31181;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#34920;&#29616;&#20986;&#27604;Mixup&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.13381</link><description>&lt;p&gt;
MixupE&#65306;&#20174;&#26041;&#21521;&#23548;&#25968;&#35282;&#24230;&#29702;&#35299;&#21644;&#25913;&#36827;Mixup&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
MixupE: Understanding and Improving Mixup from Directional Derivative Perspective. (arXiv:2212.13381v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#26041;&#21521;&#23548;&#25968;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#24120;&#29992;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;Mixup&#65292;&#21457;&#29616;&#23427;&#38544;&#21547;&#22320;&#23545;&#25152;&#26377;&#38454;&#25968;&#30340;&#26080;&#38480;&#22810;&#20010;&#26041;&#21521;&#23548;&#25968;&#36827;&#34892;&#20102;&#27491;&#21017;&#21270;&#12290;&#22522;&#20110;&#36825;&#19968;&#24605;&#36335;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#25913;&#36827;&#29256;&#30340;Mixup&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#22312;&#21508;&#31181;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#34920;&#29616;&#20986;&#27604;Mixup&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Mixup&#26159;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#27969;&#34892;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#32447;&#24615;&#25554;&#20540;&#36755;&#20837;&#21644;&#23427;&#20204;&#30340;&#26631;&#31614;&#29983;&#25104;&#39069;&#22806;&#30340;&#26679;&#26412;&#12290;&#35813;&#25216;&#26415;&#24050;&#34987;&#35777;&#23454;&#22312;&#35768;&#22810;&#23398;&#20064;&#33539;&#24335;&#21644;&#24212;&#29992;&#20013;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;&#26412;&#25991;&#39318;&#20808;&#23545;Mixup&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#23427;&#38544;&#21547;&#22320;&#23545;&#25152;&#26377;&#38454;&#25968;&#30340;&#26080;&#38480;&#22810;&#20010;&#26041;&#21521;&#23548;&#25968;&#36827;&#34892;&#20102;&#27491;&#21017;&#21270;&#12290;&#22522;&#20110;&#36825;&#19968;&#26032;&#30340;&#27934;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#29256;&#26412;&#30340;Mixup&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#23427;&#21487;&#20197;&#27604;&#21407;&#22987;&#29256;&#26412;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#39046;&#22495;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20363;&#22914;&#22270;&#20687;&#12289;&#34920;&#26684;&#25968;&#25454;&#12289;&#35821;&#38899;&#21644;&#22270;&#24418;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25913;&#36827;&#20102;Mixup&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#22312;&#20351;&#29992;&#21508;&#31181;&#26550;&#26500;&#26102;&#37117;&#34920;&#29616;&#20986;&#27604;Mixup&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#22312;ImageNet&#30340;top-1&#31934;&#24230;&#19978;&#27604;Mixup&#25552;&#39640;&#20102;0.8%&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixup is a popular data augmentation technique for training deep neural networks where additional samples are generated by linearly interpolating pairs of inputs and their labels. This technique is known to improve the generalization performance in many learning paradigms and applications. In this work, we first analyze Mixup and show that it implicitly regularizes infinitely many directional derivatives of all orders. Based on this new insight, we propose an improved version of Mixup, theoretically justified to deliver better generalization performance than the vanilla Mixup. To demonstrate the effectiveness of the proposed method, we conduct experiments across various domains such as images, tabular data, speech, and graphs. Our results show that the proposed method improves Mixup across multiple datasets using a variety of architectures, for instance, exhibiting an improvement over Mixup by 0.8% in ImageNet top-1 accuracy.
&lt;/p&gt;</description></item><item><title>Refiner&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#38450;&#24481;&#33539;&#24335;&#65292;&#36890;&#36807;&#26500;&#24314;&#19982;&#21407;&#22987;&#25968;&#25454;&#20855;&#26377;&#20302;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#20581;&#22766;&#25968;&#25454;&#65292;&#26377;&#25928;&#22320;&#28151;&#28102;&#26799;&#24230;&#27844;&#28431;&#25915;&#20987;&#32773;&#65292;&#20174;&#32780;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#30340;&#38544;&#31169;&#20445;&#25252;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.02042</link><description>&lt;p&gt;
Refiner: &#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26799;&#24230;&#27844;&#28431;&#25915;&#20987;&#30340;&#25968;&#25454;&#31934;&#28860;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Refiner: Data Refining against Gradient Leakage Attacks in Federated Learning. (arXiv:2212.02042v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02042
&lt;/p&gt;
&lt;p&gt;
Refiner&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#38450;&#24481;&#33539;&#24335;&#65292;&#36890;&#36807;&#26500;&#24314;&#19982;&#21407;&#22987;&#25968;&#25454;&#20855;&#26377;&#20302;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#20581;&#22766;&#25968;&#25454;&#65292;&#26377;&#25928;&#22320;&#28151;&#28102;&#26799;&#24230;&#27844;&#28431;&#25915;&#20987;&#32773;&#65292;&#20174;&#32780;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#30340;&#38544;&#31169;&#20445;&#25252;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24341;&#36215;&#20102;&#23545;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#26131;&#21463;&#26799;&#24230;&#27844;&#28431;&#25915;&#20987;&#30340;&#20851;&#27880;&#12290;&#36825;&#31867;&#25915;&#20987;&#21033;&#29992;&#23458;&#25143;&#31471;&#19978;&#20256;&#30340;&#26799;&#24230;&#26469;&#37325;&#26500;&#20854;&#25935;&#24863;&#25968;&#25454;&#65292;&#20174;&#32780;&#30772;&#22351;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#38544;&#31169;&#20445;&#25252;&#33021;&#21147;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#23041;&#32961;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#38450;&#24481;&#26426;&#21046;&#26469;&#20943;&#36731;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#26426;&#21046;&#36890;&#36807;&#25805;&#32437;&#19978;&#20256;&#30340;&#26799;&#24230;&#26469;&#38450;&#27490;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#36825;&#20123;&#38450;&#24481;&#25514;&#26045;&#22312;&#38754;&#23545;&#22797;&#26434;&#25915;&#20987;&#26102;&#20855;&#26377;&#26377;&#38480;&#30340;&#24377;&#24615;&#65292;&#36825;&#34920;&#26126;&#36843;&#20999;&#38656;&#35201;&#26356;&#26377;&#25928;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38450;&#24481;&#33539;&#24335;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;&#30340;&#26799;&#24230;&#25200;&#21160;&#26041;&#27861;&#65292;&#32780;&#26159;&#19987;&#27880;&#20110;&#26500;&#24314;&#20581;&#22766;&#25968;&#25454;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#22914;&#26524;&#20581;&#22766;&#25968;&#25454;&#19982;&#23458;&#25143;&#31471;&#21407;&#22987;&#25968;&#25454;&#20855;&#26377;&#24456;&#20302;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#19982;&#20581;&#22766;&#25968;&#25454;&#30456;&#20851;&#30340;&#26799;&#24230;&#21487;&#20197;&#26377;&#25928;&#22320;&#28151;&#28102;&#25915;&#20987;&#32773;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;Refiner&#65292;&#23427;&#21516;&#26102;&#20248;&#21270;&#20102;&#20004;&#20010;&#25351;&#26631;&#65292;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Recent works have brought attention to the vulnerability of Federated Learning (FL) systems to gradient leakage attacks. Such attacks exploit clients' uploaded gradients to reconstruct their sensitive data, thereby compromising the privacy protection capability of FL. In response, various defense mechanisms have been proposed to mitigate this threat by manipulating the uploaded gradients. Unfortunately, empirical evaluations have demonstrated limited resilience of these defenses against sophisticated attacks, indicating an urgent need for more effective defenses. In this paper, we explore a novel defensive paradigm that departs from conventional gradient perturbation approaches and instead focuses on the construction of robust data. Intuitively, if robust data exhibits low semantic similarity with clients' raw data, the gradients associated with robust data can effectively obfuscate attackers. To this end, we design Refiner that jointly optimizes two metrics for privacy protection and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25913;&#36827;&#20102;&#20808;&#21069;&#24369;&#19968;&#33268;&#24615;&#38543;&#26426;&#26862;&#26519;&#21464;&#20307;&#30340;&#35777;&#26126;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#22810;&#39033;&#24335;&#38543;&#26426;&#26862;&#26519;&#65288;DMRF&#65289;&#65292;&#24182;&#34920;&#26126;DMRF&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#38382;&#39064;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#27010;&#29575;1&#30340;&#24378;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.15154</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#22810;&#39033;&#24335;&#38543;&#26426;&#26862;&#26519;: &#19968;&#31181;&#20855;&#26377;&#24378;&#19968;&#33268;&#24615;&#30340;&#26032;&#38543;&#26426;&#26862;&#26519;&#21464;&#20307;
&lt;/p&gt;
&lt;p&gt;
Data-driven multinomial random forest: A new random forest variant with strong consistency. (arXiv:2211.15154v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25913;&#36827;&#20102;&#20808;&#21069;&#24369;&#19968;&#33268;&#24615;&#38543;&#26426;&#26862;&#26519;&#21464;&#20307;&#30340;&#35777;&#26126;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#22810;&#39033;&#24335;&#38543;&#26426;&#26862;&#26519;&#65288;DMRF&#65289;&#65292;&#24182;&#34920;&#26126;DMRF&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#38382;&#39064;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#27010;&#29575;1&#30340;&#24378;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#19968;&#20123;&#20808;&#21069;&#24369;&#19968;&#33268;&#24615;&#38543;&#26426;&#26862;&#26519;&#21464;&#20307;&#30340;&#35777;&#26126;&#26041;&#27861;&#20462;&#25913;&#20026;&#24378;&#19968;&#33268;&#24615;&#35777;&#26126;&#26041;&#27861;&#65292;&#24182;&#25913;&#36827;&#20102;&#36825;&#20123;&#21464;&#20307;&#30340;&#25968;&#25454;&#21033;&#29992;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#29702;&#35770;&#24615;&#36136;&#21644;&#23454;&#39564;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#22810;&#39033;&#24335;&#38543;&#26426;&#26862;&#26519;&#65288;DMRF&#65289;&#65292;&#20854;&#19982;BreimanRF&#65288;&#30001;Breiman&#25552;&#20986;&#65289;&#20855;&#26377;&#30456;&#21516;&#30340;&#22797;&#26434;&#24230;&#65292;&#21516;&#26102;&#20197;&#27010;&#29575;1&#28385;&#36275;&#24378;&#19968;&#33268;&#24615;&#12290;&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#38382;&#39064;&#19978;&#65292;&#23427;&#27604;&#20808;&#21069;&#20165;&#28385;&#36275;&#24369;&#19968;&#33268;&#24615;&#30340;RF&#21464;&#20307;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#29978;&#33267;&#36229;&#36807;&#20102;BreimanRF&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;DMRF&#26159;&#24403;&#21069;&#23454;&#29616;&#20102;&#27010;&#29575;1&#30340;&#24378;&#19968;&#33268;&#24615;&#30340;&#20302;&#22797;&#26434;&#24615;&#21644;&#39640;&#24615;&#33021;&#38543;&#26426;&#26862;&#26519;&#30340;&#19968;&#31181;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we modify the proof methods of some previously weakly consistent variants of random forests into strongly consistent proof methods, and improve the data utilization of these variants in order to obtain better theoretical properties and experimental performance. In addition, we propose a data-driven multinomial random forest (DMRF), which has the same complexity with BreimanRF (proposed by Breiman) while satisfying strong consistency with probability 1. It has better performance in classification and regression problems than previous RF variants that only satisfy weak consistency, and in most cases even surpasses BreimanRF in classification tasks. To the best of our knowledge, DMRF is currently a low-complexity and high-performing variation of random forests that achieves strong consistency with probability 1.
&lt;/p&gt;</description></item><item><title>Powderworld&#26159;&#19968;&#20010;&#30452;&#25509;&#22312;GPU&#19978;&#36816;&#34892;&#30340;&#36731;&#37327;&#32423;&#20294;&#34920;&#29616;&#21147;&#24378;&#30340;&#27169;&#25311;&#29615;&#22659;&#65292;&#29992;&#20110;&#25552;&#20379;&#27867;&#21270;&#24615;&#30340;&#30740;&#31350;&#24179;&#21488;&#65292;&#21253;&#25324;&#19990;&#30028;&#24314;&#27169;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22686;&#21152;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#21487;&#20197;&#25913;&#21892;&#19990;&#30028;&#27169;&#22411;&#21644;&#26576;&#20123;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.13051</link><description>&lt;p&gt;
Powderworld&#65306;&#36890;&#36807;&#22810;&#26679;&#21270;&#20219;&#21153;&#20998;&#24067;&#26469;&#29702;&#35299;&#27867;&#21270;&#30340;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Powderworld: A Platform for Understanding Generalization via Rich Task Distributions. (arXiv:2211.13051v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13051
&lt;/p&gt;
&lt;p&gt;
Powderworld&#26159;&#19968;&#20010;&#30452;&#25509;&#22312;GPU&#19978;&#36816;&#34892;&#30340;&#36731;&#37327;&#32423;&#20294;&#34920;&#29616;&#21147;&#24378;&#30340;&#27169;&#25311;&#29615;&#22659;&#65292;&#29992;&#20110;&#25552;&#20379;&#27867;&#21270;&#24615;&#30340;&#30740;&#31350;&#24179;&#21488;&#65292;&#21253;&#25324;&#19990;&#30028;&#24314;&#27169;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22686;&#21152;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#21487;&#20197;&#25913;&#21892;&#19990;&#30028;&#27169;&#22411;&#21644;&#26576;&#20123;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30340;&#37325;&#22823;&#25361;&#25112;&#20043;&#19968;&#26159;&#33021;&#22815;&#27867;&#21270;&#21040;&#26032;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#27867;&#21270;&#20195;&#29702;&#38656;&#35201;&#19968;&#32452;&#20016;&#23500;&#12289;&#22810;&#26679;&#21270;&#30340;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#12290;&#20026;&#36825;&#20123;&#20219;&#21153;&#35774;&#35745;&#19968;&#20010;&#29702;&#24819;&#30340;&#29615;&#22659;&#24456;&#22256;&#38590;&#8212;&#8212;&#29702;&#24819;&#30340;&#29615;&#22659;&#24212;&#25903;&#25345;&#19968;&#31995;&#21015;&#26032;&#20852;&#29616;&#35937;&#12289;&#20016;&#23500;&#30340;&#20219;&#21153;&#31354;&#38388;&#21644;&#24555;&#36895;&#30340;&#36816;&#34892;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#29942;&#39048;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Powderworld&#65292;&#19968;&#20010;&#30452;&#25509;&#22312;GPU&#19978;&#36816;&#34892;&#30340;&#36731;&#37327;&#32423;&#20294;&#34920;&#29616;&#21147;&#24378;&#30340;&#27169;&#25311;&#29615;&#22659;&#12290;&#22312;Powderworld&#20869;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#28608;&#21457;&#25361;&#25112;&#30340;&#20998;&#24067;&#65292;&#19968;&#20010;&#29992;&#20110;&#19990;&#30028;&#24314;&#27169;&#65292;&#19968;&#20010;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#12290;&#27599;&#20010;&#20998;&#24067;&#37117;&#21253;&#21547;&#25163;&#21160;&#35774;&#35745;&#30340;&#27979;&#35797;&#20219;&#21153;&#65292;&#20197;&#26816;&#26597;&#27867;&#21270;&#24615;&#33021;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22686;&#21152;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#21487;&#20197;&#25913;&#21892;&#19990;&#30028;&#27169;&#22411;&#21644;&#26576;&#20123;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#20294;&#21487;&#33021;&#20250;&#25233;&#21046;&#39640;&#26041;&#24046;&#29615;&#22659;&#19979;&#30340;&#23398;&#20064;&#12290;Powderworld&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#19968;&#31181;&#25903;&#25345;&#27867;&#21270;&#30740;&#31350;&#30340;&#29615;&#22659;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the grand challenges of reinforcement learning is the ability to generalize to new tasks. However, general agents require a set of rich, diverse tasks to train on. Designing a `foundation environment' for such tasks is tricky -- the ideal environment would support a range of emergent phenomena, an expressive task space, and fast runtime. To take a step towards addressing this research bottleneck, this work presents Powderworld, a lightweight yet expressive simulation environment running directly on the GPU. Within Powderworld, two motivating challenges distributions are presented, one for world-modelling and one for reinforcement learning. Each contains hand-designed test tasks to examine generalization. Experiments indicate that increasing the environment's complexity improves generalization for world models and certain reinforcement learning agents, yet may inhibit learning in high-variance environments. Powderworld aims to support the study of generalization by providing a so
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#19988;&#39640;&#36136;&#37327;&#30340;&#20154;&#33041;&#21151;&#33021;&#32593;&#32476;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#31070;&#32463;&#31185;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#20998;&#26512;&#30340;&#20132;&#21449;&#30740;&#31350;&#12290;&#23427;&#21033;&#29992;&#35299;&#21078;&#23398;&#21644;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#26469;&#29702;&#35299;&#20154;&#33041;&#30340;&#21151;&#33021;&#36830;&#25509;&#65292;&#24182;&#20855;&#26377;&#35782;&#21035;&#28508;&#22312;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#20998;&#26512;&#30740;&#31350;&#22823;&#33041;&#30340;&#32593;&#32476;&#24418;&#24335;&#65292;&#33021;&#22815;&#39044;&#27979;&#36825;&#20123;&#30142;&#30149;&#30340;&#26089;&#26399;&#21457;&#29983;&#12290;&#33041;&#32593;&#32476;&#20197;&#22270;&#24418;&#26041;&#24335;&#34920;&#31034;&#65292;&#20445;&#30041;&#20102;&#20016;&#23500;&#30340;&#32467;&#26500;&#21644;&#20301;&#32622;&#20449;&#24687;&#65292;&#20256;&#32479;&#30340;&#26816;&#26597;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#21040;&#36825;&#20123;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#33041;&#32593;&#32476;&#25968;&#25454;&#38480;&#21046;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2211.12421</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#32593;&#32476;&#31070;&#32463;&#31185;&#23398;&#65306;&#20851;&#20110;&#25968;&#25454;&#25910;&#38598;&#19982;&#22522;&#20934;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Network Neuroscience: On Data Collection and Benchmark. (arXiv:2211.12421v3 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#19988;&#39640;&#36136;&#37327;&#30340;&#20154;&#33041;&#21151;&#33021;&#32593;&#32476;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#31070;&#32463;&#31185;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#20998;&#26512;&#30340;&#20132;&#21449;&#30740;&#31350;&#12290;&#23427;&#21033;&#29992;&#35299;&#21078;&#23398;&#21644;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#26469;&#29702;&#35299;&#20154;&#33041;&#30340;&#21151;&#33021;&#36830;&#25509;&#65292;&#24182;&#20855;&#26377;&#35782;&#21035;&#28508;&#22312;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#20998;&#26512;&#30740;&#31350;&#22823;&#33041;&#30340;&#32593;&#32476;&#24418;&#24335;&#65292;&#33021;&#22815;&#39044;&#27979;&#36825;&#20123;&#30142;&#30149;&#30340;&#26089;&#26399;&#21457;&#29983;&#12290;&#33041;&#32593;&#32476;&#20197;&#22270;&#24418;&#26041;&#24335;&#34920;&#31034;&#65292;&#20445;&#30041;&#20102;&#20016;&#23500;&#30340;&#32467;&#26500;&#21644;&#20301;&#32622;&#20449;&#24687;&#65292;&#20256;&#32479;&#30340;&#26816;&#26597;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#21040;&#36825;&#20123;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#33041;&#32593;&#32476;&#25968;&#25454;&#38480;&#21046;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#19988;&#39640;&#36136;&#37327;&#30340;&#20154;&#33041;&#21151;&#33021;&#32593;&#32476;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#31070;&#32463;&#31185;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#20998;&#26512;&#30340;&#20132;&#21449;&#30740;&#31350;&#12290;&#35299;&#21078;&#23398;&#21644;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#34987;&#29992;&#20110;&#29702;&#35299;&#20154;&#33041;&#30340;&#21151;&#33021;&#36830;&#25509;&#65292;&#24182;&#19988;&#22312;&#35782;&#21035;&#38463;&#23572;&#33576;&#28023;&#40664;&#27663;&#30151;&#12289;&#24085;&#37329;&#26862;&#30151;&#21644;&#33258;&#38381;&#30151;&#31561;&#28508;&#22312;&#30340;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#26041;&#38754;&#23588;&#20026;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#20998;&#26512;&#30740;&#31350;&#20197;&#33041;&#32593;&#32476;&#30340;&#24418;&#24335;&#26469;&#30740;&#31350;&#22823;&#33041;&#30340;&#26041;&#27861;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#39044;&#27979;&#36825;&#20123;&#30142;&#30149;&#30340;&#26089;&#26399;&#21457;&#29983;&#12290;&#20316;&#20026;&#19968;&#20010;&#22270;&#24418;&#34920;&#31034;&#30340;&#33041;&#32593;&#32476;&#20445;&#30041;&#20102;&#20016;&#23500;&#30340;&#32467;&#26500;&#21644;&#20301;&#32622;&#20449;&#24687;&#65292;&#20256;&#32479;&#30340;&#26816;&#26597;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#21040;&#36825;&#20123;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#33041;&#32593;&#32476;&#25968;&#25454;&#38480;&#21046;&#20102;&#30740;&#31350;&#20154;&#21592;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#30340;&#25506;&#32034;&#12290;&#20854;&#20013;&#20027;&#35201;&#30340;&#22256;&#38590;&#22312;&#20110;&#22797;&#26434;&#30340;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive and quality collection of functional human brain \emph{network} data for potential research in the intersection of neuroscience, machine learning, and graph analytics. Anatomical and functional MRI images have been used to understand the functional connectivity of the human brain and are particularly important in identifying underlying neurodegenerative conditions such as Alzheimer's, Parkinson's, and Autism. Recently, the study of the brain in the form of brain networks using machine learning and graph analytics has become increasingly popular, especially to predict the early onset of these conditions. A brain network, represented as a graph, retains rich structural and positional information that traditional examination methods are unable to capture. However, the lack of publicly accessible brain network data prevents researchers from data-driven explorations. One of the main difficulties lies in the complicated domain-specific preprocessing steps 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#37325;&#26032;&#23450;&#20041;&#20026;&#28508;&#22312;&#20449;&#21495;&#34920;&#31034;&#30340;&#36845;&#20195;&#26144;&#23556;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2211.11917</link><description>&lt;p&gt;
&#27169;&#22359;&#21270;&#28304;&#20998;&#31163;&#30340;&#28508;&#22312;&#36845;&#20195;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Latent Iterative Refinement for Modular Source Separation. (arXiv:2211.11917v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11917
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#37325;&#26032;&#23450;&#20041;&#20026;&#28508;&#22312;&#20449;&#21495;&#34920;&#31034;&#30340;&#36845;&#20195;&#26144;&#23556;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#28304;&#20998;&#31163;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#25972;&#20010;&#35757;&#32451;&#38598;&#19978;&#30340;&#32463;&#39564;&#39118;&#38505;&#65292;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#31471;&#21040;&#31471;&#22320;&#35757;&#32451;&#12290;&#22312;&#25512;&#29702;&#38454;&#27573;&#65292;&#29992;&#25143;&#36890;&#36807;&#33719;&#21462;&#38745;&#24577;&#35745;&#31639;&#22270;&#65292;&#24182;&#22312;&#29305;&#23450;&#30340;&#28151;&#21512;&#20449;&#21495;&#19978;&#36816;&#34892;&#23436;&#25972;&#30340;&#27169;&#22411;&#65292;&#26469;&#33719;&#21462;&#20272;&#35745;&#30340;&#28304;&#20449;&#21495;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#27169;&#22411;&#30001;&#22810;&#20010;&#22522;&#26412;&#22788;&#29702;&#22359;&#32452;&#25104;&#65292;&#36825;&#20123;&#22788;&#29702;&#22359;&#25353;&#39034;&#24207;&#24212;&#29992;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#37325;&#26032;&#23450;&#20041;&#20026;&#28508;&#22312;&#20449;&#21495;&#34920;&#31034;&#30340;&#36845;&#20195;&#26144;&#23556;&#65292;&#25105;&#20204;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21487;&#20197;&#22810;&#27425;&#23545;&#36755;&#20986;&#24212;&#29992;&#30456;&#21516;&#30340;&#22788;&#29702;&#22359;&#65292;&#20197;&#25913;&#36827;&#36755;&#20837;&#20449;&#21495;&#24182;&#25552;&#39640;&#21442;&#25968;&#25928;&#29575;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#37319;&#29992;&#22522;&#22359;&#32423;&#30340;&#31243;&#24207;&#65292;&#36825;&#26679;&#21487;&#20197;&#20943;&#23569;&#20869;&#23384;&#38656;&#27714;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#35757;&#32451;&#19968;&#20010;&#38750;&#24120;&#22797;&#26434;&#30340;&#27169;&#22411;...
&lt;/p&gt;
&lt;p&gt;
Traditional source separation approaches train deep neural network models end-to-end with all the data available at once by minimizing the empirical risk on the whole training set. On the inference side, after training the model, the user fetches a static computation graph and runs the full model on some specified observed mixture signal to get the estimated source signals. Additionally, many of those models consist of several basic processing blocks which are applied sequentially. We argue that we can significantly increase resource efficiency during both training and inference stages by reformulating a model's training and inference procedures as iterative mappings of latent signal representations. First, we can apply the same processing block more than once on its output to refine the input signal and consequently improve parameter efficiency. During training, we can follow a block-wise procedure which enables a reduction on memory requirements. Thus, one can train a very complicate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#32437;&#21521;&#32593;&#32476;&#20272;&#35745;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#21512;&#24182;&#12289;&#24352;&#37327;&#20998;&#35299;&#21644;&#28857;&#36807;&#31243;&#31561;&#26041;&#27861;&#26469;&#20943;&#23569;&#20272;&#35745;&#20559;&#24046;&#21644;&#26041;&#24046;&#12290;</title><link>http://arxiv.org/abs/2211.07866</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#21512;&#24182;&#19979;&#30340;&#32437;&#21521;&#32593;&#32476;&#26377;&#25928;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Efficient Estimation for Longitudinal Network via Adaptive Merging. (arXiv:2211.07866v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#32437;&#21521;&#32593;&#32476;&#20272;&#35745;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#21512;&#24182;&#12289;&#24352;&#37327;&#20998;&#35299;&#21644;&#28857;&#36807;&#31243;&#31561;&#26041;&#27861;&#26469;&#20943;&#23569;&#20272;&#35745;&#20559;&#24046;&#21644;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32437;&#21521;&#32593;&#32476;&#30001;&#22810;&#20010;&#33410;&#28857;&#20043;&#38388;&#30340;&#26102;&#38388;&#36793;&#24207;&#21015;&#32452;&#25104;&#65292;&#20854;&#20013;&#26102;&#38388;&#36793;&#22312;&#23454;&#26102;&#20013;&#34987;&#35266;&#23519;&#21040;&#12290;&#38543;&#30528;&#22312;&#32447;&#31038;&#20132;&#24179;&#21488;&#21644;&#30005;&#23376;&#21830;&#21153;&#30340;&#20852;&#36215;&#65292;&#23427;&#24050;&#32463;&#21464;&#24471;&#26222;&#36941;&#65292;&#20294;&#22312;&#25991;&#29486;&#20013;&#24448;&#24448;&#34987;&#24573;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#32437;&#21521;&#32593;&#32476;&#20272;&#35745;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#32593;&#32476;&#21512;&#24182;&#12289;&#24352;&#37327;&#20998;&#35299;&#21644;&#28857;&#36807;&#31243;&#30340;&#20248;&#21183;&#12290;&#23427;&#21512;&#24182;&#30456;&#37051;&#30340;&#31232;&#30095;&#32593;&#32476;&#65292;&#20197;&#25193;&#22823;&#35266;&#27979;&#36793;&#30340;&#25968;&#37327;&#24182;&#20943;&#23569;&#20272;&#35745;&#26041;&#24046;&#65292;&#21516;&#26102;&#36890;&#36807;&#21033;&#29992;&#26412;&#22320;&#26102;&#38388;&#32467;&#26500;&#36827;&#34892;&#33258;&#36866;&#24212;&#32593;&#32476;&#37051;&#22495;&#25511;&#21046;&#24341;&#20837;&#30340;&#20272;&#35745;&#20559;&#24046;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#26469;&#20419;&#36827;&#20272;&#35745;&#65292;&#20854;&#20013;&#27599;&#27425;&#36845;&#20195;&#30340;&#20272;&#35745;&#38169;&#35823;&#19978;&#30028;&#34987;&#24314;&#31435;&#12290;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#20998;&#26512;&#65292;&#20197;&#37327;&#21270;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#28176;&#36817;&#34892;&#20026;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#21487;&#20197;&#26174;&#30528;&#20943;&#23569;&#20272;&#35745;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Longitudinal network consists of a sequence of temporal edges among multiple nodes, where the temporal edges are observed in real time. It has become ubiquitous with the rise of online social platform and e-commerce, but largely under-investigated in literature. In this paper, we propose an efficient estimation framework for longitudinal network, leveraging strengths of adaptive network merging, tensor decomposition and point process. It merges neighboring sparse networks so as to enlarge the number of observed edges and reduce estimation variance, whereas the estimation bias introduced by network merging is controlled by exploiting local temporal structures for adaptive network neighborhood. A projected gradient descent algorithm is proposed to facilitate estimation, where the upper bound of the estimation error in each iteration is established. A thorough analysis is conducted to quantify the asymptotic behavior of the proposed method, which shows that it can significantly reduce the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20855;&#26377;&#20108;&#38454;&#21160;&#21147;&#23398;&#30340;&#36710;&#36742;&#39640;&#25928;&#22495;&#35206;&#30422;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#32593;&#32476;&#26550;&#26500;&#37319;&#29992;&#20102;LSTM&#21644;&#33258;&#27880;&#24847;&#21147;&#25216;&#26415;&#65292;&#35757;&#32451;&#22909;&#30340;&#31574;&#30053;&#22312;&#24615;&#33021;&#19978;&#26126;&#26174;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#32463;&#20856;&#25511;&#21046;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2211.05952</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#20855;&#26377;&#20108;&#38454;&#21160;&#21147;&#23398;&#30340;&#36710;&#36742;&#30340;&#39640;&#25928;&#22495;&#35206;&#30422;
&lt;/p&gt;
&lt;p&gt;
Efficient Domain Coverage for Vehicles with Second-Order Dynamics via Multi-Agent Reinforcement Learning. (arXiv:2211.05952v4 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20855;&#26377;&#20108;&#38454;&#21160;&#21147;&#23398;&#30340;&#36710;&#36742;&#39640;&#25928;&#22495;&#35206;&#30422;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#32593;&#32476;&#26550;&#26500;&#37319;&#29992;&#20102;LSTM&#21644;&#33258;&#27880;&#24847;&#21147;&#25216;&#26415;&#65292;&#35757;&#32451;&#22909;&#30340;&#31574;&#30053;&#22312;&#24615;&#33021;&#19978;&#26126;&#26174;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#32463;&#20856;&#25511;&#21046;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#20316;&#30340;&#33258;&#20027;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#35206;&#30422;&#25351;&#23450;&#21306;&#22495;&#20855;&#26377;&#35768;&#22810;&#28508;&#22312;&#24212;&#29992;&#65292;&#22914;&#26080;&#20154;&#26426;&#25628;&#32034;&#21644;&#25937;&#25588;&#12289;&#26862;&#26519;&#28779;&#28798;&#24212;&#23545;&#21644;&#23454;&#26102;&#39640;&#20998;&#36776;&#29575;&#30417;&#27979;&#12290;&#20256;&#32479;&#26041;&#27861;&#28041;&#21450;&#22522;&#20110;&#20256;&#24863;&#22120;&#25968;&#25454;&#35774;&#35745;&#22522;&#20110;&#27169;&#22411;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#22522;&#20110;&#27169;&#22411;&#30340;&#25511;&#21046;&#22120;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#26368;&#20808;&#36827;&#30340;&#32463;&#20856;&#25511;&#21046;&#31574;&#30053;&#20173;&#28982;&#23384;&#22312;&#36739;&#22823;&#31243;&#24230;&#30340;&#27425;&#20248;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#28041;&#21450;&#20855;&#26377;&#20108;&#38454;&#21160;&#21147;&#23398;&#30340;&#22810;&#26234;&#33021;&#20307;&#39640;&#25928;&#22495;&#35206;&#30422;&#38382;&#39064;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65288;MAPPO&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#32593;&#32476;&#26550;&#26500;&#21253;&#25324;LSTM&#21644;&#33258;&#27880;&#24847;&#21147;&#30340;&#32467;&#21512;&#65292;&#20351;&#24471;&#35757;&#32451;&#22909;&#30340;&#31574;&#30053;&#33021;&#22815;&#36866;&#24212;&#21487;&#21464;&#25968;&#37327;&#30340;&#26234;&#33021;&#20307;&#12290;&#25105;&#20204;&#35757;&#32451;&#22909;&#30340;&#31574;&#30053;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#32463;&#20856;&#25511;&#21046;&#31574;&#30053;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Collaborative autonomous multi-agent systems covering a specified area have many potential applications, such as UAV search and rescue, forest fire fighting, and real-time high-resolution monitoring. Traditional approaches for such coverage problems involve designing a model-based control policy based on sensor data. However, designing model-based controllers is challenging, and the state-of-the-art classical control policy still exhibits a large degree of sub-optimality. In this paper, we present a reinforcement learning (RL) approach for the multi-agent efficient domain coverage problem involving agents with second-order dynamics. Our approach is based on the Multi-Agent Proximal Policy Optimization Algorithm (MAPPO). Our proposed network architecture includes the incorporation of LSTM and self-attention, which allows the trained policy to adapt to a variable number of agents. Our trained policy significantly outperforms the state-of-the-art classical control policy. We demonstrate o
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#31354;&#38388;&#32452;&#23398;&#25216;&#26415;&#21644;&#21453;&#20107;&#23454;&#20248;&#21270;&#31574;&#30053;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35774;&#35745;&#32959;&#30244;&#25200;&#21160;&#20197;&#22686;&#24378;T&#32454;&#32990;&#28024;&#28070;&#30340;&#26041;&#27861;&#65292;&#20026;&#25552;&#39640;&#23454;&#20307;&#32959;&#30244;&#30340;&#20813;&#30123;&#27835;&#30103;&#25928;&#26524;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2211.04020</link><description>&lt;p&gt;
&#29983;&#25104;&#32959;&#30244;&#31354;&#38388;&#34507;&#30333;&#32452;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#20197;&#21457;&#29616;&#22686;&#24378;&#20813;&#30123;&#28183;&#36879;&#30340;&#26377;&#25928;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Generating counterfactual explanations of tumor spatial proteomes to discover effective strategies for enhancing immune infiltration. (arXiv:2211.04020v2 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04020
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#31354;&#38388;&#32452;&#23398;&#25216;&#26415;&#21644;&#21453;&#20107;&#23454;&#20248;&#21270;&#31574;&#30053;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35774;&#35745;&#32959;&#30244;&#25200;&#21160;&#20197;&#22686;&#24378;T&#32454;&#32990;&#28024;&#28070;&#30340;&#26041;&#27861;&#65292;&#20026;&#25552;&#39640;&#23454;&#20307;&#32959;&#30244;&#30340;&#20813;&#30123;&#27835;&#30103;&#25928;&#26524;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32959;&#30244;&#24494;&#29615;&#22659;(TME)&#23545;&#30284;&#30151;&#39044;&#21518;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#20854;&#20013;&#20813;&#30123;&#32452;&#25104;&#26159;&#20851;&#38190;&#12290;&#34429;&#28982;&#21487;&#20197;&#25913;&#21464;&#20813;&#30123;&#32452;&#25104;&#30340;&#27835;&#30103;&#26041;&#27861;&#65292;&#21253;&#25324;&#20813;&#30123;&#30103;&#27861;&#65292;&#22312;&#27835;&#30103;&#34880;&#28082;&#32959;&#30244;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#25104;&#26524;&#65292;&#20294;&#23545;&#20110;&#20813;&#30123;&#20919;&#21364;&#30340;&#23454;&#20307;&#32959;&#30244;&#25928;&#26524;&#36739;&#24046;&#12290;&#31354;&#38388;&#32452;&#23398;&#25216;&#26415;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#20998;&#23376;&#32454;&#33410;&#25429;&#33719;&#20102;TME&#30340;&#31354;&#38388;&#32452;&#32455;&#65292;&#25581;&#31034;&#20102;&#20813;&#30123;&#32454;&#32990;&#23450;&#20301;&#21644;&#20998;&#23376;&#20449;&#21495;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;T&#32454;&#32990;&#28024;&#28070;&#39044;&#27979;&#23450;&#20041;&#20026;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#21453;&#20107;&#23454;&#20248;&#21270;&#31574;&#30053;&#65292;&#21033;&#29992;&#24739;&#32773;&#32959;&#30244;&#30340;&#22823;&#35268;&#27169;&#31354;&#38388;&#32452;&#23398;&#25968;&#25454;&#35774;&#35745;&#39044;&#27979;&#22686;&#24378;T&#32454;&#32990;&#28024;&#28070;&#30340;&#32959;&#30244;&#25200;&#21160;&#12290;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26681;&#25454;&#25104;&#20687;&#36136;&#35889;&#32454;&#32990;&#20998;&#26512;&#25552;&#20379;&#30340;TME&#20449;&#21495;&#20998;&#23376;&#39044;&#27979;T&#32454;&#32990;&#20998;&#24067;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#35745;&#31639;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
The tumor microenvironment (TME) significantly impacts cancer prognosis due to its immune composition. While therapies for altering the immune composition, including immunotherapies, have shown exciting results for treating hematological cancers, they are less effective for immunologically-cold, solid tumors. Spatial omics technologies capture the spatial organization of the TME with unprecedented molecular detail, revealing the relationship between immune cell localization and molecular signals. Here, we formulate T-cell infiltration prediction as a self-supervised machine learning problem and develop a counterfactual optimization strategy that leverages large scale spatial omics profiles of patient tumors to design tumor perturbations predicted to boost T-cell infiltration. A convolutional neural network predicts T-cell distribution based on signaling molecules in the TME provided by imaging mass cytometry. Gradient-based counterfactual generation, then, computes perturbations predic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#34920;&#31034;&#31354;&#38388;&#30340;&#35282;&#24230;&#23545;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#25216;&#26415;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#36890;&#36807;&#20998;&#31867;&#21644;&#35752;&#35770;&#19981;&#21516;&#30340;&#25968;&#23398;&#35282;&#24230;&#21644;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;KGE&#27169;&#22411;&#21450;&#20854;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2211.03536</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65306;&#22522;&#20110;&#34920;&#31034;&#31354;&#38388;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Embedding: A Survey from the Perspective of Representation Spaces. (arXiv:2211.03536v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#34920;&#31034;&#31354;&#38388;&#30340;&#35282;&#24230;&#23545;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#25216;&#26415;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#36890;&#36807;&#20998;&#31867;&#21644;&#35752;&#35770;&#19981;&#21516;&#30340;&#25968;&#23398;&#35282;&#24230;&#21644;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;KGE&#27169;&#22411;&#21450;&#20854;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGE&#65289;&#26159;&#19968;&#31181;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#25216;&#26415;&#65292;&#26088;&#22312;&#23558;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#34920;&#31034;&#20026;&#20302;&#32500;&#35821;&#20041;&#31354;&#38388;&#65292;&#29992;&#20110;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#22914;&#38142;&#25509;&#39044;&#27979;&#65292;&#30693;&#35782;&#25512;&#29702;&#21644;&#30693;&#35782;&#34917;&#20840;&#12290;&#26412;&#25991;&#20174;&#34920;&#31034;&#31354;&#38388;&#30340;&#35282;&#24230;&#23545;&#29616;&#26377;&#30340;KGE&#25216;&#26415;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22522;&#20110;&#34920;&#31034;&#31354;&#38388;&#30340;&#19977;&#20010;&#25968;&#23398;&#35282;&#24230;&#65288;&#20195;&#25968;&#35282;&#24230;&#12289;&#20960;&#20309;&#35282;&#24230;&#21644;&#20998;&#26512;&#35282;&#24230;&#65289;&#26500;&#24314;&#20102;&#19968;&#20010;&#32454;&#31890;&#24230;&#20998;&#31867;&#65292;&#20171;&#32461;&#20102;&#22522;&#26412;&#25968;&#23398;&#31354;&#38388;&#30340;&#20005;&#26684;&#23450;&#20041;&#65292;&#28982;&#21518;&#28145;&#20837;&#30740;&#31350;&#20102;KGE&#27169;&#22411;&#21450;&#20854;&#25968;&#23398;&#29305;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#19977;&#20010;&#31867;&#21035;&#20013;&#30340;&#19981;&#21516;KGE&#26041;&#27861;&#65292;&#24182;&#24635;&#32467;&#20102;&#31354;&#38388;&#20248;&#21183;&#22312;&#19981;&#21516;&#23884;&#20837;&#38656;&#27714;&#19978;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#25972;&#29702;&#26469;&#33258;&#19979;&#28216;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;KGE&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embedding (KGE) is an increasingly popular technique that aims to represent entities and relations of knowledge graphs into low-dimensional semantic spaces for a wide spectrum of applications such as link prediction, knowledge reasoning and knowledge completion. In this paper, we provide a systematic review of existing KGE techniques based on representation spaces. Particularly, we build a fine-grained classification to categorise the models based on three mathematical perspectives of the representation spaces: (1) Algebraic perspective, (2) Geometric perspective, and (3) Analytical perspective. We introduce the rigorous definitions of fundamental mathematical spaces before diving into KGE models and their mathematical properties. We further discuss different KGE methods over the three categories, as well as summarise how spatial advantages work over different embedding needs. By collating the experimental results from downstream tasks, we also explore the advantages of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#20272;&#35745;&#22120;&#30340;&#31283;&#23450;&#24615;&#36275;&#20197;&#35828;&#26126;&#30041;&#19968;&#27861;&#20132;&#21449;&#39564;&#35777;&#26159;&#21487;&#38752;&#30340;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#38598;&#20013;&#30028;&#38480;&#36229;&#20986;Lipschitz&#36830;&#32493;&#24615;&#20551;&#35774;&#30340;&#25439;&#22833;&#20989;&#25968;&#25110;&#20272;&#35745;&#22120;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#30456;&#23545;&#20016;&#23500;&#30340;&#20998;&#24067;&#31867;&#12290;</title><link>http://arxiv.org/abs/2211.02478</link><description>&lt;p&gt;
&#29992;&#20110;&#30041;&#19968;&#27861;&#20132;&#21449;&#39564;&#35777;&#30340;&#38598;&#20013;&#19981;&#31561;&#24335;
&lt;/p&gt;
&lt;p&gt;
Concentration inequalities for leave-one-out cross validation. (arXiv:2211.02478v3 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#20272;&#35745;&#22120;&#30340;&#31283;&#23450;&#24615;&#36275;&#20197;&#35828;&#26126;&#30041;&#19968;&#27861;&#20132;&#21449;&#39564;&#35777;&#26159;&#21487;&#38752;&#30340;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#38598;&#20013;&#30028;&#38480;&#36229;&#20986;Lipschitz&#36830;&#32493;&#24615;&#20551;&#35774;&#30340;&#25439;&#22833;&#20989;&#25968;&#25110;&#20272;&#35745;&#22120;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#30456;&#23545;&#20016;&#23500;&#30340;&#20998;&#24067;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20272;&#35745;&#22120;&#30340;&#31283;&#23450;&#24615;&#36275;&#20197;&#35828;&#26126;&#30041;&#19968;&#27861;&#20132;&#21449;&#39564;&#35777;&#26159;&#19968;&#20010;&#21487;&#38752;&#30340;&#36807;&#31243;&#65292;&#36890;&#36807;&#22312;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#20013;&#25552;&#20379;&#38598;&#20013;&#30028;&#38480;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#22312;&#25439;&#22833;&#20989;&#25968;&#25110;&#20272;&#35745;&#22120;&#19978;&#25552;&#20379;&#20102;&#36229;&#36807;Lipschitz&#36830;&#32493;&#24615;&#20551;&#35774;&#30340;&#38598;&#20013;&#30028;&#38480;&#12290;&#25105;&#20204;&#36890;&#36807;&#20381;&#36182;&#20855;&#26377;&#28385;&#36275;&#23545;&#25968;Sobolev&#19981;&#31561;&#24335;&#30340;&#20998;&#24067;&#30340;&#38543;&#26426;&#21464;&#37327;&#26469;&#33719;&#24471;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#36825;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#30456;&#23545;&#20016;&#23500;&#30340;&#20998;&#24067;&#31867;&#12290;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#20960;&#20010;&#26377;&#36259;&#30340;&#20363;&#23376;&#26469;&#35828;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#32447;&#24615;&#22238;&#24402;&#65292;&#26680;&#23494;&#24230;&#20272;&#35745;&#20197;&#21450;&#31283;&#23450;/&#25130;&#26029;&#20272;&#35745;&#22120;&#65292;&#20363;&#22914;&#31283;&#23450;&#30340;&#26680;&#22238;&#24402;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article we prove that estimator stability is enough to show that leave-one-out cross validation is a sound procedure, by providing concentration bounds in a general framework. In particular, we provide concentration bounds beyond Lipschitz continuity assumptions on the loss or on the estimator. We obtain our results by relying on random variables with distribution satisfying the logarithmic Sobolev inequality, providing us a relatively rich class of distributions. We illustrate our method by considering several interesting examples, including linear regression, kernel density estimation, and stabilized/truncated estimators such as stabilized kernel regression.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#20840;&#29699;&#28779;&#28798;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#20998;&#21106;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#20840;&#29699;&#28903;&#27585;&#38754;&#31215;&#65292;&#20026;&#21322;&#23395;&#24230;&#23610;&#24230;&#25552;&#20379;&#20102;&#39640;&#25928;&#20934;&#30830;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.00534</link><description>&lt;p&gt;
&#20840;&#29699;&#37326;&#28779;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Global Wildfire Forecasting. (arXiv:2211.00534v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#20840;&#29699;&#28779;&#28798;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#20998;&#21106;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#20840;&#29699;&#28903;&#27585;&#38754;&#31215;&#65292;&#20026;&#21322;&#23395;&#24230;&#23610;&#24230;&#25552;&#20379;&#20102;&#39640;&#25928;&#20934;&#30830;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35745;&#27668;&#20505;&#21464;&#21270;&#23558;&#36890;&#36807;&#21152;&#21095;&#28779;&#28798;&#22825;&#27668;&#24773;&#20917;&#32780;&#21152;&#21095;&#37326;&#28779;&#27963;&#21160;&#12290;&#25913;&#21892;&#25105;&#20204;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#39044;&#27979;&#37326;&#28779;&#30340;&#33021;&#21147;&#23545;&#20110;&#20943;&#36731;&#20854;&#36127;&#38754;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#20840;&#29699;&#28779;&#28798;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#20998;&#21106;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23637;&#31034;&#20102;&#19968;&#20010;&#29992;&#20110;&#39044;&#27979;&#20840;&#29699;&#28903;&#27585;&#38754;&#31215;&#30340;&#21407;&#22411;&#65292;&#23454;&#29616;&#20102;&#21322;&#23395;&#24230;&#23610;&#24230;&#19978;&#30340;&#39044;&#27979;&#12290;&#23588;&#20854;&#26159;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#24320;&#25918;&#33719;&#21462;&#30340;&#20840;&#29699;&#20998;&#26512;&#23601;&#32490;&#25968;&#25454;&#31435;&#26041;&#20307;&#65292;&#20854;&#20013;&#21253;&#21547;&#19982;&#23395;&#33410;&#24615;&#21644;&#21322;&#23395;&#24230;&#24615;&#28779;&#28798;&#39537;&#21160;&#22240;&#32032;&#65288;&#27668;&#20505;&#12289;&#26893;&#34987;&#12289;&#28023;&#27915;&#25351;&#25968;&#12289;&#19982;&#20154;&#30456;&#20851;&#30340;&#21464;&#37327;&#65289;&#20197;&#21450;2001-2021&#24180;&#30340;&#21382;&#21490;&#28903;&#27585;&#38754;&#31215;&#21644;&#37326;&#28779;&#25490;&#25918;&#30456;&#20851;&#30340;&#21508;&#31181;&#21464;&#37327;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#20840;&#29699;&#37326;&#28779;&#39044;&#27979;&#35270;&#20026;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#65292;&#24182;&#25104;&#21151;&#39044;&#27979;&#20102;&#28903;&#27585;&#21306;&#22495;&#20986;&#29616;&#30340;&#21069;8&#22825;&#12289;16&#22825;&#12289;32&#22825;&#21644;64&#22825;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25512;&#21160;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#20840;&#29699;&#28903;&#27585;&#38754;&#31215;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Climate change is expected to aggravate wildfire activity through the exacerbation of fire weather. Improving our capabilities to anticipate wildfires on a global scale is of uttermost importance for mitigating their negative effects. In this work, we create a global fire dataset and demonstrate a prototype for predicting the presence of global burned areas on a sub-seasonal scale with the use of segmentation deep learning models. Particularly, we present an open-access global analysis-ready datacube, which contains a variety of variables related to the seasonal and sub-seasonal fire drivers (climate, vegetation, oceanic indices, human-related variables), as well as the historical burned areas and wildfire emissions for 2001-2021. We train a deep learning model, which treats global wildfire forecasting as an image segmentation task and skillfully predicts the presence of burned areas 8, 16, 32 and 64 days ahead of time. Our work motivates the use of deep learning for global burned area
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#31181;&#20855;&#26377;&#22266;&#23450;&#38544;&#34255;&#23618;&#20998;&#24067;&#30340;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36873;&#25321;&#31616;&#21333;&#12289;&#26131;&#35299;&#37322;&#65292;&#19981;&#38656;&#35201;&#36807;&#24230;&#21442;&#25968;&#21270;&#65292;&#21516;&#26102;&#35757;&#32451;&#26377;&#25928;&#12290;&#27169;&#22411;&#30340;&#38544;&#34255;&#21333;&#20803;&#20026;&#20108;&#20803;&#21464;&#37327;&#26102;&#20855;&#26377;&#20197;&#29305;&#24449;&#20026;&#22522;&#30784;&#30340;&#33258;&#28982;&#35299;&#37322;&#12290;&#20316;&#32773;&#35748;&#20026;&#38544;&#34255;&#21464;&#37327;&#30340;&#20998;&#24067;&#24212;&#35813;&#36981;&#24490;&#26368;&#22823;&#20851;&#32852;&#24230;&#21407;&#21017;&#65292;&#24182;&#20171;&#32461;&#20102;&#20998;&#23618;&#29305;&#24449;&#27169;&#22411;&#65288;HFM&#65289;&#20316;&#20026;&#28385;&#36275;&#36825;&#19968;&#21407;&#21017;&#24182;&#23545;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;&#20013;&#24615;&#20808;&#39564;&#32452;&#32455;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2210.13179</link><description>&lt;p&gt;
Occam&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Occam learning. (arXiv:2210.13179v2 [cond-mat.dis-nn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#31181;&#20855;&#26377;&#22266;&#23450;&#38544;&#34255;&#23618;&#20998;&#24067;&#30340;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36873;&#25321;&#31616;&#21333;&#12289;&#26131;&#35299;&#37322;&#65292;&#19981;&#38656;&#35201;&#36807;&#24230;&#21442;&#25968;&#21270;&#65292;&#21516;&#26102;&#35757;&#32451;&#26377;&#25928;&#12290;&#27169;&#22411;&#30340;&#38544;&#34255;&#21333;&#20803;&#20026;&#20108;&#20803;&#21464;&#37327;&#26102;&#20855;&#26377;&#20197;&#29305;&#24449;&#20026;&#22522;&#30784;&#30340;&#33258;&#28982;&#35299;&#37322;&#12290;&#20316;&#32773;&#35748;&#20026;&#38544;&#34255;&#21464;&#37327;&#30340;&#20998;&#24067;&#24212;&#35813;&#36981;&#24490;&#26368;&#22823;&#20851;&#32852;&#24230;&#21407;&#21017;&#65292;&#24182;&#20171;&#32461;&#20102;&#20998;&#23618;&#29305;&#24449;&#27169;&#22411;&#65288;HFM&#65289;&#20316;&#20026;&#28385;&#36275;&#36825;&#19968;&#21407;&#21017;&#24182;&#23545;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;&#20013;&#24615;&#20808;&#39564;&#32452;&#32455;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22312;&#36825;&#31181;&#27169;&#22411;&#20013;&#65292;&#38544;&#34255;&#23618;&#30340;&#20998;&#24067;&#26159;&#22266;&#23450;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#37319;&#29992;&#36825;&#31181;&#20307;&#31995;&#26550;&#26500;&#30340;&#26426;&#22120;&#23398;&#20064;&#20855;&#26377;&#35768;&#22810;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#36136;&#12290;&#20363;&#22914;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#36873;&#25321;&#20026;&#31616;&#21333;&#19988;&#26131;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#19981;&#38656;&#35201;&#36807;&#24230;&#21442;&#25968;&#21270;&#65292;&#32780;&#19988;&#22312;&#28909;&#21147;&#23398;&#24847;&#20041;&#19979;&#65292;&#35757;&#32451;&#26356;&#26377;&#25928;&#12290;&#24403;&#38544;&#34255;&#21333;&#20803;&#20026;&#20108;&#20803;&#21464;&#37327;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#20197;&#29305;&#24449;&#20026;&#22522;&#30784;&#30340;&#33258;&#28982;&#35299;&#37322;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#32570;&#20047;&#29305;&#24449;&#30340;&#29366;&#24577;&#23545;&#24212;&#20110;&#22312;&#29305;&#24449;&#26041;&#38754;&#26368;&#22823;&#31243;&#24230;&#30340;&#26080;&#30693;&#29366;&#24577;&#65292;&#24182;&#19988;&#65292;&#23398;&#20064;&#31532;&#19968;&#20010;&#29305;&#24449;&#21462;&#20915;&#20110;&#25968;&#25454;&#30340;&#38750;&#39640;&#26031;&#32479;&#35745;&#23646;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#24212;&#35813;&#26681;&#25454;&#26368;&#22823;&#20851;&#32852;&#24230;&#21407;&#21017;&#36873;&#25321;&#38544;&#34255;&#21464;&#37327;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20998;&#23618;&#29305;&#24449;&#27169;&#22411;&#65288;HFM&#65289;&#20316;&#20026;&#28385;&#36275;&#36825;&#19968;&#21407;&#21017;&#24182;&#23545;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;&#20013;&#24615;&#20808;&#39564;&#32452;&#32455;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We discuss probabilistic neural network models for unsupervised learning where the distribution of the hidden layer is fixed. We argue that learning machines with this architecture enjoy a number of desirable properties. For example, the model can be chosen as a simple and interpretable one, it does not need to be over-parametrised and training is argued to be efficient in a thermodynamic sense. When hidden units are binary variables, these models have a natural interpretation in terms of features. We show that the featureless state corresponds to a state of maximal ignorance about the features and that learning the first feature depends on non-Gaussian statistical properties of the data. We suggest that the distribution of hidden variables should be chosen according to the principle of maximal relevance. We introduce the Hierarchical Feature Model (HFM) as an example of a model that satisfies this principle, and that encodes a neutral a priori organisation of the feature space. We pre
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24418;&#24335;&#35777;&#26126;&#65292;&#23637;&#31034;&#20102;&#26368;&#20248;AdaBoost&#31639;&#27861;&#30340;&#20998;&#31867;&#22120;&#21644;&#36793;&#32536;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#32467;&#26524;&#19982;&#20960;&#21313;&#24180;&#30340;&#30740;&#31350;&#30456;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2210.07808</link><description>&lt;p&gt;
&#26368;&#20248;AdaBoost&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Optimal AdaBoost Converges. (arXiv:2210.07808v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24418;&#24335;&#35777;&#26126;&#65292;&#23637;&#31034;&#20102;&#26368;&#20248;AdaBoost&#31639;&#27861;&#30340;&#20998;&#31867;&#22120;&#21644;&#36793;&#32536;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#32467;&#26524;&#19982;&#20960;&#21313;&#24180;&#30340;&#30740;&#31350;&#30456;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#20851;&#20110;AdaBoost&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20998;&#31867;&#22120;&#21644;&#36793;&#32536;&#30340;&#25910;&#25947;&#24615;&#24615;&#36136;&#30340;&#24418;&#24335;&#35777;&#26126;&#30340;&#39044;&#21360;&#26412;&#38598;&#21512;&#12290;&#38024;&#23545;&#36825;&#20123;&#25910;&#25947;&#24615;&#24615;&#36136;&#30340;&#29468;&#24819;&#21644;&#29305;&#27530;&#24773;&#20917;&#24050;&#32463;&#32534;&#20889;&#20102;&#21508;&#31181;&#25968;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#35770;&#25991;&#12290;&#27492;&#22806;&#65292;AdaBoost&#30340;&#36793;&#32536;&#22312;&#22260;&#32469;&#35813;&#31639;&#27861;&#30340;&#30740;&#31350;&#20013;&#21344;&#25454;&#37325;&#35201;&#22320;&#20301;&#12290;&#22312;&#26412;&#25991;&#30340;&#39030;&#28857;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;AdaBoost&#30340;&#20998;&#31867;&#22120;&#21644;&#36793;&#32536;&#22914;&#20309;&#25910;&#25947;&#21040;&#19982;&#20960;&#21313;&#24180;&#30340;&#30740;&#31350;&#30456;&#19968;&#33268;&#30340;&#20540;&#12290;&#22312;&#27492;&#20043;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#32452;&#21512;&#20998;&#31867;&#22120;&#30456;&#20851;&#30340;&#21508;&#31181;&#25968;&#37327;&#26159;&#22914;&#20309;&#25910;&#25947;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The following work is a preprint collection of formal proofs regarding the convergence properties of the AdaBoost machine learning algorithm's classifier and margins. Various math and computer science papers have been written regarding conjectures and special cases of these convergence properties. Furthermore, the margins of AdaBoost feature prominently in the research surrounding the algorithm. At the zenith of this paper we present how AdaBoost's classifier and margins converge on a value that agrees with decades of research. After this, we show how various quantities associated with the combined classifier converge.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#27969;&#20307;-&#32467;&#26500;&#30456;&#20114;&#20316;&#29992;&#30340;&#38477;&#38454;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#20934;&#21333;&#20307;&#30340;&#26041;&#27861;&#23558;&#31995;&#32479;&#29366;&#24577;&#22312;&#26102;&#38388;&#19978;&#28436;&#21464;&#20026;&#20004;&#20010;&#23376;&#32593;&#32476;&#65292;&#24182;&#37319;&#29992;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#27969;&#20307;&#29366;&#24577;&#30340;&#28436;&#21464;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.04193</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#27969;&#20307;-&#32467;&#26500;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Predicting fluid-structure interaction with graph neural networks. (arXiv:2210.04193v2 [physics.flu-dyn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#27969;&#20307;-&#32467;&#26500;&#30456;&#20114;&#20316;&#29992;&#30340;&#38477;&#38454;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#20934;&#21333;&#20307;&#30340;&#26041;&#27861;&#23558;&#31995;&#32479;&#29366;&#24577;&#22312;&#26102;&#38388;&#19978;&#28436;&#21464;&#20026;&#20004;&#20010;&#23376;&#32593;&#32476;&#65292;&#24182;&#37319;&#29992;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#27969;&#20307;&#29366;&#24577;&#30340;&#28436;&#21464;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26059;&#36716;&#31561;&#21464;&#12289;&#20934;&#21333;&#20307;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#29992;&#20110;&#27969;&#20307;-&#32467;&#26500;&#30456;&#20114;&#20316;&#29992;&#31995;&#32479;&#30340;&#38477;&#38454;&#24314;&#27169;&#12290;&#36890;&#36807;&#20219;&#24847;&#25289;&#26684;&#26391;&#26085;-&#27431;&#25289;&#20844;&#24335;&#65292;&#31995;&#32479;&#29366;&#24577;&#22312;&#26102;&#38388;&#19978;&#28436;&#21464;&#26377;&#20004;&#20010;&#23376;&#32593;&#32476;&#12290;&#36890;&#36807;&#22797;&#25968;&#20540;&#30340;&#36866;&#24403;&#27491;&#20132;&#20998;&#35299;&#65292;&#23558;&#32593;&#26684;&#30340;&#31227;&#21160;&#31616;&#21270;&#20026;&#20960;&#20010;&#31995;&#25968;&#30340;&#28436;&#21464;&#65292;&#36825;&#20123;&#31995;&#25968;&#30340;&#26102;&#38388;&#39044;&#27979;&#30001;&#19968;&#20010;&#21333;&#23618;&#22810;&#23618;&#24863;&#30693;&#26426;&#22788;&#29702;&#12290;&#37319;&#29992;&#26377;&#38480;&#20803;&#21551;&#21457;&#24335;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#26681;&#25454;&#25972;&#20010;&#31995;&#32479;&#30340;&#29366;&#24577;&#39044;&#27979;&#27969;&#20307;&#29366;&#24577;&#30340;&#28436;&#21464;&#12290;&#32467;&#26500;&#29366;&#24577;&#38544;&#21547;&#22320;&#36890;&#36807;&#32593;&#26684;&#22312;&#22266;&#28082;&#30028;&#38754;&#19978;&#30340;&#36816;&#21160;&#36827;&#34892;&#24314;&#27169;&#65292;&#22240;&#27492;&#25552;&#20986;&#30340;&#26694;&#26550;&#26159;&#20934;&#21333;&#20307;&#30340;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20856;&#22411;&#30340;&#27969;&#20307;-&#32467;&#26500;&#31995;&#32479;&#19978;&#35780;&#20272;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#24377;&#24615;&#23433;&#35013;&#30340;&#22278;&#26609;&#20307;&#21608;&#22260;&#30340;&#27969;&#21160;&#21644;&#24377;&#24615;&#36798;...
&lt;/p&gt;
&lt;p&gt;
We present a rotation equivariant, quasi-monolithic graph neural network framework for the reduced-order modeling of fluid-structure interaction systems. With the aid of an arbitrary Lagrangian-Eulerian formulation, the system states are evolved temporally with two sub-networks. The movement of the mesh is reduced to the evolution of several coefficients via complex-valued proper orthogonal decomposition, and the prediction of these coefficients over time is handled by a single multi-layer perceptron. A finite element-inspired hypergraph neural network is employed to predict the evolution of the fluid state based on the state of the whole system. The structural state is implicitly modeled by the movement of the mesh on the solid-fluid interface; hence it makes the proposed framework quasi-monolithic. The effectiveness of the proposed framework is assessed on two prototypical fluid-structure systems, namely the flow around an elastically-mounted cylinder, and the flow around a hyperelas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Graph Prompt Feature&#65288;GPF&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#36890;&#29992;&#22320;&#35843;&#25972;&#39044;&#20808;&#35757;&#32451;&#36807;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25805;&#20316;&#20110;&#36755;&#20837;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#33021;&#22815;&#23545;&#24212;&#20219;&#20309;&#24418;&#24335;&#30340;Prompt&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2209.15240</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;Prompt&#35843;&#25972;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Universal Prompt Tuning for Graph Neural Networks. (arXiv:2209.15240v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Graph Prompt Feature&#65288;GPF&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#36890;&#29992;&#22320;&#35843;&#25972;&#39044;&#20808;&#35757;&#32451;&#36807;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25805;&#20316;&#20110;&#36755;&#20837;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#33021;&#22815;&#23545;&#24212;&#20219;&#20309;&#24418;&#24335;&#30340;Prompt&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;Prompt&#35843;&#25972;&#22312;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;&#26041;&#38754;&#24341;&#36215;&#20102;&#30740;&#31350;&#28909;&#28526;&#12290;&#19982;&#35821;&#35328;&#39046;&#22495;&#37319;&#29992;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#31574;&#30053;&#19981;&#21516;&#65292;&#22270;&#24418;&#39046;&#22495;&#23637;&#31034;&#20102;&#22810;&#26679;&#21270;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#35774;&#35745;&#36866;&#24403;&#30340;&#22522;&#20110;Prompt&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35843;&#25972;&#26041;&#27861;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Graph Prompt Feature (GPF) &#30340;&#36890;&#29992;Prompt&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#36866;&#29992;&#20110;&#20219;&#20309;&#39044;&#35757;&#32451;&#31574;&#30053;&#19979;&#30340;&#39044;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;GPF&#22312;&#36755;&#20837;&#22270;&#24418;&#30340;&#29305;&#24449;&#31354;&#38388;&#19978;&#25805;&#20316;&#65292;&#29702;&#35770;&#19978;&#21487;&#23454;&#29616;&#19982;&#20219;&#20309;&#24418;&#24335;&#30340;Prompt&#20989;&#25968;&#31561;&#25928;&#30340;&#25928;&#26524;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19981;&#20877;&#38656;&#35201;&#26126;&#30830;&#35828;&#26126;&#27599;&#20010;&#39044;&#35757;&#32451;&#31574;&#30053;&#23545;&#24212;&#30340;Prompt&#20989;&#25968;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#37319;&#29992;GPF&#26469;&#23454;&#29616;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, prompt tuning has sparked a research surge in adapting pre-trained models. Unlike the unified pre-training strategy employed in the language field, the graph field exhibits diverse pre-training strategies, posing challenges in designing appropriate prompt-based tuning methods for graph neural networks. While some pioneering work has devised specialized prompting functions for models that employ edge prediction as their pre-training tasks, these methods are limited to specific pre-trained GNN models and lack broader applicability. In this paper, we introduce a universal prompt-based tuning method called Graph Prompt Feature (GPF) for pre-trained GNN models under any pre-training strategy. GPF operates on the input graph's feature space and can theoretically achieve an equivalent effect to any form of prompting function. Consequently, we no longer need to illustrate the prompting function corresponding to each pre-training strategy explicitly. Instead, we employ GPF to o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Pareto Actor-Critic&#65288;Pareto-AC&#65289;&#31639;&#27861;&#26469;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22343;&#34913;&#36873;&#25321;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#26080;&#20914;&#31361;&#28216;&#25103;&#30340;&#24615;&#36136;&#65292;&#21363;Pareto&#26368;&#20248;&#22343;&#34913;&#26368;&#22823;&#21270;&#20102;&#25152;&#26377;&#26234;&#33021;&#20307;&#30340;&#22238;&#25253;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;Pareto-AC&#30456;&#27604;&#20854;&#20182;&#19971;&#31181;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#26356;&#33021;&#25910;&#25947;&#21040;&#26356;&#39640;&#30340;&#22238;&#21512;&#22238;&#25253;&#12290;</title><link>http://arxiv.org/abs/2209.14344</link><description>&lt;p&gt;
Pareto Actor-Critic&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22343;&#34913;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Pareto Actor-Critic for Equilibrium Selection in Multi-Agent Reinforcement Learning. (arXiv:2209.14344v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Pareto Actor-Critic&#65288;Pareto-AC&#65289;&#31639;&#27861;&#26469;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22343;&#34913;&#36873;&#25321;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#26080;&#20914;&#31361;&#28216;&#25103;&#30340;&#24615;&#36136;&#65292;&#21363;Pareto&#26368;&#20248;&#22343;&#34913;&#26368;&#22823;&#21270;&#20102;&#25152;&#26377;&#26234;&#33021;&#20307;&#30340;&#22238;&#25253;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;Pareto-AC&#30456;&#27604;&#20854;&#20182;&#19971;&#31181;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#26356;&#33021;&#25910;&#25947;&#21040;&#26356;&#39640;&#30340;&#22238;&#21512;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#20110;&#22312;&#26080;&#20914;&#31361;&#22810;&#26234;&#33021;&#20307;&#21338;&#24328;&#20013;&#30340;&#22343;&#34913;&#36873;&#25321;&#38382;&#39064;&#65292;&#20855;&#20307;&#30740;&#31350;&#20102;&#22312;&#22810;&#20010;&#29616;&#26377;&#22343;&#34913;&#20013;&#36873;&#25321;Pareto&#26368;&#20248;&#22343;&#34913;&#30340;&#38382;&#39064;&#12290;&#24050;&#32463;&#34920;&#26126;&#65292;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30001;&#20110;&#27599;&#20010;&#26234;&#33021;&#20307;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#20854;&#20182;&#26234;&#33021;&#20307;&#25919;&#31574;&#30340;&#19981;&#30830;&#23450;&#24615;&#32780;&#23481;&#26131;&#25910;&#25947;&#21040;Pareto&#25903;&#37197;&#30340;&#22343;&#34913;&#29366;&#24577;&#12290;&#20026;&#20102;&#35299;&#20915;&#27425;&#20248;&#22343;&#34913;&#36873;&#25321;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Pareto Actor-Critic&#65288;Pareto-AC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#26080;&#20914;&#31361;&#28216;&#25103;&#65288;&#21512;&#20316;&#28216;&#25103;&#30340;&#36229;&#38598;&#65289;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#20854;&#21033;&#29992;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#24615;&#36136;&#65306;&#26080;&#20914;&#31361;&#28216;&#25103;&#20013;&#30340;Pareto&#26368;&#20248;&#22343;&#34913;&#26368;&#22823;&#21270;&#20102;&#25152;&#26377;&#26234;&#33021;&#20307;&#30340;&#22238;&#25253;&#65292;&#22240;&#27492;&#23545;&#20110;&#25152;&#26377;&#26234;&#33021;&#20307;&#26469;&#35828;&#26159;&#39318;&#36873;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22810;&#26234;&#33021;&#20307;&#21338;&#24328;&#20013;&#35780;&#20272;&#20102;Pareto-AC&#65292;&#24182;&#26174;&#31034;&#23427;&#25910;&#25947;&#21040;&#26356;&#39640;&#30340;&#22238;&#21512;&#22238;&#25253;&#65292;&#19982;&#19971;&#31181;&#26368;&#20808;&#36827;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30456;&#27604;&#65292;Pareto-AC&#25104;&#21151;&#22320;&#25910;&#25947;&#21040;&#20102;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
This work focuses on equilibrium selection in no-conflict multi-agent games, where we specifically study the problem of selecting a Pareto-optimal equilibrium among several existing equilibria. It has been shown that many state-of-the-art multi-agent reinforcement learning (MARL) algorithms are prone to converging to Pareto-dominated equilibria due to the uncertainty each agent has about the policy of the other agents during training. To address sub-optimal equilibrium selection, we propose Pareto Actor-Critic (Pareto-AC), which is an actor-critic algorithm that utilises a simple property of no-conflict games (a superset of cooperative games): the Pareto-optimal equilibrium in a no-conflict game maximises the returns of all agents and therefore is the preferred outcome for all agents. We evaluate Pareto-AC in a diverse set of multi-agent games and show that it converges to higher episodic returns compared to seven state-of-the-art MARL algorithms and that it successfully converges to a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#39044;&#27979;&#40065;&#26834;&#24615;&#30340;&#27010;&#24565;&#26469;&#26356;&#31995;&#32479;&#22320;&#35780;&#20272;&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#20013;&#24213;&#23618;&#35859;&#35789;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25104;&#21151;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2209.07881</link><description>&lt;p&gt;
&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#35859;&#35789;&#30340;&#27169;&#22411;&#39044;&#27979;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Model Predictive Robustness of Signal Temporal Logic Predicates. (arXiv:2209.07881v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07881
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#39044;&#27979;&#40065;&#26834;&#24615;&#30340;&#27010;&#24565;&#26469;&#26356;&#31995;&#32479;&#22320;&#35780;&#20272;&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#20013;&#24213;&#23618;&#35859;&#35789;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25104;&#21151;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#30340;&#40065;&#26834;&#24615;&#19981;&#20165;&#35780;&#20272;&#20102;&#19968;&#20010;&#20449;&#21495;&#26159;&#21542;&#31526;&#21512;&#35268;&#33539;&#65292;&#32780;&#19988;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#34913;&#37327;&#20844;&#24335;&#34987;&#28385;&#36275;&#25110;&#36829;&#21453;&#30340;&#31243;&#24230;&#30340;&#25351;&#26631;&#12290;&#40065;&#26834;&#24615;&#30340;&#35745;&#31639;&#22522;&#20110;&#23545;&#24213;&#23618;&#35859;&#35789;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#35859;&#35789;&#30340;&#40065;&#26834;&#24615;&#36890;&#24120;&#20197;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;&#26041;&#24335;&#23450;&#20041;&#65292;&#21363;&#19981;&#21253;&#25324;&#31995;&#32479;&#21160;&#24577;&#12290;&#32780;&#19988;&#65292;&#31934;&#30830;&#23450;&#20041;&#22797;&#26434;&#35859;&#35789;&#30340;&#40065;&#26834;&#24615;&#36890;&#24120;&#26159;&#38750;&#24179;&#20961;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#39044;&#27979;&#40065;&#26834;&#24615;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#32771;&#34385;&#22522;&#20110;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#25552;&#20379;&#20102;&#27604;&#20043;&#21069;&#26041;&#27861;&#26356;&#31995;&#32479;&#30340;&#35780;&#20272;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26469;&#23398;&#20064;&#22522;&#20110;&#39044;&#20808;&#35745;&#31639;&#30340;&#39044;&#27979;&#30340;&#40065;&#26834;&#24615;&#65292;&#20197;&#20415;&#21487;&#20197;&#22312;&#32447;&#39640;&#25928;&#22320;&#35745;&#31639;&#40065;&#26834;&#24615;&#20540;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#22312;&#35760;&#24405;&#30340;&#25968;&#25454;&#19978;&#20351;&#29992;&#22312;&#24418;&#24335;&#21270;&#20132;&#36890;&#35268;&#21017;&#20013;&#20351;&#29992;&#30340;&#35859;&#35789;&#30340;&#33258;&#21160;&#39550;&#39542;&#29992;&#20363;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The robustness of signal temporal logic not only assesses whether a signal adheres to a specification but also provides a measure of how much a formula is fulfilled or violated. The calculation of robustness is based on evaluating the robustness of underlying predicates. However, the robustness of predicates is usually defined in a model-free way, i.e., without including the system dynamics. Moreover, it is often nontrivial to define the robustness of complicated predicates precisely. To address these issues, we propose a notion of model predictive robustness, which provides a more systematic way of evaluating robustness compared to previous approaches by considering model-based predictions. In particular, we use Gaussian process regression to learn the robustness based on precomputed predictions so that robustness values can be efficiently computed online. We evaluate our approach for the use case of autonomous driving with predicates used in formalized traffic rules on a recorded dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;Twitter&#25991;&#26412;&#20013;&#20998;&#31867;&#21069;&#25552;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RoBERTa&#27169;&#22411;&#22312;&#21069;&#25552;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.03851</link><description>&lt;p&gt;
5q032e@SMM4H'22: &#22522;&#20110;Transformer&#30340;COVID-19&#30456;&#20851;&#25512;&#25991;&#21069;&#25552;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
5q032e@SMM4H'22: Transformer-based classification of premise in tweets related to COVID-19. (arXiv:2209.03851v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;Twitter&#25991;&#26412;&#20013;&#20998;&#31867;&#21069;&#25552;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RoBERTa&#27169;&#22411;&#22312;&#21069;&#25552;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#32593;&#32476;&#25968;&#25454;&#35780;&#20272;&#30340;&#33258;&#21160;&#21270;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#32463;&#20856;&#25361;&#25112;&#20043;&#19968;&#12290;&#22312;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#65292;&#20174;&#20844;&#20849;&#20449;&#24687;&#20013;&#25366;&#25496;&#20154;&#20204;&#30340;&#31435;&#22330;&#23545;&#20110;&#29702;&#35299;&#23545;&#20581;&#24247;&#21629;&#20196;&#30340;&#24577;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20316;&#32773;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;&#25512;&#29305;&#25991;&#26412;&#20013;&#21069;&#25552;&#30340;&#20998;&#31867;&#12290;&#26412;&#24037;&#20316;&#26159;&#20316;&#20026;2022&#24180;Social Media Mining for Health (SMM4H)&#30740;&#35752;&#20250;&#30340;&#19968;&#37096;&#20998;&#23436;&#25104;&#30340;&#12290;&#25105;&#20204;&#22312;&#26500;&#24314;&#39640;&#25928;&#25429;&#25417;&#25512;&#25991;&#35821;&#20041;&#30340;&#27969;&#31243;&#26102;&#65292;&#25506;&#32034;&#20102;&#29616;&#20195;&#22522;&#20110;Transformer&#30340;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#22312;&#19968;&#20010;&#25512;&#29305;&#25968;&#25454;&#38598;&#19978;&#34920;&#26126;&#65292;&#22312;&#21069;&#25552;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;RoBERTa&#27169;&#22411;&#20248;&#20110;&#20854;&#20182;Transformer&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;ROC AUC&#20540;&#20026;0.807&#65292;F1&#20998;&#25968;&#20026;0.7648&#26041;&#38754;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automation of social network data assessment is one of the classic challenges of natural language processing. During the COVID-19 pandemic, mining people's stances from public messages have become crucial regarding understanding attitudes towards health orders. In this paper, the authors propose the predictive model based on transformer architecture to classify the presence of premise in Twitter texts. This work is completed as part of the Social Media Mining for Health (SMM4H) Workshop 2022. We explored modern transformer-based classifiers in order to construct the pipeline efficiently capturing tweets semantics. Our experiments on a Twitter dataset showed that RoBERTa is superior to the other transformer models in the case of the premise prediction task. The model achieved competitive performance with respect to ROC AUC value 0.807, and 0.7648 for the F1 score.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20110;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;(SNNs)&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#40065;&#26834;&#24615;&#21644;&#36716;&#31227;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25104;&#21151;&#30340;&#30333;&#30418;&#23545;&#25239;&#25915;&#20987;SNNs&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#26367;&#20195;&#26799;&#24230;&#25216;&#26415;&#65292;&#24182;&#19988;&#38750;SNN&#26550;&#26500;&#21019;&#24314;&#30340;&#23545;&#25239;&#26679;&#26412;&#24448;&#24448;&#19981;&#34987;SNNs&#35823;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2209.03358</link><description>&lt;p&gt;
&#25915;&#20987;&#33033;&#20914;&#65306;&#20851;&#20110;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#21487;&#36716;&#31227;&#24615;&#19982;&#23433;&#20840;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Attacking the Spike: On the Transferability and Security of Spiking Neural Networks to Adversarial Examples. (arXiv:2209.03358v3 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03358
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20110;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;(SNNs)&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#40065;&#26834;&#24615;&#21644;&#36716;&#31227;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25104;&#21151;&#30340;&#30333;&#30418;&#23545;&#25239;&#25915;&#20987;SNNs&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#26367;&#20195;&#26799;&#24230;&#25216;&#26415;&#65292;&#24182;&#19988;&#38750;SNN&#26550;&#26500;&#21019;&#24314;&#30340;&#23545;&#25239;&#26679;&#26412;&#24448;&#24448;&#19981;&#34987;SNNs&#35823;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;(SNNs)&#22240;&#20854;&#39640;&#33021;&#25928;&#21644;&#26368;&#36817;&#22312;&#20998;&#31867;&#24615;&#33021;&#19978;&#30340;&#36827;&#23637;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#19982;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;&#23545;SNNs&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#40065;&#26834;&#24615;&#30340;&#20998;&#26512;&#21644;&#30740;&#31350;&#20173;&#28982;&#30456;&#23545;&#19981;&#23436;&#21892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#20110;&#25512;&#36827;SNNs&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#38754;&#65292;&#24182;&#20570;&#20986;&#20102;&#19977;&#20010;&#20027;&#35201;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25104;&#21151;&#30340;&#30333;&#30418;&#23545;&#25239;&#25915;&#20987;SNNs&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#24213;&#23618;&#30340;&#26367;&#20195;&#26799;&#24230;&#25216;&#26415;&#65292;&#21363;&#20351;&#22312;&#23545;&#25239;&#24615;&#35757;&#32451;SNNs&#30340;&#24773;&#20917;&#19979;&#20063;&#19968;&#26679;&#12290;&#20854;&#27425;&#65292;&#21033;&#29992;&#26368;&#20339;&#30340;&#26367;&#20195;&#26799;&#24230;&#25216;&#26415;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#23545;&#25239;&#25915;&#20987;&#22312;SNNs&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26550;&#26500;&#22914;Vision Transformers(ViTs)&#21644;Big Transfer Convolutional Neural Networks(CNNs)&#20043;&#38388;&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#38750;SNN&#26550;&#26500;&#21019;&#24314;&#30340;&#23545;&#25239;&#26679;&#26412;&#24448;&#24448;&#19981;&#34987;SNNs&#35823;&#20998;&#31867;&#12290;&#31532;&#19977;&#65292;&#30001;&#20110;&#32570;&#20047;&#19968;&#20010;&#20849;&#24615;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks (SNNs) have attracted much attention for their high energy efficiency and for recent advances in their classification performance. However, unlike traditional deep learning approaches, the analysis and study of the robustness of SNNs to adversarial examples remain relatively underdeveloped. In this work, we focus on advancing the adversarial attack side of SNNs and make three major contributions. First, we show that successful white-box adversarial attacks on SNNs are highly dependent on the underlying surrogate gradient technique, even in the case of adversarially trained SNNs. Second, using the best surrogate gradient technique, we analyze the transferability of adversarial attacks on SNNs and other state-of-the-art architectures like Vision Transformers (ViTs) and Big Transfer Convolutional Neural Networks (CNNs). We demonstrate that the adversarial examples created by non-SNN architectures are not misclassified often by SNNs. Third, due to the lack of an ubi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#35757;&#32451;&#23545;&#25239;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#30333;&#30418;&#25915;&#20987;&#30340;&#31574;&#30053;&#65292;&#33021;&#22815;&#35775;&#38382;&#30446;&#26631;&#20195;&#29702;&#30340;&#20869;&#37096;&#29366;&#24577;&#65292;&#20174;&#32780;&#35782;&#21035;&#20854;&#28431;&#27934;&#65292;&#25915;&#20987;&#25104;&#21151;&#29575;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2209.02167</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#30333;&#30418;&#23545;&#25239;&#31574;&#30053;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
White-Box Adversarial Policies in Deep Reinforcement Learning. (arXiv:2209.02167v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#35757;&#32451;&#23545;&#25239;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#30333;&#30418;&#25915;&#20987;&#30340;&#31574;&#30053;&#65292;&#33021;&#22815;&#35775;&#38382;&#30446;&#26631;&#20195;&#29702;&#30340;&#20869;&#37096;&#29366;&#24577;&#65292;&#20174;&#32780;&#35782;&#21035;&#20854;&#28431;&#27934;&#65292;&#25915;&#20987;&#25104;&#21151;&#29575;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#23545;&#25239;&#31574;&#30053;&#21487;&#20197;&#36890;&#36807;&#35757;&#32451;&#23545;&#25239;&#20195;&#29702;&#26469;&#26368;&#23567;&#21270;&#30446;&#26631;&#20195;&#29702;&#30340;&#22870;&#21169;&#26469;&#24320;&#21457;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#30740;&#31350;&#20102;&#40657;&#30418;&#29256;&#26412;&#30340;&#36825;&#20123;&#25915;&#20987;&#65292;&#20854;&#20013;&#23545;&#25163;&#20165;&#35266;&#23519;&#19990;&#30028;&#29366;&#24577;&#65292;&#24182;&#23558;&#30446;&#26631;&#20195;&#29702;&#35270;&#20026;&#29615;&#22659;&#30340;&#20219;&#20309;&#20854;&#20182;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#36825;&#24182;&#27809;&#26377;&#32771;&#34385;&#38382;&#39064;&#20013;&#30340;&#38468;&#21152;&#32467;&#26500;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#30333;&#30418;&#25915;&#20987;&#30340;&#25991;&#29486;&#20013;&#33719;&#24471;&#28789;&#24863;&#65292;&#20197;&#35757;&#32451;&#26356;&#26377;&#25928;&#30340;&#23545;&#25239;&#31574;&#30053;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#30333;&#30418;&#23545;&#25239;&#31574;&#30053;&#65292;&#24182;&#26174;&#31034;&#35775;&#38382;&#30446;&#26631;&#20195;&#29702;&#30340;&#20869;&#37096;&#29366;&#24577;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#20854;&#28431;&#27934;&#12290;&#25105;&#20204;&#20570;&#20986;&#20102;&#20004;&#20010;&#36129;&#29486;&#12290;(1)&#25105;&#20204;&#20171;&#32461;&#20102;&#30333;&#30418;&#23545;&#25239;&#31574;&#30053;&#65292;&#20854;&#20013;&#25915;&#20987;&#32773;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#35266;&#23519;&#30446;&#26631;&#30340;&#20869;&#37096;&#29366;&#24577;&#21644;&#19990;&#30028;&#29366;&#24577;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#20351;&#29992;&#36825;&#20123;&#31574;&#30053;&#25915;&#20987;2&#20154;&#28216;&#25103;&#21644;&#29983;&#25104;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20195;&#29702;&#30340;&#26041;&#27861;&#12290;(2)&#25105;&#20204;&#35777;&#26126;&#20102;&#19982;&#40657;&#30418;&#25915;&#20987;&#30456;&#27604;&#65292;&#36825;&#20123;&#31574;&#30053;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#29305;&#21035;&#26159;&#24403;&#30446;&#26631;&#20195;&#29702;&#30340;&#20869;&#37096;&#29366;&#24577;&#27604;&#36739;&#22797;&#26434;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
In reinforcement learning (RL), adversarial policies can be developed by training an adversarial agent to minimize a target agent's rewards. Prior work has studied black-box versions of these attacks where the adversary only observes the world state and treats the target agent as any other part of the environment. However, this does not take into account additional structure in the problem. In this work, we take inspiration from the literature on white-box attacks to train more effective adversarial policies. We study white-box adversarial policies and show that having access to a target agent's internal state can be useful for identifying its vulnerabilities. We make two contributions. (1) We introduce white-box adversarial policies where an attacker observes both a target's internal state and the world state at each timestep. We formulate ways of using these policies to attack agents in 2-player games and text-generating language models. (2) We demonstrate that these policies can ach
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#39044;&#22788;&#29702;&#31649;&#32447;&#23545;&#31070;&#32463;&#24433;&#20687;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#31649;&#32447;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#30340;&#40065;&#26834;&#24615;&#21644;&#25429;&#25417;&#30456;&#20284;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#29420;&#29305;&#21644;&#20849;&#20139;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2208.12909</link><description>&lt;p&gt;
&#31070;&#32463;&#24433;&#20687;&#30340;&#31649;&#32447;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Pipeline-Invariant Representation Learning for Neuroimaging. (arXiv:2208.12909v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#39044;&#22788;&#29702;&#31649;&#32447;&#23545;&#31070;&#32463;&#24433;&#20687;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#31649;&#32447;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#30340;&#40065;&#26834;&#24615;&#21644;&#25429;&#25417;&#30456;&#20284;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#29420;&#29305;&#21644;&#20849;&#20139;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#31070;&#32463;&#24433;&#20687;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#21253;&#25324;&#20174;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#20307;&#31215;&#20013;&#39044;&#27979;&#33041;-&#34920;&#22411;&#20851;&#31995;&#12290;MRI&#25968;&#25454;&#36890;&#24120;&#38656;&#35201;&#22312;&#24314;&#27169;&#20043;&#21069;&#36827;&#34892;&#24191;&#27867;&#30340;&#39044;&#22788;&#29702;&#65292;&#20294;&#26159;&#19981;&#21516;&#30340;MRI&#39044;&#22788;&#29702;&#31649;&#32447;&#24341;&#20837;&#30340;&#21464;&#24322;&#24615;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#30340;&#31185;&#23398;&#21457;&#29616;&#65292;&#21363;&#20351;&#20351;&#29992;&#30456;&#21516;&#30340;&#25968;&#25454;&#12290;&#22312;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#35270;&#35282;&#19979;&#65292;&#25105;&#20204;&#39318;&#20808;&#35780;&#20272;&#39044;&#22788;&#29702;&#31649;&#32447;&#36873;&#25321;&#23545;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#19979;&#28216;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31649;&#32447;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65288;MPSL&#21644;PXL&#65289;&#65292;&#20197;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#30340;&#40065;&#26834;&#24615;&#24182;&#25429;&#25417;&#30456;&#20284;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#12290;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#33521;&#22269;&#29983;&#29289;&#24211;&#65288;UK Biobank&#65289;&#25968;&#25454;&#38598;&#30340;2000&#21517;&#20154;&#20307;&#23545;&#35937;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#29420;&#29305;&#21644;&#20849;&#20139;&#20248;&#21183;&#65292;&#29305;&#21035;&#26159;MPSL&#21487;&#29992;&#20110;&#25913;&#21892;&#26032;&#31649;&#32447;&#30340;&#22806;&#37096;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;PXL&#21487;&#29992;&#20110;&#25913;&#21892;&#21516;&#19968;&#26679;&#26412;&#22312;&#19981;&#21516;&#31649;&#32447;&#38388;&#30340;&#24615;&#33021;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has been widely applied in neuroimaging, including predicting brain-phenotype relationships from magnetic resonance imaging (MRI) volumes. MRI data usually requires extensive preprocessing prior to modeling, but variation introduced by different MRI preprocessing pipelines may lead to different scientific findings, even when using the identical data. Motivated by the data-centric perspective, we first evaluate how preprocessing pipeline selection can impact the downstream performance of a supervised learning model. We next propose two pipeline-invariant representation learning methodologies, MPSL and PXL, to improve robustness in classification performance and to capture similar neural network representations. Using 2000 human subjects from the UK Biobank dataset, we demonstrate that proposed models present unique and shared advantages, in particular that MPSL can be used to improve out-of-sample generalization to new pipelines, while PXL can be used to improve within-sam
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36229;&#22270;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#30340;&#31038;&#21306;&#21457;&#29616;&#38382;&#39064;&#65292;&#36890;&#36807;&#20998;&#26512;&#30456;&#20284;&#24615;&#30697;&#38453;&#21644;&#35774;&#35745;&#35889;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#23494;&#24230;&#26465;&#20214;&#19979;&#30340;&#31934;&#30830;&#24674;&#22797;&#21644;&#39640;&#25928;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2208.12227</link><description>&lt;p&gt;
&#36229;&#22270;SBM&#20013;&#30340;&#31038;&#21306;&#21457;&#29616;&#65306;&#32473;&#23450;&#30456;&#20284;&#24615;&#30697;&#38453;&#30340;&#26368;&#20248;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Community Detection in the Hypergraph SBM: Optimal Recovery Given the Similarity Matrix. (arXiv:2208.12227v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36229;&#22270;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#30340;&#31038;&#21306;&#21457;&#29616;&#38382;&#39064;&#65292;&#36890;&#36807;&#20998;&#26512;&#30456;&#20284;&#24615;&#30697;&#38453;&#21644;&#35774;&#35745;&#35889;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#23494;&#24230;&#26465;&#20214;&#19979;&#30340;&#31934;&#30830;&#24674;&#22797;&#21644;&#39640;&#25928;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#21457;&#29616;&#26159;&#32593;&#32476;&#31185;&#23398;&#20013;&#30340;&#19968;&#20010;&#22522;&#30784;&#38382;&#39064;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#36229;&#22270;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;HSBM&#65289;&#20013;&#30340;&#31038;&#21306;&#21457;&#29616;&#38382;&#39064;&#65292;&#37325;&#28857;&#26159;&#30830;&#20999;&#30340;&#31038;&#21306;&#24674;&#22797;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#30456;&#20284;&#24615;&#30697;&#38453;W&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;W_{ij}&#34920;&#31034;&#21516;&#26102;&#21253;&#21547;i&#21644;j&#30340;&#36229;&#36793;&#30340;&#25968;&#37327;&#12290;&#22312;&#36825;&#20010;&#20449;&#24687;&#27169;&#22411;&#19979;&#65292;Kim&#65292;Bandeira&#21644;Goemans&#30830;&#23450;&#20102;&#22312;&#23545;&#25968;&#24230;&#25968;&#21306;&#38388;&#20869;&#30340;&#31934;&#30830;&#24674;&#22797;&#30340;&#20449;&#24687;&#35770;&#38408;&#20540;&#65292;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21322;&#23450;&#35268;&#21010;&#26494;&#24347;&#65292;&#35748;&#20026;&#36825;&#26159;&#26368;&#20248;&#30340;&#29468;&#24819;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#23454;&#20102;&#36825;&#20010;&#29468;&#24819;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21333;&#19988;&#39640;&#25928;&#30340;&#35889;&#31639;&#27861;&#65292;&#20960;&#20046;&#20855;&#26377;&#32447;&#24615;&#36816;&#34892;&#26102;&#38388;&#65292;&#24182;&#19988;&#35777;&#26126;&#23427;&#36798;&#21040;&#20102;&#20449;&#24687;&#35770;&#38408;&#20540;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#35889;&#31639;&#27861;&#36824;&#33021;&#22312;&#26356;&#23494;&#38598;&#30340;&#24773;&#20917;&#19979;&#25104;&#21151;&#65292;&#24182;&#19988;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#26356;&#39640;&#25928;&#65292;&#22240;&#27492;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community detection is a fundamental problem in network science. In this paper, we consider community detection in hypergraphs drawn from the $hypergraph$ $stochastic$ $block$ $model$ (HSBM), with a focus on exact community recovery. We study the performance of polynomial-time algorithms which operate on the $similarity$ $matrix$ $W$, where $W_{ij}$ reports the number of hyperedges containing both $i$ and $j$. Under this information model, Kim, Bandeira, and Goemans determined the information-theoretic threshold for exact recovery in the logarithmic degree regime, and proposed a semidefinite programming relaxation which they conjectured to be optimal. In this paper, we confirm this conjecture. We also design a simple and highly efficient spectral algorithm with nearly linear runtime and show that it achieves the information-theoretic threshold. Moreover, the spectral algorithm also succeeds in denser regimes and is considerably more efficient than previous approaches, establishing it a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#31614;&#21517;&#32593;&#32476;&#20013;&#30340;&#24179;&#34913;&#32467;&#26500;&#21644;&#24322;&#24120;&#25928;&#24212;&#65292;&#24182;&#22312;&#31038;&#21306;&#26816;&#27979;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#32593;&#32476;&#25512;&#26029;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2207.09324</link><description>&lt;p&gt;
&#24102;&#26377;&#21516;&#26102;&#26816;&#27979;&#31038;&#21306;&#21644;&#24322;&#24120;&#30340;&#31614;&#21517;&#32593;&#32476;&#23884;&#20837;&#21450;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Signed Network Embedding with Application to Simultaneous Detection of Communities and Anomalies. (arXiv:2207.09324v3 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.09324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#31614;&#21517;&#32593;&#32476;&#20013;&#30340;&#24179;&#34913;&#32467;&#26500;&#21644;&#24322;&#24120;&#25928;&#24212;&#65292;&#24182;&#22312;&#31038;&#21306;&#26816;&#27979;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#32593;&#32476;&#25512;&#26029;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#65292;&#25105;&#20204;&#32463;&#24120;&#35266;&#23519;&#21040;&#24102;&#26377;&#38468;&#21152;&#31526;&#21495;&#20449;&#24687;&#30340;&#32593;&#32476;&#65292;&#28982;&#32780;&#36825;&#20123;&#20449;&#24687;&#22312;&#29616;&#26377;&#30340;&#32593;&#32476;&#27169;&#22411;&#20013;&#34987;&#22823;&#37096;&#20998;&#24573;&#35270;&#20102;&#12290;&#26412;&#25991;&#38024;&#23545;&#31614;&#21517;&#32593;&#32476;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#20197;&#35299;&#24320;&#38169;&#32508;&#22797;&#26434;&#30340;&#24179;&#34913;&#32467;&#26500;&#21644;&#24322;&#24120;&#25928;&#24212;&#65292;&#20174;&#32780;&#21487;&#20197;&#26497;&#22823;&#22320;&#20419;&#36827;&#19979;&#28216;&#20998;&#26512;&#65292;&#21253;&#25324;&#31038;&#21306;&#26816;&#27979;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#32593;&#32476;&#25512;&#26029;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#36890;&#36807;&#20302;&#31209;&#21152;&#31232;&#30095;&#30697;&#38453;&#20998;&#35299;&#25429;&#25417;&#24179;&#34913;&#32467;&#26500;&#21644;&#24322;&#24120;&#25928;&#24212;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#26041;&#27861;&#32852;&#21512;&#20272;&#35745;&#20108;&#32773;&#12290;&#22312;&#32593;&#32476;&#23884;&#20837;&#12289;&#31038;&#21306;&#26816;&#27979;&#21644;&#24322;&#24120;&#26816;&#27979;&#26041;&#38754;&#65292;&#23427;&#30340;&#29702;&#35770;&#20445;&#35777;&#26159;&#24314;&#31435;&#22312;&#28176;&#36817;&#19968;&#33268;&#24615;&#21644;&#26377;&#38480;&#26679;&#26412;&#27010;&#29575;&#36793;&#30028;&#30340;&#22522;&#30784;&#19978;&#12290;&#25152;&#25552;&#20986;&#30340;&#23884;&#20837;&#27169;&#22411;&#30340;&#20248;&#21183;&#36824;&#36890;&#36807;&#23545;&#21512;&#25104;&#32593;&#32476;&#21644;&#22269;&#38469;&#20851;&#31995;&#32593;&#32476;&#36827;&#34892;&#24191;&#27867;&#30340;&#25968;&#20540;&#23454;&#39564;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Signed networks are frequently observed in real life with additional sign information associated with each edge, yet such information has been largely ignored in existing network models. This paper develops a unified embedding model for signed networks to disentangle the intertwined balance structure and anomaly effect, which can greatly facilitate the downstream analysis, including community detection, anomaly detection, and network inference. The proposed model captures both balance structure and anomaly effect through a low rank plus sparse matrix decomposition, which are jointly estimated via a regularized formulation. Its theoretical guarantees are established in terms of asymptotic consistency and finite-sample probability bounds for network embedding, community detection and anomaly detection. The advantage of the proposed embedding model is also demonstrated through extensive numerical experiments on both synthetic networks and an international relation network.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22806;&#26679;&#26412;&#27169;&#22411;&#35780;&#20272;&#30340;&#35786;&#26029;&#24037;&#20855;&#65292;&#21487;&#20197;&#36890;&#36807;&#26377;&#38480;&#30340;&#26657;&#20934;&#25968;&#25454;&#38598;&#26469;&#34920;&#24449;&#27169;&#22411;&#22312;&#26410;&#26469;&#22806;&#26679;&#26412;&#19978;&#30340;&#25439;&#22833;&#65292;&#24182;&#25552;&#20379;&#20102;&#31616;&#21333;&#26131;&#29992;&#19988;&#26131;&#20110;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;&#35813;&#24037;&#20855;&#21487;&#20197;&#37327;&#21270;&#20998;&#24067;&#36716;&#21464;&#30340;&#24433;&#21709;&#65292;&#20419;&#36827;&#22238;&#24402;&#20998;&#26512;&#65292;&#24110;&#21161;&#23454;&#29616;&#27169;&#22411;&#36873;&#25321;&#21644;&#36229;&#21442;&#25968;&#35843;&#20248;&#12290;</title><link>http://arxiv.org/abs/2206.10982</link><description>&lt;p&gt;
&#29992;&#20110;&#22806;&#26679;&#26412;&#27169;&#22411;&#35780;&#20272;&#30340;&#35786;&#26029;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Diagnostic Tool for Out-of-Sample Model Evaluation. (arXiv:2206.10982v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22806;&#26679;&#26412;&#27169;&#22411;&#35780;&#20272;&#30340;&#35786;&#26029;&#24037;&#20855;&#65292;&#21487;&#20197;&#36890;&#36807;&#26377;&#38480;&#30340;&#26657;&#20934;&#25968;&#25454;&#38598;&#26469;&#34920;&#24449;&#27169;&#22411;&#22312;&#26410;&#26469;&#22806;&#26679;&#26412;&#19978;&#30340;&#25439;&#22833;&#65292;&#24182;&#25552;&#20379;&#20102;&#31616;&#21333;&#26131;&#29992;&#19988;&#26131;&#20110;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;&#35813;&#24037;&#20855;&#21487;&#20197;&#37327;&#21270;&#20998;&#24067;&#36716;&#21464;&#30340;&#24433;&#21709;&#65292;&#20419;&#36827;&#22238;&#24402;&#20998;&#26512;&#65292;&#24110;&#21161;&#23454;&#29616;&#27169;&#22411;&#36873;&#25321;&#21644;&#36229;&#21442;&#25968;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#36866;&#37197;&#24615;&#35780;&#20272;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#20851;&#38190;&#37096;&#20998;&#12290;&#26631;&#20934;&#33539;&#24335;&#26159;&#36890;&#36807;&#23545;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;&#36873;&#25321;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#24179;&#22343;&#65292;&#20197;&#23454;&#29616;&#22312;&#26410;&#26469;&#25968;&#25454;&#19978;&#33719;&#24471;&#23567;&#30340;&#25439;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#26377;&#38480;&#30340;&#26657;&#20934;&#25968;&#25454;&#38598;&#26469;&#34920;&#24449;&#27169;&#22411;&#22312;&#26410;&#26469;&#22806;&#26679;&#26412;&#19978;&#30340;&#25439;&#22833;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#27169;&#22411;&#35786;&#26029;&#24037;&#20855;&#65292;&#22312;&#24369;&#20551;&#35774;&#19979;&#25552;&#20379;&#26377;&#38480;&#26679;&#26412;&#30340;&#20445;&#35777;&#12290;&#35813;&#24037;&#20855;&#31616;&#21333;&#26131;&#29992;&#19988;&#26131;&#20110;&#35299;&#37322;&#12290;&#36890;&#36807;&#23637;&#31034;&#20960;&#20010;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25552;&#20986;&#30340;&#26041;&#27861;&#22914;&#20309;&#37327;&#21270;&#20998;&#24067;&#36716;&#21464;&#30340;&#24433;&#21709;&#65292;&#20419;&#36827;&#22238;&#24402;&#20998;&#26512;&#65292;&#20197;&#21450;&#23454;&#29616;&#27169;&#22411;&#36873;&#25321;&#21644;&#36229;&#21442;&#25968;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
Assessment of model fitness is a key part of machine learning. The standard paradigm is to learn models by minimizing a chosen loss function averaged over training data, with the aim of achieving small losses on future data. In this paper, we consider the use of a finite calibration data set to characterize the future, out-of-sample losses of a model. We propose a simple model diagnostic tool that provides finite-sample guarantees under weak assumptions. The tool is simple to compute and to interpret. Several numerical experiments are presented to show how the proposed method quantifies the impact of distribution shifts, aids the analysis of regression, and enables model selection as well as hyper-parameter tuning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#22810;&#31181;&#20256;&#24863;&#22120;&#25968;&#25454;&#21644;&#22320;&#29699;&#29289;&#29702;&#21464;&#37327;&#65292;&#21487;&#20197;&#20934;&#30830;&#20272;&#31639;&#22303;&#22756;&#39030;&#37096;&#30340;&#20307;&#31215;&#28287;&#24230;&#21547;&#37327;&#12290;&#35813;&#27169;&#22411;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;1300&#20010;&#20256;&#24863;&#22120;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#30456;&#20851;&#24615;&#21644;&#36739;&#23567;&#30340;&#35823;&#24046;&#65292;&#24182;&#21487;&#29992;&#20110;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#30340;&#22303;&#22756;&#28287;&#24230;&#22270;&#12290;</title><link>http://arxiv.org/abs/2206.09649</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22303;&#22756;&#28287;&#24230;&#33719;&#21462;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#34701;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Machine Learning Data Fusion Model for Soil Moisture Retrieval. (arXiv:2206.09649v3 [physics.ao-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#22810;&#31181;&#20256;&#24863;&#22120;&#25968;&#25454;&#21644;&#22320;&#29699;&#29289;&#29702;&#21464;&#37327;&#65292;&#21487;&#20197;&#20934;&#30830;&#20272;&#31639;&#22303;&#22756;&#39030;&#37096;&#30340;&#20307;&#31215;&#28287;&#24230;&#21547;&#37327;&#12290;&#35813;&#27169;&#22411;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;1300&#20010;&#20256;&#24863;&#22120;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#30456;&#20851;&#24615;&#21644;&#36739;&#23567;&#30340;&#35823;&#24046;&#65292;&#24182;&#21487;&#29992;&#20110;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#30340;&#22303;&#22756;&#28287;&#24230;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21367;&#31215;&#22238;&#24402;&#27169;&#22411;&#65292;&#29992;&#20110;&#20272;&#31639;&#22303;&#22756;&#39030;&#37096;&#32422;5&#21400;&#31859;&#22788;&#30340;&#20307;&#31215;&#22303;&#22756;&#28287;&#24230;&#21547;&#37327;&#12290;&#36755;&#20837;&#39044;&#27979;&#22240;&#23376;&#21253;&#25324;Sentinel-1&#65288;&#20027;&#21160;&#38647;&#36798;&#65289;&#12289;Sentinel-2&#65288;&#20809;&#23398;&#22270;&#20687;&#65289;&#21644;SMAP&#65288;&#34987;&#21160;&#38647;&#36798;&#65289;&#65292;&#20197;&#21450;&#26469;&#33258;SoilGrids&#30340;&#22320;&#29699;&#29289;&#29702;&#21464;&#37327;&#21644;&#26469;&#33258;GLDAS&#30340;&#27169;&#25311;&#22303;&#22756;&#28287;&#24230;&#22330;&#12290;&#35813;&#27169;&#22411;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#32422;1300&#20010;&#21407;&#22320;&#20256;&#24863;&#22120;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#24471;&#21040;&#20102;&#24179;&#22343;&#27599;&#20010;&#20256;&#24863;&#22120;&#30456;&#20851;&#31995;&#25968;&#20026;0.727&#21644;ubRMSE&#20026;0.054&#30340;&#32467;&#26524;&#65292;&#24182;&#21487;&#29992;&#20110;&#29983;&#25104;320m&#20998;&#36776;&#29575;&#30340;&#22303;&#22756;&#28287;&#24230;&#22270;&#12290;&#36825;&#20123;&#32467;&#26524;&#19982;&#20854;&#20182;13&#20010;&#22320;&#28857;&#30340;&#22303;&#22756;&#28287;&#24230;&#30740;&#31350;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#20351;&#29992;&#28040;&#20943;&#30740;&#31350;&#26469;&#30830;&#23450;&#37325;&#35201;&#30340;&#39044;&#27979;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a deep learning based convolutional-regression model that estimates the volumetric soil moisture content in the top ~5 cm of soil. Input predictors include Sentinel-1 (active radar), Sentinel-2 (optical imagery), and SMAP (passive radar) as well as geophysical variables from SoilGrids and modelled soil moisture fields from GLDAS. The model was trained and evaluated on data from ~1300 in-situ sensors globally over the period 2015 - 2021 and obtained an average per-sensor correlation of 0.727 and ubRMSE of 0.054, and can be used to produce a soil moisture map at a nominal 320m resolution. These results are benchmarked against 13 other soil moisture works at different locations, and an ablation study was used to identify important predictors.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#30340;&#22797;&#26434;&#26597;&#35810;&#23884;&#20837;&#26041;&#27861;TFLEX&#65292;&#33021;&#22815;&#33258;&#28982;&#22320;&#24314;&#27169;&#25152;&#26377;&#19968;&#38454;&#36923;&#36753;&#65288;FOL&#65289;&#36816;&#31639;&#65292;&#21516;&#26102;&#25193;&#23637;&#20102;&#21521;&#37327;&#36923;&#36753;&#20197;&#22788;&#29702;&#19977;&#20010;&#39069;&#22806;&#30340;&#26102;&#38388;&#36816;&#31639;&#31526;&#12290;</title><link>http://arxiv.org/abs/2205.14307</link><description>&lt;p&gt;
TFLEX: &#26102;&#38388;&#29305;&#24449;&#36923;&#36753;&#23884;&#20837;&#26694;&#26550;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#22797;&#26434;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
TFLEX: Temporal Feature-Logic Embedding Framework for Complex Reasoning over Temporal Knowledge Graph. (arXiv:2205.14307v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#30340;&#22797;&#26434;&#26597;&#35810;&#23884;&#20837;&#26041;&#27861;TFLEX&#65292;&#33021;&#22815;&#33258;&#28982;&#22320;&#24314;&#27169;&#25152;&#26377;&#19968;&#38454;&#36923;&#36753;&#65288;FOL&#65289;&#36816;&#31639;&#65292;&#21516;&#26102;&#25193;&#23637;&#20102;&#21521;&#37327;&#36923;&#36753;&#20197;&#22788;&#29702;&#19977;&#20010;&#39069;&#22806;&#30340;&#26102;&#38388;&#36816;&#31639;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#20013;&#65292;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#22810;&#36339;&#36923;&#36753;&#25512;&#29702;&#21457;&#25381;&#30528;&#22522;&#26412;&#20316;&#29992;&#12290;&#26368;&#36817;&#30340;&#22797;&#26434;&#26597;&#35810;&#23884;&#20837;&#65288;CQE&#65289;&#26041;&#27861;&#20391;&#37325;&#20110;&#38745;&#24577;&#30693;&#35782;&#22270;&#35889;&#65292;&#32780;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#65288;TKG&#65289;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;TKG&#19978;&#30340;&#25512;&#29702;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#65306;1.&#26597;&#35810;&#24212;&#35813;&#22238;&#31572;&#23454;&#20307;&#25110;&#26102;&#38388;&#25139;&#65307;2.&#36816;&#31639;&#31526;&#24212;&#35813;&#21516;&#26102;&#32771;&#34385;&#23454;&#20307;&#38598;&#19978;&#30340;&#38598;&#21512;&#36923;&#36753;&#21644;&#26102;&#38388;&#25139;&#38598;&#19978;&#30340;&#26102;&#38388;&#36923;&#36753;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;TKG&#19978;&#30340;&#22810;&#36339;&#36923;&#36753;&#25512;&#29702;&#38382;&#39064;&#12290;&#36890;&#36807;&#19977;&#20010;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21517;&#20026;TFLEX&#30340;&#26102;&#38388;CQE&#65292;&#29992;&#20110;&#22238;&#31572;&#26102;&#38388;&#22797;&#26434;&#26597;&#35810;&#12290;&#25105;&#20204;&#21033;&#29992;&#21521;&#37327;&#36923;&#36753;&#35745;&#31639;Temporal Feature-Logic&#23884;&#20837;&#30340;&#36923;&#36753;&#37096;&#20998;&#65292;&#20174;&#32780;&#33258;&#28982;&#22320;&#24314;&#27169;&#23454;&#20307;&#38598;&#19978;&#30340;&#25152;&#26377;&#19968;&#38454;&#36923;&#36753;&#65288;FOL&#65289;&#36816;&#31639;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#25193;&#23637;&#26102;&#38388;&#25139;&#38598;&#19978;&#30340;&#21521;&#37327;&#36923;&#36753;&#65292;&#20197;&#22788;&#29702;&#19977;&#20010;&#39069;&#22806;&#30340;&#26102;&#38388;&#36816;&#31639;&#31526;&#65288;After&#65292;Before&#21644;Between&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-hop logical reasoning over knowledge graph (KG) plays a fundamental role in many artificial intelligence tasks. Recent complex query embedding (CQE) methods for reasoning focus on static KGs, while temporal knowledge graphs (TKGs) have not been fully explored. Reasoning over TKGs has two challenges: 1. The query should answer entities or timestamps; 2. The operators should consider both set logic on entity set and temporal logic on timestamp set. To bridge this gap, we define the multi-hop logical reasoning problem on TKGs. With generated three datasets, we propose the first temporal CQE named Temporal Feature-Logic Embedding framework (TFLEX) to answer the temporal complex queries. We utilize vector logic to compute the logic part of Temporal Feature-Logic embeddings, thus naturally modeling all First-Order Logic (FOL) operations on entity set. In addition, our framework extends vector logic on timestamp set to cope with three extra temporal operators (After, Before and Between)
&lt;/p&gt;</description></item><item><title>Speculative Decoding&#26159;&#19968;&#31181;&#26032;&#22411;&#35299;&#30721;&#33539;&#24335;&#65292;&#32467;&#21512;&#20102;&#33258;&#22238;&#24402;&#32763;&#35793;&#65288;AT&#65289;&#21644;&#38750;&#33258;&#22238;&#24402;&#32763;&#35793;&#65288;NAT&#65289;&#30340;&#20248;&#21183;&#65292;&#25552;&#20379;&#20102;&#26080;&#25439;&#21152;&#36895;&#30340;&#32763;&#35793;&#26041;&#27861;&#12290;&#22312;&#27599;&#20010;&#35299;&#30721;&#27493;&#39588;&#20013;&#65292;&#23427;&#25512;&#27979;&#24615;&#22320;&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#65292;&#24182;&#20351;&#29992;&#39564;&#35777;&#27169;&#22411;&#30830;&#20445;&#32763;&#35793;&#32467;&#26524;&#19982;AT&#23436;&#20840;&#30456;&#21516;&#12290;&#36890;&#36807;&#25512;&#27979;&#35299;&#30721;&#21644;&#39564;&#35777;&#30340;&#21327;&#20316;&#65292;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#35299;&#30721;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#32763;&#35793;&#36136;&#37327;&#19981;&#21464;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21407;&#22987;&#30340;SpecDec&#19982;AT&#36138;&#23146;&#35299;&#30721;&#30340;&#32467;&#26524;&#23436;&#20840;&#30456;&#21516;&#12290;</title><link>http://arxiv.org/abs/2203.16487</link><description>&lt;p&gt;
Speculative Decoding: &#26080;&#25439;&#21152;&#36895;&#33258;&#22238;&#24402;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Speculative Decoding: Lossless Speedup of Autoregressive Translation. (arXiv:2203.16487v5 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.16487
&lt;/p&gt;
&lt;p&gt;
Speculative Decoding&#26159;&#19968;&#31181;&#26032;&#22411;&#35299;&#30721;&#33539;&#24335;&#65292;&#32467;&#21512;&#20102;&#33258;&#22238;&#24402;&#32763;&#35793;&#65288;AT&#65289;&#21644;&#38750;&#33258;&#22238;&#24402;&#32763;&#35793;&#65288;NAT&#65289;&#30340;&#20248;&#21183;&#65292;&#25552;&#20379;&#20102;&#26080;&#25439;&#21152;&#36895;&#30340;&#32763;&#35793;&#26041;&#27861;&#12290;&#22312;&#27599;&#20010;&#35299;&#30721;&#27493;&#39588;&#20013;&#65292;&#23427;&#25512;&#27979;&#24615;&#22320;&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#65292;&#24182;&#20351;&#29992;&#39564;&#35777;&#27169;&#22411;&#30830;&#20445;&#32763;&#35793;&#32467;&#26524;&#19982;AT&#23436;&#20840;&#30456;&#21516;&#12290;&#36890;&#36807;&#25512;&#27979;&#35299;&#30721;&#21644;&#39564;&#35777;&#30340;&#21327;&#20316;&#65292;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#35299;&#30721;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#32763;&#35793;&#36136;&#37327;&#19981;&#21464;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21407;&#22987;&#30340;SpecDec&#19982;AT&#36138;&#23146;&#35299;&#30721;&#30340;&#32467;&#26524;&#23436;&#20840;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20043;&#21069;&#19968;&#20123;&#29306;&#29298;&#32763;&#35793;&#36136;&#37327;&#21152;&#36895;&#33258;&#22238;&#24402;&#32763;&#35793;&#65288;AT&#65289;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Speculative Decoding&#65288;SpecDec&#65289;-&#19968;&#31181;&#21463;&#35745;&#31639;&#26426;&#20307;&#31995;&#32467;&#26500;&#20013;&#30340;&#25512;&#27979;&#25191;&#34892;&#21551;&#21457;&#30340;&#26032;&#22411;&#35299;&#30721;&#33539;&#24335;&#65292;&#23427;&#32467;&#21512;&#20102;AT&#21644;&#38750;&#33258;&#22238;&#24402;&#32763;&#35793;&#65288;NAT&#65289;&#30340;&#21508;&#33258;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#22312;&#32763;&#35793;&#36807;&#31243;&#20013;&#30340;&#26080;&#25439;&#21152;&#36895;&#12290;&#22312;&#27599;&#20010;&#35299;&#30721;&#27493;&#39588;&#20013;&#65292;SpecDec&#39318;&#20808;&#20351;&#29992;NAT&#27169;&#22411;&#25512;&#27979;&#24615;&#22320;&#39044;&#27979;&#65288;&#21363;&#35299;&#30721;&#65289;&#19979;&#19968;&#20010;k&#20010;&#26631;&#35760;&#65292;&#28982;&#21518;&#20351;&#29992;AT&#27169;&#22411;&#39564;&#35777;&#36825;&#20123;&#26631;&#35760;&#65292;&#21482;&#26377;&#36890;&#36807;&#39564;&#35777;&#30340;&#39044;&#27979;&#26631;&#35760;&#25165;&#20250;&#34987;&#25509;&#21463;&#20316;&#20026;&#35299;&#30721;&#32467;&#26524;&#65292;&#20197;&#30830;&#20445;&#20854;&#32763;&#35793;&#32467;&#26524;&#19982;AT&#23436;&#20840;&#30456;&#21516;&#12290;NAT&#30340;&#25512;&#27979;&#21644;AT&#30340;&#39564;&#35777;&#20043;&#38388;&#30340;&#21327;&#20316;&#20351;&#24471;&#35299;&#30721;&#36895;&#24230;&#22823;&#22823;&#25552;&#39640;&#65292;&#21516;&#26102;&#19981;&#25439;&#22833;&#32763;&#35793;&#36136;&#37327;&#65292;&#36825;&#26159;&#30001;&#20110;&#25512;&#27979;&#35299;&#30721;&#25152;&#25903;&#25345;&#30340;&#24182;&#34892;&#35745;&#31639;&#12290;&#25105;&#20204;&#22312;4&#20010;&#26631;&#20934;WMT&#32763;&#35793;&#22522;&#20934;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#35777;&#23454;&#21407;&#22987;&#30340;SpecDec&#19982;AT&#36138;&#23146;&#35299;&#30721;&#30340;&#32467;&#26524;&#23436;&#20840;&#30456;&#21516;&#65292;&#36895;&#24230;&#25552;&#39640;&#20102;&#32422; $k$&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Different from some previous work accelerating autoregressive translation (AT) at the sacrifice of quality, we propose Speculative Decoding (SpecDec) -a novel decoding paradigm inspired by speculative execution in computer architecture, which combines respective advantages of AT and non-autoregressive translation (NAT) for lossless speedup of translation. At each decoding step, SpecDec first speculatively drafts (i.e. decodes) next $k$ tokens with an NAT model and then verifies them with an AT model, where only the drafted tokens passing the verification are accepted as decoded tokens for guaranteeing its translation result is exactly the same as AT. The collaboration of NAT drafting and AT verification leads to a much higher decoding speed without quality loss due to parallel computing enabled by speculative decoding.  We conduct experiments in 4 standard WMT translation benchmarks and confirm the vanilla SpecDec yields exactly the same results as AT greedy decoding with an around $
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22870;&#21169;&#22609;&#36896;&#26041;&#27861;&#30340;&#20004;&#31181;&#20462;&#25913;&#65292;&#19968;&#31181;&#37319;&#29992;&#20808;&#36827;&#30340;&#32858;&#21512;&#20989;&#25968;&#65292;&#21478;&#19968;&#31181;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#22312;&#19977;&#32500;&#29615;&#22659;&#23548;&#33322;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2203.13424</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#31232;&#30095;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;
Dealing with Sparse Rewards Using Graph Neural Networks. (arXiv:2203.13424v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.13424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22870;&#21169;&#22609;&#36896;&#26041;&#27861;&#30340;&#20004;&#31181;&#20462;&#25913;&#65292;&#19968;&#31181;&#37319;&#29992;&#20808;&#36827;&#30340;&#32858;&#21512;&#20989;&#25968;&#65292;&#21478;&#19968;&#31181;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#22312;&#19977;&#32500;&#29615;&#22659;&#23548;&#33322;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26412;&#36523;&#23601;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#24403;&#22870;&#21169;&#20449;&#21495;&#31232;&#30095;&#26102;&#26356;&#21152;&#22797;&#26434;&#12290;&#22823;&#22810;&#25968;&#28041;&#21450;&#19977;&#32500;&#29615;&#22659;&#23548;&#33322;&#30340;&#20219;&#21153;&#21482;&#25552;&#20379;&#26377;&#38480;&#30340;&#20449;&#24687;&#32473;&#26234;&#33021;&#20307;&#12290;&#26222;&#36941;&#24773;&#20917;&#19979;&#65292;&#26234;&#33021;&#20307;&#20174;&#29615;&#22659;&#20013;&#25509;&#25910;&#21040;&#19968;&#20010;&#35270;&#35273;&#35266;&#23519;&#36755;&#20837;&#65292;&#24182;&#22312;&#19968;&#38598;&#32467;&#26463;&#26102;&#24471;&#21040;&#22870;&#21169;&#12290;&#33391;&#22909;&#30340;&#22870;&#21169;&#20989;&#25968;&#21487;&#20197;&#22823;&#22823;&#25913;&#21892;&#36825;&#31867;&#20219;&#21153;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;&#22686;&#21152;&#22870;&#21169;&#20449;&#21495;&#23494;&#24230;&#30340;&#32463;&#20856;&#26041;&#27861;&#26159;&#29992;&#34917;&#20805;&#22870;&#21169;&#26469;&#25913;&#21892;&#22870;&#21169;&#12290;&#36825;&#31181;&#25216;&#26415;&#34987;&#31216;&#20026;&#22870;&#21169;&#22609;&#36896;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25913;&#36827;&#26368;&#36817;&#19968;&#31181;&#22522;&#20110;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#22870;&#21169;&#22609;&#36896;&#26041;&#27861;&#30340;&#20462;&#25913;&#65306;&#19968;&#31181;&#28041;&#21450;&#20808;&#36827;&#30340;&#32858;&#21512;&#20989;&#25968;&#65292;&#21478;&#19968;&#31181;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#35299;&#20915;&#26041;&#26696;&#22312;&#19977;&#32500;&#29615;&#22659;&#23548;&#33322;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning in partially observable environments is a difficult task in itself, and can be further complicated by a sparse reward signal. Most tasks involving navigation in three-dimensional environments provide the agent with extremely limited information. Typically, the agent receives a visual observation input from the environment and is rewarded once at the end of the episode. A good reward function could substantially improve the convergence of reinforcement learning algorithms for such tasks. The classic approach to increase the density of the reward signal is to augment it with supplementary rewards. This technique is called the reward shaping. In this study, we propose two modifications of one of the recent reward shaping methods based on graph convolutional networks: the first involving advanced aggregation functions, and the second utilizing the attention mechanism. We empirically validate the effectiveness of our solutions for the task of navigation in a 3D e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#22522;&#20110;&#26143;&#24418;&#32454;&#32990;&#20316;&#29992;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#31361;&#35302;&#30340;&#31454;&#20105;&#21644;&#24378;&#24230;&#24179;&#34913;&#23454;&#29616;&#29616;&#26377;&#21644;&#35760;&#24518;&#24615;&#30340;&#22823;&#33041;&#21487;&#22609;&#24615;&#21644;&#31361;&#35302;&#24418;&#25104;&#65292;&#24182;&#25506;&#35752;&#20102;&#19982;&#20851;&#38190;&#26399;&#30456;&#20851;&#30340;&#31070;&#32463;&#32010;&#20081;&#21644;&#36127;&#38754;&#21644;&#27491;&#38754;&#35760;&#24518;&#30340;&#25345;&#20037;&#24615;&#23545;&#31361;&#35302;&#28608;&#27963;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2203.11740</link><description>&lt;p&gt;
&#22522;&#20110;&#26143;&#24418;&#32454;&#32990;&#23545;&#20851;&#38190;&#26399;&#30340;&#31070;&#32463;&#21487;&#22609;&#24615;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#29616;&#26377;&#21644;&#35760;&#24518;&#24615;&#30340;&#22823;&#33041;&#21487;&#22609;&#24615;&#21644;&#31361;&#35302;&#24418;&#25104;&#23454;&#29616;&#31361;&#35302;&#31454;&#20105;&#21644;&#24378;&#24230;&#24179;&#34913;&#12290;&#65288;arXiv: 2203.11740v12 [cs.NE] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
Plasticity Neural Network Based on Astrocytic effects at Critical Period, Synaptic Competition and Strength Rebalance by Current and Mnemonic Brain Plasticity and Synapse Formation. (arXiv:2203.11740v12 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.11740
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#22522;&#20110;&#26143;&#24418;&#32454;&#32990;&#20316;&#29992;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#31361;&#35302;&#30340;&#31454;&#20105;&#21644;&#24378;&#24230;&#24179;&#34913;&#23454;&#29616;&#29616;&#26377;&#21644;&#35760;&#24518;&#24615;&#30340;&#22823;&#33041;&#21487;&#22609;&#24615;&#21644;&#31361;&#35302;&#24418;&#25104;&#65292;&#24182;&#25506;&#35752;&#20102;&#19982;&#20851;&#38190;&#26399;&#30456;&#20851;&#30340;&#31070;&#32463;&#32010;&#20081;&#21644;&#36127;&#38754;&#21644;&#27491;&#38754;&#35760;&#24518;&#30340;&#25345;&#20037;&#24615;&#23545;&#31361;&#35302;&#28608;&#27963;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38500;&#20102;&#31361;&#35302;&#20849;&#20139;&#36830;&#25509;&#26435;&#37325;&#20043;&#22806;&#65292;PNN&#36824;&#21253;&#25324;&#31361;&#35302;&#26377;&#25928;&#33539;&#22260;&#30340;&#26435;&#37325;[14-25]&#12290;PNN&#32771;&#34385;&#31361;&#35302;&#24378;&#24230;&#24179;&#34913;&#22312;&#31361;&#35302;&#21534;&#22124;&#30340;&#21160;&#24577;&#21644;&#38271;&#24230;&#24120;&#25968;&#20043;&#21644;&#30340;&#38745;&#24577;&#20013;[14]&#65292;&#24182;&#21253;&#21547;&#20102;&#40060;&#32676;&#34892;&#20026;&#30340;&#20808;&#23548;&#34892;&#20026;&#12290;&#31361;&#35302;&#24418;&#25104;&#22312;&#23454;&#39564;&#21644;&#27169;&#25311;&#20013;&#20250;&#25233;&#21046;&#26641;&#31361;&#29983;&#25104;[15]&#12290;&#31867;&#20284;&#20110;Spring Boot&#20013;&#30340;&#24378;&#21046;&#38887;&#24615;&#65292;&#21453;&#21521;&#22238;&#36335;&#30340;&#35760;&#24518;&#25345;&#20037;&#24230;&#26799;&#24230;&#20063;&#23384;&#22312;&#12290;&#30456;&#23545;&#36739;&#22909;&#21644;&#36739;&#24046;&#30340;&#26799;&#24230;&#20449;&#24687;&#23384;&#20648;&#22312;&#31867;&#20284;&#20110;&#33041;&#35126;&#30340;&#35760;&#24518;&#30165;&#36857;&#32454;&#32990;&#20013;&#65292;&#22312;&#21453;&#21521;&#22238;&#36335;&#30340;&#31361;&#35302;&#24418;&#25104;&#20013;&#12290;&#20105;&#35758;&#35748;&#20026;&#20154;&#31867;&#28023;&#39532;&#31070;&#32463;&#20803;&#30340;&#20877;&#29983;&#33021;&#21147;&#26159;&#21542;&#25345;&#32493;&#21040;&#32769;&#24180;&#65292;&#24182;&#21487;&#33021;&#22312;&#21518;&#26399;&#36845;&#20195;&#20013;&#24418;&#25104;&#26032;&#30340;&#26356;&#38271;&#30340;&#22238;&#36335;[17,18]&#12290;&#20851;&#38381;&#20851;&#38190;&#26399;&#20250;&#23548;&#33268;&#31070;&#32463;&#32010;&#20081;&#22312;&#23454;&#39564;&#21644;&#27169;&#25311;&#20013;[19]&#12290;&#32771;&#34385;&#21040;&#36127;&#38754;&#21644;&#27491;&#38754;&#35760;&#24518;&#30340;&#25345;&#20037;&#24615;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#28608;&#27963;&#31361;&#35302;&#12290;
&lt;/p&gt;
&lt;p&gt;
In addition to the weights of synaptic shared connections, PNN includes weights of synaptic effective ranges [14-25]. PNN considers synaptic strength balance in dynamic of phagocytosing of synapses and static of constant sum of synapses length [14], and includes the lead behavior of the school of fish. Synapse formation will inhibit dendrites generation in experiments and simulations [15]. The memory persistence gradient of retrograde circuit similar to the Enforcing Resilience in a Spring Boot. The relatively good and inferior gradient information stored in memory engram cells in synapse formation of retrograde circuit like the folds in brain [16]. The controversy was claimed if human hippocampal neurogenesis persists throughout aging, may have a new and longer circuit in late iteration [17,18]. Closing the critical period will cause neurological disorder in experiments and simulations [19]. Considering both negative and positive memories persistence help activate synapse better than 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#38598;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;GRADCAM&#30340;&#33016;&#37096;X&#20809;&#30142;&#30149;&#22810;&#26631;&#31614;&#35786;&#26029;&#27169;&#22411;&#65292;&#33719;&#24471;&#20102;&#22312;Cardiomegaly&#26465;&#20214;&#19979;&#26368;&#39640;&#30340;AUC&#24471;&#20998;0.896&#65292;&#24182;&#20351;&#29992;&#28909;&#22270;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2202.03583</link><description>&lt;p&gt;
&#22522;&#20110;&#23494;&#38598;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#33016;&#37096;&#30142;&#30149;&#22810;&#26631;&#31614;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-Label Classification of Thoracic Diseases using Dense Convolutional Network on Chest Radiographs. (arXiv:2202.03583v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#38598;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;GRADCAM&#30340;&#33016;&#37096;X&#20809;&#30142;&#30149;&#22810;&#26631;&#31614;&#35786;&#26029;&#27169;&#22411;&#65292;&#33719;&#24471;&#20102;&#22312;Cardiomegaly&#26465;&#20214;&#19979;&#26368;&#39640;&#30340;AUC&#24471;&#20998;0.896&#65292;&#24182;&#20351;&#29992;&#28909;&#22270;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;X&#20809;&#22270;&#20687;&#30149;&#29702;&#35782;&#21035;&#26041;&#27861;&#20381;&#36182;&#20110;&#29087;&#32451;&#30340;&#20154;&#31867;&#35299;&#37322;&#65292;&#24182;&#19988;&#24448;&#24448;&#32791;&#26102;&#12290;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#20986;&#29616;&#20351;&#33258;&#21160;&#35786;&#26029;&#31995;&#32479;&#30340;&#24320;&#21457;&#25104;&#20026;&#21487;&#33021;&#65292;&#20294;&#36825;&#31867;&#31995;&#32479;&#30340;&#34920;&#29616;&#21462;&#20915;&#20110;&#27169;&#22411;&#30340;&#36136;&#37327;&#21644;&#23427;&#25552;&#20379;&#30340;&#21487;&#35299;&#37322;&#24615;&#27700;&#24179;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23494;&#38598;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;DenseNet&#65289;&#21644;GRADCAM&#36827;&#34892;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#33016;&#37096;X&#20809;&#30142;&#30149;&#22810;&#26631;&#31614;&#35786;&#26029;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#21069;&#32622;X&#20809;&#35757;&#32451;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#23450;&#37327;&#25351;&#26631;&#65288;&#21253;&#25324;&#21463;&#35797;&#32773;&#25805;&#20316;&#29305;&#24449;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#65289;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;Cardiomegaly&#26465;&#20214;&#19979;&#36798;&#21040;&#20102;&#26368;&#39640;&#30340;AUC&#24471;&#20998;0.896&#65292;&#24182;&#33719;&#24471;&#20102;0.826&#30340;&#20934;&#30830;&#24230;&#12290;&#32780;&#22312;Nodule&#26465;&#20214;&#19979;&#33719;&#24471;&#20102;&#26368;&#20302;&#30340;AUC&#24471;&#20998;0.655&#65292;&#20934;&#30830;&#24230;&#20026;0.66&#12290;&#20026;&#20102;&#25552;&#39640;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#20915;&#31574;&#26041;&#38754;&#24314;&#31435;&#20449;&#20219;&#65292;&#25105;&#20204;&#20351;&#29992;GRADCAM&#29983;&#25104;&#20102;&#28909;&#22270;&#65292;&#31361;&#20986;&#26174;&#31034;&#20102;&#23545;&#35786;&#26029;&#26368;&#37325;&#35201;&#30340;X&#20809;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional methods of identifying pathologies in X-ray images rely heavily on skilled human interpretation and are often time-consuming. The advent of deep learning techniques has enabled the development of automated disease diagnosis systems, but the performance of such systems is dependent on the quality of the model and the level of interpretability it provides. In this paper, we propose a multi-label disease diagnosis model for chest X-rays using a dense convolutional neural network (DenseNet) and model interpretability using GRADCAM. We trained our model using frontal X-rays and evaluated its performance using various quantitative metrics, including the area under the receiver operating characteristic curve (AUC). Our proposed model achieved the highest AUC score of 0.896 for the condition Cardiomegaly with an accuracy of 0.826, while the lowest AUC score was obtained for Nodule, at 0.655 with an accuracy of 0.66. To promote model interpretability and build trust in decision maki
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#20107;&#23454;&#35760;&#24518;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#25551;&#36848;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#24573;&#30053;&#20102;&#21738;&#20123;&#29305;&#23450;&#25991;&#26723;&#30340;&#39044;&#27979;&#21464;&#21270;&#12290;&#36890;&#36807;&#30740;&#31350;&#26631;&#20934;&#25991;&#26412;&#25968;&#25454;&#38598;&#20013;&#30340;&#21453;&#20107;&#23454;&#35760;&#24518;&#35757;&#32451;&#26679;&#26412;&#65292;&#25105;&#20204;&#21487;&#20197;&#20272;&#35745;&#27599;&#20010;&#35760;&#24518;&#26679;&#26412;&#23545;&#39564;&#35777;&#38598;&#21644;&#29983;&#25104;&#25991;&#26412;&#30340;&#24433;&#21709;&#65292;&#24182;&#30452;&#25509;&#25552;&#20379;&#35760;&#24518;&#26469;&#28304;&#30340;&#35777;&#25454;&#12290;</title><link>http://arxiv.org/abs/2112.12938</link><description>&lt;p&gt;
&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21453;&#20107;&#23454;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Memorization in Neural Language Models. (arXiv:2112.12938v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.12938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#20107;&#23454;&#35760;&#24518;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#25551;&#36848;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#24573;&#30053;&#20102;&#21738;&#20123;&#29305;&#23450;&#25991;&#26723;&#30340;&#39044;&#27979;&#21464;&#21270;&#12290;&#36890;&#36807;&#30740;&#31350;&#26631;&#20934;&#25991;&#26412;&#25968;&#25454;&#38598;&#20013;&#30340;&#21453;&#20107;&#23454;&#35760;&#24518;&#35757;&#32451;&#26679;&#26412;&#65292;&#25105;&#20204;&#21487;&#20197;&#20272;&#35745;&#27599;&#20010;&#35760;&#24518;&#26679;&#26412;&#23545;&#39564;&#35777;&#38598;&#21644;&#29983;&#25104;&#25991;&#26412;&#30340;&#24433;&#21709;&#65292;&#24182;&#30452;&#25509;&#25552;&#20379;&#35760;&#24518;&#26469;&#28304;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#29616;&#20195;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#35760;&#24518;&#25935;&#24863;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;&#29702;&#35299;&#36825;&#31181;&#35760;&#24518;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#21644;&#23398;&#20064;&#29702;&#35770;&#30340;&#35282;&#24230;&#37117;&#24456;&#37325;&#35201;&#12290;&#22312;&#20808;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#35760;&#24518;&#30740;&#31350;&#20013;&#23384;&#22312;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#36807;&#28388;&#25481;&#8220;&#24120;&#35265;&#8221;&#35760;&#24518;&#12290;&#20107;&#23454;&#19978;&#65292;&#22823;&#22810;&#25968;&#35760;&#24518;&#26631;&#20934;&#19982;&#22312;&#35757;&#32451;&#38598;&#20013;&#20986;&#29616;&#30340;&#27425;&#25968;&#24378;&#28872;&#30456;&#20851;&#65292;&#25429;&#25417;&#21040;&#24120;&#35265;&#30701;&#35821;&#12289;&#20844;&#20849;&#30693;&#35782;&#12289;&#27169;&#26495;&#21270;&#25991;&#26412;&#25110;&#20854;&#20182;&#37325;&#22797;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#20107;&#23454;&#35760;&#24518;&#30340;&#27010;&#24565;&#65292;&#25551;&#36848;&#20102;&#27169;&#22411;&#30340;&#39044;&#27979;&#22312;&#30465;&#30053;&#29305;&#23450;&#25991;&#26723;&#36827;&#34892;&#35757;&#32451;&#26102;&#22914;&#20309;&#25913;&#21464;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#25991;&#26412;&#25968;&#25454;&#38598;&#20013;&#30830;&#23450;&#24182;&#30740;&#31350;&#20102;&#21453;&#20107;&#23454;&#35760;&#24518;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;&#25105;&#20204;&#20272;&#35745;&#20102;&#27599;&#20010;&#35760;&#24518;&#35757;&#32451;&#26679;&#26412;&#23545;&#39564;&#35777;&#38598;&#21644;&#29983;&#25104;&#25991;&#26412;&#30340;&#24433;&#21709;&#65292;&#23637;&#31034;&#20102;&#36825;&#22914;&#20309;&#30452;&#25509;&#25552;&#20379;&#35760;&#24518;&#26469;&#28304;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern neural language models that are widely used in various NLP tasks risk memorizing sensitive information from their training data. Understanding this memorization is important in real world applications and also from a learning-theoretical perspective. An open question in previous studies of language model memorization is how to filter out "common" memorization. In fact, most memorization criteria strongly correlate with the number of occurrences in the training set, capturing memorized familiar phrases, public knowledge, templated texts, or other repeated data. We formulate a notion of counterfactual memorization which characterizes how a model's predictions change if a particular document is omitted during training. We identify and study counterfactually-memorized training examples in standard text datasets. We estimate the influence of each memorized training example on the validation set and on generated texts, showing how this can provide direct evidence of the source of memo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#39640;&#38454;&#24352;&#37327;&#27744;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#29305;&#24449;&#20540;&#24130;&#24402;&#19968;&#21270;&#65288;EPN&#65289;&#26469;&#38450;&#27490;&#29190;&#21457;&#29616;&#35937;&#24182;&#25552;&#39640;&#21160;&#20316;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2110.05216</link><description>&lt;p&gt;
&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#30340;&#39640;&#38454;&#24352;&#37327;&#27744;&#21270;&#22312;&#21160;&#20316;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
High-order Tensor Pooling with Attention for Action Recognition. (arXiv:2110.05216v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.05216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#39640;&#38454;&#24352;&#37327;&#27744;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#29305;&#24449;&#20540;&#24130;&#24402;&#19968;&#21270;&#65288;EPN&#65289;&#26469;&#38450;&#27490;&#29190;&#21457;&#29616;&#35937;&#24182;&#25552;&#39640;&#21160;&#20316;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#39640;&#38454;&#24352;&#37327;&#27744;&#21270;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25429;&#25417;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#30340;&#29305;&#24449;&#21521;&#37327;&#30340;&#39640;&#38454;&#32479;&#35745;&#20449;&#24687;&#65292;&#24418;&#25104;&#24352;&#37327;&#25551;&#36848;&#31526;&#12290;&#24352;&#37327;&#25551;&#36848;&#31526;&#35201;&#27714;&#20855;&#22791;&#40065;&#26834;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;&#20197;&#24212;&#23545;&#32858;&#21512;&#21521;&#37327;&#25968;&#37327;&#36739;&#23569;&#21644;&#29190;&#21457;&#29616;&#35937;&#65292;&#21363;&#26576;&#20123;&#29305;&#24449;&#20986;&#29616;&#30340;&#39057;&#29575;&#39640;&#20110;&#25110;&#20302;&#20110;&#32479;&#35745;&#39044;&#26399;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#23558;&#22270;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#19978;&#30340;&#28909;&#25193;&#25955;&#36807;&#31243;&#19982;&#21327;&#26041;&#24046;/&#33258;&#30456;&#20851;&#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#24130;&#24402;&#19968;&#21270;&#65288;EPN&#65289;&#23494;&#20999;&#30456;&#20851;&#65292;&#20854;&#36870;&#24418;&#25104;&#20102;&#19968;&#20010;&#29615;&#29366;&#22270;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#28909;&#25193;&#25955;&#36807;&#31243;&#19982;EPN&#20855;&#26377;&#30456;&#21516;&#30340;&#20316;&#29992;&#65292;&#21363;&#22686;&#24378;&#25110;&#20943;&#24369;&#29305;&#24449;&#20540;&#35889;&#30340;&#24133;&#24230;&#65292;&#20174;&#32780;&#38450;&#27490;&#29190;&#21457;&#29616;&#35937;&#12290;&#25105;&#20204;&#23558;&#39640;&#38454;&#24352;&#37327;&#37197;&#22791;&#20102;EPN&#65292;&#23427;&#21487;&#20197;&#20316;&#20026;&#39640;&#38454;&#20986;&#29616;&#30340;&#35889;&#26816;&#27979;&#22120;&#65292;&#20197;&#38450;&#27490;&#29190;&#21457;&#29616;&#35937;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;&#23545;&#20110;&#19968;&#20010;&#30001;d&#32500;&#29305;&#24449;&#25551;&#36848;&#31526;&#26500;&#24314;&#30340;&#38454;&#25968;&#20026;r&#30340;&#24352;&#37327;&#65292;&#36825;&#26679;&#30340;&#26816;&#27979;&#22120;&#21487;&#20197;&#32473;&#20986;&#33267;&#23569;&#23384;&#22312;&#19968;&#20010;&#39640;&#38454;&#20986;&#29616;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We aim at capturing high-order statistics of feature vectors formed by a neural network, and propose end-to-end second- and higher-order pooling to form a tensor descriptor. Tensor descriptors require a robust similarity measure due to low numbers of aggregated vectors and the burstiness phenomenon, when a given feature appears more/less frequently than statistically expected. The Heat Diffusion Process (HDP) on a graph Laplacian is closely related to the Eigenvalue Power Normalization (EPN) of the covariance/auto-correlation matrix, whose inverse forms a loopy graph Laplacian. We show that the HDP and the EPN play the same role, i.e., to boost or dampen the magnitude of the eigenspectrum thus preventing the burstiness. We equip higher-order tensors with EPN which acts as a spectral detector of higher-order occurrences to prevent burstiness. We also prove that for a tensor of order r built from d dimensional feature descriptors, such a detector gives the likelihood if at least one high
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21457;&#29616;&#20102;&#23545;&#25239;&#35757;&#32451;&#20013;&#23384;&#22312;&#30340;&#26631;&#31614;&#22122;&#22768;&#65292;&#24182;&#35299;&#37322;&#20102;&#20854;&#23545;&#40065;&#26834;&#36807;&#24230;&#25311;&#21512;&#30340;&#26222;&#36941;&#23384;&#22312;&#20197;&#21450;&#25200;&#21160;&#21322;&#24452;&#21644;&#25968;&#25454;&#36136;&#37327;&#30340;&#20381;&#36182;&#24615;&#12290;&#36890;&#36807;&#35813;&#35770;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#26657;&#20934;&#26631;&#31614;&#20197;&#24212;&#23545;&#26631;&#31614;&#22122;&#22768;&#21644;&#40065;&#26834;&#36807;&#24230;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2110.03135</link><description>&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#26631;&#31614;&#22122;&#22768;&#65306;&#30740;&#31350;&#40065;&#26834;&#36807;&#24230;&#25311;&#21512;&#30340;&#26032;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Label Noise in Adversarial Training: A Novel Perspective to Study Robust Overfitting. (arXiv:2110.03135v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.03135
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21457;&#29616;&#20102;&#23545;&#25239;&#35757;&#32451;&#20013;&#23384;&#22312;&#30340;&#26631;&#31614;&#22122;&#22768;&#65292;&#24182;&#35299;&#37322;&#20102;&#20854;&#23545;&#40065;&#26834;&#36807;&#24230;&#25311;&#21512;&#30340;&#26222;&#36941;&#23384;&#22312;&#20197;&#21450;&#25200;&#21160;&#21322;&#24452;&#21644;&#25968;&#25454;&#36136;&#37327;&#30340;&#20381;&#36182;&#24615;&#12290;&#36890;&#36807;&#35813;&#35770;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#26657;&#20934;&#26631;&#31614;&#20197;&#24212;&#23545;&#26631;&#31614;&#22122;&#22768;&#21644;&#40065;&#26834;&#36807;&#24230;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#23384;&#22312;&#26631;&#31614;&#22122;&#22768;&#12290;&#36825;&#31181;&#26631;&#31614;&#22122;&#22768;&#26159;&#30001;&#20110;&#23545;&#25239;&#26679;&#26412;&#30340;&#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#19982;&#20174;&#24178;&#20928;&#26679;&#26412;&#32487;&#25215;&#30340;&#26631;&#31614;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#36896;&#25104;&#30340; - &#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#34987;&#23545;&#25239;&#25200;&#21160;&#25197;&#26354;&#65292;&#20294;&#20174;&#24178;&#20928;&#26679;&#26412;&#32487;&#25215;&#26631;&#31614;&#30340;&#24120;&#35265;&#20570;&#27861;&#21364;&#24573;&#30053;&#20102;&#36825;&#19968;&#28857;&#12290;&#35748;&#35782;&#21040;&#26631;&#31614;&#22122;&#22768;&#26377;&#21161;&#20110;&#27934;&#23519;&#23545;&#25239;&#35757;&#32451;&#20013;&#40065;&#26834;&#36807;&#24230;&#25311;&#21512;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#24182;&#35299;&#37322;&#20102;&#20854;&#23545;&#25200;&#21160;&#21322;&#24452;&#21644;&#25968;&#25454;&#36136;&#37327;&#30340;&#22855;&#29305;&#20381;&#36182;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26631;&#31614;&#22122;&#22768;&#35270;&#35282;&#19982;&#25105;&#20204;&#23545;&#23545;&#25239;&#35757;&#32451;&#20013;&#32426;&#20803;&#21452;&#19979;&#38477;&#29616;&#35937;&#30340;&#35266;&#23519;&#30456;&#21563;&#21512;&#12290;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#33258;&#21160;&#26657;&#20934;&#26631;&#31614;&#20197;&#24212;&#23545;&#26631;&#31614;&#22122;&#22768;&#21644;&#40065;&#26834;&#36807;&#24230;&#25311;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19981;&#24341;&#20837;&#26032;&#30340;&#36229;&#21442;&#25968;&#25110;&#39069;&#22806;&#30340;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that label noise exists in adversarial training. Such label noise is due to the mismatch between the true label distribution of adversarial examples and the label inherited from clean examples - the true label distribution is distorted by the adversarial perturbation, but is neglected by the common practice that inherits labels from clean examples. Recognizing label noise sheds insights on the prevalence of robust overfitting in adversarial training, and explains its intriguing dependence on perturbation radius and data quality. Also, our label noise perspective aligns well with our observations of the epoch-wise double descent in adversarial training. Guided by our analyses, we proposed a method to automatically calibrate the label to address the label noise and robust overfitting. Our method achieves consistent performance improvements across various models and datasets without introducing new hyper-parameters or additional tuning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27969;&#24418;&#24863;&#30693;&#28145;&#24230;&#32858;&#31867;&#65288;M-DC&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#22522;&#20110;&#27491;&#21017;&#31616;&#21333;&#24418;&#20307;&#30340;&#21807;&#19968;&#25439;&#22833;&#20989;&#25968;&#65292;&#26368;&#22823;&#21270;&#36229;&#31354;&#38388;&#20013;&#30446;&#26631;&#35282;&#24230;&#65292;&#20174;&#32780;&#22312;&#25552;&#39640;&#32858;&#31867;&#24615;&#33021;&#26102;&#26356;&#26377;&#25928;&#22320;&#22686;&#24378;&#20102;&#36229;&#31354;&#38388;&#21033;&#29992;&#29575;&#12290;</title><link>http://arxiv.org/abs/2106.02331</link><description>&lt;p&gt;
&#22522;&#20110;&#27491;&#21017;&#31616;&#21333;&#24418;&#20307;&#30340;&#27969;&#24418;&#24863;&#30693;&#28145;&#24230;&#32858;&#31867;: &#26368;&#22823;&#21270;&#23884;&#20837;&#21521;&#37327;&#20043;&#38388;&#30340;&#35282;&#24230;
&lt;/p&gt;
&lt;p&gt;
Manifold-Aware Deep Clustering: Maximizing Angles between Embedding Vectors Based on Regular Simplex. (arXiv:2106.02331v3 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.02331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27969;&#24418;&#24863;&#30693;&#28145;&#24230;&#32858;&#31867;&#65288;M-DC&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#22522;&#20110;&#27491;&#21017;&#31616;&#21333;&#24418;&#20307;&#30340;&#21807;&#19968;&#25439;&#22833;&#20989;&#25968;&#65292;&#26368;&#22823;&#21270;&#36229;&#31354;&#38388;&#20013;&#30446;&#26631;&#35282;&#24230;&#65292;&#20174;&#32780;&#22312;&#25552;&#39640;&#32858;&#31867;&#24615;&#33021;&#26102;&#26356;&#26377;&#25928;&#22320;&#22686;&#24378;&#20102;&#36229;&#31354;&#38388;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#32858;&#31867;&#65288;DC&#65289;&#26041;&#27861;&#65292;&#31216;&#20026;&#27969;&#24418;&#24863;&#30693;&#28145;&#24230;&#32858;&#31867;&#65288;M-DC&#65289;&#65292;&#23427;&#33021;&#22815;&#27604;&#21407;&#22987;&#30340;DC&#26356;&#26377;&#25928;&#22320;&#22686;&#24378;&#36229;&#31354;&#38388;&#21033;&#29992;&#29575;&#12290;&#21407;&#22987;&#30340;DC&#23384;&#22312;&#19968;&#20010;&#38480;&#21046;&#65292;&#21363;&#30001;&#20110;&#20854;&#20351;&#29992;&#22522;&#20110;One-hot&#21521;&#37327;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20004;&#20010;&#35828;&#35805;&#32773;&#30340;&#23884;&#20837;&#24517;&#39035;&#20855;&#26377;&#27491;&#20132;&#20851;&#31995;&#65292;&#32780;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#27491;&#21017;&#31616;&#21333;&#24418;&#20307;&#30340;&#29305;&#24615;&#65292;&#25512;&#23548;&#20986;&#19968;&#20010;&#26088;&#22312;&#26368;&#22823;&#21270;&#36229;&#31354;&#38388;&#20013;&#30446;&#26631;&#35282;&#24230;&#30340;&#21807;&#19968;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#22312;&#23558;&#35828;&#35805;&#32773;&#38169;&#35823;&#20998;&#37197;&#26102;&#27604;&#21407;&#22987;DC&#26045;&#21152;&#20102;&#26356;&#39640;&#30340;&#24809;&#32602;&#12290;&#20174;DC&#21040;M-DC&#30340;&#36716;&#21464;&#21482;&#38656;&#26356;&#25913;DC&#20013;&#25439;&#22833;&#20989;&#25968;&#30340;&#19968;&#20010;&#26415;&#35821;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;&#32593;&#32476;&#32467;&#26500;&#25110;&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#20219;&#20309;&#20854;&#20182;&#20462;&#25913;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#24456;&#39640;&#30340;&#23454;&#29992;&#24615;&#65292;&#22240;&#20026;&#23427;&#19981;&#20250;&#24433;&#21709;&#21407;&#22987;&#30340;&#25512;&#29702;&#37096;&#20998;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#21407;&#22987;DC&#21450;&#20854;&#25193;&#23637;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new deep clustering (DC) method called manifold-aware DC (M-DC) that can enhance hyperspace utilization more effectively than the original DC. The original DC has a limitation in that a pair of two speakers has to be embedded having an orthogonal relationship due to its use of the one-hot vector-based loss function, while our method derives a unique loss function aimed at maximizing the target angle in the hyperspace based on the nature of a regular simplex. Our proposed loss imposes a higher penalty than the original DC when the speaker is assigned incorrectly. The change from DC to M-DC can be easily achieved by rewriting just one term in the loss function of DC, without any other modifications to the network architecture or model parameters. As such, our method has high practicability because it does not affect the original inference part. The experimental results show that the proposed method improves the performances of the original DC and its expansion metho
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#22609;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#22312;&#21033;&#29992;&#20808;&#21069;&#33719;&#24471;&#30693;&#35782;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#36798;&#21040;&#30456;&#21516;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36825;&#23545;&#20110;&#25345;&#32493;&#23398;&#20064;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2106.00042</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#22609;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A study on the plasticity of neural networks. (arXiv:2106.00042v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.00042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#22609;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#22312;&#21033;&#29992;&#20808;&#21069;&#33719;&#24471;&#30693;&#35782;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#36798;&#21040;&#30456;&#21516;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36825;&#23545;&#20110;&#25345;&#32493;&#23398;&#20064;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20010;&#29615;&#22659;&#20013;&#30340;&#19968;&#20010;&#20849;&#21516;&#30446;&#26631;&#65292;&#22914;&#25345;&#32493;&#23398;&#20064;&#25110;&#36801;&#31227;&#23398;&#20064;&#65292;&#26159;&#21033;&#29992;&#20808;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#26469;&#26356;&#24555;&#22320;&#22312;&#24403;&#21069;&#20219;&#21153;&#19978;&#25910;&#25947;&#12290;&#36890;&#24120;&#65292;&#36825;&#36890;&#36807;&#24494;&#35843;&#26469;&#23454;&#29616;&#65292;&#20854;&#20013;&#19968;&#20010;&#38544;&#21547;&#30340;&#20551;&#35774;&#26159;&#32593;&#32476;&#20445;&#25345;&#20854;&#21487;&#22609;&#24615;&#65292;&#21363;&#23427;&#22312;&#20219;&#20309;&#32473;&#23450;&#20219;&#21153;&#19978;&#33021;&#22815;&#36798;&#21040;&#30340;&#24615;&#33021;&#19981;&#21463;&#20808;&#21069;&#20219;&#21153;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#26368;&#36817;&#35266;&#23519;&#21040;&#65292;&#22312;&#19982;&#37325;&#26032;&#21021;&#22987;&#21270;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#19968;&#20010;&#22312;&#19982;&#24494;&#35843;&#30340;&#25968;&#25454;&#20855;&#26377;&#30456;&#21516;&#20998;&#24067;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#36798;&#21040;&#30456;&#21516;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#26500;&#24314;&#24182;&#25193;&#23637;&#20102;&#36825;&#20010;&#35266;&#23519;&#32467;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#20854;&#32972;&#21518;&#26426;&#21046;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#23545;&#20110;&#20005;&#37325;&#20381;&#36182;&#20110;&#20248;&#21270;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;&#32780;&#35328;&#65292;&#22833;&#21435;&#21487;&#22609;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
One aim shared by multiple settings, such as continual learning or transfer learning, is to leverage previously acquired knowledge to converge faster on the current task. Usually this is done through fine-tuning, where an implicit assumption is that the network maintains its plasticity, meaning that the performance it can reach on any given task is not affected negatively by previously seen tasks. It has been observed recently that a pretrained model on data from the same distribution as the one it is fine-tuned on might not reach the same generalisation as a freshly initialised one. We build and extend this observation, providing a hypothesis for the mechanics behind it. We discuss the implication of losing plasticity for continual learning which heavily relies on optimising pretrained models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20005;&#26684;&#30740;&#31350;&#20102;&#24102;&#26377;&#31070;&#32463;&#32593;&#32476;&#39033;&#30340;&#32447;&#24615;&#26925;&#22278;PDE&#27169;&#22411;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#21644;&#20276;&#38543;PDE&#26041;&#27861;&#35777;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;-PDE&#20248;&#21270;&#36807;&#31243;&#30340;&#20840;&#23616;&#25910;&#25947;&#12290;&#22312;&#27969;&#20307;&#21147;&#23398;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#36825;&#20010;&#26041;&#27861;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20316;&#20026;&#19968;&#20010;&#38381;&#29615;&#12290;</title><link>http://arxiv.org/abs/2105.08633</link><description>&lt;p&gt;
&#24102;&#26377;&#31070;&#32463;&#32593;&#32476;&#39033;&#30340;PDE&#32422;&#26463;&#27169;&#22411;&#65306;&#20248;&#21270;&#19982;&#20840;&#23616;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
PDE-constrained Models with Neural Network Terms: Optimization and Global Convergence. (arXiv:2105.08633v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.08633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20005;&#26684;&#30740;&#31350;&#20102;&#24102;&#26377;&#31070;&#32463;&#32593;&#32476;&#39033;&#30340;&#32447;&#24615;&#26925;&#22278;PDE&#27169;&#22411;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#21644;&#20276;&#38543;PDE&#26041;&#27861;&#35777;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;-PDE&#20248;&#21270;&#36807;&#31243;&#30340;&#20840;&#23616;&#25910;&#25947;&#12290;&#22312;&#27969;&#20307;&#21147;&#23398;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#36825;&#20010;&#26041;&#27861;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20316;&#20026;&#19968;&#20010;&#38381;&#29615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#21457;&#23637;&#20102;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#27169;&#22411;&#12290;PDE&#30340;&#21151;&#33021;&#24418;&#24335;&#30001;&#31070;&#32463;&#32593;&#32476;&#30830;&#23450;&#65292;&#24182;&#19988;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#26681;&#25454;&#21487;&#29992;&#25968;&#25454;&#36827;&#34892;&#26657;&#20934;&#12290;&#23884;&#20837;PDE&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#26657;&#20934;&#21487;&#20197;&#36890;&#36807;&#22312;PDE&#19978;&#36827;&#34892;&#20248;&#21270;&#26469;&#23454;&#29616;&#12290;&#21463;&#36825;&#20123;&#24212;&#29992;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23545;&#24102;&#26377;&#31070;&#32463;&#32593;&#32476;&#39033;&#30340;&#19968;&#31867;&#32447;&#24615;&#26925;&#22278;PDE&#30340;&#20248;&#21270;&#36827;&#34892;&#20102;&#20005;&#26684;&#30740;&#31350;&#12290;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;PDE&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#65292;&#20854;&#20013;&#26799;&#24230;&#26159;&#20351;&#29992;&#20276;&#38543;PDE&#35745;&#31639;&#30340;&#12290;&#38543;&#30528;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;PDE&#21644;&#20276;&#38543;PDE&#25910;&#25947;&#21040;&#19968;&#20010;&#38750;&#23616;&#37096;&#30340;PDE&#31995;&#32479;&#12290;&#20351;&#29992;&#27492;&#26497;&#38480;PDE&#31995;&#32479;&#65292;&#25105;&#20204;&#33021;&#22815;&#35777;&#26126;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#31070;&#32463;&#32593;&#32476;-PDE&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#27492;&#20276;&#38543;&#26041;&#27861;&#22312;&#27969;&#20307;&#21147;&#23398;&#20013;&#35757;&#32451;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20854;&#20013;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#19968;&#20010;&#38381;&#29615;&#36816;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has used deep learning to develop partial differential equation (PDE) models in science and engineering. The functional form of the PDE is determined by a neural network, and the neural network parameters are calibrated to available data. Calibration of the embedded neural network can be performed by optimizing over the PDE. Motivated by these applications, we rigorously study the optimization of a class of linear elliptic PDEs with neural network terms. The neural network parameters in the PDE are optimized using gradient descent, where the gradient is evaluated using an adjoint PDE. As the number of parameters become large, the PDE and adjoint PDE converge to a non-local PDE system. Using this limit PDE system, we are able to prove convergence of the neural network-PDE to a global minimum during the optimization. Finally, we use this adjoint method to train a neural network model for an application in fluid mechanics, in which the neural network functions as a closure
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;BEAUTY&#26041;&#27861;&#36827;&#34892;&#20998;&#24067;&#26080;&#20851;&#30340;&#25311;&#21512;&#20248;&#24230;&#26816;&#39564;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20108;&#36827;&#21046;&#23637;&#24320;&#36924;&#36817;&#29305;&#24449;&#20989;&#25968;&#65292;&#24182;&#23558;&#35768;&#22810;&#37325;&#35201;&#30340;&#29420;&#31435;&#24615;&#26816;&#39564;&#32479;&#19968;&#36215;&#26469;&#12290;&#20351;&#29992;&#25968;&#25454;&#33258;&#36866;&#24212;&#26435;&#37325;&#30340;BEAST&#26816;&#39564;&#25552;&#20379;&#20102;&#31283;&#20581;&#30340;&#21151;&#25928;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#34892;&#21151;&#25928;&#30340;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2103.00674</link><description>&lt;p&gt;
BEAUTY&#21160;&#21147;&#30340;BEAST
&lt;/p&gt;
&lt;p&gt;
BEAUTY Powered BEAST. (arXiv:2103.00674v5 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.00674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;BEAUTY&#26041;&#27861;&#36827;&#34892;&#20998;&#24067;&#26080;&#20851;&#30340;&#25311;&#21512;&#20248;&#24230;&#26816;&#39564;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20108;&#36827;&#21046;&#23637;&#24320;&#36924;&#36817;&#29305;&#24449;&#20989;&#25968;&#65292;&#24182;&#23558;&#35768;&#22810;&#37325;&#35201;&#30340;&#29420;&#31435;&#24615;&#26816;&#39564;&#32479;&#19968;&#36215;&#26469;&#12290;&#20351;&#29992;&#25968;&#25454;&#33258;&#36866;&#24212;&#26435;&#37325;&#30340;BEAST&#26816;&#39564;&#25552;&#20379;&#20102;&#31283;&#20581;&#30340;&#21151;&#25928;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#34892;&#21151;&#25928;&#30340;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#25552;&#20986;&#30340;&#20108;&#36827;&#21046;&#23637;&#24320;&#36817;&#20284;&#22343;&#21248;&#24615;&#65288;BEAUTY&#65289;&#26041;&#27861;&#30340;&#26080;&#20998;&#24067;&#25311;&#21512;&#20248;&#24230;&#26816;&#39564;&#12290;&#35813;&#26041;&#27861;&#25512;&#24191;&#20102;&#33879;&#21517;&#30340;&#27431;&#25289;&#20844;&#24335;&#65292;&#24182;&#36890;&#36807;&#26399;&#26395;&#30340;&#20108;&#36827;&#21046;&#20132;&#20114;&#30340;&#32447;&#24615;&#32452;&#21512;&#26469;&#36924;&#36817;&#20219;&#20309;&#32852;&#21512;&#20998;&#24067;&#20989;&#25968;&#30340;&#29305;&#24449;&#20989;&#25968;&#12290;&#36825;&#20010;&#26032;&#39062;&#30340;&#29702;&#35770;&#36890;&#36807;&#29305;&#23450;&#30340;&#20108;&#27425;&#23545;&#31216;&#32479;&#35745;&#30340;&#36924;&#36817;&#65292;&#23558;&#35768;&#22810;&#37325;&#35201;&#30340;&#29420;&#31435;&#24615;&#26816;&#39564;&#32479;&#19968;&#36215;&#26469;&#65292;&#20854;&#20013;&#30830;&#23450;&#24615;&#26435;&#37325;&#30697;&#38453;&#34920;&#24449;&#27599;&#20010;&#26816;&#39564;&#30340;&#21151;&#25928;&#24615;&#36136;&#12290;&#20026;&#20102;&#33719;&#24471;&#31283;&#20581;&#30340;&#21151;&#25928;&#65292;&#25105;&#20204;&#20351;&#29992;&#25968;&#25454;&#33258;&#36866;&#24212;&#26435;&#37325;&#26469;&#26816;&#39564;&#26816;&#39564;&#32479;&#35745;&#37327;&#65292;&#31216;&#20026;&#20108;&#36827;&#21046;&#23637;&#24320;&#33258;&#36866;&#24212;&#23545;&#31216;&#24615;&#26816;&#39564;&#65288;BEAST&#65289;&#12290;&#21033;&#29992;&#20108;&#36827;&#21046;&#23637;&#24320;&#36807;&#31243;&#30340;&#24615;&#36136;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22343;&#21248;&#24615;&#30340;Neyman-Pearson&#26816;&#39564;&#21487;&#20197;&#36890;&#36807;oracle&#21152;&#26435;&#21644;&#30340;&#23545;&#31216;&#24615;&#32479;&#35745;&#37327;&#26469;&#36817;&#20284;&#12290;&#20855;&#26377;&#36825;&#20010;oracle&#30340;BEAST&#25552;&#20379;&#20102;&#21487;&#34892;&#21151;&#25928;&#30340;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study distribution-free goodness-of-fit tests with the proposed Binary Expansion Approximation of UniformiTY (BEAUTY) approach. This method generalizes the renowned Euler's formula, and approximates the characteristic function of any copula through a linear combination of expectations of binary interactions from marginal binary expansions. This novel theory enables a unification of many important tests of independence via approximations from specific quadratic forms of symmetry statistics, where the deterministic weight matrix characterizes the power properties of each test. To achieve a robust power, we examine test statistics with data-adaptive weights, referred to as the Binary Expansion Adaptive Symmetry Test (BEAST). Using properties of the binary expansion filtration, we demonstrate that the Neyman-Pearson test of uniformity can be approximated by an oracle weighted sum of symmetry statistics. The BEAST with this oracle provides a useful benchmark of feasible power. To approac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22270;&#24418;&#24179;&#28369;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#20351;&#29992;&#36339;&#36291;&#36830;&#25509;&#21644;&#22270;&#32467;&#26500;&#20449;&#24687;&#26469;&#23398;&#20064;&#26377;&#21306;&#20998;&#24615;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2010.10274</link><description>&lt;p&gt;
&#22270;&#24418;&#24179;&#28369;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Graph Fairing Convolutional Networks for Anomaly Detection. (arXiv:2010.10274v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.10274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22270;&#24418;&#24179;&#28369;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#20351;&#29992;&#36339;&#36291;&#36830;&#25509;&#21644;&#22270;&#32467;&#26500;&#20449;&#24687;&#26469;&#23398;&#20064;&#26377;&#21306;&#20998;&#24615;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21367;&#31215;&#26159;&#35768;&#22810;&#22522;&#20110;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#26412;&#26500;&#24314;&#22359;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#38750;&#24120;&#26377;&#25928;&#30340;&#24102;&#26377;&#36339;&#36291;&#36830;&#25509;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#29992;&#20110;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#12290;&#25105;&#20204;&#27169;&#22411;&#30340;&#36880;&#23618;&#20256;&#25773;&#35268;&#21017;&#22312;&#29702;&#35770;&#19978;&#21463;&#21040;&#20960;&#20309;&#22788;&#29702;&#20013;&#38544;&#24335;&#24179;&#28369;&#27010;&#24565;&#30340;&#21551;&#21457;&#65292;&#21253;&#25324;&#29992;&#20110;&#32858;&#21512;&#26469;&#33258;&#30456;&#37051;&#33410;&#28857;&#30340;&#20449;&#24687;&#30340;&#22270;&#21367;&#31215;&#27169;&#22359;&#21644;&#29992;&#20110;&#32452;&#21512;&#36880;&#23618;&#37051;&#23621;&#34920;&#31034;&#30340;&#36339;&#36291;&#36830;&#25509;&#27169;&#22359;&#12290;&#36825;&#20010;&#20256;&#25773;&#35268;&#21017;&#26159;&#36890;&#36807;&#38597;&#21487;&#27604;&#26041;&#27861;&#20174;&#38544;&#24335;&#24179;&#28369;&#26041;&#31243;&#30340;&#36845;&#20195;&#35299;&#23548;&#20986;&#30340;&#12290;&#38500;&#20102;&#36890;&#36807;&#32593;&#32476;&#23618;&#20043;&#38388;&#30340;&#36339;&#36291;&#36830;&#25509;&#25429;&#33719;&#26469;&#33258;&#36828;&#31243;&#22270;&#33410;&#28857;&#30340;&#20449;&#24687;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#21033;&#29992;&#22270;&#32467;&#26500;&#21644;&#33410;&#28857;&#29305;&#24449;&#26469;&#23398;&#20064;&#26377;&#21306;&#20998;&#24615;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;&#36825;&#20123;&#36339;&#36291;&#36830;&#25509;&#26159;&#26681;&#25454;&#25105;&#20204;&#25552;&#20986;&#30340;&#32593;&#32476;&#26550;&#26500;&#32463;&#36807;&#35774;&#35745;&#25972;&#21512;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph convolution is a fundamental building block for many deep neural networks on graph-structured data. In this paper, we introduce a simple, yet very effective graph convolutional network with skip connections for semi-supervised anomaly detection. The proposed layerwise propagation rule of our model is theoretically motivated by the concept of implicit fairing in geometry processing, and comprises a graph convolution module for aggregating information from immediate node neighbors and a skip connection module for combining layer-wise neighborhood representations. This propagation rule is derived from the iterative solution of the implicit fairing equation via the Jacobi method. In addition to capturing information from distant graph nodes through skip connections between the network's layers, our approach exploits both the graph structure and node features for learning discriminative node representations. These skip connections are integrated by design in our proposed network archi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#21333;&#35843;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#30340;&#24555;&#36895;&#36866;&#24212;&#24615;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#32972;&#21253;&#32422;&#26463;&#19979;&#20197;O(log n)&#30340;&#22797;&#26434;&#24230;&#25509;&#36817;&#26368;&#20248;&#35299;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#20462;&#25913;&#23558;&#20989;&#25968;&#20540;&#35810;&#38382;&#30340;&#25968;&#37327;&#20943;&#23569;&#21040;O(n)&#32780;&#20445;&#25345;&#36739;&#20302;&#30340;&#36866;&#24212;&#24615;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2007.05014</link><description>&lt;p&gt;
&#24555;&#36895;&#36866;&#24212;&#24615;&#38750;&#21333;&#35843;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#22312;&#32972;&#21253;&#32422;&#26463;&#19979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Fast Adaptive Non-Monotone Submodular Maximization Subject to a Knapsack Constraint. (arXiv:2007.05014v2 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.05014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#21333;&#35843;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#30340;&#24555;&#36895;&#36866;&#24212;&#24615;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#32972;&#21253;&#32422;&#26463;&#19979;&#20197;O(log n)&#30340;&#22797;&#26434;&#24230;&#25509;&#36817;&#26368;&#20248;&#35299;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#20462;&#25913;&#23558;&#20989;&#25968;&#20540;&#35810;&#38382;&#30340;&#25968;&#37327;&#20943;&#23569;&#21040;O(n)&#32780;&#20445;&#25345;&#36739;&#20302;&#30340;&#36866;&#24212;&#24615;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23376;&#27169;&#26368;&#22823;&#21270;&#26159;&#19968;&#20010;&#32463;&#20856;&#30340;&#31639;&#27861;&#38382;&#39064;&#65292;&#22312;&#25968;&#25454;&#25366;&#25496;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#26377;&#22810;&#31181;&#24212;&#29992;&#12290;&#38024;&#23545;&#22823;&#35268;&#27169;&#23454;&#20363;&#30340;&#38656;&#27714;&#65292;&#35774;&#35745;&#33021;&#22815;&#22312;&#35299;&#30340;&#36136;&#37327;&#21644;&#36866;&#29992;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#30340;&#31639;&#27861;&#25104;&#20026;&#21160;&#21147;&#12290;&#26412;&#25991;&#38024;&#23545;&#38750;&#21333;&#35843;&#23376;&#27169;&#26368;&#22823;&#21270;&#22312;&#32972;&#21253;&#32422;&#26463;&#19979;&#30340;&#38382;&#39064;&#65292;&#24471;&#21040;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#24120;&#25968;&#36924;&#36817;&#27604;&#21644;&#25509;&#36817;&#26368;&#20248;&#30340;O(log n)&#36866;&#24212;&#24615;&#22797;&#26434;&#24230;&#30340;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#27714;&#35299;&#36807;&#31243;&#20013;&#35810;&#38382;&#30340;&#20989;&#25968;&#20540;&#25968;&#37327;&#21487;&#20197;&#36890;&#36807;&#20462;&#25913;&#21464;&#20026;O(n)&#32780;&#20445;&#25345;&#36866;&#24212;&#24615;&#22797;&#26434;&#24230;&#20026;O(log^2n)&#12290;
&lt;/p&gt;
&lt;p&gt;
Submodular maximization is a classic algorithmic problem with multiple applications in data mining and machine learning; there, the growing need to deal with massive instances motivates the design of algorithms balancing the quality of the solution with applicability. For the latter, an important measure is the \emph{adaptive complexity}, which captures the number of sequential rounds of parallel computation needed by an algorithm to terminate. In this work, we obtain the first \emph{constant factor} approximation algorithm for non-monotone submodular maximization subject to a knapsack constraint with \emph{near-optimal} $O(\log n)$ adaptive complexity. Low adaptivity by itself, however, is not enough: a crucial feature to account for is represented by the total number of function evaluations (or value queries). Our algorithm asks $\tilde{O}(n^2)$ value queries but can be modified to run with only $\tilde{O}(n)$ instead while retaining a low adaptive complexity of $O(\log^2n)$. Besides
&lt;/p&gt;</description></item><item><title>Ansor&#26159;&#19968;&#20010;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#24352;&#37327;&#31243;&#24207;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#37319;&#26679;&#31243;&#24207;&#21644;&#20351;&#29992;&#36827;&#21270;&#25628;&#32034;&#21644;&#23398;&#20064;&#30340;&#25104;&#26412;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#25214;&#21040;&#39640;&#24615;&#33021;&#30340;&#24352;&#37327;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2006.06762</link><description>&lt;p&gt;
Ansor: &#29983;&#25104;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#39640;&#24615;&#33021;&#24352;&#37327;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Ansor: Generating High-Performance Tensor Programs for Deep Learning. (arXiv:2006.06762v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.06762
&lt;/p&gt;
&lt;p&gt;
Ansor&#26159;&#19968;&#20010;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#24352;&#37327;&#31243;&#24207;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#37319;&#26679;&#31243;&#24207;&#21644;&#20351;&#29992;&#36827;&#21270;&#25628;&#32034;&#21644;&#23398;&#20064;&#30340;&#25104;&#26412;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#25214;&#21040;&#39640;&#24615;&#33021;&#30340;&#24352;&#37327;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#24615;&#33021;&#30340;&#24352;&#37327;&#31243;&#24207;&#23545;&#20110;&#20445;&#35777;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#25191;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20026;&#19981;&#21516;&#30340;&#36816;&#31639;&#31526;&#22312;&#21508;&#31181;&#30828;&#20214;&#24179;&#21488;&#19978;&#33719;&#24471;&#39640;&#24615;&#33021;&#24352;&#37327;&#31243;&#24207;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#30446;&#21069;&#65292;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20381;&#36182;&#20110;&#20379;&#24212;&#21830;&#25552;&#20379;&#30340;&#20869;&#26680;&#24211;&#25110;&#21508;&#31181;&#25628;&#32034;&#31574;&#30053;&#26469;&#33719;&#21462;&#39640;&#24615;&#33021;&#30340;&#24352;&#37327;&#31243;&#24207;&#12290;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#22823;&#37327;&#30340;&#24037;&#31243;&#24037;&#20316;&#26469;&#24320;&#21457;&#29305;&#23450;&#20110;&#24179;&#21488;&#30340;&#20248;&#21270;&#20195;&#30721;&#65292;&#35201;&#20040;&#30001;&#20110;&#21463;&#38480;&#30340;&#25628;&#32034;&#31354;&#38388;&#21644;&#26080;&#25928;&#30340;&#25506;&#32034;&#31574;&#30053;&#32780;&#26080;&#27861;&#25214;&#21040;&#39640;&#24615;&#33021;&#30340;&#31243;&#24207;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Ansor&#30340;&#24352;&#37327;&#31243;&#24207;&#29983;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#12290;&#19982;&#29616;&#26377;&#30340;&#25628;&#32034;&#31574;&#30053;&#30456;&#27604;&#65292;Ansor&#36890;&#36807;&#20174;&#25628;&#32034;&#31354;&#38388;&#30340;&#20998;&#23618;&#34920;&#31034;&#20013;&#37319;&#26679;&#31243;&#24207;&#26469;&#25506;&#32034;&#26356;&#22810;&#30340;&#20248;&#21270;&#32452;&#21512;&#12290;&#28982;&#21518;&#65292;Ansor&#20351;&#29992;&#36827;&#21270;&#25628;&#32034;&#21644;&#23398;&#20064;&#30340;&#25104;&#26412;&#27169;&#22411;&#26469;&#23545;&#37319;&#26679;&#20986;&#30340;&#31243;&#24207;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#25214;&#21040;&#26368;&#20339;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-performance tensor programs are crucial to guarantee efficient execution of deep neural networks. However, obtaining performant tensor programs for different operators on various hardware platforms is notoriously challenging. Currently, deep learning systems rely on vendor-provided kernel libraries or various search strategies to get performant tensor programs. These approaches either require significant engineering effort to develop platform-specific optimization code or fall short of finding high-performance programs due to restricted search space and ineffective exploration strategy.  We present Ansor, a tensor program generation framework for deep learning applications. Compared with existing search strategies, Ansor explores many more optimization combinations by sampling programs from a hierarchical representation of the search space. Ansor then fine-tunes the sampled programs with evolutionary search and a learned cost model to identify the best programs. Ansor can find hig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#31216;&#20026;&#26465;&#20214;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#36890;&#29992;&#31867;&#21035;&#65292;&#22312;&#36827;&#34892;&#26799;&#24230;&#36845;&#20195;&#26102;&#23545;&#26799;&#24230;&#26041;&#21521;&#36827;&#34892;&#39044;&#26465;&#20214;&#12290;&#20351;&#29992;&#31163;&#25955;&#26102;&#38388;&#26041;&#27861;&#21644;&#38789;&#24037;&#20855;&#65292;&#22312;&#36739;&#24369;&#30340;&#20551;&#35774;&#19979;&#35777;&#26126;&#20102;&#36845;&#20195;&#24207;&#21015;&#30340;&#37325;&#26032;&#32553;&#25918;&#25910;&#25947;&#24615;&#65292;&#36866;&#29992;&#20110;&#21253;&#25324;&#38543;&#26426;&#19968;&#38454;&#21644;&#20108;&#38454;&#26041;&#27861;&#22312;&#20869;&#30340;&#24191;&#27867;&#26465;&#20214;&#30697;&#38453;&#31867;&#21035;&#12290;&#21516;&#26102;&#65292;&#36824;&#20171;&#32461;&#20102;&#20855;&#26377;&#29420;&#31435;&#20852;&#36259;&#30340;&#20960;&#20046;&#32943;&#23450;&#25910;&#25947;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2006.02745</link><description>&lt;p&gt;
&#26465;&#20214;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#28176;&#36817;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Asymptotic Analysis of Conditioned Stochastic Gradient Descent. (arXiv:2006.02745v5 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.02745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#31216;&#20026;&#26465;&#20214;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#36890;&#29992;&#31867;&#21035;&#65292;&#22312;&#36827;&#34892;&#26799;&#24230;&#36845;&#20195;&#26102;&#23545;&#26799;&#24230;&#26041;&#21521;&#36827;&#34892;&#39044;&#26465;&#20214;&#12290;&#20351;&#29992;&#31163;&#25955;&#26102;&#38388;&#26041;&#27861;&#21644;&#38789;&#24037;&#20855;&#65292;&#22312;&#36739;&#24369;&#30340;&#20551;&#35774;&#19979;&#35777;&#26126;&#20102;&#36845;&#20195;&#24207;&#21015;&#30340;&#37325;&#26032;&#32553;&#25918;&#25910;&#25947;&#24615;&#65292;&#36866;&#29992;&#20110;&#21253;&#25324;&#38543;&#26426;&#19968;&#38454;&#21644;&#20108;&#38454;&#26041;&#27861;&#22312;&#20869;&#30340;&#24191;&#27867;&#26465;&#20214;&#30697;&#38453;&#31867;&#21035;&#12290;&#21516;&#26102;&#65292;&#36824;&#20171;&#32461;&#20102;&#20855;&#26377;&#29420;&#31435;&#20852;&#36259;&#30340;&#20960;&#20046;&#32943;&#23450;&#25910;&#25947;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#31216;&#20026;&#26465;&#20214;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#31639;&#27861;&#30340;&#19968;&#33324;&#31867;&#21035;&#65292;&#35813;&#31639;&#27861;&#22522;&#20110;&#26799;&#24230;&#26041;&#21521;&#30340;&#39044;&#26465;&#20214;&#12290;&#20351;&#29992;&#31163;&#25955;&#26102;&#38388;&#26041;&#27861;&#21644;&#38789;&#24037;&#20855;&#65292;&#25105;&#20204;&#22312;&#36739;&#24369;&#30340;&#20551;&#35774;&#19979;&#35777;&#26126;&#20102;&#36845;&#20195;&#24207;&#21015;&#30340;&#37325;&#26032;&#32553;&#25918;&#25910;&#25947;&#24615;&#65292;&#36866;&#29992;&#20110;&#21253;&#25324;&#38543;&#26426;&#19968;&#38454;&#21644;&#20108;&#38454;&#26041;&#27861;&#22312;&#20869;&#30340;&#24191;&#27867;&#26465;&#20214;&#30697;&#38453;&#31867;&#21035;&#12290;&#21516;&#26102;&#36824;&#20171;&#32461;&#20102;&#20855;&#26377;&#29420;&#31435;&#20852;&#36259;&#30340;&#20960;&#20046;&#32943;&#23450;&#25910;&#25947;&#30340;&#32467;&#26524;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#28176;&#36817;&#27491;&#24577;&#24615;&#32467;&#26524;&#21253;&#25324;&#19968;&#20010;&#38543;&#26426;&#31561;&#36830;&#32493;&#24615;&#24615;&#36136;&#65292;&#22240;&#27492;&#24403;&#26465;&#20214;&#30697;&#38453;&#26159;&#36870;Hessian&#30340;&#20272;&#35745;&#26102;&#65292;&#35813;&#31639;&#27861;&#26159;&#28176;&#36817;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate a general class of stochastic gradient descent (SGD) algorithms, called Conditioned SGD, based on a preconditioning of the gradient direction. Using a discrete-time approach with martingale tools, we establish under mild assumptions the weak convergence of the rescaled sequence of iterates for a broad class of conditioning matrices including stochastic first-order and second-order methods. Almost sure convergence results, which may be of independent interest, are also presented. Interestingly, the asymptotic normality result consists in a stochastic equicontinuity property so when the conditioning matrix is an estimate of the inverse Hessian, the algorithm is asymptotically optimal.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#26102;&#38388;&#21367;&#31215;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;TCAN&#65289;&#30340;&#26550;&#26500;&#65292;&#23427;&#32467;&#21512;&#20102;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#21644;&#27880;&#24847;&#26426;&#21046;&#65292;&#26082;&#33021;&#26367;&#20195;&#24490;&#29615;&#32593;&#32476;&#65292;&#21448;&#33021;&#21560;&#25910;&#21069;&#39304;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#26368;&#26032;&#30340;&#22256;&#24785;&#24230;&#32467;&#26524;&#21040;30.28&#65288;&#22522;&#20110;&#21333;&#35789;&#30340;PTB&#65289;&#65292;1.092&#65288;&#22522;&#20110;&#23383;&#31526;&#30340;PTB&#65289;&#12290;</title><link>http://arxiv.org/abs/2002.12530</link><description>&lt;p&gt;
&#26102;&#38388;&#21367;&#31215;&#27880;&#24847;&#21147;&#32593;&#32476;&#29992;&#20110;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Temporal Convolutional Attention-based Network For Sequence Modeling. (arXiv:2002.12530v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2002.12530
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#26102;&#38388;&#21367;&#31215;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;TCAN&#65289;&#30340;&#26550;&#26500;&#65292;&#23427;&#32467;&#21512;&#20102;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#21644;&#27880;&#24847;&#26426;&#21046;&#65292;&#26082;&#33021;&#26367;&#20195;&#24490;&#29615;&#32593;&#32476;&#65292;&#21448;&#33021;&#21560;&#25910;&#21069;&#39304;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#26368;&#26032;&#30340;&#22256;&#24785;&#24230;&#32467;&#26524;&#21040;30.28&#65288;&#22522;&#20110;&#21333;&#35789;&#30340;PTB&#65289;&#65292;1.092&#65288;&#22522;&#20110;&#23383;&#31526;&#30340;PTB&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21069;&#39304;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#29992;&#20110;&#24207;&#21015;&#24314;&#27169;&#30340;&#40664;&#35748;&#27169;&#22411;&#36880;&#28176;&#28436;&#21464;&#20026;&#21462;&#20195;&#24490;&#29615;&#32593;&#32476;&#12290;&#35768;&#22810;&#22522;&#20110;&#21367;&#31215;&#32593;&#32476;&#21644;&#27880;&#24847;&#26426;&#21046;&#30340;&#24378;&#22823;&#21069;&#39304;&#27169;&#22411;&#34987;&#25552;&#20986;&#65292;&#24182;&#26174;&#31034;&#20986;&#26356;&#22810;&#22788;&#29702;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#24819;&#30693;&#36947;&#26159;&#21542;&#26377;&#19968;&#31181;&#26550;&#26500;&#26082;&#33021;&#23454;&#29616;&#23545;&#24490;&#29615;&#32593;&#32476;&#30340;&#36817;&#20284;&#26367;&#20195;&#65292;&#21448;&#33021;&#21560;&#25910;&#21069;&#39304;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25506;&#32034;&#24615;&#26550;&#26500;&#65292;&#31216;&#20026;&#26102;&#38388;&#21367;&#31215;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;TCAN&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#21644;&#27880;&#24847;&#26426;&#21046;&#12290;TCAN&#21253;&#25324;&#20004;&#20010;&#37096;&#20998;&#65292;&#19968;&#20010;&#26159;&#26102;&#38388;&#27880;&#24847;&#21147;&#65288;TA&#65289;&#65292;&#29992;&#20110;&#25429;&#25417;&#24207;&#21015;&#20869;&#30340;&#30456;&#20851;&#29305;&#24449;&#65292;&#21478;&#19968;&#20010;&#26159;&#22686;&#24378;&#27531;&#24046;&#65288;ER&#65289;&#65292;&#29992;&#20110;&#25552;&#21462;&#27973;&#23618;&#30340;&#37325;&#35201;&#20449;&#24687;&#24182;&#20256;&#36882;&#32473;&#28145;&#23618;&#12290;&#25105;&#20204;&#23558;bpc/&#22256;&#24785;&#24230;&#30340;&#26368;&#26032;&#32467;&#26524;&#25913;&#36827;&#21040;30.28&#65288;&#22522;&#20110;&#21333;&#35789;&#30340;PTB&#65289;&#65292;1.092&#65288;&#22522;&#20110;&#23383;&#31526;&#30340;PTB&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of feed-forward models, the default model for sequence modeling has gradually evolved to replace recurrent networks. Many powerful feed-forward models based on convolutional networks and attention mechanism were proposed and show more potential to handle sequence modeling tasks. We wonder that is there an architecture that can not only achieve an approximate substitution of recurrent network, but also absorb the advantages of feed-forward models. So we propose an exploratory architecture referred to Temporal Convolutional Attention-based Network (TCAN) which combines temporal convolutional network and attention mechanism. TCAN includes two parts, one is Temporal Attention (TA) which captures relevant features inside the sequence, the other is Enhanced Residual (ER) which extracts shallow layer's important information and transfers to deep layers. We improve the state-of-the-art results of bpc/perplexity to 30.28 on word-level PTB, 1.092 on character-level PTB, and 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#36870;&#21521;&#22870;&#21169;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35810;&#38382;&#29992;&#25143;&#27604;&#36739;&#19981;&#21516;&#30340;&#22870;&#21169;&#20989;&#25968;&#26469;&#36873;&#25321;&#20855;&#26377;&#26368;&#22823;&#20449;&#24687;&#37327;&#30340;&#26597;&#35810;&#65292;&#20174;&#32780;&#36798;&#21040;&#20102;&#22312;&#35757;&#32451;&#29615;&#22659;&#20013;&#20445;&#35777;&#33391;&#22909;&#34892;&#20026;&#30340;&#30446;&#30340;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#24341;&#36215;&#23376;&#20248;&#34892;&#20026;&#30340;&#20559;&#22909;&#26469;&#25910;&#38598;&#39069;&#22806;&#20449;&#24687;&#65292;&#21516;&#26102;&#22312;&#27979;&#35797;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/1809.03060</link><description>&lt;p&gt;
&#20027;&#21160;&#36870;&#21521;&#22870;&#21169;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Active Inverse Reward Design. (arXiv:1809.03060v3 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1809.03060
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#36870;&#21521;&#22870;&#21169;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35810;&#38382;&#29992;&#25143;&#27604;&#36739;&#19981;&#21516;&#30340;&#22870;&#21169;&#20989;&#25968;&#26469;&#36873;&#25321;&#20855;&#26377;&#26368;&#22823;&#20449;&#24687;&#37327;&#30340;&#26597;&#35810;&#65292;&#20174;&#32780;&#36798;&#21040;&#20102;&#22312;&#35757;&#32451;&#29615;&#22659;&#20013;&#20445;&#35777;&#33391;&#22909;&#34892;&#20026;&#30340;&#30446;&#30340;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#24341;&#36215;&#23376;&#20248;&#34892;&#20026;&#30340;&#20559;&#22909;&#26469;&#25910;&#38598;&#39069;&#22806;&#20449;&#24687;&#65292;&#21516;&#26102;&#22312;&#27979;&#35797;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#20195;&#29702;&#30340;&#35774;&#35745;&#32773;&#32463;&#24120;&#36890;&#36807;&#35797;&#38169;&#30340;&#36807;&#31243;&#26469;&#36845;&#20195;&#22870;&#21169;&#20989;&#25968;&#65292;&#30452;&#21040;&#33719;&#24471;&#26399;&#26395;&#30340;&#34892;&#20026;&#65292;&#20294;&#36825;&#21482;&#33021;&#22312;&#35757;&#32451;&#29615;&#22659;&#20013;&#20445;&#35777;&#33391;&#22909;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#36825;&#20010;&#36807;&#31243;&#32467;&#26500;&#21270;&#20026;&#19968;&#31995;&#21015;&#26597;&#35810;&#65292;&#35810;&#38382;&#29992;&#25143;&#22312;&#19981;&#21516;&#30340;&#22870;&#21169;&#20989;&#25968;&#20043;&#38388;&#36827;&#34892;&#27604;&#36739;&#12290;&#22240;&#27492;&#25105;&#20204;&#21487;&#20197;&#20027;&#21160;&#36873;&#25321;&#20855;&#26377;&#26368;&#22823;&#20449;&#24687;&#37327;&#30340;&#26597;&#35810;&#26469;&#20102;&#35299;&#30495;&#23454;&#22870;&#21169;&#12290;&#19982;&#35201;&#27714;&#35774;&#35745;&#32773;&#25552;&#20379;&#26368;&#20339;&#34892;&#20026;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#24341;&#36215;&#23376;&#20248;&#34892;&#20026;&#20043;&#38388;&#30340;&#20559;&#22909;&#26469;&#25910;&#38598;&#39069;&#22806;&#30340;&#20449;&#24687;&#12290;&#22312;&#27599;&#27425;&#26597;&#35810;&#20043;&#21518;&#65292;&#25105;&#20204;&#38656;&#35201;&#36890;&#36807;&#35266;&#23519;&#35774;&#35745;&#32773;&#36873;&#25321;&#30340;&#20195;&#29702;&#22870;&#21169;&#20989;&#25968;&#26469;&#26356;&#26032;&#23545;&#30495;&#23454;&#22870;&#21169;&#20989;&#25968;&#30340;&#21518;&#39564;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#36870;&#21521;&#22870;&#21169;&#35774;&#35745; (IRD) &#21487;&#20197;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27979;&#35797;&#29615;&#22659;&#20013;&#26126;&#26174;&#20248;&#20110;IRD&#12290;&#29305;&#21035;&#26159;&#65292;&#23427;&#21487;&#20197;&#35810;&#38382;&#35774;&#35745;&#32773;&#26377;&#20851;&#21487;&#35299;&#37322;&#12289;&#32447;&#24615;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#19988;&#20173;&#28982;&#21487;&#20197;&#25512;&#26029;&#20986;&#38750;&#32447;&#24615;&#22870;&#21169;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designers of AI agents often iterate on the reward function in a trial-and-error process until they get the desired behavior, but this only guarantees good behavior in the training environment. We propose structuring this process as a series of queries asking the user to compare between different reward functions. Thus we can actively select queries for maximum informativeness about the true reward. In contrast to approaches asking the designer for optimal behavior, this allows us to gather additional information by eliciting preferences between suboptimal behaviors. After each query, we need to update the posterior over the true reward function from observing the proxy reward function chosen by the designer. The recently proposed Inverse Reward Design (IRD) enables this. Our approach substantially outperforms IRD in test environments. In particular, it can query the designer about interpretable, linear reward functions and still infer non-linear ones.
&lt;/p&gt;</description></item></channel></rss>