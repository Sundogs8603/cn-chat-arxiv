<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>MathVerse&#26159;&#19968;&#20010;&#20840;&#26041;&#20301;&#30340;&#35270;&#35273;&#25968;&#23398;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#20844;&#24179;&#32780;&#28145;&#20837;&#22320;&#35780;&#20272;MLLMs&#22312;&#35270;&#35273;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.14624</link><description>&lt;p&gt;
MathVerse&#65306;&#24744;&#30340;&#22810;&#27169;&#24335;LLM&#26159;&#21542;&#30495;&#27491;&#30475;&#21040;&#20102;&#35270;&#35273;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#22270;&#34920;&#65311;
&lt;/p&gt;
&lt;p&gt;
MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14624
&lt;/p&gt;
&lt;p&gt;
MathVerse&#26159;&#19968;&#20010;&#20840;&#26041;&#20301;&#30340;&#35270;&#35273;&#25968;&#23398;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#20844;&#24179;&#32780;&#28145;&#20837;&#22320;&#35780;&#20272;MLLMs&#22312;&#35270;&#35273;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#22312;&#35270;&#35273;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#28982;&#32780;&#23427;&#20204;&#22312;&#35270;&#35273;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#26041;&#38754;&#30340;&#33021;&#21147;&#20173;&#26410;&#20805;&#20998;&#35780;&#20272;&#21644;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#24403;&#21069;&#22522;&#20934;&#27979;&#35797;&#65292;&#23558;&#36807;&#22810;&#30340;&#35270;&#35273;&#20869;&#23481;&#34701;&#20837;&#25991;&#26412;&#38382;&#39064;&#20013;&#65292;&#36825;&#26377;&#21161;&#20110;MLLM&#22312;&#19981;&#30495;&#27491;&#35299;&#37322;&#36755;&#20837;&#22270;&#34920;&#30340;&#24773;&#20917;&#19979;&#25512;&#23548;&#31572;&#26696;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MathVerse&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#26041;&#20301;&#30340;&#35270;&#35273;&#25968;&#23398;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#20844;&#24179;&#32780;&#28145;&#20837;&#22320;&#35780;&#20272;MLLMs&#12290;&#25105;&#20204;&#31934;&#24515;&#25910;&#38598;&#20102;2,612&#20010;&#39640;&#36136;&#37327;&#30340;&#22810;&#23398;&#31185;&#25968;&#23398;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#21547;&#22270;&#34920;&#65292;&#26469;&#28304;&#20110;&#20844;&#24320;&#28192;&#36947;&#12290;&#28982;&#21518;&#65292;&#27599;&#20010;&#38382;&#39064;&#30001;&#20154;&#24037;&#27880;&#37322;&#32773;&#36716;&#21270;&#20026;&#20845;&#20010;&#19981;&#21516;&#29256;&#26412;&#65292;&#27599;&#20010;&#29256;&#26412;&#22312;&#22810;&#27169;&#24335;&#20013;&#25552;&#20379;&#19981;&#21516;&#31243;&#24230;&#30340;&#20449;&#24687;&#20869;&#23481;&#65292;&#20849;&#36129;&#29486;&#20102;15K&#20010;&#27979;&#35797;&#26679;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#24471;MathVerse&#33021;&#22815;&#21516;&#26102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14624v1 Announce Type: cross  Abstract: The remarkable progress of Multi-modal Large Language Models (MLLMs) has garnered unparalleled attention, due to their superior performance in visual contexts. However, their capabilities in visual math problem-solving remain insufficiently evaluated and understood. We investigate current benchmarks to incorporate excessive visual content within textual questions, which potentially assist MLLMs in deducing answers without truly interpreting the input diagrams. To this end, we introduce MathVerse, an all-around visual math benchmark designed for an equitable and in-depth evaluation of MLLMs. We meticulously collect 2,612 high-quality, multi-subject math problems with diagrams from publicly available sources. Each problem is then transformed by human annotators into six distinct versions, each offering varying degrees of information content in multi-modality, contributing to 15K test samples in total. This approach allows MathVerse to co
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#31616;&#21270;&#21518;&#30340;&#25193;&#25955;&#34203;&#23450;&#35860;&#26725;&#65288;DSB&#65289;&#65292;&#36890;&#36807;&#19982;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGM&#65289;&#30340;&#32479;&#19968;&#35299;&#20915;&#20102;&#22797;&#26434;&#25968;&#25454;&#29983;&#25104;&#20013;&#30340;&#38480;&#21046;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#21152;&#24555;&#20102;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.14623</link><description>&lt;p&gt;
&#31616;&#21270;&#25193;&#25955;&#34203;&#23450;&#35860;&#26725;
&lt;/p&gt;
&lt;p&gt;
Simplified Diffusion Schr\"odinger Bridge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14623
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#31616;&#21270;&#21518;&#30340;&#25193;&#25955;&#34203;&#23450;&#35860;&#26725;&#65288;DSB&#65289;&#65292;&#36890;&#36807;&#19982;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGM&#65289;&#30340;&#32479;&#19968;&#35299;&#20915;&#20102;&#22797;&#26434;&#25968;&#25454;&#29983;&#25104;&#20013;&#30340;&#38480;&#21046;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#21152;&#24555;&#20102;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#31616;&#21270;&#25193;&#25955;&#34203;&#23450;&#35860;&#26725;&#65288;DSB&#65289;&#65292;&#20415;&#20110;&#23558;&#20854;&#19982;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGM&#65289;&#32479;&#19968;&#36215;&#26469;&#65292;&#35299;&#20915;&#20102;DSB&#22312;&#22797;&#26434;&#25968;&#25454;&#29983;&#25104;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#22686;&#24378;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;SGM&#20316;&#20026;DSB&#30340;&#21021;&#22987;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#36825;&#20004;&#20010;&#26694;&#26550;&#30340;&#20248;&#21183;&#65292;&#30830;&#20445;&#20102;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#25913;&#36827;&#20102;SGM&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#26415;&#65292;&#23613;&#31649;&#23384;&#22312;&#29702;&#35770;&#36817;&#20284;&#65292;&#20294;&#23454;&#38469;&#19978;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#25311;&#21512;&#33021;&#21147;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#23454;&#65292;&#35777;&#23454;&#20102;&#31616;&#21270;&#30340;DSB&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#39033;&#24037;&#20316;&#30340;&#36129;&#29486;&#20026;&#20808;&#36827;&#30340;&#29983;&#25104;&#24314;&#27169;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14623v1 Announce Type: new  Abstract: This paper introduces a novel theoretical simplification of the Diffusion Schr\"odinger Bridge (DSB) that facilitates its unification with Score-based Generative Models (SGMs), addressing the limitations of DSB in complex data generation and enabling faster convergence and enhanced performance. By employing SGMs as an initial solution for DSB, our approach capitalizes on the strengths of both frameworks, ensuring a more efficient training process and improving the performance of SGM. We also propose a reparameterization technique that, despite theoretical approximations, practically improves the network's fitting capabilities. Our extensive experimental evaluations confirm the effectiveness of the simplified DSB, demonstrating its significant improvements. We believe the contributions of this work pave the way for advanced generative modeling. The code is available at https://github.com/tzco/Simplified-Diffusion-Schrodinger-Bridge.
&lt;/p&gt;</description></item><item><title>Videoshop&#26159;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#35270;&#39057;&#32534;&#36753;&#31639;&#27861;&#65292;&#36890;&#36807;&#22270;&#20687;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26412;&#22320;&#21270;&#35821;&#20041;&#32534;&#36753;&#65292;&#20174;&#32780;&#20801;&#35768;&#29992;&#25143;&#23545;&#35270;&#39057;&#36827;&#34892;&#31934;&#32454;&#25511;&#21046;&#65292;&#21462;&#24471;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#32534;&#36753;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.14617</link><description>&lt;p&gt;
Videoshop&#65306;&#20855;&#26377;&#22122;&#22768;&#22806;&#25512;&#25193;&#25955;&#21453;&#28436;&#30340;&#26412;&#22320;&#21270;&#35821;&#20041;&#35270;&#39057;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Videoshop: Localized Semantic Video Editing with Noise-Extrapolated Diffusion Inversion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14617
&lt;/p&gt;
&lt;p&gt;
Videoshop&#26159;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#35270;&#39057;&#32534;&#36753;&#31639;&#27861;&#65292;&#36890;&#36807;&#22270;&#20687;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26412;&#22320;&#21270;&#35821;&#20041;&#32534;&#36753;&#65292;&#20174;&#32780;&#20801;&#35768;&#29992;&#25143;&#23545;&#35270;&#39057;&#36827;&#34892;&#31934;&#32454;&#25511;&#21046;&#65292;&#21462;&#24471;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#32534;&#36753;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Videoshop&#65292;&#36825;&#26159;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#29992;&#20110;&#26412;&#22320;&#21270;&#35821;&#20041;&#32534;&#36753;&#30340;&#35270;&#39057;&#32534;&#36753;&#31639;&#27861;&#12290;Videoshop&#20801;&#35768;&#29992;&#25143;&#20351;&#29992;&#20219;&#20309;&#32534;&#36753;&#36719;&#20214;&#65292;&#21253;&#25324;Photoshop&#21644;&#29983;&#25104;&#22635;&#20805;&#65292;&#20462;&#25913;&#31532;&#19968;&#24103;&#65307;&#23427;&#20250;&#33258;&#21160;&#23558;&#36825;&#20123;&#26356;&#25913;&#20256;&#25773;&#21040;&#20854;&#20313;&#24103;&#65292;&#20445;&#25345;&#35821;&#20041;&#12289;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#30340;&#19968;&#33268;&#36816;&#21160;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#21482;&#33021;&#36890;&#36807;&#19981;&#31934;&#30830;&#30340;&#25991;&#26412;&#25351;&#20196;&#36827;&#34892;&#32534;&#36753;&#19981;&#21516;&#65292;Videoshop&#20801;&#35768;&#29992;&#25143;&#28155;&#21152;&#25110;&#21024;&#38500;&#23545;&#35937;&#65292;&#35821;&#20041;&#19978;&#26356;&#25913;&#23545;&#35937;&#65292;&#23558;&#32032;&#26448;&#29031;&#29255;&#25554;&#20837;&#35270;&#39057;&#31561;&#65292;&#24182;&#23545;&#20301;&#32622;&#21644;&#22806;&#35266;&#36827;&#34892;&#32454;&#31890;&#24230;&#25511;&#21046;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#28508;&#22312;&#20540;&#36827;&#34892;&#22122;&#22768;&#22806;&#25512;&#21453;&#28436;&#30340;&#22270;&#20687;&#20026;&#22522;&#30784;&#30340;&#35270;&#39057;&#32534;&#36753;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#20174;&#20013;&#25105;&#20204;&#29983;&#25104;&#26681;&#25454;&#32534;&#36753;&#22270;&#20687;&#35843;&#25972;&#30340;&#35270;&#39057;&#12290;Videoshop&#22312;2&#20010;&#32534;&#36753;&#22522;&#20934;&#27979;&#35797;&#20013;&#20351;&#29992;10&#20010;&#35780;&#20272;&#25351;&#26631;&#23545;6&#20010;&#22522;&#32447;&#21462;&#24471;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#32534;&#36753;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14617v1 Announce Type: cross  Abstract: We introduce Videoshop, a training-free video editing algorithm for localized semantic edits. Videoshop allows users to use any editing software, including Photoshop and generative inpainting, to modify the first frame; it automatically propagates those changes, with semantic, spatial, and temporally consistent motion, to the remaining frames. Unlike existing methods that enable edits only through imprecise textual instructions, Videoshop allows users to add or remove objects, semantically change objects, insert stock photos into videos, etc. with fine-grained control over locations and appearance. We achieve this through image-based video editing by inverting latents with noise extrapolation, from which we generate videos conditioned on the edited image. Videoshop produces higher quality edits against 6 baselines on 2 editing benchmarks using 10 evaluation metrics.
&lt;/p&gt;</description></item><item><title>DreamReward&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DreamFL&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#21644;&#25913;&#36827;&#25991;&#26412;&#21040;3D&#27169;&#22411;&#65292;&#22522;&#20110;&#20154;&#31867;&#20559;&#22909;&#21453;&#39304;&#21019;&#24314;&#20102;&#31532;&#19968;&#20010;&#36890;&#29992;&#30340;&#25991;&#26412;&#21040;3D&#20154;&#31867;&#20559;&#22909;&#22870;&#21169;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;Reward3D&#21453;&#39304;&#23398;&#20064;&#31639;&#27861;&#65292;&#25104;&#21151;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#21644;&#19968;&#33268;&#30340;3D&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.14613</link><description>&lt;p&gt;
DreamReward: &#25991;&#26412;&#21040;3D&#29983;&#25104;&#19982;&#20154;&#31867;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
DreamReward: Text-to-3D Generation with Human Preference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14613
&lt;/p&gt;
&lt;p&gt;
DreamReward&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DreamFL&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#21644;&#25913;&#36827;&#25991;&#26412;&#21040;3D&#27169;&#22411;&#65292;&#22522;&#20110;&#20154;&#31867;&#20559;&#22909;&#21453;&#39304;&#21019;&#24314;&#20102;&#31532;&#19968;&#20010;&#36890;&#29992;&#30340;&#25991;&#26412;&#21040;3D&#20154;&#31867;&#20559;&#22909;&#22870;&#21169;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;Reward3D&#21453;&#39304;&#23398;&#20064;&#31639;&#27861;&#65292;&#25104;&#21151;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#21644;&#19968;&#33268;&#30340;3D&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20174;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;3D&#20869;&#23481;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#25991;&#26412;&#21040;3D&#26041;&#27861;&#36890;&#24120;&#29983;&#25104;&#30340;3D&#32467;&#26524;&#19982;&#20154;&#31867;&#20559;&#22909;&#19981;&#19968;&#33268;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;DreamReward&#65292;&#20174;&#20154;&#31867;&#20559;&#22909;&#21453;&#39304;&#20013;&#23398;&#20064;&#21644;&#25913;&#36827;&#25991;&#26412;&#21040;3D&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#22522;&#20110;&#31995;&#32479;&#27880;&#37322;&#27969;&#27700;&#32447;&#65292;&#21253;&#25324;&#25171;&#20998;&#21644;&#25490;&#21517;&#30340;25k&#19987;&#23478;&#27604;&#36739;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;Reward3D&#8212;&#31532;&#19968;&#20010;&#36890;&#29992;&#30340;&#25991;&#26412;&#21040;3D&#20154;&#31867;&#20559;&#22909;&#22870;&#21169;&#27169;&#22411;&#65292;&#20197;&#26377;&#25928;&#22320;&#32534;&#30721;&#20154;&#31867;&#20559;&#22909;&#12290;&#22522;&#20110;3D&#22870;&#21169;&#27169;&#22411;&#65292;&#26368;&#32456;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#24182;&#25552;&#20986;&#20102;Reward3D&#21453;&#39304;&#23398;&#20064;&#65288;DreamFL&#65289;&#65292;&#19968;&#20010;&#30452;&#25509;&#35843;&#20248;&#31639;&#27861;&#65292;&#29992;&#37325;&#26032;&#23450;&#20041;&#30340;&#35780;&#20998;&#32773;&#20248;&#21270;&#22810;&#35270;&#22270;&#25193;&#25955;&#27169;&#22411;&#12290;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#21644;&#22823;&#37327;&#23454;&#39564;&#27604;&#36739;&#65292;&#25105;&#20204;&#30340;DreamReward&#25104;&#21151;&#22320;&#29983;&#25104;&#20102;&#39640;&#20445;&#30495;&#24230;&#21644;&#19968;&#33268;&#30340;3D&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14613v1 Announce Type: cross  Abstract: 3D content creation from text prompts has shown remarkable success recently. However, current text-to-3D methods often generate 3D results that do not align well with human preferences. In this paper, we present a comprehensive framework, coined DreamReward, to learn and improve text-to-3D models from human preference feedback. To begin with, we collect 25k expert comparisons based on a systematic annotation pipeline including rating and ranking. Then, we build Reward3D -- the first general-purpose text-to-3D human preference reward model to effectively encode human preferences. Building upon the 3D reward model, we finally perform theoretical analysis and present the Reward3D Feedback Learning (DreamFL), a direct tuning algorithm to optimize the multi-view diffusion models with a redefined scorer. Grounded by theoretical proof and extensive experiment comparisons, our DreamReward successfully generates high-fidelity and 3D consistent 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#27169;&#22411;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26159;&#36890;&#36807;&#35843;&#25972;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#20197;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#65292;&#24182;&#20943;&#23569;&#24341;&#20837;&#30340;&#38468;&#21152;&#21442;&#25968;&#25110;&#35745;&#31639;&#36164;&#28304;&#25968;&#37327;&#30340;&#23454;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.14608</link><description>&lt;p&gt;
&#22823;&#22411;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14608
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#27169;&#22411;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26159;&#36890;&#36807;&#35843;&#25972;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#20197;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#65292;&#24182;&#20943;&#23569;&#24341;&#20837;&#30340;&#38468;&#21152;&#21442;&#25968;&#25110;&#35745;&#31639;&#36164;&#28304;&#25968;&#37327;&#30340;&#23454;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#27169;&#22411;&#22312;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#20195;&#34920;&#20102;&#19968;&#39033;&#31361;&#30772;&#24615;&#30340;&#36827;&#23637;&#65292;&#20351;&#24471;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#23601;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#31354;&#21069;&#30340;&#35268;&#27169;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#30001;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#32452;&#25104;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#26469;&#25191;&#34892;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#20026;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#23450;&#21046;&#22823;&#22411;&#27169;&#22411;&#26102;&#65292;&#23588;&#20854;&#26159;&#22312;&#21463;&#21040;&#35745;&#31639;&#33021;&#21147;&#38480;&#21046;&#30340;&#30828;&#20214;&#24179;&#21488;&#19978;&#65292;&#35268;&#27169;&#24222;&#22823;&#21644;&#35745;&#31639;&#35201;&#27714;&#24040;&#22823;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35843;&#25972;&#22823;&#22411;&#27169;&#22411;&#20197;&#36866;&#24212;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;PEFT&#26159;&#25351;&#35843;&#25972;&#39044;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#20351;&#20854;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#30340;&#36807;&#31243;&#65292;&#21516;&#26102;&#23613;&#37327;&#20943;&#23569;&#24341;&#20837;&#30340;&#38468;&#21152;&#21442;&#25968;&#25110;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14608v1 Announce Type: new  Abstract: Large models represent a groundbreaking advancement in multiple application fields, enabling remarkable achievements across various tasks. However, their unprecedented scale comes with significant computational costs. These models, often consisting of billions of parameters, require vast amounts of computational resources for execution. Especially, the expansive scale and computational demands pose considerable challenges when customizing them for particular downstream tasks, particularly over the hardware platforms constrained by computational capabilities. Parameter Efficient Fine-Tuning (PEFT) provides a practical solution by efficiently adapt the large models over the various downstream tasks. In particular, PEFT refers to the process of adjusting the parameters of a pre-trained large models to adapt it to a specific task while minimizing the number of additional parameters introduced or computational resources required. This approac
&lt;/p&gt;</description></item><item><title>&#21487;&#24494;&#20998;&#32534;&#31243;&#26159;&#19968;&#20010;&#26032;&#30340;&#32534;&#31243;&#33539;&#24335;&#65292;&#20351;&#24471;&#22797;&#26434;&#31243;&#24207;&#33021;&#22815;&#31471;&#23545;&#31471;&#22320;&#36827;&#34892;&#24494;&#20998;&#65292;&#23454;&#29616;&#22522;&#20110;&#26799;&#24230;&#30340;&#21442;&#25968;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.14606</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#32534;&#31243;&#30340;&#35201;&#32032;
&lt;/p&gt;
&lt;p&gt;
The Elements of Differentiable Programming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14606
&lt;/p&gt;
&lt;p&gt;
&#21487;&#24494;&#20998;&#32534;&#31243;&#26159;&#19968;&#20010;&#26032;&#30340;&#32534;&#31243;&#33539;&#24335;&#65292;&#20351;&#24471;&#22797;&#26434;&#31243;&#24207;&#33021;&#22815;&#31471;&#23545;&#31471;&#22320;&#36827;&#34892;&#24494;&#20998;&#65292;&#23454;&#29616;&#22522;&#20110;&#26799;&#24230;&#30340;&#21442;&#25968;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#26368;&#36817;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#36825;&#24471;&#30410;&#20110;&#22823;&#22411;&#27169;&#22411;&#12289;&#24222;&#22823;&#25968;&#25454;&#38598;&#12289;&#21152;&#36895;&#30828;&#20214;&#65292;&#20197;&#21450;&#21487;&#24494;&#20998;&#32534;&#31243;&#30340;&#21464;&#38761;&#24615;&#21147;&#37327;&#12290;&#36825;&#31181;&#26032;&#30340;&#32534;&#31243;&#33539;&#24335;&#20351;&#22797;&#26434;&#35745;&#31639;&#26426;&#31243;&#24207;&#65288;&#21253;&#25324;&#20855;&#26377;&#25511;&#21046;&#27969;&#21644;&#25968;&#25454;&#32467;&#26500;&#30340;&#31243;&#24207;&#65289;&#33021;&#22815;&#36827;&#34892;&#31471;&#23545;&#31471;&#30340;&#24494;&#20998;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#31243;&#24207;&#21442;&#25968;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#12290;&#19981;&#20165;&#20165;&#26159;&#31243;&#24207;&#30340;&#24494;&#20998;&#65292;&#21487;&#24494;&#20998;&#32534;&#31243;&#20063;&#21253;&#25324;&#20102;&#31243;&#24207;&#20248;&#21270;&#12289;&#27010;&#29575;&#31561;&#22810;&#20010;&#39046;&#22495;&#30340;&#27010;&#24565;&#12290;&#26412;&#20070;&#20171;&#32461;&#20102;&#21487;&#24494;&#20998;&#32534;&#31243;&#25152;&#38656;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#24182;&#37319;&#29992;&#20102;&#20248;&#21270;&#21644;&#27010;&#29575;&#20004;&#20010;&#20027;&#35201;&#35270;&#35282;&#36827;&#34892;&#38416;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14606v1 Announce Type: new  Abstract: Artificial intelligence has recently experienced remarkable advances, fueled by large models, vast datasets, accelerated hardware, and, last but not least, the transformative power of differentiable programming. This new programming paradigm enables end-to-end differentiation of complex computer programs (including those with control flows and data structures), making gradient-based optimization of program parameters possible. As an emerging paradigm, differentiable programming builds upon several areas of computer science and applied mathematics, including automatic differentiation, graphical models, optimization and statistics. This book presents a comprehensive review of the fundamental concepts useful for differentiable programming. We adopt two main perspectives, that of optimization and that of probability, with clear analogies between the two. Differentiable programming is not merely the differentiation of programs, but also the t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36845;&#20195;&#28155;&#21152;&#22122;&#22768;&#36827;&#34892;&#30495;&#23454;&#22270;&#20687;&#21453;&#36716;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#22686;&#21152;&#25805;&#20316;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;&#37325;&#24314;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.14602</link><description>&lt;p&gt;
ReNoise&#65306;&#36890;&#36807;&#36845;&#20195;&#28155;&#21152;&#22122;&#22768;&#36827;&#34892;&#30495;&#23454;&#22270;&#20687;&#21453;&#36716;
&lt;/p&gt;
&lt;p&gt;
ReNoise: Real Image Inversion Through Iterative Noising
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36845;&#20195;&#28155;&#21152;&#22122;&#22768;&#36827;&#34892;&#30495;&#23454;&#22270;&#20687;&#21453;&#36716;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#22686;&#21152;&#25805;&#20316;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;&#37325;&#24314;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25991;&#26412;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#37322;&#25918;&#20986;&#24378;&#22823;&#30340;&#22270;&#20687;&#25805;&#20316;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#30495;&#23454;&#22270;&#20687;&#38656;&#35201;&#23558;&#22270;&#20687;&#21453;&#36716;&#21040;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#39046;&#22495;&#12290;&#23454;&#29616;&#24544;&#23454;&#30340;&#21453;&#36716;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26368;&#36817;&#35757;&#32451;&#29992;&#20110;&#20135;&#29983;&#20855;&#26377;&#23569;&#37327;&#21435;&#22122;&#27493;&#39588;&#30340;&#22270;&#20687;&#30340;&#27169;&#22411;&#32780;&#35328;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#36136;&#37327;&#21644;&#25805;&#20316;&#27604;&#30340;&#21453;&#36716;&#26041;&#27861;&#65292;&#22686;&#24378;&#37325;&#24314;&#31934;&#24230;&#32780;&#19981;&#22686;&#21152;&#25805;&#20316;&#25968;&#37327;&#12290;&#22522;&#20110;&#21453;&#36716;&#25193;&#25955;&#37319;&#26679;&#36807;&#31243;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27599;&#20010;&#21453;&#36716;&#37319;&#26679;&#27493;&#39588;&#20013;&#20351;&#29992;&#20102;&#36845;&#20195;&#28155;&#21152;&#22122;&#22768;&#30340;&#26426;&#21046;&#12290;&#36825;&#31181;&#26426;&#21046;&#36890;&#36807;&#22312;&#39044;&#27979;&#30340;&#28857;&#27839;&#30528;&#21069;&#21521;&#25193;&#25955;&#36712;&#36857;&#19978;&#36845;&#20195;&#22320;&#24212;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#24179;&#22343;&#36825;&#20123;&#39044;&#27979;&#20540;&#26469;&#25913;&#36827;&#36817;&#20284;&#20540;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;ReNoise&#25216;&#26415;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14602v1 Announce Type: cross  Abstract: Recent advancements in text-guided diffusion models have unlocked powerful image manipulation capabilities. However, applying these methods to real images necessitates the inversion of the images into the domain of the pretrained diffusion model. Achieving faithful inversion remains a challenge, particularly for more recent models trained to generate images with a small number of denoising steps. In this work, we introduce an inversion method with a high quality-to-operation ratio, enhancing reconstruction accuracy without increasing the number of operations. Building on reversing the diffusion sampling process, our method employs an iterative renoising mechanism at each inversion sampling step. This mechanism refines the approximation of a predicted point along the forward diffusion trajectory, by iteratively applying the pretrained diffusion model, and averaging these predictions. We evaluate the performance of our ReNoise technique 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#20027;&#30340;&#12289;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25805;&#20316;&#22120;&#26694;&#26550;&#65292;&#23558;&#20154;&#22312;&#22238;&#36335;&#21407;&#21017;&#21644;&#25193;&#23637;&#29616;&#23454;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#20419;&#36827;&#20154;&#19982;&#26426;&#22120;&#20154;&#20043;&#38388;&#30452;&#35266;&#30340;&#27807;&#36890;&#21644;&#32534;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.14597</link><description>&lt;p&gt;
&#25193;&#23637;&#29616;&#23454;&#29992;&#20110;&#22686;&#24378;&#20154;&#26426;&#21327;&#20316;&#65306;&#19968;&#31181;&#20154;&#22312;&#22238;&#36335;&#20013;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Extended Reality for Enhanced Human-Robot Collaboration: a Human-in-the-Loop Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#20027;&#30340;&#12289;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25805;&#20316;&#22120;&#26694;&#26550;&#65292;&#23558;&#20154;&#22312;&#22238;&#36335;&#21407;&#21017;&#21644;&#25193;&#23637;&#29616;&#23454;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#20419;&#36827;&#20154;&#19982;&#26426;&#22120;&#20154;&#20043;&#38388;&#30452;&#35266;&#30340;&#27807;&#36890;&#21644;&#32534;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#30340;&#23835;&#36215;&#20026;&#21046;&#36896;&#36807;&#31243;&#30340;&#39640;&#25928;&#29575;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#20294;&#24448;&#24448;&#29306;&#29298;&#20102;&#21450;&#26102;&#21709;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#24066;&#22330;&#38656;&#27714;&#21644;&#28385;&#36275;&#23450;&#21046;&#38656;&#27714;&#25152;&#38656;&#30340;&#28789;&#27963;&#24615;&#12290;&#20154;&#26426;&#21327;&#20316;&#35797;&#22270;&#36890;&#36807;&#23558;&#26426;&#22120;&#30340;&#21147;&#37327;&#21644;&#31934;&#24230;&#19982;&#20154;&#31867;&#30340;&#26426;&#26234;&#21644;&#24863;&#30693;&#29702;&#35299;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#26412;&#25991;&#27010;&#24565;&#21270;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29616;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20154;&#22312;&#22238;&#36335;&#21407;&#21017;&#19982;&#25193;&#23637;&#29616;&#23454;&#65288;XR&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#20415;&#20110;&#20154;&#19982;&#26426;&#22120;&#20154;&#20043;&#38388;&#36827;&#34892;&#30452;&#35266;&#27807;&#36890;&#21644;&#32534;&#31243;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#27010;&#24565;&#26694;&#26550;&#39044;&#35265;&#21040;&#20102;&#20154;&#30452;&#25509;&#21442;&#19982;&#26426;&#22120;&#20154;&#23398;&#20064;&#36807;&#31243;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#36866;&#24212;&#24615;&#21644;&#20219;&#21153;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#37325;&#28857;&#20171;&#32461;&#20102;&#25903;&#25345;&#25152;&#25552;&#20986;&#26694;&#26550;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#24378;&#35843;&#20102;&#23454;&#29616;&#36825;&#19968;&#26694;&#26550;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14597v1 Announce Type: cross  Abstract: The rise of automation has provided an opportunity to achieve higher efficiency in manufacturing processes, yet it often compromises the flexibility required to promptly respond to evolving market needs and meet the demand for customization. Human-robot collaboration attempts to tackle these challenges by combining the strength and precision of machines with human ingenuity and perceptual understanding. In this paper, we conceptualize and propose an implementation framework for an autonomous, machine learning-based manipulator that incorporates human-in-the-loop principles and leverages Extended Reality (XR) to facilitate intuitive communication and programming between humans and robots. Furthermore, the conceptual framework foresees human involvement directly in the robot learning process, resulting in higher adaptability and task generalization. The paper highlights key technologies enabling the proposed framework, emphasizing the im
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#24605;&#32771;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#27169;&#20223;&#21644;&#21487;&#36716;&#31227;&#22870;&#21169;&#24674;&#22797;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#26694;&#26550;PPO-AIRL + SAC&#20197;&#35299;&#20915;SAC&#31639;&#27861;&#22312;AIRL&#35757;&#32451;&#20013;&#26080;&#27861;&#20840;&#38754;&#35299;&#24320;&#22870;&#21169;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.14593</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;&#65306;&#20174;&#31574;&#30053;&#27169;&#20223;&#21644;&#21487;&#36716;&#31227;&#22870;&#21169;&#24674;&#22797;&#30340;&#35282;&#24230;
&lt;/p&gt;
&lt;p&gt;
Rethinking Adversarial Inverse Reinforcement Learning: From the Angles of Policy Imitation and Transferable Reward Recovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14593
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#27169;&#20223;&#21644;&#21487;&#36716;&#31227;&#22870;&#21169;&#24674;&#22797;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#26694;&#26550;PPO-AIRL + SAC&#20197;&#35299;&#20915;SAC&#31639;&#27861;&#22312;AIRL&#35757;&#32451;&#20013;&#26080;&#27861;&#20840;&#38754;&#35299;&#24320;&#22870;&#21169;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;AIRL&#65289;&#20316;&#20026;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#22522;&#30707;&#26041;&#27861;&#12290;&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;AIRL&#30340;&#20004;&#20010;&#19981;&#21516;&#35282;&#24230;&#65306;&#31574;&#30053;&#27169;&#20223;&#21644;&#21487;&#36716;&#31227;&#22870;&#21169;&#24674;&#22797;&#12290;&#25105;&#20204;&#20174;&#29992;Soft Actor-Critic&#65288;SAC&#65289;&#26367;&#25442;AIRL&#20013;&#30340;&#20869;&#32622;&#31639;&#27861;&#24320;&#22987;&#65292;&#20197;&#22686;&#24378;&#26679;&#26412;&#25928;&#29575;&#65292;&#36825;&#35201;&#24402;&#21151;&#20110;SAC&#30340;&#31163;&#31574;&#30053;&#24418;&#24335;&#21644;&#30456;&#23545;&#20110;AIRL&#32780;&#35328;&#21487;&#35782;&#21035;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#27169;&#22411;&#12290;&#36825;&#30830;&#23454;&#22312;&#31574;&#30053;&#27169;&#20223;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20294;&#19981;&#24910;&#32473;&#21487;&#36716;&#31227;&#22870;&#21169;&#24674;&#22797;&#24102;&#26469;&#20102;&#32570;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#38416;&#36848;&#20102;SAC&#31639;&#27861;&#26412;&#36523;&#22312;AIRL&#35757;&#32451;&#36807;&#31243;&#20013;&#26080;&#27861;&#20840;&#38754;&#35299;&#24320;&#22870;&#21169;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#26694;&#26550;&#65292;PPO-AIRL + SAC&#65292;&#20197;&#33719;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#36716;&#31227;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#29615;&#22659;&#25552;&#21462;&#35299;&#24320;&#30340;&#22870;&#21169;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14593v1 Announce Type: new  Abstract: Adversarial inverse reinforcement learning (AIRL) stands as a cornerstone approach in imitation learning. This paper rethinks the two different angles of AIRL: policy imitation and transferable reward recovery. We begin with substituting the built-in algorithm in AIRL with soft actor-critic (SAC) during the policy optimization process to enhance sample efficiency, thanks to the off-policy formulation of SAC and identifiable Markov decision process (MDP) models with respect to AIRL. It indeed exhibits a significant improvement in policy imitation but accidentally brings drawbacks to transferable reward recovery. To learn this issue, we illustrate that the SAC algorithm itself is not feasible to disentangle the reward function comprehensively during the AIRL training process, and propose a hybrid framework, PPO-AIRL + SAC, for satisfactory transfer effect. Additionally, we analyze the capability of environments to extract disentangled rewa
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;A$^3$T&#26694;&#26550;&#65292;&#36890;&#36807;ActRe&#25552;&#31034;&#20195;&#29702;&#23454;&#29616;&#20102;ReAct&#39118;&#26684;&#20195;&#29702;&#23545;&#20195;&#29702;&#36712;&#36857;&#30340;&#33258;&#20027;&#26631;&#27880;&#65292;&#21516;&#26102;&#22686;&#24378;&#20102;&#26032;&#30340;&#36712;&#36857;&#21512;&#25104;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.14589</link><description>&lt;p&gt;
ReAct&#36935;&#19978;ActRe&#65306;&#23545;&#27604;&#24615;&#33258;&#35757;&#32451;&#20013;&#30340;&#20195;&#29702;&#36712;&#36857;&#33258;&#21160;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for Contrastive Self-Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14589
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;A$^3$T&#26694;&#26550;&#65292;&#36890;&#36807;ActRe&#25552;&#31034;&#20195;&#29702;&#23454;&#29616;&#20102;ReAct&#39118;&#26684;&#20195;&#29702;&#23545;&#20195;&#29702;&#36712;&#36857;&#30340;&#33258;&#20027;&#26631;&#27880;&#65292;&#21516;&#26102;&#22686;&#24378;&#20102;&#26032;&#30340;&#36712;&#36857;&#21512;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14589v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#25991;&#25688;&#65306;&#35821;&#35328;&#20195;&#29702;&#36890;&#36807;&#19982;&#22522;&#30784;&#27169;&#22411;&#25512;&#29702;&#23637;&#31034;&#20102;&#33258;&#20027;&#20915;&#31574;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#36890;&#36807;&#22810;&#27493;&#25512;&#29702;&#21644;&#34892;&#21160;&#36712;&#36857;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#26469;&#35757;&#32451;&#35821;&#35328;&#20195;&#29702;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#36825;&#26679;&#30340;&#36712;&#36857;&#20173;&#38656;&#35201;&#30456;&#24403;&#22823;&#30340;&#20154;&#21147;&#65292;&#26080;&#35770;&#26159;&#36890;&#36807;&#20154;&#24037;&#26631;&#27880;&#36824;&#26159;&#23454;&#26045;&#22810;&#26679;&#21270;&#25552;&#31034;&#26694;&#26550;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;A$^3$T&#65292;&#19968;&#20010;&#20801;&#35768;&#20197;ReAct&#39118;&#26684;&#33258;&#20027;&#27880;&#37322;&#20195;&#29702;&#36712;&#36857;&#30340;&#26694;&#26550;&#12290;&#20854;&#20013;&#24515;&#26159;&#19968;&#20010;ActRe&#25552;&#31034;&#20195;&#29702;&#65292;&#23427;&#35299;&#37322;&#20219;&#24847;&#21160;&#20316;&#30340;&#21407;&#22240;&#12290;&#24403;&#38543;&#26426;&#25277;&#21462;&#22806;&#37096;&#21160;&#20316;&#26102;&#65292;ReAct&#39118;&#26684;&#20195;&#29702;&#21487;&#20197;&#26597;&#35810;ActRe&#20195;&#29702;&#20197;&#33719;&#21462;&#20854;&#25991;&#26412;&#29702;&#30001;&#12290;&#26032;&#39062;&#30340;&#36712;&#36857;&#28982;&#21518;&#36890;&#36807;&#23558;ActRe&#30340;&#21518;&#39564;&#25512;&#29702;&#21069;&#32622;&#21040;&#25277;&#26679;&#21160;&#20316;&#20013;&#36827;&#34892;&#32508;&#21512;&#21512;&#25104;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;ReAct&#39118;&#26684;&#20195;&#29702;&#21487;&#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14589v1 Announce Type: new  Abstract: Language agents have demonstrated autonomous decision-making abilities by reasoning with foundation models. Recently, efforts have been made to train language agents for performance improvement, with multi-step reasoning and action trajectories as the training data. However, collecting such trajectories still requires considerable human effort, by either artificial annotations or implementations of diverse prompting frameworks. In this work, we propose A$^3$T, a framework that enables the Autonomous Annotation of Agent Trajectories in the style of ReAct. The central role is an ActRe prompting agent, which explains the reason for an arbitrary action. When randomly sampling an external action, the ReAct-style agent could query the ActRe agent with the action to obtain its textual rationales. Novel trajectories are then synthesized by prepending the posterior reasoning from ActRe to the sampled action. In this way, the ReAct-style agent exe
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#20102;&#32447;&#24615;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#20960;&#31181;&#27969;&#34892;&#30340;&#32447;&#24615;&#27169;&#22411;&#21464;&#20307;&#31561;&#25928;&#20110;&#26631;&#20934;&#30340;&#32447;&#24615;&#22238;&#24402;&#65292;&#25552;&#20379;&#20102;&#23454;&#39564;&#35777;&#25454;&#25903;&#25345;&#36825;&#19968;&#32467;&#35770;&#12290;</title><link>https://arxiv.org/abs/2403.14587</link><description>&lt;p&gt;
&#32447;&#24615;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Analysis of Linear Time Series Forecasting Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14587
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#32447;&#24615;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#20960;&#31181;&#27969;&#34892;&#30340;&#32447;&#24615;&#27169;&#22411;&#21464;&#20307;&#31561;&#25928;&#20110;&#26631;&#20934;&#30340;&#32447;&#24615;&#22238;&#24402;&#65292;&#25552;&#20379;&#20102;&#23454;&#39564;&#35777;&#25454;&#25903;&#25345;&#36825;&#19968;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31616;&#21333;&#65292;&#32447;&#24615;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#21363;&#20351;&#19982;&#26356;&#28145;&#23618;&#27425;&#21644;&#26356;&#26114;&#36149;&#30340;&#27169;&#22411;&#36827;&#34892;&#23545;&#27604;&#20063;&#26159;&#22914;&#27492;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#32447;&#24615;&#27169;&#22411;&#30340;&#21464;&#20307;&#65292;&#36890;&#24120;&#21253;&#25324;&#26576;&#31181;&#24418;&#24335;&#30340;&#29305;&#24449;&#35268;&#33539;&#21270;&#65292;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20351;&#29992;&#36825;&#20123;&#32447;&#24615;&#27169;&#22411;&#26550;&#26500;&#21487;&#20197;&#34920;&#36798;&#30340;&#20989;&#25968;&#38598;&#21512;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20960;&#31181;&#27969;&#34892;&#30340;&#32447;&#24615;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#21464;&#20307;&#31561;&#25928;&#65292;&#24182;&#22312;&#21151;&#33021;&#19978;&#26080;&#27861;&#21306;&#20998;&#26631;&#20934;&#30340;&#38750;&#32422;&#26463;&#32447;&#24615;&#22238;&#24402;&#12290;&#25105;&#20204;&#20026;&#27599;&#31181;&#32447;&#24615;&#21464;&#20307;&#25551;&#36848;&#20102;&#27169;&#22411;&#31867;&#21035;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#27599;&#20010;&#27169;&#22411;&#37117;&#21487;&#20197;&#37325;&#26032;&#35299;&#37322;&#20026;&#22312;&#36866;&#24403;&#25193;&#20805;&#30340;&#29305;&#24449;&#38598;&#19978;&#30340;&#38750;&#32422;&#26463;&#32447;&#24615;&#22238;&#24402;&#65292;&#22240;&#27492;&#22312;&#20351;&#29992;&#22343;&#26041;&#25439;&#22833;&#20989;&#25968;&#26102;&#21487;&#20197;&#24471;&#21040;&#23553;&#38381;&#24418;&#24335;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20379;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#24453;&#26816;&#26597;&#30340;&#27169;&#22411;&#23398;&#20064;&#21040;&#20102;&#20960;&#20046;&#30456;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14587v1 Announce Type: new  Abstract: Despite their simplicity, linear models perform well at time series forecasting, even when pitted against deeper and more expensive models. A number of variations to the linear model have been proposed, often including some form of feature normalisation that improves model generalisation. In this paper we analyse the sets of functions expressible using these linear model architectures. In so doing we show that several popular variants of linear models for time series forecasting are equivalent and functionally indistinguishable from standard, unconstrained linear regression. We characterise the model classes for each linear variant. We demonstrate that each model can be reinterpreted as unconstrained linear regression over a suitably augmented feature set, and therefore admit closed-form solutions when using a mean-squared loss function. We provide experimental evidence that the models under inspection learn nearly identical solutions, a
&lt;/p&gt;</description></item><item><title>&#23558;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#21644;&#21608;&#22260;&#29615;&#22659;&#35270;&#20026;&#20849;&#21516;&#28436;&#21270;&#30340;&#31995;&#32479;&#65292;&#25552;&#20986;&#26234;&#33021;&#20307;-&#29615;&#22659;&#21327;&#21516;&#20248;&#21270;&#38382;&#39064;&#24182;&#24320;&#21457;&#21327;&#35843;&#31639;&#27861;&#65292;&#20197;&#25913;&#36827;&#21435;&#20013;&#24515;&#21270;&#22810;&#26234;&#33021;&#20307;&#23548;&#33322;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.14583</link><description>&lt;p&gt;
&#20026;&#21435;&#20013;&#24515;&#21270;&#22810;&#26234;&#33021;&#20307;&#23548;&#33322;&#30340;&#29615;&#22659;&#21644;&#25919;&#31574;&#36827;&#34892;&#21327;&#21516;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Co-Optimization of Environment and Policies for Decentralized Multi-Agent Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14583
&lt;/p&gt;
&lt;p&gt;
&#23558;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#21644;&#21608;&#22260;&#29615;&#22659;&#35270;&#20026;&#20849;&#21516;&#28436;&#21270;&#30340;&#31995;&#32479;&#65292;&#25552;&#20986;&#26234;&#33021;&#20307;-&#29615;&#22659;&#21327;&#21516;&#20248;&#21270;&#38382;&#39064;&#24182;&#24320;&#21457;&#21327;&#35843;&#31639;&#27861;&#65292;&#20197;&#25913;&#36827;&#21435;&#20013;&#24515;&#21270;&#22810;&#26234;&#33021;&#20307;&#23548;&#33322;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#23558;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#21450;&#20854;&#21608;&#22260;&#29615;&#22659;&#35270;&#20026;&#19968;&#20010;&#20849;&#21516;&#28436;&#21270;&#30340;&#31995;&#32479;&#65292;&#20854;&#20013;&#19968;&#20010;&#30340;&#34892;&#20026;&#20250;&#24433;&#21709;&#21478;&#19968;&#20010;&#12290;&#20854;&#30446;&#26631;&#26159;&#23558;&#26234;&#33021;&#20307;&#34892;&#20026;&#21644;&#29615;&#22659;&#37197;&#32622;&#37117;&#35270;&#20026;&#20915;&#31574;&#21464;&#37327;&#65292;&#24182;&#20197;&#21327;&#35843;&#30340;&#26041;&#24335;&#20248;&#21270;&#36825;&#20004;&#20010;&#32452;&#20214;&#65292;&#20197;&#25913;&#36827;&#26576;&#20123;&#24863;&#20852;&#36259;&#30340;&#24230;&#37327;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#25317;&#25380;&#29615;&#22659;&#20013;&#30340;&#21435;&#20013;&#24515;&#21270;&#22810;&#26234;&#33021;&#20307;&#23548;&#33322;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#26234;&#33021;&#20307;&#23548;&#33322;&#21644;&#29615;&#22659;&#20248;&#21270;&#30340;&#20004;&#20010;&#23376;&#30446;&#26631;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#26234;&#33021;&#20307;-&#29615;&#22659;&#21327;&#21516;&#20248;&#21270;&#8221;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#8220;&#21327;&#35843;&#31639;&#27861;&#8221;&#65292;&#22312;&#36825;&#20004;&#20010;&#23376;&#30446;&#26631;&#20043;&#38388;&#20132;&#26367;&#20197;&#23547;&#25214;&#26234;&#33021;&#20307;&#34892;&#20026;&#21644;&#38556;&#30861;&#29289;&#29615;&#22659;&#37197;&#32622;&#30340;&#26368;&#20339;&#32508;&#21512;&#65307;&#26368;&#32456;&#25552;&#39640;&#20102;&#23548;&#33322;&#24615;&#33021;&#12290;&#30001;&#20110;&#26126;&#30830;&#24314;&#27169;&#26234;&#33021;&#20307;&#12289;&#29615;&#22659;&#21644;&#24615;&#33021;&#20043;&#38388;&#20851;&#31995;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14583v1 Announce Type: cross  Abstract: This work views the multi-agent system and its surrounding environment as a co-evolving system, where the behavior of one affects the other. The goal is to take both agent actions and environment configurations as decision variables, and optimize these two components in a coordinated manner to improve some measure of interest. Towards this end, we consider the problem of decentralized multi-agent navigation in cluttered environments. By introducing two sub-objectives of multi-agent navigation and environment optimization, we propose an $\textit{agent-environment co-optimization}$ problem and develop a $\textit{coordinated algorithm}$ that alternates between these sub-objectives to search for an optimal synthesis of agent actions and obstacle configurations in the environment; ultimately, improving the navigation performance. Due to the challenge of explicitly modeling the relation between agents, environment and performance, we leverag
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;RAmBLA&#26694;&#26550;&#65292;&#35780;&#20272;&#22235;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;LLMs&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#26159;&#21542;&#21487;&#20197;&#20316;&#20026;&#21487;&#38752;&#21161;&#25163;&#65292;&#22312;&#20219;&#21153;&#35774;&#35745;&#21644;&#24615;&#33021;&#35780;&#20272;&#20013;&#24378;&#35843;&#20102;&#25552;&#31034;&#30340;&#20581;&#22766;&#24615;&#12289;&#39640;&#21484;&#22238;&#29575;&#21644;&#32570;&#20047;&#24187;&#35273;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14578</link><description>&lt;p&gt;
RAmBLA&#65306;&#35780;&#20272;LLMs&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#20316;&#20026;&#21161;&#25163;&#30340;&#21487;&#38752;&#24615;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
RAmBLA: A Framework for Evaluating the Reliability of LLMs as Assistants in the Biomedical Domain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14578
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;RAmBLA&#26694;&#26550;&#65292;&#35780;&#20272;&#22235;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;LLMs&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#26159;&#21542;&#21487;&#20197;&#20316;&#20026;&#21487;&#38752;&#21161;&#25163;&#65292;&#22312;&#20219;&#21153;&#35774;&#35745;&#21644;&#24615;&#33021;&#35780;&#20272;&#20013;&#24378;&#35843;&#20102;&#25552;&#31034;&#30340;&#20581;&#22766;&#24615;&#12289;&#39640;&#21484;&#22238;&#29575;&#21644;&#32570;&#20047;&#24187;&#35273;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36234;&#26469;&#36234;&#22810;&#22320;&#25903;&#25345;&#21508;&#31181;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#19968;&#20123;&#28508;&#22312;&#24433;&#21709;&#31038;&#20250;&#30340;&#39046;&#22495;&#65292;&#22914;&#29983;&#29289;&#21307;&#23398;&#65292;&#28982;&#32780;&#23427;&#20204;&#22312;&#29616;&#23454;&#29992;&#20363;&#20013;&#30340;&#21487;&#38752;&#24615;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#35780;&#20272;&#29983;&#29289;&#21307;&#23398;LLM&#21161;&#25163;&#21487;&#38752;&#24615;&#30340;RAmBLA&#26694;&#26550;&#65292;&#24182;&#35780;&#20272;&#22235;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;LLMs&#26159;&#21542;&#21487;&#20197;&#20316;&#20026;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#21487;&#38752;&#21161;&#25163;&#12290;&#25105;&#20204;&#30830;&#23450;&#25552;&#31034;&#30340;&#20581;&#22766;&#24615;&#12289;&#39640;&#21484;&#22238;&#29575;&#21644;&#32570;&#20047;&#24187;&#35273;&#26159;&#36825;&#31181;&#29992;&#20363;&#30340;&#24517;&#35201;&#26631;&#20934;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#27169;&#25311;&#30495;&#23454;&#29992;&#25143;&#20132;&#20114;&#30340;&#30701;&#34920;&#21333;&#20219;&#21153;&#21644;&#38656;&#35201;LLM&#33258;&#30001;&#24418;&#24335;&#22238;&#31572;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#35780;&#20272;&#32773;LLM&#36890;&#36807;&#19982;&#30495;&#23454;&#21709;&#24212;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#26469;&#35780;&#20272;LLM&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14578v1 Announce Type: new  Abstract: Large Language Models (LLMs) increasingly support applications in a wide range of domains, some with potential high societal impact such as biomedicine, yet their reliability in realistic use cases is under-researched. In this work we introduce the Reliability AssesMent for Biomedical LLM Assistants (RAmBLA) framework and evaluate whether four state-of-the-art foundation LLMs can serve as reliable assistants in the biomedical domain. We identify prompt robustness, high recall, and a lack of hallucinations as necessary criteria for this use case. We design shortform tasks and tasks requiring LLM freeform responses mimicking real-world user interactions. We evaluate LLM performance using semantic similarity with a ground truth response, through an evaluator LLM.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#27010;&#24565;&#26041;&#27861;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35299;&#37322;&#24615;&#25913;&#36827;&#65292;&#25552;&#20379;&#20102;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#20915;&#31574;&#35299;&#37322;&#65292;&#20351;&#24471;&#26816;&#27979;&#20266;&#20851;&#32852;&#21644;&#22266;&#26377;&#20559;&#35265;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.14566</link><description>&lt;p&gt;
&#19968;&#39033;&#20851;&#20110;&#22522;&#20110;&#27010;&#24565;&#26041;&#27861;&#25913;&#36827;&#27169;&#22411;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A survey on Concept-based Approaches For Model Improvement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14566
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27010;&#24565;&#26041;&#27861;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35299;&#37322;&#24615;&#25913;&#36827;&#65292;&#25552;&#20379;&#20102;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#20915;&#31574;&#35299;&#37322;&#65292;&#20351;&#24471;&#26816;&#27979;&#20266;&#20851;&#32852;&#21644;&#22266;&#26377;&#20559;&#35265;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30740;&#31350;&#30340;&#37325;&#28857;&#24050;&#32463;&#20174;&#20165;&#20165;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#36716;&#21464;&#20026;&#20351;DNN&#26356;&#26131;&#35299;&#37322;&#32473;&#20154;&#31867;&#12290;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#24050;&#32463;&#35266;&#23519;&#21040;&#21508;&#31181;&#25216;&#26415;&#65292;&#21253;&#25324;&#22522;&#20110;&#26174;&#33879;&#24615;&#21644;&#22522;&#20110;&#27010;&#24565;&#30340;&#26041;&#27861;&#12290;&#22522;&#20110;&#27010;&#24565;&#30340;&#26041;&#27861;&#29992;&#25152;&#35859;&#30340;&#27010;&#24565;&#22312;&#31616;&#21333;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#26415;&#35821;&#20013;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;&#27010;&#24565;&#26159;&#25968;&#25454;&#30340;&#20154;&#31867;&#21487;&#35299;&#37322;&#21333;&#20803;&#65292;&#26159;&#20154;&#31867;&#24605;&#32500;&#30340;&#22522;&#30707;&#12290;&#29992;&#27010;&#24565;&#30340;&#35299;&#37322;&#33021;&#22815;&#26816;&#27979;&#21040;&#20266;&#20851;&#32852;&#12289;&#22266;&#26377;&#20559;&#35265;&#25110;&#32874;&#26126;&#27721;&#12290;&#38543;&#30528;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#30340;&#20986;&#29616;&#65292;&#20986;&#29616;&#20102;&#21508;&#31181;&#27010;&#24565;&#34920;&#31034;&#26041;&#27861;&#21644;&#33258;&#21160;&#27010;&#24565;&#21457;&#29616;&#31639;&#27861;&#12290;&#19968;&#20123;&#26368;&#36817;&#30340;&#26041;&#27861;&#20351;&#29992;&#27010;&#24565;&#36827;&#34892;&#20107;&#21518;&#27169;&#22411;&#35299;&#32544;&#35780;&#20272;&#65292;&#32780;&#20854;&#20182;&#20154;&#20351;&#29992;&#23427;&#20204;&#36827;&#34892;&#20107;&#21069;&#35757;&#32451;&#12290;&#22522;&#20110;&#27010;&#24565;&#30340;&#26041;&#27861;&#26159;&#26032;&#30340;&#65292;&#26377;&#35768;&#22810;&#34920;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14566v1 Announce Type: new  Abstract: The focus of recent research has shifted from merely increasing the Deep Neural Networks (DNNs) performance in various tasks to DNNs, which are more interpretable to humans. The field of eXplainable Artificial Intelligence (XAI) has observed various techniques, including saliency-based and concept-based approaches. Concept-based approaches explain the model's decisions in simple human understandable terms called Concepts. Concepts are human interpretable units of data and are the thinking ground of humans. Explanations in terms of concepts enable detecting spurious correlations, inherent biases, or clever-hans. With the advent of concept-based explanations, there have been various concept representation methods and automatic concept discovery algorithms. Some recent methods use concepts for post-hoc model disentanglement evaluation, while others use them for ante-hoc training. The concept-based approaches are new, with many representatio
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;LexiContrastive Grounding (LCG)&#65292;&#23427;&#32467;&#21512;&#20102;&#35270;&#35273;&#30417;&#30563;&#21644;&#25991;&#26412;&#34920;&#31034;&#25913;&#36827;&#31574;&#30053;&#65292;&#22312;&#22810;&#20010;&#21333;&#35789;&#23398;&#20064;&#21644;&#21477;&#23376;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#27604;&#26631;&#20934;&#35821;&#35328;&#27169;&#22411;&#26356;&#39640;&#30340;&#23398;&#20064;&#25928;&#29575;&#21644;&#36827;&#27493;&#12290;</title><link>https://arxiv.org/abs/2403.14551</link><description>&lt;p&gt;
&#35789;&#27719;&#32423;&#23545;&#27604;&#35270;&#35273;&#22522;&#30784;&#25913;&#36827;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Lexicon-Level Contrastive Visual-Grounding Improves Language Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14551
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;LexiContrastive Grounding (LCG)&#65292;&#23427;&#32467;&#21512;&#20102;&#35270;&#35273;&#30417;&#30563;&#21644;&#25991;&#26412;&#34920;&#31034;&#25913;&#36827;&#31574;&#30053;&#65292;&#22312;&#22810;&#20010;&#21333;&#35789;&#23398;&#20064;&#21644;&#21477;&#23376;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#27604;&#26631;&#20934;&#35821;&#35328;&#27169;&#22411;&#26356;&#39640;&#30340;&#23398;&#20064;&#25928;&#29575;&#21644;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20170;&#22825;&#26368;&#20934;&#30830;&#30340;&#35821;&#35328;&#27169;&#22411;&#26159;&#22312;&#27604;&#20154;&#31867;&#35821;&#35328;&#23398;&#20064;&#32773;&#25509;&#25910;&#21040;&#30340;&#35821;&#35328;&#25968;&#25454;&#37327;&#22810;&#24471;&#22810;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#30340;&#65292;&#20294;&#24182;&#27809;&#26377;&#26469;&#33258;&#22312;&#20154;&#31867;&#23398;&#20064;&#20013;&#36215;&#20851;&#38190;&#20316;&#29992;&#30340;&#20854;&#20182;&#24863;&#23448;&#27169;&#24335;&#30340;&#30417;&#30563;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;LexiContrastive Grounding (LCG)&#65292;&#19968;&#31181;&#21033;&#29992;&#35270;&#35273;&#30417;&#30563;&#26469;&#25913;&#36827;&#25991;&#26412;&#34920;&#24449;&#30340;&#22522;&#20110;&#22320;&#38754;&#35821;&#35328;&#23398;&#20064;&#31243;&#24207;&#12290;LexiContrastive Grounding&#23558;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#31574;&#30053;&#19982;&#23545;&#27604;&#35270;&#35273;&#22522;&#30784;&#30446;&#26631;&#32467;&#21512;&#36215;&#26469;&#65292;&#37325;&#28857;&#25918;&#22312;&#32534;&#30721;&#35789;&#27719;&#20449;&#24687;&#30340;&#26089;&#26399;&#23618;&#34920;&#31034;&#19978;&#12290;&#22312;&#22810;&#20010;&#21333;&#35789;&#23398;&#20064;&#21644;&#21477;&#23376;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;LexiContrastive Grounding&#19981;&#20165;&#22312;&#23398;&#20064;&#25928;&#29575;&#19978;&#20248;&#20110;&#26631;&#20934;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#19988;&#22312;&#35270;&#35273;&#19982;&#35821;&#35328;&#23398;&#20064;&#31243;&#24207;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14551v1 Announce Type: cross  Abstract: Today's most accurate language models are trained on orders of magnitude more language data than human language learners receive - but with no supervision from other sensory modalities that play a crucial role in human learning. Can we make LMs' representations and predictions more accurate (and more human-like) with more ecologically plausible supervision? This paper describes LexiContrastive Grounding (LCG), a grounded language learning procedure that leverages visual supervision to improve textual representations. LexiContrastive Grounding combines a next token prediction strategy with a contrastive visual grounding objective, focusing on early-layer representations that encode lexical information. Across multiple word-learning and sentence-understanding benchmarks, LexiContrastive Grounding not only outperforms standard language-only models in learning efficiency, but also improves upon vision-and-language learning procedures inclu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20272;&#35745;&#36890;&#36947;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#23545;&#36965;&#24863;&#22270;&#20687;&#29289;&#29702;&#20449;&#24687;&#19968;&#33268;&#24615;&#24433;&#21709;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36825;&#19968;&#39046;&#22495;&#20013;&#23384;&#22312;&#30340;&#20105;&#35758;&#12290;</title><link>https://arxiv.org/abs/2403.14547</link><description>&lt;p&gt;
&#20272;&#35745;&#36890;&#36947;&#25968;&#25454;&#22686;&#24378;&#23545;&#36965;&#24863;&#22270;&#20687;&#29289;&#29702;&#20449;&#24687;&#19968;&#33268;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Estimating Physical Information Consistency of Channel Data Augmentation for Remote Sensing Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20272;&#35745;&#36890;&#36947;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#23545;&#36965;&#24863;&#22270;&#20687;&#29289;&#29702;&#20449;&#24687;&#19968;&#33268;&#24615;&#24433;&#21709;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36825;&#19968;&#39046;&#22495;&#20013;&#23384;&#22312;&#30340;&#20105;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#22312;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#36890;&#36947;&#21464;&#25442;&#34987;&#25972;&#21512;&#21040;&#36965;&#24863;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#25968;&#25454;&#22686;&#24378;&#27969;&#31243;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#36890;&#36947;&#22686;&#24378;&#25216;&#26415;&#26159;&#21542;&#20250;&#24433;&#21709;&#36965;&#24863;&#22270;&#20687;&#30340;&#29289;&#29702;&#20449;&#24687;&#65292;&#20197;&#35299;&#20915;&#20154;&#20204;&#23545;&#20854;&#22312;&#36965;&#24863;&#22270;&#20687;&#19978;&#30340;&#36866;&#29992;&#24615;&#23384;&#22312;&#20105;&#35758;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14547v1 Announce Type: cross  Abstract: The application of data augmentation for deep learning (DL) methods plays an important role in achieving state-of-the-art results in supervised, semi-supervised, and self-supervised image classification. In particular, channel transformations (e.g., solarize, grayscale, brightness adjustments) are integrated into data augmentation pipelines for remote sensing (RS) image classification tasks. However, contradicting beliefs exist about their proper applications to RS images. A common point of critique is that the application of channel augmentation techniques may lead to physically inconsistent spectral data (i.e., pixel signatures). To shed light on the open debate, we propose an approach to estimate whether a channel augmentation technique affects the physical information of RS images. To this end, the proposed approach estimates a score that measures the alignment of a pixel signature within a time series that can be naturally subject
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ObjectDR&#65292;&#21033;&#29992;&#23545;&#35937;-centric&#30340;&#22495;&#38543;&#26426;&#21270;&#21512;&#25104;&#21333;&#35270;&#22270;3D&#24418;&#29366;&#37325;&#24314;&#20013;&#32570;&#20047;&#30340;&#37197;&#23545;&#25968;&#25454;&#65292;&#36890;&#36807;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#21644;&#35299;&#32806;&#26694;&#26550;&#26469;&#29983;&#25104;&#21644;&#20445;&#30041;&#23545;&#35937;&#36718;&#24275;&#20197;&#21450;&#24191;&#27867;&#21464;&#21270;&#30340;&#25968;&#25454;&#65292;&#20174;&#32780;&#20026;&#22521;&#35757;&#27169;&#22411;&#25429;&#25417;&#22495;&#19981;&#21464;&#24615;&#20960;&#20309;&#24418;&#29366;&#12290;</title><link>https://arxiv.org/abs/2403.14539</link><description>&lt;p&gt;
Object-Centric Domain Randomization&#29992;&#20110;&#37326;&#22806;3D&#24418;&#29366;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Object-Centric Domain Randomization for 3D Shape Reconstruction in the Wild
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14539
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ObjectDR&#65292;&#21033;&#29992;&#23545;&#35937;-centric&#30340;&#22495;&#38543;&#26426;&#21270;&#21512;&#25104;&#21333;&#35270;&#22270;3D&#24418;&#29366;&#37325;&#24314;&#20013;&#32570;&#20047;&#30340;&#37197;&#23545;&#25968;&#25454;&#65292;&#36890;&#36807;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#21644;&#35299;&#32806;&#26694;&#26550;&#26469;&#29983;&#25104;&#21644;&#20445;&#30041;&#23545;&#35937;&#36718;&#24275;&#20197;&#21450;&#24191;&#27867;&#21464;&#21270;&#30340;&#25968;&#25454;&#65292;&#20174;&#32780;&#20026;&#22521;&#35757;&#27169;&#22411;&#25429;&#25417;&#22495;&#19981;&#21464;&#24615;&#20960;&#20309;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#35270;&#22270;3D&#24418;&#29366;&#22312;&#37326;&#22806;&#30340;&#37325;&#24314;&#38754;&#20020;&#30340;&#26368;&#22823;&#25361;&#25112;&#20043;&#19968;&#26159;&#26469;&#33258;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&lt;3D&#24418;&#29366;&#65292;2D&#22270;&#20687;&gt;-&#37197;&#23545;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#12290;&#21463;&#22495;&#38543;&#26426;&#21270;&#24341;&#20154;&#27880;&#30446;&#30340;&#25104;&#23601;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ObjectDR&#65292;&#36890;&#36807;&#23545;&#23545;&#35937;&#22806;&#35266;&#21644;&#32972;&#26223;&#30340;&#35270;&#35273;&#21464;&#21270;&#36827;&#34892;&#38543;&#26426;&#20223;&#30495;&#65292;&#21512;&#25104;&#36825;&#31181;&#37197;&#23545;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#21512;&#25104;&#26694;&#26550;&#21033;&#29992;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65288;&#20363;&#22914;ControlNet&#65289;&#29983;&#25104;&#31526;&#21512;&#31354;&#38388;&#26465;&#20214;&#65288;&#20363;&#22914;2.5D&#33609;&#22270;&#65289;&#30340;&#22270;&#20687;&#65292;&#36825;&#20123;&#26465;&#20214;&#21487;&#20197;&#36890;&#36807;&#20174;&#23545;&#35937;&#38598;&#21512;&#65288;&#20363;&#22914;Objaverse-XL&#65289;&#30340;&#28210;&#26579;&#36807;&#31243;&#33719;&#24471;3D&#24418;&#29366;&#12290;&#20026;&#20102;&#27169;&#25311;&#22810;&#26679;&#21270;&#30340;&#21464;&#21270;&#21516;&#26102;&#20445;&#30041;&#23884;&#20837;&#31354;&#38388;&#26465;&#20214;&#20013;&#30340;&#23545;&#35937;&#36718;&#24275;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#21033;&#29992;&#21021;&#22987;&#23545;&#35937;&#25351;&#23548;&#30340;&#35299;&#32806;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14539v1 Announce Type: cross  Abstract: One of the biggest challenges in single-view 3D shape reconstruction in the wild is the scarcity of &lt;3D shape, 2D image&gt;-paired data from real-world environments. Inspired by remarkable achievements via domain randomization, we propose ObjectDR which synthesizes such paired data via a random simulation of visual variations in object appearances and backgrounds. Our data synthesis framework exploits a conditional generative model (e.g., ControlNet) to generate images conforming to spatial conditions such as 2.5D sketches, which are obtainable through a rendering process of 3D shapes from object collections (e.g., Objaverse-XL). To simulate diverse variations while preserving object silhouettes embedded in spatial conditions, we also introduce a disentangled framework which leverages an initial object guidance. After synthesizing a wide range of data, we pre-train a model on them so that it learns to capture a domain-invariant geometry p
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#19981;&#21464;&#21494;&#36712;&#36947;&#22312;&#24378;&#36843;&#31995;&#32479;&#20013;&#35782;&#21035;&#38477;&#38454;&#27169;&#22411;&#65292;&#32467;&#21512;&#20840;&#23616;&#21644;&#23616;&#37096;&#21494;&#36712;&#36947;&#65292;&#24182;&#24378;&#35843;&#35299;&#20915;&#25968;&#23398;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.14514</link><description>&lt;p&gt;
&#22312;&#24378;&#36843;&#31995;&#32479;&#20013;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#19981;&#21464;&#21494;&#36712;&#36947;&#36827;&#34892;&#38477;&#38454;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Machine-learning invariant foliations in forced systems for reduced order modelling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14514
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#19981;&#21464;&#21494;&#36712;&#36947;&#22312;&#24378;&#36843;&#31995;&#32479;&#20013;&#35782;&#21035;&#38477;&#38454;&#27169;&#22411;&#65292;&#32467;&#21512;&#20840;&#23616;&#21644;&#23616;&#37096;&#21494;&#36712;&#36947;&#65292;&#24182;&#24378;&#35843;&#35299;&#20915;&#25968;&#23398;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21033;&#29992;&#19981;&#21464;&#21494;&#36712;&#36947;&#20174;&#25968;&#25454;&#20013;&#35782;&#21035;&#24378;&#36843;&#31995;&#32479;&#30340;&#38477;&#38454;&#27169;&#22411;&#65288;ROM&#65289;&#12290;&#24378;&#36843;&#21487;&#20197;&#26159;&#22806;&#37096;&#24378;&#36843;&#12289;&#21442;&#25968;&#24378;&#36843;&#12289;&#21608;&#26399;&#24615;&#24378;&#36843;&#25110;&#20934;&#21608;&#26399;&#24615;&#24378;&#36843;&#12290;&#35813;&#36807;&#31243;&#20998;&#20026;&#22235;&#27493;&#65306;1. &#35782;&#21035;&#36817;&#20284;&#19981;&#21464;&#29615;&#38754;&#21450;&#20854;&#21608;&#22260;&#30340;&#32447;&#24615;&#21160;&#24577;&#65307;2. &#35782;&#21035;&#22260;&#32469;&#29615;&#38754;&#30340;&#20840;&#23616;&#23450;&#20041;&#30340;&#19981;&#21464;&#21494;&#36712;&#36947;&#65307;3. &#35782;&#21035;&#20851;&#20110;&#19968;&#20010;&#34917;&#20805;&#20840;&#23616;&#21494;&#36712;&#36947;&#30340;&#19981;&#21464;&#27969;&#24418;&#30340;&#23616;&#37096;&#21494;&#36712;&#36947;&#65307;4. &#25552;&#21462;&#20316;&#20026;&#36890;&#36807;&#29615;&#38754;&#30340;&#21494;&#23376;&#30340;&#19981;&#21464;&#27969;&#24418;&#24182;&#35299;&#37322;&#32467;&#26524;&#12290;&#25105;&#20204;&#23558;&#31532;2&#27493;&#21644;&#31532;3&#27493;&#32467;&#21512;&#36215;&#26469;&#65292;&#36825;&#26679;&#25105;&#20204;&#21487;&#20197;&#36319;&#36394;&#19981;&#21464;&#29615;&#38754;&#30340;&#20301;&#32622;&#24182;&#36866;&#24403;&#35843;&#25972;&#19981;&#21464;&#24615;&#26041;&#31243;&#30340;&#27604;&#20363;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#23558;&#19981;&#21464;&#27969;&#24418;&#21644;&#21494;&#36712;&#36947;&#25311;&#21512;&#21040;&#25968;&#25454;&#26102;&#25152;&#38754;&#20020;&#30340;&#19968;&#20123;&#22522;&#26412;&#38480;&#21046;&#65292;&#36825;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#25968;&#23398;&#26469;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14514v1 Announce Type: cross  Abstract: We identify reduced order models (ROM) of forced systems from data using invariant foliations. The forcing can be external, parametric, periodic or quasi-periodic. The process has four steps: 1. identify an approximate invariant torus and the linear dynamics about the torus; 2. identify a globally defined invariant foliation about the torus; 3. identify a local foliation about an invariant manifold that complements the global foliation 4. extract the invariant manifold as the leaf going through the torus and interpret the result. We combine steps 2 and 3, so that we can track the location of the invariant torus and scale the invariance equations appropriately. We highlight some fundamental limitations of invariant manifolds and foliations when fitting them to data, that require further mathematics to resolve.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;CSAC-LB&#65292;&#36890;&#36807;&#24212;&#29992;&#32447;&#24615;&#24179;&#28369;&#23545;&#25968;&#38556;&#30861;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#33021;&#65292;&#26080;&#38656;&#20219;&#20309;&#39044;&#35757;&#32451;</title><link>https://arxiv.org/abs/2403.14508</link><description>&lt;p&gt;
&#24102;&#26377;&#24179;&#28369;&#23545;&#25968;&#38556;&#30861;&#20989;&#25968;&#30340;&#32422;&#26463;&#21152;&#24378;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Constrained Reinforcement Learning with Smoothed Log Barrier Function
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14508
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;CSAC-LB&#65292;&#36890;&#36807;&#24212;&#29992;&#32447;&#24615;&#24179;&#28369;&#23545;&#25968;&#38556;&#30861;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#33021;&#65292;&#26080;&#38656;&#20219;&#20309;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#25511;&#21046;&#20219;&#21153;&#65292;&#24182;&#22312;&#35768;&#22810;&#39046;&#22495;&#30340;&#24615;&#33021;&#19978;&#19982;&#20256;&#32479;&#25511;&#21046;&#26041;&#27861;&#30456;&#27604;&#26377;&#20102;&#26174;&#33879;&#25552;&#39640;&#65292;&#20854;&#20013;&#22870;&#21169;&#20989;&#25968;&#26159;&#24456;&#22909;&#23450;&#20041;&#30340;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#38382;&#39064;&#65292;&#20197;&#22870;&#21169;&#21644;&#32422;&#26463;&#21516;&#26102;&#21046;&#23450;&#20248;&#21270;&#38382;&#39064;&#36890;&#24120;&#26356;&#20026;&#26041;&#20415;&#12290;&#36890;&#36807;&#22870;&#21169;&#22609;&#36896;&#26469;&#20248;&#21270;&#36825;&#20123;&#21463;&#38480;&#38382;&#39064;&#21487;&#33021;&#20250;&#24456;&#22256;&#38590;&#65292;&#22240;&#20026;&#38656;&#35201;&#23545;&#24102;&#26377;&#20960;&#20010;&#20132;&#20114;&#39033;&#30340;&#22870;&#21169;&#20989;&#25968;&#36827;&#34892;&#32321;&#29712;&#30340;&#25163;&#21160;&#35843;&#25972;&#12290;&#26368;&#36817;&#21253;&#21547;&#32422;&#26463;&#30340;&#20844;&#24335;&#22312;&#22810;&#25968;&#24773;&#20917;&#19979;&#38656;&#35201;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#36825;&#36890;&#24120;&#38656;&#35201;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#26469;&#25910;&#38598;&#25968;&#25454;&#25110;&#20551;&#23450;&#26377;&#19968;&#20010;&#24453;&#29992;&#30340;&#27425;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CSAC-LB&#65288;&#24102;&#26377;&#23545;&#25968;&#38556;&#30861;&#20989;&#25968;&#30340;&#32422;&#26463;&#36719;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65289;&#30340;&#26032;&#22411;&#32422;&#26463;RL&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#32447;&#24615;&#24179;&#28369;&#23545;&#25968;&#38556;&#30861;&#20989;&#25968;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14508v1 Announce Type: cross  Abstract: Reinforcement Learning (RL) has been widely applied to many control tasks and substantially improved the performances compared to conventional control methods in many domains where the reward function is well defined. However, for many real-world problems, it is often more convenient to formulate optimization problems in terms of rewards and constraints simultaneously. Optimizing such constrained problems via reward shaping can be difficult as it requires tedious manual tuning of reward functions with several interacting terms. Recent formulations which include constraints mostly require a pre-training phase, which often needs human expertise to collect data or assumes having a sub-optimal policy readily available. We propose a new constrained RL method called CSAC-LB (Constrained Soft Actor-Critic with Log Barrier Function), which achieves competitive performance without any pre-training by applying a linear smoothed log barrier funct
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#36807;&#31243; SoftLearn&#65292;&#36890;&#36807;&#36719;&#32858;&#31867;&#36807;&#31243;&#35825;&#23548;&#20986;&#19968;&#20010; PC&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340; LearnSPN&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#65292;&#20135;&#29983;&#26356;&#22909;&#30340;&#20284;&#28982;&#20540;&#21644;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.14504</link><description>&lt;p&gt;
&#36719;&#23398;&#20064;&#27010;&#29575;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Soft Learning Probabilistic Circuits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14504
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#36807;&#31243; SoftLearn&#65292;&#36890;&#36807;&#36719;&#32858;&#31867;&#36807;&#31243;&#35825;&#23548;&#20986;&#19968;&#20010; PC&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340; LearnSPN&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#65292;&#20135;&#29983;&#26356;&#22909;&#30340;&#20284;&#28982;&#20540;&#21644;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#30005;&#36335;&#65288;PCs)&#26159;&#26480;&#20986;&#30340;&#21487;&#35745;&#31639;&#27010;&#29575;&#27169;&#22411;&#65292;&#20801;&#35768;&#36827;&#34892;&#19968;&#31995;&#21015;&#20934;&#30830;&#25512;&#29702;&#12290;&#35813;&#35770;&#25991;&#19987;&#27880;&#20110;&#20027;&#35201;&#30340; PC &#35757;&#32451;&#31639;&#27861; LearnSPN&#65292;&#30001;&#20110;&#20854;&#25928;&#29575;&#12289;&#24615;&#33021;&#21644;&#26131;&#29992;&#24615;&#32780;&#25104;&#20026;&#37329;&#26631;&#20934;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#12290;&#25105;&#20204;&#34920;&#26126;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;LearnSPN &#26159;&#19968;&#31181;&#36138;&#24515;&#20284;&#28982;&#26368;&#22823;&#21270;&#22120;&#12290;&#34429;&#28982;&#22312; PC &#20013;&#65292;&#25512;&#29702;&#21487;&#20197;&#21033;&#29992;&#25972;&#20010;&#30005;&#36335;&#32467;&#26500;&#26469;&#22788;&#29702;&#26597;&#35810;&#65292;&#20294; LearnSPN &#24212;&#29992;&#20102;&#19968;&#31181;&#30828;&#26041;&#27861;&#26469;&#23398;&#20064;&#23427;&#20204;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#27714;&#21644;&#33410;&#28857;&#19978;&#36890;&#36807;&#19968;&#20010;&#32780;&#20165;&#19968;&#20010;&#23376;/&#36793;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#20256;&#25773;&#65292;&#23601;&#20687;&#22312;&#19968;&#20010;&#30828;&#32858;&#31867;&#36807;&#31243;&#20013;&#19968;&#26679;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; SoftLearn &#30340;&#26032;&#23398;&#20064;&#31243;&#24207;&#65292;&#23427;&#36890;&#36807;&#36719;&#32858;&#31867;&#36807;&#31243;&#35825;&#23548;&#20986;&#19968;&#20010; PC&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#36825;&#31181;&#23398;&#20064;-&#25512;&#29702;&#20860;&#23481;&#24615;&#22312; PC &#20013;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SoftLearn &#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#20248;&#20110; LearnSPN&#65292;&#20135;&#29983;&#26356;&#22909;&#30340;&#20284;&#28982;&#20540;&#65292;&#21487;&#33021;&#36824;&#20135;&#29983;&#26356;&#22909;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14504v1 Announce Type: cross  Abstract: Probabilistic Circuits (PCs) are prominent tractable probabilistic models, allowing for a range of exact inferences. This paper focuses on the main algorithm for training PCs, LearnSPN, a gold standard due to its efficiency, performance, and ease of use, in particular for tabular data. We show that LearnSPN is a greedy likelihood maximizer under mild assumptions. While inferences in PCs may use the entire circuit structure for processing queries, LearnSPN applies a hard method for learning them, propagating at each sum node a data point through one and only one of the children/edges as in a hard clustering process. We propose a new learning procedure named SoftLearn, that induces a PC using a soft clustering process. We investigate the effect of this learning-inference compatibility in PCs. Our experiments show that SoftLearn outperforms LearnSPN in many situations, yielding better likelihoods and arguably better samples. We also analy
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#22240;&#26524;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#27010;&#29575;&#25512;&#29702;&#65292;&#25104;&#21151;&#39044;&#27979;&#31215;&#26408;&#22612;&#31283;&#23450;&#24615;&#24182;&#36873;&#25321;&#19979;&#19968;&#26368;&#20339;&#21160;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.14488</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#23398;&#22240;&#26524;&#25512;&#29702;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#23433;&#20840;&#31283;&#20581;&#30340;&#19979;&#19968;&#26368;&#20339;&#21160;&#20316;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Physics-Based Causal Reasoning for Safe &amp; Robust Next-Best Action Selection in Robot Manipulation Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14488
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#22240;&#26524;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#27010;&#29575;&#25512;&#29702;&#65292;&#25104;&#21151;&#39044;&#27979;&#31215;&#26408;&#22612;&#31283;&#23450;&#24615;&#24182;&#36873;&#25321;&#19979;&#19968;&#26368;&#20339;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#39640;&#25928;&#30340;&#29289;&#20307;&#25805;&#20316;&#26159;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#20851;&#38190;&#25512;&#25163;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25361;&#25112;&#22312;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#24517;&#39035;&#23545;&#19968;&#31995;&#21015;&#20256;&#24863;&#22120;&#21644;&#25191;&#34892;&#22120;&#30340;&#19981;&#30830;&#23450;&#24615;&#20855;&#26377;&#31283;&#20581;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#21644;&#22240;&#26524;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35753;&#26426;&#22120;&#20154;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#29615;&#22659;&#20013;&#23545;&#20505;&#36873;&#21160;&#20316;&#36827;&#34892;&#27010;&#29575;&#25512;&#29702;&#65292;&#20197;&#23436;&#25104;&#19968;&#20010;&#31215;&#26408;&#22534;&#21472;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;&#21018;&#20307;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#20223;&#30495;&#19982;&#22240;&#26524;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;CBN&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#23450;&#20041;&#20102;&#26426;&#22120;&#20154;&#20915;&#31574;&#36807;&#31243;&#30340;&#22240;&#26524;&#29983;&#25104;&#27010;&#29575;&#27169;&#22411;&#12290;&#36890;&#36807;&#22522;&#20110;&#20223;&#30495;&#30340;&#33945;&#29305;&#21345;&#27931;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#25104;&#21151;&#22320;&#33021;&#22815;&#65306;(1) &#39640;&#20934;&#30830;&#24230;&#22320;&#39044;&#27979;&#31215;&#26408;&#22612;&#30340;&#31283;&#23450;&#24615;&#65288;&#39044;&#27979;&#20934;&#30830;&#29575;&#65306;88.6%&#65289;&#65307;&#21644;&#65292;(2) &#20026;&#31215;&#26408;&#22534;&#21472;&#20219;&#21153;&#36873;&#25321;&#19968;&#20010;&#36817;&#20284;&#30340;&#19979;&#19968;&#26368;&#20339;&#21160;&#20316;&#65292;&#20379;&#25972;&#21512;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#25191;&#34892;&#65292;&#23454;&#29616;94.2%&#30340;&#20219;&#21153;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14488v1 Announce Type: cross  Abstract: Safe and efficient object manipulation is a key enabler of many real-world robot applications. However, this is challenging because robot operation must be robust to a range of sensor and actuator uncertainties. In this paper, we present a physics-informed causal-inference-based framework for a robot to probabilistically reason about candidate actions in a block stacking task in a partially observable setting. We integrate a physics-based simulation of the rigid-body system dynamics with a causal Bayesian network (CBN) formulation to define a causal generative probabilistic model of the robot decision-making process. Using simulation-based Monte Carlo experiments, we demonstrate our framework's ability to successfully: (1) predict block tower stability with high accuracy (Pred Acc: 88.6%); and, (2) select an approximate next-best action for the block stacking task, for execution by an integrated robot system, achieving 94.2% task succe
&lt;/p&gt;</description></item><item><title>HyperGALE&#36890;&#36807;&#23398;&#20064;&#36229;&#36793;&#30340;&#36229;&#22270;&#38376;&#25511;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#22312;&#35299;&#37322;&#22797;&#26434;&#30340;&#33041;&#22270;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#65292;&#20026;ASD&#29983;&#29289;&#26631;&#24535;&#29305;&#24449;&#21270;&#25552;&#20379;&#26356;&#28145;&#20837;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.14484</link><description>&lt;p&gt;
HyperGALE: &#36890;&#36807;&#23398;&#20064;&#36229;&#36793;&#30340;&#36229;&#22270;&#38376;&#25511;&#27880;&#24847;&#21147;&#36827;&#34892;ASD&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
HyperGALE: ASD Classification via Hypergraph Gated Attention with Learnable Hyperedges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14484
&lt;/p&gt;
&lt;p&gt;
HyperGALE&#36890;&#36807;&#23398;&#20064;&#36229;&#36793;&#30340;&#36229;&#22270;&#38376;&#25511;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#22312;&#35299;&#37322;&#22797;&#26434;&#30340;&#33041;&#22270;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#65292;&#20026;ASD&#29983;&#29289;&#26631;&#24535;&#29305;&#24449;&#21270;&#25552;&#20379;&#26356;&#28145;&#20837;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#65288;ASD&#65289;&#26159;&#19968;&#31181;&#31070;&#32463;&#21457;&#32946;&#38556;&#30861;&#65292;&#20854;&#29305;&#24449;&#26159;&#21508;&#31181;&#31038;&#20132;&#35748;&#30693;&#25361;&#25112;&#21644;&#37325;&#22797;&#34892;&#20026;&#27169;&#24335;&#12290;&#30001;&#20110;&#35889;&#31995;&#30340;&#30151;&#29366;&#22810;&#26679;&#24615;&#65292;&#20026;ASD&#35782;&#21035;&#21487;&#38752;&#30340;&#22522;&#20110;&#33041;&#25104;&#20687;&#30340;&#29983;&#29289;&#26631;&#24535;&#19968;&#30452;&#26159;&#19968;&#20010;&#25345;&#20037;&#30340;&#25361;&#25112;&#12290;&#35813;&#39046;&#22495;&#29616;&#26377;&#30340;&#22522;&#32447;&#24050;&#32463;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#20294;&#22312;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;\emph {HyperGALE}&#65292;&#23427;&#22522;&#20110;&#36229;&#22270;&#65292;&#32467;&#21512;&#20102;&#23398;&#20064;&#30340;&#36229;&#36793;&#21644;&#38376;&#25511;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#36825;&#31181;&#26041;&#27861;&#22823;&#22823;&#25552;&#39640;&#20102;&#27169;&#22411;&#35299;&#37322;&#22797;&#26434;&#33041;&#22270;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#20026;ASD&#29983;&#29289;&#26631;&#24535;&#29305;&#24449;&#21270;&#25552;&#20379;&#20102;&#26356;&#28145;&#20837;&#30340;&#35265;&#35299;&#12290;&#22312;&#24191;&#27867;&#30340;ABIDE II&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;\emph {HyperGALE}&#19981;&#20165;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#36824;&#22312;&#20851;&#38190;&#24615;&#33021;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14484v1 Announce Type: cross  Abstract: Autism Spectrum Disorder (ASD) is a neurodevelopmental condition characterized by varied social cognitive challenges and repetitive behavioral patterns. Identifying reliable brain imaging-based biomarkers for ASD has been a persistent challenge due to the spectrum's diverse symptomatology. Existing baselines in the field have made significant strides in this direction, yet there remains room for improvement in both performance and interpretability. We propose \emph{HyperGALE}, which builds upon the hypergraph by incorporating learned hyperedges and gated attention mechanisms. This approach has led to substantial improvements in the model's ability to interpret complex brain graph data, offering deeper insights into ASD biomarker characterization. Evaluated on the extensive ABIDE II dataset, \emph{HyperGALE} not only improves interpretability but also demonstrates statistically significant enhancements in key performance metrics compare
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;LightGBM&#31639;&#27861;&#23545;&#36816;&#33829;&#21830;&#29992;&#25143;&#20449;&#29992;&#35780;&#20272;&#27169;&#22411;&#36827;&#34892;&#30740;&#31350;&#65292;&#36890;&#36807;&#25552;&#21462;&#20851;&#38190;&#29305;&#24449;&#24182;&#36827;&#34892;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#29305;&#24449;&#24037;&#31243;&#65292;&#20197;&#25913;&#21892;&#29992;&#25143;&#20449;&#29992;&#35780;&#20272;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.14483</link><description>&lt;p&gt;
&#20351;&#29992;LightGBM&#31639;&#27861;&#36827;&#34892;&#36816;&#33829;&#21830;&#29992;&#25143;&#20449;&#29992;&#35780;&#20272;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Utilizing the LightGBM Algorithm for Operator User Credit Assessment Research
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14483
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;LightGBM&#31639;&#27861;&#23545;&#36816;&#33829;&#21830;&#29992;&#25143;&#20449;&#29992;&#35780;&#20272;&#27169;&#22411;&#36827;&#34892;&#30740;&#31350;&#65292;&#36890;&#36807;&#25552;&#21462;&#20851;&#38190;&#29305;&#24449;&#24182;&#36827;&#34892;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#29305;&#24449;&#24037;&#31243;&#65292;&#20197;&#25913;&#21892;&#29992;&#25143;&#20449;&#29992;&#35780;&#20272;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#20114;&#32852;&#32593;&#29992;&#25143;&#20449;&#29992;&#35780;&#20272;&#26159;&#36890;&#20449;&#36816;&#33829;&#21830;&#21046;&#23450;&#20915;&#31574;&#21644;&#25514;&#26045;&#30340;&#37325;&#35201;&#26041;&#24335;&#65292;&#20063;&#26159;&#36816;&#33829;&#21830;&#33719;&#24471;&#39044;&#26399;&#25910;&#30410;&#30340;&#20445;&#38556;&#12290;&#26412;&#25991;&#21033;&#29992;&#36890;&#20449;&#36816;&#33829;&#21830;&#25552;&#20379;&#30340;&#28023;&#37327;&#25968;&#25454;&#65292;&#22522;&#20110;&#34701;&#21512;LightGBM&#31639;&#27861;&#36827;&#34892;&#36816;&#33829;&#21830;&#29992;&#25143;&#20449;&#29992;&#35780;&#20272;&#27169;&#22411;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14483v1 Announce Type: cross  Abstract: Mobile Internet user credit assessment is an important way for communication operators to establish decisions and formulate measures, and it is also a guarantee for operators to obtain expected benefits. However, credit evaluation methods have long been monopolized by financial industries such as banks and credit. As supporters and providers of platform network technology and network resources, communication operators are also builders and maintainers of communication networks. Internet data improves the user's credit evaluation strategy. This paper uses the massive data provided by communication operators to carry out research on the operator's user credit evaluation model based on the fusion LightGBM algorithm. First, for the massive data related to user evaluation provided by operators, key features are extracted by data preprocessing and feature engineering methods, and a multi-dimensional feature set with statistical significance 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21435;&#27602;&#21270;&#65292;&#22312;&#26500;&#24314;&#20102;SafeEdit&#22522;&#20934;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861; DINM&#65292;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;&#27169;&#22411;&#30340;&#27602;&#24615;&#65292;&#21516;&#26102;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.14472</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#32534;&#36753;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21435;&#27602;&#21270;
&lt;/p&gt;
&lt;p&gt;
Detoxifying Large Language Models via Knowledge Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21435;&#27602;&#21270;&#65292;&#22312;&#26500;&#24314;&#20102;SafeEdit&#22522;&#20934;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861; DINM&#65292;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;&#27169;&#22411;&#30340;&#27602;&#24615;&#65292;&#21516;&#26102;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#26469;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#21435;&#27602;&#21270;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;SafeEdit&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20061;&#31181;&#19981;&#23433;&#20840;&#31867;&#21035;&#65292;&#20855;&#26377;&#21508;&#31181;&#24378;&#22823;&#30340;&#25915;&#20987;&#25552;&#31034;&#65292;&#24182;&#37197;&#22791;&#20102;&#20840;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#32447;&#65292;&#32467;&#26524;&#34920;&#26126;&#30693;&#35782;&#32534;&#36753;&#26377;&#28508;&#21147;&#22312;&#23545;LLMs&#36827;&#34892;&#21435;&#27602;&#21270;&#26102;&#65292;&#22312;&#23545;&#19968;&#33324;&#24615;&#33021;&#30340;&#24433;&#21709;&#30456;&#23545;&#26377;&#38480;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22522;&#20934;&#32447;&#65292;&#31216;&#20026;&#36890;&#36807;&#26415;&#20013;&#31070;&#32463;&#30417;&#27979;&#21435;&#27602;&#21270;&#65288;DINM&#65289;&#65292;&#36890;&#36807;&#20165;&#19968;&#27425;&#23454;&#20363;&#30340;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;LLMs&#30340;&#27602;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#34920;&#26126;&#20808;&#21069;&#30340;&#26041;&#27861;&#22914;SFT&#21644;DPO&#21487;&#33021;&#20165;&#25233;&#21046;&#26377;&#27602;&#21442;&#25968;&#30340;&#28608;&#27963;&#65292;&#32780;DINM&#21017;&#20943;&#36731;&#26377;&#27602;&#21442;&#25968;&#30340;&#27602;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14472v1 Announce Type: cross  Abstract: This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs). We construct a benchmark, SafeEdit, which covers nine unsafe categories with various powerful attack prompts and equips comprehensive metrics for systematic evaluation. We conduct experiments to compare knowledge editing approaches with previous baselines, indicating that knowledge editing has the potential to efficiently detoxify LLMs with limited impact on general performance. Then, we propose a simple yet effective baseline, dubbed Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the toxicity of LLMs within a few tuning steps via only one instance. We further provide an in-depth analysis of the internal mechanism for various detoxify approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to
&lt;/p&gt;</description></item><item><title>BoUTS&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#33021;&#22815;&#26222;&#36866;&#24615;&#22320;&#35782;&#21035;&#25968;&#25454;&#38598;&#20013;&#36890;&#29992;&#29305;&#24449;&#21644;&#39044;&#27979;&#29305;&#23450;&#23376;&#38598;&#30340;&#20219;&#21153;&#29305;&#23450;&#29305;&#24449;&#65292;&#22312;&#21270;&#23398;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#29305;&#24449;&#31232;&#30095;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#30528;&#19982;&#19987;&#38376;&#26041;&#27861;&#30456;&#24403;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14466</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#22810;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#26222;&#36866;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Universal Feature Selection for Simultaneous Interpretability of Multitask Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14466
&lt;/p&gt;
&lt;p&gt;
BoUTS&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#33021;&#22815;&#26222;&#36866;&#24615;&#22320;&#35782;&#21035;&#25968;&#25454;&#38598;&#20013;&#36890;&#29992;&#29305;&#24449;&#21644;&#39044;&#27979;&#29305;&#23450;&#23376;&#38598;&#30340;&#20219;&#21153;&#29305;&#23450;&#29305;&#24449;&#65292;&#22312;&#21270;&#23398;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#29305;&#24449;&#31232;&#30095;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#30528;&#19982;&#19987;&#38376;&#26041;&#27861;&#30456;&#24403;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22797;&#26434;&#12289;&#39640;&#32500;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#22312;&#31185;&#23398;&#39046;&#22495;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#22312;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#23545;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#36866;&#29992;&#24615;&#65292;&#25110;&#32773;&#23545;&#29305;&#24449;&#23646;&#24615;&#20851;&#31995;&#20570;&#20986;&#20102;&#38480;&#21046;&#24615;&#20551;&#35774;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#25429;&#33719;&#22797;&#26434;&#20132;&#20114;&#20316;&#29992;&#30340;&#33021;&#21147;&#12290;BoUTS&#30340;&#36890;&#29992;&#19988;&#21487;&#25193;&#23637;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#31361;&#30772;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#26082;&#33021;&#35782;&#21035;&#23545;&#25152;&#26377;&#25968;&#25454;&#38598;&#30456;&#20851;&#30340;&#36890;&#29992;&#29305;&#24449;&#65292;&#21448;&#33021;&#35782;&#21035;&#39044;&#27979;&#29305;&#23450;&#23376;&#38598;&#30340;&#20219;&#21153;&#29305;&#23450;&#29305;&#24449;&#12290;&#22312;&#19971;&#20010;&#19981;&#21516;&#30340;&#21270;&#23398;&#22238;&#24402;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;BoUTS&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#29305;&#24449;&#31232;&#30095;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#30528;&#19982;&#19987;&#38376;&#26041;&#27861;&#30456;&#24403;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;BoUTS&#30340;&#36890;&#29992;&#29305;&#24449;&#20351;&#24471;&#25968;&#25454;&#38598;&#20043;&#38388;&#21487;&#20197;&#36827;&#34892;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#24182;&#19988;&#26263;&#31034;&#20102;&#30475;&#20284;&#19981;&#30456;&#20851;&#30340;&#21270;&#23398;&#25968;&#25454;&#38598;&#20043;&#38388;&#23384;&#22312;&#28145;&#23618;&#36830;&#25509;&#12290;&#25105;&#20204;&#26399;&#26395;&#36825;&#20123;&#32467;&#26524;&#20250;&#20135;&#29983;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14466v1 Announce Type: new  Abstract: Extracting meaningful features from complex, high-dimensional datasets across scientific domains remains challenging. Current methods often struggle with scalability, limiting their applicability to large datasets, or make restrictive assumptions about feature-property relationships, hindering their ability to capture complex interactions. BoUTS's general and scalable feature selection algorithm surpasses these limitations to identify both universal features relevant to all datasets and task-specific features predictive for specific subsets. Evaluated on seven diverse chemical regression datasets, BoUTS achieves state-of-the-art feature sparsity while maintaining prediction accuracy comparable to specialized methods. Notably, BoUTS's universal features enable domain-specific knowledge transfer between datasets, and suggest deep connections in seemingly-disparate chemical datasets. We expect these results to have important repercussions i
&lt;/p&gt;</description></item><item><title>gTBLS&#36890;&#36807;&#20004;&#38454;&#27573;&#26041;&#27861;&#20174;&#25991;&#26412;&#20013;&#29983;&#25104;&#34920;&#26684;&#65292;&#31532;&#19968;&#38454;&#27573;&#25512;&#26029;&#34920;&#26684;&#32467;&#26500;&#65292;&#31532;&#20108;&#38454;&#27573;&#21033;&#29992;&#32467;&#26500;&#25552;&#20986;&#38382;&#39064;&#24182;&#36890;&#36807;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#26469;&#22238;&#31572;&#65292;&#33021;&#22815;&#22312;&#38646;&#30701;&#37197;&#32622;&#19979;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25913;&#36827;&#20102;&#20808;&#21069;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.14457</link><description>&lt;p&gt;
gTBLS&#65306;&#36890;&#36807;&#26465;&#20214;&#38382;&#31572;&#20174;&#25991;&#26412;&#20013;&#29983;&#25104;&#34920;&#26684;
&lt;/p&gt;
&lt;p&gt;
gTBLS: Generating Tables from Text by Conditional Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14457
&lt;/p&gt;
&lt;p&gt;
gTBLS&#36890;&#36807;&#20004;&#38454;&#27573;&#26041;&#27861;&#20174;&#25991;&#26412;&#20013;&#29983;&#25104;&#34920;&#26684;&#65292;&#31532;&#19968;&#38454;&#27573;&#25512;&#26029;&#34920;&#26684;&#32467;&#26500;&#65292;&#31532;&#20108;&#38454;&#27573;&#21033;&#29992;&#32467;&#26500;&#25552;&#20986;&#38382;&#39064;&#24182;&#36890;&#36807;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#26469;&#22238;&#31572;&#65292;&#33021;&#22815;&#22312;&#38646;&#30701;&#37197;&#32622;&#19979;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25913;&#36827;&#20102;&#20808;&#21069;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14457v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#23558;&#22823;&#27573;&#30340;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25552;&#28860;&#20026;&#32467;&#26500;&#21270;&#12289;&#31616;&#21270;&#30340;&#24418;&#24335;&#65292;&#22914;&#34920;&#26684;&#65292;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#33258;&#21160;&#29983;&#25104;&#34920;&#26684;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#30830;&#20445;&#20854;&#21477;&#27861;&#26377;&#25928;&#24615;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;Transformer&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#21253;&#21547;&#39069;&#22806;&#30340;&#21442;&#25968;&#65292;&#20197;&#20415;&#27880;&#24847;&#29305;&#23450;&#30340;&#34892;&#21644;&#21015;&#26631;&#39064;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#19982;&#36825;&#31181;&#21333;&#38454;&#27573;&#26041;&#27861;&#30456;&#21453;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29983;&#25104;&#24335;&#34920;&#26684;&#65288;gTBLS&#65289;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#12290;&#31532;&#19968;&#38454;&#27573;&#20174;&#25991;&#26412;&#20013;&#25512;&#26029;&#34920;&#26684;&#32467;&#26500;&#65288;&#34892;&#21644;&#21015;&#26631;&#39064;&#65289;&#12290;&#31532;&#20108;&#38454;&#27573;&#21033;&#29992;&#36825;&#20123;&#26631;&#39064;&#25552;&#20986;&#38382;&#39064;&#65292;&#24182;&#24494;&#35843;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#26469;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;gTBLS&#26041;&#27861;&#26131;&#20110;&#22312;&#38646;&#30701;&#37197;&#32622;&#20013;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20026;&#22312;&#19981;&#33021;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#34920;&#26684;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;gTBLS&#25552;&#39640;&#20102;&#20043;&#21069;&#26041;&#27861;&#36798;&#21040;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14457v1 Announce Type: new  Abstract: Distilling large, unstructured text into a structured, condensed form such as tables is an open research problem. One of the primary challenges in automatically generating tables is ensuring their syntactic validity. Prior approaches address this challenge by including additional parameters in the Transformer's attention mechanism to attend to specific rows and column headers. In contrast to this single-stage method, this paper presents a two-stage approach called Generative Tables (gTBLS). The first stage infers table structure (row and column headers) from the text. The second stage formulates questions using these headers and fine-tunes a causal language model to answer them. Furthermore, the gTBLS approach is amenable to the utilization of pre-trained Large Language Models in a zero-shot configuration, presenting a solution for table generation in situations where fine-tuning is not feasible. gTBLS improves prior approaches by up to 
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#26234;&#33021;&#20195;&#29702;&#22312;&#27169;&#25311;&#25968;&#23383;&#24066;&#22330;&#20013;&#23436;&#25104;&#20080;&#21334;&#20449;&#24687;&#30340;&#20219;&#21153;&#65292;&#36890;&#36807;&#20855;&#22791;&#35780;&#20272;&#20449;&#24687;&#36136;&#37327;&#21644;&#36951;&#24536;&#33021;&#21147;&#30340;&#29305;&#28857;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;&#20449;&#24687;&#24066;&#22330;&#30340;&#20080;&#26041;&#26816;&#26597;&#24726;&#35770;&#12290;</title><link>https://arxiv.org/abs/2403.14443</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#38477;&#20302;&#20449;&#24687;&#24066;&#22330;&#30340;&#19981;&#23545;&#31216;&#24615;
&lt;/p&gt;
&lt;p&gt;
Language Models Can Reduce Asymmetry in Information Markets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14443
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#26234;&#33021;&#20195;&#29702;&#22312;&#27169;&#25311;&#25968;&#23383;&#24066;&#22330;&#20013;&#23436;&#25104;&#20080;&#21334;&#20449;&#24687;&#30340;&#20219;&#21153;&#65292;&#36890;&#36807;&#20855;&#22791;&#35780;&#20272;&#20449;&#24687;&#36136;&#37327;&#21644;&#36951;&#24536;&#33021;&#21147;&#30340;&#29305;&#28857;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;&#20449;&#24687;&#24066;&#22330;&#30340;&#20080;&#26041;&#26816;&#26597;&#24726;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#35299;&#20915;&#20102;&#20449;&#24687;&#24066;&#22330;&#20013;&#20080;&#26041;&#26816;&#26597;&#24726;&#35770;&#30340;&#38382;&#39064;&#12290;&#20080;&#26041;&#38656;&#35201;&#33719;&#21462;&#20449;&#24687;&#26469;&#30830;&#23450;&#20854;&#20215;&#20540;&#65292;&#32780;&#21334;&#26041;&#38656;&#35201;&#38480;&#21046;&#35775;&#38382;&#20197;&#38450;&#27490;&#30423;&#31363;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#20010;&#24320;&#28304;&#30340;&#27169;&#25311;&#25968;&#23383;&#24066;&#22330;&#65292;&#20854;&#20013;&#30001;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#26234;&#33021;&#20195;&#29702;&#20195;&#34920;&#22806;&#37096;&#21442;&#19982;&#32773;&#20080;&#21334;&#20449;&#24687;&#12290;&#36825;&#20010;&#24066;&#22330;&#30340;&#26680;&#24515;&#26426;&#21046;&#22312;&#20110;&#20195;&#29702;&#20154;&#30340;&#21452;&#37325;&#33021;&#21147;&#65306;&#20182;&#20204;&#19981;&#20165;&#33021;&#22815;&#35780;&#20272;&#29305;&#26435;&#20449;&#24687;&#30340;&#36136;&#37327;&#65292;&#36824;&#20855;&#22791;&#36951;&#24536;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#35825;&#23548;&#20581;&#24536;&#30340;&#33021;&#21147;&#20351;&#20379;&#24212;&#21830;&#21487;&#20197;&#25480;&#20104;&#20020;&#26102;&#35775;&#38382;&#19987;&#26377;&#20449;&#24687;&#30340;&#26435;&#38480;&#65292;&#26174;&#33879;&#38477;&#20302;&#26410;&#32463;&#25480;&#26435;&#30340;&#20445;&#30041;&#39118;&#38505;&#65292;&#21516;&#26102;&#20351;&#20195;&#29702;&#20154;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#20449;&#24687;&#23545;&#29305;&#23450;&#26597;&#35810;&#25110;&#20219;&#21153;&#30340;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#34920;&#29616;&#20248;&#24322;&#65292;&#20195;&#29702;&#24517;&#39035;&#20570;&#20986;&#29702;&#24615;&#20915;&#31574;&#65292;&#25112;&#30053;&#24615;&#22320;&#25506;&#32034;&#24066;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14443v1 Announce Type: new  Abstract: This work addresses the buyer's inspection paradox for information markets. The paradox is that buyers need to access information to determine its value, while sellers need to limit access to prevent theft. To study this, we introduce an open-source simulated digital marketplace where intelligent agents, powered by language models, buy and sell information on behalf of external participants. The central mechanism enabling this marketplace is the agents' dual capabilities: they not only have the capacity to assess the quality of privileged information but also come equipped with the ability to forget. This ability to induce amnesia allows vendors to grant temporary access to proprietary information, significantly reducing the risk of unauthorized retention while enabling agents to accurately gauge the information's relevance to specific queries or tasks. To perform well, agents must make rational decisions, strategically explore the marke
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25209;&#21028;&#24615;&#22320;&#20998;&#26512;&#21644;&#35752;&#35770;&#20102;&#21307;&#23398;&#22270;&#20687;&#30340;&#25193;&#25955;&#20998;&#21106;&#19982;&#25193;&#25955;&#22270;&#20687;&#29983;&#25104;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24378;&#35843;&#20102;&#38024;&#23545;&#25193;&#25955;&#20998;&#21106;&#30340;&#26550;&#26500;&#25913;&#36827;&#24102;&#26469;&#30340;&#30410;&#22788;&#12290;</title><link>https://arxiv.org/abs/2403.14440</link><description>&lt;p&gt;
&#20998;&#26512;&#21307;&#23398;&#22270;&#20687;&#30340;&#25193;&#25955;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Analysing Diffusion Segmentation for Medical Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25209;&#21028;&#24615;&#22320;&#20998;&#26512;&#21644;&#35752;&#35770;&#20102;&#21307;&#23398;&#22270;&#20687;&#30340;&#25193;&#25955;&#20998;&#21106;&#19982;&#25193;&#25955;&#22270;&#20687;&#29983;&#25104;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24378;&#35843;&#20102;&#38024;&#23545;&#25193;&#25955;&#20998;&#21106;&#30340;&#26550;&#26500;&#25913;&#36827;&#24102;&#26469;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20854;&#33021;&#22815;&#25552;&#20379;&#27010;&#29575;&#24314;&#27169;&#21644;&#29983;&#25104;&#22810;&#26679;&#21270;&#36755;&#20986;&#30340;&#33021;&#21147;&#65292;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#36825;&#31181;&#22810;&#21151;&#33021;&#24615;&#21551;&#21457;&#20102;&#23427;&#20204;&#34987;&#29992;&#20110;&#22270;&#20687;&#20998;&#21106;&#65292;&#27169;&#22411;&#30340;&#22810;&#27425;&#39044;&#27979;&#21487;&#20197;&#20135;&#29983;&#20998;&#21106;&#32467;&#26524;&#65292;&#19981;&#20165;&#36136;&#37327;&#39640;&#65292;&#32780;&#19988;&#33021;&#22815;&#25429;&#25417;&#27169;&#22411;&#26412;&#36136;&#19978;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#25913;&#36827;&#25193;&#25955;&#20998;&#21106;&#24615;&#33021;&#30340;&#24378;&#22823;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#23545;&#25193;&#25955;&#20998;&#21106;&#21644;&#22270;&#20687;&#29983;&#25104;&#20043;&#38388;&#30340;&#24046;&#24322;&#32570;&#20047;&#20998;&#26512;&#21644;&#35752;&#35770;&#65292;&#24182;&#19988;&#32570;&#20047;&#23545;&#36825;&#20123;&#26550;&#26500;&#25552;&#20379;&#30340;&#25913;&#36827;&#22312;&#20998;&#21106;&#39046;&#22495;&#19982;&#22312;&#29305;&#23450;&#20110;&#25193;&#25955;&#20998;&#21106;&#30340;&#30410;&#22788;&#20043;&#38388;&#36827;&#34892;&#21306;&#20998;&#30340;&#28145;&#20837;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14440v1 Announce Type: cross  Abstract: Denoising Diffusion Probabilistic models have become increasingly popular due to their ability to offer probabilistic modeling and generate diverse outputs. This versatility inspired their adaptation for image segmentation, where multiple predictions of the model can produce segmentation results that not only achieve high quality but also capture the uncertainty inherent in the model. Here, powerful architectures were proposed for improving diffusion segmentation performance. However, there is a notable lack of analysis and discussions on the differences between diffusion segmentation and image generation, and thorough evaluations are missing that distinguish the improvements these architectures provide for segmentation in general from their benefit for diffusion segmentation specifically. In this work, we critically analyse and discuss how diffusion segmentation for medical images differs from diffusion image generation, with a partic
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35774;&#22791;&#23450;&#21521;&#35821;&#38899;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#25991;&#26412;&#21644;&#38899;&#39057;&#27169;&#22411;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#30456;&#31561;&#38169;&#35823;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.14438</link><description>&lt;p&gt;
&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35774;&#22791;&#23450;&#21521;&#35821;&#38899;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Multimodal Approach to Device-Directed Speech Detection with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14438
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35774;&#22791;&#23450;&#21521;&#35821;&#38899;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#25991;&#26412;&#21644;&#38899;&#39057;&#27169;&#22411;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#30456;&#31561;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#25311;&#21161;&#25163;&#30340;&#20132;&#20114;&#36890;&#24120;&#20174;&#39044;&#23450;&#20041;&#35302;&#21457;&#30701;&#35821;&#24320;&#22987;&#65292;&#28982;&#21518;&#26159;&#29992;&#25143;&#21629;&#20196;&#12290;&#20026;&#20102;&#20351;&#19982;&#21161;&#25163;&#30340;&#20132;&#20114;&#26356;&#30452;&#35266;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26159;&#21542;&#21487;&#20197;&#25918;&#24323;&#29992;&#25143;&#24517;&#39035;&#29992;&#35302;&#21457;&#30701;&#35821;&#24320;&#22987;&#27599;&#20010;&#21629;&#20196;&#30340;&#35201;&#27714;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#31181;&#26041;&#24335;&#25506;&#32034;&#20102;&#36825;&#20010;&#20219;&#21153;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#20165;&#20351;&#29992;&#20174;&#38899;&#39057;&#27874;&#24418;&#20013;&#33719;&#24471;&#30340;&#22768;&#23398;&#20449;&#24687;&#35757;&#32451;&#20998;&#31867;&#22120;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#30340;&#35299;&#30721;&#22120;&#36755;&#20986;&#65292;&#20363;&#22914;1-best&#20551;&#35774;&#65292;&#20316;&#20026;&#36755;&#20837;&#29305;&#24449;&#36755;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#31995;&#32479;&#65292;&#23558;&#22768;&#23398;&#21644;&#35789;&#27719;&#29305;&#24449;&#20197;&#21450;ASR&#35299;&#30721;&#22120;&#20449;&#21495;&#32467;&#21512;&#22312;LLM&#20013;&#12290;&#20351;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#30456;&#23545;&#20110;&#20165;&#25991;&#26412;&#21644;&#20165;&#38899;&#39057;&#27169;&#22411;&#25552;&#39640;&#20102;&#30456;&#31561;&#38169;&#35823;&#29575;&#39640;&#36798;39%&#21644;61%&#12290;&#22686;&#21152;LLM&#30340;&#22823;&#23567;&#24182;&#36890;&#36807;&#20302;&#31209;&#35843;&#25972;&#36827;&#34892;&#35757;&#32451;&#36827;&#19968;&#27493;&#20943;&#23569;&#20102;&#30456;&#23545;EER&#20540;&#30340;&#20943;&#23569;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14438v1 Announce Type: new  Abstract: Interactions with virtual assistants typically start with a predefined trigger phrase followed by the user command. To make interactions with the assistant more intuitive, we explore whether it is feasible to drop the requirement that users must begin each command with a trigger phrase. We explore this task in three ways: First, we train classifiers using only acoustic information obtained from the audio waveform. Second, we take the decoder outputs of an automatic speech recognition (ASR) system, such as 1-best hypotheses, as input features to a large language model (LLM). Finally, we explore a multimodal system that combines acoustic and lexical features, as well as ASR decoder signals in an LLM. Using multimodal information yields relative equal-error-rate improvements over text-only and audio-only models of up to 39% and 61%. Increasing the size of the LLM and training with low-rank adaption leads to further relative EER reductions o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#26799;&#24230;&#22522;&#30784;&#30340;CAM&#25216;&#26415;&#25193;&#23637;&#21040;&#20108;&#20803;&#20998;&#31867;&#22120;&#65292;&#24182;&#21487;&#35270;&#21270;&#20108;&#36827;&#21046;&#38754;&#37096;&#23646;&#24615;&#20998;&#31867;&#22120;&#30340;&#27963;&#21160;&#21306;&#22495;&#65292;&#35777;&#23454;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#20559;&#21521;&#24615;&#20998;&#31867;&#22120;&#20542;&#21521;&#20110;&#23398;&#20064;&#25552;&#21462;&#20027;&#35201;&#31867;&#21035;&#30340;&#29305;&#24449;</title><link>https://arxiv.org/abs/2403.14435</link><description>&lt;p&gt;
&#20559;&#21521;&#24615;&#30340;&#20108;&#36827;&#21046;&#23646;&#24615;&#20998;&#31867;&#22120;&#24573;&#35270;&#20102;&#20027;&#35201;&#31867;&#21035;
&lt;/p&gt;
&lt;p&gt;
Biased Binary Attribute Classifiers Ignore the Majority Classes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#26799;&#24230;&#22522;&#30784;&#30340;CAM&#25216;&#26415;&#25193;&#23637;&#21040;&#20108;&#20803;&#20998;&#31867;&#22120;&#65292;&#24182;&#21487;&#35270;&#21270;&#20108;&#36827;&#21046;&#38754;&#37096;&#23646;&#24615;&#20998;&#31867;&#22120;&#30340;&#27963;&#21160;&#21306;&#22495;&#65292;&#35777;&#23454;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#20559;&#21521;&#24615;&#20998;&#31867;&#22120;&#20542;&#21521;&#20110;&#23398;&#20064;&#25552;&#21462;&#20027;&#35201;&#31867;&#21035;&#30340;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#21487;&#35270;&#21270;&#20998;&#31867;&#22120;&#22522;&#20110;&#20854;&#20915;&#31574;&#30340;&#20852;&#36259;&#21306;&#22495;&#65292;&#21457;&#23637;&#20102;&#19981;&#21516;&#30340;Class Activation Mapping (CAM)&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#36825;&#20123;&#25216;&#26415;&#21482;&#38024;&#23545;&#20998;&#31867;&#20998;&#31867;&#22120;&#65292;&#32780;&#22823;&#22810;&#25968;&#23454;&#38469;&#20219;&#21153;&#26159;&#20108;&#20803;&#20998;&#31867;&#12290;&#26412;&#25991;&#23558;&#22522;&#20110;&#26799;&#24230;&#30340;CAM&#25216;&#26415;&#25193;&#23637;&#21040;&#19982;&#20108;&#36827;&#21046;&#20998;&#31867;&#22120;&#19968;&#36215;&#20351;&#29992;&#65292;&#24182;&#21487;&#35270;&#21270;&#20108;&#36827;&#21046;&#38754;&#37096;&#23646;&#24615;&#20998;&#31867;&#22120;&#30340;&#27963;&#21160;&#21306;&#22495;&#12290;&#24403;&#22312;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#19981;&#24179;&#34913;&#30340;&#20108;&#20803;&#20998;&#31867;&#22120;&#26102;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#21363;&#20855;&#26377;&#35768;&#22810;&#35757;&#32451;&#26679;&#26412;&#30340;&#20027;&#35201;&#31867;&#36890;&#24120;&#27604;&#20855;&#26377;&#23569;&#37327;&#35757;&#32451;&#23454;&#20363;&#30340;&#27425;&#35201;&#31867;&#39044;&#27979;&#24471;&#26356;&#22909;&#12290;&#22312;CelebA&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#36825;&#20123;&#32467;&#26524;&#65292;&#24403;&#35757;&#32451;&#19981;&#24179;&#34913;&#20998;&#31867;&#22120;&#21516;&#26102;&#25552;&#21462;40&#20010;&#38754;&#37096;&#23646;&#24615;&#12290;&#20154;&#20204;&#39044;&#26399;&#65292;&#20559;&#21521;&#24615;&#20998;&#31867;&#22120;&#24050;&#32463;&#23398;&#20250;&#20027;&#35201;&#20026;&#22810;&#25968;&#31867;&#25552;&#21462;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14435v1 Announce Type: cross  Abstract: To visualize the regions of interest that classifiers base their decisions on, different Class Activation Mapping (CAM) methods have been developed. However, all of these techniques target categorical classifiers only, though most real-world tasks are binary classification. In this paper, we extend gradient-based CAM techniques to work with binary classifiers and visualize the active regions for binary facial attribute classifiers. When training an unbalanced binary classifier on an imbalanced dataset, it is well-known that the majority class, i.e. the class with many training samples, is mostly predicted much better than minority class with few training instances. In our experiments on the CelebA dataset, we verify these results, when training an unbalanced classifier to extract 40 facial attributes simultaneously. One would expect that the biased classifier has learned to extract features mainly for the majority classes and that the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#39118;&#26684;&#25552;&#21462;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#39118;&#26684;&#35843;&#33410;&#26426;&#21046;&#21644;&#20869;&#23481;&#35843;&#33410;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#22312;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#20013;&#27880;&#20837;&#26410;&#35265;&#22270;&#20687;&#39118;&#26684;&#20449;&#24687;&#65292;&#20174;&#32780;&#20197;&#38646;-shot&#26041;&#24335;&#29983;&#25104;&#20855;&#26377;&#26410;&#35265;&#39118;&#26684;&#30340;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2403.14429</link><description>&lt;p&gt;
&#39118;&#26684;&#25552;&#21462;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#21322;&#30417;&#30563;&#32452;&#32455;&#23398;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Style-Extracting Diffusion Models for Semi-Supervised Histopathology Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14429
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#39118;&#26684;&#25552;&#21462;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#39118;&#26684;&#35843;&#33410;&#26426;&#21046;&#21644;&#20869;&#23481;&#35843;&#33410;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#22312;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#20013;&#27880;&#20837;&#26410;&#35265;&#22270;&#20687;&#39118;&#26684;&#20449;&#24687;&#65292;&#20174;&#32780;&#20197;&#38646;-shot&#26041;&#24335;&#29983;&#25104;&#20855;&#26377;&#26410;&#35265;&#39118;&#26684;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14429v1 &#20844;&#21578;&#31867;&#22411;:&#36328;&#39046;&#22495; &#25688;&#35201;:&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#29983;&#25104;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#26174;&#30528;&#36827;&#23637;&#19979;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#26126;&#26174;&#25913;&#21892;&#20102;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#29983;&#25104;&#20855;&#26377;&#23545;&#19979;&#28216;&#20219;&#21153;&#26377;&#30410;&#30340;&#26410;&#35265;&#29305;&#24449;&#30340;&#22270;&#20687;&#21364;&#21463;&#21040;&#20102;&#36739;&#23569;&#20851;&#27880;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39118;&#26684;&#25552;&#21462;&#25193;&#25955;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#21547;&#20004;&#31181;&#35843;&#33410;&#26426;&#21046;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;1)&#39118;&#26684;&#35843;&#21046;&#26426;&#21046;&#22312;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#20013;&#27880;&#20837;&#20808;&#21069;&#26410;&#35265;&#22270;&#20687;&#30340;&#39118;&#26684;&#20449;&#24687;&#65292;2)&#20869;&#23481;&#35843;&#21046;&#26426;&#21046;&#21487;&#20197;&#38024;&#23545;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#23450;&#20301;&#65292;&#20363;&#22914;&#24067;&#23616;&#29992;&#20110;&#20998;&#21106;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#35757;&#32451;&#30340;&#39118;&#26684;&#32534;&#30721;&#22120;&#65292;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#39118;&#26684;&#20449;&#24687;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#32858;&#21512;&#22359;&#65292;&#29992;&#20110;&#21512;&#24182;&#26469;&#33258;&#22810;&#20010;&#39118;&#26684;&#36755;&#20837;&#30340;&#39118;&#26684;&#20449;&#24687;&#12290;&#36825;&#31181;&#26550;&#26500;&#20351;&#24471;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#26410;&#35265;&#22270;&#20687;&#30340;&#39118;&#26684;&#65292;&#22312;&#38646;-shot&#26041;&#24335;&#19979;&#29983;&#25104;&#20855;&#26377;&#26410;&#35265;&#39118;&#26684;&#30340;&#22270;&#20687;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14429v1 Announce Type: cross  Abstract: Deep learning-based image generation has seen significant advancements with diffusion models, notably improving the quality of generated images. Despite these developments, generating images with unseen characteristics beneficial for downstream tasks has received limited attention. To bridge this gap, we propose Style-Extracting Diffusion Models, featuring two conditioning mechanisms. Specifically, we utilize 1) a style conditioning mechanism which allows to inject style information of previously unseen images during image generation and 2) a content conditioning which can be targeted to a downstream task, e.g., layout for segmentation. We introduce a trainable style encoder to extract style information from images, and an aggregation block that merges style information from multiple style inputs. This architecture enables the generation of images with unseen styles in a zero-shot manner, by leveraging styles from unseen images, result
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#20998;&#27169;&#25311;&#21644;&#20248;&#21270;&#30340;&#20219;&#21153;&#26368;&#20248;&#25968;&#25454;&#39537;&#21160;&#26367;&#20195;&#27169;&#22411;&#26041;&#27861;&#65292;&#22312;eNMPC&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#20026;&#23454;&#29616;&#26356;&#20855;&#33021;&#21147;&#30340;&#25511;&#21046;&#22120;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2403.14425</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#24494;&#20998;&#27169;&#25311;&#21644;&#20248;&#21270;&#30340;&#20219;&#21153;&#26368;&#20248;&#25968;&#25454;&#39537;&#21160;&#26367;&#20195;&#27169;&#22411;&#29992;&#20110;eNMPC
&lt;/p&gt;
&lt;p&gt;
Task-optimal data-driven surrogate models for eNMPC via differentiable simulation and optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14425
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#20998;&#27169;&#25311;&#21644;&#20248;&#21270;&#30340;&#20219;&#21153;&#26368;&#20248;&#25968;&#25454;&#39537;&#21160;&#26367;&#20195;&#27169;&#22411;&#26041;&#27861;&#65292;&#22312;eNMPC&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#20026;&#23454;&#29616;&#26356;&#20855;&#33021;&#21147;&#30340;&#25511;&#21046;&#22120;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25511;&#21046;&#20013;&#20248;&#21270;&#24615;&#33021;&#30340;Koopman&#26367;&#20195;&#27169;&#22411;&#31471;&#21040;&#31471;&#23398;&#20064;&#26041;&#27861;&#12290;&#19982;&#20043;&#21069;&#37319;&#29992;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#30340;&#36129;&#29486;&#30456;&#21453;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#35757;&#32451;&#31639;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#26426;&#26800;&#27169;&#25311;&#27169;&#22411;&#30340;&#29615;&#22659;&#30340;&#28508;&#22312;&#21487;&#24494;&#24615;&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#25991;&#29486;&#24050;&#30693;&#30340;eNMPC&#26696;&#20363;&#30740;&#31350;&#20013;&#20854;&#20182;&#25511;&#21046;&#22120;&#31867;&#22411;&#21644;&#35757;&#32451;&#31639;&#27861;&#32452;&#21512;&#30340;&#24615;&#33021;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#22240;&#27492;&#22312;&#20351;&#29992;&#21160;&#24577;&#26367;&#20195;&#27169;&#22411;&#30340;&#26356;&#26377;&#33021;&#21147;&#30340;&#25511;&#21046;&#22120;&#26041;&#38754;&#26500;&#25104;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14425v1 Announce Type: new  Abstract: We present a method for end-to-end learning of Koopman surrogate models for optimal performance in control. In contrast to previous contributions that employ standard reinforcement learning (RL) algorithms, we use a training algorithm that exploits the potential differentiability of environments based on mechanistic simulation models. We evaluate the performance of our method by comparing it to that of other controller type and training algorithm combinations on a literature known eNMPC case study. Our method exhibits superior performance on this problem, thereby constituting a promising avenue towards more capable controllers that employ dynamic surrogate models.
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#24046;&#20998;&#38544;&#31169;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#26679;&#26412;&#30340;&#21516;&#26102;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#38544;&#31169;&#20445;&#35777;</title><link>https://arxiv.org/abs/2403.14421</link><description>&lt;p&gt;
&#23558;&#25193;&#25955;&#27169;&#22411;&#35843;&#25972;&#21040;&#31169;&#26377;&#39046;&#22495;&#32780;&#26080;&#38656;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14421
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#24046;&#20998;&#38544;&#31169;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#26679;&#26412;&#30340;&#21516;&#26102;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#38544;&#31169;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14421v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25991;&#25688;&#65306;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#34987;&#35777;&#26126;&#23384;&#22312;&#26679;&#26412;&#32423;&#21035;&#30340;&#35760;&#24518;&#21270;&#38382;&#39064;&#65292;&#21487;&#33021;&#20250;&#22797;&#21046;&#20986;&#19982;&#20854;&#35757;&#32451;&#22270;&#20687;&#20960;&#20046;&#23436;&#20840;&#30456;&#21516;&#30340;&#21103;&#26412;&#65292;&#36825;&#21487;&#33021;&#26159;&#19981;&#24076;&#26395;&#30475;&#21040;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#26679;&#26412;&#24182;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#38544;&#31169;&#20445;&#35777;&#30340;&#24046;&#20998;&#38544;&#31169;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#19968;&#20010;&#22312;&#23569;&#37327;&#20844;&#20849;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#35774;&#35745;&#19968;&#20010;DP&#26816;&#32034;&#26426;&#21046;&#65292;&#20197;&#20174;&#31169;&#26377;&#26816;&#32034;&#25968;&#25454;&#38598;&#20013;&#26816;&#32034;&#30340;&#26679;&#26412;&#26469;&#22686;&#24378;&#25991;&#26412;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;\emph{&#24046;&#20998;&#38544;&#31169;&#26816;&#32034;&#22686;&#24378;&#25193;&#25955;&#27169;&#22411;}&#65288;DP-RDM&#65289;&#22312;&#36866;&#24212;&#21478;&#19968;&#20010;&#39046;&#22495;&#26102;&#26080;&#38656;&#23545;&#26816;&#32034;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#26679;&#26412;&#65292;&#21516;&#26102;&#28385;&#36275;&#20005;&#26684;&#30340;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#12290;&#20363;&#22914;&#65292;&#22312;&#35780;&#20272;&#26102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14421v1 Announce Type: new  Abstract: Text-to-image diffusion models have been shown to suffer from sample-level memorization, possibly reproducing near-perfect replica of images that they are trained on, which may be undesirable. To remedy this issue, we develop the first differentially private (DP) retrieval-augmented generation algorithm that is capable of generating high-quality image samples while providing provable privacy guarantees. Specifically, we assume access to a text-to-image diffusion model trained on a small amount of public data, and design a DP retrieval mechanism to augment the text prompt with samples retrieved from a private retrieval dataset. Our \emph{differentially private retrieval-augmented diffusion model} (DP-RDM) requires no fine-tuning on the retrieval dataset to adapt to another domain, and can use state-of-the-art generative models to generate high-quality image samples while satisfying rigorous DP guarantees. For instance, when evaluated on M
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#27604;&#36739;&#20102;&#36827;&#21270;&#20248;&#21270;&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#36741;&#21161;&#31574;&#30053;&#20197;&#22686;&#24378;&#31639;&#27861;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.14413</link><description>&lt;p&gt;
&#36827;&#21270;&#20248;&#21270;&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65306;&#19968;&#39033;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Model Uncertainty in Evolutionary Optimization and Bayesian Optimization: A Comparative Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14413
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#27604;&#36739;&#20102;&#36827;&#21270;&#20248;&#21270;&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#36741;&#21161;&#31574;&#30053;&#20197;&#22686;&#24378;&#31639;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#30418;&#20248;&#21270;&#38382;&#39064;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#24456;&#24120;&#35265;&#65292;&#38656;&#35201;&#36890;&#36807;&#36755;&#20837;&#36755;&#20986;&#20132;&#20114;&#26469;&#36827;&#34892;&#20248;&#21270;&#65292;&#32780;&#27809;&#26377;&#35775;&#38382;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#21644;&#36741;&#21161;&#20195;&#29702;&#36827;&#21270;&#31639;&#27861;&#65288;SAEA&#65289;&#26159;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26080;&#26799;&#24230;&#20248;&#21270;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#26412;&#25991;&#26088;&#22312;&#38416;&#26126;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#21033;&#29992;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#65292;&#20197;&#21450;&#27169;&#22411;&#19981;&#20934;&#30830;&#24615;&#23545;&#31639;&#27861;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#36741;&#21161;&#31574;&#30053;&#65292;&#21033;&#29992;&#26410;&#35780;&#20272;&#30340;&#35299;&#20915;&#26041;&#26696;&#29983;&#25104;&#21518;&#20195;&#65292;&#21033;&#29992;&#36827;&#21270;&#31639;&#27861;&#30340;&#22522;&#20110;&#31181;&#32676;&#30340;&#25628;&#32034;&#33021;&#21147;&#26469;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14413v1 Announce Type: cross  Abstract: Black-box optimization problems, which are common in many real-world applications, require optimization through input-output interactions without access to internal workings. This often leads to significant computational resources being consumed for simulations. Bayesian Optimization (BO) and Surrogate-Assisted Evolutionary Algorithm (SAEA) are two widely used gradient-free optimization techniques employed to address such challenges. Both approaches follow a similar iterative procedure that relies on surrogate models to guide the search process. This paper aims to elucidate the similarities and differences in the utilization of model uncertainty between these two methods, as well as the impact of model inaccuracies on algorithmic performance. A novel model-assisted strategy is introduced, which utilizes unevaluated solutions to generate offspring, leveraging the population-based search capabilities of evolutionary algorithm to enhance 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;GLC++&#26041;&#27861;&#65292;&#36890;&#36807;&#20840;&#23616;&#21644;&#23616;&#37096;&#32858;&#31867;&#20197;&#21450;&#23545;&#27604;&#20851;&#32852;&#23398;&#20064;&#23454;&#29616;&#20102;&#26080;&#28304;&#36890;&#29992;&#22495;&#33258;&#36866;&#24212;&#65292;&#33021;&#22815;&#20934;&#30830;&#20998;&#31867;&#24050;&#30693;&#25968;&#25454;&#24182;&#23558;&#20854;&#20174;&#26410;&#30693;&#25968;&#25454;&#20013;&#20998;&#31163;&#12290;</title><link>https://arxiv.org/abs/2403.14410</link><description>&lt;p&gt;
GLC++: &#20840;&#23616;&#23616;&#37096;&#32858;&#31867;&#21644;&#23545;&#27604;&#20851;&#32852;&#23398;&#20064;&#30340;&#26080;&#28304;&#36890;&#29992;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
GLC++: Source-Free Universal Domain Adaptation through Global-Local Clustering and Contrastive Affinity Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14410
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;GLC++&#26041;&#27861;&#65292;&#36890;&#36807;&#20840;&#23616;&#21644;&#23616;&#37096;&#32858;&#31867;&#20197;&#21450;&#23545;&#27604;&#20851;&#32852;&#23398;&#20064;&#23454;&#29616;&#20102;&#26080;&#28304;&#36890;&#29992;&#22495;&#33258;&#36866;&#24212;&#65292;&#33021;&#22815;&#20934;&#30830;&#20998;&#31867;&#24050;&#30693;&#25968;&#25454;&#24182;&#23558;&#20854;&#20174;&#26410;&#30693;&#25968;&#25454;&#20013;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32463;&#24120;&#22312;&#21327;&#21464;&#37327;&#21644;&#31867;&#21035;&#36716;&#31227;&#19979;&#34920;&#29616;&#20986;&#27425;&#20248;&#24615;&#33021;&#12290;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#65288;SFDA&#65289;&#20026;&#36825;&#19968;&#22256;&#22659;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#28982;&#32780;&#22823;&#22810;&#25968;SFDA&#26041;&#27861;&#23616;&#38480;&#20110;&#23553;&#38381;&#38598;&#22330;&#26223;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26088;&#22312;&#20934;&#30830;&#20998;&#31867;&#23646;&#20110;&#24120;&#35265;&#31867;&#21035;&#30340;&#8220;&#24050;&#30693;&#8221;&#25968;&#25454;&#24182;&#23558;&#20854;&#19982;&#30446;&#26631;&#19987;&#26377;&#8220;&#26410;&#30693;&#8221;&#25968;&#25454;&#38548;&#31163;&#24320;&#26469;&#30340;&#26080;&#28304;&#36890;&#29992;&#22495;&#33258;&#36866;&#24212;&#65288;SF-UniDA&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20840;&#29699;&#21644;&#23616;&#37096;&#32858;&#31867;&#65288;GLC&#65289;&#25216;&#26415;&#65292;&#20854;&#20013;&#21253;&#25324;&#33258;&#36866;&#24212;&#30340;&#19968;&#23545;&#20840;&#23616;&#32858;&#31867;&#31639;&#27861;&#26469;&#21306;&#20998;&#30446;&#26631;&#31867;&#21035;&#65292;&#36741;&#20197;&#26412;&#22320;k-NN&#32858;&#31867;&#31574;&#30053;&#20197;&#20943;&#36731;&#36127;&#38754;&#36716;&#31227;&#12290;&#23613;&#31649;&#26377;&#25928;&#65292;&#20294;&#22266;&#26377;&#30340;&#23553;&#38381;&#28304;&#26550;&#26500;&#23548;&#33268;&#23545;&#8220;&#26410;&#30693;&#8221;&#25968;&#25454;&#30340;&#32479;&#19968;&#22788;&#29702;&#65292;&#38459;&#30861;&#20102;&#23545;&#19981;&#21516;&#8220;&#26410;&#30693;&#8221;&#31867;&#21035;&#30340;&#35782;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;GLC&#21457;&#23637;&#21040;GLC++&#65292;&#25972;&#21512;&#20102;&#23545;&#27604;&#20146;&#21644;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14410v1 Announce Type: cross  Abstract: Deep neural networks often exhibit sub-optimal performance under covariate and category shifts. Source-Free Domain Adaptation (SFDA) presents a promising solution to this dilemma, yet most SFDA approaches are restricted to closed-set scenarios. In this paper, we explore Source-Free Universal Domain Adaptation (SF-UniDA) aiming to accurately classify "known" data belonging to common categories and segregate them from target-private "unknown" data. We propose a novel Global and Local Clustering (GLC) technique, which comprises an adaptive one-vs-all global clustering algorithm to discern between target classes, complemented by a local k-NN clustering strategy to mitigate negative transfer. Despite the effectiveness, the inherent closed-set source architecture leads to uniform treatment of "unknown" data, impeding the identification of distinct "unknown" categories. To address this, we evolve GLC to GLC++, integrating a contrastive affini
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#21270;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#26694;&#26550;&#65292;&#21487;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#23545;&#29983;&#25104;&#26679;&#26412;&#26045;&#21152;&#32422;&#26463;&#65292;&#20197;&#25913;&#21892;&#26679;&#26412;&#19982;&#32422;&#26463;&#30340;&#23545;&#40784;&#31243;&#24230;&#24182;&#25552;&#20379;&#33258;&#28982;&#30340;&#27491;&#21017;&#21270;&#65292;&#36866;&#29992;&#24615;&#24191;&#27867;&#12290;</title><link>https://arxiv.org/abs/2403.14404</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14404
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#21270;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#26694;&#26550;&#65292;&#21487;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#23545;&#29983;&#25104;&#26679;&#26412;&#26045;&#21152;&#32422;&#26463;&#65292;&#20197;&#25913;&#21892;&#26679;&#26412;&#19982;&#32422;&#26463;&#30340;&#23545;&#40784;&#31243;&#24230;&#24182;&#25552;&#20379;&#33258;&#28982;&#30340;&#27491;&#21017;&#21270;&#65292;&#36866;&#29992;&#24615;&#24191;&#27867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#22914;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#27491;&#24555;&#36895;&#25552;&#21319;&#20854;&#36924;&#36817;&#39640;&#24230;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#30340;&#33021;&#21147;&#12290;&#23427;&#20204;&#20063;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#36816;&#29992;&#20110;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#39044;&#26399;&#20174;&#38544;&#21547;&#25968;&#25454;&#20998;&#24067;&#20013;&#21462;&#26679;&#30340;&#26679;&#26412;&#23558;&#36981;&#23432;&#29305;&#23450;&#30340;&#25511;&#21046;&#26041;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#23545;&#29983;&#25104;&#26679;&#26412;&#30340;&#22522;&#30784;&#32422;&#26463;&#36827;&#34892;&#20449;&#24687;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#29983;&#25104;&#26679;&#26412;&#19982;&#26045;&#21152;&#32422;&#26463;&#30340;&#23545;&#40784;&#31243;&#24230;&#65292;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#32780;&#19981;&#24433;&#21709;&#25512;&#29702;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21152;&#20837;&#36825;&#20123;&#32422;&#26463;&#25552;&#20379;&#20102;&#33258;&#28982;&#30340;&#38450;&#27490;&#36807;&#25311;&#21512;&#30340;&#27491;&#21017;&#21270;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26131;&#20110;&#23454;&#29616;&#65292;&#36866;&#29992;&#24615;&#24191;&#27867;&#65292;&#21487;&#29992;&#20110;&#26045;&#21152;&#31561;&#24335;&#21644;&#19981;&#31561;&#24335;&#32422;&#26463;&#20197;&#21450;&#36741;&#21161;&#20248;&#21270;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14404v1 Announce Type: new  Abstract: Generative models such as denoising diffusion models are quickly advancing their ability to approximate highly complex data distributions. They are also increasingly leveraged in scientific machine learning, where samples from the implied data distribution are expected to adhere to specific governing equations. We present a framework to inform denoising diffusion models on underlying constraints on such generated samples during model training. Our approach improves the alignment of the generated samples with the imposed constraints and significantly outperforms existing methods without affecting inference speed. Additionally, our findings suggest that incorporating such constraints during training provides a natural regularization against overfitting. Our framework is easy to implement and versatile in its applicability for imposing equality and inequality constraints as well as auxiliary optimization objectives.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;RAMDA&#31639;&#27861;&#29992;&#20110;&#35757;&#32451;&#32467;&#26500;&#21270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24341;&#20837;&#20102;&#20351;&#29992;&#36817;&#20284;&#35299;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#22312;&#25910;&#25947;&#28857;&#38468;&#36817;RAMDA&#30340;&#36845;&#20195;&#36798;&#21040;&#20102;&#26368;&#20248;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.14398</link><description>&lt;p&gt;
&#29992;&#25928;&#29575;&#20302;&#19979;&#30340;&#36817;&#20284;&#23376;&#38382;&#39064;&#35299;&#31639;&#22120;&#35757;&#32451;&#32467;&#26500;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21017;&#21270;&#33258;&#36866;&#24212;&#21160;&#37327;&#21452;&#24179;&#22343;
&lt;/p&gt;
&lt;p&gt;
Regularized Adaptive Momentum Dual Averaging with an Efficient Inexact Subproblem Solver for Training Structured Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14398
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;RAMDA&#31639;&#27861;&#29992;&#20110;&#35757;&#32451;&#32467;&#26500;&#21270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24341;&#20837;&#20102;&#20351;&#29992;&#36817;&#20284;&#35299;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#22312;&#25910;&#25947;&#28857;&#38468;&#36817;RAMDA&#30340;&#36845;&#20195;&#36798;&#21040;&#20102;&#26368;&#20248;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#32467;&#26500;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21017;&#21270;&#33258;&#36866;&#24212;&#21160;&#37327;&#21452;&#24179;&#22343;&#65288;RAMDA&#65289;&#31639;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#27491;&#21017;&#21270;&#33258;&#36866;&#24212;&#26041;&#27861;&#31867;&#20284;&#65292;RAMDA&#30340;&#26356;&#26032;&#26041;&#21521;&#35745;&#31639;&#23376;&#38382;&#39064;&#28041;&#21450;&#38750;&#20809;&#28369;&#27491;&#21017;&#21270;&#39033;&#21644;&#23545;&#35282;&#39044;&#22788;&#29702;&#22120;&#65292;&#22240;&#27492;&#19968;&#33324;&#32780;&#35328;&#27809;&#26377;&#23553;&#38381;&#24418;&#24335;&#30340;&#35299;&#12290;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#19968;&#20010;&#21487;&#23454;&#29616;&#30340;&#36817;&#20284;&#26465;&#20214;&#65292;&#20445;&#30041;&#20102;&#31867;&#20284;&#20110;&#31934;&#30830;&#29256;&#26412;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#37197;&#22871;&#30340;&#39640;&#25928;&#27714;&#35299;&#22120;&#65292;&#29992;&#20110;&#20351;RAMDA&#21644;&#29616;&#26377;&#26041;&#27861;&#30340;&#23376;&#38382;&#39064;&#22312;&#23454;&#36341;&#20013;&#21487;&#34892;&#12290;&#25105;&#20204;&#21033;&#29992;&#21464;&#20998;&#20998;&#26512;&#20013;&#30340;&#27969;&#24418;&#35782;&#21035;&#29702;&#35770;&#34920;&#26126;&#65292;&#21363;&#20351;&#23384;&#22312;&#36825;&#31181;&#36817;&#20284;&#24615;&#65292;RAMDA&#30340;&#36845;&#20195;&#22312;&#28176;&#36817;&#25910;&#25947;&#30340;&#31283;&#23450;&#28857;&#22788;&#36798;&#21040;&#30001;&#27491;&#21017;&#21270;&#39033;&#35825;&#23548;&#30340;&#29702;&#24819;&#32467;&#26500;&#12290;&#22312;&#25910;&#25947;&#28857;&#38468;&#36817;&#65292;&#36825;&#31181;&#32467;&#26500;&#22312;&#23616;&#37096;&#19978;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14398v1 Announce Type: new  Abstract: We propose a Regularized Adaptive Momentum Dual Averaging (RAMDA) algorithm for training structured neural networks. Similar to existing regularized adaptive methods, the subproblem for computing the update direction of RAMDA involves a nonsmooth regularizer and a diagonal preconditioner, and therefore does not possess a closed-form solution in general. We thus also carefully devise an implementable inexactness condition that retains convergence guarantees similar to the exact versions, and propose a companion efficient solver for the subproblems of both RAMDA and existing methods to make them practically feasible. We leverage the theory of manifold identification in variational analysis to show that, even in the presence of such inexactness, the iterates of RAMDA attain the ideal structure induced by the regularizer at the stationary point of asymptotic convergence. This structure is locally optimal near the point of convergence, so RAM
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38024;&#23545;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#19968;&#25597;&#23376;&#25216;&#24039;&#26694;&#26550;&#65292;&#23558;&#20843;&#31181;&#20851;&#38190;&#25216;&#26415;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#25913;&#36827;&#20102;&#31283;&#23450;&#24615;&#12289;&#36866;&#24212;&#24615;&#21644;&#25972;&#20307;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.14392</link><description>&lt;p&gt;
&#29992;&#20110;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#19968;&#25597;&#23376;&#25216;&#24039;
&lt;/p&gt;
&lt;p&gt;
A Bag of Tricks for Few-Shot Class-Incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14392
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38024;&#23545;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#19968;&#25597;&#23376;&#25216;&#24039;&#26694;&#26550;&#65292;&#23558;&#20843;&#31181;&#20851;&#38190;&#25216;&#26415;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#25913;&#36827;&#20102;&#31283;&#23450;&#24615;&#12289;&#36866;&#24212;&#24615;&#21644;&#25972;&#20307;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19968;&#25597;&#23376;&#25216;&#24039;&#26694;&#26550;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;FSCIL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36830;&#32493;&#23398;&#20064;&#24418;&#24335;&#65292;&#28041;&#21450;&#23545;&#26032;&#20219;&#21153;&#36827;&#34892;&#36830;&#32493;&#36866;&#24212;&#65292;&#24182;&#19988;&#26679;&#26412;&#26377;&#38480;&#12290; FSCIL &#38656;&#35201;&#20445;&#25345;&#31283;&#23450;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#21363;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#20445;&#25345;&#20808;&#21069;&#23398;&#20064;&#20219;&#21153;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#19968;&#25597;&#23376;&#25216;&#24039;&#23558;&#20843;&#31181;&#20851;&#38190;&#19988;&#20855;&#26377;&#39640;&#24433;&#21709;&#21147;&#30340;&#25216;&#26415;&#27719;&#38598;&#22312;&#19968;&#36215;&#65292;&#38024;&#23545; FSCIL &#22312;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#19979;&#25913;&#36827;&#31283;&#23450;&#24615;&#12289;&#36866;&#24212;&#24615;&#21644;&#25972;&#20307;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#25216;&#24039;&#32452;&#32455;&#25104;&#19977;&#31867;&#65306;&#31283;&#23450;&#24615;&#25216;&#24039;&#12289;&#36866;&#24212;&#24615;&#25216;&#24039;&#21644;&#35757;&#32451;&#25216;&#24039;&#12290;&#31283;&#23450;&#24615;&#25216;&#24039;&#26088;&#22312;&#36890;&#36807;&#22686;&#24378;&#24050;&#23398;&#20064;&#31867;&#21035;&#30340;&#23884;&#20837;&#20043;&#38388;&#30340;&#20998;&#31163;&#21644;&#22312;&#23398;&#20064;&#26032;&#31867;&#21035;&#26102;&#26368;&#23567;&#21270;&#24178;&#25200;&#26469;&#20943;&#36731;&#20808;&#21069;&#23398;&#20064;&#31867;&#21035;&#30340;&#36951;&#24536;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36866;&#24212;&#24615;&#25216;&#24039;&#20391;&#37325;&#20110;&#26377;&#25928;&#23398;&#20064;&#26032;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14392v1 Announce Type: cross  Abstract: We present a bag of tricks framework for few-shot class-incremental learning (FSCIL), which is a challenging form of continual learning that involves continuous adaptation to new tasks with limited samples. FSCIL requires both stability and adaptability, i.e., preserving proficiency in previously learned tasks while learning new ones. Our proposed bag of tricks brings together eight key and highly influential techniques that improve stability, adaptability, and overall performance under a unified framework for FSCIL. We organize these tricks into three categories: stability tricks, adaptability tricks, and training tricks. Stability tricks aim to mitigate the forgetting of previously learned classes by enhancing the separation between the embeddings of learned classes and minimizing interference when learning new ones. On the other hand, adaptability tricks focus on the effective learning of new classes. Finally, training tricks improv
&lt;/p&gt;</description></item><item><title>&#21452;&#37325;/&#26080;&#20559;&#26426;&#22120;&#23398;&#20064;&#65288;DML&#65289;&#26041;&#27861;&#25913;&#36827;&#20102;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#20013;&#23545;&#38750;&#32447;&#24615;&#28151;&#28102;&#20851;&#31995;&#30340;&#35843;&#25972;&#65292;&#25670;&#33073;&#20256;&#32479;&#20989;&#25968;&#24418;&#24335;&#20551;&#35774;&#65292;&#20294;&#20173;&#28982;&#20381;&#36182;&#20110;&#26631;&#20934;&#22240;&#26524;&#20551;&#35774;&#12290;</title><link>https://arxiv.org/abs/2403.14385</link><description>&lt;p&gt;
&#29992;&#21452;&#26426;&#22120;&#23398;&#20064;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;--&#19968;&#31181;&#26041;&#27861;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Estimating Causal Effects with Double Machine Learning -- A Method Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14385
&lt;/p&gt;
&lt;p&gt;
&#21452;&#37325;/&#26080;&#20559;&#26426;&#22120;&#23398;&#20064;&#65288;DML&#65289;&#26041;&#27861;&#25913;&#36827;&#20102;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#20013;&#23545;&#38750;&#32447;&#24615;&#28151;&#28102;&#20851;&#31995;&#30340;&#35843;&#25972;&#65292;&#25670;&#33073;&#20256;&#32479;&#20989;&#25968;&#24418;&#24335;&#20551;&#35774;&#65292;&#20294;&#20173;&#28982;&#20381;&#36182;&#20110;&#26631;&#20934;&#22240;&#26524;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#35266;&#27979;&#25968;&#25454;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#20173;&#28982;&#26159;&#19968;&#20010;&#38750;&#24120;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25918;&#23485;&#20256;&#32479;&#20551;&#35774;&#20197;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#30340;&#26032;&#26694;&#26550;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#20854;&#20013;&#19968;&#20010;&#26368;&#37325;&#35201;&#30340;&#26041;&#27861;-"&#21452;/&#26080;&#20559;&#26426;&#22120;&#23398;&#20064;"&#65288;DML&#65289;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#23427;&#22312;&#27169;&#25311;&#25968;&#25454;&#19978;&#30456;&#23545;&#20110;&#26356;&#20256;&#32479;&#30340;&#32479;&#35745;&#26041;&#27861;&#30340;&#34920;&#29616;&#65292;&#28982;&#21518;&#23558;&#20854;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;DML&#20013;&#24212;&#29992;&#19968;&#20010;&#36866;&#24403;&#28789;&#27963;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#25913;&#36827;&#23545;&#21508;&#31181;&#38750;&#32447;&#24615;&#28151;&#28102;&#20851;&#31995;&#30340;&#35843;&#25972;&#12290;&#36825;&#31181;&#20248;&#21183;&#20351;&#24471;&#21487;&#20197;&#25670;&#33073;&#36890;&#24120;&#22312;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#20013;&#24517;&#38656;&#30340;&#20256;&#32479;&#20989;&#25968;&#24418;&#24335;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20851;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#26631;&#20934;&#20551;&#35774;&#26041;&#38754;&#20173;&#28982;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14385v1 Announce Type: cross  Abstract: The estimation of causal effects with observational data continues to be a very active research area. In recent years, researchers have developed new frameworks which use machine learning to relax classical assumptions necessary for the estimation of causal effects. In this paper, we review one of the most prominent methods - "double/debiased machine learning" (DML) - and empirically evaluate it by comparing its performance on simulated data relative to more traditional statistical methods, before applying it to real-world data. Our findings indicate that the application of a suitably flexible machine learning algorithm within DML improves the adjustment for various nonlinear confounding relationships. This advantage enables a departure from traditional functional form assumptions typically necessary in causal effect estimation. However, we demonstrate that the method continues to critically depend on standard assumptions about causal 
&lt;/p&gt;</description></item><item><title>&#24352;&#37327;&#21270;&#26159;&#23558;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21367;&#31215;&#26680;&#26367;&#25442;&#20026;&#32039;&#20945;&#20998;&#35299;&#65292;&#24182;&#30452;&#25509;&#35757;&#32451;&#20998;&#35299;&#22240;&#23376;&#20197;&#20559;&#21521;&#20110;&#20302;&#31209;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#24352;&#37327;&#21270;&#22914;&#20309;&#36890;&#36807;&#35780;&#20272;&#25130;&#26029;&#21367;&#31215;&#26680;&#26469;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14379</link><description>&lt;p&gt;
&#21367;&#31215;&#27169;&#22411;&#30340;&#24352;&#37327;&#32593;&#32476;&#21487;&#21387;&#32553;&#24615;
&lt;/p&gt;
&lt;p&gt;
Tensor network compressibility of convolutional models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14379
&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#21270;&#26159;&#23558;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21367;&#31215;&#26680;&#26367;&#25442;&#20026;&#32039;&#20945;&#20998;&#35299;&#65292;&#24182;&#30452;&#25509;&#35757;&#32451;&#20998;&#35299;&#22240;&#23376;&#20197;&#20559;&#21521;&#20110;&#20302;&#31209;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#24352;&#37327;&#21270;&#22914;&#20309;&#36890;&#36807;&#35780;&#20272;&#25130;&#26029;&#21367;&#31215;&#26680;&#26469;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#20195;&#34920;&#20102;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20043;&#19968;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#19968;&#33324;&#24773;&#20917;&#19979;&#26356;&#22823;&#30340;CNNs&#36890;&#24120;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#36890;&#36807;&#8220;&#24352;&#37327;&#21270;&#8221;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23567;&#23427;&#20204;&#30340;&#22823;&#23567;&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;&#24352;&#37327;&#21270;&#21253;&#25324;&#23558;&#21367;&#31215;&#26680;&#26367;&#25442;&#20026;&#22914;Tucker&#12289;Canonical Polyadic&#20998;&#35299;&#25110;&#21463;&#37327;&#23376;&#21551;&#21457;&#30340;&#20998;&#35299;&#65288;&#22914;&#30697;&#38453;&#20056;&#31215;&#29366;&#24577;&#65289;&#31561;&#32039;&#20945;&#30340;&#20998;&#35299;&#65292;&#24182;&#30452;&#25509;&#35757;&#32451;&#20998;&#35299;&#20013;&#30340;&#22240;&#23376;&#65292;&#20197;&#20559;&#21521;&#20110;&#20302;&#31209;&#20998;&#35299;&#12290;&#20294;&#20026;&#20160;&#20040;&#24352;&#37327;&#21270;&#20284;&#20046;&#23545;&#20934;&#30830;&#24615;&#27809;&#26377;&#19981;&#21033;&#24433;&#21709;&#65311;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;&#25130;&#26029;&#23494;&#38598;&#65288;&#38750;&#24352;&#37327;&#21270;&#65289;CNNs&#30340;&#21367;&#31215;&#26680;&#23545;&#20854;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#26469;&#25506;&#35752;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14379v1 Announce Type: cross  Abstract: Convolutional neural networks (CNNs) represent one of the most widely used neural network architectures, showcasing state-of-the-art performance in computer vision tasks. Although larger CNNs generally exhibit higher accuracy, their size can be effectively reduced by "tensorization" while maintaining accuracy. Tensorization consists of replacing the convolution kernels with compact decompositions such as Tucker, Canonical Polyadic decompositions, or quantum-inspired decompositions such as matrix product states, and directly training the factors in the decompositions to bias the learning towards low-rank decompositions. But why doesn't tensorization seem to impact the accuracy adversely? We explore this by assessing how truncating the convolution kernels of dense (untensorized) CNNs impact their accuracy. Specifically, we truncated the kernels of (i) a vanilla four-layer CNN and (ii) ResNet-50 pre-trained for image classification on CIF
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#29992;&#25143;&#20013;&#24515;&#23376;&#22270;&#32593;&#32476;&#25512;&#33616;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#20449;&#24687;&#21644;&#30693;&#35782;&#22270;&#20013;&#30340;&#38468;&#21152;&#20449;&#24687;&#32467;&#21512;&#21040;&#23376;&#22270;&#23398;&#20064;&#20013;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;</title><link>https://arxiv.org/abs/2403.14377</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#29992;&#25143;&#20013;&#24515;&#23376;&#22270;&#32593;&#32476;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Enhanced Recommendation with User-Centric Subgraph Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14377
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#29992;&#25143;&#20013;&#24515;&#23376;&#22270;&#32593;&#32476;&#25512;&#33616;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#20449;&#24687;&#21644;&#30693;&#35782;&#22270;&#20013;&#30340;&#38468;&#21152;&#20449;&#24687;&#32467;&#21512;&#21040;&#23376;&#22270;&#23398;&#20064;&#20013;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#24403;&#20170;&#21508;&#31181;&#24179;&#21488;&#19978;&#24471;&#21040;&#24191;&#27867;&#23454;&#26045;&#65292;&#26681;&#25454;&#29992;&#25143;&#30340;&#20559;&#22909;&#21521;&#20182;&#20204;&#25512;&#33616;&#30456;&#20851;&#30340;&#29289;&#21697;&#12290;&#20381;&#36182;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#30697;&#38453;&#30340;&#32463;&#20856;&#26041;&#27861;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#26032;&#29289;&#21697;&#32570;&#20047;&#20132;&#20114;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#12290;&#22522;&#20110;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#30340;&#25512;&#33616;&#31995;&#32479;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;KG&#30340;&#26041;&#27861;&#37319;&#29992;&#33410;&#28857;&#23884;&#20837;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#33021;&#20026;&#19981;&#21516;&#29992;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#65292;&#20063;&#26080;&#27861;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#26032;&#29289;&#21697;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#29992;&#25143;&#20013;&#24515;&#23376;&#22270;&#32593;&#32476;&#65288;KUCNet&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36827;&#34892;&#26377;&#25928;&#25512;&#33616;&#30340;&#23376;&#22270;&#23398;&#20064;&#26041;&#27861;&#12290;KUCNet&#20026;&#27599;&#20010;&#29992;&#25143;-&#29289;&#21697;&#23545;&#26500;&#24314;&#19968;&#20010;U-I&#23376;&#22270;&#65292;&#35813;&#23376;&#22270;&#25429;&#33719;&#20102;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#30340;&#21382;&#21490;&#20449;&#24687;&#21644;KG&#20013;&#25552;&#20379;&#30340;&#38468;&#21152;&#20449;&#24687;&#12290;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;GNN&#36827;&#20837;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14377v1 Announce Type: cross  Abstract: Recommendation systems, as widely implemented nowadays on various platforms, recommend relevant items to users based on their preferences. The classical methods which rely on user-item interaction matrices has limitations, especially in scenarios where there is a lack of interaction data for new items. Knowledge graph (KG)-based recommendation systems have emerged as a promising solution. However, most KG-based methods adopt node embeddings, which do not provide personalized recommendations for different users and cannot generalize well to the new items. To address these limitations, we propose Knowledge-enhanced User-Centric subgraph Network (KUCNet), a subgraph learning approach with graph neural network (GNN) for effective recommendation. KUCNet constructs a U-I subgraph for each user-item pair that captures both the historical information of user-item interactions and the side information provided in KG. An attention-based GNN is d
&lt;/p&gt;</description></item><item><title>&#24490;&#29615;&#25913;&#36827;&#65288;LI&#65289;&#26159;&#19968;&#31181;&#26080;&#38656;&#20013;&#22830;&#26381;&#21153;&#22120;&#25110;&#25968;&#25454;&#20132;&#25442;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#21487;&#25552;&#39640;&#25968;&#25454;&#24322;&#36136;&#24615;&#19979;&#30340;&#29305;&#24449;&#25552;&#21462;&#25928;&#29575;&#65292;&#34920;&#29616;&#20248;&#36234;&#20110;&#20808;&#36827;&#31639;&#27861;FedALA&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#21644;&#20840;&#23616;&#27169;&#22411;&#29615;&#22659;&#12290;</title><link>https://arxiv.org/abs/2403.14371</link><description>&lt;p&gt;
&#24490;&#29615;&#25913;&#36827;&#65306;&#19968;&#31181;&#20174;&#24322;&#26500;&#25968;&#25454;&#20013;&#25552;&#21462;&#20849;&#20139;&#29305;&#24449;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#26080;&#38656;&#20013;&#22830;&#26381;&#21153;&#22120;
&lt;/p&gt;
&lt;p&gt;
Loop Improvement: An Efficient Approach for Extracting Shared Features from Heterogeneous Data without Central Server
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14371
&lt;/p&gt;
&lt;p&gt;
&#24490;&#29615;&#25913;&#36827;&#65288;LI&#65289;&#26159;&#19968;&#31181;&#26080;&#38656;&#20013;&#22830;&#26381;&#21153;&#22120;&#25110;&#25968;&#25454;&#20132;&#25442;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#21487;&#25552;&#39640;&#25968;&#25454;&#24322;&#36136;&#24615;&#19979;&#30340;&#29305;&#24449;&#25552;&#21462;&#25928;&#29575;&#65292;&#34920;&#29616;&#20248;&#36234;&#20110;&#20808;&#36827;&#31639;&#27861;FedALA&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#21644;&#20840;&#23616;&#27169;&#22411;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#26174;&#33879;&#24433;&#21709;&#24615;&#33021;&#12290;&#19968;&#31181;&#20856;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#23558;&#36825;&#20123;&#21442;&#25968;&#20998;&#20026;&#20849;&#20139;&#21644;&#20010;&#24615;&#21270;&#32452;&#20214;&#65292;&#36825;&#20010;&#27010;&#24565;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#20063;&#24456;&#37325;&#35201;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#24490;&#29615;&#25913;&#36827;&#8221;&#65288;LI&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#22686;&#24378;&#20102;&#36825;&#31181;&#20998;&#31163;&#21644;&#29305;&#24449;&#25552;&#21462;&#65292;&#32780;&#19981;&#38656;&#35201;&#20013;&#22830;&#26381;&#21153;&#22120;&#25110;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#25968;&#25454;&#20132;&#25442;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#22312;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#65292;LI&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#22987;&#32456;&#20248;&#20110;&#20808;&#36827;&#30340;FedALA&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;LI&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#19982;&#32858;&#21512;&#25152;&#26377;&#23458;&#25143;&#31471;&#25968;&#25454;&#26102;&#23454;&#29616;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#22312;&#20840;&#23616;&#27169;&#22411;&#29615;&#22659;&#20013;&#65292;&#20351;&#29992;&#20855;&#26377;&#22534;&#21472;&#20010;&#24615;&#21270;&#23618;&#21644;&#39069;&#22806;&#32593;&#32476;&#30340;LI&#20063;&#20135;&#29983;&#20102;&#19982;&#21512;&#24182;&#23458;&#25143;&#31471;&#25968;&#25454;&#22330;&#26223;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;LI&#30340;&#36866;&#24212;&#33021;&#21147;&#24310;&#23637;&#21040;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14371v1 Announce Type: cross  Abstract: In federated learning, data heterogeneity significantly impacts performance. A typical solution involves segregating these parameters into shared and personalized components, a concept also relevant in multi-task learning. Addressing this, we propose "Loop Improvement" (LI), a novel method enhancing this separation and feature extraction without necessitating a central server or data interchange among participants. Our experiments reveal LI's superiority in several aspects: In personalized federated learning environments, LI consistently outperforms the advanced FedALA algorithm in accuracy across diverse scenarios. Additionally, LI's feature extractor closely matches the performance achieved when aggregating data from all clients. In global model contexts, employing LI with stacked personalized layers and an additional network also yields comparable results to combined client data scenarios. Furthermore, LI's adaptability extends to m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20803;&#32479;&#35745;&#23398;&#30340;&#26041;&#27861;&#65292;&#21487;&#21033;&#29992;&#39640;&#20809;&#35889;&#22270;&#20687;&#26816;&#27979;&#34562;&#34588;&#34562;&#19978;&#30340;&#23492;&#29983;&#24615;&#21464;&#24418;&#34728;&#65292;&#20026;&#34588;&#34562;&#24034;&#31348;&#30340;&#30417;&#27979;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2403.14359</link><description>&lt;p&gt;
&#20351;&#29992;&#39640;&#20809;&#35889;&#22270;&#20687;&#26816;&#27979;&#34562;&#34588;&#34562;&#19978;&#30340;&#21464;&#24418;&#34728;
&lt;/p&gt;
&lt;p&gt;
Varroa destructor detection on honey bees using hyperspectral imagery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14359
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20803;&#32479;&#35745;&#23398;&#30340;&#26041;&#27861;&#65292;&#21487;&#21033;&#29992;&#39640;&#20809;&#35889;&#22270;&#20687;&#26816;&#27979;&#34562;&#34588;&#34562;&#19978;&#30340;&#23492;&#29983;&#24615;&#21464;&#24418;&#34728;&#65292;&#20026;&#34588;&#34562;&#24034;&#31348;&#30340;&#30417;&#27979;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20892;&#19994;&#20013;&#30340;&#39640;&#20809;&#35889;&#65288;HS&#65289;&#22270;&#20687;&#36234;&#26469;&#36234;&#24120;&#35265;&#12290; &#36825;&#20123;&#22270;&#20687;&#20855;&#26377;&#26356;&#39640;&#30340;&#20809;&#35889;&#20998;&#36776;&#29575;&#20248;&#21183;&#12290; &#38656;&#35201;&#20808;&#36827;&#30340;&#20809;&#35889;&#22788;&#29702;&#25216;&#26415;&#26469;&#35299;&#38145;&#36825;&#20123;HS&#22270;&#20687;&#20013;&#30340;&#20449;&#24687;&#28508;&#21147;&#12290; &#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20803;&#32479;&#35745;&#23398;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#26816;&#27979;&#23492;&#29983;&#24615;&#21464;&#24418;&#34728;&#23545;&#35199;&#26041;&#34588;&#34562; Apis mellifera &#36523;&#20307;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#34588;&#34562;&#24034;&#31348;&#30340;&#26356;&#36731;&#26494;&#36830;&#32493;&#30417;&#27979;&#12290; &#35813;&#26041;&#27861;&#25506;&#35752;&#20102;&#29992;&#20110;&#23492;&#29983;&#29289;&#37492;&#23450;&#30340;&#26080;&#30417;&#30563;&#65288;K&#22343;&#20540;++&#65289;&#21644;&#26368;&#36817;&#24320;&#21457;&#30340;&#30417;&#30563;&#65288;&#26680;&#27969;- &#20559;&#26368;&#23567;&#20108;&#20056;&#27861;&#65292;KF-PLS&#65289; &#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#37492;&#20110;&#23450;&#21046;&#27874;&#27573;&#22810;&#20809;&#35889;&#30456;&#26426;&#30340;&#20986;&#29616;&#65292;&#26412;&#30740;&#31350;&#27010;&#36848;&#20102;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#26377;&#25928;&#30340;&#34588;&#34562;-&#34728;&#20998;&#31163;&#25152;&#38656;&#29305;&#23450;&#27874;&#38271;&#30340;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;&#22312;&#23450;&#21046;&#27874;&#27573;&#30456;&#26426;&#20013;&#23454;&#26045;&#12290;&#36890;&#36807;&#19968;&#20010;&#30495;&#23454;&#26696;&#20363;&#25968;&#25454;&#38598;&#36827;&#34892;&#35828;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14359v1 Announce Type: cross  Abstract: Hyperspectral (HS) imagery in agriculture is becoming increasingly common. These images have the advantage of higher spectral resolution. Advanced spectral processing techniques are required to unlock the information potential in these HS images. The present paper introduces a method rooted in multivariate statistics designed to detect parasitic Varroa destructor mites on the body of western honey bee Apis mellifera, enabling easier and continuous monitoring of the bee hives. The methodology explores unsupervised (K-means++) and recently developed supervised (Kernel Flows - Partial Least-Squares, KF-PLS) methods for parasitic identification. Additionally, in light of the emergence of custom-band multispectral cameras, the present research outlines a strategy for identifying the specific wavelengths necessary for effective bee-mite separation, suitable for implementation in a custom-band camera. Illustrated with a real-case dataset, our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#29983;&#25104;&#20013;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#20219;&#21153;&#35774;&#35745;&#21644;&#23454;&#39564;&#32771;&#23519;&#20102;&#20854;&#23545;&#19981;&#21516;&#22270;&#32467;&#26500;&#35268;&#21017;&#30340;&#29702;&#35299;&#12289;&#25429;&#33719;&#32467;&#26500;&#31867;&#22411;&#20998;&#24067;&#30340;&#33021;&#21147;&#20197;&#21450;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#36827;&#34892;&#22522;&#20110;&#23646;&#24615;&#30340;&#22270;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2403.14358</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#29983;&#25104;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring the Potential of Large Language Models in Graph Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#29983;&#25104;&#20013;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#20219;&#21153;&#35774;&#35745;&#21644;&#23454;&#39564;&#32771;&#23519;&#20102;&#20854;&#23545;&#19981;&#21516;&#22270;&#32467;&#26500;&#35268;&#21017;&#30340;&#29702;&#35299;&#12289;&#25429;&#33719;&#32467;&#26500;&#31867;&#22411;&#20998;&#24067;&#30340;&#33021;&#21147;&#20197;&#21450;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#36827;&#34892;&#22522;&#20110;&#23646;&#24615;&#30340;&#22270;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#25506;&#32034;LLMs&#29992;&#20110;&#22270;&#21028;&#21035;&#20219;&#21153;&#65292;&#22914;&#33410;&#28857;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#22270;&#29983;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;&#22312;&#25991;&#29486;&#20013;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#22270;&#29983;&#25104;&#35201;&#27714;LLM&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#22270;&#65292;&#36825;&#22312;&#33647;&#29289;&#21457;&#29616;&#31561;&#26377;&#20215;&#20540;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#20063;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLM4GraphGen&#26469;&#25506;&#32034;LLMs&#36827;&#34892;&#22270;&#29983;&#25104;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#31995;&#32479;&#24615;&#20219;&#21153;&#35774;&#35745;&#21644;&#22823;&#37327;&#23454;&#39564;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#20219;&#21153;&#65292;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#65292;&#20197;&#35299;&#20915;&#26377;&#20851;LLMs&#23545;&#19981;&#21516;&#22270;&#32467;&#26500;&#35268;&#21017;&#30340;&#29702;&#35299;&#12289;&#25429;&#33719;&#32467;&#26500;&#31867;&#22411;&#20998;&#24067;&#30340;&#33021;&#21147;&#20197;&#21450;&#21033;&#29992;&#22495;&#30693;&#35782;&#36827;&#34892;&#22522;&#20110;&#23646;&#24615;&#30340;&#22270;&#29983;&#25104;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;LLMs&#65292;&#29305;&#21035;&#26159;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14358v1 Announce Type: cross  Abstract: Large language models (LLMs) have achieved great success in many fields, and recent works have studied exploring LLMs for graph discriminative tasks such as node classification. However, the abilities of LLMs for graph generation remain unexplored in the literature. Graph generation requires the LLM to generate graphs with given properties, which has valuable real-world applications such as drug discovery, while tends to be more challenging. In this paper, we propose LLM4GraphGen to explore the ability of LLMs for graph generation with systematical task designs and extensive experiments. Specifically, we propose several tasks tailored with comprehensive experiments to address key questions regarding LLMs' understanding of different graph structure rules, their ability to capture structural type distributions, and their utilization of domain knowledge for property-based graph generation. Our evaluations demonstrate that LLMs, particular
&lt;/p&gt;</description></item><item><title>DomainLab&#26159;&#19968;&#20010;&#27169;&#22359;&#21270;Python&#21253;&#65292;&#20801;&#35768;&#29992;&#25143;&#35757;&#32451;&#25351;&#23450;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#20197;&#32452;&#21512;&#30340;&#24418;&#24335;&#24212;&#29992;&#19981;&#21516;&#30340;&#27491;&#21017;&#21270;&#25439;&#22833;&#39033;&#65292;&#26497;&#22823;&#22320;&#26041;&#20415;&#20102;&#23454;&#39564;&#30340;&#21487;&#37325;&#29616;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14356</link><description>&lt;p&gt;
DomainLab: &#19968;&#20010;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#27867;&#21270;&#30340;&#27169;&#22359;&#21270;Python&#21253;
&lt;/p&gt;
&lt;p&gt;
DomainLab: A modular Python package for domain generalization in deep learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14356
&lt;/p&gt;
&lt;p&gt;
DomainLab&#26159;&#19968;&#20010;&#27169;&#22359;&#21270;Python&#21253;&#65292;&#20801;&#35768;&#29992;&#25143;&#35757;&#32451;&#25351;&#23450;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#20197;&#32452;&#21512;&#30340;&#24418;&#24335;&#24212;&#29992;&#19981;&#21516;&#30340;&#27491;&#21017;&#21270;&#25439;&#22833;&#39033;&#65292;&#26497;&#22823;&#22320;&#26041;&#20415;&#20102;&#23454;&#39564;&#30340;&#21487;&#37325;&#29616;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#30475;&#19981;&#35265;&#30340;&#39046;&#22495;&#20013;&#23384;&#22312;&#30340;&#20998;&#24067;&#36716;&#31227;&#23548;&#33268;&#30340;&#27867;&#21270;&#24615;&#33021;&#19981;&#20339;&#32463;&#24120;&#38459;&#30861;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#20449;&#37096;&#32626;&#12290;&#35768;&#22810;&#39046;&#22495;&#27867;&#21270;&#25216;&#26415;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28155;&#21152;&#39046;&#22495;&#19981;&#21464;&#30340;&#27491;&#21017;&#21270;&#25439;&#22833;&#39033;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#27169;&#22359;&#21270;&#36719;&#20214;&#65292;&#20801;&#35768;&#29992;&#25143;&#20197;&#26368;&#23567;&#30340;&#21162;&#21147;&#23558;&#19981;&#21516;&#26041;&#27861;&#30340;&#20248;&#21183;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#23454;&#29616;&#21487;&#37325;&#29616;&#24615;&#12290;DomainLab&#26159;&#19968;&#20010;&#27169;&#22359;&#21270;&#30340;Python&#21253;&#65292;&#29992;&#20110;&#35757;&#32451;&#29992;&#25143;&#25351;&#23450;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#19988;&#21487;&#20197;&#32452;&#21512;&#19981;&#21516;&#30340;&#27491;&#21017;&#21270;&#25439;&#22833;&#39033;&#12290;&#23427;&#30340;&#35299;&#32806;&#35774;&#35745;&#20801;&#35768;&#23558;&#31070;&#32463;&#32593;&#32476;&#19982;&#27491;&#21017;&#21270;&#25439;&#22833;&#26500;&#24314;&#20998;&#24320;&#12290;&#31070;&#32463;&#32593;&#32476;&#30340;&#23618;&#27425;&#32452;&#21512;&#12289;&#19981;&#21516;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#21644;&#30456;&#20851;&#36229;&#21442;&#25968;&#37117;&#21487;&#20197;&#19982;&#20854;&#20182;&#23454;&#39564;&#35774;&#32622;&#19968;&#36215;&#22312;&#21333;&#20010;&#37197;&#32622;&#25991;&#20214;&#20013;&#25351;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14356v1 Announce Type: new  Abstract: Poor generalization performance caused by distribution shifts in unseen domains often hinders the trustworthy deployment of deep neural networks. Many domain generalization techniques address this problem by adding a domain invariant regularization loss terms during training. However, there is a lack of modular software that allows users to combine the advantages of different methods with minimal effort for reproducibility. DomainLab is a modular Python package for training user specified neural networks with composable regularization loss terms. Its decoupled design allows the separation of neural networks from regularization loss construction. Hierarchical combinations of neural networks, different domain generalization methods, and associated hyperparameters, can all be specified together with other experimental setup in a single configuration file. Hierarchical combinations of neural networks, different domain generalization methods,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33258;&#20027;&#31995;&#32479;&#20013;&#21152;&#36895;&#35270;&#39057;&#20998;&#26512;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36731;&#37327;&#32423;&#8220;&#23398;&#29983;&#8221;&#27169;&#22411;&#36827;&#34892;&#37096;&#32626;&#25512;&#29702;&#65292;&#21033;&#29992;&#26356;&#22823;&#30340;&#8220;&#25945;&#24072;&#8221;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#26631;&#35760;&#65292;&#23454;&#29616;&#23545;&#19981;&#26029;&#21464;&#21270;&#22330;&#26223;&#30340;&#25345;&#32493;&#33258;&#36866;&#24212;&#12290;</title><link>https://arxiv.org/abs/2403.14353</link><description>&lt;p&gt;
DaCapo&#65306;&#21152;&#24555;&#33258;&#20027;&#31995;&#32479;&#22312;&#35270;&#39057;&#20998;&#26512;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DaCapo: Accelerating Continuous Learning in Autonomous Systems for Video Analytics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14353
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33258;&#20027;&#31995;&#32479;&#20013;&#21152;&#36895;&#35270;&#39057;&#20998;&#26512;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36731;&#37327;&#32423;&#8220;&#23398;&#29983;&#8221;&#27169;&#22411;&#36827;&#34892;&#37096;&#32626;&#25512;&#29702;&#65292;&#21033;&#29992;&#26356;&#22823;&#30340;&#8220;&#25945;&#24072;&#8221;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#26631;&#35760;&#65292;&#23454;&#29616;&#23545;&#19981;&#26029;&#21464;&#21270;&#22330;&#26223;&#30340;&#25345;&#32493;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#35270;&#39057;&#20998;&#26512;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#12289;&#26080;&#20154;&#26426;&#65288;UAV&#65289;&#21644;&#23433;&#38450;&#26426;&#22120;&#20154;&#31561;&#33258;&#20027;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#30005;&#27744;&#21151;&#29575;&#65292;&#23454;&#38469;&#37096;&#32626;&#38754;&#20020;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25345;&#32493;&#23398;&#20064;&#21033;&#29992;&#22312;&#37096;&#32626;&#65288;&#25512;&#29702;&#65289;&#20013;&#30340;&#36731;&#37327;&#32423;&#8220;&#23398;&#29983;&#8221;&#27169;&#22411;&#65292;&#21033;&#29992;&#26356;&#22823;&#30340;&#8220;&#25945;&#24072;&#8221;&#27169;&#22411;&#23545;&#37319;&#26679;&#25968;&#25454;&#36827;&#34892;&#26631;&#35760;&#65288;&#26631;&#35760;&#65289;&#65292;&#24182;&#19981;&#26029;&#37325;&#26032;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#20197;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14353v1 Announce Type: cross  Abstract: Deep neural network (DNN) video analytics is crucial for autonomous systems such as self-driving vehicles, unmanned aerial vehicles (UAVs), and security robots. However, real-world deployment faces challenges due to their limited computational resources and battery power. To tackle these challenges, continuous learning exploits a lightweight "student" model at deployment (inference), leverages a larger "teacher" model for labeling sampled data (labeling), and continuously retrains the student model to adapt to changing scenarios (retraining). This paper highlights the limitations in state-of-the-art continuous learning systems: (1) they focus on computations for retraining, while overlooking the compute needs for inference and labeling, (2) they rely on power-hungry GPUs, unsuitable for battery-operated autonomous systems, and (3) they are located on a remote centralized server, intended for multi-tenant scenarios, again unsuitable for
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;GA^2E&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#29983;&#25104;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#22312;&#21333;&#20010;&#35757;&#32451;&#38454;&#27573;&#20013;&#23454;&#29616;&#20102;&#22270;&#29983;&#25104;&#12289;&#22270;&#21028;&#21035;&#21644;&#22270;&#39044;&#27979;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.14340</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#26041;&#27861;&#25506;&#32034;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#32479;&#19968;&#21270;
&lt;/p&gt;
&lt;p&gt;
Exploring Task Unification in Graph Representation Learning via Generative Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14340
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;GA^2E&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#29983;&#25104;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#22312;&#21333;&#20010;&#35757;&#32451;&#38454;&#27573;&#20013;&#23454;&#29616;&#20102;&#22270;&#29983;&#25104;&#12289;&#22270;&#21028;&#21035;&#21644;&#22270;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#26080;&#22788;&#19981;&#22312;&#65292;&#24182;&#28085;&#30422;&#20102;&#20174;&#33410;&#28857;&#32423;&#12289;&#36793;&#32423;&#21644;&#22270;&#32423;&#20219;&#21153;&#21040;&#36801;&#31227;&#23398;&#20064;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20026;&#27599;&#31181;&#31867;&#22411;&#30340;&#22270;&#25968;&#25454;&#35774;&#35745;&#29305;&#23450;&#20219;&#21153;&#36890;&#24120;&#20195;&#20215;&#39640;&#26114;&#19988;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#8220;&#39044;&#35757;&#32451;+&#24494;&#35843;&#8221;&#25110;&#8220;&#39044;&#35757;&#32451;+&#25552;&#31034;&#8221;&#33539;&#24335;&#65292;&#26088;&#22312;&#35774;&#35745;&#19968;&#20010;&#33021;&#22815;&#27867;&#21270;&#22810;&#31181;&#22270;&#20219;&#21153;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#22312;&#36825;&#20123;&#26041;&#27861;&#20013;&#65292;&#22270;&#33258;&#32534;&#30721;&#22120;&#65288;GAEs&#65289;&#12289;&#29983;&#25104;&#33258;&#30417;&#30563;&#27169;&#22411;&#24050;&#32463;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#26377;&#25928;&#35299;&#20915;&#21508;&#31181;&#22270;&#20219;&#21153;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#22810;&#38454;&#27573;&#35757;&#32451;&#24182;&#38656;&#35201;&#33258;&#36866;&#24212;&#35774;&#35745;&#65292;&#36825;&#19968;&#26041;&#38754;&#20351;&#24471;&#23558;&#20854;&#26080;&#32541;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#22270;&#20219;&#21153;&#21464;&#24471;&#22256;&#38590;&#65292;&#21478;&#19968;&#26041;&#38754;&#24573;&#30053;&#20102;&#19981;&#21516;&#38454;&#27573;&#20219;&#21153;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#24322;&#36896;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GA^2E&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#29983;&#25104;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#21487;&#20197;&#22312;&#21333;&#20010;&#35757;&#32451;&#38454;&#27573;&#20013;&#21516;&#26102;&#23454;&#29616;&#22270;&#29983;&#25104;&#12289;&#22270;&#21028;&#21035;&#21644;&#22270;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14340v1 Announce Type: cross  Abstract: Graphs are ubiquitous in real-world scenarios and encompass a diverse range of tasks, from node-, edge-, and graph-level tasks to transfer learning. However, designing specific tasks for each type of graph data is often costly and lacks generalizability. Recent endeavors under the "Pre-training + Fine-tuning" or "Pre-training + Prompt" paradigms aim to design a unified framework capable of generalizing across multiple graph tasks. Among these, graph autoencoders (GAEs), generative self-supervised models, have demonstrated their potential in effectively addressing various graph tasks. Nevertheless, these methods typically employ multi-stage training and require adaptive designs, which on one hand make it difficult to be seamlessly applied to diverse graph tasks and on the other hand overlook the negative impact caused by discrepancies in task objectives between the different stages. To address these challenges, we propose GA^2E, a unifi
&lt;/p&gt;</description></item><item><title>$\nabla \tau$ &#26159;&#19968;&#31181;&#26088;&#22312;&#39640;&#25928;&#28040;&#38500;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#24433;&#21709;&#30340;&#26426;&#22120;&#36951;&#24536;&#20248;&#21270;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.14339</link><description>&lt;p&gt;
$\nabla \tau$: &#22522;&#20110;&#26799;&#24230;&#19988;&#20219;&#21153;&#26080;&#20851;&#30340;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
$\nabla \tau$: Gradient-based and Task-Agnostic machine Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14339
&lt;/p&gt;
&lt;p&gt;
$\nabla \tau$ &#26159;&#19968;&#31181;&#26088;&#22312;&#39640;&#25928;&#28040;&#38500;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#24433;&#21709;&#30340;&#26426;&#22120;&#36951;&#24536;&#20248;&#21270;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#26159;&#19968;&#31181;&#26377;&#36873;&#25321;&#24615;&#22320;&#28040;&#38500;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#26576;&#20123;&#25968;&#25454;&#31034;&#20363;&#24433;&#21709;&#30340;&#36807;&#31243;&#65292;&#20316;&#20026;&#20174;&#19994;&#32773;&#36981;&#23432;&#26368;&#36817;&#30340;&#25968;&#25454;&#20445;&#25252;&#27861;&#35268;&#30340;&#25163;&#27573;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#26174;&#33879;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#36951;&#24536;&#26041;&#27861;&#38754;&#20020;&#30528;&#20851;&#38190;&#32570;&#28857;&#65292;&#21253;&#25324;&#20854;&#25104;&#26412;&#36807;&#39640;&#65292;&#36890;&#24120;&#19982;&#22823;&#37327;&#36229;&#21442;&#25968;&#30456;&#20851;&#65292;&#20197;&#21450;&#20165;&#24536;&#35760;&#30456;&#23545;&#36739;&#23567;&#25968;&#25454;&#37096;&#20998;&#30340;&#38480;&#21046;&#12290;&#36825;&#32463;&#24120;&#23548;&#33268;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#25104;&#20026;&#26356;&#24555;&#36895;&#21644;&#26356;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22522;&#20110;&#26799;&#24230;&#19988;&#20219;&#21153;&#26080;&#20851;&#30340;&#26426;&#22120;&#36951;&#24536;&#65288;$\nabla \tau$&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#39640;&#25928;&#28040;&#38500;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#24433;&#21709;&#30340;&#20248;&#21270;&#26694;&#26550;&#12290;&#23427;&#23545;&#24453;&#36951;&#24536;&#30340;&#25968;&#25454;&#24212;&#29992;&#33258;&#36866;&#24212;&#26799;&#24230;&#19978;&#21319;&#65292;&#21516;&#26102;&#23545;&#20854;&#20313;&#25968;&#25454;&#20351;&#29992;&#26631;&#20934;&#26799;&#24230;&#19979;&#38477;&#12290;$\nabla \tau$&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#25552;&#20379;&#20102;&#22810;&#31181;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14339v1 Announce Type: cross  Abstract: Machine Unlearning, the process of selectively eliminating the influence of certain data examples used during a model's training, has gained significant attention as a means for practitioners to comply with recent data protection regulations. However, existing unlearning methods face critical drawbacks, including their prohibitively high cost, often associated with a large number of hyperparameters, and the limitation of forgetting only relatively small data portions. This often makes retraining the model from scratch a quicker and more effective solution. In this study, we introduce Gradient-based and Task-Agnostic machine Unlearning ($\nabla \tau$), an optimization framework designed to remove the influence of a subset of training data efficiently. It applies adaptive gradient ascent to the data to be forgotten while using standard gradient descent for the remaining data. $\nabla \tau$ offers multiple benefits over existing approache
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#33391;&#22909;&#32858;&#31867;&#22270;&#30340;&#24046;&#20998;&#38544;&#31169;&#32858;&#31867;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;k&#20010;&#20960;&#20046;&#24179;&#34913;&#38598;&#32676;&#30340;&#22270;&#65292;&#35823;&#20998;&#31867;&#27604;&#29575;&#25509;&#36817;&#26368;&#20339;&#38750;&#31169;&#26377;&#31639;&#27861;&#30340;&#27700;&#24179;</title><link>https://arxiv.org/abs/2403.14332</link><description>&lt;p&gt;
&#38754;&#21521;&#33391;&#22909;&#32858;&#31867;&#22270;&#30340;&#24046;&#20998;&#38544;&#31169;&#32858;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Differentially Private Clustering Algorithm for Well-Clustered Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14332
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#33391;&#22909;&#32858;&#31867;&#22270;&#30340;&#24046;&#20998;&#38544;&#31169;&#32858;&#31867;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;k&#20010;&#20960;&#20046;&#24179;&#34913;&#38598;&#32676;&#30340;&#22270;&#65292;&#35823;&#20998;&#31867;&#27604;&#29575;&#25509;&#36817;&#26368;&#20339;&#38750;&#31169;&#26377;&#31639;&#27861;&#30340;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;&#20110;&#24674;&#22797;&#33391;&#22909;&#32858;&#31867;&#22270;&#20013;&#32858;&#31867;&#30340;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#31639;&#27861;&#65292;&#36825;&#20123;&#22270;&#30340;&#39030;&#28857;&#38598;&#21487;&#20197;&#20998;&#20026;&#23569;&#37327;&#30340;&#38598;&#21512;&#65292;&#27599;&#20010;&#38598;&#21512;&#35825;&#23548;&#20986;&#20855;&#26377;&#39640;&#20869;&#30005;&#23548;&#21644;&#23567;&#22806;&#30005;&#23548;&#30340;&#23376;&#22270;&#12290;&#36825;&#20123;&#22270;&#22312;&#35889;&#32858;&#31867;&#30340;&#29702;&#35770;&#20998;&#26512;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#20316;&#20026;&#22522;&#20934;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#36825;&#31181;&#22270;&#35774;&#35745;&#30340;&#39640;&#25928;&#65288;&#1013;&#65292;&#948;&#65289;-DP&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#27762;&#21462;&#20102;&#38472;&#31561;&#20154;&#26368;&#36817;&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#20182;&#20204;&#20026;&#22270;&#21253;&#21547;&#20004;&#20010;&#20960;&#20046;&#24179;&#34913;&#38598;&#32676;&#30340;&#24773;&#20917;&#24320;&#21457;&#20102;DP&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36866;&#29992;&#20110;&#20855;&#26377;k&#20010;&#20960;&#20046;&#24179;&#34913;&#38598;&#32676;&#30340;&#33391;&#22909;&#32858;&#31867;&#22270;&#65292;&#24182;&#19988;&#35823;&#20998;&#31867;&#27604;&#29575;&#20960;&#20046;&#19982;&#26368;&#20339;&#24050;&#30693;&#30340;&#38750;&#31169;&#26377;&#31639;&#27861;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#23545;&#24050;&#30693;&#22320;&#38754;&#30495;&#23454;&#32858;&#31867;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#20197;&#35777;&#23454;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14332v1 Announce Type: cross  Abstract: We study differentially private (DP) algorithms for recovering clusters in well-clustered graphs, which are graphs whose vertex set can be partitioned into a small number of sets, each inducing a subgraph of high inner conductance and small outer conductance. Such graphs have widespread application as a benchmark in the theoretical analysis of spectral clustering. We provide an efficient ($\epsilon$,$\delta$)-DP algorithm tailored specifically for such graphs. Our algorithm draws inspiration from the recent work of Chen et al., who developed DP algorithms for recovery of stochastic block models in cases where the graph comprises exactly two nearly-balanced clusters. Our algorithm works for well-clustered graphs with $k$ nearly-balanced clusters, and the misclassification ratio almost matches the one of the best-known non-private algorithms. We conduct experimental evaluations on datasets with known ground truth clusters to substantiate
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26799;&#24230;&#25552;&#21319;&#26426;&#21644;&#31526;&#21495;&#22238;&#24402;&#31561;&#25216;&#26415;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#36716;&#21270;&#20026;&#26356;&#21487;&#35299;&#37322;&#30340;&#24418;&#24335;&#65292;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#36816;&#21160;&#31574;&#30053;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#29702;&#35299;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14328</link><description>&lt;p&gt;
&#23558;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#25552;&#28860;&#20026;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#65306;&#26799;&#24230;&#25552;&#21319;&#26426;&#21644;&#31526;&#21495;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Distilling Reinforcement Learning Policies for Interpretable Robot Locomotion: Gradient Boosting Machines and Symbolic Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14328
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26799;&#24230;&#25552;&#21319;&#26426;&#21644;&#31526;&#21495;&#22238;&#24402;&#31561;&#25216;&#26415;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#36716;&#21270;&#20026;&#26356;&#21487;&#35299;&#37322;&#30340;&#24418;&#24335;&#65292;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#36816;&#21160;&#31574;&#30053;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#29702;&#35299;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#21457;&#23637;&#20351;&#26426;&#22120;&#20154;&#36816;&#21160;&#33021;&#21147;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;RL&#31574;&#30053;&#30340;&#22797;&#26434;&#24615;&#21644;&#8220;&#40657;&#21283;&#23376;&#8221;&#29305;&#24615;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#26356;&#24191;&#27867;&#30340;&#25509;&#21463;&#24230;&#65292;&#29305;&#21035;&#26159;&#22312;&#35201;&#27714;&#39640;&#27700;&#24179;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#24212;&#29992;&#20013;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#23558;&#31070;&#32463;RL&#31574;&#30053;&#25552;&#28860;&#20026;&#26356;&#21487;&#35299;&#37322;&#24418;&#24335;&#30340;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#26799;&#24230;&#25552;&#21319;&#26426;&#65288;GBMs&#65289;&#12289;&#21487;&#35299;&#37322;&#25552;&#21319;&#26426;&#65288;EBMs&#65289;&#21644;&#31526;&#21495;&#22238;&#24402;&#12290;&#36890;&#36807;&#21033;&#29992;&#24191;&#20041;&#21152;&#27861;&#27169;&#22411;&#12289;&#20915;&#31574;&#26641;&#21644;&#20998;&#26512;&#34920;&#36798;&#24335;&#30340;&#22266;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#23558;&#19981;&#36879;&#26126;&#30340;&#31070;&#32463;&#32593;&#32476;&#31574;&#30053;&#36716;&#21270;&#20026;&#26356;&#36879;&#26126;&#30340;&#8220;&#29627;&#29827;&#31665;&#8221;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;RL&#35757;&#32451;&#19987;&#23478;&#31070;&#32463;&#32593;&#32476;&#31574;&#30053;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#25552;&#28860;&#20026;(i) GBMs&#12289;(ii) EBMs&#21644;(iii)&#31526;&#21495;&#31574;&#30053;&#12290;&#20026;&#20102;&#35299;&#20915;&#34892;&#20026;&#20998;&#24067;&#36716;&#31227;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14328v1 Announce Type: cross  Abstract: Recent advancements in reinforcement learning (RL) have led to remarkable achievements in robot locomotion capabilities. However, the complexity and ``black-box'' nature of neural network-based RL policies hinder their interpretability and broader acceptance, particularly in applications demanding high levels of safety and reliability. This paper introduces a novel approach to distill neural RL policies into more interpretable forms using Gradient Boosting Machines (GBMs), Explainable Boosting Machines (EBMs) and Symbolic Regression. By leveraging the inherent interpretability of generalized additive models, decision trees, and analytical expressions, we transform opaque neural network policies into more transparent ``glass-box'' models. We train expert neural network policies using RL and subsequently distill them into (i) GBMs, (ii) EBMs, and (iii) symbolic policies. To address the inherent distribution shift challenge of behavioral 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#22312;&#35782;&#21035;&#24433;&#21709;&#31958;&#23615;&#30149;&#36827;&#23637;&#30340;&#39118;&#38505;&#22240;&#32032;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24378;&#35843;&#31639;&#27861;&#36873;&#25321;&#23545;&#24178;&#39044;&#32467;&#26524;&#30340;&#37325;&#22823;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.14327</link><description>&lt;p&gt;
&#30740;&#31350;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#22312;&#35782;&#21035;&#31958;&#23615;&#30149;&#24739;&#32773;&#24178;&#39044;&#39118;&#38505;&#22240;&#32032;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Investigating the validity of structure learning algorithms in identifying risk factors for intervention in patients with diabetes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#22312;&#35782;&#21035;&#24433;&#21709;&#31958;&#23615;&#30149;&#36827;&#23637;&#30340;&#39118;&#38505;&#22240;&#32032;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24378;&#35843;&#31639;&#27861;&#36873;&#25321;&#23545;&#24178;&#39044;&#32467;&#26524;&#30340;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31958;&#23615;&#30149;&#20316;&#20026;&#19968;&#31181;&#26222;&#36941;&#32780;&#38271;&#26399;&#23384;&#22312;&#30340;&#20581;&#24247;&#25361;&#25112;&#65292;&#23545;&#20840;&#29699;&#20581;&#24247;&#12289;&#37329;&#34701;&#21307;&#30103;&#31995;&#32479;&#21644;&#31038;&#20250;&#31119;&#31049;&#37117;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#20840;&#38754;&#25506;&#35752;&#20102;&#21508;&#31181;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#36776;&#35748;&#24433;&#21709;&#31958;&#23615;&#30149;&#36827;&#23637;&#30340;&#28508;&#22312;&#39118;&#38505;&#22240;&#32032;&#20043;&#38388;&#22240;&#26524;&#36335;&#24452;&#12290;&#26041;&#27861;&#28041;&#21450;&#23558;&#36825;&#20123;&#31639;&#27861;&#24212;&#29992;&#20110;&#30456;&#20851;&#31958;&#23615;&#30149;&#25968;&#25454;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#30340;&#36755;&#20986;&#22270;&#36716;&#25442;&#20026;&#22240;&#26524;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;CBNs&#65289;&#65292;&#23454;&#29616;&#39044;&#27979;&#20998;&#26512;&#24182;&#35780;&#20272;&#25105;&#20204;&#29305;&#23450;&#26696;&#20363;&#30740;&#31350;&#20869;&#20551;&#24819;&#24178;&#39044;&#25928;&#26524;&#30340;&#24046;&#24322;&#12290;&#26412;&#30740;&#31350;&#31361;&#26174;&#20102;&#31639;&#27861;&#36873;&#25321;&#23545;&#24178;&#39044;&#32467;&#26524;&#30340;&#37325;&#22823;&#24433;&#21709;&#12290;&#20026;&#20102;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;&#31639;&#27861;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#27169;&#22411;&#24179;&#22343;&#25216;&#26415;&#65292;&#24110;&#21161;&#25105;&#20204;&#20174;&#21508;&#31181;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#20013;&#33719;&#24471;&#38024;&#23545;&#31958;&#23615;&#30149;&#30340;&#29420;&#29305;&#22240;&#26524;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14327v1 Announce Type: new  Abstract: Diabetes, a pervasive and enduring health challenge, imposes significant global implications on health, financial healthcare systems, and societal well-being. This study undertakes a comprehensive exploration of various structural learning algorithms to discern causal pathways amongst potential risk factors influencing diabetes progression. The methodology involves the application of these algorithms to relevant diabetes data, followed by the conversion of their output graphs into Causal Bayesian Networks (CBNs), enabling predictive analysis and the evaluation of discrepancies in the effect of hypothetical interventions within our context-specific case study.   This study highlights the substantial impact of algorithm selection on intervention outcomes. To consolidate insights from diverse algorithms, we employ a model-averaging technique that helps us obtain a unique causal model for diabetes derived from a varied set of structural lear
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34917;&#20607;&#29983;&#29289;&#20809;&#23376;&#22270;&#20687;&#25968;&#25454;&#21463;&#25439;&#65292;&#25552;&#21319;&#29983;&#29289;&#25104;&#20687;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#21644;&#38477;&#20302;&#25104;&#26412;/&#23610;&#23544;&#12290;</title><link>https://arxiv.org/abs/2403.14324</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22788;&#29702;&#21644;&#37325;&#24314;&#21463;&#25439;&#29983;&#29289;&#20809;&#23376;&#22270;&#20687;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Neural Network-Based Processing and Reconstruction of Compromised Biophotonic Image Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14324
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34917;&#20607;&#29983;&#29289;&#20809;&#23376;&#22270;&#20687;&#25968;&#25454;&#21463;&#25439;&#65292;&#25552;&#21319;&#29983;&#29289;&#25104;&#20687;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#21644;&#38477;&#20302;&#25104;&#26412;/&#23610;&#23544;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#19982;&#29983;&#29289;&#20809;&#23376;&#35774;&#32622;&#30340;&#25972;&#21512;&#22312;&#29983;&#29289;&#25104;&#20687;&#39046;&#22495;&#24320;&#36767;&#20102;&#26032;&#30340;&#35270;&#37326;&#12290;&#35813;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#36235;&#21183;&#26159;&#26377;&#24847;&#22320;&#30772;&#22351;&#26576;&#20123;&#27979;&#37327;&#25351;&#26631;&#65292;&#20197;&#35774;&#35745;&#26356;&#22909;&#30340;&#29983;&#29289;&#25104;&#20687;&#24037;&#20855;&#65292;&#20174;&#25104;&#26412;&#12289;&#36895;&#24230;&#21644;&#24418;&#24577;&#22240;&#32032;&#19978;&#36827;&#34892;&#34917;&#20607;&#25152;&#20135;&#29983;&#30340;&#32570;&#38519;&#65292;&#38543;&#21518;&#36890;&#36807;&#21033;&#29992;&#22312;&#22823;&#37327;&#29702;&#24819;&#12289;&#20248;&#36234;&#25110;&#26367;&#20195;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#34917;&#20607;&#36825;&#20123;&#32570;&#38519;&#12290;&#36825;&#31181;&#25112;&#30053;&#24615;&#26041;&#27861;&#22240;&#20854;&#25552;&#21319;&#29983;&#29289;&#20809;&#23376;&#25104;&#20687;&#21508;&#20010;&#26041;&#38754;&#30340;&#28508;&#21147;&#32780;&#26085;&#30410;&#21463;&#21040;&#38738;&#30544;&#12290;&#37319;&#29992;&#36825;&#31181;&#31574;&#30053;&#30340;&#19968;&#20010;&#20027;&#35201;&#21160;&#26426;&#26159;&#36861;&#27714;&#26356;&#39640;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#25110;&#22686;&#21152;&#25104;&#20687;&#36895;&#24230;&#65292;&#36825;&#23545;&#25429;&#25417;&#31934;&#32454;&#30340;&#21160;&#24577;&#29983;&#29289;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31181;&#26041;&#27861;&#36824;&#25552;&#20379;&#20102;&#31616;&#21270;&#30828;&#20214;&#35201;&#27714;/&#22797;&#26434;&#24615;&#30340;&#21487;&#33021;&#24615;&#65292;&#20174;&#32780;&#20197;&#25104;&#26412;&#21644;/&#25110;&#23610;&#23544;&#19978;&#26356;&#26131;&#25509;&#21463;&#30340;&#26041;&#24335;&#20351;&#24471;&#20808;&#36827;&#30340;&#25104;&#20687;&#26631;&#20934;&#26356;&#20855;&#21487;&#21450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14324v1 Announce Type: cross  Abstract: The integration of deep learning techniques with biophotonic setups has opened new horizons in bioimaging. A compelling trend in this field involves deliberately compromising certain measurement metrics to engineer better bioimaging tools in terms of cost, speed, and form-factor, followed by compensating for the resulting defects through the utilization of deep learning models trained on a large amount of ideal, superior or alternative data. This strategic approach has found increasing popularity due to its potential to enhance various aspects of biophotonic imaging. One of the primary motivations for employing this strategy is the pursuit of higher temporal resolution or increased imaging speed, critical for capturing fine dynamic biological processes. This approach also offers the prospect of simplifying hardware requirements/complexities, thereby making advanced imaging standards more accessible in terms of cost and/or size. This ar
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#33033;&#20914;&#33258;&#27880;&#24847;&#26426;&#21046;DSSA&#20197;&#21450;&#32467;&#21512;ResNet&#30340;&#22810;&#38454;&#27573;&#26550;&#26500;&#30340;SpikingResformer&#26550;&#26500;&#65292;&#26088;&#22312;&#25913;&#21892;&#24615;&#33021;&#21644;&#33021;&#25928;&#65292;&#24182;&#20943;&#23569;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.14302</link><description>&lt;p&gt;
SpikingResformer: &#23558;ResNet&#21644;Vision Transformer&#22312;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#36827;&#34892;&#26725;&#25509;
&lt;/p&gt;
&lt;p&gt;
SpikingResformer: Bridging ResNet and Vision Transformer in Spiking Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14302
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#33033;&#20914;&#33258;&#27880;&#24847;&#26426;&#21046;DSSA&#20197;&#21450;&#32467;&#21512;ResNet&#30340;&#22810;&#38454;&#27573;&#26550;&#26500;&#30340;SpikingResformer&#26550;&#26500;&#65292;&#26088;&#22312;&#25913;&#21892;&#24615;&#33021;&#21644;&#33021;&#25928;&#65292;&#24182;&#20943;&#23569;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Vision Transformer&#22312;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#36825;&#23548;&#33268;&#20102;&#22312;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;(SNNs)&#20013;&#32467;&#21512;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#22522;&#20110;Transformer&#30340;&#32467;&#26500;&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Dual Spike Self-Attention (DSSA)&#30340;&#26032;&#22411;&#33033;&#20914;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#24102;&#26377;&#21512;&#29702;&#30340;&#25193;&#23637;&#26041;&#27861;&#12290;&#22522;&#20110;DSSA&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SpikingResformer&#30340;&#26032;&#22411;&#33033;&#20914;Vision Transformer&#26550;&#26500;&#65292;&#23558;&#22522;&#20110;ResNet&#30340;&#22810;&#38454;&#27573;&#26550;&#26500;&#19982;&#25105;&#20204;&#25552;&#20986;&#30340;DSSA&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#33021;&#25928;&#65292;&#21516;&#26102;&#20943;&#23569;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14302v1 Announce Type: cross  Abstract: The remarkable success of Vision Transformers in Artificial Neural Networks (ANNs) has led to a growing interest in incorporating the self-attention mechanism and transformer-based architecture into Spiking Neural Networks (SNNs). While existing methods propose spiking self-attention mechanisms that are compatible with SNNs, they lack reasonable scaling methods, and the overall architectures proposed by these methods suffer from a bottleneck in effectively extracting local features. To address these challenges, we propose a novel spiking self-attention mechanism named Dual Spike Self-Attention (DSSA) with a reasonable scaling method. Based on DSSA, we propose a novel spiking Vision Transformer architecture called SpikingResformer, which combines the ResNet-based multi-stage architecture with our proposed DSSA to improve both performance and energy efficiency while reducing parameters. Experimental results show that SpikingResformer ach
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#22320;&#29699;&#35266;&#27979;&#24212;&#29992;&#20013;&#32570;&#22833;&#25968;&#25454;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#38598;&#25104;&#31574;&#30053;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;100%&#30340;&#39044;&#27979;&#31283;&#20581;&#24615;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#32570;&#22833;&#24773;&#26223;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#27604;&#20998;&#31867;&#20219;&#21153;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#19988;&#20809;&#23398;&#35270;&#35282;&#26159;&#26368;&#20851;&#38190;&#30340;&#12290;</title><link>https://arxiv.org/abs/2403.14297</link><description>&lt;p&gt;
&#22320;&#29699;&#35266;&#27979;&#24212;&#29992;&#20013;&#32570;&#22833;&#25968;&#25454;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#24433;&#21709;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Impact Assessment of Missing Data in Model Predictions for Earth Observation Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#22320;&#29699;&#35266;&#27979;&#24212;&#29992;&#20013;&#32570;&#22833;&#25968;&#25454;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#38598;&#25104;&#31574;&#30053;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;100%&#30340;&#39044;&#27979;&#31283;&#20581;&#24615;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#32570;&#22833;&#24773;&#26223;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#27604;&#20998;&#31867;&#20219;&#21153;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#19988;&#20809;&#23398;&#35270;&#35282;&#26159;&#26368;&#20851;&#38190;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29699;&#35266;&#27979;&#65288;EO&#65289;&#24212;&#29992;&#28041;&#21450;&#22797;&#26434;&#21644;&#24322;&#26500;&#25968;&#25454;&#28304;&#65292;&#36890;&#24120;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#26222;&#36941;&#20551;&#35774;&#25968;&#25454;&#28304;&#23558;&#25345;&#32493;&#21487;&#29992;&#12290;&#19981;&#21516;&#24773;&#20917;&#21487;&#33021;&#24433;&#21709;EO&#25968;&#25454;&#28304;&#30340;&#21487;&#29992;&#24615;&#65292;&#22914;&#22122;&#22768;&#12289;&#20113;&#23618;&#25110;&#21355;&#26143;&#20219;&#21153;&#22833;&#36133;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#20013;&#32570;&#22833;&#26102;&#38388;&#24615;&#21644;&#38745;&#24577;EO&#25968;&#25454;&#28304;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#30340;&#39044;&#27979;&#36136;&#37327;&#65292;&#24182;&#21457;&#29616;&#19968;&#20123;&#26041;&#27861;&#22312;&#38754;&#23545;&#32570;&#22833;&#25968;&#25454;&#26102;&#33258;&#28982;&#26356;&#21152;&#31283;&#20581;&#12290;&#29305;&#21035;&#26159;&#38598;&#25104;&#31574;&#30053;&#23454;&#29616;&#20102;&#39640;&#36798;100%&#30340;&#39044;&#27979;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#32570;&#22833;&#24773;&#26223;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#27604;&#20998;&#31867;&#20219;&#21153;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#20809;&#23398;&#35270;&#35282;&#22312;&#21333;&#29420;&#32570;&#22833;&#26102;&#26159;&#26368;&#20851;&#38190;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14297v1 Announce Type: cross  Abstract: Earth observation (EO) applications involving complex and heterogeneous data sources are commonly approached with machine learning models. However, there is a common assumption that data sources will be persistently available. Different situations could affect the availability of EO sources, like noise, clouds, or satellite mission failures. In this work, we assess the impact of missing temporal and static EO sources in trained models across four datasets with classification and regression tasks. We compare the predictive quality of different methods and find that some are naturally more robust to missing data. The Ensemble strategy, in particular, achieves a prediction robustness up to 100%. We evidence that missing scenarios are significantly more challenging in regression than classification tasks. Finally, we find that the optical view is the most critical view when it is missing individually.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#21033;&#29992;&#26631;&#20934;CPU&#36164;&#28304;&#35757;&#32451;&#30340;&#38899;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#26032;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#20351;&#29992;&#39640;&#24615;&#33021;&#35745;&#31639;&#21644;&#38271;&#35757;&#32451;&#26102;&#38388;&#24102;&#26469;&#30340;&#30899;&#25490;&#25918;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.14290</link><description>&lt;p&gt;
&#25506;&#32034;&#29992;&#20110;&#38899;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#30340;&#32511;&#33394;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Exploring Green AI for Audio Deepfake Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14290
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#21033;&#29992;&#26631;&#20934;CPU&#36164;&#28304;&#35757;&#32451;&#30340;&#38899;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#26032;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#20351;&#29992;&#39640;&#24615;&#33021;&#35745;&#31639;&#21644;&#38271;&#35757;&#32451;&#26102;&#38388;&#24102;&#26469;&#30340;&#30899;&#25490;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22522;&#30784;&#30340;&#26368;&#20808;&#36827;&#38899;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#20248;&#21183;&#20276;&#38543;&#30528;&#21487;&#35266;&#30340;&#30899;&#36275;&#36857;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#20351;&#29992;&#24102;&#26377;&#21152;&#36895;&#22120;&#21644;&#38271;&#35757;&#32451;&#26102;&#38388;&#30340;&#39640;&#24615;&#33021;&#35745;&#31639;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#24179;&#22343;&#28145;&#24230;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20135;&#29983;&#32422;626&#21315;&#30917;&#30340;CO\textsubscript{2}&#65292;&#30456;&#24403;&#20110;&#20854;&#29983;&#21629;&#21608;&#26399;&#20869;&#24179;&#22343;&#32654;&#22269;&#27773;&#36710;&#25490;&#25918;&#37327;&#30340;&#20116;&#20493;&#12290;&#36825;&#26174;&#28982;&#26159;&#23545;&#29615;&#22659;&#30340;&#24040;&#22823;&#23041;&#32961;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#21033;&#29992;&#26631;&#20934;CPU&#36164;&#28304;&#26080;&#32541;&#35757;&#32451;&#30340;&#38899;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#24182;&#22312;&#20844;&#20849;&#20195;&#30721;&#24211;&#20013;&#21487;&#29992;&#30340;&#29616;&#25104;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#27169;&#22411;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#19981;&#20165;&#35843;&#25972;SSL&#27169;&#22411;&#65292;&#36824;&#37319;&#29992;&#39069;&#22806;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25191;&#34892;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14290v1 Announce Type: cross  Abstract: The state-of-the-art audio deepfake detectors leveraging deep neural networks exhibit impressive recognition performance. Nonetheless, this advantage is accompanied by a significant carbon footprint. This is mainly due to the use of high-performance computing with accelerators and high training time. Studies show that average deep NLP model produces around 626k lbs of CO\textsubscript{2} which is equivalent to five times of average US car emission at its lifetime. This is certainly a massive threat to the environment. To tackle this challenge, this study presents a novel framework for audio deepfake detection that can be seamlessly trained using standard CPU resources. Our proposed framework utilizes off-the-shelve self-supervised learning (SSL) based models which are pre-trained and available in public repositories. In contrast to existing methods that fine-tune SSL models and employ additional deep neural networks for downstream task
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#22312;&#19981;&#21516;&#39046;&#22495;&#25968;&#25454;&#38598;&#19979;&#20809;&#35889;&#32858;&#31867;&#23545;&#35828;&#35805;&#20154;&#20998;&#31163;&#31283;&#20581;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#35828;&#35805;&#20154;&#20998;&#31163;&#24615;&#33021;&#24046;&#24322;&#28304;&#20110;&#20809;&#35889;&#32858;&#31867;&#30340;&#20316;&#29992;&#21644;&#21442;&#25968;&#19981;&#21305;&#37197;&#12290;</title><link>https://arxiv.org/abs/2403.14286</link><description>&lt;p&gt;
&#35780;&#20272;&#28145;&#24230;&#35828;&#35805;&#20154;&#20998;&#31163;&#30340;&#20809;&#35889;&#32858;&#31867;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
Assessing the Robustness of Spectral Clustering for Deep Speaker Diarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14286
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#22312;&#19981;&#21516;&#39046;&#22495;&#25968;&#25454;&#38598;&#19979;&#20809;&#35889;&#32858;&#31867;&#23545;&#35828;&#35805;&#20154;&#20998;&#31163;&#31283;&#20581;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#35828;&#35805;&#20154;&#20998;&#31163;&#24615;&#33021;&#24046;&#24322;&#28304;&#20110;&#20809;&#35889;&#32858;&#31867;&#30340;&#20316;&#29992;&#21644;&#21442;&#25968;&#19981;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35828;&#35805;&#20154;&#23884;&#20837;&#30340;&#32858;&#31867;&#23545;&#35828;&#35805;&#20154;&#20998;&#31163;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#19982;&#20854;&#20182;&#32452;&#20214;&#30456;&#27604;&#65292;&#23578;&#26410;&#24471;&#21040;&#36275;&#22815;&#20851;&#27880;&#12290;&#27492;&#22806;&#65292;&#22312;&#24320;&#21457;&#21644;&#35780;&#20272;&#25968;&#25454;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#26102;&#65292;&#36328;&#21508;&#31181;&#25968;&#25454;&#38598;&#35780;&#20272;&#35828;&#35805;&#20154;&#20998;&#31163;&#30340;&#31283;&#20581;&#24615;&#20063;&#23578;&#26410;&#34987;&#25506;&#35752;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#24443;&#24213;&#30740;&#31350;&#20102;&#20809;&#35889;&#32858;&#31867;&#22312;&#30456;&#21516;&#39046;&#22495;&#21644;&#36328;&#39046;&#22495;&#35828;&#35805;&#20154;&#20998;&#31163;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#35821;&#26009;&#24211;AMI&#21644;DIHARD&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#25581;&#31034;&#20102;&#39046;&#22495;&#19981;&#21305;&#37197;&#24773;&#20917;&#19979;&#35828;&#35805;&#20154;&#20998;&#31163;&#24615;&#33021;&#36235;&#21183;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#20004;&#31181;&#19981;&#21516;&#39046;&#22495;&#26465;&#20214;&#19979;&#30340;&#24615;&#33021;&#24046;&#24322;&#21487;&#20197;&#24402;&#22240;&#20110;&#20809;&#35889;&#32858;&#31867;&#30340;&#20316;&#29992;&#12290;&#29305;&#21035;&#26159;&#22312;&#20854;&#20182;&#27169;&#22359;&#20445;&#25345;&#19981;&#21464;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#20248;&#35843;&#21442;&#21442;&#25968;&#20197;&#21450;&#35828;&#35805;&#20154;&#35745;&#25968;&#20272;&#35745;&#30340;&#24046;&#24322;&#36215;&#28304;&#20110;&#19981;&#21305;&#37197;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#35828;&#35805;&#20154;&#20998;&#31163;&#30740;&#31350;&#24320;&#36767;&#20102;&#20960;&#20010;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14286v1 Announce Type: cross  Abstract: Clustering speaker embeddings is crucial in speaker diarization but hasn't received as much focus as other components. Moreover, the robustness of speaker diarization across various datasets hasn't been explored when the development and evaluation data are from different domains. To bridge this gap, this study thoroughly examines spectral clustering for both same-domain and cross-domain speaker diarization. Our extensive experiments on two widely used corpora, AMI and DIHARD, reveal the performance trend of speaker diarization in the presence of domain mismatch. We observe that the performance difference between two different domain conditions can be attributed to the role of spectral clustering. In particular, keeping other modules unchanged, we show that differences in optimal tuning parameters as well as speaker count estimation originates due to the mismatch. This study opens several future directions for speaker diarization resear
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#25968;&#25454;&#20559;&#35265;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#20844;&#24179;&#24615;&#65292;&#25552;&#20986;&#20102;&#24314;&#31435;&#20559;&#35265;&#31867;&#22411;&#19982;&#32531;&#35299;&#25216;&#26415;&#26377;&#25928;&#24615;&#20043;&#38388;&#20851;&#31995;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.14282</link><description>&lt;p&gt;
&#22914;&#20309;&#20570;&#21040;&#20844;&#24179;&#65311;&#26631;&#31614;&#21644;&#36873;&#25321;&#20559;&#24046;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How to be fair? A study of label and selection bias
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14282
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#25968;&#25454;&#20559;&#35265;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#20844;&#24179;&#24615;&#65292;&#25552;&#20986;&#20102;&#24314;&#31435;&#20559;&#35265;&#31867;&#22411;&#19982;&#32531;&#35299;&#25216;&#26415;&#26377;&#25928;&#24615;&#20043;&#38388;&#20851;&#31995;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#20559;&#35265;&#25968;&#25454;&#20250;&#23548;&#33268;&#20559;&#35265;&#12289;&#28508;&#22312;&#19981;&#20844;&#24179;&#30340;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#29992;&#20110;&#25968;&#25454;&#21644;&#27169;&#22411;&#39044;&#27979;&#20559;&#35265;&#30340;&#25514;&#26045;&#65292;&#20197;&#21450;&#20854;&#30446;&#26631;&#26159;&#36890;&#36807;&#35774;&#35745;&#20844;&#24179;&#30340;&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#26469;&#23398;&#20064;&#27169;&#22411;&#12290;&#36817;&#21313;&#24180;&#26469;&#24050;&#32463;&#21457;&#23637;&#20102;&#22823;&#37327;&#30340;&#32531;&#35299;&#25216;&#26415;&#65292;&#28982;&#32780;&#65292;&#22312;&#20160;&#20040;&#24773;&#20917;&#19979;&#21738;&#20123;&#26041;&#27861;&#36215;&#20316;&#29992;&#20173;&#28982;&#30693;&#20043;&#29978;&#23569;&#12290;&#26368;&#36817;&#65292;Wick&#31561;&#20154;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23384;&#22312;&#19968;&#20123;&#24773;&#20917;&#65292;&#20854;&#20013;&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#23548;&#33268;&#22312;&#26080;&#20559;&#25968;&#25454;&#19978;&#27979;&#37327;&#26102;&#26356;&#31934;&#30830;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#32570;&#20047;&#24443;&#24213;&#30340;&#25968;&#23398;&#20998;&#26512;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#19981;&#28165;&#26970;&#22312;&#20309;&#31181;&#24773;&#20917;&#19979;&#21738;&#20123;&#25216;&#26415;&#26159;&#26377;&#25928;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#24314;&#31435;&#20559;&#35265;&#31867;&#22411;&#19982;&#32531;&#35299;&#25216;&#26415;&#26377;&#25928;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#32531;&#35299;&#25216;&#26415;&#25353;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14282v1 Announce Type: cross  Abstract: It is widely accepted that biased data leads to biased and thus potentially unfair models. Therefore, several measures for bias in data and model predictions have been proposed, as well as bias mitigation techniques whose aim is to learn models that are fair by design. Despite the myriad of mitigation techniques developed in the past decade, however, it is still poorly understood under what circumstances which methods work. Recently, Wick et al. showed, with experiments on synthetic data, that there exist situations in which bias mitigation techniques lead to more accurate models when measured on unbiased data. Nevertheless, in the absence of a thorough mathematical analysis, it remains unclear which techniques are effective under what circumstances. We propose to address this problem by establishing relationships between the type of bias and the effectiveness of a mitigation technique, where we categorize the mitigation techniques by 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#26080;&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#24320;&#25918;&#35789;&#27719;&#30340;&#35270;&#35273;&#20851;&#31995;&#26816;&#27979;&#65292;&#36890;&#36807;Transformer-based&#22270;&#20687;&#32534;&#30721;&#22120;&#38544;&#24335;&#24314;&#27169;&#23545;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#21462;&#20851;&#31995;&#20449;&#24687;&#65292;&#22312;&#28151;&#21512;&#25968;&#25454;&#19978;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20851;&#31995;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.14270</link><description>&lt;p&gt;
&#22330;&#26223;&#22270;ViT&#65306;&#31471;&#21040;&#31471;&#30340;&#24320;&#25918;&#35789;&#27719;&#35270;&#35273;&#20851;&#31995;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Scene-Graph ViT: End-to-End Open-Vocabulary Visual Relationship Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14270
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#26080;&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#24320;&#25918;&#35789;&#27719;&#30340;&#35270;&#35273;&#20851;&#31995;&#26816;&#27979;&#65292;&#36890;&#36807;Transformer-based&#22270;&#20687;&#32534;&#30721;&#22120;&#38544;&#24335;&#24314;&#27169;&#23545;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#21462;&#20851;&#31995;&#20449;&#24687;&#65292;&#22312;&#28151;&#21512;&#25968;&#25454;&#19978;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20851;&#31995;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#20851;&#31995;&#26816;&#27979;&#26088;&#22312;&#35782;&#21035;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#21450;&#20854;&#20851;&#31995;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#29616;&#26377;&#30446;&#26631;&#26816;&#27979;&#26550;&#26500;&#20013;&#28155;&#21152;&#21333;&#29420;&#30340;&#20851;&#31995;&#27169;&#22359;&#25110;&#35299;&#30721;&#22120;&#26469;&#22788;&#29702;&#27492;&#20219;&#21153;&#12290;&#36825;&#31181;&#20998;&#31163;&#22686;&#21152;&#20102;&#22797;&#26434;&#24615;&#65292;&#38459;&#30861;&#20102;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#38480;&#21046;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#39640;&#25928;&#30340;&#26080;&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#24320;&#25918;&#35789;&#27719;&#30340;&#35270;&#35273;&#20851;&#31995;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#30001;&#22522;&#20110;Transformer&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#32452;&#25104;&#65292;&#23558;&#23545;&#35937;&#34920;&#31034;&#20026;&#26631;&#35760;&#65292;&#24182;&#38544;&#21547;&#22320;&#24314;&#27169;&#23427;&#20204;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#25552;&#21462;&#20851;&#31995;&#20449;&#24687;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36873;&#25321;&#21487;&#33021;&#24418;&#25104;&#20851;&#31995;&#30340;&#23545;&#35937;&#23545;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21333;&#38454;&#27573;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#28151;&#21512;&#23545;&#35937;&#21644;&#20851;&#31995;&#26816;&#27979;&#25968;&#25454;&#19978;&#35757;&#32451;&#27492;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Visual Genome&#21644;&#22823;&#35789;&#27719;GQA&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20851;&#31995;&#26816;&#27979;&#24615;&#33021;&#65292;&#21487;&#23454;&#29616;&#23454;&#26102;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14270v1 Announce Type: cross  Abstract: Visual relationship detection aims to identify objects and their relationships in images. Prior methods approach this task by adding separate relationship modules or decoders to existing object detection architectures. This separation increases complexity and hinders end-to-end training, which limits performance. We propose a simple and highly efficient decoder-free architecture for open-vocabulary visual relationship detection. Our model consists of a Transformer-based image encoder that represents objects as tokens and models their relationships implicitly. To extract relationship information, we introduce an attention mechanism that selects object pairs likely to form a relationship. We provide a single-stage recipe to train this model on a mixture of object and relationship detection data. Our approach achieves state-of-the-art relationship detection performance on Visual Genome and on the large-vocabulary GQA benchmark at real-tim
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#32467;&#26500;&#30456;&#20284;&#24230;(SSIM)&#20316;&#20026;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#35780;&#20998;&#20989;&#25968;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#38598;&#25104;&#31574;&#30053;&#20026;&#19981;&#21516;&#30149;&#29702;&#23398;&#25552;&#20379;&#26356;&#20855;&#30149;&#29702;&#23398;&#26222;&#36866;&#24615;&#30340;&#35780;&#20998;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.14262</link><description>&lt;p&gt;
&#20855;&#26377;&#38598;&#25104;&#32467;&#26500;&#30340;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models with Ensembled Structure-Based Anomaly Scoring for Unsupervised Anomaly Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#32467;&#26500;&#30456;&#20284;&#24230;(SSIM)&#20316;&#20026;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#35780;&#20998;&#20989;&#25968;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#38598;&#25104;&#31574;&#30053;&#20026;&#19981;&#21516;&#30149;&#29702;&#23398;&#25552;&#20379;&#26356;&#20855;&#30149;&#29702;&#23398;&#26222;&#36866;&#24615;&#30340;&#35780;&#20998;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#38656;&#35201;&#20840;&#38754;&#30340;&#24102;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#36825;&#23545;&#20110;&#32597;&#35265;&#30142;&#30149;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;(UAD)&#20316;&#20026;&#30149;&#29702;&#20998;&#21106;&#30340;&#19968;&#31181;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#20986;&#29616;&#65292;&#22240;&#20026;&#21482;&#38656;&#35201;&#20581;&#24247;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;UAD&#24322;&#24120;&#35780;&#20998;&#20989;&#25968;&#36890;&#24120;&#21482;&#20851;&#27880;&#24378;&#24230;&#65292;&#24573;&#30053;&#32467;&#26500;&#24046;&#24322;&#65292;&#36825;&#24433;&#21709;&#20102;&#20998;&#21106;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#32467;&#26500;&#30456;&#20284;&#24230;(SSIM)&#20316;&#20026;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#30340;&#28508;&#21147;&#12290;SSIM&#25429;&#25417;&#20102;&#24378;&#24230;&#21644;&#32467;&#26500;&#24046;&#24322;&#65292;&#19988;&#21487;&#33021;&#20248;&#20110;&#20256;&#32479;&#30340;$l1$&#35823;&#24046;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#34920;&#26126;&#23545;&#20110;&#19981;&#21516;&#30149;&#29702;&#23398;&#65292;SSIM&#35745;&#31639;&#23384;&#22312;&#22810;&#20010;&#26368;&#20339;&#26680;&#22823;&#23567;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;&#20110;&#19981;&#21516;&#26680;&#22823;&#23567;&#30340;&#33258;&#36866;&#24212;&#38598;&#25104;&#31574;&#30053;&#65292;&#20197;&#25552;&#20379;&#26356;&#20855;&#30149;&#29702;&#23398;&#26222;&#36866;&#24615;&#30340;&#35780;&#20998;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14262v1 Announce Type: cross  Abstract: Supervised deep learning techniques show promise in medical image analysis. However, they require comprehensive annotated data sets, which poses challenges, particularly for rare diseases. Consequently, unsupervised anomaly detection (UAD) emerges as a viable alternative for pathology segmentation, as only healthy data is required for training. However, recent UAD anomaly scoring functions often focus on intensity only and neglect structural differences, which impedes the segmentation performance. This work investigates the potential of Structural Similarity (SSIM) to bridge this gap. SSIM captures both intensity and structural disparities and can be advantageous over the classical $l1$ error. However, we show that there is more than one optimal kernel size for the SSIM calculation for different pathologies. Therefore, we investigate an adaptive ensembling strategy for various kernel sizes to offer a more pathology-agnostic scoring mec
&lt;/p&gt;</description></item><item><title>ERD&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#21462;&#19982;&#35748;&#30693;&#22833;&#35843;&#30456;&#20851;&#30340;&#37096;&#20998;&#21644;&#36890;&#36807;&#22810;&#20010;&#20195;&#29702;&#20154;&#36827;&#34892;&#25512;&#29702;&#27493;&#39588;&#30340;&#36777;&#35770;&#65292;&#25913;&#36827;&#20102;&#22522;&#20110;LLM&#30340;&#35748;&#30693;&#22833;&#35843;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.14255</link><description>&lt;p&gt;
ERD&#65306;&#19968;&#20010;&#29992;&#20110;&#25913;&#21892;&#35748;&#30693;&#22833;&#35843;&#20998;&#31867;&#30340;LLM&#25512;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ERD: A Framework for Improving LLM Reasoning for Cognitive Distortion Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14255
&lt;/p&gt;
&lt;p&gt;
ERD&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#21462;&#19982;&#35748;&#30693;&#22833;&#35843;&#30456;&#20851;&#30340;&#37096;&#20998;&#21644;&#36890;&#36807;&#22810;&#20010;&#20195;&#29702;&#20154;&#36827;&#34892;&#25512;&#29702;&#27493;&#39588;&#30340;&#36777;&#35770;&#65292;&#25913;&#36827;&#20102;&#22522;&#20110;LLM&#30340;&#35748;&#30693;&#22833;&#35843;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25913;&#36827;&#24515;&#29702;&#27835;&#30103;&#30340;&#21487;&#35775;&#38382;&#24615;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#37325;&#35270;&#12290;&#20174;&#21463;&#35775;&#32773;&#30340;&#35805;&#35821;&#20013;&#35782;&#21035;&#35748;&#30693;&#22833;&#35843;&#21487;&#20197;&#26159;&#24515;&#29702;&#27835;&#30103;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#35748;&#30693;&#34892;&#20026;&#30103;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ERD&#65292;&#36890;&#36807;&#39069;&#22806;&#27169;&#22359;&#30340;&#65288;1&#65289;&#25552;&#21462;&#19982;&#35748;&#30693;&#22833;&#35843;&#30456;&#20851;&#30340;&#37096;&#20998;&#21644;&#65288;2&#65289;&#36890;&#36807;&#22810;&#20010;&#20195;&#29702;&#20154;&#36827;&#34892;&#25512;&#29702;&#27493;&#39588;&#30340;&#36777;&#35770;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;LLM&#30340;&#35748;&#30693;&#22833;&#35843;&#20998;&#31867;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;ERD&#25552;&#39640;&#20102;&#22810;&#31867;F1&#20998;&#25968;&#20197;&#21450;&#20108;&#20803;&#29305;&#24322;&#24615;&#20998;&#25968;&#12290;&#20851;&#20110;&#21518;&#32773;&#30340;&#20998;&#25968;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21521;LLMs&#25552;&#20379;&#22810;&#20195;&#29702;&#20154;&#36777;&#35770;&#25688;&#35201;&#26102;&#26377;&#25928;&#22320;&#28040;&#38500;&#20102;&#22522;&#20934;&#26041;&#27861;&#30340;&#20559;&#35265;&#65292;&#23588;&#20854;&#26159;&#24403;&#35813;&#25688;&#35201;&#34987;&#25552;&#20379;&#32473;LLMs&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14255v1 Announce Type: new  Abstract: Improving the accessibility of psychotherapy with the aid of Large Language Models (LLMs) is garnering a significant attention in recent years. Recognizing cognitive distortions from the interviewee's utterances can be an essential part of psychotherapy, especially for cognitive behavioral therapy. In this paper, we propose ERD, which improves LLM-based cognitive distortion classification performance with the aid of additional modules of (1) extracting the parts related to cognitive distortion, and (2) debating the reasoning steps by multiple agents. Our experimental results on a public dataset show that ERD improves the multi-class F1 score as well as binary specificity score. Regarding the latter score, it turns out that our method is effective in debiasing the baseline method which has high false positive rate, especially when the summary of multi-agent debate is provided to LLMs.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LayoutLLM&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21644;&#25991;&#26723;&#22270;&#20687;&#29702;&#35299;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#23545;&#25991;&#26723;&#22270;&#20687;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.14252</link><description>&lt;p&gt;
LayoutLLM&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#35843;&#25972;&#29992;&#20110;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
LayoutLLM: Large Language Model Instruction Tuning for Visually Rich Document Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14252
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LayoutLLM&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21644;&#25991;&#26723;&#22270;&#20687;&#29702;&#35299;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#23545;&#25991;&#26723;&#22270;&#20687;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;LayoutLLM&#65292;&#19968;&#31181;&#26356;&#28789;&#27963;&#30340;&#25991;&#26723;&#20998;&#26512;&#26041;&#27861;&#65292;&#29992;&#20110;&#29702;&#35299;&#22270;&#20687;&#25991;&#26723;&#12290;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#29702;&#35299;&#20219;&#21153;&#65292;&#22914;&#25991;&#26723;&#22270;&#20687;&#20998;&#31867;&#21644;&#20449;&#24687;&#25552;&#21462;&#65292;&#30001;&#20110;&#20854;&#37325;&#35201;&#24615;&#32780;&#21463;&#21040;&#37325;&#35270;&#12290;&#29616;&#26377;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#25972;&#21512;&#23545;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#24067;&#23616;&#32467;&#26500;&#30340;&#39044;&#35757;&#32451;&#24847;&#35782;&#26469;&#25552;&#21319;&#25991;&#26723;&#29702;&#35299;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#38024;&#23545;&#27599;&#20010;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#32780;&#19988;&#27169;&#22411;&#35757;&#32451;&#21644;&#25805;&#20316;&#25104;&#26412;&#39640;&#26114;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LayoutLLM&#65292;&#23558;&#36825;&#20123;&#19982;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30456;&#38598;&#25104;&#12290;&#36890;&#36807;&#21033;&#29992;&#29616;&#26377;&#30740;&#31350;&#22312;&#25991;&#26723;&#22270;&#20687;&#29702;&#35299;&#21644;LLMs&#21331;&#36234;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#22810;&#27169;&#24577;&#25351;&#20196;&#25968;&#25454;&#38598;&#30340;&#24494;&#35843;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#21333;&#20010;&#27169;&#22411;&#29702;&#35299;&#25991;&#26723;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14252v1 Announce Type: cross  Abstract: This paper proposes LayoutLLM, a more flexible document analysis method for understanding imaged documents. Visually Rich Document Understanding tasks, such as document image classification and information extraction, have gained significant attention due to their importance. Existing methods have been developed to enhance document comprehension by incorporating pre-training awareness of images, text, and layout structure. However, these methods require fine-tuning for each task and dataset, and the models are expensive to train and operate. To overcome this limitation, we propose a new LayoutLLM that integrates these with large-scale language models (LLMs). By leveraging the strengths of existing research in document image understanding and LLMs' superior language understanding capabilities, the proposed model, fine-tuned with multimodal instruction datasets, performs an understanding of document images in a single model. Our experime
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20351;&#29992;&#21516;&#24615;&#36136;&#39640;&#26031;&#26680;&#26367;&#20195;&#21508;&#21521;&#24322;&#24615;&#26680;&#26469;&#25552;&#39640;&#35745;&#31639;&#24615;&#33021;&#65292;&#22312;&#19981;&#22833;&#21435;&#20960;&#20309;&#34920;&#31034;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#32422;100&#20493;&#30340;&#21152;&#36895;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#38656;&#35201;&#36752;&#23556;&#22330;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2403.14244</link><description>&lt;p&gt;
&#21516;&#24615;&#36136;&#39640;&#26031;&#39128;&#23633;&#23454;&#29616;&#23454;&#26102;&#36752;&#23556;&#22330;&#28210;&#26579;
&lt;/p&gt;
&lt;p&gt;
Isotropic Gaussian Splatting for Real-Time Radiance Field Rendering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14244
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20351;&#29992;&#21516;&#24615;&#36136;&#39640;&#26031;&#26680;&#26367;&#20195;&#21508;&#21521;&#24322;&#24615;&#26680;&#26469;&#25552;&#39640;&#35745;&#31639;&#24615;&#33021;&#65292;&#22312;&#19981;&#22833;&#21435;&#20960;&#20309;&#34920;&#31034;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#32422;100&#20493;&#30340;&#21152;&#36895;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#38656;&#35201;&#36752;&#23556;&#22330;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#39640;&#26031;&#39128;&#23633;&#26041;&#27861;&#22240;&#20854;&#22312;&#35757;&#32451;&#20013;&#30340;&#39640;&#24615;&#33021;&#21644;&#28210;&#26579;&#22270;&#20687;&#30340;&#39640;&#36136;&#37327;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#20351;&#29992;&#21508;&#21521;&#24322;&#24615;&#30340;&#39640;&#26031;&#26680;&#26469;&#34920;&#31034;&#22330;&#26223;&#12290;&#34429;&#28982;&#36825;&#31181;&#21508;&#21521;&#24322;&#24615;&#26680;&#22312;&#34920;&#31034;&#20960;&#20309;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#20294;&#22312;&#35745;&#31639;&#26041;&#38754;&#20250;&#23548;&#33268;&#35832;&#22914;&#20998;&#35010;&#25110;&#21512;&#24182;&#20004;&#20010;&#26680;&#30340;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#21516;&#24615;&#36136;&#39640;&#26031;&#26680;&#26469;&#36991;&#20813;&#35745;&#31639;&#20013;&#30340;&#36825;&#20123;&#22256;&#38590;&#65292;&#20174;&#32780;&#23548;&#33268;&#19968;&#31181;&#24615;&#33021;&#26356;&#39640;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#23454;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#27604;&#21407;&#26041;&#27861;&#24555;&#32422;100&#20493;&#65292;&#32780;&#19981;&#20250;&#20002;&#22833;&#20960;&#20309;&#34920;&#31034;&#30340;&#20934;&#30830;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#24212;&#29992;&#20110;&#38656;&#35201;&#36752;&#23556;&#22330;&#30340;&#22823;&#33539;&#22260;&#24212;&#29992;&#65292;&#22914;3D&#37325;&#24314;&#12289;&#35270;&#22270;&#21512;&#25104;&#21644;&#21160;&#24577;&#23545;&#35937;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14244v1 Announce Type: cross  Abstract: The 3D Gaussian splatting method has drawn a lot of attention, thanks to its high performance in training and high quality of the rendered image. However, it uses anisotropic Gaussian kernels to represent the scene. Although such anisotropic kernels have advantages in representing the geometry, they lead to difficulties in terms of computation, such as splitting or merging two kernels. In this paper, we propose to use isotropic Gaussian kernels to avoid such difficulties in the computation, leading to a higher performance method. The experiments confirm that the proposed method is about {\bf 100X} faster without losing the geometry representation accuracy. The proposed method can be applied in a large range applications where the radiance field is needed, such as 3D reconstruction, view synthesis, and dynamic object modeling.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#32479;&#19968;&#26694;&#26550;&#32467;&#21512;&#20102;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65292;&#26368;&#22823;&#21270;&#20445;&#30041;&#26576;&#20123;&#21521;&#37327;&#34920;&#31034;&#24182;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.14236</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#32534;&#36753;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Framework for Model Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14236
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#32479;&#19968;&#26694;&#26550;&#32467;&#21512;&#20102;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65292;&#26368;&#22823;&#21270;&#20445;&#30041;&#26576;&#20123;&#21521;&#37327;&#34920;&#31034;&#24182;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#26159;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#19987;&#27880;&#20110;&#26356;&#26032;&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#30693;&#35782;&#12290;&#22312;&#21508;&#31181;&#26041;&#27861;&#20013;&#65292;ROME&#21644;MEMIT&#20316;&#20026;&#20027;&#35201;&#30340;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#33073;&#39062;&#32780;&#20986;&#12290;&#32780;MEMIT&#21487;&#20197;&#25209;&#37327;&#32534;&#36753;&#35760;&#24518;&#65292;ROME&#21017;&#19968;&#27425;&#21482;&#33021;&#25913;&#21464;&#19968;&#20010;&#20107;&#23454;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;ROME&#21644;MEMIT&#32435;&#20837;&#19968;&#20010;&#21333;&#19968;&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#20248;&#21270;&#21516;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#20445;&#23384;-&#35760;&#24518;&#8221;&#30446;&#26631;&#12290;&#35813;&#30446;&#26631;&#26088;&#22312;&#22312;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#30340;&#21516;&#26102;&#20445;&#30041;&#26576;&#20123;&#36873;&#23450;&#21521;&#37327;&#30340;&#34920;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;ROME&#20351;&#29992;&#31561;&#24335;&#32422;&#26463;&#20248;&#21270;&#27492;&#30446;&#26631;&#65292;&#32780;MEMIT&#37319;&#29992;&#26356;&#28789;&#27963;&#30340;&#26368;&#23567;&#20108;&#20056;&#32422;&#26463;&#12290;&#38500;&#20102;&#25209;&#37327;&#32534;&#36753;&#22806;&#65292;MEMIT&#36824;&#21487;&#20197;&#22312;&#22810;&#20010;&#23618;&#38754;&#32534;&#36753;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#32534;&#36753;&#30340;&#20998;&#24067;&#20174;&#22810;&#20010;&#23618;&#38754;&#20998;&#24320;&#65292;&#21306;&#21035;&#20110;&#20248;&#21270;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14236v1 Announce Type: cross  Abstract: Model editing is a growing area focused on updating the knowledge embedded within models. Among the various methodologies, ROME and MEMIT stand out as leading "locate-and-edit" model editing techniques. While MEMIT enables batched editing of memories, ROME is limited to changing one fact at a time. This paper introduces a unifying framework that brings ROME and MEMIT under a single conceptual umbrella, optimizing for the same goal, which we call the "preservation-memorization" objective. This objective aims to preserve the representations of certain selected vectors while memorizing the representations of new factual information. Specifically, ROME optimizes this objective using an equality constraint, whereas MEMIT employs a more flexible least-square constraint. In addition to making batched edits, MEMIT also edits the model at multiple layers. We disentangle the distribution of edits to multiple layers from the optimization objectiv
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Gal-DINO&#35745;&#31639;&#26426;&#35270;&#35273;&#32593;&#32476;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;EMU Pilot Survey&#20013;&#30340;&#23556;&#30005;&#26143;&#31995;&#30446;&#24405;&#65292;&#21487;&#39640;&#25928;&#39044;&#27979;&#26080;&#32447;&#30005;&#28304;&#24418;&#24577;&#12289;&#20301;&#32622;&#21644;&#32418;&#22806;&#20027;&#26426;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.14235</link><description>&lt;p&gt;
RG-CAT: EMU Pilot Survey&#20013;&#22825;&#20307;&#26816;&#27979;&#31649;&#32447;&#21644;&#26080;&#32447;&#30005;&#26143;&#31995;&#30446;&#24405;
&lt;/p&gt;
&lt;p&gt;
RG-CAT: Detection Pipeline and Catalogue of Radio Galaxies in the EMU Pilot Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14235
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Gal-DINO&#35745;&#31639;&#26426;&#35270;&#35273;&#32593;&#32476;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;EMU Pilot Survey&#20013;&#30340;&#23556;&#30005;&#26143;&#31995;&#30446;&#24405;&#65292;&#21487;&#39640;&#25928;&#39044;&#27979;&#26080;&#32447;&#30005;&#28304;&#24418;&#24577;&#12289;&#20301;&#32622;&#21644;&#32418;&#22806;&#20027;&#26426;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#28304;&#26816;&#27979;&#21644;&#30446;&#24405;&#26500;&#24314;&#31649;&#32447;&#65292;&#20197;&#26500;&#24314;&#36890;&#36807;&#28595;&#22823;&#21033;&#20122;&#24179;&#26041;&#21315;&#31859;&#38453;&#21015;&#25506;&#27979;&#22120;&#65288;ASKAP&#65289;&#26395;&#36828;&#38236;&#36827;&#34892;&#30340;Evolutionary Map of the Universe&#65288;EMU-PS&#65289;270 $\rm deg^2$&#35797;&#39564;&#35266;&#27979;&#30340;&#31532;&#19968;&#20010;&#26080;&#32447;&#30005;&#26143;&#31995;&#30446;&#24405;&#12290;&#26816;&#27979;&#31649;&#32447;&#20351;&#29992;Gal-DINO&#35745;&#31639;&#26426;&#35270;&#35273;&#32593;&#32476;&#65288;Gupta&#31561;&#20154;&#65292;2024&#24180;&#65289;&#26469;&#39044;&#27979;&#26080;&#32447;&#30005;&#28304;&#30340;&#31867;&#21035;&#21644;&#36793;&#30028;&#26694;&#65292;&#20197;&#21450;&#23427;&#20204;&#28508;&#22312;&#30340;&#32418;&#22806;&#20027;&#26426;&#20301;&#32622;&#12290;Gal-DINO&#32593;&#32476;&#22312;&#22823;&#32422;5,000&#20010;&#32463;&#36807;&#35270;&#35273;&#26816;&#39564;&#30340;&#26080;&#32447;&#30005;&#26143;&#31995;&#21450;&#20854;&#32418;&#22806;&#20027;&#26426;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#21253;&#25324;&#32039;&#20945;&#21644;&#25193;&#23637;&#30340;&#26080;&#32447;&#30005;&#24418;&#24577;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;99%&#30340;&#26080;&#32447;&#30005;&#28304;&#65292;&#39044;&#27979;&#21644;&#30495;&#20540;&#36793;&#30028;&#26694;&#30340;&#20132;&#38598;&#36229;&#36807;0.5&#65292;98%&#30340;&#39044;&#27979;&#20027;&#26426;&#20301;&#32622;&#19982;&#30495;&#20540;&#32418;&#22806;&#20027;&#26426;&#22312;&#35780;&#20272;&#20013;&#36317;&#31163;&#23567;&#20110;$3^{\prime \prime}$&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14235v1 Announce Type: cross  Abstract: We present source detection and catalogue construction pipelines to build the first catalogue of radio galaxies from the 270 $\rm deg^2$ pilot survey of the Evolutionary Map of the Universe (EMU-PS) conducted with the Australian Square Kilometre Array Pathfinder (ASKAP) telescope. The detection pipeline uses Gal-DINO computer-vision networks (Gupta et al., 2024) to predict the categories of radio morphology and bounding boxes for radio sources, as well as their potential infrared host positions. The Gal-DINO network is trained and evaluated on approximately 5,000 visually inspected radio galaxies and their infrared hosts, encompassing both compact and extended radio morphologies. We find that the Intersection over Union (IoU) for the predicted and ground truth bounding boxes is larger than 0.5 for 99% of the radio sources, and 98% of predicted host positions are within $3^{\prime \prime}$ of the ground truth infrared host in the evalua
&lt;/p&gt;</description></item><item><title>&#39318;&#27425;&#32771;&#34385;&#22270;&#20687;&#20256;&#24863;&#22120;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#26631;&#31614;&#32423;&#21035;&#22122;&#22768;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#26377;&#25928;&#21435;&#22122;&#34917;&#19969;&#32423;&#21035;&#25968;&#25454;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;SoftPatch&#12290;</title><link>https://arxiv.org/abs/2403.14233</link><description>&lt;p&gt;
SoftPatch&#65306;&#26080;&#30417;&#30563;&#22122;&#22768;&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
SoftPatch: Unsupervised Anomaly Detection with Noisy Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14233
&lt;/p&gt;
&lt;p&gt;
&#39318;&#27425;&#32771;&#34385;&#22270;&#20687;&#20256;&#24863;&#22120;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#26631;&#31614;&#32423;&#21035;&#22122;&#22768;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#26377;&#25928;&#21435;&#22122;&#34917;&#19969;&#32423;&#21035;&#25968;&#25454;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;SoftPatch&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20027;&#27969;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#22312;&#23398;&#26415;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#30001;&#20110;&#29702;&#24819;&#30340;&#24178;&#20928;&#35757;&#32451;&#25968;&#25454;&#30340;&#23454;&#39564;&#35774;&#32622;&#65292;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290; &#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24322;&#24120;&#26816;&#27979;&#20013;&#65292;&#20351;&#29992;&#22122;&#22768;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26159;&#19968;&#20010;&#19981;&#21487;&#36991;&#20813;&#30340;&#38382;&#39064;&#65292;&#20294;&#24456;&#23569;&#26377;&#35752;&#35770;&#12290; &#26412;&#25991;&#39318;&#27425;&#32771;&#34385;&#20102;&#22270;&#20687;&#20256;&#24863;&#22120;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#26631;&#31614;&#32423;&#21035;&#22122;&#22768;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#23384;&#30340;&#26080;&#30417;&#30563;AD&#26041;&#27861;SoftPatch&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#23545;&#34917;&#19969;&#32423;&#21035;&#30340;&#25968;&#25454;&#36827;&#34892;&#21435;&#22122;&#12290; &#22122;&#22768;&#21028;&#21035;&#22120;&#29992;&#20110;&#29983;&#25104;&#29992;&#20110;&#34917;&#19969;&#32423;&#21035;&#22122;&#22768;&#28040;&#38500;&#30340;&#24322;&#24120;&#28857;&#35780;&#20998;&#65292;&#28982;&#21518;&#23558;&#36825;&#20123;&#35780;&#20998;&#23384;&#20648;&#22312;&#20869;&#23384;&#23384;&#20648;&#22120;&#20013;&#65292;&#20197;&#36719;&#21270;&#24322;&#24120;&#26816;&#27979;&#36793;&#30028;&#12290; &#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;SoftPatch&#20445;&#25345;&#20102;&#23545;&#27491;&#24120;&#25968;&#25454;&#30340;&#24378;&#24314;&#27169;&#33021;&#21147;&#65292;&#24182;&#20943;&#36731;&#20102;coreset&#20013;&#30340;&#36807;&#24230;&#33258;&#20449;&#38382;&#39064;&#12290;&#22312;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#32508;&#21512;&#24615;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14233v1 Announce Type: cross  Abstract: Although mainstream unsupervised anomaly detection (AD) algorithms perform well in academic datasets, their performance is limited in practical application due to the ideal experimental setting of clean training data. Training with noisy data is an inevitable problem in real-world anomaly detection but is seldom discussed. This paper considers label-level noise in image sensory anomaly detection for the first time. To solve this problem, we proposed a memory-based unsupervised AD method, SoftPatch, which efficiently denoises the data at the patch level. Noise discriminators are utilized to generate outlier scores for patch-level noise elimination before coreset construction. The scores are then stored in the memory bank to soften the anomaly detection boundary. Compared with existing methods, SoftPatch maintains a strong modeling ability of normal data and alleviates the overconfidence problem in coreset. Comprehensive experiments in v
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#20102;&#24179;&#34913;&#21644;&#39044;&#21518;&#34920;&#31034;&#23545;&#20110;&#26080;&#20559;&#20272;&#35745;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#24179;&#34913;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.14232</link><description>&lt;p&gt;
&#24322;&#36136;&#21058;&#37327;-&#21709;&#24212;&#26354;&#32447;&#20272;&#35745;&#30340;&#23545;&#27604;&#24179;&#34913;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contrastive Balancing Representation Learning for Heterogeneous Dose-Response Curves Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14232
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#20102;&#24179;&#34913;&#21644;&#39044;&#21518;&#34920;&#31034;&#23545;&#20110;&#26080;&#20559;&#20272;&#35745;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#24179;&#34913;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#20010;&#20307;&#23545;&#19981;&#21516;&#27835;&#30103;&#21058;&#37327;&#30340;&#28508;&#22312;&#21709;&#24212;&#23545;&#20110;&#20915;&#31574;&#21046;&#23450;&#33267;&#20851;&#37325;&#35201;&#65292;&#28041;&#21450;&#31934;&#20934;&#21307;&#23398;&#21644;&#31649;&#29702;&#31185;&#23398;&#31561;&#39046;&#22495;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#22823;&#22810;&#36890;&#36807;&#23398;&#20064;&#19982;&#27835;&#30103;&#21464;&#37327;&#26080;&#20851;&#30340;&#21327;&#21464;&#37327;&#34920;&#31034;&#26469;&#39044;&#27979;&#21453;&#20107;&#23454;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#29420;&#31435;&#24615;&#32422;&#26463;&#24573;&#30053;&#20102;&#35768;&#22810;&#26377;&#29992;&#20110;&#21453;&#20107;&#23454;&#39044;&#27979;&#30340;&#21327;&#21464;&#37327;&#20449;&#24687;&#65292;&#29305;&#21035;&#26159;&#24403;&#27835;&#30103;&#21464;&#37327;&#26159;&#36830;&#32493;&#30340;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#26412;&#25991;&#39318;&#20808;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#24179;&#34913;&#21644;&#39044;&#21518;&#34920;&#31034;&#23545;&#20110;&#24322;&#36136;&#21058;&#37327;-&#21709;&#24212;&#26354;&#32447;&#30340;&#26080;&#20559;&#20272;&#35745;&#30340;&#37325;&#35201;&#24615;&#65292;&#21363;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#21463;&#21040;&#38480;&#21046;&#65292;&#20197;&#28385;&#36275;&#21327;&#21464;&#37327;&#19982;&#27835;&#30103;&#21464;&#37327;&#21644;&#28508;&#22312;&#21709;&#24212;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#20851;&#31995;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#24179;&#34913;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14232v1 Announce Type: new  Abstract: Estimating the individuals' potential response to varying treatment doses is crucial for decision-making in areas such as precision medicine and management science. Most recent studies predict counterfactual outcomes by learning a covariate representation that is independent of the treatment variable. However, such independence constraints neglect much of the covariate information that is useful for counterfactual prediction, especially when the treatment variables are continuous. To tackle the above issue, in this paper, we first theoretically demonstrate the importance of the balancing and prognostic representations for unbiased estimation of the heterogeneous dose-response curves, that is, the learned representations are constrained to satisfy the conditional independence between the covariates and both of the treatment variables and the potential responses. Based on this, we propose a novel Contrastive balancing Representation learni
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20195;&#29702;&#28151;&#28102;&#22240;&#23376;&#20998;&#35299; (PCF) &#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#39640;&#32500;&#28151;&#21512;&#20195;&#29702;&#21464;&#37327;&#26469;&#20272;&#35745;&#36830;&#32493;&#22788;&#29702;&#25928;&#24212;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#39640;&#26679;&#26412;&#22823;&#23567;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#22312;&#22240;&#26524;&#25928;&#26524;&#20272;&#35745;&#20013;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#30456;&#20851;&#24615;&#21644;&#36739;&#20302;&#30340;&#35823;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.14228</link><description>&lt;p&gt;
&#20174;&#39640;&#32500;&#20195;&#29702;&#21464;&#37327;&#20013;&#24674;&#22797;&#28508;&#22312;&#28508;&#22312;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Recovering Latent Confounders from High-dimensional Proxy Variables
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14228
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20195;&#29702;&#28151;&#28102;&#22240;&#23376;&#20998;&#35299; (PCF) &#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#39640;&#32500;&#28151;&#21512;&#20195;&#29702;&#21464;&#37327;&#26469;&#20272;&#35745;&#36830;&#32493;&#22788;&#29702;&#25928;&#24212;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#39640;&#26679;&#26412;&#22823;&#23567;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#22312;&#22240;&#26524;&#25928;&#26524;&#20272;&#35745;&#20013;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#30456;&#20851;&#24615;&#21644;&#36739;&#20302;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#28508;&#22312;&#28508;&#20239;&#32773;&#65292;&#20174;&#20195;&#29702;&#21464;&#37327;&#26159;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#20197;&#21069;&#30340;&#26041;&#27861;&#23616;&#38480;&#20110;&#20302;&#32500;&#20195;&#29702;&#65292;&#25490;&#24207;&#20195;&#29702;&#21644;&#20108;&#20803;&#27835;&#30103;&#12290;&#25105;&#20204;&#28040;&#38500;&#20102;&#36825;&#20123;&#20551;&#35774;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20195;&#29702;&#28151;&#28102;&#22240;&#23376;&#20998;&#35299; (PCF) &#26694;&#26550;&#65292;&#29992;&#20110;&#36830;&#32493;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#65292;&#24403;&#28508;&#22312;&#28151;&#28102;&#22240;&#23376;&#36890;&#36807;&#39640;&#32500;&#65292;&#28151;&#21512;&#20195;&#29702;&#21464;&#37327;&#32780;&#26174;&#29616;&#12290;&#23545;&#20110;&#29305;&#23450;&#26679;&#26412;&#22823;&#23567;&#65292;&#25105;&#20204;&#30340;&#20004;&#27493; PCF &#23454;&#26045;&#65292;&#20351;&#29992;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512; (ICA-PCF) &#21644;&#31471;&#21040;&#31471;&#23454;&#26045;&#65292;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477; (GD-PCF)&#65292;&#22312;&#39640;&#26679;&#26412;&#22823;&#23567;&#33539;&#22260;&#20869;&#65292;&#19982;&#28508;&#22312;&#28151;&#28102;&#22240;&#23376;&#30340;&#30456;&#20851;&#24615;&#36739;&#39640;&#65292;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#30340;&#32477;&#23545;&#35823;&#24046;&#36739;&#20302;&#12290;&#65292;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#12290;&#21363;&#20351;&#38754;&#23545;&#27668;&#20505;&#25968;&#25454;&#65292;ICA-PCF &#24674;&#22797;&#20102;&#35299;&#37322;&#27431;&#27954;&#38477;&#38632;&#27169;&#24335;&#30340; North Atlantic Oscillation $75.9\%$ &#26041;&#24046;&#30340;&#22235;&#20010;&#20998;&#37327;&#65292;&#19968;&#20010;&#24050;&#30693;&#30340;&#38477;&#27700;&#27169;&#24335;&#30340;&#28151;&#28102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14228v1 Announce Type: cross  Abstract: Detecting latent confounders from proxy variables is an essential problem in causal effect estimation. Previous approaches are limited to low-dimensional proxies, sorted proxies, and binary treatments. We remove these assumptions and present a novel Proxy Confounder Factorization (PCF) framework for continuous treatment effect estimation when latent confounders manifest through high-dimensional, mixed proxy variables. For specific sample sizes, our two-step PCF implementation, using Independent Component Analysis (ICA-PCF), and the end-to-end implementation, using Gradient Descent (GD-PCF), achieve high correlation with the latent confounder and low absolute error in causal effect estimation with synthetic datasets in the high sample size regime. Even when faced with climate data, ICA-PCF recovers four components that explain $75.9\%$ of the variance in the North Atlantic Oscillation, a known confounder of precipitation patterns in Eur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36817;&#20284;&#29702;&#35770;&#65292;&#34920;&#26126;&#20855;&#26377;&#38750;&#31232;&#30095;&#36890;&#29992;&#20808;&#39564;&#30340;BNNs&#21487;&#20197;&#23454;&#29616;&#25509;&#36817;&#26368;&#23567;&#21270;&#26368;&#20248;&#21518;&#39564;&#27987;&#24230;&#36895;&#29575;&#33267;&#30495;&#23454;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.14225</link><description>&lt;p&gt;
&#20855;&#26377;&#26435;&#37325;&#36890;&#29992;&#20808;&#39564;&#30340;&#20840;&#36830;&#25509;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#21518;&#39564;&#27987;&#24230;
&lt;/p&gt;
&lt;p&gt;
Posterior concentrations of fully-connected Bayesian neural networks with general priors on the weights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36817;&#20284;&#29702;&#35770;&#65292;&#34920;&#26126;&#20855;&#26377;&#38750;&#31232;&#30095;&#36890;&#29992;&#20808;&#39564;&#30340;BNNs&#21487;&#20197;&#23454;&#29616;&#25509;&#36817;&#26368;&#23567;&#21270;&#26368;&#20248;&#21518;&#39564;&#27987;&#24230;&#36895;&#29575;&#33267;&#30495;&#23454;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#22791;&#21463;&#20851;&#27880;&#65292;&#24182;&#24050;&#22312;&#24191;&#27867;&#30340;&#24212;&#29992;&#20013;&#24471;&#21040;&#26377;&#25928;&#21033;&#29992;&#12290;&#20808;&#21069;&#26377;&#20851;BNNs&#21518;&#39564;&#27987;&#24230;&#24615;&#36136;&#30340;&#30740;&#31350;&#24050;&#26377;&#20960;&#39033;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#30740;&#31350;&#20165;&#22312;&#20855;&#26377;&#31232;&#30095;&#25110;&#37325;&#23614;&#20808;&#39564;&#30340;BNN&#27169;&#22411;&#20013;&#23637;&#31034;&#32467;&#26524;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#30446;&#21069;&#23578;&#26080;&#20851;&#20110;&#20351;&#29992;&#39640;&#26031;&#20808;&#39564;&#30340;BNNs&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#32780;&#39640;&#26031;&#20808;&#39564;&#26159;&#26368;&#24120;&#29992;&#30340;&#20808;&#39564;&#20043;&#19968;&#12290;&#36825;&#31181;&#29702;&#35770;&#32570;&#22833;&#28304;&#20110;&#32570;&#20047;&#36817;&#20284;&#38750;&#31232;&#30095;&#19988;&#20855;&#26377;&#26377;&#30028;&#21442;&#25968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#20855;&#26377;&#26377;&#30028;&#21442;&#25968;&#30340;&#38750;&#31232;&#30095;DNNs&#30340;&#26032;&#36817;&#20284;&#29702;&#35770;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#36825;&#19968;&#36817;&#20284;&#29702;&#35770;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20855;&#26377;&#38750;&#31232;&#30095;&#36890;&#29992;&#20808;&#39564;&#30340;BNNs&#21487;&#20197;&#23454;&#29616;&#25509;&#36817;&#26368;&#23567;&#21270;&#26368;&#20248;&#21518;&#39564;&#27987;&#24230;&#36895;&#29575;&#33267;&#30495;&#23454;&#27169;&#22411;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14225v1 Announce Type: cross  Abstract: Bayesian approaches for training deep neural networks (BNNs) have received significant interest and have been effectively utilized in a wide range of applications. There have been several studies on the properties of posterior concentrations of BNNs. However, most of these studies only demonstrate results in BNN models with sparse or heavy-tailed priors. Surprisingly, no theoretical results currently exist for BNNs using Gaussian priors, which are the most commonly used one. The lack of theory arises from the absence of approximation results of Deep Neural Networks (DNNs) that are non-sparse and have bounded parameters. In this paper, we present a new approximation theory for non-sparse DNNs with bounded parameters. Additionally, based on the approximation theory, we show that BNNs with non-sparse general priors can achieve near-minimax optimal posterior concentration rates to the true model.
&lt;/p&gt;</description></item><item><title>&#35777;&#26126;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#23384;&#22312;&#19968;&#20123;&#26080;&#20559;&#23376;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20381;&#36182;&#31639;&#27861;&#20559;&#35265;&#30340;&#24773;&#20917;&#19979;&#34987;&#25552;&#21462;&#20986;&#26469;&#65292;&#24182;&#19988;&#36825;&#31181;&#29305;&#23450;&#26550;&#26500;&#26080;&#27861;&#23398;&#20064;&#20219;&#20309;&#29305;&#23450;&#30340;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2403.14200</link><description>&lt;p&gt;
&#25163;&#26415;&#21592;&#21435;&#20559;&#35265;&#65306;&#31070;&#22855;&#30340;&#26435;&#37325;&#21450;&#22914;&#20309;&#25214;&#21040;&#23427;&#20204;
&lt;/p&gt;
&lt;p&gt;
Debiasing surgeon: fantastic weights and how to find them
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14200
&lt;/p&gt;
&lt;p&gt;
&#35777;&#26126;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#23384;&#22312;&#19968;&#20123;&#26080;&#20559;&#23376;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20381;&#36182;&#31639;&#27861;&#20559;&#35265;&#30340;&#24773;&#20917;&#19979;&#34987;&#25552;&#21462;&#20986;&#26469;&#65292;&#24182;&#19988;&#36825;&#31181;&#29305;&#23450;&#26550;&#26500;&#26080;&#27861;&#23398;&#20064;&#20219;&#20309;&#29305;&#23450;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20170;&#19968;&#20010;&#26085;&#30410;&#20851;&#27880;&#30340;&#29616;&#35937;&#26159;&#31639;&#27861;&#20559;&#35265;&#30340;&#20986;&#29616;&#65292;&#23427;&#21487;&#33021;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#27169;&#22411;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#21435;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#26356;&#25110;&#22810;&#25110;&#23569;&#22797;&#26434;&#30340;&#26041;&#27861;&#26469;&#38459;&#27490;&#36825;&#20123;&#27169;&#22411;&#22823;&#35268;&#27169;&#22320;&#20351;&#29992;&#36825;&#20123;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#36825;&#31181;&#39069;&#22806;&#30340;&#22797;&#26434;&#24615;&#30495;&#30340;&#26377;&#24517;&#35201;&#21527;&#65311;&#19968;&#20010;&#26222;&#36890;&#35757;&#32451;&#30340;&#27169;&#22411;&#26159;&#21542;&#24050;&#32463;&#21253;&#21547;&#20102;&#19968;&#20123;&#21487;&#20197;&#29420;&#31435;&#20351;&#29992;&#30340;&#8220;&#26080;&#20559;&#23376;&#32593;&#32476;&#8221;&#65292;&#24182;&#19988;&#21487;&#20197;&#25552;&#20986;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#32780;&#19981;&#20381;&#36182;&#20110;&#31639;&#27861;&#20559;&#35265;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#26679;&#30340;&#23376;&#32593;&#32476;&#36890;&#24120;&#23384;&#22312;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#19968;&#20010;&#26222;&#36890;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#25552;&#21462;&#20986;&#26469;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#36825;&#31181;&#29305;&#23450;&#30340;&#26550;&#26500;&#26080;&#27861;&#23398;&#20064;&#29305;&#23450;&#30340;&#20559;&#35265;&#65292;&#34920;&#26126;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#26377;&#21487;&#33021;&#36890;&#36807;&#26550;&#26500;&#19978;&#30340;&#23545;&#31574;&#26469;&#35299;&#20915;&#20559;&#35265;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14200v1 Announce Type: cross  Abstract: Nowadays an ever-growing concerning phenomenon, the emergence of algorithmic biases that can lead to unfair models, emerges. Several debiasing approaches have been proposed in the realm of deep learning, employing more or less sophisticated approaches to discourage these models from massively employing these biases. However, a question emerges: is this extra complexity really necessary? Is a vanilla-trained model already embodying some ``unbiased sub-networks'' that can be used in isolation and propose a solution without relying on the algorithmic biases? In this work, we show that such a sub-network typically exists, and can be extracted from a vanilla-trained model without requiring additional training. We further validate that such specific architecture is incapable of learning a specific bias, suggesting that there are possible architectural countermeasures to the problem of biases in deep neural networks.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;OTSeg&#20013;&#30340;Multi-Prompts Sinkhorn Attention&#26426;&#21046;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#22810;&#20010;&#25991;&#26412;&#25552;&#31034;&#26469;&#21305;&#37197;&#30456;&#20851;&#20687;&#32032;&#23884;&#20837;&#65292;&#20174;&#32780;&#25552;&#21319;&#38646;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.14183</link><description>&lt;p&gt;
OTSeg&#65306;&#22810;&#25552;&#31034;Sinkhorn&#27880;&#24847;&#21147;&#29992;&#20110;&#38646;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
OTSeg: Multi-prompt Sinkhorn Attention for Zero-Shot Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14183
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;OTSeg&#20013;&#30340;Multi-Prompts Sinkhorn Attention&#26426;&#21046;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#22810;&#20010;&#25991;&#26412;&#25552;&#31034;&#26469;&#21305;&#37197;&#30456;&#20851;&#20687;&#32032;&#23884;&#20837;&#65292;&#20174;&#32780;&#25552;&#21319;&#38646;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CLIP&#30340;&#26368;&#26032;&#25104;&#21151;&#35777;&#26126;&#20102;&#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#30693;&#35782;&#36716;&#31227;&#21040;&#20687;&#32032;&#32423;&#20998;&#31867;&#26469;&#36827;&#34892;&#38646;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#30340;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;CLIP&#30693;&#35782;&#26469;&#32039;&#23494;&#23545;&#40784;&#25991;&#26412;&#23884;&#20837;&#21644;&#20687;&#32032;&#23884;&#20837;&#20173;&#28982;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OTSeg&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#26088;&#22312;&#22686;&#24378;&#22810;&#20010;&#25991;&#26412;&#25552;&#31034;&#21305;&#37197;&#30456;&#20851;&#20687;&#32032;&#23884;&#20837;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#31639;&#27861;&#30340;&#22810;&#25552;&#31034;Sinkhorn&#65288;MPS&#65289;&#65292;&#36825;&#20351;&#24471;&#22810;&#20010;&#25991;&#26412;&#25552;&#31034;&#21487;&#20197;&#26377;&#36873;&#25321;&#22320;&#20851;&#27880;&#22270;&#20687;&#20687;&#32032;&#20869;&#30340;&#21508;&#31181;&#35821;&#20041;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#21463;&#21040;Sinkformers&#22312;&#21333;&#27169;&#24577;&#35774;&#32622;&#20013;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MPS&#30340;&#25193;&#23637;&#65292;&#31216;&#20026;&#22810;&#25552;&#31034;Sinkhorn&#27880;&#24847;&#21147;&#65288;MPSA&#65289;&#65292;&#23427;&#26377;&#25928;&#22320;&#21462;&#20195;&#20102;Transformer&#26694;&#26550;&#20013;&#22810;&#27169;&#24577;&#35774;&#32622;&#20013;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14183v1 Announce Type: cross  Abstract: The recent success of CLIP has demonstrated promising results in zero-shot semantic segmentation by transferring muiltimodal knowledge to pixel-level classification. However, leveraging pre-trained CLIP knowledge to closely align text embeddings with pixel embeddings still has limitations in existing approaches. To address this issue, we propose OTSeg, a novel multimodal attention mechanism aimed at enhancing the potential of multiple text prompts for matching associated pixel embeddings. We first propose Multi-Prompts Sinkhorn (MPS) based on the Optimal Transport (OT) algorithm, which leads multiple text prompts to selectively focus on various semantic features within image pixels. Moreover, inspired by the success of Sinkformers in unimodal settings, we introduce the extension of MPS, called Multi-Prompts Sinkhorn Attention (MPSA), which effectively replaces cross-attention mechanisms within Transformer framework in multimodal settin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31867;&#21035;&#30340;&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;$h$-PMD&#65292;&#23427;&#36890;&#36807;&#22312;PMD&#26356;&#26032;&#35268;&#21017;&#20013;&#32467;&#21512;&#22810;&#27493;&#36138;&#24515;&#31574;&#30053;&#25913;&#36827;&#21644;&#21069;&#30651;&#28145;&#24230;$h&#65292;&#20197;&#35299;&#20915;&#25240;&#25187;&#26080;&#38480;&#26102;&#38388;&#35270;&#35282;&#19979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.14156</link><description>&lt;p&gt;
&#20855;&#26377;&#21069;&#30651;&#29305;&#24615;&#30340;&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Policy Mirror Descent with Lookahead
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14156
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31867;&#21035;&#30340;&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;$h$-PMD&#65292;&#23427;&#36890;&#36807;&#22312;PMD&#26356;&#26032;&#35268;&#21017;&#20013;&#32467;&#21512;&#22810;&#27493;&#36138;&#24515;&#31574;&#30053;&#25913;&#36827;&#21644;&#21069;&#30651;&#28145;&#24230;$h&#65292;&#20197;&#35299;&#20915;&#25240;&#25187;&#26080;&#38480;&#26102;&#38388;&#35270;&#35282;&#19979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#65288;PMD&#65289;&#20316;&#20026;&#19968;&#31181;&#22810;&#21151;&#33021;&#31639;&#27861;&#26694;&#26550;&#65292;&#21253;&#25324;&#20960;&#31181;&#37325;&#35201;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#22914;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#65288;&#22914;TRPO&#21644;PPO&#65289;&#30456;&#32852;&#31995;&#12290;PMD&#21487;&#20197;&#30475;&#20316;&#26159;&#23454;&#29616;&#27491;&#21017;&#21270;1&#27493;&#36138;&#24515;&#31574;&#30053;&#25913;&#36827;&#30340;&#36719;&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;1&#27493;&#36138;&#24515;&#31574;&#30053;&#21487;&#33021;&#19981;&#26159;&#26368;&#20339;&#36873;&#25321;&#65292;&#26368;&#36817;&#22312;RL&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#23454;&#35777;&#25104;&#21151;&#65292;&#22914;AlphaGo&#21644;AlphaZero&#24050;&#32463;&#35777;&#26126;&#65292;&#30456;&#23545;&#20110;&#22810;&#27493;&#39588;&#65292;&#36138;&#24515;&#26041;&#27861;&#21487;&#20197;&#36229;&#36234;&#23427;&#20204;&#30340;1&#27493;&#39588;&#23545;&#24212;&#29289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31867;&#21035;&#30340;PMD&#31639;&#27861;&#65292;&#31216;&#20026;$h$-PMD&#65292;&#23427;&#23558;&#20855;&#26377;&#21069;&#30651;&#28145;&#24230;$h$&#30340;&#22810;&#27493;&#36138;&#24515;&#31574;&#30053;&#25913;&#36827;&#32467;&#21512;&#21040;PMD&#26356;&#26032;&#35268;&#21017;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#25240;&#25187;&#26080;&#38480;&#26102;&#38388;&#35270;&#35282;&#19979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#20854;&#20013;&#25240;&#25187;&#22240;&#23376;&#20026;$\gamma$&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;$h$-PMD&#21487;&#20197;&#25512;&#24191;&#26631;&#20934;&#30340;PMD&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14156v1 Announce Type: cross  Abstract: Policy Mirror Descent (PMD) stands as a versatile algorithmic framework encompassing several seminal policy gradient algorithms such as natural policy gradient, with connections with state-of-the-art reinforcement learning (RL) algorithms such as TRPO and PPO. PMD can be seen as a soft Policy Iteration algorithm implementing regularized 1-step greedy policy improvement. However, 1-step greedy policies might not be the best choice and recent remarkable empirical successes in RL such as AlphaGo and AlphaZero have demonstrated that greedy approaches with respect to multiple steps outperform their 1-step counterpart. In this work, we propose a new class of PMD algorithms called $h$-PMD which incorporates multi-step greedy policy improvement with lookahead depth $h$ to the PMD update rule. To solve discounted infinite horizon Markov Decision Processes with discount factor $\gamma$, we show that $h$-PMD which generalizes the standard PMD enj
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#36712;&#36857;&#25968;&#25454;&#31649;&#29702;&#19982;&#25366;&#25496;&#20013;&#30340;&#21457;&#23637;&#21644;&#26368;&#26032;&#36827;&#23637;&#65292;&#25506;&#35752;&#20102;&#20854;&#22312;&#39044;&#22788;&#29702;&#12289;&#23384;&#20648;&#12289;&#20998;&#26512;&#12289;&#39044;&#27979;&#12289;&#25512;&#33616;&#12289;&#20998;&#31867;&#12289;&#20272;&#35745;&#21644;&#26816;&#27979;&#31561;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.14151</link><description>&lt;p&gt;
&#36712;&#36857;&#25968;&#25454;&#31649;&#29702;&#19982;&#25366;&#25496;&#30340;&#28145;&#24230;&#23398;&#20064;&#65306;&#35843;&#26597;&#19982;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Trajectory Data Management and Mining: A Survey and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#36712;&#36857;&#25968;&#25454;&#31649;&#29702;&#19982;&#25366;&#25496;&#20013;&#30340;&#21457;&#23637;&#21644;&#26368;&#26032;&#36827;&#23637;&#65292;&#25506;&#35752;&#20102;&#20854;&#22312;&#39044;&#22788;&#29702;&#12289;&#23384;&#20648;&#12289;&#20998;&#26512;&#12289;&#39044;&#27979;&#12289;&#25512;&#33616;&#12289;&#20998;&#31867;&#12289;&#20272;&#35745;&#21644;&#26816;&#27979;&#31561;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14151v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#36234; &#25277;&#35937;&#65306;&#36712;&#36857;&#35745;&#31639;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#39046;&#22495;&#65292;&#28085;&#30422;&#36712;&#36857;&#25968;&#25454;&#31649;&#29702;&#21644;&#25366;&#25496;&#65292;&#22240;&#20854;&#22312;&#35832;&#22914;&#20301;&#32622;&#26381;&#21153;&#12289;&#22478;&#24066;&#20132;&#36890;&#21644;&#20844;&#20849;&#23433;&#20840;&#31561;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#20256;&#32479;&#26041;&#27861;&#20391;&#37325;&#20110;&#31616;&#21333;&#30340;&#26102;&#31354;&#29305;&#24449;&#65292;&#38754;&#20020;&#22797;&#26434;&#35745;&#31639;&#12289;&#26377;&#38480;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#19981;&#36275;&#20197;&#36866;&#24212;&#29616;&#23454;&#22797;&#26434;&#24615;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#36712;&#36857;&#35745;&#31639;&#20013;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#21644;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#65288;DL4Traj&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#36712;&#36857;&#25968;&#25454;&#65292;&#24182;&#31616;&#35201;&#20171;&#32461;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#31995;&#32479;&#22320;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#36712;&#36857;&#31649;&#29702;&#65288;&#39044;&#22788;&#29702;&#12289;&#23384;&#20648;&#12289;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#65289;&#21644;&#25366;&#25496;&#65288;&#19982;&#36712;&#36857;&#30456;&#20851;&#30340;&#39044;&#27979;&#12289;&#36712;&#36857;&#30456;&#20851;&#30340;&#25512;&#33616;&#12289;&#36712;&#36857;&#20998;&#31867;&#12289;&#26053;&#34892;&#26102;&#38388;&#20272;&#35745;&#12289;&#24322;&#24120;&#26816;&#27979;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14151v1 Announce Type: cross  Abstract: Trajectory computing is a pivotal domain encompassing trajectory data management and mining, garnering widespread attention due to its crucial role in various practical applications such as location services, urban traffic, and public safety. Traditional methods, focusing on simplistic spatio-temporal features, face challenges of complex calculations, limited scalability, and inadequate adaptability to real-world complexities. In this paper, we present a comprehensive review of the development and recent advances in deep learning for trajectory computing (DL4Traj). We first define trajectory data and provide a brief overview of widely-used deep learning models. Systematically, we explore deep learning applications in trajectory management (pre-processing, storage, analysis, and visualization) and mining (trajectory-related forecasting, trajectory-related recommendation, trajectory classification, travel time estimation, anomaly detecti
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;CMD&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#21644;&#26032;&#30340;&#36731;&#37327;&#32423;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#20869;&#23481;&#24103;&#21644;&#21160;&#24577;&#28508;&#34920;&#31034;&#65292;&#20197;&#35299;&#20915;&#39640;&#20869;&#23384;&#21644;&#35745;&#31639;&#35201;&#27714;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.14148</link><description>&lt;p&gt;
&#36890;&#36807;&#20869;&#23481;-&#24103;&#21160;&#24577;&#28508;&#20998;&#35299;&#23454;&#29616;&#39640;&#25928;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Efficient Video Diffusion Models via Content-Frame Motion-Latent Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14148
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;CMD&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#21644;&#26032;&#30340;&#36731;&#37327;&#32423;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#20869;&#23481;&#24103;&#21644;&#21160;&#24577;&#28508;&#34920;&#31034;&#65292;&#20197;&#35299;&#20915;&#39640;&#20869;&#23384;&#21644;&#35745;&#31639;&#35201;&#27714;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#21462;&#24471;&#20102;&#38271;&#36275;&#36827;&#23637;&#65292;&#20294;&#20173;&#21463;&#21046;&#20110;&#39640;&#20869;&#23384;&#21644;&#35745;&#31639;&#35201;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20869;&#23481;-&#21160;&#24577;&#28508;&#25193;&#25955;&#27169;&#22411;&#65288;CMD&#65289;&#65292;&#20316;&#20026;&#39044;&#35757;&#32451;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#35270;&#39057;&#29983;&#25104;&#20013;&#30340;&#39640;&#25928;&#25193;&#23637;&#12290;CMD&#36890;&#36807;&#33258;&#21160;&#32534;&#30721;&#22120;&#23558;&#35270;&#39057;&#31616;&#27905;&#22320;&#32534;&#30721;&#20026;&#20869;&#23481;&#24103;&#21644;&#20302;&#32500;&#21160;&#24577;&#28508;&#34920;&#31034;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26469;&#29983;&#25104;&#20869;&#23481;&#24103;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#26032;&#30340;&#36731;&#37327;&#32423;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#21160;&#24577;&#28508;&#34920;&#31034;&#12290;&#36825;&#37324;&#30340;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#35774;&#35745;&#20102;&#19968;&#31181;&#32039;&#20945;&#30340;&#28508;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14148v1 Announce Type: cross  Abstract: Video diffusion models have recently made great progress in generation quality, but are still limited by the high memory and computational requirements. This is because current video diffusion models often attempt to process high-dimensional videos directly. To tackle this issue, we propose content-motion latent diffusion model (CMD), a novel efficient extension of pretrained image diffusion models for video generation. Specifically, we propose an autoencoder that succinctly encodes a video as a combination of a content frame (like an image) and a low-dimensional motion latent representation. The former represents the common content, and the latter represents the underlying motion in the video, respectively. We generate the content frame by fine-tuning a pretrained image diffusion model, and we generate the motion latent representation by training a new lightweight diffusion model. A key innovation here is the design of a compact laten
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21435;&#20559;&#35265;&#26694;&#26550;&#65292;&#24341;&#20837;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20449;&#24687;&#29942;&#39048;&#65292;&#29992;&#20110;&#23398;&#20064;&#23646;&#24615;&#30340;&#32452;&#21512;&#34920;&#31034;&#65292;&#32780;&#26080;&#38656;&#23450;&#20041;&#29305;&#23450;&#30340;&#20559;&#35265;&#31867;&#22411;</title><link>https://arxiv.org/abs/2403.14140</link><description>&lt;p&gt;
&#36890;&#36807;&#23646;&#24615;&#20013;&#24515;&#20449;&#24687;&#29942;&#39048;&#23398;&#20064;&#21487;&#20998;&#35299;&#19988;&#26080;&#20559;&#35265;&#30340;&#34920;&#31034;&#24418;&#24335;
&lt;/p&gt;
&lt;p&gt;
Learning Decomposable and Debiased Representations via Attribute-Centric Information Bottlenecks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14140
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21435;&#20559;&#35265;&#26694;&#26550;&#65292;&#24341;&#20837;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20449;&#24687;&#29942;&#39048;&#65292;&#29992;&#20110;&#23398;&#20064;&#23646;&#24615;&#30340;&#32452;&#21512;&#34920;&#31034;&#65292;&#32780;&#26080;&#38656;&#23450;&#20041;&#29305;&#23450;&#30340;&#20559;&#35265;&#31867;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#20559;&#35265;&#30340;&#23646;&#24615;&#22312;&#25968;&#25454;&#38598;&#20013;&#19982;&#30446;&#26631;&#26631;&#31614;&#21576;&#29616;&#34394;&#20551;&#30456;&#20851;&#65292;&#21487;&#33021;&#23548;&#33268;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#19981;&#24403;&#30340;&#20998;&#31867;&#24555;&#25463;&#26041;&#24335;&#65292;&#24182;&#19988;&#38480;&#21046;&#23427;&#20204;&#22312;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21435;&#20559;&#35265;&#26694;&#26550;&#65292;&#24341;&#20837;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20449;&#24687;&#29942;&#39048;&#65292;&#29992;&#20110;&#23398;&#20064;&#23646;&#24615;&#30340;&#32452;&#21512;&#34920;&#31034;&#65292;&#32780;&#26080;&#38656;&#23450;&#20041;&#29305;&#23450;&#30340;&#20559;&#35265;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14140v1 Announce Type: cross  Abstract: Biased attributes, spuriously correlated with target labels in a dataset, can problematically lead to neural networks that learn improper shortcuts for classifications and limit their capabilities for out-of-distribution (OOD) generalization. Although many debiasing approaches have been proposed to ensure correct predictions from biased datasets, few studies have considered learning latent embedding consisting of intrinsic and biased attributes that contribute to improved performance and explain how the model pays attention to attributes. In this paper, we propose a novel debiasing framework, Debiasing Global Workspace, introducing attention-based information bottlenecks for learning compositional representations of attributes without defining specific bias types. Based on our observation that learning shape-centric representation helps robust performance on OOD datasets, we adopt those abilities to learn robust and generalizable repre
&lt;/p&gt;</description></item><item><title>&#36951;&#20256;&#35268;&#21010;&#26041;&#27861;&#25552;&#20986;&#20102;&#29992;&#20110;&#21487;&#35299;&#37322;&#27969;&#24418;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#24110;&#21161;&#35299;&#20915;&#24403;&#21069;&#27969;&#24418;&#23398;&#20064;&#20013;&#21151;&#33021;&#26144;&#23556;&#19981;&#26126;&#30830;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.14139</link><description>&lt;p&gt;
&#29992;&#20110;&#21487;&#35299;&#37322;&#27969;&#24418;&#23398;&#20064;&#30340;&#36951;&#20256;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Genetic Programming for Explainable Manifold Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14139
&lt;/p&gt;
&lt;p&gt;
&#36951;&#20256;&#35268;&#21010;&#26041;&#27861;&#25552;&#20986;&#20102;&#29992;&#20110;&#21487;&#35299;&#37322;&#27969;&#24418;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#24110;&#21161;&#35299;&#20915;&#24403;&#21069;&#27969;&#24418;&#23398;&#20064;&#20013;&#21151;&#33021;&#26144;&#23556;&#19981;&#26126;&#30830;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24418;&#23398;&#20064;&#25216;&#26415;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#36890;&#36807;&#25581;&#31034;&#39640;&#32500;&#25968;&#25454;&#20013;&#30340;&#20302;&#32500;&#23884;&#20837;&#65292;&#20174;&#32780;&#23558;&#25968;&#25454;&#36716;&#25442;&#20026;&#26356;&#20302;&#32500;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#20998;&#26512;&#30340;&#25928;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#27969;&#24418;&#23398;&#20064;&#26041;&#27861;&#30340;&#19968;&#20010;&#26174;&#33879;&#25361;&#25112;&#26159;&#23427;&#20204;&#32570;&#20047;&#26126;&#30830;&#30340;&#21151;&#33021;&#26144;&#23556;&#65292;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#35299;&#37322;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#36951;&#20256;&#35268;&#21010;&#20197;&#20854;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#21151;&#33021;&#26641;&#30340;&#27169;&#22411;&#32780;&#38395;&#21517;&#65292;&#24050;&#25104;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21033;&#29992;&#22810;&#30446;&#26631;&#36951;&#20256;&#35268;&#21010;&#26469;&#24179;&#34913;&#27969;&#24418;&#36136;&#37327;&#19982;&#23884;&#20837;&#32500;&#24230;&#65292;&#20135;&#29983;&#20102;&#19968;&#31995;&#21015;&#23884;&#20837;&#22823;&#23567;&#19979;&#30340;&#21151;&#33021;&#26144;&#23556;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26144;&#23556;&#26641;&#32463;&#24120;&#21464;&#24471;&#22797;&#26434;&#65292;&#38459;&#30861;&#20102;&#35299;&#37322;&#24615;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#21487;&#35299;&#37322;&#27969;&#24418;&#23398;&#20064;&#30340;&#36951;&#20256;&#35268;&#21010;&#65288;GP-EMaL&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14139v1 Announce Type: cross  Abstract: Manifold learning techniques play a pivotal role in machine learning by revealing lower-dimensional embeddings within high-dimensional data, thus enhancing both the efficiency and interpretability of data analysis by transforming the data into a lower-dimensional representation. However, a notable challenge with current manifold learning methods is their lack of explicit functional mappings, crucial for explainability in many real-world applications. Genetic programming, known for its interpretable functional tree-based models, has emerged as a promising approach to address this challenge. Previous research leveraged multi-objective GP to balance manifold quality against embedding dimensionality, producing functional mappings across a range of embedding sizes. Yet, these mapping trees often became complex, hindering explainability. In response, in this paper, we introduce Genetic Programming for Explainable Manifold Learning (GP-EMaL),
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;mixup&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#38024;&#23545;&#31867;&#20869;&#28151;&#21512;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14137</link><description>&lt;p&gt;
&#36890;&#36807;&#20114;&#34917;&#30340;&#31867;&#20869;&#21644;&#31867;&#38388;Mixup&#25552;&#39640;&#22270;&#20687;&#20998;&#31867;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Image Classification Accuracy through Complementary Intra-Class and Inter-Class Mixup
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14137
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;mixup&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#38024;&#23545;&#31867;&#20869;&#28151;&#21512;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MixUp&#21450;&#20854;&#21464;&#20307;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#36890;&#24120;&#24573;&#30053;&#20102;&#21516;&#19968;&#31867;&#21035;&#20869;&#30340;&#28151;&#21512;&#65288;&#31867;&#20869;Mixup&#65289;&#65292;&#23548;&#33268;&#21516;&#19968;&#31867;&#21035;&#26679;&#26412;&#20043;&#38388;&#30340;&#20851;&#31995;&#34987;&#20302;&#20272;&#12290;&#20854;&#27425;&#65292;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#19981;&#21516;&#31867;&#21035;&#20043;&#38388;&#30340;&#28151;&#21512;&#65288;&#31867;&#38388;Mixup&#65289;&#26377;&#25928;&#22686;&#24378;&#20102;&#31867;&#38388;&#21487;&#20998;&#31163;&#24615;&#65292;&#20294;&#22312;&#36890;&#36807;&#20854;&#28151;&#21512;&#25805;&#20316;&#25913;&#36827;&#31867;&#20869;&#20957;&#32858;&#21147;&#26041;&#38754;&#34920;&#29616;&#19981;&#36275;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;mixup&#26041;&#27861;&#21644;&#19968;&#20010;&#20840;&#38754;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;mixup&#26041;&#27861;&#19987;&#38376;&#38024;&#23545;&#24120;&#24120;&#34987;&#24573;&#35270;&#30340;&#31867;&#20869;Mixup&#65292;&#20197;&#21152;&#24378;&#31867;&#20869;&#20957;&#32858;&#24615;-&#36825;&#26159;&#30446;&#21069;&#30340;mixup&#25216;&#26415;&#27809;&#26377;&#25552;&#20379;&#30340;&#29305;&#24615;&#12290;&#23545;&#20110;&#27599;&#20010;&#23567;&#25209;&#37327;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#23567;&#25209;&#37327;&#20013;&#27599;&#20010;&#31867;&#21035;&#30340;&#26410;&#22686;&#24378;&#21407;&#22987;&#22270;&#20687;&#30340;&#29305;&#24449;&#34920;&#31034;&#26469;&#29983;&#25104;a
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14137v1 Announce Type: cross  Abstract: MixUp and its variants, such as Manifold MixUp, have two key limitations in image classification tasks. First, they often neglect mixing within the same class (intra-class mixup), leading to an underutilization of the relationships among samples within the same class. Second, although these methods effectively enhance inter-class separability by mixing between different classes (inter-class mixup), they fall short in improving intra-class cohesion through their mixing operations, limiting their classification performance. To tackle these issues, we propose a novel mixup method and a comprehensive integrated solution.Our mixup approach specifically targets intra-class mixup, an aspect commonly overlooked, to strengthen intra-class cohesion-a feature not provided by current mixup techniques.For each mini-batch, our method utilizes feature representations of unaugmented original images from each class within the mini-batch to generate a s
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20998;&#27835;&#26041;&#27861;&#23558;&#21464;&#37327;&#20998;&#32452;&#65292;&#25353;&#29031;&#26465;&#20214;&#29420;&#31435;&#20851;&#31995;&#23398;&#20064;&#22240;&#26524;&#22270;&#65292;&#20197;&#25552;&#39640;&#22312;&#26679;&#26412;&#37327;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#30340;&#20272;&#31639;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14125</link><description>&lt;p&gt;
&#20351;&#29992;&#26681;&#31062;&#20851;&#31995;&#23545;&#21464;&#37327;&#36827;&#34892;&#20998;&#32452;&#23398;&#20064;&#22240;&#26524;&#22270;
&lt;/p&gt;
&lt;p&gt;
Learning causal graphs using variable grouping according to ancestral relationship
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14125
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20998;&#27835;&#26041;&#27861;&#23558;&#21464;&#37327;&#20998;&#32452;&#65292;&#25353;&#29031;&#26465;&#20214;&#29420;&#31435;&#20851;&#31995;&#23398;&#20064;&#22240;&#26524;&#22270;&#65292;&#20197;&#25552;&#39640;&#22312;&#26679;&#26412;&#37327;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#30340;&#20272;&#31639;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#26679;&#26412;&#37327;&#30456;&#23545;&#20110;&#21464;&#37327;&#25968;&#37327;&#36739;&#23567;&#26102;&#65292;&#20351;&#29992;&#29616;&#26377;&#26041;&#27861;&#20272;&#31639;&#22240;&#26524;&#22270;&#30340;&#20934;&#30830;&#24615;&#20250;&#38477;&#20302;&#12290;&#26377;&#20123;&#26041;&#27861;&#22312;&#26679;&#26412;&#37327;&#23567;&#20110;&#21464;&#37327;&#25968;&#37327;&#26102;&#24182;&#19981;&#21487;&#34892;&#12290;&#20026;&#20102;&#35268;&#36991;&#36825;&#20123;&#38382;&#39064;&#65292;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#37319;&#29992;&#20998;&#27835;&#26041;&#27861;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#12290;&#20026;&#20102;&#23398;&#20064;&#25972;&#20010;&#22240;&#26524;&#22270;&#65292;&#36825;&#20123;&#26041;&#27861;&#39318;&#20808;&#26681;&#25454;&#21464;&#37327;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#20851;&#31995;&#23558;&#21464;&#37327;&#20998;&#21106;&#25104;&#20960;&#20010;&#23376;&#38598;&#65292;&#28982;&#21518;&#23558;&#24120;&#35268;&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#24212;&#29992;&#20110;&#27599;&#20010;&#23376;&#38598;&#24182;&#21512;&#24182;&#20272;&#35745;&#32467;&#26524;&#12290;&#30001;&#20110;&#20998;&#27835;&#26041;&#27861;&#20943;&#23569;&#20102;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#24212;&#29992;&#30340;&#21464;&#37327;&#25968;&#37327;&#65292;&#22240;&#27492;&#39044;&#35745;&#21487;&#20197;&#25913;&#21892;&#22240;&#26524;&#22270;&#30340;&#20272;&#31639;&#20934;&#30830;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#26679;&#26412;&#37327;&#30456;&#23545;&#36739;&#23567;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14125v1 Announce Type: cross  Abstract: Several causal discovery algorithms have been proposed. However, when the sample size is small relative to the number of variables, the accuracy of estimating causal graphs using existing methods decreases. And some methods are not feasible when the sample size is smaller than the number of variables. To circumvent these problems, some researchers proposed causal structure learning algorithms using divide-and-conquer approaches. For learning the entire causal graph, the approaches first split variables into several subsets according to the conditional independence relationships among the variables, then apply a conventional causal discovery algorithm to each subset and merge the estimated results. Since the divide-and-conquer approach reduces the number of variables to which a causal structure learning algorithm is applied, it is expected to improve the estimation accuracy of causal graphs, especially when the sample size is small rela
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20998;&#26512;&#20102;AI&#24212;&#29992;&#20013;&#20869;&#23384;&#24102;&#23485;&#25104;&#20026;&#20027;&#35201;&#29942;&#39048;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#27169;&#22411;&#26550;&#26500;&#12289;&#35757;&#32451;&#21644;&#37096;&#32626;&#31574;&#30053;&#36827;&#34892;&#37325;&#26032;&#35774;&#35745;&#30340;&#35266;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.14123</link><description>&lt;p&gt;
AI&#19982;&#20869;&#23384;&#22681;
&lt;/p&gt;
&lt;p&gt;
AI and Memory Wall
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14123
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20998;&#26512;&#20102;AI&#24212;&#29992;&#20013;&#20869;&#23384;&#24102;&#23485;&#25104;&#20026;&#20027;&#35201;&#29942;&#39048;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#27169;&#22411;&#26550;&#26500;&#12289;&#35757;&#32451;&#21644;&#37096;&#32626;&#31574;&#30053;&#36827;&#34892;&#37325;&#26032;&#35774;&#35745;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20986;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#26080;&#30417;&#30563;&#35757;&#32451;&#25968;&#25454;&#21487;&#29992;&#24615;&#65292;&#21152;&#19978;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#65292;&#23548;&#33268;&#29992;&#20110;&#26381;&#21153;/&#35757;&#32451;LLMs&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#35745;&#31639;&#38656;&#27714;&#20986;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#28608;&#22686;&#12290;&#28982;&#32780;&#65292;&#20027;&#35201;&#24615;&#33021;&#29942;&#39048;&#26085;&#30410;&#36716;&#21521;&#20869;&#23384;&#24102;&#23485;&#12290;&#22312;&#36807;&#21435;&#30340;20&#24180;&#20013;&#65292;&#26381;&#21153;&#22120;&#30828;&#20214;FLOPS&#30340;&#23792;&#20540;&#27599;2&#24180;&#22686;&#38271;3.0&#20493;&#65292;&#36229;&#36807;&#20102;DRAM&#21644;&#20114;&#36830;&#24102;&#23485;&#30340;&#22686;&#38271;&#65292;&#23427;&#20204;&#20998;&#21035;&#20165;&#27599;2&#24180;&#22686;&#38271;1.6&#20493;&#21644;1.4&#20493;&#12290;&#36825;&#31181;&#19981;&#24179;&#34913;&#20351;&#24471;&#20869;&#23384;&#65292;&#32780;&#38750;&#35745;&#31639;&#65292;&#25104;&#20026;AI&#24212;&#29992;&#20013;&#30340;&#20027;&#35201;&#29942;&#39048;&#65292;&#29305;&#21035;&#26159;&#22312;&#26381;&#21153;&#26041;&#38754;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;Transformer&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#20869;&#23384;&#24102;&#23485;&#22914;&#20309;&#25104;&#20026;&#35299;&#30721;&#22120;&#27169;&#22411;&#30340;&#20027;&#35201;&#29942;&#39048;&#12290;&#25105;&#20204;&#20027;&#24352;&#23545;&#27169;&#22411;&#26550;&#26500;&#12289;&#35757;&#32451;&#21644;&#37096;&#32626;&#31574;&#30053;&#36827;&#34892;&#37325;&#26032;&#35774;&#35745;&#65292;&#20197;&#20811;&#26381;&#36825;&#19968;&#20869;&#23384;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14123v1 Announce Type: new  Abstract: The availability of unprecedented unsupervised training data, along with neural scaling laws, has resulted in an unprecedented surge in model size and compute requirements for serving/training LLMs. However, the main performance bottleneck is increasingly shifting to memory bandwidth. Over the past 20 years, peak server hardware FLOPS has been scaling at 3.0x/2yrs, outpacing the growth of DRAM and interconnect bandwidth, which have only scaled at 1.6 and 1.4 times every 2 years, respectively. This disparity has made memory, rather than compute, the primary bottleneck in AI applications, particularly in serving. Here, we analyze encoder and decoder Transformer models and show how memory bandwidth can become the dominant bottleneck for decoder models. We argue for a redesign in model architecture, training, and deployment strategies to overcome this memory limitation.
&lt;/p&gt;</description></item><item><title>&#32852;&#37030;&#23398;&#20064;&#22312;&#24037;&#19994;&#29289;&#32852;&#32593;&#20013;&#30340;&#24212;&#29992;&#20419;&#36827;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#38544;&#31169;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25552;&#21319;PIUs&#24615;&#33021;&#30340;&#36845;&#20195;&#24133;&#20540;&#21098;&#26525;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2403.14120</link><description>&lt;p&gt;
&#29992;&#36845;&#20195;&#24133;&#20540;&#21098;&#26525;&#25512;&#36827;&#24037;&#19994;&#29289;&#32852;&#32593;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Advancing IIoT with Over-the-Air Federated Learning: The Role of Iterative Magnitude Pruning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14120
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#22312;&#24037;&#19994;&#29289;&#32852;&#32593;&#20013;&#30340;&#24212;&#29992;&#20419;&#36827;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#38544;&#31169;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25552;&#21319;PIUs&#24615;&#33021;&#30340;&#36845;&#20195;&#24133;&#20540;&#21098;&#26525;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;&#29289;&#32852;&#32593;&#65288;IIoT&#65289;&#22312;&#24037;&#19994;4.0&#30340;&#32972;&#26223;&#19979;&#36814;&#26469;&#20102;&#19968;&#31181;&#20114;&#32852;&#30340;&#26234;&#33021;&#35774;&#22791;&#26102;&#20195;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#35265;&#35299;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#34701;&#21512;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#21046;&#36896;&#19994;&#12290; IIoT&#20013;&#19968;&#20010;&#20540;&#24471;&#20851;&#27880;&#30340;&#21457;&#23637;&#26159;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#25972;&#21512;&#65292;&#35813;&#25216;&#26415;&#35299;&#20915;&#20102;&#35774;&#22791;&#20043;&#38388;&#25968;&#25454;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#30340;&#38382;&#39064;&#12290; FL&#20351;&#36793;&#32536;&#20256;&#24863;&#22120;&#65288;&#20063;&#31216;&#20026;&#22806;&#22260;&#26234;&#33021;&#21333;&#20803;&#65288;PIUs&#65289;&#65289;&#33021;&#22815;&#20351;&#29992;&#26412;&#22320;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#21644;&#36866;&#24212;&#65292;&#26080;&#38656;&#26174;&#24335;&#20849;&#20139;&#26426;&#23494;&#25968;&#25454;&#65292;&#20174;&#32780;&#20419;&#36827;&#21327;&#20316;&#20294;&#26426;&#23494;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;PIUs&#36739;&#20302;&#30340;&#20869;&#23384;&#21344;&#29992;&#21644;&#35745;&#31639;&#33021;&#21147;&#22266;&#26377;&#22320;&#38656;&#35201;&#20855;&#26377;&#38750;&#24120;&#32043;&#32039;&#20945;&#23610;&#23544;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#12290;&#21098;&#26525;&#31561;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#21487;&#29992;&#20110;&#36890;&#36807;&#31227;&#38500;&#23545;&#27169;&#22411;&#24615;&#33021;&#24433;&#21709;&#36739;&#23567;&#30340;&#19981;&#24517;&#35201;&#36830;&#25509;&#26469;&#20943;&#23567;DNN&#27169;&#22411;&#30340;&#22823;&#23567;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#26356;&#36866;&#21512;&#26377;&#38480;&#30340;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14120v1 Announce Type: cross  Abstract: The industrial Internet of Things (IIoT) under Industry 4.0 heralds an era of interconnected smart devices where data-driven insights and machine learning (ML) fuse to revolutionize manufacturing. A noteworthy development in IIoT is the integration of federated learning (FL), which addresses data privacy and security among devices. FL enables edge sensors, also known as peripheral intelligence units (PIUs) to learn and adapt using their data locally, without explicit sharing of confidential data, to facilitate a collaborative yet confidential learning process. However, the lower memory footprint and computational power of PIUs inherently require deep neural network (DNN) models that have a very compact size. Model compression techniques such as pruning can be used to reduce the size of DNN models by removing unnecessary connections that have little impact on the model's performance, thus making the models more suitable for the limited 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#36807;&#31243;&#20013;&#36890;&#36807;&#21033;&#29992;CLIP&#30340;&#22266;&#26377;&#23646;&#24615;&#26469;&#25506;&#35752;&#26657;&#20934;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#25552;&#31034;&#36873;&#25321;&#26174;&#33879;&#24433;&#21709;&#20102;CLIP&#20013;&#30340;&#26657;&#20934;&#65292;&#20854;&#20013;&#23548;&#33268;&#26356;&#39640;&#25991;&#26412;&#29305;&#24449;&#31163;&#25955;&#24615;&#30340;&#25552;&#31034;&#20250;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.14119</link><description>&lt;p&gt;
C-TPT&#65306;&#36890;&#36807;&#25991;&#26412;&#29305;&#24449;&#31163;&#25955;&#24615;&#30340;&#26657;&#20934;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#36807;&#31243;&#20013;&#36890;&#36807;&#21033;&#29992;CLIP&#30340;&#22266;&#26377;&#23646;&#24615;&#26469;&#25506;&#35752;&#26657;&#20934;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#25552;&#31034;&#36873;&#25321;&#26174;&#33879;&#24433;&#21709;&#20102;CLIP&#20013;&#30340;&#26657;&#20934;&#65292;&#20854;&#20013;&#23548;&#33268;&#26356;&#39640;&#25991;&#26412;&#29305;&#24449;&#31163;&#25955;&#24615;&#30340;&#25552;&#31034;&#20250;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#27979;&#35797;&#26102;&#36866;&#24212;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#20316;&#20026;&#19968;&#31181;&#22312;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#26041;&#27861;&#12290;&#19968;&#20010;&#20027;&#35201;&#30340;&#20363;&#35777;&#26159;&#26368;&#36817;&#25552;&#20986;&#30340;&#29992;&#20110;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#30340;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25552;&#31034;&#20027;&#35201;&#26159;&#20026;&#20102;&#25552;&#39640;&#20934;&#30830;&#24615;&#32780;&#24320;&#21457;&#30340;&#65292;&#24573;&#35270;&#20102;&#26657;&#20934;&#30340;&#37325;&#35201;&#24615;&#8212;&#8212;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#26657;&#20934;&#26041;&#27861;&#20381;&#36182;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#27979;&#35797;&#26102;&#22330;&#26223;&#19979;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;CLIP&#30340;&#22266;&#26377;&#23646;&#24615;&#65292;&#22312;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#36807;&#31243;&#20013;&#25506;&#35752;&#26657;&#20934;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#35266;&#23519;&#65292;&#25105;&#20204;&#21457;&#29616;&#25552;&#31034;&#36873;&#25321;&#26174;&#33879;&#24433;&#21709;&#20102;CLIP&#20013;&#30340;&#26657;&#20934;&#65292;&#20854;&#20013;&#23548;&#33268;&#26356;&#39640;&#25991;&#26412;&#29305;&#24449;&#31163;&#25955;&#24615;&#30340;&#25552;&#31034;&#20250;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14119v1 Announce Type: cross  Abstract: In deep learning, test-time adaptation has gained attention as a method for model fine-tuning without the need for labeled data. A prime exemplification is the recently proposed test-time prompt tuning for large-scale vision-language models such as CLIP. Unfortunately, these prompts have been mainly developed to improve accuracy, overlooking the importance of calibration-a crucial aspect for quantifying prediction uncertainty. However, traditional calibration methods rely on substantial amounts of labeled data, making them impractical for test-time scenarios. To this end, this paper explores calibration during test-time prompt tuning by leveraging the inherent properties of CLIP. Through a series of observations, we find that the prompt choice significantly affects the calibration in CLIP, where the prompts leading to higher text feature dispersion result in better-calibrated predictions. Introducing the Average Text Feature Dispersion
&lt;/p&gt;</description></item><item><title>HETAL&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;&#21516;&#24577;&#21152;&#23494;&#30340;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#35757;&#32451;&#20013;&#20445;&#25252;&#23458;&#25143;&#38544;&#31169;&#65292;&#23454;&#29616;&#20102;&#21152;&#23494;&#35757;&#32451;&#30340;&#20934;&#30830;&#24615;&#65292;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#21152;&#23494;&#30697;&#38453;&#20056;&#27861;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.14111</link><description>&lt;p&gt;
HETAL&#65306;&#20351;&#29992;&#21516;&#24577;&#21152;&#23494;&#36827;&#34892;&#39640;&#25928;&#30340;&#38544;&#31169;&#20445;&#25252;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
HETAL: Efficient Privacy-preserving Transfer Learning with Homomorphic Encryption
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14111
&lt;/p&gt;
&lt;p&gt;
HETAL&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;&#21516;&#24577;&#21152;&#23494;&#30340;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#35757;&#32451;&#20013;&#20445;&#25252;&#23458;&#25143;&#38544;&#31169;&#65292;&#23454;&#29616;&#20102;&#21152;&#23494;&#35757;&#32451;&#30340;&#20934;&#30830;&#24615;&#65292;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#21152;&#23494;&#30697;&#38453;&#20056;&#27861;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21521;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#28155;&#21152;&#21644;&#24494;&#35843;&#26032;&#30340;&#20998;&#31867;&#23618;&#65292;&#20026;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#39640;&#25928;&#22320;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#34429;&#28982;&#20197;&#24448;&#26377;&#35768;&#22810;&#30740;&#31350;&#25552;&#20986;&#22312;&#26426;&#22120;&#23398;&#20064;&#26381;&#21153;&#26041;&#38754;&#20351;&#29992;&#21516;&#24577;&#21152;&#23494;&#26469;&#35299;&#20915;&#36716;&#31227;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#65292;&#20294;&#22823;&#22810;&#25968;&#30740;&#31350;&#20165;&#20851;&#27880;&#21152;&#23494;&#25512;&#26029;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HETAL&#65292;&#19968;&#31181;&#22522;&#20110;&#21516;&#24577;&#21152;&#23494;&#30340;&#39640;&#25928;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;CKKS&#21516;&#24577;&#21152;&#23494;&#26041;&#26696;&#23545;&#23458;&#25143;&#25968;&#25454;&#36827;&#34892;&#21152;&#23494;&#65292;&#20174;&#32780;&#20445;&#25252;&#23458;&#25143;&#22312;&#35757;&#32451;&#20219;&#21153;&#20013;&#30340;&#38544;&#31169;&#12290;HETAL&#26159;&#31532;&#19968;&#20010;&#20005;&#26684;&#25552;&#20379;&#21152;&#23494;&#35757;&#32451;&#30340;&#23454;&#29992;&#26041;&#26696;&#65292;&#37319;&#29992;&#22522;&#20110;&#39564;&#35777;&#30340;&#26089;&#20572;&#25216;&#26415;&#65292;&#24182;&#23454;&#29616;&#20102;&#38750;&#21152;&#23494;&#35757;&#32451;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21152;&#23494;&#30697;&#38453;&#20056;&#27861;&#31639;&#27861;&#65292;&#36895;&#24230;&#27604;&#29616;&#26377;&#26041;&#27861;&#24555;1.8&#21040;323&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14111v1 Announce Type: cross  Abstract: Transfer learning is a de facto standard method for efficiently training machine learning models for data-scarce problems by adding and fine-tuning new classification layers to a model pre-trained on large datasets. Although numerous previous studies proposed to use homomorphic encryption to resolve the data privacy issue in transfer learning in the machine learning as a service setting, most of them only focused on encrypted inference. In this study, we present HETAL, an efficient Homomorphic Encryption based Transfer Learning algorithm, that protects the client's privacy in training tasks by encrypting the client data using the CKKS homomorphic encryption scheme. HETAL is the first practical scheme that strictly provides encrypted training, adopting validation-based early stopping and achieving the accuracy of nonencrypted training. We propose an efficient encrypted matrix multiplication algorithm, which is 1.8 to 323 times faster th
&lt;/p&gt;</description></item><item><title>HAAM-RL&#26041;&#27861;&#32467;&#21512;&#20102;&#21551;&#21457;&#24335;&#31639;&#27861;&#21160;&#20316;&#23631;&#34109;&#21644;&#38598;&#25104;&#25512;&#26029;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#27773;&#36710;&#21943;&#28422;&#36807;&#31243;&#20013;&#30340;&#39068;&#33394;&#25209;&#22788;&#29702;&#37325;&#26032;&#25490;&#24207;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#21462;&#24471;&#20102;16.25%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.14110</link><description>&lt;p&gt;
&#22522;&#20110;&#21551;&#21457;&#24335;&#31639;&#27861;&#21160;&#20316;&#23631;&#34109;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;HAAM-RL&#65289;&#19982;&#38598;&#25104;&#25512;&#26029;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Heuristic Algorithm-based Action Masking Reinforcement Learning (HAAM-RL) with Ensemble Inference Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14110
&lt;/p&gt;
&lt;p&gt;
HAAM-RL&#26041;&#27861;&#32467;&#21512;&#20102;&#21551;&#21457;&#24335;&#31639;&#27861;&#21160;&#20316;&#23631;&#34109;&#21644;&#38598;&#25104;&#25512;&#26029;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#27773;&#36710;&#21943;&#28422;&#36807;&#31243;&#20013;&#30340;&#39068;&#33394;&#25209;&#22788;&#29702;&#37325;&#26032;&#25490;&#24207;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#21462;&#24471;&#20102;16.25%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HAAM-RL&#65288;&#22522;&#20110;&#21551;&#21457;&#24335;&#31639;&#27861;&#21160;&#20316;&#23631;&#34109;&#30340;&#24378;&#21270;&#23398;&#20064;&#65289;&#30340;&#26032;&#22411;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#27773;&#36710;&#21943;&#28422;&#36807;&#31243;&#20013;&#30340;&#39068;&#33394;&#25209;&#22788;&#29702;&#37325;&#26032;&#25490;&#24207;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#22810;&#20010;&#20851;&#38190;&#25216;&#26415;&#65292;&#21253;&#25324;&#23450;&#21046;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#24418;&#24335;&#21270;&#65292;&#22870;&#21169;&#35774;&#32622;&#21253;&#25324;&#22522;&#20110;&#28508;&#21147;&#30340;&#22870;&#21169;&#22609;&#36896;&#65292;&#20351;&#29992;&#21551;&#21457;&#24335;&#31639;&#27861;&#36827;&#34892;&#21160;&#20316;&#23631;&#34109;&#65288;HAAM-RL&#65289;&#65292;&#20197;&#21450;&#19968;&#20010;&#32467;&#21512;&#22810;&#20010;RL&#27169;&#22411;&#30340;&#38598;&#25104;&#25512;&#26029;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;HAAM-RL&#19982;&#38598;&#25104;&#25512;&#26029;&#26041;&#27861;&#22312;30&#20010;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;16.25%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14110v1 Announce Type: cross  Abstract: This paper presents a novel reinforcement learning (RL) approach called HAAM-RL (Heuristic Algorithm-based Action Masking Reinforcement Learning) for optimizing the color batching re-sequencing problem in automobile painting processes. The existing heuristic algorithms have limitations in adequately reflecting real-world constraints and accurately predicting logistics performance. Our methodology incorporates several key techniques including a tailored Markov Decision Process (MDP) formulation, reward setting including Potential-Based Reward Shaping, action masking using heuristic algorithms (HAAM-RL), and an ensemble inference method that combines multiple RL models. The RL agent is trained and evaluated using FlexSim, a commercial 3D simulation software, integrated with our RL MLOps platform BakingSoDA. Experimental results across 30 scenarios demonstrate that HAAM-RL with an ensemble inference method achieves a 16.25% performance im
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;DouZero&#27169;&#22411;&#20013;&#24341;&#20837;&#27531;&#24046;&#32593;&#32476;&#65292;&#24182;&#25506;&#32034;&#19981;&#21516;&#30340;&#26550;&#26500;&#35774;&#35745;&#65292;&#25105;&#20204;&#26174;&#33879;&#25552;&#39640;&#20102;&#26007;&#22320;&#20027;&#28216;&#25103;&#20013;&#33719;&#32988;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.14102</link><description>&lt;p&gt;
DouRN: &#36890;&#36807;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;DouZero
&lt;/p&gt;
&lt;p&gt;
DouRN: Improving DouZero by Residual Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14102
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;DouZero&#27169;&#22411;&#20013;&#24341;&#20837;&#27531;&#24046;&#32593;&#32476;&#65292;&#24182;&#25506;&#32034;&#19981;&#21516;&#30340;&#26550;&#26500;&#35774;&#35745;&#65292;&#25105;&#20204;&#26174;&#33879;&#25552;&#39640;&#20102;&#26007;&#22320;&#20027;&#28216;&#25103;&#20013;&#33719;&#32988;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#20855;&#26377;&#19981;&#23436;&#20840;&#20449;&#24687;&#30340;&#28216;&#25103;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22312;&#21345;&#29260;&#28216;&#25103;&#26007;&#22320;&#20027;&#26041;&#38754;&#30340;&#34920;&#29616;&#20173;&#28982;&#20196;&#20154;&#19981;&#28385;&#24847;&#12290;&#26007;&#22320;&#20027;&#19981;&#21516;&#20110;&#20256;&#32479;&#28216;&#25103;&#65292;&#23427;&#28041;&#21450;&#19977;&#21517;&#29609;&#23478;&#65292;&#32467;&#21512;&#20102;&#21512;&#20316;&#21644;&#23545;&#25239;&#30340;&#20803;&#32032;&#65292;&#23548;&#33268;&#29366;&#24577;&#31354;&#38388;&#21644;&#21160;&#20316;&#31354;&#38388;&#36739;&#22823;&#12290;2021&#24180;&#65292;&#19968;&#27454;&#21517;&#20026;DouZero&#30340;&#26007;&#22320;&#20027;&#31243;&#24207;&#36890;&#36807;&#21033;&#29992;&#20256;&#32479;&#30340;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65292;&#36229;&#36234;&#20102;&#20197;&#24448;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#27169;&#22411;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#23558;&#27531;&#24046;&#32593;&#32476;&#32435;&#20837;&#27169;&#22411;&#20013;&#65292;&#25506;&#32034;&#19981;&#21516;&#30340;&#26550;&#26500;&#35774;&#35745;&#65292;&#24182;&#36827;&#34892;&#22810;&#35282;&#33394;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#30456;&#21516;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#26174;&#33879;&#25552;&#39640;&#20102;&#33719;&#32988;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21628;&#21483;&#24471;&#20998;&#31995;&#32479;&#65292;&#24110;&#21161;&#20195;&#29702;&#20915;&#23450;&#26159;&#21542;&#25104;&#20026;&#22320;&#20027;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14102v1 Announce Type: new  Abstract: Deep reinforcement learning has made significant progress in games with imperfect information, but its performance in the card game Doudizhu (Chinese Poker/Fight the Landlord) remains unsatisfactory. Doudizhu is different from conventional games as it involves three players and combines elements of cooperation and confrontation, resulting in a large state and action space. In 2021, a Doudizhu program called DouZero\cite{zha2021douzero} surpassed previous models without prior knowledge by utilizing traditional Monte Carlo methods and multilayer perceptrons. Building on this work, our study incorporates residual networks into the model, explores different architectural designs, and conducts multi-role testing. Our findings demonstrate that this model significantly improves the winning rate within the same training time. Additionally, we introduce a call scoring system to assist the agent in deciding whether to become a landlord. With these
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#30340;&#26631;&#31614;&#25991;&#26412;&#23884;&#20837;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;LANDER&#30340;&#26041;&#27861;&#65292;&#21363;&#38754;&#21521;&#32852;&#37030;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#26080;&#25968;&#25454;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;DFKT&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#25968;&#25454;&#26102;&#36935;&#21040;&#30340;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2403.14101</link><description>&lt;p&gt;
&#25991;&#26412;&#22686;&#24378;&#30340;&#38754;&#21521;&#32852;&#37030;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#26080;&#25968;&#25454;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Text-Enhanced Data-free Approach for Federated Class-Incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14101
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#30340;&#26631;&#31614;&#25991;&#26412;&#23884;&#20837;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;LANDER&#30340;&#26041;&#27861;&#65292;&#21363;&#38754;&#21521;&#32852;&#37030;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#26080;&#25968;&#25454;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;DFKT&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#25968;&#25454;&#26102;&#36935;&#21040;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;FCIL&#65289;&#26159;&#19968;&#20010;&#23578;&#26410;&#20805;&#20998;&#25506;&#35752;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#28041;&#21450;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#21160;&#24577;&#28155;&#21152;&#26032;&#31867;&#21035;&#12290;&#22312;&#36825;&#19968;&#39046;&#22495;&#65292;&#26080;&#25968;&#25454;&#30693;&#35782;&#36801;&#31227;&#65288;DFKT&#65289;&#22312;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#26041;&#27861;&#32570;&#20047;DFKT&#19982;&#27169;&#22411;&#35757;&#32451;&#38454;&#27573;&#20043;&#38388;&#30340;&#20851;&#38190;&#21327;&#21516;&#20316;&#29992;&#65292;&#23548;&#33268;DFKT&#22312;&#29983;&#25104;&#26087;&#20219;&#21153;&#27169;&#22411;&#30340;&#38750;&#38170;&#23450;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;LANDER&#65288;&#26631;&#31614;&#25991;&#26412;&#20013;&#24515;&#21270;&#26080;&#25968;&#25454;&#30693;&#35782;&#36801;&#31227;&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#30340;&#26631;&#31614;&#25991;&#26412;&#23884;&#20837;&#65288;LTE&#65289;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#27169;&#22411;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;LTE&#35270;&#20026;&#38170;&#28857;&#65292;&#24182;&#32422;&#26463;&#30456;&#24212;&#35757;&#32451;&#26679;&#26412;&#30340;&#29305;&#24449;&#23884;&#20837;&#22260;&#32469;&#20854;&#21608;&#22260;&#65292;&#20197;&#26356;&#20016;&#23500;&#30340;&#20449;&#24687;&#20016;&#23500;&#21608;&#22260;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14101v1 Announce Type: cross  Abstract: Federated Class-Incremental Learning (FCIL) is an underexplored yet pivotal issue, involving the dynamic addition of new classes in the context of federated learning. In this field, Data-Free Knowledge Transfer (DFKT) plays a crucial role in addressing catastrophic forgetting and data privacy problems. However, prior approaches lack the crucial synergy between DFKT and the model training phases, causing DFKT to encounter difficulties in generating high-quality data from a non-anchored latent space of the old task model. In this paper, we introduce LANDER (Label Text Centered Data-Free Knowledge Transfer) to address this issue by utilizing label text embeddings (LTE) produced by pretrained language models. Specifically, during the model training phase, our approach treats LTE as anchor points and constrains the feature embeddings of corresponding training samples around them, enriching the surrounding area with more meaningful informati
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Data Center Carbon Footprint Reduction (DC-CFR) &#22810;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#26102;&#20248;&#21270;&#25968;&#25454;&#20013;&#24515;&#20197;&#20943;&#23569;&#30899;&#36275;&#36857;&#12290;</title><link>https://arxiv.org/abs/2403.14092</link><description>&lt;p&gt;
&#21487;&#25345;&#32493;&#25968;&#25454;&#20013;&#24515;&#23454;&#26102;&#20943;&#23569;&#30899;&#36275;&#36857;
&lt;/p&gt;
&lt;p&gt;
Carbon Footprint Reduction for Sustainable Data Centers in Real-Time
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14092
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Data Center Carbon Footprint Reduction (DC-CFR) &#22810;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#26102;&#20248;&#21270;&#25968;&#25454;&#20013;&#24515;&#20197;&#20943;&#23569;&#30899;&#36275;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#26174;&#33879;&#22686;&#21152;&#33021;&#28304;&#28040;&#32791;&#65292;&#30899;&#25490;&#25918;&#20302;&#30340;&#21487;&#25345;&#32493;&#25968;&#25454;&#20013;&#24515;&#27491;&#25104;&#20026;&#20840;&#29699;&#25919;&#24220;&#21644;&#20225;&#19994;&#20851;&#27880;&#30340;&#37325;&#28857;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#38656;&#35201;&#22312;&#20919;&#21364;&#21644;IT&#36127;&#36733;&#20013;&#36827;&#34892;&#21151;&#32791;&#20248;&#21270;&#30340;&#33539;&#24335;&#36716;&#21464;&#65292;&#22522;&#20110;&#21487;&#20877;&#29983;&#33021;&#28304;&#22312;&#30005;&#32593;&#20013;&#30340;&#21487;&#29992;&#24615;&#26469;&#35843;&#25972;&#28789;&#27963;&#36127;&#36733;&#65292;&#21033;&#29992;&#25968;&#25454;&#20013;&#24515;&#19981;&#38388;&#26029;&#30005;&#28304;&#20013;&#30340;&#30005;&#27744;&#23384;&#20648;&#65292;&#20351;&#29992;&#21327;&#20316;&#20195;&#29702;&#12290;&#36825;&#20123;&#20248;&#21270;&#31574;&#30053;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#20197;&#21450;&#23427;&#20204;&#23545;&#21464;&#21270;&#30340;&#22806;&#37096;&#22240;&#32032;&#65288;&#22914;&#22825;&#27668;&#21644;&#30005;&#32593;&#30899;&#25490;&#25918;&#24378;&#24230;&#65289;&#30340;&#20381;&#36182;&#20351;&#24471;&#36825;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#30446;&#21069;&#32570;&#20047;&#19968;&#20010;&#33021;&#22815;&#22312;&#21160;&#24577;&#23454;&#38469;&#29615;&#22659;&#20013;&#21516;&#26102;&#20248;&#21270;&#25152;&#26377;&#36825;&#20123;&#30446;&#26631;&#30340;&#23454;&#26102;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#30899;&#36275;&#36857;&#20943;&#23569;&#65288;DC-CFR&#65289;&#22810;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#65292;&#33021;&#22815;&#20248;&#21270;&#22810;&#20010;&#35282;&#24230;&#30340;&#25968;&#25454;&#20013;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14092v1 Announce Type: cross  Abstract: As machine learning workloads significantly increase energy consumption, sustainable data centers with low carbon emissions are becoming a top priority for governments and corporations worldwide. This requires a paradigm shift in optimizing power consumption in cooling and IT loads, shifting flexible loads based on the availability of renewable energy in the power grid, and leveraging battery storage from the uninterrupted power supply in data centers, using collaborative agents. The complex association between these optimization strategies and their dependencies on variable external factors like weather and the power grid carbon intensity makes this a hard problem. Currently, a real-time controller to optimize all these goals simultaneously in a dynamic real-world setting is lacking. We propose a Data Center Carbon Footprint Reduction (DC-CFR) multi-agent Reinforcement Learning (MARL) framework that optimizes data centers for the mult
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21147;&#24341;&#23548;SE(3)&#25193;&#25955;&#27169;&#22411;ConfDiff&#65292;&#29992;&#20110;&#34507;&#30333;&#36136;&#26500;&#35937;&#29983;&#25104;&#65292;&#36890;&#36807;&#32467;&#21512;&#21147;&#24341;&#23548;&#32593;&#32476;&#19982;&#22522;&#20110;&#25968;&#25454;&#30340;&#20998;&#25968;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#34507;&#30333;&#36136;&#26500;&#35937;&#30340;&#20934;&#30830;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2403.14088</link><description>&lt;p&gt;
&#22522;&#20110;&#21147;&#24341;&#23548;SE(3)&#25193;&#25955;&#27169;&#22411;&#30340;&#34507;&#30333;&#36136;&#26500;&#35937;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Protein Conformation Generation via Force-Guided SE(3) Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21147;&#24341;&#23548;SE(3)&#25193;&#25955;&#27169;&#22411;ConfDiff&#65292;&#29992;&#20110;&#34507;&#30333;&#36136;&#26500;&#35937;&#29983;&#25104;&#65292;&#36890;&#36807;&#32467;&#21512;&#21147;&#24341;&#23548;&#32593;&#32476;&#19982;&#22522;&#20110;&#25968;&#25454;&#30340;&#20998;&#25968;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#34507;&#30333;&#36136;&#26500;&#35937;&#30340;&#20934;&#30830;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#30340;&#26500;&#35937;&#26223;&#35266;&#23545;&#20110;&#29702;&#35299;&#22797;&#26434;&#29983;&#29289;&#36807;&#31243;&#20013;&#30340;&#21151;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#22522;&#20110;&#29289;&#29702;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#22914;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#27169;&#25311;&#65292;&#23384;&#22312;&#31232;&#26377;&#20107;&#20214;&#37319;&#26679;&#21644;&#38271;&#26102;&#38388;&#24179;&#34913;&#38382;&#39064;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#19968;&#33324;&#34507;&#30333;&#36136;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#25193;&#25955;&#27169;&#22411;&#65292;&#24050;&#34987;&#24212;&#29992;&#20110;&#29983;&#25104;&#26032;&#39062;&#30340;&#34507;&#30333;&#36136;&#26500;&#35937;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#26041;&#27861;&#26080;&#27861;&#24456;&#22909;&#22320;&#32467;&#21512;&#37325;&#35201;&#30340;&#29289;&#29702;&#20808;&#39564;&#30693;&#35782;&#26469;&#25351;&#23548;&#29983;&#25104;&#36807;&#31243;&#65292;&#23548;&#33268;&#37319;&#26679;&#34507;&#30333;&#36136;&#26500;&#35937;&#19982;&#24179;&#34913;&#20998;&#24067;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#20559;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#34507;&#30333;&#36136;&#26500;&#35937;&#29983;&#25104;&#30340;&#21147;&#24341;&#23548;SE(3)&#25193;&#25955;&#27169;&#22411;ConfDiff&#65292;&#20197;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#36890;&#36807;&#23558;&#21147;&#24341;&#23548;&#32593;&#32476;&#19982;&#19968;&#31995;&#21015;&#22522;&#20110;&#25968;&#25454;&#30340;&#20998;&#25968;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;ConfDiff&#33021;&#22815;&#23454;&#29616;&#23545;&#34507;&#30333;&#36136;&#26500;&#35937;&#30340;&#20934;&#30830;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14088v1 Announce Type: cross  Abstract: The conformational landscape of proteins is crucial to understanding their functionality in complex biological processes. Traditional physics-based computational methods, such as molecular dynamics (MD) simulations, suffer from rare event sampling and long equilibration time problems, hindering their applications in general protein systems. Recently, deep generative modeling techniques, especially diffusion models, have been employed to generate novel protein conformations. However, existing score-based diffusion methods cannot properly incorporate important physical prior knowledge to guide the generation process, causing large deviations in the sampled protein conformations from the equilibrium distribution. In this paper, to overcome these limitations, we propose a force-guided SE(3) diffusion model, ConfDiff, for protein conformation generation. By incorporating a force-guided network with a mixture of data-based score models, Conf
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#22810;&#36830;&#32493;&#20307;&#27169;&#22411;&#65292;&#29992;&#20110;&#25913;&#36827;&#22810;&#23610;&#24230;&#38382;&#39064;&#20013;&#21333;&#19968;&#36830;&#32493;&#20307;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;</title><link>https://arxiv.org/abs/2403.14084</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#22810;&#23380;&#20171;&#36136;&#27169;&#22411;&#29992;&#20110;&#22810;&#23610;&#24230;&#27969;&#21160;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Learning-based Multi-continuum Model for Multiscale Flow Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14084
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#22810;&#36830;&#32493;&#20307;&#27169;&#22411;&#65292;&#29992;&#20110;&#25913;&#36827;&#22810;&#23610;&#24230;&#38382;&#39064;&#20013;&#21333;&#19968;&#36830;&#32493;&#20307;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23610;&#24230;&#38382;&#39064;&#36890;&#24120;&#21487;&#20197;&#36890;&#36807;&#25968;&#20540;&#22343;&#36136;&#21270;&#26469;&#36817;&#20284;&#65292;&#36890;&#36807;&#20855;&#26377;&#26576;&#20123;&#26377;&#25928;&#21442;&#25968;&#30340;&#26041;&#31243;&#26469;&#25429;&#33719;&#21407;&#22987;&#31995;&#32479;&#22312;&#31895;&#32593;&#26684;&#19978;&#30340;&#23439;&#35266;&#34892;&#20026;&#65292;&#20197;&#21152;&#24555;&#27169;&#25311;&#36895;&#24230;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#23610;&#24230;&#20998;&#31163;&#65292;&#24182;&#19988;&#35299;&#30340;&#24322;&#36136;&#24615;&#21487;&#20197;&#36890;&#36807;&#27599;&#20010;&#31895;&#22359;&#20013;&#30340;&#35299;&#30340;&#24179;&#22343;&#20540;&#26469;&#36817;&#20284;&#12290;&#23545;&#20110;&#22797;&#26434;&#30340;&#22810;&#23610;&#24230;&#38382;&#39064;&#65292;&#35745;&#31639;&#30340;&#21333;&#19968;&#26377;&#25928;&#24615;&#29305;&#24615;/&#36830;&#32493;&#20307;&#21487;&#33021;&#19981;&#36275;&#22815;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#22810;&#36830;&#32493;&#20307;&#27169;&#22411;&#65292;&#29992;&#20110;&#20016;&#23500;&#22343;&#36136;&#21270;&#26041;&#31243;&#24182;&#25552;&#39640;&#22810;&#23610;&#24230;&#38382;&#39064;&#21333;&#19968;&#36830;&#32493;&#20307;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#32473;&#23450;&#19968;&#20123;&#25968;&#25454;&#12290;&#19981;&#22833;&#19968;&#33324;&#24615;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#21452;&#36830;&#32493;&#20307;&#30340;&#24773;&#20917;&#12290;&#31532;&#19968;&#20010;&#27969;&#21160;&#26041;&#31243;&#20445;&#30041;&#20102;&#21407;&#22987;&#22343;&#36136;&#21270;&#26041;&#31243;&#30340;&#20449;&#24687;&#65292;&#20855;&#26377;&#39069;&#22806;&#30340;&#20132;&#20114;&#39033;&#12290;&#31532;&#20108;&#20010;&#36830;&#32493;&#20307;&#26159;&#26032;&#24341;&#20837;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14084v1 Announce Type: cross  Abstract: Multiscale problems can usually be approximated through numerical homogenization by an equation with some effective parameters that can capture the macroscopic behavior of the original system on the coarse grid to speed up the simulation. However, this approach usually assumes scale separation and that the heterogeneity of the solution can be approximated by the solution average in each coarse block. For complex multiscale problems, the computed single effective properties/continuum might be inadequate. In this paper, we propose a novel learning-based multi-continuum model to enrich the homogenized equation and improve the accuracy of the single continuum model for multiscale problems with some given data. Without loss of generalization, we consider a two-continuum case. The first flow equation keeps the information of the original homogenized equation with an additional interaction term. The second continuum is newly introduced, and t
&lt;/p&gt;</description></item><item><title>emoDARTS&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;CNN&#21644;&#39034;&#24207;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#25552;&#21319;&#20102;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.14083</link><description>&lt;p&gt;
emoDARTS&#65306;&#32852;&#21512;&#20248;&#21270;CNN&#21644;&#39034;&#24207;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#23454;&#29616;&#20248;&#36234;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
emoDARTS: Joint Optimisation of CNN &amp; Sequential Neural Network Architectures for Superior Speech Emotion Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14083
&lt;/p&gt;
&lt;p&gt;
emoDARTS&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;CNN&#21644;&#39034;&#24207;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#25552;&#21319;&#20102;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;emoDARTS&#65292;&#19968;&#31181;DARTS&#20248;&#21270;&#30340;&#32852;&#21512;CNN&#21644;&#39034;&#24207;&#31070;&#32463;&#32593;&#32476;&#65288;SeqNN&#65306;LSTM&#12289;RNN&#65289;&#26550;&#26500;&#65292;&#20197;&#25552;&#21319;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#24615;&#33021;&#12290;&#30740;&#31350;&#34920;&#26126;&#36873;&#25321;CNN&#21644;LSTM&#32806;&#21512;&#21487;&#20197;&#25913;&#21892;&#24615;&#33021;&#12290;&#34429;&#28982;DARTS&#20808;&#21069;&#24050;&#29992;&#20110;&#29420;&#31435;&#36873;&#25321;CNN&#21644;LSTM&#25805;&#20316;&#65292;&#20294;&#25105;&#20204;&#30340;&#25216;&#26415;&#20026;&#36873;&#25321;&#26368;&#20339;&#27169;&#22411;&#28155;&#21152;&#20102;&#19968;&#31181;&#26032;&#39062;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14083v1 Announce Type: cross  Abstract: Speech Emotion Recognition (SER) is crucial for enabling computers to understand the emotions conveyed in human communication. With recent advancements in Deep Learning (DL), the performance of SER models has significantly improved. However, designing an optimal DL architecture requires specialised knowledge and experimental assessments. Fortunately, Neural Architecture Search (NAS) provides a potential solution for automatically determining the best DL model. The Differentiable Architecture Search (DARTS) is a particularly efficient method for discovering optimal models. This study presents emoDARTS, a DARTS-optimised joint CNN and Sequential Neural Network (SeqNN: LSTM, RNN) architecture that enhances SER performance. The literature supports the selection of CNN and LSTM coupling to improve performance.   While DARTS has previously been used to choose CNN and LSTM operations independently, our technique adds a novel mechanism for sel
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24402;&#19968;&#21270;&#27969;&#36827;&#34892;&#39046;&#22495;&#33258;&#36866;&#24212;&#65292;&#25104;&#21151;&#25913;&#36827;&#20102;$\Lambda$&#20449;&#21495;&#25552;&#21462;&#65292;&#20943;&#23569;&#20102;&#23545;&#20998;&#31867;&#22120;&#36755;&#20986;&#20999;&#21106;&#32447;&#30340;&#20381;&#36182;&#24615;</title><link>https://arxiv.org/abs/2403.14076</link><description>&lt;p&gt;
&#36890;&#36807;&#24402;&#19968;&#21270;&#27969;&#23454;&#29616;&#39046;&#22495;&#33258;&#36866;&#24212;&#25913;&#36827;$\Lambda$&#20449;&#21495;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Improving $\Lambda$ Signal Extraction with Domain Adaptation via Normalizing Flows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14076
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24402;&#19968;&#21270;&#27969;&#36827;&#34892;&#39046;&#22495;&#33258;&#36866;&#24212;&#65292;&#25104;&#21151;&#25913;&#36827;&#20102;$\Lambda$&#20449;&#21495;&#25552;&#21462;&#65292;&#20943;&#23569;&#20102;&#23545;&#20998;&#31867;&#22120;&#36755;&#20986;&#20999;&#21106;&#32447;&#30340;&#20381;&#36182;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24402;&#19968;&#21270;&#27969;&#36827;&#34892;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26032;&#24212;&#29992;&#12290;&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#22522;&#20110;&#27969;&#30340;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;CLAS12&#20013;$\Lambda$&#36229;&#23376;&#20449;&#21495;&#25552;&#21462;&#30340;&#33021;&#21147;&#12290;&#24402;&#19968;&#21270;&#27969;&#21487;&#20197;&#24110;&#21161;&#24314;&#27169;&#25551;&#36848;&#29289;&#29702;&#36807;&#31243;&#30340;&#22797;&#26434;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65292;&#23454;&#29616;&#20107;&#20214;&#29983;&#25104;&#31561;&#29992;&#36884;&#12290;&#34429;&#28982;&#36890;&#36807;&#20351;&#29992;&#20998;&#31867;&#22120;&#32593;&#32476;&#25913;&#36827;&#20102;$\Lambda$&#20449;&#21495;&#25552;&#21462;&#65292;&#20294;&#27169;&#25311;&#21644;&#25968;&#25454;&#39046;&#22495;&#30340;&#24046;&#24322;&#38480;&#21046;&#20102;&#20998;&#31867;&#22120;&#24615;&#33021;&#65307;&#35813;&#30740;&#31350;&#21033;&#29992;&#27969;&#36827;&#34892;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#21644;&#25968;&#25454;&#20043;&#38388;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;&#25105;&#20204;&#25104;&#21151;&#35757;&#32451;&#20102;&#19968;&#20010;&#27969;&#32593;&#32476;&#65292;&#23558;&#38544;&#21547;&#30340;&#29289;&#29702;&#31354;&#38388;&#36716;&#25442;&#20026;&#27491;&#24577;&#20998;&#24067;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#24212;&#29992;&#27969;&#20943;&#23569;&#20102;merit&#22270;&#34920;&#23545;&#20998;&#31867;&#22120;&#36755;&#20986;&#20999;&#21106;&#32447;&#30340;&#20381;&#36182;&#24615;&#65292;&#36825;&#24847;&#21619;&#30528;&#23384;&#22312;&#26356;&#24191;&#27867;&#30340;&#33539;&#22260;&#65292;&#22312;&#35813;&#33539;&#22260;&#20869;&#20999;&#21106;&#20250;&#20135;&#29983;&#31867;&#20284;&#30340;merit&#22270;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14076v1 Announce Type: cross  Abstract: The present study presents a novel application for normalizing flows for domain adaptation. The study investigates the ability of flow based neural networks to improve signal extraction of $\Lambda$ Hyperons at CLAS12. Normalizing Flows can help model complex probability density functions that describe physics processes, enabling uses such as event generation. $\Lambda$ signal extraction has been improved through the use of classifier networks, but differences in simulation and data domains limit classifier performance; this study utilizes the flows for domain adaptation between Monte Carlo simulation and data. We were successful in training a flow network to transform between the latent physics space and a normal distribution. We also found that applying the flows lessened the dependence of the figure of merit on the cut on the classifier output, meaning that there was a broader range where the cut results in a similar figure of merit
&lt;/p&gt;</description></item><item><title>M3&#26159;&#19968;&#20010;&#22810;&#20219;&#21153;&#28151;&#21512;&#30446;&#26631;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20165;&#20381;&#36182;&#23545;&#27604;&#23398;&#20064;&#21487;&#33021;&#23548;&#33268;&#30340;&#27425;&#20248;&#26816;&#32034;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;FEVER&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.14074</link><description>&lt;p&gt;
M3: &#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#22810;&#36339;&#23494;&#38598;&#21477;&#23376;&#26816;&#32034;&#30340;&#22810;&#20219;&#21153;&#28151;&#21512;&#30446;&#26631;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
M3: A Multi-Task Mixed-Objective Learning Framework for Open-Domain Multi-Hop Dense Sentence Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14074
&lt;/p&gt;
&lt;p&gt;
M3&#26159;&#19968;&#20010;&#22810;&#20219;&#21153;&#28151;&#21512;&#30446;&#26631;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20165;&#20381;&#36182;&#23545;&#27604;&#23398;&#20064;&#21487;&#33021;&#23548;&#33268;&#30340;&#27425;&#20248;&#26816;&#32034;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;FEVER&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#23545;&#27604;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#38750;&#24120;&#26377;&#25928;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#24191;&#27867;&#29992;&#20110;&#23494;&#38598;&#26816;&#32034;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#20165;&#20381;&#36182;&#23545;&#27604;&#23398;&#20064;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23613;&#31649;&#35768;&#22810;&#26816;&#32034;&#25968;&#25454;&#38598;&#25903;&#25345;&#21508;&#31181;&#36229;&#36234;&#23545;&#27604;&#23398;&#20064;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#20294;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#22330;&#26223;&#20013;&#39640;&#25928;&#22320;&#32452;&#21512;&#23427;&#20204;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;M3&#65292;&#36825;&#26159;&#19968;&#20010;&#20808;&#36827;&#30340;&#36882;&#24402;&#22810;&#36339;&#23494;&#38598;&#21477;&#23376;&#26816;&#32034;&#31995;&#32479;&#65292;&#23427;&#24314;&#31435;&#22312;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#28151;&#21512;&#30446;&#26631;&#26041;&#27861;&#20043;&#19978;&#65292;&#29992;&#20110;&#23494;&#38598;&#25991;&#26412;&#34920;&#31034;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#19978;&#36848;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#24320;&#25918;&#39046;&#22495;&#20107;&#23454;&#39564;&#35777;&#22522;&#20934;&#25968;&#25454;&#38598;FEVER&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#22312;&#20197;&#19979;&#38142;&#25509;&#33719;&#21462;: https://github.com/TonyBY/M3
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14074v1 Announce Type: cross  Abstract: In recent research, contrastive learning has proven to be a highly effective method for representation learning and is widely used for dense retrieval. However, we identify that relying solely on contrastive learning can lead to suboptimal retrieval performance. On the other hand, despite many retrieval datasets supporting various learning objectives beyond contrastive learning, combining them efficiently in multi-task learning scenarios can be challenging. In this paper, we introduce M3, an advanced recursive Multi-hop dense sentence retrieval system built upon a novel Multi-task Mixed-objective approach for dense text representation learning, addressing the aforementioned challenges. Our approach yields state-of-the-art performance on a large-scale open-domain fact verification benchmark dataset, FEVER. Code and data are available at: https://github.com/TonyBY/M3
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#25277;&#26679;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#23545;&#23457;&#35745;&#35777;&#25454;&#36827;&#34892;&#25277;&#26679;&#30340;&#26041;&#27861;&#65292;&#20197;&#36991;&#20813;&#25277;&#26679;&#20559;&#24046;&#65292;&#20445;&#25345;&#38543;&#26426;&#24615;&#21644;&#21464;&#24322;&#24615;&#65292;&#24182;&#38024;&#23545;&#26356;&#26377;&#39118;&#38505;&#30340;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.14069</link><description>&lt;p&gt;
&#20351;&#29992;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#23545;&#23457;&#35745;&#35777;&#25454;&#36827;&#34892;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Sampling Audit Evidence Using a Naive Bayes Classifier
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14069
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#25277;&#26679;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#23545;&#23457;&#35745;&#35777;&#25454;&#36827;&#34892;&#25277;&#26679;&#30340;&#26041;&#27861;&#65292;&#20197;&#36991;&#20813;&#25277;&#26679;&#20559;&#24046;&#65292;&#20445;&#25345;&#38543;&#26426;&#24615;&#21644;&#21464;&#24322;&#24615;&#65292;&#24182;&#38024;&#23545;&#26356;&#26377;&#39118;&#38505;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv&#65306;2403.14069v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#21488;&#28286;&#30340;&#23457;&#35745;&#24072;&#22312;&#22788;&#29702;&#36807;&#22810;&#30340;&#23457;&#35745;&#25968;&#25454;&#26102;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#21253;&#25324;&#25552;&#21462;&#23457;&#35745;&#35777;&#25454;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#25277;&#26679;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#25512;&#36827;&#20102;&#25277;&#26679;&#25216;&#26415;&#12290;&#36825;&#31181;&#26426;&#22120;&#23398;&#20064;&#38598;&#25104;&#26377;&#21161;&#20110;&#36991;&#20813;&#25277;&#26679;&#20559;&#24046;&#65292;&#20445;&#25345;&#38543;&#26426;&#24615;&#21644;&#21464;&#24322;&#24615;&#65292;&#24182;&#38024;&#23545;&#26356;&#26377;&#39118;&#38505;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#23558;&#25968;&#25454;&#20998;&#31867;&#20026;&#20960;&#20010;&#31867;&#21035;&#12290;&#25509;&#19979;&#26469;&#65292;&#37319;&#29992;&#22522;&#20110;&#29992;&#25143;&#12289;&#22522;&#20110;&#39033;&#30446;&#25110;&#28151;&#21512;&#26041;&#27861;&#26469;&#25552;&#21462;&#23457;&#35745;&#35777;&#25454;&#12290;&#20195;&#34920;&#24615;&#25351;&#25968;&#26159;&#34913;&#37327;&#20854;&#20195;&#34920;&#24615;&#30340;&#20027;&#35201;&#25351;&#26631;&#12290;&#22522;&#20110;&#29992;&#25143;&#30340;&#26041;&#27861;&#23545;&#31216;&#22320;&#22260;&#32469;&#31867;&#21035;&#30340;&#20013;&#20301;&#25968;&#37319;&#26679;&#25968;&#25454;&#20316;&#20026;&#23457;&#35745;&#35777;&#25454;&#12290;&#23427;&#21487;&#33021;&#30456;&#24403;&#20110;&#36135;&#24065;&#21644;&#21464;&#37327;&#25277;&#26679;&#30340;&#32452;&#21512;&#12290;&#22522;&#20110;&#39033;&#30446;&#30340;&#26041;&#27861;&#26681;&#25454;&#21518;&#39564;&#27010;&#29575;&#34920;&#31034;&#38750;&#23545;&#31216;&#25277;&#26679;&#65292;&#20197;&#33719;&#24471;&#26377;&#39118;&#38505;&#30340;&#26679;&#26412;&#20316;&#20026;&#23457;&#35745;&#35777;&#25454;&#12290;&#23427;&#21487;&#33021;&#30456;&#24403;&#20110;&#38750;&#32479;&#35745;&#21644;&#36135;&#24065;&#25277;&#26679;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14069v1 Announce Type: new  Abstract: Taiwan's auditors have suffered from processing excessive audit data, including drawing audit evidence. This study advances sampling techniques by integrating machine learning with sampling. This machine learning integration helps avoid sampling bias, keep randomness and variability, and target risker samples. We first classify data using a Naive Bayes classifier into some classes. Next, a user-based, item-based, or hybrid approach is employed to draw audit evidence. The representativeness index is the primary metric for measuring its representativeness. The user-based approach samples data symmetric around the median of a class as audit evidence. It may be equivalent to a combination of monetary and variable samplings. The item-based approach represents asymmetric sampling based on posterior probabilities for obtaining risky samples as audit evidence. It may be identical to a combination of non-statistical and monetary samplings. Audito
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#24322;&#24120;&#20540;&#30699;&#27491;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#30699;&#27491;&#21644;&#20272;&#35745;&#38598;&#25104;&#21040;&#32852;&#21512;&#20248;&#21270;&#26694;&#26550;&#20013;&#65292;&#21033;&#29992;&#26368;&#20248;&#36755;&#36816;&#21644;&#20985;&#25104;&#26412;&#20989;&#25968;&#26469;&#26816;&#27979;&#21644;&#31227;&#38500;&#24322;&#24120;&#20540;&#65292;&#24182;&#36873;&#25321;&#26368;&#20339;&#20998;&#24067;&#26469;&#25191;&#34892;&#20272;&#35745;&#20219;&#21153;</title><link>https://arxiv.org/abs/2403.14067</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#30340;&#33258;&#21160;&#24322;&#24120;&#20540;&#30699;&#27491;
&lt;/p&gt;
&lt;p&gt;
Automatic Outlier Rectification via Optimal Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14067
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#24322;&#24120;&#20540;&#30699;&#27491;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#30699;&#27491;&#21644;&#20272;&#35745;&#38598;&#25104;&#21040;&#32852;&#21512;&#20248;&#21270;&#26694;&#26550;&#20013;&#65292;&#21033;&#29992;&#26368;&#20248;&#36755;&#36816;&#21644;&#20985;&#25104;&#26412;&#20989;&#25968;&#26469;&#26816;&#27979;&#21644;&#31227;&#38500;&#24322;&#24120;&#20540;&#65292;&#24182;&#36873;&#25321;&#26368;&#20339;&#20998;&#24067;&#26469;&#25191;&#34892;&#20272;&#35745;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#20351;&#29992;&#20855;&#26377;&#20985;&#25104;&#26412;&#20989;&#25968;&#30340;&#26368;&#20248;&#36755;&#36816;&#26469;&#26816;&#27979;&#24322;&#24120;&#20540;&#12290;&#20256;&#32479;&#30340;&#24322;&#24120;&#20540;&#26816;&#27979;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#20004;&#38454;&#27573;&#27969;&#31243;&#65306;&#39318;&#20808;&#26816;&#27979;&#24182;&#31227;&#38500;&#24322;&#24120;&#20540;&#65292;&#28982;&#21518;&#22312;&#28165;&#27905;&#25968;&#25454;&#19978;&#25191;&#34892;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24182;&#27809;&#26377;&#23558;&#24322;&#24120;&#20540;&#31227;&#38500;&#19982;&#20272;&#35745;&#20219;&#21153;&#32852;&#31995;&#36215;&#26469;&#65292;&#30041;&#19979;&#20102;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#24322;&#24120;&#20540;&#30699;&#27491;&#26426;&#21046;&#65292;&#23558;&#30699;&#27491;&#21644;&#20272;&#35745;&#38598;&#25104;&#21040;&#19968;&#20010;&#32852;&#21512;&#20248;&#21270;&#26694;&#26550;&#20013;&#12290;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#20855;&#26377;&#20985;&#25104;&#26412;&#20989;&#25968;&#30340;&#26368;&#20248;&#36755;&#36816;&#36317;&#31163;&#26469;&#26500;&#24314;&#27010;&#29575;&#20998;&#24067;&#31354;&#38388;&#20013;&#30340;&#30699;&#27491;&#38598;&#21512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36873;&#25321;&#22312;&#30699;&#27491;&#38598;&#21512;&#20013;&#30340;&#26368;&#20339;&#20998;&#24067;&#26469;&#25191;&#34892;&#20272;&#35745;&#20219;&#21153;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#24341;&#20837;&#30340;&#20985;&#25104;&#26412;&#20989;&#25968;&#26159;&#20351;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#20855;&#26377;&#20851;&#38190;&#24615;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14067v1 Announce Type: cross  Abstract: In this paper, we propose a novel conceptual framework to detect outliers using optimal transport with a concave cost function. Conventional outlier detection approaches typically use a two-stage procedure: first, outliers are detected and removed, and then estimation is performed on the cleaned data. However, this approach does not inform outlier removal with the estimation task, leaving room for improvement. To address this limitation, we propose an automatic outlier rectification mechanism that integrates rectification and estimation within a joint optimization framework. We take the first step to utilize an optimal transport distance with a concave cost function to construct a rectification set in the space of probability distributions. Then, we select the best distribution within the rectification set to perform the estimation task. Notably, the concave cost function we introduced in this paper is the key to making our estimator e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#27010;&#29575;&#20851;&#31995;&#22411;&#32929;&#24066;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#33021;&#26356;&#22909;&#22320;&#22788;&#29702;&#37329;&#34701;&#25968;&#25454;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14063</link><description>&lt;p&gt;
DiffSTOCK&#65306;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#27010;&#29575;&#20851;&#31995;&#22411;&#32929;&#24066;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
DiffSTOCK: Probabilistic relational Stock Market Predictions using Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14063
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#27010;&#29575;&#20851;&#31995;&#22411;&#32929;&#24066;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#33021;&#26356;&#22909;&#22320;&#22788;&#29702;&#37329;&#34701;&#25968;&#25454;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25512;&#24191;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#29992;&#20110;&#32929;&#24066;&#39044;&#27979;&#21644;&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#24050;&#32463;&#23637;&#31034;&#20102;&#24314;&#27169;&#32929;&#31080;&#38388;&#20851;&#31995;&#29992;&#20110;&#24066;&#22330;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#21151;&#25928;&#65292;&#24182;&#21033;&#29992;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20215;&#20540;&#39044;&#27979;&#21644;&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;&#12290;&#23613;&#31649;&#20196;&#20154;&#20449;&#26381;&#65292;&#36825;&#20123;&#30830;&#23450;&#24615;&#26041;&#27861;&#20173;&#28982;&#26080;&#27861;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#65292;&#21363;&#30001;&#20110;&#37329;&#34701;&#25968;&#25454;&#30340;&#20449;&#22122;&#27604;&#36739;&#20302;&#65292;&#23398;&#20064;&#26377;&#25928;&#30340;&#30830;&#23450;&#24615;&#27169;&#22411;&#30456;&#24403;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30001;&#20110;&#27010;&#29575;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#33021;&#26377;&#25928;&#27169;&#25311;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26356;&#39640;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;(DDPM)&#30340;&#26377;&#25928;&#21033;&#29992;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#21382;&#21490;&#36130;&#21153;&#25351;&#26631;&#21644;&#32929;&#31080;&#38388;&#20851;&#31995;&#25552;&#20379;&#26356;&#22909;&#24066;&#22330;&#39044;&#27979;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14063v1 Announce Type: new  Abstract: In this work, we propose an approach to generalize denoising diffusion probabilistic models for stock market predictions and portfolio management. Present works have demonstrated the efficacy of modeling interstock relations for market time-series forecasting and utilized Graph-based learning models for value prediction and portfolio management. Though convincing, these deterministic approaches still fall short of handling uncertainties i.e., due to the low signal-to-noise ratio of the financial data, it is quite challenging to learn effective deterministic models. Since the probabilistic methods have shown to effectively emulate higher uncertainties for time-series predictions. To this end, we showcase effective utilisation of Denoising Diffusion Probabilistic Models (DDPM), to develop an architecture for providing better market predictions conditioned on the historical financial indicators and inter-stock relations. Additionally, we al
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20551;&#35774;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#37327;&#21270;&#26032;&#26679;&#26412;&#26159;&#21542;&#23646;&#20110;&#20869;&#37096;&#20998;&#24067;&#25110;&#22806;&#37096;&#20998;&#24067;&#65292;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#22914;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2403.14058</link><description>&lt;p&gt;
&#22522;&#20110;&#20551;&#35774;&#30340;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#22806;&#22495;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Hypothesis-Driven Deep Learning for Out of Distribution Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20551;&#35774;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#37327;&#21270;&#26032;&#26679;&#26412;&#26159;&#21542;&#23646;&#20110;&#20869;&#37096;&#20998;&#24067;&#25110;&#22806;&#37096;&#20998;&#24067;&#65292;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#22914;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#36879;&#26126;&#40657;&#30418;&#31995;&#32479;&#30340;&#39044;&#27979;&#32463;&#24120;&#29992;&#20110;&#35832;&#22914;&#21307;&#30103;&#20445;&#20581;&#31561;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#12290;&#23545;&#20110;&#36825;&#31867;&#24212;&#29992;&#65292;&#35780;&#20272;&#27169;&#22411;&#22788;&#29702;&#36229;&#20986;&#35757;&#32451;&#25968;&#25454;&#22495;&#30340;&#26679;&#26412;&#30340;&#26041;&#24335;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#23384;&#22312;&#20960;&#31181;&#24230;&#37327;&#21644;&#27979;&#35797;&#26469;&#26816;&#27979;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20013;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OoD&#65289;&#25968;&#25454;&#21644;&#20998;&#24067;&#20869;&#65288;InD&#65289;&#25968;&#25454;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#22312;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20551;&#35774;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#26032;&#26679;&#26412;&#26159;InD&#36824;&#26159;OoD&#12290;&#32473;&#23450;&#19968;&#20010;&#35757;&#32451;&#36807;&#30340;DNN&#21644;&#19968;&#20123;&#36755;&#20837;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;DNN&#39304;&#36865;&#36755;&#20837;&#24182;&#35745;&#31639;&#19968;&#32452;OoD&#24230;&#37327;&#65292;&#31216;&#20026;&#28508;&#22312;&#21709;&#24212;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;OoD&#26816;&#27979;&#38382;&#39064;&#34920;&#36848;&#20026;&#28508;&#22312;&#21709;&#24212;&#20043;&#38388;&#30340;&#20551;&#35774;&#26816;&#39564;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#25490;&#21015;&#30340;&#37325;&#26032;&#37319;&#26679;&#26469;&#25512;&#26029;&#22312;&#38646;&#20551;&#35774;&#19979;&#35266;&#23519;&#21040;&#30340;&#28508;&#22312;&#21709;&#24212;&#30340;&#26174;&#33879;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14058v1 Announce Type: new  Abstract: Predictions of opaque black-box systems are frequently deployed in high-stakes applications such as healthcare. For such applications, it is crucial to assess how models handle samples beyond the domain of training data. While several metrics and tests exist to detect out-of-distribution (OoD) data from in-distribution (InD) data to a deep neural network (DNN), their performance varies significantly across datasets, models, and tasks, which limits their practical use. In this paper, we propose a hypothesis-driven approach to quantify whether a new sample is InD or OoD. Given a trained DNN and some input, we first feed the input through the DNN and compute an ensemble of OoD metrics, which we term latent responses. We then formulate the OoD detection problem as a hypothesis test between latent responses of different groups, and use permutation-based resampling to infer the significance of the observed latent responses under a null hypothe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#23481;&#37327;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;CVRP&#65289;&#21644;&#32422;&#26463;&#20013;&#24515;&#22522;&#30784;&#32858;&#31867;&#65288;CCBC&#65289;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#23558;CVRP&#31616;&#21270;&#20026;CCBC&#30456;&#24403;&#20110;&#20351;&#29992;&#24120;&#35265;&#30340;&#32858;&#31867;&#31639;&#27861;&#23558;&#20174;&#25351;&#25968;&#22797;&#26434;&#24615;&#36716;&#25442;&#20026;&#22810;&#39033;&#24335;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14013</link><description>&lt;p&gt;
&#26397;&#21521;&#23481;&#37327;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#21644;&#32422;&#26463;&#20013;&#24515;&#22522;&#30784;&#32858;&#31867;&#20043;&#38388;&#30340;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
Towards a connection between the capacitated vehicle routing problem and the constrained centroid-based clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14013
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#23481;&#37327;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;CVRP&#65289;&#21644;&#32422;&#26463;&#20013;&#24515;&#22522;&#30784;&#32858;&#31867;&#65288;CCBC&#65289;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#23558;CVRP&#31616;&#21270;&#20026;CCBC&#30456;&#24403;&#20110;&#20351;&#29992;&#24120;&#35265;&#30340;&#32858;&#31867;&#31639;&#27861;&#23558;&#20174;&#25351;&#25968;&#22797;&#26434;&#24615;&#36716;&#25442;&#20026;&#22810;&#39033;&#24335;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#35299;&#20915;&#23454;&#38469;&#36816;&#34892;&#26102;&#38388;&#20869;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;VRP&#65289;&#26159;&#20132;&#20184;&#31649;&#29702;&#20844;&#21496;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23481;&#37327;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;CVRP&#65289;&#21644;&#32422;&#26463;&#20013;&#24515;&#22522;&#30784;&#32858;&#31867;&#65288;CCBC&#65289;&#20043;&#38388;&#30340;&#29702;&#35770;&#21644;&#23454;&#39564;&#32852;&#31995;&#12290;&#23558;CVRP&#31616;&#21270;&#20026;CCBC&#30456;&#24403;&#20110;&#20351;&#29992;&#24120;&#35265;&#30340;&#32858;&#31867;&#31639;&#27861;&#65288;&#22914;K-means&#65289;&#23558;&#20174;&#25351;&#25968;&#22797;&#26434;&#24615;&#36716;&#25442;&#20026;&#22810;&#39033;&#24335;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#19968;&#39033;&#25506;&#32034;&#24615;&#20998;&#26512;&#65292;&#36890;&#36807;&#35828;&#26126;&#24615;&#30340;&#23567;&#35268;&#27169;&#31034;&#20363;&#31361;&#20986;&#20102;&#36825;&#20004;&#20010;&#38382;&#39064;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21516;&#26102;&#25512;&#23548;&#20102;&#19968;&#20123;&#25968;&#23398;&#30456;&#20851;&#30340;&#20844;&#24335;&#21644;&#24615;&#36136;&#12290;&#22312;&#31532;&#20108;&#23618;&#27425;&#19978;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CCBC&#30340;&#26041;&#27861;&#24182;&#21152;&#20197;&#19968;&#20123;&#22686;&#24378;&#12290;&#25552;&#20986;&#30340;&#26694;&#26550;&#30001;&#19977;&#20010;&#38454;&#27573;&#32452;&#25104;&#12290;&#22312;&#31532;&#19968;&#27493;&#65292;&#32422;&#26463;&#20013;&#24515;&#22522;&#30784;&#32858;&#31867;&#31639;&#27861;&#29983;&#25104;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14013v1 Announce Type: cross  Abstract: Efficiently solving a vehicle routing problem (VRP) in a practical runtime is a critical challenge for delivery management companies. This paper explores both a theoretical and experimental connection between the Capacitated Vehicle Routing Problem (CVRP) and the Constrained Centroid-Based Clustering (CCBC). Reducing a CVRP to a CCBC is a synonym for a transition from an exponential to a polynomial complexity using commonly known algorithms for clustering, i.e K-means. At the beginning, we conduct an exploratory analysis to highlight the existence of such a relationship between the two problems through illustrative small-size examples and simultaneously deduce some mathematically-related formulations and properties. On a second level, the paper proposes a CCBC based approach endowed with some enhancements. The proposed framework consists of three stages. At the first step, a constrained centroid-based clustering algorithm generates fea
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25277;&#26679;&#26041;&#27861;M3ID&#26469;&#20943;&#23569;&#29983;&#25104;&#24335;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#65292;&#36890;&#36807;&#25918;&#22823;&#21442;&#32771;&#22270;&#20687;&#23545;&#35821;&#35328;&#20808;&#39564;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29983;&#25104;&#19982;&#35270;&#35273;&#25552;&#31034;&#30456;&#20851;&#30340;&#26631;&#35760;&#12290;</title><link>https://arxiv.org/abs/2403.14003</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#35270;&#35273;&#20449;&#24687;&#22522;&#30784;&#19979;&#30340;&#24187;&#35273;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Multi-Modal Hallucination Control by Visual Information Grounding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14003
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25277;&#26679;&#26041;&#27861;M3ID&#26469;&#20943;&#23569;&#29983;&#25104;&#24335;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#65292;&#36890;&#36807;&#25918;&#22823;&#21442;&#32771;&#22270;&#20687;&#23545;&#35821;&#35328;&#20808;&#39564;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29983;&#25104;&#19982;&#35270;&#35273;&#25552;&#31034;&#30456;&#20851;&#30340;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#24448;&#24448;&#20250;&#29983;&#25104;&#21548;&#36215;&#26469;&#21512;&#29702;&#30340;&#25991;&#26412;&#31572;&#26696;&#65292;&#20294;&#36825;&#20123;&#31572;&#26696;&#24182;&#19981;&#24635;&#26159;&#19982;&#36755;&#20837;&#22270;&#20687;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#29616;&#35937;&#65292;&#36890;&#24120;&#31216;&#20026;&#8220;&#24187;&#35273;&#8221;&#65292;&#24182;&#34920;&#26126;&#23427;&#28304;&#20110;&#23545;&#35821;&#35328;&#20808;&#39564;&#30340;&#36807;&#24230;&#20381;&#36182;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#38543;&#30528;&#29983;&#25104;&#30340;&#26631;&#35760;&#25968;&#37327;&#22686;&#21152;&#65292;&#23545;&#35270;&#35273;&#25552;&#31034;&#30340;&#20381;&#36182;&#24615;&#20943;&#23569;&#65292;&#36825;&#31181;&#34892;&#20026;&#19982;&#24187;&#35273;&#30340;&#20986;&#29616;&#24378;&#28872;&#30456;&#20851;&#12290;&#20026;&#20102;&#20943;&#23569;&#24187;&#35273;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25277;&#26679;&#26041;&#27861;Multi-Modal Mutual-Information Decoding&#65288;M3ID&#65289;&#29992;&#20110;&#25552;&#31034;&#25918;&#22823;&#12290;M3ID&#22686;&#21152;&#20102;&#21442;&#32771;&#22270;&#20687;&#23545;&#35821;&#35328;&#20808;&#39564;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#26377;&#21033;&#20110;&#29983;&#25104;&#19982;&#35270;&#35273;&#25552;&#31034;&#20855;&#26377;&#26356;&#39640;&#20114;&#20449;&#24687;&#30340;&#26631;&#35760;&#12290;M3ID&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#39044;&#35757;&#32451;&#30340;&#33258;&#22238;&#24402;VLM&#65292;&#22312;&#25512;&#26029;&#38454;&#27573;&#32780;&#26080;&#38656;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#35757;&#32451;&#65292;&#24182;&#19988;&#35745;&#31639;&#24320;&#38144;&#26368;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14003v1 Announce Type: cross  Abstract: Generative Vision-Language Models (VLMs) are prone to generate plausible-sounding textual answers that, however, are not always grounded in the input image. We investigate this phenomenon, usually referred to as "hallucination" and show that it stems from an excessive reliance on the language prior. In particular, we show that as more tokens are generated, the reliance on the visual prompt decreases, and this behavior strongly correlates with the emergence of hallucinations. To reduce hallucinations, we introduce Multi-Modal Mutual-Information Decoding (M3ID), a new sampling method for prompt amplification. M3ID amplifies the influence of the reference image over the language prior, hence favoring the generation of tokens with higher mutual information with the visual prompt. M3ID can be applied to any pre-trained autoregressive VLM at inference time without necessitating further training and with minimal computational overhead. If tra
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#27700;&#19979;&#22522;&#30784;&#35774;&#26045;&#26816;&#27979;&#20013;&#24212;&#29992;&#20027;&#21160;&#23398;&#20064;&#36827;&#34892;&#22270;&#20687;&#20998;&#21106;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#20351;&#29992;&#20114;&#20449;&#24687;&#21644;&#33945;&#29305;&#21345;&#27931;dropout&#35745;&#31639;&#26469;&#25552;&#39640;&#27169;&#22411;&#25928;&#26524;&#65292;&#22312;&#31649;&#36947;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.14002</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#39537;&#21160;&#30340;&#20027;&#21160;&#23398;&#20064;&#22312;&#27700;&#19979;&#26816;&#27979;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Driven Active Learning for Image Segmentation in Underwater Inspection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#27700;&#19979;&#22522;&#30784;&#35774;&#26045;&#26816;&#27979;&#20013;&#24212;&#29992;&#20027;&#21160;&#23398;&#20064;&#36827;&#34892;&#22270;&#20687;&#20998;&#21106;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#20351;&#29992;&#20114;&#20449;&#24687;&#21644;&#33945;&#29305;&#21345;&#27931;dropout&#35745;&#31639;&#26469;&#25552;&#39640;&#27169;&#22411;&#25928;&#26524;&#65292;&#22312;&#31649;&#36947;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#26088;&#22312;&#36873;&#25321;&#26368;&#23567;&#37327;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#19968;&#20010;&#19982;&#25972;&#20010;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#30456;&#20284;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20027;&#21160;&#23398;&#20064;&#22312;&#27700;&#19979;&#22522;&#30784;&#35774;&#26045;&#26816;&#39564;&#20219;&#21153;&#20013;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#28508;&#21147;&#65292;&#36825;&#37324;&#36890;&#24120;&#38656;&#35201;&#25910;&#38598;&#22823;&#37327;&#30340;&#25968;&#25454;&#12290;&#31649;&#36947;&#26816;&#27979;&#22270;&#20687;&#36890;&#24120;&#22312;&#35821;&#20041;&#19978;&#26159;&#37325;&#22797;&#30340;&#65292;&#20294;&#36136;&#37327;&#21364;&#23384;&#22312;&#24456;&#22823;&#21464;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;&#20114;&#20449;&#24687;&#20316;&#20026;&#37319;&#38598;&#20989;&#25968;&#65292;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;dropout&#35745;&#31639;&#12290;&#20026;&#20102;&#35780;&#20272;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;CamVid&#25968;&#25454;&#38598;&#23545;DenseNet&#21644;HyperSeg&#36827;&#34892;&#20102;&#20027;&#21160;&#23398;&#20064;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;50,000&#24352;&#22270;&#20687;&#30340;&#31649;&#36947;&#26816;&#27979;&#25968;&#25454;&#38598;&#23545;HyperSeg&#36827;&#34892;&#35757;&#32451;&#12290;&#23545;&#20110;&#31649;&#36947;&#25968;&#25454;&#38598;&#65292;HyperSeg&#22312;&#20351;&#29992;12.5% &#25968;&#25454;&#26102;&#23454;&#29616;&#20102; 67.5%&#30340;meanIoU&#65292;&#32780;&#22312;&#21516;&#26679;&#25968;&#37327;&#30340;&#38543;&#26426;&#36873;&#25321;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#20165;&#20026;61.4%&#12290;&#36825;&#34920;&#26126;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#23545;&#20998;&#21106;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14002v1 Announce Type: cross  Abstract: Active learning aims to select the minimum amount of data to train a model that performs similarly to a model trained with the entire dataset. We study the potential of active learning for image segmentation in underwater infrastructure inspection tasks, where large amounts of data are typically collected. The pipeline inspection images are usually semantically repetitive but with great variations in quality. We use mutual information as the acquisition function, calculated using Monte Carlo dropout. To assess the effectiveness of the framework, DenseNet and HyperSeg are trained with the CamVid dataset using active learning. In addition, HyperSeg is trained with a pipeline inspection dataset of over 50,000 images. For the pipeline dataset, HyperSeg with active learning achieved 67.5% meanIoU using 12.5% of the data, and 61.4% with the same amount of randomly selected images. This shows that using active learning for segmentation models
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#21183;&#22312;&#33258;&#30001;&#33021;&#35745;&#31639;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#20351;&#29992;&#31561;&#21464;&#24615;&#22270;&#31070;&#32463;&#32593;&#32476;MLPs&#26469;&#20934;&#30830;&#39044;&#27979;&#33258;&#30001;&#33021;&#21644;&#36807;&#28193;&#24577;&#65292;&#32771;&#34385;&#21040;&#20998;&#23376;&#26500;&#22411;&#30340;&#33021;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13952</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30456;&#20114;&#20316;&#29992;&#21183;&#22312;&#33258;&#30001;&#33021;&#35745;&#31639;&#20013;&#30340;&#24212;&#29992;&#32771;&#34385;
&lt;/p&gt;
&lt;p&gt;
Considerations in the use of ML interaction potentials for free energy calculations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13952
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#21183;&#22312;&#33258;&#30001;&#33021;&#35745;&#31639;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#20351;&#29992;&#31561;&#21464;&#24615;&#22270;&#31070;&#32463;&#32593;&#32476;MLPs&#26469;&#20934;&#30830;&#39044;&#27979;&#33258;&#30001;&#33021;&#21644;&#36807;&#28193;&#24577;&#65292;&#32771;&#34385;&#21040;&#20998;&#23376;&#26500;&#22411;&#30340;&#33021;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21183;&#65288;MLPs&#65289;&#20855;&#26377;&#20934;&#30830;&#24314;&#27169;&#20998;&#23376;&#33021;&#37327;&#21644;&#33258;&#30001;&#33021;&#26223;&#35266;&#30340;&#28508;&#21147;&#65292;&#35813;&#20934;&#30830;&#24615;&#21487;&#23218;&#32654;&#37327;&#23376;&#21147;&#23398;&#65292;&#24182;&#20855;&#26377;&#31867;&#20284;&#32463;&#20856;&#27169;&#25311;&#30340;&#25928;&#29575;&#12290;&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#20351;&#29992;&#31561;&#21464;&#24615;&#22270;&#31070;&#32463;&#32593;&#32476;MLPs&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#24314;&#27169;&#24179;&#34913;&#20998;&#23376;&#36712;&#36857;&#20013;&#24050;&#34987;&#35777;&#26126;&#26377;&#25928;&#12290;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;MLPs&#33021;&#21542;&#20934;&#30830;&#39044;&#27979;&#33258;&#30001;&#33021;&#21644;&#36807;&#28193;&#24577;&#65292;&#35201;&#32771;&#34385;&#20998;&#23376;&#26500;&#22411;&#30340;&#33021;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#26816;&#26597;&#20102;&#35757;&#32451;&#25968;&#25454;&#20013;&#38598;&#20307;&#21464;&#37327;&#65288;CVs&#65289;&#30340;&#20998;&#24067;&#22914;&#20309;&#24433;&#21709;MLP&#22312;&#30830;&#23450;&#31995;&#32479;&#33258;&#30001;&#33021;&#38754;&#65288;FES&#65289;&#26102;&#30340;&#20934;&#30830;&#24615;&#65292;&#20351;&#29992;Metadynamics&#27169;&#25311;&#23545;&#19969;&#28919;&#21644;&#19993;&#27688;&#37240;&#20108;&#32957;&#65288;ADP&#65289;&#36827;&#34892;&#23454;&#39564;&#12290;&#35813;&#30740;&#31350;&#28041;&#21450;&#23545;&#22235;&#21313;&#19977;&#20010;MLP&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20013;&#19968;&#21322;&#22522;&#20110;&#32463;&#20856;&#20998;&#23376;&#21160;&#21147;&#23398;&#25968;&#25454;&#65292;&#20854;&#20313;&#30340;&#22522;&#20110;&#20174;&#22836;&#35745;&#31639;&#30340;&#33021;&#37327;&#12290;&#36825;&#20123;MLPs&#36827;&#34892;&#20102;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13952v1 Announce Type: cross  Abstract: Machine learning potentials (MLPs) offer the potential to accurately model the energy and free energy landscapes of molecules with the precision of quantum mechanics and an efficiency similar to classical simulations. This research focuses on using equivariant graph neural networks MLPs due to their proven effectiveness in modeling equilibrium molecular trajectories. A key issue addressed is the capability of MLPs to accurately predict free energies and transition states by considering both the energy and the diversity of molecular configurations. We examined how the distribution of collective variables (CVs) in the training data affects MLP accuracy in determining the free energy surface (FES) of systems, using Metadynamics simulations for butane and alanine dipeptide (ADP). The study involved training forty-three MLPs, half based on classical molecular dynamics data and the rest on ab initio computed energies. The MLPs were trained u
&lt;/p&gt;</description></item><item><title>Evo* 2023&#20250;&#35758;&#25910;&#24405;&#20102;&#20851;&#20110;&#23558;&#19981;&#21516;&#30340;&#29983;&#29289;&#21551;&#21457;&#26041;&#27861;&#65288;&#20027;&#35201;&#26159;&#36827;&#21270;&#35745;&#31639;&#65289;&#24212;&#29992;&#20110;&#35299;&#20915;&#19981;&#21516;&#38382;&#39064;&#65288;&#22823;&#22810;&#20026;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#65289;&#30340;&#30740;&#31350;&#21644;&#21021;&#27493;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.13950</link><description>&lt;p&gt;
Evo* 2023 -- &#26202;&#26399;&#25688;&#35201;&#38598;
&lt;/p&gt;
&lt;p&gt;
Evo* 2023 -- Late-Breaking Abstracts Volume
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13950
&lt;/p&gt;
&lt;p&gt;
Evo* 2023&#20250;&#35758;&#25910;&#24405;&#20102;&#20851;&#20110;&#23558;&#19981;&#21516;&#30340;&#29983;&#29289;&#21551;&#21457;&#26041;&#27861;&#65288;&#20027;&#35201;&#26159;&#36827;&#21270;&#35745;&#31639;&#65289;&#24212;&#29992;&#20110;&#35299;&#20915;&#19981;&#21516;&#38382;&#39064;&#65288;&#22823;&#22810;&#20026;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#65289;&#30340;&#30740;&#31350;&#21644;&#21021;&#27493;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13950v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;&#25688;&#35201;: &#35813;&#21367;&#25910;&#24405;&#20102;&#25552;&#20132;&#32473;&#22312;&#25463;&#20811;&#24067;&#23572;&#35834;&#20030;&#21150;&#30340;Evo* 2023&#20250;&#35758;&#30340;&#26202;&#26399;&#25688;&#35201;&#65292;&#20250;&#35758;&#20110;4&#26376;12&#26085;&#33267;14&#26085;&#20030;&#34892;&#12290;&#36825;&#20123;&#35770;&#25991;&#23637;&#31034;&#20102;&#27491;&#22312;&#36827;&#34892;&#30340;&#30740;&#31350;&#20197;&#21450;&#21021;&#27493;&#32467;&#26524;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#29983;&#29289;&#21551;&#21457;&#26041;&#27861;&#65288;&#20027;&#35201;&#26159;&#36827;&#21270;&#35745;&#31639;&#65289;&#22312;&#35299;&#20915;&#19981;&#21516;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#26159;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13950v1 Announce Type: cross  Abstract: Volume with the Late-Breaking Abstracts submitted to the Evo* 2023 Conference, held in Brno (Czech Republic), from 12 to 14 of April. These papers present ongoing research and preliminary results investigating on the application of different approaches of Bioinspired Methods (mainly Evolutionary Computation) to different problems, most of them real world ones.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#26631;&#20934;&#20998;&#26512;&#36873;&#25321;&#21333;&#20010;&#21453;&#20107;&#23454;&#65292;&#36991;&#20813;&#20102;&#29992;&#25143;&#27979;&#35797;&#22810;&#31181;&#19981;&#21516;&#35299;&#37322;&#26041;&#27861;&#21644;&#20998;&#26512;&#20914;&#31361;&#35299;&#20915;&#26041;&#26696;&#30340;&#22256;&#38590;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#22312;&#22810;&#20010;&#36136;&#37327;&#24230;&#37327;&#19978;&#24471;&#20998;&#24456;&#39640;&#30340;&#22949;&#21327;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.13940</link><description>&lt;p&gt;
&#20174;&#35299;&#37322;&#22120;&#38598;&#21512;&#20013;&#36873;&#25321;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#22810;&#26631;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-criteria approach for selecting an explanation from the set of counterfactuals produced by an ensemble of explainers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#26631;&#20934;&#20998;&#26512;&#36873;&#25321;&#21333;&#20010;&#21453;&#20107;&#23454;&#65292;&#36991;&#20813;&#20102;&#29992;&#25143;&#27979;&#35797;&#22810;&#31181;&#19981;&#21516;&#35299;&#37322;&#26041;&#27861;&#21644;&#20998;&#26512;&#20914;&#31361;&#35299;&#20915;&#26041;&#26696;&#30340;&#22256;&#38590;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#22312;&#22810;&#20010;&#36136;&#37327;&#24230;&#37327;&#19978;&#24471;&#20998;&#24456;&#39640;&#30340;&#22949;&#21327;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#34987;&#24191;&#27867;&#29992;&#20110;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#25552;&#20379;&#33719;&#21462;&#26356;&#29702;&#24819;&#39044;&#27979;&#30340;&#26367;&#20195;&#22330;&#26223;&#12290;&#23427;&#20204;&#21487;&#20197;&#30001;&#22810;&#31181;&#26041;&#27861;&#29983;&#25104;&#65292;&#36825;&#20123;&#26041;&#27861;&#20248;&#21270;&#19981;&#21516;&#12289;&#26377;&#26102;&#26159;&#20914;&#31361;&#30340;&#36136;&#37327;&#24230;&#37327;&#65292;&#24182;&#20135;&#29983;&#23436;&#20840;&#19981;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#35299;&#37322;&#26041;&#27861;&#21644;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#20043;&#19968;&#24182;&#19981;&#26159;&#19968;&#20214;&#23481;&#26131;&#30340;&#20107;&#24773;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#22810;&#38454;&#27573;&#38598;&#25104;&#26041;&#27861;&#65292;&#22522;&#20110;&#22810;&#26631;&#20934;&#20998;&#26512;&#26469;&#36873;&#25321;&#21333;&#20010;&#21453;&#20107;&#23454;&#65292;&#32780;&#19981;&#26159;&#24378;&#36843;&#29992;&#25143;&#27979;&#35797;&#35768;&#22810;&#19981;&#21516;&#30340;&#35299;&#37322;&#26041;&#27861;&#24182;&#20998;&#26512;&#20914;&#31361;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#22949;&#21327;&#26041;&#26696;&#65292;&#22312;&#20960;&#20010;&#27969;&#34892;&#30340;&#36136;&#37327;&#24230;&#37327;&#19978;&#24471;&#20998;&#36739;&#39640;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#25903;&#37197;&#20851;&#31995;&#21644;&#29702;&#24819;&#28857;&#20915;&#31574;&#36741;&#21161;&#26041;&#27861;&#65292;&#20174;&#24085;&#32047;&#25176;&#21069;&#27839;&#20013;&#36873;&#25321;&#19968;&#20010;&#21453;&#20107;&#23454;&#12290;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13940v1 Announce Type: cross  Abstract: Counterfactuals are widely used to explain ML model predictions by providing alternative scenarios for obtaining the more desired predictions. They can be generated by a variety of methods that optimize different, sometimes conflicting, quality measures and produce quite different solutions. However, choosing the most appropriate explanation method and one of the generated counterfactuals is not an easy task. Instead of forcing the user to test many different explanation methods and analysing conflicting solutions, in this paper, we propose to use a multi-stage ensemble approach that will select single counterfactual based on the multiple-criteria analysis. It offers a compromise solution that scores well on several popular quality measures. This approach exploits the dominance relation and the ideal point decision aid method, which selects one counterfactual from the Pareto front. The conducted experiments demonstrated that the propos
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#8220;&#21463;&#38480;&#34892;&#19994;&#8221;&#36827;&#34892;&#33258;&#21160;&#25968;&#25454;&#38598;&#22686;&#24378;&#20197;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#30340;&#26032;&#26426;&#21046;&#65292;&#24182;&#21019;&#24314;&#20102;mb-index&#21644;db-index&#20004;&#20010;&#26032;&#30340;&#20559;&#35265;&#37327;&#21270;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.13925</link><description>&lt;p&gt;
&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#65306;&#37325;&#28857;&#20851;&#27880;&#8220;&#21463;&#38480;&#34892;&#19994;&#8221;&#30340;&#33258;&#21160;&#25968;&#25454;&#38598;&#22686;&#24378;&#21644;&#20559;&#35265;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Reducing Large Language Model Bias with Emphasis on 'Restricted Industries': Automated Dataset Augmentation and Prejudice Quantification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#8220;&#21463;&#38480;&#34892;&#19994;&#8221;&#36827;&#34892;&#33258;&#21160;&#25968;&#25454;&#38598;&#22686;&#24378;&#20197;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#30340;&#26032;&#26426;&#21046;&#65292;&#24182;&#21019;&#24314;&#20102;mb-index&#21644;db-index&#20004;&#20010;&#26032;&#30340;&#20559;&#35265;&#37327;&#21270;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#19981;&#26029;&#22686;&#24378;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#23545;&#20854;&#20135;&#29983;&#20559;&#35265;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#8220;&#21463;&#38480;&#34892;&#19994;&#8221;&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#36890;&#36807;&#25351;&#23450;&#25968;&#25454;&#38598;&#22686;&#24378;&#26469;&#21435;&#20559;&#35265;&#30340;&#26032;&#39062;&#33258;&#21160;&#26426;&#21046;&#12290;&#25105;&#20204;&#36824;&#21019;&#24314;&#20102;&#20004;&#20010;&#26032;&#30340;&#34913;&#37327;&#25351;&#26631;mb-index&#21644;db-index&#26469;&#37327;&#21270;&#20559;&#35265;&#65292;&#32771;&#34385;&#21040;&#20559;&#35265;&#26159;&#30001;&#20869;&#22312;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#20849;&#21516;&#23548;&#33268;&#30340;&#36825;&#19968;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13925v1 Announce Type: cross  Abstract: Despite the growing capabilities of large language models, there exists concerns about the biases they develop. In this paper, we propose a novel, automated mechanism for debiasing through specified dataset augmentation in the lens of bias producers and in the context of 'restricted industries' with limited data. We additionally create two new additional metrics, the mb-index and db-index, to quantify bias, considering the idea that bias occurs due to both intrinsic model architecture and dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#12289;&#25193;&#25955;&#27169;&#22411;&#21644;&#39118;&#26684;&#36716;&#31227;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#30495;&#23454;&#21644;&#20266;&#36896;&#25351;&#32441;&#22270;&#20687;&#65292;&#20445;&#30041;&#20102;&#25351;&#32441;&#30340;&#29420;&#29305;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13916</link><description>&lt;p&gt;
&#20351;&#29992;GAN&#12289;&#25193;&#25955;&#27169;&#22411;&#21644;&#39118;&#26684;&#36716;&#31227;&#25216;&#26415;&#22686;&#24378;&#25351;&#32441;&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Enhancing Fingerprint Image Synthesis with GANs, Diffusion Models, and Style Transfer Techniques
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#12289;&#25193;&#25955;&#27169;&#22411;&#21644;&#39118;&#26684;&#36716;&#31227;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#30495;&#23454;&#21644;&#20266;&#36896;&#25351;&#32441;&#22270;&#20687;&#65292;&#20445;&#30041;&#20102;&#25351;&#32441;&#30340;&#29420;&#29305;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#28041;&#21450;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#30495;&#23454;&#21644;&#20266;&#36896;&#25351;&#32441;&#22270;&#20687;&#65292;&#21516;&#26102;&#20445;&#30041;&#29420;&#29305;&#24615;&#21644;&#22810;&#26679;&#24615;&#31561;&#29305;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#26041;&#27861;&#20174;&#22122;&#22768;&#20013;&#29983;&#25104;&#30495;&#23454;&#25351;&#32441;&#65292;&#24182;&#21033;&#29992;&#22270;&#20687;&#36716;&#25442;&#25216;&#26415;&#23558;&#30495;&#23454;&#25351;&#32441;&#22270;&#20687;&#36716;&#25442;&#25104;&#20266;&#36896;&#25351;&#32441;&#12290;&#20026;&#20102;&#22522;&#20110;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#19981;&#21516;&#31867;&#22411;&#30340;&#20266;&#36896;&#22270;&#20687;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#39118;&#26684;&#36716;&#31227;&#25216;&#26415;&#65292;&#36890;&#36807;&#37197;&#22791;Wasserstein&#24230;&#37327;&#21644;&#26799;&#24230;&#24809;&#32602;&#30340;&#24490;&#29615;&#33258;&#21160;&#32534;&#30721;&#22120;(CycleWGAN-GP)&#26469;&#36991;&#20813;&#27169;&#24335;&#23849;&#28291;&#21644;&#19981;&#31283;&#23450;&#24615;&#12290;&#24403;&#20266;&#36896;&#35757;&#32451;&#25968;&#25454;&#21253;&#21547;&#26126;&#26174;&#30340;&#20266;&#36896;&#29305;&#24449;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#20250;&#23548;&#33268;&#25913;&#36827;&#30340;&#30495;&#23454;&#21040;&#20266;&#36896;&#30340;&#36716;&#25442;&#12290;&#25105;&#20204;&#20027;&#35201;&#36890;&#36807;Fr\'echet Inception Distance (FID)&#21644;False Acceptance Rate (FAR)&#26469;&#35780;&#20272;&#29983;&#25104;&#30340;&#30495;&#23454;&#25351;&#32441;&#22270;&#20687;&#30340;&#22810;&#26679;&#24615;&#21644;&#36924;&#30495;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13916v1 Announce Type: cross  Abstract: We present novel approaches involving generative adversarial networks and diffusion models in order to synthesize high quality, live and spoof fingerprint images while preserving features such as uniqueness and diversity. We generate live fingerprints from noise with a variety of methods, and we use image translation techniques to translate live fingerprint images to spoof. To generate different types of spoof images based on limited training data we incorporate style transfer techniques through a cycle autoencoder equipped with a Wasserstein metric along with Gradient Penalty (CycleWGAN-GP) in order to avoid mode collapse and instability. We find that when the spoof training data includes distinct spoof characteristics, it leads to improved live-to-spoof translation. We assess the diversity and realism of the generated live fingerprint images mainly through the Fr\'echet Inception Distance (FID) and the False Acceptance Rate (FAR). Ou
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22686;&#24378;&#29616;&#23454;&#28436;&#31034;&#26694;&#26550;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#20351;&#38750;&#26426;&#22120;&#20154;&#19987;&#23478;&#29992;&#25143;&#33021;&#22815;&#20026;&#26426;&#22120;&#20154;&#27169;&#20223;&#23398;&#20064;&#25552;&#20379;&#28436;&#31034;&#65292;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#19988;&#22810;&#26679;&#21270;&#30340;&#28436;&#31034;&#25910;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.13910</link><description>&lt;p&gt;
&#25193;&#23637;&#29616;&#23454;&#28436;&#31034;&#30340;&#21487;&#25193;&#23637;&#26426;&#22120;&#20154;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Augmented Reality Demonstrations for Scalable Robot Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13910
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22686;&#24378;&#29616;&#23454;&#28436;&#31034;&#26694;&#26550;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#20351;&#38750;&#26426;&#22120;&#20154;&#19987;&#23478;&#29992;&#25143;&#33021;&#22815;&#20026;&#26426;&#22120;&#20154;&#27169;&#20223;&#23398;&#20064;&#25552;&#20379;&#28436;&#31034;&#65292;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#19988;&#22810;&#26679;&#21270;&#30340;&#28436;&#31034;&#25910;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#20154;&#25191;&#34892;&#28041;&#21450;&#27169;&#20223;&#20154;&#31867;&#28436;&#31034;&#20197;&#33719;&#21462;&#25216;&#33021;&#30340;&#25805;&#32437;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#35201;&#27714;&#29992;&#25143;&#25509;&#21463;&#25805;&#20316;&#30495;&#23454;&#26426;&#22120;&#20154;&#25163;&#33218;&#30340;&#22521;&#35757;&#26469;&#25552;&#20379;&#28436;&#31034;&#65292;&#20854;&#23454;&#29992;&#24615;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#24615;&#35299;&#20915;&#26041;&#26696;&#65306;&#19968;&#31181;&#36741;&#21161;&#22686;&#24378;&#29616;&#23454;&#65288;AR&#65289;&#26694;&#26550;&#29992;&#20110;&#28436;&#31034;&#25910;&#38598;&#65292;&#36171;&#20104;&#38750;&#26426;&#22120;&#20154;&#19987;&#23478;&#29992;&#25143;&#20351;&#29992;HoloLens 2&#31561;&#35774;&#22791;&#20026;&#26426;&#22120;&#20154;IL&#21046;&#20316;&#28436;&#31034;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20026;&#29616;&#23454;&#19990;&#30028;&#30340;&#20219;&#21153;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#21644;&#22810;&#26679;&#21270;&#30340;&#28436;&#31034;&#25910;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19977;&#20010;&#32463;&#20856;&#26426;&#22120;&#20154;&#20219;&#21153;&#65288;&#25235;&#21462;&#12289;&#25512;&#21160;&#21644;&#25342;&#21462;&#25918;&#32622;&#65289;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#30495;&#23454;&#26426;&#22120;&#20154;&#22312;&#22238;&#25918;&#36890;&#36807;AR&#25910;&#38598;&#30340;&#28436;&#31034;&#26102;&#25104;&#21151;&#25191;&#34892;&#27599;&#39033;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13910v1 Announce Type: cross  Abstract: Robot Imitation Learning (IL) is a widely used method for training robots to perform manipulation tasks that involve mimicking human demonstrations to acquire skills. However, its practicality has been limited due to its requirement that users be trained in operating real robot arms to provide demonstrations. This paper presents an innovative solution: an Augmented Reality (AR)-assisted framework for demonstration collection, empowering non-roboticist users to produce demonstrations for robot IL using devices like the HoloLens 2. Our framework facilitates scalable and diverse demonstration collection for real-world tasks. We validate our approach with experiments on three classical robotics tasks: reach, push, and pick-and-place. The real robot performs each task successfully while replaying demonstrations collected via AR.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23545;&#33337;&#33334;&#29123;&#27833;&#28040;&#32791;&#36827;&#34892;&#24314;&#27169;&#65292;&#21019;&#36896;&#20102;&#39044;&#27979;&#21160;&#24577;&#29366;&#24577;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#25104;&#20026;&#35780;&#20272;&#33337;&#33334;&#36816;&#33829;&#29087;&#32451;&#31243;&#24230;&#21644;&#26410;&#26469;&#20248;&#21270;&#31639;&#27861;&#30340;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2403.13909</link><description>&lt;p&gt;
&#22797;&#26434;&#28023;&#27915;&#33322;&#34892;&#30340;&#39034;&#24207;&#24314;&#27169;&#65306;&#20197;&#19968;&#33368;&#23458;&#36816;&#33337;&#20026;&#26696;&#20363;&#30740;&#31350;&#65288;&#23398;&#29983;&#25688;&#35201;&#65289;
&lt;/p&gt;
&lt;p&gt;
Sequential Modeling of Complex Marine Navigation: Case Study on a Passenger Vessel (Student Abstract)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13909
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23545;&#33337;&#33334;&#29123;&#27833;&#28040;&#32791;&#36827;&#34892;&#24314;&#27169;&#65292;&#21019;&#36896;&#20102;&#39044;&#27979;&#21160;&#24577;&#29366;&#24577;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#25104;&#20026;&#35780;&#20272;&#33337;&#33334;&#36816;&#33829;&#29087;&#32451;&#31243;&#24230;&#21644;&#26410;&#26469;&#20248;&#21270;&#31639;&#27861;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#20107;&#34892;&#19994;&#19981;&#26029;&#33268;&#21147;&#20110;&#21487;&#25345;&#32493;&#21457;&#23637;&#65292;&#33268;&#21147;&#20110;&#25506;&#32034;&#20943;&#23569;&#33337;&#33334;&#29123;&#27833;&#28040;&#32791;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#21033;&#29992;&#21152;&#25343;&#22823;&#35199;&#28023;&#23736;&#19968;&#33368;&#28193;&#36718;&#20004;&#24180;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#36890;&#36807;&#21019;&#36896;&#19968;&#20010;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#22522;&#20110;&#21160;&#24577;&#21644;&#38745;&#24577;&#29366;&#24577;&#12289;&#34892;&#21160;&#21644;&#24178;&#25200;&#65292;&#26469;&#39044;&#27979;&#21160;&#24577;&#29366;&#24577;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#26681;&#25454;&#25552;&#20379;&#30340;&#34892;&#21160;&#39044;&#27979;&#21160;&#24577;&#29366;&#24577;&#65292;&#38543;&#21518;&#20316;&#20026;&#35780;&#20272;&#24037;&#20855;&#65292;&#35780;&#20272;&#33337;&#38271;&#25351;&#23548;&#19979;&#28193;&#36718;&#36816;&#33829;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#21478;&#22806;&#65292;&#23427;&#20026;&#26410;&#26469;&#30340;&#20248;&#21270;&#31639;&#27861;&#22880;&#23450;&#22522;&#30784;&#65292;&#20026;&#20915;&#31574;&#36807;&#31243;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#21453;&#39304;&#12290;&#20026;&#20102;&#20419;&#36827;&#26410;&#26469;&#30740;&#31350;&#65292;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;\url{https://github.com/pagand/model_optimze_vessel/tree/AAAI}&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13909v1 Announce Type: new  Abstract: The maritime industry's continuous commitment to sustainability has led to a dedicated exploration of methods to reduce vessel fuel consumption. This paper undertakes this challenge through a machine learning approach, leveraging a real-world dataset spanning two years of a ferry in west coast Canada. Our focus centers on the creation of a time series forecasting model given the dynamic and static states, actions, and disturbances. This model is designed to predict dynamic states based on the actions provided, subsequently serving as an evaluative tool to assess the proficiency of the ferry's operation under the captain's guidance. Additionally, it lays the foundation for future optimization algorithms, providing valuable feedback on decision-making processes. To facilitate future studies, our code is available at \url{https://github.com/pagand/model_optimze_vessel/tree/AAAI}
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#32447;&#24615;&#23454;&#39564;&#35774;&#35745;&#21551;&#21457;&#30340;&#32852;&#37030;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25968;&#25454;&#24066;&#22330;&#20013;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#23454;&#29616;&#26356;&#20302;&#30340;&#39044;&#27979;&#35823;&#24046;&#65292;&#26080;&#38656;&#26631;&#35760;&#30340;&#39564;&#35777;&#25968;&#25454;&#65292;&#26356;&#36866;&#29992;&#20110;&#21435;&#20013;&#24515;&#21270;&#24066;&#22330;&#35774;&#32622;&#12290;</title><link>https://arxiv.org/abs/2403.13893</link><description>&lt;p&gt;
&#36890;&#36807;&#23454;&#39564;&#35774;&#35745;&#23454;&#29616;&#21435;&#20013;&#24515;&#21270;&#25968;&#25454;&#24066;&#22330;&#30340;&#25968;&#25454;&#37319;&#38598;
&lt;/p&gt;
&lt;p&gt;
Data Acquisition via Experimental Design for Decentralized Data Markets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13893
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#32447;&#24615;&#23454;&#39564;&#35774;&#35745;&#21551;&#21457;&#30340;&#32852;&#37030;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25968;&#25454;&#24066;&#22330;&#20013;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#23454;&#29616;&#26356;&#20302;&#30340;&#39044;&#27979;&#35823;&#24046;&#65292;&#26080;&#38656;&#26631;&#35760;&#30340;&#39564;&#35777;&#25968;&#25454;&#65292;&#26356;&#36866;&#29992;&#20110;&#21435;&#20013;&#24515;&#21270;&#24066;&#22330;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#23545;&#20110;&#24403;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#25968;&#25454;&#24066;&#22330;&#36890;&#36807;&#28608;&#21169;&#28508;&#22312;&#30340;&#25968;&#25454;&#21334;&#23478;&#21152;&#20837;&#24066;&#22330;&#30340;&#26041;&#24335;&#26469;&#22686;&#21152;&#25968;&#25454;&#20379;&#24212;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#39046;&#22495;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#32447;&#24615;&#23454;&#39564;&#35774;&#35745;&#21551;&#21457;&#30340;&#21435;&#20013;&#24515;&#21270;&#25968;&#25454;&#36873;&#25321;&#38382;&#39064;&#30340;&#32852;&#37030;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#26631;&#35760;&#30340;&#39564;&#35777;&#25968;&#25454;&#21363;&#21487;&#23454;&#29616;&#26356;&#20302;&#30340;&#39044;&#27979;&#35823;&#24046;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#24555;&#36895;&#21644;&#32852;&#37030;&#30340;&#36807;&#31243;&#20013;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#19968;&#31181;&#30452;&#25509;&#20272;&#35745;&#33719;&#21462;&#25968;&#25454;&#23545;&#20110;&#27979;&#35797;&#38598;&#39044;&#27979;&#30340;&#22909;&#22788;&#30340;&#26041;&#27861;&#29305;&#21035;&#36866;&#29992;&#20110;&#21435;&#20013;&#24515;&#21270;&#24066;&#22330;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13893v1 Announce Type: new  Abstract: Acquiring high-quality training data is essential for current machine learning models. Data markets provide a way to increase the supply of data, particularly in data-scarce domains such as healthcare, by incentivizing potential data sellers to join the market. A major challenge for a data buyer in such a market is selecting the most valuable data points from a data seller. Unlike prior work in data valuation, which assumes centralized data access, we propose a federated approach to the data selection problem that is inspired by linear experimental design. Our proposed data selection method achieves lower prediction error without requiring labeled validation data and can be optimized in a fast and federated procedure. The key insight of our work is that a method that directly estimates the benefit of acquiring data for test set prediction is particularly compatible with a decentralized market setting.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#26469;&#23398;&#20064;&#23545;&#27604;&#21160;&#21147;&#23398;&#65292;&#20197;&#20943;&#23569;&#23545;&#38745;&#33033;&#20869;&#23545;&#27604;&#21058;&#30340;&#20381;&#36182;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13890</link><description>&lt;p&gt;
&#20197;&#22810;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#23545;&#27604;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Towards Learning Contrast Kinetics with Multi-Condition Latent Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13890
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#26469;&#23398;&#20064;&#23545;&#27604;&#21160;&#21147;&#23398;&#65292;&#20197;&#20943;&#23569;&#23545;&#38745;&#33033;&#20869;&#23545;&#27604;&#21058;&#30340;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#23545;&#27604;&#22686;&#24378;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#30340;&#23545;&#27604;&#21058;&#21487;&#20197;&#23450;&#20301;&#32959;&#30244;&#24182;&#35266;&#23519;&#20854;&#23545;&#27604;&#21160;&#21147;&#23398;&#65292;&#36825;&#23545;&#20110;&#30284;&#30151;&#34920;&#24449;&#21644;&#27835;&#30103;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23545;&#27604;&#21058;&#30340;&#20351;&#29992;&#19981;&#20165;&#19982;&#19981;&#33391;&#20581;&#24247;&#39118;&#38505;&#30456;&#20851;&#65292;&#32780;&#19988;&#23545;&#20110;&#24576;&#23381;&#24739;&#32773;&#12289;&#32958;&#21151;&#33021;&#38556;&#30861;&#24739;&#32773;&#25110;&#20854;&#20182;&#19981;&#33391;&#21453;&#24212;&#24739;&#32773;&#23384;&#22312;&#38480;&#21046;&#12290;&#30001;&#20110;&#23545;&#27604;&#21058;&#25668;&#21462;&#26159;&#30149;&#28790;&#24694;&#24615;&#12289;&#30284;&#30151;&#22797;&#21457;&#39118;&#38505;&#21644;&#27835;&#30103;&#21453;&#24212;&#30340;&#20851;&#38190;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#22240;&#27492;&#20943;&#23569;&#38745;&#33033;&#20869;&#23545;&#27604;&#21058;&#30340;&#20381;&#36182;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#36827;&#34892;DCE-MRI&#26102;&#38388;&#24207;&#21015;&#30340;&#33719;&#21462;&#26102;&#38388;&#26465;&#20214;&#22270;&#20687;&#21512;&#25104;&#30340;&#22810;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#12290;&#20026;&#20102;&#35780;&#20272;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#24182;&#39564;&#35777;&#20102;&#22522;&#20110;&#29983;&#29289;&#26631;&#24535;&#29289;&#21464;&#24322;&#24615;&#30340;Fr\'echet&#25918;&#23556;&#32452;&#23398;&#36317;&#31163;&#20316;&#20026;&#22270;&#20687;&#36136;&#37327;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13890v1 Announce Type: cross  Abstract: Contrast agents in dynamic contrast enhanced magnetic resonance imaging allow to localize tumors and observe their contrast kinetics, which is essential for cancer characterization and respective treatment decision-making. However, contrast agent administration is not only associated with adverse health risks, but also restricted for patients during pregnancy, and for those with kidney malfunction, or other adverse reactions. With contrast uptake as key biomarker for lesion malignancy, cancer recurrence risk, and treatment response, it becomes pivotal to reduce the dependency on intravenous contrast agent administration. To this end, we propose a multi-conditional latent diffusion model capable of acquisition time-conditioned image synthesis of DCE-MRI temporal sequences. To evaluate medical image synthesis, we additionally propose and validate the Fr\'echet radiomics distance as an image quality measure based on biomarker variability 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#22270;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65288;STGED&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#25112;&#26415;&#36890;&#20449;&#32593;&#32476;&#65292;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#32593;&#32476;&#29366;&#24577;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#26469;&#29366;&#24577;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.13872</link><description>&lt;p&gt;
&#31354;&#38388;-&#26102;&#38388;&#22270;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#25112;&#26415;&#32593;&#32476;&#26410;&#26469;&#29366;&#24577;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Spatial-Temporal Graph Representation Learning for Tactical Networks Future State Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#22270;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65288;STGED&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#25112;&#26415;&#36890;&#20449;&#32593;&#32476;&#65292;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#32593;&#32476;&#29366;&#24577;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#26469;&#29366;&#24577;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
tbd:&#25112;&#26415;&#33258;&#32452;&#32455;&#32593;&#32476;&#20013;&#30340;&#36164;&#28304;&#20998;&#37197;&#23384;&#22312;&#29420;&#29305;&#25361;&#25112;&#65292;&#22240;&#20026;&#20854;&#21160;&#24577;&#21644;&#22810;&#36339;&#29305;&#24615;&#12290;&#22312;&#36825;&#31181;&#29615;&#22659;&#20013;&#65292;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#32593;&#32476;&#36830;&#25509;&#23545;&#20110;&#26377;&#25928;&#30340;&#36164;&#28304;&#20998;&#37197;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31354;&#38388;-&#26102;&#38388;&#22270;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65288;STGED&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#25112;&#26415;&#36890;&#20449;&#32593;&#32476;&#65292;&#26377;&#25928;&#21033;&#29992;&#32593;&#32476;&#29366;&#24577;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#26469;&#23398;&#20064;&#28508;&#22312;&#30340;&#25112;&#26415;&#34892;&#20026;&#12290;STGED&#23618;&#27425;&#22320;&#21033;&#29992;&#22522;&#20110;&#22270;&#30340;&#27880;&#24847;&#26426;&#21046;&#23545;&#19968;&#31995;&#21015;&#36890;&#20449;&#32593;&#32476;&#29366;&#24577;&#36827;&#34892;&#31354;&#38388;&#32534;&#30721;&#65292;&#21033;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23545;&#29366;&#24577;&#30340;&#28436;&#21464;&#36827;&#34892;&#26102;&#38388;&#32534;&#30721;&#65292;&#24182;&#21033;&#29992;&#20840;&#36830;&#25509;&#21069;&#39304;&#32593;&#32476;&#26469;&#35299;&#30721;&#26410;&#26469;&#29366;&#24577;&#19979;&#30340;&#36830;&#25509;&#24615;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;STGED&#22312;&#19981;&#21516;&#26102;&#38388;&#27493;&#36755;&#20837;&#19979;&#19968;&#30452;&#27604;&#22522;&#32447;&#27169;&#22411;&#34920;&#29616;&#26356;&#20986;&#33394;&#65292;&#33719;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13872v1 Announce Type: new  Abstract: Resource allocation in tactical ad-hoc networks presents unique challenges due to their dynamic and multi-hop nature. Accurate prediction of future network connectivity is essential for effective resource allocation in such environments. In this paper, we introduce the Spatial-Temporal Graph Encoder-Decoder (STGED) framework for Tactical Communication Networks that leverages both spatial and temporal features of network states to learn latent tactical behaviors effectively. STGED hierarchically utilizes graph-based attention mechanism to spatially encode a series of communication network states, leverages a recurrent neural network to temporally encode the evolution of states, and a fully-connected feed-forward network to decode the connectivity in the future state. Through extensive experiments, we demonstrate that STGED consistently outperforms baseline models by large margins across different time-steps input, achieving an accuracy of
&lt;/p&gt;</description></item><item><title>ExMap &#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#32676;&#20307;&#40065;&#26834;&#24615;&#31574;&#30053;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#28909;&#22270;&#25512;&#26029;&#27169;&#22411;&#30340;&#20998;&#31867;&#31574;&#30053;&#65292;&#36890;&#36807;&#32858;&#31867;&#27169;&#22359;&#25512;&#26029;&#20266;&#26631;&#31614;&#26469;&#22686;&#24378;&#20256;&#32479;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13870</link><description>&lt;p&gt;
ExMap&#65306;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#28909;&#22270;&#23454;&#29616;&#23545;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#26080;&#30417;&#30563;&#32676;&#20307;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
ExMap: Leveraging Explainability Heatmaps for Unsupervised Group Robustness to Spurious Correlations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13870
&lt;/p&gt;
&lt;p&gt;
ExMap &#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#32676;&#20307;&#40065;&#26834;&#24615;&#31574;&#30053;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#28909;&#22270;&#25512;&#26029;&#27169;&#22411;&#30340;&#20998;&#31867;&#31574;&#30053;&#65292;&#36890;&#36807;&#32858;&#31867;&#27169;&#22359;&#25512;&#26029;&#20266;&#26631;&#31614;&#26469;&#22686;&#24378;&#20256;&#32479;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13870v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#32452;&#40065;&#26834;&#24615;&#31574;&#30053;&#26088;&#22312;&#20943;&#36731;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30001;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#20135;&#29983;&#30340;&#23398;&#20064;&#20559;&#24046;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#23545;&#32676;&#20307;&#26631;&#31614;&#20998;&#24067;&#30340;&#35775;&#38382;&#65292;&#36825;&#26159;&#32791;&#26102;&#19988;&#26114;&#36149;&#30340;&#12290;&#22240;&#27492;&#65292;&#20154;&#20204;&#23547;&#27714;&#26080;&#30417;&#30563;&#30340;&#32676;&#20307;&#40065;&#26834;&#24615;&#31574;&#30053;&#12290;&#22522;&#20110;&#36825;&#26679;&#19968;&#31181;&#30475;&#27861;&#65306;&#21487;&#20197;&#26681;&#25454;&#21487;&#35299;&#37322;&#24615;&#28909;&#22270;&#20934;&#30830;&#25512;&#26029;&#35757;&#32451;&#27169;&#22411;&#30340;&#20998;&#31867;&#31574;&#30053;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ExMap&#65292;&#36825;&#26159;&#19968;&#31181;&#35774;&#35745;&#29992;&#20110;&#22686;&#24378;&#20256;&#32479;&#20998;&#31867;&#22120;&#20013;&#32676;&#20307;&#40065;&#26834;&#24615;&#30340;&#26080;&#30417;&#30563;&#20004;&#38454;&#27573;&#26426;&#21046;&#12290;ExMap&#21033;&#29992;&#32858;&#31867;&#27169;&#22359;&#26681;&#25454;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#28909;&#22270;&#25512;&#26029;&#20266;&#26631;&#31614;&#65292;&#28982;&#21518;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#36825;&#20123;&#20266;&#26631;&#31614;&#20195;&#26367;&#23454;&#38469;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#39564;&#35777;&#20102;ExMap&#30340;&#26377;&#25928;&#24615;-&#25105;&#20204;&#23637;&#31034;&#23427;&#33021;&#22815;&#24357;&#21512;&#19982;&#30417;&#30563;&#23545;&#24212;&#26041;&#27861;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#20248;&#20110;&#29616;&#26377;&#30340;&#37096;&#20998;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13870v1 Announce Type: cross  Abstract: Group robustness strategies aim to mitigate learned biases in deep learning models that arise from spurious correlations present in their training datasets. However, most existing methods rely on the access to the label distribution of the groups, which is time-consuming and expensive to obtain. As a result, unsupervised group robustness strategies are sought. Based on the insight that a trained model's classification strategies can be inferred accurately based on explainability heatmaps, we introduce ExMap, an unsupervised two stage mechanism designed to enhance group robustness in traditional classifiers. ExMap utilizes a clustering module to infer pseudo-labels based on a model's explainability heatmaps, which are then used during training in lieu of actual labels. Our empirical studies validate the efficacy of ExMap - We demonstrate that it bridges the performance gap with its supervised counterparts and outperforms existing partia
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#33268;&#21147;&#20110;&#21457;&#23637;&#19968;&#20010;&#22312;&#35780;&#20272;&#23433;&#20840;&#20851;&#38190;&#33258;&#20027;&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#65292;&#22312;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#33394;&#30340;&#20851;&#38190;&#24615;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.13869</link><description>&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#26234;&#33021;&#31995;&#32479;&#30340;&#23433;&#20840;&#20851;&#38190;&#31232;&#26377;&#20107;&#20214;&#27010;&#29575;
&lt;/p&gt;
&lt;p&gt;
Accurately Predicting Probabilities of Safety-Critical Rare Events for Intelligent Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13869
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#33268;&#21147;&#20110;&#21457;&#23637;&#19968;&#20010;&#22312;&#35780;&#20272;&#23433;&#20840;&#20851;&#38190;&#33258;&#20027;&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#65292;&#22312;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#33394;&#30340;&#20851;&#38190;&#24615;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#31995;&#32479;&#36234;&#26469;&#36234;&#25104;&#20026;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#28982;&#32780;&#32597;&#35265;&#30340;&#23433;&#20840;&#20851;&#38190;&#20107;&#20214;&#23545;&#23427;&#20204;&#30340;&#23454;&#38469;&#37096;&#32626;&#26500;&#25104;&#20102;&#37325;&#22823;&#28508;&#22312;&#23041;&#32961;&#12290;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#30340;&#20851;&#38190;&#22312;&#20110;&#20934;&#30830;&#39044;&#27979;&#22312;&#32473;&#23450;&#26102;&#38388;&#27493;&#38271;&#20869;&#20174;&#24403;&#21069;&#29366;&#24577;&#21457;&#29983;&#23433;&#20840;&#20851;&#38190;&#20107;&#20214;&#30340;&#27010;&#29575;&#65292;&#19968;&#20010;&#25105;&#20204;&#23450;&#20041;&#20026;&#8220;&#37325;&#35201;&#24615;&#8221;&#30340;&#25351;&#26631;&#12290;&#39044;&#27979;&#37325;&#35201;&#24615;&#30340;&#22797;&#26434;&#24615;&#28304;&#33258;&#20110;&#26497;&#31471;&#25968;&#25454;&#19981;&#24179;&#34913;&#65292;&#36825;&#26159;&#30001;&#39640;&#32500;&#21464;&#37327;&#20013;&#19982;&#32597;&#35265;&#20107;&#20214;&#30456;&#20851;&#32852;&#24341;&#36215;&#30340;&#19968;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#32597;&#35265;&#24615;&#35781;&#21650;&#12290;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#35201;&#20040;&#36807;&#20110;&#20445;&#23432;&#65292;&#35201;&#20040;&#23481;&#26131;&#24573;&#35270;&#23433;&#20840;&#20851;&#38190;&#20107;&#20214;&#65292;&#22240;&#27492;&#24456;&#38590;&#21516;&#26102;&#23454;&#29616;&#39640;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#65292;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#37325;&#35201;&#24615;&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;&#35780;&#20272;&#23433;&#20840;&#20851;&#38190;&#33258;&#20027;&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#65292;&#22312;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13869v1 Announce Type: cross  Abstract: Intelligent systems are increasingly integral to our daily lives, yet rare safety-critical events present significant latent threats to their practical deployment. Addressing this challenge hinges on accurately predicting the probability of safety-critical events occurring within a given time step from the current state, a metric we define as 'criticality'. The complexity of predicting criticality arises from the extreme data imbalance caused by rare events in high dimensional variables associated with the rare events, a challenge we refer to as the curse of rarity. Existing methods tend to be either overly conservative or prone to overlooking safety-critical events, thus struggling to achieve both high precision and recall rates, which severely limits their applicability. This study endeavors to develop a criticality prediction model that excels in both precision and recall rates for evaluating the criticality of safety-critical auton
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#38543;&#26426;&#36882;&#24402;&#26041;&#31243;&#30340;&#27010;&#29575;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#37325;&#23614;&#29305;&#24615;&#65292;&#24182;&#36890;&#36807;i-p&#30697;&#38453;&#29702;&#35770;&#25193;&#23637;&#20102;G\"{u}rb\"{u}zbalaban&#31561;&#20154;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.13868</link><description>&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#36882;&#24402;&#26041;&#31243;&#20998;&#26512;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#37325;&#23614;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Analysing heavy-tail properties of Stochastic Gradient Descent by means of Stochastic Recurrence Equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#38543;&#26426;&#36882;&#24402;&#26041;&#31243;&#30340;&#27010;&#29575;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#37325;&#23614;&#29305;&#24615;&#65292;&#24182;&#36890;&#36807;i-p&#30697;&#38453;&#29702;&#35770;&#25193;&#23637;&#20102;G\"{u}rb\"{u}zbalaban&#31561;&#20154;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#29702;&#35770;&#30340;&#26368;&#36817;&#30740;&#31350;&#20013;&#65292;&#35266;&#23519;&#21040;&#21487;&#20197;&#22312;&#38543;&#26426;&#36882;&#24402;&#30340;&#27010;&#29575;&#26694;&#26550;&#19979;&#30740;&#31350;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#37325;&#23614;&#29305;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;G\"{u}rb\"{u}zbalaban&#31561;&#20154;&#65288;arXiv:2006.04740&#65289;&#32771;&#34385;&#20102;&#19968;&#20010;&#23545;&#24212;&#20110;&#32447;&#24615;&#22238;&#24402;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;SGD&#30340;&#36845;&#20195;&#21487;&#20197;&#36890;&#36807;&#22810;&#21464;&#37327;&#20223;&#23556;&#38543;&#26426;&#36882;&#24402;$X_k=A_k X_{k-1}+B_k$&#26469;&#24314;&#27169;&#65292;&#20854;&#20013;$(A_k, B_k)$&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#23545;&#65292;$A_k$&#26159;&#19968;&#20010;&#38543;&#26426;&#23545;&#31216;&#30697;&#38453;&#65292;$B_k$&#26159;&#19968;&#20010;&#38543;&#26426;&#21521;&#37327;&#12290;&#26412;&#25991;&#23558;&#22238;&#31572;&#24341;&#29992;&#35770;&#25991;&#20013;&#30340;&#20960;&#20010;&#26410;&#35299;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24212;&#29992;&#19981;&#21487;&#32422;-&#36817;&#31471;&#65288;i-p&#65289;&#30697;&#38453;&#29702;&#35770;&#25193;&#23637;&#20182;&#20204;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13868v1 Announce Type: cross  Abstract: In recent works on the theory of machine learning, it has been observed that heavy tail properties of Stochastic Gradient Descent (SGD) can be studied in the probabilistic framework of stochastic recursions. In particular, G\"{u}rb\"{u}zbalaban et al. (arXiv:2006.04740) considered a setup corresponding to linear regression for which iterations of SGD can be modelled by a multivariate affine stochastic recursion $X_k=A_k X_{k-1}+B_k$, for independent and identically distributed pairs $(A_k, B_k)$, where $A_k$ is a random symmetric matrix and $B_k$ is a random vector. In this work, we will answer several open questions of the quoted paper and extend their results by applying the theory of irreducible-proximal (i-p) matrices.
&lt;/p&gt;</description></item><item><title>Capsule&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#26512;&#26102;&#38388;&#24207;&#21015;&#20256;&#24863;&#22120;&#25968;&#25454;&#26102;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#22122;&#22768;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#24471;&#20986;&#20854;&#20316;&#20026;&#22122;&#22768;&#31283;&#23450;&#22120;&#30340;&#26377;&#25928;&#24615;&#65292;&#20987;&#36133;&#20102;&#21407;&#22987;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>https://arxiv.org/abs/2403.13867</link><description>&lt;p&gt;
&#33014;&#22218;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22122;&#22768;&#31283;&#23450;&#22120;
&lt;/p&gt;
&lt;p&gt;
Capsule Neural Networks as Noise Stabilizer for Time Series Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13867
&lt;/p&gt;
&lt;p&gt;
Capsule&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#26512;&#26102;&#38388;&#24207;&#21015;&#20256;&#24863;&#22120;&#25968;&#25454;&#26102;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#22122;&#22768;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#24471;&#20986;&#20854;&#20316;&#20026;&#22122;&#22768;&#31283;&#23450;&#22120;&#30340;&#26377;&#25928;&#24615;&#65292;&#20987;&#36133;&#20102;&#21407;&#22987;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33014;&#22218;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;&#33014;&#22218;&#23558;&#31070;&#32463;&#20803;&#32465;&#23450;&#25104;&#21333;&#20010;&#21521;&#37327;&#24182;&#23398;&#20064;&#20301;&#32622;&#31561;&#21464;&#29305;&#24449;&#65292;&#36825;&#20351;&#23427;&#20204;&#27604;&#21407;&#22987;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26356;&#21152;&#31283;&#20581;&#12290;CapsNets&#37319;&#29992;&#20223;&#23556;&#21464;&#25442;&#30697;&#38453;&#21644;&#21160;&#24577;&#36335;&#30001;&#19982;&#32806;&#21512;&#31995;&#25968;&#26469;&#23398;&#20064;&#31283;&#20581;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;CapsNets&#22312;&#20998;&#26512;&#39640;&#24230;&#25935;&#24863;&#21644;&#22024;&#26434;&#30340;&#26102;&#38388;&#24207;&#21015;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#23637;&#31034;CapsNets&#30340;&#31283;&#20581;&#24615;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#22312;&#22797;&#26434;&#27169;&#24335;&#21644;&#22122;&#22768;&#30340;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#20256;&#24863;&#22120;&#25968;&#25454;&#8212;&#8212;&#24515;&#30005;&#22270;&#25968;&#25454;&#19978;&#19982;&#21407;&#22987;CNN&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#23454;&#35777;&#35777;&#25454;&#65292;&#35777;&#26126;CapsNets&#20316;&#20026;&#22122;&#22768;&#31283;&#23450;&#22120;&#65292;&#22312;&#20351;&#29992;&#24555;&#36895;&#26799;&#24230;&#31526;&#21495;&#26041;&#27861;&#21644;&#19977;&#31181;&#25163;&#21160;&#25915;&#20987;&#65288;&#21253;&#25324;&#20559;&#31227;&#31227;&#20301;&#12289;&#36880;&#28176;&#28418;&#31227;&#21644;&#26102;&#38388;&#28382;&#21518;&#65289;&#30340;&#25163;&#21160;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;&#24635;&#32780;&#35328;&#20043;&#65292;CapsNets&#22312;&#25163;&#21160;&#21644;&#23545;&#25239;&#24615;&#26041;&#38754;&#20248;&#20110;CNNs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13867v1 Announce Type: new  Abstract: Capsule Neural Networks utilize capsules, which bind neurons into a single vector and learn position equivariant features, which makes them more robust than original Convolutional Neural Networks. CapsNets employ an affine transformation matrix and dynamic routing with coupling coefficients to learn robustly. In this paper, we investigate the effectiveness of CapsNets in analyzing highly sensitive and noisy time series sensor data. To demonstrate CapsNets robustness, we compare their performance with original CNNs on electrocardiogram data, a medical time series sensor data with complex patterns and noise. Our study provides empirical evidence that CapsNets function as noise stabilizers, as investigated by manual and adversarial attack experiments using the fast gradient sign method and three manual attacks, including offset shifting, gradual drift, and temporal lagging. In summary, CapsNets outperform CNNs in both manual and adversarial
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25293;&#21334;&#21551;&#21457;&#30340;&#22810;&#20154;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#32531;&#35299;GAN&#30340;&#27169;&#24335;&#22349;&#22604;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.13866</link><description>&lt;p&gt;
&#25293;&#21334;&#21551;&#21457;&#30340;&#22810;&#20154;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
The Bid Picture: Auction-Inspired Multi-player Generative Adversarial Networks Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25293;&#21334;&#21551;&#21457;&#30340;&#22810;&#20154;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#32531;&#35299;GAN&#30340;&#27169;&#24335;&#22349;&#22604;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25293;&#21334;&#21551;&#21457;&#30340;&#22810;&#20154;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#32531;&#35299;GAN&#30340;&#27169;&#24335;&#22349;&#22604;&#38382;&#39064;&#12290;&#27169;&#24335;&#22349;&#22604;&#25351;&#30340;&#26159;&#24403;&#19968;&#20010;&#36807;&#25311;&#21512;&#30340;&#29983;&#25104;&#22120;&#29983;&#25104;&#19968;&#31181;&#26377;&#38480;&#33539;&#22260;&#30340;&#26679;&#26412;&#26102;&#65292;&#36890;&#24120;&#38598;&#20013;&#22312;&#25968;&#25454;&#20998;&#24067;&#30340;&#19968;&#23567;&#37096;&#20998;&#19978;&#12290;&#23613;&#31649;&#29983;&#25104;&#30340;&#26679;&#26412;&#22810;&#26679;&#24615;&#21463;&#38480;&#65292;&#37492;&#21035;&#22120;&#20173;&#28982;&#21487;&#20197;&#34987;&#27450;&#39575;&#65292;&#23558;&#36825;&#20123;&#26679;&#26412;&#35823;&#35748;&#20026;&#26469;&#33258;&#23454;&#38469;&#20998;&#24067;&#30340;&#30495;&#23454;&#26679;&#26412;&#12290;&#22312;&#27809;&#26377;&#22806;&#37096;&#26631;&#20934;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#26080;&#27861;&#22312;&#35757;&#32451;&#38454;&#27573;&#35782;&#21035;&#20854;&#22833;&#36133;&#12290;&#25105;&#20204;&#23558;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#21452;&#26041;&#28216;&#25103;&#25193;&#23637;&#21040;&#22810;&#26041;&#28216;&#25103;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#27599;&#20010;&#27169;&#22411;&#30340;&#20215;&#20540;&#30001;&#20854;&#20182;&#29609;&#23478;&#22312;&#31867;&#20284;&#25293;&#21334;&#30340;&#36807;&#31243;&#20013;&#25552;&#20132;&#30340;&#20986;&#20215;&#20915;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13866v1 Announce Type: cross  Abstract: This article proposes auction-inspired multi-player generative adversarial networks training, which mitigates the mode collapse problem of GANs. Mode collapse occurs when an over-fitted generator generates a limited range of samples, often concentrating on a small subset of the data distribution. Despite the restricted diversity of generated samples, the discriminator can still be deceived into distinguishing these samples as real samples from the actual distribution. In the absence of external standards, a model cannot recognize its failure during the training phase. We extend the two-player game of generative adversarial networks to the multi-player game. During the training, the values of each model are determined by the bids submitted by other players in an auction-like process.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#30446;&#26631;&#33410;&#28857;&#29228;&#21462;&#65292;&#34920;&#26126;&#20854;&#22312;&#20010;&#21035;&#26696;&#20363;&#20013;&#20248;&#20110;&#20256;&#32479;&#20998;&#31867;&#22120;&#65292;&#24182;&#25552;&#20986;&#20102;&#35757;&#32451;&#26679;&#26412;&#22686;&#24378;&#25216;&#26415;&#20197;&#25913;&#36827;&#39044;&#27979;&#22120;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.13865</link><description>&lt;p&gt;
&#31038;&#20132;&#32593;&#32476;&#20013;&#29992;&#20110;&#29228;&#21462;&#30446;&#26631;&#33410;&#28857;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network for Crawling Target Nodes in Social Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#30446;&#26631;&#33410;&#28857;&#29228;&#21462;&#65292;&#34920;&#26126;&#20854;&#22312;&#20010;&#21035;&#26696;&#20363;&#20013;&#20248;&#20110;&#20256;&#32479;&#20998;&#31867;&#22120;&#65292;&#24182;&#25552;&#20986;&#20102;&#35757;&#32451;&#26679;&#26412;&#22686;&#24378;&#25216;&#26415;&#20197;&#25913;&#36827;&#39044;&#27979;&#22120;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#32593;&#32476;&#29228;&#21462;&#26159;&#36817;&#24180;&#26469;&#31215;&#26497;&#30740;&#31350;&#30340;&#28966;&#28857;&#12290;&#20854;&#20013;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#26159;&#22312;&#32473;&#23450;&#29228;&#21462;&#27493;&#25968;&#39044;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#26368;&#21021;&#26410;&#30693;&#30340;&#22270;&#20013;&#25910;&#38598;&#30446;&#26631;&#33410;&#28857;&#12290;&#22522;&#20110;&#20854;&#37096;&#20998;&#24050;&#30693;&#37051;&#22495;&#39044;&#27979;&#33410;&#28857;&#23646;&#24615;&#26159;&#25104;&#21151;&#29228;&#34411;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#37319;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#19982;&#20256;&#32479;&#20998;&#31867;&#22120;&#31454;&#20105;&#65292;&#24182;&#20026;&#20010;&#21035;&#26696;&#20363;&#25928;&#26524;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#26679;&#26412;&#22686;&#24378;&#25216;&#26415;&#65292;&#26377;&#21161;&#20110;&#22312;&#29228;&#21462;&#30340;&#26089;&#26399;&#38454;&#27573;&#20351;&#35757;&#32451;&#38598;&#22810;&#26679;&#21270;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#27979;&#22120;&#30340;&#36136;&#37327;&#12290;&#38024;&#23545;&#19977;&#31181;&#30446;&#26631;&#38598;&#25299;&#25169;&#30340;&#23454;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;GNN&#30340;&#26041;&#27861;&#22312;&#29228;&#21462;&#20219;&#21153;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#20998;&#24067;&#24335;&#30446;&#26631;&#33410;&#28857;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13865v1 Announce Type: cross  Abstract: Social networks crawling is in the focus of active research the last years. One of the challenging task is to collect target nodes in an initially unknown graph given a budget of crawling steps. Predicting a node property based on its partially known neighbourhood is at the heart of a successful crawler. In this paper we adopt graph neural networks for this purpose and show they are competitive to traditional classifiers and are better for individual cases. Additionally we suggest a training sample boosting technique, which helps to diversify the training set at early stages of crawling and thus improves the predictor quality. The experimental study on three types of target set topology indicates GNN based approach has a potential in crawling task, especially in the case of distributed target nodes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#23567;&#22411;&#30740;&#31350;&#25968;&#25454;&#38598;&#36827;&#34892;&#20844;&#24179;&#24615;&#30340;&#26368;&#20248;&#36816;&#36755;&#20462;&#22797;&#21382;&#21490;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#22522;&#20110;&#26368;&#20248;&#36816;&#36755;&#30340;&#20462;&#22797;&#26041;&#26696;&#26469;&#20943;&#23567;&#25903;&#25345;&#22823;&#23567;&#65292;&#23454;&#29616;&#25104;&#26412;&#33410;&#32422;&#21644;&#23545;&#31163;&#26679;&#26412;&#25968;&#25454;&#30340;&#20462;&#22797;&#12290;</title><link>https://arxiv.org/abs/2403.13864</link><description>&lt;p&gt;
&#20351;&#29992;&#23567;&#22411;&#30740;&#31350;&#25968;&#25454;&#38598;&#36827;&#34892;&#20844;&#24179;&#24615;&#30340;&#26368;&#20248;&#36816;&#36755;: &#23545;&#21382;&#21490;&#25968;&#25454;&#36827;&#34892;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Optimal Transport for Fairness: Archival Data Repair using Small Research Data Sets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#23567;&#22411;&#30740;&#31350;&#25968;&#25454;&#38598;&#36827;&#34892;&#20844;&#24179;&#24615;&#30340;&#26368;&#20248;&#36816;&#36755;&#20462;&#22797;&#21382;&#21490;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#22522;&#20110;&#26368;&#20248;&#36816;&#36755;&#30340;&#20462;&#22797;&#26041;&#26696;&#26469;&#20943;&#23567;&#25903;&#25345;&#22823;&#23567;&#65292;&#23454;&#29616;&#25104;&#26412;&#33410;&#32422;&#21644;&#23545;&#31163;&#26679;&#26412;&#25968;&#25454;&#30340;&#20462;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;AI&#27861;&#26696;&#21644;&#20854;&#20182;&#27861;&#35268;&#30340;&#20986;&#21488;&#65292;&#36843;&#20999;&#38656;&#35201;&#20462;&#22797;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#19981;&#20844;&#24179;&#24615;&#30340;&#31639;&#27861;&#12290;&#26412;&#25991;&#23558;&#20844;&#24179;&#24615;&#23450;&#20041;&#20026;&#20445;&#25252;&#23646;&#24615;&#65288;$S$&#65289;&#21644;&#29305;&#24449;&#65288;$X$&#65289;&#22312;&#32473;&#23450;&#26410;&#21463;&#20445;&#25252;&#23646;&#24615;&#65288;$U$&#65289;&#26465;&#20214;&#19979;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#37325;&#35201;&#24773;&#26223;&#65292;&#21363;&#38656;&#35201;&#20462;&#22797;&#22823;&#37327;&#21382;&#21490;&#25968;&#25454;&#65292;&#20294;&#21482;&#33021;&#20351;&#29992;&#20854;&#20013;&#19968;&#23567;&#37096;&#20998;&#24050;&#26631;&#35760;$S|U$&#30340;&#30740;&#31350;&#25968;&#25454;&#12290;&#25105;&#20204;&#21033;&#29992;&#21518;&#32773;&#35774;&#35745;&#22522;&#20110;&#26368;&#20248;&#36816;&#36755;&#65288;OT&#65289;&#30340;&#20462;&#22797;&#26041;&#26696;&#26469;&#25554;&#20540;&#25903;&#25345;&#12290;&#36825;&#20801;&#35768;&#38024;&#23545;&#26631;&#35760;&#12289;&#21382;&#21490;&#25968;&#25454;&#30340;&#20462;&#22797;&#65292;&#21516;&#26102;&#21463;&#21046;&#20110;&#24179;&#31283;&#24615;&#20551;&#35774;&#12290;&#36825;&#26174;&#33879;&#20943;&#23567;&#20102;OT&#26041;&#26696;&#30340;&#25903;&#25345;&#22823;&#23567;&#65292;&#30456;&#24212;&#22320;&#22823;&#24133;&#33410;&#30465;&#20102;&#35774;&#35745;&#25104;&#26412;&#21644;&#39034;&#24207;&#24212;&#29992;&#20110;&#31163;&#26679;&#26412;&#25968;&#25454;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20351;&#29992;&#27169;&#25311;&#21644;&#22522;&#20934;&#25968;&#25454;&#36827;&#34892;&#30340;&#35814;&#32454;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13864v1 Announce Type: new  Abstract: With the advent of the AI Act and other regulations, there is now an urgent need for algorithms that repair unfairness in training data. In this paper, we define fairness in terms of conditional independence between protected attributes ($S$) and features ($X$), given unprotected attributes ($U$). We address the important setting in which torrents of archival data need to be repaired, using only a small proportion of these data, which are $S|U$-labelled (the research data). We use the latter to design optimal transport (OT)-based repair plans on interpolated supports. This allows {\em off-sample}, labelled, archival data to be repaired, subject to stationarity assumptions. It also significantly reduces the size of the supports of the OT plans, with correspondingly large savings in the cost of their design and of their {\em sequential\/} application to the off-sample data. We provide detailed experimental results with simulated and benchm
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;DiffImpute&#65292;&#19968;&#31181;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#34920;&#26684;&#25968;&#25454;&#25554;&#34917;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;&#32570;&#22833;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#23450;&#21046;&#22810;&#20010;&#34920;&#26684;&#21435;&#22122;&#32593;&#32476;&#26469;&#22686;&#24378;&#25554;&#34917;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13863</link><description>&lt;p&gt;
&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#36827;&#34892;&#34920;&#26684;&#25968;&#25454;&#25554;&#34917;&#30340;DiffImpute
&lt;/p&gt;
&lt;p&gt;
DiffImpute: Tabular Data Imputation With Denoising Diffusion Probabilistic Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13863
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;DiffImpute&#65292;&#19968;&#31181;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#34920;&#26684;&#25968;&#25454;&#25554;&#34917;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;&#32570;&#22833;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#23450;&#21046;&#22810;&#20010;&#34920;&#26684;&#21435;&#22122;&#32593;&#32476;&#26469;&#22686;&#24378;&#25554;&#34917;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20294;&#24448;&#24448;&#23384;&#22312;&#32570;&#22833;&#20540;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#28508;&#22312;&#25928;&#29992;&#12290;&#20256;&#32479;&#30340;&#25554;&#20540;&#25216;&#26415;&#36890;&#24120;&#20250;&#20135;&#29983;&#27425;&#20248;&#32467;&#26524;&#65292;&#24182;&#32473;&#21518;&#32493;&#24314;&#27169;&#20219;&#21153;&#24102;&#26469;&#37325;&#22823;&#35745;&#31639;&#36127;&#25285;&#65292;&#23548;&#33268;&#20986;&#29616;&#19981;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DiffImpute&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;DiffImpute&#22312;&#23436;&#25972;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#30830;&#20445;&#21487;&#20197;&#20026;&#32570;&#22833;&#26465;&#30446;&#29983;&#25104;&#21487;&#20449;&#30340;&#25554;&#34917;&#65292;&#32780;&#19981;&#20250;&#30772;&#22351;&#29616;&#26377;&#25968;&#25454;&#30340;&#30495;&#23454;&#24615;&#12290;&#21019;&#26032;&#22320;&#65292;&#23427;&#21487;&#20197;&#24212;&#29992;&#20110;&#32570;&#22833;&#23436;&#20840;&#38543;&#26426;&#65288;MCAR&#65289;&#21644;&#32570;&#22833;&#38543;&#26426;&#65288;MAR&#65289;&#30340;&#21508;&#31181;&#24773;&#22659;&#12290;&#20026;&#20102;&#26377;&#25928;&#22788;&#29702;DDPM&#20013;&#30340;&#34920;&#26684;&#29305;&#24449;&#65292;&#25105;&#20204;&#23450;&#21046;&#20102;&#22235;&#20010;&#34920;&#26684;&#21435;&#22122;&#32593;&#32476;&#65292;&#28085;&#30422;MLP&#12289;ResNet&#12289;Transformer&#21644;U-Net&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;Harmonization&#26469;&#22686;&#24378;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#21644;&#25554;&#34917;&#25968;&#25454;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13863v1 Announce Type: cross  Abstract: Tabular data plays a crucial role in various domains but often suffers from missing values, thereby curtailing its potential utility. Traditional imputation techniques frequently yield suboptimal results and impose substantial computational burdens, leading to inaccuracies in subsequent modeling tasks. To address these challenges, we propose DiffImpute, a novel Denoising Diffusion Probabilistic Model (DDPM). Specifically, DiffImpute is trained on complete tabular datasets, ensuring that it can produce credible imputations for missing entries without undermining the authenticity of the existing data. Innovatively, it can be applied to various settings of Missing Completely At Random (MCAR) and Missing At Random (MAR). To effectively handle the tabular features in DDPM, we tailor four tabular denoising networks, spanning MLP, ResNet, Transformer, and U-Net. We also propose Harmonization to enhance coherence between observed and imputed d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20809;&#30005;&#20108;&#26497;&#31649;&#25968;&#25454;&#36827;&#34892;&#36880;&#23618;&#26816;&#27979;LPBF&#20013;&#36807;&#28909;&#24322;&#24120;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#25104;&#26412;&#25935;&#24863;&#23398;&#20064;&#22788;&#29702;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.13861</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21033;&#29992;&#20809;&#30005;&#20108;&#26497;&#31649;&#25968;&#25454;&#22312;LPBF&#20013;&#36880;&#23618;&#26816;&#27979;&#36807;&#28909;&#24322;&#24120;
&lt;/p&gt;
&lt;p&gt;
Machine Learning-based Layer-wise Detection of Overheating Anomaly in LPBF using Photodiode Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20809;&#30005;&#20108;&#26497;&#31649;&#25968;&#25454;&#36827;&#34892;&#36880;&#23618;&#26816;&#27979;LPBF&#20013;&#36807;&#28909;&#24322;&#24120;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#25104;&#26412;&#25935;&#24863;&#23398;&#20064;&#22788;&#29702;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#28909;&#24322;&#24120;&#26816;&#27979;&#23545;&#20110;&#28608;&#20809;&#31881;&#24202;&#29076;&#21270;&#25104;&#24418;&#65288;LPBF&#65289;&#22686;&#26448;&#21046;&#36896;&#65288;AM&#65289;&#25152;&#29983;&#20135;&#38646;&#20214;&#30340;&#36136;&#37327;&#21644;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#21033;&#29992;&#20809;&#30005;&#20108;&#26497;&#31649;&#20256;&#24863;&#22120;&#25968;&#25454;&#26816;&#27979;&#36807;&#28909;&#24322;&#24120;&#12290;&#20809;&#30005;&#20108;&#26497;&#31649;&#20256;&#24863;&#22120;&#21487;&#20197;&#20174;&#29076;&#27744;&#20013;&#25910;&#38598;&#39640;&#39057;&#25968;&#25454;&#65292;&#21453;&#26144;&#20986;&#36807;&#31243;&#21160;&#24577;&#21644;&#28909;&#21382;&#21490;&#12290;&#22240;&#27492;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#20809;&#30005;&#20108;&#26497;&#31649;&#20256;&#24863;&#22120;&#25968;&#25454;&#36827;&#34892;&#36880;&#23618;&#26816;&#27979;&#36807;&#28909;&#24322;&#24120;&#12290;&#20026;&#27492;&#65292;&#20174;&#21407;&#22987;&#20809;&#30005;&#20108;&#26497;&#31649;&#25968;&#25454;&#20013;&#25552;&#21462;&#20102;&#19977;&#32452;&#29305;&#24449;&#65306;MSMM&#65288;&#24179;&#22343;&#20540;&#12289;&#26631;&#20934;&#24046;&#12289;&#20013;&#20301;&#25968;&#12289;&#26368;&#22823;&#20540;&#65289;&#12289;MSQ&#65288;&#24179;&#22343;&#20540;&#12289;&#26631;&#20934;&#24046;&#12289;&#22235;&#20998;&#20301;&#25968;&#65289;&#21644;MSD&#65288;&#24179;&#22343;&#20540;&#12289;&#26631;&#20934;&#24046;&#12289;&#21313;&#20998;&#20301;&#25968;&#65289;&#12290;&#36825;&#19977;&#20010;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;&#22810;&#20010;ML&#20998;&#31867;&#22120;&#12290;&#37319;&#29992;&#25104;&#26412;&#25935;&#24863;&#23398;&#20064;&#26469;&#22788;&#29702;&#8220;&#24322;&#24120;&#8221;&#23618;&#65288;&#21463;&#36807;&#28909;&#24433;&#21709;&#65289;&#21644;&#8220;&#27491;&#24120;&#8221;&#23618;&#20043;&#38388;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13861v1 Announce Type: new  Abstract: Overheating anomaly detection is essential for the quality and reliability of parts produced by laser powder bed fusion (LPBF) additive manufacturing (AM). In this research, we focus on the detection of overheating anomalies using photodiode sensor data. Photodiode sensors can collect high-frequency data from the melt pool, reflecting the process dynamics and thermal history. Hence, the proposed method offers a machine learning (ML) framework to utilize photodiode sensor data for layer-wise detection of overheating anomalies. In doing so, three sets of features are extracted from the raw photodiode data: MSMM (mean, standard deviation, median, maximum), MSQ (mean, standard deviation, quartiles), and MSD (mean, standard deviation, deciles). These three datasets are used to train several ML classifiers. Cost-sensitive learning is used to handle the class imbalance between the "anomalous" layers (affected by overheating) and "nominal" layer
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CLARM&#30340;&#20004;&#27493;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#21152;&#36895;&#22120;&#20013;&#24102;&#30005;&#31890;&#23376;&#30340;&#26102;&#31354;&#21160;&#24577;&#65292;&#24182;&#33021;&#22815;&#29983;&#25104;&#19981;&#21516;&#21152;&#36895;&#22120;&#27169;&#22359;&#30340;&#25237;&#24433;&#21644;&#39044;&#27979;&#31890;&#23376;&#30340;&#26410;&#26469;&#29366;&#24577;&#12290;</title><link>https://arxiv.org/abs/2403.13858</link><description>&lt;p&gt;
&#29992;&#20110;&#31890;&#23376;&#21152;&#36895;&#22120;&#26463;&#27969;&#21160;&#21147;&#23398;&#29983;&#25104;&#21644;&#39044;&#27979;&#30340;&#26465;&#20214;&#28508;&#22312;&#33258;&#22238;&#24402;&#36882;&#24402;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A conditional latent autoregressive recurrent model for generation and forecasting of beam dynamics in particle accelerators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13858
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CLARM&#30340;&#20004;&#27493;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#21152;&#36895;&#22120;&#20013;&#24102;&#30005;&#31890;&#23376;&#30340;&#26102;&#31354;&#21160;&#24577;&#65292;&#24182;&#33021;&#22815;&#29983;&#25104;&#19981;&#21516;&#21152;&#36895;&#22120;&#27169;&#22359;&#30340;&#25237;&#24433;&#21644;&#39044;&#27979;&#31890;&#23376;&#30340;&#26410;&#26469;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31890;&#23376;&#21152;&#36895;&#22120;&#26159;&#23558;&#24378;&#28872;&#30340;&#24102;&#30005;&#31890;&#23376;&#26463;&#27969;&#38598;&#20013;&#12289;&#24341;&#23548;&#21644;&#21152;&#36895;&#21040;&#39640;&#33021;&#30340;&#22797;&#26434;&#31995;&#32479;&#12290;&#26463;&#27969;&#35786;&#26029;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#21463;&#38480;&#30340;&#38750;&#30772;&#22351;&#24615;&#27979;&#37327;&#12289;&#35745;&#31639;&#23494;&#38598;&#22411;&#27169;&#25311;&#20197;&#21450;&#31995;&#32479;&#22266;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#26465;&#20214;&#28508;&#22312;&#33258;&#22238;&#24402;&#36882;&#24402;&#27169;&#22411;&#65288;CLARM&#65289;&#30340;&#20004;&#27493;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#21152;&#36895;&#22120;&#20013;&#24102;&#30005;&#31890;&#23376;&#30340;&#26102;&#31354;&#21160;&#24577;&#12290;CLARM&#21253;&#25324;&#19968;&#20010;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;CVAE&#65289;&#65292;&#23558;&#20845;&#32500;&#30456;&#31354;&#38388;&#36716;&#25442;&#20026;&#20302;&#32500;&#28508;&#22312;&#20998;&#24067;&#65292;&#20197;&#21450;&#19968;&#20010;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#65292;&#20197;&#33258;&#22238;&#24402;&#26041;&#24335;&#25429;&#33719;&#26102;&#38388;&#21160;&#24577;&#12290;CLARM&#33021;&#22815;&#36890;&#36807;&#23545;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#36827;&#34892;&#37319;&#26679;&#21644;&#35299;&#30721;&#26469;&#29983;&#25104;&#21508;&#31181;&#21152;&#36895;&#22120;&#27169;&#22359;&#30340;&#25237;&#24433;&#12290;&#35813;&#27169;&#22411;&#36824;&#21487;&#20197;&#39044;&#27979;&#20805;&#30005;&#31890;&#23376;&#30340;&#26410;&#26469;&#29366;&#24577;&#65288;&#19979;&#28216;&#20301;&#32622;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13858v1 Announce Type: cross  Abstract: Particle accelerators are complex systems that focus, guide, and accelerate intense charged particle beams to high energy. Beam diagnostics present a challenging problem due to limited non-destructive measurements, computationally demanding simulations, and inherent uncertainties in the system. We propose a two-step unsupervised deep learning framework named as Conditional Latent Autoregressive Recurrent Model (CLARM) for learning the spatiotemporal dynamics of charged particles in accelerators. CLARM consists of a Conditional Variational Autoencoder (CVAE) transforming six-dimensional phase space into a lower-dimensional latent distribution and a Long Short-Term Memory (LSTM) network capturing temporal dynamics in an autoregressive manner. The CLARM can generate projections at various accelerator modules by sampling and decoding the latent space representation. The model also forecasts future states (downstream locations) of charged p
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26469;&#25511;&#21046;&#21307;&#23398;&#25968;&#23383;&#23402;&#29983;&#20307;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.13851</link><description>&lt;p&gt;
&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#21307;&#23398;&#25968;&#23383;&#23402;&#29983;&#20307;
&lt;/p&gt;
&lt;p&gt;
Control of Medical Digital Twins with Artificial Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13851
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26469;&#25511;&#21046;&#21307;&#23398;&#25968;&#23383;&#23402;&#29983;&#20307;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#21307;&#23398;&#30340;&#30446;&#26631;&#26159;&#23558;&#24178;&#39044;&#25514;&#26045;&#35843;&#25972;&#21040;&#20010;&#20307;&#24739;&#32773;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#29992;&#20110;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#30340;&#20851;&#38190;&#25216;&#26415;&#28041;&#21450;&#21307;&#23398;&#25968;&#23383;&#23402;&#29983;&#20307;&#65292;&#36825;&#26159;&#20154;&#20307;&#29983;&#29289;&#23398;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#21487;&#20197;&#20010;&#24615;&#21270;&#24182;&#21160;&#24577;&#26356;&#26032;&#65292;&#20197;&#32435;&#20837;&#38543;&#26102;&#38388;&#25910;&#38598;&#30340;&#29305;&#23450;&#24739;&#32773;&#25968;&#25454;&#12290;&#20154;&#20307;&#29983;&#29289;&#23398;&#30340;&#26576;&#20123;&#26041;&#38754;&#65292;&#22914;&#20813;&#30123;&#31995;&#32479;&#65292;&#19981;&#23481;&#26131;&#29992;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#65288;&#22914;&#24494;&#20998;&#26041;&#31243;&#65289;&#25429;&#25417;&#12290;&#30456;&#21453;&#65292;&#23427;&#20204;&#24448;&#24448;&#26159;&#22810;&#23610;&#24230;&#30340;&#12289;&#38543;&#26426;&#30340;&#21644;&#28151;&#21512;&#30340;&#12290;&#23545;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#25511;&#21046;&#21644;&#20248;&#21270;&#26041;&#27861;&#32780;&#35328;&#65292;&#36825;&#26500;&#25104;&#20102;&#19968;&#20010;&#25361;&#25112;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#36731;&#26494;&#24212;&#29992;&#20110;&#36825;&#31181;&#27169;&#22411;&#12290;&#33258;&#21160;&#24494;&#20998;&#21644;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#26377;&#26395;&#35299;&#20915;&#22797;&#26434;&#30340;&#25511;&#21046;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#31995;&#32479;&#20173;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#21147;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#26041;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13851v1 Announce Type: cross  Abstract: The objective of personalized medicine is to tailor interventions to an individual patient's unique characteristics. A key technology for this purpose involves medical digital twins, computational models of human biology that can be personalized and dynamically updated to incorporate patient-specific data collected over time. Certain aspects of human biology, such as the immune system, are not easily captured with physics-based models, such as differential equations. Instead, they are often multi-scale, stochastic, and hybrid. This poses a challenge to existing model-based control and optimization approaches that cannot be readily applied to such models. Recent advances in automatic differentiation and neural-network control methods hold promise in addressing complex control problems. However, the application of these approaches to biomedical systems is still in its early stages. This work introduces dynamics-informed neural-network co
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ST-PAD&#26694;&#26550;&#65292;&#36890;&#36807;&#26102;&#31354;&#29289;&#29702;&#23398;&#24847;&#35782;&#21644;&#21442;&#25968;&#25193;&#25955;&#24341;&#23548;&#23454;&#29616;&#27969;&#20307;&#21160;&#21147;&#23398;&#30340;&#39640;&#31934;&#24230;&#20223;&#30495;&#21644;&#39044;&#27979;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#24403;&#21069;&#20027;&#27969;&#27169;&#22411;</title><link>https://arxiv.org/abs/2403.13850</link><description>&lt;p&gt;
&#32463;&#36807;&#29289;&#29702;&#24847;&#35782;&#21644;&#21442;&#25968;&#25193;&#25955;&#25351;&#23548;&#30340;&#26102;&#31354;&#27969;&#20307;&#21160;&#21147;&#23398;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Spatio-Temporal Fluid Dynamics Modeling via Physical-Awareness and Parameter Diffusion Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13850
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ST-PAD&#26694;&#26550;&#65292;&#36890;&#36807;&#26102;&#31354;&#29289;&#29702;&#23398;&#24847;&#35782;&#21644;&#21442;&#25968;&#25193;&#25955;&#24341;&#23548;&#23454;&#29616;&#27969;&#20307;&#21160;&#21147;&#23398;&#30340;&#39640;&#31934;&#24230;&#20223;&#30495;&#21644;&#39044;&#27979;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#24403;&#21069;&#20027;&#27969;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ST-PAD&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#29992;&#20110;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#30340;&#26102;&#31354;&#27969;&#20307;&#21160;&#21147;&#23398;&#24314;&#27169;&#65292;&#26088;&#22312;&#36890;&#36807;&#26102;&#31354;&#29289;&#29702;&#23398;&#24847;&#35782;&#21644;&#21442;&#25968;&#25193;&#25955;&#24341;&#23548;&#23454;&#29616;&#27969;&#20307;&#21160;&#21147;&#23398;&#30340;&#39640;&#31934;&#24230;&#20223;&#30495;&#21644;&#39044;&#27979;&#12290;&#22312;&#19978;&#28216;&#38454;&#27573;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20855;&#26377;&#26102;&#38388;&#28436;&#21464;&#29305;&#24615;&#30340;&#30690;&#37327;&#37327;&#21270;&#37325;&#26500;&#27169;&#22359;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#33324;&#30340;&#29289;&#29702;&#32422;&#26463;&#65292;&#30830;&#20445;&#20102;&#24179;&#34913;&#21644;&#26377;&#24377;&#24615;&#30340;&#21442;&#25968;&#20998;&#24067;&#12290;&#22312;&#19979;&#28216;&#38454;&#27573;&#65292;&#21033;&#29992;&#28041;&#21450;&#21442;&#25968;&#30340;&#25193;&#25955;&#27010;&#29575;&#32593;&#32476;&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26410;&#26469;&#27969;&#20307;&#29366;&#24577;&#65292;&#21516;&#26102;&#36890;&#36807;&#24863;&#30693;&#19981;&#21516;&#29289;&#29702;&#35774;&#32622;&#20013;&#30340;&#21442;&#25968;&#65292;&#22686;&#24378;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23545;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;ST-PAD&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#26174;&#31034;ST-PAD&#22312;&#27969;&#20307;&#21160;&#21147;&#23398;&#26041;&#38754;&#20248;&#20110;&#24403;&#21069;&#20027;&#27969;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13850v1 Announce Type: cross  Abstract: This paper proposes a two-stage framework named ST-PAD for spatio-temporal fluid dynamics modeling in the field of earth sciences, aiming to achieve high-precision simulation and prediction of fluid dynamics through spatio-temporal physics awareness and parameter diffusion guidance. In the upstream stage, we design a vector quantization reconstruction module with temporal evolution characteristics, ensuring balanced and resilient parameter distribution by introducing general physical constraints. In the downstream stage, a diffusion probability network involving parameters is utilized to generate high-quality future states of fluids, while enhancing the model's generalization ability by perceiving parameters in various physical setups. Extensive experiments on multiple benchmark datasets have verified the effectiveness and robustness of the ST-PAD framework, which showcase that ST-PAD outperforms current mainstream models in fluid dyna
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#24182;&#20171;&#32461;&#20102;GNNs&#20013;&#30340;&#20808;&#36827;&#39046;&#22495;&#65306;&#22270;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2403.13849</link><description>&lt;p&gt;
&#25581;&#31192;&#22270;&#65306;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Graphs Unveiled: Graph Neural Networks and Graph Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13849
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#24182;&#20171;&#32461;&#20102;GNNs&#20013;&#30340;&#20808;&#36827;&#39046;&#22495;&#65306;&#22270;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#28909;&#28857;&#35805;&#39064;&#20043;&#19968;&#26159;GNN&#39046;&#22495;&#12290;&#22270;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#32473;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#28044;&#29616;&#20102;&#35768;&#22810;&#20851;&#20110;&#25193;&#23637;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#22270;&#25968;&#25454;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#39033;&#35843;&#30740;&#65292;&#20840;&#38754;&#27010;&#36848;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GNNs&#20013;&#30340;&#19968;&#39033;&#20808;&#36827;&#39046;&#22495;&#65306;&#22270;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13849v1 Announce Type: cross  Abstract: One of the hot topics in machine learning is the field of GNN. The complexity of graph data has imposed significant challenges on existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. This paper represents a survey, providing a comprehensive overview of Graph Neural Networks (GNNs). We discuss the applications of graph neural networks across various domains. Finally, we present an advanced field in GNNs: graph generation.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24314;&#31435;Gini&#19981;&#32431;&#24230;&#30340;&#24179;&#28369;&#25935;&#24863;&#24230;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25552;&#20986;DP&#36138;&#23146;&#35268;&#21017;&#21015;&#34920;&#31639;&#27861;&#65292;&#26412;&#25991;&#25913;&#21892;&#20102;&#24046;&#24322;&#20445;&#25252;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.13848</link><description>&lt;p&gt;
&#29992;&#20110;&#23398;&#20064;&#24046;&#24322;&#20445;&#25252;&#20294;&#20934;&#30830;&#35268;&#21017;&#21015;&#34920;&#30340;&#24179;&#28369;&#25935;&#24863;&#24230;
&lt;/p&gt;
&lt;p&gt;
Smooth Sensitivity for Learning Differentially-Private yet Accurate Rule Lists
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13848
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24314;&#31435;Gini&#19981;&#32431;&#24230;&#30340;&#24179;&#28369;&#25935;&#24863;&#24230;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25552;&#20986;DP&#36138;&#23146;&#35268;&#21017;&#21015;&#34920;&#31639;&#27861;&#65292;&#26412;&#25991;&#25913;&#21892;&#20102;&#24046;&#24322;&#20445;&#25252;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#24322;&#20445;&#25252;&#65288;DP&#65289;&#26426;&#21046;&#21487;&#20197;&#23884;&#20837;&#21040;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#35774;&#35745;&#20013;&#65292;&#20197;&#20445;&#25252;&#25152;&#24471;&#27169;&#22411;&#20813;&#21463;&#38544;&#31169;&#27844;&#38706;&#30340;&#24433;&#21709;&#65292;&#23613;&#31649;&#36825;&#36890;&#24120;&#20276;&#38543;&#30528;&#26126;&#26174;&#30340;&#20934;&#30830;&#24615;&#25439;&#22833;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24314;&#31435;Gini&#19981;&#32431;&#24230;&#30340;&#24179;&#28369;&#25935;&#24863;&#24230;&#24182;&#21033;&#29992;&#36825;&#19968;&#29305;&#24615;&#26469;&#25552;&#20986;&#19968;&#20010;DP&#36138;&#23146;&#35268;&#21017;&#21015;&#34920;&#31639;&#27861;&#65292;&#20197;&#25913;&#21892;&#36825;&#31181;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38598;&#25104;&#24179;&#28369;&#25935;&#24863;&#24230;&#30340;DP&#35268;&#21017;&#21015;&#34920;&#27169;&#22411;&#20855;&#26377;&#27604;&#20351;&#29992;&#20840;&#23616;&#25935;&#24863;&#24230;&#30340;&#20854;&#20182;DP&#26694;&#26550;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13848v1 Announce Type: cross  Abstract: Differentially-private (DP) mechanisms can be embedded into the design of a machine learningalgorithm to protect the resulting model against privacy leakage, although this often comes with asignificant loss of accuracy. In this paper, we aim at improving this trade-off for rule lists modelsby establishing the smooth sensitivity of the Gini impurity and leveraging it to propose a DP greedyrule list algorithm. In particular, our theoretical analysis and experimental results demonstrate thatthe DP rule lists models integrating smooth sensitivity have higher accuracy that those using otherDP frameworks based on global sensitivity.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#36827;&#34892;&#22495;&#33258;&#36866;&#24212;&#30340;&#26368;&#20248;&#36755;&#36816;&#65292;&#21487;&#20197;&#23454;&#29616;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#28151;&#21512;&#25104;&#20998;&#20043;&#38388;&#30340;&#21305;&#37197;&#65292;&#20174;&#32780;&#22312;&#22833;&#25928;&#35786;&#26029;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13847</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#36827;&#34892;&#22495;&#33258;&#36866;&#24212;&#30340;&#26368;&#20248;&#36755;&#36816;
&lt;/p&gt;
&lt;p&gt;
Optimal Transport for Domain Adaptation through Gaussian Mixture Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13847
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#36827;&#34892;&#22495;&#33258;&#36866;&#24212;&#30340;&#26368;&#20248;&#36755;&#36816;&#65292;&#21487;&#20197;&#23454;&#29616;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#28151;&#21512;&#25104;&#20998;&#20043;&#38388;&#30340;&#21305;&#37197;&#65292;&#20174;&#32780;&#22312;&#22833;&#25928;&#35786;&#26029;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#36827;&#34892;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#23545;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#12290;&#36825;&#31181;&#31574;&#30053;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#31561;&#20215;&#30340;&#31163;&#25955;&#38382;&#39064;&#35299;&#20915;&#36830;&#32493;&#26368;&#20248;&#36755;&#36816;&#12290;&#26368;&#20248;&#36755;&#36816;&#35299;&#20915;&#26041;&#26696;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#28151;&#21512;&#25104;&#20998;&#20043;&#38388;&#30340;&#21305;&#37197;&#12290;&#36890;&#36807;&#36825;&#31181;&#21305;&#37197;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#22495;&#20043;&#38388;&#26144;&#23556;&#25968;&#25454;&#28857;&#65292;&#25110;&#32773;&#23558;&#26631;&#31614;&#20174;&#28304;&#22495;&#32452;&#20214;&#36716;&#31227;&#21040;&#30446;&#26631;&#22495;&#12290;&#25105;&#20204;&#22312;&#22833;&#25928;&#35786;&#26029;&#30340;&#20004;&#20010;&#22495;&#33258;&#36866;&#24212;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13847v1 Announce Type: cross  Abstract: In this paper we explore domain adaptation through optimal transport. We propose a novel approach, where we model the data distributions through Gaussian mixture models. This strategy allows us to solve continuous optimal transport through an equivalent discrete problem. The optimal transport solution gives us a matching between source and target domain mixture components. From this matching, we can map data points between domains, or transfer the labels from the source domain components towards the target domain. We experiment with 2 domain adaptation benchmarks in fault diagnosis, showing that our methods have state-of-the-art performance.
&lt;/p&gt;</description></item><item><title>CMDI&#32858;&#31867;&#26041;&#27861;&#21019;&#26032;&#24615;&#22320;&#23558;&#20108;&#32500;&#32467;&#26500;&#20449;&#24687;&#29702;&#35770;&#34701;&#20837;&#32858;&#31867;&#36807;&#31243;&#20013;&#65292;&#24357;&#34917;&#20102;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#32858;&#31867;&#26041;&#27861;&#20013;&#24573;&#30053;&#30340;&#38543;&#26426;&#28216;&#36208;&#35775;&#38382;&#33410;&#28857;&#21644;&#25968;&#25454;&#20013;&#23884;&#20837;&#30340;&#32467;&#26500;&#20449;&#24687;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13846</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#22270;&#26368;&#22823;&#35299;&#30721;&#20449;&#24687;&#30340;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Clustering Method with Graph Maximum Decoding Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13846
&lt;/p&gt;
&lt;p&gt;
CMDI&#32858;&#31867;&#26041;&#27861;&#21019;&#26032;&#24615;&#22320;&#23558;&#20108;&#32500;&#32467;&#26500;&#20449;&#24687;&#29702;&#35770;&#34701;&#20837;&#32858;&#31867;&#36807;&#31243;&#20013;&#65292;&#24357;&#34917;&#20102;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#32858;&#31867;&#26041;&#27861;&#20013;&#24573;&#30053;&#30340;&#38543;&#26426;&#28216;&#36208;&#35775;&#38382;&#33410;&#28857;&#21644;&#25968;&#25454;&#20013;&#23884;&#20837;&#30340;&#32467;&#26500;&#20449;&#24687;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#27169;&#22411;&#30340;&#32858;&#31867;&#26041;&#27861;&#22240;&#20854;&#22312;&#21508;&#31181;&#30693;&#35782;&#39046;&#22495;&#20013;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#20854;&#33021;&#22815;&#19982;&#20854;&#20182;&#30456;&#20851;&#24212;&#29992;&#26080;&#32541;&#38598;&#25104;&#30340;&#36866;&#24212;&#24615;&#36171;&#20104;&#20102;&#22522;&#20110;&#22270;&#27169;&#22411;&#30340;&#32858;&#31867;&#20998;&#26512;&#33021;&#21147;&#65292;&#21487;&#20197;&#24378;&#22823;&#22320;&#20174;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#8220;&#33258;&#28982;&#20851;&#32852;&#8221;&#25110;&#8220;&#22270;&#32467;&#26500;&#8221;&#65292;&#26377;&#21161;&#20110;&#24314;&#27169;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#25928;&#26524;&#26174;&#33879;&#65292;&#20294;&#24403;&#21069;&#21033;&#29992;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#30340;&#32858;&#31867;&#26041;&#27861;&#24573;&#30053;&#20102;&#33410;&#28857;&#20043;&#38388;&#38543;&#26426;&#28216;&#36208;&#35775;&#38382;&#20197;&#21450;&#25968;&#25454;&#20013;&#23884;&#20837;&#30340;&#32467;&#26500;&#20449;&#24687;&#25152;&#24102;&#26469;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#20869;&#26368;&#22823;&#21270;&#35299;&#30721;&#20449;&#24687;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;CMDI&#12290;CMDI&#21019;&#26032;&#22320;&#23558;&#20108;&#32500;&#32467;&#26500;&#20449;&#24687;&#29702;&#35770;&#32435;&#20837;&#21040;&#32858;&#31867;&#36807;&#31243;&#20013;&#65292;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#22270;&#32467;&#26500;&#25552;&#21462;&#21644;&#22270;&#39030;&#28857;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13846v1 Announce Type: cross  Abstract: The clustering method based on graph models has garnered increased attention for its widespread applicability across various knowledge domains. Its adaptability to integrate seamlessly with other relevant applications endows the graph model-based clustering analysis with the ability to robustly extract "natural associations" or "graph structures" within datasets, facilitating the modelling of relationships between data points. Despite its efficacy, the current clustering method utilizing the graph-based model overlooks the uncertainty associated with random walk access between nodes and the embedded structural information in the data. To address this gap, we present a novel Clustering method for Maximizing Decoding Information within graph-based models, named CMDI. CMDI innovatively incorporates two-dimensional structural information theory into the clustering process, consisting of two phases: graph structure extraction and graph vert
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22686;&#37327;&#24335;ZSFD&#33539;&#24335;&#65292;&#24320;&#21457;&#20102;&#24191;&#28145;&#28151;&#21512;&#21453;&#36951;&#24536;&#26694;&#26550;&#65288;BDMAFF&#65289;&#65292;&#26088;&#22312;&#23398;&#20064;&#21644;&#36866;&#24212;&#26032;&#30340;&#25925;&#38556;&#31867;&#21035;&#21644;&#23646;&#24615;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;ZSFD&#33539;&#24335;&#22312;&#24037;&#19994;&#22330;&#26223;&#20013;&#26080;&#27861;&#20174;&#19981;&#26029;&#21464;&#21270;&#30340;&#35757;&#32451;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.13845</link><description>&lt;p&gt;
&#23398;&#20250;&#26356;&#22909;&#22320;&#30475;&#35265;&#30475;&#19981;&#35265;&#30340;&#19996;&#35199;&#65306;&#29992;&#20110;&#22686;&#37327;&#24335;&#38646;&#26679;&#26412;&#25925;&#38556;&#35786;&#26029;&#30340;&#24191;&#28145;&#28151;&#21512;&#21453;&#36951;&#24536;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Learning to better see the unseen: Broad-Deep Mixed Anti-Forgetting Framework for Incremental Zero-Shot Fault Diagnosis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13845
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22686;&#37327;&#24335;ZSFD&#33539;&#24335;&#65292;&#24320;&#21457;&#20102;&#24191;&#28145;&#28151;&#21512;&#21453;&#36951;&#24536;&#26694;&#26550;&#65288;BDMAFF&#65289;&#65292;&#26088;&#22312;&#23398;&#20064;&#21644;&#36866;&#24212;&#26032;&#30340;&#25925;&#38556;&#31867;&#21035;&#21644;&#23646;&#24615;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;ZSFD&#33539;&#24335;&#22312;&#24037;&#19994;&#22330;&#26223;&#20013;&#26080;&#27861;&#20174;&#19981;&#26029;&#21464;&#21270;&#30340;&#35757;&#32451;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#25925;&#38556;&#35786;&#26029;&#65288;ZSFD&#65289;&#33021;&#22815;&#36890;&#36807;&#39044;&#27979;&#20154;&#31867;&#19987;&#23478;&#26631;&#27880;&#30340;&#25925;&#38556;&#23646;&#24615;&#26469;&#35782;&#21035;&#30475;&#19981;&#35265;&#30340;&#25925;&#38556;&#12290;&#20026;&#20102;&#35299;&#20915;&#24037;&#19994;&#36807;&#31243;&#20013;&#25345;&#32493;&#21464;&#21270;&#30340;&#38656;&#27714;&#65292;&#21363;&#27169;&#22411;&#36866;&#24212;&#26032;&#30340;&#25925;&#38556;&#31867;&#21035;&#21644;&#23646;&#24615;&#32780;&#36991;&#20813;&#24536;&#35760;&#20808;&#21069;&#23398;&#21040;&#30340;&#35786;&#26029;&#33021;&#21147;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#22686;&#37327;&#24335;ZSFD&#65288;IZSFD&#65289;&#33539;&#24335;&#65292;&#23427;&#32467;&#21512;&#20102;&#20256;&#32479;ZSFD&#21644;&#24191;&#20041;ZSFD&#33539;&#24335;&#30340;&#31867;&#21035;&#22686;&#37327;&#21644;&#23646;&#24615;&#22686;&#37327;&#12290;&#20026;&#20102;&#23454;&#29616;IZSFD&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#23398;&#20064;&#26032;&#30340;&#25925;&#38556;&#31867;&#21035;&#21644;&#23646;&#24615;&#30340;&#24191;&#28145;&#28151;&#21512;&#21453;&#36951;&#24536;&#26694;&#26550;&#65288;BDMAFF&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36951;&#24536;&#38382;&#39064;&#65292;BDMAFF&#26377;&#25928;&#22320;&#20174;&#20004;&#20010;&#35282;&#24230;&#31215;&#32047;&#20808;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13845v1 Announce Type: cross  Abstract: Zero-shot fault diagnosis (ZSFD) is capable of identifying unseen faults via predicting fault attributes labeled by human experts. We first recognize the demand of ZSFD to deal with continuous changes in industrial processes, i.e., the model's ability to adapt to new fault categories and attributes while avoiding forgetting the diagnosis ability learned previously. To overcome the issue that the existing ZSFD paradigm cannot learn from evolving streams of training data in industrial scenarios, the incremental ZSFD (IZSFD) paradigm is proposed for the first time, which incorporates category increment and attribute increment for both traditional ZSFD and generalized ZSFD paradigms. To achieve IZSFD, we present a broad-deep mixed anti-forgetting framework (BDMAFF) that aims to learn from new fault categories and attributes. To tackle the issue of forgetting, BDMAFF effectively accumulates previously acquired knowledge from two perspective
&lt;/p&gt;</description></item><item><title>&#20302;&#32500;&#24230;&#35745;&#31639;&#20998;&#31867;&#22120;&#22522;&#20110;&#21521;&#37327;&#31526;&#21495;&#20307;&#31995;&#32467;&#26500;&#65288;VSA&#65289;&#65292;&#36890;&#36807;&#23450;&#26102;&#30693;&#35782;&#33719;&#21462;&#26041;&#27861;&#65292;&#25552;&#39640;&#23567;&#27169;&#22411;&#20934;&#30830;&#29575;&#65292;&#35299;&#20915;&#22788;&#29702;&#22797;&#26434;&#33041;&#20449;&#21495;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.13844</link><description>&lt;p&gt;
&#38754;&#21521;&#33041;&#26426;&#25509;&#21475;&#30340;&#36731;&#37327;&#32423;&#21521;&#37327;&#31526;&#21495;&#20307;&#31995;&#26550;&#26500;&#30340;&#23450;&#26102;&#30693;&#35782;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
Scheduled Knowledge Acquisition on Lightweight Vector Symbolic Architectures for Brain-Computer Interfaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13844
&lt;/p&gt;
&lt;p&gt;
&#20302;&#32500;&#24230;&#35745;&#31639;&#20998;&#31867;&#22120;&#22522;&#20110;&#21521;&#37327;&#31526;&#21495;&#20307;&#31995;&#32467;&#26500;&#65288;VSA&#65289;&#65292;&#36890;&#36807;&#23450;&#26102;&#30693;&#35782;&#33719;&#21462;&#26041;&#27861;&#65292;&#25552;&#39640;&#23567;&#27169;&#22411;&#20934;&#30830;&#29575;&#65292;&#35299;&#20915;&#22788;&#29702;&#22797;&#26434;&#33041;&#20449;&#21495;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#26426;&#25509;&#21475;&#65288;BCIs&#65289;&#36890;&#24120;&#35774;&#35745;&#20026;&#36731;&#37327;&#32423;&#19988;&#23454;&#26102;&#21709;&#24212;&#65292;&#20197;&#20026;&#29992;&#25143;&#25552;&#20379;&#21450;&#26102;&#21453;&#39304;&#12290;&#32463;&#20856;&#29305;&#24449;&#24037;&#31243;&#35745;&#31639;&#25928;&#29575;&#39640;&#20294;&#20934;&#30830;&#29575;&#20302;&#65292;&#32780;&#36817;&#26399;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#25552;&#39640;&#20934;&#30830;&#29575;&#20294;&#35745;&#31639;&#20195;&#20215;&#39640;&#12289;&#24310;&#36831;&#22823;&#12290;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#36873;&#25321;&#65292;&#22522;&#20110;&#21521;&#37327;&#31526;&#21495;&#20307;&#31995;&#32467;&#26500;&#65288;VSA&#65289;&#30340;&#20302;&#32500;&#24230;&#35745;&#31639;&#65288;LDC&#65289;&#20998;&#31867;&#22120;&#23454;&#29616;&#20102;&#23567;&#27169;&#22411;&#22823;&#23567;&#65292;&#20294;&#20934;&#30830;&#29575;&#39640;&#20110;&#32463;&#20856;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#30340;&#20934;&#30830;&#29575;&#20173;&#33853;&#21518;&#20110;&#29616;&#20195;DNNs&#65292;&#36825;&#20351;&#24471;&#22788;&#29702;&#22797;&#26434;&#30340;&#33041;&#20449;&#21495;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#25552;&#39640;&#23567;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#65292;&#30693;&#35782;&#33976;&#39311;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#23398;&#29983;&#27169;&#22411;&#22312;&#20854;&#19981;&#26029;&#23398;&#20064;&#38454;&#27573;&#26102;&#20445;&#25345;&#25945;&#24072;&#27169;&#22411;&#21644;&#23398;&#29983;&#27169;&#22411;&#20043;&#38388;&#30340;&#24658;&#23450;&#33976;&#39311;&#27700;&#24179;&#21487;&#33021;&#24182;&#38750;&#26368;&#20339;&#36873;&#25321;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#26102;&#30693;&#35782;&#33719;&#21462;&#30340;&#26041;&#27861;&#65292;&#20351;&#23398;&#29983;&#27169;&#22411;&#33021;&#22815;&#22312;&#36880;&#27493;&#23398;&#20064;&#38454;&#27573;&#25913;&#21892;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13844v1 Announce Type: cross  Abstract: Brain-Computer interfaces (BCIs) are typically designed to be lightweight and responsive in real-time to provide users timely feedback. Classical feature engineering is computationally efficient but has low accuracy, whereas the recent neural networks (DNNs) improve accuracy but are computationally expensive and incur high latency. As a promising alternative, the low-dimensional computing (LDC) classifier based on vector symbolic architecture (VSA), achieves small model size yet higher accuracy than classical feature engineering methods. However, its accuracy still lags behind that of modern DNNs, making it challenging to process complex brain signals. To improve the accuracy of a small model, knowledge distillation is a popular method. However, maintaining a constant level of distillation between the teacher and student models may not be the best way for a growing student during its progressive learning stages. In this work, we propos
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#22823;&#25968;&#25454;&#20998;&#26512;&#32467;&#21512;transformer&#35780;&#20272;&#30002;&#29366;&#33146;&#30284;&#39044;&#21518;&#30340;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#26032;&#30340;&#20998;&#31867;&#31995;&#32479;&#65292;&#24182;&#24378;&#35843;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#36741;&#21161;&#30002;&#29366;&#33146;&#30284;&#35786;&#26029;&#21644;&#27835;&#30103;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13843</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#35270;&#35273;Transformer&#22312;&#30002;&#29366;&#33146;&#30284;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Machine Learning and Vision Transformers for Thyroid Carcinoma Diagnosis: A review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13843
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#22823;&#25968;&#25454;&#20998;&#26512;&#32467;&#21512;transformer&#35780;&#20272;&#30002;&#29366;&#33146;&#30284;&#39044;&#21518;&#30340;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#26032;&#30340;&#20998;&#31867;&#31995;&#32479;&#65292;&#24182;&#24378;&#35843;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#36741;&#21161;&#30002;&#29366;&#33146;&#30284;&#35786;&#26029;&#21644;&#27835;&#30103;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#21457;&#23637;&#26234;&#33021;&#35786;&#26029;&#31995;&#32479;&#20197;&#24110;&#21161;&#21307;&#23398;&#19987;&#23478;&#22788;&#29702;&#22823;&#37327;&#25968;&#25454;&#20197;&#27835;&#30103;&#19981;&#21487;&#27835;&#24840;&#30142;&#30149;&#30340;&#20852;&#36259;&#19981;&#26029;&#22686;&#38271;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#35782;&#21035;&#30002;&#29366;&#33146;&#30284;&#65288;TC&#65289;&#30340;&#25361;&#25112;&#26041;&#38754;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#22823;&#25968;&#25454;&#20998;&#26512;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#32467;&#21512;transformer&#35780;&#20272;TC&#39044;&#21518;&#65292;&#24182;&#30830;&#23450;&#20010;&#20307;&#30340;&#24694;&#24615;&#39118;&#38505;&#12290;&#26412;&#32508;&#36848;&#25991;&#31456;&#24635;&#32467;&#20102;&#21508;&#31181;&#20851;&#20110;&#20197;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31639;&#27861;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#30340;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#37319;&#29992;transformer&#36827;&#34892;&#30002;&#29366;&#33146;&#30284;&#35786;&#26029;&#30340;&#26041;&#27861;&#12290;&#23427;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;AI&#31639;&#27861;&#12289;&#26694;&#26550;&#30446;&#26631;&#21644;&#20351;&#29992;&#30340;&#35745;&#31639;&#29615;&#22659;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#30340;&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#23427;&#36890;&#36807;&#20854;&#29305;&#24449;&#23457;&#26597;&#21644;&#23545;&#27604;&#20102;&#21487;&#29992;&#30340;TC&#25968;&#25454;&#38598;&#12290;&#35813;&#35770;&#25991;&#24378;&#35843;&#20102;AI&#24037;&#20855;&#22312;&#36890;&#36807;&#30417;&#30563;&#12289;&#26080;&#30417;&#30563;&#25110;&#28151;&#21512;&#26041;&#24335;&#21327;&#21161;&#35786;&#26029;&#21644;&#27835;&#30103;TC&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13843v1 Announce Type: cross  Abstract: The growing interest in developing smart diagnostic systems to help medical experts process extensive data for treating incurable diseases has been notable. In particular, the challenge of identifying thyroid cancer (TC) has seen progress with the use of machine learning (ML) and big data analysis, incorporating transformers to evaluate TC prognosis and determine the risk of malignancy in individuals. This review article presents a summary of various studies on AIbased approaches, especially those employing transformers, for diagnosing TC. It introduces a new categorization system for these methods based on artifcial intelligence (AI) algorithms, the goals of the framework, and the computing environments used. Additionally, it scrutinizes and contrasts the available TC datasets by their features. The paper highlights the importance of AI instruments in aiding the diagnosis and treatment of TC through supervised, unsupervised, or mixed 
&lt;/p&gt;</description></item><item><title>&#23545;&#39321;&#28207;&#24613;&#35786;&#23460;&#20505;&#35786;&#26102;&#38388;&#21464;&#21270;&#36827;&#34892;&#20998;&#26512;&#65292;&#36890;&#36807;&#28151;&#21512;CNN-LSTM&#27169;&#22411;&#25506;&#35752;&#39044;&#27979;&#27169;&#22411;&#30340;&#21487;&#20256;&#36882;&#24615;&#65292;&#22312;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#35266;&#23519;&#21040;&#27874;&#27425;&#22235;&#21644;&#20116;&#20043;&#38388;&#20986;&#29616;&#26368;&#22810;&#30340;ED&#20505;&#35786;&#22825;&#25968;&#65292;&#24182;&#21457;&#29616;&#22312;&#27874;&#27425;&#22235;&#21644;&#20116;&#20043;&#38388;&#20986;&#29616;&#34920;&#29616;&#26368;&#20339;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.13842</link><description>&lt;p&gt;
&#20998;&#26512;&#39321;&#28207;&#24613;&#35786;&#23460;&#20505;&#35786;&#26102;&#38388;&#21464;&#21270;&#65292;&#24182;&#27979;&#35797;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#36328;&#27874;&#27425;&#39044;&#27979;&#27169;&#22411;&#30340;&#21487;&#20256;&#36882;&#24615;&#65306;&#28151;&#21512;CNN-LSTM&#26041;&#27861;&#29992;&#20110;&#37327;&#21270;&#24314;&#31569;&#32423;&#31038;&#20250;&#29983;&#24577;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Analyzing the Variations in Emergency Department Boarding and Testing the Transferability of Forecasting Models across COVID-19 Pandemic Waves in Hong Kong: Hybrid CNN-LSTM approach to quantifying building-level socioecological risk
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13842
&lt;/p&gt;
&lt;p&gt;
&#23545;&#39321;&#28207;&#24613;&#35786;&#23460;&#20505;&#35786;&#26102;&#38388;&#21464;&#21270;&#36827;&#34892;&#20998;&#26512;&#65292;&#36890;&#36807;&#28151;&#21512;CNN-LSTM&#27169;&#22411;&#25506;&#35752;&#39044;&#27979;&#27169;&#22411;&#30340;&#21487;&#20256;&#36882;&#24615;&#65292;&#22312;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#35266;&#23519;&#21040;&#27874;&#27425;&#22235;&#21644;&#20116;&#20043;&#38388;&#20986;&#29616;&#26368;&#22810;&#30340;ED&#20505;&#35786;&#22825;&#25968;&#65292;&#24182;&#21457;&#29616;&#22312;&#27874;&#27425;&#22235;&#21644;&#20116;&#20043;&#38388;&#20986;&#29616;&#34920;&#29616;&#26368;&#20339;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24613;&#35786;&#31185;&#65288;ED&#65289;&#30340;&#20505;&#35786;&#65288;&#23450;&#20041;&#20026;&#24613;&#35786;&#23460;&#31561;&#24453;&#26102;&#38388;&#36229;&#36807;&#22235;&#23567;&#26102;&#65289;&#19982;&#30149;&#20154;&#19981;&#33391;&#32467;&#26524;&#21450;&#21307;&#30103;&#31995;&#32479;&#34920;&#29616;&#19981;&#20339;&#26377;&#20851;&#12290;&#28982;&#32780;&#65292;&#22312;COVID-19&#20043;&#21069;&#32570;&#20047;&#26377;&#25928;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;COVID-19&#26399;&#38388;&#32570;&#20047;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#28151;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;-&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#27169;&#22411;&#23545;&#39321;&#28207;&#21307;&#38498;&#31649;&#29702;&#23616;&#12289;&#21355;&#29983;&#32626;&#21644;&#25151;&#23627;&#23616;&#30340;&#20844;&#20849;&#25968;&#25454;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35797;&#22270;&#20351;&#29992;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#35782;&#21035;&#22312;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#23545;&#25105;&#20204;&#22797;&#26434;&#30340;&#33258;&#36866;&#24212;&#21307;&#30103;&#31995;&#32479;&#20135;&#29983;&#26126;&#26174;&#24178;&#25200;&#30340;&#38454;&#27573;&#65292;&#20174;&#32780;&#25581;&#31034;&#20854;&#32452;&#25104;&#37096;&#20998;&#20043;&#38388;&#30340;&#31283;&#23450;&#30456;&#20114;&#20851;&#31995;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13842v1 Announce Type: new  Abstract: Emergency department's (ED) boarding (defined as ED waiting time greater than four hours) has been linked to poor patient outcomes and health system performance. Yet, effective forecasting models is rare before COVID-19, lacking during the peri-COVID era. Here, a hybrid convolutional neural network (CNN)-Long short-term memory (LSTM) model was applied to public-domain data sourced from Hong Kong's Hospital Authority, Department of Health, and Housing Authority. In addition, we sought to identify the phase of the COVID-19 pandemic that most significantly perturbed our complex adaptive healthcare system, thereby revealing a stable pattern of interconnectedness among its components, using deep transfer learning methodology.   Our result shows that 1) the greatest proportion of days with ED boarding was found between waves four and five; 2) the best-performing model for forecasting ED boarding was observed between waves four and five, which 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;&#20102;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25968;&#25454;&#21644;&#33258;&#25105;&#25253;&#21578;&#26085;&#35760;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#24773;&#24863;&#29366;&#24577;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.13841</link><description>&lt;p&gt;
&#25972;&#21512;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25968;&#25454;&#21644;&#33258;&#25105;&#25253;&#21578;&#26085;&#35760;&#29992;&#20110;&#20010;&#24615;&#21270;&#24773;&#24863;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Integrating Wearable Sensor Data and Self-reported Diaries for Personalized Affect Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;&#20102;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25968;&#25454;&#21644;&#33258;&#25105;&#25253;&#21578;&#26085;&#35760;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#24773;&#24863;&#29366;&#24577;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#32490;&#29366;&#24577;&#20316;&#20026;&#24773;&#24863;&#30340;&#25351;&#26631;&#23545;&#25972;&#20307;&#20581;&#24247;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#27492;&#22312;&#20854;&#21457;&#20316;&#21069;&#20934;&#30830;&#39044;&#27979;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20351;&#29992;&#26469;&#33258;&#21487;&#31359;&#25140;&#21644;&#31227;&#21160;&#35774;&#22791;&#30340;&#25968;&#25454;&#36827;&#34892;&#19982;&#30701;&#26399;&#24773;&#24863;&#26816;&#27979;&#12290;&#36825;&#20123;&#30740;&#31350;&#36890;&#24120;&#19987;&#27880;&#20110;&#23458;&#35266;&#30340;&#24863;&#23448;&#27979;&#37327;&#65292;&#24448;&#24448;&#24573;&#30053;&#20854;&#20182;&#24418;&#24335;&#30340;&#33258;&#25105;&#25253;&#21578;&#20449;&#24687;&#65292;&#22914;&#26085;&#35760;&#21644;&#31508;&#35760;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24773;&#24863;&#29366;&#24577;&#39044;&#27979;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#19968;&#20010;transformer&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23458;&#35266;&#25351;&#26631;&#21644;&#33258;&#25105;&#25253;&#21578;&#26085;&#35760;&#30340;&#32508;&#21512;&#20998;&#26512;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#32437;&#21521;&#30740;&#31350;&#65292;&#25307;&#21215;&#20102;&#22823;&#23398;&#29983;&#24182;&#22312;&#19968;&#24180;&#20869;&#23545;&#20854;&#36827;&#34892;&#30417;&#27979;&#65292;&#25910;&#38598;&#20102;&#21253;&#25324;&#29983;&#29702;&#12289;&#29615;&#22659;&#12289;&#30561;&#30496;&#12289;&#20195;&#35874;&#21644;&#36523;&#20307;&#27963;&#21160;&#21442;&#25968;&#22312;&#20869;&#30340;&#24191;&#27867;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#21442;&#19982;&#32773;&#25552;&#20379;&#20102;&#24320;&#25918;&#24335;&#25991;&#26412;&#26085;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13841v1 Announce Type: cross  Abstract: Emotional states, as indicators of affect, are pivotal to overall health, making their accurate prediction before onset crucial. Current studies are primarily centered on immediate short-term affect detection using data from wearable and mobile devices. These studies typically focus on objective sensory measures, often neglecting other forms of self-reported information like diaries and notes. In this paper, we propose a multimodal deep learning model for affect status forecasting. This model combines a transformer encoder with a pre-trained language model, facilitating the integrated analysis of objective metrics and self-reported diaries. To validate our model, we conduct a longitudinal study, enrolling college students and monitoring them over a year, to collect an extensive dataset including physiological, environmental, sleep, metabolic, and physical activity parameters, alongside open-ended textual diaries provided by the partici
&lt;/p&gt;</description></item><item><title>depyf&#26159;&#19968;&#20010;&#38750;&#20405;&#20837;&#24615;&#21644;&#29992;&#25143;&#21451;&#22909;&#30340;&#24037;&#20855;&#65292;&#26088;&#22312;&#24110;&#21161;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#32773;&#25581;&#24320;PyTorch&#32534;&#35793;&#22120;&#30340;&#20869;&#37096;&#24037;&#20316;&#26426;&#21046;&#65292;&#24182;&#36890;&#36807;&#21453;&#32534;&#35793;&#23558;&#23383;&#33410;&#30721;&#36716;&#25442;&#20026;&#28304;&#20195;&#30721;&#65292;&#20174;&#32780;&#22686;&#36827;&#29992;&#25143;&#23545;&#24213;&#23618;&#36807;&#31243;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.13839</link><description>&lt;p&gt;
depyf&#65306;&#20026;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#32773;&#25171;&#24320;PyTorch&#32534;&#35793;&#22120;&#30340;&#31070;&#31192;&#30418;&#23376;
&lt;/p&gt;
&lt;p&gt;
depyf: Open the Opaque Box of PyTorch Compiler for Machine Learning Researchers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13839
&lt;/p&gt;
&lt;p&gt;
depyf&#26159;&#19968;&#20010;&#38750;&#20405;&#20837;&#24615;&#21644;&#29992;&#25143;&#21451;&#22909;&#30340;&#24037;&#20855;&#65292;&#26088;&#22312;&#24110;&#21161;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#32773;&#25581;&#24320;PyTorch&#32534;&#35793;&#22120;&#30340;&#20869;&#37096;&#24037;&#20316;&#26426;&#21046;&#65292;&#24182;&#36890;&#36807;&#21453;&#32534;&#35793;&#23558;&#23383;&#33410;&#30721;&#36716;&#25442;&#20026;&#28304;&#20195;&#30721;&#65292;&#20174;&#32780;&#22686;&#36827;&#29992;&#25143;&#23545;&#24213;&#23618;&#36807;&#31243;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PyTorch 2.x&#24341;&#20837;&#20102;&#19968;&#20010;&#26088;&#22312;&#21152;&#36895;&#28145;&#24230;&#23398;&#20064;&#31243;&#24207;&#30340;&#32534;&#35793;&#22120;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#32773;&#26469;&#35828;&#65292;&#20805;&#20998;&#21033;&#29992;PyTorch&#32534;&#35793;&#22120;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#32534;&#35793;&#22120;&#22312;Python&#23383;&#33410;&#30721;&#32423;&#21035;&#36816;&#34892;&#65292;&#20351;&#20854;&#30475;&#36215;&#26469;&#20687;&#19968;&#20010;&#31070;&#31192;&#30340;&#30418;&#23376;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;depyf&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#25581;&#24320;PyTorch&#32534;&#35793;&#22120;&#20869;&#37096;&#26426;&#21046;&#30340;&#24037;&#20855;&#12290;depyf&#21487;&#20197;&#23558;PyTorch&#29983;&#25104;&#30340;&#23383;&#33410;&#30721;&#21453;&#32534;&#35793;&#20026;&#31561;&#25928;&#30340;&#28304;&#20195;&#30721;&#65292;&#24182;&#24314;&#31435;&#20869;&#23384;&#20013;&#20195;&#30721;&#23545;&#35937;&#19982;&#30913;&#30424;&#19978;&#28304;&#20195;&#30721;&#23545;&#24212;&#30340;&#32852;&#31995;&#12290;&#36825;&#20010;&#29305;&#24615;&#20351;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#35843;&#35797;&#22120;&#36880;&#34892;&#26597;&#30475;&#28304;&#20195;&#30721;&#65292;&#20174;&#32780;&#22686;&#36827;&#23545;&#24213;&#23618;&#36807;&#31243;&#30340;&#29702;&#35299;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;depyf&#26159;&#38750;&#20405;&#20837;&#24615;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;&#65292;&#20027;&#35201;&#20381;&#36182;&#20110;&#20004;&#20010;&#26041;&#20415;&#30340;&#19978;&#19979;&#25991;&#31649;&#29702;&#22120;&#26469;&#23454;&#29616;&#20854;&#26680;&#24515;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13839v1 Announce Type: cross  Abstract: PyTorch \texttt{2.x} introduces a compiler designed to accelerate deep learning programs. However, for machine learning researchers, adapting to the PyTorch compiler to full potential can be challenging. The compiler operates at the Python bytecode level, making it appear as an opaque box. To address this, we introduce \texttt{depyf}, a tool designed to demystify the inner workings of the PyTorch compiler. \texttt{depyf} decompiles bytecode generated by PyTorch back into equivalent source code, and establishes connections between in-memory code objects and their on-disk source code counterparts. This feature enables users to step through the source code line by line using debuggers, thus enhancing their understanding of the underlying processes. Notably, \texttt{depyf} is non-intrusive and user-friendly, primarily relying on two convenient context managers for its core functionality. The project is \href{https://github.com/thuml/depyf}
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#36890;&#36807;&#39044;&#27979;&#19979;&#19968;&#20010;&#36923;&#36753;&#38376;&#26469;&#23454;&#29616;&#30005;&#36335;&#35774;&#35745;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13838</link><description>&lt;p&gt;
&#30005;&#36335;&#21464;&#21387;&#22120;&#65306;&#36890;&#36807;&#39044;&#27979;&#19979;&#19968;&#20010;&#38376;&#23454;&#29616;&#31471;&#21040;&#31471;&#30005;&#36335;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Circuit Transformer: End-to-end Circuit Design by Predicting the Next Gate
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#36890;&#36807;&#39044;&#27979;&#19979;&#19968;&#20010;&#36923;&#36753;&#38376;&#26469;&#23454;&#29616;&#30005;&#36335;&#35774;&#35745;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26159;&#20154;&#31867;&#36890;&#36807;&#24207;&#21015;&#31526;&#21495;&#34920;&#36798;&#30340;&#31361;&#20986;&#33021;&#21147;&#65292;&#36817;&#24180;&#26469;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#22312;&#35745;&#31639;&#19978;&#25484;&#25569;&#20102;&#36825;&#31181;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#24040;&#22823;&#30340;&#31070;&#32463;&#27169;&#22411;&#19981;&#26029;&#39044;&#27979;&#19979;&#19968;&#20010;&#21333;&#35789;&#65292;LLMs&#23637;&#29616;&#20986;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#30005;&#36335;&#20316;&#20026;&#30005;&#23376;&#35774;&#35745;&#30340;&#8220;&#35821;&#35328;&#8221;&#65292;&#36890;&#36807;&#36923;&#36753;&#38376;&#30340;&#32423;&#32852;&#36830;&#25509;&#26469;&#25351;&#23450;&#30005;&#23376;&#35774;&#22791;&#30340;&#21151;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25506;&#32034;&#20102;&#36825;&#31181;&#21487;&#33021;&#24615;&#65292;&#20197;&#36890;&#36807;&#31616;&#21333;&#22320;&#39044;&#27979;&#19979;&#19968;&#20010;&#36923;&#36753;&#38376;&#26469;&#24449;&#26381;&#30005;&#23376;&#35774;&#35745;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13838v1 Announce Type: new  Abstract: Language, a prominent human ability to express through sequential symbols, has been computationally mastered by recent advances of large language models (LLMs). By predicting the next word recurrently with huge neural models, LLMs have shown unprecedented capabilities in understanding and reasoning. Circuit, as the "language" of electronic design, specifies the functionality of an electronic device by cascade connections of logic gates. Then, can circuits also be mastered by a a sufficiently large "circuit model", which can conquer electronic design tasks by simply predicting the next logic gate? In this work, we take the first step to explore such possibilities. Two primary barriers impede the straightforward application of LLMs to circuits: their complex, non-sequential structure, and the intolerance of hallucination due to strict constraints (e.g., equivalence). For the first barrier, we encode a circuit as a memory-less, depth-first 
&lt;/p&gt;</description></item><item><title>TreeDOX&#26159;&#19968;&#31181;&#22522;&#20110;&#26641;&#30340;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#20351;&#29992;&#26102;&#38388;&#24310;&#36831;&#36807;&#24230;&#23884;&#20837;&#21644;&#39069;&#22806;&#26641;&#22238;&#24402;&#22120;&#36827;&#34892;&#29305;&#24449;&#38477;&#32500;&#21644;&#39044;&#27979;&#65292;&#24182;&#22312;&#28145;&#24230;&#39044;&#27979;&#28151;&#27788;&#31995;&#32479;&#20013;&#34920;&#29616;&#20986;state-of-the-art&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13836</link><description>&lt;p&gt;
&#22522;&#20110;&#26641;&#30340;&#23398;&#20064;&#29992;&#20110;&#28145;&#24230;&#39044;&#27979;&#28151;&#27788;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Tree-based Learning for High-Fidelity Prediction of Chaos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13836
&lt;/p&gt;
&lt;p&gt;
TreeDOX&#26159;&#19968;&#31181;&#22522;&#20110;&#26641;&#30340;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#20351;&#29992;&#26102;&#38388;&#24310;&#36831;&#36807;&#24230;&#23884;&#20837;&#21644;&#39069;&#22806;&#26641;&#22238;&#24402;&#22120;&#36827;&#34892;&#29305;&#24449;&#38477;&#32500;&#21644;&#39044;&#27979;&#65292;&#24182;&#22312;&#28145;&#24230;&#39044;&#27979;&#28151;&#27788;&#31995;&#32479;&#20013;&#34920;&#29616;&#20986;state-of-the-art&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#39044;&#27979;&#28151;&#27788;&#31995;&#32479;&#30340;&#26102;&#38388;&#28436;&#21464;&#26159;&#33267;&#20851;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#36827;&#34892;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#36825;&#20005;&#37325;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#22522;&#20110;&#26641;&#30340;&#26041;&#27861;&#65306;TreeDOX&#12290;&#23427;&#20351;&#29992;&#26102;&#38388;&#24310;&#36831;&#36807;&#24230;&#23884;&#20837;&#20316;&#20026;&#26174;&#24335;&#30701;&#26399;&#35760;&#24518;&#65292;&#20197;&#21450;&#39069;&#22806;&#26641;&#22238;&#24402;&#22120;&#26469;&#25191;&#34892;&#29305;&#24449;&#38477;&#32500;&#21644;&#39044;&#27979;&#12290;&#25105;&#20204;&#20351;&#29992;Henon&#26144;&#23556;&#65292;Lorenz&#21644;Kuramoto-Sivashinsky&#31995;&#32479;&#20197;&#21450;&#29616;&#23454;&#19990;&#30028;&#30340;Southern Oscillation Index&#23637;&#31034;&#20102;TreeDOX&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13836v1 Announce Type: new  Abstract: Model-free forecasting of the temporal evolution of chaotic systems is crucial but challenging. Existing solutions require hyperparameter tuning, significantly hindering their wider adoption. In this work, we introduce a tree-based approach not requiring hyperparameter tuning: TreeDOX. It uses time delay overembedding as explicit short-term memory and Extra-Trees Regressors to perform feature reduction and forecasting. We demonstrate the state-of-the-art performance of TreeDOX using the Henon map, Lorenz and Kuramoto-Sivashinsky systems, and the real-world Southern Oscillation Index.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SMART&#26694;&#26550;&#65292;&#21487;&#36890;&#36807;&#20934;&#30830;&#24615;&#32422;&#26463;&#26368;&#23567;&#21270;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#25512;&#29702;&#25104;&#26412;&#65292;&#21516;&#26102;&#30830;&#20445;&#32467;&#26524;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.13835</link><description>&lt;p&gt;
&#20351;&#29992;&#20934;&#30830;&#24615;&#20445;&#35777;&#33258;&#21160;&#32553;&#20943;&#35821;&#35328;&#27169;&#22411;&#35268;&#27169;&#20197;&#38477;&#20302;&#22788;&#29702;&#36153;&#29992;&#30340;SMART
&lt;/p&gt;
&lt;p&gt;
SMART: Automatically Scaling Down Language Models with Accuracy Guarantees for Reduced Processing Fees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13835
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SMART&#26694;&#26550;&#65292;&#21487;&#36890;&#36807;&#20934;&#30830;&#24615;&#32422;&#26463;&#26368;&#23567;&#21270;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#25512;&#29702;&#25104;&#26412;&#65292;&#21516;&#26102;&#30830;&#20445;&#32467;&#26524;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#27493;&#26174;&#33879;&#25552;&#39640;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#37096;&#32626;&#39640;&#24615;&#33021;LLMs&#20250;&#20135;&#29983;&#24040;&#22823;&#30340;&#25104;&#26412;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#22686;&#21152;&#30340;&#21442;&#25968;&#25968;&#37327;&#26088;&#22312;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;&#36825;&#20351;&#24471;&#26368;&#20808;&#36827;&#30340;LLMs&#23545;&#32456;&#31471;&#29992;&#25143;&#32780;&#35328;&#21464;&#24471;&#26356;&#21152;&#26114;&#36149;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;SMART&#65292;&#21363;&#20026;&#38477;&#20302;&#26631;&#35760;&#36153;&#29992;&#32780;&#33258;&#36866;&#24212;&#32553;&#25918;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;LLM&#26694;&#26550;&#65292;&#26088;&#22312;&#26368;&#22823;&#31243;&#24230;&#22320;&#38477;&#20302;NLP&#20219;&#21153;&#30340;&#25512;&#29702;&#25104;&#26412;&#65292;&#21516;&#26102;&#30830;&#20445;&#36275;&#22815;&#30340;&#32467;&#26524;&#36136;&#37327;&#12290;&#23427;&#20351;&#29992;&#25143;&#33021;&#22815;&#20197;&#36755;&#20986;&#30340;&#31561;&#25928;&#24615;&#25351;&#23450;&#20934;&#30830;&#24615;&#32422;&#26463;&#19982;&#26368;&#24378;&#22823;&#30340;LLM&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13835v1 Announce Type: cross  Abstract: The advancement of Large Language Models (LLMs) has significantly boosted performance in natural language processing (NLP) tasks. However, the deployment of high-performance LLMs incurs substantial costs, primarily due to the increased number of parameters aimed at enhancing model performance. This has made the use of state-of-the-art LLMs more expensive for end-users. AI service providers, such as OpenAI and Anthropic, often offer multiple versions of LLMs with varying prices and performance. However, end-users still face challenges in choosing the appropriate LLM for their tasks that balance result quality with cost.   We introduce SMART, Scaling Models Adaptively for Reduced Token Fees, a novel LLM framework designed to minimize the inference costs of NLP tasks while ensuring sufficient result quality. It enables users to specify an accuracy constraint in terms of the equivalence of outputs to those of the most powerful LLM. SMART t
&lt;/p&gt;</description></item><item><title>&#24322;&#36136;&#22270;&#19978;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#38754;&#20020;&#26631;&#31614;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;FLHG&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#21333;&#19968;&#12289;&#21452;&#37325;&#21644;&#22810;&#37325;&#24322;&#36136;&#24615;FLHG&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.13834</link><description>&lt;p&gt;
&#24322;&#36136;&#22270;&#19978;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#65306;&#25361;&#25112;&#12289;&#36827;&#23637;&#21644;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Few-shot Learning on Heterogeneous Graphs: Challenges, Progress, and Prospects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13834
&lt;/p&gt;
&lt;p&gt;
&#24322;&#36136;&#22270;&#19978;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#38754;&#20020;&#26631;&#31614;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;FLHG&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#21333;&#19968;&#12289;&#21452;&#37325;&#21644;&#22810;&#37325;&#24322;&#36136;&#24615;FLHG&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#36136;&#22270;&#19978;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;(FLHG)&#36234;&#26469;&#36234;&#21463;&#21040;&#23398;&#26415;&#30028;&#21644;&#20135;&#19994;&#30028;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#29616;&#26377;&#30340;&#24322;&#36136;&#22270;&#30740;&#31350;&#24448;&#24448;&#21463;&#21040;&#26631;&#31614;&#31232;&#30095;&#24615;&#30340;&#22256;&#25200;&#12290; FLHG&#26088;&#22312;&#35299;&#20915;&#38754;&#23545;&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#26102;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#65292;&#24182;&#24050;&#26377;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#21644;&#24212;&#29992;&#12290; &#26412;&#25991;&#23545;&#29616;&#26377;&#30340;FLHG&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#25361;&#25112;&#12289;&#30740;&#31350;&#36827;&#23637;&#21644;&#26410;&#26469;&#23637;&#26395;&#12290; &#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;FLHG&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#25551;&#36848;&#65292;&#24182;&#23558;&#20854;&#26041;&#27861;&#20998;&#20026;&#19977;&#31181;&#31867;&#22411;&#65306;&#21333;&#24322;&#36136;&#24615;FLHG&#12289;&#21452;&#24322;&#36136;&#24615;FLHG&#21644;&#22810;&#24322;&#36136;&#24615;FLHG&#12290; &#28982;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#27599;&#20010;&#31867;&#21035;&#20869;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#26368;&#26032;&#21644;&#20195;&#34920;&#24615;&#30340;&#21457;&#23637;&#12290; &#26368;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#24182;&#35752;&#35770;&#20102;FLHG&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290; &#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#26412;&#25991;&#26159;&#31532;&#19968;&#31687;&#31995;&#32479;&#32780;&#20840;&#38754;&#30340;FLHG&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13834v1 Announce Type: new  Abstract: Few-shot learning on heterogeneous graphs (FLHG) is attracting more attention from both academia and industry because prevailing studies on heterogeneous graphs often suffer from label sparsity. FLHG aims to tackle the performance degradation in the face of limited annotated data and there have been numerous recent studies proposing various methods and applications. In this paper, we provide a comprehensive review of existing FLHG methods, covering challenges, research progress, and future prospects. Specifically, we first formalize FLHG and categorize its methods into three types: single-heterogeneity FLHG, dual-heterogeneity FLHG, and multi-heterogeneity FLHG. Then, we analyze the research progress within each category, highlighting the most recent and representative developments. Finally, we identify and discuss promising directions for future research in FLHG. To the best of our knowledge, this paper is the first systematic and compr
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#32593;&#32476;&#20013;&#24341;&#20837;&#32447;&#24615;&#32422;&#26463;&#26435;&#37325;&#65288;LCW&#65289;&#26469;&#20943;&#23569;&#28608;&#27963;&#20559;&#31227;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#21069;&#21521;&#32593;&#32476;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.13833</link><description>&lt;p&gt;
&#32447;&#24615;&#32422;&#26463;&#26435;&#37325;&#65306;&#20943;&#23569;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#28608;&#27963;&#20559;&#31227;
&lt;/p&gt;
&lt;p&gt;
Linearly Constrained Weights: Reducing Activation Shift for Faster Training of Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13833
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#24341;&#20837;&#32447;&#24615;&#32422;&#26463;&#26435;&#37325;&#65288;LCW&#65289;&#26469;&#20943;&#23569;&#28608;&#27963;&#20559;&#31227;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#21069;&#21521;&#32593;&#32476;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#30830;&#23450;&#20102;&#28608;&#27963;&#20559;&#31227;&#65292;&#36825;&#26159;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#19968;&#20010;&#31616;&#21333;&#20294;&#26174;&#33879;&#30340;&#29616;&#35937;&#65292;&#21363;&#31070;&#32463;&#20803;&#30340;&#39044;&#28608;&#27963;&#20540;&#20855;&#26377;&#38750;&#38646;&#22343;&#20540;&#65292;&#35813;&#22343;&#20540;&#21462;&#20915;&#20110;&#31070;&#32463;&#20803;&#30340;&#26435;&#37325;&#21521;&#37327;&#19982;&#21069;&#19968;&#23618;&#28608;&#27963;&#21521;&#37327;&#22343;&#20540;&#20043;&#38388;&#30340;&#22841;&#35282;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32447;&#24615;&#32422;&#26463;&#26435;&#37325;&#65288;LCW&#65289;&#65292;&#20197;&#20943;&#23569;&#20840;&#36830;&#25509;&#21644;&#21367;&#31215;&#23618;&#20013;&#30340;&#28608;&#27963;&#20559;&#31227;&#12290;&#20174;&#32593;&#32476;&#21464;&#37327;&#30340;&#26041;&#24046;&#22914;&#20309;&#36890;&#36807;&#21069;&#21521;&#21644;&#21453;&#21521;&#38142;&#20013;&#30340;&#23618;&#25805;&#20316;&#26469;&#25913;&#21464;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#20943;&#23569;&#31070;&#32463;&#32593;&#32476;&#20013;&#28608;&#27963;&#20559;&#31227;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#23427;&#19982;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#30340;&#20851;&#31995;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LCW&#20351;&#20855;&#26377;sigmoid&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#24230;&#21069;&#21521;&#32593;&#32476;&#33021;&#22815;&#36890;&#36807;&#35299;&#20915;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#32780;&#24471;&#20197;&#26377;&#25928;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#19982;&#25209;&#24402;&#19968;&#21270;&#32467;&#21512;&#20351;&#29992;&#65292;LCW&#25913;&#36827;&#20102;genera
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13833v1 Announce Type: cross  Abstract: In this paper, we first identify activation shift, a simple but remarkable phenomenon in a neural network in which the preactivation value of a neuron has non-zero mean that depends on the angle between the weight vector of the neuron and the mean of the activation vector in the previous layer. We then propose linearly constrained weights (LCW) to reduce the activation shift in both fully connected and convolutional layers. The impact of reducing the activation shift in a neural network is studied from the perspective of how the variance of variables in the network changes through layer operations in both forward and backward chains. We also discuss its relationship to the vanishing gradient problem. Experimental results show that LCW enables a deep feedforward network with sigmoid activation functions to be trained efficiently by resolving the vanishing gradient problem. Moreover, combined with batch normalization, LCW improves genera
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#31995;&#32479;&#35843;&#30740;&#20102;&#38024;&#23545;&#20998;&#23376;&#30740;&#31350;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#25991;&#26412;&#19982;&#20998;&#23376;&#20043;&#38388;&#30340;&#20851;&#32852;&#12289;&#19981;&#21516;&#27169;&#22411;&#26550;&#26500;&#21644;&#39044;&#35757;&#32451;&#20219;&#21153;&#12290;&#21516;&#26102;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#25552;&#31034;&#25216;&#26415;&#22312;&#20998;&#23376;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.13830</link><description>&lt;p&gt;
&#25991;&#26412;&#19982;&#20998;&#23376;&#20043;&#38388;&#30340;&#26725;&#26753;&#65306;&#20998;&#23376;&#22810;&#27169;&#24577;&#26694;&#26550;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Bridging Text and Molecule: A Survey on Multimodal Frameworks for Molecule
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#31995;&#32479;&#35843;&#30740;&#20102;&#38024;&#23545;&#20998;&#23376;&#30740;&#31350;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#25991;&#26412;&#19982;&#20998;&#23376;&#20043;&#38388;&#30340;&#20851;&#32852;&#12289;&#19981;&#21516;&#27169;&#22411;&#26550;&#26500;&#21644;&#39044;&#35757;&#32451;&#20219;&#21153;&#12290;&#21516;&#26102;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#25552;&#31034;&#25216;&#26415;&#22312;&#20998;&#23376;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#22312;&#20998;&#23376;&#31185;&#23398;&#39046;&#22495;&#65292;&#23427;&#27491;&#22312;&#25913;&#21464;&#20256;&#32479;&#30340;&#35745;&#31639;&#26426;&#36741;&#21161;&#33539;&#24335;&#65292;&#24341;&#39046;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#26102;&#20195;&#12290;&#38543;&#30528;&#22810;&#27169;&#24577;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#19968;&#31181;&#26032;&#20852;&#36235;&#21183;&#26159;&#26500;&#24314;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#20197;&#20849;&#21516;&#24314;&#27169;&#20998;&#23376;&#21644;&#25991;&#26412;&#39046;&#22495;&#30693;&#35782;&#12290;&#26412;&#25991;&#39318;&#27425;&#31995;&#32479;&#22320;&#35843;&#30740;&#20102;&#38024;&#23545;&#20998;&#23376;&#30740;&#31350;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20174;&#20998;&#23376;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#20837;&#25163;&#65292;&#25351;&#20986;&#28041;&#21450;&#25991;&#26412;&#27169;&#24577;&#30340;&#24517;&#35201;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20851;&#27880;&#20102;&#25991;&#26412;-&#20998;&#23376;&#23545;&#40784;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#26681;&#25454;&#23427;&#20204;&#30340;&#26550;&#26500;&#23558;&#24403;&#21069;&#27169;&#22411;&#20998;&#20026;&#20004;&#32452;&#65292;&#24182;&#21015;&#20986;&#30456;&#20851;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25552;&#31034;&#25216;&#26415;&#22312;&#20998;&#23376;&#20219;&#21153;&#21644;&#39044;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13830v1 Announce Type: cross  Abstract: Artificial intelligence has demonstrated immense potential in scientific research. Within molecular science, it is revolutionizing the traditional computer-aided paradigm, ushering in a new era of deep learning. With recent progress in multimodal learning and natural language processing, an emerging trend has targeted at building multimodal frameworks to jointly model molecules with textual domain knowledge. In this paper, we present the first systematic survey on multimodal frameworks for molecules research. Specifically,we begin with the development of molecular deep learning and point out the necessity to involve textual modality. Next, we focus on recent advances in text-molecule alignment methods, categorizing current models into two groups based on their architectures and listing relevant pre-training tasks. Furthermore, we delves into the utilization of large language models and prompting techniques for molecular tasks and prese
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#25511;&#20998;&#35299;&#25193;&#25955;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#20998;&#23376;&#20248;&#21270;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.13829</link><description>&lt;p&gt;
DecompOpt&#65306;&#29992;&#20110;&#22522;&#20110;&#32467;&#26500;&#30340;&#20998;&#23376;&#20248;&#21270;&#30340;&#21487;&#25511;&#20998;&#35299;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DecompOpt: Controllable and Decomposed Diffusion Models for Structure-based Molecular Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13829
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#25511;&#20998;&#35299;&#25193;&#25955;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#20998;&#23376;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;3D&#29983;&#25104;&#27169;&#22411;&#22312;&#22522;&#20110;&#32467;&#26500;&#30340;&#33647;&#29289;&#35774;&#35745;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;&#65292;&#36890;&#36807;&#23398;&#20064;&#22312;&#32473;&#23450;&#38774;&#26631;&#32467;&#21512;&#20301;&#28857;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#37197;&#20307;&#12290;&#28982;&#32780;&#65292;&#20165;&#24314;&#27169;&#38774;-&#37197;&#20307;&#20998;&#24067;&#20960;&#20046;&#26080;&#27861;&#23454;&#29616;&#33647;&#29289;&#21457;&#29616;&#30340;&#20027;&#35201;&#30446;&#26631;&#20043;&#19968;&#8212;&#8212;&#35774;&#35745;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#65288;&#22914;&#39640;&#32467;&#21512;&#20146;&#21644;&#21147;&#65292;&#26131;&#21512;&#25104;&#31561;&#65289;&#30340;&#26032;&#22411;&#37197;&#20307;&#12290;&#24403;&#29992;&#20110;&#35757;&#32451;&#30340;&#38774;-&#37197;&#20307;&#23545;&#19981;&#31526;&#21512;&#36825;&#20123;&#26399;&#26395;&#23646;&#24615;&#26102;&#65292;&#36825;&#19968;&#25361;&#25112;&#23588;&#20026;&#26126;&#26174;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;\textit{&#20174;&#26080;&#21040;&#26377;}&#30340;&#35774;&#35745;&#20219;&#21153;&#65292;&#32780;&#35768;&#22810;&#38656;&#35201;&#28789;&#27963;&#21487;&#25511;&#24615;&#30340;&#29983;&#25104;&#22330;&#26223;&#65292;&#22914;R&#22522;&#22242;&#20248;&#21270;&#21644;&#39592;&#26550;&#36339;&#36291;&#31561;&#65292;&#21364;&#21463;&#21040;&#20102;&#24456;&#23569;&#20851;&#27880;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DecompOpt&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#30340;&#20998;&#23376;&#20248;&#21270;&#26041;&#27861;&#65292;&#22522;&#20110;&#19968;&#20010;&#21487;&#25511;&#19988;&#20998;&#35299;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;DecompOpt&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#33539;&#24335;&#65292;&#32467;&#21512;&#20102;&#20248;&#21270;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13829v1 Announce Type: cross  Abstract: Recently, 3D generative models have shown promising performances in structure-based drug design by learning to generate ligands given target binding sites. However, only modeling the target-ligand distribution can hardly fulfill one of the main goals in drug discovery -- designing novel ligands with desired properties, e.g., high binding affinity, easily synthesizable, etc. This challenge becomes particularly pronounced when the target-ligand pairs used for training do not align with these desired properties. Moreover, most existing methods aim at solving \textit{de novo} design task, while many generative scenarios requiring flexible controllability, such as R-group optimization and scaffold hopping, have received little attention. In this work, we propose DecompOpt, a structure-based molecular optimization method based on a controllable and decomposed diffusion model. DecompOpt presents a new generation paradigm which combines optimi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#25512;&#29702;&#30340;&#33258;&#30417;&#30563;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#20351;&#24471;&#26080;&#20154;&#26426;&#33021;&#22815;&#36890;&#36807;&#19990;&#30028;&#27169;&#22411;&#21644;&#20027;&#21160;&#25512;&#29702;&#36827;&#34892;&#23454;&#26102;&#33258;&#20027;&#20915;&#31574;&#21644;&#22312;&#32447;&#35268;&#21010;&#65292;&#26377;&#21161;&#20110;&#26356;&#24555;&#22320;&#36866;&#24212;&#26032;&#24773;&#20917;&#65292;&#24182;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;RL&#12290;</title><link>https://arxiv.org/abs/2403.13827</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#21160;&#25512;&#29702;&#30340;&#33258;&#30417;&#30563;&#36335;&#24452;&#35268;&#21010;&#22312;&#26080;&#20154;&#26426;&#36741;&#21161;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Path Planning in UAV-aided Wireless Networks based on Active Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13827
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#25512;&#29702;&#30340;&#33258;&#30417;&#30563;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#20351;&#24471;&#26080;&#20154;&#26426;&#33021;&#22815;&#36890;&#36807;&#19990;&#30028;&#27169;&#22411;&#21644;&#20027;&#21160;&#25512;&#29702;&#36827;&#34892;&#23454;&#26102;&#33258;&#20027;&#20915;&#31574;&#21644;&#22312;&#32447;&#35268;&#21010;&#65292;&#26377;&#21161;&#20110;&#26356;&#24555;&#22320;&#36866;&#24212;&#26032;&#24773;&#20917;&#65292;&#24182;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;RL&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#26080;&#20154;&#26426;&#36741;&#21161;&#32593;&#32476;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#20248;&#21270;&#22120;&#31163;&#32447;&#35299;&#20915;&#35757;&#32451;&#31034;&#20363;&#65292;&#28982;&#21518;&#20351;&#29992;&#24471;&#21040;&#30340;&#35299;&#20915;&#26041;&#26696;&#20316;&#20026;&#31034;&#33539;&#65292;&#35753;&#26080;&#20154;&#26426;&#21487;&#20197;&#23398;&#20064;&#19990;&#30028;&#27169;&#22411;&#20197;&#29702;&#35299;&#29615;&#22659;&#65292;&#24182;&#38544;&#24335;&#22320;&#21457;&#29616;&#20248;&#21270;&#22120;&#30340;&#31574;&#30053;&#12290;&#37197;&#22791;&#19990;&#30028;&#27169;&#22411;&#30340;&#26080;&#20154;&#26426;&#21487;&#20197;&#20570;&#20986;&#23454;&#26102;&#33258;&#20027;&#20915;&#31574;&#65292;&#24182;&#21033;&#29992;&#20027;&#21160;&#25512;&#29702;&#36827;&#34892;&#22312;&#32447;&#35268;&#21010;&#12290;&#22312;&#35268;&#21010;&#36807;&#31243;&#20013;&#65292;&#26080;&#20154;&#26426;&#21487;&#20197;&#26681;&#25454;&#39044;&#26399;&#30340;&#24778;&#21916;&#20026;&#19981;&#21516;&#31574;&#30053;&#25171;&#20998;&#65292;&#20174;&#32780;&#36873;&#25321;&#26368;&#20248;&#30340;&#26410;&#26469;&#12290;&#27492;&#22806;&#65292;&#26080;&#20154;&#26426;&#21487;&#20197;&#21033;&#29992;&#19990;&#30028;&#27169;&#22411;&#39044;&#27979;&#20854;&#34892;&#20026;&#30340;&#32467;&#26524;&#65292;&#24182;&#20197;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#35780;&#20272;&#39044;&#26399;&#30340;&#24778;&#21916;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#24471;&#26080;&#20154;&#26426;&#33021;&#22815;&#26356;&#24555;&#22320;&#36866;&#24212;&#26032;&#24773;&#20917;&#65292;&#24182;&#34920;&#29616;&#27604;&#20256;&#32479;RL&#26356;&#22909;&#65292;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13827v1 Announce Type: cross  Abstract: This paper presents a novel self-supervised path-planning method for UAV-aided networks. First, we employed an optimizer to solve training examples offline and then used the resulting solutions as demonstrations from which the UAV can learn the world model to understand the environment and implicitly discover the optimizer's policy. UAV equipped with the world model can make real-time autonomous decisions and engage in online planning using active inference. During planning, UAV can score different policies based on the expected surprise, allowing it to choose among alternative futures. Additionally, UAV can anticipate the outcomes of its actions using the world model and assess the expected surprise in a self-supervised manner. Our method enables quicker adaptation to new situations and better performance than traditional RL, leading to broader generalizability.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#29109;&#30340;&#26041;&#27861;&#65292;&#20197;&#27604;&#36739;&#22270;&#20687;&#38598;&#21512;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#32780;&#26080;&#38656;&#30495;&#23454;&#26631;&#20934;&#30693;&#35782;&#19988;&#26131;&#20110;&#35745;&#31639;&#65292;&#24182;&#23637;&#31034;&#20102;&#19981;&#21516;&#39044;&#35757;&#32451;&#32593;&#32476;&#36873;&#25321;&#23545;&#25105;&#20204;&#35201;&#35780;&#20272;&#30340;&#22810;&#26679;&#24615;&#27010;&#24565;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.13826</link><description>&lt;p&gt;
&#22312;&#20849;&#21019;&#22270;&#20687;&#29983;&#25104;&#20013;&#34913;&#37327;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Measuring Diversity in Co-creative Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13826
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#29109;&#30340;&#26041;&#27861;&#65292;&#20197;&#27604;&#36739;&#22270;&#20687;&#38598;&#21512;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#32780;&#26080;&#38656;&#30495;&#23454;&#26631;&#20934;&#30693;&#35782;&#19988;&#26131;&#20110;&#35745;&#31639;&#65292;&#24182;&#23637;&#31034;&#20102;&#19981;&#21516;&#39044;&#35757;&#32451;&#32593;&#32476;&#36873;&#25321;&#23545;&#25105;&#20204;&#35201;&#35780;&#20272;&#30340;&#22810;&#26679;&#24615;&#27010;&#24565;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#24050;&#34987;&#25552;&#20986;&#20316;&#20026;&#35780;&#20272;&#20849;&#21019;&#31995;&#32479;&#29983;&#25104;&#20869;&#23481;&#30340;&#21512;&#29702;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#65292;&#20154;&#20204;&#23545;&#21518;&#32773;&#30340;&#26500;&#25104;&#20197;&#21450;&#22914;&#20309;&#34913;&#37327;&#23427;&#23578;&#26080;&#32479;&#19968;&#24847;&#35265;&#12290;&#30446;&#21069;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#23558;&#27169;&#22411;&#36755;&#20986;&#19982;&#19968;&#20010;&#21487;&#33021;&#19981;&#23384;&#22312;&#30340;&#30495;&#23454;&#26631;&#20934;&#36827;&#34892;&#27604;&#36739;&#65292;&#25110;&#32773;&#28041;&#21450;&#21040;&#19981;&#20999;&#23454;&#38469;&#30340;&#35745;&#31639;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#29109;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#29992;&#20110;&#27604;&#36739;&#22270;&#20687;&#38598;&#21512;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#32780;&#26080;&#38656;&#30495;&#23454;&#26631;&#20934;&#30693;&#35782;&#19988;&#26131;&#20110;&#35745;&#31639;&#12290;&#25105;&#20204;&#36824;&#27604;&#36739;&#20102;&#20004;&#20010;&#39044;&#35757;&#32451;&#32593;&#32476;&#65292;&#24182;&#23637;&#31034;&#20102;&#36873;&#25321;&#22914;&#20309;&#19982;&#25105;&#20204;&#24819;&#35201;&#35780;&#20272;&#30340;&#22810;&#26679;&#24615;&#27010;&#24565;&#30456;&#20851;&#32852;&#12290;&#26368;&#21518;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#20123;&#27979;&#37327;&#26041;&#27861;&#22312;&#20132;&#20114;&#24335;&#31995;&#32479;&#30340;&#26500;&#24605;&#12289;&#27169;&#22411;&#35780;&#20272;&#20197;&#21450;&#20854;&#20182;&#24212;&#29992;&#39046;&#22495;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13826v1 Announce Type: cross  Abstract: Quality and diversity have been proposed as reasonable heuristics for assessing content generated by co-creative systems, but to date there has been little agreement around what constitutes the latter or how to measure it. Proposed approaches for assessing generative models in terms of diversity have limitations in that they compare the model's outputs to a ground truth that in the era of large pre-trained generative models might not be available, or entail an impractical number of computations. We propose an alternative based on entropy of neural network encodings for comparing diversity between sets of images that does not require ground-truth knowledge and is easy to compute. We also compare two pre-trained networks and show how the choice relates to the notion of diversity that we want to evaluate. We conclude with a discussion of the potential applications of these measures for ideation in interactive systems, model evaluation, an
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20960;&#20309;&#24863;&#30693;&#29983;&#25104;&#27169;&#22411;IEA-GAN&#65292;&#20197;&#21450;&#19968;&#31181;&#36814;&#25509;&#19979;&#28216;&#29289;&#29702;&#20998;&#26512;&#25361;&#25112;&#30340;YonedaVAE&#27169;&#22411;&#65292;&#20026;&#36229;&#39640;&#32454;&#31890;&#24230;&#31890;&#23376;&#29289;&#29702;&#25506;&#27979;&#22120;&#27169;&#25311;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.13825</link><description>&lt;p&gt;
&#28145;&#23618;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#36229;&#39640;&#32454;&#31890;&#24230;&#31890;&#23376;&#29289;&#29702;&#25506;&#27979;&#22120;&#27169;&#25311;&#65306;&#20174;&#20223;&#30495;&#21040;&#22806;&#25512;&#30340;&#33322;&#31243;
&lt;/p&gt;
&lt;p&gt;
Deep Generative Models for Ultra-High Granularity Particle Physics Detector Simulation: A Voyage From Emulation to Extrapolation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13825
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20960;&#20309;&#24863;&#30693;&#29983;&#25104;&#27169;&#22411;IEA-GAN&#65292;&#20197;&#21450;&#19968;&#31181;&#36814;&#25509;&#19979;&#28216;&#29289;&#29702;&#20998;&#26512;&#25361;&#25112;&#30340;YonedaVAE&#27169;&#22411;&#65292;&#20026;&#36229;&#39640;&#32454;&#31890;&#24230;&#31890;&#23376;&#29289;&#29702;&#25506;&#27979;&#22120;&#27169;&#25311;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31890;&#23376;&#29289;&#29702;&#20013;&#27169;&#25311;&#36229;&#39640;&#32454;&#31890;&#24230;&#25506;&#27979;&#22120;&#21709;&#24212;&#26159;&#19968;&#39033;&#33267;&#20851;&#37325;&#35201;&#20294;&#35745;&#31639;&#37327;&#24040;&#22823;&#30340;&#20219;&#21153;&#12290;&#26412;&#35770;&#25991;&#26088;&#22312;&#20026;Belle II&#23454;&#39564;&#30340;Pixel Vertex Detector (PXD) &#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#35813;&#23454;&#39564;&#20855;&#26377;&#36229;&#36807;750&#19975;&#20687;&#32032;&#36890;&#36947;-&#26159;&#26377;&#21490;&#20197;&#26469;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#20998;&#26512;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#26368;&#39640;&#30340;&#25506;&#27979;&#22120;&#27169;&#25311;&#25968;&#25454;&#38598;&#12290;&#35770;&#25991;&#39318;&#20808;&#23545;&#29992;&#20110;&#27169;&#25311;&#25506;&#27979;&#22120;&#29305;&#24449;&#30340;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#21644;&#20998;&#31867;&#23398;&#30340;&#22238;&#39038;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#20960;&#20309;&#24863;&#30693;&#30340;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;"Intra-Event Aware Generative Adversarial Network (IEA-GAN)"&#65292;&#24341;&#20837;&#20851;&#31995;&#24335;&#27880;&#24847;&#25512;&#29702;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#36817;&#20284;&#25506;&#27979;&#22120;&#20013;&#30340;"&#20107;&#20214;"&#12290;&#35813;&#30740;&#31350;&#24378;&#35843;&#20102;&#23545;&#19979;&#28216;&#29289;&#29702;&#20998;&#26512;&#32780;&#35328;&#8220;&#20107;&#20214;&#20869;&#20851;&#32852;&#8221;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#30740;&#31350;&#26397;&#30528;&#26356;&#36890;&#29992;&#30340;&#26041;&#27861;&#36808;&#36827;&#65292;&#24182;&#25552;&#20986;&#20102;YonedaVAE&#65292;&#36825;&#26159;&#21463;&#33539;&#30068;&#35770;&#21551;&#21457;&#30340;&#19968;&#31181;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13825v1 Announce Type: cross  Abstract: Simulating ultra-high-granularity detector responses in Particle Physics represents a critical yet computationally demanding task. This thesis aims to overcome this challenge for the Pixel Vertex Detector (PXD) at the Belle II experiment, which features over 7.5M pixel channels-the highest spatial resolution detector simulation dataset ever analysed with generative models. This thesis starts off by a comprehensive and taxonomic review on generative models for simulating detector signatures. Then, it presents the Intra-Event Aware Generative Adversarial Network (IEA-GAN), a new geometry-aware generative model that introduces a relational attentive reasoning and Self-Supervised Learning to approximate an "event" in the detector. This study underscores the importance of intra-event correlation for downstream physics analyses. Building upon this, the work drifts towards a more generic approach and presents YonedaVAE, a Category Theory-insp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25237;&#31726;&#39118;&#26684;&#32858;&#31867;&#21644;&#22522;&#20110;&#27880;&#37322;&#30340;&#36827;&#25915;&#35282;&#33394;&#32858;&#31867;&#65292;&#26356;&#19987;&#38376;&#22320;&#20998;&#26512;&#20102;&#27604;&#36187;&#39118;&#26684;&#20860;&#23481;&#24615;&#23545;&#24471;&#20998;&#25928;&#29575;&#30340;&#24433;&#21709;&#65292;&#38598;&#20013;&#22312;&#36827;&#25915;&#31471;&#12290;</title><link>https://arxiv.org/abs/2403.13821</link><description>&lt;p&gt;
&#29992;&#25237;&#31726;&#39118;&#26684;&#21644;&#36827;&#25915;&#35282;&#33394;&#23545;&#31726;&#29699;&#29699;&#21592;&#36827;&#34892;&#32858;&#31867;&#30340;&#36827;&#25915;&#38453;&#23481;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Offensive Lineup Analysis in Basketball with Clustering Players Based on Shooting Style and Offensive Role
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25237;&#31726;&#39118;&#26684;&#32858;&#31867;&#21644;&#22522;&#20110;&#27880;&#37322;&#30340;&#36827;&#25915;&#35282;&#33394;&#32858;&#31867;&#65292;&#26356;&#19987;&#38376;&#22320;&#20998;&#26512;&#20102;&#27604;&#36187;&#39118;&#26684;&#20860;&#23481;&#24615;&#23545;&#24471;&#20998;&#25928;&#29575;&#30340;&#24433;&#21709;&#65292;&#38598;&#20013;&#22312;&#36827;&#25915;&#31471;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31726;&#29699;&#27604;&#36187;&#20013;&#65292;&#24471;&#20998;&#25928;&#29575;&#30001;&#20110;&#27599;&#22330;&#27604;&#36187;&#20013;&#30340;&#22823;&#37327;&#36827;&#25915;&#27425;&#25968;&#32780;&#26174;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#25552;&#39640;&#24471;&#20998;&#25928;&#29575;&#38656;&#35201;&#29699;&#21592;&#20043;&#38388;&#20855;&#26377;&#19981;&#21516;&#27604;&#36187;&#39118;&#26684;&#30340;&#26377;&#25928;&#21327;&#20316;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20013;&#65292;&#26366;&#23545;&#31726;&#29699;&#38453;&#23481;&#36827;&#34892;&#20998;&#26512;&#65292;&#20294;&#23427;&#20204;&#30340;&#27604;&#36187;&#39118;&#26684;&#20860;&#23481;&#24615;&#23578;&#26410;&#24471;&#21040;&#37327;&#21270;&#26816;&#39564;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#26356;&#20855;&#20307;&#22320;&#20998;&#26512;&#27604;&#36187;&#39118;&#26684;&#20860;&#23481;&#24615;&#23545;&#24471;&#20998;&#25928;&#29575;&#30340;&#24433;&#21709;&#65292;&#37325;&#28857;&#25918;&#22312;&#36827;&#25915;&#31471;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#20004;&#31181;&#26041;&#27861;&#26469;&#25429;&#25417;&#29699;&#21592;&#22312;&#36827;&#25915;&#31471;&#30340;&#27604;&#36187;&#39118;&#26684;&#65306;&#21033;&#29992;&#36319;&#36394;&#25968;&#25454;&#36827;&#34892;&#25237;&#31726;&#39118;&#26684;&#32858;&#31867;&#65292;&#20197;&#21450;&#22522;&#20110;&#27880;&#37322;&#30340;&#27604;&#36187;&#31867;&#22411;&#21644;&#39640;&#32423;&#32479;&#35745;&#25968;&#25454;&#36827;&#34892;&#36827;&#25915;&#35282;&#33394;&#32858;&#31867;&#12290;&#23545;&#20110;&#21069;&#32773;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#25163;&#24037;&#21046;&#20316;&#30340;&#25237;&#31726;&#29305;&#24449;&#21644;&#25237;&#31726;&#39118;&#26684;&#20998;&#24067;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#12290;&#23545;&#20110;&#21518;&#32773;&#65292;&#39318;&#27425;&#23558;&#36719;&#32858;&#31867;&#24212;&#29992;&#20110;&#27604;&#36187;&#31867;&#22411;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13821v1 Announce Type: new  Abstract: In a basketball game, scoring efficiency holds significant importance due to the numerous offensive possessions per game. Enhancing scoring efficiency necessitates effective collaboration among players with diverse playing styles. In previous studies, basketball lineups have been analyzed, but their playing style compatibility has not been quantitatively examined. The purpose of this study is to analyze more specifically the impact of playing style compatibility on scoring efficiency, focusing only on offense. This study employs two methods to capture the playing styles of players on offense: shooting style clustering using tracking data, and offensive role clustering based on annotated playtypes and advanced statistics. For the former, interpretable hand-crafted shot features and Wasserstein distances between shooting style distributions were utilized. For the latter, soft clustering was applied to playtype data for the first time. Subs
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#20154;&#31867;&#30913;&#24515;&#22270;&#20449;&#21495;&#30340;&#20010;&#20154;&#35782;&#21035;&#31995;&#32479;&#65292;&#21033;&#29992;&#31354;&#38388;&#20449;&#24687;&#21644;CNN&#20998;&#31867;&#25216;&#26415;&#65292;&#21462;&#24471;&#20102;97.04%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.13820</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#31867;&#30913;&#24515;&#22270;&#20449;&#21495;&#30340;&#36523;&#20221;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Identity information based on human magnetocardiography signals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13820
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#20154;&#31867;&#30913;&#24515;&#22270;&#20449;&#21495;&#30340;&#20010;&#20154;&#35782;&#21035;&#31995;&#32479;&#65292;&#21033;&#29992;&#31354;&#38388;&#20449;&#24687;&#21644;CNN&#20998;&#31867;&#25216;&#26415;&#65292;&#21462;&#24471;&#20102;97.04%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#20351;&#29992;&#20809;&#25277;&#36816;&#30913;&#24378;&#35745;&#65288;OPMs&#65289;&#25429;&#33719;&#30340;&#30913;&#24515;&#22270;&#65288;MCG&#65289;&#20449;&#21495;&#30340;&#20010;&#20154;&#35782;&#21035;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21033;&#29992;&#27169;&#24335;&#35782;&#21035;&#20998;&#26512;&#19981;&#21516;&#37096;&#20301;&#33719;&#21462;&#30340;&#20449;&#21495;&#65292;&#36890;&#36807;&#22312;&#30001;MCG&#20449;&#21495;&#32452;&#25104;&#30340;&#30697;&#38453;&#19978;&#20351;&#29992;2*2&#31383;&#21475;&#36827;&#34892;&#25195;&#25551;&#12290;&#20026;&#20102;&#21033;&#29992;MCG&#20449;&#21495;&#30340;&#31354;&#38388;&#20449;&#24687;&#65292;&#25105;&#20204;&#23558;&#37051;&#36817;&#23567;&#21306;&#22495;&#30340;&#20449;&#21495;&#36716;&#25442;&#20026;&#25968;&#25454;&#38598;&#30340;&#22235;&#20010;&#36890;&#36947;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#23567;&#27874;&#21464;&#25442;&#23558;&#25968;&#25454;&#36716;&#25442;&#20026;&#26102;&#39057;&#30697;&#38453;&#65292;&#24182;&#37319;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#36827;&#34892;&#20998;&#31867;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#36776;&#35748;&#20010;&#20154;&#26041;&#38754;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;97.04%&#12290;&#36825;&#19968;&#21457;&#29616;&#34920;&#26126;MCG&#20449;&#21495;&#20855;&#26377;&#28508;&#21147;&#29992;&#20110;&#20010;&#20154;&#35782;&#21035;&#31995;&#32479;&#65292;&#20026;&#20010;&#24615;&#21270;&#20581;&#24247;&#31649;&#29702;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13820v1 Announce Type: new  Abstract: We have developed an individual identification system based on magnetocardiography (MCG) signals captured using optically pumped magnetometers (OPMs). Our system utilizes pattern recognition to analyze the signals obtained at different positions on the body, by scanning the matrices composed of MCG signals with a 2*2 window. In order to make use of the spatial information of MCG signals, we transform the signals from adjacent small areas into four channels of a dataset. We further transform the data into time-frequency matrices using wavelet transforms and employ a convolutional neural network (CNN) for classification. As a result, our system achieves an accuracy rate of 97.04% in identifying individuals. This finding indicates that the MCG signal holds potential for use in individual identification systems, offering a valuable tool for personalized healthcare management.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24847;&#22823;&#21033;&#39640;&#20013;&#23398;&#29983;&#30340;&#39640;&#20013;&#32972;&#26223;&#26469;&#39044;&#27979;&#22823;&#23398;&#24405;&#21462;&#36873;&#25321;&#65292;&#30740;&#31350;&#21457;&#29616;&#39640;&#20013;&#25104;&#23601;&#23545;&#24405;&#21462;&#36873;&#25321;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.13819</link><description>&lt;p&gt;
&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#24847;&#22823;&#21033;&#39640;&#20013;&#23398;&#29983;&#30340;&#39640;&#20013;&#32972;&#26223;&#39044;&#27979;&#22823;&#23398;&#24405;&#21462;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
A machine learning approach to predict university enrolment choices through students' high school background in Italy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13819
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24847;&#22823;&#21033;&#39640;&#20013;&#23398;&#29983;&#30340;&#39640;&#20013;&#32972;&#26223;&#26469;&#39044;&#27979;&#22823;&#23398;&#24405;&#21462;&#36873;&#25321;&#65292;&#30740;&#31350;&#21457;&#29616;&#39640;&#20013;&#25104;&#23601;&#23545;&#24405;&#21462;&#36873;&#25321;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#24847;&#22823;&#21033;&#39640;&#20013;&#23398;&#29983;&#22312;&#25968;&#23398;&#21644;&#24847;&#22823;&#21033;&#35821;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#23545;&#20854;&#22823;&#23398;&#24405;&#21462;&#36873;&#25321;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#20851;&#27880;STEM&#65288;&#31185;&#23398;&#12289;&#25216;&#26415;&#12289;&#24037;&#31243;&#21644;&#25968;&#23398;&#65289;&#35838;&#31243;&#12290;&#25105;&#20204;&#21306;&#20998;&#20102;&#39640;&#20013;&#31185;&#23398;&#21644;&#20154;&#25991;&#32972;&#26223;&#30340;&#23398;&#29983;&#65292;&#20026;&#20182;&#20204;&#30340;&#24405;&#21462;&#20559;&#22909;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#24615;&#21035;&#22312;&#23545;&#31867;&#20284;&#20808;&#21069;&#25945;&#32946;&#36873;&#25321;&#21644;&#25104;&#23601;&#30340;&#21453;&#24212;&#26041;&#38754;&#21487;&#33021;&#23384;&#22312;&#30340;&#24046;&#24322;&#12290;&#30740;&#31350;&#37319;&#29992;&#20102;&#26799;&#24230;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#20854;&#39640;&#39044;&#27979;&#24615;&#33021;&#21644;&#25429;&#25417;&#25968;&#25454;&#20013;&#38750;&#32447;&#24615;&#20851;&#31995;&#30340;&#33021;&#21147;&#32780;&#38395;&#21517;&#65292;&#24182;&#35843;&#25972;&#20102;&#19982;&#23398;&#29983;&#30340;&#31038;&#20250;&#20154;&#21475;&#29305;&#24449;&#21644;&#20808;&#21069;&#25945;&#32946;&#25104;&#23601;&#30456;&#20851;&#30340;&#21464;&#37327;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#22522;&#20110;&#20808;&#21069;&#39640;&#20013;&#25104;&#23601;&#30340;&#24405;&#21462;&#36873;&#25321;&#20013;&#23384;&#22312;&#30528;&#26174;&#33879;&#24046;&#24322;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13819v1 Announce Type: new  Abstract: This paper explores the influence of Italian high school students' proficiency in mathematics and the Italian language on their university enrolment choices, specifically focusing on STEM (Science, Technology, Engineering, and Mathematics) courses. We distinguish between students from scientific and humanistic backgrounds in high school, providing valuable insights into their enrolment preferences. Furthermore, we investigate potential gender differences in response to similar previous educational choices and achievements. The study employs gradient boosting methodology, known for its high predicting performance and ability to capture non-linear relationships within data, and adjusts for variables related to the socio-demographic characteristics of the students and their previous educational achievements. Our analysis reveals significant differences in the enrolment choices based on previous high school achievements. The findings shed li
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65292;&#23454;&#29616;&#20102;&#33258;&#20027;&#25628;&#32034;$\mathbf{k}$&#21644;&#23454;&#31354;&#38388;&#65292;&#20197;&#22312;&#24494;&#22411;ARPES&#23454;&#39564;&#20013;&#25214;&#21040;&#24863;&#20852;&#36259;&#20301;&#32622;&#12290;</title><link>https://arxiv.org/abs/2403.13815</link><description>&lt;p&gt;
&#33258;&#20027;&#24494;&#22411;ARPES
&lt;/p&gt;
&lt;p&gt;
Autonomous microARPES
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13815
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65292;&#23454;&#29616;&#20102;&#33258;&#20027;&#25628;&#32034;$\mathbf{k}$&#21644;&#23454;&#31354;&#38388;&#65292;&#20197;&#22312;&#24494;&#22411;ARPES&#23454;&#39564;&#20013;&#25214;&#21040;&#24863;&#20852;&#36259;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35282;&#20998;&#36776;&#20809;&#30005;&#21457;&#23556;&#20809;&#35889;&#23398;&#65288;ARPES&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#26144;&#23556;&#22266;&#20307;&#26448;&#26009;&#21344;&#25454;&#24577;&#30005;&#23376;&#32467;&#26500;&#30340;&#25216;&#26415;&#12290;X&#23556;&#32447;&#32858;&#28966;&#20809;&#23398;&#30340;&#26368;&#26032;&#36827;&#23637;&#23548;&#33268;ARPES&#21457;&#23637;&#20026;&#19968;&#31181;&#24494;&#35266;&#24037;&#20855;&#65292;&#20801;&#35768;&#31354;&#38388;&#26144;&#23556;&#26679;&#21697;&#34920;&#38754;&#19978;&#30340;&#30005;&#23376;&#32467;&#26500;&#12290;&#36825;&#26159;&#20197;&#32791;&#26102;&#30340;&#25195;&#25551;&#36807;&#31243;&#20026;&#20195;&#20215;&#30340;&#65292;&#19981;&#20165;&#35201;&#35206;&#30422;&#19977;&#32500;&#33021;&#37327;-&#21160;&#37327;&#65288;$E, k_z, k_y$&#65289;&#31354;&#38388;&#65292;&#36824;&#35201;&#35206;&#30422;&#20108;&#32500;&#34920;&#38754;&#31215;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#21327;&#35758;&#65292;&#21487;&#20197;&#33258;&#20027;&#25628;&#32034;$\mathbf{k}$&#21644;&#23454;&#31354;&#38388;&#65292;&#20197;&#25214;&#21040;&#29305;&#23450;&#20301;&#32622;&#65292;&#26080;&#35770;&#26159;&#22240;&#20026;&#20854;&#39640;&#20809;&#30005;&#21457;&#23556;&#24378;&#24230;&#36824;&#26159;&#22240;&#20026;&#23574;&#38160;&#30340;&#20809;&#35889;&#29305;&#24449;&#12290;&#35813;&#25628;&#32034;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65292;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#20197;&#21253;&#25324;&#39069;&#22806;&#30340;&#21442;&#25968;&#25110;&#20248;&#21270;&#26631;&#20934;&#12290;&#35813;&#33258;&#20027;&#23454;&#39564;&#25511;&#21046;&#24050;&#22312;SGM4&#24494;&#32858;&#28966;&#31995;&#32479;&#19978;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13815v1 Announce Type: cross  Abstract: Angle-resolved photoemission spectroscopy (ARPES) is a technique used to map the occupied electronic structure of solids. Recent progress in X-ray focusing optics has led to the development of ARPES into a microscopic tool, permitting the electronic structure to be spatially mapped across the surface of a sample. This comes at the expense of a time-consuming scanning process to cover not only a three-dimensional energy-momentum ($E, k_z, k_y$) space but also the two-dimensional surface area. Here, we implement a protocol to autonomously search both $\mathbf{k}$- and real space in order to find positions of particular interest, either because of their high photoemission intensity or because of sharp spectral features. The search is based on the use of Gaussian process regression and can easily be expanded to include additional parameters or optimisation criteria. This autonomous experimental control is implemented on the SGM4 micro-focu
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#33021;&#22815;&#26816;&#27979;Arxiv&#25237;&#31295;&#20013;AI&#25104;&#20998;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#29289;&#29702;&#12289;&#25968;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#25991;&#31456;&#21019;&#24314;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;Originality.ai&#36827;&#34892;&#20998;&#26512;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;98%&#12290;</title><link>https://arxiv.org/abs/2403.13812</link><description>&lt;p&gt;
AI&#29983;&#25104;&#25991;&#26412;&#22312;&#23398;&#26415;&#30740;&#31350;&#20013;&#30340;&#23450;&#37327;&#20998;&#26512;&#65306;&#21033;&#29992;AI&#26816;&#27979;&#24037;&#20855;&#30740;&#31350;Arxiv&#25237;&#31295;&#20013;&#30340;AI&#23384;&#22312;&#24615;
&lt;/p&gt;
&lt;p&gt;
Quantitative Analysis of AI-Generated Texts in Academic Research: A Study of AI Presence in Arxiv Submissions using AI Detection Tool
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13812
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#33021;&#22815;&#26816;&#27979;Arxiv&#25237;&#31295;&#20013;AI&#25104;&#20998;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#29289;&#29702;&#12289;&#25968;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#25991;&#31456;&#21019;&#24314;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;Originality.ai&#36827;&#34892;&#20998;&#26512;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;98%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20154;&#23545;ChatGPT&#24863;&#20852;&#36259;&#65292;&#22240;&#20026;&#23427;&#24050;&#25104;&#20026;&#19968;&#20010;&#31361;&#20986;&#30340;AIGC&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#24773;&#22659;&#19979;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#21709;&#24212;&#65292;&#22914;&#36719;&#20214;&#24320;&#21457;&#21644;&#32500;&#25252;&#12290;ChatGPT&#30340;&#35823;&#29992;&#21487;&#33021;&#24341;&#36215;&#37325;&#22823;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#20844;&#20849;&#23433;&#20840;&#21644;&#25945;&#32946;&#39046;&#22495;&#65292;&#23613;&#31649;&#20854;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#30740;&#31350;&#20154;&#21592;&#22823;&#22810;&#36873;&#25321;&#22312;Arxiv&#19978;&#21457;&#34920;&#20182;&#20204;&#30340;&#20316;&#21697;&#12290;&#26410;&#26469;&#24037;&#20316;&#30340;&#26377;&#25928;&#24615;&#21644;&#29420;&#21019;&#24615;&#21462;&#20915;&#20110;&#22312;&#36825;&#20123;&#36129;&#29486;&#20013;&#26816;&#27979;&#21040;AI&#32452;&#20214;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#19968;&#38656;&#27714;&#65292;&#26412;&#30740;&#31350;&#23558;&#20998;&#26512;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26597;&#30475;&#23398;&#26415;&#26426;&#26500;&#29992;&#20110;&#22312;Arxiv&#19978;&#21457;&#24067;&#30340;&#21051;&#24847;&#21046;&#36896;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#36827;&#34892;&#36825;&#39033;&#30740;&#31350;&#65292;&#20351;&#29992;&#20102;&#29289;&#29702;&#12289;&#25968;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#25991;&#31456;&#21019;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#26032;&#24314;&#31435;&#30340;&#25968;&#25454;&#38598;&#65292;&#25509;&#19979;&#26469;&#30340;&#27493;&#39588;&#26159;&#23558;originality.ai&#25237;&#20837;&#20351;&#29992;&#12290;&#32479;&#35745;&#20998;&#26512;&#26174;&#31034;&#65292;Originality.ai&#38750;&#24120;&#31934;&#20934;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;98%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13812v1 Announce Type: cross  Abstract: Many people are interested in ChatGPT since it has become a prominent AIGC model that provides high-quality responses in various contexts, such as software development and maintenance. Misuse of ChatGPT might cause significant issues, particularly in public safety and education, despite its immense potential. The majority of researchers choose to publish their work on Arxiv. The effectiveness and originality of future work depend on the ability to detect AI components in such contributions. To address this need, this study will analyze a method that can see purposely manufactured content that academic organizations use to post on Arxiv. For this study, a dataset was created using physics, mathematics, and computer science articles. Using the newly built dataset, the following step is to put originality.ai through its paces. The statistical analysis shows that Originality.ai is very accurate, with a rate of 98%.
&lt;/p&gt;</description></item><item><title>PyVRP&#26159;&#19968;&#20010;&#23454;&#29616;&#28151;&#21512;&#36951;&#20256;&#25628;&#32034;&#20316;&#20026;&#26368;&#20808;&#36827;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;VRP&#65289;&#27714;&#35299;&#22120;&#30340;Python&#21253;&#65292;&#22312;&#22810;&#20010;&#31454;&#36187;&#20013;&#21462;&#24471;&#20102;&#31532;&#19968;&#21517;&#65292;&#32467;&#21512;&#20102;Python&#30340;&#28789;&#27963;&#24615;&#21644;C++&#30340;&#24615;&#33021;&#65292;&#20195;&#30721;&#36136;&#37327;&#39640;&#19988;&#22312;VRPTW&#21644;&#26377;&#23481;&#37327;&#38480;&#21046;&#30340;VRP&#38382;&#39064;&#19978;&#36798;&#21040;&#20102;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.13795</link><description>&lt;p&gt;
PyVRP: &#19968;&#20010;&#39640;&#24615;&#33021;&#30340;VRP&#27714;&#35299;&#22120;&#21253;
&lt;/p&gt;
&lt;p&gt;
PyVRP: a high-performance VRP solver package
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13795
&lt;/p&gt;
&lt;p&gt;
PyVRP&#26159;&#19968;&#20010;&#23454;&#29616;&#28151;&#21512;&#36951;&#20256;&#25628;&#32034;&#20316;&#20026;&#26368;&#20808;&#36827;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;VRP&#65289;&#27714;&#35299;&#22120;&#30340;Python&#21253;&#65292;&#22312;&#22810;&#20010;&#31454;&#36187;&#20013;&#21462;&#24471;&#20102;&#31532;&#19968;&#21517;&#65292;&#32467;&#21512;&#20102;Python&#30340;&#28789;&#27963;&#24615;&#21644;C++&#30340;&#24615;&#33021;&#65292;&#20195;&#30721;&#36136;&#37327;&#39640;&#19988;&#22312;VRPTW&#21644;&#26377;&#23481;&#37327;&#38480;&#21046;&#30340;VRP&#38382;&#39064;&#19978;&#36798;&#21040;&#20102;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;PyVRP&#65292;&#19968;&#20010;&#23454;&#29616;&#28151;&#21512;&#36951;&#20256;&#25628;&#32034;&#20316;&#20026;&#26368;&#20808;&#36827;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;VRP&#65289;&#27714;&#35299;&#22120;&#30340;Python&#21253;&#12290;&#35813;&#21253;&#26088;&#22312;&#29992;&#20110;&#24102;&#26377;&#26102;&#38388;&#31383;&#21475;&#65288;VRPTW&#65289;&#30340;VRP&#65292;&#20294;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#20197;&#25903;&#25345;&#20854;&#20182;VRP&#21464;&#20307;&#12290;PyVRP&#32467;&#21512;&#20102;Python&#30340;&#28789;&#27963;&#24615;&#21644;C++&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#22312;C++&#20013;&#23454;&#29616;&#31639;&#27861;&#30340;&#24615;&#33021;&#20851;&#38190;&#37096;&#20998;&#65292;&#21516;&#26102;&#22312;Python&#32423;&#21035;&#20840;&#38754;&#23450;&#21046;&#21270;&#12290;PyVRP&#26159;&#35813;&#31639;&#27861;&#30340;&#19968;&#20010;&#23436;&#21892;&#23454;&#29616;&#65292;&#22312;2021&#24180;DIMACS VRPTW&#25361;&#25112;&#36187;&#20013;&#21517;&#21015;&#31532;&#19968;&#65292;&#22312;&#25913;&#36827;&#21518;&#65292;&#22312;2022&#24180;EURO Meets NeurIPS&#36710;&#36742;&#36335;&#24452;&#31454;&#36187;&#30340;&#38745;&#24577;&#21464;&#20307;&#19978;&#21517;&#21015;&#31532;&#19968;&#12290;&#35813;&#20195;&#30721;&#36981;&#24490;&#33391;&#22909;&#30340;&#36719;&#20214;&#24037;&#31243;&#23454;&#36341;&#65292;&#24182;&#19988;&#26377;&#30528;&#33391;&#22909;&#30340;&#25991;&#26723;&#21644;&#21333;&#20803;&#27979;&#35797;&#12290;PyVRP&#22312;&#33258;&#30001;&#30340;MIT&#35768;&#21487;&#35777;&#19979;&#20813;&#36153;&#25552;&#20379;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;PyVRP&#22312;VRPTW&#21644;&#26377;&#23481;&#37327;&#38480;&#21046;&#30340;VRP&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13795v1 Announce Type: cross  Abstract: We introduce PyVRP, a Python package that implements Hybrid Genetic Search as a state-of-the-art Vehicle Routing Problem (VRP) solver. The package is designed for the VRP with Time Windows (VRPTW), but can be easily extended to support other VRP variants. PyVRP combines the flexibility of Python with the performance of C++, by implementing (only) performance critical parts of the algorithm in C++, while being fully customisable at the Python level. PyVRP is a polished implementation of the algorithm that ranked 1st in the 2021 DIMACS VRPTW Challenge and, after improvements, ranked 1st on the static variant of the EURO Meets NeurIPS 2022 Vehicle Routing Competition. The code follows good software engineering practices, and is well-documented and unit tested. PyVRP is freely available under the liberal MIT license. Through numerical experiments we show that PyVRP achieves state-of-the-art results on the VRPTW and Capacitated VRP. We hope
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#22312;&#33258;&#21160;&#25511;&#21046;&#31995;&#32479;&#20013;&#37096;&#32626;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#21644;&#19981;&#21516;&#38450;&#24481;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#22810;&#31181;&#38450;&#24481;&#26041;&#27861;&#30340;&#26032;&#39062;&#20445;&#25252;&#26041;&#27861;&#65292;&#24182;&#20026;&#30830;&#20445;&#24037;&#19994;&#20013;&#30340;&#31283;&#20581;&#25925;&#38556;&#35786;&#26029;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.13502</link><description>&lt;p&gt;
&#22312;&#33258;&#21160;&#25511;&#21046;&#31995;&#32479;&#20013;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#19982;&#38450;&#24481;&#65306;&#19968;&#39033;&#32508;&#21512;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attacks and Defenses in Automated Control Systems: A Comprehensive Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13502
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#22312;&#33258;&#21160;&#25511;&#21046;&#31995;&#32479;&#20013;&#37096;&#32626;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#21644;&#19981;&#21516;&#38450;&#24481;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#22810;&#31181;&#38450;&#24481;&#26041;&#27861;&#30340;&#26032;&#39062;&#20445;&#25252;&#26041;&#27861;&#65292;&#24182;&#20026;&#30830;&#20445;&#24037;&#19994;&#20013;&#30340;&#31283;&#20581;&#25925;&#38556;&#35786;&#26029;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#26426;&#22120;&#23398;&#20064;&#25972;&#21512;&#21040;&#33258;&#21160;&#25511;&#21046;&#31995;&#32479;&#65288;ACS&#65289;&#20013;&#22686;&#24378;&#20102;&#24037;&#19994;&#36807;&#31243;&#31649;&#29702;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;&#20854;&#20013;&#19968;&#39033;&#38480;&#21046;&#24037;&#19994;&#26222;&#36941;&#37319;&#29992;&#36825;&#20123;&#25216;&#26415;&#30340;&#26159;&#31070;&#32463;&#32593;&#32476;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;Tennessee Eastman&#36807;&#31243;&#25968;&#25454;&#38598;&#22312;ACS&#20013;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#25925;&#38556;&#35786;&#26029;&#26102;&#30340;&#23041;&#32961;&#12290;&#36890;&#36807;&#35780;&#20272;&#19977;&#31181;&#19981;&#21516;&#26550;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#23545;&#20854;&#36827;&#34892;&#20102;&#20845;&#31181;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#24182;&#25506;&#35752;&#20102;&#20116;&#31181;&#19981;&#21516;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#20986;&#20102;&#27169;&#22411;&#23545;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#24378;&#22823;&#33030;&#24369;&#24615;&#20197;&#21450;&#38450;&#24481;&#31574;&#30053;&#30340;&#19981;&#21516;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20445;&#25252;&#26041;&#27861;&#65292;&#23558;&#22810;&#31181;&#38450;&#24481;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#30830;&#20445;&#24037;&#19994;&#20013;&#26426;&#22120;&#23398;&#20064;&#30340;&#23433;&#20840;&#24615;&#25552;&#20379;&#20102;&#19968;&#20123;&#35265;&#35299;&#65292;&#30830;&#20445;&#20102;&#24037;&#19994;&#20013;&#31283;&#20581;&#30340;&#25925;&#38556;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13502v1 Announce Type: new  Abstract: Integrating machine learning into Automated Control Systems (ACS) enhances decision-making in industrial process management. One of the limitations to the widespread adoption of these technologies in industry is the vulnerability of neural networks to adversarial attacks. This study explores the threats in deploying deep learning models for fault diagnosis in ACS using the Tennessee Eastman Process dataset. By evaluating three neural networks with different architectures, we subject them to six types of adversarial attacks and explore five different defense methods. Our results highlight the strong vulnerability of models to adversarial samples and the varying effectiveness of defense strategies. We also propose a novel protection approach by combining multiple defense methods and demonstrate it's efficacy. This research contributes several insights into securing machine learning within ACS, ensuring robust fault diagnosis in industrial 
&lt;/p&gt;</description></item><item><title>&#21512;&#24182;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#21363;&#21487;&#21019;&#24314;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#35299;&#20915;AI&#20013;&#30340;&#22797;&#26434;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.13257</link><description>&lt;p&gt;
Arcee&#30340;MergeKit&#65306;&#29992;&#20110;&#21512;&#24182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
Arcee's MergeKit: A Toolkit for Merging Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13257
&lt;/p&gt;
&lt;p&gt;
&#21512;&#24182;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#21363;&#21487;&#21019;&#24314;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#35299;&#20915;AI&#20013;&#30340;&#22797;&#26434;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#30340;&#24555;&#36895;&#25193;&#24352;&#20026;&#36890;&#36807;&#21512;&#24182;&#20854;&#21442;&#25968;&#26469;&#32467;&#21512;&#36825;&#20123;&#27169;&#22411;&#26816;&#26597;&#28857;&#30340;&#33021;&#21147;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#36801;&#31227;&#23398;&#20064;&#30340;&#36827;&#27493;&#23548;&#33268;&#20102;&#22823;&#37327;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#30340;&#24320;&#21457;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#19987;&#38376;&#38024;&#23545;&#20010;&#21035;&#20219;&#21153;&#36827;&#34892;&#19987;&#38376;&#21270;&#65292;&#26080;&#27861;&#21033;&#29992;&#24444;&#27492;&#30340;&#20248;&#21183;&#12290;&#27169;&#22411;&#21512;&#24182;&#20419;&#36827;&#20102;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#21019;&#24314;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#65292;&#20026;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#21644;&#22810;&#21151;&#33021;&#24615;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;&#36890;&#36807;&#20445;&#30041;&#21407;&#22987;&#27169;&#22411;&#30340;&#22266;&#26377;&#33021;&#21147;&#65292;&#27169;&#22411;&#21512;&#24182;&#35299;&#20915;&#20102;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#22797;&#26434;&#25361;&#25112;&#65292;&#21253;&#25324;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#22256;&#38590;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#19968;&#19981;&#26029;&#25193;&#22823;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MergeKit&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#12289;&#24320;&#28304;&#30340;&#24211;&#65292;&#26088;&#22312;&#20419;&#36827;&#27169;&#22411;&#21512;&#24182;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13257v1 Announce Type: new  Abstract: The rapid expansion of the open-source language model landscape presents an opportunity to merge the competencies of these model checkpoints by combining their parameters. Advances in transfer learning, the process of fine-tuning pre-trained models for specific tasks, has resulted in the development of vast amounts of task-specific models, typically specialized in individual tasks and unable to utilize each other's strengths. Model merging facilitates the creation of multitask models without the need for additional training, offering a promising avenue for enhancing model performance and versatility. By preserving the intrinsic capabilities of the original models, model merging addresses complex challenges in AI - including the difficulties of catastrophic forgetting and multi-task learning. To support this expanding area of research, we introduce MergeKit, a comprehensive, open-source library designed to facilitate the application of mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#38024;&#23545;&#34920;&#29616;&#24615;&#20260;&#23475;&#21644;&#26381;&#21153;&#36136;&#37327;&#20260;&#23475;&#30340;&#32650;&#39548;2&#23433;&#20840;&#20445;&#38556;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.13213</link><description>&lt;p&gt;
&#20174;&#34920;&#29616;&#24615;&#20260;&#23475;&#21040;&#26381;&#21153;&#36136;&#37327;&#20260;&#23475;:&#32650;&#39548;2&#23433;&#20840;&#20445;&#38556;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#38024;&#23545;&#34920;&#29616;&#24615;&#20260;&#23475;&#21644;&#26381;&#21153;&#36136;&#37327;&#20260;&#23475;&#30340;&#32650;&#39548;2&#23433;&#20840;&#20445;&#38556;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#23548;&#33268;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36827;&#27493;&#20063;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#23433;&#20840;&#39118;&#38505;&#65292;&#24182;&#24341;&#21457;&#20102;&#23545;&#20854;&#23545;&#24050;&#32463;&#36793;&#32536;&#21270;&#20154;&#32676;&#30340;&#19981;&#21033;&#24433;&#21709;&#30340;&#25285;&#24551;&#12290;&#23613;&#31649;&#23384;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#20943;&#36731;&#25514;&#26045;&#26469;&#24320;&#21457;&#23433;&#20840;&#20445;&#38556;&#25514;&#26045;&#65292;&#27604;&#22914;&#30417;&#30563;&#24335;&#30340;&#23433;&#20840;&#23450;&#21521;&#24494;&#35843;&#21644;&#21033;&#29992;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65292;&#20294;&#20851;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#20869;&#22312;&#20559;&#35265;&#20173;&#23384;&#22312;&#22810;&#37325;&#20851;&#27880;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#65292;&#20026;&#20102;&#23433;&#20840;&#32780;&#20248;&#21270;&#30340;&#27169;&#22411;&#36890;&#24120;&#20250;&#23637;&#31034;&#22840;&#22823;&#30340;&#23433;&#20840;&#34892;&#20026;&#65292;&#27604;&#22914;&#20986;&#20110;&#39044;&#38450;&#25514;&#26045;&#32780;&#20542;&#21521;&#20110;&#19981;&#22238;&#24212;&#26576;&#20123;&#35831;&#27714;&#12290;&#22240;&#27492;&#65292;&#25991;&#29486;&#20013;&#24050;&#32463;&#35760;&#24405;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#23454;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#20043;&#38388;&#30340;&#26126;&#26174;&#26435;&#34913;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#23433;&#20840;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#35780;&#20272;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13213v1 Announce Type: cross  Abstract: Recent progress in large language models (LLMs) has led to their widespread adoption in various domains. However, these advancements have also introduced additional safety risks and raised concerns regarding their detrimental impact on already marginalized populations. Despite growing mitigation efforts to develop safety safeguards, such as supervised safety-oriented fine-tuning and leveraging safe reinforcement learning from human feedback, multiple concerns regarding the safety and ingrained biases in these models remain. Furthermore, previous work has demonstrated that models optimized for safety often display exaggerated safety behaviors, such as a tendency to refrain from responding to certain requests as a precautionary measure. As such, a clear trade-off between the helpfulness and safety of these models has been documented in the literature. In this paper, we further investigate the effectiveness of safety measures by evaluatin
&lt;/p&gt;</description></item><item><title>FlowerFormer&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#22270;&#21464;&#25442;&#22120;&#65292;&#36890;&#36807;&#21452;&#21521;&#24322;&#27493;&#28040;&#24687;&#20256;&#36882;&#21644;&#22522;&#20110;&#27969;&#31243;&#30340;&#20840;&#23616;&#27880;&#24847;&#21147;&#65292;&#21487;&#20197;&#22686;&#24378;&#31070;&#32463;&#32467;&#26500;&#30340;&#34920;&#24449;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.12821</link><description>&lt;p&gt;
FlowerFormer: &#20351;&#29992;&#22522;&#20110;&#27969;&#24863;&#30693;&#30340;&#22270;&#21464;&#25442;&#22120;&#22686;&#24378;&#31070;&#32463;&#32467;&#26500;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
FlowerFormer: Empowering Neural Architecture Encoding using a Flow-aware Graph Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12821
&lt;/p&gt;
&lt;p&gt;
FlowerFormer&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#22270;&#21464;&#25442;&#22120;&#65292;&#36890;&#36807;&#21452;&#21521;&#24322;&#27493;&#28040;&#24687;&#20256;&#36882;&#21644;&#22522;&#20110;&#27969;&#31243;&#30340;&#20840;&#23616;&#27880;&#24847;&#21147;&#65292;&#21487;&#20197;&#22686;&#24378;&#31070;&#32463;&#32467;&#26500;&#30340;&#34920;&#24449;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#23450;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#25104;&#21151;&#19982;&#20854;&#22788;&#29702;&#30340;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#23494;&#20999;&#30456;&#20851;&#65307;&#27809;&#26377;&#19968;&#31181;&#36866;&#21512;&#25152;&#26377;&#24773;&#20917;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#20154;&#20204;&#20184;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#65292;&#20197;&#24555;&#36895;&#20934;&#30830;&#22320;&#20272;&#35745;&#31070;&#32463;&#32467;&#26500;&#22312;&#29305;&#23450;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#23436;&#25972;&#30340;&#35757;&#32451;&#25110;&#35780;&#20272;&#12290;&#31070;&#32463;&#32467;&#26500;&#32534;&#30721;&#22312;&#20272;&#35745;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#32780;&#23558;&#26550;&#26500;&#35270;&#20026;&#22270;&#30340;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#33394;&#12290;&#20026;&#20102;&#22686;&#24378;&#31070;&#32463;&#32467;&#26500;&#30340;&#34920;&#24449;&#23398;&#20064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FlowerFormer&#65292;&#19968;&#31181;&#24378;&#22823;&#30340;&#22270;&#21464;&#25442;&#22120;&#65292;&#23427;&#34701;&#20837;&#20102;&#31070;&#32463;&#32467;&#26500;&#20869;&#30340;&#20449;&#24687;&#27969;&#12290; FlowerFormer&#30001;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#32452;&#25104;&#65306;&#65288;a&#65289;&#21463;&#27969;&#31243;&#21551;&#21457;&#30340;&#21452;&#21521;&#24322;&#27493;&#28040;&#24687;&#20256;&#36882;&#65307;&#65288;b&#65289;&#24314;&#31435;&#22312;&#22522;&#20110;&#27969;&#31243;&#30340;&#25513;&#30721;&#19978;&#30340;&#20840;&#23616;&#20851;&#27880;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FlowerFormer&#20248;&#20110;&#29616;&#26377;&#31070;&#32463;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12821v1 Announce Type: cross  Abstract: The success of a specific neural network architecture is closely tied to the dataset and task it tackles; there is no one-size-fits-all solution. Thus, considerable efforts have been made to quickly and accurately estimate the performances of neural architectures, without full training or evaluation, for given tasks and datasets. Neural architecture encoding has played a crucial role in the estimation, and graphbased methods, which treat an architecture as a graph, have shown prominent performance. For enhanced representation learning of neural architectures, we introduce FlowerFormer, a powerful graph transformer that incorporates the information flows within a neural architecture. FlowerFormer consists of two key components: (a) bidirectional asynchronous message passing, inspired by the flows; (b) global attention built on flow-based masking. Our extensive experiments demonstrate the superiority of FlowerFormer over existing neural 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#25104;&#26412;&#21644;&#24037;&#20316;&#37327;&#32422;&#26463;&#19979;&#30340;&#25512;&#36831;&#26694;&#26550;&#65288;DeCCaF&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#25104;&#26412;&#25935;&#24863;&#22330;&#26223;&#12289;&#24182;&#21457;&#39044;&#27979;&#21644;&#20154;&#31867;&#24037;&#20316;&#33021;&#21147;&#32422;&#26463;&#31561;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.06906</link><description>&lt;p&gt;
&#25104;&#26412;&#25935;&#24863;&#23398;&#20064;&#22312;&#32771;&#34385;&#24037;&#20316;&#37327;&#32422;&#26463;&#19979;&#25512;&#36831;&#22810;&#20301;&#19987;&#23478;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Cost-Sensitive Learning to Defer to Multiple Experts with Workload Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06906
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#25104;&#26412;&#21644;&#24037;&#20316;&#37327;&#32422;&#26463;&#19979;&#30340;&#25512;&#36831;&#26694;&#26550;&#65288;DeCCaF&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#25104;&#26412;&#25935;&#24863;&#22330;&#26223;&#12289;&#24182;&#21457;&#39044;&#27979;&#21644;&#20154;&#31867;&#24037;&#20316;&#33021;&#21147;&#32422;&#26463;&#31561;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#25512;&#36831;&#65288;L2D&#65289;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#22914;&#20309;&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#31995;&#32479;&#20013;&#23558;&#20915;&#31574;&#25512;&#36831;&#32473;&#20154;&#31867;&#65292;&#20174;&#32780;&#22312;&#20154;&#31867;&#26356;&#26377;&#21487;&#33021;&#27491;&#30830;&#26102;&#25512;&#36831;&#20915;&#31574;&#12290;&#29616;&#26377;L2D&#30740;&#31350;&#24573;&#35270;&#20102;&#38459;&#30861;&#20854;&#23454;&#38469;&#37319;&#29992;&#30340;&#30495;&#23454;&#31995;&#32479;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#21363;&#65306;&#24573;&#35270;&#25104;&#26412;&#25935;&#24863;&#22330;&#26223;&#65292;&#20854;&#20013;&#31532;1&#31867;&#21644;&#31532;2&#31867;&#38169;&#35823;&#30340;&#25104;&#26412;&#19981;&#21516;&#65307;&#35201;&#27714;&#27599;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#23454;&#20363;&#30340;&#24182;&#21457;&#20154;&#31867;&#39044;&#27979;&#65307;&#19981;&#22788;&#29702;&#20154;&#31867;&#24037;&#20316;&#33021;&#21147;&#32422;&#26463;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25104;&#26412;&#21644;&#24037;&#20316;&#37327;&#32422;&#26463;&#19979;&#30340;&#25512;&#36831;&#26694;&#26550;&#65288;DeCCaF&#65289;&#12290;DeCCaF&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;L2D&#26041;&#27861;&#65292;&#37319;&#29992;&#30417;&#30563;&#23398;&#20064;&#26469;&#24314;&#27169;&#20154;&#31867;&#38169;&#35823;&#30340;&#27010;&#29575;&#65292;&#20943;&#23569;&#25968;&#25454;&#35201;&#27714;&#30340;&#38480;&#21046;&#65292;&#24182;&#20351;&#29992;&#32422;&#26463;&#32534;&#31243;&#26469;&#20840;&#23616;&#26368;&#23567;&#21270;&#38169;&#35823;&#25104;&#26412;&#65292;&#21516;&#26102;&#32771;&#34385;&#24037;&#20316;&#37327;&#38480;&#21046;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#31995;&#21015;&#20013;&#27979;&#35797;&#20102;DeCCaF
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06906v1 Announce Type: cross  Abstract: Learning to defer (L2D) aims to improve human-AI collaboration systems by learning how to defer decisions to humans when they are more likely to be correct than an ML classifier. Existing research in L2D overlooks key aspects of real-world systems that impede its practical adoption, namely: i) neglecting cost-sensitive scenarios, where type 1 and type 2 errors have different costs; ii) requiring concurrent human predictions for every instance of the training dataset and iii) not dealing with human work capacity constraints. To address these issues, we propose the deferral under cost and capacity constraints framework (DeCCaF). DeCCaF is a novel L2D approach, employing supervised learning to model the probability of human error under less restrictive data requirements (only one expert prediction per instance) and using constraint programming to globally minimize the error cost subject to workload limitations. We test DeCCaF in a series 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#22320;&#29702;&#31354;&#38388;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#27801;&#28448;&#34647;&#34411;&#30340;&#32321;&#27542;&#22320;&#65292;&#26377;&#26395;&#25552;&#21319;&#26089;&#26399;&#39044;&#35686;&#31995;&#32479;&#21644;&#26377;&#38024;&#23545;&#24615;&#30340;&#25511;&#21046;&#25514;&#26045;&#12290;</title><link>https://arxiv.org/abs/2403.06860</link><description>&lt;p&gt;
&#38750;&#27954;&#27801;&#28448;&#34647;&#34411;&#32321;&#27542;&#22320;&#39044;&#27979;&#30340;&#22320;&#29702;&#31354;&#38388;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Geospatial Approach to Predicting Desert Locust Breeding Grounds in Africa
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06860
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#22320;&#29702;&#31354;&#38388;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#27801;&#28448;&#34647;&#34411;&#30340;&#32321;&#27542;&#22320;&#65292;&#26377;&#26395;&#25552;&#21319;&#26089;&#26399;&#39044;&#35686;&#31995;&#32479;&#21644;&#26377;&#38024;&#23545;&#24615;&#30340;&#25511;&#21046;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27801;&#28448;&#34647;&#34411;&#25104;&#32676;&#25104;&#38431;&#23545;&#20892;&#19994;&#21644;&#39135;&#21697;&#23433;&#20840;&#26500;&#25104;&#37325;&#22823;&#23041;&#32961;&#12290;&#26412;&#30740;&#31350;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#25805;&#20316;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#34647;&#34411;&#30340;&#32321;&#27542;&#22320;&#65292;&#26377;&#26395;&#22686;&#24378;&#26089;&#26399;&#39044;&#35686;&#31995;&#32479;&#21644;&#26377;&#38024;&#23545;&#24615;&#30340;&#25511;&#21046;&#25514;&#26045;&#12290;&#25105;&#20204;&#20174;&#32852;&#21512;&#22269;&#31918;&#39135;&#21644;&#20892;&#19994;&#32452;&#32455;(UN-FAO)&#30340;&#34647;&#34411;&#35266;&#27979;&#35760;&#24405;&#20013;&#25972;&#29702;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#20004;&#31181;&#31867;&#22411;&#30340;&#26102;&#31354;&#36755;&#20837;&#29305;&#24449;&#36827;&#34892;&#20998;&#26512;&#65306;&#36965;&#24863;&#29615;&#22659;&#21644;&#27668;&#20505;&#25968;&#25454;&#65292;&#20197;&#21450;&#22810;&#20809;&#35889;&#22320;&#29699;&#35266;&#27979;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#23450;&#21046;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;(&#19977;&#32500;&#21644;&#22522;&#20110;LSTM&#30340;&#24490;&#29615;&#21367;&#31215;&#32593;&#32476;)&#65292;&#20197;&#21450;Jakubik&#31561;&#20154;&#20110;2023&#24180;&#26368;&#26032;&#21457;&#24067;&#30340;&#22320;&#29702;&#31354;&#38388;&#22522;&#30784;&#27169;&#22411;Prithvi&#12290;&#36825;&#20123;&#27169;&#22411;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#32447;&#65292;&#22522;&#20110;Prithvi&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#26469;&#33258;NASA&#30340;&#35856;&#35843;Landsat&#21644;&#21736;&#20853;-2(HLS)&#22810;&#20809;&#35889;&#22270;&#20687;&#36827;&#34892;&#24494;&#35843;&#32780;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06860v1 Announce Type: new  Abstract: Desert locust swarms present a major threat to agriculture and food security. Addressing this challenge, our study develops an operationally-ready model for predicting locust breeding grounds, which has the potential to enhance early warning systems and targeted control measures. We curated a dataset from the United Nations Food and Agriculture Organization's (UN-FAO) locust observation records and analyzed it using two types of spatio-temporal input features: remotely-sensed environmental and climate data as well as multi-spectral earth observation images. Our approach employed custom deep learning models (three-dimensional and LSTM-based recurrent convolutional networks), along with the geospatial foundational model Prithvi recently released by Jakubik et al., 2023. These models notably outperformed existing baselines, with the Prithvi-based model, fine-tuned on multi-spectral images from NASA's Harmonized Landsat and Sentinel-2 (HLS) 
&lt;/p&gt;</description></item><item><title>&#30830;&#35748;&#32553;&#25918;&#23450;&#24459;&#21407;&#21017;&#22312;&#27169;&#22411;&#39044;&#35757;&#32451;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#25581;&#31034;OpenAI&#21407;&#22987;&#32553;&#25918;&#23450;&#24459;&#35770;&#25991;&#30340;&#19981;&#23436;&#25972;&#32454;&#33410;&#65292;&#24182;&#25506;&#31350;&#39044;&#27979;&#27979;&#35797;&#25439;&#22833;&#36712;&#36857;&#21487;&#38752;&#20844;&#24335;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.06563</link><description>&lt;p&gt;
&#25581;&#24320;&#32553;&#25918;&#23450;&#24459;&#20043;&#35868;&#65306;&#31532;&#19968;&#37096;&#20998;
&lt;/p&gt;
&lt;p&gt;
Unraveling the Mystery of Scaling Laws: Part I
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06563
&lt;/p&gt;
&lt;p&gt;
&#30830;&#35748;&#32553;&#25918;&#23450;&#24459;&#21407;&#21017;&#22312;&#27169;&#22411;&#39044;&#35757;&#32451;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#25581;&#31034;OpenAI&#21407;&#22987;&#32553;&#25918;&#23450;&#24459;&#35770;&#25991;&#30340;&#19981;&#23436;&#25972;&#32454;&#33410;&#65292;&#24182;&#25506;&#31350;&#39044;&#27979;&#27979;&#35797;&#25439;&#22833;&#36712;&#36857;&#21487;&#38752;&#20844;&#24335;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32553;&#25918;&#23450;&#24459;&#21407;&#21017;&#34920;&#26126;&#22312;&#27169;&#22411;&#22823;&#23567;&#12289;&#25968;&#25454;&#38598;&#22823;&#23567;&#21644;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#35745;&#31639;&#36164;&#28304;&#31561;&#21464;&#37327;&#20043;&#38388;&#23384;&#22312;&#24130;&#23450;&#24459;&#30456;&#20851;&#24615;&#12290;&#36825;&#20123;&#21407;&#21017;&#22312;&#20248;&#21270;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#21508;&#20010;&#26041;&#38754;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#26368;&#32456;&#26377;&#21161;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT-4&#12289;Llama&#21644;Gemini&#65289;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;OpenAI&#30340;&#21407;&#22987;&#32553;&#25918;&#23450;&#24459;&#35770;&#25991;&#24182;&#26410;&#25259;&#38706;&#25512;&#23548;&#31934;&#30830;&#32553;&#25918;&#23450;&#24459;&#20844;&#24335;&#25152;&#24517;&#38656;&#30340;&#23436;&#25972;&#32454;&#33410;&#65292;&#20182;&#20204;&#30340;&#32467;&#35770;&#20165;&#22522;&#20110;&#21253;&#21547;&#39640;&#36798;15&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#12290;&#23613;&#31649;&#19968;&#20123;&#21518;&#32493;&#20316;&#21697;&#35797;&#22270;&#25581;&#31034;&#36825;&#20123;&#32454;&#33410;&#24182;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#24573;&#30053;&#20102;&#37325;&#35201;&#22240;&#32032;&#30340;&#35757;&#32451;&#20381;&#36182;&#24615;&#65292;&#22914;&#23398;&#20064;&#36895;&#29575;&#12289;&#19978;&#19979;&#25991;&#38271;&#24230;&#21644;&#25209;&#37327;&#22823;&#23567;&#65292;&#23548;&#33268;&#23427;&#20204;&#26410;&#33021;&#24314;&#31435;&#19968;&#20010;&#21487;&#38752;&#30340;&#39044;&#27979;&#27979;&#35797;&#25439;&#22833;&#36712;&#36857;&#30340;&#20844;&#24335;&#12290;&#22312;&#26412;&#25216;&#26415;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#30830;&#35748;&#20102;&#32553;&#25918;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06563v1 Announce Type: cross  Abstract: Scaling law principles indicate a power-law correlation between loss and variables such as model size, dataset size, and computational resources utilized during training. These principles play a vital role in optimizing various aspects of model pre-training, ultimately contributing to the success of large language models such as GPT-4, Llama and Gemini. However, the original scaling law paper by OpenAI did not disclose the complete details necessary to derive the precise scaling law formulas, and their conclusions are only based on models containing up to 1.5 billion parameters. Though some subsequent works attempt to unveil these details and scale to larger models, they often neglect the training dependency of important factors such as the learning rate, context length and batch size, leading to their failure to establish a reliable formula for predicting the test loss trajectory. In this technical report, we confirm that the scaling 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Vision Mamba&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#65292;&#32467;&#21512;&#20102;&#21367;&#31215;&#23618;&#30340;&#23616;&#37096;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#21644;SSM&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#24615;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.03849</link><description>&lt;p&gt;
MedMamba: &#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#30340;Vision Mamba
&lt;/p&gt;
&lt;p&gt;
MedMamba: Vision Mamba for Medical Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03849
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Vision Mamba&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#65292;&#32467;&#21512;&#20102;&#21367;&#31215;&#23618;&#30340;&#23616;&#37096;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#21644;SSM&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#24615;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#38750;&#24120;&#22522;&#30784;&#21644;&#20851;&#38190;&#30340;&#20219;&#21153;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;CNN&#21644;Transformer&#30340;&#27169;&#22411;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#20998;&#31867;&#21508;&#31181;&#21307;&#23398;&#22270;&#20687;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;CNN&#22312;&#38271;&#36317;&#31163;&#24314;&#27169;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#65292;&#26080;&#27861;&#26377;&#25928;&#25552;&#21462;&#21307;&#23398;&#22270;&#20687;&#20013;&#30340;&#32454;&#31890;&#24230;&#29305;&#24449;&#65292;&#32780;Transformers&#21463;&#21040;&#20108;&#27425;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#38459;&#30861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;Mamba&#34920;&#31034;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#21487;&#20197;&#39640;&#25928;&#22320;&#24314;&#27169;&#38271;&#36317;&#31163;&#20132;&#20114;&#20316;&#29992;&#21516;&#26102;&#20445;&#25345;&#32447;&#24615;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#30340;Vision Mamba&#65288;MedMamba&#65289;&#12290;&#26356;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;Conv-SSM&#27169;&#22359;&#65292;&#23558;&#21367;&#31215;&#23618;&#30340;&#23616;&#37096;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#19982;SSM&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#24615;&#30340;&#33021;&#21147;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;&#20026;&#20102;&#23637;&#31034;MedMamba&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03849v1 Announce Type: cross  Abstract: Medical image classification is a very fundamental and crucial task in the field of computer vision. These years, CNN-based and Transformer-based models are widely used in classifying various medical images. Unfortunately, The limitation of CNNs in long-range modeling capabilities prevent them from effectively extracting fine-grained features in medical images , while Transformers are hampered by their quadratic computational complexity. Recent research has shown that the state space model (SSM) represented by Mamba can efficiently model long-range interactions while maintaining linear computational complexity. Inspired by this, we propose Vision Mamba for medical image classification (MedMamba). More specifically, we introduce a novel Conv-SSM module, which combines the local feature extraction ability of convolutional layers with the ability of SSM to capture long-range dependency. To demonstrate the potential of MedMamba, we conduct
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#24180;&#40836;&#21644;&#24615;&#21035;&#20272;&#35745;&#20013;&#30340;&#33021;&#21147;&#65292;&#23545;&#19981;&#21516;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.02302</link><description>&lt;p&gt;
&#36229;&#36234;&#19987;&#19994;&#21270;&#65306;&#35780;&#20272;MLLMs&#22312;&#24180;&#40836;&#21644;&#24615;&#21035;&#20272;&#35745;&#20013;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Beyond Specialization: Assessing the Capabilities of MLLMs in Age and Gender Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#24180;&#40836;&#21644;&#24615;&#21035;&#20272;&#35745;&#20013;&#30340;&#33021;&#21147;&#65292;&#23545;&#19981;&#21516;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#21464;&#24471;&#24322;&#24120;&#27969;&#34892;&#12290;&#20687;ChatGPT-4V&#21644;Gemini&#36825;&#26679;&#21151;&#33021;&#24378;&#22823;&#30340;&#21830;&#29992;&#27169;&#22411;&#65292;&#20197;&#21450;&#20687;LLaVA&#36825;&#26679;&#30340;&#24320;&#28304;&#27169;&#22411;&#65292;&#26412;&#36136;&#19978;&#37117;&#26159;&#36890;&#29992;&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#35299;&#20915;&#21508;&#31181;&#21508;&#26679;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#20219;&#21153;&#12290;&#36825;&#20123;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#22914;&#27492;&#24378;&#22823;&#30340;&#36890;&#29992;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#20197;&#33267;&#20110;&#23427;&#20204;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#22788;&#29702;&#29978;&#33267;&#26410;&#32463;&#19987;&#38376;&#35757;&#32451;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;&#36804;&#20170;&#20026;&#27490;&#26368;&#24378;&#22823;&#30340;MLLMs&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#27604;&#36739;&#65306;ShareGPT4V&#12289;ChatGPT&#12289;LLaVA-Next &#36827;&#34892;&#20102;&#19987;&#38376;&#20219;&#21153;&#30340;&#24180;&#40836;&#21644;&#24615;&#21035;&#20272;&#35745;&#65292;&#19982;&#25105;&#20204;&#30340;&#26368;&#26032;&#19987;&#19994;&#21270;&#27169;&#22411;MiVOLO&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#36824;&#26356;&#26032;&#20102;MiVOLO&#65292;&#24182;&#22312;&#26412;&#25991;&#20013;&#25552;&#20379;&#20102;&#35814;&#32454;&#20449;&#24687;&#21644;&#26032;&#30340;&#25351;&#26631;&#12290;&#36825;&#31181;&#27604;&#36739;&#20135;&#29983;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#32467;&#26524;&#21644;&#20851;&#20110;&#21442;&#19982;&#27169;&#22411;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#30340;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#21508;&#31181;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02302v1 Announce Type: cross  Abstract: Multimodal Large Language Models (MLLMs) have recently gained immense popularity. Powerful commercial models like ChatGPT-4V and Gemini, as well as open-source ones such as LLaVA, are essentially general-purpose models and are applied to solve a wide variety of tasks, including those in computer vision. These neural networks possess such strong general knowledge and reasoning abilities that they have proven capable of working even on tasks for which they were not specifically trained. We compared the capabilities of the most powerful MLLMs to date: ShareGPT4V, ChatGPT, LLaVA-Next in a specialized task of age and gender estimation with our state-of-the-art specialized model, MiVOLO. We also updated MiVOLO and provide details and new metrics in this article. This comparison has yielded some interesting results and insights about the strengths and weaknesses of the participating models. Furthermore, we attempted various ways to fine-tune 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;OSCaR&#65292;&#26088;&#22312;&#35299;&#20915;&#25551;&#36848;&#22797;&#26434;&#35270;&#35273;&#29615;&#22659;&#20013;&#23545;&#35937;&#29366;&#24577;&#21464;&#21270;&#30340;&#38382;&#39064;&#65292;&#20026;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#39564;&#24179;&#21488;&#12290;</title><link>https://arxiv.org/abs/2402.17128</link><description>&lt;p&gt;
OSCaR:&#23545;&#35937;&#29366;&#24577;&#23383;&#24149;&#21644;&#29366;&#24577;&#21464;&#21270;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
OSCaR: Object State Captioning and State Change Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;OSCaR&#65292;&#26088;&#22312;&#35299;&#20915;&#25551;&#36848;&#22797;&#26434;&#35270;&#35273;&#29615;&#22659;&#20013;&#23545;&#35937;&#29366;&#24577;&#21464;&#21270;&#30340;&#38382;&#39064;&#65292;&#20026;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#39564;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17128v3 &#20844;&#21578;&#31867;&#22411;: &#36328; &#38754;&#21521;&#20154;&#31867;&#22312;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#20132;&#20114;&#35270;&#35282;&#65292;&#26234;&#33021;&#27169;&#22411;&#25512;&#26029;&#21644;&#29702;&#35299;&#23545;&#35937;&#29366;&#24577;&#30340;&#21464;&#21270;&#33021;&#21147;&#26159;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26041;&#38754;&#12290;&#35813;&#20219;&#21153;&#28041;&#21450;&#25551;&#36848;&#22797;&#26434;&#30340;&#35270;&#35273;&#29615;&#22659;&#65292;&#35782;&#21035;&#27963;&#36291;&#23545;&#35937;&#65292;&#20197;&#21450;&#36890;&#36807;&#35821;&#35328;&#35299;&#37322;&#23427;&#20204;&#30340;&#21464;&#21270;&#12290;&#20256;&#32479;&#26041;&#27861;&#23558;&#23545;&#35937;&#23383;&#24149;&#21644;&#29366;&#24577;&#21464;&#21270;&#26816;&#27979;&#36827;&#34892;&#38548;&#31163;&#65292;&#25552;&#20379;&#20102;&#23545;&#21160;&#24577;&#29615;&#22659;&#30340;&#26377;&#38480;&#35270;&#22270;&#12290;&#27492;&#22806;&#65292;&#20381;&#36182;&#20110;&#19968;&#23567;&#22871;&#31526;&#21495;&#21270;&#35789;&#27719;&#26469;&#34920;&#31034;&#21464;&#21270;&#38480;&#21046;&#20102;&#35821;&#35328;&#30340;&#34920;&#36798;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#23545;&#35937;&#29366;&#24577;&#23383;&#24149;&#21644;&#29366;&#24577;&#21464;&#21270;&#34920;&#31034;&#65288;OSCaR&#65289;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#12290;OSCaR&#21253;&#25324;&#26469;&#33258;&#21508;&#31181;&#20027;&#35266;&#35270;&#35282;&#35270;&#39057;&#38598;&#21512;&#30340;14,084&#20010;&#24102;&#27880;&#37322;&#35270;&#39057;&#29255;&#27573;&#65292;&#28085;&#30422;&#36817;1,000&#20010;&#29420;&#29305;&#23545;&#35937;&#12290;&#23427;&#20026;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#39564;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17128v3 Announce Type: cross  Abstract: The capability of intelligent models to extrapolate and comprehend changes in object states is a crucial yet demanding aspect of AI research, particularly through the lens of human interaction in real-world settings. This task involves describing complex visual environments, identifying active objects, and interpreting their changes as conveyed through language. Traditional methods, which isolate object captioning and state change detection, offer a limited view of dynamic environments. Moreover, relying on a small set of symbolic words to represent changes has restricted the expressiveness of the language. To address these challenges, in this paper, we introduce the Object State Captioning and State Change Representation (OSCaR) dataset and benchmark. OSCaR consists of 14,084 annotated video segments with nearly 1,000 unique objects from various egocentric video collections. It sets a new testbed for evaluating multimodal large langua
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#21453;&#20107;&#23454;&#30284;&#30151;&#27835;&#30103;&#24314;&#35758;&#65292;&#38598;&#25104;&#20102;&#22810;&#31181;&#22810;&#32452;&#23398;&#25216;&#26415;&#30340;&#19987;&#23478;&#65292;&#21487;&#25552;&#20379;&#20248;&#36234;&#24615;&#33021;&#21644;&#20915;&#31574;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2402.12190</link><description>&lt;p&gt;
&#22522;&#20110;AI&#30340;&#31934;&#20934;&#32959;&#30244;&#23398;&#65306;&#22522;&#20110;&#22810;&#32452;&#23398;&#25968;&#25454;&#30340;&#20010;&#24615;&#21270;&#21453;&#20107;&#23454;&#27835;&#30103;&#24314;&#35758;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Towards AI-Based Precision Oncology: A Machine Learning Framework for Personalized Counterfactual Treatment Suggestions based on Multi-Omics Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12190
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#21453;&#20107;&#23454;&#30284;&#30151;&#27835;&#30103;&#24314;&#35758;&#65292;&#38598;&#25104;&#20102;&#22810;&#31181;&#22810;&#32452;&#23398;&#25216;&#26415;&#30340;&#19987;&#23478;&#65292;&#21487;&#25552;&#20379;&#20248;&#36234;&#24615;&#33021;&#21644;&#20915;&#31574;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#39537;&#21160;&#30340;&#31934;&#20934;&#32959;&#30244;&#23398;&#20855;&#26377;&#36890;&#36807;&#21033;&#29992;AI&#27169;&#22411;&#20998;&#26512;&#22797;&#26434;&#24739;&#32773;&#29305;&#24449;&#19982;&#23545;&#24212;&#27835;&#30103;&#32467;&#26524;&#20043;&#38388;&#20114;&#21160;&#30340;&#28508;&#21147;&#65292;&#26377;&#26395;&#37325;&#22609;&#30284;&#30151;&#27835;&#30103;&#12290;&#26032;&#25216;&#26415;&#24179;&#21488;&#20419;&#36827;&#20102;&#21450;&#26102;&#33719;&#21462;&#22810;&#27169;&#24577;&#32959;&#30244;&#29983;&#29289;&#23398;&#25968;&#25454;&#65292;&#22914;&#21333;&#32454;&#32990;&#22810;&#32452;&#23398;&#25968;&#25454;&#65292;&#20351;&#24471;&#36825;&#31181;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#25968;&#37327;&#21487;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#25913;&#36827;&#20020;&#24202;&#20915;&#31574;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#22522;&#20110;&#35757;&#32451;&#26377;&#20851;&#22810;&#31181;&#22810;&#32452;&#23398;&#25216;&#26415;&#30340;&#26426;&#22120;&#23398;&#20064;&#19987;&#23478;&#32452;&#25104;&#30340;&#38598;&#25104;&#26469;&#36827;&#34892;&#20010;&#24615;&#21270;&#21453;&#20107;&#23454;&#30284;&#30151;&#27835;&#30103;&#24314;&#35758;&#12290;&#36825;&#20123;&#19987;&#38376;&#30340;&#21453;&#20107;&#23454;&#19987;&#23478;&#26681;&#25454;&#25216;&#26415;&#19981;&#26029;&#32858;&#21512;&#20026;&#24615;&#33021;&#26356;&#20248;&#36234;&#30340;&#19987;&#23478;&#65292;&#21487;&#25552;&#20379;&#20915;&#31574;&#30340;&#32622;&#20449;&#24230;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12190v1 Announce Type: cross  Abstract: AI-driven precision oncology has the transformative potential to reshape cancer treatment by leveraging the power of AI models to analyze the interaction between complex patient characteristics and their corresponding treatment outcomes. New technological platforms have facilitated the timely acquisition of multimodal data on tumor biology at an unprecedented resolution, such as single-cell multi-omics data, making this quality and quantity of data available for data-driven improved clinical decision-making. In this work, we propose a modular machine learning framework designed for personalized counterfactual cancer treatment suggestions based on an ensemble of machine learning experts trained on diverse multi-omics technologies. These specialized counterfactual experts per technology are consistently aggregated into a more powerful expert with superior performance and can provide both confidence and an explanation of its decision. The
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#39640;&#26031;&#22122;&#22768;&#30340;&#36873;&#25321;&#26426;&#21046;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#24213;&#23618;&#26597;&#35810;&#26159;&#26377;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#25552;&#20379;&#32431;&#31929;&#30340;&#21069;&#26399;&#21644;&#21518;&#26399;&#24046;&#20998;&#38544;&#31169;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.06137</link><description>&lt;p&gt;
&#20851;&#20110;&#24102;&#26377;&#39640;&#26031;&#22122;&#22768;&#30340;&#36873;&#25321;&#26426;&#21046;&#30340;&#38544;&#31169;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On the Privacy of Selection Mechanisms with Gaussian Noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06137
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#39640;&#26031;&#22122;&#22768;&#30340;&#36873;&#25321;&#26426;&#21046;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#24213;&#23618;&#26597;&#35810;&#26159;&#26377;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#25552;&#20379;&#32431;&#31929;&#30340;&#21069;&#26399;&#21644;&#21518;&#26399;&#24046;&#20998;&#38544;&#31169;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25253;&#21578;&#22122;&#22768;&#26368;&#22823;&#20540;&#21644;&#38408;&#20540;&#20197;&#19978;&#26159;&#20004;&#20010;&#32463;&#20856;&#30340;&#24046;&#20998;&#38544;&#31169;(DP)&#36873;&#25321;&#26426;&#21046;&#12290;&#23427;&#20204;&#30340;&#36755;&#20986;&#26159;&#36890;&#36807;&#23545;&#19968;&#31995;&#21015;&#20302;&#28789;&#25935;&#24230;&#30340;&#26597;&#35810;&#28155;&#21152;&#22122;&#22768;&#65292;&#24182;&#25253;&#21578;&#28385;&#36275;&#26576;&#20010;&#26465;&#20214;&#30340;&#26597;&#35810;(&#22122;&#22768;&#30340;)&#31572;&#26696;&#30340;&#36523;&#20221;&#26469;&#33719;&#24471;&#30340;&#12290;&#24403;&#22312;&#26597;&#35810;&#19978;&#28155;&#21152;&#25289;&#26222;&#25289;&#26031;&#22122;&#22768;&#26102;&#65292;&#36825;&#20123;&#26426;&#21046;&#30340;&#32431;DP&#20445;&#35777;&#24456;&#23481;&#26131;&#33719;&#24471;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24403;&#20351;&#29992;&#39640;&#26031;&#22122;&#22768;&#23454;&#20363;&#21270;&#26102;&#65292;&#26631;&#20934;&#20998;&#26512;&#21482;&#33021;&#25552;&#20379;&#36817;&#20284;&#30340;DP&#20445;&#35777;&#65292;&#23613;&#31649;&#36825;&#20123;&#26426;&#21046;&#30340;&#36755;&#20986;&#20301;&#20110;&#31163;&#25955;&#31354;&#38388;&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#20351;&#29992;&#39640;&#26031;&#22122;&#22768;&#30340;&#25253;&#21578;&#22122;&#22768;&#26368;&#22823;&#20540;&#21644;&#38408;&#20540;&#20197;&#19978;&#30340;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#39069;&#22806;&#30340;&#20551;&#35774;&#19979;&#65292;&#21363;&#24213;&#23618;&#26597;&#35810;&#26159;&#26377;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#20026;&#25253;&#21578;&#22122;&#22768;&#26368;&#22823;&#20540;&#25552;&#20379;&#32431;&#31929;&#30340;&#21069;&#26399;DP&#30028;&#38480;&#65292;&#20197;&#21450;&#20026;&#38408;&#20540;&#20197;&#19978;&#25552;&#20379;&#32431;&#31929;&#30340;&#21518;&#26399;DP&#30028;&#38480;&#12290;&#24471;&#21040;&#30340;&#30028;&#38480;&#26159;&#32039;&#23494;&#30340;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#21487;&#20197;&#20351;&#29992;&#26631;&#20934;&#20803;&#26041;&#27861;&#25968;&#20540;&#35780;&#20272;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Report Noisy Max and Above Threshold are two classical differentially private (DP) selection mechanisms. Their output is obtained by adding noise to a sequence of low-sensitivity queries and reporting the identity of the query whose (noisy) answer satisfies a certain condition. Pure DP guarantees for these mechanisms are easy to obtain when Laplace noise is added to the queries. On the other hand, when instantiated using Gaussian noise, standard analyses only yield approximate DP guarantees despite the fact that the outputs of these mechanisms lie in a discrete space. In this work, we revisit the analysis of Report Noisy Max and Above Threshold with Gaussian noise and show that, under the additional assumption that the underlying queries are bounded, it is possible to provide pure ex-ante DP bounds for Report Noisy Max and pure ex-post DP bounds for Above Threshold. The resulting bounds are tight and depend on closed-form expressions that can be numerically evaluated using standard met
&lt;/p&gt;</description></item><item><title>EasyInstruct&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#25351;&#20196;&#29983;&#25104;&#12289;&#36873;&#25321;&#21644;&#25552;&#31034;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#30340;&#32452;&#21512;&#21644;&#20132;&#20114;&#65292;&#20351;&#25351;&#20196;&#22788;&#29702;&#26356;&#21152;&#26041;&#20415;&#21644;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.03049</link><description>&lt;p&gt;
EasyInstruct&#65306;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03049
&lt;/p&gt;
&lt;p&gt;
EasyInstruct&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#25351;&#20196;&#29983;&#25104;&#12289;&#36873;&#25321;&#21644;&#25552;&#31034;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#30340;&#32452;&#21512;&#21644;&#20132;&#20114;&#65292;&#20351;&#25351;&#20196;&#22788;&#29702;&#26356;&#21152;&#26041;&#20415;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25351;&#20196;&#35843;&#25972;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#24182;&#25104;&#20026;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21147;&#30340;&#19968;&#31181;&#20851;&#38190;&#25216;&#26415;&#12290;&#20026;&#20102;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#25351;&#20196;&#22788;&#29702;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#25968;&#25454;&#25968;&#37327;&#21644;&#25968;&#25454;&#36136;&#37327;&#20043;&#38388;&#36798;&#21040;&#31934;&#24039;&#30340;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21508;&#31181;&#25351;&#20196;&#22788;&#29702;&#26041;&#27861;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#19981;&#19968;&#33268;&#65292;&#30446;&#21069;&#27809;&#26377;&#26631;&#20934;&#30340;&#24320;&#28304;&#25351;&#20196;&#22788;&#29702;&#23454;&#29616;&#26694;&#26550;&#21487;&#20379;&#31038;&#21306;&#20351;&#29992;&#65292;&#36825;&#20351;&#24471;&#20174;&#19994;&#32773;&#26080;&#27861;&#36827;&#19968;&#27493;&#24320;&#21457;&#21644;&#25512;&#36827;&#12290;&#20026;&#20102;&#20419;&#36827;&#25351;&#20196;&#22788;&#29702;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EasyInstruct&#65292;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;LLMs&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;&#65292;&#23427;&#23558;&#25351;&#20196;&#29983;&#25104;&#12289;&#36873;&#25321;&#21644;&#25552;&#31034;&#27169;&#22359;&#21270;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#30340;&#32452;&#21512;&#21644;&#20132;&#20114;&#12290;EasyInstruct&#24050;&#32463;&#22312;https://github.com/zjunlp/EasyInstruct&#19978;&#20844;&#24320;&#21457;&#24067;&#65292;&#24182;&#24471;&#21040;&#20102;&#31215;&#26497;&#32500;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, instruction tuning has gained increasing attention and emerged as a crucial technique to enhance the capabilities of Large Language Models (LLMs). To construct high-quality instruction datasets, many instruction processing approaches have been proposed, aiming to achieve a delicate balance between data quantity and data quality. Nevertheless, due to inconsistencies that persist among various instruction processing methods, there is no standard open-source instruction processing implementation framework available for the community, which hinders practitioners from further developing and advancing. To facilitate instruction processing research and development, we present EasyInstruct, an easy-to-use instruction processing framework for LLMs, which modularizes instruction generation, selection, and prompting, while also considering their combination and interaction. EasyInstruct is publicly released and actively maintained at https://github.com/zjunlp/EasyInstruct, along 
&lt;/p&gt;</description></item><item><title>DenseFormer&#26159;&#23545;Transformer&#30340;&#31616;&#21333;&#20462;&#25913;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;transformer&#22359;&#20043;&#21518;&#36827;&#34892;&#28145;&#24230;&#21152;&#26435;&#24179;&#22343;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#22256;&#24785;&#24230;&#12290;&#23398;&#21040;&#30340;&#21152;&#26435;&#24179;&#22343;&#26435;&#37325;&#25581;&#31034;&#20102;&#20449;&#24687;&#27969;&#30340;&#36830;&#36143;&#27169;&#24335;&#65292;&#20351;&#24471;DenseFormer&#20855;&#26377;&#26356;&#39640;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#19988;&#22312;&#30456;&#21516;&#22256;&#24785;&#24230;&#19979;&#32988;&#36807;&#20256;&#32479;&#30340;Transformer&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.02622</link><description>&lt;p&gt;
DenseFormer: &#36890;&#36807;&#28145;&#24230;&#21152;&#26435;&#24179;&#22343;&#22686;&#24378;Transformer&#20013;&#30340;&#20449;&#24687;&#27969;
&lt;/p&gt;
&lt;p&gt;
DenseFormer: Enhancing Information Flow in Transformers via Depth Weighted Averaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02622
&lt;/p&gt;
&lt;p&gt;
DenseFormer&#26159;&#23545;Transformer&#30340;&#31616;&#21333;&#20462;&#25913;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;transformer&#22359;&#20043;&#21518;&#36827;&#34892;&#28145;&#24230;&#21152;&#26435;&#24179;&#22343;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#22256;&#24785;&#24230;&#12290;&#23398;&#21040;&#30340;&#21152;&#26435;&#24179;&#22343;&#26435;&#37325;&#25581;&#31034;&#20102;&#20449;&#24687;&#27969;&#30340;&#36830;&#36143;&#27169;&#24335;&#65292;&#20351;&#24471;DenseFormer&#20855;&#26377;&#26356;&#39640;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#19988;&#22312;&#30456;&#21516;&#22256;&#24785;&#24230;&#19979;&#32988;&#36807;&#20256;&#32479;&#30340;Transformer&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;Vaswani&#31561;&#20154;&#65288;2017&#65289;&#30340;Transformer&#26550;&#26500;&#29616;&#24050;&#26222;&#36941;&#24212;&#29992;&#20110;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#65292;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21040;&#35821;&#38899;&#22788;&#29702;&#21644;&#22270;&#20687;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DenseFormer&#65292;&#36825;&#26159;&#23545;&#26631;&#20934;&#26550;&#26500;&#30340;&#31616;&#21333;&#20462;&#25913;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#22256;&#24785;&#24230;&#65292;&#32780;&#19981;&#22686;&#21152;&#20854;&#22823;&#23567;-&#23545;&#20110;&#25317;&#26377;100B&#21442;&#25968;&#33539;&#22260;&#30340;&#22823;&#35268;&#27169;&#27169;&#22411;&#65292;&#21482;&#38656;&#28155;&#21152;&#20960;&#21315;&#20010;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27599;&#20010;transformer&#22359;&#20043;&#21518;&#20381;&#38752;&#39069;&#22806;&#30340;&#24179;&#22343;&#27493;&#39588;&#65292;&#35745;&#31639;&#24403;&#21069;&#21644;&#36807;&#21435;&#34920;&#31034;&#30340;&#21152;&#26435;&#24179;&#22343;-&#25105;&#20204;&#23558;&#36825;&#20010;&#25805;&#20316;&#31216;&#20026;&#28145;&#24230;&#21152;&#26435;&#24179;&#22343;&#65288;DWA&#65289;&#12290;&#23398;&#21040;&#30340;DWA&#26435;&#37325;&#23637;&#29616;&#20102;&#20449;&#24687;&#27969;&#30340;&#36830;&#36143;&#27169;&#24335;&#65292;&#25581;&#31034;&#20102;&#26469;&#33258;&#36828;&#23618;&#30340;&#28608;&#27963;&#30340;&#24378;&#22823;&#19988;&#32467;&#26500;&#21270;&#30340;&#37325;&#22797;&#20351;&#29992;&#12290;&#23454;&#39564;&#35777;&#26126;DenseFormer&#20855;&#26377;&#26356;&#39640;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#33021;&#22815;&#36798;&#21040;&#27604;&#26356;&#28145;&#30340;transformer&#27169;&#22411;&#30456;&#21516;&#30340;&#22256;&#24785;&#24230;&#65292;&#24182;&#19988;&#22312;&#30456;&#21516;&#22256;&#24785;&#24230;&#19979;&#65292;&#36825;&#20123;&#26032;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;transformer&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transformer architecture from Vaswani et al. (2017) is now ubiquitous across application domains, from natural language processing to speech processing and image understanding. We propose DenseFormer, a simple modification to the standard architecture that improves the perplexity of the model without increasing its size -- adding a few thousand parameters for large-scale models in the 100B parameters range. Our approach relies on an additional averaging step after each transformer block, which computes a weighted average of current and past representations -- we refer to this operation as Depth-Weighted-Average (DWA). The learned DWA weights exhibit coherent patterns of information flow, revealing the strong and structured reuse of activations from distant layers. Experiments demonstrate that DenseFormer is more data efficient, reaching the same perplexity of much deeper transformer models, and that for the same perplexity, these new models outperform transformer baselines in terms
&lt;/p&gt;</description></item><item><title>SLIM&#26159;&#19968;&#31181;&#22810;&#21028;&#21035;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#32452;&#21512;&#22810;&#20010;&#21028;&#21035;&#22120;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#28508;&#21464;&#37327;&#25216;&#33021;&#21457;&#29616;&#65292;&#20811;&#26381;&#20102;&#22870;&#21169;&#20043;&#38388;&#30340;&#24178;&#25200;&#12290;</title><link>https://arxiv.org/abs/2402.00823</link><description>&lt;p&gt;
SLIM: &#22810;&#21028;&#21035;&#22120;&#22312;&#25216;&#33021;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
SLIM: Skill Learning with Multiple Critics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00823
&lt;/p&gt;
&lt;p&gt;
SLIM&#26159;&#19968;&#31181;&#22810;&#21028;&#21035;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#32452;&#21512;&#22810;&#20010;&#21028;&#21035;&#22120;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#28508;&#21464;&#37327;&#25216;&#33021;&#21457;&#29616;&#65292;&#20811;&#26381;&#20102;&#22870;&#21169;&#20043;&#38388;&#30340;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#30340;&#25216;&#33021;&#23398;&#20064;&#26088;&#22312;&#33719;&#21462;&#21033;&#29992;&#29615;&#22659;&#30340;&#24213;&#23618;&#21160;&#24577;&#30340;&#26377;&#29992;&#34892;&#20026;&#12290;&#22522;&#20110;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#22312;&#27492;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#39046;&#22495;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;&#30001;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#21487;&#33021;&#28041;&#21450;&#21040;&#29615;&#22659;&#20013;&#24456;&#22810;&#33258;&#30001;&#24230;&#65292;&#21333;&#32431;&#30340;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#26080;&#27861;&#20135;&#29983;&#26377;&#29992;&#30340;&#25805;&#20316;&#34892;&#20026;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SLIM&#65292;&#19968;&#31181;&#38024;&#23545;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#22810;&#21028;&#21035;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35266;&#28857;&#26159;&#65292;&#22312;&#28436;&#21592;-&#35780;&#35770;&#32773;&#26694;&#26550;&#20013;&#21033;&#29992;&#22810;&#20010;&#21028;&#21035;&#22120;&#26469;&#20248;&#38597;&#22320;&#32452;&#21512;&#22810;&#20010;&#22870;&#21169;&#20989;&#25968;&#65292;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#28508;&#21464;&#37327;&#25216;&#33021;&#21457;&#29616;&#65292;&#21516;&#26102;&#20811;&#26381;&#22870;&#21169;&#20043;&#38388;&#21487;&#33021;&#21457;&#29983;&#30340;&#24178;&#25200;&#65292;&#38459;&#30861;&#23545;&#26377;&#29992;&#25216;&#33021;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised skill learning aims to acquire useful behaviors that leverage the underlying dynamics of the environment. Latent variable models, based on mutual information maximization, have been particularly successful in this task but still struggle in the context of robotic manipulation. As it requires impacting a possibly large set of degrees of freedom composing the environment, mutual information maximization fails alone in producing useful manipulation behaviors. To address this limitation, we introduce SLIM, a multi-critic learning approach for skill discovery with a particular focus on robotic manipulation. Our main insight is that utilizing multiple critics in an actor-critic framework to gracefully combine multiple reward functions leads to a significant improvement in latent-variable skill discovery for robotic manipulation while overcoming possible interference occurring among rewards which hinders convergence to useful skills. Furthermore, in the context of tabletop man
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#22270;&#32534;&#36753;&#20316;&#20026;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#20808;&#21069;&#21162;&#21147;&#65292;&#25506;&#35752;&#20102;&#23558;&#36755;&#20837;&#25968;&#25454;&#34920;&#31034;&#20026;&#22270;&#24418;&#23545;&#20110;&#29983;&#25104;&#40657;&#31665;&#22270;&#20687;&#20998;&#31867;&#22120;&#26368;&#23567;&#19988;&#26377;&#24847;&#20041;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#24615;&#33021;&#21644;&#26102;&#38388;&#25928;&#29575;&#26368;&#20339;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2401.11609</link><description>&lt;p&gt;
&#22270;&#32534;&#36753;&#29992;&#20110;&#21453;&#20107;&#23454;&#35299;&#37322;&#65306;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Graph Edits for Counterfactual Explanations: A comparative study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11609
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#22270;&#32534;&#36753;&#20316;&#20026;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#20808;&#21069;&#21162;&#21147;&#65292;&#25506;&#35752;&#20102;&#23558;&#36755;&#20837;&#25968;&#25454;&#34920;&#31034;&#20026;&#22270;&#24418;&#23545;&#20110;&#29983;&#25104;&#40657;&#31665;&#22270;&#20687;&#20998;&#31867;&#22120;&#26368;&#23567;&#19988;&#26377;&#24847;&#20041;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#24615;&#33021;&#21644;&#26102;&#38388;&#25928;&#29575;&#26368;&#20339;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#24050;&#34987;&#30830;&#31435;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#65292;&#21033;&#29992;&#19968;&#32452;&#26368;&#23567;&#30340;&#32534;&#36753;&#26469;&#25913;&#21464;&#20998;&#31867;&#22120;&#30340;&#39044;&#27979;&#12290;&#22312;&#32771;&#34385;&#22270;&#20687;&#19978;&#30340;&#27010;&#24565;&#21453;&#20107;&#23454;&#26102;&#65292;&#35831;&#27714;&#30340;&#32534;&#36753;&#24212;&#23545;&#24212;&#36755;&#20837;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#26174;&#33879;&#27010;&#24565;&#12290;&#21516;&#26102;&#65292;&#27010;&#24565;&#36317;&#31163;&#30001;&#30693;&#35782;&#22270;&#35889;&#23450;&#20041;&#65292;&#30830;&#20445;&#27010;&#24565;&#32534;&#36753;&#30340;&#26368;&#20248;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#25193;&#23637;&#20102;&#20197;&#22270;&#32534;&#36753;&#20026;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#20808;&#21069;&#21162;&#21147;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#28085;&#30422;&#20102;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26041;&#27861;&#12290;&#21040;&#27492;&#20026;&#27490;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20197;&#19979;&#37325;&#35201;&#30340;&#30740;&#31350;&#38382;&#39064;&#65306;&#25105;&#20204;&#24212;&#35813;&#23558;&#36755;&#20837;&#25968;&#25454;&#34920;&#31034;&#20026;&#22270;&#24418;&#65292;&#36825;&#26159;&#29983;&#25104;&#40657;&#31665;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#26368;&#23567;&#21644;&#26377;&#24847;&#20041;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#22312;&#24615;&#33021;&#21644;&#26102;&#38388;&#25928;&#29575;&#26041;&#38754;&#26368;&#20339;&#30340;GNN&#26041;&#27861;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11609v2 Announce Type: replace-cross  Abstract: Counterfactuals have been established as a popular explainability technique which leverages a set of minimal edits to alter the prediction of a classifier. When considering conceptual counterfactuals on images, the edits requested should correspond to salient concepts present in the input data. At the same time, conceptual distances are defined by knowledge graphs, ensuring the optimality of conceptual edits. In this work, we extend previous endeavors on graph edits as counterfactual explanations by conducting a comparative study which encompasses both supervised and unsupervised Graph Neural Network (GNN) approaches. To this end, we pose the following significant research question: should we represent input data as graphs, which is the optimal GNN approach in terms of performance and time efficiency to generate minimal and meaningful counterfactual explanations for black-box image classifiers?
&lt;/p&gt;</description></item><item><title>&#21512;&#25104;&#25968;&#25454;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#28041;&#21450;&#21508;&#31181;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;&#38544;&#31169;&#12289;&#20844;&#24179;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30456;&#20851;&#38382;&#39064;&#65292;&#23545;&#20854;&#36136;&#37327;&#21644;&#25928;&#26524;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2401.00081</link><description>&lt;p&gt;
&#37329;&#34701;&#39046;&#22495;&#20013;&#30340;&#21512;&#25104;&#25968;&#25454;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Synthetic Data Applications in Finance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.00081
&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#28041;&#21450;&#21508;&#31181;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;&#38544;&#31169;&#12289;&#20844;&#24179;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30456;&#20851;&#38382;&#39064;&#65292;&#23545;&#20854;&#36136;&#37327;&#21644;&#25928;&#26524;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#22312;&#21253;&#25324;&#37329;&#34701;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#34394;&#25311;&#29616;&#23454;&#22312;&#20869;&#30340;&#21508;&#31181;&#21830;&#19994;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#21512;&#25104;&#25968;&#25454;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#21407;&#22411;&#24212;&#29992;&#65292;&#24182;&#20026;&#20854;&#20013;&#30340;&#19968;&#20123;&#29305;&#23450;&#24212;&#29992;&#25552;&#20379;&#20102;&#26356;&#20016;&#23500;&#30340;&#32454;&#33410;&#12290;&#36825;&#20123;&#24212;&#29992;&#28085;&#30422;&#20102;&#26469;&#33258;&#24066;&#22330;&#21644;&#38646;&#21806;&#37329;&#34701;&#24212;&#29992;&#30340;&#34920;&#26684;&#12289;&#26102;&#38388;&#24207;&#21015;&#12289;&#20107;&#20214;&#24207;&#21015;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#27169;&#24577;&#30340;&#24191;&#27867;&#25968;&#25454;&#31867;&#22411;&#12290;&#30001;&#20110;&#37329;&#34701;&#26159;&#19968;&#20010;&#21463;&#39640;&#24230;&#30417;&#31649;&#30340;&#34892;&#19994;&#65292;&#21512;&#25104;&#25968;&#25454;&#26159;&#22788;&#29702;&#19982;&#38544;&#31169;&#12289;&#20844;&#24179;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30456;&#20851;&#38382;&#39064;&#30340;&#19968;&#31181;&#28508;&#22312;&#26041;&#27861;&#12290;&#22312;&#36825;&#20123;&#24212;&#29992;&#20013;&#65292;&#20351;&#29992;&#21508;&#31181;&#25351;&#26631;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#36136;&#37327;&#21644;&#26377;&#25928;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#37329;&#34701;&#39046;&#22495;&#21512;&#25104;&#25968;&#25454;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.00081v2 Announce Type: replace  Abstract: Synthetic data has made tremendous strides in various commercial settings including finance, healthcare, and virtual reality. We present a broad overview of prototypical applications of synthetic data in the financial sector and in particular provide richer details for a few select ones. These cover a wide variety of data modalities including tabular, time-series, event-series, and unstructured arising from both markets and retail financial applications. Since finance is a highly regulated industry, synthetic data is a potential approach for dealing with issues related to privacy, fairness, and explainability. Various metrics are utilized in evaluating the quality and effectiveness of our approaches in these applications. We conclude with open directions in synthetic data in the context of the financial domain.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#21387;&#32553;&#29616;&#26377;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#25913;&#36827;&#26435;&#37325;&#26356;&#26032;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20197;&#26356;&#26377;&#25928;&#22320;&#25429;&#25417;&#26356;&#22810;&#26435;&#37325;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2312.17244</link><description>&lt;p&gt;
LLM&#22806;&#31185;&#21307;&#29983;
&lt;/p&gt;
&lt;p&gt;
The LLM Surgeon
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.17244
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#21387;&#32553;&#29616;&#26377;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#25913;&#36827;&#26435;&#37325;&#26356;&#26032;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20197;&#26356;&#26377;&#25928;&#22320;&#25429;&#25417;&#26356;&#22810;&#26435;&#37325;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#36234;&#26469;&#36234;&#24222;&#22823;&#65292;&#20197;&#26399;&#22312;&#22823;&#37327;&#21487;&#29992;&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;Transformer&#26550;&#26500;&#30340;&#24040;&#22823;&#35268;&#27169;&#20351;&#24471;&#22312;&#35745;&#31639;&#12289;&#29615;&#22659;&#25110;&#35774;&#22791;&#29305;&#23450;&#32422;&#26463;&#19979;&#37096;&#32626;&#27169;&#22411;&#21464;&#24471;&#22256;&#38590;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#23545;&#29616;&#26377;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#21387;&#32553;&#20316;&#20026;&#35757;&#32451;&#36739;&#23567;&#27169;&#22411;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#30446;&#26631;&#25439;&#22833;&#26223;&#35266;&#30340;Kronecker&#20998;&#35299;&#26354;&#29575;&#36817;&#20284;&#25193;&#23637;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#26082;&#21487;&#20197;&#35745;&#31639;&#21487;&#21024;&#38500;&#32467;&#26500;&#30340;&#21160;&#24577;&#20998;&#37197;&#65292;&#20063;&#21487;&#20197;&#26356;&#26032;&#21097;&#20313;&#26435;&#37325;&#20197;&#32771;&#34385;&#21024;&#38500;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#38750;&#32467;&#26500;&#21270;&#12289;&#21322;&#32467;&#26500;&#21270;&#21644;&#32467;&#26500;&#21270;&#20462;&#21098;&#65292;&#24182;&#25913;&#36827;&#20102;&#26435;&#37325;&#26356;&#26032;&#20197;&#25429;&#25417;&#26356;&#22810;&#26435;&#37325;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.17244v2 Announce Type: replace-cross  Abstract: State-of-the-art language models are becoming increasingly large in an effort to achieve the highest performance on large corpora of available textual data. However, the sheer size of the Transformer architectures makes it difficult to deploy models within computational, environmental or device-specific constraints. We explore data-driven compression of existing pretrained models as an alternative to training smaller models from scratch. To do so, we scale Kronecker-factored curvature approximations of the target loss landscape to large language models. In doing so, we can compute both the dynamic allocation of structures that can be removed as well as updates of remaining weights that account for the removal. We provide a general framework for unstructured, semi-structured and structured pruning and improve upon weight updates to capture more correlations between weights, while remaining computationally efficient. Experimental
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#32771;&#34385;&#20102;&#25439;&#22833;&#21644;&#19981;&#30830;&#23450;&#24615;&#22522;&#30784;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#22312;&#32447;&#24615;&#20998;&#31867;&#22120;&#21644;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#38598;&#19978;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#24182;&#23637;&#31034;&#20102;&#20854;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2312.13927</link><description>&lt;p&gt;
&#20851;&#20110;&#25439;&#22833;&#21644;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the convergence of loss and uncertainty-based active learning algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.13927
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#32771;&#34385;&#20102;&#25439;&#22833;&#21644;&#19981;&#30830;&#23450;&#24615;&#22522;&#30784;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#22312;&#32447;&#24615;&#20998;&#31867;&#22120;&#21644;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#38598;&#19978;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#24182;&#23637;&#31034;&#20102;&#20854;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#19981;&#21516;&#20551;&#35774;&#19979;&#25439;&#22833;&#21644;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#32452;&#26465;&#20214;&#65292;&#30830;&#20445;&#22312;&#24212;&#29992;&#20110;&#32447;&#24615;&#20998;&#31867;&#22120;&#21644;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#38598;&#26102;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#36825;&#21253;&#25324;&#35777;&#26126;&#21508;&#31181;&#25439;&#22833;&#20989;&#25968;&#30340;&#22522;&#20110;&#25439;&#22833;&#30340;&#37319;&#26679;&#30340;&#25910;&#25947;&#36895;&#24230;&#20445;&#35777;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#24050;&#30693;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#30028;&#38480;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23548;&#20986;&#25439;&#22833;&#37319;&#26679;&#30340;&#25910;&#25947;&#36895;&#29575;&#30028;&#38480;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#23558;&#28857;&#37319;&#26679;&#21644;&#38543;&#26426;Polyak&#27493;&#38271;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#20851;&#20110;&#37319;&#26679;&#36807;&#31243;&#30340;&#26465;&#20214;&#65292;&#30830;&#20445;&#35813;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#20445;&#35777;&#65292;&#29305;&#21035;&#26159;&#22312;&#20809;&#28369;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#32467;&#26524;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.13927v2 Announce Type: replace-cross  Abstract: We consider the convergence rates of loss and uncertainty-based active learning algorithms under various assumptions. Firstly, we establish a set of conditions that ensure convergence rates when applied to linear classifiers and linearly separable datasets. This includes demonstrating convergence rate guarantees for loss-based sampling with various loss functions. Secondly, we introduce a framework that allows us to derive convergence rate bounds for loss-based sampling by leveraging known convergence rate bounds for stochastic gradient descent algorithms. Lastly, we propose a new algorithm that combines point sampling and stochastic Polyak's step size. We establish a condition on the sampling process, ensuring a convergence rate guarantee for this algorithm, particularly in the case of smooth convex loss functions. Our numerical results showcase the efficiency of the proposed algorithm.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20998;&#24067;&#25968;&#25454;&#20998;&#26512;&#30340;&#21487;&#35299;&#37322;&#26041;&#27861; ADD MALTS&#65292;&#26377;&#21161;&#20110;&#30830;&#20445;&#21487;&#38752;&#21644;&#31283;&#20581;&#30340;&#20915;&#31574;&#21046;&#23450;&#65292;&#33021;&#22815;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#20272;&#35745;&#22788;&#29702;&#25928;&#26524;</title><link>https://arxiv.org/abs/2312.10569</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#20998;&#26512;&#21487;&#31359;&#25140;&#35774;&#22791;&#12289;&#20256;&#24863;&#22120;&#21644;&#20998;&#24067;&#25968;&#25454;&#30340;&#21487;&#35299;&#37322;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Interpretable Causal Inference for Analyzing Wearable, Sensor, and Distributional Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10569
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20998;&#24067;&#25968;&#25454;&#20998;&#26512;&#30340;&#21487;&#35299;&#37322;&#26041;&#27861; ADD MALTS&#65292;&#26377;&#21161;&#20110;&#30830;&#20445;&#21487;&#38752;&#21644;&#31283;&#20581;&#30340;&#20915;&#31574;&#21046;&#23450;&#65292;&#33021;&#22815;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#20272;&#35745;&#22788;&#29702;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#20195;&#22240;&#26524;&#38382;&#39064;&#37117;&#28041;&#21450;&#21040;&#27835;&#30103;&#22914;&#20309;&#24433;&#21709;&#36890;&#36807;&#21487;&#31359;&#25140;&#35774;&#22791;&#21644;&#20256;&#24863;&#22120;&#27979;&#37327;&#30340;&#22797;&#26434;&#32467;&#26524;&#12290;&#24403;&#21069;&#30340;&#20998;&#26512;&#26041;&#27861;&#35201;&#27714;&#23558;&#36825;&#20123;&#25968;&#25454;&#27719;&#24635;&#20026;&#26631;&#37327;&#32479;&#35745;&#25968;&#25454;&#65288;&#20363;&#22914;&#22343;&#20540;&#65289;&#65292;&#20294;&#36825;&#20123;&#27719;&#24635;&#21487;&#33021;&#20855;&#26377;&#35823;&#23548;&#24615;&#12290;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#23558;&#25968;&#25454;&#34920;&#31034;&#20026;&#20998;&#24067;&#26469;&#20811;&#26381;&#20449;&#24687;&#20002;&#22833;&#38382;&#39064;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20998;&#24067;&#25968;&#25454;&#20998;&#26512;&#30340;&#21487;&#35299;&#37322;&#26041;&#27861;&#65292;&#30830;&#20445;&#20102;&#20540;&#24471;&#20449;&#36182;&#21644;&#31283;&#20581;&#30340;&#20915;&#31574;&#21046;&#23450;&#65306;&#36890;&#36807;&#23398;&#20064;&#25289;&#20280;&#21518;&#21305;&#37197;&#30340;&#20998;&#24067;&#25968;&#25454;&#20998;&#26512;&#65288;ADD MALTS&#65289;&#12290;&#25105;&#20204;&#65288;i&#65289;&#25552;&#20379;&#20102;&#23545;&#25105;&#20204;&#20272;&#35745;&#31574;&#30053;&#27491;&#30830;&#24615;&#30340;&#20998;&#26512;&#20445;&#35777;&#65292;&#65288;ii&#65289;&#36890;&#36807;&#27169;&#25311;&#34920;&#26126;ADD MALTS&#22312;&#20272;&#35745;&#22788;&#29702;&#25928;&#26524;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#20998;&#24067;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#65288;iii&#65289;&#23637;&#31034;&#20102;ADD MALTS&#39564;&#35777;&#26159;&#21542;&#23384;&#22312;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10569v2 Announce Type: replace  Abstract: Many modern causal questions ask how treatments affect complex outcomes that are measured using wearable devices and sensors. Current analysis approaches require summarizing these data into scalar statistics (e.g., the mean), but these summaries can be misleading. For example, disparate distributions can have the same means, variances, and other statistics. Researchers can overcome the loss of information by instead representing the data as distributions. We develop an interpretable method for distributional data analysis that ensures trustworthy and robust decision-making: Analyzing Distributional Data via Matching After Learning to Stretch (ADD MALTS). We (i) provide analytical guarantees of the correctness of our estimation strategy, (ii) demonstrate via simulation that ADD MALTS outperforms other distributional data analysis methods at estimating treatment effects, and (iii) illustrate ADD MALTS' ability to verify whether there i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#28608;&#27963;&#19987;&#38376;&#30340;&#38450;&#24481;&#25514;&#26045;&#65292;&#22914;&#38543;&#26426;&#22122;&#22768;&#38450;&#24481;&#21644;&#38543;&#26426;&#22270;&#20687;&#21464;&#25442;&#65292;&#21482;&#38024;&#23545;&#20302;&#32622;&#20449;&#24230;&#30340;&#36755;&#20837;&#65292;&#22312;&#27979;&#35797;&#38454;&#27573;&#23454;&#29616;&#20102;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26377;&#25928;&#26435;&#34913;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#21508;&#31181;&#29616;&#26377;&#38450;&#24481;&#25514;&#26045;&#12290;</title><link>https://arxiv.org/abs/2312.10132</link><description>&lt;p&gt;
&#32553;&#23567;&#24046;&#36317;&#65306;&#38024;&#23545;&#22522;&#20110;&#26597;&#35810;&#25915;&#20987;&#23454;&#29616;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;-&#40065;&#26834;&#24615;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Closing the Gap: Achieving Better Accuracy-Robustness Tradeoffs against Query-Based Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#28608;&#27963;&#19987;&#38376;&#30340;&#38450;&#24481;&#25514;&#26045;&#65292;&#22914;&#38543;&#26426;&#22122;&#22768;&#38450;&#24481;&#21644;&#38543;&#26426;&#22270;&#20687;&#21464;&#25442;&#65292;&#21482;&#38024;&#23545;&#20302;&#32622;&#20449;&#24230;&#30340;&#36755;&#20837;&#65292;&#22312;&#27979;&#35797;&#38454;&#27573;&#23454;&#29616;&#20102;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26377;&#25928;&#26435;&#34913;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#21508;&#31181;&#29616;&#26377;&#38450;&#24481;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29616;&#26377;&#30340;&#38450;&#24481;&#25514;&#26045;&#23545;&#25239;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#21069;&#26223;&#24191;&#38420;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#23616;&#38480;&#24615;&#65306;&#23427;&#20204;&#22312;&#25552;&#39640;&#23545;&#25239;&#25915;&#20987;&#40065;&#26834;&#24615;&#30340;&#21516;&#26102;&#65292;&#20250;&#26174;&#33879;&#38477;&#20302;&#23545;&#24178;&#20928;&#26679;&#26412;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#27979;&#35797;&#38454;&#27573;&#26377;&#25928;&#24314;&#31435;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340; solid tradeoff&#65292;&#20197;&#32531;&#35299;&#22522;&#20110;&#26597;&#35810;&#25915;&#20987;&#12290;&#32771;&#34385;&#21040;&#36825;&#20123;&#25915;&#20987;&#24517;&#28982;&#25506;&#32034;&#20302;&#32622;&#20449;&#21306;&#22495;&#65292;&#25105;&#20204;&#30340;&#35266;&#28857;&#26159;&#65292;&#20165;&#38024;&#23545;&#20302;&#32622;&#20449;&#24230;&#30340;&#36755;&#20837;&#28608;&#27963;&#19987;&#38376;&#30340;&#38450;&#24481;&#25514;&#26045;&#65292;&#22914;&#38543;&#26426;&#22122;&#22768;&#38450;&#24481;&#21644;&#38543;&#26426;&#22270;&#20687;&#21464;&#25442;&#65292;&#23601;&#36275;&#20197;&#38459;&#27490;&#23427;&#20204;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#35757;&#32451;&#26080;&#20851;&#65292;&#26377;&#29702;&#35770;&#25903;&#25345;&#12290;&#36890;&#36807;&#22312;CIFAR-10&#12289;CIFAR-100&#21644;ImageNet&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25552;&#35758;&#30830;&#23454;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#22686;&#24378;&#36825;&#20123;&#38450;&#24481;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10132v2 Announce Type: replace-cross  Abstract: Although promising, existing defenses against query-based attacks share a common limitation: they offer increased robustness against attacks at the price of a considerable accuracy drop on clean samples. In this work, we show how to efficiently establish, at test-time, a solid tradeoff between robustness and accuracy when mitigating query-based attacks. Given that these attacks necessarily explore low-confidence regions, our insight is that activating dedicated defenses, such as random noise defense and random image transformations, only for low-confidence inputs is sufficient to prevent them. Our approach is independent of training and supported by theory. We verify the effectiveness of our approach for various existing defenses by conducting extensive experiments on CIFAR-10, CIFAR-100, and ImageNet. Our results confirm that our proposal can indeed enhance these defenses by providing better tradeoffs between robustness and ac
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21152;&#26435;&#38598;&#25104;&#27169;&#22411;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#30340;&#25345;&#32493;&#23398;&#20064;&#65292;&#20860;&#39038;&#21487;&#22609;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.08977</link><description>&lt;p&gt;
&#21152;&#26435;&#38598;&#25104;&#27169;&#22411;&#26159;&#24378;&#22823;&#30340;&#25345;&#32493;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Weighted Ensemble Models Are Strong Continual Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08977
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21152;&#26435;&#38598;&#25104;&#27169;&#22411;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#30340;&#25345;&#32493;&#23398;&#20064;&#65292;&#20860;&#39038;&#21487;&#22609;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#30446;&#26631;&#26159;&#20174;&#19968;&#31995;&#21015;&#20219;&#21153;&#20013;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#24471;&#20197;&#21069;&#20219;&#21153;&#30340;&#25968;&#25454;&#22312;&#23398;&#20064;&#24403;&#21069;&#20219;&#21153;&#25968;&#25454;&#26102;&#19981;&#21487;&#29992;&#12290;CL&#26412;&#36136;&#19978;&#26159;&#22312;&#33021;&#22815;&#23398;&#20064;&#26032;&#20219;&#21153;&#65288;&#21363;&#21487;&#22609;&#24615;&#65289;&#21644;&#20445;&#25345;&#20808;&#21069;&#23398;&#20064;&#27010;&#24565;&#30340;&#24615;&#33021;&#65288;&#21363;&#31283;&#23450;&#24615;&#65289;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#31283;&#23450;&#24615;-&#21487;&#22609;&#24615;&#30340;&#26435;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#23545;&#20808;&#21069;&#21644;&#24403;&#21069;&#20219;&#21153;&#30340;&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#21152;&#26435;&#38598;&#25104;&#12290;&#36825;&#31181;&#21152;&#26435;&#38598;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#25345;&#32493;&#27169;&#22411;&#24179;&#22343;&#65288;&#25110;CoMA&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#22609;&#24615;&#22312;&#24403;&#21069;&#20219;&#21153;&#19978;&#33719;&#24471;&#39640;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#19981;&#20250;&#20559;&#31163;&#22826;&#36828;&#30340;&#20808;&#21069;&#26435;&#37325;&#37197;&#32622;&#65292;&#20174;&#32780;&#30830;&#20445;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;CoMA&#30340;&#25913;&#36827;&#22411;&#21464;&#20307;&#65292;&#21517;&#20026;&#25345;&#32493;&#36153;&#33293;&#23572;&#21152;&#26435;&#27169;&#22411;&#24179;&#22343;&#65288;&#25110;CoFiMA&#65289;&#65292;&#35813;&#27169;&#22411;&#23545;&#27599;&#19968;&#20010;&#21442;&#25968;&#36827;&#34892;&#36873;&#25321;&#24615;&#21152;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08977v2 Announce Type: replace-cross  Abstract: In this work, we study the problem of continual learning (CL) where the goal is to learn a model on a sequence of tasks, such that the data from the previous tasks becomes unavailable while learning on the current task data. CL is essentially a balancing act between being able to learn on the new task (i.e., plasticity) and maintaining the performance on the previously learned concepts (i.e., stability). Intending to address the stability-plasticity trade-off, we propose to perform weight-ensembling of the model parameters of the previous and current tasks. This weighted-ensembled model, which we call Continual Model Averaging (or CoMA), attains high accuracy on the current task by leveraging plasticity, while not deviating too far from the previous weight configuration, ensuring stability. We also propose an improved variant of CoMA, named Continual Fisher-weighted Model Averaging (or CoFiMA), that selectively weighs each para
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;UNITE&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#20687;&#25945;&#24072;&#27169;&#22411;&#21644;&#35270;&#39057;&#23398;&#29983;&#27169;&#22411;&#36827;&#34892;&#36974;&#34109;&#39044;&#35757;&#32451;&#21644;&#21327;&#20316;&#33258;&#35757;&#32451;&#65292;&#22312;&#22810;&#20010;&#35270;&#39057;&#39046;&#22495;&#33258;&#36866;&#24212;&#22522;&#20934;&#19978;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.02914</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#35270;&#39057;&#22495;&#33258;&#36866;&#24212;&#65306;&#37319;&#29992;&#36974;&#34109;&#39044;&#35757;&#32451;&#21644;&#21327;&#20316;&#33258;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Video Domain Adaptation with Masked Pre-Training and Collaborative Self-Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02914
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;UNITE&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#20687;&#25945;&#24072;&#27169;&#22411;&#21644;&#35270;&#39057;&#23398;&#29983;&#27169;&#22411;&#36827;&#34892;&#36974;&#34109;&#39044;&#35757;&#32451;&#21644;&#21327;&#20316;&#33258;&#35757;&#32451;&#65292;&#22312;&#22810;&#20010;&#35270;&#39057;&#39046;&#22495;&#33258;&#36866;&#24212;&#22522;&#20934;&#19978;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#35270;&#39057;&#21160;&#20316;&#35782;&#21035;&#30340;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#31216;&#20026;UNITE&#65292;&#20351;&#29992;&#22270;&#20687;&#25945;&#24072;&#27169;&#22411;&#26469;&#35843;&#25972;&#35270;&#39057;&#23398;&#29983;&#27169;&#22411;&#21040;&#30446;&#26631;&#22495;&#12290;UNITE&#39318;&#20808;&#37319;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#36890;&#36807;&#25945;&#24072;&#24341;&#23548;&#30340;&#36974;&#34109;&#33976;&#39311;&#30446;&#26631;&#24471;&#21040;&#20855;&#26377;&#21306;&#20998;&#24615;&#30340;&#29305;&#24449;&#23398;&#20064;&#12290;&#28982;&#21518;&#25105;&#20204;&#23545;&#30446;&#26631;&#25968;&#25454;&#36827;&#34892;&#36974;&#34109;&#33258;&#35757;&#32451;&#65292;&#21033;&#29992;&#35270;&#39057;&#23398;&#29983;&#27169;&#22411;&#21644;&#22270;&#20687;&#25945;&#24072;&#27169;&#22411;&#19968;&#36215;&#20026;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#35270;&#39057;&#29983;&#25104;&#25913;&#36827;&#30340;&#20266;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#33258;&#35757;&#32451;&#36807;&#31243;&#25104;&#21151;&#21033;&#29992;&#20102;&#20004;&#20010;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#36328;&#22495;&#24378;&#22823;&#30340;&#36716;&#31227;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#35270;&#39057;&#22495;&#33258;&#36866;&#24212;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35266;&#23519;&#21040;&#30456;&#27604;&#20808;&#21069;&#25253;&#36947;&#30340;&#32467;&#26524;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02914v3 Announce Type: replace-cross  Abstract: In this work, we tackle the problem of unsupervised domain adaptation (UDA) for video action recognition. Our approach, which we call UNITE, uses an image teacher model to adapt a video student model to the target domain. UNITE first employs self-supervised pre-training to promote discriminative feature learning on target domain videos using a teacher-guided masked distillation objective. We then perform self-training on masked target data, using the video student model and image teacher model together to generate improved pseudolabels for unlabeled target videos. Our self-training process successfully leverages the strengths of both models to achieve strong transfer performance across domains. We evaluate our approach on multiple video domain adaptation benchmarks and observe significant improvements upon previously reported results.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36870;&#21521;&#25235;&#21462;&#36807;&#31243;&#24182;&#21033;&#29992;&#25342;&#21462;&#21644;&#25918;&#32622;&#38382;&#39064;&#30340;&#23545;&#31216;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25342;&#21462;&#30340;&#25918;&#32622;&#26041;&#27861;&#65292;&#24182;&#29992;&#33258;&#20027;&#25910;&#38598;&#30340;&#28436;&#31034;&#30452;&#25509;&#35757;&#32451;&#31574;&#30053;&#65292;&#23454;&#29616;&#22312;&#25509;&#35302;&#21463;&#38480;&#29615;&#22659;&#19979;&#29289;&#20307;&#25918;&#32622;&#20219;&#21153;&#30340;&#33258;&#20027;&#25910;&#38598;&#21644;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2312.02352</link><description>&lt;p&gt;
&#36870;&#21521;&#23398;&#20064;&#65306;&#36890;&#36807;&#25441;&#21462;&#23398;&#20064;&#25918;&#32622;
&lt;/p&gt;
&lt;p&gt;
Working Backwards: Learning to Place by Picking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02352
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36870;&#21521;&#25235;&#21462;&#36807;&#31243;&#24182;&#21033;&#29992;&#25342;&#21462;&#21644;&#25918;&#32622;&#38382;&#39064;&#30340;&#23545;&#31216;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25342;&#21462;&#30340;&#25918;&#32622;&#26041;&#27861;&#65292;&#24182;&#29992;&#33258;&#20027;&#25910;&#38598;&#30340;&#28436;&#31034;&#30452;&#25509;&#35757;&#32451;&#31574;&#30053;&#65292;&#23454;&#29616;&#22312;&#25509;&#35302;&#21463;&#38480;&#29615;&#22659;&#19979;&#29289;&#20307;&#25918;&#32622;&#20219;&#21153;&#30340;&#33258;&#20027;&#25910;&#38598;&#21644;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25342;&#21462;&#65288;PvP&#65289;&#30340;&#25918;&#32622;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#20027;&#25910;&#38598;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#25918;&#32622;&#20219;&#21153;&#30340;&#29616;&#23454;&#19990;&#30028;&#28436;&#31034;&#65292;&#20854;&#20013;&#29289;&#20307;&#24517;&#39035;&#34987;&#25805;&#32437;&#21040;&#29305;&#23450;&#30340;&#25509;&#35302;&#38480;&#21046;&#20301;&#32622;&#12290;&#36890;&#36807;PvP&#65292;&#25105;&#20204;&#36890;&#36807;&#39072;&#20498;&#25235;&#21462;&#36807;&#31243;&#24182;&#21033;&#29992;&#25342;&#21462;&#21644;&#25918;&#32622;&#38382;&#39064;&#22266;&#26377;&#30340;&#23545;&#31216;&#24615;&#65292;&#25509;&#36817;&#20110;&#26426;&#22120;&#20154;&#29289;&#20307;&#25918;&#32622;&#28436;&#31034;&#30340;&#25910;&#38598;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#19968;&#32452;&#26368;&#21021;&#20301;&#20110;&#30446;&#26631;&#25918;&#32622;&#20301;&#32622;&#30340;&#29289;&#20307;&#30340;&#25235;&#21462;&#24207;&#21015;&#20013;&#33719;&#24471;&#25918;&#32622;&#28436;&#31034;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#22312;&#25509;&#35302;&#21463;&#38480;&#29615;&#22659;&#20013;&#25910;&#38598;&#25968;&#30334;&#20010;&#28436;&#31034;&#65292;&#32780;&#26080;&#38656;&#20154;&#31867;&#24178;&#39044;&#65292;&#36825;&#26159;&#36890;&#36807;&#32467;&#21512;&#20004;&#20010;&#27169;&#22359;&#23454;&#29616;&#30340;&#65306;&#35302;&#35273;&#37325;&#26032;&#25235;&#21462;&#21644;&#29992;&#20110;&#25235;&#21462;&#30340;&#39034;&#20174;&#25511;&#21046;&#12290;&#25105;&#20204;&#36890;&#36807;&#34892;&#20026;&#20811;&#38534;&#30452;&#25509;&#20174;&#35270;&#35273;&#35266;&#23519;&#20013;&#36890;&#36807;&#33258;&#20027;&#25910;&#38598;&#30340;&#28436;&#31034;&#20013;&#35757;&#32451;&#31574;&#30053;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#31574;&#30053;&#21487;&#20197;&#25512;&#24191;&#21040;&#36229;&#20986;&#35757;&#32451;&#29615;&#22659;&#33539;&#22260;&#30340;&#29289;&#20307;&#25918;&#32622;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02352v2 Announce Type: replace-cross  Abstract: We present placing via picking (PvP), a method to autonomously collect real-world demonstrations for a family of placing tasks in which objects must be manipulated to specific contact-constrained locations. With PvP, we approach the collection of robotic object placement demonstrations by reversing the grasping process and exploiting the inherent symmetry of the pick and place problems. Specifically, we obtain placing demonstrations from a set of grasp sequences of objects initially located at their target placement locations. Our system can collect hundreds of demonstrations in contact-constrained environments without human intervention by combining two modules: tactile regrasping and compliant control for grasps. We train a policy directly from visual observations through behavioral cloning, using the autonomously-collected demonstrations. By doing so, the policy can generalize to object placement scenarios outside of the tra
&lt;/p&gt;</description></item><item><title>RO-LMM&#26159;&#19968;&#20010;&#38024;&#23545;&#25918;&#23556;&#32959;&#30244;&#23398;&#39046;&#22495;&#35774;&#35745;&#30340;&#22810;&#21151;&#33021;&#22823;&#22411;&#22810;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;Consistency Embedding Fine-Tuning&#65288;CEFTune&#65289;&#25216;&#26415;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#20445;&#25345;&#22788;&#29702;&#24178;&#20928;&#36755;&#20837;&#33021;&#21147;&#30340;&#21516;&#26102;&#25552;&#21319;&#23545;&#22024;&#26434;&#36755;&#20837;&#30340;&#40065;&#26834;&#24615;&#65292;&#29992;&#20110;&#25918;&#23556;&#27835;&#30103;&#35745;&#21010;&#21644;&#30446;&#26631;&#20307;&#31215;&#20998;&#21106;&#12290;</title><link>https://arxiv.org/abs/2311.15876</link><description>&lt;p&gt;
LMM&#36741;&#21161;&#30340;&#19968;&#33268;&#24615;&#23884;&#20837;&#19979;&#20083;&#33146;&#30284;&#27835;&#30103;&#30446;&#26631;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
LMM-Assisted Breast Cancer Treatment Target Segmentation with Consistency Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15876
&lt;/p&gt;
&lt;p&gt;
RO-LMM&#26159;&#19968;&#20010;&#38024;&#23545;&#25918;&#23556;&#32959;&#30244;&#23398;&#39046;&#22495;&#35774;&#35745;&#30340;&#22810;&#21151;&#33021;&#22823;&#22411;&#22810;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;Consistency Embedding Fine-Tuning&#65288;CEFTune&#65289;&#25216;&#26415;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#20445;&#25345;&#22788;&#29702;&#24178;&#20928;&#36755;&#20837;&#33021;&#21147;&#30340;&#21516;&#26102;&#25552;&#21319;&#23545;&#22024;&#26434;&#36755;&#20837;&#30340;&#40065;&#26834;&#24615;&#65292;&#29992;&#20110;&#25918;&#23556;&#27835;&#30103;&#35745;&#21010;&#21644;&#30446;&#26631;&#20307;&#31215;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#28145;&#21051;&#24433;&#21709;&#20102;&#21307;&#23398;&#39046;&#22495;&#65292;&#20026;&#38477;&#20302;&#20020;&#24202;&#24037;&#20316;&#37327;&#25552;&#20379;&#20102;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21463;&#38480;&#20110;&#25191;&#34892;&#21333;&#27169;&#24335;&#20219;&#21153;&#65292;&#19982;&#21307;&#23398;&#19987;&#19994;&#20154;&#21592;&#25152;&#20351;&#29992;&#30340;&#32508;&#21512;&#26041;&#27861;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;RO-LMM&#65292;&#19968;&#20010;&#19987;&#20026;&#25918;&#23556;&#32959;&#30244;&#23398;&#39046;&#22495;&#35774;&#35745;&#30340;&#22810;&#21151;&#33021;&#22823;&#22411;&#22810;&#27169;&#22411;&#65288;LMM&#65289;&#12290;&#35813;&#27169;&#22411;&#28085;&#30422;&#20102;&#20020;&#24202;&#24037;&#20316;&#27969;&#20013;&#30340;&#19968;&#31995;&#21015;&#20219;&#21153;&#65292;&#25797;&#38271;&#20020;&#24202;&#25253;&#21578;&#25688;&#35201;&#12289;&#25918;&#30103;&#27835;&#30103;&#35745;&#21010;&#24314;&#35758;&#21644;&#35745;&#21010;&#24341;&#23548;&#30340;&#30446;&#26631;&#20307;&#31215;&#20998;&#21106;&#12290;&#20026;&#20102;&#25191;&#34892;&#36830;&#32493;&#30340;&#20020;&#24202;&#20219;&#21153;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#33268;&#24615;&#23884;&#20837;&#24494;&#35843;&#65288;CEFTune&#65289;&#25216;&#26415;&#65292;&#25552;&#21319;&#20102;LMM&#23545;&#22024;&#26434;&#36755;&#20837;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#22788;&#29702;&#24178;&#20928;&#36755;&#20837;&#30340;&#33021;&#21147;&#65292;&#24182;&#23558;&#35813;&#27010;&#24565;&#36716;&#21270;&#20026;LMM&#39537;&#21160;&#30340;&#20998;&#21106;&#26694;&#26550;&#65292;&#21363;&#19968;&#33268;&#24615;&#23884;&#20837;S&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15876v2 Announce Type: replace-cross  Abstract: Recent advancements in Artificial Intelligence (AI) have profoundly influenced medical fields, by providing tools to reduce clinical workloads. However, most AI models are constrained to execute unimodal tasks, in stark contrast to the comprehensive approaches utilized by medical professionals. To address this, here we present RO-LMM, a multi-purpose large multimodal model (LMM) tailored for the field of radiation oncology. This model covers series of tasks within clinical workflow, adept at clinical report summarization, radiation treatment plan suggestion, and plan-guided target volume segmentation. In particular, to perform consecutive clinical tasks, we further present a novel Consistency Embedding Fine-Tuning (CEFTune) technique, which boosts LMM's robustness to noisy inputs while preserving the capability of handling clean inputs, and transform this concept into LMM-driven segmentation framework as Consistency Embedding S
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31995;&#32479;&#65292;&#21517;&#20026;DROC&#65292;&#33021;&#22815;&#22238;&#24212;&#20219;&#24847;&#24418;&#24335;&#30340;&#35821;&#35328;&#21453;&#39304;&#65292;&#20174;&#32416;&#27491;&#20013;&#25552;&#28860;&#20986;&#36890;&#29992;&#30693;&#35782;&#65292;&#24182;&#26681;&#25454;&#25991;&#26412;&#21644;&#35270;&#35273;&#30456;&#20284;&#24615;&#26816;&#32034;&#30456;&#20851;&#30340;&#36807;&#21435;&#32463;&#39564;&#65292;&#20197;&#25913;&#36827;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.10678</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#32416;&#27491;&#25552;&#28860;&#21644;&#26816;&#32034;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#36890;&#29992;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Distilling and Retrieving Generalizable Knowledge for Robot Manipulation via Language Corrections
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31995;&#32479;&#65292;&#21517;&#20026;DROC&#65292;&#33021;&#22815;&#22238;&#24212;&#20219;&#24847;&#24418;&#24335;&#30340;&#35821;&#35328;&#21453;&#39304;&#65292;&#20174;&#32416;&#27491;&#20013;&#25552;&#28860;&#20986;&#36890;&#29992;&#30693;&#35782;&#65292;&#24182;&#26681;&#25454;&#25991;&#26412;&#21644;&#35270;&#35273;&#30456;&#20284;&#24615;&#26816;&#32034;&#30456;&#20851;&#30340;&#36807;&#21435;&#32463;&#39564;&#65292;&#20197;&#25913;&#36827;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20170;&#22825;&#30340;&#26426;&#22120;&#20154;&#25919;&#31574;&#22312;&#38754;&#23545;&#25512;&#24191;&#21040;&#26032;&#29615;&#22659;&#30340;&#25361;&#25112;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;&#20154;&#31867;&#30340;&#32416;&#27491;&#21453;&#39304;&#26159;&#19968;&#31181;&#33267;&#20851;&#37325;&#35201;&#30340;&#25351;&#23548;&#24418;&#24335;&#65292;&#21487;&#20197;&#23454;&#29616;&#36825;&#31181;&#27867;&#21270;&#12290;&#28982;&#32780;&#65292;&#36866;&#24212;&#21644;&#20174;&#22312;&#32447;&#20154;&#31867;&#32416;&#27491;&#20013;&#23398;&#20064;&#26159;&#19968;&#39033;&#19981;&#23481;&#26131;&#30340;&#20219;&#21153;&#65306;&#26426;&#22120;&#20154;&#19981;&#20165;&#38656;&#35201;&#38543;&#26102;&#38388;&#35760;&#20303;&#20154;&#31867;&#30340;&#21453;&#39304;&#20197;&#20415;&#22312;&#26032;&#29615;&#22659;&#20013;&#26816;&#32034;&#27491;&#30830;&#30340;&#20449;&#24687;&#24182;&#38477;&#20302;&#24178;&#28041;&#29575;&#65292;&#32780;&#19988;&#36824;&#38656;&#35201;&#33021;&#22815;&#22238;&#24212;&#21487;&#33021;&#26159;&#20851;&#20110;&#39640;&#32423;&#20154;&#31867;&#20559;&#22909;&#30340;&#20219;&#24847;&#32416;&#27491;&#21040;&#25216;&#33021;&#21442;&#25968;&#30340;&#20302;&#32423;&#35843;&#25972;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#22312;&#32447;&#32416;&#27491;&#33976;&#39311;&#21644;&#26816;&#32034;(DROC)&#31995;&#32479;&#65292;&#23427;&#21487;&#20197;&#22238;&#24212;&#20219;&#24847;&#24418;&#24335;&#30340;&#35821;&#35328;&#21453;&#39304;&#65292;&#20174;&#32416;&#27491;&#20013;&#25552;&#28860;&#20986;&#36890;&#29992;&#30693;&#35782;&#65292;&#24182;&#26681;&#25454;&#25991;&#26412;&#21644;&#35270;&#35273;&#30456;&#20284;&#24615;&#26816;&#32034;&#30456;&#20851;&#30340;&#36807;&#21435;&#32463;&#39564;&#65292;&#20197;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10678v2 Announce Type: replace-cross  Abstract: Today's robot policies exhibit subpar performance when faced with the challenge of generalizing to novel environments. Human corrective feedback is a crucial form of guidance to enable such generalization. However, adapting to and learning from online human corrections is a non-trivial endeavor: not only do robots need to remember human feedback over time to retrieve the right information in new settings and reduce the intervention rate, but also they would need to be able to respond to feedback that can be arbitrary corrections about high-level human preferences to low-level adjustments to skill parameters. In this work, we present Distillation and Retrieval of Online Corrections (DROC), a large language model (LLM)-based system that can respond to arbitrary forms of language feedback, distill generalizable knowledge from corrections, and retrieve relevant past experiences based on textual and visual similarity for improving p
&lt;/p&gt;</description></item><item><title>TD-MPC2&#26159;&#23545;TD-MPC&#31639;&#27861;&#30340;&#19968;&#31995;&#21015;&#25913;&#36827;&#65292;&#36890;&#36807;&#19968;&#32452;&#36229;&#21442;&#25968;&#22312;104&#20010;&#22312;&#32447;RL&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#25104;&#21151;&#35757;&#32451;&#21333;&#19968;&#30340;317M&#21442;&#25968;&#20195;&#29702;&#25191;&#34892;80&#20010;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2310.16828</link><description>&lt;p&gt;
TD-MPC2&#65306;&#21487;&#25193;&#23637;&#12289;&#31283;&#20581;&#30340;&#36830;&#32493;&#25511;&#21046;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TD-MPC2: Scalable, Robust World Models for Continuous Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.16828
&lt;/p&gt;
&lt;p&gt;
TD-MPC2&#26159;&#23545;TD-MPC&#31639;&#27861;&#30340;&#19968;&#31995;&#21015;&#25913;&#36827;&#65292;&#36890;&#36807;&#19968;&#32452;&#36229;&#21442;&#25968;&#22312;104&#20010;&#22312;&#32447;RL&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#25104;&#21151;&#35757;&#32451;&#21333;&#19968;&#30340;317M&#21442;&#25968;&#20195;&#29702;&#25191;&#34892;80&#20010;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
TD-MPC&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#65292;&#23427;&#22312;&#23398;&#20064;&#30340;&#38544;&#24335;&#65288;&#26080;&#35299;&#30721;&#22120;&#65289;&#19990;&#30028;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#25191;&#34892;&#23616;&#37096;&#36712;&#36857;&#20248;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;TD-MPC2&#65306;&#23545;TD-MPC&#31639;&#27861;&#30340;&#19968;&#31995;&#21015;&#25913;&#36827;&#12290;&#25105;&#20204;&#35777;&#26126;TD-MPC2&#22312;&#36328;&#36234;4&#20010;&#19981;&#21516;&#20219;&#21153;&#39046;&#22495;&#30340;104&#20010;&#22312;&#32447;RL&#20219;&#21153;&#20013;&#26174;&#33879;&#25913;&#21892;&#20102;&#22522;&#32447;&#65292;&#36890;&#36807;&#19968;&#32452;&#36229;&#21442;&#25968;&#23454;&#29616;&#20102;&#25345;&#32493;&#24378;&#22823;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#38543;&#30528;&#27169;&#22411;&#21644;&#25968;&#25454;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#20195;&#29702;&#30340;&#33021;&#21147;&#20063;&#22312;&#22686;&#24378;&#65292;&#24182;&#25104;&#21151;&#35757;&#32451;&#20102;&#19968;&#20010;&#21333;&#19968;&#30340;317M&#21442;&#25968;&#20195;&#29702;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#39046;&#22495;&#12289;&#20855;&#35937;&#21644;&#21160;&#20316;&#31354;&#38388;&#20013;&#25191;&#34892;&#20102;80&#20010;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#22823;&#22411;TD-MPC2&#20195;&#29702;&#25152;&#28041;&#21450;&#30340;&#25945;&#35757;&#12289;&#26426;&#20250;&#21644;&#39118;&#38505;&#12290;&#22312;https://tdmpc2.com&#19978;&#25506;&#32034;&#35270;&#39057;&#12289;&#27169;&#22411;&#12289;&#25968;&#25454;&#12289;&#20195;&#30721;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.16828v2 Announce Type: replace-cross  Abstract: TD-MPC is a model-based reinforcement learning (RL) algorithm that performs local trajectory optimization in the latent space of a learned implicit (decoder-free) world model. In this work, we present TD-MPC2: a series of improvements upon the TD-MPC algorithm. We demonstrate that TD-MPC2 improves significantly over baselines across 104 online RL tasks spanning 4 diverse task domains, achieving consistently strong results with a single set of hyperparameters. We further show that agent capabilities increase with model and data size, and successfully train a single 317M parameter agent to perform 80 tasks across multiple task domains, embodiments, and action spaces. We conclude with an account of lessons, opportunities, and risks associated with large TD-MPC2 agents. Explore videos, models, data, code, and more at https://tdmpc2.com
&lt;/p&gt;</description></item><item><title>&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#20316;&#20026;&#19968;&#31181;&#20195;&#34920;&#24615;&#30340;&#22270;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26088;&#22312;&#20943;&#36731;&#20551;&#38452;&#24615;&#24433;&#21709;&#30340;&#26497;&#20854;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2310.14525</link><description>&lt;p&gt;
&#22270;&#25490;&#21517;&#23545;&#27604;&#23398;&#20064;&#65306;&#19968;&#31181;&#26497;&#20854;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph Ranking Contrastive Learning: A Extremely Simple yet Efficient Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14525
&lt;/p&gt;
&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#20316;&#20026;&#19968;&#31181;&#20195;&#34920;&#24615;&#30340;&#22270;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26088;&#22312;&#20943;&#36731;&#20551;&#38452;&#24615;&#24433;&#21709;&#30340;&#26497;&#20854;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#20316;&#20026;&#19968;&#31181;&#20195;&#34920;&#24615;&#30340;&#22270;&#33258;&#30417;&#30563;&#26041;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#30446;&#21069;&#26222;&#36941;&#37319;&#29992;&#30340;GCL&#20248;&#21270;&#30446;&#26631;&#26159;InfoNCE&#12290;&#36890;&#24120;&#65292;&#23427;&#37319;&#29992;&#22686;&#24378;&#25216;&#26415;&#33719;&#24471;&#20004;&#20010;&#35270;&#22270;&#65292;&#20854;&#20013;&#19968;&#20010;&#35270;&#22270;&#20013;&#30340;&#33410;&#28857;&#20805;&#24403;&#38170;&#28857;&#65292;&#21478;&#19968;&#20010;&#35270;&#22270;&#20013;&#30340;&#23545;&#24212;&#33410;&#28857;&#20805;&#24403;&#27491;&#26679;&#26412;&#65292;&#25152;&#26377;&#20854;&#20182;&#33410;&#28857;&#34987;&#35270;&#20026;&#36127;&#26679;&#26412;&#12290;&#20854;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#38170;&#28857;&#19982;&#27491;&#26679;&#26412;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#24182;&#26368;&#22823;&#21270;&#21040;&#36127;&#26679;&#26412;&#30340;&#36317;&#31163;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#30001;&#20110;&#32570;&#20047;&#26631;&#31614;&#20449;&#24687;&#65292;InfoNCE&#24517;&#28982;&#23558;&#26469;&#33258;&#30456;&#21516;&#31867;&#21035;&#30340;&#26679;&#26412;&#35270;&#20026;&#36127;&#26679;&#26412;&#65292;&#23548;&#33268;&#20551;&#36127;&#26679;&#26412;&#38382;&#39064;&#12290;&#36825;&#21487;&#33021;&#25439;&#23475;&#23398;&#21040;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#24182;&#38543;&#21518;&#38459;&#30861;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#20943;&#36731;&#20551;&#38452;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.14525v2 Announce Type: replace-cross  Abstract: Graph contrastive learning (GCL) has emerged as a representative graph self-supervised method, achieving significant success. The currently prevalent optimization objective for GCL is InfoNCE. Typically, it employs augmentation techniques to obtain two views, where a node in one view acts as the anchor, the corresponding node in the other view serves as the positive sample, and all other nodes are regarded as negative samples. The goal is to minimize the distance between the anchor node and positive samples and maximize the distance to negative samples. However, due to the lack of label information during training, InfoNCE inevitably treats samples from the same class as negative samples, leading to the issue of false negative samples. This can impair the learned node representations and subsequently hinder performance in downstream tasks. While numerous methods have been proposed to mitigate the impact of false negatives, they
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#35780;&#20272;&#20154;&#36947;&#20027;&#20041;&#25588;&#21161;&#23545;&#31918;&#39135;&#21361;&#26426;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#31918;&#39135;&#23433;&#20840;&#31995;&#32479;&#20869;&#65292;&#20154;&#36947;&#20027;&#20041;&#24178;&#39044;&#23545;&#33829;&#20859;&#19981;&#33391;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2310.11287</link><description>&lt;p&gt;
&#35780;&#20272;&#20154;&#36947;&#20027;&#20041;&#25588;&#21161;&#23545;&#31918;&#39135;&#23433;&#20840;&#30340;&#22240;&#26524;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Assessing the Causal Impact of Humanitarian Aid on Food Security
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.11287
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#35780;&#20272;&#20154;&#36947;&#20027;&#20041;&#25588;&#21161;&#23545;&#31918;&#39135;&#21361;&#26426;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#31918;&#39135;&#23433;&#20840;&#31995;&#32479;&#20869;&#65292;&#20154;&#36947;&#20027;&#20041;&#24178;&#39044;&#23545;&#33829;&#20859;&#19981;&#33391;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27668;&#20505;&#21464;&#21270;&#24341;&#21457;&#24178;&#26097;&#30340;&#24773;&#20917;&#19979;&#65292;&#26131;&#21463;&#23041;&#32961;&#30340;&#22320;&#21306;&#38754;&#20020;&#20005;&#37325;&#30340;&#31918;&#39135;&#23433;&#20840;&#23041;&#32961;&#65292;&#38656;&#35201;&#32039;&#24613;&#30340;&#20154;&#36947;&#20027;&#20041;&#25588;&#21161;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#38750;&#27954;&#20043;&#35282;&#30340;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#65292;&#26088;&#22312;&#35780;&#20272;&#22522;&#20110;&#29616;&#37329;&#30340;&#24178;&#39044;&#23545;&#31918;&#39135;&#21361;&#26426;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#22312;&#31918;&#39135;&#23433;&#20840;&#31995;&#32479;&#20869;&#35782;&#21035;&#22240;&#26524;&#20851;&#31995;&#65292;&#21327;&#35843;&#21253;&#25324;&#31038;&#20250;&#32463;&#27982;&#12289;&#22825;&#27668;&#21644;&#36965;&#24863;&#25968;&#25454;&#22312;&#20869;&#30340;&#20840;&#38754;&#25968;&#25454;&#24211;&#65292;&#20197;&#21450;&#20272;&#35745;&#20154;&#36947;&#20027;&#20041;&#24178;&#39044;&#23545;&#33829;&#20859;&#19981;&#33391;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#23601;&#22269;&#23478;&#32423;&#21035;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#27809;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#26679;&#26412;&#37327;&#26377;&#38480;&#12289;&#25968;&#25454;&#36136;&#37327;&#19981;&#20339;&#65292;&#20197;&#21450;&#30001;&#20110;&#25105;&#20204;&#23545;&#31918;&#39135;&#23433;&#20840;&#31561;&#36328;&#23398;&#31185;&#31995;&#32479;&#30340;&#26377;&#38480;&#29702;&#35299;&#32780;&#23548;&#33268;&#30340;&#19981;&#23436;&#32654;&#22240;&#26524;&#22270;&#12290;&#30456;&#21453;&#65292;&#22312;&#21306;&#19968;&#32423;&#21035;&#19978;&#65292;&#32467;&#26524;&#26174;&#31034;&#20986;&#26174;&#33879;&#24433;&#21709;&#65292;&#36827;&#19968;&#27493;&#26263;&#31034;&#20102;&#31995;&#32479;&#29305;&#23450;&#32972;&#26223;&#30340;&#26412;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.11287v2 Announce Type: replace  Abstract: In the face of climate change-induced droughts, vulnerable regions encounter severe threats to food security, demanding urgent humanitarian assistance. This paper introduces a causal inference framework for the Horn of Africa, aiming to assess the impact of cash-based interventions on food crises. Our contributions include identifying causal relationships within the food security system, harmonizing a comprehensive database including socio-economic, weather and remote sensing data, and estimating the causal effect of humanitarian interventions on malnutrition. On a country level, our results revealed no significant effects, likely due to limited sample size, suboptimal data quality, and an imperfect causal graph resulting from our limited understanding of multidisciplinary systems like food security. Instead, on a district level, results revealed significant effects, further implying the context-specific nature of the system. This un
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#24577;&#24863;&#30693;Transformer&#27169;&#22411;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#21033;&#29992;&#20998;&#31867;&#25991;&#26412;&#21644;&#25968;&#20540;&#26102;&#38388;&#24207;&#21015;&#30340;&#20449;&#24687;&#26469;&#39044;&#27979;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#36890;&#36807;&#31070;&#32463;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2310.01232</link><description>&lt;p&gt;
&#38754;&#21521;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#27169;&#24577;&#24863;&#30693;Transformer
&lt;/p&gt;
&lt;p&gt;
Modality-aware Transformer for Financial Time series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.01232
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#24577;&#24863;&#30693;Transformer&#27169;&#22411;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#21033;&#29992;&#20998;&#31867;&#25991;&#26412;&#21644;&#25968;&#20540;&#26102;&#38388;&#24207;&#21015;&#30340;&#20449;&#24687;&#26469;&#39044;&#27979;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#36890;&#36807;&#31070;&#32463;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#24403;&#20854;&#20934;&#30830;&#24615;&#20381;&#36182;&#20110;&#22806;&#37096;&#25968;&#25454;&#28304;&#32780;&#19981;&#20165;&#20165;&#22522;&#20110;&#21382;&#21490;&#25968;&#20540;&#26102;&#12290;&#37329;&#34701;&#39046;&#22495;&#26222;&#36941;&#23384;&#22312;&#36825;&#19968;&#38382;&#39064;&#65292;&#26102;&#38388;&#24207;&#21015;&#30340;&#26410;&#26469;&#34892;&#20026;&#24120;&#24120;&#19982;&#20174;&#21508;&#31181;&#25991;&#26412;&#25253;&#21578;&#21644;&#22823;&#37327;&#32463;&#27982;&#25351;&#26631;&#20013;&#24471;&#20986;&#30340;&#20449;&#24687;&#23494;&#20999;&#30456;&#20851;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#26500;&#24314;&#19968;&#20010;&#21487;&#38752;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#21033;&#29992;&#19981;&#21516;&#26469;&#28304;&#30340;&#25968;&#25454;&#24182;&#25552;&#21462;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#20174;&#32780;&#20934;&#30830;&#39044;&#27979;&#30446;&#26631;&#26102;&#38388;&#24207;&#21015;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#19968;&#25361;&#25112;&#24615;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#21629;&#21517;&#20026;\textit{&#27169;&#24577;&#24863;&#30693;Transformer}&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#25797;&#38271;&#20110;&#25506;&#32034;&#20998;&#31867;&#25991;&#26412;&#21644;&#25968;&#20540;&#26102;&#38388;&#24207;&#21015;&#30340;&#21147;&#37327;&#65292;&#26377;&#25928;&#39044;&#27979;&#30446;&#26631;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#36890;&#36807;&#20854;&#31070;&#32463;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#20379;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.01232v2 Announce Type: replace  Abstract: Time series forecasting presents a significant challenge, particularly when its accuracy relies on external data sources rather than solely on historical values. This issue is prevalent in the financial sector, where the future behavior of time series is often intricately linked to information derived from various textual reports and a multitude of economic indicators. In practice, the key challenge lies in constructing a reliable time series forecasting model capable of harnessing data from diverse sources and extracting valuable insights to predict the target time series accurately. In this work, we tackle this challenging problem and introduce a novel multimodal transformer-based model named the \textit{Modality-aware Transformer}. Our model excels in exploring the power of both categorical text and numerical timeseries to forecast the target time series effectively while providing insights through its neural attention mechanism. 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35775;&#38382;&#21407;&#22987;&#25968;&#25454;&#30340;&#27169;&#22411;-&#26080;&#20851;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;CAKE&#65292;&#21487;&#20197;&#27169;&#25311;&#28145;&#24230;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#22122;&#22768;&#21512;&#25104;&#26679;&#26412;&#23545;&#27604;&#22320;&#25193;&#25955;&#21040;&#27169;&#22411;&#30340;&#20915;&#31574;&#36793;&#30028;&#12290;</title><link>https://arxiv.org/abs/2306.02090</link><description>&lt;p&gt;
&#27809;&#26377;&#25968;&#25454;&#35775;&#38382;&#30340;&#28145;&#24230;&#20998;&#31867;&#22120;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Deep Classifier Mimicry without Data Access
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.02090
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35775;&#38382;&#21407;&#22987;&#25968;&#25454;&#30340;&#27169;&#22411;-&#26080;&#20851;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;CAKE&#65292;&#21487;&#20197;&#27169;&#25311;&#28145;&#24230;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#22122;&#22768;&#21512;&#25104;&#26679;&#26412;&#23545;&#27604;&#22320;&#25193;&#25955;&#21040;&#27169;&#22411;&#30340;&#20915;&#31574;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#30340;&#35775;&#38382;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#26631;&#20934;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#21487;&#33021;&#26080;&#27861;&#31561;&#21516;&#22320;&#33719;&#24471;&#27169;&#22411;&#35757;&#32451;&#25152;&#38656;&#30340;&#21407;&#22987;&#25968;&#25454;&#12290;&#36825;&#20351;&#24471;&#24494;&#35843;&#12289;&#21387;&#32553;&#27169;&#22411;&#12289;&#25345;&#32493;&#35843;&#25972;&#25110;&#36827;&#34892;&#20219;&#20309;&#20854;&#20182;&#31867;&#22411;&#30340;&#25968;&#25454;&#39537;&#21160;&#26356;&#26032;&#21464;&#24471;&#26497;&#20855;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#21487;&#33021;&#26080;&#38656;&#21407;&#22987;&#25968;&#25454;&#35775;&#38382;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#25512;&#29702;&#30693;&#35782;&#25552;&#21462;&#65288;CAKE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;&#65292;&#21487;&#20197;&#27169;&#25311;&#28145;&#24230;&#20998;&#31867;&#22120;&#32780;&#26080;&#38656;&#35775;&#38382;&#21407;&#22987;&#25968;&#25454;&#12290;&#20026;&#27492;&#65292;CAKE&#29983;&#25104;&#19968;&#23545;&#22122;&#22768;&#21512;&#25104;&#26679;&#26412;&#65292;&#24182;&#23558;&#23427;&#20204;&#23545;&#27604;&#22320;&#25193;&#25955;&#21040;&#27169;&#22411;&#30340;&#20915;&#31574;&#36793;&#30028;&#12290;&#25105;&#20204;&#36890;&#36807;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#21508;&#31181;&#26550;&#26500;&#36873;&#25321;&#22312;&#23454;&#35777;&#19978;&#35777;&#23454;&#20102;CAKE&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#24191;&#27867;&#24212;&#29992;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.02090v2 Announce Type: replace-cross  Abstract: Access to pre-trained models has recently emerged as a standard across numerous machine learning domains. Unfortunately, access to the original data the models were trained on may not equally be granted. This makes it tremendously challenging to fine-tune, compress models, adapt continually, or to do any other type of data-driven update. We posit that original data access may however not be required. Specifically, we propose Contrastive Abductive Knowledge Extraction (CAKE), a model-agnostic knowledge distillation procedure that mimics deep classifiers without access to the original data. To this end, CAKE generates pairs of noisy synthetic samples and diffuses them contrastively toward a model's decision boundary. We empirically corroborate CAKE's effectiveness using several benchmark datasets and various architectural choices, paving the way for broad application.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#25552;&#31034;&#27744;&#21644;&#26500;&#24314;&#22522;&#20110;&#23454;&#20363;&#30340;&#25552;&#31034;&#20197;&#21450;&#24341;&#20837;&#26032;&#39062;&#30340;&#36719;&#35821;&#35328;&#21270;&#22120;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20803;&#23398;&#20064;&#21644;&#20195;&#34920;&#24615;&#35821;&#35328;&#21270;&#22120;&#23454;&#29616;&#26377;&#25928;&#30340;&#32467;&#26500;&#21270;&#25552;&#31034;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2306.00618</link><description>&lt;p&gt;
&#36890;&#36807;&#20803;&#23398;&#20064;&#21644;&#20195;&#34920;&#24615;&#35821;&#35328;&#21270;&#22120;&#23454;&#29616;&#26377;&#25928;&#30340;&#32467;&#26500;&#21270;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Effective Structured Prompting by Meta-Learning and Representative Verbalizer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.00618
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#25552;&#31034;&#27744;&#21644;&#26500;&#24314;&#22522;&#20110;&#23454;&#20363;&#30340;&#25552;&#31034;&#20197;&#21450;&#24341;&#20837;&#26032;&#39062;&#30340;&#36719;&#35821;&#35328;&#21270;&#22120;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20803;&#23398;&#20064;&#21644;&#20195;&#34920;&#24615;&#35821;&#35328;&#21270;&#22120;&#23454;&#29616;&#26377;&#25928;&#30340;&#32467;&#26500;&#21270;&#25552;&#31034;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#65288;MLM&#65289;&#30340;&#25552;&#31034;&#35843;&#25972;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#26377;&#38480;&#26631;&#35760;&#31034;&#20363;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;&#23427;&#20026;&#19979;&#28216;&#20219;&#21153;&#35843;&#25972;&#25552;&#31034;&#65292;&#24182;&#20351;&#29992;&#35821;&#35328;&#21270;&#22120;&#26469;&#36830;&#25509;&#39044;&#27979;&#30340;&#26631;&#35760;&#21644;&#26631;&#31614;&#39044;&#27979;&#12290;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#65292;&#25552;&#31034;&#21021;&#22987;&#21270;&#23545;&#20110;&#25552;&#31034;&#35843;&#25972;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;MetaPrompting&#65288;Hou&#31561;&#65292;2022&#65289;&#20351;&#29992;&#20803;&#23398;&#20064;&#26469;&#23398;&#20064;&#25152;&#26377;&#29305;&#23450;&#20219;&#21153;&#25552;&#31034;&#30340;&#20849;&#20139;&#21021;&#22987;&#21270;&#12290;&#28982;&#32780;&#65292;&#24403;&#20219;&#21153;&#22797;&#26434;&#26102;&#65292;&#21333;&#19968;&#21021;&#22987;&#21270;&#26080;&#27861;&#33719;&#24471;&#25152;&#26377;&#20219;&#21153;&#21644;&#26679;&#26412;&#30340;&#33391;&#22909;&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;MLM&#36890;&#24120;&#24456;&#22823;&#65292;MetaPrompting&#38656;&#35201;&#35843;&#25972;&#25972;&#20010;MLM&#65292;&#23548;&#33268;&#35745;&#31639;&#21644;&#20869;&#23384;&#36127;&#25285;&#27785;&#37325;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#25552;&#31034;&#27744;&#25552;&#21462;&#26356;&#22810;&#20219;&#21153;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#26500;&#24314;&#22522;&#20110;&#23454;&#20363;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36719;&#35821;&#35328;&#21270;&#22120;&#65288;RepVerb&#65289;...&#65288;&#21097;&#20313;&#20869;&#23481;&#26410;&#25552;&#20379;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.00618v2 Announce Type: replace-cross  Abstract: Prompt tuning for pre-trained masked language models (MLM) has shown promising performance in natural language processing tasks with few labeled examples. It tunes a prompt for the downstream task, and a verbalizer is used to bridge the predicted token and label prediction. Due to the limited training data, prompt initialization is crucial for prompt tuning. Recently, MetaPrompting (Hou et al., 2022) uses meta-learning to learn a shared initialization for all task-specific prompts. However, a single initialization is insufficient to obtain good prompts for all tasks and samples when the tasks are complex. Moreover, MetaPrompting requires tuning the whole MLM, causing a heavy burden on computation and memory as the MLM is usually large. To address these issues, we use a prompt pool to extract more task knowledge and construct instance-dependent prompts via attention. We further propose a novel soft verbalizer (RepVerb) which con
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#28145;&#24230;&#23398;&#20064;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20174;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#30340;&#35282;&#24230;&#20998;&#26512;&#19981;&#21516;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;DNN&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2302.13425</link><description>&lt;p&gt;
&#23545;&#28145;&#24230;&#23398;&#20064;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#36827;&#34892;&#35843;&#26597;&#65306;&#20174;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#30340;&#35282;&#24230;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Survey on Uncertainty Quantification for Deep Learning: An Uncertainty Source Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.13425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#28145;&#24230;&#23398;&#20064;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20174;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#30340;&#35282;&#24230;&#20998;&#26512;&#19981;&#21516;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;DNN&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20197;&#21450;&#31185;&#23398;&#19982;&#24037;&#31243;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#20063;&#35748;&#35782;&#21040;DNNs&#26377;&#26102;&#20250;&#20570;&#20986;&#24847;&#22806;&#12289;&#38169;&#35823;&#20294;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#22312;&#33258;&#21160;&#39550;&#39542;&#12289;&#21307;&#23398;&#35786;&#26029;&#21644;&#28798;&#38590;&#21709;&#24212;&#31561;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#20986;&#29616;&#20005;&#37325;&#21518;&#26524;&#12290;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#26088;&#22312;&#20272;&#35745;DNN&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#65292;&#36229;&#36234;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#38024;&#23545;DNNs&#30340;UQ&#26041;&#27861;&#12290;&#31995;&#32479;&#22320;&#23545;&#36825;&#20123;UQ&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#24182;&#27604;&#36739;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#20855;&#26377;&#26497;&#22823;&#30340;&#23454;&#38469;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#35843;&#26597;&#22823;&#22810;&#38598;&#20013;&#22312;&#20174;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#35282;&#24230;&#25110;&#36125;&#21494;&#26031;&#35282;&#24230;&#23545;UQ&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#65292;&#24573;&#30053;&#20102;&#27599;&#31181;&#26041;&#27861;&#21487;&#33021;&#24341;&#20837;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.13425v3 Announce Type: replace  Abstract: Deep neural networks (DNNs) have achieved tremendous success in making accurate predictions for computer vision, natural language processing, as well as science and engineering domains. However, it is also well-recognized that DNNs sometimes make unexpected, incorrect, but overconfident predictions. This can cause serious consequences in high-stake applications, such as autonomous driving, medical diagnosis, and disaster response. Uncertainty quantification (UQ) aims to estimate the confidence of DNN predictions beyond prediction accuracy. In recent years, many UQ methods have been developed for DNNs. It is of great practical value to systematically categorize these UQ methods and compare their advantages and disadvantages. However, existing surveys mostly focus on categorizing UQ methodologies from a neural network architecture perspective or a Bayesian perspective and ignore the source of uncertainty that each methodology can incor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;$do_{code}$&#30340;&#21518;&#39564;&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#31070;&#32463;&#20195;&#30721;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#65292;&#26088;&#22312;&#23454;&#29616;&#38754;&#21521;&#32534;&#31243;&#35821;&#35328;&#30340;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2302.03788</link><description>&lt;p&gt;
&#38754;&#21521;&#35299;&#37322;&#31070;&#32463;&#20195;&#30721;&#27169;&#22411;&#30340;&#22240;&#26524;&#35770;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Toward a Theory of Causation for Interpreting Neural Code Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.03788
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;$do_{code}$&#30340;&#21518;&#39564;&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#31070;&#32463;&#20195;&#30721;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#65292;&#26088;&#22312;&#23454;&#29616;&#38754;&#21521;&#32534;&#31243;&#35821;&#35328;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Neural Language Models of Code&#65292;&#25110;&#32773;&#31216;&#20026;&#31070;&#32463;&#20195;&#30721;&#27169;&#22411;&#65288;NCMs&#65289;&#65292;&#27491;&#22312;&#36805;&#36895;&#20174;&#30740;&#31350;&#21407;&#22411;&#21457;&#23637;&#20026;&#21830;&#19994;&#24320;&#21457;&#32773;&#24037;&#20855;&#12290;&#22240;&#27492;&#65292;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#36890;&#24120;&#26159;&#20351;&#29992;&#33258;&#21160;&#21270;&#25351;&#26631;&#26469;&#34913;&#37327;&#30340;&#65292;&#36825;&#20123;&#25351;&#26631;&#36890;&#24120;&#21482;&#33021;&#25581;&#31034;&#23427;&#20204;&#30495;&#23454;&#24615;&#33021;&#30340;&#19968;&#37096;&#20998;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;NCMs&#30340;&#24615;&#33021;&#20284;&#20046;&#24456;&#26377;&#21069;&#36884;&#65292;&#20294;&#30446;&#21069;&#20851;&#20110;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#20570;&#20986;&#20915;&#31574;&#20173;&#26377;&#24456;&#22810;&#26410;&#30693;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;$do_{code}$&#30340;&#21518;&#39564;&#35299;&#37322;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19987;&#38376;&#38024;&#23545;NCMs&#65292;&#33021;&#22815;&#35299;&#37322;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;$do_{code}$&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#65292;&#20197;&#23454;&#29616;&#38754;&#21521;&#32534;&#31243;&#35821;&#35328;&#30340;&#35299;&#37322;&#12290;&#34429;&#28982;$do_{code}$&#30340;&#29702;&#35770;&#22522;&#30784;&#21487;&#25193;&#23637;&#21040;&#25506;&#32034;&#19981;&#21516;&#30340;&#27169;&#22411;&#23646;&#24615;&#65292;&#20294;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#20307;&#30340;&#23454;&#20363;&#65292;&#26088;&#22312;&#20943;&#23569;&#24433;&#21709;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.03788v2 Announce Type: replace-cross  Abstract: Neural Language Models of Code, or Neural Code Models (NCMs), are rapidly progressing from research prototypes to commercial developer tools. As such, understanding the capabilities and limitations of such models is becoming critical. However, the abilities of these models are typically measured using automated metrics that often only reveal a portion of their real-world performance. While, in general, the performance of NCMs appears promising, currently much is unknown about how such models arrive at decisions. To this end, this paper introduces $do_{code}$, a post hoc interpretability method specific to NCMs that is capable of explaining model predictions. $do_{code}$ is based upon causal inference to enable programming language-oriented explanations. While the theoretical underpinnings of $do_{code}$ are extensible to exploring different model properties, we provide a concrete instantiation that aims to mitigate the impact o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Antigone&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#25968;&#25454;&#25110;&#39564;&#35777;&#25968;&#25454;&#20013;&#37117;&#26080;&#38656;&#35775;&#38382;&#25935;&#24863;&#23646;&#24615;&#21363;&#21487;&#35757;&#32451;&#20844;&#24179;&#30340;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#20266;&#25935;&#24863;&#23646;&#24615;&#26469;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2302.01385</link><description>&lt;p&gt;
&#26080;&#38656;&#35775;&#38382;&#25935;&#24863;&#23646;&#24615;&#30340;&#20844;&#24179;&#20998;&#31867;&#36229;&#21442;&#25968;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Hyper-parameter Tuning for Fair Classification without Sensitive Attribute Access
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.01385
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Antigone&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#25968;&#25454;&#25110;&#39564;&#35777;&#25968;&#25454;&#20013;&#37117;&#26080;&#38656;&#35775;&#38382;&#25935;&#24863;&#23646;&#24615;&#21363;&#21487;&#35757;&#32451;&#20844;&#24179;&#30340;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#20266;&#25935;&#24863;&#23646;&#24615;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26088;&#22312;&#35757;&#32451;&#33021;&#22815;&#22312;&#22522;&#20110;&#31181;&#26063;&#21644;&#24615;&#21035;&#31561;&#25935;&#24863;&#23646;&#24615;&#23450;&#20041;&#30340;&#20154;&#21475;&#32479;&#35745;&#20122;&#32452;&#20043;&#38388;&#24179;&#34913;&#27169;&#22411;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#36890;&#24120;&#20551;&#23450;&#25935;&#24863;&#23646;&#24615;&#24050;&#30693;&#65292;&#20294;&#30001;&#20110;&#38544;&#31169;&#21644;&#20854;&#20182;&#21518;&#21220;&#26041;&#38754;&#30340;&#32771;&#34385;&#65292;&#23454;&#36341;&#20013;&#21487;&#33021;&#26080;&#27861;&#33719;&#21462;&#36825;&#20123;&#23646;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#27809;&#26377;&#25935;&#24863;&#23646;&#24615;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#20844;&#24179;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#25165;&#33021;&#33719;&#24471;&#33391;&#22909;&#32467;&#26524;&#65292;&#22240;&#27492;&#20551;&#35774;&#22312;&#39564;&#35777;&#25968;&#25454;&#19978;&#24050;&#30693;&#25935;&#24863;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20551;&#35774;&#22312;&#23454;&#36341;&#20013;&#20063;&#21487;&#33021;&#19981;&#20999;&#23454;&#38469;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Antigone&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#25968;&#25454;&#25110;&#39564;&#35777;&#25968;&#25454;&#20013;&#37117;&#26080;&#38656;&#35775;&#38382;&#25935;&#24863;&#23646;&#24615;&#21363;&#21487;&#35757;&#32451;&#20844;&#24179;&#30340;&#20998;&#31867;&#22120;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#26377;&#20559;&#35265;&#30340;&#20998;&#31867;&#22120;&#24182;&#20351;&#29992;&#20998;&#31867;&#22120;&#38169;&#35823;&#65288;&#27491;&#30830;&#65289;&#26631;&#35760;&#30340;&#26041;&#24335;&#22312;&#39564;&#35777;&#25968;&#25454;&#19978;&#29983;&#25104;&#20266;&#25935;&#24863;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.01385v2 Announce Type: replace-cross  Abstract: Fair machine learning methods seek to train models that balance model performance across demographic subgroups defined over sensitive attributes like race and gender. Although sensitive attributes are typically assumed to be known during training, they may not be available in practice due to privacy and other logistical concerns. Recent work has sought to train fair models without sensitive attributes on training data. However, these methods need extensive hyper-parameter tuning to achieve good results, and hence assume that sensitive attributes are known on validation data. However, this assumption too might not be practical. Here, we propose Antigone, a framework to train fair classifiers without access to sensitive attributes on either training or validation data. Instead, we generate pseudo sensitive attributes on the validation data by training a biased classifier and using the classifier's incorrectly (correctly) labeled 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AI-KD&#30340;&#23545;&#25239;&#23398;&#20064;&#21644;&#38544;&#24335;&#27491;&#21017;&#21270;&#33258;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#21644;&#38544;&#24335;&#33976;&#39311;&#35268;&#33539;&#35757;&#32451;&#36807;&#31243;&#65292;&#20351;&#24471;&#23398;&#29983;&#27169;&#22411;&#21487;&#20197;&#20174;&#39044;&#35757;&#32451;&#21644;&#20808;&#21069;&#26102;&#26399;&#30340;&#39044;&#27979;&#27010;&#29575;&#20013;&#33976;&#39311;&#30830;&#23450;&#24615;&#21644;&#28176;&#36827;&#24335;&#30693;&#35782;&#65292;&#21516;&#26102;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#20256;&#36755;&#30830;&#23450;&#24615;&#39044;&#27979;&#20998;&#24067;&#30340;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2211.10938</link><description>&lt;p&gt;
AI-KD: &#23545;&#25239;&#23398;&#20064;&#21644;&#38544;&#24335;&#27491;&#21017;&#21270;&#29992;&#20110;&#33258;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AI-KD: Adversarial learning and Implicit regularization for self-Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.10938
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AI-KD&#30340;&#23545;&#25239;&#23398;&#20064;&#21644;&#38544;&#24335;&#27491;&#21017;&#21270;&#33258;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#21644;&#38544;&#24335;&#33976;&#39311;&#35268;&#33539;&#35757;&#32451;&#36807;&#31243;&#65292;&#20351;&#24471;&#23398;&#29983;&#27169;&#22411;&#21487;&#20197;&#20174;&#39044;&#35757;&#32451;&#21644;&#20808;&#21069;&#26102;&#26399;&#30340;&#39044;&#27979;&#27010;&#29575;&#20013;&#33976;&#39311;&#30830;&#23450;&#24615;&#21644;&#28176;&#36827;&#24335;&#30693;&#35782;&#65292;&#21516;&#26102;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#20256;&#36755;&#30830;&#23450;&#24615;&#39044;&#27979;&#20998;&#24067;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#24809;&#32602;&#33258;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#21517;&#20026;&#23545;&#25239;&#23398;&#20064;&#21644;&#38544;&#24335;&#27491;&#21017;&#21270;&#33258;&#30693;&#35782;&#33976;&#39311;&#65288;AI-KD&#65289;&#65292;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#21644;&#38544;&#24335;&#33976;&#39311;&#26469;&#35268;&#33539;&#35757;&#32451;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#20174;&#39044;&#35757;&#32451;&#21644;&#20808;&#21069;&#26102;&#26399;&#30340;&#39044;&#27979;&#27010;&#29575;&#20013;&#33976;&#39311;&#30830;&#23450;&#24615;&#21644;&#28176;&#36827;&#24335;&#30693;&#35782;&#65292;&#36824;&#21487;&#20197;&#20351;&#29992;&#23545;&#25239;&#23398;&#20064;&#20256;&#36755;&#30830;&#23450;&#24615;&#39044;&#27979;&#20998;&#24067;&#30340;&#30693;&#35782;&#12290;&#26041;&#27861;&#30340;&#21160;&#26426;&#22312;&#20110;&#33258;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#36890;&#36807;&#36719;&#30446;&#26631;&#35268;&#33539;&#39044;&#27979;&#27010;&#29575;&#65292;&#20294;&#30830;&#20999;&#30340;&#20998;&#24067;&#21487;&#33021;&#38590;&#20197;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37096;&#32626;&#20102;&#19968;&#20010;&#37492;&#21035;&#22120;&#26469;&#21306;&#20998;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#23398;&#29983;&#27169;&#22411;&#20043;&#38388;&#30340;&#20998;&#24067;&#65292;&#32780;&#23398;&#29983;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#34987;&#35757;&#32451;&#26469;&#24858;&#24324;&#37492;&#21035;&#22120;&#12290;&#22240;&#27492;&#65292;&#23398;&#29983;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#23398;&#20064;&#21040;&#30830;&#23450;&#24615;&#39044;&#27979;&#20998;&#24067;&#65292;&#36824;&#21487;&#20197;&#20174;&#23545;&#25239;&#23398;&#20064;&#20013;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.10938v2 Announce Type: replace-cross  Abstract: We present a novel adversarial penalized self-knowledge distillation method, named adversarial learning and implicit regularization for self-knowledge distillation (AI-KD), which regularizes the training procedure by adversarial learning and implicit distillations. Our model not only distills the deterministic and progressive knowledge which are from the pre-trained and previous epoch predictive probabilities but also transfers the knowledge of the deterministic predictive distributions using adversarial learning. The motivation is that the self-knowledge distillation methods regularize the predictive probabilities with soft targets, but the exact distributions may be hard to predict. Our method deploys a discriminator to distinguish the distributions between the pre-trained and student models while the student model is trained to fool the discriminator in the trained procedure. Thus, the student model not only can learn the pr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#30340;&#26032;&#39062;&#22270;&#23884;&#20837;&#31639;&#27861;&#65292;&#19987;&#38376;&#20026;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#32780;&#35774;&#35745;&#65292;&#24182;&#38024;&#23545;hub&#33410;&#28857;&#35774;&#35745;&#20102;&#29305;&#27530;&#30340;&#37319;&#26679;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2209.07603</link><description>&lt;p&gt;
&#38754;&#21521;&#33410;&#28857;&#20998;&#31867;&#30340;&#22522;&#20110;hub&#24863;&#30693;&#30340;&#38543;&#26426;&#28216;&#36208;&#22270;&#23884;&#20837;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hub-aware Random Walk Graph Embedding Methods for Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.07603
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#30340;&#26032;&#39062;&#22270;&#23884;&#20837;&#31639;&#27861;&#65292;&#19987;&#38376;&#20026;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#32780;&#35774;&#35745;&#65292;&#24182;&#38024;&#23545;hub&#33410;&#28857;&#35774;&#35745;&#20102;&#29305;&#27530;&#30340;&#37319;&#26679;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20108;&#21313;&#24180;&#20013;&#65292;&#25105;&#20204;&#30446;&#30585;&#20102;&#20197;&#22270;&#25110;&#32593;&#32476;&#24418;&#24335;&#32467;&#26500;&#21270;&#30340;&#23453;&#36149;&#22823;&#25968;&#25454;&#30340;&#24040;&#22823;&#22686;&#38271;&#12290;&#23558;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#20998;&#26512;&#25216;&#26415;&#24212;&#29992;&#20110;&#36825;&#20123;&#25968;&#25454;&#65292;&#38656;&#35201;&#23558;&#22270;&#36716;&#25442;&#20026;&#20445;&#30041;&#22270;&#30340;&#26368;&#22522;&#26412;&#32467;&#26500;&#29305;&#24615;&#30340;&#22522;&#20110;&#21521;&#37327;&#30340;&#34920;&#31034;&#12290;&#20026;&#27492;&#65292;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#22823;&#37327;&#22270;&#23884;&#20837;&#26041;&#27861;&#12290;&#20854;&#20013;&#22823;&#22810;&#25968;&#20135;&#29983;&#36890;&#29992;&#23884;&#20837;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#65292;&#22914;&#33410;&#28857;&#32858;&#31867;&#12289;&#33410;&#28857;&#20998;&#31867;&#12289;&#22270;&#21487;&#35270;&#21270;&#21644;&#38142;&#25509;&#39044;&#27979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#30340;&#26032;&#39062;&#22270;&#23884;&#20837;&#31639;&#27861;&#65292;&#19987;&#38376;&#20026;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#32780;&#35774;&#35745;&#12290;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#38543;&#26426;&#28216;&#36208;&#37319;&#26679;&#31574;&#30053;&#34987;&#35774;&#35745;&#25104;&#29305;&#21035;&#20851;&#27880;hub -- &#20855;&#26377;&#23545;&#24635;&#20307;&#20855;&#26377;&#20851;&#38190;&#20316;&#29992;&#30340;&#26368;&#39640;&#24230;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.07603v3 Announce Type: replace  Abstract: In the last two decades we are witnessing a huge increase of valuable big data structured in the form of graphs or networks. To apply traditional machine learning and data analytic techniques to such data it is necessary to transform graphs into vector-based representations that preserve the most essential structural properties of graphs. For this purpose, a large number of graph embedding methods have been proposed in the literature. Most of them produce general-purpose embeddings suitable for a variety of applications such as node clustering, node classification, graph visualisation and link prediction. In this paper, we propose two novel graph embedding algorithms based on random walks that are specifically designed for the node classification problem. Random walk sampling strategies of the proposed algorithms have been designed to pay special attention to hubs -- high-degree nodes that have the most critical role for the overall 
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#20004;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#25968;&#25454;&#30697;&#38453;&#30340;&#35889;&#20998;&#35299;&#21644;&#38750;&#21442;&#25968;&#33258;&#20030;&#25277;&#26679;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#35889;&#32858;&#31867;&#20013;&#25910;&#25947;&#21040;&#27425;&#20248;&#35299;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20986;&#20102;&#20854;&#22312;&#20272;&#35745;&#26377;&#38480;&#28151;&#21512;&#27169;&#22411;&#26102;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2209.05812</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#35889;&#32858;&#31867;&#30340;&#38750;&#21442;&#25968;&#33258;&#20030;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Non-Parametric Bootstrap for Spectral Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.05812
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#20004;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#25968;&#25454;&#30697;&#38453;&#30340;&#35889;&#20998;&#35299;&#21644;&#38750;&#21442;&#25968;&#33258;&#20030;&#25277;&#26679;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#35889;&#32858;&#31867;&#20013;&#25910;&#25947;&#21040;&#27425;&#20248;&#35299;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20986;&#20102;&#20854;&#22312;&#20272;&#35745;&#26377;&#38480;&#28151;&#21512;&#27169;&#22411;&#26102;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#38480;&#28151;&#21512;&#27169;&#22411;&#26159;&#32858;&#31867;&#39046;&#22495;&#20013;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#20854;&#36719;&#32858;&#31867;&#25104;&#21592;&#27010;&#29575;&#24456;&#26377;&#30410;&#22788;&#12290;&#25311;&#21512;&#26377;&#38480;&#28151;&#21512;&#27169;&#22411;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#20351;&#29992;&#35889;&#32858;&#31867;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;EM&#31639;&#27861;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#21253;&#25324;&#25910;&#25947;&#21040;&#27425;&#20248;&#35299;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#20004;&#31181;&#26032;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#20004;&#31181;&#31639;&#27861;&#32467;&#21512;&#20102;&#25968;&#25454;&#30697;&#38453;&#30340;&#35889;&#20998;&#35299;&#21644;&#38750;&#21442;&#25968;&#33258;&#20030;&#25277;&#26679;&#26041;&#26696;&#12290;&#27169;&#25311;&#26174;&#31034;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#23637;&#31034;&#20986;&#23427;&#20204;&#19981;&#20165;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#32780;&#19988;&#22312;&#20272;&#35745;&#26377;&#38480;&#28151;&#21512;&#27169;&#22411;&#26102;&#65292;&#19982;&#20854;&#20182;&#32858;&#31867;&#31639;&#27861;&#30456;&#27604;&#65292;&#23427;&#20204;&#36824;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#21644;&#36991;&#20813;&#31967;&#31957;&#35299;&#30340;&#33021;&#21147;&#12290;&#30456;&#36739;&#20110;&#20854;&#20182;&#25311;&#21512;&#26377;&#38480;&#28151;&#21512;&#27169;&#22411;&#30340;&#33258;&#20030;&#31639;&#27861;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;&#25910;&#25947;&#24615;&#26041;&#38754;&#26356;&#21152;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.05812v2 Announce Type: replace-cross  Abstract: Finite mixture modelling is a popular method in the field of clustering and is beneficial largely due to its soft cluster membership probabilities. A common method for fitting finite mixture models is to employ spectral clustering, which can utilize the expectation-maximization (EM) algorithm. However, the EM algorithm falls victim to a number of issues, including convergence to sub-optimal solutions. We address this issue by developing two novel algorithms that incorporate the spectral decomposition of the data matrix and a non-parametric bootstrap sampling scheme. Simulations display the validity of our algorithms and demonstrate not only their flexibility, but also their computational efficiency and ability to avoid poor solutions when compared to other clustering algorithms for estimating finite mixture models. Our techniques are more consistent in their convergence when compared to other bootstrapped algorithms that fit fi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MulCo&#65306;&#22810;&#23610;&#24230;&#23545;&#27604;&#30693;&#35782;&#20849;&#21516;&#33976;&#39311;&#29992;&#20110;&#20840;&#38754;&#25552;&#39640;&#25152;&#26377;&#31867;&#22411;&#26102;&#38388;&#25968;&#25454;&#38598;&#24615;&#33021;</title><link>https://arxiv.org/abs/2209.00568</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#23545;&#27604;&#30693;&#35782;&#20849;&#21516;&#33976;&#39311;&#29992;&#20110;&#20107;&#20214;&#26102;&#38388;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Multi-Scale Contrastive Knowledge Co-Distillation for Event Temporal Relation Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.00568
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MulCo&#65306;&#22810;&#23610;&#24230;&#23545;&#27604;&#30693;&#35782;&#20849;&#21516;&#33976;&#39311;&#29992;&#20110;&#20840;&#38754;&#25552;&#39640;&#25152;&#26377;&#31867;&#22411;&#26102;&#38388;&#25968;&#25454;&#38598;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#26102;&#38388;&#20851;&#31995;&#25277;&#21462;&#65288;ETRE&#65289;&#26159;&#19968;&#20010;&#20851;&#38190;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20107;&#20214;&#23545;&#20301;&#20110;&#19981;&#21516;&#36317;&#31163;&#30340;&#35805;&#35821;&#20013;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#25509;&#36817;&#24615;&#24102;&#12290;&#20851;&#20110;&#20301;&#20110;&#26356;&#36828;&#65288;&#21363;&#8220;&#38271;&#8221;&#65289;&#25110;&#26356;&#36817;&#65288;&#21363;&#8220;&#30701;&#8221;&#65289;&#25509;&#36817;&#24615;&#24102;&#30340;&#20107;&#20214;&#23545;&#30340;&#26102;&#38388;&#39034;&#24207;&#20256;&#36798;&#26041;&#24335;&#19981;&#21516;&#12290;&#30446;&#21069;ETRE&#27169;&#22411;&#24448;&#24448;&#22312;&#20301;&#20110;&#30701;&#25110;&#38271;&#25509;&#36817;&#24615;&#24102;&#30340;&#20107;&#20214;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#19981;&#33021;&#21516;&#26102;&#34920;&#29616;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#33258;&#28982;&#25991;&#26412;&#21253;&#21547;&#25152;&#26377;&#31867;&#22411;&#30340;&#26102;&#38388;&#20107;&#20214;&#23545;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MulCo&#65306;&#22810;&#23610;&#24230;&#23545;&#27604;&#30693;&#35782;&#20849;&#21516;&#33976;&#39311;&#65292;&#36825;&#26159;&#19968;&#31181;&#34701;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#36328;&#22810;&#20010;&#20107;&#20214;&#23545;&#25509;&#36817;&#24615;&#24102;&#20849;&#20139;&#30693;&#35782;&#65292;&#20197;&#25552;&#39640;&#23545;&#25152;&#26377;&#31867;&#22411;&#26102;&#38388;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MulCo&#25104;&#21151;&#22320;&#25972;&#21512;&#20102;&#36328;&#30701;&#21644;&#38271;&#25509;&#36817;&#24615;&#24102;&#30340;&#19982;&#26102;&#38388;&#25512;&#29702;&#30456;&#20851;&#30340;&#35821;&#35328;&#32447;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.00568v2 Announce Type: replace-cross  Abstract: Event Temporal Relation Extraction (ETRE) is a crucial yet challenging problem. Event pairs are situated within a discourse at different distances, which we refer to as proximity bands. The temporal ordering communicated about event pairs situated at more remote (i.e., ``long'') or less remote (i.e., ``short'') proximity bands is encoded differently. SOTA ETRE models have tended to perform well on events situated at either short or long proximity bands, but not both. Yet, real-world, natural texts contain all types of temporal event-pairs. In this paper, we present MulCo: Multi-Scale Contrastive Knowledge Co-Distillation, a fusion approach that shares knowledge across multiple event pair proximity bands in order to improve performance on all types of temporal datasets. Our experimental results show that MulCo successfully integrates linguistic cues pertaining to temporal reasoning across both short and long proximity bands and 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#37096;&#20998;&#20998;&#24067;&#24335;&#21453;&#39304;&#30340;&#24046;&#20998;&#31169;&#26377;&#32447;&#24615;Bandits&#65292;&#22312;&#38544;&#31169;&#20445;&#25252;&#30340;&#21069;&#25552;&#19979;&#23454;&#29616;&#20102;&#20840;&#23616;&#22870;&#21169;&#30340;&#26368;&#22823;&#21270;</title><link>https://arxiv.org/abs/2207.05827</link><description>&lt;p&gt;
&#20855;&#26377;&#37096;&#20998;&#20998;&#24067;&#24335;&#21453;&#39304;&#30340;&#24046;&#20998;&#31169;&#26377;&#32447;&#24615;Bandits
&lt;/p&gt;
&lt;p&gt;
Differentially Private Linear Bandits with Partial Distributed Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.05827
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#37096;&#20998;&#20998;&#24067;&#24335;&#21453;&#39304;&#30340;&#24046;&#20998;&#31169;&#26377;&#32447;&#24615;Bandits&#65292;&#22312;&#38544;&#31169;&#20445;&#25252;&#30340;&#21069;&#25552;&#19979;&#23454;&#29616;&#20102;&#20840;&#23616;&#22870;&#21169;&#30340;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20165;&#20855;&#26377;&#37096;&#20998;&#20998;&#24067;&#24335;&#21453;&#39304;&#26102;&#30340;&#20840;&#23616;&#22870;&#21169;&#26368;&#22823;&#21270;&#38382;&#39064;&#12290; &#36825;&#20010;&#38382;&#39064;&#21463;&#21040;&#20960;&#20010;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#21551;&#21457;&#65288;&#20363;&#22914;&#65292;&#34562;&#31389;&#32593;&#32476;&#37197;&#32622;&#65292;&#21160;&#24577;&#23450;&#20215;&#21644;&#31574;&#30053;&#36873;&#25321;&#65289;&#65292;&#22312;&#36825;&#20123;&#24212;&#29992;&#20013;&#65292;&#20013;&#22830;&#23454;&#20307;&#37319;&#21462;&#30340;&#19968;&#20010;&#34892;&#21160;&#24433;&#21709;&#20102;&#20026;&#20840;&#23616;&#22870;&#21169;&#20570;&#20986;&#36129;&#29486;&#30340;&#22823;&#37327;&#20154;&#21475;&#12290; &#28982;&#32780;&#65292;&#20174;&#25972;&#20010;&#20154;&#21475;&#25910;&#38598;&#36825;&#26679;&#30340;&#22870;&#21169;&#21453;&#39304;&#19981;&#20165;&#25104;&#26412;&#36807;&#39640;&#65292;&#32780;&#19988;&#36890;&#24120;&#20250;&#24341;&#36215;&#38544;&#31169;&#38382;&#39064;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#32771;&#34385;&#20855;&#26377;&#24046;&#20998;&#31169;&#26377;&#20998;&#24067;&#24335;&#32447;&#24615;Bandits&#65292;&#20854;&#20013;&#20165;&#36873;&#25321;&#20154;&#21475;&#30340;&#19968;&#20010;&#23376;&#38598;&#65288;&#31216;&#20026;&#23458;&#25143;&#31471;&#65289;&#21442;&#19982;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#19988;&#20013;&#22830;&#26381;&#21153;&#22120;&#36890;&#36807;&#22312;&#19981;&#21516;&#20110;&#24046;&#20998;&#30340;&#38544;&#31169;&#26041;&#24335;&#20013;&#36845;&#20195;&#32858;&#21512;&#36825;&#20123;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#21453;&#39304;&#26469;&#20174;&#36825;&#20123;&#37096;&#20998;&#21453;&#39304;&#20013;&#23398;&#20064;&#20840;&#23616;&#27169;&#22411;&#12290; &#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#32479;&#19968;&#30340;&#31639;&#27861;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026; differ
&lt;/p&gt;
&lt;p&gt;
arXiv:2207.05827v2 Announce Type: replace  Abstract: In this paper, we study the problem of global reward maximization with only partial distributed feedback. This problem is motivated by several real-world applications (e.g., cellular network configuration, dynamic pricing, and policy selection) where an action taken by a central entity influences a large population that contributes to the global reward. However, collecting such reward feedback from the entire population not only incurs a prohibitively high cost but often leads to privacy concerns. To tackle this problem, we consider differentially private distributed linear bandits, where only a subset of users from the population are selected (called clients) to participate in the learning process and the central server learns the global model from such partial feedback by iteratively aggregating these clients' local feedback in a differentially private fashion. We then propose a unified algorithmic learning framework, called differ
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#38543;&#26426;&#38598;&#25104;&#30340;&#35299;&#37322;&#30340;&#31283;&#20581;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#31283;&#20581;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#27010;&#29575;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#38598;&#25104;&#27169;&#22411;&#31283;&#20581;&#24615;&#19982;&#22522;&#23398;&#20064;&#22120;&#31283;&#20581;&#24615;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#20026;&#20984;&#22522;&#23398;&#20064;&#22120;&#30340;&#38598;&#25104;&#25552;&#20379;&#20102;&#23454;&#29992;&#26041;&#27861;&#21644;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2205.14116</link><description>&lt;p&gt;
&#19981;&#35299;&#37322;&#22122;&#38899;&#65306;&#38754;&#21521;&#38543;&#26426;&#38598;&#25104;&#30340;&#31283;&#20581;&#21453;&#20107;&#23454;&#34920;&#36798;
&lt;/p&gt;
&lt;p&gt;
Don't Explain Noise: Robust Counterfactuals for Randomized Ensembles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.14116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#38543;&#26426;&#38598;&#25104;&#30340;&#35299;&#37322;&#30340;&#31283;&#20581;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#31283;&#20581;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#27010;&#29575;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#38598;&#25104;&#27169;&#22411;&#31283;&#20581;&#24615;&#19982;&#22522;&#23398;&#20064;&#22120;&#31283;&#20581;&#24615;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#20026;&#20984;&#22522;&#23398;&#20064;&#22120;&#30340;&#38598;&#25104;&#25552;&#20379;&#20102;&#23454;&#29992;&#26041;&#27861;&#21644;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#25551;&#36848;&#20102;&#22914;&#20309;&#20462;&#25913;&#29305;&#24449;&#21521;&#37327;&#20197;&#25913;&#21464;&#32463;&#36807;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#30340;&#32467;&#26524;&#12290;&#33719;&#24471;&#31283;&#20581;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#23545;&#20110;&#25552;&#20379;&#26377;&#25928;&#30340;&#31639;&#27861;&#34917;&#25937;&#21644;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#38543;&#26426;&#38598;&#25104;&#30340;&#35299;&#37322;&#30340;&#31283;&#20581;&#24615;&#65292;&#21363;&#20351;&#22312;&#35757;&#32451;&#25968;&#25454;&#22266;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#22987;&#32456;&#21463;&#21040;&#31639;&#27861;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#31283;&#20581;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#29983;&#25104;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#27010;&#29575;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#38598;&#25104;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#19982;&#22522;&#23398;&#20064;&#22120;&#30340;&#31283;&#20581;&#24615;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#20984;&#22522;&#23398;&#20064;&#22120;&#30340;&#38598;&#25104;&#19978;&#20855;&#26377;&#33391;&#22909;&#23454;&#35777;&#34920;&#29616;&#30340;&#23454;&#29992;&#26041;&#27861;&#65292;&#24182;&#29992;&#29702;&#35770;&#20445;&#35777;&#25903;&#25345;&#23427;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#26377;&#26041;&#27861;&#30340;&#31283;&#20581;&#24615;&#20196;&#20154;&#24778;&#35766;&#22320;&#20302;&#65306;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#19978;&#22825;&#30495;&#21453;&#20107;&#23454;&#30340;&#26377;&#25928;&#24615;&#20302;&#20110;50&#65285;&#65292;&#22312;&#20855;&#26377;&#35768;&#22810;&#29305;&#24449;&#30340;&#38382;&#39064;&#19978;&#21487;&#33021;&#20250;&#38477;&#33267;20&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2205.14116v3 Announce Type: replace  Abstract: Counterfactual explanations describe how to modify a feature vector in order to flip the outcome of a trained classifier. Obtaining robust counterfactual explanations is essential to provide valid algorithmic recourse and meaningful explanations. We study the robustness of explanations of randomized ensembles, which are always subject to algorithmic uncertainty even when the training data is fixed. We formalize the generation of robust counterfactual explanations as a probabilistic problem and show the link between the robustness of ensemble models and the robustness of base learners. We develop a practical method with good empirical performance and support it with theoretical guarantees for ensembles of convex base learners. Our results show that existing methods give surprisingly low robustness: the validity of naive counterfactuals is below $50\%$ on most data sets and can fall to $20\%$ on problems with many features. In contrast
&lt;/p&gt;</description></item><item><title>&#20004;&#31181;&#26041;&#27861;&#22312;&#24102;&#32570;&#22833;&#20540;&#30340;&#30417;&#30563;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#19968;&#33268;&#24615;&#65292;&#24403;&#32570;&#22833;&#20540;&#19981;&#20855;&#20449;&#24687;&#24615;&#26102;&#65292;&#20351;&#29992;&#24120;&#25968;&#36827;&#34892;&#25554;&#34917;&#26159;&#19968;&#31181;&#31616;&#21333;&#19988;&#37325;&#35201;&#30340;&#23454;&#36341;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/1902.06931</link><description>&lt;p&gt;
&#20851;&#20110;&#24102;&#32570;&#22833;&#20540;&#30340;&#30417;&#30563;&#23398;&#20064;&#30340;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the consistency of supervised learning with missing values
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1902.06931
&lt;/p&gt;
&lt;p&gt;
&#20004;&#31181;&#26041;&#27861;&#22312;&#24102;&#32570;&#22833;&#20540;&#30340;&#30417;&#30563;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#19968;&#33268;&#24615;&#65292;&#24403;&#32570;&#22833;&#20540;&#19981;&#20855;&#20449;&#24687;&#24615;&#26102;&#65292;&#20351;&#29992;&#24120;&#25968;&#36827;&#34892;&#25554;&#34917;&#26159;&#19968;&#31181;&#31616;&#21333;&#19988;&#37325;&#35201;&#30340;&#23454;&#36341;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#35774;&#32622;&#20013;&#65292;&#25968;&#25454;&#23384;&#22312;&#32570;&#22833;&#20540;&#65292;&#36825;&#20351;&#24471;&#20998;&#26512;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20016;&#23500;&#30340;&#25991;&#29486;&#28041;&#21450;&#32570;&#22833;&#20540;&#22312;&#25512;&#26029;&#26694;&#26550;&#20013;&#30340;&#22788;&#29702;&#65306;&#20174;&#19981;&#23436;&#25972;&#30340;&#34920;&#20013;&#20272;&#35745;&#21442;&#25968;&#21450;&#20854;&#26041;&#24046;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#32771;&#34385;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#65306;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20013;&#20986;&#29616;&#32570;&#22833;&#20540;&#26102;&#39044;&#27979;&#30446;&#26631;&#12290;&#25105;&#20204;&#34920;&#26126;&#20102;&#20004;&#31181;&#26041;&#27861;&#22312;&#39044;&#27979;&#20013;&#30340;&#19968;&#33268;&#24615;&#12290;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#32467;&#26524;&#26159;&#65292;&#24403;&#32570;&#22833;&#20540;&#19981;&#20855;&#20449;&#24687;&#24615;&#26102;&#65292;&#20351;&#29992;&#24120;&#25968;&#36827;&#34892;&#25554;&#34917;&#65292;&#20363;&#22914;&#22312;&#23398;&#20064;&#20043;&#21069;&#20351;&#29992;&#22343;&#20540;&#65292;&#26159;&#19968;&#33268;&#30340;&#12290;&#36825;&#19982;&#25512;&#26029;&#35774;&#32622;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#65292;&#25512;&#26029;&#35774;&#32622;&#20013;&#24120;&#29992;&#30340;&#22343;&#20540;&#25554;&#34917;&#26041;&#27861;&#34987;&#25351;&#36131;&#25197;&#26354;&#25968;&#25454;&#30340;&#20998;&#24067;&#12290;&#36825;&#26679;&#19968;&#20010;&#31616;&#21333;&#30340;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#33021;&#22815;&#20445;&#25345;&#19968;&#33268;&#24615;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36866;&#29992;&#20110;&#23436;&#25972;&#35266;&#27979;&#30340;&#39044;&#27979;&#22120;&#21487;&#20197;&#36890;&#36807;&#22810;&#37325;&#25554;&#34917;&#22312;&#19981;&#23436;&#25972;&#25968;&#25454;&#19978;&#36827;&#34892;&#26368;&#20339;&#39044;&#27979;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#27604;&#36739;&#25554;&#34917;
&lt;/p&gt;
&lt;p&gt;
arXiv:1902.06931v4 Announce Type: replace-cross  Abstract: In many application settings, the data have missing entries which make analysis challenging. An abundant literature addresses missing values in an inferential framework: estimating parameters and their variance from incomplete tables. Here, we consider supervised-learning settings: predicting a target when missing values appear in both training and testing data. We show the consistency of two approaches in prediction. A striking result is that the widely-used method of imputing with a constant, such as the mean prior to learning is consistent when missing values are not informative. This contrasts with inferential settings where mean imputation is pointed at for distorting the distribution of the data. That such a simple approach can be consistent is important in practice. We also show that a predictor suited for complete observations can predict optimally on incomplete data,through multiple imputation.Finally, to compare imput
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19968;&#33268;&#24615;&#22686;&#24378;&#30340;&#28145;&#24230;&#22810;&#35270;&#22270;&#32858;&#31867;&#26041;&#27861;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#65288;CCEC&#65289;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#35821;&#20041;&#36830;&#25509;&#22359;&#24182;&#20837;&#29305;&#24449;&#34920;&#31034;&#20013;&#65292;&#20197;&#20445;&#25345;&#22810;&#20010;&#35270;&#22270;&#38388;&#30340;&#19968;&#33268;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#35889;&#32858;&#31867;&#25913;&#21892;&#32858;&#31867;&#30340;&#34920;&#31034;&#36807;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.12648</link><description>&lt;p&gt;
&#22522;&#20110;&#19968;&#33268;&#24615;&#22686;&#24378;&#30340;&#28145;&#24230;&#22810;&#35270;&#22270;&#32858;&#31867;&#26041;&#27861;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Consistency Enhancement-Based Deep Multiview Clustering via Contrastive Learning. (arXiv:2401.12648v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19968;&#33268;&#24615;&#22686;&#24378;&#30340;&#28145;&#24230;&#22810;&#35270;&#22270;&#32858;&#31867;&#26041;&#27861;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#65288;CCEC&#65289;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#35821;&#20041;&#36830;&#25509;&#22359;&#24182;&#20837;&#29305;&#24449;&#34920;&#31034;&#20013;&#65292;&#20197;&#20445;&#25345;&#22810;&#20010;&#35270;&#22270;&#38388;&#30340;&#19968;&#33268;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#35889;&#32858;&#31867;&#25913;&#21892;&#32858;&#31867;&#30340;&#34920;&#31034;&#36807;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#22270;&#32858;&#31867;&#65288;MVC&#65289;&#36890;&#36807;&#32508;&#21512;&#22810;&#20010;&#35270;&#22270;&#30340;&#20449;&#24687;&#65292;&#23558;&#25968;&#25454;&#26679;&#26412;&#20998;&#20026;&#26377;&#24847;&#20041;&#30340;&#32858;&#31867;&#12290;&#32780;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;MVC&#22330;&#26223;&#20013;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#29305;&#24449;&#23398;&#20064;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22320;&#27867;&#21270;&#29305;&#24449;&#34920;&#31034;&#24182;&#20445;&#25345;&#19968;&#33268;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#26840;&#25163;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#29616;&#26377;&#28145;&#24230;&#32858;&#31867;&#26041;&#27861;&#22312;&#32858;&#31867;&#36807;&#31243;&#20013;&#24573;&#30053;&#20102;&#32858;&#31867;&#34920;&#31034;&#30340;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#30340;&#19968;&#33268;&#22686;&#24378;&#22411;&#28145;&#24230;MVC&#26041;&#27861;&#65288;CCEC&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23558;&#35821;&#20041;&#36830;&#25509;&#22359;&#24182;&#20837;&#29305;&#24449;&#34920;&#31034;&#20013;&#65292;&#20197;&#20445;&#25345;&#22810;&#20010;&#35270;&#22270;&#38388;&#30340;&#19968;&#33268;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#35889;&#32858;&#31867;&#25913;&#21892;&#32858;&#31867;&#30340;&#34920;&#31034;&#36807;&#31243;&#65292;&#24182;&#25552;&#39640;&#20102;&#22810;&#20010;&#35270;&#22270;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiview clustering (MVC) segregates data samples into meaningful clusters by synthesizing information across multiple views. Moreover, deep learning-based methods have demonstrated their strong feature learning capabilities in MVC scenarios. However, effectively generalizing feature representations while maintaining consistency is still an intractable problem. In addition, most existing deep clustering methods based on contrastive learning overlook the consistency of the clustering representations during the clustering process. In this paper, we show how the above problems can be overcome and propose a consistent enhancement-based deep MVC method via contrastive learning (CCEC). Specifically, semantic connection blocks are incorporated into a feature representation to preserve the consistent information among multiple views. Furthermore, the representation process for clustering is enhanced through spectral clustering, and the consistency across multiple views is improved. Experiment
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#25903;&#37197;&#31561;&#32423;&#29616;&#35937;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#27809;&#26377;&#26126;&#30830;&#32534;&#31243;&#21644;&#20869;&#22312;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#33021;&#22815;&#33258;&#20027;&#21457;&#26126;&#12289;&#23398;&#20064;&#12289;&#23454;&#26045;&#21644;&#20256;&#36882;&#25903;&#37197;&#31561;&#32423;&#32473;&#26032;&#30340;&#32676;&#20307;&#12290;</title><link>http://arxiv.org/abs/2401.12258</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20013;&#30340;&#26032;&#20852;&#25903;&#37197;&#31561;&#32423;
&lt;/p&gt;
&lt;p&gt;
Emergent Dominance Hierarchies in Reinforcement Learning Agents. (arXiv:2401.12258v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#25903;&#37197;&#31561;&#32423;&#29616;&#35937;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#27809;&#26377;&#26126;&#30830;&#32534;&#31243;&#21644;&#20869;&#22312;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#33021;&#22815;&#33258;&#20027;&#21457;&#26126;&#12289;&#23398;&#20064;&#12289;&#23454;&#26045;&#21644;&#20256;&#36882;&#25903;&#37197;&#31561;&#32423;&#32473;&#26032;&#30340;&#32676;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#33021;&#22815;&#32988;&#36807;&#20154;&#31867;&#12290;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#35774;&#32622;&#25552;&#20986;&#20102;&#39069;&#22806;&#30340;&#25361;&#25112;&#65292;&#25104;&#21151;&#30340;&#28151;&#21512;&#21160;&#26426;&#20195;&#29702;&#21327;&#20316;&#21462;&#20915;&#20110;&#20010;&#20307;&#21644;&#32676;&#20307;&#30446;&#26631;&#20043;&#38388;&#30340;&#24494;&#22937;&#24179;&#34913;&#12290;&#31038;&#20250;&#20064;&#24815;&#21644;&#35268;&#33539;&#65292;&#24448;&#24448;&#21463;&#21040;&#20154;&#31867;&#26426;&#26500;&#30340;&#21551;&#21457;&#65292;&#34987;&#29992;&#20316;&#23454;&#29616;&#36825;&#31181;&#24179;&#34913;&#30340;&#24037;&#20855;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#26412;&#19988;&#32463;&#36807;&#28145;&#20837;&#30740;&#31350;&#30340;&#31038;&#20250;&#20064;&#24815;&#65292;&#21363;&#25903;&#37197;&#31561;&#32423;&#65292;&#23427;&#22312;&#21160;&#29289;&#21644;&#20154;&#31867;&#31038;&#20250;&#20013;&#37117;&#23384;&#22312;&#12290;&#25105;&#20204;&#23558;&#25903;&#37197;&#31561;&#32423;&#30340;&#34892;&#20026;&#29702;&#35770;&#24212;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65292;&#24182;&#23613;&#21487;&#33021;&#23569;&#22320;&#20462;&#25913;&#29616;&#26377;&#30340;&#26415;&#35821;&#21644;&#23450;&#20041;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#27809;&#26377;&#26126;&#30830;&#32534;&#31243;&#25110;&#20869;&#22312;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#32676;&#20307;&#33021;&#22815;&#21457;&#26126;&#12289;&#23398;&#20064;&#12289;&#23454;&#26045;&#21644;&#20256;&#36882;&#25903;&#37197;&#31561;&#32423;&#32473;&#26032;&#30340;&#32676;&#20307;&#12290;&#25152;&#20135;&#29983;&#30340;&#25903;&#37197;&#31561;&#32423;&#26377;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
Modern Reinforcement Learning (RL) algorithms are able to outperform humans in a wide variety of tasks. Multi-agent reinforcement learning (MARL) settings present additional challenges, and successful cooperation in mixed-motive groups of agents depends on a delicate balancing act between individual and group objectives. Social conventions and norms, often inspired by human institutions, are used as tools for striking this balance.  In this paper, we examine a fundamental, well-studied social convention that underlies cooperation in both animal and human societies: Dominance hierarchies.  We adapt the ethological theory of dominance hierarchies to artificial agents, borrowing the established terminology and definitions with as few amendments as possible. We demonstrate that populations of RL agents, operating without explicit programming or intrinsic rewards, can invent, learn, enforce, and transmit a dominance hierarchy to new populations. The dominance hierarchies that emerge have a 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#34892;&#21015;&#24335;&#28857;&#36807;&#31243;&#21644;&#24191;&#20041;&#20307;&#31215;&#21462;&#26679;&#36827;&#34892;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#36924;&#36817;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#24191;&#20041;&#29256;&#26412;&#30340;&#20307;&#31215;&#26631;&#20934;&#21270;&#21462;&#26679;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#22312;&#26399;&#26395;&#19978;&#30340;&#20934;&#26368;&#20248;&#24615;&#20197;&#21450;&#22312;&#26576;&#20123;&#35268;&#33539;&#21521;&#37327;&#31354;&#38388;&#20013;&#30340;&#36924;&#36817;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.14057</link><description>&lt;p&gt;
&#22522;&#20110;&#34892;&#21015;&#24335;&#28857;&#36807;&#31243;&#21644;&#24191;&#20041;&#20307;&#31215;&#21462;&#26679;&#30340;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Weighted least-squares approximation with determinantal point processes and generalized volume sampling. (arXiv:2312.14057v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14057
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#34892;&#21015;&#24335;&#28857;&#36807;&#31243;&#21644;&#24191;&#20041;&#20307;&#31215;&#21462;&#26679;&#36827;&#34892;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#36924;&#36817;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#24191;&#20041;&#29256;&#26412;&#30340;&#20307;&#31215;&#26631;&#20934;&#21270;&#21462;&#26679;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#22312;&#26399;&#26395;&#19978;&#30340;&#20934;&#26368;&#20248;&#24615;&#20197;&#21450;&#22312;&#26576;&#20123;&#35268;&#33539;&#21521;&#37327;&#31354;&#38388;&#20013;&#30340;&#36924;&#36817;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#32473;&#23450;&#30340;m&#32500;&#31354;&#38388;V_m&#20013;&#30340;&#20803;&#32032;&#65292;&#20511;&#21161;&#20110;&#19968;&#20123;&#29305;&#24449;&#26144;&#23556;&#966;&#65292;&#36890;&#36807;&#23545;&#38543;&#26426;&#28857;x_1&#65292;...&#65292;x_n&#22788;&#30340;&#20989;&#25968;&#36827;&#34892;&#35780;&#20272;&#65292;&#26469;&#36924;&#36817;&#20989;&#25968;&#20174;L^2&#21040;&#20989;&#25968;&#12290;&#22312;&#22238;&#39038;&#19968;&#20123;&#20851;&#20110;&#20351;&#29992;&#29420;&#31435;&#21516;&#20998;&#24067;&#28857;&#30340;&#26368;&#20248;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#30340;&#32467;&#26524;&#20043;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#25237;&#24433;&#34892;&#21015;&#24335;&#28857;&#36807;&#31243;&#65288;DPP&#65289;&#25110;&#20307;&#31215;&#21462;&#26679;&#30340;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#12290;&#36825;&#20123;&#20998;&#24067;&#22312;&#36873;&#23450;&#30340;&#29305;&#24449;&#966;(x_i)&#20013;&#24341;&#20837;&#20102;&#28857;&#20043;&#38388;&#30340;&#20381;&#36182;&#24615;&#65292;&#20197;&#20419;&#36827;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#24191;&#20041;&#29256;&#26412;&#30340;&#20307;&#31215;&#26631;&#20934;&#21270;&#21462;&#26679;&#65292;&#20351;&#29992;&#26679;&#26412;&#25968;n = O(mlog(m))&#24471;&#21040;&#20102;&#26399;&#26395;&#19978;&#30340;&#20934;&#26368;&#20248;&#32467;&#26524;&#65292;&#36825;&#24847;&#21619;&#30528;&#26399;&#26395;&#30340;L^2&#35823;&#24046;&#21463;&#21040;&#19968;&#20010;&#24120;&#25968;&#20056;&#20197;&#22312;L^2&#20013;&#30340;&#26368;&#20339;&#36924;&#36817;&#35823;&#24046;&#30340;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#36827;&#19968;&#27493;&#20551;&#35774;&#20989;&#25968;&#22312;&#26576;&#20010;&#23884;&#20837;&#22312;L^2&#20013;&#30340;&#35268;&#33539;&#21521;&#37327;&#31354;&#38388;H&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#36924;&#36817;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of approximating a function from $L^2$ by an element of a given $m$-dimensional space $V_m$, associated with some feature map $\varphi$, using evaluations of the function at random points $x_1,\dots,x_n$. After recalling some results on optimal weighted least-squares using independent and identically distributed points, we consider weighted least-squares using projection determinantal point processes (DPP) or volume sampling. These distributions introduce dependence between the points that promotes diversity in the selected features $\varphi(x_i)$. We first provide a generalized version of volume-rescaled sampling yielding quasi-optimality results in expectation with a number of samples $n = O(m\log(m))$, that means that the expected $L^2$ error is bounded by a constant times the best approximation error in $L^2$. Also, further assuming that the function is in some normed vector space $H$ continuously embedded in $L^2$, we further prove that the approximation is
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#12289;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#31867;&#21644;&#34920;&#24449;&#21160;&#21147;&#23398;&#21464;&#21270;&#30340;&#25299;&#25169;&#19981;&#21464;&#29305;&#24449;&#25552;&#21462;&#65292;&#29305;&#21035;&#20851;&#27880;&#36229;&#20020;&#30028;&#38669;&#26222;&#20998;&#27495;&#12290;&#36825;&#20010;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#39044;&#27979;&#31995;&#32479;&#30340;&#36136;&#21464;&#21644;&#24120;&#21457;&#34892;&#20026;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2312.09234</link><description>&lt;p&gt;
&#20570;&#26102;&#38388;&#25197;&#26354;&#21543;&#65306;&#23398;&#20064;&#21160;&#21147;&#31995;&#32479;&#30340;&#25299;&#25169;&#19981;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
Let's do the time-warp-attend: Learning topological invariants of dynamical systems. (arXiv:2312.09234v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.09234
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#12289;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#31867;&#21644;&#34920;&#24449;&#21160;&#21147;&#23398;&#21464;&#21270;&#30340;&#25299;&#25169;&#19981;&#21464;&#29305;&#24449;&#25552;&#21462;&#65292;&#29305;&#21035;&#20851;&#27880;&#36229;&#20020;&#30028;&#38669;&#26222;&#20998;&#27495;&#12290;&#36825;&#20010;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#39044;&#27979;&#31995;&#32479;&#30340;&#36136;&#21464;&#21644;&#24120;&#21457;&#34892;&#20026;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#39046;&#22495;&#20013;&#30340;&#21160;&#21147;&#31995;&#32479;&#65292;&#20174;&#30005;&#36335;&#21040;&#29983;&#24577;&#32593;&#32476;&#65292;&#24403;&#20854;&#22522;&#26412;&#21442;&#25968;&#36328;&#36234;&#38408;&#20540;&#26102;&#65292;&#20250;&#21457;&#29983;&#36136;&#21464;&#21644;&#24120;&#21457;&#24615;&#30340;&#34892;&#20026;&#21464;&#21270;&#65292;&#31216;&#20026;&#20998;&#27495;&#12290;&#29616;&#26377;&#26041;&#27861;&#33021;&#22815;&#39044;&#27979;&#21333;&#20010;&#31995;&#32479;&#20013;&#21363;&#23558;&#21457;&#29983;&#30340;&#28798;&#38590;&#65292;&#20294;&#20027;&#35201;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#19988;&#22312;&#20998;&#31867;&#19981;&#21516;&#31995;&#32479;&#30340;&#23450;&#24615;&#21160;&#21147;&#23398;&#21464;&#21270;&#21644;&#25512;&#24191;&#21040;&#30495;&#23454;&#25968;&#25454;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#12289;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#21160;&#21147;&#23398;&#21464;&#21270;&#36827;&#34892;&#20998;&#31867;&#24182;&#34920;&#24449;&#20998;&#27495;&#36793;&#30028;&#30340;&#25299;&#25169;&#19981;&#21464;&#29305;&#24449;&#25552;&#21462;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#36229;&#20020;&#30028;&#38669;&#26222;&#20998;&#27495;&#30340;&#20856;&#22411;&#26696;&#20363;&#65292;&#20854;&#29992;&#20110;&#27169;&#25311;&#24191;&#27867;&#24212;&#29992;&#30340;&#21608;&#26399;&#24615;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#30340;&#21367;&#31215;&#20851;&#27880;&#26041;&#27861;&#32463;&#36807;&#20102;&#25968;&#25454;&#22686;&#24378;&#35757;&#32451;&#65292;&#40723;&#21169;&#23398;&#20064;&#21487;&#20197;&#29992;&#20110;&#26816;&#27979;&#20998;&#27495;&#36793;&#30028;&#30340;&#25299;&#25169;&#19981;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamical systems across the sciences, from electrical circuits to ecological networks, undergo qualitative and often catastrophic changes in behavior, called bifurcations, when their underlying parameters cross a threshold. Existing methods predict oncoming catastrophes in individual systems but are primarily time-series-based and struggle both to categorize qualitative dynamical regimes across diverse systems and to generalize to real data. To address this challenge, we propose a data-driven, physically-informed deep-learning framework for classifying dynamical regimes and characterizing bifurcation boundaries based on the extraction of topologically invariant features. We focus on the paradigmatic case of the supercritical Hopf bifurcation, which is used to model periodic dynamics across a wide range of applications. Our convolutional attention method is trained with data augmentations that encourage the learning of topological invariants which can be used to detect bifurcation boun
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#31561;&#21464;&#37327;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;EQNN&#65289;&#21644;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNN&#65289;&#19982;&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#20998;&#26512;&#65292;&#32467;&#26524;&#34920;&#26126;&#12298;$\mathbb{Z}_2\times \mathbb{Z}_2$&#12299;EQNN&#21644;QNN&#22312;&#36739;&#23567;&#30340;&#21442;&#25968;&#38598;&#21644;&#36866;&#20013;&#30340;&#35757;&#32451;&#25968;&#25454;&#26679;&#26412;&#19978;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2311.18744</link><description>&lt;p&gt;
&#12298;$\mathbb{Z}_2\times \mathbb{Z}_2$&#12299;&#31561;&#21464;&#37327;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65306;&#19982;&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#20934;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
$\mathbb{Z}_2\times \mathbb{Z}_2$ Equivariant Quantum Neural Networks: Benchmarking against Classical Neural Networks. (arXiv:2311.18744v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.18744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#31561;&#21464;&#37327;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;EQNN&#65289;&#21644;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNN&#65289;&#19982;&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#20998;&#26512;&#65292;&#32467;&#26524;&#34920;&#26126;&#12298;$\mathbb{Z}_2\times \mathbb{Z}_2$&#12299;EQNN&#21644;QNN&#22312;&#36739;&#23567;&#30340;&#21442;&#25968;&#38598;&#21644;&#36866;&#20013;&#30340;&#35757;&#32451;&#25968;&#25454;&#26679;&#26412;&#19978;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#31561;&#21464;&#37327;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;EQNN&#65289;&#21644;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNN&#65289;&#19982;&#23427;&#20204;&#30340;&#32463;&#20856;&#23545;&#24212;&#29289;&#65306;&#31561;&#21464;&#37327;&#31070;&#32463;&#32593;&#32476;&#65288;ENN&#65289;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#20998;&#26512;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#30340;&#29609;&#20855;&#31034;&#20363;&#35780;&#20272;&#27599;&#20010;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#20851;&#27880;&#27169;&#22411;&#22797;&#26434;&#24230;&#65288;&#30001;&#21442;&#25968;&#25968;&#37327;&#27979;&#37327;&#65289;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#12298;$\mathbb{Z}_2\times \mathbb{Z}_2$&#12299;EQNN&#21644;QNN&#22312;&#36739;&#23567;&#30340;&#21442;&#25968;&#38598;&#21644;&#36866;&#20013;&#30340;&#35757;&#32451;&#25968;&#25454;&#26679;&#26412;&#19978;&#25552;&#20379;&#20102;&#26356;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive comparative analysis of the performance of Equivariant Quantum Neural Networks (EQNN) and Quantum Neural Networks (QNN), juxtaposed against their classical counterparts: Equivariant Neural Networks (ENN) and Deep Neural Networks (DNN). We evaluate the performance of each network with two toy examples for a binary classification task, focusing on model complexity (measured by the number of parameters) and the size of the training data set. Our results show that the $\mathbb{Z}_2\times \mathbb{Z}_2$ EQNN and the QNN provide superior performance for smaller parameter sets and modest training data samples.
&lt;/p&gt;</description></item><item><title>RiskQ&#26159;&#19968;&#31181;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#39118;&#38505;&#25935;&#24863;&#21327;&#35843;&#35201;&#27714;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#39118;&#38505;&#25935;&#24863;&#30340;&#20010;&#20307;-&#20840;&#23616;&#26368;&#22823;&#65288;RIGM&#65289;&#21407;&#21017;&#21644;&#24314;&#27169;&#32852;&#21512;&#22238;&#25253;&#20998;&#24067;&#23454;&#29616;&#20215;&#20540;&#22240;&#23376;&#20998;&#35299;&#12290;</title><link>http://arxiv.org/abs/2311.01753</link><description>&lt;p&gt;
RiskQ: &#39118;&#38505;&#25935;&#24863;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20215;&#20540;&#22240;&#23376;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
RiskQ: Risk-sensitive Multi-Agent Reinforcement Learning Value Factorization. (arXiv:2311.01753v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01753
&lt;/p&gt;
&lt;p&gt;
RiskQ&#26159;&#19968;&#31181;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#39118;&#38505;&#25935;&#24863;&#21327;&#35843;&#35201;&#27714;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#39118;&#38505;&#25935;&#24863;&#30340;&#20010;&#20307;-&#20840;&#23616;&#26368;&#22823;&#65288;RIGM&#65289;&#21407;&#21017;&#21644;&#24314;&#27169;&#32852;&#21512;&#22238;&#25253;&#20998;&#24067;&#23454;&#29616;&#20215;&#20540;&#22240;&#23376;&#20998;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#29305;&#28857;&#26159;&#29615;&#22659;&#19981;&#30830;&#23450;&#24615;&#12289;&#26234;&#33021;&#20307;&#30340;&#31574;&#30053;&#22810;&#26679;&#24615;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#65292;&#36825;&#23548;&#33268;&#20102;&#26174;&#33879;&#30340;&#39118;&#38505;&#12290;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#23398;&#20064;&#23545;&#39118;&#38505;&#25935;&#24863;&#30340;&#21327;&#35843;&#21644;&#20998;&#25955;&#31574;&#30053;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#22312;&#39118;&#38505;&#25935;&#24863;&#30340;MARL&#20013;&#21046;&#23450;&#21327;&#35843;&#35201;&#27714;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#39118;&#38505;&#25935;&#24863;&#30340;&#20010;&#20307;-&#20840;&#23616;&#26368;&#22823;&#65288;RIGM&#65289;&#21407;&#29702;&#65292;&#20316;&#20026;&#20010;&#20307;-&#20840;&#23616;&#26368;&#22823;&#65288;IGM&#65289;&#21644;&#20998;&#24067;&#24335;IGM&#65288;DIGM&#65289;&#21407;&#29702;&#30340;&#19968;&#31181;&#25512;&#24191;&#12290;&#35813;&#21407;&#29702;&#35201;&#27714;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#39118;&#38505;&#25935;&#24863;&#21160;&#20316;&#36873;&#25321;&#38598;&#21512;&#24212;&#19982;&#20013;&#22830;&#31574;&#30053;&#30340;&#39118;&#38505;&#25935;&#24863;&#21160;&#20316;&#36873;&#25321;&#31561;&#20215;&#12290;&#24403;&#21069;&#30340;MARL&#20215;&#20540;&#22240;&#23376;&#20998;&#35299;&#26041;&#27861;&#23545;&#20110;&#24120;&#35265;&#30340;&#39118;&#38505;&#24230;&#37327;&#65288;&#20363;&#22914;&#39118;&#38505;&#20215;&#20540;&#65288;VaR&#65289;&#24230;&#37327;&#25110;&#25197;&#26354;&#30340;&#39118;&#38505;&#24230;&#37327;&#65289;&#19981;&#28385;&#36275;RIGM&#21407;&#21017;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RiskQ&#26469;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#36890;&#36807;&#24314;&#27169;&#32852;&#21512;&#22238;&#25253;&#20998;&#24067;&#26469;&#23454;&#29616;&#20215;&#20540;&#22240;&#23376;&#20998;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent systems are characterized by environmental uncertainty, varying policies of agents, and partial observability, which result in significant risks. In the context of Multi-Agent Reinforcement Learning (MARL), learning coordinated and decentralized policies that are sensitive to risk is challenging. To formulate the coordination requirements in risk-sensitive MARL, we introduce the Risk-sensitive Individual-Global-Max (RIGM) principle as a generalization of the Individual-Global-Max (IGM) and Distributional IGM (DIGM) principles. This principle requires that the collection of risk-sensitive action selections of each agent should be equivalent to the risk-sensitive action selection of the central policy. Current MARL value factorization methods do not satisfy the RIGM principle for common risk metrics such as the Value at Risk (VaR) metric or distorted risk measurements. Therefore, we propose RiskQ to address this limitation, which models the joint return distribution by modeli
&lt;/p&gt;</description></item><item><title>VQPy&#26159;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#35270;&#39057;&#20998;&#26512;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;Python&#21464;&#20307;&#20316;&#20026;&#21069;&#31471;&#65292;&#24182;&#20855;&#26377;&#21487;&#25193;&#23637;&#30340;&#21518;&#31471;&#65292;&#21487;&#20197;&#33258;&#21160;&#26500;&#24314;&#21644;&#20248;&#21270;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#30340;&#22788;&#29702;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2311.01623</link><description>&lt;p&gt;
VQPy&#65306;&#19968;&#31181;&#38754;&#21521;&#29616;&#20195;&#35270;&#39057;&#20998;&#26512;&#30340;&#38754;&#21521;&#23545;&#35937;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
VQPy: An Object-Oriented Approach to Modern Video Analytics. (arXiv:2311.01623v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01623
&lt;/p&gt;
&lt;p&gt;
VQPy&#26159;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#35270;&#39057;&#20998;&#26512;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;Python&#21464;&#20307;&#20316;&#20026;&#21069;&#31471;&#65292;&#24182;&#20855;&#26377;&#21487;&#25193;&#23637;&#30340;&#21518;&#31471;&#65292;&#21487;&#20197;&#33258;&#21160;&#26500;&#24314;&#21644;&#20248;&#21270;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#30340;&#22788;&#29702;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#20998;&#26512;&#24191;&#27867;&#24212;&#29992;&#20110;&#24403;&#20170;&#31995;&#32479;&#21644;&#26381;&#21153;&#20013;&#12290;&#22312;&#35270;&#39057;&#20998;&#26512;&#30340;&#21069;&#27839;&#26159;&#29992;&#25143;&#24320;&#21457;&#30340;&#35270;&#39057;&#26597;&#35810;&#65292;&#20197;&#25214;&#21040;&#29305;&#23450;&#24863;&#20852;&#36259;&#30340;&#23545;&#35937;&#12290;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#65288;&#20363;&#22914;&#20154;&#65292;&#21160;&#29289;&#65292;&#27773;&#36710;&#31561;&#65289;&#19982;&#20256;&#32479;&#38754;&#21521;&#23545;&#35937;&#35821;&#35328;&#24314;&#27169;&#30340;&#23545;&#35937;&#30456;&#20284;&#30340;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35270;&#39057;&#20998;&#26512;&#30340;&#38754;&#21521;&#23545;&#35937;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21517;&#20026;VQPy&#65292;&#21253;&#25324;&#19968;&#20010;&#21069;&#31471;&#65288;&#19968;&#31181;Python&#21464;&#20307;&#65292;&#20854;&#20013;&#21253;&#21547;&#29992;&#25143;&#21487;&#20197;&#34920;&#36798;&#35270;&#39057;&#23545;&#35937;&#21450;&#20854;&#20132;&#20114;&#30340;&#32467;&#26500;&#65289;&#21644;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#21518;&#31471;&#65292;&#21487;&#20197;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#33258;&#21160;&#29983;&#25104;&#21644;&#20248;&#21270;&#31649;&#36947;&#12290;&#25105;&#20204;&#24050;&#32463;&#23454;&#26045;&#21644;&#24320;&#28304;&#20102;VQPy&#65292;&#23427;&#24050;&#32463;&#20316;&#20026;Cisco DeepVision&#26694;&#26550;&#30340;&#19968;&#37096;&#20998;&#20135;&#21697;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video analytics is widely used in contemporary systems and services. At the forefront of video analytics are video queries that users develop to find objects of particular interest. Building upon the insight that video objects (e.g., human, animals, cars, etc.), the center of video analytics, are similar in spirit to objects modeled by traditional object-oriented languages, we propose to develop an object-oriented approach to video analytics. This approach, named VQPy, consists of a frontend$\unicode{x2015}$a Python variant with constructs that make it easy for users to express video objects and their interactions$\unicode{x2015}$as well as an extensible backend that can automatically construct and optimize pipelines based on video objects. We have implemented and open-sourced VQPy, which has been productized in Cisco as part of its DeepVision framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23398;&#20064;&#26377;&#25928;&#30340;&#36873;&#25321;&#31574;&#30053;&#25552;&#39640;&#20102;&#23376;&#22270;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25928;&#29575;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.20082</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#26377;&#25928;&#30340;&#36873;&#25321;&#31574;&#30053;&#25552;&#39640;&#23376;&#22270;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Efficient Subgraph GNNs by Learning Effective Selection Policies. (arXiv:2310.20082v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23398;&#20064;&#26377;&#25928;&#30340;&#36873;&#25321;&#31574;&#30053;&#25552;&#39640;&#20102;&#23376;&#22270;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25928;&#29575;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23376;&#22270;&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#21487;&#35777;&#26126;&#34920;&#36798;&#21147;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#21487;&#20197;&#20174;&#19968;&#32452;&#23376;&#22270;&#20013;&#23398;&#20064;&#22270;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22312;&#35768;&#22810;&#23376;&#22270;&#19978;&#25191;&#34892;&#20449;&#24687;&#20256;&#36882;&#25152;&#24102;&#26469;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#26412;&#25991;&#32771;&#34385;&#20197;&#25968;&#25454;&#39537;&#21160;&#26041;&#24335;&#23398;&#20064;&#22914;&#20309;&#36873;&#25321;&#19968;&#20010;&#23567;&#30340;&#23376;&#22270;&#23376;&#38598;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#35777;&#26126;&#23384;&#22312;&#19968;&#20123;WL-&#38590;&#21306;&#20998;&#30340;&#22270;&#26063;&#65292;&#36825;&#20123;&#22270;&#26063;&#23384;&#22312;&#21487;&#20197;&#35782;&#21035;&#35813;&#26063;&#20013;&#25152;&#26377;&#22270;&#30340;&#26377;&#25928;&#23376;&#22270;&#36873;&#25321;&#31574;&#30053;&#65292;&#26469;&#35299;&#37322;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Policy-Learn&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#36845;&#20195;&#26041;&#24335;&#23398;&#20064;&#22914;&#20309;&#36873;&#25321;&#23376;&#22270;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#19982;&#24120;&#29992;&#30340;&#38543;&#26426;&#31574;&#30053;&#21644;&#35299;&#20915;&#30456;&#21516;&#38382;&#39064;&#30340;&#20808;&#21069;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26550;&#26500;&#33021;&#22815;&#23398;&#20064;&#21040;&#19978;&#36848;&#39640;&#25928;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Policy-Learn&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Subgraph GNNs are provably expressive neural architectures that learn graph representations from sets of subgraphs. Unfortunately, their applicability is hampered by the computational complexity associated with performing message passing on many subgraphs. In this paper, we consider the problem of learning to select a small subset of the large set of possible subgraphs in a data-driven fashion. We first motivate the problem by proving that there are families of WL-indistinguishable graphs for which there exist efficient subgraph selection policies: small subsets of subgraphs that can already identify all the graphs within the family. We then propose a new approach, called Policy-Learn, that learns how to select subgraphs in an iterative manner. We prove that, unlike popular random policies and prior work addressing the same problem, our architecture is able to learn the efficient policies mentioned above. Our experimental results demonstrate that Policy-Learn outperforms existing basel
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#26102;&#38388;&#36830;&#32493; (TiC) &#22522;&#20934;&#65292;&#20351;&#29992;&#36825;&#20123;&#22522;&#20934;&#35780;&#20272;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#26102;&#38388;&#40065;&#26834;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#25490;&#32451;&#26041;&#27861;&#26469;&#25345;&#32493;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.16226</link><description>&lt;p&gt;
TiC-CLIP: CLIP&#27169;&#22411;&#30340;&#25345;&#32493;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
TiC-CLIP: Continual Training of CLIP Models. (arXiv:2310.16226v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16226
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#26102;&#38388;&#36830;&#32493; (TiC) &#22522;&#20934;&#65292;&#20351;&#29992;&#36825;&#20123;&#22522;&#20934;&#35780;&#20272;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#26102;&#38388;&#40065;&#26834;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#25490;&#32451;&#26041;&#27861;&#26469;&#25345;&#32493;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#25345;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#19982;&#26368;&#26032;&#25968;&#25454;&#20445;&#25345;&#21516;&#27493;&#26412;&#36523;&#23601;&#26159;&#26114;&#36149;&#30340;&#12290;&#20026;&#20102;&#36991;&#20813;&#19981;&#26029;&#37325;&#26032;&#35757;&#32451;&#30340;&#39640;&#25104;&#26412;&#65292;&#25345;&#32493;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20010;&#38382;&#39064;&#34987;&#32570;&#20047;&#22823;&#35268;&#27169;&#36830;&#32493;&#23398;&#20064;&#22522;&#20934;&#25110;&#22522;&#32447;&#25152;&#21152;&#21095;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#31532;&#19968;&#25209; Web &#35268;&#27169;&#26102;&#38388;&#36830;&#32493;&#65288;TiC&#65289;&#22522;&#20934;&#65306;TiC-DataCompt&#12289;TiC-YFCC &#21644; TiC-RedCaps&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807; 127 &#20159;&#20010;&#26102;&#38388;&#25139;&#22270;&#20687;-&#25991;&#26412;&#23545;&#65292;&#36328;&#36234;&#20102; 9 &#24180;&#30340;&#26102;&#38388;&#65288;2014-2022&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#36825;&#20123;&#22522;&#20934;&#26469;&#31574;&#21010;&#21508;&#31181;&#21160;&#24577;&#35780;&#20272;&#65292;&#20197;&#34913;&#37327;&#29616;&#26377;&#27169;&#22411;&#30340;&#26102;&#38388;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; OpenAI &#30340; CLIP &#27169;&#22411;&#65288;&#20351;&#29992; 2020 &#24180;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65289;&#22312;&#25105;&#20204;&#31574;&#21010;&#30340;&#20174; 2021 &#24180;&#21040; 2022 &#24180;&#30340;&#26816;&#32034;&#20219;&#21153;&#20013;&#65292;&#22833;&#21435;&#20102;&#32422; 8% &#30340;&#38646;-shot&#20934;&#30830;&#29575;&#65292;&#32780;&#19982; OpenCLIP &#23384;&#20648;&#24211;&#20013;&#26368;&#36817;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#27604;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#39640;&#25928;&#22320;&#23545;&#26102;&#38388;&#36830;&#32493;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25490;&#32451;&#26041;&#27861;&#65292;&#20174;&#19978;&#27425;&#30340;&#35757;&#32451;&#20013;&#32487;&#32493;&#35757;&#32451;&#65292;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Keeping large foundation models up to date on latest data is inherently expensive. To avoid the prohibitive costs of constantly retraining, it is imperative to continually train these models. This problem is exacerbated by the lack of any large scale continual learning benchmarks or baselines. We introduce the first set of web-scale Time-Continual (TiC) benchmarks for training vision-language models: TiC-DataCompt, TiC-YFCC, and TiC-RedCaps with over 12.7B timestamped image-text pairs spanning 9 years (2014--2022). We first use our benchmarks to curate various dynamic evaluations to measure temporal robustness of existing models. We show OpenAI's CLIP (trained on data up to 2020) loses $\approx 8\%$ zero-shot accuracy on our curated retrieval task from 2021--2022 compared with more recently trained models in OpenCLIP repository. We then study how to efficiently train models on time-continuous data. We demonstrate that a simple rehearsal-based approach that continues training from the l
&lt;/p&gt;</description></item><item><title>LMC&#22810;&#20219;&#21153;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#31934;&#30830;&#35299;&#20915;&#26041;&#26696;&#34920;&#26126;&#65292;&#21482;&#38656;&#23545;&#22122;&#22768;&#27169;&#22411;&#36827;&#34892;&#28201;&#21644;&#20551;&#35774;&#65292;&#21363;&#21487;&#23454;&#29616;&#39640;&#25928;&#35745;&#31639;&#12290;&#36890;&#36807;&#24341;&#20837;&#23436;&#25972;&#21442;&#25968;&#21270;&#30340;&#8220;&#25237;&#24433;LMC&#8221;&#27169;&#22411;&#21644;&#36793;&#32536;&#20284;&#28982;&#20989;&#25968;&#34920;&#36798;&#24335;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#26410;&#32463;&#22788;&#29702;&#30340;&#26041;&#27861;&#30340;&#20248;&#24322;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.12032</link><description>&lt;p&gt;
LMC&#22810;&#20219;&#21153;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#31934;&#30830;&#21644;&#39640;&#25928;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Exact and efficient solutions of the LMC Multitask Gaussian Process model. (arXiv:2310.12032v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12032
&lt;/p&gt;
&lt;p&gt;
LMC&#22810;&#20219;&#21153;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#31934;&#30830;&#35299;&#20915;&#26041;&#26696;&#34920;&#26126;&#65292;&#21482;&#38656;&#23545;&#22122;&#22768;&#27169;&#22411;&#36827;&#34892;&#28201;&#21644;&#20551;&#35774;&#65292;&#21363;&#21487;&#23454;&#29616;&#39640;&#25928;&#35745;&#31639;&#12290;&#36890;&#36807;&#24341;&#20837;&#23436;&#25972;&#21442;&#25968;&#21270;&#30340;&#8220;&#25237;&#24433;LMC&#8221;&#27169;&#22411;&#21644;&#36793;&#32536;&#20284;&#28982;&#20989;&#25968;&#34920;&#36798;&#24335;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#26410;&#32463;&#22788;&#29702;&#30340;&#26041;&#27861;&#30340;&#20248;&#24322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#20849;&#21516;&#20851;&#32852;&#27169;&#22411;&#65288;LMC&#65289;&#26159;&#19968;&#31181;&#38750;&#24120;&#36890;&#29992;&#30340;&#22810;&#20219;&#21153;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65292;&#29992;&#20110;&#22238;&#24402;&#25110;&#20998;&#31867;&#12290;&#34429;&#28982;&#20854;&#34920;&#36798;&#33021;&#21147;&#21644;&#27010;&#24565;&#31616;&#21333;&#24615;&#24456;&#26377;&#21560;&#24341;&#21147;&#65292;&#20294;&#26420;&#32032;&#23454;&#29616;&#22312;&#25968;&#25454;&#28857;&#25968;&#37327;&#21644;&#20219;&#21153;&#25968;&#37327;&#26041;&#38754;&#20855;&#26377;&#31435;&#26041;&#22797;&#26434;&#24230;&#65292;&#20351;&#24471;&#23545;&#22823;&#22810;&#25968;&#24212;&#29992;&#26469;&#35828;&#65292;&#24517;&#39035;&#36827;&#34892;&#36817;&#20284;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#65292;&#35813;&#27169;&#22411;&#30340;&#28508;&#22312;&#36807;&#31243;&#21487;&#20197;&#35299;&#32806;&#65292;&#23548;&#33268;&#20165;&#19982;&#25152;&#36848;&#36807;&#31243;&#25968;&#37327;&#21576;&#32447;&#24615;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#25193;&#23637;&#20102;&#36825;&#20123;&#32467;&#26524;&#65292;&#20174;&#26368;&#19968;&#33324;&#30340;&#20551;&#35774;&#20013;&#23637;&#31034;&#20102;&#22312;LMC&#30340;&#39640;&#25928;&#31934;&#30830;&#35745;&#31639;&#25152;&#38656;&#30340;&#21807;&#19968;&#26465;&#20214;&#26159;&#23545;&#22122;&#22768;&#27169;&#22411;&#36827;&#34892;&#28201;&#21644;&#20551;&#35774;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#32467;&#26524;&#30340;&#23436;&#25972;&#21442;&#25968;&#21270;&#8220;&#25237;&#24433;LMC&#8221;&#27169;&#22411;&#65292;&#24182;&#32473;&#20986;&#20102;&#36793;&#32536;&#20284;&#28982;&#20989;&#25968;&#30340;&#34920;&#36798;&#24335;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#20248;&#21270;&#12290;&#25105;&#20204;&#23545;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#20102;&#21442;&#25968;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30456;&#23545;&#20110;&#26410;&#32463;&#22788;&#29702;&#30340;&#26041;&#27861;&#30340;&#20248;&#24322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Linear Model of Co-regionalization (LMC) is a very general model of multitask gaussian process for regression or classification. While its expressivity and conceptual simplicity are appealing, naive implementations have cubic complexity in the number of datapoints and number of tasks, making approximations mandatory for most applications. However, recent work has shown that under some conditions the latent processes of the model can be decoupled, leading to a complexity that is only linear in the number of said processes. We here extend these results, showing from the most general assumptions that the only condition necessary to an efficient exact computation of the LMC is a mild hypothesis on the noise model. We introduce a full parametrization of the resulting \emph{projected LMC} model, and an expression of the marginal likelihood enabling efficient optimization. We perform a parametric study on synthetic data to show the excellent performance of our approach, compared to an unr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65288;SSRL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#24314;&#38543;&#26426;&#25968;&#25454;&#25237;&#24433;&#26469;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#19981;&#20381;&#36182;&#20110;&#22686;&#24378;&#25110;&#25513;&#34109;&#25216;&#26415;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#25968;&#25454;&#27169;&#24577;&#21644;&#32593;&#32476;&#26550;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;SSRL&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.07756</link><description>&lt;p&gt;
&#20174;&#38543;&#26426;&#25968;&#25454;&#25237;&#24433;&#22120;&#36827;&#34892;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Representation Learning From Random Data Projectors. (arXiv:2310.07756v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07756
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65288;SSRL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#24314;&#38543;&#26426;&#25968;&#25454;&#25237;&#24433;&#26469;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#19981;&#20381;&#36182;&#20110;&#22686;&#24378;&#25110;&#25513;&#34109;&#25216;&#26415;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#25968;&#25454;&#27169;&#24577;&#21644;&#32593;&#32476;&#26550;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;SSRL&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#20154;&#24037;&#35774;&#35745;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#19979;&#30340;&#21464;&#25442;&#19981;&#21464;&#24615;&#20551;&#35774;&#65292;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65288;SSRL&#65289;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#34429;&#28982;&#22522;&#20110;&#22686;&#24378;&#30340;SSRL&#31639;&#27861;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#25512;&#21160;&#20102;&#24615;&#33021;&#30340;&#25552;&#21319;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#19981;&#36866;&#29992;&#20110;&#20854;&#20182;&#25968;&#25454;&#27169;&#24577;&#65292;&#24182;&#19988;&#21487;&#33021;&#19982;&#24212;&#29992;&#29305;&#23450;&#30340;&#25968;&#25454;&#22686;&#24378;&#32422;&#26463;&#20914;&#31361;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;SSRL&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#25968;&#25454;&#27169;&#24577;&#21644;&#32593;&#32476;&#26550;&#26500;&#65292;&#22240;&#20026;&#23427;&#19981;&#20381;&#36182;&#20110;&#22686;&#24378;&#25110;&#25513;&#34109;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#37325;&#24314;&#38543;&#26426;&#25968;&#25454;&#25237;&#24433;&#26469;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#25105;&#20204;&#23545;&#36328;&#22810;&#31181;&#27169;&#24577;&#21644;&#23454;&#38469;&#24212;&#29992;&#30340;&#34920;&#31034;&#23398;&#20064;&#20219;&#21153;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#20248;&#20110;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;SSRL&#22522;&#32447;&#27169;&#22411;&#12290;&#30001;&#20110;&#20854;&#24191;&#27867;&#36866;&#29992;&#24615;&#21644;&#24378;&#22823;&#30340;&#23454;&#35777;&#32467;&#26524;&#65292;&#25105;&#20204;&#35748;&#20026;...
&lt;/p&gt;
&lt;p&gt;
Self-supervised representation learning~(SSRL) has advanced considerably by exploiting the transformation invariance assumption under artificially designed data augmentations. While augmentation-based SSRL algorithms push the boundaries of performance in computer vision and natural language processing, they are often not directly applicable to other data modalities, and can conflict with application-specific data augmentation constraints. This paper presents an SSRL approach that can be applied to any data modality and network architecture because it does not rely on augmentations or masking. Specifically, we show that high-quality data representations can be learned by reconstructing random data projections. We evaluate the proposed approach on a wide range of representation learning tasks that span diverse modalities and real-world applications. We show that it outperforms multiple state-of-the-art SSRL baselines. Due to its wide applicability and strong empirical results, we argue t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36127;&#36317;&#31163;&#26680;&#30340;&#26368;&#22823;&#24179;&#22343;&#36317;&#31163;(MMD)&#30340;&#26465;&#20214;&#27969;&#26041;&#27861;&#65292;&#29992;&#20110;&#21518;&#39564;&#25277;&#26679;&#21644;&#26465;&#20214;&#29983;&#25104;&#24314;&#27169;&#12290;&#36890;&#36807;&#31163;&#25955;&#30340;Wasserstein&#26799;&#24230;&#27969;&#36817;&#20284;&#32852;&#21512;&#20998;&#24067;&#65292;&#35777;&#26126;&#20102;&#31890;&#23376;&#27969;&#26159;&#36866;&#24403;&#21151;&#33021;&#30340;Wasserstein&#26799;&#24230;&#27969;&#12290;&#22312;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#21644;&#36229;&#20998;&#36776;&#29575;&#31561;&#36870;&#38382;&#39064;&#20013;&#23637;&#31034;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03054</link><description>&lt;p&gt;
&#22522;&#20110;&#36127;&#36317;&#31163;&#26680;&#30340;&#26368;&#22823;&#24179;&#22343;&#36317;&#31163;(MMD)&#26799;&#24230;&#27969;&#30340;&#21518;&#39564;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Posterior Sampling Based on Gradient Flows of the MMD with Negative Distance Kernel. (arXiv:2310.03054v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36127;&#36317;&#31163;&#26680;&#30340;&#26368;&#22823;&#24179;&#22343;&#36317;&#31163;(MMD)&#30340;&#26465;&#20214;&#27969;&#26041;&#27861;&#65292;&#29992;&#20110;&#21518;&#39564;&#25277;&#26679;&#21644;&#26465;&#20214;&#29983;&#25104;&#24314;&#27169;&#12290;&#36890;&#36807;&#31163;&#25955;&#30340;Wasserstein&#26799;&#24230;&#27969;&#36817;&#20284;&#32852;&#21512;&#20998;&#24067;&#65292;&#35777;&#26126;&#20102;&#31890;&#23376;&#27969;&#26159;&#36866;&#24403;&#21151;&#33021;&#30340;Wasserstein&#26799;&#24230;&#27969;&#12290;&#22312;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#21644;&#36229;&#20998;&#36776;&#29575;&#31561;&#36870;&#38382;&#39064;&#20013;&#23637;&#31034;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36127;&#36317;&#31163;&#26680;&#30340;&#26368;&#22823;&#24179;&#22343;&#36317;&#31163;(MMD)&#30340;&#26465;&#20214;&#27969;&#29992;&#20110;&#21518;&#39564;&#25277;&#26679;&#21644;&#26465;&#20214;&#29983;&#25104;&#24314;&#27169;&#12290;&#36825;&#20010;MMD&#65292;&#20063;&#34987;&#31216;&#20026;&#33021;&#37327;&#36317;&#31163;&#65292;&#20855;&#26377;&#20687;&#36890;&#36807;&#20999;&#29255;&#21644;&#25490;&#24207;&#36827;&#34892;&#39640;&#25928;&#35745;&#31639;&#30340;&#20960;&#20010;&#26377;&#30410;&#23646;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#31163;&#25955;&#30340;Wasserstein&#26799;&#24230;&#27969;&#26469;&#36817;&#20284;&#30495;&#23454;&#24773;&#20917;&#21644;&#35266;&#23519;&#20540;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#24182;&#20026;&#21518;&#39564;&#20998;&#24067;&#24314;&#31435;&#20102;&#35823;&#24046;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31890;&#23376;&#27969;&#30830;&#23454;&#26159;&#36866;&#24403;&#21151;&#33021;&#30340;Wasserstein&#26799;&#24230;&#27969;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#33021;&#21147;&#36890;&#36807;&#25968;&#23383;&#31034;&#20363;&#36827;&#34892;&#20102;&#28436;&#31034;&#65292;&#21253;&#25324;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#21644;&#35832;&#22914;&#36229;&#20998;&#36776;&#29575;&#12289;&#20462;&#22797;&#21644;&#20302;&#21058;&#37327;&#21644;&#26377;&#38480;&#35282;&#24230;&#35774;&#32622;&#19979;&#30340;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#31561;&#36870;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose conditional flows of the maximum mean discrepancy (MMD) with the negative distance kernel for posterior sampling and conditional generative modeling. This MMD, which is also known as energy distance, has several advantageous properties like efficient computation via slicing and sorting. We approximate the joint distribution of the ground truth and the observations using discrete Wasserstein gradient flows and establish an error bound for the posterior distributions. Further, we prove that our particle flow is indeed a Wasserstein gradient flow of an appropriate functional. The power of our method is demonstrated by numerical examples including conditional image generation and inverse problems like superresolution, inpainting and computed tomography in low-dose and limited-angle settings.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#20809;&#32420;&#31070;&#32463;&#20803;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#29289;&#29702;&#32422;&#26463;&#65292;&#23454;&#29616;&#20102;&#23545;&#20809;&#32420;&#31070;&#32463;&#26550;&#26500;&#30340;&#40065;&#26834;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2310.03049</link><description>&lt;p&gt;
QuATON: &#20809;&#32420;&#31070;&#32463;&#20803;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
QuATON: Quantization Aware Training of Optical Neurons. (arXiv:2310.03049v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03049
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#20809;&#32420;&#31070;&#32463;&#20803;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#29289;&#29702;&#32422;&#26463;&#65292;&#23454;&#29616;&#20102;&#23545;&#20809;&#32420;&#31070;&#32463;&#26550;&#26500;&#30340;&#40065;&#26834;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#23398;&#31070;&#32463;&#26550;&#26500;&#65288;ONA&#65289;&#20351;&#29992;&#20855;&#26377;&#20248;&#21270;&#29289;&#29702;&#21442;&#25968;&#30340;&#32534;&#30721;&#20803;&#20214;&#36827;&#34892;&#26234;&#33021;&#27979;&#37327;&#12290;&#28982;&#32780;&#65292;&#21046;&#36896;&#20855;&#26377;&#35774;&#35745;&#24615;&#33021;&#30340;ONA&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#21046;&#36896;&#25216;&#26415;&#30340;&#38480;&#21046;&#36890;&#24120;&#38480;&#21046;&#20102;&#35757;&#32451;&#21442;&#25968;&#30340;&#21487;&#23454;&#29616;&#31934;&#24230;&#12290;&#29289;&#29702;&#32422;&#26463;&#20063;&#21487;&#33021;&#38480;&#21046;&#29289;&#29702;&#21442;&#25968;&#21487;&#20197;&#23481;&#32435;&#30340;&#20540;&#33539;&#22260;&#12290;&#22240;&#27492;&#65292;ONA&#24212;&#22312;&#21487;&#23454;&#29616;&#30340;&#32422;&#26463;&#26465;&#20214;&#19979;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#32422;&#26463;&#23558;&#35757;&#32451;&#30446;&#26631;&#36716;&#21270;&#20026;&#19968;&#20010;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#65292;&#20351;&#20854;&#26356;&#38590;&#20197;&#21033;&#29992;&#29616;&#26377;&#30340;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#20174;&#27169;&#25311;&#21040;&#23454;&#29616;&#30340;&#20851;&#38190;&#38382;&#39064;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#19968;&#31181;ONA&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35813;ONA&#34987;&#31216;&#20026;&#34893;&#23556;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optical neural architectures (ONAs) use coding elements with optimized physical parameters to perform intelligent measurements. However, fabricating ONAs while maintaining design performances is challenging. Limitations in fabrication techniques often limit the realizable precision of the trained parameters. Physical constraints may also limit the range of values the physical parameters can hold. Thus, ONAs should be trained within the implementable constraints. However, such physics-based constraints reduce the training objective to a constrained optimization problem, making it harder to optimize with existing gradient-based methods. To alleviate these critical issues that degrade performance from simulation to realization we propose a physics-informed quantization-aware training framework. Our approach accounts for the physical constraints during the training process, leading to robust designs. We evaluate our approach on an ONA proposed in the literature, named a diffractive deep ne
&lt;/p&gt;</description></item><item><title>ED-NeRF &#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340; 3D &#22330;&#26223;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22330;&#26223;&#23884;&#20837;&#21040;&#28508;&#31354;&#38388;&#20013;&#65292;&#24471;&#21040;&#26356;&#24555;&#36895;&#19988;&#26356;&#26131;&#20110;&#32534;&#36753;&#30340; NeRF &#39592;&#24178;&#12290;</title><link>http://arxiv.org/abs/2310.02712</link><description>&lt;p&gt;
ED-NeRF: &#20351;&#29992;&#28508;&#31354;&#38388; NeRF &#23454;&#29616;&#39640;&#25928;&#30340;&#25991;&#26412;&#24341;&#23548;&#30340; 3D &#22330;&#26223;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
ED-NeRF: Efficient Text-Guided Editing of 3D Scene using Latent Space NeRF. (arXiv:2310.02712v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02712
&lt;/p&gt;
&lt;p&gt;
ED-NeRF &#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340; 3D &#22330;&#26223;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22330;&#26223;&#23884;&#20837;&#21040;&#28508;&#31354;&#38388;&#20013;&#65292;&#24471;&#21040;&#26356;&#24555;&#36895;&#19988;&#26356;&#26131;&#20110;&#32534;&#36753;&#30340; NeRF &#39592;&#24178;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#22312;&#20108;&#32500;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#36827;&#23637;&#24050;&#32463;&#25193;&#23637;&#21040;&#19977;&#32500;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#26032;&#30340;&#19977;&#32500;&#23545;&#35937;&#12290;&#36825;&#28436;&#21464;&#25104;&#20102; NeRF &#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#25991;&#26412;&#26465;&#20214;&#20801;&#35768;&#23545;&#29616;&#26377;&#30340;&#19977;&#32500;&#23545;&#35937;&#36827;&#34892;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340; NeRF &#32534;&#36753;&#25216;&#26415;&#22312;&#24615;&#33021;&#19978;&#38754;&#20020;&#30528;&#19968;&#20123;&#38480;&#21046;&#65292;&#22914;&#35757;&#32451;&#36895;&#24230;&#24930;&#21644;&#20351;&#29992;&#30340;&#25439;&#22833;&#20989;&#25968;&#19981;&#20805;&#20998;&#32771;&#34385;&#32534;&#36753;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340; 3D NeRF &#32534;&#36753;&#26041;&#27861;&#65292;&#31216;&#20026; ED-NeRF&#65292;&#36890;&#36807;&#23558;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#25104;&#21151;&#23884;&#20837;&#21040;&#28508;&#25193;&#25955;&#27169;&#22411; (LDM) &#30340;&#28508;&#31354;&#38388;&#20013;&#65292;&#36890;&#36807;&#29420;&#29305;&#30340;&#32454;&#21270;&#23618;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#33719;&#24471;&#19968;&#20010;&#19981;&#20165;&#26356;&#24555;&#65292;&#32780;&#19988;&#26356;&#36866;&#21512;&#20110;&#32534;&#36753;&#30340; NeRF &#39592;&#24178;&#65292;&#19982;&#20256;&#32479;&#30340;&#22270;&#20687;&#31354;&#38388; NeRF &#32534;&#36753;&#30456;&#27604;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there has been a significant advancement in text-to-image diffusion models, leading to groundbreaking performance in 2D image generation. These advancements have been extended to 3D models, enabling the generation of novel 3D objects from textual descriptions. This has evolved into NeRF editing methods, which allow the manipulation of existing 3D objects through textual conditioning. However, existing NeRF editing techniques have faced limitations in their performance due to slow training speeds and the use of loss functions that do not adequately consider editing. To address this, here we present a novel 3D NeRF editing approach dubbed ED-NeRF by successfully embedding real-world scenes into the latent space of the latent diffusion model (LDM) through a unique refinement layer. This approach enables us to obtain a NeRF backbone that is not only faster but also more amenable to editing compared to traditional image space NeRF editing. Furthermore, we propose an improved loss 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39034;&#24207;&#20915;&#31574;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#20154;&#30340;&#20381;&#20174;&#31243;&#24230;&#21644;&#26426;&#22120;&#25552;&#20379;&#24314;&#35758;&#30340;&#26102;&#26426;&#65292;&#24182;&#25552;&#20379;&#20102;&#23398;&#20064;&#31639;&#27861;&#26469;&#23398;&#20064;&#26368;&#20339;&#30340;&#24314;&#35758;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.00817</link><description>&lt;p&gt;
&#23398;&#20064;&#22914;&#20309;&#25552;&#20379;&#27880;&#37325;&#20381;&#20174;&#24615;&#30340;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Learning to Make Adherence-Aware Advice. (arXiv:2310.00817v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39034;&#24207;&#20915;&#31574;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#20154;&#30340;&#20381;&#20174;&#31243;&#24230;&#21644;&#26426;&#22120;&#25552;&#20379;&#24314;&#35758;&#30340;&#26102;&#26426;&#65292;&#24182;&#25552;&#20379;&#20102;&#23398;&#20064;&#31639;&#27861;&#26469;&#23398;&#20064;&#26368;&#20339;&#30340;&#24314;&#35758;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#20154;&#31867;&#20915;&#31574;&#20013;&#25198;&#28436;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#20132;&#20114;&#23384;&#22312;&#25361;&#25112;&#12290;&#30001;&#20110;&#27809;&#26377;&#20805;&#20998;&#32771;&#34385;&#21040;&#20154;&#31867;&#24573;&#35270;&#20154;&#24037;&#26234;&#33021;&#24314;&#35758;&#21644;&#20154;&#24037;&#26234;&#33021;&#36873;&#25321;&#24615;&#25552;&#20379;&#24314;&#35758;&#30340;&#38656;&#27714;&#65292;&#19968;&#20010;&#25361;&#25112;&#23601;&#26469;&#33258;&#20110;&#24213;&#23618;&#20154;&#24037;&#26234;&#33021;&#31574;&#30053;&#30340;&#19981;&#20339;&#34920;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39034;&#24207;&#20915;&#31574;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#20154;&#31867;&#30340;&#20381;&#20174;&#31243;&#24230;&#65288;&#21363;&#20154;&#31867;&#36981;&#24490;/&#25298;&#32477;&#26426;&#22120;&#24314;&#35758;&#30340;&#27010;&#29575;&#65289;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#25512;&#36831;&#36873;&#39033;&#65292;&#20351;&#24471;&#26426;&#22120;&#22312;&#26368;&#21512;&#36866;&#30340;&#26102;&#20505;&#21487;&#20197;&#26242;&#26102;&#19981;&#25552;&#20379;&#24314;&#35758;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#26368;&#20339;&#30340;&#24314;&#35758;&#31574;&#30053;&#65292;&#24182;&#20165;&#22312;&#20851;&#38190;&#26102;&#21051;&#25552;&#20379;&#24314;&#35758;&#12290;&#19982;&#38382;&#39064;&#19981;&#21487;&#30693;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#19987;&#38376;&#21270;&#23398;&#20064;&#31639;&#27861;&#19981;&#20165;&#20855;&#26377;&#26356;&#22909;&#30340;&#29702;&#35770;&#25910;&#25947;&#24615;&#33021;&#65292;&#32780;&#19988;&#22312;&#23454;&#35777;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
As artificial intelligence (AI) systems play an increasingly prominent role in human decision-making, challenges surface in the realm of human-AI interactions. One challenge arises from the suboptimal AI policies due to the inadequate consideration of humans disregarding AI recommendations, as well as the need for AI to provide advice selectively when it is most pertinent. This paper presents a sequential decision-making model that (i) takes into account the human's adherence level (the probability that the human follows/rejects machine advice) and (ii) incorporates a defer option so that the machine can temporarily refrain from making advice. We provide learning algorithms that learn the optimal advice policy and make advice only at critical time stamps. Compared to problem-agnostic reinforcement learning algorithms, our specialized learning algorithms not only enjoy better theoretical convergence properties but also show strong empirical performance.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;CDML&#35774;&#35745;&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#25351;&#23548;&#24320;&#21457;&#32773;&#35774;&#35745;&#28385;&#36275;&#29992;&#20363;&#35201;&#27714;&#30340;&#21327;&#20316;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.16584</link><description>&lt;p&gt;
&#29992;&#20110;&#24320;&#21457;&#21327;&#20316;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#35774;&#35745;&#24037;&#20855;&#31665;
&lt;/p&gt;
&lt;p&gt;
A Design Toolbox for the Development of Collaborative Distributed Machine Learning Systems. (arXiv:2309.16584v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16584
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;CDML&#35774;&#35745;&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#25351;&#23548;&#24320;&#21457;&#32773;&#35774;&#35745;&#28385;&#36275;&#29992;&#20363;&#35201;&#27714;&#30340;&#21327;&#20316;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26426;&#23494;&#24615;&#30340;&#21516;&#26102;&#21033;&#29992;&#26469;&#33258;&#22810;&#26041;&#30340;&#35757;&#32451;&#25968;&#25454;&#23545;&#27169;&#22411;&#36827;&#34892;&#20805;&#20998;&#35757;&#32451;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#21508;&#31181;&#21327;&#20316;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;CDML&#65289;&#31995;&#32479;&#35774;&#35745;&#65292;&#20363;&#22914;&#36741;&#21161;&#23398;&#20064;&#12289;&#32852;&#37030;&#23398;&#20064;&#21644;&#20998;&#35010;&#23398;&#20064;&#12290;CDML&#31995;&#32479;&#35774;&#35745;&#23637;&#31034;&#20102;&#19981;&#21516;&#30340;&#29305;&#24449;&#65292;&#20363;&#22914;&#39640;&#24230;&#30340;&#20195;&#29702;&#20154;&#33258;&#27835;&#24615;&#12289;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26426;&#23494;&#24615;&#21644;&#23481;&#38169;&#24615;&#12290;&#38754;&#23545;&#19981;&#21516;&#29305;&#24449;&#30340;&#21508;&#31181;CDML&#31995;&#32479;&#35774;&#35745;&#65292;&#24320;&#21457;&#32773;&#24456;&#38590;&#26377;&#38024;&#23545;&#24615;&#22320;&#35774;&#35745;&#28385;&#36275;&#29992;&#20363;&#35201;&#27714;&#30340;CDML&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#19981;&#21512;&#36866;&#30340;CDML&#31995;&#32479;&#35774;&#35745;&#21487;&#33021;&#23548;&#33268;CDML&#31995;&#32479;&#26080;&#27861;&#23454;&#29616;&#20854;&#39044;&#26399;&#30446;&#30340;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;CDML&#35774;&#35745;&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#25351;&#23548;CDML&#31995;&#32479;&#30340;&#24320;&#21457;&#12290;&#22522;&#20110;CDML&#35774;&#35745;&#24037;&#20855;&#31665;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#19981;&#21516;&#20851;&#38190;&#29305;&#24449;&#30340;CDML&#31995;&#32479;&#20856;&#22411;&#65292;&#21487;&#20197;&#25903;&#25345;&#35774;&#35745;&#28385;&#36275;&#29992;&#20363;&#35201;&#27714;&#30340;CDML&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
To leverage training data for the sufficient training of ML models from multiple parties in a confidentiality-preserving way, various collaborative distributed machine learning (CDML) system designs have been developed, for example, to perform assisted learning, federated learning, and split learning. CDML system designs show different traits, for example, high agent autonomy, machine learning (ML) model confidentiality, and fault tolerance. Facing a wide variety of CDML system designs with different traits, it is difficult for developers to design CDML systems with traits that match use case requirements in a targeted way. However, inappropriate CDML system designs may result in CDML systems failing their envisioned purposes. We developed a CDML design toolbox that can guide the development of CDML systems. Based on the CDML design toolbox, we present CDML system archetypes with distinct key traits that can support the design of CDML systems to meet use case requirements.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29289;&#29702;&#22686;&#24378;&#27531;&#24046;&#23398;&#20064;&#65288;PERL&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20132;&#36890;&#29366;&#24577;&#39044;&#27979;&#12290;PERL&#27169;&#22411;&#38598;&#25104;&#20102;&#29289;&#29702;&#27169;&#22411;&#21644;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#23558;&#29289;&#29702;&#27169;&#22411;&#32467;&#26524;&#21644;&#39044;&#27979;&#27531;&#24046;&#20316;&#20026;&#20462;&#27491;&#30456;&#32467;&#21512;&#65292;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#19988;&#27604;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#35201;&#27714;&#26356;&#23569;&#30340;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.15284</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#20132;&#36890;&#29366;&#24577;&#39044;&#27979;&#30340;&#29289;&#29702;&#22686;&#24378;&#27531;&#24046;&#23398;&#20064;&#65288;PERL&#65289;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Physics Enhanced Residual Learning (PERL) Framework for Traffic State Prediction. (arXiv:2309.15284v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15284
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29289;&#29702;&#22686;&#24378;&#27531;&#24046;&#23398;&#20064;&#65288;PERL&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20132;&#36890;&#29366;&#24577;&#39044;&#27979;&#12290;PERL&#27169;&#22411;&#38598;&#25104;&#20102;&#29289;&#29702;&#27169;&#22411;&#21644;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#23558;&#29289;&#29702;&#27169;&#22411;&#32467;&#26524;&#21644;&#39044;&#27979;&#27531;&#24046;&#20316;&#20026;&#20462;&#27491;&#30456;&#32467;&#21512;&#65292;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#19988;&#27604;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#35201;&#27714;&#26356;&#23569;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;&#20013;&#65292;&#29289;&#29702;&#27169;&#22411;&#21644;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#26159;&#20004;&#31181;&#20027;&#35201;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#27599;&#31181;&#26041;&#27861;&#37117;&#23384;&#22312;&#33258;&#24049;&#30340;&#25361;&#25112;&#65306;&#29289;&#29702;&#27169;&#22411;&#22312;&#21487;&#39044;&#27979;&#24615;&#26041;&#38754;&#19981;&#36275;&#65292;&#32780;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#21017;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#38024;&#23545;&#36825;&#20123;&#24050;&#30830;&#23450;&#30340;&#32570;&#28857;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21363;&#29289;&#29702;&#22686;&#24378;&#27531;&#24046;&#23398;&#20064;&#65288;PERL&#65289;&#27169;&#22411;&#12290;PERL&#23558;&#29289;&#29702;&#27169;&#22411;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;&#20248;&#21183;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#29992;&#20110;&#20132;&#36890;&#29366;&#24577;&#39044;&#27979;&#12290;PERL&#21253;&#25324;&#19968;&#20010;&#29289;&#29702;&#27169;&#22411;&#21644;&#19968;&#20010;&#27531;&#24046;&#23398;&#20064;&#27169;&#22411;&#12290;&#23427;&#30340;&#39044;&#27979;&#32467;&#26524;&#26159;&#29289;&#29702;&#27169;&#22411;&#32467;&#26524;&#21644;&#39044;&#27979;&#27531;&#24046;&#30340;&#21644;&#20316;&#20026;&#23545;&#20854;&#30340;&#20462;&#27491;&#12290;PERL&#20445;&#30041;&#20102;&#29289;&#29702;&#27169;&#22411;&#22825;&#28982;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#30456;&#27604;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#20855;&#26377;&#36739;&#23567;&#30340;&#25968;&#25454;&#38656;&#27714;&#12290;&#25105;&#20204;&#20351;&#29992;&#23454;&#38469;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;PERL&#27169;&#22411;&#65292;&#20854;&#20013;&#20197;&#26234;&#33021;&#39550;&#39542;&#27169;&#22411;&#65288;IDM&#65289;&#20316;&#20026;&#20854;&#29289;&#29702;&#36710;&#36319;&#27169;&#22411;&#65292;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#27169;&#22411;&#29992;&#20110;&#23398;&#20064;&#27531;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In vehicle trajectory prediction, physics models and data-driven models are two predominant methodologies. However, each approach presents its own set of challenges: physics models fall short in predictability, while data-driven models lack interpretability. Addressing these identified shortcomings, this paper proposes a novel framework, the Physics-Enhanced Residual Learning (PERL) model. PERL integrates the strengths of physics-based and data-driven methods for traffic state prediction. PERL contains a physics model and a residual learning model. Its prediction is the sum of the physics model result and a predicted residual as a correction to it. It preserves the interpretability inherent to physics-based models and has reduced data requirements compared to data-driven methods. Experiments were conducted using a real-world vehicle trajectory dataset. We proposed a PERL model, with the Intelligent Driver Model (IDM) as its physics car-following model and Long Short-Term Memory (LSTM) 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#35199;&#29677;&#29273;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#21508;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;BART&#12289;T5&#21644;BERT2BERT-style&#27169;&#22411;&#30340;&#35199;&#29677;&#29273;&#29256;&#26412;&#12290;</title><link>http://arxiv.org/abs/2309.11259</link><description>&lt;p&gt;
&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#35199;&#29677;&#29273;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Sequence-to-Sequence Spanish Pre-trained Language Models. (arXiv:2309.11259v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11259
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#35199;&#29677;&#29273;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#21508;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;BART&#12289;T5&#21644;BERT2BERT-style&#27169;&#22411;&#30340;&#35199;&#29677;&#29273;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#22823;&#36827;&#23637;&#20026;&#35768;&#22810;&#38750;&#33521;&#35821;&#35821;&#35328;&#29256;&#26412;&#30340;&#24320;&#21457;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#20854;&#20013;&#29305;&#21035;&#20851;&#27880;&#20102;&#20165;&#32534;&#30721;&#22120;&#21644;&#20165;&#35299;&#30721;&#22120;&#30340;&#26550;&#26500;&#12290;&#34429;&#28982;&#35199;&#29677;&#29273;&#35821;&#35821;&#35328;&#27169;&#22411;&#21253;&#25324;BERT&#12289;RoBERTa&#21644;GPT&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#20248;&#21183;&#65292;&#20294;&#22312;&#28041;&#21450;&#36755;&#20837;&#36755;&#20986;&#23545;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#20013;&#65292;&#32570;&#20047;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#23454;&#26045;&#21644;&#35780;&#20272;&#33879;&#21517;&#30340;&#20165;&#22312;&#35199;&#29677;&#29273;&#35821;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#24320;&#21019;&#20102;&#26032;&#30340;&#39046;&#22495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BART&#12289;T5&#21644;BERT2BERT&#39118;&#26684;&#27169;&#22411;&#30340;&#35199;&#29677;&#29273;&#35821;&#29256;&#26412;&#65292;&#24182;&#23545;&#23427;&#20204;&#22312;&#21508;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21253;&#25324;&#25688;&#35201;&#12289;&#37325;&#36848;&#21644;&#29983;&#25104;&#24335;&#38382;&#31572;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#25152;&#26377;&#27169;&#22411;&#30340;&#31454;&#20105;&#24615;&#33021;&#65292;&#20854;&#20013;BART&#21644;T5&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, substantial advancements in pre-trained language models have paved the way for the development of numerous non-English language versions, with a particular focus on encoder-only and decoder-only architectures. While Spanish language models encompassing BERT, RoBERTa, and GPT have exhibited prowess in natural language understanding and generation, there remains a scarcity of encoder-decoder models designed for sequence-to-sequence tasks involving input-output pairs. This paper breaks new ground by introducing the implementation and evaluation of renowned encoder-decoder architectures, exclusively pre-trained on Spanish corpora. Specifically, we present Spanish versions of BART, T5, and BERT2BERT-style models and subject them to a comprehensive assessment across a diverse range of sequence-to-sequence tasks, spanning summarization, rephrasing, and generative question answering. Our findings underscore the competitive performance of all models, with BART and T5 emerging a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#29615;&#22659;&#19979;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#20219;&#21153;&#22270;&#31163;&#36733;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#24448;&#24448;&#26080;&#27861;&#36866;&#24212;&#29615;&#22659;&#21464;&#21270;&#65292;&#23548;&#33268;&#29992;&#25143;&#20307;&#39564;&#19979;&#38477;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20219;&#21153;&#22270;&#35843;&#24230;&#24314;&#27169;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#20197;&#36866;&#24212;&#35745;&#31639;&#33021;&#21147;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2309.10569</link><description>&lt;p&gt;
&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#20013;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20219;&#21153;&#22270;&#31163;&#36733;
&lt;/p&gt;
&lt;p&gt;
Task Graph offloading via Deep Reinforcement Learning in Mobile Edge Computing. (arXiv:2309.10569v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#29615;&#22659;&#19979;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#20219;&#21153;&#22270;&#31163;&#36733;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#24448;&#24448;&#26080;&#27861;&#36866;&#24212;&#29615;&#22659;&#21464;&#21270;&#65292;&#23548;&#33268;&#29992;&#25143;&#20307;&#39564;&#19979;&#38477;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20219;&#21153;&#22270;&#35843;&#24230;&#24314;&#27169;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#20197;&#36866;&#24212;&#35745;&#31639;&#33021;&#21147;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#20854;&#20013;&#21253;&#21547;&#30340;&#20381;&#36182;&#20219;&#21153;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#36825;&#20123;&#24212;&#29992;&#31243;&#24207;&#36890;&#24120;&#20855;&#26377;&#20302;&#24310;&#36831;&#35201;&#27714;&#65292;&#20174;&#32780;&#23548;&#33268;&#23545;&#35745;&#31639;&#36164;&#28304;&#30340;&#38656;&#27714;&#24613;&#21095;&#22686;&#21152;&#12290;&#38543;&#30528;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#65288;MEC&#65289;&#30340;&#20986;&#29616;&#65292;&#23558;&#24212;&#29992;&#31243;&#24207;&#20219;&#21153;&#21368;&#36733;&#21040;&#37096;&#32626;&#22312;&#31227;&#21160;&#32593;&#32476;&#36793;&#32536;&#30340;&#23567;&#22411;&#35774;&#22791;&#19978;&#20197;&#33719;&#24471;&#39640;&#36136;&#37327;&#29992;&#25143;&#20307;&#39564;&#25104;&#20026;&#26368;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;MEC&#29615;&#22659;&#26159;&#21160;&#24577;&#30340;&#65292;&#22823;&#22810;&#25968;&#20381;&#36182;&#19987;&#23478;&#30693;&#35782;&#25110;&#20934;&#30830;&#30340;&#20998;&#26512;&#27169;&#22411;&#30340;&#29616;&#26377;&#24037;&#20316;&#22312;&#20219;&#21153;&#22270;&#31163;&#36733;&#26041;&#38754;&#26080;&#27861;&#23436;&#20840;&#36866;&#24212;&#36825;&#31181;&#29615;&#22659;&#21464;&#21270;&#65292;&#23548;&#33268;&#29992;&#25143;&#20307;&#39564;&#38477;&#20302;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;MEC&#20013;&#30340;&#20219;&#21153;&#22270;&#31163;&#36733;&#65292;&#32771;&#34385;&#21040;&#36793;&#32536;&#35745;&#31639;&#35774;&#22791;&#30340;&#35745;&#31639;&#33021;&#21147;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#20026;&#20102;&#36866;&#24212;&#29615;&#22659;&#21464;&#21270;&#65292;&#25105;&#20204;&#23558;&#35745;&#31639;&#31163;&#36733;&#30340;&#20219;&#21153;&#22270;&#35843;&#24230;&#24314;&#27169;&#20026;&#19968;&#20010;Markov&#20915;&#31574;&#36807;&#31243;&#65288;Markov Decision Process&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various mobile applications that comprise dependent tasks are gaining widespread popularity and are increasingly complex. These applications often have low-latency requirements, resulting in a significant surge in demand for computing resources. With the emergence of mobile edge computing (MEC), it becomes the most significant issue to offload the application tasks onto small-scale devices deployed at the edge of the mobile network for obtaining a high-quality user experience. However, since the environment of MEC is dynamic, most existing works focusing on task graph offloading, which rely heavily on expert knowledge or accurate analytical models, fail to fully adapt to such environmental changes, resulting in the reduction of user experience. This paper investigates the task graph offloading in MEC, considering the time-varying computation capabilities of edge computing devices. To adapt to environmental changes, we model the task graph scheduling for computation offloading as a Mark
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#27169;&#24577;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#27599;&#20010;&#27169;&#24577;&#22312;&#26679;&#26412;&#32423;&#21035;&#30340;&#36129;&#29486;&#65292;&#24182;&#21457;&#29616;&#22810;&#27169;&#24577;&#27169;&#22411;&#20542;&#21521;&#20110;&#20381;&#36182;&#19968;&#20010;&#29305;&#23450;&#30340;&#27169;&#24577;&#65292;&#23548;&#33268;&#20854;&#20182;&#27169;&#24577;&#30340;&#36129;&#29486;&#36739;&#20302;&#12290;</title><link>http://arxiv.org/abs/2309.06255</link><description>&lt;p&gt;
&#36890;&#36807;&#31934;&#32454;&#30340;&#27169;&#24577;&#35780;&#20272;&#22686;&#24378;&#22810;&#27169;&#24577;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Enhancing Multi-modal Cooperation via Fine-grained Modality Valuation. (arXiv:2309.06255v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#27169;&#24577;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#27599;&#20010;&#27169;&#24577;&#22312;&#26679;&#26412;&#32423;&#21035;&#30340;&#36129;&#29486;&#65292;&#24182;&#21457;&#29616;&#22810;&#27169;&#24577;&#27169;&#22411;&#20542;&#21521;&#20110;&#20381;&#36182;&#19968;&#20010;&#29305;&#23450;&#30340;&#27169;&#24577;&#65292;&#23548;&#33268;&#20854;&#20182;&#27169;&#24577;&#30340;&#36129;&#29486;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#22914;&#20309;&#23558;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#24322;&#36136;&#20449;&#24687;&#20849;&#21516;&#32467;&#21512;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#22312;&#22810;&#27169;&#24577;&#21327;&#20316;&#26041;&#38754;&#24120;&#24120;&#23384;&#22312;&#19981;&#23613;&#20154;&#24847;&#30340;&#38382;&#39064;&#65292;&#19981;&#33021;&#24456;&#22909;&#22320;&#20849;&#21516;&#21033;&#29992;&#25152;&#26377;&#27169;&#24577;&#12290;&#19968;&#20123;&#26041;&#27861;&#34987;&#25552;&#20986;&#26469;&#35782;&#21035;&#21644;&#22686;&#24378;&#23398;&#20064;&#25928;&#26524;&#36739;&#24046;&#30340;&#27169;&#24577;&#65292;&#20294;&#24448;&#24448;&#38590;&#20197;&#22312;&#29702;&#35770;&#19978;&#25552;&#20379;&#23545;&#26679;&#26412;&#32423;&#21035;&#22810;&#27169;&#24577;&#21327;&#20316;&#30340;&#32454;&#31890;&#24230;&#35266;&#23519;&#21644;&#25903;&#25345;&#12290;&#22240;&#27492;&#65292;&#21512;&#29702;&#35266;&#23519;&#21644;&#25913;&#36827;&#27169;&#24577;&#20043;&#38388;&#32454;&#31890;&#24230;&#30340;&#21327;&#20316;&#23588;&#20026;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#38754;&#23545;&#27169;&#24577;&#24046;&#24322;&#22312;&#19981;&#21516;&#26679;&#26412;&#20043;&#38388;&#21487;&#33021;&#21464;&#21270;&#30340;&#23454;&#38469;&#22330;&#26223;&#26102;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#27169;&#24577;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#35780;&#20272;&#27599;&#20010;&#27169;&#24577;&#22312;&#26679;&#26412;&#32423;&#21035;&#30340;&#36129;&#29486;&#12290;&#36890;&#36807;&#27169;&#24577;&#35780;&#20272;&#65292;&#25105;&#20204;&#36951;&#25022;&#22320;&#21457;&#29616;&#22810;&#27169;&#24577;&#27169;&#22411;&#20542;&#21521;&#20110;&#20381;&#36182;&#19968;&#20010;&#29305;&#23450;&#30340;&#27169;&#24577;&#65292;&#23548;&#33268;&#20854;&#20182;&#27169;&#24577;&#30340;&#36129;&#29486;&#36739;&#20302;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
One primary topic of multi-modal learning is to jointly incorporate heterogeneous information from different modalities. However, most models often suffer from unsatisfactory multi-modal cooperation, which could not jointly utilize all modalities well. Some methods are proposed to identify and enhance the worse learnt modality, but are often hard to provide the fine-grained observation of multi-modal cooperation at sample-level with theoretical support. Hence, it is essential to reasonably observe and improve the fine-grained cooperation between modalities, especially when facing realistic scenarios where the modality discrepancy could vary across different samples. To this end, we introduce a fine-grained modality valuation metric to evaluate the contribution of each modality at sample-level. Via modality valuation, we regretfully observe that the multi-modal model tends to rely on one specific modality, resulting in other modalities being low-contributing. We further analyze this iss
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#20998;&#23618;&#30340;&#24369;&#20559;&#22909;&#21453;&#39304;&#36827;&#34892;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#19982;&#20154;&#31867;&#20559;&#22909;&#38750;&#24120;&#19968;&#33268;&#30340;&#22797;&#26434;&#22870;&#21169;&#21487;&#20197;&#24110;&#21161;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26085;&#30410;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.02632</link><description>&lt;p&gt;
&#20174;&#20998;&#23618;&#30340;&#24369;&#20559;&#22909;&#21453;&#39304;&#20013;&#36827;&#34892;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning from Hierarchical Weak Preference Feedback. (arXiv:2309.02632v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#20998;&#23618;&#30340;&#24369;&#20559;&#22909;&#21453;&#39304;&#36827;&#34892;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#19982;&#20154;&#31867;&#20559;&#22909;&#38750;&#24120;&#19968;&#33268;&#30340;&#22797;&#26434;&#22870;&#21169;&#21487;&#20197;&#24110;&#21161;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26085;&#30410;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22870;&#21169;&#35774;&#35745;&#26159;&#23454;&#38469;&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#20010;&#22522;&#26412;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26041;&#38754;&#12290;&#23545;&#20110;&#31616;&#21333;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#25163;&#24037;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#65292;&#20363;&#22914;&#20351;&#29992;&#33509;&#24178;&#20010;&#22870;&#21169;&#22240;&#23376;&#30340;&#32447;&#24615;&#32452;&#21512;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22870;&#21169;&#24037;&#31243;&#21463;&#21040;&#36817;&#20284;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35843;&#20248;&#25104;&#26412;&#65292;&#24182;&#19988;&#36890;&#24120;&#26080;&#27861;&#25552;&#20379;&#22797;&#26434;&#20219;&#21153;&#25152;&#38656;&#30340;&#32454;&#31890;&#24230;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#20123;&#22256;&#38590;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#36716;&#21521;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#20174;&#36712;&#36857;&#24207;&#21015;&#23545;&#20043;&#38388;&#30340;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#20559;&#22909;&#30340;&#22870;&#21169;&#24314;&#27169;&#65292;RLHF&#23398;&#20064;&#21040;&#19982;&#20154;&#31867;&#20559;&#22909;&#38750;&#24120;&#19968;&#33268;&#30340;&#22797;&#26434;&#22870;&#21169;&#65292;&#20351;&#24471;&#24378;&#21270;&#23398;&#20064;&#33021;&#22815;&#35299;&#20915;&#26085;&#30410;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;RLHF&#30340;&#36866;&#29992;&#24615;&#21463;&#21040;&#33719;&#24471;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#30340;&#39640;&#25104;&#26412;&#21644;&#22256;&#38590;&#30340;&#38480;&#21046;&#12290;&#37492;&#20110;&#36825;&#20010;&#25104;&#26412;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#20351;&#29992;&#26356;&#23569;&#20154;&#21147;&#25237;&#20837;&#30340;&#26041;&#24335;&#26469;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reward design is a fundamental, yet challenging aspect of practical reinforcement learning (RL). For simple tasks, researchers typically handcraft the reward function, e.g., using a linear combination of several reward factors. However, such reward engineering is subject to approximation bias, incurs large tuning cost, and often cannot provide the granularity required for complex tasks. To avoid these difficulties, researchers have turned to reinforcement learning from human feedback (RLHF), which learns a reward function from human preferences between pairs of trajectory sequences. By leveraging preference-based reward modeling, RLHF learns complex rewards that are well aligned with human preferences, allowing RL to tackle increasingly difficult problems. Unfortunately, the applicability of RLHF is limited due to the high cost and difficulty of obtaining human preference data. In light of this cost, we investigate learning reward functions for complex tasks with less human effort; sim
&lt;/p&gt;</description></item><item><title>TensorBank&#26159;&#19968;&#20010;&#22522;&#20110;Tensor&#30340;&#28246;&#20179;&#24211;&#65292;&#33021;&#22815;&#20197;&#39640;&#36895;&#20174;&#20113;&#23545;&#35937;&#23384;&#20648;&#27969;&#24335;&#20256;&#36755;&#24352;&#37327;&#21040;GPU&#20869;&#23384;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#20998;&#23618;&#32479;&#35745;&#25351;&#26631;&#36827;&#34892;&#26597;&#35810;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2309.02094</link><description>&lt;p&gt;
TensorBank: &#22522;&#20110;Tensor&#30340;&#28246;&#20179;&#24211;&#29992;&#20110;&#22522;&#30784;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
TensorBank:Tensor Lakehouse for Foundation Model Training. (arXiv:2309.02094v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02094
&lt;/p&gt;
&lt;p&gt;
TensorBank&#26159;&#19968;&#20010;&#22522;&#20110;Tensor&#30340;&#28246;&#20179;&#24211;&#65292;&#33021;&#22815;&#20197;&#39640;&#36895;&#20174;&#20113;&#23545;&#35937;&#23384;&#20648;&#27969;&#24335;&#20256;&#36755;&#24352;&#37327;&#21040;GPU&#20869;&#23384;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#20998;&#23618;&#32479;&#35745;&#25351;&#26631;&#36827;&#34892;&#26597;&#35810;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#20043;&#22806;&#30340;&#39046;&#22495;&#30340;&#20852;&#36215;&#65292;&#23384;&#20648;&#21644;&#27969;&#24335;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#25104;&#20026;&#22522;&#30784;&#27169;&#22411;&#35757;&#32451;&#30340;&#20851;&#38190;&#38656;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TensorBank&#65292;&#19968;&#20010;&#33021;&#22815;&#22522;&#20110;&#22797;&#26434;&#20851;&#31995;&#26597;&#35810;&#20174;&#20113;&#23545;&#35937;&#23384;&#20648;&#65288;COS&#65289;&#27969;&#24335;&#20256;&#36755;&#24352;&#37327;&#21040;GPU&#20869;&#23384;&#30340;&#30334;&#20159;&#32423;&#24352;&#37327;&#28246;&#20179;&#24211;&#12290;&#25105;&#20204;&#20351;&#29992;&#20998;&#23618;&#32479;&#35745;&#25351;&#26631;&#65288;HSI&#65289;&#26469;&#21152;&#36895;&#26597;&#35810;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#20801;&#35768;&#20351;&#29992;HTTP&#33539;&#22260;&#35835;&#21462;&#26469;&#30452;&#25509;&#35775;&#38382;&#22359;&#32423;&#21035;&#30340;&#24352;&#37327;&#12290;&#19968;&#26086;&#22312;GPU&#20869;&#23384;&#20013;&#65292;&#25968;&#25454;&#21487;&#20197;&#20351;&#29992;PyTorch&#36716;&#25442;&#36827;&#34892;&#36716;&#25442;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;PyTorch&#25968;&#25454;&#38598;&#31867;&#22411;&#65292;&#37197;&#26377;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#24037;&#21378;&#65292;&#29992;&#20110;&#23558;&#20851;&#31995;&#26597;&#35810;&#21644;&#35831;&#27714;&#30340;&#36716;&#25442;&#20316;&#20026;&#19968;&#20010;&#23454;&#20363;&#36827;&#34892;&#32763;&#35793;&#12290;&#36890;&#36807;&#20351;&#29992;HSI&#65292;&#21487;&#20197;&#36339;&#36807;&#19981;&#30456;&#20851;&#30340;&#22359;&#65292;&#32780;&#26080;&#38656;&#35835;&#21462;&#23427;&#20204;&#65292;&#22240;&#20026;&#36825;&#20123;&#32034;&#24341;&#21253;&#21547;&#19981;&#21516;&#23618;&#27425;&#20998;&#36776;&#29575;&#32423;&#21035;&#19978;&#20869;&#23481;&#30340;&#32479;&#35745;&#20449;&#24687;&#12290;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#24320;&#25918;&#26631;&#20934;&#30340;&#26377;&#20027;&#35266;&#35266;&#28857;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Storing and streaming high dimensional data for foundation model training became a critical requirement with the rise of foundation models beyond natural language. In this paper we introduce TensorBank, a petabyte scale tensor lakehouse capable of streaming tensors from Cloud Object Store (COS) to GPU memory at wire speed based on complex relational queries. We use Hierarchical Statistical Indices (HSI) for query acceleration. Our architecture allows to directly address tensors on block level using HTTP range reads. Once in GPU memory, data can be transformed using PyTorch transforms. We provide a generic PyTorch dataset type with a corresponding dataset factory translating relational queries and requested transformations as an instance. By making use of the HSI, irrelevant blocks can be skipped without reading them as those indices contain statistics on their content at different hierarchical resolution levels. This is an opinionated architecture powered by open standards and making h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#20851;&#20110;&#35780;&#20272;LLM&#21487;&#20449;&#24230;&#30340;&#32508;&#21512;&#35843;&#26597;&#65292;&#24182;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;&#35843;&#26597;&#28085;&#30422;&#20102;&#21487;&#38752;&#24615;&#12289;&#23433;&#20840;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#25269;&#25239;&#28389;&#29992;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;&#12289;&#36981;&#23432;&#31038;&#20250;&#35268;&#33539;&#20197;&#21450;&#40065;&#26834;&#24615;&#31561;&#19971;&#20010;&#20027;&#35201;&#31867;&#21035;&#65292;&#20849;&#35745;29&#20010;&#23376;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2308.05374</link><description>&lt;p&gt;
&#21487;&#20449;&#30340;LLMs&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#35843;&#26597;&#21644;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment. (arXiv:2308.05374v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#20851;&#20110;&#35780;&#20272;LLM&#21487;&#20449;&#24230;&#30340;&#32508;&#21512;&#35843;&#26597;&#65292;&#24182;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;&#35843;&#26597;&#28085;&#30422;&#20102;&#21487;&#38752;&#24615;&#12289;&#23433;&#20840;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#25269;&#25239;&#28389;&#29992;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;&#12289;&#36981;&#23432;&#31038;&#20250;&#35268;&#33539;&#20197;&#21450;&#40065;&#26834;&#24615;&#31561;&#19971;&#20010;&#20027;&#35201;&#31867;&#21035;&#65292;&#20849;&#35745;29&#20010;&#23376;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20043;&#21069;&#65292;&#30830;&#20445;&#23545;&#40784;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20174;&#19994;&#32773;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#32570;&#20047;&#26126;&#30830;&#30340;&#25351;&#23548;&#26469;&#35780;&#20272;LLMs&#30340;&#36755;&#20986;&#26159;&#21542;&#31526;&#21512;&#31038;&#20250;&#35268;&#33539;&#12289;&#20215;&#20540;&#35266;&#21644;&#27861;&#35268;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35843;&#26597;&#65292;&#28085;&#30422;&#20102;&#35780;&#20272;LLM&#21487;&#20449;&#24230;&#26102;&#24517;&#39035;&#32771;&#34385;&#30340;&#20851;&#38190;&#32500;&#24230;&#12290;&#35843;&#26597;&#28085;&#30422;&#20102;LLM&#21487;&#20449;&#24230;&#30340;&#19971;&#20010;&#20027;&#35201;&#31867;&#21035;&#65306;&#21487;&#38752;&#24615;&#12289;&#23433;&#20840;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#25269;&#25239;&#28389;&#29992;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;&#12289;&#36981;&#23432;&#31038;&#20250;&#35268;&#33539;&#20197;&#21450;&#40065;&#26834;&#24615;&#12290;&#27599;&#20010;&#20027;&#35201;&#31867;&#21035;&#36827;&#19968;&#27493;&#32454;&#20998;&#20026;&#33509;&#24178;&#23376;&#31867;&#21035;&#65292;&#20849;&#35745;29&#20010;&#23376;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensuring alignment, which refers to making models behave in accordance with human intentions [1,2], has become a critical task before deploying large language models (LLMs) in real-world applications. For instance, OpenAI devoted six months to iteratively aligning GPT-4 before its release [3]. However, a major challenge faced by practitioners is the lack of clear guidance on evaluating whether LLM outputs align with social norms, values, and regulations. This obstacle hinders systematic iteration and deployment of LLMs. To address this issue, this paper presents a comprehensive survey of key dimensions that are crucial to consider when assessing LLM trustworthiness. The survey covers seven major categories of LLM trustworthiness: reliability, safety, fairness, resistance to misuse, explainability and reasoning, adherence to social norms, and robustness. Each major category is further divided into several sub-categories, resulting in a total of 29 sub-categories. Additionally, a subset 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#30452;&#25509;&#31574;&#30053;&#25628;&#32034;&#30340;&#26089;&#20572;&#27490;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#30340;&#30446;&#26631;&#20540;&#26469;&#20915;&#23450;&#26159;&#21542;&#20572;&#27490;&#35780;&#20272;&#65292;&#32780;&#26080;&#38656;&#38382;&#39064;&#29305;&#23450;&#30340;&#30693;&#35782;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;&#35813;&#26041;&#27861;&#22312;&#28216;&#25103;&#12289;&#26426;&#22120;&#20154;&#21644;&#32463;&#20856;&#25511;&#21046;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#33410;&#30465;&#35745;&#31639;&#26102;&#38388;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.03574</link><description>&lt;p&gt;
&#28436;&#21270;&#30452;&#25509;&#31574;&#30053;&#25628;&#32034;&#20013;&#30340;&#24191;&#20041;&#26089;&#20572;&#27490;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Generalized Early Stopping in Evolutionary Direct Policy Search. (arXiv:2308.03574v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#30452;&#25509;&#31574;&#30053;&#25628;&#32034;&#30340;&#26089;&#20572;&#27490;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#30340;&#30446;&#26631;&#20540;&#26469;&#20915;&#23450;&#26159;&#21542;&#20572;&#27490;&#35780;&#20272;&#65292;&#32780;&#26080;&#38656;&#38382;&#39064;&#29305;&#23450;&#30340;&#30693;&#35782;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;&#35813;&#26041;&#27861;&#22312;&#28216;&#25103;&#12289;&#26426;&#22120;&#20154;&#21644;&#32463;&#20856;&#25511;&#21046;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#33410;&#30465;&#35745;&#31639;&#26102;&#38388;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#20248;&#21270;&#38382;&#39064;&#20013;&#65292;&#23588;&#20854;&#26159;&#28041;&#21450;&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#36827;&#34892;&#35780;&#20272;&#30340;&#30452;&#25509;&#31574;&#30053;&#25628;&#32034;&#20219;&#21153;&#20013;&#65292;&#35780;&#20272;&#26102;&#38388;&#36890;&#24120;&#36739;&#38271;&#12290;&#24403;&#22312;&#22266;&#23450;&#26102;&#38388;&#27573;&#20869;&#35780;&#20272;&#35299;&#20915;&#26041;&#26696;&#26102;&#65292;&#24448;&#24448;&#20250;&#26126;&#30830;&#26080;&#27861;&#36890;&#36807;&#22686;&#21152;&#35745;&#31639;&#26102;&#38388;&#26469;&#25552;&#39640;&#30446;&#26631;&#20540;&#65288;&#20363;&#22914;&#65292;&#24403;&#20004;&#36718;&#26426;&#22120;&#20154;&#25345;&#32493;&#22312;&#21407;&#22320;&#26059;&#36716;&#26102;&#65289;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21450;&#26089;&#20572;&#27490;&#35780;&#20272;&#20197;&#33410;&#30465;&#35745;&#31639;&#26102;&#38388;&#26159;&#26377;&#24847;&#20041;&#30340;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#35780;&#20272;&#20572;&#27490;&#26041;&#27861;&#37117;&#26159;&#38382;&#39064;&#29305;&#23450;&#30340;&#65292;&#24182;&#19988;&#38656;&#35201;&#19987;&#38376;&#20026;&#24403;&#21069;&#20219;&#21153;&#35774;&#35745;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#31574;&#30053;&#25628;&#32034;&#30340;&#26089;&#20572;&#27490;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21482;&#26597;&#30475;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#30340;&#30446;&#26631;&#20540;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#38382;&#39064;&#29305;&#23450;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#26469;&#33258;&#28216;&#25103;&#12289;&#26426;&#22120;&#20154;&#21644;&#32463;&#20856;&#25511;&#21046;&#39046;&#22495;&#30340;&#30452;&#25509;&#31574;&#30053;&#25628;&#32034;&#29615;&#22659;&#20013;&#27979;&#35797;&#20102;&#24341;&#20837;&#30340;&#20572;&#27490;&#20934;&#21017;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#33410;&#30465;&#20102;&#35745;&#31639;&#26102;&#38388;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lengthy evaluation times are common in many optimization problems such as direct policy search tasks, especially when they involve conducting evaluations in the physical world, e.g. in robotics applications. Often, when evaluating a solution over a fixed time period, it becomes clear that the objective value will not increase with additional computation time (for example, when a two-wheeled robot continuously spins on the spot). In such cases, it makes sense to stop the evaluation early to save computation time. However, most approaches to stop the evaluation are problem-specific and need to be specifically designed for the task at hand. Therefore, we propose an early stopping method for direct policy search. The proposed method only looks at the objective value at each time step and requires no problem-specific knowledge.  We test the introduced stopping criterion in five direct policy search environments drawn from games, robotics, and classic control domains, and show that it can sa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#27425;&#24615;&#20986;&#20215;&#25293;&#21334;&#20013;&#36879;&#26126;&#24230;&#23545;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#23545;&#25293;&#21334;&#36879;&#26126;&#24230;&#19982;&#29615;&#22659;&#29305;&#24615;&#30340;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#22312;&#25293;&#21334;&#20013;&#23398;&#20064;&#26368;&#20248;&#20986;&#20215;&#30340;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.09478</link><description>&lt;p&gt;
&#36879;&#26126;&#24230;&#22312;&#26410;&#30693;&#20272;&#20540;&#30340;&#37325;&#22797;&#19968;&#27425;&#24615;&#20986;&#20215;&#25293;&#21334;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Role of Transparency in Repeated First-Price Auctions with Unknown Valuations. (arXiv:2307.09478v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#27425;&#24615;&#20986;&#20215;&#25293;&#21334;&#20013;&#36879;&#26126;&#24230;&#23545;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#23545;&#25293;&#21334;&#36879;&#26126;&#24230;&#19982;&#29615;&#22659;&#29305;&#24615;&#30340;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#22312;&#25293;&#21334;&#20013;&#23398;&#20064;&#26368;&#20248;&#20986;&#20215;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19968;&#31995;&#21015;&#19968;&#27425;&#24615;&#20986;&#20215;&#25293;&#21334;&#20013;&#65292;&#21333;&#20010;&#31454;&#26631;&#20154;&#22312;&#21482;&#26377;&#22312;&#36194;&#24471;&#25293;&#21334;&#26102;&#25165;&#30693;&#36947;&#29289;&#21697;&#20215;&#20540;&#30340;&#24773;&#20917;&#19979;&#65292;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#23436;&#25972;&#22320;&#21051;&#30011;&#20102;&#25293;&#21334;&#30340;&#36879;&#26126;&#24230;&#23545;&#26368;&#23567;&#21270;&#24863;&#21040;&#36951;&#25022;&#30340;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#36879;&#26126;&#24230;&#35843;&#25511;&#20102;&#25293;&#21334;&#24072;&#22312;&#27599;&#27425;&#25293;&#21334;&#32467;&#26463;&#26102;&#20844;&#24320;&#31454;&#20105;&#20986;&#20215;&#30340;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#19981;&#21516;&#20551;&#35774;&#65288;&#38543;&#26426;&#30340;&#12289;&#23545;&#25239;&#24615;&#30340;&#21644;&#24179;&#28369;&#21464;&#20307;&#65289;&#29983;&#25104;&#31454;&#26631;&#20154;&#20272;&#20540;&#21644;&#31454;&#20105;&#20986;&#20215;&#30340;&#29615;&#22659;&#12290;&#36825;&#20123;&#26497;&#23567;&#26497;&#22823;&#29575;&#25581;&#31034;&#20102;&#36879;&#26126;&#24230;&#21644;&#29615;&#22659;&#24615;&#36136;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#21450;&#24433;&#21709;&#19968;&#20010;&#20154;&#23398;&#20064;&#22312;&#19968;&#27425;&#24615;&#20986;&#20215;&#25293;&#21334;&#20013;&#22914;&#20309;&#26368;&#20248;&#20986;&#20215;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of regret minimization for a single bidder in a sequence of first-price auctions where the bidder knows the item's value only if the auction is won. Our main contribution is a complete characterization, up to logarithmic factors, of the minimax regret in terms of the auction's transparency, which regulates the amount of information on competing bids disclosed by the auctioneer at the end of each auction. Our results hold under different assumptions (stochastic, adversarial, and their smoothed variants) on the environment generating the bidder's valuations and competing bids. These minimax rates reveal how the interplay between transparency and the nature of the environment affects how fast one can learn to bid optimally in first-price auctions.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#25968;&#25454;&#38598;QH9&#65292;&#29992;&#20110;&#20026;&#21508;&#31181;&#20998;&#23376;&#25552;&#20379;&#31934;&#30830;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;&#36890;&#36807;&#35774;&#35745;&#22522;&#20934;&#20219;&#21153;&#65292;&#23637;&#31034;&#20102;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26377;&#33021;&#21147;&#39044;&#27979;&#20219;&#24847;&#20998;&#23376;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;</title><link>http://arxiv.org/abs/2306.09549</link><description>&lt;p&gt;
QH9&#65306;QM9&#20998;&#23376;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#39044;&#27979;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
QH9: A Quantum Hamiltonian Prediction Benchmark for QM9 Molecules. (arXiv:2306.09549v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09549
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#25968;&#25454;&#38598;QH9&#65292;&#29992;&#20110;&#20026;&#21508;&#31181;&#20998;&#23376;&#25552;&#20379;&#31934;&#30830;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;&#36890;&#36807;&#35774;&#35745;&#22522;&#20934;&#20219;&#21153;&#65292;&#23637;&#31034;&#20102;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26377;&#33021;&#21147;&#39044;&#27979;&#20219;&#24847;&#20998;&#23376;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36234;&#26469;&#36234;&#34987;&#29992;&#20110;&#21152;&#36895;&#30005;&#23376;&#32467;&#26500;&#39044;&#27979;&#65292;&#20316;&#20026;&#31532;&#19968;&#24615;&#21407;&#29702;&#35745;&#31639;&#26041;&#27861;&#65288;&#22914;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#65288;DFT&#65289;&#65289;&#30340;&#26367;&#20195;&#21697;&#12290;&#34429;&#28982;&#35768;&#22810;&#37327;&#23376;&#21270;&#23398;&#25968;&#25454;&#38598;&#20391;&#37325;&#20110;&#21270;&#23398;&#24615;&#36136;&#21644;&#21407;&#23376;&#21147;&#65292;&#20294;&#20934;&#30830;&#19988;&#39640;&#25928;&#22320;&#39044;&#27979;&#21704;&#23494;&#39039;&#30697;&#38453;&#30340;&#33021;&#21147;&#26159;&#38750;&#24120;&#37325;&#35201;&#21644;&#22522;&#26412;&#30340;&#29289;&#29702;&#37327;&#65292;&#23427;&#30830;&#23450;&#20102;&#29289;&#29702;&#31995;&#32479;&#21644;&#21270;&#23398;&#24615;&#36136;&#30340;&#37327;&#23376;&#29366;&#24577;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#26032;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#25968;&#25454;&#38598;&#65292;&#21629;&#21517;&#20026;QH9&#65292;&#22522;&#20110;QM9&#25968;&#25454;&#38598;&#20026;2,399&#20010;&#20998;&#23376;&#21160;&#21147;&#23398;&#36712;&#36857;&#21644;130,831&#20010;&#31283;&#23450;&#20998;&#23376;&#20960;&#20309;&#24418;&#24577;&#25552;&#20379;&#31934;&#30830;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;&#36890;&#36807;&#35774;&#35745;&#21508;&#31181;&#20998;&#23376;&#30340;&#22522;&#20934;&#20219;&#21153;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26377;&#33021;&#21147;&#39044;&#27979;&#20219;&#24847;&#20998;&#23376;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;QH9&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27169;&#22411;&#37117;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised machine learning approaches have been increasingly used in accelerating electronic structure prediction as surrogates of first-principle computational methods, such as density functional theory (DFT). While numerous quantum chemistry datasets focus on chemical properties and atomic forces, the ability to achieve accurate and efficient prediction of the Hamiltonian matrix is highly desired, as it is the most important and fundamental physical quantity that determines the quantum states of physical systems and chemical properties. In this work, we generate a new Quantum Hamiltonian dataset, named as QH9, to provide precise Hamiltonian matrices for 2,399 molecular dynamics trajectories and 130,831 stable molecular geometries, based on the QM9 dataset. By designing benchmark tasks with various molecules, we show that current machine learning models have the capacity to predict Hamiltonian matrices for arbitrary molecules. Both the QH9 dataset and the baseline models are provided
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20250;&#21592;&#25512;&#26029;&#23041;&#32961;&#27169;&#22411;TMI&#65292;&#29992;&#20110;&#35780;&#20272;&#24494;&#35843;&#27169;&#22411;&#23545;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#27844;&#38706;&#65292;&#31361;&#26174;&#20102;&#22312;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#24182;&#38656;&#35201;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#36827;&#34892;&#26356;&#20005;&#26684;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.01181</link><description>&lt;p&gt;
&#36807;&#25311;&#21512;&#30340;&#27169;&#22411;&#20250;&#27844;&#38706;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#38544;&#31169;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
TMI! Finetuned Models Leak Private Information from their Pretraining Data. (arXiv:2306.01181v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20250;&#21592;&#25512;&#26029;&#23041;&#32961;&#27169;&#22411;TMI&#65292;&#29992;&#20110;&#35780;&#20272;&#24494;&#35843;&#27169;&#22411;&#23545;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#27844;&#38706;&#65292;&#31361;&#26174;&#20102;&#22312;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#24182;&#38656;&#35201;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#36827;&#34892;&#26356;&#20005;&#26684;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#36234;&#26469;&#36234;&#27969;&#34892;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#21033;&#29992;&#20026;&#19968;&#20010;&#20219;&#21153;&#35757;&#32451;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#21327;&#21161;&#26500;&#24314;&#30456;&#20851;&#20219;&#21153;&#30340;&#24494;&#35843;&#27169;&#22411;&#12290;&#35813;&#33539;&#20363;&#22312;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#26041;&#38754;&#23588;&#20854;&#21463;&#27426;&#36814;&#65292;&#20854;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#20844;&#24320;&#30340;&#65292;&#21482;&#26377;&#24494;&#35843;&#25968;&#25454;&#34987;&#35270;&#20026;&#25935;&#24863;&#30340;&#12290;&#28982;&#32780;&#65292;&#26377;&#29702;&#30001;&#35748;&#20026;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;&#25968;&#25454;&#20173;&#28982;&#26159;&#25935;&#24863;&#30340;&#65292;&#22240;&#27492;&#24517;&#39035;&#20102;&#35299;&#24494;&#35843;&#27169;&#22411;&#27844;&#38706;&#26377;&#20851;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#20449;&#24687;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20250;&#21592;&#25512;&#29702;&#23041;&#32961;&#27169;&#22411;&#65292;&#20854;&#20013;&#23545;&#25163;&#21482;&#33021;&#35775;&#38382;&#24050;&#32463;&#24494;&#35843;&#22909;&#30340;&#27169;&#22411;&#65292;&#24182;&#24819;&#25512;&#26029;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#21592;&#36164;&#26684;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#23041;&#32961;&#27169;&#22411;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#20803;&#20998;&#31867;&#22120;&#30340;&#25915;&#20987;TMI&#65292;&#23427;&#21033;&#29992;&#20102;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#35760;&#24518;&#30340;&#39044;&#35757;&#32451;&#26679;&#26412;&#23545;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;TMI&#65292;&#24182;&#34920;&#26126;&#23427;&#22312;&#20165;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#30340;&#25512;&#26029;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#21592;&#36164;&#26684;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#26174;&#20102;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#20197;&#21450;&#38656;&#35201;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#36827;&#34892;&#26356;&#20005;&#26684;&#30340;&#35780;&#20272;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning has become an increasingly popular technique in machine learning as a way to leverage a pretrained model trained for one task to assist with building a finetuned model for a related task. This paradigm has been especially popular for privacy in machine learning, where the pretrained model is considered public, and only the data for finetuning is considered sensitive. However, there are reasons to believe that the data used for pretraining is still sensitive, making it essential to understand how much information the finetuned model leaks about the pretraining data. In this work we propose a new membership-inference threat model where the adversary only has access to the finetuned model and would like to infer the membership of the pretraining data. To realize this threat model, we implement a novel metaclassifier-based attack, TMI, that leverages the influence of memorized pretraining samples on predictions in the downstream task. We evaluate TMI on both vision and na
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#20108;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#21508;&#31181;&#20551;&#35774;&#19979;&#36807;&#25311;&#21512;&#30340;&#31867;&#22411;&#20250;&#20174;&#19968;&#32500;&#25968;&#25454;&#30340;&#26497;&#31471;&#24773;&#20917;&#19979;&#32531;&#21644;&#21040;&#39640;&#32500;&#30340;&#33391;&#24615;&#65292;&#25581;&#31034;&#20102;&#36755;&#20837;&#32500;&#24230;&#22312;&#31070;&#32463;&#32593;&#32476;&#36807;&#25311;&#21512;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.15141</link><description>&lt;p&gt;
&#20174;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#32531;&#21644;&#36807;&#25311;&#21512;&#21040;&#33391;&#24615;&#36807;&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
From Tempered to Benign Overfitting in ReLU Neural Networks. (arXiv:2305.15141v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#20108;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#21508;&#31181;&#20551;&#35774;&#19979;&#36807;&#25311;&#21512;&#30340;&#31867;&#22411;&#20250;&#20174;&#19968;&#32500;&#25968;&#25454;&#30340;&#26497;&#31471;&#24773;&#20917;&#19979;&#32531;&#21644;&#21040;&#39640;&#32500;&#30340;&#33391;&#24615;&#65292;&#25581;&#31034;&#20102;&#36755;&#20837;&#32500;&#24230;&#22312;&#31070;&#32463;&#32593;&#32476;&#36807;&#25311;&#21512;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#34987;&#35266;&#23519;&#21040;&#21363;&#20351;&#35757;&#32451;&#27169;&#22411;&#26469;&#23436;&#32654;&#22320;&#36866;&#24212;&#22024;&#26434;&#30340;&#25968;&#25454;&#20063;&#33021;&#24456;&#22909;&#22320;&#25512;&#24191;&#12290;&#36825;&#19968;&#29616;&#35937;&#24341;&#21457;&#20102;&#22823;&#37327;&#20851;&#20110;&#8220;&#33391;&#24615;&#36807;&#25311;&#21512;&#8221;&#30340;&#24037;&#20316;&#65292;&#20854;&#20013;&#20869;&#25554;&#39044;&#27979;&#22120;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;&#26377;&#20154;&#29468;&#27979;&#24182;&#32463;&#39564;&#24615;&#22320;&#35266;&#23519;&#21040;&#31070;&#32463;&#32593;&#32476;&#30340;&#34892;&#20026;&#36890;&#24120;&#26356;&#22909;&#22320;&#25551;&#36848;&#20026;&#8220;&#32531;&#21644;&#36807;&#25311;&#21512;&#8221;&#65292;&#20854;&#20013;&#24615;&#33021;&#26082;&#38750;&#26368;&#20248;&#65292;&#20063;&#38750;&#24494;&#19981;&#36275;&#36947;&#65292;&#24182;&#38543;&#22122;&#22768;&#27700;&#24179;&#30340;&#21464;&#21270;&#32780;&#38477;&#20302;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#19968;&#20027;&#24352;&#23578;&#32570;&#20047;&#20851;&#20110;&#38750;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#29702;&#35770;&#30340;&#35777;&#26126;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20960;&#20010;&#32467;&#26524;&#65292;&#26088;&#22312;&#24357;&#21512;&#36825;&#20123;&#20114;&#34917;&#30340;&#35266;&#28857;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20998;&#31867;&#35774;&#32622;&#65292;&#20351;&#29992;&#20108;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#35777;&#26126;&#22312;&#21508;&#31181;&#20551;&#35774;&#19979;&#65292;&#36807;&#25311;&#21512;&#30340;&#31867;&#22411;&#20174;&#19968;&#32500;&#25968;&#25454;&#30340;&#26497;&#31471;&#24773;&#20917;&#19979;&#32531;&#21644;&#21040;&#39640;&#32500;&#30340;&#33391;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35777;&#26126;&#36755;&#20837;&#32500;&#24230;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#26377;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Overparameterized neural networks (NNs) are observed to generalize well even when trained to perfectly fit noisy data. This phenomenon motivated a large body of work on "benign overfitting", where interpolating predictors achieve near-optimal performance. Recently, it was conjectured and empirically observed that the behavior of NNs is often better described as "tempered overfitting", where the performance is non-optimal yet also non-trivial, and degrades as a function of the noise level. However, a theoretical justification of this claim for non-linear NNs has been lacking so far. In this work, we provide several results that aim at bridging these complementing views. We study a simple classification setting with 2-layer ReLU NNs, and prove that under various assumptions, the type of overfitting transitions from tempered in the extreme case of one-dimensional data, to benign in high dimensions. Thus, we show that the input dimension has a crucial role on the type of overfitting in thi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#23454;&#29616;&#20102;&#23545;&#24314;&#31569;&#29289;&#25104;&#26412;&#38477;&#20302;&#21644;&#33021;&#32791;&#24433;&#21709;&#22240;&#32032;&#30340;&#35782;&#21035;&#65292;&#32467;&#26524;&#34920;&#26126;&#38548;&#28909;&#12289;&#24314;&#31569;&#29289;&#23610;&#23544;&#21644;&#21046;&#20919;&#31995;&#32479;&#31867;&#22411;&#26159;&#24433;&#21709;&#33021;&#28304;&#28040;&#32791;&#21644;&#25104;&#26412;&#38477;&#20302;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2305.08886</link><description>&lt;p&gt;
&#21033;&#29992;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#35782;&#21035;&#24433;&#21709;&#24314;&#31569;&#29289;&#33021;&#32791;&#21644;&#25104;&#26412;&#38477;&#20302;&#30340;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Identification of the Factors Affecting the Reduction of Energy Consumption and Cost in Buildings Using Data Mining Techniques. (arXiv:2305.08886v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#23454;&#29616;&#20102;&#23545;&#24314;&#31569;&#29289;&#25104;&#26412;&#38477;&#20302;&#21644;&#33021;&#32791;&#24433;&#21709;&#22240;&#32032;&#30340;&#35782;&#21035;&#65292;&#32467;&#26524;&#34920;&#26126;&#38548;&#28909;&#12289;&#24314;&#31569;&#29289;&#23610;&#23544;&#21644;&#21046;&#20919;&#31995;&#32479;&#31867;&#22411;&#26159;&#24433;&#21709;&#33021;&#28304;&#28040;&#32791;&#21644;&#25104;&#26412;&#38477;&#20302;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#33021;&#32791;&#21644;&#21327;&#35843;&#20844;&#29992;&#20107;&#19994;&#31995;&#32479;&#19968;&#30452;&#26159;&#24314;&#31569;&#34892;&#19994;&#20851;&#27880;&#30340;&#28966;&#28857;&#12290;&#24314;&#31569;&#26159;&#19990;&#30028;&#19978;&#26368;&#22823;&#30340;&#33021;&#28304;&#28040;&#32791;&#32773;&#20043;&#19968;&#65292;&#20854;&#33021;&#25928;&#23545;&#20110;&#38450;&#27490;&#28010;&#36153;&#21644;&#38477;&#20302;&#25104;&#26412;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#24314;&#31569;&#29289;&#20135;&#29983;&#22823;&#37327;&#30340;&#21407;&#22987;&#25968;&#25454;&#65292;&#21487;&#20197;&#29992;&#20110;&#20102;&#35299;&#33021;&#28304;&#28040;&#32791;&#27169;&#24335;&#24182;&#21327;&#21161;&#24320;&#21457;&#20248;&#21270;&#31574;&#30053;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35782;&#21035;&#24433;&#21709;&#24314;&#31569;&#29289;&#25104;&#26412;&#38477;&#20302;&#21644;&#33021;&#32791;&#30340;&#22240;&#32032;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#21033;&#29992;&#19977;&#31181;&#22238;&#24402;&#27169;&#22411;&#65288;Lasso&#22238;&#24402;&#12289;&#20915;&#31574;&#26641;&#21644;&#38543;&#26426;&#26862;&#26519;&#65289;&#26469;&#39044;&#27979;&#24314;&#31569;&#29289;&#30340;&#20027;&#35201;&#29123;&#26009;&#20351;&#29992;&#12289;&#30005;&#33021;&#28040;&#32791;&#21644;&#25104;&#26412;&#33410;&#30465;&#12290;&#36827;&#34892;&#20102;&#19968;&#39033;&#24433;&#21709;&#33021;&#32791;&#21644;&#25104;&#26412;&#38477;&#20302;&#22240;&#32032;&#30340;&#20998;&#26512;&#65292;&#24182;&#20351;&#29992;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#20248;&#21270;&#20915;&#31574;&#26641;&#31639;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#20803;&#21551;&#21457;&#25216;&#26415;&#65292;&#25105;&#20204;&#23545;&#20915;&#31574;&#26641;&#31639;&#27861;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#27169;&#22411;&#31934;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24433;&#21709;&#24314;&#31569;&#29289;&#33021;&#32791;&#21644;&#25104;&#26412;&#38477;&#20302;&#30340;&#20851;&#38190;&#22240;&#32032;&#26159;&#38548;&#28909;&#12289;&#24314;&#31569;&#29289;&#23610;&#23544;&#21644;&#20351;&#29992;&#30340;&#21046;&#20919;&#31995;&#32479;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimizing energy consumption and coordination of utility systems have long been a concern of the building industry. Buildings are one of the largest energy consumers in the world, making their energy efficiency crucial for preventing waste and reducing costs. Additionally, buildings generate substantial amounts of raw data, which can be used to understand energy consumption patterns and assist in developing optimization strategies. Using a real-world dataset, this research aims to identify the factors that influence building cost reduction and energy consumption. To achieve this, we utilize three regression models (Lasso Regression, Decision Tree, and Random Forest) to predict primary fuel usage, electrical energy consumption, and cost savings in buildings. An analysis of the factors influencing energy consumption and cost reduction is conducted, and the decision tree algorithm is optimized using metaheuristics. By employing metaheuristic techniques, we fine-tune the decision tree alg
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23398;&#20064;&#28145;&#24230;&#21327;&#26041;&#24046;&#20989;&#25968;&#65292;&#24182;&#21033;&#29992;&#35813;&#26041;&#27861;&#23545;&#28145;&#24230;&#34917;&#20840;&#12289;&#25414;&#38598;&#35843;&#25972;&#21644;&#21333;&#30446;&#23494;&#38598;&#35270;&#35273;&#37324;&#31243;&#35745;&#31561;&#20960;&#20309;&#35270;&#35273;&#20219;&#21153;&#36827;&#34892;&#20102;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2303.12157</link><description>&lt;p&gt;
&#23398;&#20064;&#28145;&#24230;&#21327;&#26041;&#24046;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Learning a Depth Covariance Function. (arXiv:2303.12157v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12157
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23398;&#20064;&#28145;&#24230;&#21327;&#26041;&#24046;&#20989;&#25968;&#65292;&#24182;&#21033;&#29992;&#35813;&#26041;&#27861;&#23545;&#28145;&#24230;&#34917;&#20840;&#12289;&#25414;&#38598;&#35843;&#25972;&#21644;&#21333;&#30446;&#23494;&#38598;&#35270;&#35273;&#37324;&#31243;&#35745;&#31561;&#20960;&#20309;&#35270;&#35273;&#20219;&#21153;&#36827;&#34892;&#20102;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#28145;&#24230;&#21327;&#26041;&#24046;&#20989;&#25968;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20960;&#20309;&#35270;&#35273;&#20219;&#21153;&#12290;&#32473;&#23450;RGB&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#65292;&#21327;&#26041;&#24046;&#20989;&#25968;&#21487;&#28789;&#27963;&#22320;&#29992;&#20110;&#23450;&#20041;&#28145;&#24230;&#20989;&#25968;&#20808;&#39564;&#65292;&#32473;&#23450;&#35266;&#27979;&#30340;&#39044;&#27979;&#20998;&#24067;&#20197;&#21450;&#20027;&#21160;&#28857;&#36873;&#25321;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#25216;&#26415;&#26469;&#35299;&#20915;&#19968;&#31995;&#21015;&#19979;&#28216;&#20219;&#21153;&#65306;&#28145;&#24230;&#34917;&#20840;&#12289;&#25414;&#38598;&#35843;&#25972;&#21644;&#21333;&#30446;&#23494;&#38598;&#35270;&#35273;&#37324;&#31243;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose learning a depth covariance function with applications to geometric vision tasks. Given RGB images as input, the covariance function can be flexibly used to define priors over depth functions, predictive distributions given observations, and methods for active point selection. We leverage these techniques for a selection of downstream tasks: depth completion, bundle adjustment, and monocular dense visual odometry.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#36229;&#32423;&#30417;&#27979;&#30340;&#36828;&#31243;&#23454;&#29616;&#29492;&#30168;&#26089;&#26399;&#35786;&#26029;&#30340;&#31574;&#30053;&#12290;&#35813;&#31574;&#30053;&#20197;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20026;&#22522;&#30784;&#65292;&#21487;&#23454;&#29616;&#39640;&#28789;&#25935;&#24230;&#21644;&#20934;&#30830;&#24615;&#30340;&#30149;&#30151;&#20998;&#31867;&#65292;&#21516;&#26102;&#25104;&#26412;&#20302;&#12289;&#26131;&#29992;&#24615;&#39640;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2303.09780</link><description>&lt;p&gt;
Mpox-AISM&#65306;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36229;&#32423;&#30417;&#27979;&#20197;&#36943;&#21046;&#29492;&#30168;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Mpox-AISM: AI-Mediated Super Monitoring for Forestalling Monkeypox Spread. (arXiv:2303.09780v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#36229;&#32423;&#30417;&#27979;&#30340;&#36828;&#31243;&#23454;&#29616;&#29492;&#30168;&#26089;&#26399;&#35786;&#26029;&#30340;&#31574;&#30053;&#12290;&#35813;&#31574;&#30053;&#20197;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20026;&#22522;&#30784;&#65292;&#21487;&#23454;&#29616;&#39640;&#28789;&#25935;&#24230;&#21644;&#20934;&#30830;&#24615;&#30340;&#30149;&#30151;&#20998;&#31867;&#65292;&#21516;&#26102;&#25104;&#26412;&#20302;&#12289;&#26131;&#29992;&#24615;&#39640;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#21450;&#26102;&#12289;&#20415;&#25463;&#21644;&#20934;&#30830;&#35786;&#26029;&#26089;&#26399;&#24739;&#32773;&#26159;&#36943;&#21046;&#29492;&#30168;&#20256;&#25773;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36828;&#31243;&#12289;&#23454;&#26102;&#30340;&#22312;&#32447;&#21487;&#35270;&#21270;&#31574;&#30053;&#65292;&#31216;&#20026;&#8220;&#36229;&#32423;&#30417;&#27979;&#8221;&#65292;&#29992;&#20110;&#26500;&#24314;&#20302;&#25104;&#26412;&#12289;&#26041;&#20415;&#12289;&#21450;&#26102;&#21644;&#26080;&#19987;&#19994;&#30693;&#35782;&#30340;&#29492;&#30168;&#26089;&#26399;&#35786;&#26029;&#12290;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#12289;&#25968;&#25454;&#22686;&#24378;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#32452;&#35013;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#20171;&#23548;&#30340;&#8220;&#36229;&#32423;&#30417;&#27979;&#8221;&#65288;Mpox-AISM&#65289;&#65292;&#26681;&#25454;&#25968;&#25454;&#38598;&#29305;&#24449;&#21644;&#29492;&#30168;&#28436;&#21464;&#36235;&#21183;&#20197;&#21450;&#19982;&#39640;&#30456;&#20284;&#24230;&#30340;&#20854;&#20182;&#19971;&#31181;&#30382;&#32932;&#30149;&#30340;&#19987;&#19994;&#20998;&#31867;&#65292;&#22240;&#27492;&#36825;&#20123;&#21151;&#33021;&#19982;&#21512;&#29702;&#30340;&#31243;&#24207;&#30028;&#38754;&#21644;&#38408;&#20540;&#35774;&#32622;&#30830;&#20445;&#20102;&#20854;&#28789;&#25935;&#24230;&#36229;&#36807;95.9&#65285;&#65292;&#29305;&#24322;&#24230;&#20960;&#20046;&#36798;&#21040;100&#65285;&#12290;&#22240;&#27492;&#65292;&#22312;&#20114;&#32852;&#32593;&#21644;&#36890;&#35759;&#32456;&#31471;&#30340;&#20113;&#26381;&#21153;&#30340;&#24110;&#21161;&#19979;&#65292;&#36825;&#31181;&#31574;&#30053;&#21487;&#20197;&#28508;&#22312;&#22320;&#29992;&#20110;&#23454;&#26102;&#26816;&#27979;&#29492;&#30168;&#30340;&#26089;&#26399;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
The challenge on forestalling monkeypox (Mpox) spread is the timely, convenient and accurate diagnosis for earlystage infected individuals. Here, we propose a remote and realtime online visualization strategy, called "Super Monitoring" to construct a low cost, convenient, timely and unspecialized diagnosis of early-stage Mpox. Such AI-mediated "Super Monitoring" (Mpox-AISM) invokes a framework assembled by deep learning, data augmentation and self-supervised learning, as well as professionally classifies four subtypes according to dataset characteristics and evolution trend of Mpox and seven other types of dermatopathya with high similarity, hence these features together with reasonable program interface and threshold setting ensure that its Recall (Sensitivity) was beyond 95.9% and the specificity was almost 100%. As a result, with the help of cloud service on Internet and communication terminal, this strategy can be potentially utilized for the real-time detection of earlystage Mpox 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Davis-Yin&#20998;&#35010;&#26041;&#27861;&#23454;&#29616;&#26356;&#24555;&#30340;&#39044;&#27979;&#19982;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20511;&#37492;&#20102;&#29616;&#20195;&#20984;&#20248;&#21270;&#30340;&#24605;&#24819;&#65292;&#33021;&#22815;&#22312;&#20855;&#26377;&#25968;&#21315;&#20010;&#21464;&#37327;&#30340;&#38382;&#39064;&#19978;&#36731;&#26494;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2301.13395</link><description>&lt;p&gt;
&#20351;&#29992;Davis-Yin&#20998;&#35010;&#23454;&#29616;&#26356;&#24555;&#30340;&#39044;&#27979;&#19982;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Faster Predict-and-Optimize with Davis-Yin Splitting. (arXiv:2301.13395v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Davis-Yin&#20998;&#35010;&#26041;&#27861;&#23454;&#29616;&#26356;&#24555;&#30340;&#39044;&#27979;&#19982;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20511;&#37492;&#20102;&#29616;&#20195;&#20984;&#20248;&#21270;&#30340;&#24605;&#24819;&#65292;&#33021;&#22815;&#22312;&#20855;&#26377;&#25968;&#21315;&#20010;&#21464;&#37327;&#30340;&#38382;&#39064;&#19978;&#36731;&#26494;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#38656;&#35201;&#21453;&#22797;&#35299;&#20915;&#20855;&#26377;&#30456;&#20284;&#20294;&#19981;&#21516;&#21442;&#25968;&#30340;&#32452;&#21512;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#21442;&#25968;$w$&#24182;&#38750;&#30452;&#25509;&#35266;&#23519;&#21040;&#30340;&#65307;&#21482;&#26377;&#19982;$w$&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#25968;&#25454;$d$&#21487;&#29992;&#12290;&#25105;&#20204;&#24456;&#23481;&#26131;&#23601;&#20250;&#24819;&#21040;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#26681;&#25454;$d$&#39044;&#27979;$w$&#65292;&#20294;&#26159;&#35757;&#32451;&#36825;&#26679;&#30340;&#27169;&#22411;&#38656;&#35201;&#23558;&#32452;&#21512;&#20248;&#21270;&#30340;&#31163;&#25955;&#24615;&#19982;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#26799;&#24230;&#20248;&#21270;&#26694;&#26550;&#30456;&#32467;&#21512;&#12290;&#24403;&#25152;&#35752;&#35770;&#30340;&#38382;&#39064;&#26159;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65288;ILP&#65289;&#26102;&#65292;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#32771;&#34385;&#32452;&#21512;&#38382;&#39064;&#30340;&#36830;&#32493;&#25918;&#26494;&#12290;&#34429;&#28982;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#22312;&#23567;&#22411;&#38382;&#39064;&#65288;10-100&#20010;&#21464;&#37327;&#65289;&#19978;&#26174;&#31034;&#20986;&#20102;&#39640;&#24230;&#30340;&#25928;&#26524;&#65292;&#20294;&#22312;&#22823;&#22411;&#38382;&#39064;&#19978;&#25193;&#23637;&#33021;&#21147;&#19981;&#36275;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#29616;&#20195;&#20984;&#20248;&#21270;&#30340;&#24605;&#24819;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#32593;&#32476;&#21644;&#35757;&#32451;&#26041;&#26696;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#25193;&#23637;&#21040;&#20855;&#26377;&#25968;&#21315;&#20010;&#21464;&#37327;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many applications, a combinatorial problem must be repeatedly solved with similar, but distinct parameters. Yet, the parameters $w$ are not directly observed; only contextual data $d$ that correlates with $w$ is available. It is tempting to use a neural network to predict $w$ given $d$, but training such a model requires reconciling the discrete nature of combinatorial optimization with the gradient-based frameworks used to train neural networks. When the problem in question is an Integer Linear Program (ILP), one approach to overcoming this issue is to consider a continuous relaxation of the combinatorial problem. While existing methods utilizing this approach have shown to be highly effective on small problems (10-100 variables), they do not scale well to large problems. In this work, we draw on ideas from modern convex optimization to design a network and training scheme which scales effortlessly to problems with thousands of variables.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#29992;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;Jordan&#12289;Kinderlehrer&#21644;Otto&#30340;&#21453;&#21521;&#26041;&#26696;&#20197;&#21450;&#19968;&#31181;&#21069;&#21521;&#26041;&#26696;&#65292;&#29992;&#20110;&#35745;&#31639;&#38750;&#20809;&#28369;Riesz&#26680;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#20989;&#25968;&#30340;Wasserstein&#26799;&#24230;&#27969;&#12290;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#36866;&#24403;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#36817;&#20284;&#22788;&#29702;&#35745;&#21010;&#30340;&#20998;&#35299;&#65292;&#24182;&#22312;&#20132;&#20114;&#33021;&#19978;&#22522;&#20934;&#27979;&#37327;&#31070;&#32463;&#32593;&#32476;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2301.11624</link><description>&lt;p&gt;
&#29992;&#31070;&#32463;Wasserstein&#26799;&#24230;&#27969;&#27714;&#35299;&#24102;&#26377;Riesz&#26680;&#30340;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Neural Wasserstein Gradient Flows for Maximum Mean Discrepancies with Riesz Kernels. (arXiv:2301.11624v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#29992;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;Jordan&#12289;Kinderlehrer&#21644;Otto&#30340;&#21453;&#21521;&#26041;&#26696;&#20197;&#21450;&#19968;&#31181;&#21069;&#21521;&#26041;&#26696;&#65292;&#29992;&#20110;&#35745;&#31639;&#38750;&#20809;&#28369;Riesz&#26680;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#20989;&#25968;&#30340;Wasserstein&#26799;&#24230;&#27969;&#12290;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#36866;&#24403;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#36817;&#20284;&#22788;&#29702;&#35745;&#21010;&#30340;&#20998;&#35299;&#65292;&#24182;&#22312;&#20132;&#20114;&#33021;&#19978;&#22522;&#20934;&#27979;&#37327;&#31070;&#32463;&#32593;&#32476;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#20809;&#28369;&#30340;Riesz&#26680;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#20989;&#25968;&#30340;Wasserstein&#26799;&#24230;&#27969;&#26174;&#31034;&#20986;&#20016;&#23500;&#30340;&#32467;&#26500;&#65292;&#22855;&#24322;&#27979;&#24230;&#21487;&#20197;&#21464;&#25104;&#32477;&#23545;&#36830;&#32493;&#30340;&#27979;&#24230;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#26412;&#25991;&#26088;&#22312;&#36129;&#29486;&#20110;&#23545;&#36825;&#31181;&#27969;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#29992;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#36924;&#36817;Jordan&#12289;Kinderlehrer&#21644;Otto&#30340;&#21453;&#21521;&#26041;&#26696;&#65292;&#20197;&#35745;&#31639;&#36825;&#31181;Wasserstein&#26799;&#24230;&#27969;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#21069;&#21521;&#26041;&#26696;&#65292;&#29992;&#20110;&#25152;&#35859;&#30340;Wasserstein&#26368;&#38497;&#19979;&#38477;&#27969;&#12290;&#22240;&#20026;&#25105;&#20204;&#19981;&#33021;&#25226;&#33258;&#24049;&#38480;&#21046;&#22312;&#32477;&#23545;&#36830;&#32493;&#30340;&#37327;&#24230;&#19978;&#65292;&#25152;&#20197;&#25105;&#20204;&#24517;&#39035;&#22788;&#29702;&#20256;&#36755;&#35745;&#21010;&#21644;&#36895;&#24230;&#35745;&#21010;&#65292;&#32780;&#19981;&#26159;&#36890;&#24120;&#30340;&#20256;&#36755;&#26144;&#23556;&#21644;&#36895;&#24230;&#22330;&#12290;&#30340;&#30830;&#65292;&#25105;&#20204;&#29992;&#29983;&#25104;&#30340;NN&#36817;&#20284;&#20102;&#20004;&#20010;&#35745;&#21010;&#30340;&#20998;&#35299;&#65292;&#36825;&#20123;&#35745;&#21010;&#26159;&#26681;&#25454;&#36866;&#24403;&#30340;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#30340;&#12290;&#20026;&#20102;&#35780;&#20272;&#20004;&#20010;&#31070;&#32463;&#31995;&#32479;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#23558;&#20854;&#22522;&#20934;&#21270;&#20026;&#30456;&#20114;&#20316;&#29992;&#33021;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#30001;Dirac&#27979;&#24230;&#24320;&#22987;&#30340;Wasserstein&#26041;&#26696;&#30340;&#35299;&#26512;&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wasserstein gradient flows of maximum mean discrepancy (MMD) functionals with non-smooth Riesz kernels show a rich structure as singular measures can become absolutely continuous ones and conversely. In this paper we contribute to the understanding of such flows. We propose to approximate the backward scheme of Jordan, Kinderlehrer and Otto for computing such Wasserstein gradient flows as well as a forward scheme for so-called Wasserstein steepest descent flows by neural networks (NNs). Since we cannot restrict ourselves to absolutely continuous measures, we have to deal with transport plans and velocity plans instead of usual transport maps and velocity fields. Indeed, we approximate the disintegration of both plans by generative NNs which are learned with respect to appropriate loss functions. In order to evaluate the quality of both neural schemes, we benchmark them on the interaction energy. Here we provide analytic formulas for Wasserstein schemes starting at a Dirac measure and s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29366;&#24577;&#20272;&#35745;&#22120;&#65288;DeNSE&#65289;&#65292;&#21033;&#29992;&#36125;&#21494;&#26031;&#26694;&#26550;&#32508;&#21512;&#21033;&#29992;&#24930;&#36895;&#20294;&#24191;&#27867;&#23384;&#22312;&#30340;SCADA&#25968;&#25454;&#21644;&#24555;&#36895;&#20294;&#23616;&#37096;&#30340;PMU&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#23545;&#25972;&#20010;&#31995;&#32479;&#30340;&#20122;&#31186;&#32423;&#29366;&#24577;&#20272;&#35745;&#12290;&#36890;&#36807;&#32771;&#34385;&#23454;&#38469;&#25361;&#25112;&#65292;&#22914;&#25299;&#25169;&#21464;&#21270;&#12289;&#38750;&#39640;&#26031;&#27979;&#37327;&#22122;&#22768;&#21644;&#38169;&#35823;&#25968;&#25454;&#26816;&#27979;&#19982;&#26657;&#27491;&#65292;&#35777;&#26126;&#20102;DeNSE&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.01729</link><description>&lt;p&gt;
&#32771;&#34385;&#23454;&#38469;&#23454;&#26045;&#25361;&#25112;&#30340;&#21516;&#27493;&#20840;&#31995;&#32479;&#29366;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Time-Synchronized Full System State Estimation Considering Practical Implementation Challenges. (arXiv:2212.01729v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29366;&#24577;&#20272;&#35745;&#22120;&#65288;DeNSE&#65289;&#65292;&#21033;&#29992;&#36125;&#21494;&#26031;&#26694;&#26550;&#32508;&#21512;&#21033;&#29992;&#24930;&#36895;&#20294;&#24191;&#27867;&#23384;&#22312;&#30340;SCADA&#25968;&#25454;&#21644;&#24555;&#36895;&#20294;&#23616;&#37096;&#30340;PMU&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#23545;&#25972;&#20010;&#31995;&#32479;&#30340;&#20122;&#31186;&#32423;&#29366;&#24577;&#20272;&#35745;&#12290;&#36890;&#36807;&#32771;&#34385;&#23454;&#38469;&#25361;&#25112;&#65292;&#22914;&#25299;&#25169;&#21464;&#21270;&#12289;&#38750;&#39640;&#26031;&#27979;&#37327;&#22122;&#22768;&#21644;&#38169;&#35823;&#25968;&#25454;&#26816;&#27979;&#19982;&#26657;&#27491;&#65292;&#35777;&#26126;&#20102;DeNSE&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#30456;&#37327;&#27979;&#37327;&#35013;&#32622;&#65288;PMUs&#65289;&#36890;&#24120;&#25918;&#32622;&#22312;&#26368;&#39640;&#30005;&#21387;&#30340;&#27597;&#32447;&#19978;&#65292;&#22240;&#27492;&#35768;&#22810;&#20302;&#30005;&#21387;&#32423;&#21035;&#30340;&#22823;&#35268;&#27169;&#30005;&#21147;&#31995;&#32479;&#26080;&#27861;&#34987;&#20854;&#35266;&#27979;&#21040;&#12290;&#36825;&#31181;&#35266;&#27979;&#32570;&#22833;&#20351;&#24471;&#21516;&#27493;&#26102;&#38388;&#29366;&#24577;&#20272;&#35745;&#25104;&#20026;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29366;&#24577;&#20272;&#35745;&#22120;&#65288;DeNSE&#65289;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#35813;DeNSE&#37319;&#29992;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#36890;&#36807;&#38388;&#25509;&#32467;&#21512;&#26469;&#33258;&#24930;&#26102;&#38388;&#23610;&#24230;&#20294;&#26222;&#36941;&#23384;&#22312;&#30340;&#30417;&#30563;&#25511;&#21046;&#19982;&#25968;&#25454;&#37319;&#38598;&#65288;SCADA&#65289;&#25968;&#25454;&#19982;&#24555;&#26102;&#38388;&#23610;&#24230;&#20294;&#23616;&#37096;&#30340;PMU&#25968;&#25454;&#30340;&#25512;&#26029;&#65292;&#23454;&#29616;&#23545;&#25972;&#20010;&#31995;&#32479;&#30340;&#20122;&#31186;&#32423;&#24773;&#26223;&#24863;&#30693;&#12290;&#36890;&#36807;&#32771;&#34385;&#25299;&#25169;&#21464;&#21270;&#12289;&#38750;&#39640;&#26031;&#27979;&#37327;&#22122;&#22768;&#21644;&#38169;&#35823;&#25968;&#25454;&#26816;&#27979;&#21644;&#26657;&#27491;&#65292;&#23637;&#31034;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#23454;&#38469;&#25928;&#29992;&#12290;&#20351;&#29992;IEEE 118-bus&#31995;&#32479;&#24471;&#21040;&#30340;&#32467;&#26524;&#34920;&#26126;&#20102;DeNSE&#22312;&#32431;SCADA&#29366;&#24577;&#20272;&#35745;&#22120;&#12289;SCADA-PMU&#28151;&#21512;&#29366;&#24577;&#20272;&#35745;&#22120;&#21644;&#20165;PMU&#32447;&#24615;&#29366;&#24577;&#20272;&#35745;&#22120;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As phasor measurement units (PMUs) are usually placed on the highest voltage buses, many lower voltage levels of the bulk power system are not observed by them. This lack of visibility makes time-synchronized state estimation of the full system a challenging problem. We propose a Deep Neural network-based State Estimator (DeNSE) to overcome this problem. The DeNSE employs a Bayesian framework to indirectly combine inferences drawn from slow timescale but widespread supervisory control and data acquisition (SCADA) data with fast timescale but local PMU data to attain sub-second situational awareness of the entire system. The practical utility of the proposed approach is demonstrated by considering topology changes, non-Gaussian measurement noise, and bad data detection and correction. The results obtained using the IEEE 118-bus system show the superiority of the DeNSE over a purely SCADA state estimator, a SCADA-PMU hybrid state estimator, and a PMU-only linear state estimator from a te
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;TD&#23398;&#20064;&#35270;&#20026;&#36866;&#24403;&#36873;&#25321;&#20989;&#25968;&#30340;&#26799;&#24230;&#20998;&#21106;&#65292;&#23558;TD&#21644;SVRG&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#20960;&#20309;&#25910;&#25947;&#36895;&#24230;&#30340;&#31574;&#30053;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#22312;&#29702;&#35770;&#21644;&#23454;&#39564;&#19978;&#24471;&#21040;&#20102;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2211.16237</link><description>&lt;p&gt;
&#29992;&#26799;&#24230;&#20998;&#21106;&#26041;&#27861;&#32553;&#23567;SVRG&#19982;TD-SVRG&#20043;&#38388;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Closing the gap between SVRG and TD-SVRG with Gradient Splitting. (arXiv:2211.16237v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;TD&#23398;&#20064;&#35270;&#20026;&#36866;&#24403;&#36873;&#25321;&#20989;&#25968;&#30340;&#26799;&#24230;&#20998;&#21106;&#65292;&#23558;TD&#21644;SVRG&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#20960;&#20309;&#25910;&#25947;&#36895;&#24230;&#30340;&#31574;&#30053;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#22312;&#29702;&#35770;&#21644;&#23454;&#39564;&#19978;&#24471;&#21040;&#20102;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
TD&#65288;&#26102;&#24207;&#24046;&#20998;&#65289;&#23398;&#20064;&#26159;&#19968;&#31181;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#35780;&#20272;&#26041;&#27861;&#65292;&#20854;&#24615;&#33021;&#21487;&#20197;&#36890;&#36807;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#36827;&#34892;&#22686;&#24378;&#12290;&#26368;&#36817;&#65292;&#22810;&#20010;&#24037;&#20316;&#23581;&#35797;&#23558;TD&#23398;&#20064;&#19982;SVRG&#30456;&#32467;&#21512;&#65292;&#20197;&#33719;&#24471;&#19968;&#31181;&#20855;&#26377;&#20960;&#20309;&#25910;&#25947;&#36895;&#24230;&#30340;&#31574;&#30053;&#35780;&#20272;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#20984;&#20248;&#21270;&#35774;&#32622;&#19979;&#65292;&#25152;&#24471;&#21040;&#30340;&#25910;&#25947;&#36895;&#24230;&#26126;&#26174;&#19981;&#21450;SVRG&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#26368;&#36817;&#23545;TD&#23398;&#20064;&#30340;&#35299;&#37322;&#65292;&#23558;&#20854;&#35270;&#20026;&#19968;&#20010;&#36866;&#24403;&#36873;&#25321;&#20989;&#25968;&#30340;&#26799;&#24230;&#30340;&#20998;&#21106;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#31639;&#27861;&#65292;&#24182;&#23558;TD&#19982;SVRG&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#19968;&#20010;&#20855;&#26377;&#39044;&#23450;&#23398;&#20064;&#36895;&#29575;&#20026;1/8&#30340;&#20960;&#20309;&#25910;&#25947;&#30028;&#38480;&#65292;&#19982;&#20984;&#35774;&#32622;&#19979;SVRG&#30340;&#25910;&#25947;&#30028;&#38480;&#30456;&#21516;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#24471;&#21040;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal difference (TD) learning is a policy evaluation in reinforcement learning whose performance can be enhanced by variance reduction techniques. Recently, multiple works have sought to fuse TD learning with SVRG to obtain a policy evaluation method with a geometric rate of convergence. However, the resulting convergence rate is significantly weaker than what is achieved by SVRG in the setting of convex optimization. In this work we utilize a recent interpretation of TD-learning as the splitting of the gradient of an appropriately chosen function, thus simplifying the algorithm and fusing TD with SVRG. Our main result is a geometric convergence bound with predetermined learning rate of $1/8$, which is identical to the convergence bound available for SVRG in the convex setting. Our theoretical findings are supported by a set of experiments.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28176;&#36827;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;PGCN&#65289;&#30340;&#20132;&#36890;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#38454;&#27573;&#36880;&#27493;&#36866;&#24212;&#36755;&#20837;&#25968;&#25454;&#26469;&#26500;&#24314;&#19968;&#32452;&#22270;&#65292;&#20197;&#35299;&#20915;&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#22797;&#26434;&#26102;&#31354;&#30456;&#20851;&#24615;&#21644;&#25968;&#25454;&#21464;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2202.08982</link><description>&lt;p&gt;
PGCN&#65306;&#29992;&#20110;&#26102;&#31354;&#20132;&#36890;&#39044;&#27979;&#30340;&#28176;&#36827;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
PGCN: Progressive Graph Convolutional Networks for Spatial-Temporal Traffic Forecasting. (arXiv:2202.08982v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.08982
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28176;&#36827;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;PGCN&#65289;&#30340;&#20132;&#36890;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#38454;&#27573;&#36880;&#27493;&#36866;&#24212;&#36755;&#20837;&#25968;&#25454;&#26469;&#26500;&#24314;&#19968;&#32452;&#22270;&#65292;&#20197;&#35299;&#20915;&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#22797;&#26434;&#26102;&#31354;&#30456;&#20851;&#24615;&#21644;&#25968;&#25454;&#21464;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#32593;&#32476;&#20013;&#22797;&#26434;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#20351;&#24471;&#20132;&#36890;&#39044;&#27979;&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30001;&#20110;&#20132;&#36890;&#31995;&#32479;&#26412;&#36136;&#19978;&#20855;&#26377;&#22270;&#32467;&#26500;&#65292;&#22240;&#27492;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#30740;&#31350;&#12290;&#26368;&#36817;&#65292;&#26500;&#24314;&#36866;&#24212;&#24615;&#22270;&#23545;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#22312;&#21333;&#19968;&#38745;&#24577;&#22270;&#32467;&#26500;&#19978;&#30340;&#27169;&#22411;&#19978;&#26174;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22270;&#36866;&#24212;&#24615;&#20165;&#22312;&#35757;&#32451;&#38454;&#27573;&#24212;&#29992;&#65292;&#24182;&#19981;&#21453;&#26144;&#35757;&#32451;&#38454;&#27573;&#20351;&#29992;&#30340;&#25968;&#25454;&#12290;&#36825;&#31181;&#32570;&#28857;&#22312;&#20132;&#36890;&#39044;&#27979;&#20013;&#21487;&#33021;&#26377;&#38382;&#39064;&#65292;&#22240;&#20026;&#20132;&#36890;&#25968;&#25454;&#24120;&#24120;&#23384;&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#24847;&#22806;&#21464;&#21270;&#21644;&#19981;&#35268;&#21017;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20132;&#36890;&#39044;&#27979;&#26694;&#26550;&#65292;&#31216;&#20026;&#28176;&#36827;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;PGCN&#65289;&#12290;PGCN&#36890;&#36807;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#38454;&#27573;&#36880;&#27493;&#36866;&#24212;&#36755;&#20837;&#25968;&#25454;&#26469;&#26500;&#24314;&#19968;&#32452;&#22270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#35813;&#27169;&#22411;&#26469;&#26500;&#24314;&#28176;&#36827;&#34892;&#30340;&#37051;&#25509;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;
The complex spatial-temporal correlations in transportation networks make the traffic forecasting problem challenging. Since transportation system inherently possesses graph structures, much research efforts have been put with graph neural networks. Recently, constructing adaptive graphs to the data has shown promising results over the models relying on a single static graph structure. However, the graph adaptations are applied during the training phases, and do not reflect the data used during the testing phases. Such shortcomings can be problematic especially in traffic forecasting since the traffic data often suffers from the unexpected changes and irregularities in the time series. In this study, we propose a novel traffic forecasting framework called Progressive Graph Convolutional Network (PGCN). PGCN constructs a set of graphs by progressively adapting to input data during the training and the testing phases. Specifically, we implemented the model to construct progressive adjace
&lt;/p&gt;</description></item></channel></rss>