<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#35268;&#21017;&#21644;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#25214;&#21040;&#25968;&#25454;&#30340;&#23616;&#37096;&#21306;&#22495;&#21644;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25551;&#36848;&#65292;&#25105;&#20204;&#25945;&#23548;&#20154;&#31867;&#22914;&#20309;&#19982;AI&#21512;&#20316;&#12290;&#36890;&#36807;&#30446;&#26631;&#26816;&#27979;&#21644;&#38382;&#31572;&#20219;&#21153;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#26356;&#21152;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2311.01007</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#35268;&#21017;&#21644;&#24341;&#23548;&#26469;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Effective Human-AI Teams via Learned Natural Language Rules and Onboarding. (arXiv:2311.01007v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#35268;&#21017;&#21644;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#25214;&#21040;&#25968;&#25454;&#30340;&#23616;&#37096;&#21306;&#22495;&#21644;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25551;&#36848;&#65292;&#25105;&#20204;&#25945;&#23548;&#20154;&#31867;&#22914;&#20309;&#19982;AI&#21512;&#20316;&#12290;&#36890;&#36807;&#30446;&#26631;&#26816;&#27979;&#21644;&#38382;&#31572;&#20219;&#21153;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#26356;&#21152;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#36234;&#26469;&#36234;&#20381;&#36182;&#20110;AI&#20195;&#29702;&#26469;&#24110;&#21161;&#20182;&#20204;&#23436;&#25104;&#21508;&#31181;&#20219;&#21153;&#12290;&#20154;&#31867;&#24517;&#39035;&#30693;&#36947;&#20309;&#26102;&#20381;&#36182;&#20110;&#20195;&#29702;&#65292;&#19982;&#20195;&#29702;&#21512;&#20316;&#25110;&#24573;&#30053;&#20854;&#24314;&#35758;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#21306;&#22495;&#21644;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#23398;&#20064;&#35268;&#21017;&#30340;&#26041;&#27861;&#65292;&#20197;&#35828;&#26126;&#20154;&#31867;&#24212;&#35813;&#22914;&#20309;&#19982;AI&#21512;&#20316;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#21306;&#22495;&#21457;&#29616;&#31639;&#27861;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#25214;&#21040;&#25968;&#25454;&#30340;&#23616;&#37096;&#21306;&#22495;&#20316;&#20026;&#37051;&#22495;&#65292;&#32416;&#27491;&#20102;&#20154;&#31867;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#27599;&#20010;&#21306;&#22495;&#37117;&#36890;&#36807;&#36845;&#20195;&#21644;&#23545;&#27604;&#36807;&#31243;&#36827;&#34892;&#25551;&#36848;&#65292;&#20854;&#20013;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25551;&#36848;&#35813;&#21306;&#22495;&#12290;&#28982;&#21518;&#25105;&#20204;&#36890;&#36807;&#24341;&#23548;&#38454;&#27573;&#23558;&#36825;&#20123;&#35268;&#21017;&#25945;&#32473;&#20154;&#31867;&#12290;&#36890;&#36807;&#22312;&#30446;&#26631;&#26816;&#27979;&#21644;&#38382;&#31572;&#20219;&#21153;&#19978;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#26356;&#21152;&#20934;&#30830;&#12290;&#25105;&#20204;&#36824;&#20998;&#21035;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#21306;&#22495;&#21457;&#29616;&#21644;&#25551;&#36848;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
People are relying on AI agents to assist them with various tasks. The human must know when to rely on the agent, collaborate with the agent, or ignore its suggestions. In this work, we propose to learn rules grounded in data regions and described in natural language that illustrate how the human should collaborate with the AI. Our novel region discovery algorithm finds local regions in the data as neighborhoods in an embedding space that corrects the human prior. Each region is then described using an iterative and contrastive procedure where a large language model describes the region. We then teach these rules to the human via an onboarding stage. Through user studies on object detection and question-answering tasks, we show that our method can lead to more accurate human-AI teams. We also evaluate our region discovery and description algorithms separately.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32508;&#21512;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#23545;&#31216;&#33258;&#36866;&#24212;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#20877;&#29616;&#37327;&#23376;&#21147;&#23398;&#35745;&#31639;&#20013;&#30340;&#30005;&#23376;&#28608;&#21457;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#38024;&#23545;&#26356;&#22823;&#26356;&#22797;&#26434;&#20998;&#23376;&#30340;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21407;&#23376;&#20013;&#24515;&#22522;&#30784;&#23545;&#24212;&#30340;&#21442;&#25968;&#21270;&#23454;&#29616;&#20102;&#26497;&#22823;&#30340;&#35745;&#31639;&#25928;&#30410;&#12290;</title><link>http://arxiv.org/abs/2311.00844</link><description>&lt;p&gt;
&#36890;&#36807;&#29289;&#29702;&#32422;&#26463;&#30340;&#26426;&#22120;&#23398;&#20064;&#26469;&#30740;&#31350;&#30005;&#23376;&#28608;&#21457;&#24577;
&lt;/p&gt;
&lt;p&gt;
Electronic excited states from physically-constrained machine learning. (arXiv:2311.00844v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32508;&#21512;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#23545;&#31216;&#33258;&#36866;&#24212;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#20877;&#29616;&#37327;&#23376;&#21147;&#23398;&#35745;&#31639;&#20013;&#30340;&#30005;&#23376;&#28608;&#21457;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#38024;&#23545;&#26356;&#22823;&#26356;&#22797;&#26434;&#20998;&#23376;&#30340;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21407;&#23376;&#20013;&#24515;&#22522;&#30784;&#23545;&#24212;&#30340;&#21442;&#25968;&#21270;&#23454;&#29616;&#20102;&#26497;&#22823;&#30340;&#35745;&#31639;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#25216;&#26415;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#26367;&#20195;&#29289;&#36136;&#30340;&#30005;&#23376;&#32467;&#26500;&#35745;&#31639;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#30456;&#20851;&#30340;&#38382;&#39064;&#26159;&#65292;&#26426;&#22120;&#23398;&#20064;(ML)&#26159;&#21542;&#24212;&#24403;&#30452;&#25509;&#24212;&#29992;&#20110;&#39044;&#27979;&#25152;&#38656;&#30340;&#24615;&#36136;&#65292;&#36824;&#26159;&#26126;&#30830;&#22320;&#19982;&#29289;&#29702;&#22522;&#30784;&#25805;&#20316;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#24314;&#27169;&#26041;&#27861;&#30340;&#31034;&#20363;&#65292;&#20854;&#20013;&#23545;&#19968;&#20010;&#26377;&#25928;&#21704;&#23494;&#39039;&#37327;&#30340;&#23545;&#31216;&#33258;&#36866;&#24212;ML&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#20877;&#29616;&#37327;&#23376;&#21147;&#23398;&#35745;&#31639;&#20013;&#30340;&#30005;&#23376;&#28608;&#21457;&#12290;&#25152;&#24471;&#27169;&#22411;&#21487;&#20197;&#39044;&#27979;&#27604;&#20854;&#35757;&#32451;&#38598;&#26356;&#22823;&#26356;&#22797;&#26434;&#30340;&#20998;&#23376;&#65292;&#24182;&#36890;&#36807;&#38388;&#25509;&#38024;&#23545;&#33391;&#22909;&#25910;&#25947;&#35745;&#31639;&#30340;&#36755;&#20986;&#32780;&#20351;&#29992;&#19982;&#26368;&#23567;&#21407;&#23376;&#20013;&#24515;&#22522;&#30784;&#23545;&#24212;&#30340;&#21442;&#25968;&#21270;&#65292;&#23454;&#29616;&#20102;&#26497;&#22823;&#30340;&#35745;&#31639;&#25928;&#30410;&#12290;&#36825;&#20123;&#32467;&#26524;&#24378;&#35843;&#20102;&#23558;&#25968;&#25454;&#39537;&#21160;&#25216;&#26415;&#19982;&#29289;&#29702;&#36817;&#20284;&#30456;&#32467;&#21512;&#30340;&#20248;&#28857;&#65292;&#25552;&#39640;&#20102;ML&#27169;&#22411;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#32780;&#19981;&#38656;&#35201;&#26126;&#30830;&#24314;&#31435;&#29289;&#29702;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven techniques are increasingly used to replace electronic-structure calculations of matter. In this context, a relevant question is whether machine learning (ML) should be applied directly to predict the desired properties or be combined explicitly with physically-grounded operations. We present an example of an integrated modeling approach, in which a symmetry-adapted ML model of an effective Hamiltonian is trained to reproduce electronic excitations from a quantum-mechanical calculation. The resulting model can make predictions for molecules that are much larger and more complex than those that it is trained on, and allows for dramatic computational savings by indirectly targeting the outputs of well-converged calculations while using a parameterization corresponding to a minimal atom-centered basis. These results emphasize the merits of intertwining data-driven techniques with physical approximations, improving the transferability and interpretability of ML models without a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#22522;&#20110;&#20803;&#26435;&#37325;&#30340;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#33410;&#28857;&#23545;&#24212;&#26435;&#37325;&#30340;&#33258;&#36866;&#24212;&#35843;&#25972;&#65292;&#33021;&#22815;&#22312;&#23384;&#22312;&#22122;&#22768;&#36793;&#30340;&#22270;&#20013;&#25214;&#21040;&#26377;&#24847;&#20041;&#30340;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2311.00322</link><description>&lt;p&gt;
&#22122;&#22768;&#22270;&#20013;&#30340;&#40065;&#26834;&#22270;&#32858;&#31867;&#36890;&#36807;&#20803;&#26435;&#37325;
&lt;/p&gt;
&lt;p&gt;
Robust Graph Clustering via Meta Weighting for Noisy Graphs. (arXiv:2311.00322v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00322
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#22522;&#20110;&#20803;&#26435;&#37325;&#30340;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#33410;&#28857;&#23545;&#24212;&#26435;&#37325;&#30340;&#33258;&#36866;&#24212;&#35843;&#25972;&#65292;&#33021;&#22815;&#22312;&#23384;&#22312;&#22122;&#22768;&#36793;&#30340;&#22270;&#20013;&#25214;&#21040;&#26377;&#24847;&#20041;&#30340;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#22312;&#22122;&#22768;&#36793;&#19978;&#40065;&#26834;&#22320;&#25214;&#21040;&#22270;&#20013;&#30340;&#26377;&#24847;&#20041;&#30340;&#32858;&#31867;&#65311;&#22270;&#32858;&#31867;&#26159;&#22270;&#20998;&#26512;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#26041;&#27861;&#22312;&#22270;&#32858;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23427;&#20204;&#22312;&#23384;&#22312;&#22122;&#22768;&#36793;&#30340;&#22270;&#19978;&#30340;&#24615;&#33021;&#26126;&#26174;&#19979;&#38477;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#40065;&#26834;GNN-based&#22270;&#32858;&#31867;&#30340;MetaGC&#12290;MetaGC&#37319;&#29992;&#21487;&#20998;&#35299;&#30340;&#32858;&#31867;&#25439;&#22833;&#20989;&#25968;&#65292;&#23558;&#20854;&#37325;&#26032;&#34920;&#36848;&#20026;&#33410;&#28857;&#23545;&#20043;&#38388;&#25439;&#22833;&#30340;&#27714;&#21644;&#12290;&#25105;&#20204;&#20026;&#27599;&#20010;&#33410;&#28857;&#23545;&#28155;&#21152;&#21487;&#23398;&#20064;&#30340;&#26435;&#37325;&#65292;&#24182;&#20351;&#29992;&#20803;&#26435;&#37325;&#26469;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#33410;&#28857;&#23545;&#30340;&#26435;&#37325;&#65292;&#20351;&#26377;&#24847;&#20041;&#30340;&#33410;&#28857;&#23545;&#30340;&#26435;&#37325;&#22686;&#21152;&#65292;&#32780;&#19981;&#37027;&#20040;&#26377;&#24847;&#20041;&#30340;&#33410;&#28857;&#23545;&#65288;&#20363;&#22914;&#22122;&#22768;&#36793;&#65289;&#30340;&#26435;&#37325;&#20943;&#23567;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;MetaGC&#25353;&#29031;&#39044;&#26399;&#23398;&#20064;&#26435;&#37325;&#65292;&#24182;&#19988;&#22240;&#27492;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we find meaningful clusters in a graph robustly against noise edges? Graph clustering (i.e., dividing nodes into groups of similar ones) is a fundamental problem in graph analysis with applications in various fields. Recent studies have demonstrated that graph neural network (GNN) based approaches yield promising results for graph clustering. However, we observe that their performance degenerates significantly on graphs with noise edges, which are prevalent in practice. In this work, we propose MetaGC for robust GNN-based graph clustering. MetaGC employs a decomposable clustering loss function, which can be rephrased as a sum of losses over node pairs. We add a learnable weight to each node pair, and MetaGC adaptively adjusts the weights of node pairs using meta-weighting so that the weights of meaningful node pairs increase and the weights of less-meaningful ones (e.g., noise edges) decrease. We show empirically that MetaGC learns weights as intended and consequently outperfor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#35270;&#20026;&#21442;&#25968;&#27010;&#29575;&#27169;&#22411;&#30340;&#26102;&#38388;&#28436;&#21270;&#36807;&#31243;&#65292;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#21442;&#25968;&#19982;&#29983;&#25104;&#26679;&#26412;&#20043;&#38388;&#30340;&#28909;&#21147;&#23398;&#20132;&#25442;&#65292;&#21457;&#29616;&#27169;&#22411;&#36890;&#36807;&#32791;&#25955;&#28909;&#37327;&#26469;&#23398;&#20064;&#65292;&#21442;&#25968;&#23376;&#31995;&#32479;&#20805;&#24403;&#28909;&#24211;&#23384;&#20648;&#23398;&#21040;&#30340;&#20449;&#24687;&#12290;&#36825;&#20026;&#36229;&#21442;&#25968;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#28909;&#21147;&#23398;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2310.19802</link><description>&lt;p&gt;
&#23398;&#20064;&#29983;&#25104;&#21442;&#25968;&#27010;&#29575;&#27169;&#22411;&#30340;&#38543;&#26426;&#28909;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Stochastic Thermodynamics of Learning Generative Parametric Probabilistic Models. (arXiv:2310.19802v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#35270;&#20026;&#21442;&#25968;&#27010;&#29575;&#27169;&#22411;&#30340;&#26102;&#38388;&#28436;&#21270;&#36807;&#31243;&#65292;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#21442;&#25968;&#19982;&#29983;&#25104;&#26679;&#26412;&#20043;&#38388;&#30340;&#28909;&#21147;&#23398;&#20132;&#25442;&#65292;&#21457;&#29616;&#27169;&#22411;&#36890;&#36807;&#32791;&#25955;&#28909;&#37327;&#26469;&#23398;&#20064;&#65292;&#21442;&#25968;&#23376;&#31995;&#32479;&#20805;&#24403;&#28909;&#24211;&#23384;&#20648;&#23398;&#21040;&#30340;&#20449;&#24687;&#12290;&#36825;&#20026;&#36229;&#21442;&#25968;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#28909;&#21147;&#23398;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#21442;&#25968;&#21270;&#27010;&#29575;&#27169;&#22411;&#65288;PPM&#65289;&#30340;&#26102;&#38388;&#28436;&#21270;&#65292;&#20174;&#26412;&#36136;&#19978;&#26469;&#35828;&#65292;&#36825;&#26159;&#19968;&#20010;&#28909;&#21147;&#23398;&#36807;&#31243;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27169;&#22411;&#21442;&#25968;&#65288;&#35760;&#20026;$\Theta$&#65289;&#19982;&#27169;&#22411;&#29983;&#25104;&#26679;&#26412;&#65288;&#35760;&#20026;$X$&#65289;&#20043;&#38388;&#30340;&#28909;&#21147;&#23398;&#20132;&#25442;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#20248;&#21270;&#22120;&#30340;&#20316;&#29992;&#26159;&#39537;&#21160;&#36825;&#20004;&#20010;&#23376;&#31995;&#32479;&#30340;&#26102;&#38388;&#28436;&#21270;&#30340;&#33021;&#28304;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;&#29983;&#25104;&#26679;&#26412;$X$&#30340;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#36890;&#36807;&#32791;&#25955;&#28909;&#37327;&#26469;&#23398;&#20064;&#65292;&#23548;&#33268;&#27169;&#22411;&#21442;&#25968;$\Theta$&#30340;&#29109;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#21442;&#25968;&#23376;&#31995;&#32479;&#20805;&#24403;&#20102;&#19968;&#20010;&#28909;&#24211;&#65292;&#26377;&#25928;&#22320;&#23384;&#20648;&#20102;&#23398;&#21040;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#21442;&#25968;&#20316;&#20026;&#28909;&#24211;&#30340;&#35282;&#33394;&#20026;&#36229;&#21442;&#25968;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#28909;&#21147;&#23398;&#27934;&#23519;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#26126;&#30830;&#19988;&#19968;&#33268;&#30340;&#26041;&#24335;&#26469;&#29702;&#35299;&#29983;&#25104;&#27169;&#22411;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#28909;&#21147;&#23398;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We have formulated generative machine learning problems as the time evolution of Parametric Probabilistic Models (PPMs), inherently rendering a thermodynamic process. Then, we have studied the thermodynamic exchange between the model's parameters, denoted as $\Theta$, and the model's generated samples, denoted as $X$. We demonstrate that the training dataset and the action of the Stochastic Gradient Descent (SGD) optimizer serve as a work source that governs the time evolution of these two subsystems. Our findings reveal that the model learns through the dissipation of heat during the generation of samples $X$, leading to an increase in the entropy of the model's parameters, $\Theta$. Thus, the parameter subsystem acts as a heat reservoir, effectively storing the learned information. Furthermore, the role of the model's parameters as a heat reservoir provides valuable thermodynamic insights into the generalization power of over-parameterized models. This approach offers an unambiguous 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#20197;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#24050;&#30693;&#29289;&#29702;&#23398;&#30693;&#35782;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.18471</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#22240;&#26524;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Causal disentanglement of multimodal data. (arXiv:2310.18471v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18471
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#20197;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#24050;&#30693;&#29289;&#29702;&#23398;&#30693;&#35782;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#21457;&#29616;&#20102;&#25968;&#25454;&#30340;&#36739;&#20302;&#32500;&#24230;&#34920;&#31034;&#65292;&#21487;&#20197;&#23545;&#22240;&#26524;&#20851;&#31995;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#65307;&#30001;&#20110;&#23454;&#29616;&#36825;&#26679;&#30340;&#21487;&#35299;&#37322;&#34920;&#31034;&#24456;&#20855;&#25361;&#25112;&#24615;&#65292;&#35768;&#22810;&#22240;&#26524;&#23398;&#20064;&#31639;&#27861;&#21033;&#29992;&#20102;&#25351;&#31034;&#20808;&#39564;&#20449;&#24687;&#30340;&#20803;&#32032;&#65292;&#20363;&#22914;&#65288;&#32447;&#24615;&#65289;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#12289;&#24178;&#39044;&#25968;&#25454;&#25110;&#24369;&#30417;&#30563;&#12290;&#28982;&#32780;&#65292;&#22312;&#25506;&#32034;&#24615;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#36825;&#20123;&#20803;&#32032;&#21644;&#20808;&#39564;&#20449;&#24687;&#21487;&#33021;&#19981;&#21487;&#29992;&#25110;&#19981;&#21512;&#36866;&#12290;&#30456;&#21453;&#65292;&#31185;&#23398;&#25968;&#25454;&#38598;&#36890;&#24120;&#20855;&#26377;&#22810;&#20010;&#27169;&#24577;&#25110;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#32422;&#26463;&#65292;&#24182;&#19988;&#24050;&#32463;&#35777;&#26126;&#22312;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#35774;&#32622;&#20013;&#20351;&#29992;&#36825;&#31181;&#31185;&#23398;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#21487;&#20197;&#25913;&#21892;&#22240;&#26524;&#20998;&#35299;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#65288;causalPIMA&#65289;&#65292;&#23427;&#21487;&#20197;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#24050;&#30693;&#30340;&#29289;&#29702;&#23398;&#30693;&#35782;&#21457;&#29616;&#20855;&#26377;&#22240;&#26524;&#20851;&#31995;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#31639;&#27861;&#21033;&#29992;&#26032;&#30340;&#21487;&#24494;&#21442;&#25968;&#21270;&#26469;&#23398;&#20064;&#36825;&#31181;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal representation learning algorithms discover lower-dimensional representations of data that admit a decipherable interpretation of cause and effect; as achieving such interpretable representations is challenging, many causal learning algorithms utilize elements indicating prior information, such as (linear) structural causal models, interventional data, or weak supervision. Unfortunately, in exploratory causal representation learning, such elements and prior information may not be available or warranted. Alternatively, scientific datasets often have multiple modalities or physics-based constraints, and the use of such scientific, multimodal data has been shown to improve disentanglement in fully unsupervised settings. Consequently, we introduce a causal representation learning algorithm (causalPIMA) that can use multimodal data and known physics to discover important features with causal relationships. Our innovative algorithm utilizes a new differentiable parametrization to lear
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#24335;&#22240;&#23376;&#36712;&#36857;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26102;&#38388;&#24352;&#37327;&#20998;&#35299;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#26469;&#28789;&#27963;&#20272;&#35745;&#22240;&#23376;&#30340;&#26102;&#38388;&#28436;&#21464;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22312;&#32447;&#28388;&#27874;&#31639;&#27861;&#26469;&#20272;&#35745;&#22240;&#23376;&#29366;&#24577;&#30340;&#28436;&#21464;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2310.17021</link><description>&lt;p&gt;
&#27969;&#24335;&#22240;&#23376;&#36712;&#36857;&#23398;&#20064;&#29992;&#20110;&#26102;&#38388;&#24352;&#37327;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Streaming Factor Trajectory Learning for Temporal Tensor Decomposition. (arXiv:2310.17021v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17021
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#24335;&#22240;&#23376;&#36712;&#36857;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26102;&#38388;&#24352;&#37327;&#20998;&#35299;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#26469;&#28789;&#27963;&#20272;&#35745;&#22240;&#23376;&#30340;&#26102;&#38388;&#28436;&#21464;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22312;&#32447;&#28388;&#27874;&#31639;&#27861;&#26469;&#20272;&#35745;&#22240;&#23376;&#29366;&#24577;&#30340;&#28436;&#21464;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#30340;&#24352;&#37327;&#25968;&#25454;&#36890;&#24120;&#20855;&#26377;&#26102;&#38388;&#20449;&#24687;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26102;&#38388;&#20998;&#35299;&#26041;&#27861;&#20272;&#35745;&#27599;&#20010;&#24352;&#37327;&#27169;&#24335;&#20013;&#23545;&#35937;&#30340;&#19968;&#32452;&#22266;&#23450;&#22240;&#23376;&#65292;&#22240;&#27492;&#26080;&#27861;&#25429;&#25417;&#23545;&#35937;&#34920;&#31034;&#30340;&#26102;&#38388;&#28436;&#21464;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#32570;&#20047;&#20174;&#27969;&#25968;&#25454;&#20013;&#25429;&#25417;&#36825;&#31181;&#28436;&#21464;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#24456;&#24120;&#35265;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#26102;&#38388;&#24352;&#37327;&#20998;&#35299;&#30340;&#27969;&#24335;&#22240;&#23376;&#36712;&#36857;&#23398;&#20064;&#65288;SFTL&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#65288;GPs&#65289;&#26469;&#24314;&#27169;&#22240;&#23376;&#30340;&#36712;&#36857;&#65292;&#20174;&#32780;&#28789;&#27963;&#22320;&#20272;&#35745;&#23427;&#20204;&#30340;&#26102;&#38388;&#28436;&#21464;&#12290;&#20026;&#20102;&#35299;&#20915;&#22788;&#29702;&#27969;&#25968;&#25454;&#26102;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#25105;&#20204;&#36890;&#36807;&#26500;&#36896;&#31561;&#25928;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#23558;GPs&#36716;&#25442;&#20026;&#29366;&#24577;&#31354;&#38388;&#20808;&#39564;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22312;&#32447;&#28388;&#27874;&#31639;&#27861;&#65292;&#22312;&#25509;&#25910;&#21040;&#26032;&#25968;&#25454;&#26102;&#20272;&#35745;&#28041;&#21450;&#30340;&#22240;&#23376;&#29366;&#24577;&#30340;&#20998;&#35299;&#24335;&#36816;&#34892;&#21518;&#39564;&#12290;&#20998;&#35299;&#20272;&#35745;&#20351;&#25105;&#20204;&#33021;&#22815;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#22240;&#23376;&#30340;&#28436;&#21464;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Practical tensor data is often along with time information. Most existing temporal decomposition approaches estimate a set of fixed factors for the objects in each tensor mode, and hence cannot capture the temporal evolution of the objects' representation. More important, we lack an effective approach to capture such evolution from streaming data, which is common in real-world applications. To address these issues, we propose Streaming Factor Trajectory Learning (SFTL) for temporal tensor decomposition. We use Gaussian processes (GPs) to model the trajectory of factors so as to flexibly estimate their temporal evolution. To address the computational challenges in handling streaming data, we convert the GPs into a state-space prior by constructing an equivalent stochastic differential equation (SDE). We develop an efficient online filtering algorithm to estimate a decoupled running posterior of the involved factor states upon receiving new data. The decoupled estimation enables us to co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MACP&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#21333;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#37197;&#22791;&#21512;&#20316;&#33021;&#21147;&#26469;&#25552;&#39640;&#36830;&#25509;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;&#36890;&#36807;&#20923;&#32467;&#22823;&#37096;&#20998;&#21442;&#25968;&#24182;&#28155;&#21152;&#20960;&#20010;&#36731;&#37327;&#32423;&#27169;&#22359;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#21512;&#20316;&#35266;&#27979;&#65292;&#24182;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#32988;&#36807;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.16870</link><description>&lt;p&gt;
MACP&#65306;&#39640;&#25928;&#30340;&#21512;&#20316;&#24863;&#30693;&#27169;&#22411;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
MACP: Efficient Model Adaptation for Cooperative Perception. (arXiv:2310.16870v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MACP&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#21333;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#37197;&#22791;&#21512;&#20316;&#33021;&#21147;&#26469;&#25552;&#39640;&#36830;&#25509;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;&#36890;&#36807;&#20923;&#32467;&#22823;&#37096;&#20998;&#21442;&#25968;&#24182;&#28155;&#21152;&#20960;&#20010;&#36731;&#37327;&#32423;&#27169;&#22359;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#21512;&#20316;&#35266;&#27979;&#65292;&#24182;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#32988;&#36807;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36710;&#36742;&#38388;&#30340;&#36890;&#20449;&#26497;&#22823;&#22320;&#22686;&#24378;&#20102;&#36830;&#25509;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#24863;&#30693;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#29616;&#20449;&#24687;&#20849;&#20139;&#20197;"&#31359;&#36879;"&#36974;&#25377;&#29289;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20174;&#22836;&#24320;&#22987;&#24320;&#21457;&#21644;&#35757;&#32451;&#22797;&#26434;&#30340;&#22810;&#26234;&#33021;&#20307;&#24863;&#30693;&#27169;&#22411;&#21487;&#33021;&#26114;&#36149;&#19988;&#19981;&#24517;&#35201;&#65292;&#22240;&#20026;&#29616;&#26377;&#30340;&#21333;&#26234;&#33021;&#20307;&#27169;&#22411;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MACP&#30340;&#26032;&#26694;&#26550;&#65292;&#23558;&#21333;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#37197;&#22791;&#20102;&#21512;&#20316;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#30830;&#23450;&#20174;&#21333;&#26234;&#33021;&#20307;&#21040;&#21512;&#20316;&#29615;&#22659;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#36890;&#36807;&#20923;&#32467;&#22823;&#37096;&#20998;&#21442;&#25968;&#24182;&#28155;&#21152;&#20960;&#20010;&#36731;&#37327;&#32423;&#27169;&#22359;&#26469;&#20351;&#27169;&#22411;&#36866;&#24212;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#21512;&#20316;&#35266;&#27979;&#65292;&#24182;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#32988;&#36807;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vehicle-to-vehicle (V2V) communications have greatly enhanced the perception capabilities of connected and automated vehicles (CAVs) by enabling information sharing to "see through the occlusions", resulting in significant performance improvements. However, developing and training complex multi-agent perception models from scratch can be expensive and unnecessary when existing single-agent models show remarkable generalization capabilities. In this paper, we propose a new framework termed MACP, which equips a single-agent pre-trained model with cooperation capabilities. We approach this objective by identifying the key challenges of shifting from single-agent to cooperative settings, adapting the model by freezing most of its parameters and adding a few lightweight modules. We demonstrate in our experiments that the proposed framework can effectively utilize cooperative observations and outperform other state-of-the-art approaches in both simulated and real-world cooperative perception
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#29289;&#29702;&#20449;&#24687;&#22270;&#21367;&#31215;&#32593;&#32476;&#20316;&#20026;&#35299;&#20915;&#22797;&#26434;&#20960;&#20309;&#20307;&#20013;&#20559;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;&#30340;&#24191;&#20041;&#26694;&#26550;&#65292;&#24182;&#32467;&#21512;&#32463;&#20856;&#25968;&#20540;&#27714;&#35299;&#22120;&#20174;&#32780;&#35299;&#20915;&#20102;&#29289;&#29702;&#20449;&#24687;&#26694;&#26550;&#22312;&#22797;&#26434;&#20960;&#20309;&#20307;&#19978;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.14948</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#22270;&#21367;&#31215;&#32593;&#32476;&#65306;&#38754;&#21521;&#22797;&#26434;&#20960;&#20309;&#30340;&#24191;&#20041;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Graph Convolutional Networks: Towards a generalized framework for complex geometries. (arXiv:2310.14948v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#29289;&#29702;&#20449;&#24687;&#22270;&#21367;&#31215;&#32593;&#32476;&#20316;&#20026;&#35299;&#20915;&#22797;&#26434;&#20960;&#20309;&#20307;&#20013;&#20559;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;&#30340;&#24191;&#20041;&#26694;&#26550;&#65292;&#24182;&#32467;&#21512;&#32463;&#20856;&#25968;&#20540;&#27714;&#35299;&#22120;&#20174;&#32780;&#35299;&#20915;&#20102;&#29289;&#29702;&#20449;&#24687;&#26694;&#26550;&#22312;&#22797;&#26434;&#20960;&#20309;&#20307;&#19978;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;[9]&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;&#20197;&#21450;&#20182;&#20204;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#20043;&#21518;&#65292;&#24050;&#32463;&#26377;&#24456;&#22810;&#24037;&#20316;&#33268;&#21147;&#20110;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#20363;&#22914;&#23558;&#36825;&#20123;&#27169;&#22411;&#25193;&#23637;&#21040;&#22797;&#26434;&#30340;&#19977;&#32500;&#20960;&#20309;&#20307;&#65292;&#20197;&#21450;&#22914;&#20309;&#23558;&#36825;&#20123;&#26041;&#27861;&#19982;&#32463;&#20856;&#30340;&#25968;&#20540;&#27714;&#35299;&#22120;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#30740;&#31350;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#20256;&#32479;&#25968;&#20540;&#25216;&#26415;&#20013;&#29992;&#20110;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#32593;&#26684;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#35777;&#26126;&#20102;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36825;&#20123;&#38382;&#39064;&#30340;&#21512;&#29702;&#24615;&#12290;&#22312;&#35777;&#26126;&#20102;&#29289;&#29702;&#20449;&#24687;&#26694;&#26550;&#22312;&#22797;&#26434;&#20960;&#20309;&#20307;&#19978;&#35745;&#31639;PDE&#27531;&#24046;&#26102;&#23384;&#22312;&#38382;&#39064;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#65292;&#23558;&#32463;&#20856;&#30340;&#25968;&#20540;&#27714;&#35299;&#22120;&#19982;&#29289;&#29702;&#20449;&#24687;&#26694;&#26550;&#30456;&#32467;&#21512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#35813;&#26041;&#27861;&#30340;&#26041;&#26696;&#65292;&#24182;&#22312;&#19968;&#20010;&#19981;&#35268;&#21017;&#20960;&#20309;&#20307;&#19978;&#30340;&#19977;&#32500;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the seminal work of [9] and their Physics-Informed neural networks (PINNs), many efforts have been conducted towards solving partial differential equations (PDEs) with Deep Learning models. However, some challenges remain, for instance the extension of such models to complex three-dimensional geometries, and a study on how such approaches could be combined to classical numerical solvers. In this work, we justify the use of graph neural networks for these problems, based on the similarity between these architectures and the meshes used in traditional numerical techniques for solving partial differential equations. After proving an issue with the Physics-Informed framework for complex geometries, during the computation of PDE residuals, an alternative procedure is proposed, by combining classical numerical solvers and the Physics-Informed framework. Finally, we propose an implementation of this approach, that we test on a three-dimensional problem on an irregular geometry.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;AI&#21453;&#39304;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#65288;QDAIF&#65289;&#31639;&#27861;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#21644;&#35780;&#20272;&#21019;&#36896;&#24615;&#20889;&#20316;&#65292;&#27604;&#20256;&#32479;&#31639;&#27861;&#26356;&#24191;&#27867;&#22320;&#35206;&#30422;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2310.13032</link><description>&lt;p&gt;
AI&#21453;&#39304;&#20419;&#36827;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Quality-Diversity through AI Feedback. (arXiv:2310.13032v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13032
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;AI&#21453;&#39304;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#65288;QDAIF&#65289;&#31639;&#27861;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#21644;&#35780;&#20272;&#21019;&#36896;&#24615;&#20889;&#20316;&#65292;&#27604;&#20256;&#32479;&#31639;&#27861;&#26356;&#24191;&#27867;&#22320;&#35206;&#30422;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#20013;&#65292;&#29992;&#25143;&#21487;&#33021;&#19981;&#20165;&#20559;&#22909;&#21333;&#19968;&#22238;&#22797;&#65292;&#32780;&#26159;&#24076;&#26395;&#24471;&#21040;&#22810;&#26679;&#24615;&#30340;&#39640;&#36136;&#37327;&#36755;&#20986;&#20197;&#20379;&#36873;&#25321;&#12290;&#36136;&#37327;-&#22810;&#26679;&#24615;&#65288;QD&#65289;&#25628;&#32034;&#31639;&#27861;&#26088;&#22312;&#36890;&#36807;&#19981;&#26029;&#25913;&#36827;&#21644;&#22810;&#26679;&#21270;&#20505;&#36873;&#20154;&#32676;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;QD&#22312;&#21019;&#20316;&#24615;&#20889;&#20316;&#31561;&#36136;&#24615;&#39046;&#22495;&#30340;&#24212;&#29992;&#21463;&#21040;&#31639;&#27861;&#25351;&#23450;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#24230;&#37327;&#30340;&#22256;&#38590;&#30340;&#38480;&#21046;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#21457;&#23637;&#20351;&#24471;&#36890;&#36807;AI&#21453;&#39304;&#25351;&#23548;&#25628;&#32034;&#25104;&#20026;&#21487;&#33021;&#65292;&#20854;&#20013;LMs&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#34987;&#25552;&#31034;&#26469;&#35780;&#20272;&#25991;&#26412;&#30340;&#36136;&#24615;&#26041;&#38754;&#12290;&#20511;&#21161;&#36825;&#19968;&#36827;&#23637;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36890;&#36807;AI&#21453;&#39304;&#23454;&#29616;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#31639;&#27861;&#65288;QDAIF&#65289;&#65292;&#20854;&#20013;&#36827;&#21270;&#31639;&#27861;&#24212;&#29992;LMs&#26469;&#29983;&#25104;&#21464;&#24322;&#24182;&#35780;&#20272;&#20505;&#36873;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;&#22312;&#21019;&#20316;&#24615;&#20889;&#20316;&#39046;&#22495;&#30340;&#35780;&#20272;&#20013;&#65292;&#19982;&#38750;QDAIF&#31639;&#27861;&#30456;&#27604;&#65292;QDAIF&#26356;&#24191;&#27867;&#22320;&#35206;&#30422;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25351;&#23450;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many text-generation problems, users may prefer not only a single response, but a diverse range of high-quality outputs from which to choose. Quality-diversity (QD) search algorithms aim at such outcomes, by continually improving and diversifying a population of candidates. However, the applicability of QD to qualitative domains, like creative writing, has been limited by the difficulty of algorithmically specifying measures of quality and diversity. Interestingly, recent developments in language models (LMs) have enabled guiding search through AI feedback, wherein LMs are prompted in natural language to evaluate qualitative aspects of text. Leveraging this development, we introduce Quality-Diversity through AI Feedback (QDAIF), wherein an evolutionary algorithm applies LMs to both generate variation and evaluate the quality and diversity of candidate text. When assessed on creative writing domains, QDAIF covers more of a specified search space with high-quality samples than do non-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#30340;&#38382;&#39064;&#22238;&#31572;&#36827;&#34892;&#20102;&#33539;&#22260;&#22238;&#39038;&#12290;&#19982;&#20854;&#20182;&#21307;&#23398;QA&#20219;&#21153;&#19981;&#21516;&#65292;EHR QA&#36890;&#36807;&#20174;&#24739;&#32773;&#30340;&#21307;&#30103;&#35760;&#24405;&#20013;&#33719;&#21462;&#31572;&#26696;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#29616;&#26377;&#30340;EHR QA&#20316;&#21697;&#25552;&#20379;&#20102;&#26041;&#27861;&#35770;&#22238;&#39038;&#12290;</title><link>http://arxiv.org/abs/2310.08759</link><description>&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#38382;&#39064;&#22238;&#31572;&#65306;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#30340;&#33539;&#22260;&#22238;&#39038;
&lt;/p&gt;
&lt;p&gt;
Question Answering for Electronic Health Records: A Scoping Review of datasets and models. (arXiv:2310.08759v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#30340;&#38382;&#39064;&#22238;&#31572;&#36827;&#34892;&#20102;&#33539;&#22260;&#22238;&#39038;&#12290;&#19982;&#20854;&#20182;&#21307;&#23398;QA&#20219;&#21153;&#19981;&#21516;&#65292;EHR QA&#36890;&#36807;&#20174;&#24739;&#32773;&#30340;&#21307;&#30103;&#35760;&#24405;&#20013;&#33719;&#21462;&#31572;&#26696;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#29616;&#26377;&#30340;EHR QA&#20316;&#21697;&#25552;&#20379;&#20102;&#26041;&#27861;&#35770;&#22238;&#39038;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#24739;&#32773;&#30456;&#20851;&#30340;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#31995;&#32479;&#21487;&#20197;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#21644;&#24739;&#32773;&#12290;&#23427;&#20204;&#21487;&#20197;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#20570;&#20915;&#31574;&#65292;&#24182;&#20351;&#24739;&#32773;&#26356;&#22909;&#22320;&#20102;&#35299;&#20182;&#20204;&#30340;&#30149;&#21382;&#12290;&#22823;&#37327;&#30340;&#24739;&#32773;&#25968;&#25454;&#23384;&#20648;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#65292;&#20351;&#24471;EHR QA&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#22312;EHR QA&#20013;&#65292;&#31572;&#26696;&#26159;&#20174;&#24739;&#32773;&#30340;&#21307;&#30103;&#35760;&#24405;&#20013;&#33719;&#24471;&#30340;&#12290;&#30001;&#20110;&#25968;&#25454;&#26684;&#24335;&#21644;&#27169;&#24335;&#30340;&#24046;&#24322;&#65292;&#36825;&#19982;&#20854;&#20182;&#20351;&#29992;&#21307;&#23398;&#32593;&#31449;&#25110;&#31185;&#23398;&#35770;&#25991;&#26816;&#32034;&#31572;&#26696;&#30340;&#21307;&#23398;QA&#20219;&#21153;&#26377;&#24456;&#22823;&#30340;&#19981;&#21516;&#65292;&#36825;&#20351;&#24471;&#30740;&#31350;EHR&#38382;&#39064;&#22238;&#31572;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#23545;&#29616;&#26377;&#20851;&#20110;EHR QA&#30340;&#20316;&#21697;&#36827;&#34892;&#26041;&#27861;&#35770;&#22238;&#39038;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;Google Scholar&#12289;ACL Anthology&#12289;ACM Digital Library&#21644;PubMed&#22312;&#20869;&#30340;&#22235;&#20010;&#25968;&#23383;&#36164;&#28304;&#20013;&#25628;&#32034;&#20102;&#20174;2005&#24180;1&#26376;1&#26085;&#21040;2023&#24180;9&#26376;30&#26085;&#30340;&#25991;&#31456;&#65292;&#20197;&#25910;&#38598;&#26377;&#20851;EHR QA&#30340;&#30456;&#20851;&#20986;&#29256;&#29289;&#12290;&#20849;&#21457;&#29616;&#20102;4111&#31687;&#35770;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question Answering (QA) systems on patient-related data can assist both clinicians and patients. They can, for example, assist clinicians in decision-making and enable patients to have a better understanding of their medical history. Significant amounts of patient data are stored in Electronic Health Records (EHRs), making EHR QA an important research area. In EHR QA, the answer is obtained from the medical record of the patient. Because of the differences in data format and modality, this differs greatly from other medical QA tasks that employ medical websites or scientific papers to retrieve answers, making it critical to research EHR question answering. This study aimed to provide a methodological review of existing works on QA over EHRs. We searched for articles from January 1st, 2005 to September 30th, 2023 in four digital sources including Google Scholar, ACL Anthology, ACM Digital Library, and PubMed to collect relevant publications on EHR QA. 4111 papers were identified for our
&lt;/p&gt;</description></item><item><title>LGL-BCI&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#20960;&#20309;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22788;&#29702;EEG&#25968;&#25454;&#22312;&#38750;&#27431;&#20960;&#37324;&#24503;&#24230;&#37327;&#31354;&#38388;&#20013;&#25429;&#25417;&#36816;&#21160;&#24819;&#35937;&#20219;&#21153;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#20998;&#35299;&#31639;&#27861;&#36827;&#34892;EEG&#36890;&#36947;&#36873;&#25321;&#20197;&#25552;&#39640;&#25512;&#26029;&#36895;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;LGL-BCI&#30456;&#27604;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.08051</link><description>&lt;p&gt;
LGL-BCI&#65306;&#19968;&#31181;&#36731;&#37327;&#32423;&#20960;&#20309;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#22522;&#20110;&#36816;&#21160;&#24819;&#35937;&#30340;&#33041;&#26426;&#25509;&#21475;
&lt;/p&gt;
&lt;p&gt;
LGL-BCI: A Lightweight Geometric Learning Framework for Motor Imagery-Based Brain-Computer Interfaces. (arXiv:2310.08051v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08051
&lt;/p&gt;
&lt;p&gt;
LGL-BCI&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#20960;&#20309;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22788;&#29702;EEG&#25968;&#25454;&#22312;&#38750;&#27431;&#20960;&#37324;&#24503;&#24230;&#37327;&#31354;&#38388;&#20013;&#25429;&#25417;&#36816;&#21160;&#24819;&#35937;&#20219;&#21153;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#20998;&#35299;&#31639;&#27861;&#36827;&#34892;EEG&#36890;&#36947;&#36873;&#25321;&#20197;&#25552;&#39640;&#25512;&#26029;&#36895;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;LGL-BCI&#30456;&#27604;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#26426;&#25509;&#21475;&#26159;&#19968;&#31181;&#20351;&#29992;&#33041;&#20449;&#21495;&#19982;&#22806;&#37096;&#35774;&#22791;&#36827;&#34892;&#20132;&#20114;&#30340;&#24320;&#21019;&#24615;&#25216;&#26415;&#12290;&#23613;&#31649;&#26377;&#25152;&#36827;&#23637;&#65292;&#22522;&#20110;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#30340;&#36816;&#21160;&#24819;&#35937;&#20219;&#21153;&#38754;&#20020;&#25361;&#25112;&#65292;&#22914;&#24133;&#24230;&#21644;&#30456;&#20301;&#21464;&#24322;&#65292;&#20197;&#21450;&#22797;&#26434;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#38656;&#35201;&#26356;&#23567;&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#26356;&#24555;&#30340;&#25512;&#26029;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;LGL-BCI&#26694;&#26550;&#65292;&#37319;&#29992;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#22788;&#29702;&#38750;&#27431;&#20960;&#37324;&#24503;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;EEG&#65292;&#29305;&#21035;&#26159;&#23545;&#31216;&#27491;&#23450;&#65288;SPD&#65289;&#27969;&#24418;&#31354;&#38388;&#12290;LGL-BCI&#25552;&#20379;&#20102;&#31283;&#20581;&#30340;EEG&#25968;&#25454;&#34920;&#31034;&#65292;&#24182;&#25429;&#25417;&#20102;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29305;&#24449;&#20998;&#35299;&#31639;&#27861;&#36827;&#34892;EEG&#36890;&#36947;&#36873;&#25321;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#20943;&#23569;SPD&#30697;&#38453;&#30340;&#32500;&#24230;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#25512;&#26029;&#36895;&#24230;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#19982;&#24403;&#21069;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65292;LGL-BCI&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#31361;&#20986;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#22312;&#36816;&#21160;&#24819;&#35937;-&#33041;&#26426;&#25509;&#21475;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain-Computer Interfaces (BCIs) are a groundbreaking technology for interacting with external devices using brain signals. Despite advancements, electroencephalogram (EEG)-based Motor Imagery (MI) tasks face challenges like amplitude and phase variability, and complex spatial correlations, with a need for smaller model size and faster inference. This study introduces the LGL-BCI framework, employing a Geometric Deep Learning Framework for EEG processing in non-Euclidean metric spaces, particularly the Symmetric Positive Definite (SPD) Manifold space. LGL-BCI offers robust EEG data representation and captures spatial correlations. We propose an EEG channel selection solution via a feature decomposition algorithm to reduce SPD matrix dimensionality, with a lossless transformation boosting inference speed. Extensive experiments show LGL-BCI's superior accuracy and efficiency compared to current solutions, highlighting geometric deep learning's potential in MI-BCI applications. The effici
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#39640;&#36125;&#21494;&#26031;&#20248;&#21270;&#27169;&#22411;&#24314;&#27169;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#26426;&#21046;&#34701;&#20837;&#28145;&#24230;&#26680;&#23398;&#20064;&#20013;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#36866;&#24212;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03912</link><description>&lt;p&gt;
RTDK-BO&#65306;&#20855;&#26377;Reinforced Transformer&#28145;&#24230;&#26680;&#20989;&#25968;&#30340;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
RTDK-BO: High Dimensional Bayesian Optimization with Reinforced Transformer Deep kernels. (arXiv:2310.03912v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#39640;&#36125;&#21494;&#26031;&#20248;&#21270;&#27169;&#22411;&#24314;&#27169;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#26426;&#21046;&#34701;&#20837;&#28145;&#24230;&#26680;&#23398;&#20064;&#20013;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#36866;&#24212;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#36890;&#36807;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#20195;&#29702;&#25351;&#23548;&#65292;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#23545;&#20110;&#39640;&#32500;&#40657;&#30418;&#20248;&#21270;&#38750;&#24120;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;&#22312;&#24037;&#19994;&#35774;&#35745;&#21644;&#31185;&#23398;&#35745;&#31639;&#31561;&#35768;&#22810;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#21333;&#20989;&#25968;&#20248;&#21270;&#21644;&#23569;&#26679;&#26412;&#22810;&#30446;&#26631;&#20248;&#21270;&#19978;&#24341;&#20837;&#20102;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26469;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#23569;&#26679;&#26412;&#25216;&#26415;&#20063;&#19981;&#33021;&#20805;&#20998;&#21033;&#29992;&#32039;&#23494;&#30456;&#20851;&#30446;&#26631;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#26412;&#25991;&#32467;&#21512;&#20102;&#28145;&#24230;&#26680;&#23398;&#20064;&#65288;DKL&#65289;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;Transformer&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25913;&#36827;&#20102;GP&#20195;&#29702;&#30340;&#24314;&#27169;&#33021;&#21147;&#19982;&#20803;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#26426;&#21046;&#34701;&#20837;DKL&#20013;&#26469;&#25913;&#36827;&#20803;&#23398;&#20064;BO&#20195;&#29702;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#22312;BO&#36807;&#31243;&#20013;&#36866;&#24212;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;Transformer&#28145;&#24230;&#26680;&#26041;&#27861;&#19982;&#23569;&#26679;&#26412;&#20803;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#26469;&#25552;&#39640;BO&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization (BO), guided by Gaussian process (GP) surrogates, has proven to be an invaluable technique for efficient, high-dimensional, black-box optimization, a critical problem inherent to many applications such as industrial design and scientific computing. Recent contributions have introduced reinforcement learning (RL) to improve the optimization performance on both single function optimization and \textit{few-shot} multi-objective optimization. However, even few-shot techniques fail to exploit similarities shared between closely related objectives. In this paper, we combine recent developments in Deep Kernel Learning (DKL) and attention-based Transformer models to improve the modeling powers of GP surrogates with meta-learning. We propose a novel method for improving meta-learning BO surrogates by incorporating attention mechanisms into DKL, empowering the surrogates to adapt to contextual information gathered during the BO process. We combine this Transformer Deep Kern
&lt;/p&gt;</description></item><item><title>PyDCM&#26159;&#19968;&#20010;&#29992;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#30340;&#21487;&#23450;&#21046;&#30340;&#25968;&#25454;&#20013;&#24515;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#23450;&#20041;&#37197;&#32622;&#21644;&#21521;&#37327;&#21270;&#30340;&#28909;&#35745;&#31639;&#65292;&#23454;&#29616;&#20102;&#23545;&#25968;&#25454;&#20013;&#24515;&#30340;&#20248;&#21270;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.03906</link><description>&lt;p&gt;
&#29992;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#30340;PyDCM&#65306;&#20026;&#21487;&#25345;&#32493;&#24615;&#23450;&#21046;&#25968;&#25454;&#20013;&#24515;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PyDCM: Custom Data Center Models with Reinforcement Learning for Sustainability. (arXiv:2310.03906v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03906
&lt;/p&gt;
&lt;p&gt;
PyDCM&#26159;&#19968;&#20010;&#29992;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#30340;&#21487;&#23450;&#21046;&#30340;&#25968;&#25454;&#20013;&#24515;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#23450;&#20041;&#37197;&#32622;&#21644;&#21521;&#37327;&#21270;&#30340;&#28909;&#35745;&#31639;&#65292;&#23454;&#29616;&#20102;&#23545;&#25968;&#25454;&#20013;&#24515;&#30340;&#20248;&#21270;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#23545;&#21487;&#25345;&#32493;&#24615;&#21644;&#20943;&#23569;&#30899;&#25490;&#25918;&#30340;&#24378;&#35843;&#26085;&#30410;&#22686;&#21152;&#65292;&#20419;&#20351;&#25919;&#24220;&#21644;&#20225;&#19994;&#37325;&#26032;&#24605;&#32771;&#25968;&#25454;&#20013;&#24515;&#30340;&#35774;&#35745;&#21644;&#36816;&#33829;&#26041;&#27861;&#12290;&#37492;&#20110;&#25968;&#25454;&#20013;&#24515;&#30340;&#39640;&#33021;&#32791;&#21644;&#25351;&#25968;&#32423;&#35745;&#31639;&#24037;&#20316;&#37327;&#65292;&#20248;&#21270;&#33021;&#32791;&#29305;&#21035;&#26159;&#22312;&#20919;&#21364;&#21644;IT&#33021;&#28304;&#20351;&#29992;&#26041;&#38754;&#65292;&#25968;&#25454;&#20013;&#24515;&#26159;&#20248;&#21270;&#30005;&#21147;&#28040;&#32791;&#30340;&#29702;&#24819;&#20505;&#36873;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#32570;&#20047;&#21487;&#37197;&#32622;&#21644;&#21487;&#25193;&#23637;&#30340;&#28909;&#25968;&#25454;&#20013;&#24515;&#27169;&#22411;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31649;&#36947;&#12290;&#25968;&#25454;&#20013;&#24515;&#30001;&#22810;&#20010;IT&#32452;&#20214;&#32452;&#25104;&#65292;&#20854;&#20960;&#20309;&#37197;&#32622;&#21644;&#25955;&#28909;&#20351;&#24471;&#28909;&#24314;&#27169;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PyDCM&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;Python&#23454;&#29616;&#30340;&#21487;&#23450;&#21046;&#30340;&#25968;&#25454;&#20013;&#24515;&#27169;&#22411;&#65292;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#33258;&#23450;&#20041;&#30340;&#26381;&#21153;&#22120;&#35268;&#26684;&#21644;IT&#26426;&#26588;&#30340;&#20960;&#20309;&#24067;&#32622;&#21019;&#24314;&#29420;&#29305;&#30340;&#37197;&#32622;&#12290;&#20351;&#29992;&#21521;&#37327;&#21270;&#30340;&#28909;&#35745;&#31639;&#20351;&#24471;PyDCM&#27604;&#24403;&#21069;&#26041;&#27861;&#24555;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#65288;30&#20493;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing global emphasis on sustainability and reducing carbon emissions is pushing governments and corporations to rethink their approach to data center design and operation. Given their high energy consumption and exponentially large computational workloads, data centers are prime candidates for optimizing power consumption, especially in areas such as cooling and IT energy usage. A significant challenge in this pursuit is the lack of a configurable and scalable thermal data center model that offers an end-to-end pipeline. Data centers consist of multiple IT components whose geometric configuration and heat dissipation make thermal modeling difficult. This paper presents PyDCM, a customizable Data Center Model implemented in Python, that allows users to create unique configurations of IT equipment with custom server specifications and geometric arrangements of IT cabinets. The use of vectorized thermal calculations makes PyDCM orders of magnitude faster (30 times) than current 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#23436;&#20840;&#36830;&#25509;&#30340;ReLU&#23618;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#27599;&#20010;&#21010;&#20998;&#21306;&#22495;&#20869;&#65292;ReLU&#23618;&#21487;&#20197;&#34987;&#22823;&#22823;&#31616;&#21270;&#65292;&#21487;&#20197;&#23558;&#20854;&#35299;&#37322;&#20026;&#19968;&#20010;&#25237;&#24433;&#21040;&#22810;&#38754;&#20307;&#38181;&#20307;&#65292;&#28982;&#21518;&#36827;&#34892;&#20223;&#23556;&#21464;&#25442;&#12290;&#27492;&#32467;&#26500;&#36824;&#31616;&#21270;&#20102;&#20998;&#21306;&#21306;&#22495;&#19982;&#36229;&#24179;&#38754;&#20132;&#38598;&#30340;&#21453;&#20687;&#34920;&#36798;&#24335;&#65292;&#23545;&#20110;&#25551;&#36848;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#20915;&#31574;&#36793;&#30028;&#38750;&#24120;&#26377;&#29992;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#20855;&#26377;&#19968;&#20010;&#38544;&#34255;ReLU&#23618;&#30340;&#21069;&#39304;&#32593;&#32476;&#65292;&#35770;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#36825;&#20123;&#32593;&#32476;&#29983;&#25104;&#30340;&#20915;&#31574;&#36793;&#30028;&#20960;&#20309;&#22797;&#26434;&#24615;&#30340;&#32467;&#26524;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#20915;&#31574;&#36793;&#30028;&#22312;&#20223;&#23556;&#21464;&#25442;&#30340;&#27169;&#19979;&#26159;&#30456;&#31561;&#30340;&#12290;</title><link>http://arxiv.org/abs/2310.03482</link><description>&lt;p&gt;
&#23436;&#20840;&#36830;&#25509;&#30340;ReLU&#23618;&#30340;&#20960;&#20309;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
The Geometric Structure of Fully-Connected ReLU-Layers. (arXiv:2310.03482v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03482
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#23436;&#20840;&#36830;&#25509;&#30340;ReLU&#23618;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#27599;&#20010;&#21010;&#20998;&#21306;&#22495;&#20869;&#65292;ReLU&#23618;&#21487;&#20197;&#34987;&#22823;&#22823;&#31616;&#21270;&#65292;&#21487;&#20197;&#23558;&#20854;&#35299;&#37322;&#20026;&#19968;&#20010;&#25237;&#24433;&#21040;&#22810;&#38754;&#20307;&#38181;&#20307;&#65292;&#28982;&#21518;&#36827;&#34892;&#20223;&#23556;&#21464;&#25442;&#12290;&#27492;&#32467;&#26500;&#36824;&#31616;&#21270;&#20102;&#20998;&#21306;&#21306;&#22495;&#19982;&#36229;&#24179;&#38754;&#20132;&#38598;&#30340;&#21453;&#20687;&#34920;&#36798;&#24335;&#65292;&#23545;&#20110;&#25551;&#36848;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#20915;&#31574;&#36793;&#30028;&#38750;&#24120;&#26377;&#29992;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#20855;&#26377;&#19968;&#20010;&#38544;&#34255;ReLU&#23618;&#30340;&#21069;&#39304;&#32593;&#32476;&#65292;&#35770;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#36825;&#20123;&#32593;&#32476;&#29983;&#25104;&#30340;&#20915;&#31574;&#36793;&#30028;&#20960;&#20309;&#22797;&#26434;&#24615;&#30340;&#32467;&#26524;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#20915;&#31574;&#36793;&#30028;&#22312;&#20223;&#23556;&#21464;&#25442;&#30340;&#27169;&#19979;&#26159;&#30456;&#31561;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#31070;&#32463;&#32593;&#32476;&#20013;d&#32500;&#23436;&#20840;&#36830;&#25509;&#30340;ReLU&#23618;&#30340;&#20960;&#20309;&#32467;&#26500;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#21644;&#35299;&#37322;&#12290;ReLU&#23618;&#30340;&#21442;&#25968;&#20250;&#24341;&#23548;&#36755;&#20837;&#22495;&#30340;&#33258;&#28982;&#21010;&#20998;&#65292;&#20351;&#24471;&#22312;&#21010;&#20998;&#30340;&#27599;&#20010;&#21306;&#22495;&#20869;&#65292;ReLU&#23618;&#21487;&#20197;&#34987;&#22823;&#22823;&#31616;&#21270;&#12290;&#36825;&#23548;&#33268;&#20102;&#23558;ReLU&#23618;&#35299;&#37322;&#20026;&#19968;&#20010;&#25237;&#24433;&#21040;&#22810;&#38754;&#20307;&#38181;&#20307;&#65292;&#28982;&#21518;&#36827;&#34892;&#20223;&#23556;&#21464;&#25442;&#30340;&#20960;&#20309;&#35299;&#37322;&#65292;&#19982;&#22312;&#20855;&#26377;ReLU&#28608;&#27963;&#30340;&#21367;&#31215;&#32593;&#32476;&#20013;&#30340;&#25551;&#36848;&#19968;&#33268;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#32467;&#26500;&#20415;&#20110;&#31616;&#21270;&#20998;&#21306;&#21306;&#22495;&#19982;&#36229;&#24179;&#38754;&#20132;&#38598;&#30340;&#21453;&#20687;&#34920;&#36798;&#24335;&#65292;&#36825;&#22312;&#25551;&#36848;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#20915;&#31574;&#36793;&#30028;&#26102;&#38750;&#24120;&#26377;&#29992;&#12290;&#25105;&#20204;&#35814;&#32454;&#30740;&#31350;&#20102;&#20855;&#26377;&#19968;&#20010;&#38544;&#34255;ReLU&#23618;&#30340;&#21069;&#39304;&#32593;&#32476;&#65292;&#22312;&#36825;&#20010;&#32593;&#32476;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#36825;&#20123;&#32593;&#32476;&#29983;&#25104;&#30340;&#20915;&#31574;&#36793;&#30028;&#30340;&#20960;&#20309;&#22797;&#26434;&#24615;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#35777;&#26126;&#22312;&#20223;&#23556;&#21464;&#25442;&#30340;&#27169;&#19979;&#65292;&#36825;&#20123;&#20915;&#31574;&#36793;&#30028;&#30456;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
We formalize and interpret the geometric structure of $d$-dimensional fully connected ReLU-layers in neural networks. The parameters of a ReLU-layer induce a natural partition of the input domain, such that in each sector of the partition, the ReLU-layer can be greatly simplified. This leads to a geometric interpretation of a ReLU-layer as a projection onto a polyhedral cone followed by an affine transformation, in line with the description in [doi:10.48550/arXiv.1905.08922] for convolutional networks with ReLU activations. Further, this structure facilitates simplified expressions for preimages of the intersection between partition sectors and hyperplanes, which is useful when describing decision boundaries in a classification setting. We investigate this in detail for a feed-forward network with one hidden ReLU-layer, where we provide results on the geometric complexity of the decision boundary generated by such networks, as well as proving that modulo an affine transformation, such 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;PB-LLM&#26159;&#19968;&#31181;&#37096;&#20998;&#20108;&#20540;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#30340;&#21516;&#26102;&#23454;&#29616;&#26497;&#20302;&#27604;&#29305;&#37327;&#21270;&#65292;&#24182;&#36890;&#36807;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31561;&#26041;&#27861;&#24674;&#22797;&#37327;&#21270;LLMM&#30340;&#23481;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.00034</link><description>&lt;p&gt;
PB-LLM: &#37096;&#20998;&#20108;&#20540;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PB-LLM: Partially Binarized Large Language Models. (arXiv:2310.00034v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;PB-LLM&#26159;&#19968;&#31181;&#37096;&#20998;&#20108;&#20540;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#30340;&#21516;&#26102;&#23454;&#29616;&#26497;&#20302;&#27604;&#29305;&#37327;&#21270;&#65292;&#24182;&#36890;&#36807;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31561;&#26041;&#27861;&#24674;&#22797;&#37327;&#21270;LLMM&#30340;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#32593;&#32476;&#20108;&#20540;&#21270;&#65292;&#19968;&#31181;&#21387;&#32553;&#27169;&#22411;&#26435;&#37325;&#20026;&#21333;&#20010;&#27604;&#29305;&#30340;&#37327;&#21270;&#30340;&#28608;&#36827;&#24418;&#24335;&#65292;&#19987;&#38376;&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21387;&#32553;&#12290;&#30001;&#20110;&#20043;&#21069;&#30340;&#20108;&#20540;&#21270;&#26041;&#27861;&#20250;&#23548;&#33268;LLMs&#23849;&#28291;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#37096;&#20998;&#20108;&#20540;&#21270;LLM&#65288;PB-LLM&#65289;&#65292;&#21487;&#20197;&#23454;&#29616;&#26497;&#20302;&#27604;&#29305;&#37327;&#21270;&#65292;&#24182;&#21516;&#26102;&#20445;&#25345;&#37327;&#21270;LLMs&#30340;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#39318;&#20808;&#25581;&#31034;&#20102;&#29616;&#26377;&#20108;&#20540;&#21270;&#31639;&#27861;&#30340;&#21407;&#29983;&#24212;&#29992;&#30340;&#26080;&#25928;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#26174;&#33879;&#26435;&#37325;&#22312;&#23454;&#29616;&#20302;&#20301;&#37327;&#21270;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;PB-LLM&#22312;&#20108;&#36827;&#21046;&#21270;&#36807;&#31243;&#20013;&#36807;&#28388;&#20102;&#19968;&#23567;&#37096;&#20998;&#26174;&#33879;&#26435;&#37325;&#65292;&#23558;&#23427;&#20204;&#20998;&#37197;&#21040;&#39640;&#20301;&#23384;&#20648;&#20013;&#65292;&#21363;&#37096;&#20998;&#20108;&#20540;&#21270;&#12290;PB-LLM&#22312;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#21644;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65288;QAT&#65289;&#30340;&#35282;&#24230;&#20998;&#26512;&#21518;&#65292;&#25193;&#23637;&#20102;&#24674;&#22797;&#37327;&#21270;LLMM&#23481;&#37327;&#30340;&#33021;&#21147;&#12290;&#22312;PTQ&#19979;&#65292;&#32467;&#21512;&#20102;GPTQ&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#37325;&#26500;&#20102;...
&lt;/p&gt;
&lt;p&gt;
This paper explores network binarization, a radical form of quantization, compressing model weights to a single bit, specifically for Large Language Models (LLMs) compression. Due to previous binarization methods collapsing LLMs, we propose a novel approach, Partially-Binarized LLM (PB-LLM), which can achieve extreme low-bit quantization while maintaining the linguistic reasoning capacity of quantized LLMs. Specifically, our exploration first uncovers the ineffectiveness of naive applications of existing binarization algorithms and highlights the imperative role of salient weights in achieving low-bit quantization. Thus, PB-LLM filters a small ratio of salient weights during binarization, allocating them to higher-bit storage, i.e., partially-binarization. PB-LLM is extended to recover the capacities of quantized LMMs, by analyzing from the perspective of post-training quantization (PTQ) and quantization-aware training (QAT). Under PTQ, combining the concepts from GPTQ, we reconstruct 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#22270;&#20687;&#20998;&#21106;&#21644;&#33945;&#29305;&#21345;&#27931;&#36749;&#23398;&#26469;&#37327;&#21270;&#21980;&#37240;&#24615;&#31890;&#32454;&#32990;&#65292;&#20026;&#35786;&#26029;&#21980;&#37240;&#24615;&#39135;&#31649;&#28814;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#21644;&#27169;&#22411;&#24615;&#33021;&#21487;&#35270;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.16536</link><description>&lt;p&gt;
&#23545;&#21980;&#37240;&#24615;&#31890;&#32454;&#32990;&#20998;&#21106;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification for Eosinophil Segmentation. (arXiv:2309.16536v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#22270;&#20687;&#20998;&#21106;&#21644;&#33945;&#29305;&#21345;&#27931;&#36749;&#23398;&#26469;&#37327;&#21270;&#21980;&#37240;&#24615;&#31890;&#32454;&#32990;&#65292;&#20026;&#35786;&#26029;&#21980;&#37240;&#24615;&#39135;&#31649;&#28814;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#21644;&#27169;&#22411;&#24615;&#33021;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21980;&#37240;&#24615;&#39135;&#31649;&#28814;&#65288;EoE&#65289;&#26159;&#19968;&#31181;&#26085;&#30410;&#26222;&#21450;&#30340;&#36807;&#25935;&#24615;&#30142;&#30149;&#12290;&#20026;&#20102;&#35786;&#26029;EoE&#65292;&#30149;&#29702;&#23398;&#23478;&#24517;&#39035;&#22312;&#19968;&#20010;&#39640;&#20493;&#35270;&#22330;&#65288;400&#20493;&#25918;&#22823;&#29575;&#65289;&#20869;&#25214;&#21040;15&#20010;&#25110;&#26356;&#22810;&#30340;&#21980;&#37240;&#24615;&#31890;&#32454;&#32990;&#12290;&#30830;&#23450;&#19968;&#20010;&#24739;&#32773;&#26159;&#21542;&#24739;&#26377;EoE&#21487;&#20197;&#26159;&#19968;&#20010;&#33392;&#24040;&#30340;&#36807;&#31243;&#65292;&#24182;&#19988;&#29992;&#20110;&#36741;&#21161;&#35786;&#26029;&#30340;&#20219;&#20309;&#21307;&#23398;&#25104;&#20687;&#26041;&#27861;&#37117;&#24517;&#39035;&#32771;&#34385;&#25928;&#29575;&#21644;&#31934;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;Adorno&#31561;&#20154;&#23545;&#20351;&#29992;&#28145;&#24230;&#22270;&#20687;&#20998;&#21106;&#36827;&#34892;&#21980;&#37240;&#24615;&#31890;&#32454;&#32990;&#23450;&#37327;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#21033;&#29992;&#20102;&#33945;&#29305;&#21345;&#27931;&#36749;&#23398;&#65288;Monte Carlo Dropout&#65289;&#65292;&#36825;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#24120;&#29992;&#30340;&#19968;&#31181;&#20943;&#23569;&#36807;&#25311;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#24403;&#21069;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#19981;&#30830;&#23450;&#24615;&#21487;&#20197;&#22312;&#36755;&#20986;&#22270;&#20687;&#20013;&#21487;&#35270;&#21270;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#24037;&#20316;&#21407;&#29702;&#65292;&#24182;&#24110;&#21161;&#30149;&#29702;&#23398;&#23478;&#35782;&#21035;&#21980;&#37240;&#24615;&#31890;&#32454;&#32990;&#12290;
&lt;/p&gt;
&lt;p&gt;
Eosinophilic Esophagitis (EoE) is an allergic condition increasing in prevalence. To diagnose EoE, pathologists must find 15 or more eosinophils within a single high-power field (400X magnification). Determining whether or not a patient has EoE can be an arduous process and any medical imaging approaches used to assist diagnosis must consider both efficiency and precision. We propose an improvement of Adorno et al's approach for quantifying eosinphils using deep image segmentation. Our new approach leverages Monte Carlo Dropout, a common approach in deep learning to reduce overfitting, to provide uncertainty quantification on current deep learning models. The uncertainty can be visualized in an output image to evaluate model performance, provide insight to how deep learning algorithms function, and assist pathologists in identifying eosinophils.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnglE&#30340;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#26469;&#32531;&#35299;&#25991;&#26412;&#23884;&#20837;&#20013;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#36896;&#25104;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;STS&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#22312;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12871</link><description>&lt;p&gt;
&#35282;&#24230;&#20248;&#21270;&#30340;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
AnglE-Optimized Text Embeddings. (arXiv:2309.12871v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnglE&#30340;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#26469;&#32531;&#35299;&#25991;&#26412;&#23884;&#20837;&#20013;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#36896;&#25104;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;STS&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#22312;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#23545;&#20110;&#25552;&#21319;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#65288;STS&#65289;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#36825;&#20123;&#20219;&#21153;&#21448;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#38754;&#20020;&#30340;&#19968;&#20010;&#26222;&#36941;&#25361;&#25112;&#26159;&#28176;&#21464;&#28040;&#22833;&#38382;&#39064;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#23427;&#20204;&#22312;&#20248;&#21270;&#30446;&#26631;&#20013;&#20381;&#36182;&#20313;&#24358;&#20989;&#25968;&#65292;&#32780;&#20313;&#24358;&#20989;&#25968;&#20855;&#26377;&#39281;&#21644;&#21306;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;AnglE&#30340;&#26032;&#22411;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#12290;AnglE&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#19968;&#20010;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#20135;&#29983;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#20174;&#32780;&#21487;&#20197;&#38459;&#30861;&#26799;&#24230;&#24182;&#38459;&#30861;&#20248;&#21270;&#36807;&#31243;&#12290;&#20026;&#20102;&#24314;&#31435;&#20840;&#38754;&#30340;STS&#35780;&#20272;&#65292;&#25105;&#20204;&#22312;&#29616;&#26377;&#30340;&#30701;&#25991;&#26412;STS&#25968;&#25454;&#38598;&#21644;&#20174;GitHub Issues&#20013;&#26032;&#25910;&#38598;&#30340;&#38271;&#25991;&#26412;STS&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20855;&#26377;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#65292;&#24182;&#25506;&#35752;&#20102;AnglE&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-quality text embedding is pivotal in improving semantic textual similarity (STS) tasks, which are crucial components in Large Language Model (LLM) applications. However, a common challenge existing text embedding models face is the problem of vanishing gradients, primarily due to their reliance on the cosine function in the optimization objective, which has saturation zones. To address this issue, this paper proposes a novel angle-optimized text embedding model called AnglE. The core idea of AnglE is to introduce angle optimization in a complex space. This novel approach effectively mitigates the adverse effects of the saturation zone in the cosine function, which can impede gradient and hinder optimization processes. To set up a comprehensive STS evaluation, we experimented on existing short-text STS datasets and a newly collected long-text STS dataset from GitHub Issues. Furthermore, we examine domain-specific STS scenarios with limited labeled data and explore how AnglE works w
&lt;/p&gt;</description></item><item><title>PDFTriage&#26159;&#19968;&#31181;&#22788;&#29702;&#38271;&#31687;&#32467;&#26500;&#21270;&#25991;&#26723;&#38382;&#31572;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32467;&#26500;&#25110;&#20869;&#23481;&#26469;&#26816;&#32034;&#19978;&#19979;&#25991;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38382;&#31572;&#20013;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08872</link><description>&lt;p&gt;
PDFTriage: &#23545;&#38271;&#31687;&#32467;&#26500;&#21270;&#25991;&#26723;&#36827;&#34892;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
PDFTriage: Question Answering over Long, Structured Documents. (arXiv:2309.08872v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08872
&lt;/p&gt;
&lt;p&gt;
PDFTriage&#26159;&#19968;&#31181;&#22788;&#29702;&#38271;&#31687;&#32467;&#26500;&#21270;&#25991;&#26723;&#38382;&#31572;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32467;&#26500;&#25110;&#20869;&#23481;&#26469;&#26816;&#32034;&#19978;&#19979;&#25991;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38382;&#31572;&#20013;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#30340;&#38382;&#31572;&#26102;&#23384;&#22312;&#38382;&#39064;&#65292;&#22240;&#20026;&#25991;&#26723;&#26080;&#27861;&#36866;&#24212;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#38598;&#20013;&#20110;&#20174;&#25991;&#26723;&#20013;&#26816;&#32034;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#65292;&#24182;&#23558;&#20854;&#34920;&#31034;&#20026;&#32431;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#20687;PDF&#12289;&#32593;&#39029;&#21644;&#28436;&#31034;&#25991;&#31295;&#36825;&#26679;&#30340;&#25991;&#26723;&#26159;&#26377;&#32467;&#26500;&#30340;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#39029;&#30721;&#12289;&#34920;&#26684;&#12289;&#31456;&#33410;&#31561;&#12290;&#23558;&#36825;&#26679;&#30340;&#32467;&#26500;&#21270;&#25991;&#26723;&#34920;&#31034;&#20026;&#32431;&#25991;&#26412;&#19982;&#29992;&#25143;&#23545;&#36825;&#20123;&#20855;&#26377;&#20016;&#23500;&#32467;&#26500;&#30340;&#25991;&#26723;&#30340;&#35748;&#30693;&#27169;&#22411;&#19981;&#31526;&#12290;&#24403;&#31995;&#32479;&#38656;&#35201;&#20174;&#25991;&#26723;&#20013;&#26597;&#35810;&#19978;&#19979;&#25991;&#26102;&#65292;&#36825;&#31181;&#19981;&#31526;&#20250;&#26174;&#29616;&#20986;&#26469;&#65292;&#29978;&#33267;&#31616;&#21333;&#30340;&#38382;&#39064;&#20063;&#21487;&#33021;&#20351;&#38382;&#31572;&#31995;&#32479;&#20986;&#38169;&#12290;&#20026;&#20102;&#24357;&#21512;&#22788;&#29702;&#32467;&#26500;&#21270;&#25991;&#26723;&#20013;&#30340;&#22522;&#26412;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PDFTriage&#30340;&#26041;&#27861;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#32467;&#26500;&#25110;&#20869;&#23481;&#26816;&#32034;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;PDFTriage&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have issues with document question answering (QA) in situations where the document is unable to fit in the small context length of an LLM. To overcome this issue, most existing works focus on retrieving the relevant context from the document, representing them as plain text. However, documents such as PDFs, web pages, and presentations are naturally structured with different pages, tables, sections, and so on. Representing such structured documents as plain text is incongruous with the user's mental model of these documents with rich structure. When a system has to query the document for context, this incongruity is brought to the fore, and seemingly trivial questions can trip up the QA system. To bridge this fundamental gap in handling structured documents, we propose an approach called PDFTriage that enables models to retrieve the context based on either structure or content. Our experiments demonstrate the effectiveness of the proposed PDFTriage-augmente
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#20849;&#32447;&#32422;&#26463;&#27880;&#24847;&#21147;&#65288;CoCA&#65289;&#32467;&#26500;&#65292;&#35299;&#20915;Transformer&#27169;&#22411;&#20013;&#30340;&#22836;&#30171;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20986;&#33394;&#30340;&#22806;&#25512;&#24615;&#33021;&#21644;&#25552;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.08646</link><description>&lt;p&gt;
&#36890;&#36807;&#20849;&#32447;&#32422;&#26463;&#27880;&#24847;&#21147;&#35299;&#20915;Transformer&#30340;&#22836;&#30171;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Cure the headache of Transformers via Collinear Constrained Attention. (arXiv:2309.08646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08646
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#20849;&#32447;&#32422;&#26463;&#27880;&#24847;&#21147;&#65288;CoCA&#65289;&#32467;&#26500;&#65292;&#35299;&#20915;Transformer&#27169;&#22411;&#20013;&#30340;&#22836;&#30171;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20986;&#33394;&#30340;&#22806;&#25512;&#24615;&#33021;&#21644;&#25552;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#38469;&#24212;&#29992;&#30340;&#24555;&#36895;&#36827;&#23637;&#65292;&#25512;&#26029;&#24615;&#33021;&#30340;&#22806;&#25512;&#21464;&#24471;&#22312;&#30740;&#31350;&#39046;&#22495;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;Transformer&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#34987;&#20043;&#21069;&#24573;&#35270;&#30340;&#24322;&#24120;&#34892;&#20026;&#65292;&#23548;&#33268;&#20102;&#26368;&#25509;&#36817;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#28151;&#20081;&#65292;&#36825;&#20123;&#26631;&#35760;&#25658;&#24102;&#20102;&#26368;&#37325;&#35201;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#21457;&#29616;&#31216;&#20026;&#8220;Transformer&#30340;&#22836;&#30171;&#38382;&#39064;&#8221;&#12290;&#20026;&#20102;&#20174;&#26681;&#26412;&#19978;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#27880;&#24847;&#32467;&#26500;&#65292;&#21629;&#21517;&#20026;Collinear Constrained Attention&#65288;CoCA&#65289;&#12290;&#36825;&#20010;&#32467;&#26500;&#21487;&#20197;&#26080;&#32541;&#22320;&#19982;&#29616;&#26377;&#30340;&#25512;&#26029;&#12289;&#25554;&#20540;&#26041;&#27861;&#21644;&#20854;&#20182;&#38024;&#23545;&#20256;&#32479;Transformer&#27169;&#22411;&#35774;&#35745;&#30340;&#20248;&#21270;&#31574;&#30053;&#38598;&#25104;&#12290;&#25105;&#20204;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#22806;&#25512;&#24615;&#33021;&#65292;&#21363;&#20351;&#26159;16&#21040;24&#20493;&#30340;&#24207;&#21015;&#38271;&#24230;&#65292;&#32780;&#19988;&#27809;&#26377;&#23545;&#25105;&#20204;&#30340;&#27169;&#22411;&#36827;&#34892;&#20219;&#20309;&#24494;&#35843;&#12290;&#25105;&#20204;&#36824;&#22686;&#24378;&#20102;CoCA&#30340;&#35745;&#31639;&#21644;&#31354;&#38388;&#25928;&#29575;&#65292;&#20197;&#30830;&#20445;&#20854;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#35745;&#21010;...
&lt;/p&gt;
&lt;p&gt;
As the rapid progression of practical applications based on Large Language Models continues, the importance of extrapolating performance has grown exponentially in the research domain. In our study, we identified an anomalous behavior in Transformer models that had been previously overlooked, leading to a chaos around closest tokens which carried the most important information. We've coined this discovery the "headache of Transformers". To address this at its core, we introduced a novel self-attention structure named Collinear Constrained Attention (CoCA). This structure can be seamlessly integrated with existing extrapolation, interpolation methods, and other optimization strategies designed for traditional Transformer models. We have achieved excellent extrapolating performance even for 16 times to 24 times of sequence lengths during inference without any fine-tuning on our model. We have also enhanced CoCA's computational and spatial efficiency to ensure its practicality. We plan to
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;SECToR&#36890;&#36807;&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#25104;&#21151;&#22320;&#33258;&#23398;&#26032;&#25216;&#33021;&#65292;</title><link>http://arxiv.org/abs/2309.08589</link><description>&lt;p&gt;
&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#26159;&#19968;&#31181;&#31574;&#30053;&#25913;&#36827;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought Reasoning is a Policy Improvement Operator. (arXiv:2309.08589v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08589
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;SECToR&#36890;&#36807;&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#25104;&#21151;&#22320;&#33258;&#23398;&#26032;&#25216;&#33021;&#65292;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#20854;&#20196;&#20154;&#36190;&#21497;&#30340;&#26032;&#33021;&#21147;&#20196;&#19990;&#30028;&#20026;&#20043;&#24778;&#21497;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30446;&#21069;&#32570;&#20047;&#33258;&#25105;&#23398;&#20064;&#26032;&#25216;&#33021;&#30340;&#33021;&#21147;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#25509;&#21463;&#22823;&#37327;&#30001;&#20154;&#31867;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;SECToR&#65288;&#36890;&#36807;&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#23454;&#29616;&#33258;&#25105;&#25945;&#32946;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#65292;&#35777;&#26126;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#25104;&#21151;&#22320;&#33258;&#23398;&#26032;&#25216;&#33021;&#12290;&#21463;&#21040;&#20197;&#21069;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;Silver&#31561;&#20154;&#65292;2017&#65289;&#21644;&#20154;&#31867;&#35748;&#30693;&#65288;Kahneman&#65292;2011&#65289;&#20013;&#30340;&#30456;&#20851;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;SECToR&#39318;&#20808;&#20351;&#29992;&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#36880;&#28176;&#24605;&#32771;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;SECToR&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#29983;&#25104;&#30456;&#21516;&#30340;&#31572;&#26696;&#65292;&#36825;&#27425;&#19981;&#20877;&#20351;&#29992;&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#12290;&#36890;&#36807;SECToR&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#33258;&#20027;&#23398;&#20250;&#20102;&#36827;&#34892;&#22810;&#36798;29&#20301;&#25968;&#23383;&#30340;&#21152;&#27861;&#36816;&#31639;&#65292;&#32780;&#27809;&#26377;&#20219;&#20309;&#36229;&#36807;6&#20301;&#25968;&#23383;&#30340;&#22522;&#20934;&#30495;&#23454;&#31034;&#20363;&#65292;&#20165;&#36890;&#36807;&#21021;&#22987;&#30340;&#30417;&#30563;&#24494;&#35843;&#38454;&#27573;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#20551;&#35774;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Large language models have astounded the world with fascinating new capabilities. However, they currently lack the ability to teach themselves new skills, relying instead on being trained on large amounts of human-generated data. We introduce SECToR (Self-Education via Chain-of-Thought Reasoning), a proof-of-concept demonstration that language models can successfully teach themselves new skills using chain-of-thought reasoning. Inspired by previous work in both reinforcement learning (Silver et al., 2017) and human cognition (Kahneman, 2011), SECToR first uses chain-of-thought reasoning to slowly think its way through problems. SECToR then fine-tunes the model to generate those same answers, this time without using chain-of-thought reasoning. Language models trained via SECToR autonomously learn to add up to 29-digit numbers without any access to any ground truth examples beyond an initial supervised fine-tuning phase consisting only of numbers with 6 or fewer digits. Our central hypot
&lt;/p&gt;</description></item><item><title>Oobleck&#37319;&#29992;&#27969;&#27700;&#32447;&#27169;&#26495;&#21644;&#24050;&#22797;&#21046;&#27169;&#22411;&#29366;&#24577;&#26469;&#23454;&#29616;&#23545;&#22823;&#22411;&#27169;&#22411;&#30340;&#24377;&#24615;&#20998;&#24067;&#24335;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#36164;&#28304;&#21644;&#24555;&#36895;&#24674;&#22797;&#26469;&#25552;&#20379;&#39640;&#21534;&#21520;&#37327;&#12290;&#22312;&#35780;&#20272;&#20013;&#65292;Oobleck&#22312;&#21534;&#21520;&#37327;&#19978;&#32988;&#36807;&#20102;Bamboo&#21644;Varuna&#31561;&#26368;&#20808;&#36827;&#30340;&#23481;&#38169;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.08125</link><description>&lt;p&gt;
Oobleck&#65306;&#20351;&#29992;&#27969;&#27700;&#32447;&#27169;&#26495;&#23454;&#29616;&#22823;&#22411;&#27169;&#22411;&#30340;&#24377;&#24615;&#20998;&#24067;&#24335;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Oobleck: Resilient Distributed Training of Large Models Using Pipeline Templates. (arXiv:2309.08125v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08125
&lt;/p&gt;
&lt;p&gt;
Oobleck&#37319;&#29992;&#27969;&#27700;&#32447;&#27169;&#26495;&#21644;&#24050;&#22797;&#21046;&#27169;&#22411;&#29366;&#24577;&#26469;&#23454;&#29616;&#23545;&#22823;&#22411;&#27169;&#22411;&#30340;&#24377;&#24615;&#20998;&#24067;&#24335;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#36164;&#28304;&#21644;&#24555;&#36895;&#24674;&#22797;&#26469;&#25552;&#20379;&#39640;&#21534;&#21520;&#37327;&#12290;&#22312;&#35780;&#20272;&#20013;&#65292;Oobleck&#22312;&#21534;&#21520;&#37327;&#19978;&#32988;&#36807;&#20102;Bamboo&#21644;Varuna&#31561;&#26368;&#20808;&#36827;&#30340;&#23481;&#38169;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Oobleck&#36890;&#36807;&#37319;&#29992;&#35268;&#23450;&#30340;&#23481;&#38169;&#29575;&#65292;&#21487;&#23454;&#29616;&#23545;&#22823;&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#24377;&#24615;&#20998;&#24067;&#24335;&#35757;&#32451;&#12290;&#23427;&#37319;&#29992;&#20102;&#35268;&#21010;-&#25191;&#34892;&#30340;&#21327;&#21516;&#35774;&#35745;&#26041;&#27861;&#65292;&#39318;&#20808;&#29983;&#25104;&#19968;&#32452;&#24322;&#26500;&#30340;&#27969;&#27700;&#32447;&#27169;&#26495;&#65292;&#24182;&#23454;&#20363;&#21270;&#33267;&#23569;$ f + 1 $&#20010;&#36923;&#36753;&#31561;&#25928;&#30340;&#27969;&#27700;&#32447;&#21103;&#26412;&#65292;&#20197;&#23481;&#32435;&#20219;&#20309;$f$&#20010;&#21516;&#26102;&#25925;&#38556;&#12290;&#22312;&#25191;&#34892;&#36807;&#31243;&#20013;&#65292;&#23427;&#20381;&#36182;&#20110;&#36328;&#21103;&#26412;&#30340;&#24050;&#22797;&#21046;&#27169;&#22411;&#29366;&#24577;&#26469;&#25552;&#20379;&#24555;&#36895;&#24674;&#22797;&#12290;Oobleck&#21487;&#20197;&#21487;&#38752;&#22320;&#20445;&#35777;&#22312;$f$&#20010;&#25110;&#26356;&#23569;&#30340;&#21516;&#26102;&#25925;&#38556;&#21518;&#65292;&#21021;&#22987;&#21019;&#24314;&#30340;&#27969;&#27700;&#32447;&#27169;&#26495;&#30340;&#26576;&#31181;&#32452;&#21512;&#21487;&#20197;&#29992;&#20110;&#35206;&#30422;&#25152;&#26377;&#21487;&#29992;&#36164;&#28304;&#65292;&#20174;&#32780;&#22987;&#32456;&#36991;&#20813;&#36164;&#28304;&#38386;&#32622;&#12290;&#22312;&#20855;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#19978;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;Oobleck&#25552;&#20379;&#20102;&#19968;&#33268;&#39640;&#21534;&#21520;&#37327;&#65292;&#24182;&#19988;&#22312;&#21534;&#21520;&#37327;&#19978;&#32988;&#36807;&#20102;Bamboo&#21644;Varuna&#31561;&#26368;&#20808;&#36827;&#30340;&#23481;&#38169;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Oobleck enables resilient distributed training of large DNN models with guaranteed fault tolerance. It takes a planning-execution co-design approach, where it first generates a set of heterogeneous pipeline templates and instantiates at least $f+1$ logically equivalent pipeline replicas to tolerate any $f$ simultaneous failures. During execution, it relies on already-replicated model states across the replicas to provide fast recovery. Oobleck provably guarantees that some combination of the initially created pipeline templates can be used to cover all available resources after $f$ or fewer simultaneous failures, thereby avoiding resource idling at all times. Evaluation on large DNN models with billions of parameters shows that Oobleck provides consistently high throughput, and it outperforms state-of-the-art fault tolerance solutions like Bamboo and Varuna by up to $13.9x$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23383;&#20856;&#23398;&#20064;&#32463;&#39564;&#20998;&#24067;&#26469;&#35299;&#20915;&#23458;&#25143;&#31471;&#38388;&#20998;&#24067;&#20559;&#31227;&#21644;&#37096;&#20998;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#21327;&#20316;&#36890;&#20449;&#21327;&#35758;&#21644;&#32858;&#21512;&#25805;&#20316;&#65292;&#20445;&#25252;&#20102;&#23458;&#25143;&#31471;&#25968;&#25454;&#38544;&#31169;&#65292;&#24182;&#25104;&#21151;&#22312;&#30446;&#26631;&#39046;&#22495;&#29983;&#25104;&#20102;&#26631;&#35760;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.07670</link><description>&lt;p&gt;
&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#32852;&#37030;&#25968;&#25454;&#38598;&#23383;&#20856;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Dataset Dictionary Learning for Multi-Source Domain Adaptation. (arXiv:2309.07670v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23383;&#20856;&#23398;&#20064;&#32463;&#39564;&#20998;&#24067;&#26469;&#35299;&#20915;&#23458;&#25143;&#31471;&#38388;&#20998;&#24067;&#20559;&#31227;&#21644;&#37096;&#20998;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#21327;&#20316;&#36890;&#20449;&#21327;&#35758;&#21644;&#32858;&#21512;&#25805;&#20316;&#65292;&#20445;&#25252;&#20102;&#23458;&#25143;&#31471;&#25968;&#25454;&#38544;&#31169;&#65292;&#24182;&#25104;&#21151;&#22312;&#30446;&#26631;&#39046;&#22495;&#29983;&#25104;&#20102;&#26631;&#35760;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#23458;&#25143;&#31471;&#20013;&#23384;&#22312;&#20998;&#24067;&#20559;&#31227;&#19988;&#37096;&#20998;&#23458;&#25143;&#31471;&#20855;&#26377;&#26080;&#26631;&#31614;&#30340;&#25968;&#25454;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;FedDaDiL&#36890;&#36807;&#23383;&#20856;&#23398;&#20064;&#32463;&#39564;&#20998;&#24067;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#22312;&#25105;&#20204;&#30340;&#35774;&#32622;&#20013;&#65292;&#23458;&#25143;&#31471;&#30340;&#20998;&#24067;&#20195;&#34920;&#30528;&#29305;&#23450;&#30340;&#39046;&#22495;&#65292;&#32780;FedDaDiL&#21017;&#20849;&#21516;&#35757;&#32451;&#20102;&#19968;&#20010;&#32852;&#37030;&#32463;&#39564;&#20998;&#24067;&#23383;&#20856;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#25968;&#25454;&#38598;&#23383;&#20856;&#23398;&#20064;&#26694;&#26550;&#19978;&#35774;&#35745;&#20102;&#21327;&#20316;&#36890;&#20449;&#21327;&#35758;&#21644;&#32858;&#21512;&#25805;&#20316;&#12290;&#25152;&#36873;&#25321;&#30340;&#21327;&#35758;&#20445;&#25252;&#20102;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#38544;&#31169;&#65292;&#30456;&#27604;&#20110;&#38598;&#20013;&#24335;&#26041;&#27861;&#25552;&#39640;&#20102;&#25972;&#20307;&#38544;&#31169;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;Caltech-Office&#12289;TEP&#21644;CWRU&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#22312;&#30446;&#26631;&#39046;&#22495;&#29983;&#25104;&#20102;&#26631;&#35760;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20854;&#38598;&#20013;&#24335;&#26041;&#27861;&#21644;&#20854;&#20182;&#32852;&#37030;&#39046;&#22495;&#22522;&#20934;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article, we propose an approach for federated domain adaptation, a setting where distributional shift exists among clients and some have unlabeled data. The proposed framework, FedDaDiL, tackles the resulting challenge through dictionary learning of empirical distributions. In our setting, clients' distributions represent particular domains, and FedDaDiL collectively trains a federated dictionary of empirical distributions. In particular, we build upon the Dataset Dictionary Learning framework by designing collaborative communication protocols and aggregation operations. The chosen protocols keep clients' data private, thus enhancing overall privacy compared to its centralized counterpart. We empirically demonstrate that our approach successfully generates labeled data on the target domain with extensive experiments on (i) Caltech-Office, (ii) TEP, and (iii) CWRU benchmarks. Furthermore, we compare our method to its centralized counterpart and other benchmarks in federated doma
&lt;/p&gt;</description></item><item><title>Rank2Tell&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#39550;&#39542;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#32852;&#21512;&#37325;&#35201;&#24615;&#25490;&#24207;&#21644;&#25512;&#29702;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#22797;&#26434;&#20132;&#36890;&#24773;&#26223;&#20013;&#21508;&#31181;&#37325;&#35201;&#23545;&#35937;&#30340;&#23494;&#38598;&#27880;&#37322;&#21644;&#29420;&#29305;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06597</link><description>&lt;p&gt;
Rank2Tell: &#19968;&#20010;&#29992;&#20110;&#32852;&#21512;&#37325;&#35201;&#24615;&#25490;&#24207;&#21644;&#25512;&#29702;&#30340;&#22810;&#27169;&#24577;&#39550;&#39542;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Rank2Tell: A Multimodal Driving Dataset for Joint Importance Ranking and Reasoning. (arXiv:2309.06597v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06597
&lt;/p&gt;
&lt;p&gt;
Rank2Tell&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#39550;&#39542;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#32852;&#21512;&#37325;&#35201;&#24615;&#25490;&#24207;&#21644;&#25512;&#29702;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#22797;&#26434;&#20132;&#36890;&#24773;&#26223;&#20013;&#21508;&#31181;&#37325;&#35201;&#23545;&#35937;&#30340;&#23494;&#38598;&#27880;&#37322;&#21644;&#29420;&#29305;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21830;&#29992;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#21644;&#20808;&#36827;&#39550;&#39542;&#36741;&#21161;&#31995;&#32479;&#65288;ADAS&#65289;&#30340;&#24191;&#27867;&#24212;&#29992;&#21487;&#33021;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#31038;&#20250;&#23545;&#23427;&#20204;&#30340;&#25509;&#21463;&#31243;&#24230;&#65292;&#32780;&#23545;&#39569;&#36710;&#20154;&#26469;&#35828;&#65292;&#23427;&#20204;&#34987;&#35270;&#20026;&#21487;&#20449;&#21644;&#21487;&#35299;&#37322;&#24615;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#36825;&#20010;&#20219;&#21153;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#29616;&#20195;&#33258;&#20027;&#31995;&#32479;&#36719;&#20214;&#20005;&#37325;&#20381;&#36182;&#20110;&#40657;&#30418;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;Rank2Tell&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#37325;&#35201;&#24615;&#32423;&#21035;&#25490;&#24207;&#21644;&#21407;&#22240;&#35299;&#37322;&#30340;&#22810;&#27169;&#24577;&#39550;&#39542;&#25968;&#25454;&#38598;&#12290;&#20351;&#29992;&#21508;&#31181;&#38381;&#21512;&#21644;&#24320;&#25918;&#24335;&#35270;&#35273;&#38382;&#31572;&#65292;&#35813;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#22797;&#26434;&#20132;&#36890;&#24773;&#26223;&#20013;&#21508;&#31181;&#37325;&#35201;&#23545;&#35937;&#30340;&#21508;&#31181;&#35821;&#20041;&#12289;&#31354;&#38388;&#12289;&#26102;&#38388;&#21644;&#20851;&#31995;&#23646;&#24615;&#30340;&#23494;&#38598;&#27880;&#37322;&#12290;&#25968;&#25454;&#38598;&#30340;&#23494;&#38598;&#27880;&#37322;&#21644;&#29420;&#29305;&#23646;&#24615;&#20351;&#20854;&#25104;&#20026;&#20174;&#20107;&#35270;&#35273;&#22330;&#26223;&#29702;&#35299;&#21644;&#30456;&#20851;&#39046;&#22495;&#30740;&#31350;&#30340;&#30740;&#31350;&#20154;&#21592;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#32852;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#32852;&#21512;&#34920;&#31034;&#21644;&#25512;&#29702;&#37325;&#35201;&#24615;&#21644;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread adoption of commercial autonomous vehicles (AVs) and advanced driver assistance systems (ADAS) may largely depend on their acceptance by society, for which their perceived trustworthiness and interpretability to riders are crucial. In general, this task is challenging because modern autonomous systems software relies heavily on black-box artificial intelligence models. Towards this goal, this paper introduces a novel dataset, Rank2Tell, a multi-modal ego-centric dataset for Ranking the importance level and Telling the reason for the importance. Using various close and open-ended visual question answering, the dataset provides dense annotations of various semantic, spatial, temporal, and relational attributes of various important objects in complex traffic scenarios. The dense annotations and unique attributes of the dataset make it a valuable resource for researchers working on visual scene understanding and related fields. Further, we introduce a joint model for joint i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#27867;&#21270;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#23398;&#20064;&#22411;&#35821;&#38899;&#22686;&#24378;&#31995;&#32479;&#22312;&#22122;&#22768;&#21644;&#28151;&#21709;&#29615;&#22659;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#22312;&#27979;&#35797;&#26465;&#20214;&#19979;&#35757;&#32451;&#30340;&#21442;&#32771;&#27169;&#22411;&#20316;&#20026;&#38590;&#24230;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2309.06183</link><description>&lt;p&gt;
&#35780;&#20272;&#23398;&#20064;&#22411;&#35821;&#38899;&#22686;&#24378;&#31995;&#32479;&#22312;&#22122;&#22768;&#21644;&#28151;&#21709;&#29615;&#22659;&#20013;&#30340;&#27867;&#21270;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Assessing the Generalization Gap of Learning-Based Speech Enhancement Systems in Noisy and Reverberant Environments. (arXiv:2309.06183v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#27867;&#21270;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#23398;&#20064;&#22411;&#35821;&#38899;&#22686;&#24378;&#31995;&#32479;&#22312;&#22122;&#22768;&#21644;&#28151;&#21709;&#29615;&#22659;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#22312;&#27979;&#35797;&#26465;&#20214;&#19979;&#35757;&#32451;&#30340;&#21442;&#32771;&#27169;&#22411;&#20316;&#20026;&#38590;&#24230;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22122;&#22768;&#21644;&#28151;&#21709;&#35821;&#38899;&#28151;&#21512;&#29289;&#30340;&#22768;&#23398;&#21464;&#24322;&#24615;&#21463;&#22810;&#20010;&#22240;&#32032;&#24433;&#21709;&#65292;&#20363;&#22914;&#30446;&#26631;&#35828;&#35805;&#32773;&#21644;&#24178;&#25200;&#22122;&#22768;&#30340;&#39057;&#35889;&#26102;&#22495;&#29305;&#24615;&#65292;&#20449;&#22122;&#27604;&#21644;&#25151;&#38388;&#29305;&#24615;&#12290;&#36825;&#31181;&#22823;&#30340;&#21464;&#24322;&#24615;&#32473;&#22522;&#20110;&#23398;&#20064;&#30340;&#35821;&#38899;&#22686;&#24378;&#31995;&#32479;&#24102;&#26469;&#20102;&#24456;&#22823;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#35757;&#32451;&#21644;&#27979;&#35797;&#26465;&#20214;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#20250;&#22823;&#22823;&#38477;&#20302;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#36890;&#24120;&#36890;&#36807;&#20351;&#29992;&#19982;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#30340;&#19981;&#21516;&#30340;&#26032;&#35821;&#38899;&#12289;&#22122;&#22768;&#25110;&#21452;&#32819;&#25151;&#38388;&#33033;&#20914;&#21709;&#24212;&#65288;BRIR&#65289;&#25968;&#25454;&#24211;&#23545;&#31995;&#32479;&#36827;&#34892;&#27979;&#35797;&#26469;&#35780;&#20272;&#23545;&#26410;&#30693;&#26465;&#20214;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35821;&#38899;&#22686;&#24378;&#20219;&#21153;&#30340;&#38590;&#24230;&#21487;&#33021;&#20250;&#38543;&#30528;&#25968;&#25454;&#24211;&#30340;&#21464;&#21270;&#32780;&#25913;&#21464;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#32467;&#26524;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#27867;&#21270;&#35780;&#20272;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#22312;&#27979;&#35797;&#26465;&#20214;&#19979;&#35757;&#32451;&#30340;&#21442;&#32771;&#27169;&#22411;&#20316;&#20026;&#27979;&#35797;&#26465;&#20214;&#38590;&#24230;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
The acoustic variability of noisy and reverberant speech mixtures is influenced by multiple factors, such as the spectro-temporal characteristics of the target speaker and the interfering noise, the signal-to-noise ratio (SNR) and the room characteristics. This large variability poses a major challenge for learning-based speech enhancement systems, since a mismatch between the training and testing conditions can substantially reduce the performance of the system. Generalization to unseen conditions is typically assessed by testing the system with a new speech, noise or binaural room impulse response (BRIR) database different from the one used during training. However, the difficulty of the speech enhancement task can change across databases, which can substantially influence the results. The present study introduces a generalization assessment framework that uses a reference model trained on the test condition, such that it can be used as a proxy for the difficulty of the test conditio
&lt;/p&gt;</description></item><item><title>LoopTune&#26159;&#19968;&#20010;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#24352;&#37327;&#35745;&#31639;&#30340;&#32534;&#35793;&#22120;&#65292;&#36890;&#36807;&#20248;&#21270;&#24352;&#37327;&#36941;&#21382;&#39034;&#24207;&#21644;&#20351;&#29992;&#20195;&#30721;&#29983;&#25104;&#22120;LoopNest&#25191;&#34892;&#30828;&#20214;&#29305;&#23450;&#20248;&#21270;&#65292;LoopTune&#33021;&#22815;&#29983;&#25104;&#27604;&#20854;&#20182;&#32534;&#35793;&#22120;&#26356;&#24555;&#30340;&#20195;&#30721;&#12290;&#36890;&#36807;&#37319;&#29992;&#26032;&#30340;&#22270;&#24418;&#34920;&#31034;&#21644;&#21160;&#20316;&#31354;&#38388;&#65292;LoopTune&#27604;TVM&#24555;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#27604;MetaSchedule&#24555;2.8&#20493;&#65292;&#27604;AutoTVM&#24555;1.08&#20493;&#65292;&#24182;&#25345;&#32493;&#22312;&#19982;&#25163;&#24037;&#35843;&#20248;&#30340;&#24211;Numpy&#30456;&#24403;&#30340;&#27700;&#24179;&#19978;&#24037;&#20316;&#12290;&#27492;&#22806;&#65292;LoopTune&#20248;&#21270;&#20195;&#30721;&#30340;&#26102;&#38388;&#21482;&#38656;&#20960;&#31186;&#38047;&#12290;</title><link>http://arxiv.org/abs/2309.01825</link><description>&lt;p&gt;
LoopTune: &#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#24352;&#37327;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
LoopTune: Optimizing Tensor Computations with Reinforcement Learning. (arXiv:2309.01825v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01825
&lt;/p&gt;
&lt;p&gt;
LoopTune&#26159;&#19968;&#20010;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#24352;&#37327;&#35745;&#31639;&#30340;&#32534;&#35793;&#22120;&#65292;&#36890;&#36807;&#20248;&#21270;&#24352;&#37327;&#36941;&#21382;&#39034;&#24207;&#21644;&#20351;&#29992;&#20195;&#30721;&#29983;&#25104;&#22120;LoopNest&#25191;&#34892;&#30828;&#20214;&#29305;&#23450;&#20248;&#21270;&#65292;LoopTune&#33021;&#22815;&#29983;&#25104;&#27604;&#20854;&#20182;&#32534;&#35793;&#22120;&#26356;&#24555;&#30340;&#20195;&#30721;&#12290;&#36890;&#36807;&#37319;&#29992;&#26032;&#30340;&#22270;&#24418;&#34920;&#31034;&#21644;&#21160;&#20316;&#31354;&#38388;&#65292;LoopTune&#27604;TVM&#24555;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#27604;MetaSchedule&#24555;2.8&#20493;&#65292;&#27604;AutoTVM&#24555;1.08&#20493;&#65292;&#24182;&#25345;&#32493;&#22312;&#19982;&#25163;&#24037;&#35843;&#20248;&#30340;&#24211;Numpy&#30456;&#24403;&#30340;&#27700;&#24179;&#19978;&#24037;&#20316;&#12290;&#27492;&#22806;&#65292;LoopTune&#20248;&#21270;&#20195;&#30721;&#30340;&#26102;&#38388;&#21482;&#38656;&#20960;&#31186;&#38047;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#36827;&#30340;&#32534;&#35793;&#22120;&#25216;&#26415;&#23545;&#20110;&#20351;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#22312;&#26032;&#22411;&#30828;&#20214;&#19978;&#36816;&#34892;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20256;&#32479;&#32534;&#35793;&#22120;&#26080;&#27861;&#25552;&#20379;&#24615;&#33021;&#65292;&#26222;&#36890;&#30340;&#33258;&#21160;&#35843;&#33410;&#22120;&#25628;&#32034;&#26102;&#38388;&#38271;&#65292;&#32463;&#19987;&#23478;&#20248;&#21270;&#30340;&#24211;&#23548;&#33268;&#19981;&#21487;&#25345;&#32493;&#30340;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;LoopTune&#65292;&#36825;&#26159;&#19968;&#20010;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#32534;&#35793;&#22120;&#65292;&#29992;&#20110;&#20248;&#21270;CPU&#20013;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#24352;&#37327;&#35745;&#31639;&#12290;LoopTune&#22312;&#20351;&#29992;&#36229;&#24555;&#36731;&#37327;&#32423;&#20195;&#30721;&#29983;&#25104;&#22120;LoopNest&#25191;&#34892;&#30828;&#20214;&#29305;&#23450;&#20248;&#21270;&#30340;&#21516;&#26102;&#65292;&#20248;&#21270;&#24352;&#37327;&#36941;&#21382;&#39034;&#24207;&#12290;&#36890;&#36807;&#37319;&#29992;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#21644;&#21160;&#20316;&#31354;&#38388;&#65292;LoopTune&#23558;LoopNest&#30340;&#36895;&#24230;&#25552;&#39640;&#20102;3.2&#20493;&#65292;&#29983;&#25104;&#30340;&#20195;&#30721;&#27604;TVM&#24555;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#27604;MetaSchedule&#24555;2.8&#20493;&#65292;&#27604;AutoTVM&#24555;1.08&#20493;&#65292;&#24182;&#25345;&#32493;&#22312;&#25163;&#24037;&#35843;&#25972;&#30340;&#24211;Numpy&#30340;&#27700;&#24179;&#19978;&#36816;&#34892;&#12290;&#27492;&#22806;&#65292;LoopTune&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#20248;&#21270;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advanced compiler technology is crucial for enabling machine learning applications to run on novel hardware, but traditional compilers fail to deliver performance, popular auto-tuners have long search times and expert-optimized libraries introduce unsustainable costs. To address this, we developed LoopTune, a deep reinforcement learning compiler that optimizes tensor computations in deep learning models for the CPU. LoopTune optimizes tensor traversal order while using the ultra-fast lightweight code generator LoopNest to perform hardware-specific optimizations. With a novel graph-based representation and action space, LoopTune speeds up LoopNest by 3.2x, generating an order of magnitude faster code than TVM, 2.8x faster than MetaSchedule, and 1.08x faster than AutoTVM, consistently performing at the level of the hand-tuned library Numpy. Moreover, LoopTune tunes code in order of seconds.
&lt;/p&gt;</description></item><item><title>ToddlerBERTa&#26159;&#19968;&#20010;&#31867;&#20284;&#20110;BabyBERTa&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23613;&#31649;&#22312;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#23427;&#23637;&#31034;&#20102;&#20196;&#20154;&#31216;&#36190;&#30340;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;RoBERTa-base&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2308.16336</link><description>&lt;p&gt;
ToddlerBERTa: &#21033;&#29992;BabyBERTa&#36827;&#34892;&#35821;&#27861;&#23398;&#20064;&#21644;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
ToddlerBERTa: Exploiting BabyBERTa for Grammar Learning and Language Understanding. (arXiv:2308.16336v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16336
&lt;/p&gt;
&lt;p&gt;
ToddlerBERTa&#26159;&#19968;&#20010;&#31867;&#20284;&#20110;BabyBERTa&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23613;&#31649;&#22312;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#23427;&#23637;&#31034;&#20102;&#20196;&#20154;&#31216;&#36190;&#30340;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;RoBERTa-base&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;ToddlerBERTa&#65292;&#36825;&#26159;&#19968;&#20010;&#31867;&#20284;&#20110;BabyBERTa&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20116;&#31181;&#19981;&#21516;&#30340;&#20855;&#26377;&#19981;&#21516;&#36229;&#21442;&#25968;&#30340;&#27169;&#22411;&#26469;&#25506;&#32034;&#20854;&#33021;&#21147;&#12290;&#22312;BLiMP&#65292;SuperGLUE&#65292;MSGS&#21644;BabyLM&#25361;&#25112;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#36739;&#23567;&#30340;&#27169;&#22411;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#36739;&#22823;&#30340;&#27169;&#22411;&#22312;&#22823;&#37327;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;&#23613;&#31649;&#22312;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;ToddlerBERTa&#23637;&#31034;&#20102;&#20196;&#20154;&#31216;&#36190;&#30340;&#24615;&#33021;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;RoBERTa-base&#30456;&#23218;&#32654;&#12290;&#35813;&#27169;&#22411;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#21363;&#20351;&#26159;&#22312;&#21333;&#21477;&#39044;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#19982;&#21033;&#29992;&#26356;&#24191;&#27867;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#22522;&#32447;&#31454;&#20105;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#36229;&#21442;&#25968;&#36873;&#25321;&#21644;&#25968;&#25454;&#21033;&#29992;&#25552;&#20379;&#20102;&#27934;&#23519;&#65292;&#24182;&#20026;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ToddlerBERTa, a BabyBERTa-like language model, exploring its capabilities through five different models with varied hyperparameters. Evaluating on BLiMP, SuperGLUE, MSGS, and a Supplement benchmark from the BabyLM challenge, we find that smaller models can excel in specific tasks, while larger models perform well with substantial data. Despite training on a smaller dataset, ToddlerBERTa demonstrates commendable performance, rivalling the state-of-the-art RoBERTa-base. The model showcases robust language understanding, even with single-sentence pretraining, and competes with baselines that leverage broader contextual information. Our work provides insights into hyperparameter choices, and data utilization, contributing to the advancement of language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21407;&#22987;&#26041;&#27861;&#26469;&#35299;&#20915;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26465;&#20214;&#37319;&#26679;&#20013;&#30340;&#22256;&#38590;&#65292;&#24182;&#22312;&#37319;&#26679;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#25913;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09078</link><description>&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#36817;&#20284;&#31062;&#20808;&#37319;&#26679;&#23454;&#29616;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#26465;&#20214;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Conditional Sampling of Variational Autoencoders via Iterated Approximate Ancestral Sampling. (arXiv:2308.09078v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21407;&#22987;&#26041;&#27861;&#26469;&#35299;&#20915;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26465;&#20214;&#37319;&#26679;&#20013;&#30340;&#22256;&#38590;&#65292;&#24182;&#22312;&#37319;&#26679;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#65292;&#22914;&#32570;&#22833;&#25968;&#25454;&#22635;&#20805;&#65292;&#38656;&#35201;&#23545;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#36827;&#34892;&#26465;&#20214;&#37319;&#26679;&#65292;&#20294;&#36825;&#26159;&#35745;&#31639;&#19978;&#19981;&#21487;&#34892;&#30340;&#12290;&#28176;&#36817;&#31934;&#30830;&#26465;&#20214;&#37319;&#26679;&#30340;&#21407;&#21017;&#36873;&#25321;&#26159;Metropolis-within-Gibbs&#65288;MWG&#65289;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;VAE&#20542;&#21521;&#20110;&#23398;&#20064;&#32467;&#26500;&#21270;&#28508;&#21464;&#37327;&#31354;&#38388;&#65292;&#36825;&#26159;&#19968;&#20010;&#24120;&#35265;&#30340;&#26399;&#26395;&#29305;&#24615;&#65292;&#20294;&#21364;&#23548;&#33268;MWG&#37319;&#26679;&#22120;&#36828;&#31163;&#30446;&#26631;&#20998;&#24067;&#12290;&#26412;&#25991;&#20811;&#26381;&#20102;MWG&#30340;&#23616;&#38480;&#24615;&#65306;&#25105;&#20204;&#31995;&#32479;&#22320;&#27010;&#36848;&#20102;&#22312;VAE&#19978;&#19978;&#36848;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#21407;&#22987;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#22312;&#19968;&#32452;&#37319;&#26679;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conditional sampling of variational autoencoders (VAEs) is needed in various applications, such as missing data imputation, but is computationally intractable. A principled choice for asymptotically exact conditional sampling is Metropolis-within-Gibbs (MWG). However, we observe that the tendency of VAEs to learn a structured latent space, a commonly desired property, can cause the MWG sampler to get "stuck" far from the target distribution. This paper mitigates the limitations of MWG: we systematically outline the pitfalls in the context of VAEs, propose two original methods that address these pitfalls, and demonstrate an improved performance of the proposed methods on a set of sampling tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20154;&#29289;&#21644;&#29289;&#20307;&#20132;&#20114;&#26816;&#27979;&#20013;&#30340;&#35859;&#35789;&#35270;&#35273;&#32972;&#26223;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#30418;&#23376;&#37197;&#23545;&#20301;&#32622;&#23884;&#20837;&#31561;&#26041;&#24335;&#26469;&#25913;&#36827;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06202</link><description>&lt;p&gt;
&#22312;&#26816;&#27979;&#20154;&#29289;&#21644;&#29289;&#20307;&#20132;&#20114;&#20013;&#25506;&#32034;&#35859;&#35789;&#35270;&#35273;&#32972;&#26223;
&lt;/p&gt;
&lt;p&gt;
Exploring Predicate Visual Context in Detecting of Human-Object Interactions. (arXiv:2308.06202v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20154;&#29289;&#21644;&#29289;&#20307;&#20132;&#20114;&#26816;&#27979;&#20013;&#30340;&#35859;&#35789;&#35270;&#35273;&#32972;&#26223;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#30418;&#23376;&#37197;&#23545;&#20301;&#32622;&#23884;&#20837;&#31561;&#26041;&#24335;&#26469;&#25913;&#36827;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;DETR&#26694;&#26550;&#24050;&#25104;&#20026;&#20154;&#29289;&#21644;&#29289;&#20307;&#20132;&#20114;&#65288;HOI&#65289;&#30740;&#31350;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#29305;&#21035;&#26159;&#65292;&#22522;&#20110;&#20004;&#38454;&#27573;&#21464;&#25442;&#22120;&#30340;HOI&#26816;&#27979;&#22120;&#26159;&#24615;&#33021;&#26368;&#22909;&#21644;&#35757;&#32451;&#26368;&#39640;&#25928;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20197;&#32570;&#20047;&#32454;&#31890;&#24230;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#29289;&#20307;&#29305;&#24449;&#20316;&#20026;HOI&#20998;&#31867;&#30340;&#26465;&#20214;&#65292;&#32780;&#24573;&#35270;&#20102;&#23039;&#21183;&#21644;&#26041;&#21521;&#20449;&#24687;&#65292;&#32780;&#26356;&#27880;&#37325;&#20851;&#20110;&#29289;&#20307;&#36523;&#20221;&#21644;&#36793;&#30028;&#30340;&#35270;&#35273;&#25552;&#31034;&#12290;&#36825;&#33258;&#28982;&#22320;&#38459;&#30861;&#20102;&#23545;&#22797;&#26434;&#25110;&#27169;&#31946;&#20132;&#20114;&#30340;&#35782;&#21035;&#12290;&#26412;&#25991;&#36890;&#36807;&#21487;&#35270;&#21270;&#21644;&#31934;&#24515;&#35774;&#35745;&#30340;&#23454;&#39564;&#30740;&#31350;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#37325;&#26032;&#24341;&#20837;&#22270;&#20687;&#29305;&#24449;&#65292;&#24182;&#25913;&#36827;&#20102;&#26597;&#35810;&#35774;&#35745;&#65292;&#24191;&#27867;&#25506;&#32034;&#20102;&#38190;&#21644;&#20540;&#65292;&#20197;&#21450;&#20351;&#29992;&#30418;&#23376;&#37197;&#23545;&#20301;&#32622;&#23884;&#20837;&#20316;&#20026;&#31354;&#38388;&#25351;&#23548;&#12290;&#25105;&#20204;&#30340;&#25913;&#36827;&#35859;&#35789;&#35270;&#35273;&#32972;&#26223;&#65288;PViC&#65289;&#27169;&#22411;&#22312;HICO-DET&#21644;V-COCO&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the DETR framework has emerged as the dominant approach for human--object interaction (HOI) research. In particular, two-stage transformer-based HOI detectors are amongst the most performant and training-efficient approaches. However, these often condition HOI classification on object features that lack fine-grained contextual information, eschewing pose and orientation information in favour of visual cues about object identity and box extremities. This naturally hinders the recognition of complex or ambiguous interactions. In this work, we study these issues through visualisations and carefully designed experiments. Accordingly, we investigate how best to re-introduce image features via cross-attention. With an improved query design, extensive exploration of keys and values, and box pair positional embeddings as spatial guidance, our model with enhanced predicate visual context (PViC) outperforms state-of-the-art methods on the HICO-DET and V-COCO benchmarks, while maintaini
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#20445;&#30495;&#22810;&#39057;&#24102;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#20302;&#27604;&#29305;&#29575;&#30340;&#31163;&#25955;&#34920;&#31034;&#29983;&#25104;&#20219;&#20309;&#31867;&#22411;&#30340;&#38899;&#39057;&#24418;&#24335;&#65292;&#20855;&#26377;&#20248;&#20110;&#26368;&#20808;&#36827;&#29983;&#25104;&#25216;&#26415;&#30340;&#24863;&#30693;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.02560</link><description>&lt;p&gt;
&#20174;&#31163;&#25955;&#26631;&#35760;&#21040;&#39640;&#20445;&#30495;&#38899;&#39057;&#65306;&#20351;&#29992;&#22810;&#39057;&#24102;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
From Discrete Tokens to High-Fidelity Audio Using Multi-Band Diffusion. (arXiv:2308.02560v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02560
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#20445;&#30495;&#22810;&#39057;&#24102;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#20302;&#27604;&#29305;&#29575;&#30340;&#31163;&#25955;&#34920;&#31034;&#29983;&#25104;&#20219;&#20309;&#31867;&#22411;&#30340;&#38899;&#39057;&#24418;&#24335;&#65292;&#20855;&#26377;&#20248;&#20110;&#26368;&#20808;&#36827;&#29983;&#25104;&#25216;&#26415;&#30340;&#24863;&#30693;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#26681;&#25454;&#21508;&#31181;&#34920;&#31034;&#65288;&#22914;mel&#39057;&#35889;&#12289;MFCC&#65289;&#29983;&#25104;&#39640;&#20445;&#30495;&#38899;&#39057;&#12290;&#26368;&#36817;&#65292;&#36825;&#26679;&#30340;&#27169;&#22411;&#24050;&#32463;&#34987;&#29992;&#20110;&#26681;&#25454;&#39640;&#24230;&#21387;&#32553;&#30340;&#34920;&#31034;&#21512;&#25104;&#38899;&#39057;&#27874;&#24418;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#20135;&#29983;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#26159;&#24403;&#26465;&#20214;&#19981;&#23436;&#32654;&#26102;&#65292;&#26131;&#20110;&#20135;&#29983;&#21487;&#21548;&#21040;&#30340;&#20266;&#24433;&#12290;&#21478;&#19968;&#31181;&#24314;&#27169;&#26041;&#27861;&#26159;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20027;&#35201;&#29992;&#20316;&#35821;&#38899;&#27169;&#22411;&#65288;&#25110;&#22522;&#20110;mel&#39057;&#35889;&#30340;&#26465;&#20214;&#27169;&#22411;&#65289;&#25110;&#29983;&#25104;&#30456;&#23545;&#36739;&#20302;&#37319;&#26679;&#29575;&#30340;&#20449;&#21495;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#20445;&#30495;&#22810;&#39057;&#24102;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#20302;&#27604;&#29305;&#29575;&#30340;&#31163;&#25955;&#34920;&#31034;&#29983;&#25104;&#20219;&#20309;&#31867;&#22411;&#30340;&#38899;&#39057;&#24418;&#24335;&#65288;&#22914;&#35821;&#38899;&#12289;&#38899;&#20048;&#12289;&#29615;&#22659;&#22768;&#38899;&#65289;&#12290;&#22312;&#30456;&#21516;&#30340;&#27604;&#29305;&#29575;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#24863;&#30693;&#36136;&#37327;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models can generate high-fidelity audio conditioned on various types of representations (e.g., mel-spectrograms, Mel-frequency Cepstral Coefficients (MFCC)). Recently, such models have been used to synthesize audio waveforms conditioned on highly compressed representations. Although such methods produce impressive results, they are prone to generate audible artifacts when the conditioning is flawed or imperfect. An alternative modeling approach is to use diffusion models. However, these have mainly been used as speech vocoders (i.e., conditioned on mel-spectrograms) or generating relatively low sampling rate signals. In this work, we propose a high-fidelity multi-band diffusion-based framework that generates any type of audio modality (e.g., speech, music, environmental sounds) from low-bitrate discrete representations. At equal bit rate, the proposed approach outperforms state-of-the-art generative techniques in terms of perceptual quality. Training and, evaluation cod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23383;&#20856;&#23398;&#20064;&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;MSDA&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#22495;&#34920;&#31034;&#20026;&#23383;&#20856;&#21407;&#23376;&#30340;Wasserstein&#37325;&#24515;&#26469;&#32531;&#35299;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#12290;&#26681;&#25454;&#35813;&#23383;&#20856;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;MSDA&#26041;&#27861;&#65292;&#20998;&#21035;&#22522;&#20110;&#30446;&#26631;&#22495;&#26631;&#35760;&#26679;&#26412;&#30340;&#37325;&#26500;&#21644;&#22312;&#21407;&#23376;&#20998;&#24067;&#19978;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#30340;&#38598;&#25104;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.14953</link><description>&lt;p&gt;
&#22312;Wasserstein&#31354;&#38388;&#20013;&#36890;&#36807;&#25968;&#25454;&#38598;&#23383;&#20856;&#23398;&#20064;&#36827;&#34892;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Multi-Source Domain Adaptation through Dataset Dictionary Learning in Wasserstein Space. (arXiv:2307.14953v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14953
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23383;&#20856;&#23398;&#20064;&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;MSDA&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#22495;&#34920;&#31034;&#20026;&#23383;&#20856;&#21407;&#23376;&#30340;Wasserstein&#37325;&#24515;&#26469;&#32531;&#35299;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#12290;&#26681;&#25454;&#35813;&#23383;&#20856;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;MSDA&#26041;&#27861;&#65292;&#20998;&#21035;&#22522;&#20110;&#30446;&#26631;&#22495;&#26631;&#35760;&#26679;&#26412;&#30340;&#37325;&#26500;&#21644;&#22312;&#21407;&#23376;&#20998;&#24067;&#19978;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#30340;&#38598;&#25104;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;&#65288;MSDA&#65289;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#26088;&#22312;&#22312;&#20174;&#22810;&#20010;&#26631;&#35760;&#30340;&#28304;&#22495;&#36716;&#31227;&#30693;&#35782;&#21040;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#22495;&#26102;&#32531;&#35299;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23383;&#20856;&#23398;&#20064;&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#26032;&#22411;MSDA&#26694;&#26550;&#12290;&#25105;&#20204;&#23558;MSDA&#20013;&#30340;&#27599;&#20010;&#22495;&#35299;&#37322;&#20026;&#32463;&#39564;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;&#22495;&#34920;&#36798;&#20026;&#23383;&#20856;&#21407;&#23376;&#30340;Wasserstein&#37325;&#24515;&#65292;&#36825;&#20123;&#21407;&#23376;&#26159;&#32463;&#39564;&#20998;&#24067;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#36807;&#23567;&#25209;&#37327;&#23398;&#20064;&#30340;&#31639;&#27861;DaDiL&#65306;&#65288;i&#65289;&#21407;&#23376;&#20998;&#24067;&#65307;&#65288;ii&#65289;&#37325;&#24515;&#22352;&#26631;&#30697;&#38453;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#23383;&#20856;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;MSDA&#26041;&#27861;&#65306;DaDiL-R&#65292;&#22522;&#20110;&#30446;&#26631;&#22495;&#26631;&#35760;&#26679;&#26412;&#30340;&#37325;&#26500;&#65307;DaDiL-E&#65292;&#22522;&#20110;&#22312;&#21407;&#23376;&#20998;&#24067;&#19978;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#30340;&#38598;&#25104;&#12290;&#25105;&#20204;&#22312;3&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;Caltech-Office&#12289;Office 31&#21644;CRWU&#65292;&#22312;&#20998;&#31867;&#19978;&#25913;&#36827;&#20102;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;3.15&#65285;&#12289;2.29&#65285;&#21644;7.71&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper seeks to solve Multi-Source Domain Adaptation (MSDA), which aims to mitigate data distribution shifts when transferring knowledge from multiple labeled source domains to an unlabeled target domain. We propose a novel MSDA framework based on dictionary learning and optimal transport. We interpret each domain in MSDA as an empirical distribution. As such, we express each domain as a Wasserstein barycenter of dictionary atoms, which are empirical distributions. We propose a novel algorithm, DaDiL, for learning via mini-batches: (i) atom distributions; (ii) a matrix of barycentric coordinates. Based on our dictionary, we propose two novel methods for MSDA: DaDil-R, based on the reconstruction of labeled samples in the target domain, and DaDiL-E, based on the ensembling of classifiers learned on atom distributions. We evaluate our methods in 3 benchmarks: Caltech-Office, Office 31, and CRWU, where we improved previous state-of-the-art by 3.15%, 2.29%, and 7.71% in classification 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;Beam Tree&#36882;&#24402;&#31639;&#27861;&#65288;BT-RvNN&#65289;&#65292;&#36890;&#36807;&#35299;&#20915;&#35780;&#20998;&#20989;&#25968;&#21644;&#36882;&#24402;&#21333;&#20803;&#20989;&#25968;&#30340;&#32416;&#32544;&#38382;&#39064;&#20197;&#21450;&#31616;&#21270;&#20869;&#23384;&#20351;&#29992;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;BT-RvNN&#30340;&#20869;&#23384;&#20351;&#29992;&#12290;&#36825;&#20010;&#31639;&#27861;&#22312;ListOps&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#24182;&#22312;&#20854;&#20182;&#20219;&#21153;&#20013;&#20445;&#25345;&#20102;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.10779</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;Beam Tree&#36882;&#24402;
&lt;/p&gt;
&lt;p&gt;
Efficient Beam Tree Recursion. (arXiv:2307.10779v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;Beam Tree&#36882;&#24402;&#31639;&#27861;&#65288;BT-RvNN&#65289;&#65292;&#36890;&#36807;&#35299;&#20915;&#35780;&#20998;&#20989;&#25968;&#21644;&#36882;&#24402;&#21333;&#20803;&#20989;&#25968;&#30340;&#32416;&#32544;&#38382;&#39064;&#20197;&#21450;&#31616;&#21270;&#20869;&#23384;&#20351;&#29992;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;BT-RvNN&#30340;&#20869;&#23384;&#20351;&#29992;&#12290;&#36825;&#20010;&#31639;&#27861;&#22312;ListOps&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#24182;&#22312;&#20854;&#20182;&#20219;&#21153;&#20013;&#20445;&#25345;&#20102;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#30340;Beam Tree&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;BT-RvNN&#65289;&#26159;Gumbel Tree RvNN&#30340;&#31616;&#21333;&#25193;&#23637;&#65292;&#24050;&#32463;&#22312;ListOps&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#38271;&#24230;&#27867;&#21270;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#20445;&#25345;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#19981;&#26159;&#26368;&#24046;&#30340;&#65292;&#20294;BT-RvNN&#30340;&#20869;&#23384;&#20351;&#29992;&#20173;&#28982;&#38750;&#24120;&#26114;&#36149;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;BT-RvNN&#20869;&#23384;&#20351;&#29992;&#30340;&#20027;&#35201;&#29942;&#39048;&#26159;&#35780;&#20998;&#20989;&#25968;&#21644;&#36882;&#24402;&#21333;&#20803;&#20989;&#25968;&#30340;&#32416;&#32544;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31574;&#30053;&#26469;&#35299;&#20915;&#36825;&#20010;&#29942;&#39048;&#65292;&#24182;&#36827;&#19968;&#27493;&#31616;&#21270;&#20869;&#23384;&#20351;&#29992;&#12290;&#24635;&#20307;&#19978;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#19981;&#20165;&#23558;BT-RvNN&#30340;&#20869;&#23384;&#20351;&#29992;&#20943;&#23569;&#20102;10-16&#20493;&#65292;&#32780;&#19988;&#22312;ListOps&#20013;&#21019;&#36896;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#21516;&#26102;&#22312;&#20854;&#20182;&#20219;&#21153;&#20013;&#20445;&#25345;&#20102;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;BT-RvNN&#20135;&#29983;&#30340;&#24341;&#23548;&#38544;&#23618;&#26641;&#33410;&#28857;&#34920;&#31034;&#30340;&#31574;&#30053;&#65292;&#23558;BT-RvNN&#20174;&#24418;&#24335;&#20026;f&#65306;R^n&#215;d -&gt;&#30340;&#21477;&#23376;&#32534;&#30721;&#22120;&#36716;&#25442;&#25104;&#12290;&#8221;
&lt;/p&gt;
&lt;p&gt;
Beam Tree Recursive Neural Network (BT-RvNN) was recently proposed as a simple extension of Gumbel Tree RvNN and it was shown to achieve state-of-the-art length generalization performance in ListOps while maintaining comparable performance on other tasks. However, although not the worst in its kind, BT-RvNN can be still exorbitantly expensive in memory usage. In this paper, we identify the main bottleneck in BT-RvNN's memory usage to be the entanglement of the scorer function and the recursive cell function. We propose strategies to remove this bottleneck and further simplify its memory usage. Overall, our strategies not only reduce the memory usage of BT-RvNN by $10$-$16$ times but also create a new state-of-the-art in ListOps while maintaining similar performance in other tasks. In addition, we also propose a strategy to utilize the induced latent-tree node representations produced by BT-RvNN to turn BT-RvNN from a sentence encoder of the form $f:\mathbb{R}^{n \times d} \rightarrow \
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#21644;&#31639;&#27861;&#25552;&#20986;&#65292;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#21033;&#29992;&#19981;&#31283;&#23450;&#29305;&#24449;&#26469;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.09933</link><description>&lt;p&gt;
Spuriosity&#24182;&#27809;&#26377;&#23548;&#33268;&#20998;&#31867;&#22120;&#22833;&#36133;&#65306;&#21033;&#29992;&#19981;&#21464;&#30340;&#39044;&#27979;&#26469;&#21033;&#29992;&#34394;&#20551;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Spuriosity Didn't Kill the Classifier: Using Invariant Predictions to Harness Spurious Features. (arXiv:2307.09933v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#21644;&#31639;&#27861;&#25552;&#20986;&#65292;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#21033;&#29992;&#19981;&#31283;&#23450;&#29305;&#24449;&#26469;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36991;&#20813;&#22312;&#22495;&#22806;&#25968;&#25454;&#19978;&#30340;&#22833;&#36133;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#35797;&#22270;&#25552;&#21462;&#20855;&#26377;&#19982;&#26631;&#31614;&#22312;&#19981;&#21516;&#22495;&#20043;&#38388;&#31283;&#23450;&#25110;&#19981;&#21464;&#20851;&#31995;&#30340;&#29305;&#24449;&#65292;&#33293;&#24323;&#19982;&#26631;&#31614;&#22312;&#19981;&#21516;&#22495;&#20043;&#38388;&#20851;&#31995;&#21464;&#21270;&#30340;"&#34394;&#20551;"&#25110;&#19981;&#31283;&#23450;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#19981;&#31283;&#23450;&#29305;&#24449;&#24120;&#24120;&#25658;&#24102;&#20851;&#20110;&#26631;&#31614;&#30340;&#34917;&#20805;&#20449;&#24687;&#65292;&#22914;&#26524;&#22312;&#27979;&#35797;&#22495;&#20013;&#27491;&#30830;&#20351;&#29992;&#65292;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#26174;&#31034;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#22914;&#20309;&#22312;&#27979;&#35797;&#22495;&#20013;&#20351;&#29992;&#36825;&#20123;&#19981;&#31283;&#23450;&#29305;&#24449;&#26159;&#21487;&#33021;&#30340;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#22522;&#20110;&#31283;&#23450;&#29305;&#24449;&#30340;&#20266;&#26631;&#31614;&#25552;&#20379;&#20102;&#36275;&#22815;&#30340;&#25351;&#23548;&#26469;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#21069;&#25552;&#26159;&#22312;&#32473;&#23450;&#26631;&#31614;&#30340;&#26465;&#20214;&#19979;&#65292;&#31283;&#23450;&#29305;&#24449;&#21644;&#19981;&#31283;&#23450;&#29305;&#24449;&#26159;&#26465;&#20214;&#29420;&#31435;&#30340;&#12290;&#22522;&#20110;&#36825;&#20010;&#29702;&#35770;&#27934;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31283;&#23450;&#29305;&#24449;&#22686;&#24378;&#65288;SFB&#65289;&#31639;&#27861;&#65306;(i)&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#20998;&#31163;&#31283;&#23450;&#29305;&#24449;&#21644;&#26465;&#20214;&#29420;&#31435;&#19981;&#31283;&#23450;&#29305;&#24449;&#30340;&#39044;&#27979;&#22120;&#65307;(ii)&#20351;&#29992;&#31283;&#23450;&#29305;&#24449;&#39044;&#27979;&#26469;&#36866;&#24212;&#27979;&#35797;&#22495;
&lt;/p&gt;
&lt;p&gt;
To avoid failures on out-of-distribution data, recent works have sought to extract features that have a stable or invariant relationship with the label across domains, discarding the "spurious" or unstable features whose relationship with the label changes across domains. However, unstable features often carry complementary information about the label that could boost performance if used correctly in the test domain. Our main contribution is to show that it is possible to learn how to use these unstable features in the test domain without labels. In particular, we prove that pseudo-labels based on stable features provide sufficient guidance for doing so, provided that stable and unstable features are conditionally independent given the label. Based on this theoretical insight, we propose Stable Feature Boosting (SFB), an algorithm for: (i) learning a predictor that separates stable and conditionally-independent unstable features; and (ii) using the stable-feature predictions to adapt t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36873;&#25321;&#20266;&#24067;&#23572;&#21644;&#32447;&#24615;&#25972;&#25968;&#32422;&#26463;&#30340;SAT&#32534;&#30721;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#19968;&#32452;&#29305;&#24449;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36873;&#25321;&#32534;&#30721;&#26041;&#24335;&#65292;&#24182;&#19988;&#19987;&#38376;&#20026;&#20266;&#24067;&#23572;&#21644;&#32447;&#24615;&#32422;&#26463;&#35774;&#35745;&#30340;&#26032;&#29305;&#24449;&#33021;&#22815;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.09342</link><description>&lt;p&gt;
&#23398;&#20064;&#36873;&#25321;&#20266;&#24067;&#23572;&#21644;&#32447;&#24615;&#25972;&#25968;&#32422;&#26463;&#30340;SAT&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Learning to Select SAT Encodings for Pseudo-Boolean and Linear Integer Constraints. (arXiv:2307.09342v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09342
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36873;&#25321;&#20266;&#24067;&#23572;&#21644;&#32447;&#24615;&#25972;&#25968;&#32422;&#26463;&#30340;SAT&#32534;&#30721;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#19968;&#32452;&#29305;&#24449;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36873;&#25321;&#32534;&#30721;&#26041;&#24335;&#65292;&#24182;&#19988;&#19987;&#38376;&#20026;&#20266;&#24067;&#23572;&#21644;&#32447;&#24615;&#32422;&#26463;&#35774;&#35745;&#30340;&#26032;&#29305;&#24449;&#33021;&#22815;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#32422;&#26463;&#28385;&#36275;&#21644;&#20248;&#21270;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#23558;&#23427;&#20204;&#32534;&#30721;&#20026;&#24067;&#23572;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#65288;SAT&#65289;&#30340;&#23454;&#20363;&#26469;&#26377;&#25928;&#22320;&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#26368;&#31616;&#21333;&#30340;&#32422;&#26463;&#31867;&#22411;&#22312;&#25991;&#29486;&#20013;&#20063;&#26377;&#24456;&#22810;&#32534;&#30721;&#26041;&#24335;&#65292;&#24615;&#33021;&#24046;&#24322;&#24456;&#22823;&#65292;&#36873;&#25321;&#36866;&#24403;&#30340;&#32534;&#30721;&#26041;&#24335;&#23545;&#20110;&#32473;&#23450;&#30340;&#38382;&#39064;&#23454;&#20363;&#24182;&#19981;&#26159;&#19968;&#20214;&#31616;&#21333;&#30340;&#20107;&#24773;&#12290;&#25105;&#20204;&#37319;&#29992;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30740;&#31350;&#36873;&#25321;&#20266;&#24067;&#23572;&#21644;&#32447;&#24615;&#32422;&#26463;&#30340;&#32534;&#30721;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#20351;&#29992;&#26631;&#20934;&#32422;&#26463;&#38382;&#39064;&#30340;&#29305;&#24449;&#38598;&#26469;&#26377;&#25928;&#22320;&#36873;&#25321;&#32534;&#30721;&#26041;&#24335;&#65307;&#28982;&#32780;&#65292;&#25105;&#20204;&#20351;&#29992;&#19987;&#38376;&#20026;&#20266;&#24067;&#23572;&#21644;&#32447;&#24615;&#32422;&#26463;&#35774;&#35745;&#30340;&#19968;&#32452;&#26032;&#29305;&#24449;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#20107;&#23454;&#19978;&#65292;&#22312;&#36873;&#25321;&#26410;&#35265;&#36807;&#30340;&#38382;&#39064;&#31867;&#21035;&#30340;&#32534;&#30721;&#26041;&#24335;&#26102;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#32467;&#26524;&#12290;&#24403;&#20351;&#29992;&#30456;&#21516;&#30340;&#29305;&#24449;&#38598;&#26102;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982;AutoFolio&#30456;&#27604;&#34920;&#29616;&#33391;&#22909;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#23454;&#20363;&#29305;&#24449;&#23545;&#20110;&#36873;&#25321;&#32534;&#30721;&#26041;&#24335;&#20219;&#21153;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many constraint satisfaction and optimisation problems can be solved effectively by encoding them as instances of the Boolean Satisfiability problem (SAT). However, even the simplest types of constraints have many encodings in the literature with widely varying performance, and the problem of selecting suitable encodings for a given problem instance is not trivial. We explore the problem of selecting encodings for pseudo-Boolean and linear constraints using a supervised machine learning approach. We show that it is possible to select encodings effectively using a standard set of features for constraint problems; however we obtain better performance with a new set of features specifically designed for the pseudo-Boolean and linear constraints. In fact, we achieve good results when selecting encodings for unseen problem classes. Our results compare favourably to AutoFolio when using the same feature set. We discuss the relative importance of instance features to the task of selecting the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#33258;&#36866;&#24212;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#38750;&#27491;&#24577;&#28176;&#36817;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#36866;&#24212;&#32447;&#24615;&#20272;&#35745;&#26041;&#31243;&#26500;&#24314;&#21435;&#20559;&#20272;&#35745;&#37327;&#65292;&#24182;&#22312;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#32972;&#26223;&#19979;&#20445;&#25345;&#20102;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#37327;&#30340;&#38750;&#28176;&#36817;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.07320</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#32447;&#24615;&#20272;&#35745;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Adaptive Linear Estimating Equations. (arXiv:2307.07320v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#33258;&#36866;&#24212;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#38750;&#27491;&#24577;&#28176;&#36817;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#36866;&#24212;&#32447;&#24615;&#20272;&#35745;&#26041;&#31243;&#26500;&#24314;&#21435;&#20559;&#20272;&#35745;&#37327;&#65292;&#24182;&#22312;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#32972;&#26223;&#19979;&#20445;&#25345;&#20102;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#37327;&#30340;&#38750;&#28176;&#36817;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#25968;&#25454;&#25910;&#38598;&#24050;&#25104;&#20026;&#22686;&#24378;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#25928;&#29575;&#30340;&#24191;&#27867;&#37319;&#29992;&#30340;&#25216;&#26415;&#12290;&#23613;&#31649;&#20855;&#26377;&#20248;&#21183;&#65292;&#20294;&#36825;&#31181;&#25968;&#25454;&#25910;&#38598;&#26426;&#21046;&#24120;&#24120;&#32473;&#32479;&#35745;&#25512;&#26029;&#36807;&#31243;&#24341;&#20837;&#22797;&#26434;&#24615;&#12290;&#20363;&#22914;&#65292;&#22312;&#33258;&#36866;&#24212;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#65292;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#65288;OLS&#65289;&#20272;&#35745;&#37327;&#21487;&#33021;&#34920;&#29616;&#20986;&#38750;&#27491;&#24577;&#30340;&#28176;&#36817;&#34892;&#20026;&#65292;&#20174;&#32780;&#23545;&#20934;&#30830;&#30340;&#25512;&#26029;&#21644;&#35299;&#37322;&#25552;&#20986;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;&#21435;&#20559;&#20272;&#35745;&#37327;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#33258;&#36866;&#24212;&#32447;&#24615;&#20272;&#35745;&#26041;&#31243;&#30340;&#24605;&#24819;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#20445;&#35777;&#20102;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#23454;&#29616;&#36817;&#20284;&#26368;&#20248;&#28176;&#36817;&#26041;&#24046;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20272;&#35745;&#37327;&#30340;&#19968;&#20010;&#26174;&#33879;&#29305;&#28857;&#26159;&#65292;&#22312;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#30340;&#20272;&#35745;&#37327;&#20445;&#30041;&#20102;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#37327;&#30340;&#38750;&#28176;&#36817;&#24615;&#33021;&#65292;&#21516;&#26102;&#33719;&#24471;&#20102;&#28176;&#36817;&#27491;&#24577;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#24037;&#20316;&#35299;&#20915;&#20102;&#33258;&#36866;&#24212;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#38750;&#27491;&#24577;&#28176;&#36817;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#24182;&#20026;&#32479;&#35745;&#25512;&#26029;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential data collection has emerged as a widely adopted technique for enhancing the efficiency of data gathering processes. Despite its advantages, such data collection mechanism often introduces complexities to the statistical inference procedure. For instance, the ordinary least squares (OLS) estimator in an adaptive linear regression model can exhibit non-normal asymptotic behavior, posing challenges for accurate inference and interpretation. In this paper, we propose a general method for constructing debiased estimator which remedies this issue. It makes use of the idea of adaptive linear estimating equations, and we establish theoretical guarantees of asymptotic normality, supplemented by discussions on achieving near-optimal asymptotic variance. A salient feature of our estimator is that in the context of multi-armed bandits, our estimator retains the non-asymptotic performance of the least square estimator while obtaining asymptotic normality property. Consequently, this work
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#31354;&#38388;&#22810;&#26679;&#24615;&#24230;&#37327;&#65292;&#24182;&#36890;&#36807;&#23558;&#20854;&#32435;&#20837;&#31574;&#30053;&#31354;&#38388;&#21709;&#24212;&#39044;&#35328;&#26426;&#65288;PSRO&#65289;&#20013;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36924;&#36817;&#32435;&#20160;&#22343;&#34913;&#65288;NE&#65289;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.16884</link><description>&lt;p&gt;
&#38750;&#20256;&#36882;&#24615;&#28216;&#25103;&#30340;&#31574;&#30053;&#31354;&#38388;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Policy Space Diversity for Non-Transitive Games. (arXiv:2306.16884v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16884
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#31354;&#38388;&#22810;&#26679;&#24615;&#24230;&#37327;&#65292;&#24182;&#36890;&#36807;&#23558;&#20854;&#32435;&#20837;&#31574;&#30053;&#31354;&#38388;&#21709;&#24212;&#39044;&#35328;&#26426;&#65288;PSRO&#65289;&#20013;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36924;&#36817;&#32435;&#20160;&#22343;&#34913;&#65288;NE&#65289;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31574;&#30053;&#31354;&#38388;&#21709;&#24212;&#39044;&#35328;&#26426;&#65288;PSRO&#65289;&#26159;&#19968;&#31181;&#22312;&#22810;&#26234;&#33021;&#20307;&#38750;&#20256;&#36882;&#24615;&#28216;&#25103;&#20013;&#36817;&#20284;&#32435;&#20160;&#22343;&#34913;&#65288;NE&#65289;&#30340;&#37325;&#35201;&#31639;&#27861;&#26694;&#26550;&#12290;&#35768;&#22810;&#20043;&#21069;&#30340;&#30740;&#31350;&#19968;&#30452;&#22312;&#23581;&#35797;&#20419;&#36827;PSRO&#20013;&#30340;&#31574;&#30053;&#22810;&#26679;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#22810;&#26679;&#24615;&#24230;&#37327;&#30340;&#19968;&#20010;&#20027;&#35201;&#24369;&#28857;&#22312;&#20110;&#26356;&#22810;&#26679;&#21270;&#30340;&#20154;&#21475;&#65288;&#26681;&#25454;&#22810;&#26679;&#24615;&#24230;&#37327;&#65289;&#19981;&#19968;&#23450;&#24847;&#21619;&#30528;&#26356;&#22909;&#22320;&#36924;&#36817;NE&#65288;&#27491;&#22914;&#25105;&#20204;&#22312;&#35770;&#25991;&#20013;&#35777;&#26126;&#30340;&#37027;&#26679;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#26679;&#24615;&#24230;&#37327;&#65292;&#20854;&#25913;&#36827;&#20445;&#35777;&#20102;&#26356;&#22909;&#22320;&#36924;&#36817;NE&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#23454;&#29992;&#19988;&#26377;&#27491;&#24403;&#29702;&#30001;&#30340;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;&#29366;&#24577;-&#34892;&#21160;&#26679;&#26412;&#26469;&#20248;&#21270;&#25105;&#20204;&#30340;&#22810;&#26679;&#24615;&#24230;&#37327;&#12290;&#36890;&#36807;&#23558;&#22810;&#26679;&#24615;&#27491;&#21017;&#21270;&#32435;&#20837;PSRO&#20013;&#30340;&#26368;&#20339;&#24212;&#31572;&#27714;&#35299;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#26032;&#30340;PSRO&#21464;&#31181;&#65292;&#21363;&#31574;&#30053;&#31354;&#38388;&#22810;&#26679;&#24615;PSRO&#65288;PSD-PSRO&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;PSD-PSRO&#30340;&#25910;&#25947;&#24615;&#12290;&#32463;&#39564;&#19978;&#65292;&#22312;&#21508;&#31181;&#28216;&#25103;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;PSD-PSRO&#22312;&#20419;&#36827;&#31574;&#30053;&#22810;&#26679;&#24615;&#12289;&#25552;&#39640;&#36924;&#36817;NE&#25928;&#26524;&#26041;&#38754;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Policy-Space Response Oracles (PSRO) is an influential algorithm framework for approximating a Nash Equilibrium (NE) in multi-agent non-transitive games. Many previous studies have been trying to promote policy diversity in PSRO. A major weakness in existing diversity metrics is that a more diverse (according to their diversity metrics) population does not necessarily mean (as we proved in the paper) a better approximation to a NE. To alleviate this problem, we propose a new diversity metric, the improvement of which guarantees a better approximation to a NE. Meanwhile, we develop a practical and well-justified method to optimize our diversity metric using only state-action samples. By incorporating our diversity regularization into the best response solving in PSRO, we obtain a new PSRO variant, Policy Space Diversity PSRO (PSD-PSRO). We present the convergence property of PSD-PSRO. Empirically, extensive experiments on various games demonstrate that PSD-PSRO is more effective in prod
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26680;&#23725;&#22238;&#24402;&#38382;&#39064;&#65292;&#36890;&#36807;&#31561;&#20215;&#30340;&#30446;&#26631;&#20989;&#25968;&#24418;&#24335;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#19981;&#20165;&#21487;&#20197;&#20351;&#29992;&#20854;&#20182;&#24809;&#32602;&#26041;&#27861;&#65292;&#36824;&#33021;&#22815;&#20174;&#26799;&#24230;&#19979;&#38477;&#30340;&#35282;&#24230;&#30740;&#31350;&#26680;&#23725;&#22238;&#24402;&#12290;&#36890;&#36807;&#25552;&#21069;&#20572;&#27490;&#30340;&#27491;&#21017;&#21270;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#38381;&#21512;&#35299;&#65292;&#21363;&#26680;&#26799;&#24230;&#27969;&#65288;KGF&#65289;&#65292;&#24182;&#35777;&#26126;&#20102;KGF&#21644;KRR&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36824;&#23558;KRR&#27867;&#21270;&#65292;&#20351;&#29992;$\ell_1$&#21644;$\ell_\infty$&#24809;&#32602;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#24471;&#21040;&#30340;&#35299;&#19982;&#21069;&#21521;&#20998;&#27493;&#22238;&#24402;&#21644;&#31526;&#21495;&#26799;&#24230;&#19979;&#38477;&#32467;&#21512;&#25552;&#21069;&#20572;&#27490;&#24471;&#21040;&#30340;&#35299;&#38750;&#24120;&#30456;&#20284;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#37325;&#30340;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2306.16838</link><description>&lt;p&gt;
&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#26041;&#27861;&#35299;&#20915;&#26680;&#23725;&#22238;&#24402;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Kernel Ridge Regression with Gradient-Based Optimization Methods. (arXiv:2306.16838v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26680;&#23725;&#22238;&#24402;&#38382;&#39064;&#65292;&#36890;&#36807;&#31561;&#20215;&#30340;&#30446;&#26631;&#20989;&#25968;&#24418;&#24335;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#19981;&#20165;&#21487;&#20197;&#20351;&#29992;&#20854;&#20182;&#24809;&#32602;&#26041;&#27861;&#65292;&#36824;&#33021;&#22815;&#20174;&#26799;&#24230;&#19979;&#38477;&#30340;&#35282;&#24230;&#30740;&#31350;&#26680;&#23725;&#22238;&#24402;&#12290;&#36890;&#36807;&#25552;&#21069;&#20572;&#27490;&#30340;&#27491;&#21017;&#21270;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#38381;&#21512;&#35299;&#65292;&#21363;&#26680;&#26799;&#24230;&#27969;&#65288;KGF&#65289;&#65292;&#24182;&#35777;&#26126;&#20102;KGF&#21644;KRR&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36824;&#23558;KRR&#27867;&#21270;&#65292;&#20351;&#29992;$\ell_1$&#21644;$\ell_\infty$&#24809;&#32602;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#24471;&#21040;&#30340;&#35299;&#19982;&#21069;&#21521;&#20998;&#27493;&#22238;&#24402;&#21644;&#31526;&#21495;&#26799;&#24230;&#19979;&#38477;&#32467;&#21512;&#25552;&#21069;&#20572;&#27490;&#24471;&#21040;&#30340;&#35299;&#38750;&#24120;&#30456;&#20284;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#37325;&#30340;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#23725;&#22238;&#24402;&#65288;KRR&#65289;&#26159;&#32447;&#24615;&#23725;&#22238;&#24402;&#30340;&#38750;&#32447;&#24615;&#25512;&#24191;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;KRR&#30446;&#26631;&#20989;&#25968;&#30340;&#31561;&#20215;&#24418;&#24335;&#65292;&#20026;&#20351;&#29992;&#20854;&#20182;&#24809;&#32602;&#26041;&#27861;&#21644;&#20174;&#26799;&#24230;&#19979;&#38477;&#30340;&#35282;&#24230;&#30740;&#31350;&#26680;&#23725;&#22238;&#24402;&#25171;&#24320;&#20102;&#21487;&#33021;&#12290;&#36890;&#36807;&#36830;&#32493;&#26102;&#38388;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#38381;&#21512;&#35299;&#8212;&#8212;&#26680;&#26799;&#24230;&#27969;&#65288;KGF&#65289;&#65292;&#36890;&#36807;&#25552;&#21069;&#20572;&#27490;&#30340;&#27491;&#21017;&#21270;&#65292;&#35753;&#25105;&#20204;&#33021;&#22815;&#22312;KGF&#21644;KRR&#20043;&#38388;&#29702;&#35770;&#19978;&#30028;&#23450;&#24046;&#24322;&#12290;&#25105;&#20204;&#29992;$\ell_1$&#21644;$\ell_\infty$&#24809;&#32602;&#26041;&#27861;&#23558;KRR&#27867;&#21270;&#65292;&#24182;&#21033;&#29992;&#31867;&#20284;KGF&#21644;KRR&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20351;&#29992;&#36825;&#20123;&#24809;&#32602;&#26041;&#27861;&#24471;&#21040;&#30340;&#35299;&#19982;&#20351;&#29992;&#21069;&#21521;&#20998;&#27493;&#22238;&#24402;&#65288;&#20063;&#31216;&#20026;&#22352;&#26631;&#19979;&#38477;&#65289;&#21644;&#31526;&#21495;&#26799;&#24230;&#19979;&#38477;&#32467;&#21512;&#25552;&#21069;&#20572;&#27490;&#24471;&#21040;&#30340;&#35299;&#38750;&#24120;&#30456;&#20284;&#12290;&#22240;&#27492;&#65292;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#37325;&#30340;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kernel ridge regression, KRR, is a non-linear generalization of linear ridge regression. Here, we introduce an equivalent formulation of the objective function of KRR, opening up both for using other penalties than the ridge penalty and for studying kernel ridge regression from the perspective of gradient descent. Using a continuous-time perspective, we derive a closed-form solution, kernel gradient flow, KGF, with regularization through early stopping, which allows us to theoretically bound the differences between KGF and KRR. We generalize KRR by replacing the ridge penalty with the $\ell_1$ and $\ell_\infty$ penalties and utilize the fact that analogously to the similarities between KGF and KRR, the solutions obtained when using these penalties are very similar to those obtained from forward stagewise regression (also known as coordinate descent) and sign gradient descent in combination with early stopping. Thus the need for computationally heavy proximal gradient descent algorithms
&lt;/p&gt;</description></item><item><title>ERC&#26159;&#19968;&#31181;&#26032;&#30340;&#20540;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#21033;&#29992;&#26102;&#38388;&#24046;&#20998;&#21160;&#21147;&#23398;&#30340;&#29305;&#24449;&#23376;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#21644;&#31283;&#23450;&#30340;&#20540;&#20272;&#35745;&#36335;&#24452;&#12290;&#23454;&#39564;&#35777;&#26126;ERC&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#20540;&#20989;&#25968;&#30340;&#26041;&#24046;&#65292;&#24182;&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.16750</link><description>&lt;p&gt;
&#26102;&#38388;&#24046;&#20998;&#21160;&#21147;&#23398;&#30340;&#29305;&#24449;&#23376;&#31354;&#38388;&#21450;&#20854;&#22914;&#20309;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25913;&#36827;&#20540;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Eigensubspace of Temporal-Difference Dynamics and How It Improves Value Approximation in Reinforcement Learning. (arXiv:2306.16750v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16750
&lt;/p&gt;
&lt;p&gt;
ERC&#26159;&#19968;&#31181;&#26032;&#30340;&#20540;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#21033;&#29992;&#26102;&#38388;&#24046;&#20998;&#21160;&#21147;&#23398;&#30340;&#29305;&#24449;&#23376;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#21644;&#31283;&#23450;&#30340;&#20540;&#20272;&#35745;&#36335;&#24452;&#12290;&#23454;&#39564;&#35777;&#26126;ERC&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#20540;&#20989;&#25968;&#30340;&#26041;&#24046;&#65292;&#24182;&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20540;&#20272;&#35745;&#26041;&#27861;&#65292;&#21363;&#29305;&#24449;&#23376;&#31354;&#38388;&#35268;&#33539;&#21270;&#25209;&#35780;&#23478;&#65288;ERC&#65289;&#65292;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#12290; ERC&#21463;&#21040;&#20102;&#23545;&#26102;&#24207;&#24046;&#20998;&#65288;TD&#65289;&#26041;&#27861;&#20013;Q&#20540;&#20272;&#35745;&#35823;&#24046;&#21160;&#21147;&#23398;&#30340;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#35813;&#26041;&#27861;&#36981;&#24490;&#30001;&#19982;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#30456;&#20851;&#30340;&#36716;&#31227;&#26680;&#20851;&#32852;&#30340;1-&#29305;&#24449;&#23376;&#31354;&#38388;&#23450;&#20041;&#30340;&#36335;&#24452;&#12290;&#23427;&#25581;&#31034;&#20102;TD&#23398;&#20064;&#30340;&#19968;&#20010;&#22522;&#26412;&#24615;&#36136;&#65292;&#22312;&#20808;&#21069;&#30340;&#28145;&#24230;RL&#26041;&#27861;&#20013;&#26410;&#34987;&#20351;&#29992;&#12290;&#22312;ERC&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#21017;&#21270;&#22120;&#65292;&#25351;&#23548;&#36817;&#20284;&#35823;&#24046;&#36235;&#21521;&#20110;1-&#29305;&#24449;&#23376;&#31354;&#38388;&#65292;&#20174;&#32780;&#24471;&#21040;&#26356;&#39640;&#25928;&#31283;&#23450;&#30340;&#20540;&#20272;&#35745;&#36335;&#24452;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;ERC&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;&#27492;&#22806;&#65292;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;ERC&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#20540;&#20989;&#25968;&#30340;&#26041;&#24046;&#12290;&#22312;DMControl&#22522;&#20934;&#27979;&#35797;&#30340;26&#20010;&#20219;&#21153;&#20013;&#65292;ERC&#20248;&#20110;20&#20010;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#22312;Q&#20540;&#20272;&#35745;&#26041;&#38754;&#20063;&#26174;&#31034;&#20986;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel value approximation method, namely Eigensubspace Regularized Critic (ERC) for deep reinforcement learning (RL). ERC is motivated by an analysis of the dynamics of Q-value approximation error in the Temporal-Difference (TD) method, which follows a path defined by the 1-eigensubspace of the transition kernel associated with the Markov Decision Process (MDP). It reveals a fundamental property of TD learning that has remained unused in previous deep RL approaches. In ERC, we propose a regularizer that guides the approximation error tending towards the 1-eigensubspace, resulting in a more efficient and stable path of value approximation. Moreover, we theoretically prove the convergence of the ERC method. Besides, theoretical analysis and experiments demonstrate that ERC effectively reduces the variance of value functions. Among 26 tasks in the DMControl benchmark, ERC outperforms state-of-the-art methods for 20. Besides, it shows significant advantages in Q-value approxim
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25913;&#21892;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#30340;&#20844;&#24179;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#26816;&#27979;&#22120;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16635</link><description>&lt;p&gt;
&#25913;&#36827;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#20013;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Fairness in Deepfake Detection. (arXiv:2306.16635v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25913;&#21892;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#30340;&#20844;&#24179;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#26816;&#27979;&#22120;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#24180;&#26469;&#24050;&#32463;&#24320;&#21457;&#20986;&#26377;&#25928;&#30340;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#27169;&#22411;&#65292;&#20294;&#26159;&#19968;&#20123;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#24320;&#21457;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#27169;&#22411;&#26102;&#25152;&#20351;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#20559;&#35265;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#31181;&#26063;&#21644;/&#25110;&#24615;&#21035;&#30340;&#20154;&#32676;&#30340;&#19981;&#20844;&#24179;&#34920;&#29616;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#36825;&#20123;&#32676;&#20307;&#21463;&#21040;&#19981;&#20844;&#24179;&#30340;&#23450;&#20301;&#25110;&#34987;&#25490;&#38500;&#22312;&#26816;&#27979;&#20043;&#22806;&#65292;&#20174;&#32780;&#35753;&#34987;&#38169;&#35823;&#20998;&#31867;&#30340;&#28145;&#24230;&#20266;&#36896;&#25805;&#32437;&#33286;&#35770;&#24182;&#30772;&#22351;&#23545;&#27169;&#22411;&#30340;&#20449;&#20219;&#12290;&#34429;&#28982;&#36825;&#20123;&#30740;&#31350;&#30528;&#37325;&#20110;&#30830;&#23450;&#21644;&#35780;&#20272;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#20013;&#30340;&#19981;&#20844;&#24179;&#24615;&#65292;&#20294;&#30446;&#21069;&#36824;&#27809;&#26377;&#24320;&#21457;&#20986;&#35299;&#20915;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#31639;&#27861;&#23618;&#38754;&#20844;&#24179;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25913;&#36827;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#30340;&#20844;&#24179;&#24615;&#65292;&#20197;&#22312;&#19981;&#32771;&#34385;&#25110;&#32771;&#34385;&#20154;&#21475;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#20844;&#24179;&#30340;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#27169;&#22411;&#12290;&#23545;&#22235;&#20010;&#28145;&#24230;&#20266;&#36896;&#25968;&#25454;&#38598;&#21644;&#20116;&#20010;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the development of effective deepfake detection models in recent years, several recent studies have demonstrated that biases in the training data utilized to develop deepfake detection models can lead to unfair performance for demographic groups of different races and/or genders. Such can result in these groups being unfairly targeted or excluded from detection, allowing misclassified deepfakes to manipulate public opinion and erode trust in the model. While these studies have focused on identifying and evaluating the unfairness in deepfake detection, no methods have been developed to address the fairness issue of deepfake detection at the algorithm level. In this work, we make the first attempt to improve deepfake detection fairness by proposing novel loss functions to train fair deepfake detection models in ways that are agnostic or aware of demographic factors. Extensive experiments on four deepfake datasets and five deepfake detectors demonstrate the effectiveness and flexi
&lt;/p&gt;</description></item><item><title>&#39044;&#35757;&#32451;&#30340;transformer&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#23637;&#29616;&#20102;&#38750;&#36125;&#21494;&#26031;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#20854;&#22312;&#20219;&#21153;&#22810;&#26679;&#24615;&#38408;&#20540;&#20197;&#19979;&#34920;&#29616;&#31867;&#20284;&#20110;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#65292;&#32780;&#22312;&#38408;&#20540;&#20197;&#19978;&#26126;&#26174;&#20248;&#20110;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#65292;&#19982;&#23725;&#22238;&#24402;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2306.15063</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#20219;&#21153;&#22810;&#26679;&#24615;&#19982;&#22238;&#24402;&#38382;&#39064;&#20013;&#38750;&#36125;&#21494;&#26031;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Pretraining task diversity and the emergence of non-Bayesian in-context learning for regression. (arXiv:2306.15063v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15063
&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;transformer&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#23637;&#29616;&#20102;&#38750;&#36125;&#21494;&#26031;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#20854;&#22312;&#20219;&#21153;&#22810;&#26679;&#24615;&#38408;&#20540;&#20197;&#19979;&#34920;&#29616;&#31867;&#20284;&#20110;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#65292;&#32780;&#22312;&#38408;&#20540;&#20197;&#19978;&#26126;&#26174;&#20248;&#20110;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#65292;&#19982;&#23725;&#22238;&#24402;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;transformer&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#38054;&#20329;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65288;ICL&#65289;&#65306;&#23427;&#20204;&#21487;&#20197;&#20174;&#20165;&#25552;&#20379;&#22312;&#25552;&#31034;&#20013;&#30340;&#23569;&#37327;&#31034;&#20363;&#20013;&#23398;&#20064;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#26356;&#26032;&#20219;&#20309;&#26435;&#37325;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;ICL&#33021;&#22815;&#35299;&#20915;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#12289;&#22312;&#26412;&#36136;&#19978;&#19982;&#20043;&#21069;&#20219;&#21153;&#38750;&#24120;&#19981;&#21516;&#30340;&#26032;&#20219;&#21153;&#21527;&#65311;&#20026;&#20102;&#25506;&#32034;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#25913;&#21464;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#65292;&#30740;&#31350;&#20102;ICL&#22312;&#32447;&#24615;&#22238;&#24402;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#20102;&#20986;&#29616;ICL&#30340;&#20219;&#21153;&#22810;&#26679;&#24615;&#38408;&#20540;&#12290;&#22312;&#36825;&#20010;&#38408;&#20540;&#20197;&#19979;&#65292;&#39044;&#35757;&#32451;&#30340;transformer&#26080;&#27861;&#35299;&#20915;&#26410;&#35265;&#30340;&#22238;&#24402;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#30340;&#34892;&#20026;&#31867;&#20284;&#20110;&#20855;&#26377;&#38750;&#22810;&#26679;&#24615;&#39044;&#35757;&#32451;&#20219;&#21153;&#20998;&#24067;&#20316;&#20026;&#20808;&#39564;&#30340;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#12290;&#36229;&#36807;&#36825;&#20010;&#38408;&#20540;&#21518;&#65292;transformer&#26126;&#26174;&#20248;&#20110;&#36825;&#20010;&#20272;&#35745;&#22120;&#65307;&#23427;&#30340;&#34892;&#20026;&#19982;&#23725;&#22238;&#24402;&#19968;&#33268;&#65292;&#23545;$\textit{&#25152;&#26377;&#20219;&#21153;}$&#65292;&#21253;&#25324;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#65292;&#20855;&#26377;&#39640;&#26031;&#20808;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained transformers exhibit the remarkable ability of in-context learning (ICL): they can learn tasks from just a few examples provided in the prompt without updating any weights. This raises a foundational question: can ICL solve fundamentally $\textit{new}$ tasks that are very different from those seen during pretraining? To probe this question, we examine ICL's performance on linear regression while varying the diversity of tasks in the pretraining dataset. We empirically demonstrate a $\textit{task diversity threshold}$ for the emergence of ICL. Below this threshold, the pretrained transformer cannot solve unseen regression tasks as it behaves like a Bayesian estimator with the $\textit{non-diverse pretraining task distribution}$ as the prior. Beyond this threshold, the transformer significantly outperforms this estimator; its behavior aligns with that of ridge regression, corresponding to a Gaussian prior over $\textit{all tasks}$, including those not seen during pretraining. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#27010;&#29575;PAC-Bayes&#30028;&#38480;&#65292;&#22312;&#26377;&#30028;&#21644;&#19968;&#33324;&#23614;&#37096;&#34892;&#20026;&#30340;&#25439;&#22833;&#20013;&#22343;&#36866;&#29992;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#30028;&#38480;&#36824;&#33021;&#22815;&#20445;&#25345;&#38543;&#26102;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.12214</link><description>&lt;p&gt;
&#26356;&#22810;&#30340;PAC-Bayes Bounds&#65306;&#20174;&#26377;&#30028;&#25439;&#22833;&#21040;&#20855;&#26377;&#19968;&#33324;&#24615;&#23614;&#37096;&#34892;&#20026;&#30340;&#25439;&#22833;&#65292;&#21040;&#20219;&#20309;&#26102;&#38388;&#22343;&#26377;&#25928;&#30340;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
More PAC-Bayes bounds: From bounded losses, to losses with general tail behaviors, to anytime-validity. (arXiv:2306.12214v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#27010;&#29575;PAC-Bayes&#30028;&#38480;&#65292;&#22312;&#26377;&#30028;&#21644;&#19968;&#33324;&#23614;&#37096;&#34892;&#20026;&#30340;&#25439;&#22833;&#20013;&#22343;&#36866;&#29992;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#30028;&#38480;&#36824;&#33021;&#22815;&#20445;&#25345;&#38543;&#26102;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#25439;&#22833;&#25552;&#20986;&#20102;&#26032;&#30340;&#39640;&#27010;&#29575;PAC-Bayes&#30028;&#38480;&#12290;&#39318;&#20808;&#65292;&#38024;&#23545;&#26377;&#30028;&#33539;&#22260;&#30340;&#25439;&#22833;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Catoni&#30028;&#30340;&#21152;&#24378;&#29256;&#26412;&#65292;&#36866;&#29992;&#20110;&#25152;&#26377;&#21442;&#25968;&#20540;&#30340;&#32479;&#19968;&#30028;&#12290;&#36825;&#23548;&#33268;&#20102;&#26032;&#30340;&#24555;&#36895;&#36895;&#29575;&#21644;&#28151;&#21512;&#36895;&#29575;&#19978;&#38480;&#65292;&#36825;&#20123;&#19978;&#38480;&#21487;&#35299;&#37322;&#24615;&#24378;&#19988;&#27604;&#25991;&#29486;&#20013;&#20808;&#21069;&#30028;&#38480;&#26356;&#32039;&#12290;&#20854;&#27425;&#65292;&#38024;&#23545;&#26356;&#19968;&#33324;&#30340;&#23614;&#37096;&#34892;&#20026;&#30340;&#25439;&#22833;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;&#26080;&#21442;&#25968;&#19978;&#38480;&#65306;&#24403;&#25439;&#22833;&#30340;&#32047;&#31215;&#29983;&#25104;&#20989;&#25968;&#26377;&#30028;&#26102;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;PAC-Bayes Chernoff&#31867;&#27604;&#65292;&#21478;&#19968;&#20010;&#19978;&#38480;&#26159;&#25439;&#22833;&#30340;&#20108;&#38454;&#30697;&#26377;&#30028;&#12290;&#36825;&#20004;&#20010;&#19978;&#38480;&#26159;&#21033;&#29992;&#19968;&#31181;&#22522;&#20110;&#21487;&#33021;&#20107;&#20214;&#31354;&#38388;&#30340;&#31163;&#25955;&#21270;&#30340;&#26032;&#25216;&#26415;&#33719;&#24471;&#30340;&#65292;&#8220;&#22312;&#27010;&#29575;&#8221;&#21442;&#25968;&#20248;&#21270;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#36866;&#29992;&#20110;&#20219;&#20309;&#29616;&#26377;&#30028;&#38480;&#30340;&#31616;&#21333;&#25216;&#26415;&#23558;&#25152;&#26377;&#20808;&#21069;&#32467;&#26524;&#25193;&#23637;&#21040;&#20219;&#20309;&#26102;&#38388;&#26377;&#25928;&#30340;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present new high-probability PAC-Bayes bounds for different types of losses. Firstly, for losses with a bounded range, we present a strengthened version of Catoni's bound that holds uniformly for all parameter values. This leads to new fast rate and mixed rate bounds that are interpretable and tighter than previous bounds in the literature. Secondly, for losses with more general tail behaviors, we introduce two new parameter-free bounds: a PAC-Bayes Chernoff analogue when the loss' cumulative generating function is bounded, and a bound when the loss' second moment is bounded. These two bounds are obtained using a new technique based on a discretization of the space of possible events for the "in probability" parameter optimization problem. Finally, we extend all previous results to anytime-valid bounds using a simple technique applicable to any existing bound.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#21487;&#38752;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;TopP\&amp;R&#65292;&#36890;&#36807;&#24341;&#20837;&#25299;&#25169;&#21644;&#32479;&#35745;&#22788;&#29702;&#36827;&#34892;&#20005;&#26684;&#30340;&#25903;&#25345;&#20272;&#35745;&#12290;TopP\&amp;R&#20165;&#20445;&#30041;&#20855;&#26377;&#19968;&#23450;&#32622;&#20449;&#27700;&#24179;&#30340;&#20855;&#26377;&#25299;&#25169;&#21644;&#32479;&#35745;&#19978;&#37325;&#35201;&#24615;&#30340;&#29305;&#24449;&#65292;&#23545;&#20110;&#22122;&#22768;&#29305;&#24449;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08013</link><description>&lt;p&gt;
TopP\&amp;R: &#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#25903;&#25345;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
TopP\&amp;R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models. (arXiv:2306.08013v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#21487;&#38752;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;TopP\&amp;R&#65292;&#36890;&#36807;&#24341;&#20837;&#25299;&#25169;&#21644;&#32479;&#35745;&#22788;&#29702;&#36827;&#34892;&#20005;&#26684;&#30340;&#25903;&#25345;&#20272;&#35745;&#12290;TopP\&amp;R&#20165;&#20445;&#30041;&#20855;&#26377;&#19968;&#23450;&#32622;&#20449;&#27700;&#24179;&#30340;&#20855;&#26377;&#25299;&#25169;&#21644;&#32479;&#35745;&#19978;&#37325;&#35201;&#24615;&#30340;&#29305;&#24449;&#65292;&#23545;&#20110;&#22122;&#22768;&#29305;&#24449;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#21487;&#38752;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;&#24341;&#20837;&#25299;&#25169;&#21644;&#32479;&#35745;&#22788;&#29702;&#36827;&#34892;&#20005;&#26684;&#30340;&#25903;&#25345;&#20272;&#35745;&#12290;&#29616;&#26377;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22914;Inception Score&#65288;IS&#65289;&#65292;Fr\'echet Inception Distance&#65288;FID&#65289;&#20197;&#21450;Precision and Recall&#65288;P\&amp;R&#65289;&#30340;&#21464;&#20307;&#65292;&#20005;&#37325;&#20381;&#36182;&#20110;&#20174;&#26679;&#26412;&#29305;&#24449;&#20272;&#35745;&#30340;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#35780;&#20272;&#30340;&#36136;&#37327;&#23436;&#20840;&#21462;&#20915;&#20110;&#20854;&#21487;&#38752;&#24615;&#65292;&#20294;&#20854;&#20272;&#35745;&#30340;&#21487;&#38752;&#24615;&#24182;&#27809;&#26377;&#24471;&#21040;&#20005;&#32899;&#30340;&#35752;&#35770;&#65288;&#24182;&#34987;&#24573;&#35270;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25299;&#25169;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#65288;TopP\&amp;R&#65292;&#21457;&#38899;&#20026;&#8220;topper&#8221;&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#25903;&#25345;&#65292;&#20165;&#20445;&#30041;&#20855;&#26377;&#19968;&#23450;&#32622;&#20449;&#27700;&#24179;&#30340;&#20855;&#26377;&#25299;&#25169;&#21644;&#32479;&#35745;&#19978;&#37325;&#35201;&#24615;&#30340;&#29305;&#24449;&#12290;&#36825;&#19981;&#20165;&#20351;TopP\&amp;R&#23545;&#20110;&#22122;&#22768;&#29305;&#24449;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#19988;&#36824;&#25552;&#20379;&#20102;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TopP\&amp;R&#23545;&#20110;&#31163;&#32676;&#20540;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a robust and reliable evaluation metric for generative models by introducing topological and statistical treatments for rigorous support estimation. Existing metrics, such as Inception Score (IS), Fr\'echet Inception Distance (FID), and the variants of Precision and Recall (P\&amp;R), heavily rely on supports that are estimated from sample features. However, the reliability of their estimation has not been seriously discussed (and overlooked) even though the quality of the evaluation entirely depends on it. In this paper, we propose Topological Precision and Recall (TopP\&amp;R, pronounced 'topper'), which provides a systematic approach to estimating supports, retaining only topologically and statistically important features with a certain level of confidence. This not only makes TopP\&amp;R strong for noisy features, but also provides statistical consistency. Our theoretical and experimental results show that TopP\&amp;R is robust to outliers and non-independent and identically distributed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QHNet&#30340;SE(3)-&#31561;&#21464;&#32593;&#32476;&#65292;&#20855;&#26377;&#39640;&#25928;&#21644;&#31561;&#21464;&#24615;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;QHNet&#22312;&#26356;&#24555;&#30340;&#36895;&#24230;&#19979;&#23454;&#29616;&#20102;&#19982;&#20854;&#21487;&#27604;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#28040;&#32791;&#30340;&#20869;&#23384;&#23569;&#20102;50&#65285;&#12290;</title><link>http://arxiv.org/abs/2306.04922</link><description>&lt;p&gt;
&#39640;&#25928;&#21644;&#31561;&#21464;&#22270;&#32593;&#32476;&#39044;&#27979;&#37327;&#23376;&#21704;&#23494;&#39039;
&lt;/p&gt;
&lt;p&gt;
Efficient and Equivariant Graph Networks for Predicting Quantum Hamiltonian. (arXiv:2306.04922v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QHNet&#30340;SE(3)-&#31561;&#21464;&#32593;&#32476;&#65292;&#20855;&#26377;&#39640;&#25928;&#21644;&#31561;&#21464;&#24615;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;QHNet&#22312;&#26356;&#24555;&#30340;&#36895;&#24230;&#19979;&#23454;&#29616;&#20102;&#19982;&#20854;&#21487;&#27604;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#28040;&#32791;&#30340;&#20869;&#23384;&#23569;&#20102;50&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#29992;&#20110;&#37327;&#23376;&#21270;&#23398;&#21644;&#20957;&#32858;&#24577;&#29289;&#29702;&#20013;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#39044;&#27979;&#12290;&#25928;&#29575;&#21644;&#31561;&#21464;&#24615;&#26159;&#20004;&#20010;&#37325;&#35201;&#20294;&#20914;&#31361;&#30340;&#22240;&#32032;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;SE(3)-&#31561;&#21464;&#32593;&#32476;&#65292;&#31216;&#20026;QHNet&#65292;&#26082;&#23454;&#29616;&#20102;&#25928;&#29575;&#21448;&#23454;&#29616;&#20102;&#31561;&#21464;&#24615;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#36827;&#23637;&#22312;&#20110;QHNet&#26550;&#26500;&#30340;&#21019;&#26032;&#35774;&#35745;&#65292;&#23427;&#19981;&#20165;&#36981;&#23432;&#22522;&#26412;&#30340;&#23545;&#31216;&#24615;&#65292;&#36824;&#21487;&#20197;&#36890;&#36807;92&#65285;&#30340;&#24352;&#37327;&#31215;&#25968;&#37327;&#26469;&#20943;&#23569;&#25968;&#37327;&#12290;&#27492;&#22806;&#65292;QHNet&#22312;&#28041;&#21450;&#26356;&#22810;&#21407;&#23376;&#31867;&#22411;&#26102;&#65292;&#21487;&#20197;&#38450;&#27490;&#36890;&#36947;&#32500;&#24230;&#30340;&#25351;&#25968;&#22686;&#38271;&#12290;&#25105;&#20204;&#22312;MD17&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21253;&#25324;&#22235;&#20010;&#20998;&#23376;&#31995;&#32479;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;QHNet&#21487;&#20197;&#22312;&#26174;&#33879;&#26356;&#24555;&#30340;&#36895;&#24230;&#19979;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#21487;&#27604;&#25311;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#20854;&#31616;&#21270;&#30340;&#26550;&#26500;&#65292;&#25105;&#20204;&#30340;QHNet&#28040;&#32791;&#30340;&#20869;&#23384;&#23569;&#20102;50&#65285;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20316;&#20026;AIRS&#24211;&#30340;&#19968;&#37096;&#20998;&#20844;&#24320;&#20351;&#29992;&#65288;\url{https://github.com/divelab/AIRS}&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the prediction of the Hamiltonian matrix, which finds use in quantum chemistry and condensed matter physics. Efficiency and equivariance are two important, but conflicting factors. In this work, we propose a SE(3)-equivariant network, named QHNet, that achieves efficiency and equivariance. Our key advance lies at the innovative design of QHNet architecture, which not only obeys the underlying symmetries, but also enables the reduction of number of tensor products by 92\%. In addition, QHNet prevents the exponential growth of channel dimension when more atom types are involved. We perform experiments on MD17 datasets, including four molecular systems. Experimental results show that our QHNet can achieve comparable performance to the state of the art methods at a significantly faster speed. Besides, our QHNet consumes 50\% less memory due to its streamlined architecture. Our code is publicly available as part of the AIRS library (\url{https://github.com/divelab/AIRS}).
&lt;/p&gt;</description></item><item><title>&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21363;&#20351;&#20351;&#29992;&#38169;&#35823;&#30340;&#22870;&#21169;&#26631;&#31614;&#65292;&#20063;&#33021;&#20135;&#29983;&#33391;&#22909;&#30340;&#34920;&#29616;&#21644;&#23433;&#20840;&#30340;&#31574;&#30053;&#65292;&#36825;&#31181;&#40065;&#26834;&#24615;&#23646;&#24615;&#26159;&#30001;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24754;&#35266;&#20027;&#20041;&#21644;&#24120;&#35265;&#25968;&#25454;&#25910;&#38598;&#23454;&#36341;&#20013;&#30340;&#20559;&#35265;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#32467;&#26524;&#65292;&#36171;&#20104;&#20102;&#20195;&#29702;&#29983;&#23384;&#26412;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.03286</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#29983;&#23384;&#26412;&#33021;
&lt;/p&gt;
&lt;p&gt;
Survival Instinct in Offline Reinforcement Learning. (arXiv:2306.03286v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03286
&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21363;&#20351;&#20351;&#29992;&#38169;&#35823;&#30340;&#22870;&#21169;&#26631;&#31614;&#65292;&#20063;&#33021;&#20135;&#29983;&#33391;&#22909;&#30340;&#34920;&#29616;&#21644;&#23433;&#20840;&#30340;&#31574;&#30053;&#65292;&#36825;&#31181;&#40065;&#26834;&#24615;&#23646;&#24615;&#26159;&#30001;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24754;&#35266;&#20027;&#20041;&#21644;&#24120;&#35265;&#25968;&#25454;&#25910;&#38598;&#23454;&#36341;&#20013;&#30340;&#20559;&#35265;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#32467;&#26524;&#65292;&#36171;&#20104;&#20102;&#20195;&#29702;&#29983;&#23384;&#26412;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#34892;&#20026;&#30340;&#26032;&#35266;&#23519;&#65306;&#22312;&#35768;&#22810;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21363;&#20351;&#20351;&#29992;&#8220;&#38169;&#35823;&#8221;&#30340;&#22870;&#21169;&#26631;&#31614;&#65288;&#20363;&#22914;&#22312;&#25152;&#26377;&#22320;&#26041;&#37117;&#20026;&#38646;&#25110;&#26159;&#30495;&#23454;&#22870;&#21169;&#30340;&#36127;&#25968;&#65289;&#65292;&#20063;&#33021;&#20135;&#29983;&#33391;&#22909;&#30340;&#34920;&#29616;&#21644;&#23433;&#20840;&#30340;&#31574;&#30053;&#12290;&#36825;&#31181;&#29616;&#35937;&#19981;&#33021;&#20165;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#22238;&#25253;&#26368;&#22823;&#21270;&#30446;&#26631;&#26469;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;&#23427;&#36171;&#20104;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19968;&#23450;&#30340;&#40065;&#26834;&#24615;&#65292;&#36825;&#22312;&#20854;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#23545;&#24212;&#29289;&#20013;&#26159;&#19981;&#20856;&#22411;&#30340;&#65292;&#22240;&#20026;&#21518;&#32773;&#23545;&#22870;&#21169;&#35774;&#35745;&#25935;&#24863;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#27492;&#24778;&#20154;&#30340;&#40065;&#26834;&#24615;&#23646;&#24615;&#26159;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#24754;&#35266;&#20027;&#20041;&#27010;&#24565;&#21644;&#24120;&#35265;&#25968;&#25454;&#25910;&#38598;&#23454;&#36341;&#20013;&#26576;&#31181;&#20559;&#35265;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#32467;&#26524;&#12290;&#24754;&#35266;&#20027;&#20041;&#36171;&#20104;&#20102;&#20195;&#29702;&#29983;&#23384;&#26412;&#33021;&#65292;&#21363;&#38271;&#26399;&#20869;&#30041;&#22312;&#25968;&#25454;&#25903;&#25345;&#20013;&#30340;&#28608;&#21169;&#65292;&#32780;&#26377;&#38480;&#19988;&#26377;&#20559;&#35265;&#30340;&#25968;&#25454;&#35206;&#30422;&#36827;&#19968;&#27493;&#38480;&#21046;&#20102;&#29983;&#23384;&#34892;&#20026;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel observation about the behavior of offline reinforcement learning (RL) algorithms: on many benchmark datasets, offline RL can produce well-performing and safe policies even when trained with "wrong" reward labels, such as those that are zero everywhere or are negatives of the true rewards. This phenomenon cannot be easily explained by offline RL's return maximization objective. Moreover, it gives offline RL a degree of robustness that is uncharacteristic of its online RL counterparts, which are known to be sensitive to reward design. We demonstrate that this surprising robustness property is attributable to an interplay between the notion of pessimism in offline RL algorithms and a certain bias implicit in common data collection practices. As we prove in this work, pessimism endows the agent with a "survival instinct", i.e., an incentive to stay within the data support in the long term, while the limited and biased data coverage further constrains the set of survival 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#31243;&#24207;&#27169;&#25311;&#20102;&#33707;&#37324;&#26031;&#27700;&#36855;&#23467;&#65292;&#36890;&#36807;&#33258;&#21160;&#20998;&#31867;&#23548;&#33322;&#31574;&#30053;&#65292;&#20998;&#26512;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#31243;&#24207;&#20351;&#29992;&#30340;&#31574;&#30053;&#20998;&#24067;&#65292;&#24182;&#19982;&#23454;&#39564;&#32467;&#26524;&#20570;&#23545;&#27604;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;&#20102;&#31867;&#20284;&#20110;&#20154;&#21644;&#21870;&#40831;&#31867;&#21160;&#29289;&#30340;&#23398;&#20064;&#35268;&#24459;&#65292;&#21457;&#23637;&#20102;&#20869;&#37096;&#34920;&#24449;&#19982;&#23548;&#33322;&#31574;&#30053;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#20026;&#29983;&#29289;&#23398;&#29305;&#24449;&#30340;&#25506;&#32034;&#25552;&#20379;&#20102;&#19968;&#23450;&#30340;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2306.01066</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#33707;&#37324;&#26031;&#27700;&#36855;&#23467;&#23548;&#33322;&#31574;&#30053;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigating Navigation Strategies in the Morris Water Maze through Deep Reinforcement Learning. (arXiv:2306.01066v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#31243;&#24207;&#27169;&#25311;&#20102;&#33707;&#37324;&#26031;&#27700;&#36855;&#23467;&#65292;&#36890;&#36807;&#33258;&#21160;&#20998;&#31867;&#23548;&#33322;&#31574;&#30053;&#65292;&#20998;&#26512;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#31243;&#24207;&#20351;&#29992;&#30340;&#31574;&#30053;&#20998;&#24067;&#65292;&#24182;&#19982;&#23454;&#39564;&#32467;&#26524;&#20570;&#23545;&#27604;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;&#20102;&#31867;&#20284;&#20110;&#20154;&#21644;&#21870;&#40831;&#31867;&#21160;&#29289;&#30340;&#23398;&#20064;&#35268;&#24459;&#65292;&#21457;&#23637;&#20102;&#20869;&#37096;&#34920;&#24449;&#19982;&#23548;&#33322;&#31574;&#30053;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#20026;&#29983;&#29289;&#23398;&#29305;&#24449;&#30340;&#25506;&#32034;&#25552;&#20379;&#20102;&#19968;&#23450;&#30340;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23548;&#33322;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#25216;&#33021;&#65292;&#20854;&#22312;&#21160;&#29289;&#21644;&#20154;&#31867;&#30340;&#30740;&#31350;&#21382;&#21490;&#24736;&#20037;&#12290;&#26412;&#25991;&#36890;&#36807;&#27169;&#25311;2D&#30340;&#33707;&#37324;&#26031;&#27700;&#36855;&#23467;&#65292;&#35757;&#32451;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#31243;&#24207;&#65292;&#33258;&#21160;&#20998;&#31867;&#23548;&#33322;&#31574;&#30053;&#65292;&#20998;&#26512;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#31243;&#24207;&#20351;&#29992;&#30340;&#31574;&#30053;&#20998;&#24067;&#65292;&#24182;&#19982;&#23454;&#39564;&#25968;&#25454;&#36827;&#34892;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102;&#19982;&#20154;&#31867;&#21644;&#21870;&#40831;&#31867;&#21160;&#29289;&#30456;&#20284;&#30340;&#23398;&#20064;&#21160;&#24577;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#29305;&#23450;&#20110;&#29615;&#22659;&#30340;&#36741;&#21161;&#20219;&#21153;&#65292;&#24182;&#30740;&#31350;&#23545;&#20854;&#26377;&#29992;&#24615;&#30340;&#24433;&#21709;&#22240;&#32032;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#26368;&#20855;&#30410;&#22788;&#30340;&#20219;&#21153;&#21487;&#33021;&#26356;&#31526;&#21512;&#30495;&#23454;&#20195;&#29702;&#20351;&#29992;&#30340;&#29983;&#29289;&#23398;&#29305;&#24449;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20195;&#29702;&#31243;&#24207;&#31070;&#32463;&#32593;&#32476;&#28608;&#27963;&#20013;&#20869;&#37096;&#34920;&#24449;&#30340;&#21457;&#23637;&#12290;&#36825;&#20123;&#34920;&#24449;&#31867;&#20284;&#20110;&#21457;&#29616;&#20110;&#40736;&#33041;&#20013;&#30340;&#20301;&#32622;&#32454;&#32990;&#21644;&#22836;&#26041;&#21521;&#32454;&#32990;&#65292;&#20854;&#23384;&#22312;&#19982;&#20195;&#29702;&#31243;&#24207;&#20351;&#29992;&#30340;&#23548;&#33322;&#31574;&#30053;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Navigation is a complex skill with a long history of research in animals and humans. In this work, we simulate the Morris Water Maze in 2D to train deep reinforcement learning agents. We perform automatic classification of navigation strategies, analyze the distribution of strategies used by artificial agents, and compare them with experimental data to show similar learning dynamics as those seen in humans and rodents. We develop environment-specific auxiliary tasks and examine factors affecting their usefulness. We suggest that the most beneficial tasks are potentially more biologically feasible for real agents to use. Lastly, we explore the development of internal representations in the activations of artificial agent neural networks. These representations resemble place cells and head-direction cells found in mouse brains, and their presence has correlation to the navigation strategies that artificial agents employ.
&lt;/p&gt;</description></item><item><title>&#26080;&#27861;&#23398;&#20064;&#30340;&#25968;&#25454;&#38598;&#26041;&#27861;&#20855;&#26377;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#28508;&#21147;&#65292;&#20294;&#23454;&#38469;&#20351;&#29992;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#22312;&#26080;&#27861;&#23398;&#20064;&#30340;&#25968;&#25454;&#38598;&#19978;&#21487;&#20197;&#23398;&#20064;&#21040;&#26377;&#29992;&#30340;&#29305;&#24449;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#31616;&#21333;&#35268;&#21017;&#65292;&#36825;&#23545;&#22270;&#20687;&#20445;&#25252;&#30340;&#25928;&#26524;&#19981;&#30830;&#23450;&#12290;&#27492;&#22806;&#65292;&#32447;&#24615;&#21487;&#20998;&#30340;&#25200;&#21160;&#24182;&#19981;&#26159;&#35825;&#23548;&#23398;&#20064;&#25463;&#24452;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#22240;&#27492;&#19981;&#33021;&#20381;&#36182;&#23427;&#20204;&#12290;</title><link>http://arxiv.org/abs/2305.19254</link><description>&lt;p&gt;
&#26080;&#27861;&#23398;&#20064;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#32473;&#25105;&#20204;&#24102;&#26469;&#21738;&#20123;&#21551;&#31034;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Can We Learn from Unlearnable Datasets?. (arXiv:2305.19254v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19254
&lt;/p&gt;
&lt;p&gt;
&#26080;&#27861;&#23398;&#20064;&#30340;&#25968;&#25454;&#38598;&#26041;&#27861;&#20855;&#26377;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#28508;&#21147;&#65292;&#20294;&#23454;&#38469;&#20351;&#29992;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#22312;&#26080;&#27861;&#23398;&#20064;&#30340;&#25968;&#25454;&#38598;&#19978;&#21487;&#20197;&#23398;&#20064;&#21040;&#26377;&#29992;&#30340;&#29305;&#24449;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#31616;&#21333;&#35268;&#21017;&#65292;&#36825;&#23545;&#22270;&#20687;&#20445;&#25252;&#30340;&#25928;&#26524;&#19981;&#30830;&#23450;&#12290;&#27492;&#22806;&#65292;&#32447;&#24615;&#21487;&#20998;&#30340;&#25200;&#21160;&#24182;&#19981;&#26159;&#35825;&#23548;&#23398;&#20064;&#25463;&#24452;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#22240;&#27492;&#19981;&#33021;&#20381;&#36182;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26222;&#36941;&#36827;&#34892;&#32593;&#32476;&#29228;&#34411;&#30340;&#26102;&#20195;&#65292;&#26080;&#27861;&#23398;&#20064;&#30340;&#25968;&#25454;&#38598;&#26041;&#27861;&#20855;&#26377;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12289;&#38450;&#27490;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#30340;&#28508;&#21147;&#12290;&#20294;&#38500;&#20102;&#19968;&#20123;&#23454;&#38469;&#38480;&#21046;&#20351;&#24471;&#23427;&#20204;&#30340;&#20351;&#29992;&#19981;&#22826;&#21487;&#33021;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#20123;&#32467;&#26524;&#23545;&#20854;&#20445;&#25252;&#25968;&#25454;&#33021;&#21147;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#39318;&#20808;&#65292;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#22312;&#26080;&#27861;&#23398;&#20064;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#21482;&#20250;&#23398;&#20064;&#21040;&#25463;&#24452;&#65292;&#21363;&#24182;&#19981;&#36866;&#29992;&#20110;&#27867;&#21270;&#30340;&#31616;&#21333;&#35268;&#21017;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#32593;&#32476;&#23454;&#38469;&#19978;&#21487;&#20197;&#23398;&#20064;&#21040;&#26377;&#29992;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#36825;&#20123;&#29305;&#24449;&#21487;&#20197;&#37325;&#26032;&#21152;&#26435;&#20197;&#33719;&#24471;&#39640;&#27979;&#35797;&#24615;&#33021;&#65292;&#36825;&#34920;&#26126;&#22270;&#20687;&#30340;&#20445;&#25252;&#24182;&#19981;&#33021;&#24471;&#21040;&#20445;&#35777;&#12290;&#26080;&#27861;&#23398;&#20064;&#30340;&#25968;&#25454;&#38598;&#25454;&#20449;&#36890;&#36807;&#28155;&#21152;&#25200;&#21160;&#30340;&#32447;&#24615;&#21487;&#20998;&#24615;&#26469;&#35825;&#23548;&#23398;&#20064;&#25463;&#24452;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21453;&#20363;&#65292;&#35777;&#26126;&#20102;&#25200;&#21160;&#30340;&#32447;&#24615;&#21487;&#20998;&#24615;&#24182;&#19981;&#26159;&#24517;&#35201;&#26465;&#20214;&#12290;&#20026;&#20102;&#24378;&#35843;&#20026;&#20160;&#20040;&#19981;&#33021;&#20381;&#36182;&#32447;&#24615;&#21487;&#20998;&#30340;&#25200;&#21160;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#20132;&#25237;&#24433;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In an era of widespread web scraping, unlearnable dataset methods have the potential to protect data privacy by preventing deep neural networks from generalizing. But in addition to a number of practical limitations that make their use unlikely, we make a number of findings that call into question their ability to safeguard data. First, it is widely believed that neural networks trained on unlearnable datasets only learn shortcuts, simpler rules that are not useful for generalization. In contrast, we find that networks actually can learn useful features that can be reweighed for high test performance, suggesting that image protection is not assured. Unlearnable datasets are also believed to induce learning shortcuts through linear separability of added perturbations. We provide a counterexample, demonstrating that linear separability of perturbations is not a necessary condition. To emphasize why linearly separable perturbations should not be relied upon, we propose an orthogonal proje
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#28151;&#21512;&#31995;&#32479;&#20013;&#36890;&#36807;&#22686;&#24378;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#25490;&#38431;&#32593;&#32476;&#38382;&#39064;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#38477;&#20302;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.16483</link><description>&lt;p&gt;
&#22686;&#24378;&#26679;&#26412;&#19979;&#28151;&#21512;&#31995;&#32479;&#20013;&#26377;&#25928;&#24378;&#21270;&#23398;&#20064;&#21450;&#20854;&#22312;&#25490;&#38431;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sample Efficient Reinforcement Learning in Mixed Systems through Augmented Samples and Its Applications to Queueing Networks. (arXiv:2305.16483v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#28151;&#21512;&#31995;&#32479;&#20013;&#36890;&#36807;&#22686;&#24378;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#25490;&#38431;&#32593;&#32476;&#38382;&#39064;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#38477;&#20302;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#28041;&#21450;&#20855;&#26377;&#20004;&#31181;&#29366;&#24577;&#30340;&#31995;&#32479;&#65306;&#38543;&#26426;&#29366;&#24577;&#21644;&#20266;&#38543;&#26426;&#29366;&#24577;&#12290;&#22312;&#36825;&#31181;&#31995;&#32479;&#20013;&#65292;&#38543;&#26426;&#29366;&#24577;&#36981;&#24490;&#38543;&#26426;&#36716;&#31227;&#26680;&#32780;&#20266;&#38543;&#26426;&#29366;&#24577;&#30340;&#36716;&#31227;&#26159;&#22312;&#32473;&#23450;&#38543;&#26426;&#29366;&#24577;/&#36716;&#31227;&#30340;&#24773;&#20917;&#19979;&#26159;&#30830;&#23450;&#30340;&#12290;&#25105;&#20204;&#31216;&#36825;&#26679;&#30340;&#31995;&#32479;&#20026;&#28151;&#21512;&#31995;&#32479;&#12290;&#36825;&#31181;&#31995;&#32479;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#65292;&#21253;&#25324;&#21046;&#36896;&#31995;&#32479;&#12289;&#36890;&#20449;&#32593;&#32476;&#21644;&#25490;&#38431;&#32593;&#32476;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#20135;&#29983;&#22686;&#24378;&#30340;&#25968;&#25454;&#26679;&#26412;&#26469;&#21152;&#36895;&#23398;&#20064;&#30340;&#26679;&#26412;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#31639;&#27861;&#26159;&#25968;&#25454;&#39537;&#21160;&#30340;&#65292;&#24182;&#20174;&#23454;&#38469;&#21644;&#22686;&#24378;&#26679;&#26412;&#30340;&#25968;&#25454;&#26679;&#26412;&#20013;&#23398;&#20064;&#31574;&#30053;&#65292;&#20174;&#32780;&#26174;&#33879;&#25913;&#21892;&#20102;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;Fitted Q Iteration&#65288;FQI&#65289;&#19979;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#35777;&#26126;&#20102;&#23398;&#20064;&#31574;&#30053;&#30340;&#26368;&#20248;&#24615;&#24046;&#38543;&#30528;&#30495;&#23454;&#25968;&#25454;&#26679;&#26412;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#20943;&#23569;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#35299;&#20915;&#19968;&#31867;&#25490;&#38431;&#32593;&#32476;&#38382;&#39064;&#65292;&#20854;&#20013;&#28151;&#21512;&#29366;&#24577;&#31354;&#38388;&#33258;&#28982;&#32780;&#28982;&#22320;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers a class of reinforcement learning problems, which involve systems with two types of states: stochastic and pseudo-stochastic. In such systems, stochastic states follow a stochastic transition kernel while the transitions of pseudo-stochastic states are deterministic given the stochastic states/transitions. We refer to such systems as mixed systems, which are widely used in various applications, including manufacturing systems, communication networks, and queueing networks. We propose a sample efficient RL method that accelerates learning by generating augmented data samples. The proposed algorithm is data-driven and learns the policy from data samples from both real and augmented samples. This method significantly improves learning by reducing the sample complexity such that the dataset only needs to have sufficient coverage of the stochastic states. We analyze the sample complexity of the proposed method under Fitted Q Iteration (FQI) and demonstrate that the opti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#27010;&#24565;&#23398;&#20064;&#65292;&#23427;&#21487;&#20197;&#25552;&#39640;&#22522;&#20110;&#27010;&#24565;&#30340;Transformer&#27169;&#22411;&#30340;&#20998;&#31867;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15775</link><description>&lt;p&gt;
&#20197;&#27010;&#24565;&#20026;&#20013;&#24515;&#30340;Transformer&#65306;&#20855;&#26377;&#38754;&#21521;&#29289;&#20307;&#30340;&#27010;&#24565;&#23398;&#20064;&#65292;&#20197;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept-Centric Transformers: Concept Transformers with Object-Centric Concept Learning for Interpretability. (arXiv:2305.15775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#27010;&#24565;&#23398;&#20064;&#65292;&#23427;&#21487;&#20197;&#25552;&#39640;&#22522;&#20110;&#27010;&#24565;&#30340;Transformer&#27169;&#22411;&#30340;&#20998;&#31867;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#26426;&#21046;&#22823;&#22823;&#25552;&#39640;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35270;&#35273;&#12289;NLP&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20063;&#25552;&#20379;&#20102;&#24037;&#20855;&#26469;&#24110;&#21161;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#27010;&#24565;Transformer&#65288;CT&#65289;&#23558;Transformer&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#20174;&#20302;&#32423;&#36755;&#20837;&#29305;&#24449;&#27867;&#21270;&#21040;&#26356;&#25277;&#35937;&#30340;&#20013;&#38388;&#23618;&#28508;&#22312;&#27010;&#24565;&#65292;&#26356;&#22909;&#22320;&#20801;&#35768;&#20154;&#31867;&#20998;&#26512;&#21592;&#30452;&#25509;&#35780;&#20272;&#35299;&#37322;&#20851;&#20110;&#20219;&#20309;&#29305;&#23450;&#36755;&#20986;&#20998;&#31867;&#30340;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;CT&#37319;&#29992;&#30340;&#27010;&#24565;&#23398;&#20064;&#40664;&#35748;&#20551;&#35774;&#31867;&#21035;&#20013;&#30340;&#27599;&#20010;&#22270;&#20687;&#37117;&#23545;&#34920;&#24449;&#35813;&#31867;&#21035;&#30340;&#27010;&#24565;&#20316;&#20986;&#20102;&#30456;&#21516;&#30340;&#36129;&#29486;&#65292;&#32780;&#20351;&#29992;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#27010;&#24565;&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#22909;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention mechanisms have greatly improved the performance of deep-learning models on visual, NLP, and multimodal tasks while also providing tools to aid in the model's interpretability. In particular, attention scores over input regions or concrete image features can be used to measure how much the attended elements contribute to the model inference. The recently proposed Concept Transformer (CT) generalizes the Transformer attention mechanism from such low-level input features to more abstract, intermediate-level latent concepts that better allow human analysts to more directly assess an explanation for the reasoning of the model about any particular output classification. However, the concept learning employed by CT implicitly assumes that across every image in a class, each image patch makes the same contribution to concepts that characterize membership in that class. Instead of using the CT's image-patch-centric concepts, object-centric concepts could lead to better classification
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#19968;&#31181;&#22522;&#20110;&#28857;&#31215;&#30340;&#23618;&#27425;&#32858;&#31867;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#26368;&#22823;&#24179;&#22343;&#28857;&#31215;&#21512;&#24182;&#32858;&#31867;&#65292;&#24182;&#19988;&#36755;&#20986;&#30340;&#26641;&#32467;&#26500;&#21487;&#29992;&#20110;&#20934;&#30830;&#20272;&#35745;&#25968;&#25454;&#30340;&#29983;&#25104;&#23618;&#27425;&#32467;&#26500;&#65292;&#26641;&#24418;&#24674;&#22797;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.15022</link><description>&lt;p&gt;
&#22522;&#20110;&#28857;&#31215;&#30340;&#23618;&#27425;&#32858;&#31867;&#21487;&#20197;&#24674;&#22797;&#38544;&#34255;&#30340;&#26641;&#24418;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Hierarchical clustering with dot products recovers hidden tree structure. (arXiv:2305.15022v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#19968;&#31181;&#22522;&#20110;&#28857;&#31215;&#30340;&#23618;&#27425;&#32858;&#31867;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#26368;&#22823;&#24179;&#22343;&#28857;&#31215;&#21512;&#24182;&#32858;&#31867;&#65292;&#24182;&#19988;&#36755;&#20986;&#30340;&#26641;&#32467;&#26500;&#21487;&#29992;&#20110;&#20934;&#30830;&#20272;&#35745;&#25968;&#25454;&#30340;&#29983;&#25104;&#23618;&#27425;&#32467;&#26500;&#65292;&#26641;&#24418;&#24674;&#22797;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;&#20110;&#24050;&#26377;&#20957;&#32858;&#32858;&#31867;&#31639;&#27861;&#30340;&#26032;&#35270;&#35282;&#65292;&#19987;&#27880;&#20110;&#23618;&#27425;&#32467;&#26500;&#30340;&#24674;&#22797;&#12290;&#25105;&#20204;&#24314;&#35758;&#19968;&#31181;&#31616;&#21333;&#30340;&#26631;&#20934;&#31639;&#27861;&#21464;&#20307;&#65292;&#20854;&#20013;&#32858;&#31867;&#26159;&#36890;&#36807;&#26368;&#22823;&#24179;&#22343;&#28857;&#31215;&#32780;&#19981;&#26159;&#26368;&#23567;&#36317;&#31163;&#25110;&#31751;&#20869;&#26041;&#24046;&#26469;&#21512;&#24182;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#27492;&#31639;&#27861;&#36755;&#20986;&#30340;&#26641;&#21487;&#20197;&#20316;&#20026;&#25968;&#25454;&#29983;&#25104;&#23618;&#27425;&#32467;&#26500;&#30340;&#21487;&#38752;&#20272;&#35745;&#12290;&#20851;&#38190;&#25216;&#26415;&#21019;&#26032;&#22312;&#20110;&#29702;&#35299;&#27169;&#22411;&#20013;&#30340;&#23618;&#27425;&#20449;&#24687;&#22914;&#20309;&#36716;&#21270;&#20026;&#21487;&#20174;&#25968;&#25454;&#20013;&#24674;&#22797;&#30340;&#26641;&#24418;&#20960;&#20309;&#20449;&#24687;&#65292;&#24182;&#21516;&#26102;&#22686;&#38271;&#26679;&#26412;&#22823;&#23567;&#21644;&#25968;&#25454;&#32500;&#25968;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65288;&#22914;UPGMA&#12289;Ward's&#26041;&#27861;&#21644;HDBSCAN&#65289;&#30340;&#26641;&#24418;&#24674;&#22797;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we offer a new perspective on the well established agglomerative clustering algorithm, focusing on recovery of hierarchical structure. We recommend a simple variant of the standard algorithm, in which clusters are merged by maximum average dot product and not, for example, by minimum distance or within-cluster variance. We demonstrate that the tree output by this algorithm provides a bona fide estimate of generative hierarchical structure in data, under a generic probabilistic graphical model. The key technical innovations are to understand how hierarchical information in this model translates into tree geometry which can be recovered from data, and to characterise the benefits of simultaneously growing sample size and data dimension. We demonstrate superior tree recovery performance with real data over existing approaches such as UPGMA, Ward's method, and HDBSCAN.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32852;&#37030;&#22522;&#30784;&#27169;&#22411;&#65288;FFMs&#65289;&#30340;&#27010;&#24565;&#65292;&#32467;&#21512;&#20102;&#22522;&#30784;&#27169;&#22411;&#21644;&#32852;&#37030;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#21487;&#23454;&#29616;&#36328;&#22810;&#20010;&#26426;&#26500;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#21327;&#20316;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.11414</link><description>&lt;p&gt;
&#32852;&#37030;&#22522;&#30784;&#27169;&#22411;&#65306;&#29992;&#20110;&#22823;&#27169;&#22411;&#30340;&#38544;&#31169;&#20445;&#25252;&#21327;&#20316;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models. (arXiv:2305.11414v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32852;&#37030;&#22522;&#30784;&#27169;&#22411;&#65288;FFMs&#65289;&#30340;&#27010;&#24565;&#65292;&#32467;&#21512;&#20102;&#22522;&#30784;&#27169;&#22411;&#21644;&#32852;&#37030;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#21487;&#23454;&#29616;&#36328;&#22810;&#20010;&#26426;&#26500;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#21327;&#20316;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#22914;BERT&#12289;GPT&#12289;ViT&#21644;CLIP&#65292;&#20294;&#20854;&#20248;&#21270;&#36890;&#24120;&#38656;&#35201;&#35775;&#38382;&#25935;&#24863;&#25968;&#25454;&#65292;&#24341;&#21457;&#38544;&#31169;&#38382;&#39064;&#24182;&#38480;&#21046;&#20854;&#36866;&#29992;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#32852;&#37030;&#22522;&#30784;&#27169;&#22411;&#65288;FFMs&#65289;&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#22522;&#30784;&#27169;&#22411;&#21644;&#32852;&#37030;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#21487;&#23454;&#29616;&#36328;&#22810;&#20010;&#26426;&#26500;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#21327;&#20316;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation Models (FMs), such as BERT, GPT, ViT, and CLIP, have demonstrated remarkable success in a wide range of applications, driven by their ability to leverage vast amounts of data for pre-training. However, optimizing FMs often requires access to sensitive data, raising privacy concerns and limiting their applicability in certain domains. In this paper, we introduce the concept of Federated Foundation Models (FFMs), a novel approach that combines the benefits of FMs and Federated Learning (FL) to enable privacy-preserving and collaborative learning across multiple institutions. We discuss the potential benefits and challenges of integrating FL into the lifespan of FMs, covering pre-training, fine-tuning, and application. We further provide formal definitions of FFM tasks, including FFM pre-training, FFM fine-tuning, and federated prompt engineering, allowing for more personalized and context-aware models while maintaining data privacy. Moreover, we explore the possibility of cont
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21464;&#20998;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#28508;&#21464;&#37327;&#24314;&#27169;&#26469;&#20248;&#21270;&#35757;&#32451;&#65292;&#20801;&#35768;&#28789;&#27963;&#30340;&#35774;&#35745;&#36873;&#25321;&#20197;&#25913;&#21892;&#26657;&#20934;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#23545;&#20110;&#22495;&#22806;&#25968;&#25454;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#24471;&#21040;&#20102;&#20445;&#25345;&#12290;</title><link>http://arxiv.org/abs/2305.10406</link><description>&lt;p&gt;
&#21464;&#20998;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Variational Classification. (arXiv:2305.10406v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10406
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21464;&#20998;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#28508;&#21464;&#37327;&#24314;&#27169;&#26469;&#20248;&#21270;&#35757;&#32451;&#65292;&#20801;&#35768;&#28789;&#27963;&#30340;&#35774;&#35745;&#36873;&#25321;&#20197;&#25913;&#21892;&#26657;&#20934;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#23545;&#20110;&#22495;&#22806;&#25968;&#25454;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#24471;&#21040;&#20102;&#20445;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30340;&#26032;&#22411;&#25193;&#23637;&#65292;&#31216;&#20026;&#21464;&#20998;&#20998;&#31867; (VC)&#12290;&#36890;&#36807;&#24341;&#20837;&#28508;&#21464;&#37327;&#24314;&#27169;&#65292;&#31867;&#20284;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#20256;&#32479;&#33258;&#32534;&#30721;&#22120;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#22522;&#20110;&#35777;&#25454;&#19979;&#30028; (ELBO) &#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#37319;&#29992;&#23545;&#25239;&#24615;&#26041;&#27861;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;VC&#27169;&#22411;&#20801;&#35768;&#22312;&#35774;&#35745;&#36873;&#25321;&#26041;&#38754;&#26356;&#21152;&#28789;&#27963;&#65292;&#29305;&#21035;&#26159;&#31867;&#26465;&#20214;&#28508;&#20808;&#39564;&#65292;&#32780;&#19981;&#26159;&#22312;&#29616;&#25104;&#30340;softmax&#20998;&#31867;&#22120;&#20013;&#20570;&#20986;&#30340;&#38544;&#24335;&#20551;&#35774;&#12290;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#25913;&#21892;&#20102;&#20854;&#20182;&#33391;&#22909;&#29305;&#24615;&#65292;&#22914;&#26657;&#20934;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21363;&#20351;&#24212;&#29992;&#20110;&#22495;&#22806;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel extension of the traditional neural network approach to classification tasks, referred to as variational classification (VC). By incorporating latent variable modeling, akin to the relationship between variational autoencoders and traditional autoencoders, we derive a training objective based on the evidence lower bound (ELBO), optimized using an adversarial approach. Our VC model allows for more flexibility in design choices, in particular class-conditional latent priors, in place of the implicit assumptions made in off-the-shelf softmax classifiers. Empirical evaluation on image and text classification datasets demonstrates the effectiveness of our approach in terms of maintaining prediction accuracy while improving other desirable properties such as calibration and adversarial robustness, even when applied to out-of-domain data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;GNN&#23398;&#20064;&#29615;&#22659;&#19979;&#30340;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#27604;&#36739;&#26694;&#26550;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#25991;&#29486;&#20013;&#23545;&#20110;&#22522;&#20110;GNN&#30340;&#31038;&#21306;&#26816;&#27979;&#32570;&#20047;&#20844;&#24179;&#19988;&#20005;&#35880;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.06026</link><description>&lt;p&gt;
&#25628;&#32034;UGLE&#30495;&#30456;&#65306;&#26080;&#30417;&#30563;GNN&#23398;&#20064;&#29615;&#22659;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Search for the UGLE Truth: An Investigation into Unsupervised GNN Learning Environments. (arXiv:2305.06026v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;GNN&#23398;&#20064;&#29615;&#22659;&#19979;&#30340;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#27604;&#36739;&#26694;&#26550;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#25991;&#29486;&#20013;&#23545;&#20110;&#22522;&#20110;GNN&#30340;&#31038;&#21306;&#26816;&#27979;&#32570;&#20047;&#20844;&#24179;&#19988;&#20005;&#35880;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476; (GNN) &#26159;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#23398;&#20064;&#22270;&#32467;&#26500;&#19978;&#30340;&#20989;&#25968;&#65292;&#36825;&#26159;&#19968;&#31181;&#24378;&#22823;&#21644;&#34920;&#36798;&#24615;&#24378;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#31038;&#21306;&#26816;&#27979;&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#20219;&#21153;&#65292;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;GNN&#36827;&#34892;&#12290;&#21033;&#29992;&#33410;&#28857;&#29305;&#24449;&#30340;&#22810;&#32500;&#24230;&#19982;&#22270;&#30340;&#36830;&#25509;&#24615;&#23545;&#22270;&#20013;&#30340;&#33410;&#28857;&#36827;&#34892;&#32858;&#31867;&#65292;&#23545;&#20174;&#31038;&#20132;&#32593;&#32476;&#21040;&#22522;&#22240;&#32452;&#23398;&#30340;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#26377;&#35768;&#22810;&#24212;&#29992;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30446;&#21069;&#25991;&#29486;&#20013;&#32570;&#20047;&#20844;&#24179;&#19988;&#20005;&#35880;&#35780;&#20272;&#22522;&#20110;GNN&#30340;&#31038;&#21306;&#26816;&#27979;&#30340;&#20805;&#20998;&#22522;&#20934;&#29615;&#22659;&#65292;&#20174;&#32780;&#21487;&#33021;&#38459;&#30861;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#29305;&#23450;&#22256;&#38590;&#26159;&#27169;&#31946;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#29615;&#22659;&#19982;&#24615;&#33021;&#21644;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#20914;&#31361;&#25351;&#26631;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21644;&#35780;&#20272;&#20102;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;GNN&#23398;&#20064;&#29615;&#22659;&#20013;&#36827;&#34892;&#19968;&#33268;&#30340;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#27604;&#36739;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#35780;&#20272;&#25351;&#26631;&#65292;&#21453;&#26144;&#20102;&#26816;&#27979;&#21040;&#30340;&#31038;&#21306;&#30340;&#20869;&#22312;&#36136;&#37327;&#20197;&#21450;&#32858;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are a pertinent tool for any machine learning task due to their ability to learn functions over graph structures, a powerful and expressive data representation. The detection of communities, an unsupervised task has increasingly been performed with GNNs. Clustering nodes in a graph using the multi-dimensionality of node features with the connectivity of the graph has many applications to real world tasks from social networks to genomics. Unfortunately, there is currently a gap in the literature with no established sufficient benchmarking environment for fairly and rigorously evaluating GNN based community detection, thereby potentially impeding progress in this nascent field. We observe the particular difficulties in this setting is the ambiguous hyperparameter tuning environments combined with conflicting metrics of performance and evaluation datasets. In this work, we propose and evaluate frameworks for the consistent comparisons of community detection al
&lt;/p&gt;</description></item><item><title>PTW&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#29983;&#25104;&#22120;&#27700;&#21360;&#25216;&#26415;&#65292;&#21487;&#20197;&#27604;&#20174;&#22836;&#24320;&#22987;&#27700;&#21360;&#25216;&#26415;&#24555;&#19977;&#20010;&#25968;&#37327;&#32423;&#21644;&#26356;&#22909;&#30340;&#20445;&#30041;&#20102;&#29983;&#25104;&#22120;&#30340;&#22270;&#20687;&#36136;&#37327;&#65292;&#35299;&#20915;&#20102;&#24694;&#24847;&#29992;&#25143;&#21033;&#29992;&#25552;&#20379;&#30340;&#27169;&#22411;&#21046;&#20316;&#20986;&#26377;&#23475;&#30340;&#28145;&#24230;&#20266;&#36896;&#32780;&#19981;&#20250;&#34987;&#21457;&#29616;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.07361</link><description>&lt;p&gt;
PTW: &#38024;&#23545;&#39044;&#35757;&#32451;&#22270;&#20687;&#29983;&#25104;&#22120;&#30340;&#20851;&#38190;&#35843;&#25972;&#22411;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
PTW: Pivotal Tuning Watermarking for Pre-Trained Image Generators. (arXiv:2304.07361v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07361
&lt;/p&gt;
&lt;p&gt;
PTW&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#29983;&#25104;&#22120;&#27700;&#21360;&#25216;&#26415;&#65292;&#21487;&#20197;&#27604;&#20174;&#22836;&#24320;&#22987;&#27700;&#21360;&#25216;&#26415;&#24555;&#19977;&#20010;&#25968;&#37327;&#32423;&#21644;&#26356;&#22909;&#30340;&#20445;&#30041;&#20102;&#29983;&#25104;&#22120;&#30340;&#22270;&#20687;&#36136;&#37327;&#65292;&#35299;&#20915;&#20102;&#24694;&#24847;&#29992;&#25143;&#21033;&#29992;&#25552;&#20379;&#30340;&#27169;&#22411;&#21046;&#20316;&#20986;&#26377;&#23475;&#30340;&#28145;&#24230;&#20266;&#36896;&#32780;&#19981;&#20250;&#34987;&#21457;&#29616;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20266;&#36896;&#26159;&#27867;&#25351;&#21033;&#29992;&#28145;&#24230;&#29983;&#25104;&#22120;&#32508;&#21512;&#20986;&#26469;&#30340;&#20869;&#23481;&#65292;&#33509;&#34987;&#8220;&#35823;&#29992;&#8221;&#65292;&#21487;&#20197;&#30772;&#22351;&#25968;&#23383;&#23186;&#20307;&#30340;&#20449;&#20219;&#12290;&#32780;&#21046;&#20316;&#39640;&#36136;&#37327;&#30340;&#28145;&#24230;&#20266;&#36896;&#38656;&#35201;&#25509;&#35302;&#22823;&#22411;&#21644;&#22797;&#26434;&#30340;&#29983;&#25104;&#22120;&#65292;&#21482;&#26377;&#23569;&#25968;&#23454;&#20307;&#21487;&#20197;&#35757;&#32451;&#21644;&#25552;&#20379;&#36825;&#20123;&#29983;&#25104;&#22120;&#12290;&#23041;&#32961;&#30340;&#26159;&#65292;&#24694;&#24847;&#29992;&#25143;&#20250;&#21033;&#29992;&#25552;&#20379;&#30340;&#27169;&#22411;&#21046;&#20316;&#20986;&#26377;&#23475;&#30340;&#28145;&#24230;&#20266;&#36896;&#65292;&#32780;&#19981;&#20250;&#34987;&#21457;&#29616;&#12290;&#20026;&#20102;&#21487;&#25506;&#27979;&#36215;&#26469;&#65292;&#28145;&#24230;&#20266;&#36896;&#38656;&#35201;&#22312;&#29983;&#25104;&#22120;&#20013;&#23884;&#20837;&#35782;&#21035;&#30721;&#65292;&#20197;&#20415;&#22312;&#21518;&#32493;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#36827;&#34892;&#25552;&#21462;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Pivotal Tuning Watermarking (PTW)&#65292;&#36825;&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#29983;&#25104;&#22120;&#27700;&#21360;&#25216;&#26415;&#65292;(i) &#27604;&#20174;&#22836;&#24320;&#22987;&#27700;&#21360;&#25216;&#26415;&#24555;&#19977;&#20010;&#25968;&#37327;&#32423;&#65292;(ii) &#19981;&#38656;&#35201;&#20219;&#20309;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#27700;&#21360;&#25216;&#26415;&#65292;&#21516;&#26102;&#32553;&#25918;&#21040;&#20102;&#27604;&#30456;&#20851;&#24037;&#20316;&#22823;4&#20493;&#30340;&#29983;&#25104;&#22120;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;PTW&#21487;&#20197;&#23884;&#20837;&#26356;&#38271;&#30340;&#35782;&#21035;&#30721;&#65292;&#21516;&#26102;&#26356;&#22909;&#22320;&#20445;&#30041;&#20102;&#29983;&#25104;&#22120;&#30340;&#22270;&#20687;&#36136;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20005;&#26684;&#30340;&#22522;&#20110;&#21338;&#24328;&#35770;&#30340;&#23450;&#20041;&#26469;&#35780;&#20272;&#27700;&#21360;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deepfakes refer to content synthesized using deep generators, which, when \emph{misused}, have the potential to erode trust in digital media. Synthesizing high-quality deepfakes requires access to large and complex generators only few entities can train and provide. The threat are malicious users that exploit access to the provided model and generate harmful deepfakes without risking detection. Watermarking makes deepfakes detectable by embedding an identifiable code into the generator that is later extractable from its generated images. We propose Pivotal Tuning Watermarking (PTW), a method for watermarking pre-trained generators (i) three orders of magnitude faster than watermarking from scratch and (ii) without the need for any training data. We improve existing watermarking methods and scale to generators $4 \times$ larger than related work. PTW can embed longer codes than existing methods while better preserving the generator's image quality. We propose rigorous, game-based defini
&lt;/p&gt;</description></item><item><title>FetMRQC &#26159;&#19968;&#31181;&#38024;&#23545;&#32974;&#20799;&#33041; MRI &#30340;&#33258;&#21160;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#21462;&#19968;&#31995;&#21015;&#36136;&#37327;&#25351;&#26631;&#21487;&#20197;&#39044;&#27979;&#19987;&#23478;&#35780;&#20998;&#65292;&#24182;&#33021;&#22815;&#22312;&#22823;&#37096;&#20998;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20934;&#30830;&#35780;&#20272;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.05879</link><description>&lt;p&gt;
FetMRQC: &#33258;&#21160;&#21270;&#32974;&#20799;&#33041; MRI &#36136;&#37327;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
FetMRQC: Automated Quality Control for fetal brain MRI. (arXiv:2304.05879v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05879
&lt;/p&gt;
&lt;p&gt;
FetMRQC &#26159;&#19968;&#31181;&#38024;&#23545;&#32974;&#20799;&#33041; MRI &#30340;&#33258;&#21160;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#21462;&#19968;&#31995;&#21015;&#36136;&#37327;&#25351;&#26631;&#21487;&#20197;&#39044;&#27979;&#19987;&#23478;&#35780;&#20998;&#65292;&#24182;&#33021;&#22815;&#22312;&#22823;&#37096;&#20998;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20934;&#30830;&#35780;&#20272;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36136;&#37327;&#25511;&#21046;&#19968;&#30452;&#34987;&#35748;&#20026;&#26159;&#30830;&#20445;&#31070;&#32463;&#24433;&#20687;&#23398;&#30740;&#31350;&#21487;&#38752;&#24615;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#23545;&#20110;&#32974;&#20799;&#33041; MRI &#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#32974;&#21160;&#39057;&#32321;&#19988;&#19981;&#21487;&#39044;&#27979;&#65292;&#20250;&#23548;&#33268;&#22270;&#20687;&#20013;&#20135;&#29983;&#20005;&#37325;&#20266;&#24433;&#12290;&#29616;&#26377;&#30340;&#32974;&#20799;&#33041;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;&#20165;&#20174; \textit{&#23618;&#38754;} &#32771;&#34385;&#65292;&#26080;&#27861;&#20840;&#38754;&#20102;&#35299;&#22270;&#20687;&#36136;&#37327;&#65292;&#32780;&#35780;&#20272;&#25972;&#20010;&#33041;&#23481;&#31215;&#25165;&#33021;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; FetMRQC&#65292;&#19968;&#31181;&#38024;&#23545;&#32974;&#20799;&#33041; MRI &#30340;&#33258;&#21160;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#25552;&#21462;&#19968;&#31995;&#21015;&#36136;&#37327;&#25351;&#26631;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#25351;&#26631;&#39044;&#27979;&#19987;&#23478;&#35780;&#20998;&#12290;&#22522;&#20110;&#22312;&#20004;&#20010;&#19981;&#21516;&#26426;&#26500;&#25910;&#38598;&#30340;&#36229;&#36807; 1000 &#20010;&#20302;&#20998;&#36776;&#29575;&#32974;&#20799;&#33041; MRI &#26679;&#26412;&#30340;&#25163;&#21160;&#35780;&#20998;&#65292;&#25105;&#20204;&#21457;&#29616; FetMRQC &#33021;&#22815;&#19968;&#33324;&#21270;&#21040;&#20854;&#20182;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#26159;&#21487;&#35299;&#37322;&#30340;&#21644;&#39640;&#25928;&#30340;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#39640;&#36136;&#37327;&#21644;&#20302;&#36136;&#37327;&#30340;&#32974;&#20799;&#33041; MRI &#23481;&#31215;&#21644;&#19987;&#23478;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quality control (QC) has long been considered essential to guarantee the reliability of neuroimaging studies. It is particularly important for fetal brain MRI, where large and unpredictable fetal motion can lead to substantial artifacts in the acquired images. Existing methods for fetal brain quality assessment operate at the \textit{slice} level, and fail to get a comprehensive picture of the quality of an image, that can only be achieved by looking at the \textit{entire} brain volume. In this work, we propose FetMRQC, a machine learning framework for automated image quality assessment tailored to fetal brain MRI, which extracts an ensemble of quality metrics that are then used to predict experts' ratings. Based on the manual ratings of more than 1000 low-resolution stacks acquired across two different institutions, we show that, compared with existing quality metrics, FetMRQC is able to generalize out-of-domain, while being interpretable and data efficient. We also release a novel ma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21435;&#20559;&#32622;&#30340;&#26041;&#27861;&#65292;&#19968;&#31181;&#36890;&#36807;&#22686;&#21152;2D&#25193;&#25955;&#27169;&#22411;&#24471;&#20986;&#30340;&#20998;&#25968;&#30340;&#25130;&#26029;&#20540;&#65292;&#19968;&#31181;&#36890;&#36807;&#35843;&#25972;&#35270;&#35282;&#25552;&#31034;&#21644;&#29289;&#20307;&#31354;&#38388;&#25668;&#20687;&#26426;&#23039;&#24577;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20266;&#24433;&#65292;&#25552;&#39640;&#30495;&#23454;&#24863;&#12290;</title><link>http://arxiv.org/abs/2303.15413</link><description>&lt;p&gt;
2D&#25193;&#25955;&#31639;&#27861;&#30340;&#21435;&#20559;&#32622;&#26041;&#27861;&#29992;&#20110;&#25991;&#26412;&#21040;3D&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Debiasing Scores and Prompts of 2D Diffusion for Robust Text-to-3D Generation. (arXiv:2303.15413v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21435;&#20559;&#32622;&#30340;&#26041;&#27861;&#65292;&#19968;&#31181;&#36890;&#36807;&#22686;&#21152;2D&#25193;&#25955;&#27169;&#22411;&#24471;&#20986;&#30340;&#20998;&#25968;&#30340;&#25130;&#26029;&#20540;&#65292;&#19968;&#31181;&#36890;&#36807;&#35843;&#25972;&#35270;&#35282;&#25552;&#31034;&#21644;&#29289;&#20307;&#31354;&#38388;&#25668;&#20687;&#26426;&#23039;&#24577;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20266;&#24433;&#65292;&#25552;&#39640;&#30495;&#23454;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#20013;&#20986;&#29616;&#30340;&#35270;&#35282;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#20063;&#31216;&#20026;Janus&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#26469;&#33258;&#20110;2D&#25193;&#25955;&#27169;&#22411;&#30340;&#22266;&#26377;&#20559;&#32622;&#65292;&#23548;&#33268;&#29983;&#25104;&#30340;3D&#23545;&#35937;&#19981;&#30495;&#23454;&#12290;&#36890;&#36807;&#23545;&#20854;&#36827;&#34892;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#21435;&#38500;&#20559;&#32622;&#20197;&#23454;&#29616;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#30340;&#40065;&#26834;&#24615;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#21483;&#20570;score debiasing&#65292;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;2D&#25193;&#25955;&#27169;&#22411;&#24471;&#20986;&#30340;&#20998;&#25968;&#30340;&#25130;&#26029;&#20540;&#65292;&#26469;&#36798;&#21040;&#21435;&#38500;&#20559;&#32622;&#30340;&#25928;&#26524;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#21483;&#20570;prompt debiasing&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30830;&#23450;&#29992;&#25143;&#25552;&#31034;&#21644;&#35270;&#35282;&#25552;&#31034;&#20043;&#38388;&#30340;&#30683;&#30462;&#35789;&#35821;&#65292;&#24182;&#35843;&#25972;&#35270;&#35282;&#25552;&#31034;&#21644;&#29289;&#20307;&#31354;&#38388;&#25668;&#20687;&#26426;&#23039;&#24577;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26174;&#33879;&#20943;&#23569;&#20266;&#24433;&#65292;&#25552;&#39640;&#20102;&#30495;&#23454;&#24863;&#65292;&#24182;&#22312;&#36136;&#37327;&#19982;&#36895;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The view inconsistency problem in score-distilling text-to-3D generation, also known as the Janus problem, arises from the intrinsic bias of 2D diffusion models, which leads to the unrealistic generation of 3D objects. In this work, we explore score-distilling text-to-3D generation and identify the main causes of the Janus problem. Based on these findings, we propose two approaches to debias the score-distillation frameworks for robust text-to-3D generation. Our first approach, called score debiasing, involves gradually increasing the truncation value for the score estimated by 2D diffusion models throughout the optimization process. Our second approach, called prompt debiasing, identifies conflicting words between user prompts and view prompts utilizing a language model and adjusts the discrepancy between view prompts and object-space camera poses. Our experimental results show that our methods improve realism by significantly reducing artifacts and achieve a good trade-off between fa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PU&#26041;&#27861;PULSNAR&#65292;&#21363;&#20351;&#22312;SCAR&#19981;&#25104;&#31435;&#26102;&#65292;&#20063;&#21487;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#945;&#20272;&#35745;&#21644;&#26657;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.08269</link><description>&lt;p&gt;
PULSNAR -- &#22312;SCAR&#20551;&#35774;&#19981;&#25104;&#31435;&#26102;&#36873;&#25321;&#27491;&#26080;&#26631;&#35760;&#23398;&#20064;&#65306;&#20998;&#31867;&#27604;&#20363;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
PULSNAR -- Positive unlabeled learning selected not at random: class proportion estimation when the SCAR assumption does not hold. (arXiv:2303.08269v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PU&#26041;&#27861;PULSNAR&#65292;&#21363;&#20351;&#22312;SCAR&#19981;&#25104;&#31435;&#26102;&#65292;&#20063;&#21487;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#945;&#20272;&#35745;&#21644;&#26657;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#26080;&#26631;&#35760;&#65288;PU&#65289;&#23398;&#20064;&#26159;&#21322;&#30417;&#30563;&#20108;&#20803;&#20998;&#31867;&#30340;&#19968;&#31181;&#65292;&#20854;&#20013;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21306;&#20998;&#19968;&#32452;&#27491;&#23454;&#20363;&#65288;&#24102;&#26377;&#26631;&#31614;&#65289;&#21644;&#19968;&#32452;&#26082;&#26377;&#27491;&#31867;&#21448;&#26377;&#36127;&#31867;&#23454;&#20363;&#65288;&#27809;&#26377;&#26631;&#31614;&#65289;&#12290;&#22312;&#30830;&#35748;&#36127;&#20363;&#19981;&#21487;&#29992;&#25110;&#38590;&#20197;&#33719;&#21462;&#65292;&#24182;&#19988;&#22312;&#26410;&#26631;&#35760;&#30340;&#23454;&#20363;&#20013;&#21457;&#29616;&#27491;&#20363;&#20855;&#26377;&#20215;&#20540;&#30340;&#24773;&#20917;&#19979;&#65288;&#20363;&#22914;&#65292;&#22312;&#26410;&#27979;&#35797;&#30340;&#21270;&#21512;&#29289;&#20013;&#25214;&#21040;&#21487;&#34892;&#33647;&#29289;&#65289;&#65292;PU&#23398;&#20064;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#22823;&#22810;&#25968;PU&#23398;&#20064;&#31639;&#27861;&#35748;&#20026;&#36873;&#25321;&#27491;&#23454;&#20363;&#29420;&#31435;&#20110;&#23427;&#20204;&#30340;&#29305;&#24449;&#65292;&#21363;&#36827;&#34892;&#23436;&#20840;&#38543;&#26426;&#36873;&#25321;&#65288;SCAR&#65289;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#20363;&#22914;&#21307;&#30103;&#20445;&#20581;&#65292;&#27491;&#23454;&#20363;&#19981;&#26159;SCAR&#65288;&#20363;&#22914;&#65292;&#20005;&#37325;&#24773;&#20917;&#26356;&#23481;&#26131;&#34987;&#35786;&#26029;&#20986;&#65289;&#65292;&#23548;&#33268;&#22312;&#26080;&#26631;&#35760;&#31034;&#20363;&#20013;&#20272;&#35745;&#38451;&#24615;&#27604;&#20363;&#945;&#21644;&#27169;&#22411;&#26657;&#20934;&#24615;&#33021;&#24046;&#65292;&#36827;&#32780;&#23548;&#33268;&#36873;&#25321;&#27491;&#20363;&#30340;&#19981;&#30830;&#23450;&#20915;&#31574;&#38408;&#20540;&#12290;PU&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#20272;&#35745;&#27599;&#20010;&#26410;&#26631;&#35760;&#23454;&#20363;&#26159;&#38451;&#24615;&#30340;&#27010;&#29575;&#26469;&#20272;&#35745;&#945;&#24182;&#25552;&#20379;&#26657;&#20934;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#24403;SCAR&#20551;&#35774;&#19981;&#25104;&#31435;&#26102;&#65292;&#29616;&#26377;&#26041;&#27861;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PU&#26041;&#27861;PULSNAR&#65292;&#21363;&#20351;&#22312;SCAR&#19981;&#25104;&#31435;&#26102;&#65292;&#20063;&#21487;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#945;&#20272;&#35745;&#21644;&#26657;&#20934;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#23545;&#26410;&#26631;&#35760;&#31034;&#20363;&#20998;&#24067;&#30340;&#26032;&#20551;&#35774;&#65292;&#31216;&#20026;&#38451;&#24615;&#22343;&#21248;&#26465;&#20214;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#20013;&#22343;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;PU&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Positive and Unlabeled (PU) learning is a type of semi-supervised binary classification where the machine learning algorithm differentiates between a set of positive instances (labeled) and a set of both positive and negative instances (unlabeled). PU learning has broad applications in settings where confirmed negatives are unavailable or difficult to obtain, and there is value in discovering positives among the unlabeled (e.g., viable drugs among untested compounds). Most PU learning algorithms make the selected completely at random (SCAR) assumption, namely that positives are selected independently of their features. However, in many real-world applications, such as healthcare, positives are not SCAR (e.g., severe cases are more likely to be diagnosed), leading to a poor estimate of the proportion, $\alpha$, of positives among unlabeled examples and poor model calibration, resulting in an uncertain decision threshold for selecting positives. PU learning algorithms can estimate $\alph
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#33258;&#36866;&#24212;&#20013;&#24515;&#34920;&#31034;&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#25209;&#27425;&#32423;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#25209;&#37327;&#24402;&#19968;&#21270;&#26469;&#35757;&#32451;&#29616;&#25104;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#22120;&#65292;&#21487;&#20197;&#33258;&#21160;&#38646;&#26679;&#26412;&#27867;&#21270;&#20026;&#26410;&#35265;&#36807;&#30340;AD&#20219;&#21153;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#31168;&#34920;&#29616;&#65292;&#23545;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#20102;&#38646;&#26679;&#26412;AD&#12290;</title><link>http://arxiv.org/abs/2302.07849</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#25209;&#27425;&#32423;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Batch-Level Anomaly Detection. (arXiv:2302.07849v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#33258;&#36866;&#24212;&#20013;&#24515;&#34920;&#31034;&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#25209;&#27425;&#32423;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#25209;&#37327;&#24402;&#19968;&#21270;&#26469;&#35757;&#32451;&#29616;&#25104;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#22120;&#65292;&#21487;&#20197;&#33258;&#21160;&#38646;&#26679;&#26412;&#27867;&#21270;&#20026;&#26410;&#35265;&#36807;&#30340;AD&#20219;&#21153;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#31168;&#34920;&#29616;&#65292;&#23545;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#20102;&#38646;&#26679;&#26412;AD&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#65288;AD&#65289;&#22312;&#35768;&#22810;&#23433;&#20840;&#20851;&#38190;&#30340;&#24212;&#29992;&#39046;&#22495;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#36866;&#24212;&#27491;&#24120;&#25968;&#25454;&#20998;&#24067;&#28418;&#31227;&#30340;&#24322;&#24120;&#26816;&#27979;&#22120;&#35843;&#25972;&#65292;&#29305;&#21035;&#26159;&#24403;&#27809;&#26377;&#38024;&#23545;&#8220;&#26032;&#27491;&#24120;&#8221;&#36827;&#34892;&#35757;&#32451;&#30340;&#25968;&#25454;&#26102;&#65292;&#36825;&#19968;&#25361;&#25112;&#23548;&#33268;&#20135;&#29983;&#20102;&#38646;&#26679;&#26412;AD&#25216;&#26415;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#36866;&#24212;&#20013;&#24515;&#34920;&#31034;&#65288;ACR&#65289;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#25209;&#27425;&#32423;AD&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#25209;&#37327;&#24402;&#19968;&#21270;&#26469;&#35757;&#32451;&#29616;&#25104;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#22120;&#65288;&#20363;&#22914;&#28145;&#24230;SVDD&#65289;&#26469;&#36866;&#24212;&#19968;&#32452;&#30456;&#20114;&#20851;&#32852;&#30340;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#65292;&#20351;&#20854;&#33021;&#22815;&#33258;&#21160;&#38646;&#26679;&#26412;&#27867;&#21270;&#20026;&#26410;&#35265;&#36807;&#30340;AD&#20219;&#21153;&#12290;&#36825;&#20010;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#25209;&#37327;&#24402;&#19968;&#21270;&#21152;&#20803;&#35757;&#32451;&#65292;&#26159;&#19968;&#31181;&#38750;&#24120;&#26377;&#25928;&#21644;&#22810;&#21151;&#33021;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#31532;&#19968;&#20010;&#38646;&#26679;&#26412;AD&#32467;&#26524;&#65292;&#24182;&#22312;&#26469;&#33258;&#19987;&#19994;&#39046;&#22495;&#30340;&#22270;&#20687;&#25968;&#25454;&#30340;&#38646;&#26679;&#26412;&#24322;&#24120;&#26816;&#27979;&#21644;&#20998;&#27573;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection (AD) plays a crucial role in many safety-critical application domains. The challenge of adapting an anomaly detector to drift in the normal data distribution, especially when no training data is available for the "new normal," has led to the development of zero-shot AD techniques. In this paper, we propose a simple yet effective method called Adaptive Centered Representations (ACR) for zero-shot batch-level AD. Our approach trains off-the-shelf deep anomaly detectors (such as deep SVDD) to adapt to a set of inter-related training data distributions in combination with batch normalization, enabling automatic zero-shot generalization for unseen AD tasks. This simple recipe, batch normalization plus meta-training, is a highly effective and versatile tool. Our results demonstrate the first zero-shot AD results for tabular data and outperform existing methods in zero-shot anomaly detection and segmentation on image data from specialized domains.
&lt;/p&gt;</description></item><item><title>MarioGPT&#26159;&#31532;&#19968;&#20010;&#25991;&#26412;&#21040;&#36229;&#32423;&#39532;&#37324;&#22885;&#20804;&#24351;&#28216;&#25103;&#20851;&#21345;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#24320;&#25918;&#24335;&#30340;&#12289;&#21487;&#25511;&#21046;&#30340;&#20851;&#21345;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2302.05981</link><description>&lt;p&gt;
MarioGPT: &#36890;&#36807;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24320;&#25918;&#24335;&#25991;&#26412;&#20851;&#21345;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
MarioGPT: Open-Ended Text2Level Generation through Large Language Models. (arXiv:2302.05981v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05981
&lt;/p&gt;
&lt;p&gt;
MarioGPT&#26159;&#31532;&#19968;&#20010;&#25991;&#26412;&#21040;&#36229;&#32423;&#39532;&#37324;&#22885;&#20804;&#24351;&#28216;&#25103;&#20851;&#21345;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#24320;&#25918;&#24335;&#30340;&#12289;&#21487;&#25511;&#21046;&#30340;&#20851;&#21345;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#31243;&#20869;&#23481;&#29983;&#25104;&#31639;&#27861;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#22797;&#26434;&#25968;&#19968;&#33268;&#30340;&#29615;&#22659;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#27969;&#31243;&#20869;&#23481;&#29983;&#25104;&#26041;&#27861;&#29983;&#25104;&#21453;&#26144;&#29305;&#23450;&#24847;&#22270;&#21644;&#38480;&#21046;&#30340;&#26377;&#24847;&#20041;&#20869;&#23481;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#27969;&#31243;&#20869;&#23481;&#29983;&#25104;&#31639;&#27861;&#32570;&#20047;&#20197;&#24320;&#25918;&#24335;&#26041;&#24335;&#29983;&#25104;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#19981;&#21516;&#39046;&#22495;&#37117;&#34920;&#29616;&#20986;&#20102;&#38750;&#24120;&#39640;&#30340;&#25928;&#29575;&#12290;&#36825;&#20123;&#35757;&#32451;&#26377;&#32032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#24494;&#35843;&#65292;&#37325;&#22797;&#20351;&#29992;&#20449;&#24687;&#24182;&#21152;&#36895;&#26032;&#20219;&#21153;&#30340;&#22521;&#35757;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MarioGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#20248;&#21270;&#30340;GPT2&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#22522;&#20110;&#29943;&#30742;&#30340;&#28216;&#25103;&#20851;&#21345;&#65292;&#25105;&#20204;&#20197;&#36229;&#32423;&#39532;&#37324;&#22885;&#20804;&#24351;&#30340;&#20851;&#21345;&#20026;&#20363;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;MarioGPT&#19981;&#20165;&#21487;&#20197;&#29983;&#25104;&#19981;&#21516;&#30340;&#28216;&#25103;&#20851;&#21345;&#65292;&#32780;&#19988;&#21487;&#20197;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#25511;&#21046;&#20851;&#21345;&#29983;&#25104;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;PCG&#25216;&#26415;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;MarioGPT&#26159;&#31532;&#19968;&#20010;&#25991;&#26412;&#21040;&#20851;&#21345;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Procedural Content Generation (PCG) algorithms provide a technique to generate complex and diverse environments in an automated way. However, while generating content with PCG methods is often straightforward, generating meaningful content that reflects specific intentions and constraints remains challenging. Furthermore, many PCG algorithms lack the ability to generate content in an open-ended manner. Recently, Large Language Models (LLMs) have shown to be incredibly effective in many diverse domains. These trained LLMs can be fine-tuned, re-using information and accelerating training for new tasks. In this work, we introduce MarioGPT, a fine-tuned GPT2 model trained to generate tile-based game levels, in our case Super Mario Bros levels. We show that MarioGPT can not only generate diverse levels, but can be text-prompted for controllable level generation, addressing one of the key challenges of current PCG techniques. As far as we know, MarioGPT is the first text-to-level model. We a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Koopman&#31639;&#23376;&#30340;&#38750;&#28176;&#36827;&#23398;&#20064;&#30028;&#38480;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#26102;&#38388;&#21487;&#36870;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#25193;&#23637;&#21160;&#24577;&#27169;&#20998;&#35299;&#65288;EDMD&#65289;&#21644;&#38477;&#20302;&#31209;&#22238;&#24402;&#65288;RRR&#65289;&#20004;&#31181;&#27969;&#34892;&#30340;&#20272;&#35745;&#22120;&#65292;&#24182;&#27604;&#36739;&#20102;&#23427;&#20204;&#30340;&#26041;&#24046;&#12290;</title><link>http://arxiv.org/abs/2302.02004</link><description>&lt;p&gt;
Koopman&#31639;&#23376;&#23398;&#20064;&#30340;&#23574;&#38160;&#35889;&#29575;
&lt;/p&gt;
&lt;p&gt;
Sharp Spectral Rates for Koopman Operator Learning. (arXiv:2302.02004v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Koopman&#31639;&#23376;&#30340;&#38750;&#28176;&#36827;&#23398;&#20064;&#30028;&#38480;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#26102;&#38388;&#21487;&#36870;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#25193;&#23637;&#21160;&#24577;&#27169;&#20998;&#35299;&#65288;EDMD&#65289;&#21644;&#38477;&#20302;&#31209;&#22238;&#24402;&#65288;RRR&#65289;&#20004;&#31181;&#27969;&#34892;&#30340;&#20272;&#35745;&#22120;&#65292;&#24182;&#27604;&#36739;&#20102;&#23427;&#20204;&#30340;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#21487;&#20197;&#26041;&#20415;&#22320;&#25551;&#36848;&#20026;&#30456;&#20851;&#30340;Koopman&#31639;&#23376;&#65292;&#20854;&#20316;&#29992;&#20351;&#31995;&#32479;&#30340;&#27599;&#20010;&#21487;&#35266;&#27979;&#37327;&#38543;&#26102;&#38388;&#21521;&#21069;&#28436;&#21270;&#12290;&#36890;&#36807;&#35768;&#22810;&#31639;&#27861;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;Koopman&#31639;&#23376;&#21450;&#20854;&#35889;&#20998;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;Koopman&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#20989;&#25968;&#30340;&#38750;&#28176;&#36827;&#23398;&#20064;&#30028;&#38480;&#12290;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#20102;&#26102;&#38388;&#21487;&#36870;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#65292;&#21253;&#25324;&#37325;&#35201;&#30340;Langevin&#21160;&#21147;&#23398;&#31034;&#20363;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20004;&#31181;&#27969;&#34892;&#30340;&#20272;&#35745;&#22120;&#65306;&#25193;&#23637;&#21160;&#24577;&#27169;&#20998;&#35299;&#65288;EDMD&#65289;&#21644;&#38477;&#20302;&#31209;&#22238;&#24402;&#65288;RRR&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20851;&#38190;&#20381;&#36182;&#20110;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#35823;&#24046;&#30340;&#26032;&#22855;&#26497;&#23567;&#21270;&#30028;&#38480;&#65292;&#36825;&#21487;&#33021;&#26159;&#29420;&#31435;&#24037;&#20316;&#30340;&#37325;&#35201;&#20869;&#23481;&#12290;&#25105;&#20204;&#30340;&#35889;&#23398;&#20064;&#36793;&#30028;&#21463;&#21040;&#31639;&#23376;&#33539;&#25968;&#35823;&#24046;&#21644;&#20272;&#35745;&#29305;&#24449;&#20989;&#25968;&#30340;&#26032;&#22411;&#24230;&#37327;&#22833;&#30495;&#20989;&#25968;&#30340;&#21516;&#26102;&#25511;&#21046;&#39537;&#21160;&#12290;&#36793;&#30028;&#34920;&#26126;&#65292;EDMD&#21644;RRR&#30340;&#26041;&#24046;&#30456;&#20284;&#65292;&#20294;EDMD&#20248;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Non-linear dynamical systems can be handily described by the associated Koopman operator, whose action evolves every observable of the system forward in time. Learning the Koopman operator and its spectral decomposition from data is enabled by a number of algorithms. In this work we present for the first time non-asymptotic learning bounds for the Koopman eigenvalues and eigenfunctions. We focus on time-reversal-invariant stochastic dynamical systems, including the important example of Langevin dynamics. We analyze two popular estimators: Extended Dynamic Mode Decomposition (EDMD) and Reduced Rank Regression (RRR). Our results critically hinge on novel minimax estimation bounds for the operator norm error, that may be of independent interest. Our spectral learning bounds are driven by the simultaneous control of the operator norm error and a novel metric distortion functional of the estimated eigenfunctions. The bounds indicates that both EDMD and RRR have similar variance, but EDMD su
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#33021;&#37327;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#39640;&#33021;&#29289;&#29702;&#20107;&#20214;&#65292;&#21487;&#29992;&#20110;&#21442;&#25968;&#21270;&#30340;&#20107;&#20214;&#29983;&#25104;&#65292;&#24322;&#24120;&#20449;&#21495;&#25506;&#27979;&#20197;&#21450;&#31890;&#23376;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2302.00695</link><description>&lt;p&gt;
&#22810;&#21151;&#33021;&#33021;&#37327;&#27010;&#29575;&#27169;&#22411;&#22312;&#39640;&#33021;&#29289;&#29702;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Versatile Energy-Based Probabilistic Models for High Energy Physics. (arXiv:2302.00695v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#33021;&#37327;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#39640;&#33021;&#29289;&#29702;&#20107;&#20214;&#65292;&#21487;&#29992;&#20110;&#21442;&#25968;&#21270;&#30340;&#20107;&#20214;&#29983;&#25104;&#65292;&#24322;&#24120;&#20449;&#21495;&#25506;&#27979;&#20197;&#21450;&#31890;&#23376;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#31181;&#32463;&#20856;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#20855;&#26377;&#33021;&#37327;&#20989;&#25968;&#24418;&#24335;&#28789;&#27963;&#24615;&#30340;&#22825;&#28982;&#20248;&#21183;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24314;&#27169;&#39640;&#32500;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#19982;&#36825;&#20123;&#36827;&#23637;&#19968;&#33268;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#33021;&#37327;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#26469;&#33258;&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#30340;&#39640;&#33021;&#29289;&#29702;&#20107;&#20214;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#19968;&#20010;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#25551;&#36848;&#20102;&#26356;&#39640;&#38454;&#30340;&#31890;&#23376;&#38388;&#30456;&#20114;&#20316;&#29992;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#32534;&#30721;&#20307;&#31995;&#32467;&#26500;&#21644;&#38544;&#24335;&#29983;&#25104;&#12290;&#22312;&#24212;&#29992;&#26041;&#38754;&#65292;&#23427;&#21487;&#20197;&#20316;&#20026;&#24378;&#22823;&#30340;&#21442;&#25968;&#21270;&#20107;&#20214;&#29983;&#25104;&#22120;&#29992;&#20110;&#29289;&#29702;&#20223;&#30495;&#65292;&#19968;&#31181;&#27867;&#29992;&#30340;&#26080;&#20551;&#35774;&#20851;&#32852;&#30340;&#24322;&#24120;&#20449;&#21495;&#25506;&#27979;&#22120;&#65292;&#20197;&#21450;&#29992;&#20110;&#31890;&#23376;&#35782;&#21035;&#30340;&#22686;&#24378;&#20107;&#20214;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a classical generative modeling approach, energy-based models have the natural advantage of flexibility in the form of the energy function. Recently, energy-based models have achieved great success in modeling high-dimensional data in computer vision and natural language processing. In line with these advancements, we build a multi-purpose energy-based probabilistic model for High Energy Physics events at the Large Hadron Collider. This framework builds on a powerful generative model and describes higher-order inter-particle interactions.It suits different encoding architectures and builds on implicit generation. As for applicational aspects, it can serve as a powerful parameterized event generator for physics simulation, a generic anomalous signal detector free from spurious correlations, and an augmented event classifier for particle identification.
&lt;/p&gt;</description></item><item><title>&#20449;&#24687;&#35770;&#21578;&#35785;&#25105;&#20204;&#65292;&#20026;&#20102;&#26368;&#22823;&#21270;&#27169;&#22411;&#30340;&#20449;&#24687;&#37327;&#65292;&#36873;&#25321;&#21487;&#33021;&#24615;&#26368;&#39640;&#25110;&#34920;&#31034;&#35823;&#24046;&#27604;&#29305;&#26368;&#23569;&#30340;&#23458;&#35266;&#20989;&#25968;&#12290;&#23558;&#19981;&#21516;&#30340;&#23458;&#35266;&#20989;&#25968;&#36716;&#25442;&#20026;&#20284;&#28982;&#20989;&#25968;&#65292;&#23427;&#20204;&#30340;&#30456;&#23545;&#22823;&#23567;&#34920;&#31034;&#25105;&#20204;&#24212;&#35813;&#26356;&#21916;&#27426;&#21738;&#20010;&#23458;&#35266;&#20989;&#25968;&#65292;&#32780;&#20854;&#22823;&#23567;&#30340;&#23545;&#25968;&#34920;&#31034;&#27169;&#22411;&#30340;&#39044;&#26399;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.06566</link><description>&lt;p&gt;
&#22914;&#20309;&#20351;&#29992;&#20449;&#24687;&#35770;&#36873;&#25321;&#23458;&#35266;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
How to select an objective function using information theory. (arXiv:2212.06566v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06566
&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#35770;&#21578;&#35785;&#25105;&#20204;&#65292;&#20026;&#20102;&#26368;&#22823;&#21270;&#27169;&#22411;&#30340;&#20449;&#24687;&#37327;&#65292;&#36873;&#25321;&#21487;&#33021;&#24615;&#26368;&#39640;&#25110;&#34920;&#31034;&#35823;&#24046;&#27604;&#29305;&#26368;&#23569;&#30340;&#23458;&#35266;&#20989;&#25968;&#12290;&#23558;&#19981;&#21516;&#30340;&#23458;&#35266;&#20989;&#25968;&#36716;&#25442;&#20026;&#20284;&#28982;&#20989;&#25968;&#65292;&#23427;&#20204;&#30340;&#30456;&#23545;&#22823;&#23567;&#34920;&#31034;&#25105;&#20204;&#24212;&#35813;&#26356;&#21916;&#27426;&#21738;&#20010;&#23458;&#35266;&#20989;&#25968;&#65292;&#32780;&#20854;&#22823;&#23567;&#30340;&#23545;&#25968;&#34920;&#31034;&#27169;&#22411;&#30340;&#39044;&#26399;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#25110;&#31185;&#23398;&#35745;&#31639;&#20013;&#65292;&#27169;&#22411;&#24615;&#33021;&#26159;&#36890;&#36807;&#23458;&#35266;&#20989;&#25968;&#34913;&#37327;&#30340;&#12290;&#20294;&#26159;&#20026;&#20160;&#20040;&#35201;&#36873;&#25321;&#26576;&#20010;&#23458;&#35266;&#20989;&#25968;&#32780;&#19981;&#26159;&#21478;&#19968;&#20010;&#65311;&#20449;&#24687;&#35770;&#32473;&#20986;&#20102;&#19968;&#20010;&#31572;&#26696;&#65306;&#20026;&#20102;&#26368;&#22823;&#21270;&#27169;&#22411;&#20013;&#30340;&#20449;&#24687;&#37327;&#65292;&#36873;&#25321;&#26368;&#26377;&#21487;&#33021;&#30340;&#23458;&#35266;&#20989;&#25968;&#25110;&#32773;&#20195;&#34920;&#35823;&#24046;&#30340;&#27604;&#29305;&#26368;&#23569;&#30340;&#20989;&#25968;&#12290;&#35201;&#35780;&#20272;&#19981;&#21516;&#30340;&#23458;&#35266;&#20989;&#25968;&#65292;&#23558;&#23427;&#20204;&#36716;&#25442;&#20026;&#20284;&#28982;&#20989;&#25968;&#12290;&#20316;&#20026;&#20284;&#28982;&#20989;&#25968;&#65292;&#23427;&#20204;&#30340;&#30456;&#23545;&#22823;&#23567;&#34920;&#31034;&#25105;&#20204;&#24212;&#35813;&#26356;&#21916;&#27426;&#21738;&#20010;&#23458;&#35266;&#20989;&#25968;&#65292;&#32780;&#20854;&#22823;&#23567;&#30340;&#23545;&#25968;&#34920;&#31034;&#27169;&#22411;&#30340;&#39044;&#26399;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In machine learning or scientific computing, model performance is measured with an objective function. But why choose one objective over another? Information theory gives one answer: To maximize the information in the model, select the most likely objective function or whichever represents the error in the fewest bits. To evaluate different objectives, transform them into likelihood functions. As likelihoods, their relative magnitudes represent how much we should prefer one objective versus another, and the log of their magnitude represents the expected uncertainty of the model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31038;&#21306;&#26816;&#27979;&#30446;&#26631;&#19982;&#20854;&#23545;&#24212;&#30340;&#38544;&#24335;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;&#30456;&#32852;&#31995;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#35745;&#31639;&#32593;&#32476;&#22312;&#20219;&#24847;&#30446;&#26631;&#19979;&#30340;&#25551;&#36848;&#38271;&#24230;&#65292;&#27604;&#36739;&#19981;&#21516;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#35775;&#38382;&#38544;&#24335;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2210.09186</link><description>&lt;p&gt;
&#38544;&#24335;&#27169;&#22411;&#12289;&#28508;&#22312;&#21387;&#32553;&#12289;&#20869;&#22312;&#20559;&#24046;&#21644;&#24265;&#20215;&#21320;&#39184;&#22312;&#31038;&#21306;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Implicit models, latent compression, intrinsic biases, and cheap lunches in community detection. (arXiv:2210.09186v6 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31038;&#21306;&#26816;&#27979;&#30446;&#26631;&#19982;&#20854;&#23545;&#24212;&#30340;&#38544;&#24335;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;&#30456;&#32852;&#31995;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#35745;&#31639;&#32593;&#32476;&#22312;&#20219;&#24847;&#30446;&#26631;&#19979;&#30340;&#25551;&#36848;&#38271;&#24230;&#65292;&#27604;&#36739;&#19981;&#21516;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#35775;&#38382;&#38544;&#24335;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#26816;&#27979;&#30340;&#20219;&#21153;&#26088;&#22312;&#23558;&#32593;&#32476;&#21010;&#20998;&#20026;&#33410;&#28857;&#38598;&#32676;&#65292;&#20197;&#24635;&#32467;&#20854;&#22823;&#35268;&#27169;&#32467;&#26500;&#65292;&#24050;&#32463;&#24341;&#20986;&#20102;&#35768;&#22810;&#20855;&#26377;&#19981;&#21516;&#30446;&#26631;&#30340;&#31454;&#20105;&#31639;&#27861;&#12290; &#19968;&#20123;&#31038;&#21306;&#26816;&#27979;&#26041;&#27861;&#26159;&#25512;&#26029;&#24615;&#30340;&#65292;&#36890;&#36807;&#27010;&#29575;&#29983;&#25104;&#27169;&#22411;&#26126;&#30830;&#22320;&#23548;&#20986;&#32858;&#31867;&#30446;&#26631;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#26159;&#25551;&#36848;&#24615;&#30340;&#65292;&#26681;&#25454;&#29305;&#23450;&#24212;&#29992;&#30340;&#30446;&#26631;&#23558;&#32593;&#32476;&#20998;&#25104;&#23376;&#38598;&#65292;&#36825;&#20351;&#24471;&#22312;&#21516;&#19968;&#35268;&#27169;&#19979;&#27604;&#36739;&#36825;&#20123;&#26041;&#27861;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#20219;&#20309;&#31038;&#21306;&#26816;&#27979;&#30446;&#26631;&#65288;&#25512;&#26029;&#24615;&#25110;&#25551;&#36848;&#24615;&#65289;&#19982;&#20854;&#30456;&#24212;&#30340;&#38544;&#24335;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;&#30456;&#32852;&#31995;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#35745;&#31639;&#32593;&#32476;&#21450;&#20854;&#22312;&#20219;&#24847;&#30446;&#26631;&#19979;&#30340;&#20998;&#21306;&#30340;&#25551;&#36848;&#38271;&#24230;&#65292;&#26080;&#38656;&#8220;&#22320;&#38754;&#23454;&#20917;&#8221;&#26631;&#31614;&#21363;&#21487;&#27604;&#36739;&#19981;&#21516;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#35775;&#38382;&#38544;&#24335;&#27169;&#22411;&#65292;&#36825;&#26159;&#20854;&#20182;&#26041;&#27861;&#25152;&#19981;&#20855;&#22791;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of community detection, which aims to partition a network into clusters of nodes to summarize its large-scale structure, has spawned the development of many competing algorithms with varying objectives. Some community detection methods are inferential, explicitly deriving the clustering objective through a probabilistic generative model, while other methods are descriptive, dividing a network according to an objective motivated by a particular application, making it challenging to compare these methods on the same scale. Here we present a solution to this problem that associates any community detection objective, inferential or descriptive, with its corresponding implicit network generative model. This allows us to compute the description length of a network and its partition under arbitrary objectives, providing a principled measure to compare the performance of different algorithms without the need for "ground truth" labels. Our approach also gives access to instances of the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21452;&#25511;&#21046;&#21464;&#37327;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#20943;&#23569;&#25968;&#25454;&#23376;&#25277;&#26679;&#21644;&#33945;&#29305;&#21345;&#32599;&#25277;&#26679;&#24102;&#26469;&#30340;&#26799;&#24230;&#20272;&#35745;&#26041;&#24046;&#65292;&#25552;&#39640;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.07290</link><description>&lt;p&gt;
&#21452;&#25511;&#21046;&#21464;&#37327;&#21152;&#36895;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Dual control variate for faster black-box variational inference. (arXiv:2210.07290v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21452;&#25511;&#21046;&#21464;&#37327;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#20943;&#23569;&#25968;&#25454;&#23376;&#25277;&#26679;&#21644;&#33945;&#29305;&#21345;&#32599;&#25277;&#26679;&#24102;&#26469;&#30340;&#26799;&#24230;&#20272;&#35745;&#26041;&#24046;&#65292;&#25552;&#39640;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#36125;&#21494;&#26031;&#21518;&#39564;&#25512;&#26029;&#26694;&#26550;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#26799;&#24230;&#20272;&#35745;&#20013;&#30340;&#39640;&#26041;&#24046;&#20250;&#25439;&#23475;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#36825;&#31181;&#26041;&#24046;&#26469;&#33258;&#20004;&#20010;&#38543;&#26426;&#28304;&#65306;&#25968;&#25454;&#23376;&#25277;&#26679;&#21644;&#33945;&#29305;&#21345;&#32599;&#25277;&#26679;&#12290;&#29616;&#26377;&#30340;&#25511;&#21046;&#21464;&#37327;&#20165;&#35299;&#20915;&#33945;&#29305;&#21345;&#32599;&#22122;&#22768;&#65292;&#32780;&#22686;&#37327;&#26799;&#24230;&#26041;&#27861;&#36890;&#24120;&#20165;&#35299;&#20915;&#25968;&#25454;&#23376;&#25277;&#26679;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#21452;&#8221;&#25511;&#21046;&#21464;&#37327;&#65292;&#33021;&#22815;&#21516;&#26102;&#20943;&#23569;&#20004;&#31181;&#22122;&#22768;&#28304;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#30830;&#35748;&#36825;&#23548;&#33268;&#20102;&#20943;&#23569;&#26041;&#24046;&#21644;&#22312;&#22810;&#20010;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#25552;&#39640;&#20248;&#21270;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Black-box variational inference is a widely-used framework for Bayesian posterior inference, but in some cases suffers from high variance in gradient estimates, harming accuracy and efficiency. This variance comes from two sources of randomness: Data subsampling and Monte Carlo sampling. Whereas existing control variates only address Monte Carlo noise and incremental gradient methods typically only address data subsampling, we propose a new "dual" control variate capable of jointly reducing variance from both sources of noise. We confirm that this leads to reduced variance and improved optimization in several real-world applications.
&lt;/p&gt;</description></item></channel></rss>