<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#35270;&#39057;&#27169;&#22411;&#8212;&#8212;VideoTaskformer&#12290;&#36890;&#36807;&#20174;&#25972;&#20307;&#19978;&#23398;&#20064;&#34920;&#31034;&#26469;&#39564;&#35777;&#35270;&#39057;&#30340;&#27491;&#30830;&#25191;&#34892;&#65292;&#24182;&#39044;&#27979;&#19979;&#19968;&#27493;&#39588;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#20063;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#26816;&#27979;&#25945;&#23398;&#35270;&#39057;&#20013;&#38169;&#35823;&#30340;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2303.13519</link><description>&lt;p&gt;
&#25945;&#23398;&#35270;&#39057;&#20013;&#20219;&#21153;&#32467;&#26500;&#30340;&#23398;&#20064;&#21644;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Learning and Verification of Task Structure in Instructional Videos. (arXiv:2303.13519v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13519
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#35270;&#39057;&#27169;&#22411;&#8212;&#8212;VideoTaskformer&#12290;&#36890;&#36807;&#20174;&#25972;&#20307;&#19978;&#23398;&#20064;&#34920;&#31034;&#26469;&#39564;&#35777;&#35270;&#39057;&#30340;&#27491;&#30830;&#25191;&#34892;&#65292;&#24182;&#39044;&#27979;&#19979;&#19968;&#27493;&#39588;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#20063;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#26816;&#27979;&#25945;&#23398;&#35270;&#39057;&#20013;&#38169;&#35823;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22312;&#32447;&#25945;&#23398;&#35270;&#39057;&#25968;&#37327;&#24222;&#22823;&#65292;&#20174;&#35270;&#39057;&#20013;&#23398;&#20064;&#22810;&#27493;&#39588;&#20219;&#21153;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#26159;&#19968;&#20010;&#35825;&#20154;&#30340;&#30446;&#26631;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#35270;&#39057;&#27169;&#22411;&#8212;&#8212;VideoTaskformer&#65292;&#19987;&#27880;&#20110;&#34920;&#31034;&#25945;&#23398;&#35270;&#39057;&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#30446;&#26631;&#26469;&#23545;VideoTaskformer&#36827;&#34892;&#39044;&#35757;&#32451;&#65306;&#20174;&#25945;&#23398;&#35270;&#39057;&#20013;&#38543;&#26426;&#23631;&#34109;&#30340;&#27493;&#39588;&#39044;&#27979;&#24369;&#30417;&#30563;&#30340;&#25991;&#26412;&#26631;&#31614;&#65288;&#36974;&#30422;&#27493;&#39588;&#24314;&#27169;&#65289;&#12290;&#19982;&#20808;&#21069;&#23398;&#20064;&#23616;&#37096;&#27493;&#39588;&#34920;&#31034;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20840;&#23616;&#23398;&#20064;&#65292;&#21033;&#29992;&#25972;&#20010;&#21608;&#22260;&#20219;&#21153;&#30340;&#35270;&#39057;&#20316;&#20026;&#19978;&#19979;&#25991;&#12290;&#20174;&#36825;&#20123;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#39564;&#35777;&#19968;&#20010;&#26410;&#35265;&#36807;&#30340;&#35270;&#39057;&#26159;&#21542;&#27491;&#30830;&#25191;&#34892;&#32473;&#23450;&#30340;&#20219;&#21153;&#65292;&#20197;&#21450;&#39044;&#27979;&#22312;&#32473;&#23450;&#27493;&#39588;&#20043;&#21518;&#21487;&#33021;&#37319;&#21462;&#21738;&#20123;&#27493;&#39588;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;&#22522;&#20934;&#26469;&#26816;&#27979;&#25945;&#23398;&#35270;&#39057;&#20013;&#30340;&#38169;&#35823;&#65292;&#20197;&#39564;&#35777;&#26159;&#21542;&#23384;&#22312;&#24322;&#24120;&#27493;&#39588;&#24182;&#26816;&#26597;&#27493;&#39588;&#26159;&#21542;&#25353;&#27491;&#30830;&#30340;&#39034;&#24207;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the enormous number of instructional videos available online, learning a diverse array of multi-step task models from videos is an appealing goal. We introduce a new pre-trained video model, VideoTaskformer, focused on representing the semantics and structure of instructional videos. We pre-train VideoTaskformer using a simple and effective objective: predicting weakly supervised textual labels for steps that are randomly masked out from an instructional video (masked step modeling). Compared to prior work which learns step representations locally, our approach involves learning them globally, leveraging video of the entire surrounding task as context. From these learned representations, we can verify if an unseen video correctly executes a given task, as well as forecast which steps are likely to be taken after a given step. We introduce two new benchmarks for detecting mistakes in instructional videos, to verify if there is an anomalous step and if steps are executed in the rig
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26041;&#27861;&#26469;&#25913;&#21892;&#24320;&#25918;&#24335;&#35789;&#27719;&#26816;&#27979;&#20013;&#29305;&#24449;&#23545;&#40784;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#22686;&#24378;&#25991;&#26412;&#23884;&#20837;&#12289;&#20462;&#25913;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#21644;&#26816;&#27979;&#22836;&#37096;&#12289;&#20197;&#21450;&#37319;&#29992;&#33258;&#23398;&#20064;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#32531;&#35299;&#27169;&#22411;&#22312;&#26410;&#35265;&#31867;&#19978;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.13518</link><description>&lt;p&gt;
&#25913;&#21892;&#24320;&#25918;&#24335;&#35789;&#27719;&#26816;&#27979;&#20013;&#29305;&#24449;&#23545;&#40784;&#30340;&#19977;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Three ways to improve feature alignment for open vocabulary detection. (arXiv:2303.13518v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13518
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26041;&#27861;&#26469;&#25913;&#21892;&#24320;&#25918;&#24335;&#35789;&#27719;&#26816;&#27979;&#20013;&#29305;&#24449;&#23545;&#40784;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#22686;&#24378;&#25991;&#26412;&#23884;&#20837;&#12289;&#20462;&#25913;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#21644;&#26816;&#27979;&#22836;&#37096;&#12289;&#20197;&#21450;&#37319;&#29992;&#33258;&#23398;&#20064;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#32531;&#35299;&#27169;&#22411;&#22312;&#26410;&#35265;&#31867;&#19978;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#24320;&#25918;&#24335;&#35789;&#27719;&#26816;&#27979;&#30340;&#26680;&#24515;&#38382;&#39064;&#22312;&#20110;&#22914;&#20309;&#23545;&#40784;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#65292;&#20197;&#20351;&#26816;&#27979;&#22120;&#22312;&#26410;&#35265;&#31867;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#29305;&#24449;&#37329;&#23383;&#22612;&#21644;&#26816;&#27979;&#22836;&#37096;&#65292;&#36825;&#30772;&#22351;&#20102;&#39044;&#35757;&#32451;&#26399;&#38388;&#24314;&#31435;&#30340;&#35270;&#35273;-&#25991;&#26412;&#29305;&#24449;&#23545;&#40784;&#65292;&#24182;&#19988;&#38590;&#20197;&#38450;&#27490;&#35821;&#35328;&#27169;&#22411;&#24536;&#35760;&#26410;&#35265;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#31616;&#21333;&#30340;&#26041;&#26696;&#26469;&#22686;&#24378;&#25991;&#26412;&#23884;&#20837;&#65292;&#38450;&#27490;&#36807;&#24230;&#25311;&#21512;&#21040;&#35757;&#32451;&#26399;&#38388;&#35265;&#21040;&#30340;&#23569;&#37327;&#31867;&#21035;&#65292;&#24182;&#21516;&#26102;&#33410;&#30465;&#20869;&#23384;&#21644;&#35745;&#31639;&#12290;&#20854;&#27425;&#65292;&#20462;&#25913;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#21644;&#26816;&#27979;&#22836;&#37096;&#65292;&#21253;&#25324;&#21487;&#35757;&#32451;&#38376;&#25511;&#24555;&#25463;&#26041;&#24335;&#65292;&#36825;&#40723;&#21169;&#35270;&#35273;-&#25991;&#26412;&#29305;&#24449;&#23545;&#40784;&#65292;&#24182;&#30830;&#20445;&#22312;&#26816;&#27979;&#35757;&#32451;&#24320;&#22987;&#26102;&#23454;&#29616;&#29305;&#24449;&#23545;&#40784;&#12290;&#26368;&#21518;&#65292;&#37319;&#29992;&#33258;&#23398;&#20064;&#26041;&#27861;&#21033;&#29992;&#26356;&#22823;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#35821;&#26009;&#24211;&#65292;&#20174;&#32780;&#25913;&#21892;&#26080;&#20154;&#31867;&#27880;&#37322;&#31867;&#21035;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The core problem in zero-shot open vocabulary detection is how to align visual and text features, so that the detector performs well on unseen classes. Previous approaches train the feature pyramid and detection head from scratch, which breaks the vision-text feature alignment established during pretraining, and struggles to prevent the language model from forgetting unseen classes.  We propose three methods to alleviate these issues. Firstly, a simple scheme is used to augment the text embeddings which prevents overfitting to a small number of classes seen during training, while simultaneously saving memory and computation. Secondly, the feature pyramid network and the detection head are modified to include trainable gated shortcuts, which encourages vision-text feature alignment and guarantees it at the start of detection training. Finally, a self-training approach is used to leverage a larger corpus of image-text pairs thus improving detection performance on classes with no human an
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#27010;&#24565;&#28040;&#34701;&#65292;&#21487;&#20197;&#28040;&#38500;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#29256;&#26435;&#38382;&#39064;&#21644;&#26679;&#26412;&#35760;&#24518;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.13516</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#27010;&#24565;&#28040;&#34701;
&lt;/p&gt;
&lt;p&gt;
Ablating Concepts in Text-to-Image Diffusion Models. (arXiv:2303.13516v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#27010;&#24565;&#28040;&#34701;&#65292;&#21487;&#20197;&#28040;&#38500;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#29256;&#26435;&#38382;&#39064;&#21644;&#26679;&#26412;&#35760;&#24518;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20855;&#26377;&#24378;&#22823;&#30340;&#32452;&#21512;&#33021;&#21147;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#22270;&#29255;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22312;&#25968;&#37327;&#24222;&#22823;&#30340;&#32593;&#32476;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24448;&#24448;&#21253;&#21547;&#26377;&#29256;&#26435;&#26448;&#26009;&#12289;&#25480;&#26435;&#22270;&#20687;&#21644;&#20010;&#20154;&#29031;&#29255;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#24050;&#32463;&#34987;&#21457;&#29616;&#33021;&#22815;&#27169;&#20223;&#19981;&#21516;&#33402;&#26415;&#23478;&#30340;&#39118;&#26684;&#25110;&#35760;&#20303;&#20934;&#30830;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;&#22914;&#20309;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#21435;&#38500;&#36825;&#20123;&#29256;&#26435;&#27010;&#24565;&#25110;&#22270;&#20687;&#65311;&#20026;&#20102;&#36798;&#25104;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#23454;&#29616;&#27010;&#24565;&#28040;&#34701;&#65292;&#21363;&#38450;&#27490;&#29983;&#25104;&#30446;&#26631;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#23398;&#20064;&#22914;&#20309;&#21305;&#37197;&#19968;&#20010;&#38170;&#23450;&#27010;&#24565;&#23545;&#24212;&#30340;&#22270;&#20687;&#20998;&#24067;&#21644;&#19982;&#30446;&#26631;&#39118;&#26684;&#12289;&#23454;&#20363;&#25110;&#25991;&#26412;&#25552;&#31034;&#30456;&#20851;&#30340;&#22270;&#20687;&#20998;&#24067;&#65292;&#20197;&#38450;&#27490;&#27169;&#22411;&#26681;&#25454;&#20854;&#25991;&#26412;&#26465;&#20214;&#29983;&#25104;&#30446;&#26631;&#27010;&#24565;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#22320;&#38450;&#27490;&#28040;&#34701;&#27010;&#24565;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale text-to-image diffusion models can generate high-fidelity images with powerful compositional ability. However, these models are typically trained on an enormous amount of Internet data, often containing copyrighted material, licensed images, and personal photos. Furthermore, they have been found to replicate the style of various living artists or memorize exact training samples. How can we remove such copyrighted concepts or images without retraining the model from scratch? To achieve this goal, we propose an efficient method of ablating concepts in the pretrained model, i.e., preventing the generation of a target concept. Our algorithm learns to match the image distribution for a target style, instance, or text prompt we wish to ablate to the distribution corresponding to an anchor concept. This prevents the model from generating target concepts given its text condition. Extensive experiments show that our method can successfully prevent the generation of the ablated conce
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29983;&#25104;&#26080;&#30028;3D&#19990;&#30028;&#30340;&#27169;&#22411;&#65292;&#25506;&#32034;&#20102;&#26080;&#26465;&#20214;&#21512;&#25104;&#26080;&#30028;&#33258;&#28982;&#22330;&#26223;&#30340;&#20219;&#21153;&#65292;&#20351;&#24471;&#22312;&#20445;&#25345;&#25345;&#20037;&#30340;3D&#19990;&#30028;&#27169;&#22411;&#30340;&#21516;&#26102;&#21487;&#20197;&#36827;&#34892;&#20219;&#24847;&#22823;&#30340;&#25668;&#20687;&#26426;&#36816;&#21160;&#12290;&#20854;&#22330;&#26223;&#34920;&#31034;&#21253;&#25324;&#21487;&#25193;&#23637;&#30340;&#24179;&#38754;&#22330;&#26223;&#24067;&#23616;&#32593;&#26684;&#20197;&#21450;&#20840;&#26223;&#22825;&#31354;&#31353;&#39030;&#65292;&#24182;&#20165;&#20174;&#21333;&#35270;&#22270;&#20114;&#32852;&#32593;&#29031;&#29255;&#20013;&#23398;&#20064;&#29983;&#25104;&#19990;&#30028;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21487;&#27169;&#25311;&#22312;3D&#26223;&#35266;&#20013;&#38271;&#26102;&#38388;&#30340;&#39134;&#34892;&#65292;&#21516;&#26102;&#25903;&#25345;&#19968;&#20010;&#25345;&#32493;&#30340;&#12289;&#25668;&#20687;&#26426;&#29420;&#31435;&#30340;&#19990;&#30028;&#34920;&#31034;&#65292;&#20855;&#26377;&#39640;&#25928;&#30340;&#35745;&#31639;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.13515</link><description>&lt;p&gt;
&#25345;&#32493;&#30340;&#33258;&#28982;&#65306;&#19968;&#20010;&#29983;&#25104;&#26080;&#30028;3D&#19990;&#30028;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Persistent Nature: A Generative Model of Unbounded 3D Worlds. (arXiv:2303.13515v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29983;&#25104;&#26080;&#30028;3D&#19990;&#30028;&#30340;&#27169;&#22411;&#65292;&#25506;&#32034;&#20102;&#26080;&#26465;&#20214;&#21512;&#25104;&#26080;&#30028;&#33258;&#28982;&#22330;&#26223;&#30340;&#20219;&#21153;&#65292;&#20351;&#24471;&#22312;&#20445;&#25345;&#25345;&#20037;&#30340;3D&#19990;&#30028;&#27169;&#22411;&#30340;&#21516;&#26102;&#21487;&#20197;&#36827;&#34892;&#20219;&#24847;&#22823;&#30340;&#25668;&#20687;&#26426;&#36816;&#21160;&#12290;&#20854;&#22330;&#26223;&#34920;&#31034;&#21253;&#25324;&#21487;&#25193;&#23637;&#30340;&#24179;&#38754;&#22330;&#26223;&#24067;&#23616;&#32593;&#26684;&#20197;&#21450;&#20840;&#26223;&#22825;&#31354;&#31353;&#39030;&#65292;&#24182;&#20165;&#20174;&#21333;&#35270;&#22270;&#20114;&#32852;&#32593;&#29031;&#29255;&#20013;&#23398;&#20064;&#29983;&#25104;&#19990;&#30028;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21487;&#27169;&#25311;&#22312;3D&#26223;&#35266;&#20013;&#38271;&#26102;&#38388;&#30340;&#39134;&#34892;&#65292;&#21516;&#26102;&#25903;&#25345;&#19968;&#20010;&#25345;&#32493;&#30340;&#12289;&#25668;&#20687;&#26426;&#29420;&#31435;&#30340;&#19990;&#30028;&#34920;&#31034;&#65292;&#20855;&#26377;&#39640;&#25928;&#30340;&#35745;&#31639;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#30340;3D&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#23613;&#31649;&#22270;&#20687;&#36136;&#37327;&#36234;&#26469;&#36234;&#36924;&#30495;&#65292;&#20294;&#20854;&#25805;&#20316;&#30340;3D&#20307;&#31215;&#36890;&#24120;&#26159;&#22266;&#23450;&#30340;&#65292;&#19988;&#25668;&#20687;&#26426;&#36816;&#21160;&#21463;&#38480;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#26080;&#26465;&#20214;&#21512;&#25104;&#26080;&#30028;&#33258;&#28982;&#22330;&#26223;&#30340;&#20219;&#21153;&#65292;&#20351;&#24471;&#22312;&#20445;&#25345;&#25345;&#20037;&#30340;3D&#19990;&#30028;&#27169;&#22411;&#30340;&#21516;&#26102;&#21487;&#20197;&#36827;&#34892;&#20219;&#24847;&#22823;&#30340;&#25668;&#20687;&#26426;&#36816;&#21160;&#12290;&#25105;&#20204;&#30340;&#22330;&#26223;&#34920;&#31034;&#21253;&#25324;&#21487;&#25193;&#23637;&#30340;&#24179;&#38754;&#22330;&#26223;&#24067;&#23616;&#32593;&#26684;&#65292;&#21487;&#20197;&#36890;&#36807;3D&#35299;&#30721;&#22120;&#21644;&#20307;&#31215;&#28210;&#26579;&#20174;&#20219;&#24847;&#25668;&#20687;&#26426;&#23039;&#24577;&#36827;&#34892;&#28210;&#26579;&#65292;&#20197;&#21450;&#20840;&#26223;&#22825;&#31354;&#31353;&#39030;&#12290;&#22522;&#20110;&#36825;&#31181;&#34920;&#31034;&#65292;&#25105;&#20204;&#20165;&#20174;&#21333;&#35270;&#22270;&#20114;&#32852;&#32593;&#29031;&#29255;&#20013;&#23398;&#20064;&#29983;&#25104;&#19990;&#30028;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#27169;&#25311;&#22312;3D&#26223;&#35266;&#20013;&#38271;&#26102;&#38388;&#30340;&#39134;&#34892;&#65292;&#21516;&#26102;&#20445;&#25345;&#20840;&#23616;&#22330;&#26223;&#30340;&#19968;&#33268;&#24615;&#8212;&#8212;&#20363;&#22914;&#65292;&#36820;&#22238;&#36215;&#28857;&#20250;&#20135;&#29983;&#30456;&#21516;&#30340;&#22330;&#26223;&#35270;&#22270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#24471;&#22330;&#26223;&#25512;&#26029;&#36229;&#36234;&#20102;&#24403;&#21069;3D&#29983;&#25104;&#27169;&#22411;&#30340;&#22266;&#23450;&#33539;&#22260;&#65292;&#21516;&#26102;&#25903;&#25345;&#19968;&#20010;&#25345;&#32493;&#30340;&#12289;&#25668;&#20687;&#26426;&#29420;&#31435;&#30340;&#19990;&#30028;&#34920;&#31034;&#65292;&#32780;&#36825;&#31181;&#34920;&#31034;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#26159;&#38750;&#24120;&#39640;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite increasingly realistic image quality, recent 3D image generative models often operate on 3D volumes of fixed extent with limited camera motions. We investigate the task of unconditionally synthesizing unbounded nature scenes, enabling arbitrarily large camera motion while maintaining a persistent 3D world model. Our scene representation consists of an extendable, planar scene layout grid, which can be rendered from arbitrary camera poses via a 3D decoder and volume rendering, and a panoramic skydome. Based on this representation, we learn a generative world model solely from single-view internet photos. Our method enables simulating long flights through 3D landscapes, while maintaining global scene consistency--for instance, returning to the starting point yields the same view of the scene. Our approach enables scene extrapolation beyond the fixed bounds of current 3D generative models, while also supporting a persistent, camera-independent world representation that stands in c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#39044;&#35774;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#30830;&#23450;&#24615;&#31070;&#32463;&#39068;&#33394;&#26144;&#23556;&#26041;&#27861;&#65288;DNCM&#65289;&#21644;&#20004;&#38454;&#27573;&#27969;&#27700;&#32447;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#39068;&#33394;&#39118;&#26684;&#36716;&#31227;&#65292;&#24182;&#19988;&#20855;&#26377;&#21508;&#31181;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2303.13511</link><description>&lt;p&gt;
&#31070;&#32463;&#39044;&#35774;&#65306;&#29992;&#20110;&#39068;&#33394;&#39118;&#26684;&#36716;&#31227;&#30340;&#26032;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Neural Preset for Color Style Transfer. (arXiv:2303.13511v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#39044;&#35774;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#30830;&#23450;&#24615;&#31070;&#32463;&#39068;&#33394;&#26144;&#23556;&#26041;&#27861;&#65288;DNCM&#65289;&#21644;&#20004;&#38454;&#27573;&#27969;&#27700;&#32447;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#39068;&#33394;&#39118;&#26684;&#36716;&#31227;&#65292;&#24182;&#19988;&#20855;&#26377;&#21508;&#31181;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#39044;&#35774;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#29616;&#26377;&#39068;&#33394;&#39118;&#26684;&#36716;&#31227;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#21253;&#25324;&#21487;&#35270;&#21270;&#20266;&#24433;&#12289;&#22823;&#37327;&#20869;&#23384;&#38656;&#27714;&#21644;&#32531;&#24930;&#30340;&#39118;&#26684;&#20999;&#25442;&#36895;&#24230;&#12290;&#26412;&#26041;&#27861;&#22522;&#20110;&#20004;&#20010;&#26680;&#24515;&#35774;&#35745;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30830;&#23450;&#24615;&#31070;&#32463;&#39068;&#33394;&#26144;&#23556;&#26041;&#27861;&#65288;DNCM&#65289;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#39068;&#33394;&#26144;&#23556;&#30697;&#38453;&#22312;&#27599;&#20010;&#20687;&#32032;&#19978;&#36827;&#34892;&#19968;&#33268;&#30340;&#25805;&#20316;&#65292;&#36991;&#20813;&#20102;&#20266;&#24433;&#65292;&#24182;&#25903;&#25345;&#20855;&#26377;&#23567;&#20869;&#23384;&#21344;&#29992;&#30340;&#39640;&#20998;&#36776;&#29575;&#36755;&#20837;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#20026;&#39068;&#33394;&#24402;&#19968;&#21270;&#21644;&#39118;&#26684;&#21270;&#20004;&#20010;&#38454;&#27573;&#26469;&#24320;&#21457;&#19968;&#20010;&#20004;&#38454;&#27573;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#39068;&#33394;&#39118;&#26684;&#20316;&#20026;&#39044;&#35774;&#25552;&#21462;&#65292;&#24182;&#22312;&#24402;&#19968;&#21270;&#30340;&#36755;&#20837;&#22270;&#20687;&#19978;&#37325;&#22797;&#20351;&#29992;&#23427;&#20204;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;&#39118;&#26684;&#20999;&#25442;&#12290;&#30001;&#20110;&#23384;&#22312;&#25104;&#23545;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#22914;&#20309;&#36890;&#36807;&#33258;&#30417;&#30563;&#31574;&#30053;&#35757;&#32451;&#31070;&#32463;&#39044;&#35774;&#27169;&#22411;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#35780;&#20272;&#23637;&#31034;&#20102;&#31070;&#32463;&#39044;&#35774;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#21508;&#31181;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#33258;&#28982;&#22320;&#25903;&#25345;&#22810;&#20010;&#39118;&#26684;&#65292;&#24182;&#22312;&#23450;&#37327;&#21644;&#23450;&#24615;&#24615;&#33021;&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a Neural Preset technique to address the limitations of existing color style transfer methods, including visual artifacts, vast memory requirement, and slow style switching speed. Our method is based on two core designs. First, we propose Deterministic Neural Color Mapping (DNCM) to consistently operate on each pixel via an image-adaptive color mapping matrix, avoiding artifacts and supporting high-resolution inputs with a small memory footprint. Second, we develop a two-stage pipeline by dividing the task into color normalization and stylization, which allows efficient style switching by extracting color styles as presets and reusing them on normalized input images. Due to the unavailability of pairwise datasets, we describe how to train Neural Preset via a self-supervised strategy. Various advantages of Neural Preset over existing methods are demonstrated through comprehensive evaluations. Besides, we show that our trained model can naturally support multipl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#32593;&#32476;&#32553;&#25918;&#30340;&#37327;&#21270;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21040;&#30340;&#21151;&#33021;&#20998;&#20026;&#37327;&#23376;&#24182;&#23545;&#37327;&#23376;&#36827;&#34892;&#36882;&#20943;&#20351;&#29992;&#39057;&#29575;&#30340;&#39034;&#24207;&#23398;&#20064;&#65292;&#21487;&#20197;&#35299;&#37322;&#25439;&#22833;&#32553;&#25918;&#23450;&#24459;&#65292;&#24182;&#33258;&#21160;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#26679;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.13506</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#32553;&#25918;&#30340;&#37327;&#21270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
The Quantization Model of Neural Scaling. (arXiv:2303.13506v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13506
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#32593;&#32476;&#32553;&#25918;&#30340;&#37327;&#21270;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21040;&#30340;&#21151;&#33021;&#20998;&#20026;&#37327;&#23376;&#24182;&#23545;&#37327;&#23376;&#36827;&#34892;&#36882;&#20943;&#20351;&#29992;&#39057;&#29575;&#30340;&#39034;&#24207;&#23398;&#20064;&#65292;&#21487;&#20197;&#35299;&#37322;&#25439;&#22833;&#32553;&#25918;&#23450;&#24459;&#65292;&#24182;&#33258;&#21160;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#26679;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#31070;&#32463;&#32593;&#32476;&#32553;&#25918;&#23450;&#24459;&#30340;&#37327;&#21270;&#27169;&#22411;&#65292;&#35299;&#37322;&#20102;&#35266;&#23519;&#21040;&#30340;&#25439;&#22833;&#20989;&#25968;&#38543;&#30528;&#27169;&#22411;&#21644;&#25968;&#25454;&#35268;&#27169;&#30340;&#24130;&#24459;&#19979;&#38477;&#20197;&#21450;&#38543;&#30528;&#35268;&#27169;&#30340;&#22686;&#21152;&#20986;&#29616;&#26032;&#33021;&#21147;&#30340;&#31361;&#28982;&#31361;&#30772;&#12290;&#25105;&#20204;&#20174;&#25152;&#35859;&#30340;&#8220;&#37327;&#21270;&#20551;&#35774;&#8221;&#20013;&#25512;&#23548;&#20986;&#36825;&#20010;&#27169;&#22411;&#65292;&#20854;&#20013;&#23398;&#20064;&#21040;&#30340;&#31070;&#32463;&#32593;&#32476;&#21151;&#33021;&#34987;&#37327;&#21270;&#20026;&#31163;&#25955;&#22359;&#65288;&#8220;&#37327;&#23376;&#8221;&#65289;&#12290;&#25105;&#20204;&#22312;&#38477;&#24207;&#23398;&#20064;&#39057;&#29575;&#20013;&#23398;&#20064;&#37327;&#23376;&#65292;&#24182;&#34920;&#26126;&#24403;&#37327;&#23376;&#34987;&#20197;&#36882;&#20943;&#20351;&#29992;&#39057;&#29575;&#30340;&#39034;&#24207;&#23398;&#20064;&#26102;&#65292;&#22312;&#20351;&#29992;&#39057;&#29575;&#20013;&#20351;&#29992;&#24130;&#24459;&#21487;&#20197;&#35299;&#37322;&#35266;&#23519;&#21040;&#30340;&#25439;&#22833;&#32553;&#25918;&#23450;&#24459;&#12290;&#25105;&#20204;&#22312;&#29609;&#20855;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#36825;&#20010;&#39044;&#27979;&#65292;&#28982;&#21518;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32553;&#25918;&#26354;&#32447;&#22914;&#20309;&#20998;&#35299;&#12290;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#65292;&#25105;&#20204;&#33258;&#21160;&#21457;&#29616;&#22810;&#26679;&#30340;&#27169;&#22411;&#33021;&#21147;&#65288;&#37327;&#23376;&#65289;&#65292;&#24182;&#21457;&#29616;&#23545;&#24212;&#23376;&#38382;&#39064;&#30340;&#20998;&#24067;&#19982;&#25105;&#20204;&#29702;&#35770;&#39044;&#27979;&#30340;&#31070;&#32463;&#32553;&#25918;&#25351;&#25968;&#20135;&#29983;&#20102;&#20860;&#23481;&#24615;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the $\textit{Quantization Model}$ of neural scaling laws, explaining both the observed power law dropoff of loss with model and data size, and also the sudden emergence of new capabilities with scale. We derive this model from what we call the $\textit{Quantization Hypothesis}$, where learned network capabilities are quantized into discrete chunks ($\textit{quanta}$). We show that when quanta are learned in order of decreasing use frequency, then a power law in use frequencies explains observed power law scaling of loss. We validate this prediction on toy datasets, then study how scaling curves decompose for large language models. Using language model internals, we auto-discover diverse model capabilities (quanta) and find tentative evidence that the distribution over corresponding subproblems in the prediction of natural text is compatible with the power law predicted from the neural scaling exponent as predicted from our theory.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#25910;&#25947;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#26071;&#22411;&#27969;&#24418;&#19978;&#35745;&#31639;&#19968;&#32452;&#28857;&#30340;&#26071;&#24418;&#22343;&#20540;&#21644;&#26071;&#24418;&#20013;&#20540;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#20013;&#38750;&#24120;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.13501</link><description>&lt;p&gt;
&#26071;&#22411;&#27969;&#24418;&#19978;&#30340;&#24358;&#22343;&#20540;&#21450;&#20854;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Chordal Averaging on Flag Manifolds and Its Applications. (arXiv:2303.13501v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#25910;&#25947;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#26071;&#22411;&#27969;&#24418;&#19978;&#35745;&#31639;&#19968;&#32452;&#28857;&#30340;&#26071;&#24418;&#22343;&#20540;&#21644;&#26071;&#24418;&#20013;&#20540;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#20013;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#21487;&#35777;&#26126;&#25910;&#25947;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#24358;&#24230;&#37327;&#19979;&#35745;&#31639;&#26071;&#22411;&#27969;&#24418;&#19978;&#19968;&#32452;&#28857;&#30340;&#26071;&#24418;&#22343;&#20540;&#21644;&#26071;&#24418;&#20013;&#20540;&#12290;&#26071;&#22411;&#27969;&#24418;&#26159;&#19968;&#31181;&#25968;&#23398;&#31354;&#38388;&#65292;&#30001;&#23884;&#22871;&#30340;&#21521;&#37327;&#31354;&#38388;&#23376;&#31354;&#38388;&#24207;&#21015;&#32452;&#25104;&#65292;&#24182;&#19988;&#22312;&#32500;&#24230;&#19978;&#36880;&#28176;&#22686;&#21152;&#12290;&#26071;&#22411;&#27969;&#24418;&#26159;&#24050;&#30693;&#30340;&#35768;&#22810;&#30697;&#38453;&#32676;&#30340;&#36229;&#38598;&#65292;&#21253;&#25324;Stiefel&#21644;Grassmanians&#65292;&#20351;&#20854;&#25104;&#20026;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#20013;&#38750;&#24120;&#26377;&#29992;&#30340;&#36890;&#29992;&#23545;&#35937;&#12290;&#20026;&#20102;&#35299;&#20915;&#35745;&#31639;&#19968;&#38454;&#26071;&#24092;&#32479;&#35745;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#28041;&#21450;&#36741;&#21161;&#21464;&#37327;&#21463;Stiefel&#27969;&#24418;&#32422;&#26463;&#30340;&#38382;&#39064;&#12290;Stiefel&#27969;&#24418;&#26159;&#19968;&#32452;&#27491;&#20132;&#26694;&#26550;&#30340;&#31354;&#38388;&#65292;&#21033;&#29992;Stiefel&#27969;&#24418;&#20248;&#21270;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#21644;&#25928;&#29575;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35745;&#31639;&#26071;&#24418;&#22343;&#20540;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Grassmann&#21644;&#26059;&#36716;&#22343;&#20540;&#20197;&#21450;&#20027;&#25104;&#20998;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new, provably-convergent algorithm for computing the flag-mean and flag-median of a set of points on a flag manifold under the chordal metric. The flag manifold is a mathematical space consisting of flags, which are sequences of nested subspaces of a vector space that increase in dimension. The flag manifold is a superset of a wide range of known matrix groups, including Stiefel and Grassmanians, making it a general object that is useful in a wide variety computer vision problems.  To tackle the challenge of computing first order flag statistics, we first transform the problem into one that involves auxiliary variables constrained to the Stiefel manifold. The Stiefel manifold is a space of orthogonal frames, and leveraging the numerical stability and efficiency of Stiefel-manifold optimization enables us to compute the flag-mean effectively. Through a series of experiments, we show the competence of our method in Grassmann and rotation averaging, as well as princi
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36807;&#24230;&#25311;&#21512;&#25968;&#25454;&#20013;&#31616;&#21333;&#27169;&#24335;&#30340;&#20542;&#21521;&#30340;&#26131;&#24863;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#25511;&#31616;&#21333;&#24615;&#27491;&#21017;&#21270;&#65288;CSR&#65289;&#26041;&#27861;&#26469;&#38480;&#21046;&#36807;&#24230;&#25311;&#21512;&#65292;&#23436;&#25104;&#23545;&#27169;&#22411;&#30340;&#26356;&#28145;&#20837;&#30740;&#31350;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.13500</link><description>&lt;p&gt;
&#36890;&#36807;&#29305;&#24449;&#25197;&#26354;&#21644;&#31616;&#21333;&#24615;&#20559;&#24046;&#26469;&#36866;&#24212;&#27169;&#22411;&#30340;&#26356;&#28145;&#20837;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Closer Look at Model Adaptation using Feature Distortion and Simplicity Bias. (arXiv:2303.13500v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13500
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36807;&#24230;&#25311;&#21512;&#25968;&#25454;&#20013;&#31616;&#21333;&#27169;&#24335;&#30340;&#20542;&#21521;&#30340;&#26131;&#24863;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#25511;&#31616;&#21333;&#24615;&#27491;&#21017;&#21270;&#65288;CSR&#65289;&#26041;&#27861;&#26469;&#38480;&#21046;&#36807;&#24230;&#25311;&#21512;&#65292;&#23436;&#25104;&#23545;&#27169;&#22411;&#30340;&#26356;&#28145;&#20837;&#30740;&#31350;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#27169;&#22411;&#34920;&#36798;&#33021;&#21147;&#30340;&#25552;&#39640;&#22686;&#21152;&#20102;&#23545;&#21551;&#29992;&#23433;&#20840;&#26377;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#30340;&#36866;&#24212;&#21327;&#35758;&#35774;&#35745;&#30340;&#20852;&#36259;&#12290;&#22312;&#20256;&#32479;&#30340;&#32447;&#24615;&#25506;&#27979;&#65288;LP&#65289;&#21644;&#24494;&#35843;&#65288;FT&#65289;&#31574;&#30053;&#20043;&#22806;&#65292;&#21457;&#29616;&#21487;&#20197;&#26377;&#25928;&#25511;&#21046;&#29305;&#24449;&#25197;&#26354;&#65288;&#21363;&#26080;&#27861;&#26356;&#26032;&#27491;&#20132;&#20110;&#20998;&#24067;&#20869;&#37096;&#30340;&#29305;&#24449;&#65289;&#30340;&#21327;&#35758;&#21487;&#20197;&#23454;&#29616;&#25913;&#36827;&#30340;&#36234;&#30028;&#27867;&#21270;&#65288;OOD&#65289;&#12290;&#20026;&#20102;&#38480;&#21046;&#36825;&#31181;&#25197;&#26354;&#65292;&#25552;&#20986;&#20102;LP+FT&#21327;&#35758;&#65292;&#35813;&#21327;&#35758;&#39318;&#20808;&#23398;&#20064;&#32447;&#24615;&#25506;&#27979;&#65292;&#28982;&#21518;&#20351;&#29992;&#27492;&#21021;&#22987;&#21270;&#36827;&#34892;&#21518;&#32493;FT&#12290;&#20294;&#26159;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#36866;&#24212;&#21327;&#35758;&#65288;LP&#12289;FT&#12289;LP+FT&#65289;&#20063;&#22312;&#22810;&#31181;&#23433;&#20840;&#30446;&#26631;&#65288;&#20363;&#22914;&#26657;&#20934;&#12289;&#40065;&#26834;&#24615;&#31561;&#65289;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#23545;&#29305;&#24449;&#25197;&#26354;&#30340;&#20114;&#34917;&#35270;&#35282;&#26377;&#21161;&#20110;&#35299;&#37322;&#21327;&#35758;&#34892;&#20026;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21327;&#35758;&#23545;&#31616;&#21333;&#24615;&#20559;&#35265;&#65288;SB&#65289;&#30340;&#26131;&#24863;&#24615;&#65292;&#21363;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36807;&#24230;&#25311;&#21512;&#25968;&#25454;&#20013;&#31616;&#21333;&#27169;&#24335;&#30340;&#20542;&#21521;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21463;&#25511;&#31616;&#21333;&#24615;&#27491;&#21017;&#21270;&#65288;CSR&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#40723;&#21169;&#23545;&#26356;&#22797;&#26434;&#30340;&#29305;&#24449;&#36827;&#34892;&#27867;&#21270;&#24182;&#38480;&#21046;&#36807;&#24230;&#25311;&#21512;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;CSR&#21487;&#20197;&#26377;&#25928;&#22320;&#34917;&#20805;&#26088;&#22312;&#38480;&#21046;&#29305;&#24449;&#25197;&#26354;&#30340;&#21327;&#35758;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24378;&#30340;OOD&#27867;&#21270;&#21644;&#25913;&#36827;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in the expressivity of pretrained models have increased interest in the design of adaptation protocols which enable safe and effective transfer learning. Going beyond conventional linear probing (LP) and fine tuning (FT) strategies, protocols that can effectively control feature distortion, i.e., the failure to update features orthogonal to the in-distribution, have been found to achieve improved out-of-distribution generalization (OOD). In order to limit this distortion, the LP+FT protocol, which first learns a linear probe and then uses this initialization for subsequent FT, was proposed. However, in this paper, we find when adaptation protocols (LP, FT, LP+FT) are also evaluated on a variety of safety objectives (e.g., calibration, robustness, etc.), a complementary perspective to feature distortion is helpful to explain protocol behavior. To this end, we study the susceptibility of protocols to simplicity bias (SB), i.e. the well-known propensity of deep neural networks to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;MAE&#25216;&#26415;&#39044;&#21069;&#32622;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20159;&#32423;&#39044;&#35757;&#32451;&#35268;&#27169;&#65292;&#24182;&#21487;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#25910;&#25947;&#24615;&#21644;&#19979;&#28216;&#36716;&#31227;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.13496</link><description>&lt;p&gt;
MAE&#39044;&#21069;&#32622;&#35757;&#32451;&#23545;&#20110;&#20159;&#32423;&#39044;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
The effectiveness of MAE pre-pretraining for billion-scale pretraining. (arXiv:2303.13496v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;MAE&#25216;&#26415;&#39044;&#21069;&#32622;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20159;&#32423;&#39044;&#35757;&#32451;&#35268;&#27169;&#65292;&#24182;&#21487;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#25910;&#25947;&#24615;&#21644;&#19979;&#28216;&#36716;&#31227;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#29992;&#20110;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#30340;&#26631;&#20934;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;&#27169;&#22411;&#20351;&#29992;&#25968;&#21313;&#20159;&#24352;&#22270;&#20687;&#30340;&#22823;&#35268;&#27169;&#65288;&#24369;&#65289;&#30417;&#30563;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#39044;&#21069;&#32622;&#35757;&#32451;&#38454;&#27573;&#65292;&#23427;&#20351;&#29992;&#20102;&#33258;&#25105;&#30417;&#30563;&#30340;MAE&#25216;&#26415;&#26469;&#21021;&#22987;&#21270;&#27169;&#22411;&#12290;&#34429;&#28982;MAE&#25216;&#26415;&#20165;&#34987;&#35777;&#26126;&#33021;&#22815;&#19982;&#27169;&#22411;&#22823;&#23567;&#30456;&#32553;&#25918;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23427;&#20063;&#21487;&#20197;&#38543;&#25968;&#25454;&#38598;&#22823;&#23567;&#32553;&#25918;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22522;&#20110;MAE&#30340;&#39044;&#21069;&#32622;&#35757;&#32451;&#21487;&#21516;&#26102;&#36866;&#29992;&#20110;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#35268;&#27169;&#12290;&#39044;&#21069;&#32622;&#35757;&#32451;&#22312;&#19968;&#31995;&#21015;&#27169;&#22411;&#35268;&#27169;&#65288;&#21442;&#25968;&#25968;&#30334;&#19975;&#21040;&#25968;&#21313;&#20159;&#65289;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#65288;&#22270;&#20687;&#25968;&#30334;&#19975;&#21040;&#25968;&#21313;&#20159;&#65289;&#19978;&#19968;&#33268;&#25552;&#39640;&#20102;&#27169;&#22411;&#25910;&#25947;&#24615;&#21644;&#19979;&#28216;&#36716;&#31227;&#24615;&#33021;&#65292;&#19988;&#25105;&#20204;&#36824;&#27979;&#37327;&#20102;&#20854;&#22312;10&#20010;&#19981;&#21516;&#30340;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#12289;&#35270;&#39057;&#35782;&#21035;&#21644;&#30446;&#26631;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper revisits the standard pretrain-then-finetune paradigm used in computer vision for visual recognition tasks. Typically, state-of-the-art foundation models are pretrained using large scale (weakly) supervised datasets with billions of images. We introduce an additional pre-pretraining stage that is simple and uses the self-supervised MAE technique to initialize the model. While MAE has only been shown to scale with the size of models, we find that it scales with the size of the training dataset as well. Thus, our MAE-based pre-pretraining scales with both model and data size making it applicable for training foundation models. Pre-pretraining consistently improves both the model convergence and the downstream transfer performance across a range of model scales (millions to billions of parameters), and dataset sizes (millions to billions of images). We measure the effectiveness of pre-pretraining on 10 different visual recognition tasks spanning image classification, video reco
&lt;/p&gt;</description></item><item><title>&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#31181;&#20943;&#23569;&#35797;&#38169;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#31034;&#33539;&#65292;&#26412;&#25991;&#32508;&#36848;&#20102;&#22914;&#20309;&#20351;&#29992;&#31034;&#33539;&#26469;&#20419;&#36827;&#23398;&#20064;&#20915;&#31574;&#27169;&#22411;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;ManiSkill&#26426;&#22120;&#20154;&#23398;&#20064;&#22522;&#20934;&#30340;&#31034;&#33539;&#29983;&#25104;&#21644;&#21033;&#29992;&#31649;&#36947;&#30340;&#23454;&#20363;&#12290;</title><link>http://arxiv.org/abs/2303.13489</link><description>&lt;p&gt;
&#20351;&#29992;&#31034;&#33539;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#19982;&#35268;&#21010;&#65306;&#19968;&#20221;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Boosting Reinforcement Learning and Planning with Demonstrations: A Survey. (arXiv:2303.13489v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13489
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#31181;&#20943;&#23569;&#35797;&#38169;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#31034;&#33539;&#65292;&#26412;&#25991;&#32508;&#36848;&#20102;&#22914;&#20309;&#20351;&#29992;&#31034;&#33539;&#26469;&#20419;&#36827;&#23398;&#20064;&#20915;&#31574;&#27169;&#22411;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;ManiSkill&#26426;&#22120;&#20154;&#23398;&#20064;&#22522;&#20934;&#30340;&#31034;&#33539;&#29983;&#25104;&#21644;&#21033;&#29992;&#31649;&#36947;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#26368;&#36817;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#26159;&#36825;&#31181;&#35797;&#38169;&#24335;&#30340;&#23398;&#20064;&#26041;&#27861;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#21487;&#33021;&#25928;&#29575;&#20302;&#19979;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#20351;&#29992;&#31034;&#33539;&#21487;&#20197;&#35753;&#26234;&#33021;&#20307;&#21463;&#30410;&#20110;&#19987;&#23478;&#30340;&#30693;&#35782;&#65292;&#32780;&#26080;&#38656;&#25506;&#32034;&#26368;&#20339;&#34892;&#21160;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#39034;&#24207;&#20915;&#31574;&#20013;&#20351;&#29992;&#31034;&#33539;&#30340;&#20248;&#28857;&#65292;&#20197;&#21450;&#23398;&#20064;&#20026;&#22522;&#30784;&#30340;&#20915;&#31574;&#21046;&#23450;&#33539;&#24335;&#65288;&#20363;&#22914;&#65292;&#24378;&#21270;&#23398;&#20064;&#21644;&#35268;&#21010;&#22312;&#23398;&#20064;&#30340;&#27169;&#22411;&#20013;&#22914;&#20309;&#24212;&#29992;&#31034;&#33539;&#65289;&#65292;&#20197;&#21450;&#22914;&#20309;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#25910;&#38598;&#31034;&#33539;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20030;&#20102;&#19968;&#20010;&#23454;&#38469;&#30340;&#31034;&#33539;&#29983;&#25104;&#21644;&#21033;&#29992;&#31649;&#36947;&#30340;&#20363;&#23376;&#65292;&#24182;&#22312;&#26368;&#36817;&#25552;&#20986;&#30340;ManiSkill&#26426;&#22120;&#20154;&#23398;&#20064;&#22522;&#20934;&#20013;&#36827;&#34892;&#20102;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although reinforcement learning has seen tremendous success recently, this kind of trial-and-error learning can be impractical or inefficient in complex environments. The use of demonstrations, on the other hand, enables agents to benefit from expert knowledge rather than having to discover the best action to take through exploration. In this survey, we discuss the advantages of using demonstrations in sequential decision making, various ways to apply demonstrations in learning-based decision making paradigms (for example, reinforcement learning and planning in the learned models), and how to collect the demonstrations in various scenarios. Additionally, we exemplify a practical pipeline for generating and utilizing demonstrations in the recently proposed ManiSkill robot learning benchmark.
&lt;/p&gt;</description></item><item><title>TactoFind&#26159;&#19968;&#20010;&#32431;&#35302;&#35273;&#29289;&#21697;&#33719;&#21462;&#31995;&#32479;&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#25163;&#25351;&#19978;&#30340;&#35302;&#25511;&#20256;&#24863;&#22120;&#65292;&#22312;&#27809;&#26377;&#20219;&#20309;&#35270;&#35273;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#23450;&#20301;&#12289;&#35782;&#21035;&#21644;&#25235;&#21462;&#26032;&#30340;&#29289;&#20307;&#12290;</title><link>http://arxiv.org/abs/2303.13482</link><description>&lt;p&gt;
TactoFind&#65306;&#19968;&#31181;&#32431;&#35302;&#35273;&#29289;&#21697;&#33719;&#21462;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
TactoFind: A Tactile Only System for Object Retrieval. (arXiv:2303.13482v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13482
&lt;/p&gt;
&lt;p&gt;
TactoFind&#26159;&#19968;&#20010;&#32431;&#35302;&#35273;&#29289;&#21697;&#33719;&#21462;&#31995;&#32479;&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#25163;&#25351;&#19978;&#30340;&#35302;&#25511;&#20256;&#24863;&#22120;&#65292;&#22312;&#27809;&#26377;&#20219;&#20309;&#35270;&#35273;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#23450;&#20301;&#12289;&#35782;&#21035;&#21644;&#25235;&#21462;&#26032;&#30340;&#29289;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32570;&#20047;&#35270;&#35273;&#24863;&#30693;&#12289;&#26410;&#30693;&#29289;&#20307;&#24418;&#29366;&#20197;&#21450;&#29289;&#20307;&#21487;&#20197;&#33258;&#30001;&#31227;&#21160;&#30340;&#22330;&#26223;&#19979;&#36827;&#34892;&#29289;&#21697;&#26816;&#32034;&#30340;&#38382;&#39064;&#12290;&#25104;&#21151;&#30340;&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#23450;&#20301;&#33258;&#30001;&#29289;&#20307;&#12289;&#35782;&#21035;&#29305;&#23450;&#30340;&#29289;&#20307;&#23454;&#20363;&#24182;&#20351;&#29992;&#35302;&#35273;&#21453;&#39304;&#26469;&#25235;&#21462;&#24050;&#35782;&#21035;&#30340;&#29289;&#20307;&#12290;&#19982;&#25668;&#24433;&#26426;&#21487;&#35266;&#23519;&#25972;&#20010;&#22330;&#26223;&#30340;&#35270;&#35273;&#19981;&#21516;&#65292;&#35302;&#35273;&#20256;&#24863;&#22120;&#26159;&#23616;&#37096;&#30340;&#65292;&#24182;&#19988;&#20165;&#35266;&#23519;&#19982;&#25805;&#32437;&#22120;&#25509;&#35302;&#30340;&#22330;&#26223;&#37096;&#20998;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#35302;&#35273;&#20256;&#24863;&#22120;&#25910;&#38598;&#20449;&#24687;&#38656;&#35201;&#22312;&#35302;&#25720;&#34920;&#38754;&#26045;&#21152;&#21147;&#65292;&#36825;&#21487;&#33021;&#20250;&#25200;&#20081;&#22330;&#26223;&#26412;&#36523;&#12290;&#22240;&#27492;&#65292;&#35302;&#25720;&#24863;&#30693;&#38656;&#35201;&#36890;&#36807;&#26102;&#38388;&#19978;&#30340;&#31934;&#32454;&#25506;&#32034;&#21644;&#20449;&#24687;&#38598;&#25104;&#26469;&#36827;&#34892;&#25512;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#21487;&#20197;&#21033;&#29992;&#25163;&#25351;&#35302;&#25720;&#20256;&#24863;&#22120;&#19978;&#30340;&#31232;&#30095;&#35302;&#35273;&#21453;&#39304;&#65292;&#22312;&#27809;&#26377;&#20219;&#20309;&#35270;&#35273;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#23450;&#20301;&#12289;&#35782;&#21035;&#21644;&#25235;&#21462;&#26032;&#30340;&#29289;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of object retrieval in scenarios where visual sensing is absent, object shapes are unknown beforehand and objects can move freely, like grabbing objects out of a drawer. Successful solutions require localizing free objects, identifying specific object instances, and then grasping the identified objects, only using touch feedback. Unlike vision, where cameras can observe the entire scene, touch sensors are local and only observe parts of the scene that are in contact with the manipulator. Moreover, information gathering via touch sensors necessitates applying forces on the touched surface which may disturb the scene itself. Reasoning with touch, therefore, requires careful exploration and integration of information over time -- a challenge we tackle. We present a system capable of using sparse tactile feedback from fingertip touch sensors on a dexterous hand to localize, identify and grasp novel objects without any visual feedback. Videos are available at https://ta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20351;&#29992;&#25968;&#25454;&#30340;&#37327;&#23376;&#36153;&#33293;&#23572;&#20449;&#24687;&#24230;&#37327;&#26469;&#35780;&#20272;&#25104;&#21151;&#35757;&#32451;&#21644;&#27867;&#21270;&#25152;&#38656;&#30340;&#30005;&#36335;&#21442;&#25968;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#65292;&#24182;&#23637;&#31034;&#36890;&#36807;&#21435;&#38500;&#23545;&#31216;&#24615;&#26469;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#21457;&#29616;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#33021;&#21147;&#21487;&#20197;&#27604;&#20351;&#29992;&#30456;&#21516;&#20998;&#24067;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2303.13462</link><description>&lt;p&gt;
&#21033;&#29992;&#37327;&#23376;&#20960;&#20309;&#36827;&#34892;&#23398;&#20064;&#24186;&#27491;&#21464;&#25442;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Generalization with quantum geometry for learning unitaries. (arXiv:2303.13462v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20351;&#29992;&#25968;&#25454;&#30340;&#37327;&#23376;&#36153;&#33293;&#23572;&#20449;&#24687;&#24230;&#37327;&#26469;&#35780;&#20272;&#25104;&#21151;&#35757;&#32451;&#21644;&#27867;&#21270;&#25152;&#38656;&#30340;&#30005;&#36335;&#21442;&#25968;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#65292;&#24182;&#23637;&#31034;&#36890;&#36807;&#21435;&#38500;&#23545;&#31216;&#24615;&#26469;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#21457;&#29616;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#33021;&#21147;&#21487;&#20197;&#27604;&#20351;&#29992;&#30456;&#21516;&#20998;&#24067;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27867;&#21270;&#26159;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20174;&#35757;&#32451;&#25968;&#25454;&#23398;&#20064;&#20934;&#30830;&#39044;&#27979;&#26032;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#25968;&#25454;&#30340;&#37327;&#23376;&#36153;&#33293;&#23572;&#20449;&#24687;&#24230;&#37327;(DQFIM)&#26469;&#30830;&#23450;&#27169;&#22411;&#20309;&#26102;&#33021;&#22815;&#27867;&#21270;&#12290;&#23545;&#20110;&#24186;&#27491;&#21464;&#25442;&#30340;&#21487;&#21464;&#23398;&#20064;&#65292;DQFIM&#37327;&#21270;&#20102;&#25104;&#21151;&#35757;&#32451;&#21644;&#27867;&#21270;&#25152;&#38656;&#30340;&#30005;&#36335;&#21442;&#25968;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#24212;&#29992;DQFIM&#26469;&#35299;&#37322;&#20309;&#26102;&#24658;&#23450;&#25968;&#37327;&#30340;&#35757;&#32451;&#29366;&#24577;&#21644;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#21442;&#25968;&#36275;&#20197;&#23454;&#29616;&#27867;&#21270;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#21024;&#38500;&#23545;&#31216;&#24615;&#65292;&#21487;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#26174;&#31034;&#65292;&#20351;&#29992;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#30340;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#33021;&#21147;&#21487;&#20197;&#27604;&#20351;&#29992;&#30456;&#21516;&#20998;&#24067;&#30340;&#33021;&#21147;&#26356;&#20248;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#25552;&#39640;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#24320;&#36767;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalization is the ability of quantum machine learning models to make accurate predictions on new data by learning from training data. Here, we introduce the data quantum Fisher information metric (DQFIM) to determine when a model can generalize. For variational learning of unitaries, the DQFIM quantifies the amount of circuit parameters and training data needed to successfully train and generalize. We apply the DQFIM to explain when a constant number of training states and polynomial number of parameters are sufficient for generalization. Further, we can improve generalization by removing symmetries from training data. Finally, we show that out-of-distribution generalization, where training and testing data are drawn from different data distributions, can be better than using the same distribution. Our work opens up new approaches to improve generalization in quantum machine learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23545;&#31216;&#25968;&#25454;&#19978;&#20248;&#21270;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#31561;&#21464;&#21644;&#22686;&#24378;&#20004;&#31181;&#31574;&#30053;&#30340;&#20248;&#32570;&#28857;&#65292;&#35777;&#26126;&#20102;&#22312;&#33258;&#28982;&#20551;&#35774;&#19979;&#31561;&#21464;&#31283;&#23450;&#28857;&#30340;&#38598;&#21512;&#21644;&#31561;&#21464;&#23618;&#30340;&#38598;&#21512;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#20294;&#22686;&#24378;&#27169;&#22411;&#30340;&#31283;&#23450;&#28857;&#21487;&#33021;&#26159;&#19981;&#31283;&#23450;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.13458</link><description>&lt;p&gt;
&#31561;&#21464;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Optimization Dynamics of Equivariant and Augmented Neural Networks. (arXiv:2303.13458v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23545;&#31216;&#25968;&#25454;&#19978;&#20248;&#21270;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#31561;&#21464;&#21644;&#22686;&#24378;&#20004;&#31181;&#31574;&#30053;&#30340;&#20248;&#32570;&#28857;&#65292;&#35777;&#26126;&#20102;&#22312;&#33258;&#28982;&#20551;&#35774;&#19979;&#31561;&#21464;&#31283;&#23450;&#28857;&#30340;&#38598;&#21512;&#21644;&#31561;&#21464;&#23618;&#30340;&#38598;&#21512;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#20294;&#22686;&#24378;&#27169;&#22411;&#30340;&#31283;&#23450;&#28857;&#21487;&#33021;&#26159;&#19981;&#31283;&#23450;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#23545;&#31216;&#25968;&#25454;&#19978;&#20248;&#21270;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#38480;&#21046;&#26550;&#26500;&#31561;&#21464;&#21644;&#20351;&#29992;&#22686;&#24378;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#23545;&#25439;&#22833;&#21644;&#38750;&#32447;&#24615;&#24615;&#36827;&#34892;&#33258;&#28982;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#31561;&#21464;&#31283;&#23450;&#28857;&#30340;&#38598;&#21512;&#23545;&#20110;&#36825;&#20004;&#31181;&#31574;&#30053;&#26159;&#30456;&#21516;&#30340;&#65292;&#24182;&#19988;&#31561;&#21464;&#23618;&#30340;&#38598;&#21512;&#22312;&#22686;&#24378;&#27169;&#22411;&#30340;&#26799;&#24230;&#27969;&#19979;&#26159;&#19981;&#21464;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#23613;&#31649;&#31561;&#21464;&#27169;&#22411;&#30340;&#31283;&#23450;&#28857;&#26159;&#31283;&#23450;&#30340;&#65292;&#22686;&#24378;&#35757;&#32451;&#30340;&#31283;&#23450;&#28857;&#21487;&#33021;&#26159;&#19981;&#31283;&#23450;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the optimization of multilayer perceptrons on symmetric data. We compare the strategy of constraining the architecture to be equivariant to that of using augmentation. We show that, under natural assumptions on the loss and non-linearities, the sets of equivariant stationary points are identical for the two strategies, and that the set of equivariant layers is invariant under the gradient flow for augmented models. Finally, we show that stationary points may be unstable for augmented training although they are stable for the equivariant models
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32508;&#36848;&#20102;&#20351;&#29992;&#22823;&#25968;&#25454;&#25216;&#26415;&#30740;&#31350;COVID-19&#32972;&#26223;&#19979;&#20154;&#31867;&#34892;&#20026;&#30340;&#29616;&#26377;&#30740;&#31350;&#65292;&#20998;&#20026;&#20351;&#29992;&#22823;&#25968;&#25454;&#27979;&#37327;&#12289;&#27169;&#25311;&#21644;&#24178;&#39044;&#20154;&#31867;&#34892;&#20026;&#19977;&#31867;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2303.13452</link><description>&lt;p&gt;
COVID-19&#32972;&#26223;&#19979;&#30340;&#20154;&#31867;&#34892;&#20026;&#65306;&#20174;&#22823;&#25968;&#25454;&#20013;&#23398;&#20064; (arXiv:2303.13452v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
Human Behavior in the Time of COVID-19: Learning from Big Data. (arXiv:2303.13452v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32508;&#36848;&#20102;&#20351;&#29992;&#22823;&#25968;&#25454;&#25216;&#26415;&#30740;&#31350;COVID-19&#32972;&#26223;&#19979;&#20154;&#31867;&#34892;&#20026;&#30340;&#29616;&#26377;&#30740;&#31350;&#65292;&#20998;&#20026;&#20351;&#29992;&#22823;&#25968;&#25454;&#27979;&#37327;&#12289;&#27169;&#25311;&#21644;&#24178;&#39044;&#20154;&#31867;&#34892;&#20026;&#19977;&#31867;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;2020&#24180;3&#26376;&#19990;&#30028;&#21355;&#29983;&#32452;&#32455;(WHO)&#23558;COVID-19&#23450;&#24615;&#20026;&#22823;&#27969;&#34892;&#30149;&#20197;&#26469;&#65292;&#25130;&#33267;2022&#24180;10&#26376;&#24050;&#26377;&#36229;&#36807;6&#20159;&#20363;COVID-19&#30830;&#35786;&#30149;&#20363;&#21644;&#36229;&#36807;600&#19975;&#20363;&#27515;&#20129;&#12290;COVID-19&#22823;&#27969;&#34892;&#19982;&#20154;&#31867;&#34892;&#20026;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#22797;&#26434;&#30340;&#12290;&#19968;&#26041;&#38754;&#65292;&#20154;&#31867;&#34892;&#20026;&#34987;&#21457;&#29616;&#21487;&#20197;&#22609;&#36896;&#30142;&#30149;&#30340;&#20256;&#25773;&#65292;&#21478;&#19968;&#26041;&#38754;&#65292;&#30123;&#24773;&#24433;&#21709;&#24182;&#29978;&#33267;&#25913;&#21464;&#20102;&#20960;&#20046;&#27599;&#20010;&#26041;&#38754;&#30340;&#20154;&#31867;&#34892;&#20026;&#12290;&#20026;&#20102;&#25552;&#20379;&#23545;&#20154;&#31867;&#34892;&#20026;&#19982;COVID-19&#22823;&#27969;&#34892;&#20043;&#38388;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#30340;&#20840;&#38754;&#29702;&#35299;&#65292;&#30740;&#31350;&#20154;&#21592;&#19968;&#30452;&#22312;&#37319;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#38899;&#39057;&#20449;&#21495;&#22788;&#29702;&#12289;&#39057;&#32321;&#27169;&#24335;&#25366;&#25496;&#21644;&#26426;&#22120;&#23398;&#20064;&#31561;&#22823;&#25968;&#25454;&#25216;&#26415;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#20351;&#29992;&#22823;&#25968;&#25454;&#25216;&#26415;&#30740;&#31350;COVID-19&#32972;&#26223;&#19979;&#20154;&#31867;&#34892;&#20026;&#30340;&#29616;&#26377;&#30740;&#31350;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#30740;&#31350;&#20998;&#20026;&#19977;&#31867;&#8212;&#8212;&#20351;&#29992;&#22823;&#25968;&#25454;&#27979;&#37327;&#12289;&#27169;&#25311;&#21644;&#24178;&#39044;&#20154;&#31867;&#34892;&#20026;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the World Health Organization (WHO) characterized COVID-19 as a pandemic in March 2020, there have been over 600 million confirmed cases of COVID-19 and more than six million deaths as of October 2022. The relationship between the COVID-19 pandemic and human behavior is complicated. On one hand, human behavior is found to shape the spread of the disease. On the other hand, the pandemic has impacted and even changed human behavior in almost every aspect. To provide a holistic understanding of the complex interplay between human behavior and the COVID-19 pandemic, researchers have been employing big data techniques such as natural language processing, computer vision, audio signal processing, frequent pattern mining, and machine learning. In this study, we present an overview of the existing studies on using big data techniques to study human behavior in the time of the COVID-19 pandemic. In particular, we categorize these studies into three groups - using big data to measure, mode
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#23616;-&#23616;&#37096;&#35757;&#32451;&#26694;&#26550;&#65292;&#20351;&#29992;&#23545;&#35937;&#20195;&#29702;&#21512;&#25104; 3D &#22330;&#26223;&#65292;&#23558;&#27599;&#20010;&#23545;&#35937;&#34920;&#31034;&#20026;&#29420;&#31435;&#30340; NeRF&#65292;&#24182;&#20132;&#26367;&#20248;&#21270;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#27599;&#20010;&#23545;&#35937;&#30340;&#23436;&#25972;&#34920;&#31034;&#30340;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2303.13450</link><description>&lt;p&gt;
Set-the-Scene: &#20840;&#23616;-&#23616;&#37096;&#35757;&#32451;&#29992;&#20110;&#29983;&#25104;&#21487;&#25511;&#30340; NeRF &#22330;&#26223;
&lt;/p&gt;
&lt;p&gt;
Set-the-Scene: Global-Local Training for Generating Controllable NeRF Scenes. (arXiv:2303.13450v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13450
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#23616;-&#23616;&#37096;&#35757;&#32451;&#26694;&#26550;&#65292;&#20351;&#29992;&#23545;&#35937;&#20195;&#29702;&#21512;&#25104; 3D &#22330;&#26223;&#65292;&#23558;&#27599;&#20010;&#23545;&#35937;&#34920;&#31034;&#20026;&#29420;&#31435;&#30340; NeRF&#65292;&#24182;&#20132;&#26367;&#20248;&#21270;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#27599;&#20010;&#23545;&#35937;&#30340;&#23436;&#25972;&#34920;&#31034;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22312;&#25991;&#26412;&#24341;&#23548;&#19979;&#30340;&#22270;&#20687;&#21512;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#20248;&#21270;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#30452;&#25509;&#20174;&#25991;&#26412;&#20013;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#27599;&#20010;&#23545;&#35937;&#30340;&#25918;&#32622;&#25110;&#22806;&#35266;&#25511;&#21046;&#26041;&#38754;&#21463;&#21040;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#20204;&#20195;&#34920;&#25972;&#20010;&#22330;&#26223;&#12290;&#36825;&#22312;&#38656;&#35201;&#32454;&#21270;&#25110;&#25805;&#32437;&#22330;&#26223;&#20013;&#30340;&#23545;&#35937;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#19981;&#36275;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20840;&#23616;-&#23616;&#37096;&#35757;&#32451;&#26694;&#26550;&#65292;&#20351;&#29992;&#23545;&#35937;&#20195;&#29702;&#21512;&#25104; 3D &#22330;&#26223;&#12290;&#20195;&#29702;&#34920;&#31034;&#29983;&#25104;&#22330;&#26223;&#20013;&#23545;&#35937;&#30340;&#25918;&#32622;&#65292;&#24182;&#21487;&#36873;&#22320;&#23450;&#20041;&#20854;&#31895;&#30053;&#20960;&#20309;&#24418;&#29366;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#22312;&#20110;&#23558;&#27599;&#20010;&#23545;&#35937;&#34920;&#31034;&#20026;&#29420;&#31435;&#30340; NeRF&#12290;&#25105;&#20204;&#22312;&#20248;&#21270;&#27599;&#20010; NeRF &#33258;&#36523;&#21644;&#23436;&#25972;&#22330;&#26223;&#30340;&#32452;&#25104;&#37096;&#20998;&#20043;&#38388;&#20132;&#26367;&#36827;&#34892;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#27599;&#20010;&#23545;&#35937;&#30340;&#23436;&#25972;&#34920;&#31034;&#65292;&#21516;&#26102;&#21019;&#24314;&#20855;&#26377;&#39118;&#26684;&#21644;&#29031;&#26126;&#21305;&#37197;&#30340;&#21644;&#35856;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent breakthroughs in text-guided image generation have led to remarkable progress in the field of 3D synthesis from text. By optimizing neural radiance fields (NeRF) directly from text, recent methods are able to produce remarkable results. Yet, these methods are limited in their control of each object's placement or appearance, as they represent the scene as a whole. This can be a major issue in scenarios that require refining or manipulating objects in the scene. To remedy this deficit, we propose a novel GlobalLocal training framework for synthesizing a 3D scene using object proxies. A proxy represents the object's placement in the generated scene and optionally defines its coarse geometry. The key to our approach is to represent each object as an independent NeRF. We alternate between optimizing each NeRF on its own and as part of the full scene. Thus, a complete representation of each object can be learned, while also creating a harmonious scene with style and lighting match. W
&lt;/p&gt;</description></item><item><title>GiveMeLabeledIssues&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#20219;&#21153;&#25512;&#33616;&#31995;&#32479;&#65292;&#21033;&#29992;API&#39046;&#22495;&#20195;&#29702;&#25216;&#33021;&#65292;&#20174;&#32780;&#21487;&#20197;&#24110;&#21161;&#24320;&#21457;&#32773;&#26356;&#22909;&#22320;&#21305;&#37197;&#20219;&#21153;&#65292;&#24182;&#20943;&#36731;&#39033;&#30446;&#32500;&#25252;&#32773;&#30340;&#36127;&#25285;&#12290;&#22312;&#39044;&#27979;API&#39046;&#22495;&#26102;&#65292;&#35813;&#24037;&#20855;&#30340;&#31934;&#24230;&#36798;&#21040;&#20102;83.9%&#12290;</title><link>http://arxiv.org/abs/2303.13418</link><description>&lt;p&gt;
GiveMeLabeledIssues&#65306;&#19968;&#20010;&#24320;&#25918;&#28304;&#20195;&#30721;&#30340;&#20219;&#21153;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
GiveMeLabeledIssues: An Open Source Issue Recommendation System. (arXiv:2303.13418v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13418
&lt;/p&gt;
&lt;p&gt;
GiveMeLabeledIssues&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#20219;&#21153;&#25512;&#33616;&#31995;&#32479;&#65292;&#21033;&#29992;API&#39046;&#22495;&#20195;&#29702;&#25216;&#33021;&#65292;&#20174;&#32780;&#21487;&#20197;&#24110;&#21161;&#24320;&#21457;&#32773;&#26356;&#22909;&#22320;&#21305;&#37197;&#20219;&#21153;&#65292;&#24182;&#20943;&#36731;&#39033;&#30446;&#32500;&#25252;&#32773;&#30340;&#36127;&#25285;&#12290;&#22312;&#39044;&#27979;API&#39046;&#22495;&#26102;&#65292;&#35813;&#24037;&#20855;&#30340;&#31934;&#24230;&#36798;&#21040;&#20102;83.9%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#32773;&#32463;&#24120;&#38590;&#20197;&#22312;&#24320;&#25918;&#28304;&#20195;&#30721;&#65288;OSS&#65289;&#39033;&#30446;&#30340;&#38382;&#39064;&#36319;&#36394;&#31995;&#32479;&#20013;&#25214;&#21040;&#21512;&#36866;&#30340;&#20219;&#21153;&#12290;&#27491;&#30830;&#30340;&#38382;&#39064;&#26631;&#35760;&#21487;&#20197;&#24110;&#21161;&#36873;&#25321;&#20219;&#21153;&#65292;&#20294;&#24403;&#21069;&#30340;&#24037;&#20855;&#20165;&#38480;&#20110;&#25353;&#29031;&#38382;&#39064;&#30340;&#31867;&#22411;&#65288;&#20363;&#22914;&#65292;&#38169;&#35823;&#12289;&#38382;&#39064;&#12289;&#22909;&#30340;&#31532;&#19968;&#20010;&#38382;&#39064;&#12289;&#21151;&#33021;&#31561;&#65289;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#12290;&#30456;&#21453;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24037;&#20855;&#65288;GiveMeLabeledIssues&#65289;&#65292;&#23427;&#21487;&#20197;&#25366;&#25496;&#39033;&#30446;&#23384;&#20648;&#24211;&#24182;&#22522;&#20110;&#35299;&#20915;&#38382;&#39064;&#25152;&#38656;&#30340;&#25216;&#33021;&#23545;&#38382;&#39064;&#36827;&#34892;&#26631;&#35760;&#12290;&#25105;&#20204;&#21033;&#29992;&#35299;&#20915;&#26041;&#26696;&#28041;&#21450;&#30340;API&#65288;&#20363;&#22914;&#65292;&#29992;&#25143;&#30028;&#38754;&#65288;UI&#65289;&#12289;&#27979;&#35797;&#12289;&#25968;&#25454;&#24211;&#65288;DB&#65289;&#31561;&#65289;&#39046;&#22495;&#20316;&#20026;&#25152;&#38656;&#25216;&#33021;&#30340;&#20195;&#29702;&#12290;GiveMeLabeledIssues&#26377;&#21161;&#20110;&#23558;&#24320;&#21457;&#32773;&#30340;&#25216;&#33021;&#19982;&#20219;&#21153;&#21305;&#37197;&#65292;&#20943;&#36731;&#39033;&#30446;&#32500;&#25252;&#32773;&#30340;&#36127;&#25285;&#12290;&#35813;&#24037;&#20855;&#22312;&#39044;&#27979;&#38382;&#39064;&#20013;&#28041;&#21450;&#30340;API&#39046;&#22495;&#26102;&#33719;&#24471;&#20102;83.9&#65285;&#30340;&#31934;&#24230;&#12290;&#22797;&#21046;&#21253;&#21547;&#26377;&#20851;&#25191;&#34892;&#35813;&#24037;&#20855;&#21644;&#21253;&#21547;&#26032;&#39033;&#30446;&#30340;&#35828;&#26126;&#12290;&#28436;&#31034;&#35270;&#39057;&#21487;&#22312;https://www.youtube.com/watch?v=ic2quUue7i8&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developers often struggle to navigate an Open Source Software (OSS) project's issue-tracking system and find a suitable task. Proper issue labeling can aid task selection, but current tools are limited to classifying the issues according to their type (e.g., bug, question, good first issue, feature, etc.). In contrast, this paper presents a tool (GiveMeLabeledIssues) that mines project repositories and labels issues based on the skills required to solve them. We leverage the domain of the APIs involved in the solution (e.g., User Interface (UI), Test, Databases (DB), etc.) as a proxy for the required skills. GiveMeLabeledIssues facilitates matching developers' skills to tasks, reducing the burden on project maintainers. The tool obtained a precision of 83.9% when predicting the API domains involved in the issues. The replication package contains instructions on executing the tool and including new projects. A demo video is available at https://www.youtube.com/watch?v=ic2quUue7i8
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#35821;&#20041;&#36716;&#25442;&#23545;&#20110;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#35757;&#32451;&#30340;&#35821;&#20041;&#36716;&#25442;&#29983;&#25104;&#27169;&#22411;&#25104;&#21151;&#28151;&#28102;&#20102;&#22810;&#20010;&#26816;&#27979;&#22120;&#12290;</title><link>http://arxiv.org/abs/2303.13408</link><description>&lt;p&gt;
&#35821;&#20041;&#36716;&#25442;&#28151;&#28102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#65292;&#32780;&#26816;&#32034;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#38450;&#24481;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense. (arXiv:2303.13408v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#35821;&#20041;&#36716;&#25442;&#23545;&#20110;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#35757;&#32451;&#30340;&#35821;&#20041;&#36716;&#25442;&#29983;&#25104;&#27169;&#22411;&#25104;&#21151;&#28151;&#28102;&#20102;&#22810;&#20010;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#26377;&#22810;&#31181;&#26041;&#27861;&#34987;&#25552;&#20986;&#26469;&#29992;&#20110;&#35782;&#21035;&#24694;&#24847;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (&#20363;&#22914;&#34394;&#20551;&#20869;&#23481;&#21019;&#24314;&#25110;&#23398;&#26415;&#25220;&#34989;)&#20013;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#65292;&#21253;&#25324;&#36890;&#36807;&#27700;&#21360;&#25110;&#32479;&#35745;&#24322;&#24120;&#28857;&#12290;&#26412;&#25991;&#25506;&#31350;&#36825;&#20123;&#25991;&#26412;&#26816;&#27979;&#31639;&#27861;&#23545;&#20110;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#21547;&#20041;&#36716;&#25442;&#30340;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#27979;&#35797;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#39318;&#20808;&#35757;&#32451;&#20102;&#19968;&#20010;11B&#21442;&#25968;&#30340;&#35821;&#20041;&#36716;&#25442;&#29983;&#25104;&#27169;&#22411;(DIPPER)&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#23558;&#27573;&#33853;&#36827;&#34892;&#35821;&#20041;&#36716;&#25442;&#65292;&#21487;&#36873;&#25321;&#21033;&#29992;&#21608;&#22260;&#25991;&#26412;(&#20363;&#22914;&#29992;&#25143;&#20889;&#30340;&#25552;&#31034;)&#20316;&#20026;&#19978;&#19979;&#25991;&#12290;DIPPER&#36824;&#20351;&#29992;&#26631;&#37327;&#26059;&#38062;&#26469;&#25511;&#21046;&#35821;&#20041;&#36716;&#25442;&#20013;&#35789;&#27719;&#22810;&#26679;&#24615;&#21644;&#37325;&#26032;&#25490;&#21015;&#30340;&#31243;&#24230;&#12290;&#36890;&#36807;&#20351;&#29992;DIPPER&#26469;&#36827;&#34892;&#19977;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#30340;&#35821;&#20041;&#36716;&#25442;&#65292;&#25104;&#21151;&#22320;&#28151;&#28102;&#20102;&#22810;&#20010;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;&#21253;&#25324;&#27700;&#21360;&#26816;&#27979;&#12289;GPTZero&#12289;DetectGPT&#21644;OpenAI&#30340;&#25991;&#26412;&#20998;&#31867;&#22120;&#12290;&#20363;&#22914;&#65292;DIPPER&#23558;DetectGPT&#30340;&#26816;&#27979;&#20934;&#30830;&#29575;&#20174;70.3%&#38477;&#33267;4.6%&#65288;&#22312;&#24658;&#23450;&#30340;1%&#35823;&#25253;&#29575;&#19979;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
To detect the deployment of large language models for malicious use cases (e.g., fake content creation or academic plagiarism), several approaches have recently been proposed for identifying AI-generated text via watermarks or statistical irregularities. How robust are these detection algorithms to paraphrases of AI-generated text? To stress test these detectors, we first train an 11B parameter paraphrase generation model (DIPPER) that can paraphrase paragraphs, optionally leveraging surrounding text (e.g., user-written prompts) as context. DIPPER also uses scalar knobs to control the amount of lexical diversity and reordering in the paraphrases. Paraphrasing text generated by three large language models (including GPT3.5-davinci-003) with DIPPER successfully evades several detectors, including watermarking, GPTZero, DetectGPT, and OpenAI's text classifier. For example, DIPPER drops the detection accuracy of DetectGPT from 70.3% to 4.6% (at a constant false positive rate of 1%), withou
&lt;/p&gt;</description></item><item><title>&#22312;&#32447;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#31471;&#28857;&#26816;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;&#28145;&#24230;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#26426;&#65292;&#36991;&#20813;&#20351;&#29992;&#26114;&#36149;&#30340;&#32593;&#26684;&#25628;&#32034;&#65292;&#19981;&#38656;&#35201;&#30495;&#20540;&#26631;&#31614;&#65292;&#24182;&#25104;&#21151;&#20943;&#23569;&#20102;&#26089;&#26399;&#25130;&#27490;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2303.13407</link><description>&lt;p&gt;
&#28145;&#24230;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#33258;&#36866;&#24212;&#31471;&#28857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Adaptive Endpointing with Deep Contextual Multi-armed Bandits. (arXiv:2303.13407v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13407
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#31471;&#28857;&#26816;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;&#28145;&#24230;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#26426;&#65292;&#36991;&#20813;&#20351;&#29992;&#26114;&#36149;&#30340;&#32593;&#26684;&#25628;&#32034;&#65292;&#19981;&#38656;&#35201;&#30495;&#20540;&#26631;&#31614;&#65292;&#24182;&#25104;&#21151;&#20943;&#23569;&#20102;&#26089;&#26399;&#25130;&#27490;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#31471;&#28857;&#26816;&#27979;&#65288;EP&#65289;&#35299;&#20915;&#26041;&#26696;&#26159;&#22312;&#30417;&#30563;&#26694;&#26550;&#19979;&#36827;&#34892;&#23398;&#20064;&#30340;&#65292;&#36825;&#19981;&#20801;&#35768;&#27169;&#22411;&#33719;&#24471;&#21453;&#39304;&#24182;&#22312;&#22312;&#32447;&#35774;&#32622;&#20013;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#36890;&#24120;&#20351;&#29992;&#26114;&#36149;&#30340;&#32593;&#26684;&#25628;&#32034;&#26469;&#25214;&#21040;&#31471;&#28857;&#26816;&#27979;&#27169;&#22411;&#30340;&#26368;&#20339;&#37197;&#32622;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20026;&#32473;&#23450;&#35821;&#38899;&#32423;&#21035;&#30340;&#38899;&#39057;&#29305;&#24449;&#22312;&#22312;&#32447;&#35774;&#32622;&#20013;&#36873;&#25321;&#26368;&#20339;&#30340;&#31471;&#28857;&#26816;&#27979;&#37197;&#32622;&#65292;&#21516;&#26102;&#36991;&#20813;&#36229;&#21442;&#25968;&#30340;&#32593;&#26684;&#25628;&#32034;&#65292;&#20174;&#32780;&#20026;&#33258;&#36866;&#24212;&#31471;&#28857;&#26816;&#27979;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#22320;&#38754;&#30495;&#20540;&#26631;&#31614;&#65292;&#24182;&#20165;&#20351;&#29992;&#26469;&#33258;&#22870;&#21169;&#20449;&#21495;&#30340;&#22312;&#32447;&#23398;&#20064;&#32780;&#19981;&#38656;&#35201;&#27880;&#37322;&#26631;&#31614;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#24449;&#33021;&#21147;&#21644;&#27748;&#26222;&#26862;&#24314;&#27169;&#31639;&#27861;&#30340;&#34892;&#20026;&#25506;&#32034;&#34892;&#20026;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20960;&#20010;&#22522;&#32447;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#28145;&#24230;&#36172;&#21338;&#27169;&#22411;&#20063;&#25104;&#21151;&#20943;&#23569;&#20102;&#26089;&#26399;&#25130;&#27490;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current endpointing (EP) solutions learn in a supervised framework, which does not allow the model to incorporate feedback and improve in an online setting. Also, it is a common practice to utilize costly grid-search to find the best configuration for an endpointing model. In this paper, we aim to provide a solution for adaptive endpointing by proposing an efficient method for choosing an optimal endpointing configuration given utterance-level audio features in an online setting, while avoiding hyperparameter grid-search. Our method does not require ground truth labels, and only uses online learning from reward signals without requiring annotated labels. Specifically, we propose a deep contextual multi-armed bandit-based approach, which combines the representational power of neural networks with the action exploration behavior of Thompson modeling algorithms. We compare our approach to several baselines, and show that our deep bandit models also succeed in reducing early cutoff errors 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#23558;&#36890;&#29992;&#30340;&#32422;&#26463;&#20248;&#21270;&#27714;&#35299;&#22120;&#19982;&#32508;&#21512;&#32422;&#26463;&#25240;&#21472;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#21644;&#24191;&#27867;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.13401</link><description>&lt;p&gt;
&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#20248;&#21270;&#21644;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Optimization and Optimizers for Adversarial Robustness. (arXiv:2303.13401v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#23558;&#36890;&#29992;&#30340;&#32422;&#26463;&#20248;&#21270;&#27714;&#35299;&#22120;&#19982;&#32508;&#21512;&#32422;&#26463;&#25240;&#21472;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#21644;&#24191;&#27867;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#35780;&#20272;&#38656;&#35201;&#35299;&#20915;&#38750;&#24179;&#20961;&#30340;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#25968;&#20540;&#31639;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#25237;&#24433;&#26799;&#24230;&#65292;&#24182;&#19988;&#20027;&#35201;&#22788;&#29702;&#30001;$\ell_1$&#65292;$\ell_2$&#21644;$\ell_\infty$&#36317;&#31163;&#24314;&#27169;&#30340;&#25200;&#21160;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#26694;&#26550;&#65292;&#23427;&#23558;&#36890;&#29992;&#30340;&#32422;&#26463;&#20248;&#21270;&#27714;&#35299;&#22120;PyGRANSO&#19982;Constraint Folding (PWCF)&#32467;&#21512;&#36215;&#26469;&#65292;&#21487;&#20197;&#22686;&#21152;&#26356;&#22810;&#21487;&#38752;&#24615;&#21644;&#24191;&#27867;&#24615;&#21040;&#26368;&#20808;&#36827;&#30340;RE&#36719;&#20214;&#21253;&#65292;&#20363;&#22914;AutoAttack&#12290;&#20851;&#20110;&#21487;&#38752;&#24615;&#65292;PWCF&#25552;&#20379;&#24102;&#26377;&#31283;&#23450;&#24615;&#27979;&#37327;&#21644;&#21487;&#34892;&#24615;&#27979;&#35797;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#35780;&#20272;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#12290;&#23545;&#20110;&#24191;&#27867;&#24615;&#65292;PWCF&#21487;&#20197;&#22788;&#29702;&#36890;&#24120;&#23545;&#29616;&#26377;&#25237;&#24433;&#26799;&#24230;&#26041;&#27861;&#19981;&#21487;&#35775;&#38382;&#30340;&#25200;&#21160;&#27169;&#22411;&#65307;&#20027;&#35201;&#35201;&#27714;&#26159;&#36317;&#31163;&#24230;&#37327;&#22312;&#20960;&#20046;&#25152;&#26377;&#22320;&#26041;&#37117;&#21487;&#24494;&#20998;&#12290;&#21033;&#29992;PWCF&#30340;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;
Empirical robustness evaluation (RE) of deep learning models against adversarial perturbations entails solving nontrivial constrained optimization problems. Existing numerical algorithms that are commonly used to solve them in practice predominantly rely on projected gradient, and mostly handle perturbations modeled by the $\ell_1$, $\ell_2$ and $\ell_\infty$ distances. In this paper, we introduce a novel algorithmic framework that blends a general-purpose constrained-optimization solver PyGRANSO with Constraint Folding (PWCF), which can add more reliability and generality to the state-of-the-art RE packages, e.g., AutoAttack. Regarding reliability, PWCF provides solutions with stationarity measures and feasibility tests to assess the solution quality. For generality, PWCF can handle perturbation models that are typically inaccessible to the existing projected gradient methods; the main requirement is the distance metric to be almost everywhere differentiable. Taking advantage of PWCF 
&lt;/p&gt;</description></item><item><title>Xplainer&#26159;&#19968;&#20010;&#36879;&#26126;&#19988;&#21487;&#35299;&#37322;&#30340;&#38646;&#26679;&#26412;&#35786;&#26029;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#23384;&#22312;&#30340;&#25551;&#36848;&#24615;&#35266;&#23519;&#36827;&#34892;&#20998;&#31867;&#26469;&#25552;&#39640;&#33258;&#21160;&#35786;&#26029;&#38598;&#25104;&#21040;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#20013;&#30340;&#25928;&#29575;&#65292;&#21516;&#26102;&#36991;&#20813;&#38656;&#35201;&#22823;&#37327;&#27880;&#37322;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.13391</link><description>&lt;p&gt;
Xplainer&#65306;&#20174;X&#23556;&#32447;&#35266;&#23519;&#21040;&#21487;&#35299;&#37322;&#30340;&#38646;&#26679;&#26412;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Xplainer: From X-Ray Observations to Explainable Zero-Shot Diagnosis. (arXiv:2303.13391v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13391
&lt;/p&gt;
&lt;p&gt;
Xplainer&#26159;&#19968;&#20010;&#36879;&#26126;&#19988;&#21487;&#35299;&#37322;&#30340;&#38646;&#26679;&#26412;&#35786;&#26029;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#23384;&#22312;&#30340;&#25551;&#36848;&#24615;&#35266;&#23519;&#36827;&#34892;&#20998;&#31867;&#26469;&#25552;&#39640;&#33258;&#21160;&#35786;&#26029;&#38598;&#25104;&#21040;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#20013;&#30340;&#25928;&#29575;&#65292;&#21516;&#26102;&#36991;&#20813;&#38656;&#35201;&#22823;&#37327;&#27880;&#37322;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21307;&#23398;&#22270;&#20687;&#36827;&#34892;&#33258;&#21160;&#35786;&#26029;&#39044;&#27979;&#65292;&#26159;&#25903;&#25345;&#20020;&#24202;&#20915;&#31574;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#31995;&#32479;&#36890;&#24120;&#38656;&#35201;&#22312;&#22823;&#37327;&#27880;&#37322;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#21307;&#23398;&#39046;&#22495;&#30340;&#27880;&#37322;&#25968;&#25454;&#24448;&#24448;&#24456;&#23569;&#12290;&#38646;&#26679;&#26412;&#26041;&#27861;&#36890;&#36807;&#20801;&#35768;&#22312;&#19981;&#20381;&#36182;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#28789;&#27963;&#36866;&#24212;&#20855;&#26377;&#19981;&#21516;&#20020;&#24202;&#32467;&#26524;&#30340;&#26032;&#35774;&#32622;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23558;&#33258;&#21160;&#35786;&#26029;&#38598;&#25104;&#21040;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#20013;&#65292;&#26041;&#27861;&#24212;&#35813;&#26159;&#36879;&#26126;&#19988;&#21487;&#35299;&#37322;&#30340;&#65292;&#22686;&#21152;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#30340;&#20449;&#20219;&#24182;&#20419;&#36827;&#27491;&#30830;&#24615;&#39564;&#35777;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Xplainer&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#20020;&#24202;&#35774;&#32622;&#20013;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#38646;&#26679;&#26412;&#35786;&#26029;&#30340;&#26032;&#26694;&#26550;&#12290;Xplainer&#23558;&#23545;&#27604;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#31867;&#21363;&#25551;&#36848;&#26041;&#27861;&#36866;&#24212;&#20110;&#22810;&#26631;&#31614;&#21307;&#23398;&#35786;&#26029;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#31034;&#27169;&#22411;&#23545;&#23384;&#22312;&#30340;&#25551;&#36848;&#24615;&#35266;&#23519;&#36827;&#34892;&#20998;&#31867;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#39044;&#27979;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated diagnosis prediction from medical images is a valuable resource to support clinical decision-making. However, such systems usually need to be trained on large amounts of annotated data, which often is scarce in the medical domain. Zero-shot methods address this challenge by allowing a flexible adaption to new settings with different clinical findings without relying on labeled data. Further, to integrate automated diagnosis in the clinical workflow, methods should be transparent and explainable, increasing medical professionals' trust and facilitating correctness verification. In this work, we introduce Xplainer, a novel framework for explainable zero-shot diagnosis in the clinical setting. Xplainer adapts the classification-by-description approach of contrastive vision-language models to the multi-label medical diagnosis task. Specifically, instead of directly predicting a diagnosis, we prompt the model to classify the existence of descriptive observations, which a radiologi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#36716;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#38646;&#26679;&#26412;&#39046;&#22495;&#36716;&#31227;&#65292;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#39046;&#22495;&#33258;&#30001;&#25991;&#26412;&#36827;&#34892;&#39046;&#22495;&#21644;&#20219;&#21153;&#30693;&#35782;&#30340;&#20849;&#21516;&#23398;&#20064;&#65292;&#36890;&#36807; NLGU &#31574;&#30053;&#23454;&#29616;&#39046;&#22495;&#25968;&#25454;&#22686;&#24378;&#21644;&#26631;&#31614;&#39044;&#27979;&#65292;&#32463;&#23454;&#39564;&#35777;&#26126;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#21644;&#25918;&#23556;&#23398;&#23376;&#39046;&#22495;&#20855;&#26377;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#32988;&#36807;&#20102;&#29616;&#26377;&#30340; SOTA&#12290;</title><link>http://arxiv.org/abs/2303.13386</link><description>&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#27169;&#22411;&#30340;&#32452;&#21512;&#38646;&#26679;&#26412;&#39046;&#22495;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Compositional Zero-Shot Domain Transfer with Text-to-Text Models. (arXiv:2303.13386v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13386
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#36716;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#38646;&#26679;&#26412;&#39046;&#22495;&#36716;&#31227;&#65292;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#39046;&#22495;&#33258;&#30001;&#25991;&#26412;&#36827;&#34892;&#39046;&#22495;&#21644;&#20219;&#21153;&#30693;&#35782;&#30340;&#20849;&#21516;&#23398;&#20064;&#65292;&#36890;&#36807; NLGU &#31574;&#30053;&#23454;&#29616;&#39046;&#22495;&#25968;&#25454;&#22686;&#24378;&#21644;&#26631;&#31614;&#39044;&#27979;&#65292;&#32463;&#23454;&#39564;&#35777;&#26126;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#21644;&#25918;&#23556;&#23398;&#23376;&#39046;&#22495;&#20855;&#26377;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#32988;&#36807;&#20102;&#29616;&#26377;&#30340; SOTA&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19987;&#19994;&#39046;&#22495;&#20013;&#65292;&#26631;&#31614;&#31232;&#32570;&#26159;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#30340;&#29942;&#39048;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32452;&#21512;&#36716;&#31227;&#23398;&#20064;&#26694;&#26550;&#65288;DoT5 &#39046;&#22495;&#32452;&#21512;&#38646;&#26679;&#26412; T5&#65289;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#39046;&#22495;&#36716;&#31227;&#12290;&#22312;&#27809;&#26377;&#35775;&#38382;&#39046;&#22495;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;DoT5&#20197;&#22810;&#20219;&#21153;&#30340;&#26041;&#24335;&#20849;&#21516;&#23398;&#20064;&#39046;&#22495;&#30693;&#35782;&#65288;&#20174;&#26410;&#26631;&#35760;&#30340;&#39046;&#22495;&#33258;&#30001;&#25991;&#26412;&#30340; MLM &#20013;&#23398;&#20064;&#65289;&#21644;&#20219;&#21153;&#30693;&#35782;&#65288;&#20174;&#26356;&#23481;&#26131;&#33719;&#21462;&#30340;&#36890;&#29992;&#39046;&#22495;&#25968;&#25454;&#30340;&#20219;&#21153;&#35757;&#32451;&#20013;&#23398;&#20064;&#65289;&#12290;&#20026;&#20102;&#25552;&#39640;&#20219;&#21153;&#35757;&#32451;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026; NLGU &#30340;&#31574;&#30053;&#65306;&#25105;&#20204;&#21516;&#26102;&#20026;&#39046;&#22495;&#26631;&#31614;&#21040;&#25968;&#25454;&#29983;&#25104;&#35757;&#32451; NLG&#65292;&#20174;&#32780;&#23454;&#29616;&#29992;&#20110;&#33258;&#25105;&#24494;&#35843;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#29992;&#20110;&#26631;&#31614;&#39044;&#27979;&#30340; NLU &#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#21644;&#25918;&#23556;&#23398;&#30340;&#36164;&#28304;&#36139;&#20047;&#23376;&#39046;&#22495;&#19978;&#35780;&#20272;&#20102; DoT5&#65292;&#37325;&#28857;&#20851;&#27880; NLI&#12289;&#25991;&#26412;&#25688;&#35201;&#21644;&#23884;&#20837;&#23398;&#20064;&#12290;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;DoT5&#35777;&#26126;&#20102;&#32452;&#21512;&#36716;&#31227;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#38646;&#26679;&#26412;&#36716;&#31227;&#26041;&#38754;&#32988;&#36807;&#20102;&#29616;&#26377;&#30340; SOTA&#12290;
&lt;/p&gt;
&lt;p&gt;
Label scarcity is a bottleneck for improving task performance in specialised domains. We propose a novel compositional transfer learning framework (DoT5 domain compositional zero-shot T5) for zero-shot domain transfer. Without access to in-domain labels, DoT5 jointly learns domain knowledge (from MLM of unlabelled in-domain free text) and task knowledge (from task training on more readily available general-domain data) in a multi-task manner. To improve the transferability of task training, we design a strategy named NLGU: we simultaneously train NLG for in-domain label-to-data generation which enables data augmentation for self-finetuning and NLU for label prediction. We evaluate DoT5 on the biomedical domain and the resource-lean subdomain of radiology, focusing on NLI, text summarisation and embedding learning. DoT5 demonstrates the effectiveness of compositional transfer learning through multi-task learning. In particular, DoT5 outperforms the current SOTA in zero-shot transfer b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#23545;&#25239;&#24615;&#34917;&#19969;&#65288;UAP&#65289;&#25915;&#20987;&#26041;&#27861;&#65292;&#21482;&#38656;&#38468;&#21152;&#19968;&#20010;&#30456;&#23545;&#36739;&#23567;&#30340;&#23383;&#33410;&#32423;&#34917;&#19969;&#21363;&#21487;&#32469;&#36807;MalConv&#20998;&#31867;&#22120;&#30340;&#26816;&#27979;&#65292;&#21487;&#20197;&#23558;&#24694;&#24847;&#36719;&#20214;&#25991;&#20214;&#30340;&#26816;&#27979;&#29575;&#38477;&#20302;80&#65285;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#31383;&#21475;&#28040;&#38500;&#22788;&#29702;&#20316;&#20026;&#24212;&#23545;&#27492;&#31181;&#25915;&#20987;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.13372</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#38745;&#24577;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Adversarial Robustness of Learning-based Static Malware Classifiers. (arXiv:2303.13372v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#23545;&#25239;&#24615;&#34917;&#19969;&#65288;UAP&#65289;&#25915;&#20987;&#26041;&#27861;&#65292;&#21482;&#38656;&#38468;&#21152;&#19968;&#20010;&#30456;&#23545;&#36739;&#23567;&#30340;&#23383;&#33410;&#32423;&#34917;&#19969;&#21363;&#21487;&#32469;&#36807;MalConv&#20998;&#31867;&#22120;&#30340;&#26816;&#27979;&#65292;&#21487;&#20197;&#23558;&#24694;&#24847;&#36719;&#20214;&#25991;&#20214;&#30340;&#26816;&#27979;&#29575;&#38477;&#20302;80&#65285;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#31383;&#21475;&#28040;&#38500;&#22788;&#29702;&#20316;&#20026;&#24212;&#23545;&#27492;&#31181;&#25915;&#20987;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#19968;&#30452;&#26159;&#24694;&#24847;&#36719;&#20214;&#20316;&#32773;&#21644;&#21453;&#30149;&#27602;&#31995;&#32479;&#20043;&#38388;&#25345;&#32493;&#30340;&#20891;&#22791;&#31454;&#36187;&#38454;&#27573;&#12290;&#38543;&#30528;&#36825;&#22330;&#31454;&#36187;&#35268;&#27169;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#24471;&#21040;&#20102;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36235;&#21183;&#20351;&#24471;&#30452;&#25509;&#23545;ML&#36827;&#34892;&#25915;&#20987;&#23545;&#20110;&#23545;&#25163;&#32780;&#35328;&#25104;&#20026;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#21069;&#26223;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#22330;&#20891;&#22791;&#31454;&#36187;&#30340;&#20004;&#20010;&#26041;&#38754;&#65292;&#21363;&#20174;&#24694;&#24847;&#36719;&#20214;&#25991;&#20214;&#30340;&#21407;&#22987;&#23383;&#33410;&#20013;&#25805;&#20316;&#30340;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#27969;&#34892;&#20998;&#31867;&#22120;MalConv&#30340;&#35282;&#24230;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#34920;&#26126;MalConv&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#34917;&#19969;&#25915;&#20987;&#30340;&#24433;&#21709;:&#23558;&#19968;&#20010;&#23383;&#33410;&#32423;&#30340;&#34917;&#19969;&#38468;&#21152;&#21040;&#24694;&#24847;&#36719;&#20214;&#25991;&#20214;&#20013;&#65292;&#20351;&#20854;&#32469;&#36807;&#26816;&#27979;&#30340;&#27010;&#29575;&#39640;&#36798;94.3&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#23545;&#25239;&#24615;&#34917;&#19969;&#65288;UAP&#65289;&#25915;&#20987;&#65292;&#22312;&#20219;&#20309;&#21253;&#21547;&#35813;&#34917;&#19969;&#30340;&#24694;&#24847;&#36719;&#20214;&#25991;&#20214;&#30340;&#24658;&#23450;&#26102;&#38388;&#20869;&#65292;&#21487;&#20197;&#23558;&#20854;&#26816;&#27979;&#29575;&#38477;&#20302;80&#65285;&#12290;&#21363;&#20351;&#30456;&#23545;&#20110;&#21407;&#22987;&#25991;&#20214;&#22823;&#23567;&#32780;&#35328;&#65292;&#36825;&#20123;&#34917;&#19969;&#30340;&#22823;&#23567;&#20063;&#30456;&#23545;&#36739;&#23567;-&#22312;2&#65285;-8&#65285;&#20043;&#38388;&#12290;&#20026;&#20102;&#25269;&#24481;&#36825;&#31181;&#25915;&#20987;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31383;&#21475;&#28040;&#38500;&#22788;&#29702;&#65292;&#20801;&#35768;&#35782;&#21035;&#24694;&#24847;&#20195;&#30721;&#30340;&#37096;&#20998;&#19981;&#21463;&#23545;&#25239;&#24615;&#34917;&#19969;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Malware detection has long been a stage for an ongoing arms race between malware authors and anti-virus systems. Solutions that utilize machine learning (ML) gain traction as the scale of this arms race increases. This trend, however, makes performing attacks directly on ML an attractive prospect for adversaries. We study this arms race from both perspectives in the context of MalConv, a popular convolutional neural network-based malware classifier that operates on raw bytes of files. First, we show that MalConv is vulnerable to adversarial patch attacks: appending a byte-level patch to malware files bypasses detection 94.3% of the time. Moreover, we develop a universal adversarial patch (UAP) attack where a single patch can drop the detection rate in constant time of any malware file that contains it by 80%. These patches are effective even being relatively small with respect to the original file size -between 2%-8%. As a countermeasure, we then perform window ablation that allows u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#22312;&#38656;&#27714;&#35268;&#33539;&#21270;&#20013;&#30340;&#24212;&#29992;&#29616;&#29366;&#65292;&#21457;&#29616;&#21551;&#21457;&#24335;NLP&#26041;&#27861;&#26159;&#33258;&#21160;RF&#20013;&#26368;&#24120;&#29992;&#30340;NLP&#25216;&#26415;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21017;&#21253;&#25324;&#22522;&#20110;&#20998;&#31867;&#21644;&#32858;&#31867;&#30340;&#25216;&#26415;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;NLP&#21644;ML&#25216;&#26415;&#22312;RF&#39046;&#22495;&#20013;&#30340;&#36739;&#22823;&#36827;&#23637;&#65292;&#24182;&#25351;&#20986;&#20102;&#22312;&#24212;&#29992;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2303.13365</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#22312;&#38656;&#27714;&#35268;&#33539;&#21270;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#31687;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Requirement Formalisation using Natural Language Processing and Machine Learning: A Systematic Review. (arXiv:2303.13365v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#22312;&#38656;&#27714;&#35268;&#33539;&#21270;&#20013;&#30340;&#24212;&#29992;&#29616;&#29366;&#65292;&#21457;&#29616;&#21551;&#21457;&#24335;NLP&#26041;&#27861;&#26159;&#33258;&#21160;RF&#20013;&#26368;&#24120;&#29992;&#30340;NLP&#25216;&#26415;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21017;&#21253;&#25324;&#22522;&#20110;&#20998;&#31867;&#21644;&#32858;&#31867;&#30340;&#25216;&#26415;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;NLP&#21644;ML&#25216;&#26415;&#22312;RF&#39046;&#22495;&#20013;&#30340;&#36739;&#22823;&#36827;&#23637;&#65292;&#24182;&#25351;&#20986;&#20102;&#22312;&#24212;&#29992;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#24320;&#21457;&#26041;&#27861;&#30340;&#25913;&#36827;&#21560;&#24341;&#20102;&#24320;&#21457;&#20154;&#21592;&#22312;&#38656;&#27714;&#24037;&#31243;&#39046;&#22495;&#33258;&#21160;&#21270;&#38656;&#27714;&#35268;&#33539;&#21270;&#65288;RF&#65289;&#20013;&#24212;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#65292;&#25253;&#21578;&#20102;&#24212;&#29992;NLP&#21644;ML&#22312;&#20943;&#23569;&#33258;&#28982;&#35821;&#35328;&#32534;&#20889;&#30340;&#38656;&#27714;&#19981;&#30830;&#23450;&#24615;&#21644;&#19981;&#23436;&#25972;&#24615;&#26041;&#38754;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#35843;&#26597;&#21644;&#20998;&#31867;&#29616;&#26377;&#30340;NLP&#21644;ML&#22312;RF&#19978;&#30340;&#24037;&#20316;&#65292;&#35782;&#21035;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#24182;&#25552;&#20379;&#26377;&#21069;&#36884;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#36873;&#21462;&#20102;&#26469;&#33258;&#24120;&#29992;&#24211;&#30340;257&#31687;&#35770;&#25991;&#12290;&#36890;&#36807;&#23450;&#20041;&#21253;&#21547;&#21644;&#25490;&#38500;&#26631;&#20934;&#26469;&#36807;&#28388;&#25628;&#32034;&#32467;&#26524;&#65292;&#24182;&#36873;&#25321;&#20102;47&#39033;&#30456;&#20851;&#30740;&#31350;&#65292;&#26102;&#38388;&#36328;&#24230;&#22312;2012&#24180;&#33267;2022&#24180;&#20043;&#38388;&#12290;&#25105;&#20204;&#21457;&#29616;&#21551;&#21457;&#24335;NLP&#26041;&#27861;&#26159;&#33258;&#21160;RF&#20013;&#26368;&#24120;&#29992;&#30340;NLP&#25216;&#26415;&#65292;&#20027;&#35201;&#24212;&#29992;&#20110;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#32780;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21017;&#21253;&#25324;&#22522;&#20110;&#20998;&#31867;&#21644;&#32858;&#31867;&#30340;&#25216;&#26415;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;NLP&#21644;ML&#25216;&#26415;&#22312;RF&#39046;&#22495;&#20013;&#30340;&#36739;&#22823;&#36827;&#23637;&#65292;&#24182;&#25351;&#20986;&#20102;&#22312;&#24212;&#29992;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improvement of software development methodologies attracts developers to automatic Requirement Formalisation (RF) in the Requirement Engineering (RE) field. The potential advantages by applying Natural Language Processing (NLP) and Machine Learning (ML) in reducing the ambiguity and incompleteness of requirement written in natural languages is reported in different studies. The goal of this paper is to survey and classify existing work on NLP and ML for RF, identifying challenges in this domain and providing promising future research directions. To achieve this, we conducted a systematic literature review to outline the current state-of-the-art of NLP and ML techniques in RF by selecting 257 papers from common used libraries. The search result is filtered by defining inclusion and exclusion criteria and 47 relevant studies between 2012 and 2022 are selected. We found that heuristic NLP approaches are the most common NLP techniques used for automatic RF, primary operating on structured 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#35780;&#20272;&#20102;EmoWOZ&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#21010;&#20998;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#23618;&#25277;&#26679;&#26041;&#27861;&#29992;&#20110;&#22788;&#29702;&#39640;&#24230;&#19981;&#24179;&#34913;&#21644;&#19981;&#22343;&#21248;&#20998;&#24067;&#30340;&#24773;&#24863;&#26631;&#31614;&#12290;&#20351;&#29992;&#36825;&#20010;&#26041;&#27861;&#24314;&#31435;&#22312;EmoWoz&#19978;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#26410;&#26469;&#30740;&#31350;&#32773;&#24212;&#35813;&#37319;&#21462;&#36825;&#31181;&#21010;&#20998;&#20197;&#30830;&#20445;&#19968;&#33268;&#21644;&#20934;&#30830;&#30340;&#24615;&#33021;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2303.13364</link><description>&lt;p&gt;
&#37325;&#26032;&#35780;&#20272;EmoWOZ&#20013;&#38024;&#23545;&#24773;&#24863;&#26816;&#27979;&#30340;&#25968;&#25454;&#21010;&#20998;
&lt;/p&gt;
&lt;p&gt;
Reevaluating Data Partitioning for Emotion Detection in EmoWOZ. (arXiv:2303.13364v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#35780;&#20272;&#20102;EmoWOZ&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#21010;&#20998;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#23618;&#25277;&#26679;&#26041;&#27861;&#29992;&#20110;&#22788;&#29702;&#39640;&#24230;&#19981;&#24179;&#34913;&#21644;&#19981;&#22343;&#21248;&#20998;&#24067;&#30340;&#24773;&#24863;&#26631;&#31614;&#12290;&#20351;&#29992;&#36825;&#20010;&#26041;&#27861;&#24314;&#31435;&#22312;EmoWoz&#19978;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#26410;&#26469;&#30740;&#31350;&#32773;&#24212;&#35813;&#37319;&#21462;&#36825;&#31181;&#21010;&#20998;&#20197;&#30830;&#20445;&#19968;&#33268;&#21644;&#20934;&#30830;&#30340;&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32858;&#28966;&#20110;EmoWOZ&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;MultiWOZ&#25968;&#25454;&#38598;&#30340;&#25193;&#23637;&#65292;&#25552;&#20379;&#20102;&#23545;&#35805;&#30340;&#24773;&#24863;&#26631;&#31614;&#12290;&#19982;&#21407;&#22987;&#30340;MultiWOZ&#25968;&#25454;&#38598;&#22240;&#20854;&#23427;&#30446;&#30340;&#34987;&#21010;&#20998;&#19981;&#21516;&#65292;EmoWOZ&#20013;&#24773;&#24863;&#26631;&#31614;&#39640;&#24230;&#19981;&#24179;&#34913;&#65292;&#20998;&#24067;&#22312;&#19981;&#21516;&#21010;&#20998;&#20013;&#20063;&#19981;&#22343;&#21248;&#65292;&#23548;&#33268;&#27169;&#22411;&#27604;&#36739;&#25928;&#26524;&#27424;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12289;&#25913;&#21892;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#24182;&#20943;&#23569;&#25968;&#25454;&#38598;&#20998;&#24067;&#20559;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24773;&#24863;&#26631;&#31614;&#30340;&#20998;&#23618;&#25277;&#26679;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#29305;&#27530;&#25216;&#26415;&#26469;&#22788;&#29702;&#26377;&#22810;&#20010;&#24773;&#24863;&#26631;&#31614;&#30340;&#23545;&#35805;&#65288;&#24207;&#21015;&#65289;&#25968;&#25454;&#12290;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#25277;&#26679;&#26041;&#27861;&#65292;&#24314;&#31435;&#22312;EmoWoz&#19978;&#30340;&#27169;&#22411;&#21487;&#20197;&#34920;&#29616;&#26356;&#22909;&#65292;&#20351;&#23427;&#25104;&#20026;&#35757;&#32451;&#20855;&#26377;&#24773;&#24863;&#26234;&#33021;&#30340;&#23545;&#35805;&#20195;&#29702;&#26356;&#20026;&#21487;&#38752;&#30340;&#36164;&#28304;&#12290;&#25105;&#20204;&#25512;&#33616;&#26410;&#26469;&#30340;&#30740;&#31350;&#32773;&#20351;&#29992;&#36825;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21010;&#20998;&#26469;&#30830;&#20445;&#19968;&#33268;&#21644;&#20934;&#30830;&#30340;&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on the EmoWoz dataset, an extension of MultiWOZ that provides emotion labels for the dialogues. MultiWOZ was partitioned initially for another purpose, resulting in a distributional shift when considering the new purpose of emotion recognition. The emotion tags in EmoWoz are highly imbalanced and unevenly distributed across the partitions, which causes sub-optimal performance and poor comparison of models. We propose a stratified sampling scheme based on emotion tags to address this issue, improve the dataset's distribution, and reduce dataset shift. We also introduce a special technique to handle conversation (sequential) data with many emotional tags. Using our proposed sampling method, models built upon EmoWoz can perform better, making it a more reliable resource for training conversational agents with emotional intelligence. We recommend that future researchers use this new partitioning to ensure consistent and accurate performance evaluations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#24322;&#26500;&#35774;&#22791;&#21644;&#35268;&#27169;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#19968;&#31181;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#21407;&#22411;&#31995;&#32479;&#65292;&#20197;&#25903;&#25345;&#36328;&#35774;&#22791;&#32852;&#37030;&#23398;&#20064;&#65292;&#20174;&#32780;&#24357;&#34917;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#38388;&#22312;&#32852;&#37030;&#23398;&#20064;&#30740;&#31350;&#20013;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2303.13363</link><description>&lt;p&gt;
FS-Real&#65306;&#21521;&#30495;&#23454;&#36328;&#35774;&#22791;&#32852;&#37030;&#23398;&#20064;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
FS-Real: Towards Real-World Cross-Device Federated Learning. (arXiv:2303.13363v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#24322;&#26500;&#35774;&#22791;&#21644;&#35268;&#27169;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#19968;&#31181;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#21407;&#22411;&#31995;&#32479;&#65292;&#20197;&#25903;&#25345;&#36328;&#35774;&#22791;&#32852;&#37030;&#23398;&#20064;&#65292;&#20174;&#32780;&#24357;&#34917;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#38388;&#22312;&#32852;&#37030;&#23398;&#20064;&#30740;&#31350;&#20013;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26088;&#22312;&#22312;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#30340;&#24110;&#21161;&#19979;&#35757;&#32451;&#39640;&#36136;&#37327;&#30340;&#27169;&#22411;&#65292;&#32780;&#19981;&#19978;&#20256;&#20182;&#20204;&#30340;&#26412;&#22320;&#25968;&#25454;&#65292;&#36825;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#37117;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#20316;&#21697;&#22823;&#22810;&#20351;&#29992;&#19981;&#21516;&#30340;&#21516;&#26500;&#35774;&#22791;&#36827;&#34892;&#35780;&#20272;&#65292;&#36825;&#19982;&#30495;&#23454;&#22330;&#26223;&#20013;&#24322;&#26500;&#35774;&#22791;&#30340;&#22810;&#26679;&#24615;&#21644;&#21464;&#24322;&#24615;&#19981;&#31526;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#36164;&#28304;&#26377;&#38480;&#21644;&#36719;&#20214;&#22534;&#26632;&#22797;&#26434;&#65292;&#20351;&#29992;&#24322;&#26500;&#35774;&#22791;&#36827;&#34892;&#30740;&#31350;&#21644;&#24320;&#21457;&#20063;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#36825;&#20123;&#22240;&#32032;&#26159;&#32852;&#37030;&#23398;&#20064;&#30740;&#31350;&#20013;&#37325;&#35201;&#20294;&#40092;&#26377;&#25506;&#32034;&#30340;&#65292;&#23427;&#20204;&#30452;&#25509;&#24433;&#21709;&#32852;&#37030;&#19982;&#26412;&#22320;&#27169;&#22411;&#20043;&#38388;&#30340;&#35757;&#32451;&#21160;&#24577;&#21644;&#26368;&#32456;&#24615;&#33021;&#65292;&#20351;&#24471;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#29992;&#24615;&#19981;&#26126;&#30830;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#21407;&#22411;&#31995;&#32479;&#65292;&#20197;&#25903;&#25345;&#24322;&#26500;&#35774;&#22791;&#19978;&#30340;&#36328;&#35774;&#22791;&#32852;&#37030;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) aims to train high-quality models in collaboration with distributed clients while not uploading their local data, which attracts increasing attention in both academia and industry. However, there is still a considerable gap between the flourishing FL research and real-world scenarios, mainly caused by the characteristics of heterogeneous devices and its scales. Most existing works conduct evaluations with homogeneous devices, which are mismatched with the diversity and variability of heterogeneous devices in real-world scenarios. Moreover, it is challenging to conduct research and development at scale with heterogeneous devices due to limited resources and complex software stacks. These two key factors are important yet underexplored in FL research as they directly impact the FL training dynamics and final performance, making the effectiveness and usability of FL algorithms unclear. To bridge the gap, in this paper, we propose an efficient and scalable prototypi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#26088;&#22312;&#23545;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#21512;&#20316;&#24615;&#35780;&#20272;&#30340;&#21487;&#25193;&#23637;&#24615;&#36827;&#34892;&#30740;&#31350;&#65292;&#36890;&#36807;&#29983;&#25104;&#29305;&#23450;&#21338;&#24328;&#35770;&#32467;&#26500;&#22330;&#26223;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#19981;&#36807;&#30446;&#21069;&#29983;&#25104;&#36136;&#37327;&#36739;&#19968;&#33324;&#12290;</title><link>http://arxiv.org/abs/2303.13360</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#21512;&#20316;&#24615;&#35780;&#20272;&#30340;&#21487;&#25193;&#23637;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards the Scalable Evaluation of Cooperativeness in Language Models. (arXiv:2303.13360v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26088;&#22312;&#23545;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#21512;&#20316;&#24615;&#35780;&#20272;&#30340;&#21487;&#25193;&#23637;&#24615;&#36827;&#34892;&#30740;&#31350;&#65292;&#36890;&#36807;&#29983;&#25104;&#29305;&#23450;&#21338;&#24328;&#35770;&#32467;&#26500;&#22330;&#26223;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#19981;&#36807;&#30446;&#21069;&#29983;&#25104;&#36136;&#37327;&#36739;&#19968;&#33324;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#39537;&#21160;&#30340;AI&#31995;&#32479;&#21487;&#33021;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#36741;&#21161;&#20154;&#31867;&#36827;&#34892;&#28041;&#21450;&#20854;&#20182;&#20195;&#29702;&#20154;&#30340;&#39640; stakes &#20132;&#20114;&#65292;&#20363;&#22914;&#21327;&#21830;&#25110;&#20914;&#31361;&#35299;&#20915;&#12290;&#31526;&#21512;&#21512;&#20316;&#30340;AI&#30340;&#30446;&#26631;&#65292;&#25105;&#20204;&#24076;&#26395;&#20197;&#20146;&#31038;&#20250;&#30340;&#26041;&#24335;&#29702;&#35299;&#21644;&#22609;&#36896;PLM&#30340;&#22810;&#20195;&#29702;&#34892;&#20026;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#31532;&#19968;&#27493;&#26159;&#23545;&#27169;&#22411;&#22312;&#21508;&#31181;&#21512;&#20316;&#38382;&#39064;&#19978;&#34892;&#20026;&#30340;&#35780;&#20272;&#12290;&#30001;&#20110;&#20132;&#20114;&#20013;&#26399;&#26395;&#30340;&#34892;&#20026;&#21462;&#20915;&#20110;&#31934;&#30830;&#30340;&#21338;&#24328;&#32467;&#26500;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20351;&#29992;&#20247;&#21253;&#24037;&#20154;&#21644;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#29305;&#23450;&#32467;&#26500;&#30340;&#22330;&#26223;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22914;&#19979;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#29983;&#25104;&#29305;&#23450;&#21338;&#24328;&#35770;&#32467;&#26500;&#22330;&#26223;&#30340;&#20851;&#38190;&#26041;&#27861;&#38382;&#39064;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#20247;&#21253;&#24037;&#20154;&#21644;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#36825;&#20123;&#22330;&#26223;&#12290;&#25105;&#20204;&#21457;&#29616;&#20004;&#31181;&#24773;&#20917;&#19979;&#30340;&#29983;&#25104;&#36136;&#37327;&#24448;&#24448;&#26159;&#20013;&#31561;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#20197;&#19979;&#32467;&#35770;&#65306;
&lt;/p&gt;
&lt;p&gt;
It is likely that AI systems driven by pre-trained language models (PLMs) will increasingly be used to assist humans in high-stakes interactions with other agents, such as negotiation or conflict resolution. Consistent with the goals of Cooperative AI \citep{dafoe_open_2020}, we wish to understand and shape the multi-agent behaviors of PLMs in a pro-social manner. An important first step is the evaluation of model behaviour across diverse cooperation problems. Since desired behaviour in an interaction depends upon precise game-theoretic structure, we focus on generating scenarios with particular structures with both crowdworkers and a language model. Our work proceeds as follows. First, we discuss key methodological issues in the generation of scenarios corresponding to particular game-theoretic structures. Second, we employ both crowdworkers and a language model to generate such scenarios. We find that the quality of generations tends to be mediocre in both cases. We additionally get 
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#28369;&#21160;&#31383;&#21475;&#25216;&#26415;&#22686;&#21152;&#20102;&#25991;&#26412;&#19978;&#19979;&#25991;&#30340;&#32534;&#30721;&#65292;&#25552;&#39640;&#20102;&#21307;&#30103;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#30340;&#20934;&#30830;&#24615;&#65292;&#26032;&#27169;&#22411;ClipMD&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#37117;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.13340</link><description>&lt;p&gt;
&#22686;&#21152;&#25991;&#26412;&#19978;&#19979;&#25991;&#22823;&#23567;&#25552;&#39640;&#21307;&#30103;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Increasing Textual Context Size Boosts Medical Image-Text Matching. (arXiv:2303.13340v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13340
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#28369;&#21160;&#31383;&#21475;&#25216;&#26415;&#22686;&#21152;&#20102;&#25991;&#26412;&#19978;&#19979;&#25991;&#30340;&#32534;&#30721;&#65292;&#25552;&#39640;&#20102;&#21307;&#30103;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#30340;&#20934;&#30830;&#24615;&#65292;&#26032;&#27169;&#22411;ClipMD&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#37117;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#30701;&#25216;&#26415;&#25253;&#21578;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#21307;&#30103;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#20219;&#21153;&#20013;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20351;&#29992;OpenAI&#30340;CLIP&#36827;&#34892;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#30340;&#27169;&#22411;&#65292;&#24182;&#35266;&#23519;&#21040;CLIP&#22312;&#21307;&#30103;&#39046;&#22495;&#20013;&#38656;&#35201;&#32534;&#30721;&#26356;&#38271;&#25991;&#26412;&#19978;&#19979;&#25991;&#30340;&#22320;&#26041;&#65292;&#20854;&#26377;&#38480;&#30340;&#25991;&#26412;&#36755;&#20837;&#22823;&#23567;&#20250;&#23545;&#19979;&#28216;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#31616;&#21333;&#30340;&#28369;&#21160;&#31383;&#21475;&#25216;&#26415;&#35757;&#32451;&#21644;&#21457;&#24067;&#20102;ClipMD&#65292;&#29992;&#20110;&#32534;&#30721;&#25991;&#26412;&#26631;&#39064;&#12290;ClipMD&#22312;&#20004;&#20010;&#21307;&#30103;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#19982;&#20854;&#20182;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ClipMD&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#37117;&#27604;&#20854;&#20182;&#27169;&#22411;&#35201;&#22909;&#24471;&#22810;&#12290;&#25105;&#20204;&#20844;&#24320;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This short technical report demonstrates a simple technique that yields state of the art results in medical image-text matching tasks. We analyze the use of OpenAI's CLIP, a general image-text matching model, and observe that CLIP's limited textual input size has negative impact on downstream performance in the medical domain where encoding longer textual contexts is often required. We thus train and release ClipMD, which is trained with a simple sliding window technique to encode textual captions. ClipMD was tested on two medical image-text datasets and compared with other image-text matching models. The results show that ClipMD outperforms other models on both datasets by a large margin. We make our code and pretrained model publicly available.
&lt;/p&gt;</description></item><item><title>&#27492;&#25991;&#20171;&#32461;&#20102;&#38899;&#39057;&#25193;&#25955;&#27169;&#22411;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#20004;&#20010;&#27963;&#36291;&#20219;&#21153;&#65306;&#25991;&#26412;&#21040;&#35821;&#38899;&#21644;&#35821;&#38899;&#22686;&#24378;&#65292;&#24182;&#23545;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2303.13336</link><description>&lt;p&gt;
&#35821;&#38899;&#21512;&#25104;&#30340;&#38899;&#39057;&#25193;&#25955;&#27169;&#22411;&#65306;&#22522;&#20110;&#29983;&#25104;AI&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#21644;&#35821;&#38899;&#22686;&#24378;&#30340;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
Audio Diffusion Model for Speech Synthesis: A Survey on Text To Speech and Speech Enhancement in Generative AI. (arXiv:2303.13336v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13336
&lt;/p&gt;
&lt;p&gt;
&#27492;&#25991;&#20171;&#32461;&#20102;&#38899;&#39057;&#25193;&#25955;&#27169;&#22411;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#20004;&#20010;&#27963;&#36291;&#20219;&#21153;&#65306;&#25991;&#26412;&#21040;&#35821;&#38899;&#21644;&#35821;&#38899;&#22686;&#24378;&#65292;&#24182;&#23545;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;AI&#22312;&#21508;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#35821;&#38899;&#21512;&#25104;&#26159;&#19968;&#20010;&#26377;&#36259;&#30340;&#26041;&#21521;&#12290;&#38543;&#30528;&#25193;&#25955;&#27169;&#22411;&#25104;&#20026;&#26368;&#27969;&#34892;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#35768;&#22810;&#24037;&#20316;&#24050;&#32463;&#23581;&#35797;&#20102;&#20004;&#20010;&#27963;&#36291;&#20219;&#21153;&#65306;&#25991;&#26412;&#21040;&#35821;&#38899;&#21644;&#35821;&#38899;&#22686;&#24378;&#12290;&#26412;&#25991;&#23545;&#38899;&#39057;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#27010;&#36848;&#65292;&#36825;&#26159;&#23545;&#29616;&#26377;&#35843;&#26597;&#30340;&#34917;&#20805;&#65292;&#36825;&#20123;&#35843;&#26597;&#35201;&#20040;&#32570;&#20047;&#22522;&#20110;&#25193;&#25955;&#30340;&#35821;&#38899;&#21512;&#25104;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#35201;&#20040;&#24378;&#35843;&#22312;&#22810;&#20010;&#39046;&#22495;&#24212;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#25972;&#20307;&#24773;&#20917;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#39318;&#20808;&#31616;&#35201;&#20171;&#32461;&#20102;&#38899;&#39057;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#32972;&#26223;&#12290;&#23545;&#20110;&#25991;&#26412;&#21040;&#35821;&#38899;&#20219;&#21153;&#65292;&#25105;&#20204;&#23558;&#26041;&#27861;&#20998;&#20026;&#19977;&#31867;&#65292;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#37319;&#29992;&#30340;&#38454;&#27573;&#65306;&#22768;&#23398;&#27169;&#22411;&#12289;&#22768;&#30721;&#22120;&#21644;&#31471;&#21040;&#31471;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#26576;&#20123;&#20449;&#21495;&#20174;&#36755;&#20837;&#35821;&#38899;&#20013;&#21024;&#38500;&#25110;&#28155;&#21152;&#26469;&#23558;&#21508;&#31181;&#35821;&#38899;&#22686;&#24378;&#20219;&#21153;&#36827;&#34892;&#20998;&#31867;&#12290;&#26412;&#25991;&#36824;&#28085;&#30422;&#20102;&#23454;&#39564;&#32467;&#26524;&#30340;&#27604;&#36739;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI has demonstrated impressive performance in various fields, among which speech synthesis is an interesting direction. With the diffusion model as the most popular generative model, numerous works have attempted two active tasks: text to speech and speech enhancement. This work conducts a survey on audio diffusion model, which is complementary to existing surveys that either lack the recent progress of diffusion-based speech synthesis or highlight an overall picture of applying diffusion model in multiple fields. Specifically, this work first briefly introduces the background of audio and diffusion model. As for the text-to-speech task, we divide the methods into three categories based on the stage where diffusion model is adopted: acoustic model, vocoder and end-to-end framework. Moreover, we categorize various speech enhancement tasks by either certain signals are removed or added into the input speech. Comparisons of experimental results and discussions are also covered 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22270;&#19978;&#30340;&#21435;&#20013;&#24515;&#21270;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#21033;&#29992;&#25193;&#25955;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#35757;&#32451;&#26694;&#26550;&#65292;&#22686;&#24378;&#20102;&#22810;&#20010;&#20195;&#29702;&#30340;&#40065;&#26834;&#24615;&#20197;&#23545;&#25239;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2303.13326</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#21435;&#20013;&#24515;&#21270;&#23545;&#25239;&#24615;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Decentralized Adversarial Training over Graphs. (arXiv:2303.13326v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22270;&#19978;&#30340;&#21435;&#20013;&#24515;&#21270;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#21033;&#29992;&#25193;&#25955;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#35757;&#32451;&#26694;&#26550;&#65292;&#22686;&#24378;&#20102;&#22810;&#20010;&#20195;&#29702;&#30340;&#40065;&#26834;&#24615;&#20197;&#23545;&#25239;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#28431;&#27934;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#29420;&#31435;&#21333;&#19968;&#20195;&#29702;&#23398;&#20064;&#32773;&#30340;&#34892;&#20026;&#19978;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22270;&#19978;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#20854;&#20013;&#21508;&#20010;&#21333;&#29420;&#30340;&#20195;&#29702;&#20250;&#21463;&#21040;&#31354;&#38388;&#20013;&#19981;&#21516;&#24378;&#24230;&#30340;&#25200;&#21160;&#12290;&#39044;&#26399;&#36890;&#36807;&#38142;&#25509;&#20195;&#29702;&#21644;&#21487;&#33021;&#22312;&#22270;&#19978;&#23454;&#29616;&#30340;&#25915;&#20987;&#27169;&#22411;&#30340;&#24322;&#36136;&#24615;&#65292;&#21327;&#35843;&#25972;&#20010;&#22242;&#38431;&#30340;&#24378;&#22823;&#21327;&#21516;&#20316;&#29992;&#21487;&#20197;&#24110;&#21161;&#22686;&#24378;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#20351;&#29992;&#25193;&#25955;&#23398;&#20064;&#30340;&#26497;&#23567;-&#26497;&#22823;&#20844;&#24335;&#65292;&#20026;&#22810;&#20195;&#29702;&#31995;&#32479;&#24320;&#21457;&#20102;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26694;&#26550;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#35813;&#26041;&#26696;&#22312;&#20984;&#21644;&#38750;&#20984;&#29615;&#22659;&#19979;&#30340;&#25910;&#25947;&#29305;&#24615;&#65292;&#24182;&#35828;&#26126;&#20102;&#22686;&#24378;&#30340;&#40065;&#26834;&#24615;&#23545;&#25239;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
The vulnerability of machine learning models to adversarial attacks has been attracting considerable attention in recent years. Most existing studies focus on the behavior of stand-alone single-agent learners. In comparison, this work studies adversarial training over graphs, where individual agents are subjected to perturbations of varied strength levels across space. It is expected that interactions by linked agents, and the heterogeneity of the attack models that are possible over the graph, can help enhance robustness in view of the coordination power of the group. Using a min-max formulation of diffusion learning, we develop a decentralized adversarial training framework for multi-agent systems. We analyze the convergence properties of the proposed scheme for both convex and non-convex environments, and illustrate the enhanced robustness to adversarial attacks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340; AI &#26234;&#33021;&#20307;&#20316;&#20026;&#35745;&#31639;&#22522;&#20934;&#65292;&#20197;&#35780;&#20272;&#20154;&#31867;&#22312;&#22256;&#38590;&#30340;&#28041;&#21450;&#22810;&#20010;&#20154;&#31867;&#21644;&#24773;&#22659;&#22240;&#32032;&#30340;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#21450;&#21457;&#29616;&#26377;&#24453;&#25913;&#36827;&#30340;&#39046;&#22495;&#12290;&#35813;&#35770;&#25991;&#20197;&#36275;&#29699;&#34920;&#29616;&#20998;&#26512;&#20026;&#20363;&#65292;&#20351;&#29992;&#22823;&#22411;&#29699;&#21592;&#21644;&#29699;&#20301;&#32622;&#36319;&#36394;&#25968;&#25454;&#38598;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#25104;&#21151;&#36827;&#34892;&#36275;&#29699;&#27604;&#36187;&#20013;&#30340;&#20132;&#20114;&#27169;&#20223;&#12290;</title><link>http://arxiv.org/abs/2303.13323</link><description>&lt;p&gt;
&#22797;&#26434;&#20114;&#21160;&#20219;&#21153;&#20013;&#30340;&#20154;&#31867;&#34920;&#29616;&#35780;&#20272;&#65306;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#29983;&#25104;&#22810;&#26234;&#33021;&#20307;&#27169;&#22411;&#30340;&#35745;&#31639;&#22522;&#20934;&#26041;&#27861;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#36275;&#29699;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Deep Generative Multi-Agent Imitation Model as a Computational Benchmark for Evaluating Human Performance in Complex Interactive Tasks: A Case Study in Football. (arXiv:2303.13323v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13323
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340; AI &#26234;&#33021;&#20307;&#20316;&#20026;&#35745;&#31639;&#22522;&#20934;&#65292;&#20197;&#35780;&#20272;&#20154;&#31867;&#22312;&#22256;&#38590;&#30340;&#28041;&#21450;&#22810;&#20010;&#20154;&#31867;&#21644;&#24773;&#22659;&#22240;&#32032;&#30340;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#21450;&#21457;&#29616;&#26377;&#24453;&#25913;&#36827;&#30340;&#39046;&#22495;&#12290;&#35813;&#35770;&#25991;&#20197;&#36275;&#29699;&#34920;&#29616;&#20998;&#26512;&#20026;&#20363;&#65292;&#20351;&#29992;&#22823;&#22411;&#29699;&#21592;&#21644;&#29699;&#20301;&#32622;&#36319;&#36394;&#25968;&#25454;&#38598;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#25104;&#21151;&#36827;&#34892;&#36275;&#29699;&#27604;&#36187;&#20013;&#30340;&#20132;&#20114;&#27169;&#20223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#22914;&#24037;&#31243;&#21644;&#20307;&#32946;&#20013;&#65292;&#35780;&#20272;&#20154;&#31867;&#30340;&#34920;&#29616;&#26159;&#24120;&#35265;&#30340;&#38656;&#27714;&#12290;&#35780;&#20272;&#20154;&#31867;&#22312;&#23436;&#25104;&#22797;&#26434;&#30340;&#20114;&#21160;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#26368;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#24050;&#34987;&#35777;&#26126;&#22312;&#35813;&#24773;&#22659;&#19979;&#26377;&#25928;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#25110;&#20351;&#29992;&#20027;&#35266;&#27979;&#37327;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#36825;&#21487;&#33021;&#26159;&#19968;&#20010;&#23481;&#26131;&#20986;&#38169;&#21644;&#19981;&#21487;&#38752;&#30340;&#36807;&#31243;&#65292;&#22240;&#20026;&#38745;&#24577;&#24230;&#37327;&#26631;&#20934;&#26080;&#27861;&#25429;&#25417;&#21040;&#19982;&#36825;&#20123;&#20219;&#21153;&#30456;&#20851;&#30340;&#25152;&#26377;&#22797;&#26434;&#24773;&#22659;&#65292;&#24182;&#19988;&#20027;&#35266;&#27979;&#37327;&#23384;&#22312;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#21019;&#24314;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340; AI &#26234;&#33021;&#20307;&#65292;&#20316;&#20026;&#35745;&#31639;&#22522;&#20934;&#26469;&#35780;&#20272;&#20154;&#31867;&#22312;&#35299;&#20915;&#28041;&#21450;&#22810;&#20010;&#20154;&#31867;&#21644;&#24773;&#22659;&#22240;&#32032;&#30340;&#22256;&#38590;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#22312;&#36275;&#29699;&#34920;&#29616;&#20998;&#26512;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#20102;&#28436;&#31034;&#12290;&#25105;&#20204;&#22312;&#22823;&#22411;&#29699;&#21592;&#21644;&#29699;&#20301;&#32622;&#36319;&#36394;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#22522;&#20110; Conditional Variational Recurrent Neural Network&#65288;VRNN&#65289;&#27169;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#35757;&#32451;&#21518;&#30340;&#27169;&#22411;&#29992;&#20110;&#27169;&#20223;&#20004;&#20010;&#22242;&#38431;&#22312;&#36275;&#29699;&#27604;&#36187;&#20013;&#30340;&#20132;&#20114;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#27169;&#25311;&#27604;&#36187;&#20013;&#36924;&#30495;&#30340;&#29699;&#21592;&#31227;&#21160;&#21644;&#22242;&#38431;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#36825;&#20010;&#27169;&#22411;&#21487;&#20197;&#29992;&#20316;&#35780;&#20272;&#20154;&#31867;&#36275;&#29699;&#34920;&#29616;&#30340;&#22522;&#20934;&#65292;&#24182;&#30830;&#23450;&#38656;&#35201;&#25552;&#39640;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating the performance of human is a common need across many applications, such as in engineering and sports. When evaluating human performance in completing complex and interactive tasks, the most common way is to use a metric having been proved efficient for that context, or to use subjective measurement techniques. However, this can be an error prone and unreliable process since static metrics cannot capture all the complex contexts associated with such tasks and biases exist in subjective measurement. The objective of our research is to create data-driven AI agents as computational benchmarks to evaluate human performance in solving difficult tasks involving multiple humans and contextual factors. We demonstrate this within the context of football performance analysis. We train a generative model based on Conditional Variational Recurrent Neural Network (VRNN) Model on a large player and ball tracking dataset. The trained model is used to imitate the interactions between two te
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#38024;&#23545;&#21518;&#32493;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#25152;&#23384;&#22312;&#30340;&#19981;&#21516;&#35299;&#37322;&#30340;&#38382;&#39064;&#65292;&#24341;&#20837;PEAR&#25439;&#22833;&#39033;&#65292;&#20174;&#32780;&#25552;&#21319;&#27169;&#22411;&#30340;&#35299;&#37322;&#19968;&#33268;&#24615;&#65292;&#36798;&#21040;&#27169;&#22411;&#34892;&#20026;&#30340;&#21487;&#29702;&#35299;&#21644;&#21487;&#20449;&#20219;&#12290;</title><link>http://arxiv.org/abs/2303.13299</link><description>&lt;p&gt;
&#35770;&#22914;&#20309;&#35299;&#20915;&#19981;&#21516;&#35299;&#37322;&#26041;&#27861;&#25152;&#24102;&#26469;&#30340;&#38382;&#39064;&#65306;&#36890;&#36807;&#35757;&#32451;&#30446;&#26631;&#36798;&#25104;&#35299;&#37322;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Reckoning with the Disagreement Problem: Explanation Consensus as a Training Objective. (arXiv:2303.13299v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#38024;&#23545;&#21518;&#32493;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#25152;&#23384;&#22312;&#30340;&#19981;&#21516;&#35299;&#37322;&#30340;&#38382;&#39064;&#65292;&#24341;&#20837;PEAR&#25439;&#22833;&#39033;&#65292;&#20174;&#32780;&#25552;&#21319;&#27169;&#22411;&#30340;&#35299;&#37322;&#19968;&#33268;&#24615;&#65292;&#36798;&#21040;&#27169;&#22411;&#34892;&#20026;&#30340;&#21487;&#29702;&#35299;&#21644;&#21487;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36880;&#28176;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#20013;&#20570;&#20986;&#20851;&#38190;&#20915;&#31574;&#65292;&#30417;&#25511;&#21644;&#35299;&#37322;&#20854;&#34892;&#20026;&#25104;&#20026;&#24517;&#38656;&#12290;&#21518;&#32493;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#21487;&#20026;&#36755;&#20837;&#20013;&#30340;&#27599;&#20010;&#29305;&#24449;&#20998;&#37197;&#24471;&#20998;&#65292;&#20197;&#34913;&#37327;&#20854;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#24433;&#21709;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#23616;&#38480;&#26159;&#19981;&#21516;&#30340;&#35299;&#37322;&#26041;&#27861;&#23545;&#20110;&#21738;&#20123;&#29305;&#24449;&#26356;&#37325;&#35201;&#21487;&#33021;&#26377;&#19981;&#21516;&#30340;&#30475;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#19981;&#21516;&#35299;&#37322;&#26041;&#27861;&#30340;&#35757;&#32451;&#27169;&#22411;&#26041;&#27861;&#65292;&#25105;&#20204;&#24341;&#20837;Post hoc Explainer Agreement Regularization (PEAR)&#25439;&#22833;&#39033;&#20197;&#25552;&#21319;&#35299;&#37322;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#35266;&#23519;&#21040;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#27492;&#25439;&#22833;&#39033;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#22312;&#26410;&#30475;&#35265;&#30340;&#25968;&#25454;&#19978;&#33719;&#24471;&#35299;&#37322;&#19968;&#33268;&#24615;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
As neural networks increasingly make critical decisions in high-stakes settings, monitoring and explaining their behavior in an understandable and trustworthy manner is a necessity. One commonly used type of explainer is post hoc feature attribution, a family of methods for giving each feature in an input a score corresponding to its influence on a model's output. A major limitation of this family of explainers in practice is that they can disagree on which features are more important than others. Our contribution in this paper is a method of training models with this disagreement problem in mind. We do this by introducing a Post hoc Explainer Agreement Regularization (PEAR) loss term alongside the standard term corresponding to accuracy, an additional term that measures the difference in feature attribution between a pair of explainers. We observe on three datasets that we can train a model with this loss term to improve explanation consensus on unseen data, and see improved consensus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22495;&#27867;&#21270;&#35270;&#35282;&#65292;&#23558;&#20854;&#37325;&#26032;&#35299;&#37322;&#20026;&#22495;&#20043;&#38388;&#30340;&#20984;&#21338;&#24328;&#65292;&#24182;&#36890;&#36807;&#40723;&#21169;&#27599;&#20010;&#22810;&#26679;&#21270;&#30340;&#22495;&#22686;&#24378;&#27169;&#22411;&#27867;&#21270;&#21644;&#26500;&#24314;&#26679;&#26412;&#36807;&#28388;&#22120;&#26469;&#25552;&#39640;&#22495;&#22686;&#24378;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.13297</link><description>&lt;p&gt;
&#22522;&#20110;&#22495;&#20984;&#21338;&#24328;&#30340;&#36890;&#29992;&#24615;&#25913;&#36827;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Improving Generalization with Domain Convex Game. (arXiv:2303.13297v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22495;&#27867;&#21270;&#35270;&#35282;&#65292;&#23558;&#20854;&#37325;&#26032;&#35299;&#37322;&#20026;&#22495;&#20043;&#38388;&#30340;&#20984;&#21338;&#24328;&#65292;&#24182;&#36890;&#36807;&#40723;&#21169;&#27599;&#20010;&#22810;&#26679;&#21270;&#30340;&#22495;&#22686;&#24378;&#27169;&#22411;&#27867;&#21270;&#21644;&#26500;&#24314;&#26679;&#26412;&#36807;&#28388;&#22120;&#26469;&#25552;&#39640;&#22495;&#22686;&#24378;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22495;&#27867;&#21270;&#65288;DG&#65289;&#36890;&#36807;&#23398;&#20064;&#20855;&#26377;&#22810;&#20010;&#28304;&#22495;&#30340;&#27169;&#22411;&#26469;&#32531;&#35299;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24046;&#27867;&#21270;&#33021;&#21147;&#12290; DG&#30340;&#32463;&#20856;&#35299;&#20915;&#26041;&#26696;&#26159;&#22495;&#22686;&#24378;&#65292;&#23427;&#30340;&#26222;&#36941;&#20449;&#20208;&#26159;&#36890;&#36807;&#22810;&#26679;&#21270;&#28304;&#22495;&#26377;&#21161;&#20110;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#27867;&#21270;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20027;&#24352;&#20165;&#22522;&#20110;&#30452;&#35266;&#29702;&#35299;&#65292;&#32570;&#20047;&#25968;&#23398;&#35777;&#26126;&#12290;&#25105;&#20204;&#30340;&#25506;&#32034;&#23454;&#39564;&#34920;&#26126;&#27169;&#22411;&#27867;&#21270;&#21644;&#22495;&#22810;&#26679;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21487;&#33021;&#19981;&#26159;&#20005;&#26684;&#27491;&#30456;&#20851;&#30340;&#65292;&#36825;&#38480;&#21046;&#20102;&#22495;&#22686;&#24378;&#30340;&#26377;&#25928;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#20445;&#35777;&#21644;&#36827;&#19968;&#27493;&#22686;&#24378;&#35813;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;DG&#35270;&#35282;&#65292;&#23558;&#20854;&#37325;&#26032;&#35299;&#37322;&#20026;&#22495;&#20043;&#38388;&#30340;&#20984;&#21338;&#24328;&#12290;&#36890;&#36807;&#22522;&#20110;&#36229;&#27169;&#24615;&#30340;&#24039;&#22937;&#35774;&#35745;&#27491;&#21017;&#21270;&#39033;&#26469;&#40723;&#21169;&#27599;&#20010;&#22810;&#26679;&#21270;&#30340;&#22495;&#22686;&#24378;&#27169;&#22411;&#27867;&#21270;&#65292;&#24182;&#26500;&#24314;&#26679;&#26412;&#36807;&#28388;&#22120;&#26469;&#28040;&#38500;&#20302;&#36136;&#37327;&#26679;&#26412;&#65292;&#21516;&#26102;&#26377;&#25928;&#22320;&#25552;&#39640;&#22495;&#22686;&#24378;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization (DG) tends to alleviate the poor generalization capability of deep neural networks by learning model with multiple source domains. A classical solution to DG is domain augmentation, the common belief of which is that diversifying source domains will be conducive to the out-of-distribution generalization. However, these claims are understood intuitively, rather than mathematically. Our explorations empirically reveal that the correlation between model generalization and the diversity of domains may be not strictly positive, which limits the effectiveness of domain augmentation. This work therefore aim to guarantee and further enhance the validity of this strand. To this end, we propose a new perspective on DG that recasts it as a convex game between domains. We first encourage each diversified domain to enhance model generalization by elaborately designing a regularization term based on supermodularity. Meanwhile, a sample filter is constructed to eliminate low-qua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#20016;&#23500;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#20943;&#23569;&#26368;&#22351;&#24773;&#20917;&#30340;&#36829;&#35268;&#65292;&#24182;&#25552;&#39640;&#20854;&#24615;&#33021;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2303.13228</link><description>&lt;p&gt;
&#20016;&#23500;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25968;&#25454;&#38598;&#20197;&#25552;&#39640;&#26368;&#22351;&#24773;&#20917;&#24615;&#33021;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Enriching Neural Network Training Dataset to Improve Worst-Case Performance Guarantees. (arXiv:2303.13228v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#20016;&#23500;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#20943;&#23569;&#26368;&#22351;&#24773;&#20917;&#30340;&#36829;&#35268;&#65292;&#24182;&#25552;&#39640;&#20854;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#65292;&#26159;&#29992;&#20110;&#36817;&#20284;&#38750;&#32447;&#24615;&#20851;&#31995;&#65288;&#20363;&#22914;AC-OPF&#65289;&#30340;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#24182;&#22312;&#37096;&#32626;&#26102;&#23454;&#29616;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#21152;&#36895;&#12290;&#36890;&#24120;&#22312;&#30005;&#21147;&#31995;&#32479;&#25991;&#29486;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#26159;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20043;&#21069;&#29983;&#25104;&#30340;&#22266;&#23450;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#26412;&#25991;&#35777;&#26126;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#35843;&#25972;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25968;&#25454;&#38598;&#21487;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#24182;&#22823;&#24133;&#20943;&#23569;&#20854;&#26368;&#22351;&#24773;&#20917;&#36829;&#35268;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#20016;&#23500;&#20851;&#38190;&#25968;&#25454;&#28857;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20197;&#20943;&#23569;&#26368;&#22351;&#24773;&#20917;&#36829;&#35268;&#65292;&#25552;&#20379;&#20855;&#26377;&#25913;&#36827;&#26368;&#22351;&#24773;&#20917;&#24615;&#33021;&#20445;&#35777;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#27979;&#35797;&#30005;&#21147;&#31995;&#32479;&#20013;&#28436;&#31034;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#33539;&#22260;&#20174;39&#20010;&#24635;&#32447;&#21040;162&#20010;&#24635;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms, especially Neural Networks (NNs), are a valuable tool used to approximate non-linear relationships, like the AC-Optimal Power Flow (AC-OPF), with considerable accuracy -- and achieving a speedup of several orders of magnitude when deployed for use. Often in power systems literature, the NNs are trained with a fixed dataset generated prior to the training process. In this paper, we show that adapting the NN training dataset during training can improve the NN performance and substantially reduce its worst-case violations. This paper proposes an algorithm that identifies and enriches the training dataset with critical datapoints that reduce the worst-case violations and deliver a neural network with improved worst-case performance guarantees. We demonstrate the performance of our algorithm in four test power systems, ranging from 39-buses to 162-buses.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;DNN&#23545;&#28165;&#27905;&#26679;&#26412;&#21644;&#27602;&#21270;&#26679;&#26412;&#30340;&#39057;&#29575;&#25935;&#24863;&#24615;&#24046;&#24322;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;FREAK&#22522;&#20110;&#39057;&#29575;&#30340;&#27602;&#21270;&#26679;&#26412;&#26816;&#27979;&#31639;&#27861;&#65292;&#21487;&#26377;&#25928;&#38450;&#24481;&#39057;&#29575;&#21518;&#38376;&#25915;&#20987;&#21644;&#19968;&#20123;&#31354;&#38388;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2303.13211</link><description>&lt;p&gt;
&#19981;&#35201;&#24908;&#24352;&#65306;&#19968;&#31181;&#22522;&#20110;&#39057;&#29575;&#30340;&#26041;&#27861;&#26816;&#27979;DNN&#20013;&#30340;&#21518;&#38376;&#27602;&#21270;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Don't FREAK Out: A Frequency-Inspired Approach to Detecting Backdoor Poisoned Samples in DNNs. (arXiv:2303.13211v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;DNN&#23545;&#28165;&#27905;&#26679;&#26412;&#21644;&#27602;&#21270;&#26679;&#26412;&#30340;&#39057;&#29575;&#25935;&#24863;&#24615;&#24046;&#24322;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;FREAK&#22522;&#20110;&#39057;&#29575;&#30340;&#27602;&#21270;&#26679;&#26412;&#26816;&#27979;&#31639;&#27861;&#65292;&#21487;&#26377;&#25928;&#38450;&#24481;&#39057;&#29575;&#21518;&#38376;&#25915;&#20987;&#21644;&#19968;&#20123;&#31354;&#38388;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24403;DNN&#38754;&#23545;&#28165;&#27905;&#26679;&#26412;&#21644;&#27602;&#21270;&#26679;&#26412;&#26102;&#30340;&#39057;&#29575;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#36825;&#20004;&#31181;&#31867;&#22411;&#26679;&#26412;&#30340;&#39057;&#29575;&#25935;&#24863;&#24615;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FREAK&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#39057;&#29575;&#30340;&#27602;&#21270;&#26679;&#26412;&#26816;&#27979;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#19981;&#20165;&#35777;&#26126;&#20102;FREAK&#23545;&#39057;&#29575;&#21518;&#38376;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#65292;&#20063;&#23545;&#19968;&#20123;&#31354;&#38388;&#25915;&#20987;&#20855;&#26377;&#38450;&#24481;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21482;&#26159;&#21033;&#29992;&#36825;&#20123;&#27934;&#35265;&#30340;&#31532;&#19968;&#27493;&#12290;&#25105;&#20204;&#30456;&#20449;&#25105;&#20204;&#30340;&#20998;&#26512;&#21644;&#25552;&#20986;&#30340;&#38450;&#24481;&#26426;&#21046;&#23558;&#20026;&#26410;&#26469;&#30340;&#21518;&#38376;&#38450;&#24481;&#30740;&#31350;&#21644;&#24320;&#21457;&#25552;&#20379;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we investigate the frequency sensitivity of Deep Neural Networks (DNNs) when presented with clean samples versus poisoned samples. Our analysis shows significant disparities in frequency sensitivity between these two types of samples. Building on these findings, we propose FREAK, a frequency-based poisoned sample detection algorithm that is simple yet effective. Our experimental results demonstrate the efficacy of FREAK not only against frequency backdoor attacks but also against some spatial attacks. Our work is just the first step in leveraging these insights. We believe that our analysis and proposed defense mechanism will provide a foundation for future research and development of backdoor defenses.
&lt;/p&gt;</description></item><item><title>&#20154;&#20204;&#24120;&#24120;&#26080;&#27861;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Sparse Low-Dimensional Decision&#27169;&#22411;&#65292;&#23427;&#21482;&#20351;&#29992;&#19968;&#23567;&#37096;&#20998;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#36827;&#34892;&#20915;&#31574;&#65292;&#36825;&#20351;&#24471;&#35813;&#27169;&#22411;&#26356;&#23481;&#26131;&#34987;&#20154;&#29702;&#35299;&#65292;&#21516;&#26102;&#20063;&#20855;&#26377;&#19982;&#20854;&#20182;&#23494;&#38598;&#39640;&#32500;&#27169;&#22411;&#30456;&#20284;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.13166</link><description>&lt;p&gt;
&#20351;&#29992;&#23569;&#37327;&#29305;&#24449;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Take 5: Interpretable Image Classification with a Handful of Features. (arXiv:2303.13166v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13166
&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#24120;&#24120;&#26080;&#27861;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Sparse Low-Dimensional Decision&#27169;&#22411;&#65292;&#23427;&#21482;&#20351;&#29992;&#19968;&#23567;&#37096;&#20998;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#36827;&#34892;&#20915;&#31574;&#65292;&#36825;&#20351;&#24471;&#35813;&#27169;&#22411;&#26356;&#23481;&#26131;&#34987;&#20154;&#29702;&#35299;&#65292;&#21516;&#26102;&#20063;&#20855;&#26377;&#19982;&#20854;&#20182;&#23494;&#38598;&#39640;&#32500;&#27169;&#22411;&#30456;&#20284;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20351;&#29992;&#25968;&#21315;&#20010;&#22823;&#22810;&#19981;&#21487;&#29702;&#35299;&#30340;&#29305;&#24449;&#26469;&#35782;&#21035;&#21333;&#20010;&#31867;&#21035;&#65292;&#36825;&#26159;&#20219;&#20309;&#20154;&#37117;&#26080;&#27861;&#29702;&#35299;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#37319;&#29992;&#21487;&#35299;&#37322;&#30340;&#31232;&#30095;&#20302;&#32500;&#20915;&#31574;&#23618;&#65292;&#33021;&#22815;&#37327;&#21270;&#21487;&#35299;&#37322;&#24615;&#65292;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#32454;&#39063;&#31890;&#24230;&#22270;&#20687;&#20998;&#31867;&#20013;&#36827;&#34892;&#20102;&#28436;&#31034;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#21482;&#26377;&#24403;&#29305;&#24449;&#26159;&#21487;&#35299;&#37322;&#30340;&#65292;&#24182;&#19988;&#21482;&#26377;&#26497;&#23569;&#37327;&#30340;&#29305;&#24449;&#29992;&#20110;&#21333;&#20010;&#20915;&#31574;&#26102;&#65292;&#20154;&#25165;&#33021;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;&#20026;&#27492;&#65292;&#26368;&#21518;&#19968;&#23618;&#24517;&#39035;&#26159;&#31232;&#30095;&#19988;&#32500;&#25968;&#36739;&#20302;&#30340;&#12290;&#25105;&#20204;&#23558;&#20855;&#26377;&#31232;&#30095;&#20302;&#32500;&#20915;&#31574;&#30340;&#27169;&#22411;&#31216;&#20026;Sparse Low-Dimensional Decision&#65288;SLDD&#65289;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#30456;&#27604;&#23494;&#38598;&#39640;&#32500;&#20915;&#31574;&#23618;&#65292;SLDD&#27169;&#22411;&#22312;&#26412;&#22320;&#21644;&#20840;&#23616;&#19978;&#26356;&#23481;&#26131;&#35299;&#37322;&#65292;&#24182;&#33021;&#22815;&#20445;&#25345;&#31454;&#20105;&#24615;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#29305;&#24449;&#22810;&#26679;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks use thousands of mostly incomprehensible features to identify a single class, a decision no human can follow. We propose an interpretable sparse and low dimensional final decision layer in a deep neural network with measurable aspects of interpretability and demonstrate it on fine-grained image classification. We argue that a human can only understand the decision of a machine learning model, if the features are interpretable and only very few of them are used for a single decision. For that matter, the final layer has to be sparse and, to make interpreting the features feasible, low dimensional. We call a model with a Sparse Low-Dimensional Decision SLDD-Model. We show that a SLDD-Model is easier to interpret locally and globally than a dense high-dimensional decision layer while being able to maintain competitive accuracy. Additionally, we propose a loss function that improves a model's feature diversity and accuracy. Our more interpretable SLDD-Model only uses 5
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#32477;&#28909;&#37325;&#25918;&#30340;&#37325;&#25918;&#36830;&#32493;&#23398;&#20064;&#31574;&#30053;&#65292;&#23427;&#33021;&#22815;&#26377;&#36873;&#25321;&#24615;&#22320;&#37325;&#25918;&#19982;&#26032;&#25968;&#25454;&#30456;&#20284;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.13157</link><description>&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#30340;&#32477;&#28909;&#37325;&#25918;
&lt;/p&gt;
&lt;p&gt;
Adiabatic replay for continual learning. (arXiv:2303.13157v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#32477;&#28909;&#37325;&#25918;&#30340;&#37325;&#25918;&#36830;&#32493;&#23398;&#20064;&#31574;&#30053;&#65292;&#23427;&#33021;&#22815;&#26377;&#36873;&#25321;&#24615;&#22320;&#37325;&#25918;&#19982;&#26032;&#25968;&#25454;&#30456;&#20284;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22522;&#20110;&#37325;&#25918;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#22312;&#27599;&#20010;&#26032;&#25968;&#25454;&#30340;&#23398;&#20064;&#38454;&#27573;&#37325;&#25918;&#20195;&#34920;&#20808;&#21069;&#23398;&#20064;&#21040;&#30340;&#25152;&#26377;&#30693;&#35782;&#30340;&#26679;&#26412;&#65292;&#20197;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#30001;&#20110;&#22312;&#36830;&#32493;&#23398;&#20064;&#38382;&#39064;&#20013;&#23398;&#21040;&#30340;&#30693;&#35782;&#37327;&#38543;&#26102;&#38388;&#22686;&#38271;&#65292;&#29983;&#25104;&#24335;&#37325;&#25918;&#20250;&#33457;&#36153;&#36234;&#26469;&#36234;&#22810;&#30340;&#26102;&#38388;&#26469;&#37325;&#26032;&#23398;&#20064;&#24050;&#30693;&#20869;&#23481;&#12290;&#22312;&#36825;&#20010;&#27010;&#24565;&#39564;&#35777;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25105;&#20204;&#31216;&#20043;&#20026;&#32477;&#28909;&#37325;&#25918;&#65288;AR&#65289;&#30340;&#37325;&#25918;&#36830;&#32493;&#23398;&#20064;&#31574;&#30053;&#65292;&#20854;&#25928;&#29575;&#26469;&#33258;&#20110;&#65288;&#21512;&#29702;&#30340;&#65289;&#20551;&#35774;&#27599;&#20010;&#26032;&#30340;&#23398;&#20064;&#38454;&#27573;&#37117;&#26159;&#32477;&#28909;&#30340;&#65292;&#21363;&#20165;&#20195;&#34920;&#29616;&#26377;&#30693;&#35782;&#30340;&#23567;&#24133;&#22686;&#21152;&#12290;&#27599;&#20010;&#26032;&#30340;&#23398;&#20064;&#38454;&#27573;&#20250;&#35302;&#21457;&#19968;&#20010;&#36873;&#25321;&#24615;&#37325;&#25918;&#30340;&#37319;&#26679;&#36807;&#31243;&#65292;&#20174;&#29616;&#26377;&#30693;&#35782;&#24211;&#20013;&#36873;&#25321;&#30456;&#20284;&#20110;&#26032;&#25968;&#25454;&#30340;&#26679;&#26412;&#36827;&#34892;&#37325;&#25918;&#65292;&#32780;&#19981;&#26159;&#20840;&#37096;&#37325;&#25918;&#12290;&#23436;&#20840;&#37325;&#25918;&#19981;&#26159;&#24517;&#39035;&#30340;&#65292;&#22240;&#20026;AR&#36890;&#36807;GMMs&#34920;&#31034;&#25968;&#25454;&#20998;&#24067;&#65292;&#36825;&#20123;&#20998;&#24067;&#33021;&#22815;&#26377;&#36873;&#25321;&#24615;&#22320;&#26356;&#26032;&#23427;&#20204;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional replay-based approaches to continual learning (CL) require, for each learning phase with new data, the replay of samples representing all of the previously learned knowledge in order to avoid catastrophic forgetting. Since the amount of learned knowledge grows over time in CL problems, generative replay spends an increasing amount of time just re-learning what is already known. In this proof-of-concept study, we propose a replay-based CL strategy that we term adiabatic replay (AR), which derives its efficiency from the (reasonable) assumption that each new learning phase is adiabatic, i.e., represents only a small addition to existing knowledge. Each new learning phase triggers a sampling process that selectively replays, from the body of existing knowledge, just such samples that are similar to the new data, in contrast to replaying all of it. Complete replay is not required since AR represents the data distribution by GMMs, which are capable of selectively updating their
&lt;/p&gt;</description></item><item><title>FedGH&#26159;&#19968;&#31181;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#23458;&#25143;&#31471;&#25345;&#26377;&#20855;&#26377;&#19981;&#21516;&#32467;&#26500;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#20849;&#20139;&#30340;&#24191;&#20041;&#20840;&#23616;&#39044;&#27979;&#22836;&#26469;&#25552;&#39640;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.13137</link><description>&lt;p&gt;
FedGH:&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#19982;&#24191;&#20041;&#20840;&#23616;&#22836;
&lt;/p&gt;
&lt;p&gt;
FedGH: Heterogeneous Federated Learning with Generalized Global Header. (arXiv:2303.13137v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13137
&lt;/p&gt;
&lt;p&gt;
FedGH&#26159;&#19968;&#31181;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#23458;&#25143;&#31471;&#25345;&#26377;&#20855;&#26377;&#19981;&#21516;&#32467;&#26500;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#20849;&#20139;&#30340;&#24191;&#20041;&#20840;&#23616;&#39044;&#27979;&#22836;&#26469;&#25552;&#39640;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(Federated learning, FL)&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#20801;&#35768;&#22810;&#20010;&#21442;&#19982;&#26041;&#22312;&#38544;&#31169;&#20445;&#25252;&#30340;&#24773;&#20917;&#19979;&#21327;&#20316;&#35757;&#32451;&#20849;&#20139;&#27169;&#22411;&#12290;&#29616;&#26377;&#27178;&#21521;FL&#26041;&#27861;&#36890;&#24120;&#20551;&#23450;FL&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#25345;&#26377;&#30456;&#21516;&#30340;&#27169;&#22411;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31995;&#32479;&#24322;&#26500;&#21644;&#20010;&#24615;&#21270;&#38656;&#27714;&#65292;&#20351;&#24471;&#20801;&#35768;&#23458;&#25143;&#31471;&#25345;&#26377;&#20855;&#26377;&#19981;&#21516;&#32467;&#26500;&#30340;&#27169;&#22411;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#26041;&#21521;&#12290;&#29616;&#26377;&#30340;&#27169;&#22411;&#24322;&#26500;FL&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#20135;&#29983;&#39640;&#36890;&#20449;&#21644;/&#25110;&#35745;&#31639;&#25104;&#26412;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#20840;&#23616;&#39044;&#27979;&#22836;(FedGH)&#26041;&#27861;&#12290;&#23427;&#26159;&#19968;&#31181;&#36890;&#20449;&#21644;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#27169;&#22411;&#24322;&#26500;FL&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;FL&#26381;&#21153;&#22120;&#19978;&#23545;&#23458;&#25143;&#31471;&#27169;&#22411;&#25552;&#21462;&#30340;&#34920;&#31034;&#36827;&#34892;&#35757;&#32451;&#26469;&#35757;&#32451;&#20849;&#20139;&#30340;&#24191;&#20041;&#20840;&#23616;&#39044;&#27979;&#22836;&#12290;&#36890;&#36807;FedGH&#35757;&#32451;&#30340;&#24191;&#20041;&#20840;&#23616;&#39044;&#27979;&#22836;&#21487;&#20197;&#30452;&#25509;&#37096;&#32626;&#22312;&#23458;&#25143;&#31471;&#35774;&#22791;&#19978;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#26412;&#22320;&#25512;&#29702;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FedGH&#22312;&#20934;&#30830;&#24615;&#12289;&#36890;&#20449;&#25928;&#29575;&#21644;&#27169;&#22411;&#20010;&#24615;&#21270;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#27169;&#22411;&#24322;&#26500;FL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is an emerging machine learning paradigm that allows multiple parties to train a shared model collaboratively in a privacy-preserving manner. Existing horizontal FL methods generally assume that the FL server and clients hold the same model structure. However, due to system heterogeneity and the need for personalization, enabling clients to hold models with diverse structures has become an important direction. Existing model-heterogeneous FL approaches often require publicly available datasets and incur high communication and/or computational costs, which limit their performances. To address these limitations, we propose the Federated Global prediction Header (FedGH) approach. It is a communication and computation-efficient model-heterogeneous FL framework which trains a shared generalized global prediction header with representations extracted by heterogeneous extractors for clients' models at the FL server. The trained generalized global prediction header lear
&lt;/p&gt;</description></item><item><title>&#25152;&#25552;&#20986;&#30340;&#25289;&#26222;&#25289;&#26031;&#20998;&#21106;&#32593;&#32476;&#21487;&#21516;&#26102;&#25429;&#33719;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#35748;&#30693;&#21644;&#38543;&#26426;&#19981;&#30830;&#23450;&#24615;&#65292;&#25104;&#21151;&#23558;&#39640;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#20998;&#37197;&#21040;OOF&#30446;&#26631;&#20013;&#12290;</title><link>http://arxiv.org/abs/2303.13123</link><description>&lt;p&gt;
&#25289;&#26222;&#25289;&#26031;&#20998;&#21106;&#32593;&#32476;: &#20174;&#31354;&#38388;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#21040;&#25913;&#36827;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Laplacian Segmentation Networks: Improved Epistemic Uncertainty from Spatial Aleatoric Uncertainty. (arXiv:2303.13123v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13123
&lt;/p&gt;
&lt;p&gt;
&#25152;&#25552;&#20986;&#30340;&#25289;&#26222;&#25289;&#26031;&#20998;&#21106;&#32593;&#32476;&#21487;&#21516;&#26102;&#25429;&#33719;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#35748;&#30693;&#21644;&#38543;&#26426;&#19981;&#30830;&#23450;&#24615;&#65292;&#25104;&#21151;&#23558;&#39640;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#20998;&#37197;&#21040;OOF&#30446;&#26631;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23186;&#20307;&#22270;&#20687;&#24120;&#24120;&#20250;&#20986;&#29616;&#38750;&#27491;&#24120;&#30340;&#24773;&#20917;&#65292;&#20363;&#22914;&#22240;&#20026;&#20301;&#32622;&#25110;&#25195;&#25551;&#22120;&#30340;&#19981;&#21516;&#25110;&#22270;&#20687;&#25439;&#22351;&#31561;&#21407;&#22240;&#32780;&#32463;&#24120;&#20986;&#29616;&#65292;&#36825;&#31181;&#23186;&#20307;&#22270;&#20687;&#21487;&#33021;&#20250;&#23545;&#19979;&#28216;&#20020;&#24202;&#35786;&#26029;&#25110;&#27835;&#30103;&#20135;&#29983;&#24433;&#21709;&#12290;&#20026;&#20102;&#30830;&#20445;&#23545;&#36825;&#31181;&#38169;&#35823;&#20998;&#21106;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25289;&#26222;&#25289;&#26031;&#20998;&#21106;&#32593;&#32476;&#65288;LSN&#65289;&#65292;&#20854;&#33021;&#22815;&#20849;&#21516;&#24314;&#27169;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#35748;&#30693;&#65288;&#27169;&#22411;&#65289;&#19981;&#30830;&#23450;&#24615;&#21644;&#31354;&#38388;&#25968;&#25454;&#65288;&#38543;&#26426;&#65289;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#31354;&#38388;&#30456;&#20851;&#24615;&#30340;logit&#20998;&#24067;&#25429;&#33719;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#23545;&#20110;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#39640;&#32500;&#36755;&#20986;&#21644;&#20855;&#26377;&#36339;&#36807;&#36830;&#25509;&#30340;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#31532;&#19968;&#20010;&#25289;&#26222;&#25289;&#26031;&#26435;&#37325;&#21518;&#39564;&#30340;&#36924;&#36817;&#12290;&#20174;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24314;&#27169;&#31354;&#38388;&#20687;&#32032;&#30456;&#20851;&#24615;&#20351;&#24471;&#25289;&#26222;&#25289;&#26031;&#20998;&#21106;&#32593;&#32476;&#33021;&#22815;&#23558;&#39640;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#25104;&#21151;&#20998;&#37197;&#21040;&#22270;&#20687;&#20013;&#30340;OOF&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out of distribution (OOD) medical images are frequently encountered, e.g. because of site- or scanner differences, or image corruption. OOD images come with a risk of incorrect image segmentation, potentially negatively affecting downstream diagnoses or treatment. To ensure robustness to such incorrect segmentations, we propose Laplacian Segmentation Networks (LSN) that jointly model epistemic (model) and aleatoric (data) uncertainty in image segmentation. We capture data uncertainty with a spatially correlated logit distribution. For model uncertainty, we propose the first Laplace approximation of the weight posterior that scales to large neural networks with skip connections that have high-dimensional outputs. Empirically, we demonstrate that modelling spatial pixel correlation allows the Laplacian Segmentation Network to successfully assign high epistemic uncertainty to out-of-distribution objects appearing within images.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#28789;&#27963;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;RLOR&#65292;&#33021;&#22815;&#29992;&#20110;&#21508;&#31181;&#36816;&#31609;&#23398;&#38382;&#39064;&#12290;&#25105;&#20204;&#37325;&#26032;&#23454;&#29616;&#20102;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#20174;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#20013;&#21463;&#30410;&#65292;&#21516;&#26102;&#20063;&#25552;&#39640;&#20102;&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.13117</link><description>&lt;p&gt;
RLOR:&#19968;&#31181;&#28789;&#27963;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#36816;&#31609;&#23398;
&lt;/p&gt;
&lt;p&gt;
RLOR: A Flexible Framework of Deep Reinforcement Learning for Operation Research. (arXiv:2303.13117v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#28789;&#27963;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;RLOR&#65292;&#33021;&#22815;&#29992;&#20110;&#21508;&#31181;&#36816;&#31609;&#23398;&#38382;&#39064;&#12290;&#25105;&#20204;&#37325;&#26032;&#23454;&#29616;&#20102;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#20174;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#20013;&#21463;&#30410;&#65292;&#21516;&#26102;&#20063;&#25552;&#39640;&#20102;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#24212;&#29992;&#20110;&#36816;&#31609;&#23398;&#20013;&#65292;&#26174;&#31034;&#20986;&#22312;&#35299;&#20915;&#22823;&#22411;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20391;&#37325;&#20110;&#38024;&#23545;&#26576;&#20123;&#38382;&#39064;&#24320;&#21457;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36825;&#20123;&#30740;&#31350;&#32570;&#20047;&#23558;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#33258;&#23450;&#20041;&#27169;&#22411;&#26550;&#26500;&#29992;&#20110;&#36816;&#31609;&#23398;&#38382;&#39064;&#30340;&#28789;&#27963;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#31471;&#21040;&#31471;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20180;&#32454;&#37325;&#26032;&#23454;&#26045;&#27169;&#22411;&#26550;&#26500;&#26469;&#33719;&#30410;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#37325;&#26032;&#23454;&#29616;&#20102;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#24182;&#22312;CleanRL&#20013;&#20351;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;(PPO)&#36827;&#34892;&#35757;&#32451;&#65292;&#23637;&#31034;&#20986;&#33267;&#23569;8&#20493;&#30340;&#35757;&#32451;&#26102;&#38388;&#21152;&#36895;&#12290;&#25105;&#20204;&#22312;&#27492;&#24341;&#20837;&#20102;RLOR&#65292;&#19968;&#31181;&#29992;&#20110;&#36816;&#31609;&#23398;&#30340;&#28789;&#27963;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#30456;&#20449;&#28789;&#27963;&#30340;&#26694;&#26550;&#23545;&#20110;&#24320;&#21457;&#21508;&#31181;&#36816;&#31609;&#23398;&#38382;&#39064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning has been applied in operation research and has shown promise in solving large combinatorial optimization problems. However, existing works focus on developing neural network architectures for certain problems. These works lack the flexibility to incorporate recent advances in reinforcement learning, as well as the flexibility of customizing model architectures for operation research problems. In this work, we analyze the end-to-end autoregressive models for vehicle routing problems and show that these models can benefit from the recent advances in reinforcement learning with a careful re-implementation of the model architecture. In particular, we re-implemented the Attention Model and trained it with Proximal Policy Optimization (PPO) in CleanRL, showing at least 8 times speed up in training time. We hereby introduce RLOR, a flexible framework for Deep Reinforcement Learning for Operation Research. We believe that a flexible framework is key to developing deep re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36866;&#24212;&#24615;&#27491;&#21017;&#21270;&#22312;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#26681;&#25454;&#20219;&#21153;&#22797;&#26434;&#24230;&#21160;&#24577;&#35843;&#25972;&#27491;&#21017;&#21270;&#24378;&#24230;&#65292;&#22312;&#23398;&#20064;&#26032;&#31867;&#21035;&#21516;&#26102;&#38450;&#27490;&#36951;&#24536;&#20043;&#21069;&#23398;&#20064;&#30340;&#31867;&#21035;&#12290;&#23454;&#39564;&#34920;&#26126;&#36866;&#24212;&#24615;&#27491;&#21017;&#21270;&#21487;&#20197;&#23454;&#29616;&#26356;&#21152;&#20934;&#30830;&#21644;&#19981;&#26131;&#36951;&#24536;&#30340;&#35270;&#35273;&#22686;&#37327;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2303.13113</link><description>&lt;p&gt;
&#36866;&#24212;&#24615;&#27491;&#21017;&#21270;&#22312;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Adaptive Regularization for Class-Incremental Learning. (arXiv:2303.13113v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36866;&#24212;&#24615;&#27491;&#21017;&#21270;&#22312;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#26681;&#25454;&#20219;&#21153;&#22797;&#26434;&#24230;&#21160;&#24577;&#35843;&#25972;&#27491;&#21017;&#21270;&#24378;&#24230;&#65292;&#22312;&#23398;&#20064;&#26032;&#31867;&#21035;&#21516;&#26102;&#38450;&#27490;&#36951;&#24536;&#20043;&#21069;&#23398;&#20064;&#30340;&#31867;&#21035;&#12290;&#23454;&#39564;&#34920;&#26126;&#36866;&#24212;&#24615;&#27491;&#21017;&#21270;&#21487;&#20197;&#23454;&#29616;&#26356;&#21152;&#20934;&#30830;&#21644;&#19981;&#26131;&#36951;&#24536;&#30340;&#35270;&#35273;&#22686;&#37327;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#22686;&#37327;&#23398;&#20064;&#26159;&#25351;&#22312;&#32500;&#25345;&#20808;&#21069;&#23398;&#20064;&#30340;&#20998;&#31867;&#20934;&#30830;&#24230;&#30340;&#21516;&#26102;&#65292;&#26356;&#26032;&#20855;&#26377;&#26032;&#31867;&#21035;&#30340;&#28145;&#24230;&#20998;&#31867;&#22120;&#12290;&#22312;&#23398;&#20064;&#26032;&#31867;&#21035;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#26469;&#38450;&#27490;&#36951;&#24536;&#20043;&#21069;&#23398;&#20064;&#30340;&#31867;&#21035;&#26159;&#24120;&#35265;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#22312;&#25972;&#20010;&#22686;&#37327;&#23398;&#20064;&#36807;&#31243;&#20013;&#20351;&#29992;&#24658;&#23450;&#30340;&#24378;&#24230;&#65292;&#21487;&#33021;&#26080;&#27861;&#21453;&#26144;&#25152;&#36935;&#21040;&#30340;&#20219;&#21153;&#38590;&#24230;&#30340;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36866;&#24212;&#24615;&#27491;&#21017;&#21270;&#22312;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#24517;&#35201;&#24615;&#65292;&#35813;&#26041;&#27861;&#26681;&#25454;&#25163;&#22836;&#20219;&#21153;&#30340;&#22797;&#26434;&#24230;&#21160;&#24577;&#35843;&#25972;&#27491;&#21017;&#21270;&#24378;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#33258;&#21160;&#30830;&#23450;&#27599;&#20010;&#23398;&#20064;&#20219;&#21153;&#30340;&#26368;&#20339;&#27491;&#21017;&#21270;&#24378;&#24230;&#12290;&#36890;&#36807;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#20004;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#36866;&#24212;&#24615;&#27491;&#21017;&#21270;&#23545;&#20110;&#23454;&#29616;&#26356;&#21152;&#20934;&#30830;&#21644;&#19981;&#26131;&#36951;&#24536;&#30340;&#35270;&#35273;&#22686;&#37327;&#23398;&#20064;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class-Incremental Learning updates a deep classifier with new categories while maintaining the previously observed class accuracy. Regularizing the neural network weights is a common method to prevent forgetting previously learned classes while learning novel ones. However, existing regularizers use a constant magnitude throughout the learning sessions, which may not reflect the varying levels of difficulty of the tasks encountered during incremental learning. This study investigates the necessity of adaptive regularization in Class-Incremental Learning, which dynamically adjusts the regularization strength according to the complexity of the task at hand. We propose a Bayesian Optimization-based approach to automatically determine the optimal regularization magnitude for each learning task. Our experiments on two datasets via two regularizers demonstrate the importance of adaptive regularization for achieving accurate and less forgetful visual incremental learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#38190;&#28857;&#24341;&#23548;&#30340;&#26368;&#20248;&#36755;&#36816;&#27169;&#22411;&#65292;&#36890;&#36807;&#25513;&#27169;&#32422;&#26463;&#21644;&#20851;&#38190;&#28857;&#30340;&#20851;&#31995;&#25351;&#23548;&#21305;&#37197;&#65292;&#24182;&#21487;&#29992;Sinkhorn&#31639;&#27861;&#27714;&#35299;&#12290;</title><link>http://arxiv.org/abs/2303.13102</link><description>&lt;p&gt;
&#20851;&#38190;&#28857;&#24341;&#23548;&#30340;&#26368;&#20248;&#36755;&#36816;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Keypoint-Guided Optimal Transport. (arXiv:2303.13102v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#38190;&#28857;&#24341;&#23548;&#30340;&#26368;&#20248;&#36755;&#36816;&#27169;&#22411;&#65292;&#36890;&#36807;&#25513;&#27169;&#32422;&#26463;&#21644;&#20851;&#38190;&#28857;&#30340;&#20851;&#31995;&#25351;&#23548;&#21305;&#37197;&#65292;&#24182;&#21487;&#29992;Sinkhorn&#31639;&#27861;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#26368;&#20248;&#36755;&#36816;&#26041;&#27861;&#20027;&#35201;&#36890;&#36807;&#36755;&#36816;&#25104;&#26412;/&#36317;&#31163;&#26368;&#23567;&#21270;&#26469;&#25512;&#23548;&#26368;&#20248;&#30340;&#36755;&#36816;&#35745;&#21010;/&#21305;&#37197;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#30340;&#21305;&#37197;&#12290;&#22312;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#22312;&#22495;&#20043;&#38388;&#27880;&#37322;&#19968;&#20123;&#21305;&#37197;&#30340;&#20851;&#38190;&#28857;&#26159;&#21512;&#29702;&#30340;&#65292;&#29978;&#33267;&#26159;&#19981;&#36153;&#21147;&#30340;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#22914;&#20309;&#21033;&#29992;&#27880;&#37322;&#30340;&#20851;&#38190;&#28857;&#26469;&#25351;&#23548;OT&#20013;&#30340;&#27491;&#30830;&#21305;&#37197;&#20855;&#26377;&#20215;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#20851;&#38190;&#28857;&#24341;&#23548;&#30340;&#26368;&#20248;&#36755;&#36816;&#27169;&#22411;&#65292;&#36890;&#36807;&#20851;&#38190;&#28857;&#24341;&#23548;&#23547;&#25214;&#26368;&#20248;&#21305;&#37197;&#65288;&#21363;&#36755;&#36816;&#35745;&#21010;&#65289;&#12290;&#20026;&#20102;&#22312;OT&#20013;&#20351;&#29992;&#20851;&#38190;&#28857;&#65292;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25513;&#27169;&#30340;&#36755;&#36816;&#35745;&#21010;&#32422;&#26463;&#65292;&#29992;&#20110;&#20445;&#30041;&#20851;&#38190;&#28857;&#25104;&#23545;&#30340;&#21305;&#37197;&#20851;&#31995;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20445;&#30041;&#27599;&#20010;&#25968;&#25454;&#28857;&#19982;&#20851;&#38190;&#28857;&#30340;&#20851;&#31995;&#20197;&#25351;&#23548;&#21305;&#37197;&#30340;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;KPG-RL&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;Sinkhorn&#30340;&#31639;&#27861;&#27714;&#35299;&#65292;&#24182;&#19988;&#22312;&#20998;&#24067;&#22312;&#19981;&#21516;&#31354;&#38388;&#19978;&#26102;&#20063;&#36866;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing Optimal Transport (OT) methods mainly derive the optimal transport plan/matching under the criterion of transport cost/distance minimization, which may cause incorrect matching in some cases. In many applications, annotating a few matched keypoints across domains is reasonable or even effortless in annotation burden. It is valuable to investigate how to leverage the annotated keypoints to guide the correct matching in OT. In this paper, we propose a novel KeyPoint-Guided model by ReLation preservation (KPG-RL) that searches for the optimal matching (i.e., transport plan) guided by the keypoints in OT. To impose the keypoints in OT, first, we propose a mask-based constraint of the transport plan that preserves the matching of keypoint pairs. Second, we propose to preserve the relation of each data point to the keypoints to guide the matching. The proposed KPG-RL model can be solved by Sinkhorn's algorithm and is applicable even when distributions are supported in different spac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23450;&#20041;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;&#27010;&#29575;&#31283;&#23450;&#24615;&#26469;&#22238;&#31572;&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#19968;&#20010;&#26681;&#26412;&#38382;&#39064;&#65306;SGD&#22914;&#20309;&#20174;&#25968;&#37327;&#24222;&#22823;&#30340;&#21487;&#33021;&#20005;&#37325;&#36807;&#25311;&#21512;&#30340;&#35299;&#20013;&#36873;&#25321;&#26377;&#24847;&#20041;&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#12290;</title><link>http://arxiv.org/abs/2303.13093</link><description>&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#27010;&#29575;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Probabilistic Stability of Stochastic Gradient Descent. (arXiv:2303.13093v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23450;&#20041;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;&#27010;&#29575;&#31283;&#23450;&#24615;&#26469;&#22238;&#31572;&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#19968;&#20010;&#26681;&#26412;&#38382;&#39064;&#65306;SGD&#22914;&#20309;&#20174;&#25968;&#37327;&#24222;&#22823;&#30340;&#21487;&#33021;&#20005;&#37325;&#36807;&#25311;&#21512;&#30340;&#35299;&#20013;&#36873;&#25321;&#26377;&#24847;&#20041;&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#24320;&#25918;&#38382;&#39064;&#26159;&#22914;&#20309;&#23450;&#20041;&#21644;&#29702;&#35299;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;(SGD)&#25509;&#36817;&#22266;&#23450;&#28857;&#30340;&#31283;&#23450;&#24615;&#12290;&#20256;&#32479;&#25991;&#29486;&#20381;&#36182;&#20110;&#21442;&#25968;&#32479;&#35745;&#30697;&#65292;&#29305;&#21035;&#26159;&#21442;&#25968;&#26041;&#24046;&#30340;&#25910;&#25947;&#26469;&#37327;&#21270;&#31283;&#23450;&#24615;&#12290;&#26412;&#25991;&#37325;&#26032;&#23450;&#20041;&#20102;SGD&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;\textit{&#27010;&#29575;&#25910;&#25947;}&#26465;&#20214;&#26469;&#23450;&#20041;SGD&#30340;\textit{&#27010;&#29575;&#31283;&#23450;&#24615;}&#12290;&#25552;&#20986;&#30340;&#31283;&#23450;&#24615;&#30452;&#25509;&#22238;&#31572;&#20102;&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#19968;&#20010;&#26681;&#26412;&#38382;&#39064;&#65306;SGD&#22914;&#20309;&#20174;&#25968;&#37327;&#24222;&#22823;&#30340;&#21487;&#33021;&#20005;&#37325;&#36807;&#25311;&#21512;&#30340;&#35299;&#20013;&#36873;&#25321;&#26377;&#24847;&#20041;&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#34920;&#26126;&#21482;&#26377;&#22312;&#27010;&#29575;&#24615;&#31283;&#23450;&#24615;&#30340;&#38236;&#22836;&#19979;&#65292;SGD&#25165;&#34920;&#29616;&#20986;&#20016;&#23500;&#32780;&#23454;&#38469;&#30456;&#20851;&#30340;&#23398;&#20064;&#38454;&#27573;&#65292;&#22914;&#23436;&#20840;&#22833;&#21435;&#31283;&#23450;&#24615;&#38454;&#27573;&#12289;&#19981;&#27491;&#30830;&#23398;&#20064;&#38454;&#27573;&#12289;&#25910;&#25947;&#21040;&#20302;&#31209;&#38797;&#28857;&#38454;&#27573;&#21644;&#27491;&#30830;&#23398;&#20064;&#38454;&#27573;&#12290;&#24403;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#36825;&#20123;&#30456;&#22270;&#24847;&#21619;&#30528;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#30340;&#31283;&#23450;&#21644;&#19981;&#31283;&#23450;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental open problem in deep learning theory is how to define and understand the stability of stochastic gradient descent (SGD) close to a fixed point. Conventional literature relies on the convergence of statistical moments, esp., the variance, of the parameters to quantify the stability. We revisit the definition of stability for SGD and use the \textit{convergence in probability} condition to define the \textit{probabilistic stability} of SGD. The proposed stability directly answers a fundamental question in deep learning theory: how SGD selects a meaningful solution for a neural network from an enormous number of solutions that may overfit badly. To achieve this, we show that only under the lens of probabilistic stability does SGD exhibit rich and practically relevant phases of learning, such as the phases of the complete loss of stability, incorrect learning, convergence to low-rank saddles, and correct learning. When applied to a neural network, these phase diagrams imply t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#30028;&#26694;&#30340;&#20027;&#21160;&#23398;&#20064;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#25511;&#21046;&#27599;&#20010;&#21608;&#26399;&#22522;&#20110;&#36793;&#30028;&#26694;&#39044;&#31639;&#30340;&#26694;&#26550; ComPAS&#65292;&#23427;&#21487;&#20197;&#20248;&#20808;&#32771;&#34385;&#26377;&#20449;&#24687;&#37327;&#30340;&#30446;&#26631;&#65292;&#24182;&#36991;&#20813;&#20887;&#20313;&#20197;&#36827;&#34892;&#20844;&#24179;&#27604;&#36739;&#21644;&#39640;&#25928;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.13089</link><description>&lt;p&gt;
Box-Level  Active Detection&#65306;&#19968;&#31181;&#22522;&#20110;&#36793;&#30028;&#26694;&#30340;&#20027;&#21160;&#23398;&#20064;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Box-Level Active Detection. (arXiv:2303.13089v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13089
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#30028;&#26694;&#30340;&#20027;&#21160;&#23398;&#20064;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#25511;&#21046;&#27599;&#20010;&#21608;&#26399;&#22522;&#20110;&#36793;&#30028;&#26694;&#39044;&#31639;&#30340;&#26694;&#26550; ComPAS&#65292;&#23427;&#21487;&#20197;&#20248;&#20808;&#32771;&#34385;&#26377;&#20449;&#24687;&#37327;&#30340;&#30446;&#26631;&#65292;&#24182;&#36991;&#20813;&#20887;&#20313;&#20197;&#36827;&#34892;&#20844;&#24179;&#27604;&#36739;&#21644;&#39640;&#25928;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#22312;&#30830;&#23450;&#39044;&#31639;&#20869;&#30340;&#22522;&#30784;&#19978;&#36873;&#25321;&#20449;&#24687;&#26679;&#26412;&#36827;&#34892;&#27880;&#37322;&#65292;&#36817;&#24180;&#26469;&#24050;&#32463;&#34987;&#35777;&#26126;&#22312;&#30446;&#26631;&#26816;&#27979;&#20013;&#39640;&#25928;&#12290;&#28982;&#32780;&#65292;&#24191;&#27867;&#20351;&#29992;&#30340;&#20027;&#21160;&#26816;&#27979;&#22522;&#20934;&#22312;&#22270;&#20687;&#32423;&#21035;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#36825;&#22312;&#20154;&#24037;&#24037;&#20316;&#37327;&#20272;&#35745;&#20013;&#26159;&#19981;&#29616;&#23454;&#30340;&#65292;&#24182;&#19988;&#23545;&#25317;&#25380;&#22270;&#20687;&#23384;&#22312;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#26041;&#27861;&#20173;&#28982;&#25191;&#34892;&#22270;&#20687;&#32423;&#21035;&#27880;&#37322;&#65292;&#20294;&#26159;&#23545;&#20110;&#30456;&#21516;&#22270;&#20687;&#20013;&#30340;&#25152;&#26377;&#30446;&#26631;&#24179;&#31561;&#35780;&#20998;&#20250;&#28010;&#36153;&#39044;&#31639;&#24182;&#20135;&#29983;&#20887;&#20313;&#26631;&#31614;&#12290;&#37492;&#20110;&#20197;&#19978;&#38382;&#39064;&#21644;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#30028;&#26694;&#30340;&#20027;&#21160;&#26816;&#27979;&#26694;&#26550;&#65292;&#22312;&#27599;&#20010;&#21608;&#26399;&#20869;&#25511;&#21046;&#22522;&#20110;&#36793;&#30028;&#26694;&#30340;&#39044;&#31639;&#65292;&#20248;&#20808;&#32771;&#34385;&#20449;&#24687;&#30446;&#26631;&#24182;&#36991;&#20813;&#20887;&#20313;&#20197;&#36827;&#34892;&#20844;&#24179;&#27604;&#36739;&#21644;&#39640;&#25928;&#24212;&#29992;&#12290;&#22312;&#25552;&#20986;&#30340;&#22522;&#20110;&#36793;&#30028;&#26694;&#30340;&#26041;&#26696;&#19979;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#27969;&#31243;&#65292;&#21363;&#20114;&#34917;&#20266;&#20027;&#21160;&#31574;&#30053;&#65288;ComPAS&#65289;&#12290; &#23427;&#20197;&#20114;&#34917;&#26041;&#24335;&#21033;&#29992;&#20154;&#31867;&#27880;&#37322;&#21644;&#27169;&#22411;&#26234;&#33021;&#65306;&#39640;&#25928;&#30340;&#36755;&#20837;&#31471;&#22996;&#21592;&#20250;&#20165;&#26597;&#35810;&#20449;&#24687;&#23545;&#35937;&#30340;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active learning selects informative samples for annotation within budget, which has proven efficient recently on object detection. However, the widely used active detection benchmarks conduct image-level evaluation, which is unrealistic in human workload estimation and biased towards crowded images. Furthermore, existing methods still perform image-level annotation, but equally scoring all targets within the same image incurs waste of budget and redundant labels. Having revealed above problems and limitations, we introduce a box-level active detection framework that controls a box-based budget per cycle, prioritizes informative targets and avoids redundancy for fair comparison and efficient application.  Under the proposed box-level setting, we devise a novel pipeline, namely Complementary Pseudo Active Strategy (ComPAS). It exploits both human annotations and the model intelligence in a complementary fashion: an efficient input-end committee queries labels for informative objects only
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#29289;&#21551;&#21457;&#30340;&#22810;&#38454;&#27573;&#33258;&#36866;&#24212;&#38408;&#20540;&#26041;&#27861;&#29992;&#20110;&#36716;&#21270;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20026;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20351;&#24471;&#31070;&#32463;&#20803;&#26356;&#24555;&#22320;&#20256;&#36882;&#33033;&#20914;&#24182;&#20256;&#36755;&#26356;&#22810;&#20449;&#24687;&#65292;&#22312;&#31934;&#24230;&#21644;&#25928;&#29575;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.13080</link><description>&lt;p&gt;
MSAT: &#29983;&#29289;&#21551;&#21457;&#30340;&#22810;&#38454;&#27573;&#33258;&#36866;&#24212;&#38408;&#20540;&#29992;&#20110;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
MSAT: Biologically Inspired Multi-Stage Adaptive Threshold for Conversion of Spiking Neural Networks. (arXiv:2303.13080v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13080
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#29289;&#21551;&#21457;&#30340;&#22810;&#38454;&#27573;&#33258;&#36866;&#24212;&#38408;&#20540;&#26041;&#27861;&#29992;&#20110;&#36716;&#21270;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20026;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20351;&#24471;&#31070;&#32463;&#20803;&#26356;&#24555;&#22320;&#20256;&#36882;&#33033;&#20914;&#24182;&#20256;&#36755;&#26356;&#22810;&#20449;&#24687;&#65292;&#22312;&#31934;&#24230;&#21644;&#25928;&#29575;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#33033;&#20914;&#31232;&#30095;&#24615;&#65292;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#21487;&#20197;&#20197;&#20302;&#21151;&#32791;&#36827;&#34892;&#25512;&#29702;&#12290;ANN-SNN&#36716;&#25442;&#26159;&#23558;&#32463;&#36807;&#33391;&#22909;&#35757;&#32451;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#36716;&#25442;&#20026;&#28145;&#24230;SNN&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#24120;&#25968;&#38408;&#20540;&#36827;&#34892;&#36716;&#25442;&#65292;&#36825;&#20250;&#38459;&#27490;&#31070;&#32463;&#20803;&#24555;&#36895;&#20256;&#36882;&#33033;&#20914;&#21040;&#26356;&#28145;&#30340;&#23618;&#65292;&#24182;&#23548;&#33268;&#39640;&#26102;&#38388;&#24310;&#36831;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#38454;&#27573;&#33258;&#36866;&#24212;&#38408;&#20540;&#65288;MSAT&#65289;&#30340;&#29983;&#29289;&#23398;&#27169;&#22411;&#26426;&#21046;&#12290;&#23545;&#20110;&#27599;&#20010;&#31070;&#32463;&#20803;&#65292;&#21160;&#24577;&#38408;&#20540;&#38543;&#30528;&#21457;&#25918;&#21382;&#21490;&#21644;&#36755;&#20837;&#23646;&#24615;&#32780;&#21464;&#21270;&#65292;&#24182;&#19988;&#19982;&#24179;&#22343;&#33180;&#30005;&#20301;&#27491;&#30456;&#20851;&#65292;&#19982;&#21435;&#26497;&#21270;&#29575;&#36127;&#30456;&#20851;&#12290;&#36825;&#31181;&#33258;&#36866;&#24212;&#33180;&#30005;&#20301;&#21644;&#36755;&#20837;&#30340;&#26041;&#24335;&#21487;&#20197;&#21450;&#26102;&#35843;&#25972;&#38408;&#20540;&#65292;&#26356;&#24555;&#22320;&#20256;&#36882;&#33033;&#20914;&#24182;&#20256;&#36755;&#26356;&#22810;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26435;&#37325;&#33258;&#36866;&#24212;&#31639;&#27861;&#26469;&#24494;&#35843;&#36716;&#25442;&#21518;&#30340;SNN&#65292;&#36825;&#26174;&#33879;&#25552;&#39640;&#20102;ANN-SNN&#36716;&#25442;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs) can do inference with low power consumption due to their spike sparsity. ANN-SNN conversion is an efficient way to achieve deep SNNs by converting well-trained Artificial Neural Networks (ANNs). However, the existing methods commonly use constant threshold for conversion, which prevents neurons from rapidly delivering spikes to deeper layers and causes high time delay. In addition, the same response for different inputs may result in information loss during the information transmission. Inspired by the biological model mechanism, we propose a multi-stage adaptive threshold (MSAT). Specifically, for each neuron, the dynamic threshold varies with firing history and input properties and is positively correlated with the average membrane potential and negatively correlated with the rate of depolarization. The self-adaptation to membrane potential and input allows a timely adjustment of the threshold to fire spike faster and transmit more information. Moreover
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#35777;&#26126;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#21453;&#21521;&#39044;&#27979;&#23431;&#23449;&#21021;&#22987;&#32447;&#24615;&#20301;&#31227;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#20943;&#23569;&#35745;&#31639;&#37327;&#30340;&#21516;&#26102;&#20934;&#30830;&#24674;&#22797;&#21021;&#22987;&#32447;&#24615;&#20301;&#31227;&#12290;</title><link>http://arxiv.org/abs/2303.13056</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#23431;&#23449;&#30340;&#21021;&#22987;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
Predicting the Initial Conditions of the Universe using Deep Learning. (arXiv:2303.13056v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#35777;&#26126;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#21453;&#21521;&#39044;&#27979;&#23431;&#23449;&#21021;&#22987;&#32447;&#24615;&#20301;&#31227;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#20943;&#23569;&#35745;&#31639;&#37327;&#30340;&#21516;&#26102;&#20934;&#30830;&#24674;&#22797;&#21021;&#22987;&#32447;&#24615;&#20301;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25214;&#21040;&#23548;&#33268;&#24403;&#21069;&#23431;&#23449;&#29366;&#24577;&#30340;&#21021;&#22987;&#26465;&#20214;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#21040;&#25628;&#32034;&#19968;&#20010;&#24040;&#22823;&#30340;&#21021;&#22987;&#26465;&#20214;&#36755;&#20837;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#35832;&#22914;N-&#20307;&#27169;&#25311;&#31561;&#24037;&#20855;&#24314;&#27169;&#23427;&#20204;&#30340;&#28436;&#21270;&#65292;&#36825;&#26159;&#35745;&#31639;&#19978;&#26114;&#36149;&#30340;&#12290;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26367;&#20195;&#24314;&#27169;&#24037;&#20855;&#65292;&#23427;&#21487;&#20197;&#23398;&#20064;N-&#20307;&#27169;&#25311;&#30340;&#32447;&#24615;&#36755;&#20837;&#19982;&#32418;&#31227;&#20026;&#38646;&#26102;&#30340;&#26368;&#32456;&#38750;&#32447;&#24615;&#20301;&#31227;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#36825;&#21487;&#20197;&#26174;&#33879;&#21152;&#36895;&#21521;&#21069;&#30340;&#27169;&#25311;&#12290;&#20294;&#26159;&#65292;&#36825;&#24182;&#19981;&#33021;&#20943;&#23569;&#21021;&#22987;&#26465;&#20214;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#35777;&#26126;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#34987;&#29992;&#20110;&#21453;&#21521;&#26144;&#23556;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#22522;&#20110;V-Net&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#21487;&#20197;&#22312;&#32473;&#23450;&#31995;&#32479;&#24403;&#21069;&#26102;&#38388;&#30340;&#38750;&#32447;&#24615;&#20301;&#31227;&#21644;&#23431;&#23449;&#23398;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#36755;&#20986;N-&#20307;&#31995;&#32479;&#30340;&#32447;&#24615;&#20301;&#31227;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20934;&#30830;&#22320;&#24674;&#22797;&#21021;&#22987;&#32447;&#24615;&#20301;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding the initial conditions that led to the current state of the universe is challenging because it involves searching over a vast input space of initial conditions, along with modeling their evolution via tools such as N-body simulations which are computationally expensive. Deep learning has emerged as an alternate modeling tool that can learn the mapping between the linear input of an N-body simulation and the final nonlinear displacements at redshift zero, which can significantly accelerate the forward modeling. However, this does not help reduce the search space for initial conditions. In this paper, we demonstrate for the first time that a deep learning model can be trained for the reverse mapping. We train a V-Net based convolutional neural network, which outputs the linear displacement of an N-body system, given the current time nonlinear displacement and the cosmological parameters of the system. We demonstrate that this neural network accurately recovers the initial linear 
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#29992;&#25143;&#30028;&#38754;&#35774;&#35745;&#20013;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#23558;&#25512;&#21160;&#36719;&#20214;&#24320;&#21457;&#34892;&#19994;&#36827;&#27493;&#30340;&#39640;&#28508;&#21147;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2303.13055</link><description>&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#37325;&#26032;&#35774;&#35745;&#24212;&#29992;&#31243;&#24207;&#29992;&#25143;&#30028;&#38754;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Reimagining Application User Interface (UI) Design using Deep Learning Methods: Challenges and Opportunities. (arXiv:2303.13055v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13055
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#29992;&#25143;&#30028;&#38754;&#35774;&#35745;&#20013;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#23558;&#25512;&#21160;&#36719;&#20214;&#24320;&#21457;&#34892;&#19994;&#36827;&#27493;&#30340;&#39640;&#28508;&#21147;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27010;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#29992;&#25143;&#30028;&#38754;&#35774;&#35745;&#20013;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#28085;&#30422;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12289;&#33258;&#32534;&#30721;&#22120;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#31561;&#20247;&#25152;&#21608;&#30693;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21644;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#35813;&#39046;&#22495;&#30340;&#37325;&#35201;&#38382;&#39064;&#21644;&#26032;&#20852;&#30740;&#31350;&#21069;&#27839;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#29992;&#25143;&#30028;&#38754;&#35774;&#35745;&#33258;&#21160;&#21270;&#20219;&#21153;&#21487;&#33021;&#26159;&#25512;&#21160;&#36719;&#20214;&#24320;&#21457;&#34892;&#19994;&#36827;&#27493;&#30340;&#39640;&#28508;&#21147;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a review of the recent work in deep learning methods for user interface design. The survey encompasses well known deep learning techniques (deep neural networks, convolutional neural networks, recurrent neural networks, autoencoders, and generative adversarial networks) and datasets widely used to design user interface applications. We highlight important problems and emerging research frontiers in this field. We believe that the use of deep learning for user interface design automation tasks could be one of the high potential fields for the advancement of the software development industry.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26032;&#22411;&#21160;&#24577;&#22270;&#23398;&#20064;&#26550;&#26500;DyGFormer&#65292;&#24182;&#24341;&#20837;&#20102;&#32479;&#19968;&#30340;&#24211;DyGLib&#65292;&#20197;&#20419;&#36827;&#21487;&#37325;&#22797;&#12289;&#21487;&#25193;&#23637;&#21644;&#21487;&#20449;&#30340;&#21160;&#24577;&#22270;&#23398;&#20064;&#30740;&#31350;&#12290;DyGFormer&#36890;&#36807;&#37051;&#23621;&#20849;&#29616;&#32534;&#30721;&#26041;&#26696;&#21644;&#20998;&#22359;&#25216;&#26415;&#23454;&#29616;&#26356;&#38271;&#26399;&#21382;&#21490;&#30340;&#39640;&#25928;&#25512;&#29702;&#65292;&#22312;13&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.13047</link><description>&lt;p&gt;
&#21521;&#26356;&#22909;&#30340;&#21160;&#24577;&#22270;&#23398;&#20064;&#36808;&#36827;&#65306;&#26032;&#30340;&#26550;&#26500;&#21644;&#32479;&#19968;&#24211;
&lt;/p&gt;
&lt;p&gt;
Towards Better Dynamic Graph Learning: New Architecture and Unified Library. (arXiv:2303.13047v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13047
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26032;&#22411;&#21160;&#24577;&#22270;&#23398;&#20064;&#26550;&#26500;DyGFormer&#65292;&#24182;&#24341;&#20837;&#20102;&#32479;&#19968;&#30340;&#24211;DyGLib&#65292;&#20197;&#20419;&#36827;&#21487;&#37325;&#22797;&#12289;&#21487;&#25193;&#23637;&#21644;&#21487;&#20449;&#30340;&#21160;&#24577;&#22270;&#23398;&#20064;&#30740;&#31350;&#12290;DyGFormer&#36890;&#36807;&#37051;&#23621;&#20849;&#29616;&#32534;&#30721;&#26041;&#26696;&#21644;&#20998;&#22359;&#25216;&#26415;&#23454;&#29616;&#26356;&#38271;&#26399;&#21382;&#21490;&#30340;&#39640;&#25928;&#25512;&#29702;&#65292;&#22312;13&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;DyGFormer&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26032;&#22411;&#21160;&#24577;&#22270;&#23398;&#20064;&#26550;&#26500;&#65292;&#20165;&#20174;&#33410;&#28857;&#21382;&#21490;&#30340;&#31532;&#19968;&#36339;&#20132;&#20114;&#24207;&#21015;&#20013;&#23398;&#20064;&#12290;DyGFormer&#32467;&#21512;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#35774;&#35745;&#65306;&#19968;&#31181;&#37051;&#23621;&#20849;&#29616;&#32534;&#30721;&#26041;&#26696;&#65292;&#25506;&#32034;&#28304;&#33410;&#28857;&#21644;&#30446;&#26631;&#33410;&#28857;&#22522;&#20110;&#23427;&#20204;&#30340;&#24207;&#21015;&#30340;&#30456;&#20851;&#24615;&#65307;&#19968;&#31181;&#20998;&#22359;&#25216;&#26415;&#65292;&#23558;&#27599;&#20010;&#24207;&#21015;&#20998;&#25104;&#22810;&#20010;&#22359;&#24182;&#23558;&#20854;&#39304;&#36865;&#32473;Transformer&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#21463;&#30410;&#20110;&#26356;&#38271;&#26399;&#30340;&#21382;&#21490;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;DyGLib&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#24211;&#65292;&#20855;&#26377;&#26631;&#20934;&#30340;&#35757;&#32451;&#31649;&#36947;&#12289;&#21487;&#25193;&#23637;&#30340;&#32534;&#30721;&#25509;&#21475;&#21644;&#32508;&#21512;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#20197;&#20419;&#36827;&#21487;&#37325;&#22797;&#12289;&#21487;&#20280;&#32553;&#21644;&#21487;&#20449;&#30340;&#21160;&#24577;&#22270;&#23398;&#20064;&#30740;&#31350;&#12290;&#36890;&#36807;&#22312;&#26469;&#33258;&#21508;&#20010;&#39046;&#22495;&#30340;13&#20010;&#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#36827;&#34892;&#25512;&#23548;/&#24402;&#32435;&#21160;&#24577;&#38142;&#25509;&#39044;&#27979;&#21644;&#21160;&#24577;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65306;DyGFormer&#22312;mo&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
We propose DyGFormer, a new Transformer-based architecture for dynamic graph learning that solely learns from the sequences of nodes' historical first-hop interactions. DyGFormer incorporates two distinct designs: a neighbor co-occurrence encoding scheme that explores the correlations of the source node and destination node based on their sequences; a patching technique that divides each sequence into multiple patches and feeds them to Transformer, allowing the model to effectively and efficiently benefit from longer histories. We also introduce DyGLib, a unified library with standard training pipelines, extensible coding interfaces, and comprehensive evaluating protocols to promote reproducible, scalable, and credible dynamic graph learning research. By performing extensive experiments on thirteen datasets from various domains for transductive/inductive dynamic link prediction and dynamic node classification tasks, we observe that: DyGFormer achieves state-of-the-art performance on mo
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#36719;&#25552;&#31034;&#23884;&#20837;&#65292;&#25552;&#20986;Soft Prompt-Based Calibration (SPeC)&#31649;&#36947;&#65292;&#26469;&#20943;&#36731;&#36755;&#20837;&#21464;&#37327;&#23545;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#65292;&#38477;&#20302;&#24615;&#33021;&#21464;&#24322;. &#27492;&#26041;&#27861;&#19981;&#20165;&#27604;&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#24615;&#33021;&#31283;&#23450;&#65292;&#32780;&#19988;&#22312;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;.</title><link>http://arxiv.org/abs/2303.13035</link><description>&lt;p&gt;
SPeC&#65306;&#36719;&#25552;&#31034;&#26657;&#20934;&#22312;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20013;&#38477;&#20302;&#24615;&#33021;&#21464;&#24322;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
SPeC: A Soft Prompt-Based Calibration on Mitigating Performance Variability in Clinical Notes Summarization. (arXiv:2303.13035v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13035
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#36719;&#25552;&#31034;&#23884;&#20837;&#65292;&#25552;&#20986;Soft Prompt-Based Calibration (SPeC)&#31649;&#36947;&#65292;&#26469;&#20943;&#36731;&#36755;&#20837;&#21464;&#37327;&#23545;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#65292;&#38477;&#20302;&#24615;&#33021;&#21464;&#24322;. &#27492;&#26041;&#27861;&#19981;&#20165;&#27604;&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#24615;&#33021;&#31283;&#23450;&#65292;&#32780;&#19988;&#22312;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#23384;&#20648;&#30528;&#21253;&#25324;&#30149;&#21382;&#12289;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#26816;&#27979;&#32467;&#26524;&#22312;&#20869;&#30340;&#22823;&#37327;&#24739;&#32773;&#20449;&#24687;&#12290;&#36825;&#20123;&#35760;&#24405;&#23545;&#20110;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#20570;&#20986;&#26126;&#26234;&#30340;&#24739;&#32773;&#25252;&#29702;&#20915;&#31574;&#38750;&#24120;&#20851;&#38190;&#12290;&#25688;&#35201;&#20020;&#24202;&#31508;&#35760;&#21487;&#20197;&#24110;&#21161;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#26356;&#22909;&#22320;&#21457;&#29616;&#28508;&#22312;&#20581;&#24247;&#39118;&#38505;&#65292;&#20197;&#21450;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;&#36825;&#19968;&#36807;&#31243;&#36890;&#36807;&#30830;&#20445;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#21487;&#20197;&#35775;&#38382;&#26368;&#30456;&#20851;&#21644;&#26368;&#26032;&#30340;&#24739;&#32773;&#25968;&#25454;&#65292;&#26377;&#21161;&#20110;&#20943;&#23569;&#38169;&#35823;&#24182;&#25552;&#39640;&#24739;&#32773;&#30340;&#25252;&#29702;&#25928;&#26524;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;&#25552;&#31034;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25688;&#35201;&#20219;&#21153;&#30340;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#20063;&#20250;&#23548;&#33268;&#36755;&#20986;&#26041;&#24046;&#22686;&#21152;&#65292;&#21363;&#20351;&#25552;&#31034;&#24847;&#20041;&#30456;&#20284;&#65292;&#36755;&#20986;&#20063;&#20250;&#26377;&#26126;&#26174;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#36719;&#25552;&#31034;&#26657;&#20934;&#65288;SPeC&#65289;&#27969;&#31243;&#65292;&#35813;&#27969;&#31243;&#37319;&#29992;&#36719;&#25552;&#31034;&#23884;&#20837;&#26469;&#20943;&#36731;&#36755;&#20837;&#21464;&#37327;&#23545;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SPeC&#19981;&#20165;&#21487;&#20197;&#38477;&#20302;LLM&#30340;&#24615;&#33021;&#21464;&#24322;&#65292;&#32780;&#19988;&#22312;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic health records (EHRs) store an extensive array of patient information, encompassing medical histories, diagnoses, treatments, and test outcomes. These records are crucial for enabling healthcare providers to make well-informed decisions regarding patient care. Summarizing clinical notes further assists healthcare professionals in pinpointing potential health risks and making better-informed decisions. This process contributes to reducing errors and enhancing patient outcomes by ensuring providers have access to the most pertinent and current patient data. Recent research has shown that incorporating prompts with large language models (LLMs) substantially boosts the efficacy of summarization tasks. However, we show that this approach also leads to increased output variance, resulting in notably divergent outputs even when prompts share similar meanings. To tackle this challenge, we introduce a model-agnostic Soft Prompt-Based Calibration (SPeC) pipeline that employs soft prom
&lt;/p&gt;</description></item><item><title>PAC-MOO&#26159;&#19968;&#20010;&#20559;&#22909;&#24863;&#30693;&#30340;&#32422;&#26463;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#22312;&#40657;&#30418;&#30446;&#26631;&#20989;&#25968;&#21644;&#20174;&#19994;&#32773;&#25351;&#23450;&#30340;&#30446;&#26631;&#20559;&#22909;&#19979;&#65292;&#22823;&#37096;&#20998;&#36755;&#20837;&#31354;&#38388;&#26159;&#19981;&#21487;&#34892;&#30340;&#32422;&#26463;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.13034</link><description>&lt;p&gt;
&#20559;&#22909;&#24863;&#30693;&#30340;&#32422;&#26463;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Preference-Aware Constrained Multi-Objective Bayesian Optimization. (arXiv:2303.13034v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13034
&lt;/p&gt;
&lt;p&gt;
PAC-MOO&#26159;&#19968;&#20010;&#20559;&#22909;&#24863;&#30693;&#30340;&#32422;&#26463;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#22312;&#40657;&#30418;&#30446;&#26631;&#20989;&#25968;&#21644;&#20174;&#19994;&#32773;&#25351;&#23450;&#30340;&#30446;&#26631;&#20559;&#22909;&#19979;&#65292;&#22823;&#37096;&#20998;&#36755;&#20837;&#31354;&#38388;&#26159;&#19981;&#21487;&#34892;&#30340;&#32422;&#26463;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#22823;&#37096;&#20998;&#36755;&#20837;&#31354;&#38388;&#26159;&#19981;&#21487;&#34892;&#65288;&#21363;&#36829;&#21453;&#32422;&#26463;&#26465;&#20214;&#65289;&#26102;&#65292;&#22522;&#20110;&#40657;&#30418;&#30446;&#26631;&#20989;&#25968;&#21644;&#20174;&#19994;&#32773;&#25351;&#23450;&#30340;&#30446;&#26631;&#20559;&#22909;&#30340;&#32422;&#26463;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#35768;&#22810;&#24037;&#31243;&#35774;&#35745;&#38382;&#39064;&#20013;&#37117;&#23384;&#22312;&#65292;&#21253;&#25324;&#27169;&#25311;&#30005;&#36335;&#21644;&#30005;&#21147;&#31995;&#32479;&#35774;&#35745;&#12290;&#25105;&#20204;&#30340;&#24635;&#20307;&#30446;&#26631;&#26159;&#22312;&#21487;&#34892;&#30340;&#36755;&#20837;&#35774;&#35745;&#30340;&#23567;&#37096;&#20998;&#19978;&#36817;&#20284;&#26368;&#20248;Pareto&#38598;&#21512;&#12290;&#20027;&#35201;&#25361;&#25112;&#21253;&#25324;&#35774;&#35745;&#31354;&#38388;&#30340;&#24040;&#22823;&#22823;&#23567;&#12289;&#22810;&#20010;&#30446;&#26631;&#21644;&#22823;&#37327;&#30340;&#32422;&#26463;&#26465;&#20214;&#20197;&#21450;&#21482;&#33021;&#22312;&#36827;&#34892;&#26114;&#36149;&#30340;&#20223;&#30495;&#21518;&#25165;&#33021;&#30830;&#35748;&#30340;&#21487;&#34892;&#30340;&#36755;&#20837;&#35774;&#35745;&#30340;&#23567;&#37096;&#20998;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#26377;&#25928;&#30340;&#20559;&#22909;&#24863;&#30693;&#30340;&#32422;&#26463;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65288;PAC-MOO&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#23398;&#20064;&#36755;&#20986;&#30446;&#26631;&#21644;&#32422;&#26463;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#24182;&#26681;&#25454;&#20174;&#19994;&#32773;&#39044;&#27979;&#30340;&#20559;&#22909;&#36873;&#25321;&#35780;&#20272;&#30446;&#26631;&#30340;&#20505;&#36873;&#36755;&#20837;&#12290;PAC-MOO&#26681;&#25454;&#39044;&#27979;&#30340;&#30446;&#26631;&#21644;&#32422;&#26463;&#30340;&#32852;&#21512;&#20559;&#22909;&#36845;&#20195;&#22320;&#36873;&#25321;&#19979;&#19968;&#20010;&#35201;&#27169;&#25311;&#30340;&#36755;&#20837;&#35774;&#35745;&#65292;&#24182;&#20351;&#29992;&#26032;&#33719;&#24471;&#30340;&#25968;&#25454;&#26356;&#26032;&#20195;&#29702;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the problem of constrained multi-objective optimization over black-box objective functions with practitioner-specified preferences over the objectives when a large fraction of the input space is infeasible (i.e., violates constraints). This problem arises in many engineering design problems including analog circuits and electric power system design. Our overall goal is to approximate the optimal Pareto set over the small fraction of feasible input designs. The key challenges include the huge size of the design space, multiple objectives and large number of constraints, and the small fraction of feasible input designs which can be identified only after performing expensive simulations. We propose a novel and efficient preference-aware constrained multi-objective Bayesian optimization approach referred to as PAC-MOO to address these challenges. The key idea is to learn surrogate models for both output objectives and constraints, and select the candidate input for eva
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#32858;&#31867;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30830;&#23450;&#24182;&#35782;&#21035;&#23545;&#20110;TBI&#31561;&#24613;&#24615;&#30142;&#30149;&#27835;&#30103;&#38750;&#24120;&#37325;&#35201;&#30340;&#29983;&#29702;&#29366;&#24577;&#12290;&#30740;&#31350;&#36824;&#21033;&#29992;&#20020;&#24202;&#25968;&#25454;&#39564;&#35777;&#24182;&#35299;&#37322;&#25152;&#35782;&#21035;&#30340;&#29983;&#29702;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2303.13024</link><description>&lt;p&gt;
&#29992;&#20110;&#35782;&#21035;TBI&#29983;&#29702;&#29366;&#24577;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#33258;&#30417;&#30563;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Clustering of Multivariate Time-Series Data for Identifying TBI Physiological States. (arXiv:2303.13024v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13024
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#32858;&#31867;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30830;&#23450;&#24182;&#35782;&#21035;&#23545;&#20110;TBI&#31561;&#24613;&#24615;&#30142;&#30149;&#27835;&#30103;&#38750;&#24120;&#37325;&#35201;&#30340;&#29983;&#29702;&#29366;&#24577;&#12290;&#30740;&#31350;&#36824;&#21033;&#29992;&#20020;&#24202;&#25968;&#25454;&#39564;&#35777;&#24182;&#35299;&#37322;&#25152;&#35782;&#21035;&#30340;&#29983;&#29702;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30830;&#23450;&#20020;&#24202;&#30456;&#20851;&#30340;&#29983;&#29702;&#29366;&#24577;&#38750;&#24120;&#37325;&#35201;&#65292;&#36825;&#23545;&#20110;&#25552;&#20379;&#24613;&#24615;&#30142;&#30149;&#65288;&#22914;&#39045;&#33041;&#25439;&#20260;&#12289;&#21628;&#21560;&#34928;&#31469;&#21644;&#24515;&#21147;&#34928;&#31469;&#65289;&#30340;&#36866;&#24403;&#27835;&#30103;&#33267;&#20851;&#37325;&#35201;&#12290;&#21033;&#29992;&#38750;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#25110;&#25968;&#25454;&#25554;&#20540;&#21644;&#32858;&#21512;&#25216;&#26415;&#21487;&#33021;&#23548;&#33268;&#26377;&#20215;&#20540;&#20449;&#24687;&#30340;&#20002;&#22833;&#21644;&#20559;&#35265;&#20998;&#26512;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#22522;&#20110;&#33258;&#30417;&#30563;&#30340;SLAC-Time&#31639;&#27861;&#65292;&#36991;&#20813;&#20102;&#25554;&#20540;&#25110;&#32858;&#21512;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#34920;&#31034;&#24613;&#24615;&#24739;&#32773;&#29366;&#24577;&#12290;&#36890;&#36807;&#20351;&#29992;SLAC-Time&#26469;&#32858;&#31867;&#22823;&#22411;&#30740;&#31350;&#25968;&#25454;&#38598;&#20013;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;TBI&#29983;&#29702;&#29366;&#24577;&#21450;&#20854;&#20855;&#20307;&#29305;&#24449;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#21508;&#31181;&#32858;&#31867;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#32467;&#21512;&#20020;&#24202;&#39046;&#22495;&#19987;&#23478;&#30340;&#24847;&#35265;&#26469;&#39564;&#35777;&#21644;&#35299;&#37322;&#25152;&#35782;&#21035;&#30340;&#29983;&#29702;&#29366;&#24577;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#29305;&#23450;&#20020;&#24202;&#20107;&#20214;&#21644;&#29983;&#29702;&#29366;&#24577;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Determining clinically relevant physiological states from multivariate time series data with missing values is essential for providing appropriate treatment for acute conditions such as Traumatic Brain Injury (TBI), respiratory failure, and heart failure. Utilizing non-temporal clustering or data imputation and aggregation techniques may lead to loss of valuable information and biased analyses. In our study, we apply the SLAC-Time algorithm, an innovative self-supervision-based approach that maintains data integrity by avoiding imputation or aggregation, offering a more useful representation of acute patient states. By using SLAC-Time to cluster data in a large research dataset, we identified three distinct TBI physiological states and their specific feature profiles. We employed various clustering evaluation metrics and incorporated input from a clinical domain expert to validate and interpret the identified physiological states. Further, we discovered how specific clinical events and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ENVIDR&#30340;&#28210;&#26579;&#21644;&#24314;&#27169;&#26694;&#26550;&#65292;&#22312;&#28210;&#26579;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38236;&#38754;&#21453;&#23556;&#34920;&#38754;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#65292;&#38598;&#25104;&#20102;&#31070;&#32463;&#28210;&#26579;&#22120;&#21644;&#22522;&#20110;SDF&#30340;&#31070;&#32463;&#34920;&#38754;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.13022</link><description>&lt;p&gt;
ENVIDR: &#20855;&#26377;&#31070;&#32463;&#29615;&#22659;&#20809;&#29031;&#30340;&#38544;&#24335;&#21487;&#24494;&#20998;&#28210;&#26579;&#22120;
&lt;/p&gt;
&lt;p&gt;
ENVIDR: Implicit Differentiable Renderer with Neural Environment Lighting. (arXiv:2303.13022v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ENVIDR&#30340;&#28210;&#26579;&#21644;&#24314;&#27169;&#26694;&#26550;&#65292;&#22312;&#28210;&#26579;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38236;&#38754;&#21453;&#23556;&#34920;&#38754;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#65292;&#38598;&#25104;&#20102;&#31070;&#32463;&#28210;&#26579;&#22120;&#21644;&#22522;&#20110;SDF&#30340;&#31070;&#32463;&#34920;&#38754;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#31070;&#32463;&#28210;&#26579;&#30340;&#36827;&#23637;&#26174;&#31034;&#20986;&#20174;&#22810;&#35270;&#22270;&#22270;&#20687;&#37325;&#24314;&#22330;&#26223;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#28982;&#32780;&#29616;&#26377;&#26041;&#27861;&#22312;&#20934;&#30830;&#34920;&#31034;&#20855;&#26377;&#20809;&#27901;&#34920;&#38754;&#30340;&#29289;&#20307;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; ENVIDR&#65292;&#19968;&#31181;&#29992;&#20110;&#39640;&#36136;&#37327;&#28210;&#26579;&#21644;&#37325;&#26500;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38236;&#38754;&#21453;&#23556;&#34920;&#38754;&#30340;&#28210;&#26579;&#21644;&#24314;&#27169;&#26694;&#26550;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#28210;&#26579;&#22120;&#65292;&#24182;&#37319;&#29992;&#20998;&#35299;&#28210;&#26579;&#32452;&#20214;&#26469;&#23398;&#20064;&#34920;&#38754;&#21644;&#29615;&#22659;&#20809;&#29031;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#35813;&#28210;&#26579;&#22120;&#20351;&#29992;&#29616;&#26377;&#30340;&#29289;&#29702;&#28210;&#26579;&#22120;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19982;&#23454;&#38469;&#22330;&#26223;&#34920;&#31034;&#20998;&#31163;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; SDF &#30340;&#31070;&#32463;&#34920;&#38754;&#27169;&#22411;&#65292;&#21033;&#29992;&#36825;&#20010;&#24050;&#23398;&#20064;&#30340;&#31070;&#32463;&#28210;&#26579;&#22120;&#26469;&#34920;&#31034;&#19968;&#33324;&#30340;&#22330;&#26223;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#36890;&#36807;&#34892;&#36827;&#34920;&#38754;&#21453;&#23556;&#30340;&#20809;&#32447;&#26469;&#21512;&#25104;&#30001;&#38378;&#20142;&#34920;&#38754;&#24341;&#36215;&#30340;&#38388;&#25509;&#20809;&#29031;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#20855;&#26377;&#20809;&#27901;&#34920;&#38754;&#21644;&#38236;&#38754;&#21453;&#23556;&#26102;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in neural rendering have shown great potential for reconstructing scenes from multiview images. However, accurately representing objects with glossy surfaces remains a challenge for existing methods. In this work, we introduce ENVIDR, a rendering and modeling framework for high-quality rendering and reconstruction of surfaces with challenging specular reflections. To achieve this, we first propose a novel neural renderer with decomposed rendering components to learn the interaction between surface and environment lighting. This renderer is trained using existing physically based renderers and is decoupled from actual scene representations. We then propose an SDF-based neural surface model that leverages this learned neural renderer to represent general scenes. Our model additionally synthesizes indirect illuminations caused by inter-reflections from shiny surfaces by marching surface-reflected rays. We demonstrate that our method outperforms state-of-art methods on chal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Tol-FL&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25153;&#24179;&#21644;&#26143;&#22411;&#25299;&#25169;&#32467;&#26500;&#30340;&#20248;&#21183;&#65292;&#22686;&#24378;&#20102;&#20998;&#24067;&#24335;&#32593;&#32476;&#20013;&#30340;&#21464;&#24322;&#26816;&#27979;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.13015</link><description>&lt;p&gt;
&#26080;&#32447;&#32593;&#32476;&#20013;&#23481;&#24525;&#25925;&#38556;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Failure-tolerant Distributed Learning for Anomaly Detection in Wireless Networks. (arXiv:2303.13015v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Tol-FL&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25153;&#24179;&#21644;&#26143;&#22411;&#25299;&#25169;&#32467;&#26500;&#30340;&#20248;&#21183;&#65292;&#22686;&#24378;&#20102;&#20998;&#24067;&#24335;&#32593;&#32476;&#20013;&#30340;&#21464;&#24322;&#26816;&#27979;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20998;&#24067;&#24335;&#25216;&#26415;&#30340;&#20998;&#26512;&#37117;&#26159;&#19987;&#27880;&#20110;&#23427;&#20204;&#30340;&#25928;&#29575;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#65288;&#25110;&#32570;&#20047;&#40065;&#26834;&#24615;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#32771;&#34385;&#23588;&#20854;&#37325;&#35201;&#65292;&#22240;&#20026;&#24403;&#35774;&#22791;&#25110;&#20013;&#22830;&#26381;&#21153;&#22120;&#20986;&#29616;&#25925;&#38556;&#26102;&#65292;&#36825;&#21487;&#33021;&#20250;&#30251;&#30186;&#20998;&#24067;&#24335;&#31995;&#32479;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#21512;&#25153;&#24179;&#21644;&#26143;&#22411;&#25299;&#25169;&#32467;&#26500;&#26469;&#35299;&#20915;&#36825;&#20123;&#39118;&#38505;&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#20004;&#32773;&#30340;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#20248;&#21183;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;&#8220;Tol-FL&#8221;&#65292;&#22240;&#20026;&#19982;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#30456;&#27604;&#65292;&#23427;&#30340;&#25925;&#38556;&#23481;&#38169;&#33021;&#21147;&#25552;&#39640;&#20102;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#25925;&#38556;&#30340;&#21508;&#31181;&#36924;&#30495;&#24773;&#20917;&#19979;&#65292;&#22312;&#24322;&#24120;&#26816;&#27979;AUROC&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#26041;&#27861;&#39640;&#36798;8&#65285;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#35774;&#22791;&#25925;&#38556;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
The analysis of distributed techniques is often focused upon their efficiency, without considering their robustness (or lack thereof). Such a consideration is particularly important when devices or central servers can fail, which can potentially cripple distributed systems. When such failures arise in wireless communications networks, important services that they use/provide (like anomaly detection) can be left inoperable and can result in a cascade of security problems. In this paper, we present a novel method to address these risks by combining both flat- and star-topologies, combining the performance and reliability benefits of both. We refer to this method as "Tol-FL", due to its increased failure-tolerance as compared to the technique of Federated Learning. Our approach both limits device failure risks while outperforming prior methods by up to 8% in terms of anomaly detection AUROC in a range of realistic settings that consider client as well as server failure, all while reducing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23545;&#25239;&#25915;&#20987;&#30340;&#26041;&#27861;&#8212;&#8212;&#35821;&#20041;&#22270;&#20687;&#25915;&#20987;&#65288;SIA&#65289;&#65292;&#21487;&#20197;&#25552;&#20379;&#35821;&#20041;&#23545;&#25239;&#22270;&#20687;&#20197;&#20415;&#36827;&#34892;&#27169;&#22411;&#35786;&#26029;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.13010</link><description>&lt;p&gt;
&#29992;&#20110;&#35270;&#35273;&#27169;&#22411;&#35786;&#26029;&#30340;&#35821;&#20041;&#22270;&#20687;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Semantic Image Attack for Visual Model Diagnosis. (arXiv:2303.13010v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23545;&#25239;&#25915;&#20987;&#30340;&#26041;&#27861;&#8212;&#8212;&#35821;&#20041;&#22270;&#20687;&#25915;&#20987;&#65288;SIA&#65289;&#65292;&#21487;&#20197;&#25552;&#20379;&#35821;&#20041;&#23545;&#25239;&#22270;&#20687;&#20197;&#20415;&#36827;&#34892;&#27169;&#22411;&#35786;&#26029;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#36341;&#20013;&#65292;&#23545;&#29305;&#23450;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#38598;&#36827;&#34892;&#24230;&#37327;&#20998;&#26512;&#19981;&#33021;&#20445;&#35777;&#21487;&#38752;&#25110;&#20844;&#24179;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36825;&#37096;&#20998;&#21407;&#22240;&#26159;&#65292;&#33719;&#24471;&#24179;&#34913;&#12289;&#22810;&#26679;&#21644;&#26631;&#35760;&#23436;&#32654;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#26159;&#26114;&#36149;&#12289;&#32791;&#26102;&#21644;&#26131;&#20986;&#38169;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#25239;&#25915;&#20987;&#30340;&#26041;&#27861;&#8212;&#8212;&#35821;&#20041;&#22270;&#20687;&#25915;&#20987;&#65288;SIA&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#35821;&#20041;&#23545;&#25239;&#22270;&#20687;&#65292;&#20197;&#20415;&#36827;&#34892;&#27169;&#22411;&#35786;&#26029;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#20256;&#32479;&#30340;&#23545;&#25239;&#35757;&#32451;&#26159;&#19968;&#31181;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23545;&#25239;&#26041;&#27861;&#19981;&#32467;&#21512;&#20004;&#20010;&#26041;&#38754;&#65292;&#26080;&#27861;&#35299;&#37322;&#21644;&#20998;&#26512;&#27169;&#22411;&#30340;&#32570;&#38519;&#65306;&#35821;&#20041;&#21487;&#36861;&#28335;&#24615;&#21644;&#24863;&#35273;&#36136;&#37327;&#12290;SIA&#36890;&#36807;&#39044;&#23450;&#20041;&#30340;&#35821;&#20041;&#23646;&#24615;&#31354;&#38388;&#21644;&#22270;&#20687;&#31354;&#38388;&#19978;&#30340;&#36845;&#20195;&#26799;&#24230;&#19978;&#21319;&#32467;&#21512;&#20102;&#20004;&#20010;&#29305;&#24449;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;SIA&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In practice, metric analysis on a specific train and test dataset does not guarantee reliable or fair ML models. This is partially due to the fact that obtaining a balanced, diverse, and perfectly labeled dataset is typically expensive, time-consuming, and error-prone. Rather than relying on a carefully designed test set to assess ML models' failures, fairness, or robustness, this paper proposes Semantic Image Attack (SIA), a method based on the adversarial attack that provides semantic adversarial images to allow model diagnosis, interpretability, and robustness. Traditional adversarial training is a popular methodology for robustifying ML models against attacks. However, existing adversarial methods do not combine the two aspects that enable the interpretation and analysis of the model's flaws: semantic traceability and perceptual quality. SIA combines the two features via iterative gradient ascent on a predefined semantic attribute space and the image space. We illustrate the validi
&lt;/p&gt;</description></item><item><title>ID3PM&#26041;&#27861;&#36890;&#36807;&#25193;&#25955;&#36807;&#31243;&#30340;&#38543;&#26426;&#24615;&#36136;&#26469;&#20135;&#29983;&#40657;&#30418;&#20154;&#33080;&#35782;&#21035;&#27169;&#22411;&#30340;&#39640;&#24230;&#21487;&#25511;&#30340;&#21453;&#28436;&#65292;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#19988;&#22810;&#26679;&#30340;&#36755;&#20986;&#65292;&#36866;&#29992;&#20110;&#25968;&#25454;&#22686;&#24378;&#12289;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#35757;&#32451;&#25968;&#25454;&#31561;&#22810;&#31181;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.13006</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#21487;&#25511;&#22320;&#21453;&#21521;&#40657;&#30418;&#20154;&#33080;&#35782;&#21035;&#27169;&#22411;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
Controllable Inversion of Black-Box Face-Recognition Models via Diffusion. (arXiv:2303.13006v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13006
&lt;/p&gt;
&lt;p&gt;
ID3PM&#26041;&#27861;&#36890;&#36807;&#25193;&#25955;&#36807;&#31243;&#30340;&#38543;&#26426;&#24615;&#36136;&#26469;&#20135;&#29983;&#40657;&#30418;&#20154;&#33080;&#35782;&#21035;&#27169;&#22411;&#30340;&#39640;&#24230;&#21487;&#25511;&#30340;&#21453;&#28436;&#65292;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#19988;&#22810;&#26679;&#30340;&#36755;&#20986;&#65292;&#36866;&#29992;&#20110;&#25968;&#25454;&#22686;&#24378;&#12289;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#35757;&#32451;&#25968;&#25454;&#31561;&#22810;&#31181;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#35782;&#21035;&#27169;&#22411;&#23558;&#20154;&#33080;&#22270;&#20687;&#23884;&#20837;&#20302;&#32500;&#36523;&#20221;&#21521;&#37327;&#20013;&#65292;&#21253;&#21547;&#36523;&#20221;&#29305;&#24449;&#30340;&#25277;&#35937;&#32534;&#30721;&#65292;&#36825;&#20123;&#29305;&#24449;&#20801;&#35768;&#21306;&#20998;&#20010;&#20307;&#12290;&#25105;&#20204;&#38754;&#20020;&#30528;&#22312;&#27809;&#26377;&#23436;&#20840;&#27169;&#22411;&#35775;&#38382;&#30340;&#24773;&#20917;&#19979;&#65288;&#21363;&#40657;&#30418;&#35774;&#32622;&#19979;&#65289;&#21453;&#21521;&#39044;&#35757;&#32451;&#20154;&#33080;&#35782;&#21035;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#30340;&#25361;&#25112;&#12290;&#24050;&#32463;&#26377;&#35768;&#22810;&#26041;&#27861;&#25552;&#20986;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#20005;&#37325;&#30340;&#32570;&#38519;&#65292;&#22914;&#32570;&#20047;&#29616;&#23454;&#36755;&#20986;&#12289;&#25512;&#29702;&#26102;&#38388;&#38271;&#20197;&#21450;&#23545;&#25968;&#25454;&#38598;&#21644;&#20154;&#33080;&#35782;&#21035;&#27169;&#22411;&#30340;&#21487;&#35775;&#38382;&#24615;&#26377;&#24378;&#28872;&#30340;&#35201;&#27714;&#12290;&#36890;&#36807;&#23545;&#40657;&#30418;&#21453;&#28436;&#38382;&#39064;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26465;&#20214;&#24615;&#25193;&#25955;&#27169;&#22411;&#28431;&#27934;&#30340;&#33258;&#28982;&#28044;&#29616;&#65292;&#24182;&#19988;&#21363;&#20351;&#27809;&#26377;&#36523;&#20221;&#29305;&#24449;&#30340;&#25439;&#22833;&#65292;&#25105;&#20204;&#20063;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#21453;&#21521;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;&#36523;&#20221;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;ID3PM&#65289;&#65292;&#21033;&#29992;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#30340;&#38543;&#26426;&#24615;&#36136;&#26469;&#20135;&#29983;&#40657;&#30418;&#20154;&#33080;&#35782;&#21035;&#27169;&#22411;&#30340;&#39640;&#24230;&#21487;&#25511;&#30340;&#21453;&#28436;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#19988;&#22810;&#26679;&#30340;&#36755;&#20986;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#25968;&#25454;&#22686;&#24378;&#12289;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#35757;&#32451;&#25968;&#25454;&#31561;&#22810;&#31181;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face recognition models embed a face image into a low-dimensional identity vector containing abstract encodings of identity-specific facial features that allow individuals to be distinguished from one another. We tackle the challenging task of inverting the latent space of pre-trained face recognition models without full model access (i.e. black-box setting). A variety of methods have been proposed in literature for this task, but they have serious shortcomings such as a lack of realistic outputs, long inference times, and strong requirements for the data set and accessibility of the face recognition model. Through an analysis of the black-box inversion problem, we show that the conditional diffusion model loss naturally emerges and that we can effectively sample from the inverse distribution even without an identity-specific loss. Our method, named identity denoising diffusion probabilistic model (ID3PM), leverages the stochastic nature of the denoising diffusion process to produce hi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#26696;&#65292;&#36890;&#36807;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#20849;&#21516;&#35757;&#32451;CNPs&#21644;EBM&#65292;&#24182;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#25913;&#36827;&#20102;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.13004</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#23545;&#27604;&#26465;&#20214;&#31070;&#32463;&#36807;&#31243;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Adversarially Contrastive Estimation of Conditional Neural Processes. (arXiv:2303.13004v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#26696;&#65292;&#36890;&#36807;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#20849;&#21516;&#35757;&#32451;CNPs&#21644;EBM&#65292;&#24182;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#25913;&#36827;&#20102;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#31070;&#32463;&#36807;&#31243;(CNPs)&#36890;&#36807;&#20934;&#30830;&#30340;&#26465;&#20214;&#20284;&#28982;&#29983;&#25104;&#20989;&#25968;&#35266;&#27979;&#20540;&#65292;&#24418;&#25104;&#20989;&#25968;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#39044;&#27979;&#20998;&#24067;&#34987;&#20998;&#35299;&#25104;&#19968;&#32452;&#26080;&#32422;&#26463;(&#36890;&#24120;&#20026;&#39640;&#26031;&#20998;&#24067;)&#30340;&#36755;&#20986;&#65292;&#22240;&#27492;CNPs&#30340;&#34920;&#36798;&#33021;&#21147;&#23545;&#20110;&#39640;&#32500;&#24230;&#35266;&#27979;&#20540;&#26159;&#26377;&#38480;&#30340;&#12290;&#20197;&#21069;&#65292;&#21487;&#20197;&#20351;&#29992;&#28508;&#21464;&#37327;&#25110;&#33258;&#22238;&#24402;&#20284;&#28982;&#26469;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#20195;&#20215;&#26159;&#38590;&#20197;&#35757;&#32451;&#21644;&#22797;&#26434;&#24230;&#30340;&#24179;&#26041;&#32423;&#22686;&#21152;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#26696;&#65292;&#20351;CNPs&#19982;&#24120;&#35268;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30456;&#32467;&#21512;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#35757;&#32451;&#20986;&#19968;&#20010;&#33021;&#37327;&#22522;&#27169;&#22411;(EBM)&#65292;&#20854;&#35201;&#27714;EBM&#23558;&#30495;&#23454;&#35266;&#27979;&#20540;&#19982;CNP&#29983;&#25104;&#30340;&#26679;&#26412;&#21306;&#20998;&#20986;&#26469;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;CNP&#24517;&#39035;&#29983;&#25104;&#26356;&#25509;&#36817;&#22522;&#20934;&#31572;&#26696;&#30340;&#39044;&#27979;&#32467;&#26524;&#26469;&#27450;&#39575;EBM&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#20248;&#21270;&#19982;&#22266;&#23450;&#24418;&#24335;&#20284;&#28982;&#26377;&#20851;&#30340;&#37096;&#20998;&#12290;&#20174;&#29983;&#25104;&#20989;&#25968;&#37325;&#26500;&#21040;&#19979;&#28216;&#22238;&#24402;&#20219;&#21153;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#22343;&#34920;&#29616;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#65292;&#36825;&#34920;&#26126;CNPs&#22312;&#24314;&#27169;&#22797;&#26434;&#39640;&#32500;&#25968;&#25454;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conditional Neural Processes~(CNPs) formulate distributions over functions and generate function observations with exact conditional likelihoods. CNPs, however, have limited expressivity for high-dimensional observations, since their predictive distribution is factorized into a product of unconstrained (typically) Gaussian outputs. Previously, this could be handled using latent variables or autoregressive likelihood, but at the expense of intractable training and quadratically increased complexity. Instead, we propose calibrating CNPs with an adversarial training scheme besides regular maximum likelihood estimates. Specifically, we train an energy-based model (EBM) with noise contrastive estimation, which enforces EBM to identify true observations from the generations of CNP. In this way, CNP must generate predictions closer to the ground-truth to fool EBM, instead of merely optimizing with respect to the fixed-form likelihood. From generative function reconstruction to downstream regr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#22312;&#26497;&#31471;&#24773;&#20917;&#19979;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#24120;&#29992;&#30340;PTQ&#26041;&#27861;&#19978;&#24320;&#23637;&#20102;&#31995;&#32479;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;PTQ&#26041;&#27861;&#22312;&#26368;&#21155;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#34920;&#29616;&#19981;&#22815;&#21487;&#38752;&#12290;&#22240;&#27492;&#38656;&#35201;&#24320;&#21457;&#26356;&#21152;&#24378;&#22823;&#30340;PTQ&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#22788;&#29702;&#20998;&#24067;&#20559;&#31227;&#21644;&#25968;&#25454;&#22122;&#22768;&#65292;&#24182;&#25913;&#21892;&#26368;&#21155;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.13003</link><description>&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;&#30340;&#21487;&#38752;&#24615;&#22522;&#20934;&#27979;&#35797;&#65306;&#29305;&#21035;&#20851;&#27880;&#26368;&#21155;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Benchmarking the Reliability of Post-training Quantization: a Particular Focus on Worst-case Performance. (arXiv:2303.13003v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#22312;&#26497;&#31471;&#24773;&#20917;&#19979;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#24120;&#29992;&#30340;PTQ&#26041;&#27861;&#19978;&#24320;&#23637;&#20102;&#31995;&#32479;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;PTQ&#26041;&#27861;&#22312;&#26368;&#21155;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#34920;&#29616;&#19981;&#22815;&#21487;&#38752;&#12290;&#22240;&#27492;&#38656;&#35201;&#24320;&#21457;&#26356;&#21152;&#24378;&#22823;&#30340;PTQ&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#22788;&#29702;&#20998;&#24067;&#20559;&#31227;&#21644;&#25968;&#25454;&#22122;&#22768;&#65292;&#24182;&#25913;&#21892;&#26368;&#21155;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21387;&#32553;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#65292;&#32780;&#19981;&#25913;&#21464;&#20854;&#21407;&#22987;&#32467;&#26500;&#25110;&#35757;&#32451;&#36807;&#31243;&#12290;&#23613;&#31649;&#20854;&#26377;&#25928;&#24615;&#21644;&#20415;&#21033;&#24615;&#65292;&#20294;&#22312;&#23384;&#22312;&#26576;&#20123;&#26497;&#31471;&#24773;&#20917;&#65288;&#22914;&#20998;&#24067;&#20559;&#31227;&#21644;&#25968;&#25454;&#22122;&#22768;&#65289;&#19979;&#65292;PTQ&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#39318;&#20808;&#22312;&#21508;&#31181;&#24120;&#29992;&#30340;PTQ&#26041;&#27861;&#19978;&#35843;&#26597;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#26088;&#22312;&#22238;&#31572;&#19982;&#26657;&#20934;&#38598;&#20998;&#24067;&#21464;&#21270;&#12289;&#26657;&#20934;&#33539;&#24335;&#36873;&#25321;&#20197;&#21450;&#25968;&#25454;&#22686;&#24378;&#25110;&#37319;&#26679;&#31574;&#30053;&#23545;PTQ&#21487;&#38752;&#24615;&#30340;&#24433;&#21709;&#30456;&#20851;&#30340;&#20960;&#20010;&#30740;&#31350;&#38382;&#39064;&#12290;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#21644;&#24120;&#29992;&#30340;PTQ&#33539;&#20363;&#19978;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;PTQ&#26041;&#27861;&#22312;&#26368;&#21155;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#34920;&#29616;&#19981;&#22815;&#21487;&#38752;&#65292;&#31361;&#20986;&#20102;&#38656;&#35201;&#26356;&#21152;&#24378;&#22823;&#30340;PTQ&#26041;&#27861;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#24320;&#21457;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#20998;&#24067;&#20559;&#31227;&#21644;&#25968;&#25454;&#22122;&#22768;&#65292;&#24182;&#25913;&#21892;PTQ&#26041;&#27861;&#26368;&#21155;&#24773;&#20917;&#19979;&#24615;&#33021;&#30340;PTQ&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20123;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization (PTQ) is a popular method for compressing deep neural networks (DNNs) without modifying their original architecture or training procedures. Despite its effectiveness and convenience, the reliability of PTQ methods in the presence of some extrem cases such as distribution shift and data noise remains largely unexplored. This paper first investigates this problem on various commonly-used PTQ methods. We aim to answer several research questions related to the influence of calibration set distribution variations, calibration paradigm selection, and data augmentation or sampling strategies on PTQ reliability. A systematic evaluation process is conducted across a wide range of tasks and commonly-used PTQ paradigms. The results show that most existing PTQ methods are not reliable enough in term of the worst-case group performance, highlighting the need for more robust methods. Our findings provide insights for developing PTQ methods that can effectively handle distr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#22522;&#20110;MAML&#30340;FL&#35774;&#35745;&#26469;&#26368;&#23567;&#21270;&#25972;&#20307;&#23398;&#20064;&#26102;&#38388;&#65292;&#20248;&#21270;FL&#36229;&#21442;&#25968;&#65288;&#20363;&#22914;&#37319;&#26679;&#25968;&#25454;&#22823;&#23567;&#21644;&#36890;&#20449;&#36718;&#25968;&#65289;&#21644;&#36164;&#28304;&#20998;&#37197;&#65288;&#20363;&#22914;&#21457;&#36865;&#21151;&#29575;&#65289;&#65292;&#21516;&#26102;&#32771;&#34385;&#27169;&#22411;&#31934;&#24230;&#21644;&#33021;&#37327;&#28040;&#32791;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.12999</link><description>&lt;p&gt;
&#31227;&#21160;&#36793;&#32536;&#32593;&#32476;&#20013;&#30340;&#33258;&#21160;&#32852;&#37030;&#23398;&#20064;&#8212;&#8212;&#24555;&#36895;&#36866;&#24212;&#21644;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
Automated Federated Learning in Mobile Edge Networks -- Fast Adaptation and Convergence. (arXiv:2303.12999v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#22522;&#20110;MAML&#30340;FL&#35774;&#35745;&#26469;&#26368;&#23567;&#21270;&#25972;&#20307;&#23398;&#20064;&#26102;&#38388;&#65292;&#20248;&#21270;FL&#36229;&#21442;&#25968;&#65288;&#20363;&#22914;&#37319;&#26679;&#25968;&#25454;&#22823;&#23567;&#21644;&#36890;&#20449;&#36718;&#25968;&#65289;&#21644;&#36164;&#28304;&#20998;&#37197;&#65288;&#20363;&#22914;&#21457;&#36865;&#21151;&#29575;&#65289;&#65292;&#21516;&#26102;&#32771;&#34385;&#27169;&#22411;&#31934;&#24230;&#21644;&#33021;&#37327;&#28040;&#32791;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#21487;&#20197;&#22312;&#31227;&#21160;&#36793;&#32536;&#32593;&#32476;&#20013;&#29992;&#20110;&#20998;&#24067;&#24335;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#26368;&#36817;&#65292;FL&#22312;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#65288;MAML&#65289;&#26694;&#26550;&#20013;&#24471;&#21040;&#35299;&#37322;&#65292;&#36825;&#20026;FL&#24102;&#26469;&#20102;&#24555;&#36895;&#36866;&#24212;&#21644;&#25910;&#25947;&#20110;&#24322;&#26500;&#25968;&#25454;&#38598;&#19978;&#30340;&#26174;&#30528;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20165;&#20165;&#23558;MAML&#21644;FL&#32467;&#21512;&#36215;&#26469;&#65292;&#27809;&#26377;&#26126;&#30830;&#35828;&#26126;MAML&#32473;FL&#24102;&#26469;&#22810;&#23569;&#22909;&#22788;&#20197;&#21450;&#22914;&#20309;&#22312;&#31227;&#21160;&#36793;&#32536;&#32593;&#32476;&#20013;&#26368;&#22823;&#21270;&#36825;&#31181;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) can be used in mobile edge networks to train machine learning models in a distributed manner. Recently, FL has been interpreted within a Model-Agnostic Meta-Learning (MAML) framework, which brings FL significant advantages in fast adaptation and convergence over heterogeneous datasets. However, existing research simply combines MAML and FL without explicitly addressing how much benefit MAML brings to FL and how to maximize such benefit over mobile edge networks. In this paper, we quantify the benefit from two aspects: optimizing FL hyperparameters (i.e., sampled data size and the number of communication rounds) and resource allocation (i.e., transmit power) in mobile edge networks. Specifically, we formulate the MAML-based FL design as an overall learning time minimization problem, under the constraints of model accuracy and energy consumption. Facilitated by the convergence analysis of MAML-based FL, we decompose the formulated problem and then solve it using a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#8220;&#21382;&#21490;&#23398;&#20064;&#65306;&#24102;&#26377;&#23398;&#20064;&#21382;&#21490;&#30340;&#23398;&#20064;&#27169;&#22411;&#8221;&#36825;&#20010;&#20027;&#39064;&#65292;&#28085;&#30422;&#21382;&#21490;&#31867;&#22411;&#12289;&#21151;&#33021;&#37096;&#20998;&#21644;&#23384;&#20648;&#24418;&#24335;&#19977;&#20010;&#26041;&#38754;&#65292;&#26159;&#39318;&#20010;&#31995;&#32479;&#30740;&#31350;&#21033;&#29992;&#21508;&#31181;&#21382;&#21490;&#32479;&#35745;&#25968;&#25454;&#36827;&#34892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#32508;&#36848;&#35770;&#25991;&#12290;</title><link>http://arxiv.org/abs/2303.12992</link><description>&lt;p&gt;
&#21382;&#21490;&#23398;&#20064;&#32508;&#36848;: &#24102;&#26377;&#23398;&#20064;&#21382;&#21490;&#30340;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Survey of Historical Learning: Learning Models with Learning History. (arXiv:2303.12992v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#8220;&#21382;&#21490;&#23398;&#20064;&#65306;&#24102;&#26377;&#23398;&#20064;&#21382;&#21490;&#30340;&#23398;&#20064;&#27169;&#22411;&#8221;&#36825;&#20010;&#20027;&#39064;&#65292;&#28085;&#30422;&#21382;&#21490;&#31867;&#22411;&#12289;&#21151;&#33021;&#37096;&#20998;&#21644;&#23384;&#20648;&#24418;&#24335;&#19977;&#20010;&#26041;&#38754;&#65292;&#26159;&#39318;&#20010;&#31995;&#32479;&#30740;&#31350;&#21033;&#29992;&#21508;&#31181;&#21382;&#21490;&#32479;&#35745;&#25968;&#25454;&#36827;&#34892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#32508;&#36848;&#35770;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#30340;&#30693;&#35782;&#28304;&#20110;&#26087;&#30340;&#30693;&#35782;&#12290;&#22312;&#35757;&#32451;&#21382;&#21490;&#35760;&#24405;&#20013;&#23384;&#20648;&#30340;&#21508;&#31181;&#31867;&#22411;&#30340;&#20803;&#32032;&#23545;&#20110;&#25913;&#21892;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23398;&#20064;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#8220;&#21382;&#21490;&#23398;&#20064;&#65306;&#24102;&#26377;&#23398;&#20064;&#21382;&#21490;&#30340;&#23398;&#20064;&#27169;&#22411;&#8221;&#36825;&#20010;&#20027;&#39064;&#65292;&#31995;&#32479;&#22320;&#30740;&#31350;&#20174;&#21382;&#21490;&#32479;&#35745;&#25968;&#25454;&#20013;&#36827;&#34892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#28085;&#30422;&#21382;&#21490;&#31867;&#22411;&#12289;&#21151;&#33021;&#37096;&#20998;&#21644;&#23384;&#20648;&#24418;&#24335;&#19977;&#20010;&#26041;&#38754;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#26159;&#39318;&#20010;&#31995;&#32479;&#30740;&#31350;&#21033;&#29992;&#21508;&#31181;&#21382;&#21490;&#32479;&#35745;&#25968;&#25454;&#36827;&#34892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#32508;&#36848;&#35770;&#25991;&#12290;&#25991;&#31456;&#36824;&#35752;&#35770;&#20102;&#19982;&#27492;&#30456;&#20851;&#30340;&#35805;&#39064;&#65292;&#22914;&#24490;&#29615;/&#35760;&#24518;&#32593;&#32476;&#12289;&#38598;&#25104;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#25351;&#20986;&#20102;&#36825;&#20010;&#20027;&#39064;&#30340;&#26410;&#26469;&#25361;&#25112;&#65292;&#24182;&#40723;&#21169;&#23398;&#26415;&#30028;&#22312;&#35774;&#35745;&#31639;&#27861;&#26102;&#35748;&#30495;&#24605;&#32771;&#21382;&#21490;&#23398;&#20064;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
New knowledge originates from the old. The various types of elements, deposited in the training history, are a large amount of wealth for improving learning deep models. In this survey, we comprehensively review and summarize the topic--``Historical Learning: Learning Models with Learning History'', which learns better neural models with the help of their learning history during its optimization, from three detailed aspects: Historical Type (what), Functional Part (where) and Storage Form (how). To our best knowledge, it is the first survey that systematically studies the methodologies which make use of various historical statistics when training deep neural networks. The discussions with related topics like recurrent/memory networks, ensemble learning, and reinforcement learning are demonstrated. We also expose future challenges of this topic and encourage the community to pay attention to the think of historical learning principles when designing algorithms. The paper list related to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CIPNN&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#25512;&#23548;&#20986;&#36830;&#32493;&#28508;&#22312;&#38543;&#26426;&#21464;&#37327;&#30340;&#35299;&#26512;&#35299;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;CIPAE&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#36890;&#36807;&#21487;&#35270;&#21270;&#28508;&#22312;&#38543;&#26426;&#21464;&#37327;&#30340;&#26041;&#27861;&#39564;&#35777;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12964</link><description>&lt;p&gt;
&#36830;&#32493;&#19981;&#23450;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Continuous Indeterminate Probability Neural Network. (arXiv:2303.12964v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CIPNN&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#25512;&#23548;&#20986;&#36830;&#32493;&#28508;&#22312;&#38543;&#26426;&#21464;&#37327;&#30340;&#35299;&#26512;&#35299;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;CIPAE&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#36890;&#36807;&#21487;&#35270;&#21270;&#28508;&#22312;&#38543;&#26426;&#21464;&#37327;&#30340;&#26041;&#27861;&#39564;&#35777;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;CIPNN&#65288;Continuous Indeterminate Probability Neural Network&#65289;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;IPNN&#65292;&#29992;&#20110;&#31163;&#25955;&#28508;&#22312;&#38543;&#26426;&#21464;&#37327;&#12290;&#30446;&#21069;&#65292;&#36830;&#32493;&#28508;&#22312;&#21464;&#37327;&#30340;&#21518;&#39564;&#34987;&#35748;&#20026;&#26159;&#19981;&#21487;&#35745;&#31639;&#30340;&#65292;&#20294;&#26159;IPNN&#25552;&#20986;&#20102;&#26032;&#30340;&#29702;&#35770;&#65292;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#26377;&#22235;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#36830;&#32493;&#28508;&#22312;&#38543;&#26426;&#21464;&#37327;&#30340;&#21518;&#39564;&#35745;&#31639;&#30340;&#35299;&#26512;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#20998;&#31867;&#27169;&#22411;&#65288;CIPNN&#65289;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#33258;&#32534;&#30721;&#22120;&#8212;&#8212;CIPAE&#65288;Continuous Indeterminate Probability Auto-Encoder&#65289;&#65292;&#20854;&#20013;&#35299;&#30721;&#22120;&#37096;&#20998;&#19981;&#26159;&#31070;&#32463;&#32593;&#32476;&#65292;&#32780;&#26159;&#31532;&#19968;&#27425;&#20351;&#29992;&#20840;&#27010;&#29575;&#25512;&#29702;&#27169;&#22411;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35270;&#21270;&#28508;&#22312;&#38543;&#26426;&#21464;&#37327;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;N&#32500;&#28508;&#22312;&#21464;&#37327;&#20043;&#19968;&#20316;&#20026;&#35299;&#30721;&#22120;&#26469;&#37325;&#24314;&#36755;&#20837;&#22270;&#20687;&#65292;&#21363;&#20351;&#26159;&#20998;&#31867;&#20219;&#21153;&#20063;&#33021;&#36798;&#21040;&#25928;&#26524;&#65292;&#36825;&#26679;&#65292;&#25105;&#20204;&#21487;&#20197;&#30475;&#21040;&#27599;&#20010;&#28508;&#22312;&#21464;&#37327;&#20195;&#34920;&#20160;&#20040;&#12290;&#31532;&#22235;&#65292;&#25105;&#20204;&#36890;&#36807;MNIST&#21644;Fashion-MNIST&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a general model called CIPNN - Continuous Indeterminate Probability Neural Network, and this model is based on IPNN, which is used for discrete latent random variables. Currently, posterior of continuous latent variables is regarded as intractable, with the new theory proposed by IPNN this problem can be solved. Our contributions are Four-fold. First, we derive the analytical solution of the posterior calculation of continuous latent random variables and propose a general classification model (CIPNN). Second, we propose a general auto-encoder called CIPAE - Continuous Indeterminate Probability Auto-Encoder, the decoder part is not a neural network and uses a fully probabilistic inference model for the first time. Third, we propose a new method to visualize the latent random variables, we use one of N dimensional latent variables as a decoder to reconstruct the input image, which can work even for classification tasks, in this way, we can see what each latent varia
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#39044;&#27979;&#24863;&#30693;&#30340;&#27169;&#22411;&#39537;&#21160;LSTM&#26469;&#35299;&#20915;&#31354;&#27668;&#36136;&#37327;&#39044;&#27979;&#20013;&#24120;&#35265;&#30340;&#20559;&#24046;&#20462;&#27491;&#38382;&#39064;&#65292;&#27492;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#26497;&#31471;&#31354;&#27668;&#36136;&#37327;&#20107;&#20214;&#12290;</title><link>http://arxiv.org/abs/2303.12963</link><description>&lt;p&gt;
&#39044;&#27979;&#24863;&#30693;&#30340;&#27169;&#22411;&#39537;&#21160;LSTM
&lt;/p&gt;
&lt;p&gt;
Forecast-Aware Model Driven LSTM. (arXiv:2303.12963v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#39044;&#27979;&#24863;&#30693;&#30340;&#27169;&#22411;&#39537;&#21160;LSTM&#26469;&#35299;&#20915;&#31354;&#27668;&#36136;&#37327;&#39044;&#27979;&#20013;&#24120;&#35265;&#30340;&#20559;&#24046;&#20462;&#27491;&#38382;&#39064;&#65292;&#27492;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#26497;&#31471;&#31354;&#27668;&#36136;&#37327;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24694;&#21155;&#30340;&#31354;&#27668;&#36136;&#37327;&#20250;&#23545;&#20154;&#31867;&#20581;&#24247;&#20135;&#29983;&#20005;&#37325;&#24433;&#21709;&#12290;&#30001;&#20110;&#26497;&#31471;&#22825;&#27668;&#20107;&#20214;&#65288;&#22914;&#37326;&#28779;&#21644;&#28909;&#28010;&#65289;&#22686;&#22810;&#65292;NOAA&#30340;&#31354;&#27668;&#36136;&#37327;&#39044;&#27979;&#21463;&#21040;&#20102;&#25361;&#25112;&#12290;&#20256;&#32479;&#26041;&#27861;&#29992;&#20110;&#25913;&#27491;&#27169;&#22411;&#20559;&#24046;&#26102;&#20570;&#20102;&#32447;&#24615;&#20551;&#35774;&#21644;&#22522;&#30784;&#20998;&#24067;&#20551;&#35774;&#65292;&#24403;&#20986;&#29616;&#26497;&#31471;&#31354;&#27668;&#36136;&#37327;&#20107;&#20214;&#26102;&#36825;&#20123;&#26041;&#27861;&#23481;&#26131;&#20135;&#29983;&#20559;&#24046;&#20462;&#27491;&#36807;&#24230;&#25110;&#32773;&#19981;&#36275;&#12290;&#28145;&#24230;&#23398;&#20064;&#22312;&#38750;&#32447;&#24615;&#38382;&#39064;&#20013;&#24191;&#27867;&#24212;&#29992;&#65292;&#36890;&#36807;&#23398;&#20064;&#36890;&#29992;&#24615;&#21487;&#20197;&#24212;&#23545;&#26497;&#31471;&#31354;&#27668;&#36136;&#37327;&#20107;&#20214;&#39044;&#27979;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#24403;&#23384;&#22312;&#24322;&#24120;&#31354;&#27668;&#36136;&#37327;&#20107;&#20214;&#26102;&#65292;&#20351;&#29992;&#21333;&#20010;&#32593;&#32476;&#36827;&#34892;&#26410;&#26469;&#39044;&#27979;&#30340;&#26631;&#20934;&#28145;&#24230;&#32593;&#32476;&#26041;&#27861;&#19981;&#19968;&#23450;&#24635;&#26159;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Poor air quality can have a significant impact on human health. The National Oceanic and Atmospheric Administration (NOAA) air quality forecasting guidance is challenged by the increasing presence of extreme air quality events due to extreme weather events such as wild fires and heatwaves. These extreme air quality events further affect human health. Traditional methods used to correct model bias make assumptions about linearity and the underlying distribution. Extreme air quality events tend to occur without a strong signal leading up to the event and this behavior tends to cause existing methods to either under or over compensate for the bias. Deep learning holds promise for air quality forecasting in the presence of extreme air quality events due to its ability to generalize and learn nonlinear problems. However, in the presence of these anomalous air quality events, standard deep network approaches that use a single network for generalizing to future forecasts, may not always provi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22238;&#39038;&#20102;&#36229;&#36807;80&#20010;&#22312;&#38750;&#25104;&#20687; EMR &#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22823;&#22810;&#33539;&#22260;&#26377;&#38480;&#12289;&#35757;&#32451;&#38598;&#26377;&#38480;&#65292;&#19988;&#35780;&#20272;&#25351;&#26631;&#26410;&#23545;&#20854;&#23545;&#21307;&#30103;&#31995;&#32479;&#36129;&#29486;&#25552;&#20379;&#26377;&#24847;&#20041;&#35265;&#35299;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#25509;&#36817;&#20110;&#21307;&#30103;&#20445;&#20581;&#37325;&#35201;&#25351;&#26631;&#30340;&#21307;&#30103;&#22522;&#30784;&#27169;&#22411;&#25928;&#30410;&#35780;&#20272;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2303.12961</link><description>&lt;p&gt;
&#20020;&#24202;&#22522;&#30784;&#27169;&#22411;&#30340;&#19981;&#31283;&#23450;&#22522;&#30784;&#65306;&#38024;&#23545; EMR &#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#30784;&#27169;&#22411;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
The Shaky Foundations of Clinical Foundation Models: A Survey of Large Language Models and Foundation Models for EMRs. (arXiv:2303.12961v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22238;&#39038;&#20102;&#36229;&#36807;80&#20010;&#22312;&#38750;&#25104;&#20687; EMR &#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22823;&#22810;&#33539;&#22260;&#26377;&#38480;&#12289;&#35757;&#32451;&#38598;&#26377;&#38480;&#65292;&#19988;&#35780;&#20272;&#25351;&#26631;&#26410;&#23545;&#20854;&#23545;&#21307;&#30103;&#31995;&#32479;&#36129;&#29486;&#25552;&#20379;&#26377;&#24847;&#20041;&#35265;&#35299;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#25509;&#36817;&#20110;&#21307;&#30103;&#20445;&#20581;&#37325;&#35201;&#25351;&#26631;&#30340;&#21307;&#30103;&#22522;&#30784;&#27169;&#22411;&#25928;&#30410;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#20284; ChatGPT &#21644; AlphaFold &#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#25104;&#21151;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20110;&#26500;&#24314;&#31867;&#20284;&#27169;&#22411;&#20197;&#25913;&#21892; EMR&#65288;&#30005;&#23376;&#30149;&#21382;&#65289;&#20197;&#25552;&#39640;&#24739;&#32773;&#25252;&#29702;&#21644;&#21307;&#38498;&#36816;&#33829;&#30340;&#26497;&#22823;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#28818;&#20316;&#25513;&#30422;&#20102;&#25105;&#20204;&#23545;&#36825;&#20123;&#27169;&#22411;&#33021;&#21147;&#30340;&#20851;&#38190;&#32570;&#22833;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#36229;&#36807;80&#20010;&#22312;&#38750;&#25104;&#20687; EMR &#25968;&#25454;&#65288;&#21363;&#20020;&#24202;&#25991;&#26412;&#21644;/&#25110;&#32467;&#26500;&#21270;&#25968;&#25454;&#65289;&#19978;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#26469;&#35828;&#26126;&#23427;&#20204;&#30340;&#20307;&#31995;&#32467;&#26500;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#28508;&#22312;&#29992;&#20363;&#12290;&#25105;&#20204;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#26159;&#22312;&#23567;&#22411;&#12289;&#33539;&#22260;&#26377;&#38480;&#30340;&#20020;&#24202;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;MIMIC-III&#65289;&#25110;&#24191;&#27867;&#30340;&#20844;&#20849;&#29983;&#29289;&#21307;&#23398;&#35821;&#26009;&#24211;&#65288;&#20363;&#22914;PubMed&#65289;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#24182;&#19988;&#22312;&#19981;&#25552;&#20379;&#23545;&#20854;&#23545;&#21307;&#30103;&#31995;&#32479;&#26377;&#29992;&#22788;&#30340;&#26377;&#24847;&#20041;&#35265;&#35299;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#25509;&#36817;&#20110;&#21307;&#30103;&#20445;&#20581;&#37325;&#35201;&#25351;&#26631;&#30340;&#21307;&#30103;&#22522;&#30784;&#27169;&#22411;&#25928;&#30410;&#30340;&#25913;&#36827;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The successes of foundation models such as ChatGPT and AlphaFold have spurred significant interest in building similar models for electronic medical records (EMRs) to improve patient care and hospital operations. However, recent hype has obscured critical gaps in our understanding of these models' capabilities. We review over 80 foundation models trained on non-imaging EMR data (i.e. clinical text and/or structured data) and create a taxonomy delineating their architectures, training data, and potential use cases. We find that most models are trained on small, narrowly-scoped clinical datasets (e.g. MIMIC-III) or broad, public biomedical corpora (e.g. PubMed) and are evaluated on tasks that do not provide meaningful insights on their usefulness to health systems. In light of these findings, we propose an improved evaluation framework for measuring the benefits of clinical foundation models that is more closely grounded to metrics that matter in healthcare.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#27493;&#20943;&#23569;&#20449;&#24687;&#29942;&#39048;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#20351;&#29992;&#21435;&#32416;&#32544;&#19981;&#21464;&#21464;&#25442;&#26469;&#24179;&#34913;&#21435;&#32416;&#32544;&#21644;&#37325;&#26500;&#20445;&#30495;&#24230;&#65292;&#36991;&#20813;&#20449;&#24687;&#25193;&#25955;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12959</link><description>&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20013;&#36880;&#27493;&#20943;&#23569;&#20449;&#24687;&#29942;&#39048;&#30340;&#21435;&#32416;&#32544;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Variantional autoencoder with decremental information bottleneck for disentanglement. (arXiv:2303.12959v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#27493;&#20943;&#23569;&#20449;&#24687;&#29942;&#39048;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#20351;&#29992;&#21435;&#32416;&#32544;&#19981;&#21464;&#21464;&#25442;&#26469;&#24179;&#34913;&#21435;&#32416;&#32544;&#21644;&#37325;&#26500;&#20445;&#30495;&#24230;&#65292;&#36991;&#20813;&#20449;&#24687;&#25193;&#25955;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20013;&#21435;&#32416;&#32544;&#23398;&#20064;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#22312;&#26435;&#34913;&#21435;&#32416;&#32544;&#21644;&#37325;&#26500;&#20445;&#30495;&#24230;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#20043;&#21069;&#20165;&#22312;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#20013;&#36827;&#34892;&#30340;&#36880;&#27493;&#26041;&#27861;&#26080;&#27861;&#21516;&#26102;&#20248;&#21270;&#36825;&#20004;&#20010;&#30446;&#26631;&#65292;&#22240;&#27492;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25193;&#23637;&#20102;&#20449;&#24687;&#29942;&#39048;&#65292;&#20197;&#20174;&#21435;&#32416;&#32544;&#21040;&#37325;&#26500;&#36827;&#34892;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#29942;&#39048;&#20250;&#22833;&#21435;&#21435;&#32416;&#32544;&#30340;&#32422;&#26463;&#65292;&#23548;&#33268;&#20449;&#24687;&#25193;&#25955;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36880;&#27493;&#20943;&#23569;&#20449;&#24687;&#29942;&#39048;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#20351;&#29992;&#21435;&#32416;&#32544;&#19981;&#21464;&#21464;&#25442;&#26469;&#20248;&#21270;&#19981;&#21516;&#23618;&#30340;&#22810;&#20010;&#30446;&#26631;&#65292;&#31216;&#20026;DeVAE&#12290;&#36890;&#36807;&#36880;&#28176;&#20943;&#23567;&#19981;&#21516;&#28508;&#22312;&#31354;&#38388;&#30340;&#20449;&#24687;&#29942;&#39048;&#65292;DeVAE &#24179;&#34913;&#20102;&#21435;&#32416;&#32544;&#21644;&#37325;&#26500;&#20445;&#30495;&#24230;&#12290;&#30001;&#20110;&#20855;&#26377;&#22810;&#20010;&#28508;&#22312;&#31354;&#38388;&#65292;DeVAE &#20801;&#35768;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#30446;&#26631;&#65292;&#20197;&#22312;&#20445;&#25345;&#21435;&#32416;&#32544;&#32422;&#26463;&#30340;&#21516;&#26102;&#20248;&#21270;&#37325;&#26500;&#65292;&#36991;&#20813;&#20449;&#24687;&#25193;&#25955;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
One major challenge of disentanglement learning with variational autoencoders is the trade-off between disentanglement and reconstruction fidelity. Previous incremental methods with only on latent space cannot optimize these two targets simultaneously, so they expand the Information Bottleneck while training to {optimize from disentanglement to reconstruction. However, a large bottleneck will lose the constraint of disentanglement, causing the information diffusion problem. To tackle this issue, we present a novel decremental variational autoencoder with disentanglement-invariant transformations to optimize multiple objectives in different layers, termed DeVAE, for balancing disentanglement and reconstruction fidelity by decreasing the information bottleneck of diverse latent spaces gradually. Benefiting from the multiple latent spaces, DeVAE allows simultaneous optimization of multiple objectives to optimize reconstruction while keeping the constraint of disentanglement, avoiding info
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;MDP&#20998;&#35299;&#20026;&#22806;&#29983;&#21644;&#20869;&#29983;&#20004;&#20010;&#37096;&#20998;&#65292;&#20248;&#21270;&#20869;&#29983;&#22870;&#21169;&#65292;&#22312;&#29366;&#24577;&#31354;&#38388;&#30340;&#20869;&#29983;&#21644;&#22806;&#29983;&#29366;&#24577;&#31354;&#38388;&#27809;&#26377;&#20107;&#20808;&#32473;&#20986;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20986;&#20102;&#27491;&#30830;&#30340;&#31639;&#27861;&#36827;&#34892;&#33258;&#21160;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.12957</link><description>&lt;p&gt;
&#20855;&#26377;&#22806;&#37096;&#29366;&#24577;&#21644;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Exogenous States and Rewards. (arXiv:2303.12957v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12957
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;MDP&#20998;&#35299;&#20026;&#22806;&#29983;&#21644;&#20869;&#29983;&#20004;&#20010;&#37096;&#20998;&#65292;&#20248;&#21270;&#20869;&#29983;&#22870;&#21169;&#65292;&#22312;&#29366;&#24577;&#31354;&#38388;&#30340;&#20869;&#29983;&#21644;&#22806;&#29983;&#29366;&#24577;&#31354;&#38388;&#27809;&#26377;&#20107;&#20808;&#32473;&#20986;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20986;&#20102;&#27491;&#30830;&#30340;&#31639;&#27861;&#36827;&#34892;&#33258;&#21160;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22806;&#37096;&#29366;&#24577;&#21464;&#37327;&#21644;&#22870;&#21169;&#20250;&#36890;&#36807;&#21521;&#22870;&#21169;&#20449;&#21495;&#27880;&#20837;&#19981;&#21487;&#25511;&#30340;&#21464;&#21270;&#32780;&#20943;&#24930;&#24378;&#21270;&#23398;&#20064;&#30340;&#36895;&#24230;&#12290;&#26412;&#25991;&#23545;&#22806;&#37096;&#29366;&#24577;&#21464;&#37327;&#21644;&#22870;&#21169;&#36827;&#34892;&#20102;&#27491;&#24335;&#21270;&#65292;&#24182;&#34920;&#26126;&#22914;&#26524;&#22870;&#21169;&#20989;&#25968;&#21152;&#27861;&#20998;&#35299;&#25104;&#20869;&#29983;&#21644;&#22806;&#29983;&#20004;&#20010;&#37096;&#20998;&#65292;MDP&#21487;&#20197;&#20998;&#35299;&#20026;&#19968;&#20010;&#22806;&#29983;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#36807;&#31243;&#65288;&#22522;&#20110;&#22806;&#37096;&#22870;&#21169;&#65289;&#21644;&#19968;&#20010;&#20869;&#29983;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;&#20248;&#21270;&#20869;&#29983;&#22870;&#21169;&#65289;&#12290;&#20869;&#29983;MDP&#30340;&#20219;&#20309;&#26368;&#20248;&#31574;&#30053;&#20063;&#26159;&#21407;&#22987;MDP&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#20294;&#30001;&#20110;&#20869;&#29983;&#22870;&#21169;&#36890;&#24120;&#20855;&#26377;&#38477;&#20302;&#30340;&#26041;&#24046;&#65292;&#22240;&#27492;&#20869;&#29983;MDP&#26356;&#23481;&#26131;&#27714;&#35299;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#29366;&#24577;&#31354;&#38388;&#20998;&#35299;&#20026;&#20869;&#22806;&#29983;&#29366;&#24577;&#31354;&#38388;&#30340;&#24773;&#20917;&#65292;&#32780;&#36825;&#31181;&#29366;&#24577;&#31354;&#38388;&#20998;&#35299;&#24182;&#27809;&#26377;&#32473;&#20986;&#65292;&#32780;&#26159;&#24517;&#39035;&#21457;&#29616;&#12290;&#26412;&#25991;&#20171;&#32461;&#24182;&#35777;&#26126;&#20102;&#22312;&#32447;&#24615;&#32452;&#21512;&#19979;&#21457;&#29616;&#20869;&#29983;&#21644;&#22806;&#29983;&#29366;&#24577;&#31354;&#38388;&#30340;&#31639;&#27861;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exogenous state variables and rewards can slow reinforcement learning by injecting uncontrolled variation into the reward signal. This paper formalizes exogenous state variables and rewards and shows that if the reward function decomposes additively into endogenous and exogenous components, the MDP can be decomposed into an exogenous Markov Reward Process (based on the exogenous reward) and an endogenous Markov Decision Process (optimizing the endogenous reward). Any optimal policy for the endogenous MDP is also an optimal policy for the original MDP, but because the endogenous reward typically has reduced variance, the endogenous MDP is easier to solve. We study settings where the decomposition of the state space into exogenous and endogenous state spaces is not given but must be discovered. The paper introduces and proves correctness of algorithms for discovering the exogenous and endogenous subspaces of the state space when they are mixed through linear combination. These algorithms
&lt;/p&gt;</description></item><item><title>TSI-GAN&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#22797;&#26434;&#27169;&#24335;&#65292;&#24182;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.12952</link><description>&lt;p&gt;
TSI-GAN: &#20351;&#29992;&#21367;&#31215;&#24490;&#29615;&#19968;&#33268;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
TSI-GAN: Unsupervised Time Series Anomaly Detection using Convolutional Cycle-Consistent Generative Adversarial Networks. (arXiv:2303.12952v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12952
&lt;/p&gt;
&lt;p&gt;
TSI-GAN&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#22797;&#26434;&#27169;&#24335;&#65292;&#24182;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#24191;&#27867;&#24212;&#29992;&#20110;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#12289;&#33258;&#21160;&#39550;&#39542;&#12289;&#21307;&#23398;&#35786;&#26029;&#12289;&#20449;&#29992;&#21345;&#27450;&#35784;&#31561;&#12290;&#28982;&#32780;&#65292;&#20173;&#23384;&#22312;&#19968;&#20123;&#20851;&#38190;&#25361;&#25112;&#65292;&#20363;&#22914;&#32570;&#20047;&#30495;&#23454;&#26631;&#31614;&#12289;&#23384;&#22312;&#22797;&#26434;&#30340;&#26102;&#38388;&#27169;&#24335;&#21644;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27867;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;TSI-GAN&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#33258;&#21160;&#23398;&#20064;&#22797;&#26434;&#30340;&#26102;&#38388;&#27169;&#24335;&#24182;&#19988;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#21363;&#19981;&#38656;&#35201;&#36873;&#25321;&#29305;&#23450;&#20110;&#25968;&#25454;&#38598;&#30340;&#21442;&#25968;&#12289;&#23545;&#24213;&#23618;&#25968;&#25454;&#20570;&#20986;&#32479;&#35745;&#20551;&#35774;&#25110;&#26356;&#25913;&#27169;&#22411;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection is widely used in network intrusion detection, autonomous driving, medical diagnosis, credit card frauds, etc. However, several key challenges remain open, such as lack of ground truth labels, presence of complex temporal patterns, and generalizing over different datasets. This paper proposes TSI-GAN, an unsupervised anomaly detection model for time-series that can learn complex temporal patterns automatically and generalize well, i.e., no need for choosing dataset-specific parameters, making statistical assumptions about underlying data, or changing model architectures. To achieve these goals, we convert each input time-series into a sequence of 2D images using two encoding techniques with the intent of capturing temporal patterns and various types of deviance. Moreover, we design a reconstructive GAN that uses convolutional layers in an encoder-decoder network and employs cycle-consistency loss during training to ensure that inverse mappings are accurate as well. In
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; FTSO &#26041;&#27861;&#65292;&#23558;&#25972;&#20010;&#26550;&#26500;&#25628;&#32034;&#20998;&#20026;&#20004;&#20010;&#23376;&#27493;&#39588;&#65306;&#25299;&#25169;&#25628;&#32034;&#21644;&#31639;&#23376;&#25628;&#32034;&#12290;FTSO&#26041;&#27861;&#22823;&#22823;&#38477;&#20302;&#20102; NAS &#25628;&#32034;&#30340;&#26102;&#38388;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#25628;&#32034;&#21040;&#30340;&#26550;&#26500;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;ImageNet&#21644;CIFAR10&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#20248;&#31168;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.12948</link><description>&lt;p&gt;
FTSO: &#36890;&#36807;&#31532;&#19968;&#25299;&#25169;&#31532;&#20108;&#31639;&#23376;&#23454;&#29616;&#39640;&#25928;&#30340; NAS
&lt;/p&gt;
&lt;p&gt;
FTSO: Effective NAS via First Topology Second Operator. (arXiv:2303.12948v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; FTSO &#26041;&#27861;&#65292;&#23558;&#25972;&#20010;&#26550;&#26500;&#25628;&#32034;&#20998;&#20026;&#20004;&#20010;&#23376;&#27493;&#39588;&#65306;&#25299;&#25169;&#25628;&#32034;&#21644;&#31639;&#23376;&#25628;&#32034;&#12290;FTSO&#26041;&#27861;&#22823;&#22823;&#38477;&#20302;&#20102; NAS &#25628;&#32034;&#30340;&#26102;&#38388;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#25628;&#32034;&#21040;&#30340;&#26550;&#26500;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;ImageNet&#21644;CIFAR10&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#20248;&#31168;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#19968;&#27425;&#24615;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26041;&#27861;&#24517;&#39035;&#22312;&#24040;&#22823;&#30340;&#36229;&#32423;&#32593;&#32476;&#19978;&#36827;&#34892;&#25628;&#32034;&#65292;&#36825;&#23548;&#33268;&#20102;&#24040;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#20102;&#38477;&#20302;&#36825;&#31181;&#25104;&#26412;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#31216;&#20026; FTSO&#65292;&#23558;&#25972;&#20010;&#26550;&#26500;&#25628;&#32034;&#20998;&#20026;&#20004;&#20010;&#23376;&#27493;&#39588;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#31532;&#19968;&#27493;&#20013;&#65292;&#25105;&#20204;&#20165;&#25628;&#32034;&#25299;&#25169;&#65292;&#32780;&#22312;&#31532;&#20108;&#27493;&#20013;&#65292;&#25105;&#20204;&#25628;&#32034;&#31639;&#23376;&#12290; FTSO &#19981;&#20165;&#23558; NAS &#30340;&#25628;&#32034;&#26102;&#38388;&#20174;&#20960;&#22825;&#32553;&#30701;&#21040; 0.68 &#31186;&#65292;&#32780;&#19988;&#26174;&#30528;&#25552;&#39640;&#20102;&#25628;&#32034;&#21040;&#30340;&#26550;&#26500;&#30340;&#20934;&#30830;&#24615;&#12290; &#22312; ImageNet &#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312; 18 &#31186;&#20869;&#65292;FTSO &#21487;&#20197;&#23454;&#29616; 76.4&#65285; &#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#27604; SOTA&#65288;PC-DARTS&#65289;&#39640; 1.5&#65285;&#12290;&#27492;&#22806;&#65292;&#22312; CIFAR10 &#19978;&#36827;&#34892;&#25628;&#32034;&#26102;&#65292;FTSO &#21487;&#36798;&#21040; 97.77&#65285;&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#65292;&#27604; SOTA &#39640; 0.27&#65285;&#65292;&#24182;&#19988;&#20960;&#20046;&#21487;&#20197;&#33410;&#30465; 100&#65285;&#65288;99.8&#65285;&#65289;&#30340;&#25628;&#32034;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing one-shot neural architecture search (NAS) methods have to conduct a search over a giant super-net, which leads to the huge computational cost. To reduce such cost, in this paper, we propose a method, called FTSO, to divide the whole architecture search into two sub-steps. Specifically, in the first step, we only search for the topology, and in the second step, we search for the operators. FTSO not only reduces NAS's search time from days to 0.68 seconds, but also significantly improves the found architecture's accuracy. Our extensive experiments on ImageNet show that within 18 seconds, FTSO can achieve a 76.4% testing accuracy, 1.5% higher than the SOTA, PC-DARTS. In addition, FTSO can reach a 97.77% testing accuracy, 0.27% higher than the SOTA, with nearly 100% (99.8%) search time saved, when searching on CIFAR10.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;5G&#26080;&#20154;&#26426;&#22330;&#26223;&#20013;&#22522;&#20110;&#28145;&#24230;&#27880;&#24847;&#21147;&#35782;&#21035;&#30340;&#25915;&#20987;&#35782;&#21035;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;SINR&#21644;RSSI&#21442;&#25968;&#35782;&#21035;&#30452;&#23556;&#35270;&#32447;&#65288;LoS&#65289;&#12289;&#38750;&#30452;&#23556;&#35270;&#32447;&#65288;NLoS&#65289;&#20197;&#21450;&#20004;&#31181;&#26465;&#20214;&#30340;&#27010;&#29575;&#24615;&#32452;&#21512;&#19979;&#30340;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2303.12947</link><description>&lt;p&gt;
5G&#26080;&#20154;&#26426;&#22330;&#26223;&#20013;&#30340;&#28145;&#24230;&#27880;&#24847;&#21147;&#35782;&#21035;&#65306;&#26032;&#22411;&#26550;&#26500;&#21644;&#31471;&#21040;&#31471;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Deep Attention Recognition for Attack Identification in 5G UAV scenarios: Novel Architecture and End-to-End Evaluation. (arXiv:2303.12947v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12947
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;5G&#26080;&#20154;&#26426;&#22330;&#26223;&#20013;&#22522;&#20110;&#28145;&#24230;&#27880;&#24847;&#21147;&#35782;&#21035;&#30340;&#25915;&#20987;&#35782;&#21035;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;SINR&#21644;RSSI&#21442;&#25968;&#35782;&#21035;&#30452;&#23556;&#35270;&#32447;&#65288;LoS&#65289;&#12289;&#38750;&#30452;&#23556;&#35270;&#32447;&#65288;NLoS&#65289;&#20197;&#21450;&#20004;&#31181;&#26465;&#20214;&#30340;&#27010;&#29575;&#24615;&#32452;&#21512;&#19979;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;5G&#26694;&#26550;&#20855;&#26377;&#24378;&#22823;&#30340;&#23433;&#20840;&#21151;&#33021;&#65292;&#25915;&#20987;&#32773;&#20173;&#23558;&#21457;&#29616;&#26041;&#27861;&#26469;&#30772;&#22351;5G&#26080;&#20154;&#26426;&#65288;UAV&#65289;&#25805;&#20316;&#65292;&#24182;&#38477;&#20302;&#31354;&#22320;&#65288;A2G&#65289;&#38142;&#36335;&#20013;&#30340;UAV&#25511;&#21046;&#36890;&#20449;&#24615;&#33021;&#12290;&#22312;&#20551;&#35774;5G UAV&#36890;&#20449;&#22522;&#30784;&#35774;&#26045;&#27704;&#36828;&#19981;&#20250;&#23436;&#20840;&#23433;&#20840;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28145;&#24230;&#27880;&#24847;&#21147;&#35782;&#21035;&#65288;DAtR&#65289;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#21487;&#20197;&#22522;&#20110;&#23884;&#20837;&#32463;&#35748;&#35777;&#30340;UAV&#20013;&#30340;&#23567;&#22411;&#28145;&#24230;&#32593;&#32476;&#26469;&#35782;&#21035;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#26041;&#26696;&#20351;&#29992;&#20004;&#20010;&#21487;&#35266;&#27979;&#21442;&#25968;&#65306;&#20449;&#24178;&#22122;&#27604;&#65288;SINR&#65289;&#21644;&#21442;&#32771;&#20449;&#21495;&#25509;&#25910;&#21151;&#29575;&#65288;RSSI&#65289;&#65292;&#20197;&#35782;&#21035;&#20855;&#26377;&#30452;&#23556;&#35270;&#32447;&#65288;LoS&#65289;&#12289;&#38750;&#30452;&#23556;&#35270;&#32447;&#65288;NLoS&#65289;&#21644;&#20004;&#31181;&#26465;&#20214;&#30340;&#27010;&#29575;&#24615;&#32452;&#21512;&#30340;&#25915;&#20987;&#12290;&#22312;&#27979;&#35797;&#30340;&#22330;&#26223;&#20013;&#65292;&#25915;&#20987;&#32773;&#20301;&#20110;&#38543;&#26426;&#20301;&#32622;&#65292;&#20854;&#21151;&#29575;&#22312;&#27599;&#20010;&#27169;&#25311;&#20013;&#37117;&#26377;&#25152;&#25913;&#21464;&#12290;&#27492;&#22806;&#65292;&#22320;&#38754;&#29992;&#25143;&#20063;&#34987;&#21253;&#21547;&#22312;&#32593;&#32476;&#20013;&#65292;&#20197;&#23545;&#25915;&#20987;&#26816;&#27979;&#26045;&#21152;&#39069;&#22806;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the robust security features inherent in the 5G framework, attackers will still discover ways to disrupt 5G unmanned aerial vehicle (UAV) operations and decrease UAV control communication performance in Air-to-Ground (A2G) links. Operating under the assumption that the 5G UAV communications infrastructure will never be entirely secure, we propose Deep Attention Recognition (DAtR) as a solution to identify attacks based on a small deep network embedded in authenticated UAVs. Our proposed solution uses two observable parameters: the Signal-to-Interference-plus-Noise Ratio (SINR) and the Reference Signal Received Power (RSSI) to recognize attacks under Line-of-Sight (LoS), Non-Line-of-Sight (NLoS), and a probabilistic combination of the two conditions. In the tested scenarios, a number of attackers are located in random positions, while their power is varied in each simulation. Moreover, terrestrial users are included in the network to impose additional complexity on attack detect
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#23558;&#29305;&#23450;&#20248;&#21270;&#38382;&#39064;&#19982;&#22810;&#26102;&#38388;Hamilton-Jacobi PDEs&#32852;&#31995;&#36215;&#26469;&#65292;&#34920;&#26126;&#24403;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#26102;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#23545;&#24212;&#30340;&#22810;&#26102;&#38388;HJ PDEs&#21644;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#12290;&#21033;&#29992;&#36825;&#31181;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2303.12928</link><description>&lt;p&gt;
&#21033;&#29992;&#22810;&#26102;&#38388; Hamilton-Jacobi PDE &#35299;&#20915;&#19968;&#20123;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Leveraging Multi-time Hamilton-Jacobi PDEs for Certain Scientific Machine Learning Problems. (arXiv:2303.12928v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#23558;&#29305;&#23450;&#20248;&#21270;&#38382;&#39064;&#19982;&#22810;&#26102;&#38388;Hamilton-Jacobi PDEs&#32852;&#31995;&#36215;&#26469;&#65292;&#34920;&#26126;&#24403;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#26102;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#23545;&#24212;&#30340;&#22810;&#26102;&#38388;HJ PDEs&#21644;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#12290;&#21033;&#29992;&#36825;&#31181;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Hamilton-Jacobi &#20559;&#24494;&#20998;&#26041;&#31243;(HJ PDEs)&#19982;&#24191;&#27867;&#39046;&#22495;&#65292;&#22914;&#26368;&#20248;&#25511;&#21046;&#12289;&#24494;&#20998;&#28216;&#25103;&#21644;&#25104;&#20687;&#31185;&#23398;&#26377;&#30528;&#28145;&#21051;&#30340;&#32852;&#31995;&#12290;&#36890;&#36807;&#23558;&#26102;&#38388;&#21464;&#37327;&#35270;&#20026;&#26356;&#39640;&#32500;&#30340;&#37327;&#65292;HJ PDEs &#21487;&#20197;&#25193;&#23637;&#21040;&#22810;&#26102;&#38388;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#22312;&#29305;&#23450;&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#38382;&#39064;&#19982;&#22810;&#26102;&#38388;Hopf&#20844;&#24335;&#20043;&#38388;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#32852;&#31995;&#65292;&#35813;&#20844;&#24335;&#23545;&#24212;&#20110;&#26576;&#20123;&#22810;&#26102;&#38388; HJ PDEs &#30340;&#35299;&#30340;&#34920;&#31034;&#12290;&#36890;&#36807;&#36825;&#31181;&#32852;&#31995;&#65292;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#24403;&#25105;&#20204;&#35299;&#20915;&#36825;&#20123;&#23398;&#20064;&#38382;&#39064;&#26102;&#65292;&#25105;&#20204;&#20063;&#35299;&#20915;&#20102;&#19968;&#20010;&#22810;&#26102;&#38388; HJ PDE &#21644;&#30456;&#24212;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#26576;&#20123;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#30340;&#35757;&#32451;&#36807;&#31243;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20316;&#20026;&#36825;&#31181;&#32852;&#31995;&#30340;&#31532;&#19968;&#20010;&#25506;&#32034;&#65292;&#25105;&#20204;&#21457;&#23637;&#20102;&#27491;&#21017;&#21270;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#19982;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120; (LQR) &#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#30340;&#29702;&#35770;&#26694;&#26550;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hamilton-Jacobi partial differential equations (HJ PDEs) have deep connections with a wide range of fields, including optimal control, differential games, and imaging sciences. By considering the time variable to be a higher dimensional quantity, HJ PDEs can be extended to the multi-time case. In this paper, we establish a novel theoretical connection between specific optimization problems arising in machine learning and the multi-time Hopf formula, which corresponds to a representation of the solution to certain multi-time HJ PDEs. Through this connection, we increase the interpretability of the training process of certain machine learning applications by showing that when we solve these learning problems, we also solve a multi-time HJ PDE and, by extension, its corresponding optimal control problem. As a first exploration of this connection, we develop the relation between the regularized linear regression problem and the Linear Quadratic Regulator (LQR). We then leverage our theoret
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24433;&#21709;&#20989;&#25968;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#22312;&#38750;&#20984;&#26465;&#20214;&#19979;&#20351;&#29992;&#28145;&#23618;&#27169;&#22411;&#21644;&#26356;&#22797;&#26434;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12922</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#24433;&#21709;&#20989;&#25968;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
Revisiting the Fragility of Influence Functions. (arXiv:2303.12922v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24433;&#21709;&#20989;&#25968;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#22312;&#38750;&#20984;&#26465;&#20214;&#19979;&#20351;&#29992;&#28145;&#23618;&#27169;&#22411;&#21644;&#26356;&#22797;&#26434;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#26377;&#24456;&#22810;&#35770;&#25991;&#33268;&#21147;&#20110;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#26041;&#27861;&#34987;&#25552;&#20986;&#26469;&#39564;&#35777;&#36825;&#20123;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#25110;&#21487;&#20449;&#24230;&#12290;&#26368;&#36817;&#65292;&#24433;&#21709;&#20989;&#25968;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21333;&#20010;&#26679;&#26412;&#19978;&#30340;&#28789;&#25935;&#24230;&#30340;&#26041;&#27861;&#12290;&#20294;&#26159;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#24433;&#21709;&#20989;&#25968;&#26131;&#21463;&#22122;&#22768;&#21644;&#25968;&#25454;&#20998;&#24067;&#19981;&#23545;&#31216;&#24615;&#24433;&#21709;&#65292;&#32570;&#20047;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#24433;&#21709;&#20989;&#25968;&#30340;&#33030;&#24369;&#24615;&#65292;&#36890;&#36807;&#25506;&#31350;&#24433;&#21709;&#20989;&#25968;&#32972;&#21518;&#30340;&#26426;&#29702;&#65292;&#20174;&#32780;&#20026;&#22686;&#24378;&#24433;&#21709;&#20989;&#25968;&#30340;&#40065;&#26834;&#24615;&#25552;&#20379;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last few years, many works have tried to explain the predictions of deep learning models. Few methods, however, have been proposed to verify the accuracy or faithfulness of these explanations. Recently, influence functions, which is a method that approximates the effect that leave-one-out training has on the loss function, has been shown to be fragile. The proposed reason for their fragility remains unclear. Although previous work suggests the use of regularization to increase robustness, this does not hold in all cases. In this work, we seek to investigate the experiments performed in the prior work in an effort to understand the underlying mechanisms of influence function fragility. First, we verify influence functions using procedures from the literature under conditions where the convexity assumptions of influence functions are met. Then, we relax these assumptions and study the effects of non-convexity by using deeper models and more complex datasets. Here, we analyze the k
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21487;&#22797;&#21046;&#31639;&#27861;&#19982;&#26631;&#20934;&#31639;&#27861;&#31283;&#23450;&#24615;&#30340;&#32852;&#31995;&#65292;&#20026;&#19968;&#31867;&#32479;&#35745;&#38382;&#39064;&#25552;&#20379;&#20102;&#21487;&#22797;&#21046;&#31639;&#27861;&#30340;&#26679;&#26412;&#26377;&#25928;&#31639;&#27861;&#32422;&#21270;&#65292;&#21516;&#26102;&#34920;&#26126;&#36825;&#31181;&#31561;&#20215;&#20851;&#31995;&#24517;&#39035;&#22312;&#35745;&#31639;&#19978;&#23849;&#28291;&#12290;</title><link>http://arxiv.org/abs/2303.12921</link><description>&lt;p&gt;
&#31283;&#23450;&#24615;&#31283;&#23450;&#65306;&#21487;&#22797;&#21046;&#24615;&#12289;&#38544;&#31169;&#21644;&#33258;&#36866;&#24212;&#25512;&#24191;&#20043;&#38388;&#30340;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
Stability is Stable: Connections between Replicability, Privacy, and Adaptive Generalization. (arXiv:2303.12921v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12921
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21487;&#22797;&#21046;&#31639;&#27861;&#19982;&#26631;&#20934;&#31639;&#27861;&#31283;&#23450;&#24615;&#30340;&#32852;&#31995;&#65292;&#20026;&#19968;&#31867;&#32479;&#35745;&#38382;&#39064;&#25552;&#20379;&#20102;&#21487;&#22797;&#21046;&#31639;&#27861;&#30340;&#26679;&#26412;&#26377;&#25928;&#31639;&#27861;&#32422;&#21270;&#65292;&#21516;&#26102;&#34920;&#26126;&#36825;&#31181;&#31561;&#20215;&#20851;&#31995;&#24517;&#39035;&#22312;&#35745;&#31639;&#19978;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;Impagliazzo et al. [STOC '22]&#20013;&#24341;&#20837;&#20102;&#21487;&#22797;&#21046;&#31639;&#27861;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#25551;&#36848;&#22312;&#36755;&#20837;&#37325;&#26032;&#37319;&#26679;&#26102;&#31283;&#23450;&#30340;&#38543;&#26426;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24403;&#20854;&#38543;&#26426;&#24615;&#34987;&#22266;&#23450;&#19988;&#22312;&#20174;&#30456;&#21516;&#20998;&#24067;&#20013;&#32472;&#21046;&#30340;&#26032;&#30340;i.i.d.&#26679;&#26412;&#19978;&#36816;&#34892;&#26102;&#65292;&#21487;&#22797;&#21046;&#31639;&#27861;&#20250;&#20197;&#24456;&#39640;&#30340;&#27010;&#29575;&#32473;&#20986;&#30456;&#21516;&#30340;&#36755;&#20986;&#12290;&#20351;&#29992;&#21487;&#22797;&#21046;&#31639;&#27861;&#36827;&#34892;&#25968;&#25454;&#20998;&#26512;&#21487;&#20197;&#36890;&#36807;&#30830;&#20445;&#20998;&#26512;&#32467;&#26524;&#22312;&#26032;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20998;&#26512;&#26102;&#20855;&#26377;&#30456;&#21516;&#30340;&#32467;&#26524;&#26469;&#31616;&#21270;&#24050;&#21457;&#24067;&#32467;&#26524;&#30340;&#39564;&#35777;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#21487;&#22797;&#21046;&#24615;&#19982;&#31639;&#27861;&#31283;&#23450;&#24615;&#26631;&#20934;&#27010;&#24565;&#20043;&#38388;&#30340;&#26032;&#32852;&#31995;&#21644;&#20998;&#31163;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20026;&#19968;&#31867;&#24191;&#27867;&#30340;&#32479;&#35745;&#38382;&#39064;&#32473;&#20986;&#20102;&#23436;&#32654;&#25512;&#24191;&#12289;&#36817;&#20284;&#24046;&#20998;&#38544;&#31169;&#21644;&#21487;&#22797;&#21046;&#24615;&#20043;&#38388;&#30340;&#26679;&#26412;&#26377;&#25928;&#31639;&#27861;&#32422;&#21270;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#34920;&#26126;&#36825;&#31181;&#31561;&#20215;&#20851;&#31995;&#24517;&#39035;&#22312;&#35745;&#31639;&#19978;&#23849;&#28291;&#65306;&#23384;&#22312;&#20855;&#26377;&#21487;&#22797;&#21046;&#31639;&#27861;&#20294;&#19981;&#20855;&#26377;&#20219;&#20309;&#21487;&#29992;&#30340;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The notion of replicable algorithms was introduced in Impagliazzo et al. [STOC '22] to describe randomized algorithms that are stable under the resampling of their inputs. More precisely, a replicable algorithm gives the same output with high probability when its randomness is fixed and it is run on a new i.i.d. sample drawn from the same distribution. Using replicable algorithms for data analysis can facilitate the verification of published results by ensuring that the results of an analysis will be the same with high probability, even when that analysis is performed on a new data set.  In this work, we establish new connections and separations between replicability and standard notions of algorithmic stability. In particular, we give sample-efficient algorithmic reductions between perfect generalization, approximate differential privacy, and replicability for a broad class of statistical problems. Conversely, we show any such equivalence must break down computationally: there exist s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#23558;&#33258;&#25105;&#33976;&#39311;&#30340;&#27010;&#24565;&#24341;&#20837;&#21040;&#25163;&#26415;&#35270;&#39057;&#20998;&#26512;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#26500;&#38598;&#25104;&#26041;&#27861;&#65292;&#20854;&#20351;&#29992;Swine Transfomers&#20316;&#20026;&#39592;&#24178;&#32593;&#32476;&#65292;&#24182;&#23558;&#33258;&#25105;&#33976;&#39311;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#24212;&#29992;&#20110;&#27169;&#22411;&#35774;&#35745;&#20013;&#12290;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#28508;&#22312;&#26631;&#31614;&#19981;&#26126;&#30830;&#30340;&#24773;&#20917;&#19979;&#65292;&#36719;&#26631;&#31614;&#36890;&#36807;&#33258;&#25105;&#33976;&#39311;&#30340;&#26041;&#24335;&#33719;&#24471;&#26159;&#24615;&#33021;&#25552;&#21319;&#26368;&#22823;&#30340;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2303.12915</link><description>&lt;p&gt;
&#33258;&#25105;&#33976;&#39311;&#29992;&#20110;&#25163;&#26415;&#21160;&#20316;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Self-distillation for surgical action recognition. (arXiv:2303.12915v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23558;&#33258;&#25105;&#33976;&#39311;&#30340;&#27010;&#24565;&#24341;&#20837;&#21040;&#25163;&#26415;&#35270;&#39057;&#20998;&#26512;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#26500;&#38598;&#25104;&#26041;&#27861;&#65292;&#20854;&#20351;&#29992;Swine Transfomers&#20316;&#20026;&#39592;&#24178;&#32593;&#32476;&#65292;&#24182;&#23558;&#33258;&#25105;&#33976;&#39311;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#24212;&#29992;&#20110;&#27169;&#22411;&#35774;&#35745;&#20013;&#12290;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#28508;&#22312;&#26631;&#31614;&#19981;&#26126;&#30830;&#30340;&#24773;&#20917;&#19979;&#65292;&#36719;&#26631;&#31614;&#36890;&#36807;&#33258;&#25105;&#33976;&#39311;&#30340;&#26041;&#24335;&#33719;&#24471;&#26159;&#24615;&#33021;&#25552;&#21319;&#26368;&#22823;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#26415;&#22330;&#26223;&#30340;&#29702;&#35299;&#26159;&#25163;&#26415;&#23460;&#20013;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#20915;&#31574;&#25903;&#25345;&#30340;&#20851;&#38190;&#21069;&#25552;&#12290;&#34429;&#28982;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#21508;&#20010;&#39046;&#22495;&#24050;&#32463;&#36798;&#21040;&#29978;&#33267;&#36229;&#36807;&#20102;&#20154;&#31867;&#30340;&#34920;&#29616;&#65292;&#20294;&#25163;&#26415;&#21160;&#20316;&#35782;&#21035;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#39318;&#27425;&#25506;&#35752;&#20102;&#33258;&#25105;&#33976;&#39311;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#24212;&#23545;&#25163;&#26415;&#35270;&#39057;&#20998;&#26512;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#28508;&#22312;&#26631;&#31614;&#27495;&#20041;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#26500;&#38598;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;Swin Transfomers&#20316;&#20026;&#39592;&#24178;&#32593;&#32476;&#65292;&#20351;&#29992;&#33258;&#25105;&#33976;&#39311;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#27010;&#24565;&#20316;&#20026;&#26680;&#24515;&#35774;&#35745;&#36873;&#25321;&#12290;&#36890;&#36807;&#20132;&#21449;&#39564;&#35777;&#20351;&#29992;CholecT45&#25361;&#25112;&#25968;&#25454;&#36827;&#34892;&#30340;&#21066;&#20943;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#33258;&#25105;&#33976;&#39311;&#33719;&#24471;&#30340;&#36719;&#26631;&#31614;&#26159;&#24615;&#33021;&#25552;&#21319;&#30340;&#26368;&#22823;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#20010;&#29420;&#31435;&#30340;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#30340;&#22806;&#37096;&#39564;&#35777;&#65292;&#36890;&#36807;&#25552;&#20379;&#25105;&#20204;&#25512;&#29702;&#27169;&#22411;&#30340;Docker&#23481;&#22120;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Surgical scene understanding is a key prerequisite for contextaware decision support in the operating room. While deep learning-based approaches have already reached or even surpassed human performance in various fields, the task of surgical action recognition remains a major challenge. With this contribution, we are the first to investigate the concept of self-distillation as a means of addressing class imbalance and potential label ambiguity in surgical video analysis. Our proposed method is a heterogeneous ensemble of three models that use Swin Transfomers as backbone and the concepts of self-distillation and multi-task learning as core design choices. According to ablation studies performed with the CholecT45 challenge data via cross-validation, the biggest performance boost is achieved by the usage of soft labels obtained by self-distillation. External validation of our method on an independent test set was achieved by providing a Docker container of our inference model to the cha
&lt;/p&gt;</description></item><item><title>TRON&#26159;&#19968;&#31181;&#22522;&#20110;&#30789;&#20809;&#23376;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;&#65292;&#33021;&#22815;&#27604;&#21516;&#31867;Transformer&#21152;&#36895;&#22120;&#39640;14&#20493;&#30340;&#21534;&#21520;&#37327;&#21644;8&#20493;&#30340;&#33021;&#25928;&#12290;</title><link>http://arxiv.org/abs/2303.12914</link><description>&lt;p&gt;
TRON&#65306;&#21033;&#29992;&#38750;&#30456;&#24178;&#30789;&#20809;&#23376;&#23398;&#36827;&#34892;Transformer&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
TRON: Transformer Neural Network Acceleration with Non-Coherent Silicon Photonics. (arXiv:2303.12914v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12914
&lt;/p&gt;
&lt;p&gt;
TRON&#26159;&#19968;&#31181;&#22522;&#20110;&#30789;&#20809;&#23376;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;&#65292;&#33021;&#22815;&#27604;&#21516;&#31867;Transformer&#21152;&#36895;&#22120;&#39640;14&#20493;&#30340;&#21534;&#21520;&#37327;&#21644;8&#20493;&#30340;&#33021;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#31070;&#32463;&#32593;&#32476;&#30446;&#21069;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#65292;&#20294;&#27169;&#22411;&#32467;&#26500;&#22797;&#26434;&#65292;&#21152;&#24555;&#30005;&#23376;&#24179;&#21488;&#19978;&#30340;&#25191;&#34892;&#36895;&#24230;&#38754;&#20020;&#30528;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30789;&#20809;&#23376;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;TRON&#65292;&#21487;&#29992;&#20110;&#21152;&#36895;BERT&#21644;Vision Transformer&#31561;Transformer&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TRON&#30340;&#21534;&#21520;&#37327;&#33267;&#23569;&#27604;&#29616;&#26377;&#30340;Transformer&#21152;&#36895;&#22120;&#39640;14&#20493;&#65292;&#33021;&#25928;&#33267;&#23569;&#39640;&#20986;8&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer neural networks are rapidly being integrated into state-of-the-art solutions for natural language processing (NLP) and computer vision. However, the complex structure of these models creates challenges for accelerating their execution on conventional electronic platforms. We propose the first silicon photonic hardware neural network accelerator called TRON for transformer-based models such as BERT, and Vision Transformers. Our analysis demonstrates that TRON exhibits at least 14x better throughput and 8x better energy efficiency, in comparison to state-of-the-art transformer accelerators.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#20132;&#21449;&#23618;&#35774;&#35745;&#26469;&#20811;&#26381;&#38750;&#30456;&#24178;&#20809;&#35745;&#31639;&#24179;&#21488;&#20013;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#36866;&#24212;&#22810;&#31181;&#31867;&#22411;&#30340;AI&#24037;&#20316;&#36127;&#36733;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#36895;&#30340;AI&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2303.12910</link><description>&lt;p&gt;
&#22522;&#20110;&#38750;&#30456;&#24178;&#20809;&#35745;&#31639;&#30340;AI&#21152;&#36895;&#20132;&#21449;&#23618;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Cross-Layer Design for AI Acceleration with Non-Coherent Optical Computing. (arXiv:2303.12910v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#20132;&#21449;&#23618;&#35774;&#35745;&#26469;&#20811;&#26381;&#38750;&#30456;&#24178;&#20809;&#35745;&#31639;&#24179;&#21488;&#20013;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#36866;&#24212;&#22810;&#31181;&#31867;&#22411;&#30340;AI&#24037;&#20316;&#36127;&#36733;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#36895;&#30340;AI&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#65292;&#22914;ChatGPT&#12289;&#22270;&#24418;&#21367;&#31215;&#32593;&#32476;&#21644;&#20854;&#20182;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#36827;&#34892;&#35757;&#32451;&#21644;&#25512;&#29702;&#12290;&#24403;&#20195;&#35745;&#31639;&#24179;&#21488;&#65292;&#22914;CPU&#12289;GPU&#21644;TPU&#27491;&#22312;&#21162;&#21147;&#28385;&#36275;&#36825;&#20123;AI&#24212;&#29992;&#30340;&#38656;&#27714;&#12290;&#38750;&#30456;&#24178;&#20809;&#35745;&#31639;&#20195;&#34920;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;AI&#24037;&#20316;&#36127;&#36733;&#30340;&#20809;&#36895;&#21152;&#36895;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20132;&#21449;&#23618;&#35774;&#35745;&#22914;&#20309;&#20811;&#26381;&#38750;&#30456;&#24178;&#20809;&#35745;&#31639;&#24179;&#21488;&#20013;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#20809;&#23398;&#22120;&#20214;&#24037;&#31243;&#12289;&#35843;&#35856;&#30005;&#36335;&#22686;&#24378;&#20197;&#21450;&#26550;&#26500;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#36866;&#24212;&#21508;&#31181;AI&#24037;&#20316;&#36127;&#36733;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#30828;&#20214;/&#36719;&#20214;&#21327;&#21516;&#35774;&#35745;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#26234;&#33021;&#22320;&#26144;&#23556;&#21644;&#36866;&#24212;AI&#36719;&#20214;&#65292;&#20197;&#25552;&#39640;&#20854;&#22312;&#38750;&#30456;&#24178;&#20809;&#35745;&#31639;&#24179;&#21488;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging AI applications such as ChatGPT, graph convolutional networks, and other deep neural networks require massive computational resources for training and inference. Contemporary computing platforms such as CPUs, GPUs, and TPUs are struggling to keep up with the demands of these AI applications. Non-coherent optical computing represents a promising approach for light-speed acceleration of AI workloads. In this paper, we show how cross-layer design can overcome challenges in non-coherent optical computing platforms. We describe approaches for optical device engineering, tuning circuit enhancements, and architectural innovations to adapt optical computing to a variety of AI workloads. We also discuss techniques for hardware/software co-design that can intelligently map and adapt AI software to improve its performance on non-coherent optical computing platforms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20110;&#29992;&#20110;&#32593;&#32476;&#23433;&#20840;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#19977;&#31181;&#29305;&#24449;&#38477;&#32500;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;&#34649;&#34656;&#31639;&#27861;&#30340;&#30456;&#20851;&#29305;&#24449;&#36873;&#25321;&#65288;CFS-BA&#65289;&#26159;&#26368;&#20026;&#39640;&#25928;&#30340;&#65292;&#20165;&#29992;&#26368;&#20339;&#38543;&#26426;&#26862;&#26519;&#20449;&#24687;&#22686;&#30410;&#65288;RF-IG&#65289;&#27169;&#22411;55%&#30340;&#26102;&#38388;&#26500;&#24314;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;99.99%&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12891</link><description>&lt;p&gt;
&#29992;&#20110;&#32593;&#32476;&#23433;&#20840;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#29305;&#24449;&#38477;&#32500;&#26041;&#27861;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Feature Reduction Method Comparison Towards Explainability and Efficiency in Cybersecurity Intrusion Detection Systems. (arXiv:2303.12891v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20110;&#29992;&#20110;&#32593;&#32476;&#23433;&#20840;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#19977;&#31181;&#29305;&#24449;&#38477;&#32500;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;&#34649;&#34656;&#31639;&#27861;&#30340;&#30456;&#20851;&#29305;&#24449;&#36873;&#25321;&#65288;CFS-BA&#65289;&#26159;&#26368;&#20026;&#39640;&#25928;&#30340;&#65292;&#20165;&#29992;&#26368;&#20339;&#38543;&#26426;&#26862;&#26519;&#20449;&#24687;&#22686;&#30410;&#65288;RF-IG&#65289;&#27169;&#22411;55%&#30340;&#26102;&#38388;&#26500;&#24314;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;99.99%&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#65292;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;IDS&#65289;&#26681;&#25454;&#25910;&#38598;&#21040;&#30340;&#35745;&#31639;&#26426;&#21644;&#32593;&#32476;&#25968;&#25454;&#26816;&#27979;&#21644;&#38450;&#27490;&#25915;&#20987;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;IDS&#27169;&#22411;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26041;&#27861;&#26500;&#24314;&#65292;&#22914;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#12290;&#29305;&#24449;&#36873;&#25321;&#65288;FS&#65289;&#21487;&#29992;&#20110;&#26500;&#24314;&#26356;&#24555;&#65292;&#26356;&#21487;&#35299;&#37322;&#21644;&#26356;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;FS&#25216;&#26415;&#65307; &#38543;&#26426;&#26862;&#26519;&#20449;&#24687;&#22686;&#30410;&#65288;RF-IG&#65289;&#65292;&#20351;&#29992;&#34649;&#34656;&#31639;&#27861;&#30340;&#30456;&#20851;&#29305;&#24449;&#36873;&#25321;&#65288;CFS-BA&#65289;&#21644;&#20351;&#29992;&#38463;&#22522;&#25289;&#20248;&#21270;&#22120;&#30340;CFS&#65288;CFS-AO&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;CFS-BA&#26159;&#26368;&#26377;&#25928;&#30340;FS&#26041;&#27861;&#65292;&#20165;&#29992;&#26368;&#20339;RF-IG&#27169;&#22411;55&#65285;&#30340;&#26102;&#38388;&#26500;&#24314;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;99.99&#65285;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#21152;&#24378;&#20102;&#20808;&#21069;&#23545;CFS-BA&#20934;&#30830;&#24615;&#30340;&#36129;&#29486;&#65292;&#24182;&#22312;&#26368;&#32456;&#32467;&#26524;&#20013;&#24314;&#31435;&#20102;&#23376;&#38598;&#22823;&#23567;&#65292;CFS&#24471;&#20998;&#21644;RF-IG&#24471;&#20998;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of cybersecurity, intrusion detection systems (IDS) detect and prevent attacks based on collected computer and network data. In recent research, IDS models have been constructed using machine learning (ML) and deep learning (DL) methods such as Random Forest (RF) and deep neural networks (DNN). Feature selection (FS) can be used to construct faster, more interpretable, and more accurate models. We look at three different FS techniques; RF information gain (RF-IG), correlation feature selection using the Bat Algorithm (CFS-BA), and CFS using the Aquila Optimizer (CFS-AO). Our results show CFS-BA to be the most efficient of the FS methods, building in 55% of the time of the best RF-IG model while achieving 99.99% of its accuracy. This reinforces prior contributions attesting to CFS-BA's accuracy while building upon the relationship between subset size, CFS score, and RF-IG score in final results.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20102;&#19968;&#20010;&#39118;&#38505;&#20998;&#23618;&#24037;&#20855;CShock&#65292;&#26088;&#22312;&#38024;&#23545;&#24613;&#24615;&#22833;&#20195;&#20607;&#24615;&#24515;&#21147;&#34928;&#31469;&#21644;/&#25110;&#24515;&#32908;&#26775;&#27515;&#24739;&#32773;&#39044;&#27979;&#24515;&#28304;&#24615;&#20241;&#20811;&#30340;&#21457;&#20316;&#12290;</title><link>http://arxiv.org/abs/2303.12888</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#21160;&#24577;&#39118;&#38505;&#35780;&#20998;&#25552;&#21069;&#39044;&#27979;&#24515;&#28304;&#24615;&#20241;&#20811;
&lt;/p&gt;
&lt;p&gt;
A dynamic risk score for early prediction of cardiogenic shock using machine learning. (arXiv:2303.12888v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12888
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20102;&#19968;&#20010;&#39118;&#38505;&#20998;&#23618;&#24037;&#20855;CShock&#65292;&#26088;&#22312;&#38024;&#23545;&#24613;&#24615;&#22833;&#20195;&#20607;&#24615;&#24515;&#21147;&#34928;&#31469;&#21644;/&#25110;&#24515;&#32908;&#26775;&#27515;&#24739;&#32773;&#39044;&#27979;&#24515;&#28304;&#24615;&#20241;&#20811;&#30340;&#21457;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#32908;&#26775;&#27515;&#21644;&#24515;&#21147;&#34928;&#31469;&#26159;&#20027;&#35201;&#30340;&#24515;&#34880;&#31649;&#30142;&#30149;&#65292;&#24433;&#21709;&#30528;&#32654;&#22269;&#25968;&#30334;&#19975;&#20154;&#30340;&#20581;&#24247;&#12290;&#21457;&#23637;&#24515;&#28304;&#24615;&#20241;&#20811;&#30340;&#24739;&#32773;&#20013;&#65292;&#21457;&#30149;&#29575;&#21644;&#27515;&#20129;&#29575;&#26368;&#39640;&#12290;&#24515;&#28304;&#24615;&#20241;&#20811;&#30340;&#26089;&#26399;&#35782;&#21035;&#33267;&#20851;&#37325;&#35201;&#65292;&#21450;&#26102;&#23454;&#26045;&#27835;&#30103;&#25514;&#26045;&#21487;&#20197;&#38450;&#27490;&#32570;&#34880;&#12289;&#20302;&#34880;&#21387;&#20197;&#21450;&#30001;&#20110;&#24515;&#28304;&#24615;&#20241;&#20811;&#23548;&#33268;&#24515;&#36755;&#20986;&#37327;&#38477;&#20302;&#30340;&#26377;&#23475;&#24490;&#29615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24515;&#33039;&#30417;&#25252;&#30149;&#25151;&#20013;&#28023;&#37327;&#25968;&#25454;&#30340;&#20449;&#24687;&#22788;&#29702;&#33021;&#21147;&#19982;&#32570;&#20047;&#26377;&#25928;&#30340;&#39118;&#38505;&#20998;&#23618;&#24037;&#20855;&#65292;&#23545;&#24515;&#28304;&#24615;&#20241;&#20811;&#30340;&#26089;&#26399;&#35782;&#21035;&#19968;&#30452;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20102;&#19968;&#20010;&#31216;&#20026;CShock&#30340;&#39118;&#38505;&#20998;&#23618;&#24037;&#20855;&#65292;&#29992;&#20110;&#39044;&#27979;&#20837;&#20303;&#24515;&#33039;&#30417;&#25252;&#30149;&#25151;&#30340;&#24613;&#24615;&#22833;&#20195;&#20607;&#24615;&#24515;&#21147;&#34928;&#31469;&#21644;/&#25110;&#24515;&#32908;&#26775;&#27515;&#24739;&#32773;&#30340;&#24515;&#28304;&#24615;&#20241;&#20811;&#21457;&#20316;&#12290;&#20026;&#20102;&#24320;&#21457;&#21644;&#39564;&#35777;CShock&#65292;&#25105;&#20204;&#20351;&#29992;&#30001;&#21307;&#24072;&#35009;&#23450;&#30340;&#32467;&#26524;&#27880;&#37322;&#20102;&#24515;&#33039;&#30417;&#25252;&#30149;&#25151;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Myocardial infarction and heart failure are major cardiovascular diseases that affect millions of people in the US. The morbidity and mortality are highest among patients who develop cardiogenic shock. Early recognition of cardiogenic shock is critical. Prompt implementation of treatment measures can prevent the deleterious spiral of ischemia, low blood pressure, and reduced cardiac output due to cardiogenic shock. However, early identification of cardiogenic shock has been challenging due to human providers' inability to process the enormous amount of data in the cardiac intensive care unit (ICU) and lack of an effective risk stratification tool. We developed a deep learning-based risk stratification tool, called CShock, for patients admitted into the cardiac ICU with acute decompensated heart failure and/or myocardial infarction to predict onset of cardiogenic shock. To develop and validate CShock, we annotated cardiac ICU datasets with physician adjudicated outcomes. CShock achieved
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#25490;&#24207;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#40065;&#26834;&#24615;&#20849;&#35782;&#38382;&#39064;&#20197;&#21450;&#35813;&#38382;&#39064;&#30456;&#20851;&#30340;&#32479;&#35745;&#26041;&#27861;&#65292;&#20854;&#20013;Consensus Ranking&#38382;&#39064;&#26159;&#37325;&#28857;&#65292;&#26088;&#22312;&#36890;&#36807;&#20013;&#20301;&#25968;&#25490;&#21517;&#26469;&#24635;&#32467;&#25490;&#21015;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2303.12878</link><description>&lt;p&gt;
&#25490;&#24207;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#40065;&#26834;&#24615;&#20849;&#35782;&#65306;&#23450;&#20041;&#12289;&#23646;&#24615;&#21644;&#35745;&#31639;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Robust Consensus in Ranking Data Analysis: Definitions, Properties and Computational Issues. (arXiv:2303.12878v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12878
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#25490;&#24207;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#40065;&#26834;&#24615;&#20849;&#35782;&#38382;&#39064;&#20197;&#21450;&#35813;&#38382;&#39064;&#30456;&#20851;&#30340;&#32479;&#35745;&#26041;&#27861;&#65292;&#20854;&#20013;Consensus Ranking&#38382;&#39064;&#26159;&#37325;&#28857;&#65292;&#26088;&#22312;&#36890;&#36807;&#20013;&#20301;&#25968;&#25490;&#21517;&#26469;&#24635;&#32467;&#25490;&#21015;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#40065;&#26834;&#24615;&#38382;&#39064;&#26085;&#30410;&#31361;&#20986;&#65292;&#38656;&#35201;&#24320;&#21457;&#21487;&#38752;&#30340;&#32479;&#35745;&#23398;&#20064;&#25216;&#26415;&#65292;&#21363;&#20351;&#22312;&#37096;&#20998;&#21463;&#25439;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#30830;&#20445;&#21487;&#38752;&#24615;&#12290;&#20559;&#22909;&#25968;&#25454;&#20197; (&#23436;&#25972;) &#25490;&#24207;&#30340;&#24418;&#24335;&#20986;&#29616;&#26102;&#20063;&#19981;&#20363;&#22806;&#65292;&#23588;&#20854;&#26159;&#39281;&#21463;&#27492;&#31867;&#25968;&#25454;&#25903;&#25345;&#25110;&#20135;&#29983;&#30340;&#25216;&#26415;(&#20363;&#22914;&#65292;&#25628;&#32034;&#24341;&#25806;&#65292;&#25512;&#33616;&#31995;&#32479;)&#22823;&#35268;&#27169;&#37096;&#32626;&#20043;&#26102;&#65292;&#38656;&#35201;&#30456;&#24212;&#30340;&#27010;&#24565;&#21644;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25490;&#21015;&#30340;&#38598;&#21512; (&#21363;&#23545;&#31216;&#32676; $\mathfrak{S}_n$) &#27809;&#26377;&#21521;&#37327;&#31354;&#38388;&#32467;&#26500;&#65292;&#19988;&#25490;&#24207;&#25968;&#25454;&#20998;&#26512;&#20013;&#32771;&#34385;&#30340;&#32479;&#35745;&#37327;&#30340;&#22797;&#26434;&#24615;&#65292;&#20351;&#24471;&#22312;&#35813;&#39046;&#22495;&#20013;&#21046;&#23450;&#40065;&#26834;&#24615;&#30446;&#26631;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#40065;&#26834;&#24615;&#27010;&#24565;&#20197;&#21450;&#19987;&#29992;&#30340;&#32479;&#35745;&#26041;&#27861;&#65292;&#38024;&#23545;&#25490;&#21517;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#26071;&#33328;&#38382;&#39064;&#65306;Consensus Ranking&#65292;&#26088;&#22312;&#36890;&#36807;&#20013;&#20301;&#25968;&#25490;&#21517;&#26469;&#24635;&#32467;&#25490;&#21015;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the issue of robustness in AI systems becomes vital, statistical learning techniques that are reliable even in presence of partly contaminated data have to be developed. Preference data, in the form of (complete) rankings in the simplest situations, are no exception and the demand for appropriate concepts and tools is all the more pressing given that technologies fed by or producing this type of data (e.g. search engines, recommending systems) are now massively deployed. However, the lack of vector space structure for the set of rankings (i.e. the symmetric group $\mathfrak{S}_n$) and the complex nature of statistics considered in ranking data analysis make the formulation of robustness objectives in this domain challenging. In this paper, we introduce notions of robustness, together with dedicated statistical methods, for Consensus Ranking the flagship problem in ranking data analysis, aiming at summarizing a probability distribution on $\mathfrak{S}_n$ by a median ranking. Precise
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#31867;&#19981;&#30830;&#23450;&#24615;&#23545;&#27010;&#24565;&#39537;&#21160;AI&#31995;&#32479;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#25511;&#21046;&#25968;&#25454;&#38598;&#30340;&#24178;&#25200;&#22240;&#32032;&#65292;&#20998;&#26512;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#22788;&#29702;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.12872</link><description>&lt;p&gt;
&#27010;&#24565;&#39537;&#21160;&#30340;AI&#31995;&#32479;&#20013;&#30340;&#20154;&#31867;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Human Uncertainty in Concept-Based AI Systems. (arXiv:2303.12872v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#31867;&#19981;&#30830;&#23450;&#24615;&#23545;&#27010;&#24565;&#39537;&#21160;AI&#31995;&#32479;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#25511;&#21046;&#25968;&#25454;&#38598;&#30340;&#24178;&#25200;&#22240;&#32032;&#65292;&#20998;&#26512;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#22788;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#20013;&#37096;&#32626;AI&#31995;&#32479;&#65288;&#22914;&#21307;&#30103;AI&#31995;&#32479;&#19982;&#20020;&#24202;&#21307;&#29983;&#19968;&#36215;&#24037;&#20316;&#65289;&#26102;&#65292;&#23558;&#20154;&#31867;&#25918;&#20837;&#20854;&#20013;&#21487;&#33021;&#20250;&#20943;&#36731;&#19968;&#20123;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#32531;&#35299;&#20154;&#38388;&#35823;&#24046;&#21644;&#19981;&#30830;&#23450;&#22240;&#32032;&#22312;&#27492;&#31867;&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#20013;&#24341;&#36215;&#30340;&#39118;&#38505;&#65292;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#19988;&#26410;&#34987;&#30740;&#31350;&#20805;&#20998;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#27010;&#24565;&#39537;&#21160;&#27169;&#22411;&#20013;&#20154;&#31867;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#31867;&#22312;AI&#31995;&#32479;&#20013;&#21551;&#29992;&#27010;&#24565;&#24178;&#39044;&#21151;&#33021;&#30340;&#27169;&#22411;&#12290;&#35813;&#21151;&#33021;&#26159;&#25351;&#22312;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#20154;&#31867;&#21487;&#35299;&#37322;&#27010;&#24565;&#19978;&#65292;&#19987;&#23478;&#23545;&#20854;&#36827;&#34892;&#24178;&#39044;&#20197;&#33719;&#24471;&#20154;&#31867;&#21453;&#39304;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#23545;&#27492;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20294;&#36890;&#24120;&#20551;&#35774;&#20154;&#31867;&#26159;&#39044;&#35328;&#23478;&#65292;&#24635;&#26159;&#30830;&#23450;&#21644;&#27491;&#30830;&#30340;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#20013;&#20154;&#31867;&#30340;&#20915;&#31574;&#36807;&#31243;&#24448;&#24448;&#20063;&#20250;&#20986;&#29616;&#20598;&#23572;&#30340;&#38169;&#35823;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#26032;&#22411;&#25968;&#25454;&#38598;&#65288;UMNIST&#21644;CUB-S&#65289;&#25506;&#35752;&#20102;&#29616;&#26377;&#30340;&#27010;&#24565;&#39537;&#21160;&#27169;&#22411;&#22914;&#20309;&#22788;&#29702;&#26469;&#33258;&#20154;&#31867;&#30340;&#19981;&#30830;&#23450;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
Placing a human in the loop may abate the risks of deploying AI systems in safety-critical settings (e.g., a clinician working with a medical AI system). However, mitigating risks arising from human error and uncertainty within such human-AI interactions is an important and understudied issue. In this work, we study human uncertainty in the context of concept-based models, a family of AI systems that enable human feedback via concept interventions where an expert intervenes on human-interpretable concepts relevant to the task. Prior work in this space often assumes that humans are oracles who are always certain and correct. Yet, real-world decision-making by humans is prone to occasional mistakes and uncertainty. We study how existing concept-based models deal with uncertain interventions from humans using two novel datasets: UMNIST, a visual dataset with controlled simulated uncertainty based on the MNIST dataset, and CUB-S, a relabeling of the popular CUB concept dataset with rich, d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#39044;&#35757;&#32451;&#30340;NeRF-GAN&#20013;&#25552;&#21462;3D&#30693;&#35782;&#36827;&#34892;&#33976;&#39311;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#23039;&#24577;&#30340;2D GAN&#30340;&#39640;&#25928;3D&#24863;&#30693;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2303.12865</link><description>&lt;p&gt;
NeRF-GAN&#33976;&#39311;&#65306;&#22522;&#20110;&#21367;&#31215;&#30340;&#39640;&#25928;3D&#24863;&#30693;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
NeRF-GAN Distillation for Efficient 3D-Aware Generation with Convolutions. (arXiv:2303.12865v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#39044;&#35757;&#32451;&#30340;NeRF-GAN&#20013;&#25552;&#21462;3D&#30693;&#35782;&#36827;&#34892;&#33976;&#39311;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#23039;&#24577;&#30340;2D GAN&#30340;&#39640;&#25928;3D&#24863;&#30693;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23039;&#24577;&#30340;&#21367;&#31215;&#29983;&#25104;&#27169;&#22411;&#22312;&#20174;&#21333;&#35270;&#22270;&#25968;&#25454;&#38598;&#36827;&#34892;&#39640;&#36136;&#37327;&#30340;3D&#19968;&#33268;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#36275;&#22815;&#30340;3D&#20808;&#39564;&#30693;&#35782;&#12290;&#26368;&#36817;&#65292;&#23558;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#31561;&#29983;&#25104;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20174;&#21333;&#35270;&#22270;&#22270;&#20687;&#20013;&#29983;&#25104;3D&#24863;&#30693;&#22270;&#20687;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;NeRF-GAN&#21033;&#29992;&#20102;&#19977;&#32500;&#31070;&#32463;&#34920;&#31034;&#21644;&#20307;&#31215;&#28210;&#26579;&#30340;&#24378;&#24402;&#32435;&#20559;&#24046;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#26356;&#39640;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20174;&#39044;&#35757;&#32451;&#30340;NeRF-GAN&#20013;&#25552;&#21462;3D&#30693;&#35782;&#36827;&#34892;&#33976;&#39311;&#65292;&#37325;&#23457;&#22522;&#20110;&#23039;&#24577;&#30340;&#20108;&#32500;GAN&#65292;&#22312;&#25512;&#29702;&#26102;&#38388;&#20869;&#23454;&#29616;&#39640;&#25928;&#30340;3D&#24863;&#30693;&#29983;&#25104;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#22312;&#22522;&#20110;&#23039;&#24577;&#30340;&#21367;&#31215;&#32593;&#32476;&#20013;&#37325;&#29992;&#39044;&#35757;&#32451;&#30340;NeRF-GAN&#30340;&#33391;&#22909;&#35299;&#32806;&#28508;&#22312;&#31354;&#38388;&#65292;&#30452;&#25509;&#29983;&#25104;&#19982;&#28508;&#22312;&#30340;3D&#34920;&#36798;&#30456;&#23545;&#24212;&#30340;3D&#19968;&#33268;&#22270;&#29255;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;3D&#24863;&#30693;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pose-conditioned convolutional generative models struggle with high-quality 3D-consistent image generation from single-view datasets, due to their lack of sufficient 3D priors. Recently, the integration of Neural Radiance Fields (NeRFs) and generative models, such as Generative Adversarial Networks (GANs), has transformed 3D-aware generation from single-view images. NeRF-GANs exploit the strong inductive bias of 3D neural representations and volumetric rendering at the cost of higher computational complexity. This study aims at revisiting pose-conditioned 2D GANs for efficient 3D-aware generation at inference time by distilling 3D knowledge from pretrained NeRF-GANS. We propose a simple and effective method, based on re-using the well-disentangled latent space of a pre-trained NeRF-GAN in a pose-conditioned convolutional network to directly generate 3D-consistent images corresponding to the underlying 3D representations. Experiments on several datasets demonstrate that the proposed met
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31435;&#26041;&#20307;&#30340;3D&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#26469;&#37325;&#24314;CBCT&#24182;&#35299;&#20915;&#20102;&#23384;&#20648;&#25972;&#20010;&#27491;&#24358;&#22270;&#30340;&#20869;&#23384;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#25972;&#20010;CBCT volume&#20998;&#25104;&#22810;&#20010;&#23567;&#31435;&#26041;&#20307;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#35745;&#31639;&#24182;&#22312;&#35270;&#35273;&#21644;&#23450;&#37327;&#35780;&#20215;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.12861</link><description>&lt;p&gt;
&#38754;&#21521;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#38181;&#26463;CT&#37325;&#24314;&#30340;&#22522;&#20110;&#31435;&#26041;&#20307;&#30340;3D&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Cube-Based 3D Denoising Diffusion Probabilistic Model for Cone Beam Computed Tomography Reconstruction with Incomplete Data. (arXiv:2303.12861v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31435;&#26041;&#20307;&#30340;3D&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#26469;&#37325;&#24314;CBCT&#24182;&#35299;&#20915;&#20102;&#23384;&#20648;&#25972;&#20010;&#27491;&#24358;&#22270;&#30340;&#20869;&#23384;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#25972;&#20010;CBCT volume&#20998;&#25104;&#22810;&#20010;&#23567;&#31435;&#26041;&#20307;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#35745;&#31639;&#24182;&#22312;&#35270;&#35273;&#21644;&#23450;&#37327;&#35780;&#20215;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#35745;&#31639;&#26426;&#26029;&#23618;&#25668;&#24433;&#65288;CT&#65289;&#37325;&#24314;&#20013;&#33719;&#24471;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#22312;&#31232;&#30095;&#35270;&#22270;CT&#37325;&#24314;&#20013;&#12290;&#28982;&#32780;&#65292;&#23558;DL&#24212;&#29992;&#20110;&#31232;&#30095;&#35270;&#22270;&#38181;&#26463;CT&#65288;CBCT&#65289;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31435;&#26041;&#20307;&#30340;3D&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#26469;&#37325;&#24314;CBCT&#65292;&#24182;&#35299;&#20915;&#20102;&#23384;&#20648;&#25972;&#20010;&#27491;&#24358;&#22270;&#30340;&#20869;&#23384;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#25972;&#20010;CBCT volume&#20998;&#25104;&#22810;&#20010;&#23567;&#31435;&#26041;&#20307;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#35745;&#31639;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#35270;&#35273;&#21644;&#23450;&#37327;&#35780;&#20215;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) has been extensively researched in the field of computed tomography (CT) reconstruction with incomplete data, particularly in sparse-view CT reconstruction. However, applying DL to sparse-view cone beam CT (CBCT) remains challenging. Many models learn the mapping from sparse-view CT images to ground truth but struggle to achieve satisfactory performance in terms of global artifact removal. Incorporating sinogram data and utilizing dual-domain information can enhance anti-artifact performance, but this requires storing the entire sinogram in memory. This presents a memory issue for high-resolution CBCT sinograms, limiting further research and application. In this paper, we propose a cube-based 3D denoising diffusion probabilistic model (DDPM) for CBCT reconstruction using down-sampled data. A DDPM network, trained on cubes extracted from paired fully sampled sinograms and down-sampled sinograms, is employed to inpaint down-sampled sinograms. Our method divides the ent
&lt;/p&gt;</description></item><item><title>&#29992;&#34892;&#21015;&#24335;&#21644;&#26377;&#25928;&#22320;&#36924;&#36817;&#21453;&#23545;&#31216;&#24052;&#40857;&#20989;&#25968;&#65292;&#20174;&#32780;&#33719;&#24471;&#20102;&#38454;&#20056;&#32423;&#30340;&#22797;&#26434;&#24230;&#25552;&#21319;&#21644;&#22312;&#20174;&#22836;&#31639;&#37327;&#23376;&#21270;&#23398;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12856</link><description>&lt;p&gt;
&#21453;&#23545;&#31216;&#24052;&#40857;&#20989;&#25968;&#21450;&#20854;&#29992;&#34892;&#21015;&#24335;&#21644;&#34920;&#31034;&#30340;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Anti-symmetric Barron functions and their approximation with sums of determinants. (arXiv:2303.12856v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12856
&lt;/p&gt;
&lt;p&gt;
&#29992;&#34892;&#21015;&#24335;&#21644;&#26377;&#25928;&#22320;&#36924;&#36817;&#21453;&#23545;&#31216;&#24052;&#40857;&#20989;&#25968;&#65292;&#20174;&#32780;&#33719;&#24471;&#20102;&#38454;&#20056;&#32423;&#30340;&#22797;&#26434;&#24230;&#25552;&#21319;&#21644;&#22312;&#20174;&#22836;&#31639;&#37327;&#23376;&#21270;&#23398;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37327;&#23376;&#29289;&#29702;&#20013;&#65292;&#19968;&#20010;&#22522;&#26412;&#30340;&#38382;&#39064;&#26159;&#22914;&#20309;&#32534;&#30721;&#22312;&#31890;&#23376;&#32622;&#25442;&#19979;&#23436;&#20840;&#21453;&#23545;&#31216;&#30340;&#20989;&#25968;&#12290;Barron&#31354;&#38388;&#30001;&#21487;&#34987;&#19968;&#23618;&#38544;&#34255;&#31070;&#32463;&#32593;&#32476;&#26080;&#38480;&#21442;&#25968;&#21270;&#30340;&#39640;&#32500;&#20989;&#25968;&#32452;&#25104;&#12290;&#36890;&#36807;&#26126;&#30830;&#22320;&#32534;&#30721;&#21453;&#23545;&#31216;&#32467;&#26500;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23646;&#20110;Barron&#31354;&#38388;&#30340;&#21453;&#23545;&#31216;&#20989;&#25968;&#21487;&#20197;&#29992;&#34892;&#21015;&#24335;&#21644;&#26377;&#25928;&#36924;&#36817;&#12290;&#36825;&#30456;&#23545;&#20110;&#22312;Barron&#31354;&#38388;&#30340;&#26631;&#20934;&#34920;&#31034;&#20013;&#30340;&#22797;&#26434;&#24230;&#25552;&#20379;&#20102;&#38454;&#20056;&#32423;&#30340;&#25913;&#36827;&#65292;&#24182;&#20026;&#22522;&#20110;&#34892;&#21015;&#24335;&#30340;&#32467;&#26500;&#22312;&#20174;&#22836;&#31639;&#37327;&#23376;&#21270;&#23398;&#20013;&#26377;&#25928;&#30340;&#29702;&#35770;&#35299;&#37322;&#25552;&#20379;&#20102;&#20381;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental problem in quantum physics is to encode functions that are completely anti-symmetric under permutations of identical particles. The Barron space consists of high-dimensional functions that can be parameterized by infinite neural networks with one hidden layer. By explicitly encoding the anti-symmetric structure, we prove that the anti-symmetric functions which belong to the Barron space can be efficiently approximated with sums of determinants. This yields a factorial improvement in complexity compared to the standard representation in the Barron space and provides a theoretical explanation for the effectiveness of determinant-based architectures in ab-initio quantum chemistry.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;WL&#27979;&#35797;&#22312;&#28857;&#20113;&#20013;&#30340;&#24212;&#29992;&#65292;&#32467;&#26524;&#21457;&#29616;&#19977;&#27425;&#36845;&#20195;&#30340;$(d-1)$-WL&#27979;&#35797;&#21487;&#20197;&#21306;&#20998;$d$&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#28857;&#20113;&#65292;&#19988;&#21482;&#38656;&#35201;&#19968;&#27425;&#36845;&#20195;&#30340;$d$-WL&#27979;&#35797;&#23601;&#21487;&#20197;&#36798;&#21040;&#23436;&#25972;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12853</link><description>&lt;p&gt;
&#19977;&#27425;&#36845;&#20195;&#30340;$(1-d)$-WL&#27979;&#35797;&#21487;&#20197;&#21306;&#20998;$d$&#32500;&#28857;&#20113;&#30340;&#38750;&#31561;&#36317;&#21464;&#25442;. (arXiv:2303.12853v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Three iterations of $(1-d)$-WL test distinguish non isometric clouds of $d$-dimensional points. (arXiv:2303.12853v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;WL&#27979;&#35797;&#22312;&#28857;&#20113;&#20013;&#30340;&#24212;&#29992;&#65292;&#32467;&#26524;&#21457;&#29616;&#19977;&#27425;&#36845;&#20195;&#30340;$(d-1)$-WL&#27979;&#35797;&#21487;&#20197;&#21306;&#20998;$d$&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#28857;&#20113;&#65292;&#19988;&#21482;&#38656;&#35201;&#19968;&#27425;&#36845;&#20195;&#30340;$d$-WL&#27979;&#35797;&#23601;&#21487;&#20197;&#36798;&#21040;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Weisfeiler-Lehman (WL)&#27979;&#35797;&#26159;&#19968;&#20010;&#26816;&#26597;&#22270;&#21516;&#26500;&#30340;&#22522;&#26412;&#36845;&#20195;&#31639;&#27861;&#12290;&#23427;&#34987;&#35266;&#23519;&#21040;&#26159;&#20960;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#35774;&#35745;&#30340;&#22522;&#30784;&#65292;&#36825;&#20123;&#32593;&#32476;&#30340;&#33021;&#21147;&#21644;&#24615;&#33021;&#21487;&#20197;&#29992;&#36825;&#20010;&#27979;&#35797;&#30340;&#34920;&#31034;&#33021;&#21147;&#26469;&#29702;&#35299;&#12290;&#21463;&#26368;&#36817;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#28041;&#21450;&#19977;&#32500;&#29289;&#20307;&#30340;&#25968;&#25454;&#38598;&#30340;&#21457;&#23637;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24403;WL&#27979;&#35797;&#23545;&#23436;&#25972;&#30340;&#36317;&#31163;&#22270;&#34920;&#31034;&#30340;&#27431;&#20960;&#37324;&#24471;&#28857;&#20113;&#26159;&#8220;&#23436;&#25972;&#30340;&#8221;&#26102;&#65292;&#23427;&#20309;&#26102;&#33021;&#22815;&#35782;&#21035;&#20986;&#20219;&#24847;&#19968;&#20010;&#20219;&#24847;&#28857;&#20113;.&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#65292;$(d-1)$-&#32500;WL&#27979;&#35797;&#21487;&#20197;&#21306;&#20998;$d$&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#28857;&#20113;&#65292;&#20219;&#20309;$d\ge 2$&#37117;&#21487;&#20197;&#65292;&#32780;&#19988;&#21482;&#38656;&#35201;&#36827;&#34892;&#19977;&#27425;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23545;&#20110;$d=2,3$&#26159;&#32039;&#30340;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;$d$&#32500;WL&#27979;&#35797;&#21482;&#38656;&#35201;&#36827;&#34892;&#19968;&#27425;&#36845;&#20195;&#23601;&#21487;&#20197;&#36798;&#21040;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Weisfeiler--Lehman (WL) test is a fundamental iterative algorithm for checking isomorphism of graphs. It has also been observed that it underlies the design of several graph neural network architectures, whose capabilities and performance can be understood in terms of the expressive power of this test. Motivated by recent developments in machine learning applications to datasets involving three-dimensional objects, we study when the WL test is {\em complete} for clouds of euclidean points represented by complete distance graphs, i.e., when it can distinguish, up to isometry, any arbitrary such cloud.  Our main result states that the $(d-1)$-dimensional WL test is complete for point clouds in $d$-dimensional Euclidean space, for any $d\ge 2$, and that only three iterations of the test suffice. Our result is tight for $d = 2, 3$. We also observe that the $d$-dimensional WL test only requires one iteration to achieve completeness.
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#20351;&#29992;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#26816;&#27979;&#21644;&#37325;&#26500;&#65292;&#19981;&#38656;&#35201;&#22312;&#27979;&#35797;&#26102;&#38388;&#26356;&#26032;&#27169;&#22411;&#26435;&#37325;&#65292;&#20063;&#19981;&#38656;&#35201;&#20351;&#29992;&#26356;&#22810;&#30340;&#23545;&#25239;&#26679;&#26412;&#26469;&#22686;&#24378;&#35757;&#32451;&#38598;&#12290;</title><link>http://arxiv.org/abs/2303.12848</link><description>&lt;p&gt;
&#23545;&#25239;&#25915;&#20987;&#30340;&#27979;&#35797;&#26102;&#38388;&#38450;&#24481;&#65306;&#22522;&#20110;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#30340;&#23545;&#25239;&#26679;&#26412;&#26816;&#27979;&#21644;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
Test-time Defense against Adversarial Attacks: Detection and Reconstruction of Adversarial Examples via Masked Autoencoder. (arXiv:2303.12848v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12848
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#20351;&#29992;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#26816;&#27979;&#21644;&#37325;&#26500;&#65292;&#19981;&#38656;&#35201;&#22312;&#27979;&#35797;&#26102;&#38388;&#26356;&#26032;&#27169;&#22411;&#26435;&#37325;&#65292;&#20063;&#19981;&#38656;&#35201;&#20351;&#29992;&#26356;&#22810;&#30340;&#23545;&#25239;&#26679;&#26412;&#26469;&#22686;&#24378;&#35757;&#32451;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#23545;&#25239;&#25915;&#20987;&#38450;&#24481;&#26041;&#27861;&#21487;&#20197;&#20998;&#20026;&#35757;&#32451;&#26102;&#38388;&#21644;&#27979;&#35797;&#26102;&#38388;&#38450;&#24481;&#12290;&#35757;&#32451;&#26102;&#38388;&#38450;&#24481;&#38656;&#35201;&#22823;&#37327;&#30340;&#39069;&#22806;&#35757;&#32451;&#26102;&#38388;&#65292;&#36890;&#24120;&#26080;&#27861;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#25915;&#20987;&#12290;&#32780;&#27979;&#35797;&#26102;&#38388;&#38450;&#24481;&#38656;&#35201;&#35775;&#38382;&#65288;&#37096;&#20998;&#65289;&#27169;&#22411;&#26435;&#37325;&#20197;&#25191;&#34892;&#26799;&#24230;&#19979;&#38477;&#65292;&#36825;&#23545;&#20110;&#20923;&#32467;&#26435;&#37325;&#30340;&#27169;&#22411;&#21487;&#33021;&#19981;&#21487;&#34892;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38450;&#24481;&#26041;&#27861;DRAM&#65292;&#23427;&#20351;&#29992;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#26816;&#27979;&#24182;&#37325;&#26500;&#22810;&#31181;&#31867;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;MAE&#25439;&#22833;&#26500;&#24314;KS&#27979;&#35797;&#26469;&#26816;&#27979;&#23545;&#25239;&#25915;&#20987;&#12290;&#27492;&#22806;&#65292;MAE&#25439;&#22833;&#21487;&#20197;&#29992;&#20110;&#20462;&#22797;&#26410;&#35265;&#25915;&#20987;&#31867;&#22411;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;&#22240;&#27492;&#65292;DRAM&#26082;&#19981;&#38656;&#35201;&#22312;&#27979;&#35797;&#26102;&#38388;&#26356;&#26032;&#27169;&#22411;&#26435;&#37325;&#65292;&#20063;&#19981;&#38656;&#35201;&#20351;&#29992;&#26356;&#22810;&#30340;&#23545;&#25239;&#26679;&#26412;&#26469;&#22686;&#24378;&#35757;&#32451;&#38598;&#12290;&#22312;&#22823;&#35268;&#27169;&#30340;ImageN&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;DRAM&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20855;&#26377;&#24456;&#39640;&#30340;&#40065;&#26834;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing defense methods against adversarial attacks can be categorized into training time and test time defenses. Training time defense, i.e., adversarial training, requires a significant amount of extra time for training and is often not able to be generalized to unseen attacks. On the other hand, test time defense by test time weight adaptation requires access to perform gradient descent on (part of) the model weights, which could be infeasible for models with frozen weights. To address these challenges, we propose DRAM, a novel defense method to Detect and Reconstruct multiple types of Adversarial attacks via Masked autoencoder (MAE). We demonstrate how to use MAE losses to build a KS-test to detect adversarial attacks. Moreover, the MAE losses can be used to repair adversarial samples from unseen attack types. In this sense, DRAM neither requires model weight updates in test time nor augments the training set with more adversarial samples. Evaluating DRAM on the large-scale ImageN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#22312;&#19981;&#30456;&#24178;&#26694;&#26550;&#19979;&#65292;&#36890;&#36807;&#27973;&#23618;&#27979;&#37327;&#21482;&#33021;&#26377;&#25928;&#22320;&#23398;&#20064;&#20302;&#32416;&#32544;&#38376;&#12290;&#34429;&#28982;&#35813;&#31867;&#26694;&#26550;&#20026;&#25105;&#20204;&#22312;&#19981;&#21516;&#29289;&#29702;&#24179;&#21488;&#20043;&#38388;&#36716;&#31227;&#37327;&#23376;&#36807;&#31243;&#25552;&#20379;&#20102;&#26041;&#27861;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#23450;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12834</link><description>&lt;p&gt;
&#37327;&#23376;&#21160;&#21147;&#23398;&#30340;&#23398;&#20064;&#30340;&#33021;&#21147;&#19982;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
The power and limitations of learning quantum dynamics incoherently. (arXiv:2303.12834v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12834
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#22312;&#19981;&#30456;&#24178;&#26694;&#26550;&#19979;&#65292;&#36890;&#36807;&#27973;&#23618;&#27979;&#37327;&#21482;&#33021;&#26377;&#25928;&#22320;&#23398;&#20064;&#20302;&#32416;&#32544;&#38376;&#12290;&#34429;&#28982;&#35813;&#31867;&#26694;&#26550;&#20026;&#25105;&#20204;&#22312;&#19981;&#21516;&#29289;&#29702;&#24179;&#21488;&#20043;&#38388;&#36716;&#31227;&#37327;&#23376;&#36807;&#31243;&#25552;&#20379;&#20102;&#26041;&#27861;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#23450;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#36807;&#31243;&#23398;&#20064;&#26159;&#30740;&#31350;&#37327;&#23376;&#31995;&#32479;&#30340;&#37325;&#35201;&#24037;&#20855;&#20043;&#19968;&#12290;&#28982;&#32780;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#25918;&#22312;&#20102;&#33258;&#26059;&#30456;&#24178;&#21644;&#22120;&#20214;&#33258;&#32806;&#21512;&#30340;&#27874;&#21160;&#20989;&#25968;&#19978;&#65292;&#30740;&#31350;&#37327;&#23376;&#21160;&#21147;&#23398;&#22312;&#31995;&#32479;&#21644;&#30446;&#26631;&#19981;&#30452;&#25509;&#20132;&#20114;&#30340;&#24773;&#20917;&#19979;&#26159;&#21542;&#21487;&#20197;&#34987;&#23398;&#20064;&#24182;&#27809;&#26377;&#24471;&#21040;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#36825;&#31867;&#19981;&#30456;&#24178;&#30340;&#26694;&#26550;&#23454;&#38469;&#19978;&#38750;&#24120;&#21560;&#24341;&#20154;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#25361;&#25112;&#24615;&#30340;&#28151;&#21512;&#32416;&#32544;&#26041;&#26696;&#20013;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#22312;&#19981;&#21516;&#29289;&#29702;&#24179;&#21488;&#20043;&#38388;&#36716;&#31227;&#37327;&#23376;&#36807;&#31243;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#38656;&#35201;&#20223;&#30495;&#30340;&#26126;&#30830;&#30340;&#30456;&#24178;&#23398;&#20064;&#31574;&#30053;&#30340;&#27979;&#37327;&#27425;&#25968;&#65292;&#25552;&#20379;&#20102;&#22312;&#19981;&#30456;&#24178;&#26694;&#26550;&#19979;&#23398;&#20064;&#24186;&#27491;&#36807;&#31243;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22914;&#26524;&#20801;&#35768;&#20219;&#24847;&#27979;&#37327;&#65292;&#21017;&#20219;&#20309;&#26377;&#25928;&#34920;&#31034;&#30340;&#24186;&#27491;&#30697;&#38453;&#37117;&#21487;&#20197;&#22312;&#19981;&#30456;&#24178;&#26694;&#26550;&#20869;&#34987;&#26377;&#25928;&#22320;&#23398;&#20064;&#65307;&#28982;&#32780;&#65292;&#22914;&#26524;&#20165;&#38480;&#20110;&#27973;&#23618;&#27979;&#37327;&#65292;&#21017;&#21482;&#33021;&#26377;&#25928;&#22320;&#23398;&#20064;&#20302;&#32416;&#32544;&#38376;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#31361;&#20986;&#20102;&#23398;&#20064;&#37327;&#23376;&#21160;&#21147;&#23398;&#22312;&#19981;&#30456;&#24178;&#26694;&#26550;&#20013;&#30340;&#33021;&#21147;&#19982;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum process learning is emerging as an important tool to study quantum systems. While studied extensively in coherent frameworks, where the target and model system can share quantum information, less attention has been paid to whether the dynamics of quantum systems can be learned without the system and target directly interacting. Such incoherent frameworks are practically appealing since they open up methods of transpiling quantum processes between the different physical platforms without the need for technically challenging hybrid entanglement schemes. Here we provide bounds on the sample complexity of learning unitary processes incoherently by analyzing the number of measurements that are required to emulate well-established coherent learning strategies. We prove that if arbitrary measurements are allowed, then any efficiently representable unitary can be efficiently learned within the incoherent framework; however, when restricted to shallow-depth measurements only low-entangl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26426;&#21046;&#65292;&#20351;&#29992;&#31163;&#25955;&#30340;&#32534;&#30721;&#26041;&#24335;&#26469;&#35299;&#20915;&#21512;&#25104;&#20849;&#24615;&#35821;&#35328;&#25163;&#21183;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#37319;&#29992;VAE&#21644;&#33258;&#22238;&#24402;&#21464;&#21387;&#22120;&#27169;&#22411;&#36827;&#34892;&#23398;&#20064;&#65292;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#36924;&#30495;&#30340;&#20849;&#24615;&#35821;&#35328;&#25163;&#21183;&#12290;</title><link>http://arxiv.org/abs/2303.12822</link><description>&lt;p&gt;
&#21033;&#29992;&#31163;&#25955;&#25163;&#21183;&#20196;&#29260;&#23398;&#20064;&#30340;&#20849;&#24615;&#35821;&#35328;&#25163;&#21183;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Co-Speech Gesture Synthesis using Discrete Gesture Token Learning. (arXiv:2303.12822v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12822
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26426;&#21046;&#65292;&#20351;&#29992;&#31163;&#25955;&#30340;&#32534;&#30721;&#26041;&#24335;&#26469;&#35299;&#20915;&#21512;&#25104;&#20849;&#24615;&#35821;&#35328;&#25163;&#21183;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#37319;&#29992;VAE&#21644;&#33258;&#22238;&#24402;&#21464;&#21387;&#22120;&#27169;&#22411;&#36827;&#34892;&#23398;&#20064;&#65292;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#36924;&#30495;&#30340;&#20849;&#24615;&#35821;&#35328;&#25163;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21046;&#20316;&#36924;&#30495;&#30340;&#20849;&#24615;&#35821;&#35328;&#25163;&#21183;&#26159;&#19968;&#20010;&#37325;&#35201;&#19988;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#29992;&#20110;&#39537;&#21160;&#20154;&#24418;&#26426;&#22120;&#20154;&#19982;&#20154;&#31867;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#21644;&#27807;&#36890;&#12290;&#36825;&#31181;&#33021;&#21147;&#23558;&#25913;&#21892;&#20154;&#31867;&#29992;&#25143;&#23545;&#26426;&#22120;&#20154;&#30340;&#21360;&#35937;&#65292;&#24182;&#22312;&#25945;&#32946;&#12289;&#22521;&#35757;&#21644;&#21307;&#30103;&#26381;&#21153;&#20013;&#25214;&#21040;&#24212;&#29992;&#12290;&#23398;&#20064;&#20849;&#24615;&#35821;&#35328;&#25163;&#21183;&#27169;&#22411;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#65292;&#23545;&#20110;&#21516;&#19968;&#35821;&#38899;&#35805;&#35821;&#65292;&#21487;&#33021;&#23384;&#22312;&#22810;&#20010;&#21512;&#29702;&#30340;&#25163;&#21183;&#36816;&#21160;&#12290;&#30830;&#23450;&#24615;&#22238;&#24402;&#26041;&#27861;&#26080;&#27861;&#35299;&#20915;&#20914;&#31361;&#26679;&#26412;&#65292;&#24182;&#21487;&#33021;&#20135;&#29983;&#36807;&#24230;&#24179;&#28369;&#25110;&#25233;&#21046;&#30340;&#36816;&#21160;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#25163;&#21183;&#29255;&#27573;&#24314;&#27169;&#20026;&#31163;&#25955;&#30340;&#28508;&#22312;&#32534;&#30721;&#26469;&#35299;&#20915;&#36825;&#20010;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;RQ-VAE&#22312;&#31532;&#19968;&#38454;&#27573;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#30001;&#25163;&#21183;&#20196;&#29260;&#32452;&#25104;&#30340;&#31163;&#25955;&#30721;&#26412;&#65292;&#31532;&#20108;&#38454;&#27573;&#20351;&#29992;&#20004;&#32423;&#33258;&#22238;&#24402;&#21464;&#21387;&#22120;&#27169;&#22411;&#23398;&#20064;&#27531;&#20313;&#30721;&#30340;&#20808;&#39564;&#20998;&#24067;&#65292;&#20197;&#21450;&#32473;&#20986;&#35821;&#38899;&#26102;&#25163;&#21183;&#20196;&#29260;&#30340;&#26465;&#20214;&#20998;&#24067;&#12290;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#36924;&#30495;&#30340;&#20849;&#24615;&#35821;&#35328;&#25163;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthesizing realistic co-speech gestures is an important and yet unsolved problem for creating believable motions that can drive a humanoid robot to interact and communicate with human users. Such capability will improve the impressions of the robots by human users and will find applications in education, training, and medical services. One challenge in learning the co-speech gesture model is that there may be multiple viable gesture motions for the same speech utterance. The deterministic regression methods can not resolve the conflicting samples and may produce over-smoothed or damped motions. We proposed a two-stage model to address this uncertainty issue in gesture synthesis by modeling the gesture segments as discrete latent codes. Our method utilizes RQ-VAE in the first stage to learn a discrete codebook consisting of gesture tokens from training data. In the second stage, a two-level autoregressive transformer model is used to learn the prior distribution of residual codes cond
&lt;/p&gt;</description></item><item><title>DeepBlocks&#26159;&#19968;&#27454;&#21487;&#35270;&#21270;&#32534;&#31243;&#24037;&#20855;&#65292;&#20801;&#35768;DL&#24320;&#21457;&#20154;&#21592;&#35774;&#35745;&#12289;&#35757;&#32451;&#21644;&#35780;&#20272;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#29305;&#23450;&#30340;&#32534;&#31243;&#35821;&#35328;&#12290;&#20854;&#36890;&#36807;&#26500;&#24314;&#20856;&#22411;&#27169;&#22411;&#32467;&#26500;&#23454;&#29616;&#20854;&#24037;&#20316;&#21407;&#29702;&#65292;&#32467;&#26524;&#34920;&#26126;&#24320;&#21457;&#20154;&#21592;&#21487;&#20197;&#35270;&#35273;&#19978;&#35774;&#35745;&#22797;&#26434;&#30340;DL&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2303.12821</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#21019;&#24314;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35270;&#21270;&#32534;&#31243;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Towards A Visual Programming Tool to Create Deep Learning Models. (arXiv:2303.12821v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12821
&lt;/p&gt;
&lt;p&gt;
DeepBlocks&#26159;&#19968;&#27454;&#21487;&#35270;&#21270;&#32534;&#31243;&#24037;&#20855;&#65292;&#20801;&#35768;DL&#24320;&#21457;&#20154;&#21592;&#35774;&#35745;&#12289;&#35757;&#32451;&#21644;&#35780;&#20272;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#29305;&#23450;&#30340;&#32534;&#31243;&#35821;&#35328;&#12290;&#20854;&#36890;&#36807;&#26500;&#24314;&#20856;&#22411;&#27169;&#22411;&#32467;&#26500;&#23454;&#29616;&#20854;&#24037;&#20316;&#21407;&#29702;&#65292;&#32467;&#26524;&#34920;&#26126;&#24320;&#21457;&#20154;&#21592;&#21487;&#20197;&#35270;&#35273;&#19978;&#35774;&#35745;&#22797;&#26434;&#30340;DL&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#24320;&#21457;&#20154;&#21592;&#26469;&#33258;&#19981;&#21516;&#30340;&#32972;&#26223;&#65292;&#20363;&#22914;&#21307;&#23398;&#12289;&#22522;&#22240;&#32452;&#23398;&#12289;&#37329;&#34701;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#12290;&#20026;&#20102;&#21019;&#24314;DL&#27169;&#22411;&#65292;&#20182;&#20204;&#24517;&#39035;&#23398;&#20064;&#21644;&#20351;&#29992;&#39640;&#32423;&#32534;&#31243;&#35821;&#35328;&#65288;&#20363;&#22914;Python&#65289;&#65292;&#22240;&#27492;&#38656;&#35201;&#22788;&#29702;&#30456;&#20851;&#35774;&#32622;&#21644;&#35299;&#20915;&#32534;&#31243;&#38169;&#35823;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DeepBlocks&#65292;&#36825;&#26159;&#19968;&#27454;&#21487;&#35270;&#21270;&#32534;&#31243;&#24037;&#20855;&#65292;&#20801;&#35768;DL&#24320;&#21457;&#20154;&#21592;&#35774;&#35745;&#12289;&#35757;&#32451;&#21644;&#35780;&#20272;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#29305;&#23450;&#30340;&#32534;&#31243;&#35821;&#35328;&#12290;DeepBlocks&#36890;&#36807;&#26500;&#24314;&#20856;&#22411;&#27169;&#22411;&#32467;&#26500;&#23454;&#29616;&#20854;&#24037;&#20316;&#21407;&#29702;&#65306;&#19968;&#31995;&#21015;&#21487;&#23398;&#20064;&#20989;&#25968;&#30340;&#39034;&#24207;&#25490;&#21015;&#23450;&#20041;&#20102;&#27169;&#22411;&#30340;&#29305;&#23450;&#29305;&#24449;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;5&#20010;&#21442;&#19982;&#32773;&#30340;&#24418;&#24335;&#21270;&#35775;&#35848;&#25512;&#23548;&#20986;&#20102;DeepBlocks&#30340;&#35774;&#35745;&#30446;&#26631;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#20856;&#22411;&#29992;&#20363;&#39564;&#35777;&#20102;&#35813;&#24037;&#20855;&#30340;&#31532;&#19968;&#20010;&#23454;&#29616;&#12290;&#32467;&#26524;&#26159;&#20196;&#20154;&#20852;&#22859;&#30340;&#65292;&#34920;&#26126;&#24320;&#21457;&#20154;&#21592;&#21487;&#20197;&#35270;&#35273;&#19978;&#35774;&#35745;&#22797;&#26434;&#30340;DL&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning (DL) developers come from different backgrounds, e.g., medicine, genomics, finance, and computer science. To create a DL model, they must learn and use high-level programming languages (e.g., Python), thus needing to handle related setups and solve programming errors. This paper presents DeepBlocks, a visual programming tool that allows DL developers to design, train, and evaluate models without relying on specific programming languages. DeepBlocks works by building on the typical model structure: a sequence of learnable functions whose arrangement defines the specific characteristics of the model. We derived DeepBlocks' design goals from a 5-participants formative interview, and we validated the first implementation of the tool through a typical use case. Results are promising and show that developers could visually design complex DL architectures.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#27604;&#36739;&#37325;&#26032;&#21442;&#25968;&#21270;&#27493;&#39588;&#19982;&#24402;&#19968;&#21270;&#27493;&#39588;&#23545;BatchNorm&#25104;&#21151;&#30340;&#36129;&#29486;&#65292;&#20197;&#30740;&#31350;BatchNorm&#20013;Shift&#21644;Scale&#21442;&#25968;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.12818</link><description>&lt;p&gt;
BatchNorm&#20013;Shift&#21644;Scale&#21442;&#25968;&#30340;&#23454;&#35777;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Empirical Analysis of the Shift and Scale Parameters in BatchNorm. (arXiv:2303.12818v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#27604;&#36739;&#37325;&#26032;&#21442;&#25968;&#21270;&#27493;&#39588;&#19982;&#24402;&#19968;&#21270;&#27493;&#39588;&#23545;BatchNorm&#25104;&#21151;&#30340;&#36129;&#29486;&#65292;&#20197;&#30740;&#31350;BatchNorm&#20013;Shift&#21644;Scale&#21442;&#25968;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Batch Normalization&#65288;BatchNorm&#65289;&#26159;&#19968;&#31181;&#25913;&#21892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12290;&#34429;&#28982;&#19981;&#28165;&#26970;&#36825;&#31181;&#25913;&#36827;&#30340;&#21407;&#22240;&#65292;&#20294;&#24050;&#32463;&#32463;&#39564;&#35777;&#26126;BatchNorm&#21487;&#20197;&#22686;&#21152;&#24615;&#33021;&#65292;&#31283;&#23450;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;BatchNorm&#21253;&#25324;&#24402;&#19968;&#21270;&#27493;&#39588;&#20197;&#21450;&#21487;&#35757;&#32451;&#30340;Shift&#21644;Scale&#21442;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#24402;&#19968;&#21270;&#27493;&#39588;&#30456;&#23545;&#20110;&#31227;&#20301;&#21644;&#32553;&#25918;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#23545;BatchNorm&#25104;&#21151;&#30340;&#36129;&#29486;&#24230;&#12290;&#20026;&#20102;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#22312;PyTorch&#20013;&#23454;&#29616;&#20102;&#20004;&#20010;&#26032;&#30340;&#20248;&#21270;&#22120;&#65292;&#20998;&#21035;&#20026;&#21253;&#21547;&#37325;&#26032;&#21442;&#25968;&#21270;&#27493;&#39588;&#20294;&#19981;&#36827;&#34892;&#24402;&#19968;&#21270;&#65288;&#31216;&#20026;AffineLayer&#65289;&#21644;&#20165;&#21253;&#21547;&#24402;&#19968;&#21270;&#27493;&#39588;&#30340;&#29256;&#26412;&#65288;&#31216;&#20026;BatchNorm-minus&#65289;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;AffineLayer&#21644;BatchNorm-minus&#30340;&#24615;&#33021;&#19982;&#26631;&#20934;BatchNorm&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#23558;&#36825;&#20123;&#19982;&#19981;&#20351;&#29992;BatchNorm&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Batch Normalization (BatchNorm) is a technique that improves the training of deep neural networks, especially Convolutional Neural Networks (CNN). It has been empirically demonstrated that BatchNorm increases performance, stability, and accuracy, although the reasons for such improvements are unclear. BatchNorm includes a normalization step as well as trainable shift and scale parameters. In this paper, we empirically examine the relative contribution to the success of BatchNorm of the normalization step, as compared to the re-parameterization via shifting and scaling. To conduct our experiments, we implement two new optimizers in PyTorch, namely, a version of BatchNorm that we refer to as AffineLayer, which includes the re-parameterization step without normalization, and a version with just the normalization step, that we call BatchNorm-minus. We compare the performance of our AffineLayer and BatchNorm-minus implementations to standard BatchNorm, and we also compare these to the case 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#36890;&#36807;&#22686;&#21152;&#28145;&#24230;&#20811;&#26381;&#22240;&#37319;&#29992;&#20302;&#32500;&#23454;&#20307;&#34920;&#31034;&#32780;&#23548;&#33268;&#30340;&#27169;&#22411;&#31934;&#24230;&#19979;&#38477;&#21644;&#27169;&#22411;&#21442;&#25968;&#20943;&#23569;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12816</link><description>&lt;p&gt;
&#20174;&#23485;&#21040;&#28145;&#65306;&#32500;&#24230;&#25552;&#21319;&#32593;&#32476;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
From Wide to Deep: Dimension Lifting Network for Parameter-efficient Knowledge Graph Embedding. (arXiv:2303.12816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#36890;&#36807;&#22686;&#21152;&#28145;&#24230;&#20811;&#26381;&#22240;&#37319;&#29992;&#20302;&#32500;&#23454;&#20307;&#34920;&#31034;&#32780;&#23548;&#33268;&#30340;&#27169;&#22411;&#31934;&#24230;&#19979;&#38477;&#21644;&#27169;&#22411;&#21442;&#25968;&#20943;&#23569;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGE&#65289;&#23558;&#23454;&#20307;&#21644;&#20851;&#31995;&#26144;&#23556;&#21040;&#21521;&#37327;&#34920;&#31034;&#23545;&#20110;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;KGE&#26041;&#27861;&#38656;&#35201;&#30456;&#23545;&#39640;&#32500;&#30340;&#23454;&#20307;&#34920;&#31034;&#26469;&#20445;&#30041;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#20294;&#20250;&#23548;&#33268;&#24222;&#22823;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#36890;&#36807;&#37319;&#29992;&#20302;&#32500;&#23454;&#20307;&#34920;&#31034;&#26469;&#38477;&#20302;&#27169;&#22411;&#21442;&#25968;&#65292;&#21516;&#26102;&#24320;&#21457;&#25216;&#26415;&#65288;&#20363;&#22914;&#30693;&#35782;&#33976;&#39311;&#65289;&#26469;&#34917;&#20607;&#38477;&#32500;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#25805;&#20316;&#20250;&#23548;&#33268;&#27169;&#22411;&#31934;&#24230;&#19979;&#38477;&#21644;&#27169;&#22411;&#21442;&#25968;&#20943;&#23569;&#26377;&#38480;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#25152;&#26377;&#23454;&#20307;&#34920;&#31034;&#30340;&#32423;&#32852;&#35270;&#20026;&#23884;&#20837;&#23618;&#65292;&#37027;&#20040;&#37319;&#29992;&#39640;&#32500;&#23454;&#20307;&#34920;&#31034;&#30340;&#20256;&#32479;KGE&#26041;&#27861;&#31561;&#21516;&#20110;&#25193;&#23637;&#23884;&#20837;&#23618;&#30340;&#23485;&#24230;&#20197;&#33719;&#24471;&#34920;&#29616;&#21147;&#12290;&#20026;&#20102;&#22312;&#19981;&#29306;&#29298;&#20934;&#30830;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#21442;&#25968;&#25928;&#29575;&#65292;&#25105;&#20204;&#30456;&#21453;&#22320;&#22686;&#21152;&#28145;&#24230;&#65292;&#24182;&#25552;&#20986;&#19968;&#20010;&#26356;&#28145;&#30340;&#23454;&#20307;&#23884;&#20837;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embedding (KGE) that maps entities and relations into vector representations is essential for downstream tasks. Conventional KGE methods require relatively high-dimensional entity representations to preserve the structural information of knowledge graph, but lead to oversized model parameters. Recent methods reduce model parameters by adopting low-dimensional entity representations, while developing techniques (e.g., knowledge distillation) to compensate for the reduced dimension. However, such operations produce degraded model accuracy and limited reduction of model parameters. Specifically, we view the concatenation of all entity representations as an embedding layer, and then conventional KGE methods that adopt high-dimensional entity representations equal to enlarging the width of the embedding layer to gain expressiveness. To achieve parameter efficiency without sacrificing accuracy, we instead increase the depth and propose a deeper embedding network for entity re
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#20855;&#26377;&#23545;&#25968;S&#22411;&#28608;&#27963;&#20989;&#25968;&#30340;&#20219;&#24847;&#28145;&#24230;&#30340;&#19968;&#32500;&#31070;&#32463;&#32593;&#32476;&#26368;&#22810;&#21482;&#26377;&#19977;&#20010;&#19981;&#21160;&#28857;&#65292;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#21644;&#29702;&#35770;&#20043;&#38388;&#26500;&#24314;&#20102;&#19968;&#20010;&#24517;&#35201;&#30340;&#26725;&#26753;&#12290;</title><link>http://arxiv.org/abs/2303.12814</link><description>&lt;p&gt;
&#20219;&#24847;&#28145;&#24230;&#30340;&#19968;&#32500;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#21160;&#28857;
&lt;/p&gt;
&lt;p&gt;
Fixed points of arbitrarily deep 1-dimensional neural networks. (arXiv:2303.12814v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#20855;&#26377;&#23545;&#25968;S&#22411;&#28608;&#27963;&#20989;&#25968;&#30340;&#20219;&#24847;&#28145;&#24230;&#30340;&#19968;&#32500;&#31070;&#32463;&#32593;&#32476;&#26368;&#22810;&#21482;&#26377;&#19977;&#20010;&#19981;&#21160;&#28857;&#65292;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#21644;&#29702;&#35770;&#20043;&#38388;&#26500;&#24314;&#20102;&#19968;&#20010;&#24517;&#35201;&#30340;&#26725;&#26753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22312;$\mathbb{R}$&#19978;&#20855;&#26377;&#21512;&#25104;&#24615;&#19988;&#21253;&#21547;&#23545;&#25968;S&#22411;&#20989;&#25968;&#30340;&#26032;&#20989;&#25968;&#31867;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#31867;&#26469;&#35777;&#26126;&#20855;&#26377;&#23545;&#25968;S&#22411;&#28608;&#27963;&#20989;&#25968;&#30340;&#20219;&#24847;&#28145;&#24230;&#30340;&#19968;&#32500;&#31070;&#32463;&#32593;&#32476;&#26368;&#22810;&#21482;&#26377;&#19977;&#20010;&#19981;&#21160;&#28857;&#12290;&#34429;&#28982;&#36825;&#26679;&#30340;&#31070;&#32463;&#32593;&#32476;&#36828;&#31163;&#23454;&#38469;&#24212;&#29992;&#65292;&#20294;&#25105;&#20204;&#33021;&#22815;&#23436;&#20840;&#29702;&#35299;&#23427;&#20204;&#30340;&#19981;&#21160;&#28857;&#65292;&#24182;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#21644;&#29702;&#35770;&#20043;&#38388;&#26500;&#24314;&#20102;&#19968;&#20010;&#24517;&#35201;&#30340;&#26725;&#26753;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a new class of functions on $\mathbb{R}$ that is closed under composition, and contains the logistic sigmoid function. We use this class to show that any 1-dimensional neural network of arbitrary depth with logistic sigmoid activation functions has at most three fixed points. While such neural networks are far from real world applications, we are able to completely understand their fixed points, providing a foundation to the much needed connection between application and theory of deep neural networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#35270;&#20026;&#22270;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#22522;&#20110;&#26412;&#22320;&#24230;&#37327;&#21078;&#38754;&#29305;&#24449;&#65292;&#20351;&#29992;&#22810;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26550;&#26500;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;GNN&#27169;&#22411;&#20248;&#20110;&#20197;&#21069;&#30340;&#21487;&#27604;&#30740;&#31350;&#65292;&#19988;&#19981;&#20250;&#36973;&#21463;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12812</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#20013;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
A Comparison of Graph Neural Networks for Malware Classification. (arXiv:2303.12812v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#35270;&#20026;&#22270;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#22522;&#20110;&#26412;&#22320;&#24230;&#37327;&#21078;&#38754;&#29305;&#24449;&#65292;&#20351;&#29992;&#22810;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26550;&#26500;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;GNN&#27169;&#22411;&#20248;&#20110;&#20197;&#21069;&#30340;&#21487;&#27604;&#30740;&#31350;&#65292;&#19988;&#19981;&#20250;&#36973;&#21463;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31649;&#29702;&#24694;&#24847;&#36719;&#20214;&#23041;&#32961;&#38656;&#35201;&#20934;&#30830;&#30340;&#26816;&#27979;&#21644;&#20998;&#31867;&#25216;&#26415;&#12290;&#20256;&#32479;&#30340;&#26816;&#27979;&#31574;&#30053;&#20381;&#36182;&#20110;&#38024;&#23545;&#24694;&#24847;&#36719;&#20214;&#30340;&#25163;&#21160;&#20998;&#26512;&#65292;&#25552;&#21462;&#30456;&#20851;&#29305;&#24449;&#30340;&#36807;&#31243;&#36153;&#26102;&#36153;&#21147;&#65292;&#38656;&#35201;&#19987;&#23478;&#30693;&#35782;&#12290;&#20989;&#25968;&#35843;&#29992;&#22270;&#30001;&#19968;&#32452;&#31243;&#24207;&#20989;&#25968;&#21644;&#23427;&#20204;&#30340;&#36807;&#31243;&#35843;&#29992;&#32452;&#25104;&#65292;&#25552;&#20379;&#20102;&#21487;&#29992;&#20110;&#20998;&#31867;&#24694;&#24847;&#36719;&#20214;&#30340;&#20016;&#23500;&#20449;&#24687;&#26469;&#28304;&#65292;&#26080;&#38656;&#20256;&#32479;&#25216;&#26415;&#20013;&#36153;&#21147;&#25552;&#21462;&#29305;&#24449;&#30340;&#27493;&#39588;&#12290;&#26412;&#30740;&#31350;&#23558;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#35270;&#20026;&#22270;&#20998;&#31867;&#38382;&#39064;&#65292;&#22522;&#20110;&#26412;&#22320;&#24230;&#37327;&#21078;&#38754;&#29305;&#24449;&#65292;&#35757;&#32451;&#20102;&#22810;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26550;&#26500;&#26469;&#29983;&#25104;&#23884;&#20837;&#65292;&#28982;&#21518;&#36827;&#34892;&#20998;&#31867;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25105;&#20204;&#26368;&#22909;&#30340;GNN&#27169;&#22411;&#20248;&#20110;&#20197;&#21069;&#30340;&#21487;&#27604;&#30740;&#31350;&#65292;&#21253;&#25324;&#30693;&#21517;&#30340;MalNet-Tiny Android&#24694;&#24847;&#36719;&#20214;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;GNN&#27169;&#22411;&#19981;&#20250;&#36973;&#21463;&#24120;&#35265;&#30340;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Managing the threat posed by malware requires accurate detection and classification techniques. Traditional detection strategies, such as signature scanning, rely on manual analysis of malware to extract relevant features, which is labor intensive and requires expert knowledge. Function call graphs consist of a set of program functions and their inter-procedural calls, providing a rich source of information that can be leveraged to classify malware without the labor intensive feature extraction step of traditional techniques. In this research, we treat malware classification as a graph classification problem. Based on Local Degree Profile features, we train a wide range of Graph Neural Network (GNN) architectures to generate embeddings which we then classify. We find that our best GNN models outperform previous comparable research involving the well-known MalNet-Tiny Android malware dataset. In addition, our GNN models do not suffer from the overfitting issues that commonly afflict non
&lt;/p&gt;</description></item><item><title>SignCRF&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26080;&#39057;&#36947;&#25968;&#25454;&#39537;&#21160;&#23556;&#39057;&#35748;&#35777;&#24179;&#21488;&#65292;&#33021;&#22815;&#39640;&#31934;&#24230;&#22320;&#35782;&#21035;&#26080;&#32447;&#35774;&#22791;&#65292;&#19981;&#21463;&#31227;&#21160;&#24615;&#24102;&#26469;&#30340;&#21160;&#24577;&#20449;&#36947;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.12811</link><description>&lt;p&gt;
SignCRF: &#21487;&#25193;&#23637;&#30340;&#26080;&#39057;&#36947;&#25968;&#25454;&#39537;&#21160;&#23556;&#39057;&#35748;&#35777;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
SignCRF: Scalable Channel-agnostic Data-driven Radio Authentication System. (arXiv:2303.12811v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12811
&lt;/p&gt;
&lt;p&gt;
SignCRF&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26080;&#39057;&#36947;&#25968;&#25454;&#39537;&#21160;&#23556;&#39057;&#35748;&#35777;&#24179;&#21488;&#65292;&#33021;&#22815;&#39640;&#31934;&#24230;&#22320;&#35782;&#21035;&#26080;&#32447;&#35774;&#22791;&#65292;&#19981;&#21463;&#31227;&#21160;&#24615;&#24102;&#26469;&#30340;&#21160;&#24577;&#20449;&#36947;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#32447;&#30005;&#39057;&#29575;&#25351;&#32441;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;(RFFDL)&#26159;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#29289;&#32852;&#32593;&#36523;&#20221;&#35748;&#35777;&#25216;&#26415;&#65292;&#21033;&#29992;&#19982;&#29305;&#23450;&#35774;&#22791;&#30456;&#20851;&#30340;&#29420;&#29305;&#30828;&#20214;&#32423;&#21046;&#36896;&#32570;&#38519;&#26469;&#35782;&#21035;&#65288;&#25351;&#32441;&#65289;&#22522;&#20110;&#20256;&#36755;&#27874;&#24418;&#24341;&#20837;&#30340;&#21464;&#21270;&#30340;&#35774;&#22791;&#12290;&#25552;&#20986;&#30340;SignCRF&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#12289;&#26080;&#39057;&#36947;&#30340;&#12289;&#25968;&#25454;&#39537;&#21160;&#30340;&#23556;&#39057;&#35748;&#35777;&#24179;&#21488;&#65292;&#22312;&#35782;&#21035;&#22522;&#20110;&#20854;&#29420;&#29305;&#30340;&#21046;&#36896;&#32570;&#38519;&#30340;&#26080;&#32447;&#35774;&#22791;&#26041;&#38754;&#20855;&#26377;&#26080;&#19982;&#20262;&#27604;&#30340;&#31934;&#24230;&#65292;&#24182;&#19988;&#29420;&#31435;&#20110;&#30001;&#31227;&#21160;&#24615;&#24341;&#36215;&#30340;&#21160;&#24577;&#20449;&#36947;&#19981;&#35268;&#21017;&#24615;&#12290;SignCRF&#30001;&#19977;&#37096;&#20998;&#32452;&#25104;&#65306;(i)&#22522;&#32447;&#20998;&#31867;&#22120;&#65292;&#32463;&#36807;&#31934;&#32454;&#35757;&#32451;&#65292;&#33021;&#22815;&#39640;&#31934;&#24230;&#22320;&#25193;&#23637;&#35748;&#35777;&#35774;&#22791;;(ii)&#29615;&#22659;&#32763;&#35793;&#22120;&#65292;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#21644;&#35757;&#32451;&#65292;&#33021;&#22815;&#20174;RF&#20449;&#21495;&#20013;&#21435;&#38500;&#21160;&#24577;&#20449;&#36947;&#24433;&#21709;&#65292;&#21516;&#26102;&#20445;&#25345;&#25910;&#21457;&#26426;&#20855;&#20307;&#30340;&#20449;&#21495;&#65307;(iii)&#26368;&#22823;&#35268;&#21017;&#27169;&#22359;&#36873;&#25321;&#22522;&#32447;&#20998;&#31867;&#22120;&#21644;&#29615;&#22659;&#32763;&#35793;&#22120;&#20043;&#38388;&#30340;&#26368;&#39640;&#31934;&#24230;&#35748;&#35777;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Radio Frequency Fingerprinting through Deep Learning (RFFDL) is a data-driven IoT authentication technique that leverages the unique hardware-level manufacturing imperfections associated with a particular device to recognize (fingerprint) the device based on variations introduced in the transmitted waveform. The proposed SignCRF is a scalable, channel-agnostic, data-driven radio authentication platform with unmatched precision in fingerprinting wireless devices based on their unique manufacturing impairments and independent of the dynamic channel irregularities caused by mobility. SignCRF consists of (i) a baseline classifier finely trained to authenticate devices with high accuracy and at scale; (ii) an environment translator carefully designed and trained to remove the dynamic channel impact from RF signals while maintaining the radio's specific signature; (iii) a Max-Rule module that selects the highest precision authentication technique between the baseline classifier and the envir
&lt;/p&gt;</description></item><item><title>&#31890;&#29699;&#20248;&#21270;&#31639;&#27861;(GBO)&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#31890;&#24230;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#24341;&#20837;&#31890;&#29699;&#35745;&#31639;&#26469;&#25552;&#39640;&#20840;&#23616;&#25628;&#32034;&#33021;&#21147;&#21644;&#25910;&#25947;&#36895;&#24230;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36825;&#20123;&#26041;&#38754;&#23427;&#27604;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2303.12807</link><description>&lt;p&gt;
&#31890;&#29699;&#20248;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Granular-ball Optimization Algorithm. (arXiv:2303.12807v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12807
&lt;/p&gt;
&lt;p&gt;
&#31890;&#29699;&#20248;&#21270;&#31639;&#27861;(GBO)&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#31890;&#24230;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#24341;&#20837;&#31890;&#29699;&#35745;&#31639;&#26469;&#25552;&#39640;&#20840;&#23616;&#25628;&#32034;&#33021;&#21147;&#21644;&#25910;&#25947;&#36895;&#24230;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36825;&#20123;&#26041;&#38754;&#23427;&#27604;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#26234;&#33021;&#20248;&#21270;&#31639;&#27861;&#37117;&#26159;&#22522;&#20110;&#26368;&#23567;&#31890;&#24230;&#21363;&#28857;&#30340;&#35774;&#35745;&#65292;&#23548;&#33268;&#20840;&#23616;&#25628;&#32034;&#33021;&#21147;&#36739;&#24369;&#19988;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#31890;&#24230;&#20248;&#21270;&#31639;&#27861;&#65292;&#21363;&#31890;&#29699;&#20248;&#21270;&#31639;&#27861;(GBO)&#65292;&#36890;&#36807;&#24341;&#20837;&#31890;&#29699;&#35745;&#31639;&#26469;&#23454;&#29616;&#12290;GBO&#20351;&#29992;&#22810;&#20010;&#31890;&#29699;&#26469;&#35206;&#30422;&#35299;&#31354;&#38388;&#65292;&#20351;&#29992;&#35768;&#22810;&#32454;&#23567;&#30340;&#32454;&#31890;&#24230;&#31890;&#29699;&#26469;&#25551;&#36848;&#37325;&#35201;&#37096;&#20998;&#65292;&#20351;&#29992;&#23569;&#37327;&#30340;&#22823;&#31895;&#31890;&#24230;&#31890;&#29699;&#26469;&#25551;&#36848;&#19981;&#37325;&#35201;&#30340;&#37096;&#20998;&#65292;&#31934;&#32454;&#30340;&#22810;&#31890;&#24230;&#25968;&#25454;&#25551;&#36848;&#33021;&#21147;&#25552;&#39640;&#20102;&#20840;&#23616;&#25628;&#32034;&#33021;&#21147;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;&#38024;&#23545;&#20108;&#21313;&#20010;&#22522;&#20934;&#20989;&#25968;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#27969;&#34892;&#30340;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#30456;&#27604;&#65292;GBO&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#24555;&#30340;&#36895;&#24230;&#65292;&#26356;&#25509;&#36817;&#26368;&#20248;&#35299;&#65292;&#27809;&#26377;&#36229;&#21442;&#25968;&#65292;&#35774;&#35745;&#26356;&#31616;&#21333;&#12290;
&lt;/p&gt;
&lt;p&gt;
The existing intelligent optimization algorithms are designed based on the finest granularity, i.e., a point. This leads to weak global search ability and inefficiency. To address this problem, we proposed a novel multi-granularity optimization algorithm, namely granular-ball optimization algorithm (GBO), by introducing granular-ball computing. GBO uses many granular-balls to cover the solution space. Quite a lot of small and fine-grained granular-balls are used to depict the important parts, and a little number of large and coarse-grained granular-balls are used to depict the inessential parts. Fine multi-granularity data description ability results in a higher global search capability and faster convergence speed. In comparison with the most popular and state-of-the-art algorithms, the experiments on twenty benchmark functions demonstrate its better performance. The faster speed, higher approximation ability of optimal solution, no hyper-parameters, and simpler design of GBO make it 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#20854;&#33021;&#22815;&#23545;&#40657;&#33394;&#32032;&#30244;&#21644;&#30179;&#36827;&#34892;&#35786;&#26029;&#65292;&#24182;&#33021;&#22815;&#32473;&#20986;&#26131;&#20110;&#35299;&#37322;&#30340;&#25991;&#26412;&#21644;&#21306;&#22495;&#35299;&#37322;&#65292;&#35813;&#31995;&#32479;&#33021;&#26174;&#33879;&#25552;&#39640;&#20020;&#24202;&#21307;&#29983;&#30340;&#20934;&#30830;&#24615;&#12289;&#20449;&#24515;&#21644;&#23545;&#20854;XAI&#25903;&#25345;&#30340;&#20449;&#20219;&#12290;</title><link>http://arxiv.org/abs/2303.12806</link><description>&lt;p&gt;
&#31867;&#30382;&#32932;&#31185;&#21307;&#29983;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#40657;&#33394;&#32032;&#30244;&#30340;&#35786;&#26029;&#20449;&#20219;&#21644;&#20449;&#24515;
&lt;/p&gt;
&lt;p&gt;
Dermatologist-like explainable AI enhances trust and confidence in diagnosing melanoma. (arXiv:2303.12806v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12806
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#20854;&#33021;&#22815;&#23545;&#40657;&#33394;&#32032;&#30244;&#21644;&#30179;&#36827;&#34892;&#35786;&#26029;&#65292;&#24182;&#33021;&#22815;&#32473;&#20986;&#26131;&#20110;&#35299;&#37322;&#30340;&#25991;&#26412;&#21644;&#21306;&#22495;&#35299;&#37322;&#65292;&#35813;&#31995;&#32479;&#33021;&#26174;&#33879;&#25552;&#39640;&#20020;&#24202;&#21307;&#29983;&#30340;&#20934;&#30830;&#24615;&#12289;&#20449;&#24515;&#21644;&#23545;&#20854;XAI&#25903;&#25345;&#30340;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#24050;&#34987;&#35777;&#26126;&#33021;&#25552;&#39640;&#40657;&#33394;&#32032;&#30244;&#30340;&#21021;&#27493;&#35786;&#26029;&#20934;&#30830;&#24615;&#65292;&#20294;&#36825;&#20123;&#31995;&#32479;&#22312;&#22914;&#20309;&#35782;&#21035;&#40657;&#32032;&#30244;&#26041;&#38754;&#32570;&#20047;&#36879;&#26126;&#24230;&#65292;&#36825;&#32473;&#29992;&#25143;&#25509;&#21463;&#24102;&#26469;&#20102;&#20005;&#37325;&#38556;&#30861;&#12290;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#21487;&#20197;&#22686;&#21152;&#36879;&#26126;&#24230;&#65292;&#20294;&#22823;&#22810;&#25968;XAI&#26041;&#27861;&#26080;&#27861;&#20135;&#29983;&#26126;&#30830;&#23450;&#20301;&#30340;&#39046;&#22495;&#29305;&#23450;&#35299;&#37322;&#65292;&#20351;&#24471;&#35299;&#37322;&#38590;&#20197;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;XAI&#26041;&#27861;&#23545;&#30382;&#32932;&#31185;&#21307;&#29983;&#30340;&#24433;&#21709;&#23578;&#26410;&#35780;&#20272;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#20004;&#20010;&#29616;&#26377;&#20998;&#31867;&#22120;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;XAI&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;&#19981;&#21516;&#35786;&#26029;&#30340;&#22522;&#30784;&#19978;&#20135;&#29983;&#20102;&#25991;&#26412;&#21644;&#21306;&#22495;&#35299;&#37322;&#65292;&#36825;&#20123;&#35299;&#37322;&#23481;&#26131;&#34987;&#30382;&#32932;&#31185;&#21307;&#29983;&#35299;&#37322;&#12290;&#20026;&#20102;&#35780;&#20272;&#27492;&#31995;&#32479;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#19977;&#37096;&#20998;&#30340;&#35835;&#32773;&#30740;&#31350;&#65292;&#20197;&#35780;&#20272;&#20854;&#23545;&#20020;&#24202;&#21307;&#29983;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#12289;&#20449;&#24515;&#21644;&#23545;XAI&#25903;&#25345;&#30340;&#20449;&#20219;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;XAI&#35299;&#37322;&#19982;&#21307;&#29983;&#30340;&#35299;&#37322;&#39640;&#24230;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although artificial intelligence (AI) systems have been shown to improve the accuracy of initial melanoma diagnosis, the lack of transparency in how these systems identify melanoma poses severe obstacles to user acceptance. Explainable artificial intelligence (XAI) methods can help to increase transparency, but most XAI methods are unable to produce precisely located domain-specific explanations, making the explanations difficult to interpret. Moreover, the impact of XAI methods on dermatologists has not yet been evaluated. Extending on two existing classifiers, we developed an XAI system that produces text and region based explanations that are easily interpretable by dermatologists alongside its differential diagnoses of melanomas and nevi. To evaluate this system, we conducted a three-part reader study to assess its impact on clinicians' diagnostic accuracy, confidence, and trust in the XAI-support. We showed that our XAI's explanations were highly aligned with clinicians' explanati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36827;&#34892;&#29305;&#24449;&#21305;&#37197;&#30340;&#26032;&#28151;&#21512;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#20943;&#23569;&#21305;&#37197;&#19981;&#21516;&#25968;&#25454;&#38598;&#25152;&#38656;&#30340;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2303.12804</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36827;&#34892;&#29305;&#24449;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Features matching using natural language processing. (arXiv:2303.12804v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36827;&#34892;&#29305;&#24449;&#21305;&#37197;&#30340;&#26032;&#28151;&#21512;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#20943;&#23569;&#21305;&#37197;&#19981;&#21516;&#25968;&#25454;&#38598;&#25152;&#38656;&#30340;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#21305;&#37197;&#26159;&#21305;&#37197;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#22522;&#26412;&#27493;&#39588;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#30001;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;BERT&#27169;&#22411;&#19982;&#22522;&#20110;Jaccard&#30456;&#20284;&#24230;&#30340;&#32479;&#35745;&#27169;&#22411;&#24182;&#34892;&#20351;&#29992;&#65292;&#20197;&#27979;&#37327;&#20004;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#20013;&#29305;&#24449;&#21015;&#34920;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#36825;&#20943;&#23569;&#20102;&#25628;&#32034;&#30456;&#20851;&#24615;&#25110;&#25163;&#21160;&#21305;&#37197;&#27599;&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;&#29305;&#24449;&#25152;&#38656;&#30340;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
The feature matching is a basic step in matching different datasets. This article proposes shows a new hybrid model of a pretrained Natural Language Processing (NLP) based model called BERT used in parallel with a statistical model based on Jaccard similarity to measure the similarity between list of features from two different datasets. This reduces the time required to search for correlations or manually match each feature from one dataset to another.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#20998;&#24067;&#24335;&#23398;&#20064;&#21644;FL&#31574;&#30053;&#30340;&#26041;&#27861;&#22914;&#20309;&#24110;&#21161;&#23454;&#29616;6G&#32593;&#32476;&#19979;&#30340;&#20005;&#26684;KPI&#35201;&#27714;&#65292;&#21516;&#26102;&#24378;&#35843;&#20102;&#23558;DL&#26041;&#27861;&#24212;&#29992;&#20110;6G&#32593;&#32476;&#26102;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.12802</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#23398;&#20064; meets 6G&#65306;&#36890;&#20449;&#21644;&#35745;&#31639;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Distributed Learning Meets 6G: A Communication and Computing Perspective. (arXiv:2303.12802v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#20998;&#24067;&#24335;&#23398;&#20064;&#21644;FL&#31574;&#30053;&#30340;&#26041;&#27861;&#22914;&#20309;&#24110;&#21161;&#23454;&#29616;6G&#32593;&#32476;&#19979;&#30340;&#20005;&#26684;KPI&#35201;&#27714;&#65292;&#21516;&#26102;&#24378;&#35843;&#20102;&#23558;DL&#26041;&#27861;&#24212;&#29992;&#20110;6G&#32593;&#32476;&#26102;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31227;&#21160;&#35774;&#22791;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;&#23384;&#20648;&#33021;&#21147;&#19981;&#26029;&#25552;&#39640;&#65292;&#24182;&#37197;&#21512;&#19981;&#26029;&#36827;&#21270;&#30340;&#30005;&#20449;&#32593;&#32476;&#33539;&#24335;&#65292;&#20154;&#20204;&#19981;&#26029;&#25506;&#32034;&#20998;&#24067;&#24335;&#23398;&#20064;&#65288;DL&#65289;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#19979;&#19968;&#20195;/6G&#34562;&#31389;&#32593;&#32476;&#20013;&#25152;&#26399;&#26395;&#30340;&#20005;&#26684;&#22522;&#26412;&#24615;&#33021;&#25351;&#26631;&#65288;KPI&#65289;&#12290;&#22312;&#36793;&#32536;&#35745;&#31639;&#30340;&#21327;&#20316;&#19979;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#32463;&#25104;&#20026;&#26480;&#20986;&#26080;&#32447;&#24212;&#29992;&#20013;&#30340;DL &#26550;&#26500;&#36873;&#25321;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;DL&#26694;&#26550;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;FL&#31574;&#30053;&#65292;&#22914;&#20309;&#26377;&#21161;&#20110;&#23454;&#29616;6G&#24895;&#26223;&#30340;&#19968;&#37096;&#20998;&#65292;&#24182;&#22312;&#36890;&#20449;&#21644;&#35745;&#31639;&#32422;&#26463;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#20316;&#20026;&#19968;&#20010;&#23454;&#38469;&#24212;&#29992;&#30340;&#26696;&#20363;&#65292;&#25105;&#20204;&#22312;FL&#26694;&#26550;&#20869;&#24212;&#29992;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#21040;&#21160;&#24577;&#39057;&#35889;&#35775;&#38382;&#65288;DSA&#65289;&#38382;&#39064;&#65292;&#24182;&#21576;&#29616;&#20102;&#21021;&#27493;&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;&#21516;&#26102;&#26412;&#25991;&#36824;&#24378;&#35843;&#20102;&#23558;DL&#26041;&#27861;&#24212;&#29992;&#20110;6G&#32593;&#32476;&#26102;&#25152;&#38754;&#20020;&#30340;&#24403;&#21069;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the ever-improving computing capabilities and storage capacities of mobile devices in line with evolving telecommunication network paradigms, there has been an explosion of research interest towards exploring Distributed Learning (DL) frameworks to realize stringent key performance indicators (KPIs) that are expected in next-generation/6G cellular networks. In conjunction with Edge Computing, Federated Learning (FL) has emerged as the DL architecture of choice in prominent wireless applications. This article lays an outline of how DL in general and FL-based strategies specifically can contribute towards realizing a part of the 6G vision and strike a balance between communication and computing constraints. As a practical use case, we apply Multi-Agent Reinforcement Learning (MARL) within the FL framework to the Dynamic Spectrum Access (DSA) problem and present preliminary evaluation results. Top contemporary challenges in applying DL approaches to 6G networks are also highlighted.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21644;&#23884;&#20837;&#26426;&#21046;&#65292;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#23567;&#26679;&#26412;&#32954;&#32467;&#33410;&#30340;&#26816;&#27979;&#21644;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2303.12801</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#23567;&#26679;&#26412;&#32954;&#32467;&#33410;&#26816;&#27979;&#21644;&#20998;&#31867;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21644;&#23884;&#20837;&#26426;&#21046; (arXiv:2303.12801v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
A Data Augmentation Method and the Embedding Mechanism for Detection and Classification of Pulmonary Nodules on Small Samples. (arXiv:2303.12801v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12801
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21644;&#23884;&#20837;&#26426;&#21046;&#65292;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#23567;&#26679;&#26412;&#32954;&#32467;&#33410;&#30340;&#26816;&#27979;&#21644;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CT&#25195;&#25551;&#32954;&#32467;&#33410;&#26816;&#27979;&#34987;&#29992;&#20110;&#32954;&#30284;&#30340;&#26089;&#26399;&#31579;&#26597;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;(CAD)&#21487;&#20197;&#35782;&#21035;CT&#22270;&#20687;&#20013;&#21487;&#30097;&#30340;&#32954;&#32467;&#33410;&#21306;&#22495;&#65292;&#20174;&#32780;&#25552;&#39640;CT&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20004;&#31181;&#31574;&#30053;&#65306;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21644;&#22522;&#20110;&#23884;&#20837;&#26426;&#21046;&#30340;&#27169;&#22411;&#32467;&#26500;&#25913;&#36827;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21644;&#19968;&#31181;&#23884;&#20837;&#26426;&#21046;&#65292;&#21069;&#32773;&#21033;&#29992;3D&#20687;&#32032;&#32423;&#32479;&#35745;&#31639;&#27861;&#29983;&#25104;&#32954;&#32467;&#33410;&#65292;&#21518;&#32773;&#36890;&#36807;&#24341;&#20837;&#38544;&#34255;&#21464;&#37327;&#26469;&#26356;&#22909;&#22320;&#29702;&#35299;&#32954;&#32467;&#33410;&#26679;&#26412;&#20687;&#32032;&#30340;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detection of pulmonary nodules by CT is used for screening lung cancer in early stages.omputer aided diagnosis (CAD) based on deep-learning method can identify the suspected areas of pulmonary nodules in CT images, thus improving the accuracy and efficiency of CT diagnosis. The accuracy and robustness of deep learning models. Method:In this paper, we explore (1) the data augmentation method based on the generation model and (2) the model structure improvement method based on the embedding mechanism. Two strategies have been introduced in this study: a new data augmentation method and a embedding mechanism. In the augmentation method, a 3D pixel-level statistics algorithm is proposed to generate pulmonary nodule and by combing the faked pulmonary nodule and healthy lung, we generate new pulmonary nodule samples. The embedding mechanism are designed to better understand the meaning of pixels of the pulmonary nodule samples by introducing hidden variables. Result: The result of the 3DVNET
&lt;/p&gt;</description></item><item><title>&#20869;&#37096;&#32593;&#32476;&#20013;&#20801;&#35768;&#36830;&#25509;&#30340;IoT&#35774;&#22791;&#21644;&#26410;&#30693;&#30340;IoT&#35774;&#22791;&#30340;&#35782;&#21035;&#21464;&#24471;&#36234;&#21457;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#35782;&#21035;&#26041;&#27861;&#65292;&#21487;&#20197;&#19981;&#38656;&#23545;&#32593;&#32476;&#36890;&#20449;&#36827;&#34892;&#22797;&#26434;&#30340;&#29305;&#24449;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2303.12800</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;IoT&#35774;&#22791;&#32593;&#32476;&#36890;&#20449;&#20998;&#26512;&#19982;&#35782;&#21035;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
IoT Device Identification Based on Network Communication Analysis Using Deep Learning. (arXiv:2303.12800v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12800
&lt;/p&gt;
&lt;p&gt;
&#20869;&#37096;&#32593;&#32476;&#20013;&#20801;&#35768;&#36830;&#25509;&#30340;IoT&#35774;&#22791;&#21644;&#26410;&#30693;&#30340;IoT&#35774;&#22791;&#30340;&#35782;&#21035;&#21464;&#24471;&#36234;&#21457;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#35782;&#21035;&#26041;&#27861;&#65292;&#21487;&#20197;&#19981;&#38656;&#23545;&#32593;&#32476;&#36890;&#20449;&#36827;&#34892;&#22797;&#26434;&#30340;&#29305;&#24449;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#19981;&#23433;&#20840;&#30340;IoT&#35774;&#22791;&#30340;&#20351;&#29992;&#65292;&#23545;&#20110;&#25915;&#20987;&#32773;&#26469;&#35828;&#65292;&#20837;&#20405;&#32452;&#32455;&#30340;&#25915;&#20987;&#26041;&#24335;&#36234;&#21457;&#22810;&#26679;&#12290;BYOD&#25919;&#31574;&#20351;&#24471;&#21592;&#24037;&#21487;&#20197;&#25658;&#24102;IoT&#35774;&#22791;&#36827;&#20837;&#32452;&#32455;&#24182;&#36830;&#25509;&#21040;&#32452;&#32455;&#30340;&#32593;&#32476;&#65292;&#21516;&#26102;&#20063;&#22686;&#21152;&#20102;&#32452;&#32455;&#32593;&#32476;&#21463;&#25915;&#20987;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#23041;&#32961;&#21644;&#20445;&#25252;&#32593;&#32476;&#65292;&#32452;&#32455;&#36890;&#24120;&#23454;&#26045;&#23433;&#20840;&#31574;&#30053;&#65292;&#21482;&#20801;&#35768;&#21015;&#20837;&#30333;&#21517;&#21333;&#30340;IoT&#35774;&#22791;&#36830;&#25509;&#21040;&#32593;&#32476;&#12290;&#20026;&#20102;&#30417;&#27979;&#36825;&#31181;&#31574;&#30053;&#30340;&#21512;&#35268;&#24615;&#65292;&#35782;&#21035;&#32452;&#32455;&#32593;&#32476;&#20869;&#20801;&#35768;&#36830;&#25509;&#30340;IoT&#35774;&#22791;&#19982;&#19981;&#22312;&#30333;&#21517;&#21333;&#20013;&#65288;&#26410;&#30693;&#30340;&#65289;IoT&#35774;&#22791;&#20043;&#38388;&#30340;&#24046;&#24322;&#24050;&#32463;&#21464;&#24471;&#38750;&#24120;&#20851;&#38190;&#12290;&#26412;&#30740;&#31350;&#23558;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#32593;&#32476;&#36890;&#20449;&#20013;&#65292;&#23454;&#29616;&#20102;&#23545;&#32593;&#32476;&#20869;IoT&#35774;&#22791;&#30340;&#33258;&#21160;&#35782;&#21035;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#23545;&#32593;&#32476;&#36890;&#20449;&#36827;&#34892;&#22797;&#26434;&#30340;&#29305;&#24449;&#24037;&#31243;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attack vectors for adversaries have increased in organizations because of the growing use of less secure IoT devices. The risk of attacks on an organization's network has also increased due to the bring your own device (BYOD) policy which permits employees to bring IoT devices onto the premises and attach them to the organization's network. To tackle this threat and protect their networks, organizations generally implement security policies in which only white listed IoT devices are allowed on the organization's network. To monitor compliance with such policies, it has become essential to distinguish IoT devices permitted within an organization's network from non white listed (unknown) IoT devices. In this research, deep learning is applied to network communication for the automated identification of IoT devices permitted on the network. In contrast to existing methods, the proposed approach does not require complex feature engineering of the network communication, because the 'communi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#32447;&#22270;&#20687;&#65292;&#24182;&#36866;&#24212;&#24378;&#22823;&#30340;&#35270;&#35273;transformer&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#31616;&#21270;&#20102;&#31639;&#27861;&#35774;&#35745;&#65292;&#20855;&#26377;&#36890;&#29992;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#21307;&#30103;&#21644;&#20154;&#20307;&#27963;&#21160;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#19987;&#19994;&#31639;&#27861;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.12799</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#35270;&#20026;&#22270;&#20687;&#65306;&#29992;&#35270;&#35273;transformer&#22788;&#29702;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Time Series as Images: Vision Transformer for Irregularly Sampled Time Series. (arXiv:2303.12799v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#32447;&#22270;&#20687;&#65292;&#24182;&#36866;&#24212;&#24378;&#22823;&#30340;&#35270;&#35273;transformer&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#31616;&#21270;&#20102;&#31639;&#27861;&#35774;&#35745;&#65292;&#20855;&#26377;&#36890;&#29992;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#21307;&#30103;&#21644;&#20154;&#20307;&#27963;&#21160;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#19987;&#19994;&#31639;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#65292;&#23588;&#20854;&#26159;&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#65292;&#19981;&#35268;&#21017;&#25277;&#26679;&#30340;&#26102;&#38388;&#24207;&#21015;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#39640;&#24230;&#23450;&#21046;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#19981;&#35268;&#21017;&#24615;&#38382;&#39064;&#65292;&#20294;&#22914;&#20309;&#26377;&#25928;&#22320;&#27169;&#25311;&#23427;&#20204;&#30340;&#22797;&#26434;&#21160;&#24577;&#21644;&#39640;&#31232;&#30095;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20174;&#20840;&#26032;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65306;&#23558;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#32447;&#22270;&#20687;&#65292;&#24182;&#35843;&#25972;&#24378;&#22823;&#30340;&#35270;&#35273;transformer&#20197;&#25191;&#34892;&#19982;&#22270;&#20687;&#20998;&#31867;&#30456;&#21516;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#20551;&#35774;&#20808;&#21069;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#22823;&#22823;&#31616;&#21270;&#20102;&#31639;&#27861;&#35774;&#35745;&#65292;&#24182;&#19988;&#21487;&#20197;&#34987;&#28508;&#22312;&#22320;&#25193;&#23637;&#20026;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#12290;&#23613;&#31649;&#20854;&#31616;&#21333;&#24615;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#22312;&#20960;&#20010;&#27969;&#34892;&#30340;&#21307;&#30103;&#20445;&#20581;&#21644;&#20154;&#20307;&#27963;&#21160;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#19987;&#19994;&#31639;&#27861;&#12290;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31163;&#20256;&#24863;&#22120;&#35774;&#32622;&#20013;&#65292;&#21363;&#22312;&#27979;&#35797;&#26399;&#38388;&#23631;&#34109;&#21464;&#37327;&#30340;&#23376;&#38598;&#20013;&#65292;&#24615;&#33021;&#27604;&#26368;&#20339;&#22522;&#20934;&#25552;&#39640;&#20102;&#39640;&#36798;11&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Irregularly sampled time series are becoming increasingly prevalent in various domains, especially in medical applications. Although different highly-customized methods have been proposed to tackle irregularity, how to effectively model their complicated dynamics and high sparsity is still an open problem. This paper studies the problem from a whole new perspective: transforming irregularly sampled time series into line graph images and adapting powerful vision transformers to perform time series classification in the same way as image classification. Our approach largely simplifies algorithm designs without assuming prior knowledge and can be potentially extended as a general-purpose framework. Despite its simplicity, we show that it substantially outperforms state-of-the-art specialized algorithms on several popular healthcare and human activity datasets. Especially in the challenging leave-sensors-out setting where a subset of variables is masked during testing, the performance impr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;ImmTrack&#65292;&#23427;&#20351;&#29992;&#27627;&#31859;&#27874;&#38647;&#36798;&#21644;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#25968;&#25454;&#26469;&#36319;&#36394;&#20154;&#38469;&#36317;&#31163;&#65292;&#24182;&#21487;&#20197;&#23558;&#24815;&#24615;&#25968;&#25454;&#36716;&#31227;&#21040;&#38647;&#36798;&#24863;&#27979;&#32467;&#26524;&#20013;&#12290;&#22312;&#26356;&#24191;&#27867;&#30340;&#24847;&#20041;&#19978;&#65292;&#35774;&#22791;&#26159;&#31532;&#19968;&#20010;&#34701;&#21512;&#27627;&#31859;&#27874;&#38647;&#36798;&#21644;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#25968;&#25454;&#29992;&#20110;&#21516;&#26102;&#29992;&#25143;&#36319;&#36394;&#21644;&#37325;&#26032;&#35782;&#21035;&#30340;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2303.12798</link><description>&lt;p&gt;
&#29992;&#27627;&#31859;&#27874;&#38647;&#36798;&#21644;IMUs&#23454;&#29616;&#20154;&#38469;&#36317;&#31163;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Interpersonal Distance Tracking with mmWave Radar and IMUs. (arXiv:2303.12798v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ImmTrack&#65292;&#23427;&#20351;&#29992;&#27627;&#31859;&#27874;&#38647;&#36798;&#21644;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#25968;&#25454;&#26469;&#36319;&#36394;&#20154;&#38469;&#36317;&#31163;&#65292;&#24182;&#21487;&#20197;&#23558;&#24815;&#24615;&#25968;&#25454;&#36716;&#31227;&#21040;&#38647;&#36798;&#24863;&#27979;&#32467;&#26524;&#20013;&#12290;&#22312;&#26356;&#24191;&#27867;&#30340;&#24847;&#20041;&#19978;&#65292;&#35774;&#22791;&#26159;&#31532;&#19968;&#20010;&#34701;&#21512;&#27627;&#31859;&#27874;&#38647;&#36798;&#21644;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#25968;&#25454;&#29992;&#20110;&#21516;&#26102;&#29992;&#25143;&#36319;&#36394;&#21644;&#37325;&#26032;&#35782;&#21035;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36319;&#36394;&#20154;&#38469;&#36317;&#31163;&#23545;&#20110;&#23454;&#26102;&#31038;&#20132;&#36317;&#31163;&#31649;&#29702;&#21644;&#20107;&#21518;&#25509;&#35302;&#36861;&#36394;&#20197;&#38450;&#27490;&#20256;&#26579;&#30149;&#30340;&#20256;&#25773;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;ImmTrack&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#27627;&#31859;&#27874;&#38647;&#36798;&#21644;&#29992;&#25143;&#25658;&#24102;&#30340;&#26234;&#33021;&#25163;&#26426;&#25110;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#24815;&#24615;&#27979;&#37327;&#25968;&#25454;&#26469;&#36319;&#36394;&#20154;&#38469;&#36317;&#31163;&#12290;&#36890;&#36807;&#23558;&#20174;&#38647;&#36798;&#21644;&#24815;&#24615;&#25968;&#25454;&#37325;&#24314;&#30340;&#31227;&#21160;&#36712;&#36857;&#36827;&#34892;&#21305;&#37197;&#65292;&#24815;&#24615;&#25968;&#25454;&#30340;&#20266;&#36523;&#20221;&#21487;&#20197;&#22312;&#20840;&#23616;&#22352;&#26631;&#31995;&#20013;&#36716;&#31227;&#21040;&#38647;&#36798;&#24863;&#27979;&#32467;&#26524;&#20013;&#12290;&#37325;&#26032;&#35782;&#21035;&#30340;&#38647;&#36798;&#24863;&#27979;&#31227;&#21160;&#36712;&#36857;&#28982;&#21518;&#29992;&#20110;&#36319;&#36394;&#20154;&#38469;&#36317;&#31163;&#12290;&#22312;&#26356;&#24191;&#27867;&#30340;&#24847;&#20041;&#19978;&#65292;ImmTrack&#26159;&#31532;&#19968;&#20010;&#34701;&#21512;&#27627;&#31859;&#27874;&#38647;&#36798;&#21644;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#25968;&#25454;&#29992;&#20110;&#21516;&#26102;&#29992;&#25143;&#36319;&#36394;&#21644;&#37325;&#26032;&#35782;&#21035;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tracking interpersonal distances is essential for real-time social distancing management and {\em ex-post} contact tracing to prevent spreads of contagious diseases. Bluetooth neighbor discovery has been employed for such purposes in combating COVID-19, but does not provide satisfactory spatiotemporal resolutions. This paper presents ImmTrack, a system that uses a millimeter wave radar and exploits the inertial measurement data from user-carried smartphones or wearables to track interpersonal distances. By matching the movement traces reconstructed from the radar and inertial data, the pseudo identities of the inertial data can be transferred to the radar sensing results in the global coordinate system. The re-identified, radar-sensed movement trajectories are then used to track interpersonal distances. In a broader sense, ImmTrack is the first system that fuses data from millimeter wave radar and inertial measurement units for simultaneous user tracking and re-identification. Evaluati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#36827;&#21270;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#39640;&#25928;&#19988;&#28789;&#27963;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24182;&#20248;&#21270;&#30456;&#20851;&#30340;&#36229;&#21442;&#25968;&#12290;&#27492;&#26694;&#26550;&#21487;&#29992;&#20110;&#20219;&#20309;&#33021;&#22815;&#22788;&#29702;&#28151;&#21512;&#25628;&#32034;&#31354;&#38388;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#24182;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#27604;&#24050;&#26377;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.12797</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#31639;&#27861;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An algorithmic framework for the optimization of deep neural networks architectures and hyperparameters. (arXiv:2303.12797v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#36827;&#21270;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#39640;&#25928;&#19988;&#28789;&#27963;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24182;&#20248;&#21270;&#30456;&#20851;&#30340;&#36229;&#21442;&#25968;&#12290;&#27492;&#26694;&#26550;&#21487;&#29992;&#20110;&#20219;&#20309;&#33021;&#22815;&#22788;&#29702;&#28151;&#21512;&#25628;&#32034;&#31354;&#38388;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#24182;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#27604;&#24050;&#26377;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#39640;&#25928;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24182;&#20248;&#21270;&#30456;&#20851;&#30340;&#36229;&#21442;&#25968;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#36827;&#21270;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;(DAG)&#65292;&#23450;&#20041;&#20102;&#27604;&#25991;&#29486;&#20013;&#29616;&#26377;&#30340;&#25628;&#32034;&#31354;&#38388;&#26356;&#20026;&#28789;&#27963;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#20801;&#35768;&#28151;&#21512;&#20351;&#29992;&#20256;&#32479;&#25805;&#20316;&#65292;&#22914;&#21367;&#31215;&#12289;&#24490;&#29615;&#21644;&#23494;&#38598;&#23618;&#65292;&#20197;&#21450;&#36739;&#20026;&#26032;&#39062;&#30340;&#25805;&#20316;&#65292;&#22914;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#22522;&#20110;&#35813;&#25628;&#32034;&#31354;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#37051;&#22495;&#25628;&#32034;&#31639;&#23376;&#21644;&#28436;&#21270;&#25628;&#32034;&#31639;&#23376;&#65292;&#20197;&#20248;&#21270;&#32593;&#32476;&#30340;&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#12290;&#36825;&#20123;&#25628;&#32034;&#31639;&#23376;&#21487;&#19982;&#20219;&#20309;&#33021;&#22815;&#22788;&#29702;&#28151;&#21512;&#25628;&#32034;&#31354;&#38388;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#19968;&#36215;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#36827;&#21270;&#31639;&#27861;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#26694;&#26550;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#25214;&#21040;&#22312;&#35768;&#22810;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose an algorithmic framework to automatically generate efficient deep neural networks and optimize their associated hyperparameters. The framework is based on evolving directed acyclic graphs (DAGs), defining a more flexible search space than the existing ones in the literature. It allows mixtures of different classical operations: convolutions, recurrences and dense layers, but also more newfangled operations such as self-attention. Based on this search space we propose neighbourhood and evolution search operators to optimize both the architecture and hyper-parameters of our networks. These search operators can be used with any metaheuristic capable of handling mixed search spaces. We tested our algorithmic framework with an evolutionary algorithm on a time series prediction benchmark. The results demonstrate that our framework was able to find models outperforming the established baseline on numerous datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#25688;&#35201;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;......</title><link>http://arxiv.org/abs/2303.12796</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Analysis of Abstractive Text Summarization Using Pre-trained Models. (arXiv:2303.12796v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#25688;&#35201;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;......
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#29616;&#22312;&#20351;&#29992;&#20687;&#35895;&#27468;&#12289;&#38597;&#34382;&#21644;&#24517;&#24212;&#36825;&#26679;&#30340;&#25628;&#32034;&#24341;&#25806;&#26469;&#26597;&#25214;&#20114;&#32852;&#32593;&#19978;&#30340;&#20449;&#24687;&#12290;&#30001;&#20110;&#25968;&#25454;&#29190;&#28856;&#65292;&#22914;&#26524;&#20026;&#29992;&#25143;&#25552;&#20379;&#30456;&#20851;&#30340;&#25628;&#32034;&#32467;&#26524;&#25688;&#35201;&#32780;&#19981;&#20165;&#20165;&#26159;&#32593;&#39029;&#38142;&#25509;&#23558;&#20250;&#24456;&#26377;&#24110;&#21161;&#12290;&#25991;&#26412;&#25688;&#35201;&#24050;&#25104;&#20026;&#24110;&#21161;&#29992;&#25143;&#36805;&#36895;&#25484;&#25569;&#22823;&#37327;&#20449;&#24687;&#30340;&#20851;&#38190;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#23545;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20102;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20998;&#21035;&#26159;google/pegasus-cnn-dailymail&#12289;T5-base&#12289;facebook/bart-large-cnn&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#26159;CNN-dailymail&#12289;SAMSum&#21644;BillSum&#65292;&#20197;&#20174;&#19978;&#36848;&#19977;&#20010;&#27169;&#22411;&#20013;&#33719;&#21462;&#36755;&#20986;&#12290;&#36890;&#36807;ROUGH&#21644;BLEU&#25351;&#26631;&#65292;&#22312;&#36825;&#20123;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#27604;&#36739;&#20102;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#26377;2000&#20010;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
People nowadays use search engines like Google, Yahoo, and Bing to find information on the Internet. Due to explosion in data, it is helpful for users if they are provided relevant summaries of the search results rather than just links to webpages. Text summarization has become a vital approach to help consumers swiftly grasp vast amounts of information.In this paper, different pre-trained models for text summarization are evaluated on different datasets. Specifically, we have used three different pre-trained models, namely, google/pegasus-cnn-dailymail, T5-base, facebook/bart-large-cnn. We have considered three different datasets, namely, CNN-dailymail, SAMSum and BillSum to get the output from the above three models. The pre-trained models are compared over these different datasets, each of 2000 examples, through ROUGH and BLEU metrics.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25216;&#26415;&#33258;&#21160;&#29983;&#25104;&#30740;&#31350;&#20142;&#28857;&#65292;&#25506;&#31350;&#20854;&#26159;&#21542;&#33021;&#25552;&#39640;&#29983;&#25104;&#20142;&#28857;&#30340;&#36136;&#37327;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#21152;&#21629;&#21517;&#23454;&#20307;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#29983;&#25104;&#20142;&#28857;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.12795</link><description>&lt;p&gt;
&#22522;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#30740;&#31350;&#20142;&#28857;&#33258;&#21160;&#29983;&#25104;&#25216;&#26415;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Named Entity Recognition Based Automatic Generation of Research Highlights. (arXiv:2303.12795v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12795
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25216;&#26415;&#33258;&#21160;&#29983;&#25104;&#30740;&#31350;&#20142;&#28857;&#65292;&#25506;&#31350;&#20854;&#26159;&#21542;&#33021;&#25552;&#39640;&#29983;&#25104;&#20142;&#28857;&#30340;&#36136;&#37327;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#21152;&#21629;&#21517;&#23454;&#20307;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#29983;&#25104;&#20142;&#28857;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#31185;&#23398;&#35770;&#25991;&#25688;&#35201;&#29992;&#20110;&#24635;&#32467;&#35770;&#25991;&#20869;&#23481;&#12290;&#36817;&#26399;&#65292;&#30740;&#31350;&#20142;&#28857;&#20316;&#20026;&#25688;&#35201;&#30340;&#34917;&#20805;&#65292;&#32858;&#28966;&#20110;&#35770;&#25991;&#30340;&#20027;&#35201;&#21457;&#29616;&#65292;&#20294;&#20351;&#29992;&#39057;&#29575;&#36824;&#19981;&#22914;&#25688;&#35201;&#26222;&#36941;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#35770;&#25991;&#19981;&#21516;&#37096;&#20998;&#30340;&#36755;&#20837;&#65292;&#33258;&#21160;&#29983;&#25104;&#30740;&#31350;&#20142;&#28857;&#12290;&#30740;&#31350;&#20351;&#29992;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25216;&#26415;&#65292;&#25506;&#31350;&#23427;&#33021;&#21542;&#25913;&#36827;&#29983;&#25104;&#30740;&#31350;&#20142;&#28857;&#30340;&#36136;&#37327;&#12290;&#30740;&#31350;&#20351;&#29992;&#20004;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65306;&#31532;&#19968;&#20010;&#26159;&#25351;&#38024;-&#29983;&#25104;&#22120;&#32593;&#32476;&#65292;&#31532;&#20108;&#20010;&#22312;&#31532;&#19968;&#20010;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#22686;&#21152;&#20102;&#35206;&#30422;&#26426;&#21046;&#12290; &#28982;&#21518;&#23558;&#19978;&#36848;&#27599;&#20010;&#27169;&#22411;&#19982;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#29305;&#24449;&#30456;&#32467;&#21512;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#20026;&#32570;&#23569;&#20142;&#28857;&#30340;&#35770;&#25991;&#29983;&#25104;&#20142;&#28857;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22686;&#21152;&#21629;&#21517;&#23454;&#20307;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30740;&#31350;&#20142;&#28857;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A scientific paper is traditionally prefaced by an abstract that summarizes the paper. Recently, research highlights that focus on the main findings of the paper have emerged as a complementary summary in addition to an abstract. However, highlights are not yet as common as abstracts, and are absent in many papers. In this paper, we aim to automatically generate research highlights using different sections of a research paper as input. We investigate whether the use of named entity recognition on the input improves the quality of the generated highlights. In particular, we have used two deep learning-based models: the first is a pointer-generator network, and the second augments the first model with coverage mechanism. We then augment each of the above models with named entity recognition features. The proposed method can be used to produce highlights for papers with missing highlights. Our experiments show that adding named entity information improves the performance of the deep learn
&lt;/p&gt;</description></item><item><title>&#26032;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#35745;&#31639;&#36125;&#21494;&#26031;&#39118;&#38505;&#19979;&#30028;&#65292;&#20801;&#35768;&#20351;&#29992;&#20960;&#20046;&#20219;&#20309;&#20449;&#24687;&#24230;&#37327;&#65292;&#33021;&#25552;&#20379;&#19982;&#20272;&#35745;&#22120;&#26080;&#20851;&#30340;&#19981;&#21487;&#33021;&#32467;&#26524;&#12290;&#24050;&#24212;&#29992;&#20110;&#31163;&#25955;&#21644;&#36830;&#32493;&#21442;&#25968;&#38382;&#39064;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2303.12497</link><description>&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#24230;&#37327;&#30340;&#19979;&#30028;&#36125;&#21494;&#26031;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Lower Bound on the Bayesian Risk via Information Measure. (arXiv:2303.12497v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12497
&lt;/p&gt;
&lt;p&gt;
&#26032;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#35745;&#31639;&#36125;&#21494;&#26031;&#39118;&#38505;&#19979;&#30028;&#65292;&#20801;&#35768;&#20351;&#29992;&#20960;&#20046;&#20219;&#20309;&#20449;&#24687;&#24230;&#37327;&#65292;&#33021;&#25552;&#20379;&#19982;&#20272;&#35745;&#22120;&#26080;&#20851;&#30340;&#19981;&#21487;&#33021;&#32467;&#26524;&#12290;&#24050;&#24212;&#29992;&#20110;&#31163;&#25955;&#21644;&#36830;&#32493;&#21442;&#25968;&#38382;&#39064;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#21442;&#25968;&#20272;&#35745;&#65292;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35745;&#31639;&#36125;&#21494;&#26031;&#39118;&#38505;&#19979;&#30028;&#12290;&#35813;&#26041;&#27861;&#20801;&#35768;&#20351;&#29992;&#20960;&#20046;&#20219;&#20309;&#20449;&#24687;&#24230;&#37327;&#65292;&#21253;&#25324;R&#233;nyi&#30340;&#945;&#65292;&#966;-&#20998;&#27495;&#21644;Sibson&#30340;&#945;-&#20114;&#20449;&#24687;&#12290;&#35813; &#26041;&#27861;&#23558;&#20998;&#27495;&#35270;&#20026;&#24230;&#37327;&#30340;&#20989;&#25968;&#65292;&#24182;&#21033;&#29992;&#24230;&#37327;&#31354;&#38388;&#21644;&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#30340;&#23545;&#20598;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#19981;&#31561;&#24335;&#23545;&#20854;&#23545;&#20598;&#36827;&#34892;&#19978;&#30028;&#38480;&#21046;&#65292;&#23601;&#21487;&#20197;&#29992;&#20219;&#20309;&#20449;&#24687;&#24230;&#37327;&#35745;&#31639;&#39118;&#38505;&#30340;&#19979;&#30028;&#12290;&#22240;&#27492;&#65292;&#30001;&#20110;&#20998;&#27495;&#28385;&#36275;&#25968;&#25454;&#22788;&#29702;&#19981;&#31561;&#24335;&#65292;&#25105;&#20204;&#33021;&#22815;&#25552;&#20379;&#19982;&#20272;&#35745;&#22120;&#26080;&#20851;&#30340;&#19981;&#21487;&#33021;&#32467;&#26524;&#12290;&#28982;&#21518;&#23558;&#36825;&#20123;&#32467;&#26524;&#24212;&#29992;&#20110;&#28041;&#21450;&#31163;&#25955;&#21644;&#36830;&#32493;&#21442;&#25968;&#30340;&#26377;&#36259;&#38382;&#39064;&#65292;&#21253;&#25324;&#8220;&#25417;&#36855;&#34255;&#8221;&#38382;&#39064;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#36827;&#34892;&#27604;&#36739;&#12290;&#37325;&#35201;&#30340;&#35266;&#23519;&#26159;&#19979;&#30028;&#22312;&#26679;&#26412;&#25968;&#19978;&#30340;&#34892;&#20026;&#21463;&#21040;t&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on parameter estimation and introduces a new method for lower bounding the Bayesian risk. The method allows for the use of virtually \emph{any} information measure, including R\'enyi's $\alpha$, $\varphi$-Divergences, and Sibson's $\alpha$-Mutual Information. The approach considers divergences as functionals of measures and exploits the duality between spaces of measures and spaces of functions. In particular, we show that one can lower bound the risk with any information measure by upper bounding its dual via Markov's inequality. We are thus able to provide estimator-independent impossibility results thanks to the Data-Processing Inequalities that divergences satisfy. The results are then applied to settings of interest involving both discrete and continuous parameters, including the ``Hide-and-Seek'' problem, and compared to the state-of-the-art techniques. An important observation is that the behaviour of the lower bound in the number of samples is influenced by t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#24310;&#36831;&#24863;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;(DFL)&#65292;&#36890;&#36807;&#35299;&#20915;&#36793;&#32536;&#21644;&#20113;&#20043;&#38388;&#30340;&#36890;&#20449;&#24310;&#36831;&#65292;&#25552;&#39640;&#20102;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#30340;&#25928;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#19968;&#20123;&#25919;&#31574;&#20197;&#20943;&#23569;&#33021;&#37327;&#28040;&#32791;&#21644;&#36793;&#32536;&#21040;&#20113;&#31471;&#30340;&#36890;&#20449;&#12290;</title><link>http://arxiv.org/abs/2303.12414</link><description>&lt;p&gt;
&#24310;&#36831;&#24863;&#30693;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Delay-Aware Hierarchical Federated Learning. (arXiv:2303.12414v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#24310;&#36831;&#24863;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;(DFL)&#65292;&#36890;&#36807;&#35299;&#20915;&#36793;&#32536;&#21644;&#20113;&#20043;&#38388;&#30340;&#36890;&#20449;&#24310;&#36831;&#65292;&#25552;&#39640;&#20102;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#30340;&#25928;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#19968;&#20123;&#25919;&#31574;&#20197;&#20943;&#23569;&#33021;&#37327;&#28040;&#32791;&#21644;&#36793;&#32536;&#21040;&#20113;&#31471;&#30340;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#19979;&#35757;&#32451;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24050;&#32463;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#24310;&#36831;&#24863;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;(DFL)&#65292;&#36890;&#36807;&#35299;&#20915;&#36793;&#32536;&#21644;&#20113;&#20043;&#38388;&#30340;&#36890;&#20449;&#24310;&#36831;&#65292;&#25552;&#39640;&#20102;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#30340;&#25928;&#29575;&#12290;DFL&#22312;&#27599;&#20010;&#20840;&#23616;&#32858;&#21512;&#38388;&#38548;&#26399;&#38388;&#23545;&#35774;&#22791;&#25968;&#25454;&#38598;&#25191;&#34892;&#22810;&#20010;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#36845;&#20195;&#65292;&#24182;&#36890;&#36807;&#36793;&#32536;&#26381;&#21153;&#22120;&#22312;&#26412;&#22320;&#23376;&#32593;&#32476;&#20013;&#38388;&#26029;&#22320;&#32858;&#21512;&#27169;&#22411;&#21442;&#25968;&#12290;&#20113;&#26381;&#21153;&#22120;&#36890;&#36807;&#23616;&#37096;-&#20840;&#23616;&#21512;&#24182;&#22120;&#23558;&#26412;&#22320;&#27169;&#22411;&#19982;&#20840;&#23616;&#37096;&#32626;&#27169;&#22411;&#21516;&#27493;&#12290;DFL&#30340;&#25910;&#25947;&#34892;&#20026;&#22312;&#24191;&#20041;&#25968;&#25454;&#24322;&#36136;&#24615;&#24230;&#37327;&#19979;&#36827;&#34892;&#20102;&#29702;&#35770;&#30740;&#31350;&#12290;&#24471;&#20986;&#20102;&#19968;&#32452;&#26465;&#20214;&#65292;&#20197;&#23454;&#29616;O(1/k)&#30340;&#27425;&#32447;&#24615;&#25910;&#25947;&#29575;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#25511;&#21046;&#31639;&#27861;&#26469;&#23454;&#29616;DFL&#65292;&#24182;&#23454;&#29616;&#20102;&#19968;&#20123;&#25919;&#31574;&#20197;&#20943;&#23569;&#33021;&#37327;&#28040;&#32791;&#21644;&#36793;&#32536;&#21040;&#20113;&#31471;&#30340;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning has gained popularity as a means of training models distributed across the wireless edge. The paper introduces delay-aware federated learning (DFL) to improve the efficiency of distributed machine learning (ML) model training by addressing communication delays between edge and cloud. DFL employs multiple stochastic gradient descent iterations on device datasets during each global aggregation interval and intermittently aggregates model parameters through edge servers in local subnetworks. The cloud server synchronizes the local models with the global deployed model computed via a local-global combiner at global synchronization. The convergence behavior of DFL is theoretically investigated under a generalized data heterogeneity metric. A set of conditions is obtained to achieve the sub-linear convergence rate of O(1/k). Based on these findings, an adaptive control algorithm is developed for DFL, implementing policies to mitigate energy consumption and edge-to-cloud co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#25968;&#25454;&#27969;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#20998;&#24067;&#21442;&#25968;&#20272;&#35745;&#21644;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#26368;&#23567;&#22343;&#26041;&#38382;&#39064;&#32479;&#19968;&#36215;&#26469;&#65292;&#24182;&#21457;&#23637;&#20102;&#19968;&#31181;&#26032;&#30340;L2-&#28176;&#36817;&#31283;&#23450;&#24615;&#29702;&#35770;&#12290;&#35813;&#31639;&#27861;&#22312;&#32593;&#32476;&#22270;&#20026;&#36830;&#36890;&#19988;&#27491;&#21521;&#31639;&#23376;&#24207;&#21015;&#28385;&#36275;&#26080;&#38480;&#32500;&#24230;&#26102;&#31354;&#21169;&#30913;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#23454;&#29616;&#22343;&#26041;&#21644;&#20960;&#20046;&#24517;&#28982;&#30340;&#24378;&#19968;&#33268;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2303.11789</link><description>&lt;p&gt;
&#22270;&#19978;&#38543;&#26426;&#36870;&#38382;&#39064;&#65306;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Random Inverse Problems Over Graphs: Decentralized Online Learning. (arXiv:2303.11789v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#25968;&#25454;&#27969;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#20998;&#24067;&#21442;&#25968;&#20272;&#35745;&#21644;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#26368;&#23567;&#22343;&#26041;&#38382;&#39064;&#32479;&#19968;&#36215;&#26469;&#65292;&#24182;&#21457;&#23637;&#20102;&#19968;&#31181;&#26032;&#30340;L2-&#28176;&#36817;&#31283;&#23450;&#24615;&#29702;&#35770;&#12290;&#35813;&#31639;&#27861;&#22312;&#32593;&#32476;&#22270;&#20026;&#36830;&#36890;&#19988;&#27491;&#21521;&#31639;&#23376;&#24207;&#21015;&#28385;&#36275;&#26080;&#38480;&#32500;&#24230;&#26102;&#31354;&#21169;&#30913;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#23454;&#29616;&#22343;&#26041;&#21644;&#20960;&#20046;&#24517;&#28982;&#30340;&#24378;&#19968;&#33268;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#38543;&#26426;&#36870;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#35813;&#38382;&#39064;&#20855;&#26377;&#23454;&#26102;&#30340;&#22270;&#19978;&#35266;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#25968;&#25454;&#27969;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#20998;&#24067;&#21442;&#25968;&#20272;&#35745;&#21644;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#26368;&#23567;&#22343;&#26041;&#38382;&#39064;&#32479;&#19968;&#36215;&#26469;&#12290;&#25105;&#20204;&#23558;&#31639;&#27861;&#25910;&#25947;&#24615;&#36716;&#21270;&#20026;&#24102;&#26377;L2&#26377;&#30028;&#38789;&#24046;&#20998;&#39033;&#30340;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#38543;&#26426;&#26102;&#21464;&#24046;&#20998;&#26041;&#31243;&#30340;&#28176;&#36817;&#31283;&#23450;&#24615;&#65292;&#24182;&#21457;&#23637;&#20102;L2-&#28176;&#36817;&#31283;&#23450;&#24615;&#29702;&#35770;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22914;&#26524;&#32593;&#32476;&#22270;&#26159;&#36830;&#36890;&#30340;&#65292;&#24182;&#19988;&#27491;&#21521;&#31639;&#23376;&#24207;&#21015;&#28385;&#36275;&#26080;&#38480;&#32500;&#24230;&#26102;&#31354;&#21169;&#30913;&#26465;&#20214;&#65292;&#21017;&#25152;&#26377;&#33410;&#28857;&#30340;&#20272;&#35745;&#22343;&#20026;&#22343;&#26041;&#21644;&#20960;&#20046;&#24517;&#28982;&#30340;&#24378;&#19968;&#33268;&#30340;&#12290;&#36890;&#36807;&#23558;RKHS&#20013;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#38382;&#39064;&#31561;&#25928;&#22320;&#36716;&#21270;&#20026;&#22270;&#19978;&#38543;&#26426;&#36870;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#20013;&#24515;&#33410;&#28857;&#30340;RKHS&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We establish a framework of random inverse problems with real-time observations over graphs, and present a decentralized online learning algorithm based on online data streams, which unifies the distributed parameter estimation in Hilbert space and the least mean square problem in reproducing kernel Hilbert space (RKHS-LMS). We transform the algorithm convergence into the asymptotic stability of randomly time-varying difference equations in Hilbert space with L2-bounded martingale difference terms and develop the L2 -asymptotic stability theory. It is shown that if the network graph is connected and the sequence of forward operators satisfies the infinitedimensional spatio-temporal persistence of excitation condition, then the estimates of all nodes are mean square and almost surely strongly consistent. By equivalently transferring the distributed learning problem in RKHS to the random inverse problem over graphs, we propose a decentralized online learning algorithm in RKHS based on no
&lt;/p&gt;</description></item><item><title>CroSel&#26159;&#19968;&#31181;&#22788;&#29702;&#20266;&#26631;&#31614;&#22122;&#22768;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;&#39044;&#27979;&#20449;&#24687;&#21644;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#39033;&#26469;&#20934;&#30830;&#35782;&#21035;&#37096;&#20998;&#26631;&#31614;&#25968;&#25454;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2303.10365</link><description>&lt;p&gt;
CroSel: &#29992;&#20110;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#30340;&#33258;&#20449;&#20266;&#26631;&#31614;&#30340;&#36328;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
CroSel: Cross Selection of Confident Pseudo Labels for Partial-Label Learning. (arXiv:2303.10365v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10365
&lt;/p&gt;
&lt;p&gt;
CroSel&#26159;&#19968;&#31181;&#22788;&#29702;&#20266;&#26631;&#31614;&#22122;&#22768;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;&#39044;&#27979;&#20449;&#24687;&#21644;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#39033;&#26469;&#20934;&#30830;&#35782;&#21035;&#37096;&#20998;&#26631;&#31614;&#25968;&#25454;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;(PLL)&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#23427;&#20801;&#35768;&#27599;&#20010;&#35757;&#32451;&#31034;&#20363;&#26377;&#19968;&#20010;&#20505;&#36873;&#26631;&#31614;&#38598;&#65292;&#32780;&#19981;&#26159;&#19968;&#20010;&#21333;&#19968;&#30340;ground-truth&#26631;&#31614;&#12290;&#24050;&#32463;&#24191;&#27867;&#25506;&#32034;&#20102;&#22522;&#20110;&#35782;&#21035;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;PLL&#20013;&#30340;&#26631;&#31614;&#27495;&#20041;&#38382;&#39064;&#65292;&#36825;&#20123;&#26041;&#27861;&#23558;&#30495;&#23454;&#26631;&#31614;&#35270;&#20026;&#35201;&#35782;&#21035;&#30340;&#28508;&#22312;&#21464;&#37327;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#21644;&#23436;&#25972;&#22320;&#35782;&#21035;&#30495;&#23454;&#26631;&#31614;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36825;&#20250;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#23548;&#33268;&#20266;&#26631;&#31614;&#20013;&#30340;&#22122;&#22768;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CroSel&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#27169;&#22411;&#30340;&#21382;&#21490;&#39044;&#27979;&#20449;&#24687;&#26469;&#35782;&#21035;&#22823;&#22810;&#25968;&#35757;&#32451;&#31034;&#20363;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20132;&#21449;&#36873;&#25321;&#31574;&#30053;&#65292;&#20351;&#24471;&#20004;&#20010;&#28145;&#24230;&#27169;&#22411;&#21487;&#20197;&#30456;&#20114;&#36873;&#25321;&#37096;&#20998;&#26631;&#35760;&#25968;&#25454;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#39033;co-mix&#65292;&#20197;&#36991;&#20813;&#22240;&#34394;&#20551;&#36873;&#25321;&#32780;&#24341;&#36215;&#30340;&#26679;&#26412;&#28010;&#36153;&#21644;&#24494;&#23567;&#22122;&#22768;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;CroSel&#33021;&#22815;&#25361;&#36873;&#20986;&#22823;&#22810;&#25968;&#31034;&#20363;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partial-label learning (PLL) is an important weakly supervised learning problem, which allows each training example to have a candidate label set instead of a single ground-truth label. Identification-based methods have been widely explored to tackle label ambiguity issues in PLL, which regard the true label as a latent variable to be identified. However, identifying the true labels accurately and completely remains challenging, causing noise in pseudo labels during model training. In this paper, we propose a new method called CroSel, which leverages historical prediction information from models to identify true labels for most training examples. First, we introduce a cross selection strategy, which enables two deep models to select true labels of partially labeled data for each other. Besides, we propose a novel consistent regularization term called co-mix to avoid sample waste and tiny noise caused by false selection. In this way, CroSel can pick out the true labels of most examples 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#20960;&#20046;&#32447;&#24615;&#20108;&#27425;&#22411;&#35843;&#33410;&#22120;&#31995;&#32479;&#20013;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#21487;&#20197;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2303.08431</link><description>&lt;p&gt;
&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#25910;&#25947;&#20110;&#20960;&#20046;&#32447;&#24615;&#20108;&#27425;&#22411;&#35843;&#33410;&#22120;&#30340;&#20840;&#23616;&#26368;&#20248;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Policy Gradient Converges to the Globally Optimal Policy for Nearly Linear-Quadratic Regulators. (arXiv:2303.08431v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#20960;&#20046;&#32447;&#24615;&#20108;&#27425;&#22411;&#35843;&#33410;&#22120;&#31995;&#32479;&#20013;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#21487;&#20197;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#32773;&#21482;&#33719;&#24471;&#20102;&#38750;&#23436;&#25972;&#20449;&#24687;&#30340;&#38750;&#32447;&#24615;&#25511;&#21046;&#31995;&#32479;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#25214;&#21040;&#20960;&#20046;&#32447;&#24615;&#20108;&#27425;&#22411;&#35843;&#33410;&#22120;&#31995;&#32479;&#20013;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#21160;&#24577;&#31995;&#32479;&#65292;&#32467;&#21512;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#30001;&#30456;&#21516;&#32467;&#26500;&#30340;&#31574;&#30053;&#36827;&#34892;&#31649;&#29702;&#12290;&#22312;&#20551;&#35774;&#38750;&#32447;&#24615;&#32452;&#25104;&#37096;&#20998;&#21253;&#21547;&#20855;&#26377;&#23567;&#22411;Lipschitz&#31995;&#25968;&#30340;&#20869;&#26680;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23545;&#25104;&#26412;&#20989;&#25968;&#30340;&#20248;&#21270;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#34429;&#28982;&#25104;&#26412;&#20989;&#25968;&#36890;&#24120;&#26159;&#38750;&#20984;&#30340;&#65292;&#20294;&#25105;&#20204;&#30830;&#31435;&#20102;&#20840;&#23616;&#26368;&#20248;&#35299;&#38468;&#36817;&#23616;&#37096;&#30340;&#24378;&#20984;&#24615;&#21644;&#20809;&#28369;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21021;&#22987;&#21270;&#26426;&#21046;&#65292;&#20197;&#21033;&#29992;&#36825;&#20123;&#23646;&#24615;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#21487;&#20197;&#20445;&#35777;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nonlinear control systems with partial information to the decision maker are prevalent in a variety of applications. As a step toward studying such nonlinear systems, this work explores reinforcement learning methods for finding the optimal policy in the nearly linear-quadratic regulator systems. In particular, we consider a dynamic system that combines linear and nonlinear components, and is governed by a policy with the same structure. Assuming that the nonlinear component comprises kernels with small Lipschitz coefficients, we characterize the optimization landscape of the cost function. Although the cost function is nonconvex in general, we establish the local strong convexity and smoothness in the vicinity of the global optimizer. Additionally, we propose an initialization mechanism to leverage these properties. Building on the developments, we design a policy gradient algorithm that is guaranteed to converge to the globally optimal policy with a linear rate.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#23398;&#20064;&#37319;&#38598;&#36712;&#36857;&#26469;&#25552;&#39640;&#21160;&#24577;MRI&#30340;&#22270;&#20687;&#37325;&#24314;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.07150</link><description>&lt;p&gt;
&#22810;&#36335;&#24452;&#23398;&#20064;&#30340;&#21160;&#24577;MRI&#21487;&#34892;&#24615;&#30340;&#22810;&#37319;&#38598;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Multi PILOT: Learned Feasible Multiple Acquisition Trajectories for Dynamic MRI. (arXiv:2303.07150v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#23398;&#20064;&#37319;&#38598;&#36712;&#36857;&#26469;&#25552;&#39640;&#21160;&#24577;MRI&#30340;&#22270;&#20687;&#37325;&#24314;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#30913;&#20849;&#25391;&#25104;&#20687;(Dynamic Magnetic Resonance Imaging, MRI)&#22240;&#20854;&#22312;&#35786;&#26029;&#39046;&#22495;&#20013;&#23545;&#20154;&#20307;&#20869;&#37096;&#22120;&#23448;&#21644;&#32452;&#32455;&#30340;&#21160;&#24577;&#25104;&#20687;&#20855;&#26377;&#24378;&#22823;&#30340;&#21487;&#38752;&#24615;&#32780;&#25104;&#20026;&#39046;&#20808;&#30340;&#35786;&#26029;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;MRI&#36827;&#34892;&#39640;&#26102;&#31354;&#20998;&#36776;&#29575;&#25104;&#20687;&#38656;&#35201;&#30456;&#23545;&#36739;&#38271;&#30340;&#37319;&#38598;&#26102;&#38388;(&#22240;&#27492;&#25104;&#26412;&#22686;&#21152;)&#65292;&#36825;&#23548;&#33268;&#20986;&#29616;&#30456;&#20851;&#30340;&#36816;&#21160;&#20266;&#24433;&#24182;&#38477;&#20302;&#20102;&#20998;&#36776;&#29575;&#12290;&#21387;&#32553;&#24863;&#30693;(Compressed Sensing, CS)&#25216;&#26415;&#25104;&#20026;&#20943;&#23569;MRI&#37319;&#38598;&#26102;&#38388;&#30340;&#24120;&#29992;&#24037;&#20855;&#65292;&#36890;&#36807;&#25353;&#26576;&#20123;&#37319;&#38598;&#36712;&#36857;&#22312;k-&#31354;&#38388;&#20013;&#36827;&#34892;&#22270;&#20687;&#23376;&#37319;&#26679;&#12290;&#19968;&#20123;&#30740;&#31350;&#29305;&#21035;&#20851;&#27880;&#20110;&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23398;&#20064;&#36825;&#20123;&#37319;&#38598;&#36712;&#36857;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#22270;&#20687;&#37325;&#24314;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#37319;&#38598;&#36712;&#36857;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23398;&#20064;&#37319;&#38598;&#36712;&#36857;&#21482;&#22312;&#38745;&#24577;MRI&#30340;&#24773;&#20917;&#19979;&#25506;&#32034;&#36807;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#37319;&#38598;&#36712;&#36857;&#22312;&#21160;&#24577;MRI&#24773;&#20917;&#19979;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic Magnetic Resonance Imaging (MRI) is known to be a powerful and reliable technique for the dynamic imaging of internal organs and tissues, making it a leading diagnostic tool. A major difficulty in using MRI in this setting is the relatively long acquisition time (and, hence, increased cost) required for imaging in high spatio-temporal resolution, leading to the appearance of related motion artifacts and decrease in resolution. Compressed Sensing (CS) techniques have become a common tool to reduce MRI acquisition time by subsampling images in the k-space according to some acquisition trajectory. Several studies have particularly focused on applying deep learning techniques to learn these acquisition trajectories in order to attain better image reconstruction, rather than using some predefined set of trajectories. To the best of our knowledge, learning acquisition trajectories has been only explored in the context of static MRI. In this study, we consider acquisition trajectory l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#20869;&#26680;&#36873;&#25321;&#30340;&#24102;&#26377;Bandit&#21453;&#39304;&#30340;&#36951;&#25022;&#30028;&#38480;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#25913;&#36827;&#30340;&#30028;&#38480;&#12290;&#20854;&#20013;&#19968;&#31181;&#36866;&#29992;&#20110;&#20809;&#28369;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21478;&#19968;&#31181;&#21017;&#36866;&#29992;&#20110;Lipschitz&#25439;&#22833;&#20989;&#25968;&#65292;&#39044;&#26399;&#30028;&#38480;&#20998;&#21035;&#20026;$O(U^{\frac{2}{3}}K^{-\frac{1}{3}}(\sum^K_{i=1}L_T(f^\ast_i))^{\frac{2}{3}})$&#21644;$O(U\sqrt{KT}\ln^{\frac{2}{3}}{T})$&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20855;&#26377;&#26102;&#38388;&#32422;&#26463;&#30340;&#22312;&#32447;&#20869;&#26680;&#36873;&#25321;&#20013;&#12290;</title><link>http://arxiv.org/abs/2303.05018</link><description>&lt;p&gt;
&#22312;Bandit&#21453;&#39304;&#19979;&#22312;&#32447;&#20869;&#26680;&#36873;&#25321;&#30340;&#25913;&#36827;&#36951;&#25022;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Improved Regret Bounds for Online Kernel Selection under Bandit Feedback. (arXiv:2303.05018v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#20869;&#26680;&#36873;&#25321;&#30340;&#24102;&#26377;Bandit&#21453;&#39304;&#30340;&#36951;&#25022;&#30028;&#38480;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#25913;&#36827;&#30340;&#30028;&#38480;&#12290;&#20854;&#20013;&#19968;&#31181;&#36866;&#29992;&#20110;&#20809;&#28369;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21478;&#19968;&#31181;&#21017;&#36866;&#29992;&#20110;Lipschitz&#25439;&#22833;&#20989;&#25968;&#65292;&#39044;&#26399;&#30028;&#38480;&#20998;&#21035;&#20026;$O(U^{\frac{2}{3}}K^{-\frac{1}{3}}(\sum^K_{i=1}L_T(f^\ast_i))^{\frac{2}{3}})$&#21644;$O(U\sqrt{KT}\ln^{\frac{2}{3}}{T})$&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20855;&#26377;&#26102;&#38388;&#32422;&#26463;&#30340;&#22312;&#32447;&#20869;&#26680;&#36873;&#25321;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#20869;&#26680;&#36873;&#25321;&#30340;&#24102;&#26377;Bandit&#21453;&#39304;&#30340;&#36951;&#25022;&#30028;&#38480;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#25913;&#36827;&#30340;&#30028;&#38480;&#12290;&#23545;&#20110;&#20809;&#28369;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20854;&#39044;&#26399;&#30028;&#38480;&#20026;$O(U^{\frac{2}{3}}K^{-\frac{1}{3}}(\sum^K_{i=1}L_T(f^\ast_i))^{\frac{2}{3}})$&#65292;&#20854;&#20013;$L_T(f^\ast_i)$&#26159;$\mathbb{H}_{i}=\{f\in\mathcal{H}_i:\Vert f\Vert_{\mathcal{H}_i}\leq U\}$&#20013;&#30340;&#26368;&#20248;&#20551;&#35774;&#30340;&#32047;&#31215;&#25439;&#22833;&#12290;&#23545;&#20110;Lipschitz&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20854;&#39044;&#26399;&#30028;&#38480;&#20026;$O(U\sqrt{KT}\ln^{\frac{2}{3}}{T})$&#12290;&#26412;&#25991;&#36824;&#23558;&#36825;&#20004;&#31181;&#31639;&#27861;&#24212;&#29992;&#20110;&#20855;&#26377;&#26102;&#38388;&#32422;&#26463;&#30340;&#22312;&#32447;&#20869;&#26680;&#36873;&#25321;&#20013;&#65292;&#24182;&#35777;&#26126;&#20102;&#26032;&#30340;&#36951;&#25022;&#30028;&#38480;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we improve the regret bound for online kernel selection under bandit feedback. Previous algorithm enjoys a $O((\Vert f\Vert^2_{\mathcal{H}_i}+1)K^{\frac{1}{3}}T^{\frac{2}{3}})$ expected bound for Lipschitz loss functions. We prove two types of regret bounds improving the previous bound. For smooth loss functions, we propose an algorithm with a $O(U^{\frac{2}{3}}K^{-\frac{1}{3}}(\sum^K_{i=1}L_T(f^\ast_i))^{\frac{2}{3}})$ expected bound where $L_T(f^\ast_i)$ is the cumulative losses of optimal hypothesis in $\mathbb{H}_{i}=\{f\in\mathcal{H}_i:\Vert f\Vert_{\mathcal{H}_i}\leq U\}$. The data-dependent bound keeps the previous worst-case bound and is smaller if most of candidate kernels match well with the data. For Lipschitz loss functions, we propose an algorithm with a $O(U\sqrt{KT}\ln^{\frac{2}{3}}{T})$ expected bound asymptotically improving the previous bound. We apply the two algorithms to online kernel selection with time constraint and prove new regret bounds matchin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#24402;&#23646;&#20998;&#25968;&#21644;&#22240;&#26524;&#21453;&#20107;&#23454;&#22312;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#22240;&#26524;&#20851;&#31995;&#39046;&#22495;&#20013;&#30340;&#36923;&#36753;&#25512;&#29702;&#21644;&#20998;&#25968;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2303.02829</link><description>&lt;p&gt;
&#20316;&#20026;&#35299;&#37322;&#30340;&#22240;&#26524;&#21453;&#20107;&#23454;&#21644;&#24402;&#23646;&#20998;&#25968;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Attribution-Scores and Causal Counterfactuals as Explanations in Artificial Intelligence. (arXiv:2303.02829v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#24402;&#23646;&#20998;&#25968;&#21644;&#22240;&#26524;&#21453;&#20107;&#23454;&#22312;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#22240;&#26524;&#20851;&#31995;&#39046;&#22495;&#20013;&#30340;&#36923;&#36753;&#25512;&#29702;&#21644;&#20998;&#25968;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31361;&#20986;&#20102;&#35299;&#37322;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#26222;&#36941;&#24433;&#21709;&#21644;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#26032;&#21457;&#23637;&#30340;&#37325;&#35201;&#24615;&#65292;&#21253;&#25324;&#36215;&#28304;&#21644;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#29992;&#31616;&#21333;&#30340;&#26415;&#35821;&#25551;&#36848;&#20102;&#22522;&#20110;&#24402;&#23646;&#20998;&#25968;&#30340;&#25968;&#25454;&#31649;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#35299;&#37322;&#65292;&#20197;&#21450;&#22312;&#22240;&#26524;&#20851;&#31995;&#39046;&#22495;&#20013;&#21457;&#29616;&#30340;&#21453;&#20107;&#23454;&#12290;&#25105;&#20204;&#38416;&#36848;&#20102;&#22312;&#22788;&#29702;&#21453;&#20107;&#23454;&#26102;&#36923;&#36753;&#25512;&#29702;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#23427;&#20204;&#29992;&#20110;&#35745;&#31639;&#20998;&#25968;&#30340;&#29992;&#36884;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this expository article we highlight the relevance of explanations for artificial intelligence, in general, and for the newer developments in {\em explainable AI}, referring to origins and connections of and among different approaches. We describe in simple terms, explanations in data management and machine learning that are based on attribution-scores, and counterfactuals as found in the area of causality. We elaborate on the importance of logical reasoning when dealing with counterfactuals, and their use for score computation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24207;&#36143;&#23398;&#20064;&#26469;&#25511;&#21046;&#20256;&#25773;&#30340;&#27979;&#35797;&#21644;&#38548;&#31163;&#31574;&#30053;&#65292;&#20197;&#26368;&#23567;&#21270;&#32047;&#31215;&#24863;&#26579;&#20154;&#25968;&#65307;&#21487;&#20197;&#36890;&#36807;&#36138;&#24515;&#36873;&#25321;&#33410;&#28857;&#36827;&#34892;&#27979;&#35797;&#24182;&#20855;&#26377;&#24615;&#33021;&#20445;&#35777;&#65292;&#24182;&#35774;&#35745;&#20102;&#22522;&#20110;&#22870;&#21169;&#30340;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#32593;&#32476;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#35745;&#31639;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.00141</link><description>&lt;p&gt;
&#36890;&#36807;&#24207;&#36143;&#23398;&#20064;&#26469;&#25511;&#21046;&#20256;&#25773;&#65306;&#21033;&#29992;&#36824;&#26159;&#25506;&#32034;&#65311;
&lt;/p&gt;
&lt;p&gt;
Containing a spread through sequential learning: to exploit or to explore?. (arXiv:2303.00141v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24207;&#36143;&#23398;&#20064;&#26469;&#25511;&#21046;&#20256;&#25773;&#30340;&#27979;&#35797;&#21644;&#38548;&#31163;&#31574;&#30053;&#65292;&#20197;&#26368;&#23567;&#21270;&#32047;&#31215;&#24863;&#26579;&#20154;&#25968;&#65307;&#21487;&#20197;&#36890;&#36807;&#36138;&#24515;&#36873;&#25321;&#33410;&#28857;&#36827;&#34892;&#27979;&#35797;&#24182;&#20855;&#26377;&#24615;&#33021;&#20445;&#35777;&#65292;&#24182;&#35774;&#35745;&#20102;&#22522;&#20110;&#22870;&#21169;&#30340;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#32593;&#32476;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#35745;&#31639;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#19981;&#33391;&#25509;&#35302;&#36807;&#31243;&#65288;&#22914;COVID-19&#65289;&#30340;&#20256;&#25773;&#21487;&#20197;&#36890;&#36807;&#26816;&#27979;&#21644;&#38548;&#31163;&#34987;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#35813;&#36807;&#31243;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#28436;&#21270;&#20197;&#21450;&#36890;&#36807;&#38548;&#31163;&#36827;&#34892;&#30340;&#25511;&#21046;&#20351;&#24471;&#26816;&#27979;&#19982;&#19968;&#33324;&#30340;&#20027;&#21160;&#25628;&#32034;&#31574;&#30053;&#26377;&#30528;&#26681;&#26412;&#21306;&#21035;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#31181;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#27979;&#35797;&#21644;&#38548;&#31163;&#31574;&#30053;&#65292;&#20197;&#25511;&#21046;&#20256;&#25773;&#24182;&#22312;&#32473;&#23450;&#30340;&#27979;&#35797;&#39044;&#31639;&#19979;&#26368;&#23567;&#21270;&#32047;&#31215;&#24863;&#26579;&#20154;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;&#36138;&#24515;&#36873;&#25321;&#33410;&#28857;&#36827;&#34892;&#27979;&#35797;&#21487;&#20197;&#20248;&#21270;&#30446;&#26631;&#65292;&#24182;&#20855;&#26377;&#24615;&#33021;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#22870;&#21169;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#26368;&#23567;&#21270;&#32047;&#31215;&#24863;&#26579;&#20154;&#25968;&#30340;&#19978;&#30028;&#65292;&#24182;&#19988;&#22312;&#22823;&#22411;&#32593;&#32476;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#35745;&#31639;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31574;&#30053;&#38656;&#35201;&#20102;&#35299;&#33410;&#28857;&#30340;&#24863;&#26579;&#27010;&#29575;&#65292;&#36825;&#20123;&#27010;&#29575;&#20250;&#21160;&#24577;&#21464;&#21270;&#65292;&#24182;&#19988;&#38656;&#35201;&#36890;&#36807;&#24207;&#21015;&#27979;&#35797;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#28040;&#24687;&#20256;&#36882;&#26694;&#26550;&#26469;&#36827;&#34892;&#24863;&#26579;&#27010;&#29575;&#30340;&#24207;&#36143;&#23398;&#20064;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31574;&#30053;&#23545;&#28436;&#21270;&#36807;&#31243;&#30340;&#21160;&#24577;&#24615;&#33021;&#20855;&#26377;&#33391;&#22909;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The spread of an undesirable contact process, such as an infectious disease (e.g. COVID-19), is contained through testing and isolation of infected nodes. The temporal and spatial evolution of the process (along with containment through isolation) render such detection as fundamentally different from active search detection strategies. In this work, through an active learning approach, we design testing and isolation strategies to contain the spread and minimize the cumulative infections under a given test budget. We prove that the objective can be optimized, with performance guarantees, by greedily selecting the nodes to test. We further design reward-based methodologies that effectively minimize an upper bound on the cumulative infections and are computationally more tractable in large networks. These policies, however, need knowledge about the nodes' infection probabilities which are dynamically changing and have to be learned by sequential testing. We develop a message-passing fram
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#21464;&#37327;&#20851;&#27880;&#26426;&#21046;&#30340;&#21452;&#21521;LSTM&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#21147;&#65292;&#29992;&#20110;&#39044;&#27979;&#28151;&#21512;&#36890;&#39118;&#24314;&#31569;&#20869;&#31383;&#25143;&#24320;&#21551;&#25110;&#20851;&#38381;&#26102;&#30340;&#23460;&#20869;&#31354;&#27668;&#28201;&#24230;&#12290;</title><link>http://arxiv.org/abs/2302.04126</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#22810;&#21464;&#37327;&#20851;&#27880;&#26426;&#21046;&#30340;&#21452;&#21521;LSTM&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#39044;&#27979;&#28151;&#21512;&#36890;&#39118;&#24314;&#31569;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting the performance of hybrid ventilation in buildings using a multivariate attention-based biLSTM Encoder-Decoder neural network. (arXiv:2302.04126v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#21464;&#37327;&#20851;&#27880;&#26426;&#21046;&#30340;&#21452;&#21521;LSTM&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#21147;&#65292;&#29992;&#20110;&#39044;&#27979;&#28151;&#21512;&#36890;&#39118;&#24314;&#31569;&#20869;&#31383;&#25143;&#24320;&#21551;&#25110;&#20851;&#38381;&#26102;&#30340;&#23460;&#20869;&#31354;&#27668;&#28201;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#36890;&#39118;&#26159;&#19968;&#31181;&#33410;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#20026;&#22823;&#22810;&#25968;&#27668;&#20505;&#25552;&#20379;&#26032;&#40092;&#30340;&#31354;&#27668;&#65292;&#21069;&#25552;&#26159;&#23427;&#20855;&#26377;&#21487;&#38752;&#30340;&#25511;&#21046;&#31995;&#32479;&#12290;&#20026;&#20102;&#20351;&#36825;&#26679;&#30340;&#31995;&#32479;&#24471;&#21040;&#26368;&#20248;&#30340;&#25805;&#20316;&#65292;&#38656;&#35201;&#39640;&#20445;&#30495;&#30340;&#25511;&#21046;&#23450;&#21521;&#27169;&#22411;&#12290;&#23427;&#24212;&#24403;&#33021;&#22815;&#22522;&#20110;&#25805;&#20316;&#26465;&#20214;&#65288;&#22914;&#31383;&#25143;&#24320;&#21551;&#21644;HVAC&#36816;&#34892;&#35745;&#21010;&#65289;&#23545;&#23460;&#20869;&#31354;&#27668;&#28201;&#24230;&#36827;&#34892;&#36817;&#23454;&#26102;&#39044;&#27979;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#65292;&#21363;&#19968;&#31181;&#22522;&#20110;&#22810;&#21464;&#37327;&#22810;&#22836;&#20851;&#27880;&#26426;&#21046;&#30340;&#38271;&#30701;&#26102;&#35760;&#24518;(LSTM)&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#21147;&#65292;&#29992;&#20110;&#39044;&#27979;&#31383;&#25143;&#24320;&#21551;&#25110;&#20851;&#38381;&#26102;&#30340;&#23460;&#20869;&#31354;&#27668;&#28201;&#24230;&#12290;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#26469;&#33258;&#20110;&#35814;&#32454;&#30340;&#22810;&#21306;&#22495;&#21150;&#20844;&#27004;&#27169;&#22411;&#65288;EnergyPlus&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hybrid ventilation is an energy-efficient solution to provide fresh air for most climates, given that it has a reliable control system. To operate such systems optimally, a high-fidelity control-oriented modesl is required. It should enable near-real time forecast of the indoor air temperature based on operational conditions such as window opening and HVAC operating schedules. However, physics-based control-oriented models (i.e., white-box models) are labour-intensive and computationally expensive. Alternatively, black-box models based on artificial neural networks can be trained to be good estimators for building dynamics. This paper investigates the capabilities of a deep neural network (DNN), which is a multivariate multi-head attention-based long short-term memory (LSTM) encoder-decoder neural network, to predict indoor air temperature when windows are opened or closed. Training and test data are generated from a detailed multi-zone office building model (EnergyPlus). Pseudo-random
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#38024;&#23545;&#29616;&#23454;&#29615;&#22659;&#20013;&#30340;&#36830;&#32493;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#22312;&#21253;&#21547;3900&#19975;&#20010;&#26102;&#38388;&#25139;&#26631;&#35760;&#22270;&#20687;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;CLOC&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#31616;&#21333;&#30340;&#22522;&#32447;&#27169;&#22411;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;CL&#26041;&#27861;&#65292;&#35777;&#26126;&#29616;&#26377;&#26041;&#27861;&#22312;&#29616;&#23454;&#29615;&#22659;&#19979;&#30340;&#36866;&#29992;&#24615;&#23384;&#22312;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.01047</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#20013;&#36827;&#34892;&#23454;&#26102;&#35780;&#20272;&#65306;&#19968;&#20010;&#26032;&#24076;&#26395;
&lt;/p&gt;
&lt;p&gt;
Real-Time Evaluation in Online Continual Learning: A New Hope. (arXiv:2302.01047v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01047
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#38024;&#23545;&#29616;&#23454;&#29615;&#22659;&#20013;&#30340;&#36830;&#32493;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#22312;&#21253;&#21547;3900&#19975;&#20010;&#26102;&#38388;&#25139;&#26631;&#35760;&#22270;&#20687;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;CLOC&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#31616;&#21333;&#30340;&#22522;&#32447;&#27169;&#22411;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;CL&#26041;&#27861;&#65292;&#35777;&#26126;&#29616;&#26377;&#26041;&#27861;&#22312;&#29616;&#23454;&#29615;&#22659;&#19979;&#30340;&#36866;&#29992;&#24615;&#23384;&#22312;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#35780;&#20272;&#36890;&#24120;&#20551;&#35774;&#22312;&#35757;&#32451;&#26102;&#38388;&#21644;&#35745;&#31639;&#26041;&#38754;&#27809;&#26377;&#38480;&#21046;&#12290;&#36825;&#23545;&#20110;&#20219;&#20309;&#23454;&#38469;&#19990;&#30028;&#30340;&#29615;&#22659;&#37117;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#35780;&#20272;&#36830;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#27969;&#19981;&#31561;&#24453;&#27169;&#22411;&#23436;&#25104;&#35757;&#32451;&#21363;&#25581;&#31034;&#19979;&#19968;&#20010;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#20174;&#35745;&#31639;&#25104;&#26412;&#30340;&#35282;&#24230;&#35780;&#20272;&#24403;&#21069;&#30340;CL&#26041;&#27861;&#65292;&#24182;&#22312;&#21253;&#21547;3900&#19975;&#20010;&#26102;&#38388;&#25139;&#26631;&#35760;&#22270;&#20687;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;CLOC&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36825;&#31181;&#35780;&#20272;&#19979;&#65292;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#32447;&#27169;&#22411;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;CL&#26041;&#27861;&#65292;&#36825;&#23545;&#29616;&#26377;&#26041;&#27861;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#30340;&#36866;&#29992;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#25991;&#29486;&#20013;&#24120;&#29992;&#30340;&#21508;&#31181;CL&#32452;&#20214;&#65292;&#21253;&#25324;&#35760;&#24518;&#37319;&#26679;&#31574;&#30053;&#21644;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25152;&#26377;&#32771;&#34385;&#30340;&#26041;&#27861;&#37117;&#26080;&#27861;&#19982;&#25105;&#20204;&#30340;&#31616;&#21333;&#22522;&#32447;&#27169;&#22411;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current evaluations of Continual Learning (CL) methods typically assume that there is no constraint on training time and computation. This is an unrealistic assumption for any real-world setting, which motivates us to propose: a practical real-time evaluation of continual learning, in which the stream does not wait for the model to complete training before revealing the next data for predictions. To do this, we evaluate current CL methods with respect to their computational costs. We conduct extensive experiments on CLOC, a large-scale dataset containing 39 million time-stamped images with geolocation labels. We show that a simple baseline outperforms state-of-the-art CL methods under this evaluation, questioning the applicability of existing methods in realistic settings. In addition, we explore various CL components commonly used in the literature, including memory sampling strategies and regularization approaches. We find that all considered methods fail to be competitive against ou
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#26469;&#25552;&#21319;&#33945;&#29305;&#21345;&#32599;&#35780;&#20272;&#26041;&#27861;&#65292;&#23454;&#29616;&#22312;&#20445;&#25345;&#30456;&#21516;&#20272;&#35745;&#20934;&#30830;&#24230;&#30340;&#21069;&#25552;&#19979;&#65292;&#20943;&#23569;&#22312;&#32447;&#26679;&#26412;&#25968;&#37327;&#30340;&#30446;&#30340;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#23450;&#21046;&#30340;&#34892;&#20026;&#31574;&#30053;&#65292;&#21487;&#20197;&#27604;&#26222;&#36890;&#30340; MC &#20272;&#35745;&#22120;&#20135;&#29983;&#26356;&#23567;&#30340;&#26041;&#24046;&#12290;&#35813;&#34892;&#20026;&#31574;&#30053;&#21487;&#20197;&#20174;&#29616;&#26377;&#30340;&#31163;&#32447;&#25968;&#25454;&#20013;&#39640;&#25928;&#23398;&#20064;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#38656;&#20351;&#29992;&#23567;&#37096;&#20998;&#22312;&#32447;&#26679;&#26412;&#23601;&#33021;&#23454;&#29616;&#30456;&#21516;&#30340;&#20272;&#35745;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2301.13734</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#32447;&#25968;&#25454;&#25552;&#21319;&#33945;&#29305;&#21345;&#32599;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Monte Carlo Evaluation with Offline Data. (arXiv:2301.13734v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#26469;&#25552;&#21319;&#33945;&#29305;&#21345;&#32599;&#35780;&#20272;&#26041;&#27861;&#65292;&#23454;&#29616;&#22312;&#20445;&#25345;&#30456;&#21516;&#20272;&#35745;&#20934;&#30830;&#24230;&#30340;&#21069;&#25552;&#19979;&#65292;&#20943;&#23569;&#22312;&#32447;&#26679;&#26412;&#25968;&#37327;&#30340;&#30446;&#30340;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#23450;&#21046;&#30340;&#34892;&#20026;&#31574;&#30053;&#65292;&#21487;&#20197;&#27604;&#26222;&#36890;&#30340; MC &#20272;&#35745;&#22120;&#20135;&#29983;&#26356;&#23567;&#30340;&#26041;&#24046;&#12290;&#35813;&#34892;&#20026;&#31574;&#30053;&#21487;&#20197;&#20174;&#29616;&#26377;&#30340;&#31163;&#32447;&#25968;&#25454;&#20013;&#39640;&#25928;&#23398;&#20064;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#38656;&#20351;&#29992;&#23567;&#37096;&#20998;&#22312;&#32447;&#26679;&#26412;&#23601;&#33021;&#23454;&#29616;&#30456;&#21516;&#30340;&#20272;&#35745;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33945;&#29305;&#21345;&#32599; (MC) &#26041;&#27861;&#26159;&#20272;&#35745;&#31574;&#30053;&#34920;&#29616;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#12290;&#32473;&#23450;&#19968;&#20010;&#24863;&#20852;&#36259;&#30340;&#31574;&#30053;&#65292;MC &#26041;&#27861;&#36890;&#36807;&#37325;&#22797;&#36816;&#34892;&#35813;&#31574;&#30053;&#20197;&#25910;&#38598;&#26679;&#26412;&#24182;&#21462;&#20986;&#32467;&#26524;&#24179;&#22343;&#20540;&#26469;&#32473;&#20986;&#20272;&#35745;&#20540;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#25910;&#38598;&#30340;&#26679;&#26412;&#31216;&#20026;&#22312;&#32447;&#26679;&#26412;&#12290;&#20026;&#20102;&#33719;&#24471;&#20934;&#30830;&#30340;&#20272;&#35745;&#20540;&#65292;MC &#26041;&#27861;&#38656;&#35201;&#28040;&#32791;&#22823;&#37327;&#22312;&#32447;&#26679;&#26412;&#12290;&#24403;&#22312;&#32447;&#26679;&#26412;&#26114;&#36149;&#26102;&#65292;&#20363;&#22914;&#22312;&#32447;&#25512;&#33616;&#21644;&#24211;&#23384;&#31649;&#29702;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#23454;&#29616;&#30456;&#21516;&#30340;&#20272;&#35745;&#20934;&#30830;&#24230;&#30340;&#21516;&#26102;&#20943;&#23569;&#22312;&#32447;&#26679;&#26412;&#25968;&#37327;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#31163;&#32447; MC &#26041;&#27861;&#65292;&#36890;&#36807;&#36816;&#34892;&#19981;&#21516;&#30340;&#31574;&#30053;&#65288;&#31216;&#20026;&#34892;&#20026;&#31574;&#30053;&#65289;&#35780;&#20272;&#24863;&#20852;&#36259;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#34892;&#20026;&#31574;&#30053;&#65292;&#20351;&#31163;&#32447; MC &#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#26126;&#26174;&#23567;&#20110;&#26222;&#36890; MC &#20272;&#35745;&#22120;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#35813;&#23450;&#21046;&#34892;&#20026;&#31574;&#30053;&#21487;&#20197;&#20174;&#29616;&#26377;&#30340;&#31163;&#32447;&#25968;&#25454;&#65292;&#21363;&#20808;&#21069;&#35760;&#24405;&#30340;&#25968;&#25454;&#20013;&#39640;&#25928;&#23398;&#20064;&#65292;&#36825;&#27604;&#22312;&#32447;&#26679;&#26412;&#35201;&#20415;&#23452;&#24471;&#22810;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#38656;&#20351;&#29992;&#23567;&#37096;&#20998;&#22312;&#32447;&#26679;&#26412;&#23601;&#33021;&#23454;&#29616;&#30456;&#21516;&#30340;&#20272;&#35745;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monte Carlo (MC) methods are the most widely used methods to estimate the performance of a policy. Given an interested policy, MC methods give estimates by repeatedly running this policy to collect samples and taking the average of the outcomes. Samples collected during this process are called online samples. To get an accurate estimate, MC methods consume massive online samples. When online samples are expensive, e.g., online recommendations and inventory management, we want to reduce the number of online samples while achieving the same estimate accuracy. To this end, we use off-policy MC methods that evaluate the interested policy by running a different policy called behavior policy. We design a tailored behavior policy such that the variance of the off-policy MC estimator is provably smaller than the ordinary MC estimator. Importantly, this tailored behavior policy can be efficiently learned from existing offline data, i,e., previously logged data, which are much cheaper than onlin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#22823;&#33041;&#35270;&#20026;&#19968;&#31181;&#29992;&#20110;&#26377;&#30417;&#30563;&#23398;&#20064;&#30340;&#32479;&#35745;&#26041;&#27861;&#65292;&#23558;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36830;&#25509;&#21442;&#25968;&#30340;&#26412;&#22320;&#26356;&#26032;&#35268;&#21017;&#19982;&#38646;&#38454;&#20248;&#21270;&#26041;&#27861;&#30456;&#20851;&#32852;&#65292;&#24182;&#34920;&#26126;&#26399;&#26395;&#20540;&#23454;&#29616;&#20102;&#26799;&#24230;&#19979;&#38477;&#30340;&#20462;&#25913;&#29256;&#12290;</title><link>http://arxiv.org/abs/2301.11777</link><description>&lt;p&gt;
&#23558;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23398;&#20064;&#35299;&#37322;&#20026;&#38646;&#38454;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Interpreting learning in biological neural networks as zero-order optimization method. (arXiv:2301.11777v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#22823;&#33041;&#35270;&#20026;&#19968;&#31181;&#29992;&#20110;&#26377;&#30417;&#30563;&#23398;&#20064;&#30340;&#32479;&#35745;&#26041;&#27861;&#65292;&#23558;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36830;&#25509;&#21442;&#25968;&#30340;&#26412;&#22320;&#26356;&#26032;&#35268;&#21017;&#19982;&#38646;&#38454;&#20248;&#21270;&#26041;&#27861;&#30456;&#20851;&#32852;&#65292;&#24182;&#34920;&#26126;&#26399;&#26395;&#20540;&#23454;&#29616;&#20102;&#26799;&#24230;&#19979;&#38477;&#30340;&#20462;&#25913;&#29256;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38024;&#23545;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#30340;&#32479;&#35745;&#29702;&#35299;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;ANNs&#21463;&#22823;&#33041;&#21151;&#33021;&#30340;&#21551;&#21457;&#65292;&#20294;&#22312;&#20960;&#20010;&#20851;&#38190;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#12290;&#29305;&#21035;&#22320;&#65292;&#36830;&#25509;&#21442;&#25968;&#30340;&#26412;&#22320;&#26356;&#26032;&#35268;&#21017;&#22312;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#20013;&#20351;&#24471;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#30340;&#23398;&#20064;&#22312;&#29983;&#29289;&#23398;&#19978;&#19981;&#21487;&#20449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#22823;&#33041;&#35270;&#20026;&#19968;&#31181;&#29992;&#20110;&#26377;&#30417;&#30563;&#23398;&#20064;&#30340;&#32479;&#35745;&#26041;&#27861;&#12290;&#20027;&#35201;&#36129;&#29486;&#26159;&#23558;BNNs&#20013;&#30340;&#36830;&#25509;&#21442;&#25968;&#30340;&#26412;&#22320;&#26356;&#26032;&#35268;&#21017;&#19982;&#38646;&#38454;&#20248;&#21270;&#26041;&#27861;&#30456;&#20851;&#32852;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36845;&#20195;&#30340;&#26399;&#26395;&#20540;&#23454;&#29616;&#20102;&#26799;&#24230;&#19979;&#38477;&#30340;&#20462;&#25913;&#29256;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, significant progress has been made regarding the statistical understanding of artificial neural networks (ANNs). ANNs are motivated by the functioning of the brain, but differ in several crucial aspects. In particular, the locality in the updating rule of the connection parameters in biological neural networks (BNNs) makes it biologically implausible that the learning of the brain is based on gradient descent. In this work, we look at the brain as a statistical method for supervised learning. The main contribution is to relate the local updating rule of the connection parameters in BNNs to a zero-order optimization method. It is shown that the expected values of the iterates implement a modification of gradient descent.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#21464;&#21338;&#24328;&#20013;&#20048;&#35266;&#26799;&#24230;&#19979;&#38477;&#27861;&#65288;OGD&#65289;&#30340;&#25910;&#25947;&#24615;&#65292;&#25552;&#20986;&#20102;&#26126;&#30830;&#30340;&#25910;&#25947;&#30028;&#38480;&#65292;&#24182;&#24314;&#31435;&#20102;&#36866;&#29992;&#20110;&#26102;&#21464;&#24635;&#21644;&#22810;&#20154;&#21338;&#24328;&#21644;&#20803;&#23398;&#20064;&#30340;&#26032;&#22411;&#21452;&#32447;&#24615;&#20844;&#24335;&#12290;</title><link>http://arxiv.org/abs/2301.11241</link><description>&lt;p&gt;
&#20851;&#20110;&#26102;&#21464;&#21338;&#24328;&#20013;&#26080;&#24724;&#23398;&#20064;&#21160;&#24577;&#30340;&#25910;&#25947;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On the Convergence of No-Regret Learning Dynamics in Time-Varying Games. (arXiv:2301.11241v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#21464;&#21338;&#24328;&#20013;&#20048;&#35266;&#26799;&#24230;&#19979;&#38477;&#27861;&#65288;OGD&#65289;&#30340;&#25910;&#25947;&#24615;&#65292;&#25552;&#20986;&#20102;&#26126;&#30830;&#30340;&#25910;&#25947;&#30028;&#38480;&#65292;&#24182;&#24314;&#31435;&#20102;&#36866;&#29992;&#20110;&#26102;&#21464;&#24635;&#21644;&#22810;&#20154;&#21338;&#24328;&#21644;&#20803;&#23398;&#20064;&#30340;&#26032;&#22411;&#21452;&#32447;&#24615;&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20851;&#20110;&#21338;&#24328;&#23398;&#20064;&#30340;&#25991;&#29486;&#37117;&#38598;&#20013;&#20110;&#24213;&#23618;&#37325;&#22797;&#21338;&#24328;&#19981;&#21457;&#29983;&#21464;&#21270;&#30340;&#20005;&#26684;&#27169;&#24335;&#19979;&#12290;&#23545;&#20110;&#21160;&#24577;&#22810;&#26234;&#20307;&#28216;&#25103;&#20013;&#26080;&#24724;&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26102;&#21464;&#21338;&#24328;&#20013;&#20048;&#35266;&#26799;&#24230;&#19979;&#38477;&#27861;&#65288;OGD&#65289;&#30340;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#38024;&#23545;&#33258;&#28982;&#21464;&#21270;&#24230;&#37327;&#30340;&#21338;&#24328;&#24207;&#21015;&#30340;&#22343;&#34913;&#38388;&#38553;&#65292;&#20026;OGD&#25552;&#20379;&#20102;&#26126;&#30830;&#30340;&#25910;&#25947;&#30028;&#38480;&#65292;&#20174;&#32780;&#28085;&#30422;&#20102;&#38745;&#24577;&#21338;&#24328;&#30340;&#24050;&#30693;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#21482;&#35201;&#27599;&#22330;&#28216;&#25103;&#37117;&#36827;&#34892;&#20102;&#22810;&#27425;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#24378;&#20984;&#24615;-&#24378;&#20985;&#24615;&#24314;&#31435;&#20102;&#25913;&#36827;&#30340;&#20108;&#38454;&#21464;&#21270;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#36866;&#29992;&#20110;&#26102;&#21464;&#30340;&#24635;&#21644;&#22810;&#20154;&#21338;&#24328;&#65292;&#36890;&#36807;&#30456;&#20851;&#22343;&#34913;&#30340;&#21452;&#32447;&#24615;&#20844;&#24335;&#65292;&#36825;&#23545;&#20803;&#23398;&#20064;&#20197;&#21450;&#33719;&#24471;&#38024;&#23545;&#21464;&#21270;&#20381;&#36182;&#24615;&#21518;&#24724;&#30028;&#38480;&#30340;&#31934;&#32454;&#38656;&#27714;&#20855;&#26377;&#26032;&#39062;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most of the literature on learning in games has focused on the restrictive setting where the underlying repeated game does not change over time. Much less is known about the convergence of no-regret learning algorithms in dynamic multiagent settings. In this paper, we characterize the convergence of optimistic gradient descent (OGD) in time-varying games. Our framework yields sharp convergence bounds for the equilibrium gap of OGD in zero-sum games parameterized on natural variation measures of the sequence of games, subsuming known results for static games. Furthermore, we establish improved second-order variation bounds under strong convexity-concavity, as long as each game is repeated multiple times. Our results also apply to time-varying general-sum multi-player games via a bilinear formulation of correlated equilibria, which has novel implications for meta-learning and for obtaining refined variation-dependent regret bounds, addressing questions left open in prior papers. Finally,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24191;&#20041;&#31574;&#30053;&#25913;&#36827;&#20248;&#20808;&#32423;&#26469;&#23454;&#29616;&#39640;&#25928;&#22810;&#30446;&#26631;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#20174;&#32780;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#27599;&#19968;&#26102;&#21051;&#26368;&#26377;&#21069;&#36884;&#30340;&#20559;&#22909;&#25110;&#30446;&#26631;&#65292;&#20197;&#26356;&#24555;&#22320;&#35299;&#20915;MORL&#38382;&#39064;&#65292;&#21516;&#26102;&#20063;&#21487;&#20197;&#35782;&#21035;&#20986;&#23398;&#20064;&#29305;&#23450;&#20195;&#29702;&#20559;&#22909;&#30340;&#31574;&#30053;&#26102;&#26368;&#30456;&#20851;&#30340;&#21382;&#21490;&#32463;&#39564;&#12290;</title><link>http://arxiv.org/abs/2301.07784</link><description>&lt;p&gt;
&#36890;&#36807;&#24191;&#20041;&#31574;&#30053;&#20248;&#21270;&#20248;&#20808;&#32423;&#23454;&#29616;&#39640;&#25928;&#22810;&#30446;&#26631;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sample-Efficient Multi-Objective Learning via Generalized Policy Improvement Prioritization. (arXiv:2301.07784v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07784
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24191;&#20041;&#31574;&#30053;&#25913;&#36827;&#20248;&#20808;&#32423;&#26469;&#23454;&#29616;&#39640;&#25928;&#22810;&#30446;&#26631;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#20174;&#32780;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#27599;&#19968;&#26102;&#21051;&#26368;&#26377;&#21069;&#36884;&#30340;&#20559;&#22909;&#25110;&#30446;&#26631;&#65292;&#20197;&#26356;&#24555;&#22320;&#35299;&#20915;MORL&#38382;&#39064;&#65292;&#21516;&#26102;&#20063;&#21487;&#20197;&#35782;&#21035;&#20986;&#23398;&#20064;&#29305;&#23450;&#20195;&#29702;&#20559;&#22909;&#30340;&#31574;&#30053;&#26102;&#26368;&#30456;&#20851;&#30340;&#21382;&#21490;&#32463;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064; (MORL) &#31639;&#27861;&#35299;&#20915;&#20102;&#20195;&#29702;&#22312;&#21487;&#33021;&#20914;&#31361;&#30340;&#22870;&#21169;&#20989;&#25968;&#19978;&#26377;&#19981;&#21516;&#20559;&#22909;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#12290;&#36825;&#20123;&#31639;&#27861;&#36890;&#24120;&#23398;&#20064;&#19968;&#32452;&#31574;&#30053;&#65288;&#27599;&#20010;&#31574;&#30053;&#37117;&#26159;&#20026;&#19981;&#21516;&#20195;&#29702;&#20559;&#22909;&#32780;&#20248;&#21270;&#30340;&#65289;&#65292;&#36825;&#32452;&#31574;&#30053;&#21487;&#20197;&#21518;&#26469;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#26032;&#20559;&#22909;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#24191;&#20041;&#31574;&#30053;&#25913;&#36827; (GPI) &#23450;&#20041;&#20102;&#21407;&#21017;&#19978;&#24471;&#20986;&#30340;&#32452;&#25968;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22266;&#23450;&#26679;&#26412;&#25968;&#30340;&#23398;&#20064;&#25928;&#29575;&#12290;&#36890;&#36807;&#35813;&#31639;&#27861;&#65292;&#20195;&#29702;&#21487;&#20197;&#23454;&#29616;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#24182;&#21487;&#20197;&#22312;&#27599;&#19968;&#26102;&#21051;&#30830;&#23450;&#26368;&#26377;&#21069;&#36884;&#30340;&#20559;&#22909;&#25110;&#30446;&#26631;&#65292;&#20197;&#26356;&#24555;&#22320;&#35299;&#20915;&#32473;&#23450;&#30340;MORL&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#35813;&#31639;&#27861;&#20063;&#21487;&#20197;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;Dyna&#39118;&#26684;&#30340;MORL&#26041;&#27861;&#65292;&#35782;&#21035;&#20986;&#23398;&#20064;&#29305;&#23450;&#20195;&#29702;&#20559;&#22909;&#30340;&#31574;&#30053;&#26102;&#26368;&#30456;&#20851;&#30340;&#20197;&#24448;&#32463;&#39564;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20445;&#35777;&#22987;&#32456;&#22312;&#26377;&#38480;&#30340;&#27493;&#25968;&#20869;&#25910;&#25947;&#21040;&#26368;&#20248;&#35299;&#65292;&#25110;&#25910;&#25947;&#21040;&#36317;&#31163;&#26368;&#20248;&#35299; $\epsilon$-o&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-objective reinforcement learning (MORL) algorithms tackle sequential decision problems where agents may have different preferences over (possibly conflicting) reward functions. Such algorithms often learn a set of policies (each optimized for a particular agent preference) that can later be used to solve problems with novel preferences. We introduce a novel algorithm that uses Generalized Policy Improvement (GPI) to define principled, formally-derived prioritization schemes that improve sample-efficient learning. They implement active-learning strategies by which the agent can (i) identify the most promising preferences/objectives to train on at each moment, to more rapidly solve a given MORL problem; and (ii) identify which previous experiences are most relevant when learning a policy for a particular agent preference, via a novel Dyna-style MORL method. We prove our algorithm is guaranteed to always converge to an optimal solution in a finite number of steps, or an $\epsilon$-o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#20197;&#23558;&#21333;&#24352;&#22270;&#20687;&#20013;&#29289;&#20307;&#30340;&#36816;&#21160;&#36716;&#31227;&#21040;&#26410;&#35843;&#25972;&#30340;3D&#27169;&#22411;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#20219;&#24847;&#31867;&#21035;&#30340;&#23545;&#35937;&#65292;&#35757;&#32451;&#26102;&#21482;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2301.02232</link><description>&lt;p&gt;
&#21333;&#24352;&#22270;&#20687;&#26080;&#31867;&#21035;3D&#20851;&#33410;&#36716;&#31227;&#30340;CA$^2$T-Net&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
CA$^2$T-Net: Category-Agnostic 3D Articulation Transfer from Single Image. (arXiv:2301.02232v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#20197;&#23558;&#21333;&#24352;&#22270;&#20687;&#20013;&#29289;&#20307;&#30340;&#36816;&#21160;&#36716;&#31227;&#21040;&#26410;&#35843;&#25972;&#30340;3D&#27169;&#22411;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#20219;&#24847;&#31867;&#21035;&#30340;&#23545;&#35937;&#65292;&#35757;&#32451;&#26102;&#21482;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#21333;&#24103;&#22270;&#20687;&#20013;&#20851;&#33410;&#29289;&#20307;&#30340;&#36816;&#21160;&#36716;&#31227;&#21040;&#26410;&#32463;&#35843;&#25972;&#30340;3D&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#30340;&#32593;&#32476;&#23398;&#20064;&#39044;&#27979;&#29289;&#20307;&#30340;&#23039;&#24577;&#12289;&#37096;&#20998;&#20998;&#21106;&#21644;&#30456;&#24212;&#30340;&#36816;&#21160;&#21442;&#25968;&#65292;&#20197;&#37325;&#29616;&#36755;&#20837;&#22270;&#20687;&#20013;&#26174;&#31034;&#30340;&#20851;&#33410;&#36816;&#21160;&#12290;&#32593;&#32476;&#30001;&#19977;&#20010;&#19981;&#21516;&#30340;&#20998;&#25903;&#32452;&#25104;&#65292;&#23427;&#20204;&#37319;&#29992;&#20849;&#20139;&#30340;&#32852;&#21512;&#22270;&#20687;&#24418;&#29366;&#23884;&#20837;&#65292;&#24182;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#23545;&#35937;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#26469;&#33258;&#20219;&#24847;&#31867;&#21035;&#30340;&#23545;&#35937;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#33258;&#21160;&#22320;&#20026;&#32593;&#26684;&#28155;&#21152;&#21160;&#30011;&#65292;&#20174;&#30495;&#23454;&#22270;&#20687;&#20013;&#25512;&#26029;&#36816;&#21160;&#65292;&#24182;&#22312;&#27979;&#35797;&#26102;&#38388;&#23558;&#36816;&#21160;&#36716;&#31227;&#21040;&#21151;&#33021;&#19978;&#30456;&#20284;&#20294;&#20960;&#20309;&#19978;&#19981;&#21516;&#30340;3D&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a neural network approach to transfer the motion from a single image of an articulated object to a rest-state (i.e., unarticulated) 3D model. Our network learns to predict the object's pose, part segmentation, and corresponding motion parameters to reproduce the articulation shown in the input image. The network is composed of three distinct branches that take a shared joint image-shape embedding and is trained end-to-end. Unlike previous methods, our approach is independent of the topology of the object and can work with objects from arbitrary categories. Our method, trained with only synthetic data, can be used to automatically animate a mesh, infer motion from real images, and transfer articulation to functionally similar but geometrically distinct 3D models at test time.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#23454;&#29992;&#30340;&#26631;&#31614;&#19981;&#21487;&#30693;&#35774;&#32622;&#65292;&#20197;&#29983;&#25104;&#19981;&#21487;&#23398;&#20064;&#30340;&#26679;&#26412;&#65292;&#38450;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2301.01217</link><description>&lt;p&gt;
&#19981;&#21487;&#23398;&#20064;&#30340;&#32858;&#31867;&#65306;&#38754;&#21521;&#26631;&#31614;&#19981;&#21487;&#30693;&#30340;&#19981;&#21487;&#23398;&#20064;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Unlearnable Clusters: Towards Label-agnostic Unlearnable Examples. (arXiv:2301.01217v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#23454;&#29992;&#30340;&#26631;&#31614;&#19981;&#21487;&#30693;&#35774;&#32622;&#65292;&#20197;&#29983;&#25104;&#19981;&#21487;&#23398;&#20064;&#30340;&#26679;&#26412;&#65292;&#38450;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a more practical label-agnostic setting to generate unlearnable examples, which can prevent unauthorized training of machine learning models.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20114;&#32852;&#32593;&#19978;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#24320;&#21457;&#19981;&#21487;&#23398;&#20064;&#30340;&#31034;&#20363;&#65288;UEs&#65289;&#26469;&#38450;&#27490;&#35270;&#35273;&#38544;&#31169;&#27844;&#38706;&#24863;&#20852;&#36259;&#12290;UEs&#26159;&#28155;&#21152;&#20102;&#19981;&#21487;&#35265;&#20294;&#19981;&#21487;&#23398;&#20064;&#22122;&#22768;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#24050;&#32463;&#21457;&#29616;&#21487;&#20197;&#38450;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#12290;UEs&#36890;&#24120;&#26159;&#36890;&#36807;&#19968;&#20010;&#21452;&#23618;&#20248;&#21270;&#26694;&#26550;&#21644;&#19968;&#20010;&#26367;&#20195;&#27169;&#22411;&#29983;&#25104;&#30340;&#65292;&#20197;&#20174;&#21407;&#22987;&#26679;&#26412;&#20013;&#21435;&#38500;&#65288;&#26368;&#23567;&#21270;&#65289;&#38169;&#35823;&#65292;&#28982;&#21518;&#24212;&#29992;&#20110;&#20445;&#25252;&#25968;&#25454;&#20813;&#21463;&#26410;&#30693;&#30446;&#26631;&#27169;&#22411;&#30340;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;UE&#29983;&#25104;&#26041;&#27861;&#37117;&#20381;&#36182;&#20110;&#19968;&#20010;&#29702;&#24819;&#30340;&#20551;&#35774;&#65292;&#31216;&#20026;&#26631;&#31614;&#19968;&#33268;&#24615;&#65292;&#21363;&#20551;&#23450;&#40657;&#23458;&#21644;&#20445;&#25252;&#32773;&#23545;&#20110;&#32473;&#23450;&#30340;&#26679;&#26412;&#25345;&#26377;&#30456;&#21516;&#30340;&#26631;&#31614;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#25512;&#24191;&#20102;&#19968;&#20010;&#26356;&#23454;&#29992;&#30340;&#26631;&#31614;&#19981;&#21487;&#30693;&#35774;&#32622;&#65292;&#20854;&#20013;&#40657;&#23458;&#21487;&#33021;&#20250;&#20197;&#19982;&#20445;&#25252;&#32773;&#23436;&#20840;&#19981;&#21516;&#30340;&#26041;&#24335;&#21033;&#29992;&#21463;&#20445;&#25252;&#30340;&#25968;&#25454;&#12290;&#20363;&#22914;&#65292;&#30001;&#20445;&#25252;&#32773;&#25345;&#26377;&#30340;m&#31867;&#19981;&#21487;&#23398;&#20064;&#25968;&#25454;&#38598;&#21487;&#33021;&#34987;&#40657;&#23458;&#20316;&#20026;n&#31867;&#25968;&#25454;&#38598;&#21033;&#29992;&#12290;&#29616;&#26377;&#30340;UE&#29983;&#25104;&#26041;&#27861;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#22833;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a growing interest in developing unlearnable examples (UEs) against visual privacy leaks on the Internet. UEs are training samples added with invisible but unlearnable noise, which have been found can prevent unauthorized training of machine learning models. UEs typically are generated via a bilevel optimization framework with a surrogate model to remove (minimize) errors from the original samples, and then applied to protect the data against unknown target models. However, existing UE generation methods all rely on an ideal assumption called label-consistency, where the hackers and protectors are assumed to hold the same label for a given sample. In this work, we propose and promote a more practical label-agnostic setting, where the hackers may exploit the protected data quite differently from the protectors. E.g., a m-class unlearnable dataset held by the protector may be exploited by the hacker as a n-class dataset. Existing UE generation methods are rendered ineffective in
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#30340; Cut-and-Paste GAN&#65292;&#29992;&#20110;&#21069;&#26223;&#23545;&#35937;&#20998;&#21106;&#21644;&#32452;&#21512;&#22270;&#20687;&#29983;&#25104;&#65292;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#65292;&#36890;&#36807;&#23398;&#20064;&#20840;&#23616;&#25968;&#25454;&#34920;&#31034;&#21644;&#35821;&#20041;&#32467;&#26500;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#26377;&#24847;&#20041;&#30340;&#36974;&#32617;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2301.00366</link><description>&lt;p&gt;
&#33258;&#30417;&#30563; Cut-and-Paste GAN &#36827;&#34892;&#23545;&#35937;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Object Segmentation with a Cut-and-Pasting GAN. (arXiv:2301.00366v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00366
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#30340; Cut-and-Paste GAN&#65292;&#29992;&#20110;&#21069;&#26223;&#23545;&#35937;&#20998;&#21106;&#21644;&#32452;&#21512;&#22270;&#20687;&#29983;&#25104;&#65292;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#65292;&#36890;&#36807;&#23398;&#20064;&#20840;&#23616;&#25968;&#25454;&#34920;&#31034;&#21644;&#35821;&#20041;&#32467;&#26500;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#26377;&#24847;&#20041;&#30340;&#36974;&#32617;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#30340; Cut-and-Paste GAN&#65292;&#29992;&#20110;&#36827;&#34892;&#21069;&#26223;&#23545;&#35937;&#20998;&#21106;&#24182;&#29983;&#25104;&#36924;&#30495;&#30340;&#32452;&#21512;&#22270;&#20687;&#65292;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#12290;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#32467;&#21512;&#22522;&#20110; U-Net &#30340;&#37492;&#21035;&#22120;&#65292;&#23436;&#25104;&#20102;&#36825;&#19968;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#25193;&#23637;&#20102;&#26631;&#20934;&#37492;&#21035;&#22120;&#30340;&#33021;&#21147;&#65292;&#19981;&#20165;&#36890;&#36807;&#20998;&#31867;&#65288;&#30495;/&#20551;&#65289;&#23398;&#20064;&#20840;&#23616;&#25968;&#25454;&#34920;&#31034;&#65292;&#32780;&#19988;&#36824;&#36890;&#36807;&#20351;&#29992;&#33258;&#30417;&#30563;&#20219;&#21153;&#21019;&#24314;&#30340;&#20266;&#26631;&#31614;&#23398;&#20064;&#35821;&#20041;&#21644;&#32467;&#26500;&#20449;&#24687;&#12290;&#35813;&#26041;&#27861;&#20351;&#29983;&#25104;&#22120;&#33021;&#22815;&#36890;&#36807;&#24378;&#21046;&#23427;&#20174;&#36776;&#21035;&#22120;&#23398;&#20064;&#27599;&#20687;&#32032;&#30340;&#26377;&#24847;&#20041;&#30340;&#20449;&#24687;&#21644;&#20840;&#23616;&#22270;&#20687;&#21453;&#39304;&#26469;&#21019;&#24314;&#26377;&#24847;&#20041;&#30340;&#36974;&#32617;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel self-supervised based Cut-and-Paste GAN to perform foreground object segmentation and generate realistic composite images without manual annotations. We accomplish this goal by a simple yet effective self-supervised approach coupled with the U-Net based discriminator. The proposed method extends the ability of the standard discriminators to learn not only the global data representations via classification (real/fake) but also learn semantic and structural information through pseudo labels created using the self-supervised task. The proposed method empowers the generator to create meaningful masks by forcing it to learn informative per-pixel as well as global image feedback from the discriminator. Our experiments demonstrate that our proposed method significantly outperforms the state-of-the-art methods on the standard benchmark datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;HAC-Net&#65292;&#32467;&#21512;&#20102;&#19977;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#20004;&#20010;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#33021;&#22815;&#39640;&#31934;&#24230;&#22320;&#39044;&#27979;&#34507;&#30333;&#36136;&#19982;&#37197;&#20307;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#65292;&#24182;&#22312;&#20844;&#35748;&#30340;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.12440</link><description>&lt;p&gt;
HAC-Net:&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#28151;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#39640;&#31934;&#24230;&#30340;&#34507;&#30333;&#36136;&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
HAC-Net: A Hybrid Attention-Based Convolutional Neural Network for Highly Accurate Protein-Ligand Binding Affinity Prediction. (arXiv:2212.12440v3 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;HAC-Net&#65292;&#32467;&#21512;&#20102;&#19977;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#20004;&#20010;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#33021;&#22815;&#39640;&#31934;&#24230;&#22320;&#39044;&#27979;&#34507;&#30333;&#36136;&#19982;&#37197;&#20307;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#65292;&#24182;&#22312;&#20844;&#35748;&#30340;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#22270;&#20687;&#26816;&#27979;&#21644;&#22270;&#35770;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#27010;&#24565;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#34507;&#30333;-&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#39044;&#27979;&#65292;&#36825;&#26159;&#33647;&#29289;&#21457;&#29616;&#21644;&#34507;&#30333;&#24037;&#31243;&#39046;&#22495;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;HAC-Net&#65292;&#21253;&#25324;&#19968;&#20010;&#19977;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#20004;&#20010;&#22522;&#20110;&#27880;&#24847;&#21147;&#32858;&#21512;&#33410;&#28857;&#29305;&#24449;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#12290;&#22312;&#34507;&#30333;&#36136;&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#30340;&#39046;&#22495;&#20844;&#35748;&#30340;&#22522;&#20934;&#27979;&#35797;&#38598;PDBbind v.2016 core set&#19978;&#65292;HAC-Net&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#22810;&#20010;&#35757;&#32451;-&#27979;&#35797;&#21010;&#20998;&#20840;&#38754;&#35780;&#20272;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#65292;&#27599;&#20010;&#21010;&#20998;&#37117;&#26368;&#22823;&#21270;&#20102;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#22797;&#21512;&#29289;&#30340;&#34507;&#30333;&#32467;&#26500;&#12289;&#34507;&#30333;&#24207;&#21015;&#25110;&#37197;&#20307;&#25193;&#23637;&#36830;&#25509;&#25351;&#32441;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#30456;&#20284;&#24615;&#27979;&#24230;&#36827;&#34892;&#20102;&#21313;&#20493;&#20132;&#21449;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Applying deep learning concepts from image detection and graph theory has greatly advanced protein-ligand binding affinity prediction, a challenge with enormous ramifications for both drug discovery and protein engineering. We build upon these advances by designing a novel deep learning architecture consisting of a 3-dimensional convolutional neural network utilizing channel-wise attention and two graph convolutional networks utilizing attention-based aggregation of node features. HAC-Net (Hybrid Attention-Based Convolutional Neural Network) obtains state-of-the-art results on the PDBbind v.2016 core set, the most widely recognized benchmark in the field. We extensively assess the generalizability of our model using multiple train-test splits, each of which maximizes differences between either protein structures, protein sequences, or ligand extended-connectivity fingerprints of complexes in the training and test sets. Furthermore, we perform 10-fold cross-validation with a similarity 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#23398;&#20064;&#20122;&#32593;&#26684;&#23610;&#24230;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#27714;&#35299;&#22120;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;NODE&#20248;&#28857;&#65292;&#21487;&#20197;&#21442;&#25968;&#21270;&#20122;&#32593;&#26684;&#23610;&#24230;&#12289;&#36817;&#20284;&#32806;&#21512;&#31639;&#23376;&#65292;&#24182;&#25552;&#39640;&#20302;&#38454;&#27714;&#35299;&#22120;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2212.09967</link><description>&lt;p&gt;
&#21033;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#23398;&#20064;&#20122;&#32593;&#26684;&#23610;&#24230;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Subgrid-scale Models with Neural Ordinary Differential Equations. (arXiv:2212.09967v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#23398;&#20064;&#20122;&#32593;&#26684;&#23610;&#24230;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#27714;&#35299;&#22120;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;NODE&#20248;&#28857;&#65292;&#21487;&#20197;&#21442;&#25968;&#21270;&#20122;&#32593;&#26684;&#23610;&#24230;&#12289;&#36817;&#20284;&#32806;&#21512;&#31639;&#23376;&#65292;&#24182;&#25552;&#39640;&#20302;&#38454;&#27714;&#35299;&#22120;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#30340;&#20122;&#32593;&#26684;&#23610;&#24230;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;(NODE)&#26469;&#35299;&#20915;&#35745;&#31639;&#19978;&#30340;&#25361;&#25112;&#65292;&#21487;&#20197;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#20174;&#31895;&#32593;&#26684;&#21040;&#32454;&#32593;&#26684;&#30340;&#26144;&#23556;&#65292;&#20174;&#32780;&#23454;&#29616;&#20122;&#32593;&#26684;&#23610;&#24230;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#21033;&#29992;NODE&#21644;&#37096;&#20998;&#30693;&#35782;&#26469;&#22312;&#36830;&#32493;&#32423;&#21035;&#19978;&#23398;&#20064;&#28304;&#21160;&#21147;&#23398;&#12290;&#35813;&#26041;&#27861;&#32487;&#25215;&#20102;NODE&#30340;&#20248;&#28857;&#65292;&#21487;&#20197;&#29992;&#20110;&#21442;&#25968;&#21270;&#20122;&#32593;&#26684;&#23610;&#24230;&#12289;&#36817;&#20284;&#32806;&#21512;&#31639;&#23376;&#65292;&#24182;&#25552;&#39640;&#20302;&#38454;&#27714;&#35299;&#22120;&#30340;&#25928;&#29575;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new approach to learning the subgrid-scale model when simulating partial differential equations (PDEs) solved by the method of lines and their representation in chaotic ordinary differential equations, based on neural ordinary differential equations (NODEs). Solving systems with fine temporal and spatial grid scales is an ongoing computational challenge, and closure models are generally difficult to tune. Machine learning approaches have increased the accuracy and efficiency of computational fluid dynamics solvers. In this approach neural networks are used to learn the coarse- to fine-grid map, which can be viewed as subgrid-scale parameterization. We propose a strategy that uses the NODE and partial knowledge to learn the source dynamics at a continuous level. Our method inherits the advantages of NODEs and can be used to parameterize subgrid scales, approximate coupling operators, and improve the efficiency of low-order solvers. Numerical results with the two-scale Loren
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19968;&#32500;&#38750;&#36127;&#27979;&#24230;&#20043;&#38388;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;&#38382;&#39064;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#20999;&#29255;&#30340;&#26041;&#24335;&#23450;&#20041;&#20102;&#20999;&#29255;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;&#36317;&#31163;&#12290;</title><link>http://arxiv.org/abs/2212.08049</link><description>&lt;p&gt;
&#20999;&#29255;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;
&lt;/p&gt;
&lt;p&gt;
Sliced Optimal Partial Transport. (arXiv:2212.08049v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19968;&#32500;&#38750;&#36127;&#27979;&#24230;&#20043;&#38388;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;&#38382;&#39064;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#20999;&#29255;&#30340;&#26041;&#24335;&#23450;&#20041;&#20102;&#20999;&#29255;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#20256;&#36755;&#65288;OT&#65289;&#24050;&#32463;&#22312;&#26426;&#22120;&#23398;&#20064;&#12289;&#25968;&#25454;&#31185;&#23398;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#21464;&#24471;&#26497;&#20854;&#27969;&#34892;&#12290;OT&#38382;&#39064;&#30340;&#26680;&#24515;&#20551;&#35774;&#26159;&#28304;&#21644;&#30446;&#26631;&#27979;&#24230;&#30340;&#24635;&#36136;&#37327;&#30456;&#31561;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#30340;&#24212;&#29992;&#12290;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;&#65288;OPT&#65289;&#26159;&#26368;&#36817;&#25552;&#20986;&#30340;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#30340;&#26041;&#27861;&#12290;&#19982;OT&#38382;&#39064;&#31867;&#20284;&#65292;OPT&#30340;&#35745;&#31639;&#20381;&#36182;&#20110;&#35299;&#20915;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#65288;&#36890;&#24120;&#22312;&#39640;&#32500;&#24230;&#20013;&#65289;&#65292;&#36825;&#21487;&#33021;&#20250;&#21464;&#24471;&#35745;&#31639;&#19978;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#19968;&#32500;&#38750;&#36127;&#27979;&#24230;&#20043;&#38388;OPT&#38382;&#39064;&#30340;&#26377;&#25928;&#31639;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#36981;&#24490;&#20999;&#29255;OT&#36317;&#31163;&#30340;&#24605;&#24819;&#65292;&#25105;&#20204;&#21033;&#29992;&#20999;&#29255;&#23450;&#20041;&#20102;&#20999;&#29255;OPT&#36317;&#31163;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20999;&#29255;OPT-based&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#20540;&#23454;&#39564;&#20013;&#30340;&#35745;&#31639;&#21644;&#31934;&#24230;&#20248;&#21183;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;Sliced-OPT&#22312;&#22122;&#22768;&#28857;&#20113;&#37197;&#20934;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal transport (OT) has become exceedingly popular in machine learning, data science, and computer vision. The core assumption in the OT problem is the equal total amount of mass in source and target measures, which limits its application. Optimal Partial Transport (OPT) is a recently proposed solution to this limitation. Similar to the OT problem, the computation of OPT relies on solving a linear programming problem (often in high dimensions), which can become computationally prohibitive. In this paper, we propose an efficient algorithm for calculating the OPT problem between two non-negative measures in one dimension. Next, following the idea of sliced OT distances, we utilize slicing to define the sliced OPT distance. Finally, we demonstrate the computational and accuracy benefits of the sliced OPT-based method in various numerical experiments. In particular, we show an application of our proposed Sliced-OPT in noisy point cloud registration.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#31354;&#38388;&#36873;&#25321;&#30340;&#28145;&#24230;&#38750;&#32447;&#24615;&#28388;&#27874;&#22120;&#65292;&#37319;&#29992;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#26465;&#20214;&#26426;&#21046;&#65292;&#21487;&#20197;&#22312;&#20219;&#24847;&#30446;&#26631;&#26041;&#21521;&#23545;&#20854;&#36827;&#34892;&#23548;&#21521;&#12290;&#35813;&#28388;&#27874;&#22120;&#21487;&#20197;&#29992;&#20110;&#22810;&#20154;&#35821;&#38899;&#20998;&#31163;&#65292;&#23454;&#29616;&#38750;&#24120;&#20934;&#30830;&#30340;&#22810;&#35828;&#35805;&#32773;&#23450;&#20301;&#12290;</title><link>http://arxiv.org/abs/2211.02420</link><description>&lt;p&gt;
&#22522;&#20110;&#31354;&#38388;&#36873;&#25321;&#30340;&#28145;&#24230;&#38750;&#32447;&#24615;&#28388;&#27874;&#22120;&#30340;&#35821;&#38899;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Spatially Selective Deep Non-linear Filters for Speaker Extraction. (arXiv:2211.02420v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#31354;&#38388;&#36873;&#25321;&#30340;&#28145;&#24230;&#38750;&#32447;&#24615;&#28388;&#27874;&#22120;&#65292;&#37319;&#29992;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#26465;&#20214;&#26426;&#21046;&#65292;&#21487;&#20197;&#22312;&#20219;&#24847;&#30446;&#26631;&#26041;&#21521;&#23545;&#20854;&#36827;&#34892;&#23548;&#21521;&#12290;&#35813;&#28388;&#27874;&#22120;&#21487;&#20197;&#29992;&#20110;&#22810;&#20154;&#35821;&#38899;&#20998;&#31163;&#65292;&#23454;&#29616;&#38750;&#24120;&#20934;&#30830;&#30340;&#22810;&#35828;&#35805;&#32773;&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20154;&#21516;&#26102;&#35828;&#35805;&#30340;&#24773;&#20917;&#19979;&#65292;&#20449;&#21495;&#30340;&#31354;&#38388;&#29305;&#24449;&#26159;&#25552;&#21462;&#30446;&#26631;&#20449;&#21495;&#26368;&#26174;&#33879;&#30340;&#29305;&#24449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#28145;&#24230;&#32852;&#21512;&#31354;&#38388;&#35889;&#38750;&#32447;&#24615;&#28388;&#27874;&#22120;&#65292;&#21487;&#20197;&#22312;&#20219;&#24847;&#30446;&#26631;&#26041;&#21521;&#19978;&#36827;&#34892;&#23548;&#21521;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26465;&#20214;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#22522;&#20110;&#30446;&#26631;&#26041;&#21521;&#35774;&#32622;&#36807;&#28388;&#22120;&#36882;&#24402;&#23618;&#30340;&#21021;&#22987;&#29366;&#24577;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#26696;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#24182;&#22686;&#21152;&#20102;&#28388;&#27874;&#22120;&#30340;&#28789;&#27963;&#24615;&#65292;&#24182;&#19988;&#19981;&#20250;&#38477;&#20302;&#24615;&#33021;&#25104;&#26412;&#12290;&#25152;&#24471;&#21040;&#30340;&#31354;&#38388;&#36873;&#25321;&#24615;&#38750;&#32447;&#24615;&#28388;&#27874;&#22120;&#20063;&#21487;&#20197;&#29992;&#20110;&#20219;&#24847;&#25968;&#37327;&#35828;&#35805;&#32773;&#30340;&#35821;&#38899;&#20998;&#31163;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#29616;&#38750;&#24120;&#20934;&#30830;&#30340;&#22810;&#35828;&#35805;&#32773;&#23450;&#20301;&#65292;&#26412;&#25991;&#20013;&#24050;&#32463;&#26377;&#25152;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a scenario with multiple persons talking simultaneously, the spatial characteristics of the signals are the most distinct feature for extracting the target signal. In this work, we develop a deep joint spatial-spectral non-linear filter that can be steered in an arbitrary target direction. For this we propose a simple and effective conditioning mechanism, which sets the initial state of the filter's recurrent layers based on the target direction. We show that this scheme is more effective than the baseline approach and increases the flexibility of the filter at no performance cost. The resulting spatially selective non-linear filters can also be used for speech separation of an arbitrary number of speakers and enable very accurate multi-speaker localization as we demonstrate in this paper.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#20272;&#35745;&#22120;&#65292;&#21517;&#20026;GMM-MI&#65292;&#23427;&#21487;&#20197;&#29992;&#20110;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20869;&#37096;&#30340;&#24037;&#20316;&#26426;&#21046;&#65292;&#24182;&#22312;&#34920;&#31034;&#23398;&#20064;&#30340;&#24773;&#22659;&#19979;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2211.00024</link><description>&lt;p&gt;
&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#21487;&#35299;&#37322;&#24615;&#30340;&#40065;&#26834;&#20114;&#20449;&#24687;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
A robust estimator of mutual information for deep learning interpretability. (arXiv:2211.00024v2 [physics.data-an] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00024
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#20272;&#35745;&#22120;&#65292;&#21517;&#20026;GMM-MI&#65292;&#23427;&#21487;&#20197;&#29992;&#20110;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20869;&#37096;&#30340;&#24037;&#20316;&#26426;&#21046;&#65292;&#24182;&#22312;&#34920;&#31034;&#23398;&#20064;&#30340;&#24773;&#22659;&#19979;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21457;&#23637;&#20102;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#22312;&#20449;&#24687;&#35770;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#24230;&#37327;&#26041;&#24335;&#65292;&#29992;&#20110;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20869;&#37096;&#30340;&#24037;&#20316;&#26426;&#21046;&#12290;&#20026;&#20102;&#20934;&#30830;&#22320;&#20174;&#26377;&#38480;&#25968;&#37327;&#30340;&#26679;&#26412;&#20013;&#20272;&#35745;MI&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GMM-MI&#65288;&#35835;&#20316;&#8220;Jimmie&#8221;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#31163;&#25955;&#21644;&#36830;&#32493;&#35774;&#32622;&#12290;GMM-MI&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#26159;&#39640;&#25928;&#30340;&#65292;&#23545;&#20110;&#36229;&#21442;&#25968;&#30340;&#36873;&#25321;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#30001;&#20110;&#26377;&#38480;&#26679;&#26412;&#22823;&#23567;&#32780;&#24341;&#36215;&#30340;MI&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#24191;&#27867;&#22320;&#39564;&#35777;&#20102;GMM-MI&#22312;&#29609;&#20855;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#65292;&#23545;&#27604;&#20102;&#24050;&#30830;&#23450;&#30340;&#20114;&#20449;&#24687;&#20272;&#35745;&#22120;&#30340;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#22312;&#34920;&#31034;&#23398;&#20064;&#30340;&#24773;&#22659;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#21644;&#25551;&#36848;&#39640;&#24230;&#38750;&#32447;&#24615;&#36807;&#31243;&#30340;&#29289;&#29702;&#25968;&#25454;&#38598;&#65292;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#39640;&#32500;&#25968;&#25454;&#32534;&#30721;&#20026;&#26377;&#24847;&#20041;&#30340;&#21387;&#32553;&#65288;&#28508;&#22312;&#65289;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;GMM-MI&#36827;&#34892;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop the use of mutual information (MI), a well-established metric in information theory, to interpret the inner workings of deep learning models. To accurately estimate MI from a finite number of samples, we present GMM-MI (pronounced $``$Jimmie$"$), an algorithm based on Gaussian mixture models that can be applied to both discrete and continuous settings. GMM-MI is computationally efficient, robust to the choice of hyperparameters and provides the uncertainty on the MI estimate due to the finite sample size. We extensively validate GMM-MI on toy data for which the ground truth MI is known, comparing its performance against established mutual information estimators. We then demonstrate the use of our MI estimator in the context of representation learning, working with synthetic data and physical datasets describing highly non-linear processes. We train deep learning models to encode high-dimensional data within a meaningful compressed (latent) representation, and use GMM-MI to q
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21457;&#29616;&#20102;&#19968;&#31181;&#36890;&#29992;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#21442;&#25968;&#31354;&#38388;&#20013;&#23547;&#25214;&#36830;&#32493;&#23545;&#31216;&#24615;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#23545;&#31216;&#24615;&#21487;&#20197;&#38613;&#21051;&#20986;&#20302;&#25439;&#22351;&#23665;&#35895;&#12290;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#32452;&#26032;&#30340;&#38750;&#32447;&#24615;&#25968;&#25454;&#30456;&#20851;&#23545;&#31216;&#24615;&#65292;&#29992;&#20110;&#23558;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#21464;&#24418;&#65292;&#25552;&#39640;&#23545;&#26576;&#20123;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#24182;&#21457;&#29616;&#20102;&#26799;&#24230;&#27969;&#30340;&#24179;&#22374;&#26497;&#23567;&#20540;&#20559;&#24046;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#21892;&#26799;&#24230;&#27969;&#23547;&#25214;&#33391;&#22909;&#35299;&#30340;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.17216</link><description>&lt;p&gt;
&#23545;&#31216;&#24615;&#12289;&#24179;&#28369;&#26497;&#23567;&#20540;&#21644;&#26799;&#24230;&#27969;&#30340;&#23432;&#24658;&#37327;
&lt;/p&gt;
&lt;p&gt;
Symmetries, flat minima, and the conserved quantities of gradient flow. (arXiv:2210.17216v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17216
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21457;&#29616;&#20102;&#19968;&#31181;&#36890;&#29992;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#21442;&#25968;&#31354;&#38388;&#20013;&#23547;&#25214;&#36830;&#32493;&#23545;&#31216;&#24615;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#23545;&#31216;&#24615;&#21487;&#20197;&#38613;&#21051;&#20986;&#20302;&#25439;&#22351;&#23665;&#35895;&#12290;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#32452;&#26032;&#30340;&#38750;&#32447;&#24615;&#25968;&#25454;&#30456;&#20851;&#23545;&#31216;&#24615;&#65292;&#29992;&#20110;&#23558;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#21464;&#24418;&#65292;&#25552;&#39640;&#23545;&#26576;&#20123;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#24182;&#21457;&#29616;&#20102;&#26799;&#24230;&#27969;&#30340;&#24179;&#22374;&#26497;&#23567;&#20540;&#20559;&#24046;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#21892;&#26799;&#24230;&#27969;&#23547;&#25214;&#33391;&#22909;&#35299;&#30340;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#32593;&#32476;&#25439;&#22833;&#26223;&#35266;&#30340;&#32463;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#35768;&#22810;&#23616;&#37096;&#26497;&#23567;&#20540;&#36890;&#36807;&#20302;&#25439;&#32791;&#23665;&#35895;&#30456;&#36830;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#36825;&#20123;&#23665;&#35895;&#30340;&#29702;&#35770;&#36215;&#28304;&#30693;&#20043;&#29978;&#23569;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#21442;&#25968;&#31354;&#38388;&#20013;&#23547;&#25214;&#36830;&#32493;&#23545;&#31216;&#24615;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#35813;&#23545;&#31216;&#24615;&#38613;&#21051;&#20102;&#20302;&#25439;&#22351;&#23665;&#35895;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#20102;&#28608;&#27963;&#20989;&#25968;&#30340;&#31561;&#21464;&#24615;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#23618;&#26550;&#26500;&#12290;&#20026;&#20102;&#23558;&#36825;&#20010;&#26694;&#26550;&#25512;&#24191;&#21040;&#38750;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#32452;&#26032;&#30340;&#38750;&#32447;&#24615;&#25968;&#25454;&#30456;&#20851;&#23545;&#31216;&#24615;&#12290;&#36825;&#20123;&#23545;&#31216;&#24615;&#21487;&#20197;&#20351;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#21464;&#24418;&#65292;&#20174;&#32780;&#22312;&#26032;&#30340;&#26679;&#26412;&#19978;&#34920;&#29616;&#20986;&#30456;&#20284;&#30340;&#24615;&#33021;&#65292;&#36825;&#20801;&#35768;&#38598;&#25104;&#24314;&#31435;&#65292;&#25552;&#39640;&#23545;&#26576;&#20123;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#32447;&#24615;&#23545;&#31216;&#24615;&#30456;&#20851;&#30340;&#23432;&#24658;&#37327;&#21487;&#29992;&#20110;&#23450;&#20041;&#27839;&#30528;&#20302;&#25439;&#22351;&#23665;&#35895;&#30340;&#22352;&#26631;&#31995;&#12290;&#36825;&#20123;&#23432;&#24658;&#37327;&#26377;&#21161;&#20110;&#25581;&#31034;&#20351;&#29992;&#24120;&#35265;&#21021;&#22987;&#21270;&#26041;&#27861;&#26102;&#65292;&#26799;&#24230;&#27969;&#21482;&#25506;&#32034;&#20102;&#25439;&#22833;&#26223;&#35266;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#24179;&#22374;&#26497;&#23567;&#20540;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#20943;&#36731;&#36825;&#31181;&#20559;&#24046;&#65292;&#25913;&#21892;&#26799;&#24230;&#27969;&#23547;&#25214;&#33391;&#22909;&#35299;&#30340;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empirical studies of the loss landscape of deep networks have revealed that many local minima are connected through low-loss valleys. Yet, little is known about the theoretical origin of such valleys. We present a general framework for finding continuous symmetries in the parameter space, which carve out low-loss valleys. Our framework uses equivariances of the activation functions and can be applied to different layer architectures. To generalize this framework to nonlinear neural networks, we introduce a novel set of nonlinear, data-dependent symmetries. These symmetries can transform a trained model such that it performs similarly on new samples, which allows ensemble building that improves robustness under certain adversarial attacks. We then show that conserved quantities associated with linear symmetries can be used to define coordinates along low-loss valleys. The conserved quantities help reveal that using common initialization methods, gradient flow only explores a small part 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22810;&#31181;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#33021;&#22815;&#20197;&#22810;&#35821;&#35328;&#26041;&#24335;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#25506;&#35752;&#20102;&#22810;&#35821;&#35328;&#27169;&#22411;&#20248;&#20110;&#21333;&#35821;&#35328;&#27169;&#22411;&#12289;&#23569;&#37327;&#25552;&#31034;&#33021;&#25945;&#25480;&#27169;&#22411;&#26032;&#35821;&#35328;&#20197;&#21450;&#22312;&#21333;&#35821;&#35328;&#35774;&#32622;&#19979;&#25317;&#26377;&#38646;-shot&#32763;&#35793;&#33021;&#21147;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.14868</link><description>&lt;p&gt;
&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Multi-lingual Evaluation of Code Generation Models. (arXiv:2210.14868v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22810;&#31181;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#33021;&#22815;&#20197;&#22810;&#35821;&#35328;&#26041;&#24335;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#25506;&#35752;&#20102;&#22810;&#35821;&#35328;&#27169;&#22411;&#20248;&#20110;&#21333;&#35821;&#35328;&#27169;&#22411;&#12289;&#23569;&#37327;&#25552;&#31034;&#33021;&#25945;&#25480;&#27169;&#22411;&#26032;&#35821;&#35328;&#20197;&#21450;&#22312;&#21333;&#35821;&#35328;&#35774;&#32622;&#19979;&#25317;&#26377;&#38646;-shot&#32763;&#35793;&#33021;&#21147;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65306;MBXP&#21644;Multilingual HumanEval&#65292;&#20197;&#21450;MathQA-X&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;10&#31181;&#20197;&#19978;&#30340;&#32534;&#31243;&#35821;&#35328;&#65292;&#24182;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#36716;&#25442;&#26694;&#26550;&#23558;&#21407;&#22987;Python&#25968;&#25454;&#38598;&#20013;&#30340;&#25552;&#31034;&#21644;&#27979;&#35797;&#29992;&#20363;&#36716;&#35793;&#25104;&#30446;&#26631;&#35821;&#35328;&#20013;&#30340;&#30456;&#24212;&#25968;&#25454;&#12290;&#21033;&#29992;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#33021;&#22815;&#20197;&#22810;&#35821;&#35328;&#26041;&#24335;&#35780;&#20272;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#39046;&#22495;&#35821;&#35328;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12289;&#22810;&#35821;&#35328;&#27169;&#22411;&#22312;&#21333;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#20248;&#21183;&#12289;&#23569;&#37327;&#25552;&#31034;&#25945;&#25480;&#27169;&#22411;&#26032;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#22312;&#21333;&#35821;&#35328;&#35774;&#32622;&#19979;&#30340;&#38646;-shot&#32763;&#35793;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#22823;&#35268;&#27169;&#24341;&#23548;&#65292;&#20197;&#33719;&#21462;&#22810;&#31181;&#35821;&#35328;&#30340;&#21512;&#25104;&#35268;&#33539;&#35299;&#65292;&#36825;&#20123;&#35299;&#21487;&#29992;&#20110;&#20854;&#20182;&#19982;&#20195;&#30721;&#30456;&#20851;&#30340;&#35780;&#20272;&#65292;&#22914;&#20195;&#30721;&#25554;&#20837;&#12289;&#40065;&#26834;&#24615;&#25110;&#25688;&#35201;&#20219;&#21153;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;
&lt;/p&gt;
&lt;p&gt;
We present new benchmarks on evaluation code generation models: MBXP and Multilingual HumanEval, and MathQA-X. These datasets cover over 10 programming languages and are generated using a scalable conversion framework that transpiles prompts and test cases from the original Python datasets into the corresponding data in the target language. Using these benchmarks, we are able to assess the performance of code generation models in a multi-lingual fashion, and discovered generalization ability of language models on out-of-domain languages, advantages of multi-lingual models over mono-lingual, the ability of few-shot prompting to teach the model new languages, and zero-shot translation abilities even on mono-lingual settings. Furthermore, we use our code generation model to perform large-scale bootstrapping to obtain synthetic canonical solutions in several languages, which can be used for other code-related evaluations such as code insertion, robustness, or summarization tasks. Overall, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#36229;&#21442;&#25968;&#21270;&#32447;&#24615;RNN&#30340;&#26435;&#37325;&#20551;&#35774;&#65292;&#21457;&#29616;GD&#21487;&#20197;&#23398;&#20064;&#20302;&#32500;&#24230;&#29366;&#24577;&#31354;&#38388;&#65292;&#25429;&#33719;&#38271;&#26399;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2210.14064</link><description>&lt;p&gt;
&#29992;&#36229;&#21442;&#25968;&#21270;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20302;&#32500;&#29366;&#24577;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Learning Low Dimensional State Spaces with Overparameterized Recurrent Neural Nets. (arXiv:2210.14064v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#36229;&#21442;&#25968;&#21270;&#32447;&#24615;RNN&#30340;&#26435;&#37325;&#20551;&#35774;&#65292;&#21457;&#29616;GD&#21487;&#20197;&#23398;&#20064;&#20302;&#32500;&#24230;&#29366;&#24577;&#31354;&#38388;&#65292;&#25429;&#33719;&#38271;&#26399;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#36807;&#24230;&#21442;&#25968;&#21270;&#36890;&#24120;&#26159;&#25351;&#35757;&#32451;&#20986;&#26469;&#30340;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#22810;&#31181;&#34920;&#36798;&#33021;&#21147;&#65292;&#21487;&#20197;&#21516;&#26102;&#36866;&#21512;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#31181;&#26041;&#24335;&#65292;&#20854;&#20013;&#19968;&#20123;&#33021;&#22815;&#24456;&#22909;&#22320;&#25512;&#24191;&#65292;&#32780;&#21478;&#19968;&#20123;&#21017;&#19981;&#33021;&#12290;&#22312;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#20013;&#65292;&#23384;&#22312;&#19968;&#23618;&#39069;&#22806;&#30340;&#36807;&#24230;&#21442;&#25968;&#21270;&#65292;&#24847;&#21619;&#30528;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#35768;&#22810;&#35299;&#65292;&#36866;&#29992;&#20110;&#35757;&#32451;&#20013;&#30475;&#21040;&#30340;&#24207;&#21015;&#38271;&#24230;&#65292;&#20854;&#20013;&#19968;&#20123;&#21487;&#20197;&#25512;&#24191;&#21040;&#36739;&#38271;&#24207;&#21015;&#65292;&#32780;&#21478;&#19968;&#20123;&#21017;&#19981;&#33021;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#24403;&#24212;&#29992;&#20110;&#36229;&#21442;&#25968;&#21270;&#32447;&#24615;RNN&#26102;&#65292;GD&#30340;&#22806;&#25512;&#23646;&#24615;&#12290;&#19982;&#26368;&#36817;&#25552;&#20986;&#30340;&#26263;&#31034;GD&#20559;&#21521;&#20110;&#30701;&#26399;&#35760;&#24518;&#30340;&#35770;&#28857;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;GD&#21487;&#20197;&#23398;&#20064;&#20302;&#32500;&#29366;&#24577;&#31354;&#38388;&#65292;&#25429;&#33719;&#38271;&#26399;&#35760;&#24518;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#23545;&#36229;&#23436;&#22791;&#32447;&#24615;RNN&#30340;&#26435;&#37325;&#20570;&#20986;&#26576;&#20123;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#25439;&#22833;&#26223;&#35266;&#21253;&#21547;&#19968;&#20010;&#27169;&#24335;&#65292;&#22312;&#35813;&#27169;&#24335;&#19979;&#65292;&#21442;&#25968;&#25968;&#37327;&#36229;&#36807;&#20102;&#34920;&#31034;&#24213;&#23618;&#26102;&#38388;&#24207;&#21015;&#38271;&#26399;&#21160;&#24577;&#25152;&#38656;&#30340;&#32500;&#25968;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;GD&#20250;&#25910;&#25947;&#20110;&#19968;&#20010;&#35299;&#65292;&#23454;&#29616;&#25429;&#33719;&#38271;&#26399;&#21160;&#24577;&#25152;&#38656;&#30340;&#26368;&#23567;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Overparameterization in deep learning typically refers to settings where a trained neural network (NN) has representational capacity to fit the training data in many ways, some of which generalize well, while others do not. In the case of Recurrent Neural Networks (RNNs), there exists an additional layer of overparameterization, in the sense that a model may exhibit many solutions that generalize well for sequence lengths seen in training, some of which extrapolate to longer sequences, while others do not. Numerous works have studied the tendency of Gradient Descent (GD) to fit overparameterized NNs with solutions that generalize well. On the other hand, its tendency to fit overparameterized RNNs with solutions that extrapolate has been discovered only recently and is far less understood. In this paper, we analyze the extrapolation properties of GD when applied to overparameterized linear RNNs. In contrast to recent arguments suggesting an implicit bias towards short-term memory, we pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#25903;&#25745;&#20989;&#25968;&#23545;&#32039;&#33268;&#38598;&#21512;&#36827;&#34892;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#36827;&#34892;&#23376;&#32447;&#24615;&#22238;&#24402;&#65292;&#20998;&#21035;&#20026;&#20984;&#35268;&#21010;&#21644;&#38750;&#20984;&#35268;&#21010;&#12290;&#26412;&#25991;&#22312;&#21463;&#25511;&#21160;&#24577;&#21040;&#36798;&#38598;&#30340;&#24212;&#29992;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2210.01919</link><description>&lt;p&gt;
&#20855;&#26377;&#20984;&#21644;&#38750;&#20984;&#23376;&#32447;&#24615;&#22238;&#24402;&#30340;&#30740;&#31350;&#21450;&#20854;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#21040;&#36798;&#38598;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convex and Nonconvex Sublinear Regression with Application to Data-driven Learning of Reach Sets. (arXiv:2210.01919v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#25903;&#25745;&#20989;&#25968;&#23545;&#32039;&#33268;&#38598;&#21512;&#36827;&#34892;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#36827;&#34892;&#23376;&#32447;&#24615;&#22238;&#24402;&#65292;&#20998;&#21035;&#20026;&#20984;&#35268;&#21010;&#21644;&#38750;&#20984;&#35268;&#21010;&#12290;&#26412;&#25991;&#22312;&#21463;&#25511;&#21160;&#24577;&#21040;&#36798;&#38598;&#30340;&#24212;&#29992;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#36890;&#36807;&#36880;&#28176;&#36924;&#36817;&#19968;&#33268;&#20989;&#25968;&#65288;support function&#65289;&#30340;&#23376;&#32447;&#24615;&#22238;&#24402;&#26041;&#27861;&#65292;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#20272;&#35745;&#19968;&#20010;&#32039;&#33268;&#38598;&#21512;&#12290;&#25903;&#25745;&#20989;&#25968;&#22312;&#20984;&#23553;&#38381;&#36816;&#31639;&#30340;&#24847;&#20041;&#19979;&#33021;&#22815;&#21807;&#19968;&#22320;&#21051;&#30011;&#19968;&#20010;&#32039;&#33268;&#38598;&#21512;&#65292;&#32780;&#19988;&#26159;&#23376;&#32447;&#24615;&#65288;&#20984;&#20197;&#21450;&#19968;&#27425;&#27491;&#40784;&#27425;&#30340;&#65289;&#12290;&#30456;&#21453;&#65292;&#20219;&#20309;&#23376;&#32447;&#24615;&#20989;&#25968;&#37117;&#26159;&#19968;&#20010;&#32039;&#33268;&#38598;&#21512;&#30340;&#25903;&#25745;&#20989;&#25968;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#24615;&#36136;&#65292;&#23558;&#23398;&#20064;&#19968;&#20010;&#32039;&#33268;&#38598;&#21512;&#30340;&#20219;&#21153;&#36716;&#21270;&#20026;&#23398;&#20064;&#23427;&#30340;&#25903;&#25745;&#20989;&#25968;&#12290;&#20026;&#20102;&#36827;&#34892;&#23376;&#32447;&#24615;&#22238;&#24402;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#19968;&#31181;&#26159;&#36890;&#36807;&#20984;&#35268;&#21010;&#27714;&#35299;&#20108;&#27425;&#35268;&#21010;&#65288;QP&#65289;&#24471;&#21040;&#25903;&#25745;&#20989;&#25968;&#65292;&#21478;&#19968;&#31181;&#26159;&#36890;&#36807;&#35757;&#32451;&#23376;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#38750;&#20984;&#35268;&#21010;&#24471;&#21040;&#25903;&#25745;&#20989;&#25968;&#12290;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#23637;&#31034;&#20102;&#36825;&#20123;&#31639;&#27861;&#22312;&#36712;&#36857;&#25968;&#25454;&#20013;&#23398;&#20064;&#20855;&#26377;&#38598;&#21512;&#20540;&#36755;&#20837;&#19981;&#30830;&#23450;&#24615;&#30340;&#21463;&#25511;&#21160;&#24577;&#21040;&#36798;&#38598;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider estimating a compact set from finite data by approximating the support function of that set via sublinear regression. Support functions uniquely characterize a compact set up to closure of convexification, and are sublinear (convex as well as positive homogeneous of degree one). Conversely, any sublinear function is the support function of a compact set. We leverage this property to transcribe the task of learning a compact set to that of learning its support function. We propose two algorithms to perform the sublinear regression, one via convex and another via nonconvex programming. The convex programming approach involves solving a quadratic program (QP). The nonconvex programming approach involves training a input sublinear neural network. We illustrate the proposed methods via numerical examples on learning the reach sets of controlled dynamics subject to set-valued input uncertainties from trajectory data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#21457;&#29616;&#35757;&#32451;&#21644;&#27979;&#35797;&#25439;&#22833;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#26159;grokking&#30340;&#21407;&#22240;&#65292;&#25552;&#20986;&#20102;&#8220;LU&#26426;&#21046;&#8221;&#65292;&#24182;&#25104;&#21151;&#35825;&#23548;&#20102;&#31639;&#27861;&#25968;&#25454;&#38598;&#30340;grokking&#21644;&#28040;&#38500;&#20102;&#20854;grokking&#29616;&#35937;&#12290;&#23427;&#20204;&#30340;dramatic grokking&#20381;&#36182;&#20110;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2210.01117</link><description>&lt;p&gt;
Omnigrok&#65306;&#29702;&#35299;&#36229;&#36234;&#31639;&#27861;&#25968;&#25454;&#30340;&#8220;Grokking&#8221;
&lt;/p&gt;
&lt;p&gt;
Omnigrok: Grokking Beyond Algorithmic Data. (arXiv:2210.01117v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#21457;&#29616;&#35757;&#32451;&#21644;&#27979;&#35797;&#25439;&#22833;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#26159;grokking&#30340;&#21407;&#22240;&#65292;&#25552;&#20986;&#20102;&#8220;LU&#26426;&#21046;&#8221;&#65292;&#24182;&#25104;&#21151;&#35825;&#23548;&#20102;&#31639;&#27861;&#25968;&#25454;&#38598;&#30340;grokking&#21644;&#28040;&#38500;&#20102;&#20854;grokking&#29616;&#35937;&#12290;&#23427;&#20204;&#30340;dramatic grokking&#20381;&#36182;&#20110;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Grokking&#26159;&#19968;&#31181;&#19981;&#23547;&#24120;&#30340;&#29616;&#35937;&#65292;&#25351;&#31639;&#27861;&#25968;&#25454;&#38598;&#22312;&#36807;&#25311;&#21512;&#35757;&#32451;&#25968;&#25454;&#21518;&#38271;&#26102;&#38388;&#20173;&#28982;&#33021;&#36827;&#34892;&#27867;&#21270;&#65292;&#19968;&#30452;&#20197;&#26469;&#19968;&#30452;&#38590;&#20197;&#29702;&#35299;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20998;&#26512;&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#26223;&#35266;&#26469;&#29702;&#35299;grokking&#65292;&#24182;&#30830;&#23450;&#35757;&#32451;&#21644;&#27979;&#35797;&#25439;&#22833;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#26159;grokking&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#8220;LU&#26426;&#21046;&#8221;&#65292;&#22240;&#20026;&#35757;&#32451;&#21644;&#27979;&#35797;&#25439;&#22833;&#65288;&#23545;&#27169;&#22411;&#26435;&#37325;&#35268;&#33539;&#65289;&#36890;&#24120;&#20998;&#21035;&#31867;&#20284;&#20110;&#8220;L&#8221;&#21644;&#8220;U&#8221;&#12290;&#36825;&#20010;&#31616;&#21333;&#30340;&#26426;&#21046;&#21487;&#20197;&#24456;&#22909;&#22320;&#35299;&#37322;grokking&#30340;&#35768;&#22810;&#26041;&#38754;&#65306;&#25968;&#25454;&#22823;&#23567;&#20381;&#36182;&#24615;&#12289;&#26435;&#37325;&#34928;&#20943;&#20381;&#36182;&#24615;&#12289;&#34920;&#31034;&#30340;&#20986;&#29616;&#31561;&#12290;&#22312;&#30452;&#35273;&#19978;&#32473;&#23450;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#28041;&#21450;&#22270;&#20687;&#12289;&#35821;&#35328;&#21644;&#20998;&#23376;&#30340;&#20219;&#21153;&#20013;&#35825;&#23548;grokking&#12290;&#21453;&#21521;&#26469;&#30475;&#65292;&#25105;&#20204;&#33021;&#22815;&#28040;&#38500;&#31639;&#27861;&#25968;&#25454;&#38598;&#30340;grokking&#12290;&#25105;&#20204;&#23558;&#31639;&#27861;&#25968;&#25454;&#38598;&#30340;dramatic grokking&#24402;&#22240;&#20110;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Grokking, the unusual phenomenon for algorithmic datasets where generalization happens long after overfitting the training data, has remained elusive. We aim to understand grokking by analyzing the loss landscapes of neural networks, identifying the mismatch between training and test losses as the cause for grokking. We refer to this as the "LU mechanism" because training and test losses (against model weight norm) typically resemble "L" and "U", respectively. This simple mechanism can nicely explain many aspects of grokking: data size dependence, weight decay dependence, the emergence of representations, etc. Guided by the intuitive picture, we are able to induce grokking on tasks involving images, language and molecules. In the reverse direction, we are able to eliminate grokking for algorithmic datasets. We attribute the dramatic nature of grokking for algorithmic datasets to representation learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20110;&#23485;&#24102;&#30005;&#21147;&#32447;&#36890;&#20449;&#27979;&#37327;&#30340;&#39318;&#20010;&#25968;&#25454;&#38598;FiN-1&#21644;FiN-2&#65292;&#20849;&#25910;&#38598;&#20102;130&#20159;&#20010;&#25968;&#25454;&#28857;&#12290;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#24212;&#29992;&#20110;&#36164;&#20135;&#31649;&#29702;&#12289;&#30005;&#32593;&#29366;&#24577;&#21487;&#35270;&#21270;&#21644;&#39044;&#27979;&#65292;&#20197;&#35299;&#20915;&#30005;&#21147;&#32593;&#22312;&#21521;&#21487;&#20877;&#29983;&#33021;&#28304;&#36807;&#28193;&#21644;&#22797;&#26434;&#36127;&#36733;&#37197;&#32622;&#30340;&#24773;&#20917;&#19979;&#38754;&#20020;&#30340;&#26032;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2209.12693</link><description>&lt;p&gt;
&#21033;&#29992;&#26032;&#25968;&#25454;&#22312;&#30005;&#21147;&#32593;&#36890;&#35759;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Leveraging the Potential of Novel Data in Power Line Communication of Electricity Grids. (arXiv:2209.12693v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20110;&#23485;&#24102;&#30005;&#21147;&#32447;&#36890;&#20449;&#27979;&#37327;&#30340;&#39318;&#20010;&#25968;&#25454;&#38598;FiN-1&#21644;FiN-2&#65292;&#20849;&#25910;&#38598;&#20102;130&#20159;&#20010;&#25968;&#25454;&#28857;&#12290;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#24212;&#29992;&#20110;&#36164;&#20135;&#31649;&#29702;&#12289;&#30005;&#32593;&#29366;&#24577;&#21487;&#35270;&#21270;&#21644;&#39044;&#27979;&#65292;&#20197;&#35299;&#20915;&#30005;&#21147;&#32593;&#22312;&#21521;&#21487;&#20877;&#29983;&#33021;&#28304;&#36807;&#28193;&#21644;&#22797;&#26434;&#36127;&#36733;&#37197;&#32622;&#30340;&#24773;&#20917;&#19979;&#38754;&#20020;&#30340;&#26032;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21147;&#32593;&#24050;&#25104;&#20026;&#26085;&#24120;&#29983;&#27963;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#65292;&#23613;&#31649;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#24448;&#24448;&#19981;&#20250;&#27880;&#24847;&#21040;&#12290;&#21482;&#26377;&#24403;&#30005;&#21147;&#32593;&#19981;&#20877;&#21487;&#29992;&#26102;&#65292;&#25105;&#20204;&#36890;&#24120;&#25165;&#20250;&#29305;&#21035;&#24847;&#35782;&#21040;&#36825;&#31181;&#20381;&#36182;&#12290;&#28982;&#32780;&#65292;&#37325;&#22823;&#21464;&#21270;&#65292;&#22914;&#21521;&#21487;&#20877;&#29983;&#33021;&#28304;&#65288;&#20809;&#20239;&#12289;&#39118;&#21147;&#28065;&#36718;&#26426;&#31561;&#65289;&#30340;&#36807;&#28193;&#20197;&#21450;&#22797;&#26434;&#36127;&#36733;&#37197;&#32622;&#65288;&#30005;&#21160;&#27773;&#36710;&#12289;&#23478;&#24237;&#30005;&#27744;&#31995;&#32479;&#31561;&#65289;&#30340;&#33021;&#28304;&#28040;&#36153;&#32773;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#32473;&#30005;&#21147;&#32593;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20110;&#23485;&#24102;&#30005;&#21147;&#32447;&#36890;&#20449;&#65288;PLC&#65289;&#22522;&#30784;&#35774;&#26045;&#27979;&#37327;&#30340;&#39318;&#20010;&#25968;&#25454;&#38598;&#12290;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598; FiN-1 &#21644; FiN-2 &#22312;&#24503;&#22269;&#20302;&#21387;&#30005;&#32593;&#30340;&#19968;&#37096;&#20998;&#23454;&#38469;&#20351;&#29992;&#20013;&#25910;&#38598;&#65292;&#21521;&#22823;&#32422;440&#19975;&#20154;&#25552;&#20379;&#26381;&#21153;&#65292;&#24182;&#26174;&#31034;5100&#22810;&#20010;&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#36229;&#36807;130&#20159;&#20010;&#25968;&#25454;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#29992;&#20363;&#65292;&#29992;&#20110;&#36164;&#20135;&#31649;&#29702;&#12289;&#30005;&#32593;&#29366;&#24577;&#21487;&#35270;&#21270;&#21644;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electricity grids have become an essential part of daily life, even if they are often not noticed in everyday life. We usually only become particularly aware of this dependence by the time the electricity grid is no longer available. However, significant changes, such as the transition to renewable energy (photovoltaic, wind turbines, etc.) and an increasing number of energy consumers with complex load profiles (electric vehicles, home battery systems, etc.), pose new challenges for the electricity grid. To address these challenges, we propose two first-of-its-kind datasets based on measurements in a broadband powerline communications (PLC) infrastructure. Both datasets FiN-1 and FiN-2, were collected during real practical use in a part of the German low-voltage grid that supplies around 4.4 million people and show more than 13 billion datapoints collected by more than 5100 sensors. In addition, we present different use cases in asset management, grid state visualization, forecasting, 
&lt;/p&gt;</description></item><item><title>GP-net &#21487;&#20197;&#20174;&#28789;&#27963;&#30340;&#35270;&#35282;&#65292;&#20363;&#22914;&#31227;&#21160;&#26426;&#26800;&#33218;&#25152;&#20307;&#39564;&#30340;&#35270;&#35282;&#65292;&#29983;&#25104;&#20845;&#33258;&#30001;&#24230;&#30340;&#25235;&#21462;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#23454;&#39564;&#20013;&#65292;&#23427;&#23454;&#29616;&#20102; 51.8% &#30340;&#25235;&#21462;&#25104;&#21151;&#29575;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#26426;&#22120;&#20154;&#25235;&#21462;&#25216;&#26415;&#30340;&#26368;&#26032;&#26041;&#27861;&#25104;&#21151;&#29575;&#26356;&#20302;&#65292;&#38656;&#35201;&#23450;&#20041;&#24037;&#20316;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2209.10404</link><description>&lt;p&gt;
GP-net: &#28789;&#27963;&#30340;&#35270;&#35282;&#25235;&#21462;&#25552;&#26696;
&lt;/p&gt;
&lt;p&gt;
GP-net: Flexible Viewpoint Grasp Proposal. (arXiv:2209.10404v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10404
&lt;/p&gt;
&lt;p&gt;
GP-net &#21487;&#20197;&#20174;&#28789;&#27963;&#30340;&#35270;&#35282;&#65292;&#20363;&#22914;&#31227;&#21160;&#26426;&#26800;&#33218;&#25152;&#20307;&#39564;&#30340;&#35270;&#35282;&#65292;&#29983;&#25104;&#20845;&#33258;&#30001;&#24230;&#30340;&#25235;&#21462;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#23454;&#39564;&#20013;&#65292;&#23427;&#23454;&#29616;&#20102; 51.8% &#30340;&#25235;&#21462;&#25104;&#21151;&#29575;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#26426;&#22120;&#20154;&#25235;&#21462;&#25216;&#26415;&#30340;&#26368;&#26032;&#26041;&#27861;&#25104;&#21151;&#29575;&#26356;&#20302;&#65292;&#38656;&#35201;&#23450;&#20041;&#24037;&#20316;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Grasp Proposal Network (GP-net) &#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#28789;&#27963;&#30340;&#35270;&#35282;&#65292;&#20363;&#22914;&#31227;&#21160;&#26426;&#26800;&#33218;&#25152;&#20307;&#39564;&#30340;&#35270;&#35282;&#65292;&#29983;&#25104;&#20845;&#33258;&#30001;&#24230;&#30340;&#25235;&#21462;&#12290;&#25105;&#20204;&#36890;&#36807;&#21512;&#25104;&#29983;&#25104;&#28145;&#24230;&#22270;&#20687;&#21644;&#26631;&#27880;&#25235;&#21462;&#20449;&#24687;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451; GP-net&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992; EGAD! &#25235;&#21462;&#22522;&#20934;&#27979;&#35797;&#23545; GP-net &#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#23558;&#20854;&#19982;&#20004;&#31181;&#24120;&#29992;&#31639;&#27861;&#8212;&#8212;Volumetric Grasping Network (VGN) &#21644; Grasp Pose Detection package (GPD) &#36827;&#34892;&#27604;&#36739;&#65292;&#22312; PAL TIAGo &#31227;&#21160;&#26426;&#22120;&#20154;&#19978;&#12290;&#19982;&#26426;&#22120;&#20154;&#25235;&#21462;&#25216;&#26415;&#30340;&#26368;&#26032;&#26041;&#27861;&#30456;&#27604;&#65292;GP-net &#21487;&#20197;&#29992;&#20110;&#20174;&#28789;&#27963;&#30340;&#26410;&#30693;&#35270;&#35282;&#25235;&#21462;&#23545;&#35937;&#65292;&#32780;&#26080;&#38656;&#23450;&#20041;&#24037;&#20316;&#31354;&#38388;&#65292;&#24182;&#19988;&#25235;&#21462;&#25104;&#21151;&#29575;&#36798;&#21040; 51.8%&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;VGN &#20026; 51.1%&#65292;GPD &#20026; 33.6%&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010; ROS &#21253;&#65292;&#20197;&#21450;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#32593;&#22336;&#20026; https://aucoroboticsmu.github.io/GP-net/&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the Grasp Proposal Network (GP-net), a Convolutional Neural Network model which can generate 6-DOF grasps from flexible viewpoints, e.g. as experienced by mobile manipulators. To train GP-net, we synthetically generate a dataset containing depth-images and ground-truth grasp information. In real-world experiments we use the EGAD! grasping benchmark to evaluate GP-net against two commonly used algorithms, the Volumetric Grasping Network (VGN) and the Grasp Pose Detection package (GPD), on a PAL TIAGo mobile manipulator. In contrast to the state-of-the-art methods in robotic grasping, GP-net can be used for grasping objects from flexible, unknown viewpoints without the need to define the workspace and achieves a grasp success of 51.8% compared to 51.1% for VGN and 33.6% for GPD. We provide a ROS package along with our code and pre-trained models at https://aucoroboticsmu.github.io/GP-net/.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24178;&#39044;&#27491;&#21017;&#21270;&#27969;&#30340;&#20840;&#21442;&#25968;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#24178;&#39044;&#21518;&#30340;&#28508;&#22312;&#32467;&#26524;&#23494;&#24230;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;nuisance flow&#21644;target flow&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26131;&#20110;&#22788;&#29702;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#21644;&#21452;&#37325;&#31283;&#20581;&#30340;&#20272;&#35745;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.06203</link><description>&lt;p&gt;
&#38024;&#23545;&#24178;&#39044;&#23494;&#24230;&#20272;&#35745;&#30340;&#27491;&#21017;&#21270;&#27969;
&lt;/p&gt;
&lt;p&gt;
Normalizing Flows for Interventional Density Estimation. (arXiv:2209.06203v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24178;&#39044;&#27491;&#21017;&#21270;&#27969;&#30340;&#20840;&#21442;&#25968;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#24178;&#39044;&#21518;&#30340;&#28508;&#22312;&#32467;&#26524;&#23494;&#24230;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;nuisance flow&#21644;target flow&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26131;&#20110;&#22788;&#29702;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#21644;&#21452;&#37325;&#31283;&#20581;&#30340;&#20272;&#35745;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#38024;&#23545;&#22240;&#26524;&#25512;&#26029;&#36890;&#24120;&#36890;&#36807;&#28508;&#22312;&#32467;&#26524;&#30340;&#22343;&#20540;&#65288;&#20363;&#22914;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65289;&#26469;&#35745;&#31639;&#25968;&#37327;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#37327;&#24182;&#19981;&#33021;&#23436;&#20840;&#25429;&#25417;&#28508;&#22312;&#32467;&#26524;&#20998;&#24067;&#30340;&#20840;&#37096;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#24178;&#39044;&#21518;&#30340;&#28508;&#22312;&#32467;&#26524;&#23494;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20840;&#21442;&#25968;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#24178;&#39044;&#27491;&#21017;&#21270;&#27969;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32452;&#21512;&#20102;&#20004;&#31181;&#27491;&#21017;&#21270;&#27969;&#65292;&#21363;&#65288;i&#65289;&#29992;&#20110;&#20272;&#35745;&#24178;&#25200;&#21442;&#25968;&#30340;nuisance flow&#21644;&#65288;ii&#65289;&#29992;&#20110;&#21442;&#25968;&#21270;&#20272;&#35745;&#28508;&#22312;&#32467;&#26524;&#23494;&#24230;&#30340;target flow&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22522;&#20110;&#21333;&#27493;&#20559;&#24046;&#26657;&#27491;&#24320;&#21457;&#20102;&#19968;&#20010;&#26131;&#20110;&#22788;&#29702;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#20197;&#26377;&#25928;&#21644;&#21452;&#37325;&#31283;&#20581;&#30340;&#26041;&#24335;&#20272;&#35745;&#30446;&#26631;&#27969;&#21442;&#25968;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#24178;&#39044;&#27491;&#21017;&#21270;&#27969;&#25552;&#20379;&#20102;&#19968;&#20010;&#27491;&#30830;&#24402;&#19968;&#21270;&#30340;&#23494;&#24230;&#20272;&#35745;&#22120;&#12290;&#22312;&#21508;&#31181;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#24178;&#39044;&#27491;&#21017;&#21270;&#27969;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#29992;&#20110;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#28508;&#22312;&#32467;&#26524;&#23494;&#24230;&#30340;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing machine learning methods for causal inference usually estimate quantities expressed via the mean of potential outcomes (e.g., average treatment effect). However, such quantities do not capture the full information about the distribution of potential outcomes. In this work, we estimate the density of potential outcomes after interventions from observational data. For this, we propose a novel, fully-parametric deep learning method called Interventional Normalizing Flows. Specifically, we combine two normalizing flows, namely (i) a nuisance flow for estimating nuisance parameters and (ii) a target flow for a parametric estimation of the density of potential outcomes. We further develop a tractable optimization objective based on a one-step bias correction for an efficient and doubly robust estimation of the target flow parameters. As a result our Interventional Normalizing Flows offer a properly normalized density estimator. Across various experiments, we demonstrate that our Int
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#27169;&#22411;&#26159;&#35270;&#35273;&#20013;&#30340;&#26032;&#20852;&#20027;&#39064;&#65292;&#20854;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#21463;&#21040;&#24191;&#27867;&#27427;&#36175;&#12290;&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#19977;&#31181;&#36890;&#29992;&#30340;&#25193;&#25955;&#24314;&#27169;&#26041;&#27861;&#20197;&#21450;&#21508;&#31181;&#31639;&#27861;&#21644;&#26550;&#26500;&#26041;&#38754;&#30340;&#35752;&#35770;&#65292;&#24182;&#24635;&#32467;&#27604;&#36739;&#20102;&#25193;&#25955;&#27169;&#22411;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.04747</link><description>&lt;p&gt;
&#35270;&#35273;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models in Vision: A Survey. (arXiv:2209.04747v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.04747
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#35270;&#35273;&#20013;&#30340;&#26032;&#20852;&#20027;&#39064;&#65292;&#20854;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#21463;&#21040;&#24191;&#27867;&#27427;&#36175;&#12290;&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#19977;&#31181;&#36890;&#29992;&#30340;&#25193;&#25955;&#24314;&#27169;&#26041;&#27861;&#20197;&#21450;&#21508;&#31181;&#31639;&#27861;&#21644;&#26550;&#26500;&#26041;&#38754;&#30340;&#35752;&#35770;&#65292;&#24182;&#24635;&#32467;&#27604;&#36739;&#20102;&#25193;&#25955;&#27169;&#22411;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#30340;&#26032;&#20852;&#20027;&#39064;&#65292;&#23637;&#29616;&#20102;&#22312;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#20013;&#38750;&#20961;&#30340;&#32467;&#26524;&#12290;&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#30001;&#20004;&#20010;&#38454;&#27573;&#32452;&#25104;&#65292;&#21069;&#21521;&#25193;&#25955;&#21644;&#21453;&#21521;&#25193;&#25955;&#12290;&#22312;&#21069;&#21521;&#25193;&#25955;&#38454;&#27573;&#65292;&#36890;&#36807;&#36880;&#27493;&#28155;&#21152;&#39640;&#26031;&#22122;&#22768;&#36880;&#28176;&#25200;&#21160;&#36755;&#20837;&#25968;&#25454;&#12290;&#22312;&#21453;&#21521;&#38454;&#27573;&#65292;&#27169;&#22411;&#34987;&#20219;&#21153;&#20026;&#36890;&#36807;&#36880;&#27493;&#23398;&#20064;&#36870;&#36716;&#25193;&#25955;&#36807;&#31243;&#65292;&#36880;&#27493;&#24674;&#22797;&#21407;&#22987;&#36755;&#20837;&#25968;&#25454;&#12290;&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#30340;&#35745;&#31639;&#36127;&#25285;&#36739;&#22823;&#65292;&#21363;&#30001;&#20110;&#22312;&#37319;&#26679;&#36807;&#31243;&#20013;&#28041;&#21450;&#30340;&#27493;&#39588;&#25968;&#37327;&#36739;&#22810;&#23548;&#33268;&#30340;&#36895;&#24230;&#36739;&#24930;&#65292;&#20294;&#20854;&#25152;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20173;&#28982;&#21463;&#21040;&#24191;&#27867;&#27427;&#36175;&#12290;&#22312;&#26412;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#20110;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#22312;&#35270;&#35273;&#20013;&#24212;&#29992;&#30340;&#32508;&#21512;&#24615;&#35780;&#35770;&#65292;&#21253;&#25324;&#35813;&#39046;&#22495;&#30340;&#29702;&#35770;&#21644;&#23454;&#36341;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30830;&#23450;&#24182;&#20171;&#32461;&#20102;&#19977;&#31181;&#36890;&#29992;&#30340;&#25193;&#25955;&#24314;&#27169;&#26041;&#27861;&#65306;&#36830;&#32493;&#12289;&#31163;&#25955;&#21644;&#28151;&#21512;&#25193;&#25955;&#27169;&#22411;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#21508;&#31181;&#31639;&#27861;&#21644;&#26550;&#26500;&#26041;&#38754;&#65292;&#22914;&#20351;&#29992; L&#233;vy &#36807;&#31243;&#12289;&#19981;&#21516;&#24418;&#24335;&#30340;&#22122;&#22768;&#12289;&#27169;&#22411;&#26465;&#20214;&#21644;&#27491;&#21017;&#21270;&#12289;&#22810;&#23610;&#24230;&#26550;&#26500;&#21644;&#24182;&#34892;&#21270;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#24182;&#27604;&#36739;&#20102;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion models represent a recent emerging topic in computer vision, demonstrating remarkable results in the area of generative modeling. A diffusion model is a deep generative model that is based on two stages, a forward diffusion stage and a reverse diffusion stage. In the forward diffusion stage, the input data is gradually perturbed over several steps by adding Gaussian noise. In the reverse stage, a model is tasked at recovering the original input data by learning to gradually reverse the diffusion process, step by step. Diffusion models are widely appreciated for the quality and diversity of the generated samples, despite their known computational burdens, i.e. low speeds due to the high number of steps involved during sampling. In this survey, we provide a comprehensive review of articles on denoising diffusion models applied in vision, comprising both theoretical and practical contributions in the field. First, we identify and present three generic diffusion modelin
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#30740;&#32508;&#21512;&#24635;&#32467;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#21644;&#24212;&#29992;&#30740;&#31350;&#65292;&#21253;&#25324;&#39640;&#25928;&#37319;&#26679;&#12289;&#20284;&#28982;&#20272;&#35745;&#19982;&#29305;&#27530;&#32467;&#26500;&#25968;&#25454;&#22788;&#29702;&#65292;&#20171;&#32461;&#20102;&#25193;&#25955;&#27169;&#22411;&#19982;&#20854;&#20182;&#29983;&#25104;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#28508;&#21147;&#24182;&#22238;&#39038;&#20102;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2209.00796</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65306;&#26041;&#27861;&#21644;&#24212;&#29992;&#30340;&#32508;&#21512;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models: A Comprehensive Survey of Methods and Applications. (arXiv:2209.00796v10 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.00796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#32508;&#21512;&#24635;&#32467;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#21644;&#24212;&#29992;&#30740;&#31350;&#65292;&#21253;&#25324;&#39640;&#25928;&#37319;&#26679;&#12289;&#20284;&#28982;&#20272;&#35745;&#19982;&#29305;&#27530;&#32467;&#26500;&#25968;&#25454;&#22788;&#29702;&#65292;&#20171;&#32461;&#20102;&#25193;&#25955;&#27169;&#22411;&#19982;&#20854;&#20182;&#29983;&#25104;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#28508;&#21147;&#24182;&#22238;&#39038;&#20102;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#25104;&#20026;&#19968;&#31867;&#20855;&#26377;&#35760;&#24405;&#24615;&#33021;&#30340;&#24378;&#22823;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#21253;&#25324;&#22270;&#20687;&#21512;&#25104;&#12289;&#35270;&#39057;&#29983;&#25104;&#21644;&#20998;&#23376;&#35774;&#35745;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#22312;&#36825;&#20221;&#35843;&#30740;&#20013;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#20851;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#24555;&#36895;&#25193;&#23637;&#30740;&#31350;&#65292;&#23558;&#30740;&#31350;&#20998;&#31867;&#20026;&#19977;&#20010;&#20851;&#38190;&#39046;&#22495;&#65306;&#39640;&#25928;&#37319;&#26679;&#12289;&#25913;&#36827;&#30340;&#20284;&#28982;&#20272;&#35745;&#21644;&#22788;&#29702;&#20855;&#26377;&#29305;&#27530;&#32467;&#26500;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#20854;&#20182;&#29983;&#25104;&#27169;&#22411;&#30456;&#32467;&#21512;&#20197;&#23454;&#29616;&#22686;&#24378;&#32467;&#26524;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22238;&#39038;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#12289;&#26102;&#38388;&#25968;&#25454;&#24314;&#27169;&#20197;&#21450;&#20854;&#20182;&#31185;&#23398;&#23398;&#31185;&#30340;&#36328;&#23398;&#31185;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#36825;&#20010;&#35843;&#30740;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#20855;&#26377;&#32972;&#26223;&#30340;&#28145;&#20837;&#20102;&#35299;&#25193;&#25955;&#27169;&#22411;&#30340;&#29616;&#29366;&#65292;&#30830;&#23450;&#20851;&#38190;&#30340;&#30740;&#31350;&#37325;&#28857;&#65292;&#25351;&#26126;&#21487;&#33021;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language generation, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration. Github: https://github.com/Yan
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20984;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;Frank-Wolfe&#31639;&#27861;&#22312;&#28151;&#21512;&#25972;&#25968;&#21487;&#34892;&#28857;&#30340;&#20984;&#21253;&#19978;&#27714;&#35299;&#65292;&#20197;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#38750;&#32447;&#24615;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2208.11010</link><description>&lt;p&gt;
Frank-Wolfe&#26041;&#27861;&#30340;&#20984;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Convex mixed-integer optimization with Frank-Wolfe methods. (arXiv:2208.11010v5 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11010
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20984;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;Frank-Wolfe&#31639;&#27861;&#22312;&#28151;&#21512;&#25972;&#25968;&#21487;&#34892;&#28857;&#30340;&#20984;&#21253;&#19978;&#27714;&#35299;&#65292;&#20197;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#38750;&#32447;&#24615;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#25972;&#25968;&#38750;&#32447;&#24615;&#20248;&#21270;&#28085;&#30422;&#20102;&#19968;&#31867;&#26082;&#26377;&#29702;&#35770;&#21448;&#26377;&#35745;&#31639;&#25361;&#25112;&#30340;&#24191;&#27867;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25903;&#23450;&#30028;&#31639;&#27861;&#30340;&#26032;&#22411;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20351;&#29992;&#20984;&#33410;&#28857;&#26494;&#24347;&#30340;Frank-Wolfe&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#36825;&#20123;&#26494;&#24347;&#26159;&#22312;&#28151;&#21512;&#25972;&#25968;&#21487;&#34892;&#28857;&#30340;&#20984;&#21253;&#19978;&#27714;&#35299;&#30340;&#65292;&#32780;&#19981;&#26159;&#36890;&#36807;&#35843;&#29992;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#27714;&#35299;&#22120;&#30340;&#36830;&#32493;&#26494;&#24347;&#20316;&#20026;&#32447;&#24615;&#39044;&#35328;&#26426;&#26469;&#35299;&#20915;&#30340;&#12290;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#21333;&#20010;&#22810;&#38754;&#20307;&#32422;&#26463;&#30340;&#34920;&#31034;&#26102;&#35745;&#31639;&#21487;&#34892;&#35299;&#65292;&#21033;&#29992;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#27714;&#35299;&#22120;&#30340;&#20840;&#37096;&#33021;&#21147;&#65292;&#32780;&#19981;&#38656;&#35201;&#22806;&#37096;&#36924;&#36817;&#26041;&#26696;&#65292;&#24182;&#19988;&#21487;&#20197;&#21033;&#29992;&#33410;&#28857;&#23376;&#38382;&#39064;&#30340;&#19981;&#31934;&#30830;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixed-integer nonlinear optimization encompasses a broad class of problems that present both theoretical and computational challenges. We propose a new type of method to solve these problems based on a branch-and-bound algorithm with convex node relaxations. These relaxations are solved with a Frank-Wolfe algorithm over the convex hull of mixed-integer feasible points instead of the continuous relaxation via calls to a mixed-integer linear solver as the linear oracle. The proposed method computes feasible solutions while working on a single representation of the polyhedral constraints, leveraging the full extent of mixed-integer linear solvers without an outer approximation scheme and can exploit inexact solutions of node subproblems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;POCS&#26041;&#27861;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#24182;&#34892;&#25237;&#24433;&#26041;&#27861;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#25214;&#21040;&#36866;&#24403;&#30340;&#32858;&#31867;&#21407;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#32858;&#31867;&#35823;&#24046;&#21644;&#25191;&#34892;&#36895;&#24230;&#26041;&#38754;&#19982;&#20256;&#32479;&#32858;&#31867;&#26041;&#27861;&#30456;&#27604;&#20248;&#21183;&#26126;&#26174;&#12290;</title><link>http://arxiv.org/abs/2208.08888</link><description>&lt;p&gt;
&#22522;&#20110;POCS&#30340;&#32858;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
POCS-based Clustering Algorithm. (arXiv:2208.08888v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;POCS&#26041;&#27861;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#24182;&#34892;&#25237;&#24433;&#26041;&#27861;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#25214;&#21040;&#36866;&#24403;&#30340;&#32858;&#31867;&#21407;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#32858;&#31867;&#35823;&#24046;&#21644;&#25191;&#34892;&#36895;&#24230;&#26041;&#38754;&#19982;&#20256;&#32479;&#32858;&#31867;&#26041;&#27861;&#30456;&#27604;&#20248;&#21183;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20984;&#38598;&#25237;&#24433;(POCS)&#26041;&#27861;&#30340;&#26032;&#22411;&#32858;&#31867;&#25216;&#26415;&#65292;&#31216;&#20026;&#22522;&#20110;POCS&#30340;&#32858;&#31867;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#21033;&#29992;POCS&#30340;&#24182;&#34892;&#25237;&#24433;&#26041;&#27861;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#25214;&#21040;&#36866;&#24403;&#30340;&#32858;&#31867;&#21407;&#22411;&#12290;&#35813;&#31639;&#27861;&#23558;&#27599;&#20010;&#25968;&#25454;&#28857;&#35270;&#20026;&#19968;&#20010;&#20984;&#38598;&#65292;&#24182;&#23558;&#32858;&#31867;&#21407;&#22411;&#24179;&#34892;&#25237;&#24433;&#21040;&#25104;&#21592;&#25968;&#25454;&#28857;&#19978;&#65292;&#26368;&#23567;&#21270;&#25968;&#25454;&#32858;&#31867;&#30446;&#26631;&#20989;&#25968;&#12290;&#22312;&#21508;&#31181;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#20256;&#32479;&#32858;&#31867;&#26041;&#27861;&#65292;&#21253;&#25324;&#27169;&#31946;C-&#22343;&#20540;(FCM)&#21644;K-means&#32858;&#31867;&#31639;&#27861;&#30456;&#27604;&#65292;&#25552;&#20986;&#30340;&#22522;&#20110;POCS&#30340;&#32858;&#31867;&#31639;&#27861;&#22312;&#32858;&#31867;&#35823;&#24046;&#21644;&#25191;&#34892;&#36895;&#24230;&#26041;&#38754;&#37117;&#20855;&#26377;&#31454;&#20105;&#21147;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
A novel clustering technique based on the projection onto convex set (POCS) method, called POCS-based clustering algorithm, is proposed in this paper. The proposed POCS-based clustering algorithm exploits a parallel projection method of POCS to find appropriate cluster prototypes in the feature space. The algorithm considers each data point as a convex set and projects the cluster prototypes parallelly to the member data points. The projections are convexly combined to minimize the objective function for data clustering purpose. The performance of the proposed POCS-based clustering algorithm is verified through experiments on various synthetic datasets. The experimental results show that the proposed POCS-based clustering algorithm is competitive and efficient in terms of clustering error and execution speed when compared with other conventional clustering methods including Fuzzy C-Means (FCM) and K-means clustering algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#20998;&#26512;&#26041;&#27861;&#20197;&#27010;&#25324;&#24182;&#25913;&#36827;&#29616;&#26377;&#30340;&#22522;&#20110;&#26410;&#35843;&#25972;Langevin&#36716;&#31227;&#30340;&#24378;&#22823;&#21464;&#20998;&#20998;&#24067;&#26500;&#24314;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2208.07743</link><description>&lt;p&gt;
Langevin&#25193;&#25955;&#21464;&#20998;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Langevin Diffusion Variational Inference. (arXiv:2208.07743v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#20998;&#26512;&#26041;&#27861;&#20197;&#27010;&#25324;&#24182;&#25913;&#36827;&#29616;&#26377;&#30340;&#22522;&#20110;&#26410;&#35843;&#25972;Langevin&#36716;&#31227;&#30340;&#24378;&#22823;&#21464;&#20998;&#20998;&#24067;&#26500;&#24314;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22522;&#20110;&#26410;&#35843;&#25972;Langevin&#36716;&#31227;&#30340;&#24378;&#22823;&#21464;&#20998;&#20998;&#24067;&#26500;&#24314;&#26041;&#27861;&#24050;&#32463;&#23384;&#22312;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#22823;&#22810;&#37319;&#29992;&#20102;&#21508;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#32570;&#20047;&#32479;&#19968;&#30340;&#20998;&#26512;&#21644;&#25512;&#23548;&#65292;&#20174;&#32780;&#20351;&#24320;&#21457;&#26032;&#26041;&#27861;&#21644;&#25512;&#29702;&#29616;&#26377;&#26041;&#27861;&#25104;&#20026;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#19968;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#20197;&#32479;&#19968;&#21644;&#27010;&#25324;&#36825;&#20123;&#29616;&#26377;&#25216;&#26415;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#27424;&#38459;&#23612;Langevin&#25193;&#25955;&#36807;&#31243;&#21450;&#20854;&#26102;&#38388;&#32763;&#36716;&#23545;&#30446;&#26631;&#21644;&#21464;&#20998;&#36827;&#34892;&#22686;&#24191;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#22909;&#22788;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;&#23427;&#20026;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#20844;&#24335;&#65292;&#24182;&#31616;&#21270;&#20102;&#26032;&#26041;&#27861;&#30340;&#24320;&#21457;&#12290;&#20107;&#23454;&#19978;&#65292;&#21033;&#29992;&#25105;&#20204;&#30340;&#20844;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#20197;&#21069;&#29616;&#26377;&#31639;&#27861;&#30340;&#20248;&#28857;&#65307;&#23427;&#20351;&#29992;&#27424;&#38459;&#23612;Langevin&#36716;&#31227;&#21644;&#30001;&#24471;&#20998;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#24378;&#22823;&#30340;&#22686;&#24191;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#26174;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19968;&#31995;&#21015;&#22522;&#20934;&#38382;&#39064;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many methods that build powerful variational distributions based on unadjusted Langevin transitions exist. Most of these were developed using a wide range of different approaches and techniques. Unfortunately, the lack of a unified analysis and derivation makes developing new methods and reasoning about existing ones a challenging task. We address this giving a single analysis that unifies and generalizes these existing techniques. The main idea is to augment the target and variational by numerically simulating the underdamped Langevin diffusion process and its time reversal. The benefits of this approach are twofold: it provides a unified formulation for many existing methods, and it simplifies the development of new ones. In fact, using our formulation we propose a new method that combines the strengths of previously existing algorithms; it uses underdamped Langevin transitions and powerful augmentations parameterized by a score network. Our empirical evaluation shows that our propos
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20379;&#21253;&#21547;47&#31181;&#19981;&#21516;&#23646;&#24615;&#27880;&#37322;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;Deepfake&#26816;&#27979;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#26088;&#22312;&#30740;&#31350;&#20844;&#20849;Deepfake&#25968;&#25454;&#38598;&#21487;&#33021;&#24102;&#26469;&#30340;AI&#20559;&#24046;&#38382;&#39064;</title><link>http://arxiv.org/abs/2208.05845</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35268;&#27169;&#27880;&#37322;&#25968;&#25454;&#24211;&#30340;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979; AI &#20559;&#24046;&#30340;&#20840;&#38754;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Analysis of AI Biases in DeepFake Detection With Massively Annotated Databases. (arXiv:2208.05845v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.05845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20379;&#21253;&#21547;47&#31181;&#19981;&#21516;&#23646;&#24615;&#27880;&#37322;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;Deepfake&#26816;&#27979;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#26088;&#22312;&#30740;&#31350;&#20844;&#20849;Deepfake&#25968;&#25454;&#38598;&#21487;&#33021;&#24102;&#26469;&#30340;AI&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;Deepfake &#23545;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#31713;&#25913;&#24050;&#32463;&#25104;&#20026;&#23433;&#20840;&#21644;&#31038;&#20250;&#30340;&#20005;&#37325;&#20851;&#27880;&#28857;&#12290;&#35768;&#22810;&#26816;&#27979;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#24050;&#32463;&#34987;&#25552;&#20986;&#65292;&#21487;&#38752;&#22320;&#26816;&#27979; Deepfake &#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#25285;&#24515;&#36825;&#20123;&#27169;&#22411;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#21487;&#33021;&#23384;&#22312;&#20559;&#24046;&#65292;&#20174;&#32780;&#23548;&#33268; Deepfake &#26816;&#27979;&#22120;&#22833;&#25928;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20379;&#20116;&#20010;&#27969;&#34892;&#30340; Deepfake &#25968;&#25454;&#38598;&#20013; 47 &#31181;&#19981;&#21516;&#23646;&#24615;&#30340;&#22823;&#35268;&#27169;&#20154;&#21475;&#32479;&#35745;&#21644;&#38750;&#20154;&#21475;&#32479;&#35745;&#23646;&#24615;&#27880;&#37322;&#65292;&#24182;&#20840;&#38754;&#20998;&#26512;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340; Deepfake &#26816;&#27979;&#27169;&#22411;&#23545;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340; AI &#20559;&#24046;&#38382;&#39064;&#65292;&#35843;&#26597;&#30740;&#31350;&#20102;&#36229;&#36807; 6500 &#19975;&#20010;&#26631;&#31614;&#30340;&#35768;&#22810;&#19981;&#21516;&#23646;&#24615;&#65288;&#21253;&#25324;&#20154;&#21475;&#32479;&#35745;&#23398;&#65288;&#24180;&#40836;&#12289;&#24615;&#21035;&#12289;&#31181;&#26063;&#65289;&#21644;&#38750;&#20154;&#21475;&#32479;&#35745;&#23398;&#65288;&#22836;&#21457;&#12289;&#30382;&#32932;&#12289;&#37197;&#39280;&#31561;&#65289;&#20449;&#24687;&#23545;&#26816;&#27979;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35843;&#26597;&#30340;&#25968;&#25454;&#24211;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#21487;&#33021;&#23548;&#33268; AI &#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, image and video manipulations with Deepfake have become a severe concern for security and society. Many detection models and datasets have been proposed to detect Deepfake data reliably. However, there is an increased concern that these models and training databases might be biased and, thus, cause Deepfake detectors to fail. In this work, we investigate the bias issue caused by public Deepfake datasets by (a) providing large-scale demographic and non-demographic attribute annotations of 47 different attributes for five popular Deepfake datasets and (b) comprehensively analysing AI-bias of three state-of-the-art Deepfake detection backbone models on these datasets. The investigation analyses the influence of a large variety of distinctive attributes (from over 65M labels) on the detection performance, including demographic (age, gender, ethnicity) and non-demographic (hair, skin, accessories, etc.) information. The results indicate that investigated databases lack dive
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#37319;&#29992;&#20998;&#25955;&#31574;&#30053;&#30340;MARL&#31639;&#27861;&#22312;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#22120;&#19979;&#30340;&#27425;&#26368;&#20248;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36716;&#21270;&#19982;&#33976;&#39311;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#23558;&#22810;&#26234;&#33021;&#20307;MDP&#36716;&#21270;&#20026;&#21333;&#26234;&#33021;&#20307;MDP&#20197;&#23454;&#29616;&#20998;&#25955;&#25191;&#34892;&#12290;</title><link>http://arxiv.org/abs/2207.11143</link><description>&lt;p&gt;
&#12298;&#37319;&#29992;&#36716;&#21270;&#19982;&#33976;&#39311;&#26694;&#26550;&#23454;&#29616;&#21512;&#20316;MARL&#20840;&#23616;&#26368;&#20248;&#24615;&#12299;
&lt;/p&gt;
&lt;p&gt;
Towards Global Optimality in Cooperative MARL with the Transformation And Distillation Framework. (arXiv:2207.11143v3 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37319;&#29992;&#20998;&#25955;&#31574;&#30053;&#30340;MARL&#31639;&#27861;&#22312;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#22120;&#19979;&#30340;&#27425;&#26368;&#20248;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36716;&#21270;&#19982;&#33976;&#39311;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#23558;&#22810;&#26234;&#33021;&#20307;MDP&#36716;&#21270;&#20026;&#21333;&#26234;&#33021;&#20307;MDP&#20197;&#23454;&#29616;&#20998;&#25955;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20998;&#25955;&#25191;&#34892;&#26159;&#19968;&#39033;&#26680;&#24515;&#38656;&#27714;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;&#27969;&#34892;&#30340;MARL&#31639;&#27861;&#37319;&#29992;&#20998;&#25955;&#31574;&#30053;&#26469;&#23454;&#29616;&#20998;&#25955;&#25191;&#34892;&#65292;&#24182;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#20316;&#20026;&#20248;&#21270;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;&#32771;&#34385;&#21040;&#20248;&#21270;&#26041;&#27861;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#31639;&#27861;&#20960;&#20046;&#27809;&#26377;&#20219;&#20309;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#26799;&#24230;&#19979;&#38477;&#34987;&#36873;&#20026;&#20248;&#21270;&#26041;&#27861;&#26102;&#65292;&#21508;&#31181;&#27969;&#34892;&#30340;&#20998;&#25955;&#31574;&#30053;MARL&#31639;&#27861;&#22312;&#29609;&#20855;&#20219;&#21153;&#20013;&#37117;&#26159;&#27425;&#26368;&#20248;&#30340;&#12290;&#26412;&#25991;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#20004;&#31181;&#24120;&#35265;&#30340;&#37319;&#29992;&#20998;&#25955;&#31574;&#30053;&#30340;&#31639;&#27861;&#8212;&#8212;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#21644;&#20540;&#20998;&#35299;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#26102;&#30340;&#27425;&#26368;&#20248;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36716;&#21270;&#19982;&#33976;&#39311;&#65288;TAD&#65289;&#26694;&#26550;&#65292;&#23427;&#23558;&#22810;&#26234;&#33021;&#20307;MDP&#37325;&#26032;&#21046;&#23450;&#20026;&#19968;&#31181;&#20855;&#26377;&#36830;&#32493;&#32467;&#26500;&#30340;&#29305;&#27530;&#21333;&#26234;&#33021;&#20307;MDP&#65292;&#24182;&#36890;&#36807;&#33976;&#39311;&#23454;&#29616;&#20998;&#25955;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decentralized execution is one core demand in cooperative multi-agent reinforcement learning (MARL). Recently, most popular MARL algorithms have adopted decentralized policies to enable decentralized execution and use gradient descent as their optimizer. However, there is hardly any theoretical analysis of these algorithms taking the optimization method into consideration, and we find that various popular MARL algorithms with decentralized policies are suboptimal in toy tasks when gradient descent is chosen as their optimization method. In this paper, we theoretically analyze two common classes of algorithms with decentralized policies -- multi-agent policy gradient methods and value-decomposition methods to prove their suboptimality when gradient descent is used. In addition, we propose the Transformation And Distillation (TAD) framework, which reformulates a multi-agent MDP as a special single-agent MDP with a sequential structure and enables decentralized execution by distilling the
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;&#21487;&#20197;&#25913;&#21892;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#23458;&#25143;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#25110;&#23458;&#25143;&#31471;&#25968;&#25454;&#36827;&#34892;&#20998;&#25955;&#24335;&#39044;&#35757;&#32451;&#20063;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#24615;&#33021;&#65292;&#24182;&#19988;&#19981;&#21516;&#30340;&#25216;&#26415;&#21487;&#20197;&#30456;&#20114;&#34917;&#20805;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.11488</link><description>&lt;p&gt;
&#20851;&#20110;&#39044;&#35757;&#32451;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#24615;&#21644;&#36866;&#29992;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Importance and Applicability of Pre-Training for Federated Learning. (arXiv:2206.11488v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.11488
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;&#21487;&#20197;&#25913;&#21892;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#23458;&#25143;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#25110;&#23458;&#25143;&#31471;&#25968;&#25454;&#36827;&#34892;&#20998;&#25955;&#24335;&#39044;&#35757;&#32451;&#20063;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#24615;&#33021;&#65292;&#24182;&#19988;&#19981;&#21516;&#30340;&#25216;&#26415;&#21487;&#20197;&#30456;&#20114;&#34917;&#20805;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22312;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#20013;&#34987;&#24191;&#27867;&#29992;&#20110;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#25991;&#29486;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#22823;&#22810;&#25968;&#26159;&#20351;&#29992;&#38543;&#26426;&#26435;&#37325;&#21021;&#22987;&#21270;&#30340;&#12290;&#36825;&#24341;&#36215;&#20102;&#25105;&#20204;&#23545;&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#25506;&#32034;FL&#39044;&#35757;&#32451;&#30340;&#20852;&#36259;&#12290;&#22312;&#22810;&#20010;&#35270;&#35273;&#35782;&#21035;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#39044;&#35757;&#32451;&#19981;&#20165;&#21487;&#20197;&#25552;&#39640;FL&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#21487;&#20197;&#32553;&#23567;&#23427;&#19982;&#20013;&#24515;&#21270;&#23398;&#20064;&#20043;&#38388;&#30340;&#20934;&#30830;&#24230;&#24046;&#36317;&#65292;&#29305;&#21035;&#26159;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#23458;&#25143;&#25968;&#25454;&#30340;&#25361;&#25112;&#24615;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#20351;&#25105;&#20204;&#30340;&#21457;&#29616;&#36866;&#29992;&#20110;&#27809;&#26377;&#30452;&#25509;&#33719;&#24471;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#25110;&#29978;&#33267;&#20351;&#29992;&#23458;&#25143;&#31471;&#25968;&#25454;&#36827;&#34892;&#20998;&#25955;&#24335;&#39044;&#35757;&#32451;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#24050;&#32463;&#26174;&#33879;&#25913;&#21892;&#20102;FL&#30340;&#24615;&#33021;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#25506;&#32034;&#30340;&#35768;&#22810;&#25216;&#26415;&#20114;&#34917;&#24615;&#24456;&#24378;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#23558;&#36825;&#35270;&#20026;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#25193;&#23637;&#28145;&#24230;FL&#30340;&#20851;&#38190;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training is prevalent in nowadays deep learning to improve the learned model's performance. However, in the literature on federated learning (FL), neural networks are mostly initialized with random weights. These attract our interest in conducting a systematic study to explore pre-training for FL. Across multiple visual recognition benchmarks, we found that pre-training can not only improve FL, but also close its accuracy gap to the counterpart centralized learning, especially in the challenging cases of non-IID clients' data. To make our findings applicable to situations where pre-trained models are not directly available, we explore pre-training with synthetic data or even with clients' data in a decentralized manner, and found that they can already improve FL notably. Interestingly, many of the techniques we explore are complementary to each other to further boost the performance, and we view this as a critical result toward scaling up deep FL for real-world applications. We con
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#38477;&#22122;&#25193;&#25955;&#38544;&#24335;&#27169;&#22411;&#65288;DDIM&#65289;&#25193;&#23637;&#21040;&#19968;&#33324;&#25193;&#25955;&#27169;&#22411;&#65292;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#24191;&#20041;DDIM&#65288;gDDIM&#65289;&#65292;&#24182;&#22312;&#27169;&#31946;&#25193;&#25955;&#27169;&#22411;&#65288;BDM&#65289;&#21644;&#20020;&#30028;&#38459;&#23612;&#26391;&#20043;&#19975;&#25193;&#25955;&#27169;&#22411;&#65288;CLD&#65289;&#20013;&#33719;&#24471;&#20102;&#36229;&#36807;20&#20493;&#21644;15&#20493;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2206.05564</link><description>&lt;p&gt;
gDDIM: &#24191;&#20041;&#38477;&#22122;&#25193;&#25955;&#38544;&#24335;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
gDDIM: Generalized denoising diffusion implicit models. (arXiv:2206.05564v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.05564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#38477;&#22122;&#25193;&#25955;&#38544;&#24335;&#27169;&#22411;&#65288;DDIM&#65289;&#25193;&#23637;&#21040;&#19968;&#33324;&#25193;&#25955;&#27169;&#22411;&#65292;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#24191;&#20041;DDIM&#65288;gDDIM&#65289;&#65292;&#24182;&#22312;&#27169;&#31946;&#25193;&#25955;&#27169;&#22411;&#65288;BDM&#65289;&#21644;&#20020;&#30028;&#38459;&#23612;&#26391;&#20043;&#19975;&#25193;&#25955;&#27169;&#22411;&#65288;CLD&#65289;&#20013;&#33719;&#24471;&#20102;&#36229;&#36807;20&#20493;&#21644;15&#20493;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23558;&#38477;&#22122;&#25193;&#25955;&#38544;&#24335;&#27169;&#22411;(DDIM)&#25299;&#23637;&#21040;&#38500;&#21516;&#21521;&#25193;&#25955;&#22806;&#30340;&#19968;&#33324;&#25193;&#25955;&#27169;&#22411;(DMs)&#12290;&#25105;&#20204;&#20174;&#25968;&#20540;&#26041;&#38754;&#30740;&#31350;DDIM&#30340;&#26426;&#21046;&#65292;&#21457;&#29616;&#21487;&#20197;&#36890;&#36807;&#22312;&#35299;&#30456;&#24212;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#26102;&#20351;&#29992;&#29305;&#23450;&#30340;&#20998;&#25968;&#36817;&#20284;&#26469;&#33719;&#24471;DDIM&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;DDIM&#21152;&#36895;&#25928;&#26524;&#30340;&#26041;&#27861;&#65292;&#24182;&#35299;&#37322;&#20102;&#30830;&#23450;&#24615;&#37319;&#26679;&#26041;&#26696;&#27604;&#38543;&#26426;&#37319;&#26679;&#26041;&#26696;&#29992;&#20110;&#24555;&#36895;&#37319;&#26679;&#30340;&#20248;&#28857;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#20998;&#25968;&#32593;&#32476;&#36827;&#34892;&#19968;&#20123;&#24494;&#23567;&#32780;&#31934;&#24039;&#30340;&#20462;&#25913;&#65292;&#23558;DDIM&#25193;&#23637;&#21040;&#19968;&#33324;DMs&#65292;&#31216;&#20026;&#24191;&#20041;DDIM(gDDIM)&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#38750;&#21516;&#21521;DMs&#65288;&#27169;&#31946;&#25193;&#25955;&#27169;&#22411;(BDM)&#21644;&#20020;&#30028;&#38459;&#23612;&#26391;&#20043;&#19975;&#25193;&#25955;&#27169;&#22411;(CLD)&#65289;&#20013;&#39564;&#35777;&#20102;gDDIM&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;BDM&#20013;&#30340;&#21152;&#36895;&#25928;&#26524;&#36229;&#36807;20&#20493;&#12290;&#22312;CLD&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#25511;&#21046;&#23792;&#24230;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#25105;&#20204;&#22312;&#32500;&#25345;&#39640;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#20351;&#25910;&#25947;&#36895;&#24230;&#25552;&#39640;&#20102;15&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our goal is to extend the denoising diffusion implicit model (DDIM) to general diffusion models~(DMs) besides isotropic diffusions. Instead of constructing a non-Markov noising process as in the original DDIM, we examine the mechanism of DDIM from a numerical perspective. We discover that the DDIM can be obtained by using some specific approximations of the score when solving the corresponding stochastic differential equation. We present an interpretation of the accelerating effects of DDIM that also explains the advantages of a deterministic sampling scheme over the stochastic one for fast sampling. Building on this insight, we extend DDIM to general DMs, coined generalized DDIM (gDDIM), with a small but delicate modification in parameterizing the score network. We validate gDDIM in two non-isotropic DMs: Blurring diffusion model (BDM) and Critically-damped Langevin diffusion model (CLD). We observe more than 20 times acceleration in BDM. In the CLD, a diffusion model by augmenting th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#38543;&#26426;&#36817;&#20284;&#30340;&#37319;&#26679;&#31639;&#27861;&#65292;&#21033;&#29992;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#32467;&#26500;&#21560;&#25910;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#36924;&#36817;&#35823;&#24046;&#24182;&#33719;&#24471;&#20102;&#25913;&#36827;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;SGLD&#21644;RBM&#65292;&#25105;&#20204;&#20998;&#21035;&#35777;&#26126;&#20102;&#19981;&#21516;&#30340;&#20551;&#35774;&#26465;&#20214;&#19979;&#36739;&#20248;&#30340;&#25910;&#25947;&#29575;&#21644;&#21442;&#25968;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2206.03792</link><description>&lt;p&gt;
&#21033;&#29992;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#32467;&#26500;&#30340;&#38543;&#26426;&#26799;&#24230;&#37319;&#26679;&#26041;&#27861;&#65306;&#25913;&#36827;&#30340;&#20998;&#26512;&#21644;&#26356;&#24555;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Utilising the CLT Structure in Stochastic Gradient based Sampling : Improved Analysis and Faster Algorithms. (arXiv:2206.03792v3 [math.PR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#38543;&#26426;&#36817;&#20284;&#30340;&#37319;&#26679;&#31639;&#27861;&#65292;&#21033;&#29992;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#32467;&#26500;&#21560;&#25910;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#36924;&#36817;&#35823;&#24046;&#24182;&#33719;&#24471;&#20102;&#25913;&#36827;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;SGLD&#21644;RBM&#65292;&#25105;&#20204;&#20998;&#21035;&#35777;&#26126;&#20102;&#19981;&#21516;&#30340;&#20551;&#35774;&#26465;&#20214;&#19979;&#36739;&#20248;&#30340;&#25910;&#25947;&#29575;&#21644;&#21442;&#25968;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#38543;&#26426;&#36817;&#20284;&#30340;&#37319;&#26679;&#31639;&#27861;&#65292;&#22914;&#38543;&#26426;&#26799;&#24230; langevin &#21160;&#21147;&#23398;&#65288;SGLD&#65289;&#21644;&#38543;&#26426;&#25209;&#22788;&#29702;&#26041;&#27861;&#65288;RBM&#65289;&#29992;&#20110;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#21160;&#21147;&#23398;&#65288;IPD&#65289;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#30001;&#20110;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#65288;CLT&#65289;&#65292;&#38543;&#26426;&#36924;&#36817;&#24341;&#20837;&#30340;&#22122;&#22768;&#20960;&#20046;&#26159;&#39640;&#26031;&#20998;&#24067;&#65292;&#32780;&#39537;&#21160;&#24067;&#26391;&#36816;&#21160;&#21017;&#26159;&#30830;&#20999;&#30340;&#39640;&#26031;&#20998;&#24067;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#32467;&#26500;&#26469;&#21560;&#25910;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#36924;&#36817;&#35823;&#24046;&#65292;&#24182;&#33719;&#24471;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#25913;&#36827;&#25910;&#25947;&#20445;&#35777;&#12290;&#23545;&#20110; SGLD&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#19981;&#38656;&#35201;&#32479;&#19968;&#28201;&#26262;&#21551;&#21160;&#30340;&#24773;&#20917;&#19979;KL&#25955;&#24230;&#30340;&#31532;&#19968;&#20010;&#31283;&#23450;&#25910;&#25947;&#29575;&#65292;&#20551;&#35774;&#30446;&#26631;&#23494;&#24230;&#28385;&#36275;&#19968;&#20010;&#23545;&#25968; Sobolev &#19981;&#31561;&#24335;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24847;&#21619;&#30528;&#22312;&#26174;&#33879;&#36739;&#36731;&#30340;&#20551;&#35774;&#26465;&#20214;&#19979;&#65292;&#30456;&#23545;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#20855;&#26377;&#26356;&#20248;&#24322;&#30340;&#19968;&#38454; oracle &#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102; SGLD &#30340;&#31532;&#19968;&#20010;&#20445;&#35777;&#65292;&#23545;&#20110;&#26356;&#24369;&#30340;&#26465;&#20214;&#65292;&#22914; H\''{o}lder &#24179;&#28369;&#24615;&#21644; Poincare&#19981;&#31561;&#24335;&#65292;&#20174;&#32780;&#22635;&#34917;&#20102;&#29616;&#26377;&#25216;&#26415;&#21644;&#23454;&#38469;&#24212;&#29992;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#23545;&#20110; RBM&#65292;&#25105;&#20204;&#22312; IPD &#30340;&#24369;&#28151;&#21512;&#26465;&#20214;&#19979;&#33719;&#24471;&#20102;&#31532;&#19968;&#27425;&#25910;&#25947;&#20998;&#26512;&#21644;&#26368;&#20339;&#21442;&#25968;&#33539;&#22260;&#65292;&#36825;&#22312;&#32479;&#35745;&#29289;&#29702;&#21644;&#23398;&#20064;&#29702;&#35770;&#20013;&#20855;&#26377;&#20960;&#20010;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider stochastic approximations of sampling algorithms, such as Stochastic Gradient Langevin Dynamics (SGLD) and the Random Batch Method (RBM) for Interacting Particle Dynamcs (IPD). We observe that the noise introduced by the stochastic approximation is nearly Gaussian due to the Central Limit Theorem (CLT) while the driving Brownian motion is exactly Gaussian. We harness this structure to absorb the stochastic approximation error inside the diffusion process, and obtain improved convergence guarantees for these algorithms. For SGLD, we prove the first stable convergence rate in KL divergence without requiring uniform warm start, assuming the target density satisfies a Log-Sobolev Inequality. Our result implies superior first-order oracle complexity compared to prior works, under significantly milder assumptions. We also prove the first guarantees for SGLD under even weaker conditions such as H\"{o}lder smoothness and Poincare Inequality, thus bridging the gap between the state-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#29289;&#29702;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20805;&#20998;&#32771;&#34385;&#36793;&#30028;&#26465;&#20214;&#65292;&#20351;&#29992;&#38544;&#24335;&#26041;&#27861;&#39044;&#27979;&#38271;&#26102;&#38388;&#21518;&#30340;&#29366;&#24577;&#65292;&#24182;&#22312;&#21508;&#31181;&#24418;&#29366;&#19978;&#20855;&#26377;&#39640;&#24230;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#20026;&#20351;&#29992;&#28151;&#21512;&#36793;&#30028;&#26465;&#20214;&#25551;&#36848;&#30340;PDE&#36793;&#30028;&#20540;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2205.11912</link><description>&lt;p&gt;
&#29289;&#29702;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#65306;&#24102;&#28151;&#21512;&#36793;&#30028;&#26465;&#20214;&#30340;&#22270;&#31070;&#32463;PDE&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Physics-Embedded Neural Networks: Graph Neural PDE Solvers with Mixed Boundary Conditions. (arXiv:2205.11912v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#29289;&#29702;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20805;&#20998;&#32771;&#34385;&#36793;&#30028;&#26465;&#20214;&#65292;&#20351;&#29992;&#38544;&#24335;&#26041;&#27861;&#39044;&#27979;&#38271;&#26102;&#38388;&#21518;&#30340;&#29366;&#24577;&#65292;&#24182;&#22312;&#21508;&#31181;&#24418;&#29366;&#19978;&#20855;&#26377;&#39640;&#24230;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#20026;&#20351;&#29992;&#28151;&#21512;&#36793;&#30028;&#26465;&#20214;&#25551;&#36848;&#30340;PDE&#36793;&#30028;&#20540;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#21644;&#39044;&#27979;&#36793;&#30028;&#20540;&#38382;&#39064;&#20013;&#25551;&#36848;&#30340;&#29289;&#29702;&#29616;&#35937;&#65292;&#20363;&#22914;&#24102;&#26377;&#36793;&#30028;&#26465;&#20214;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27169;&#22411;&#19981;&#36275;&#20197;&#20805;&#20998;&#32771;&#34385;&#36793;&#30028;&#26465;&#20214;&#65292;&#20174;&#32780;&#26080;&#27861;&#21487;&#38752;&#22320;&#39044;&#27979;&#27492;&#31867;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;GNN&#30340;&#23616;&#37096;&#36830;&#25509;&#24615;&#36136;&#65292;&#20934;&#30830;&#39044;&#27979;&#38271;&#26102;&#38388;&#21518;&#30340;&#29366;&#24577;&#65292;&#20854;&#20013;&#39030;&#28857;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#24448;&#24448;&#26159;&#20840;&#23616;&#30340;&#65292;&#26159;&#22256;&#38590;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#29289;&#29702;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#36793;&#30028;&#26465;&#20214;&#65292;&#24182;&#20351;&#29992;&#38544;&#24335;&#26041;&#27861;&#39044;&#27979;&#38271;&#26102;&#38388;&#21518;&#30340;&#29366;&#24577;&#12290;&#23427;&#22522;&#20110;E&#65288;n&#65289;-&#31561;&#21464;GNN&#26500;&#24314;&#65292;&#22240;&#27492;&#22312;&#21508;&#31181;&#24418;&#29366;&#19978;&#20855;&#26377;&#39640;&#24230;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23398;&#20064;&#20102;&#22797;&#26434;&#24418;&#29366;&#20013;&#30340;&#27969;&#21160;&#29616;&#35937;&#65292;&#24182;&#22312;&#36895;&#24230;-&#31934;&#24230;&#26435;&#34913;&#26041;&#38754;&#20248;&#20110;&#32463;&#36807;&#33391;&#22909;&#20248;&#21270;&#30340;&#32463;&#20856;&#27714;&#35299;&#22120;&#21644;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20026;&#20351;&#29992;&#28151;&#21512;&#36793;&#30028;&#26465;&#20214;&#25551;&#36848;&#30340;PDE&#36793;&#30028;&#20540;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural network (GNN) is a promising approach to learning and predicting physical phenomena described in boundary value problems, such as partial differential equations (PDEs) with boundary conditions. However, existing models inadequately treat boundary conditions essential for the reliable prediction of such problems. In addition, because of the locally connected nature of GNNs, it is difficult to accurately predict the state after a long time, where interaction between vertices tends to be global. We present our approach termed physics-embedded neural networks that considers boundary conditions and predicts the state after a long time using an implicit method. It is built based on an E(n)-equivariant GNN, resulting in high generalization performance on various shapes. We demonstrate that our model learns flow phenomena in complex shapes and outperforms a well-optimized classical solver and a state-of-the-art machine learning model in speed-accuracy trade-off. Therefore, our mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#38543;&#26426;&#37325;&#27927;&#65288;D-RR&#65289;&#31639;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#21327;&#20316;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#24179;&#28369;&#24378;&#20984;&#21644;&#24179;&#28369;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;D-RR&#31639;&#27861;&#37117;&#33021;&#22815;&#23454;&#29616;&#24456;&#22909;&#30340;&#20248;&#21270;&#32467;&#26524;&#65292;&#24182;&#19988;&#20248;&#20110;&#20998;&#24067;&#24335;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2112.15287</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#38543;&#26426;&#37325;&#27927;&#31639;&#27861;&#22312;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Distributed Random Reshuffling over Networks. (arXiv:2112.15287v5 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.15287
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#38543;&#26426;&#37325;&#27927;&#65288;D-RR&#65289;&#31639;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#21327;&#20316;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#24179;&#28369;&#24378;&#20984;&#21644;&#24179;&#28369;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;D-RR&#31639;&#27861;&#37117;&#33021;&#22815;&#23454;&#29616;&#24456;&#22909;&#30340;&#20248;&#21270;&#32467;&#26524;&#65292;&#24182;&#19988;&#20248;&#20110;&#20998;&#24067;&#24335;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#22312;&#36830;&#36890;&#32593;&#32476;&#19978;&#65292;N&#20010;&#20195;&#29702;&#20197;&#21327;&#20316;&#30340;&#26041;&#24335;&#26368;&#23567;&#21270;&#26412;&#22320;&#20195;&#20215;&#20989;&#25968;&#30340;&#24179;&#22343;&#20540;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#38543;&#26426;&#37325;&#27927;&#65288;D-RR&#65289;&#31639;&#27861;&#29992;&#26469;&#22312;&#27599;&#20010;&#20195;&#29702;&#20013;&#35843;&#29992;&#38543;&#26426;&#37325;&#27927;&#65288;RR&#65289;&#26356;&#26032;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;D-RR&#32487;&#25215;&#20102;RR&#23545;&#20110;&#24179;&#28369;&#24378;&#20984;&#21644;&#24179;&#28369;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#30340;&#20248;&#36234;&#29305;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#23545;&#20110;&#24179;&#28369;&#24378;&#20984;&#30446;&#26631;&#20989;&#25968;&#65292;D-RR&#22312;&#36845;&#20195;&#21040;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#24179;&#26041;&#36317;&#31163;&#26041;&#38754;&#36798;&#21040;&#20102;$\mathcal O(1/T^2)$&#25910;&#25947;&#29575;&#65288;&#20854;&#20013;$T$&#34920;&#31034;&#36845;&#20195;&#27425;&#25968;&#65289;&#12290;&#24403;&#30446;&#26631;&#20989;&#25968;&#34987;&#20551;&#23450;&#20026;&#24179;&#28369;&#38750;&#20984;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;D-RR&#20197;$\mathcal O(1/T^{2/3})$&#30340;&#36895;&#29575;&#23558;&#26799;&#24230;&#30340;&#24179;&#26041;&#33539;&#25968;&#39537;&#21160;&#21040;0&#12290;&#36825;&#20123;&#25910;&#25947;&#32467;&#26524;&#19982;&#38598;&#20013;&#24335;&#30340;RR&#65288;&#19978;&#21040;&#24120;&#25968;&#22240;&#23376;&#65289;&#30456;&#21305;&#37197;&#24182;&#19988;&#20248;&#20110;&#20998;&#24067;&#24335;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider distributed optimization problems where $n$ agents, each possessing a local cost function, collaboratively minimize the average of the local cost functions over a connected network. To solve the problem, we propose a distributed random reshuffling (D-RR) algorithm that invokes the random reshuffling (RR) update in each agent. We show that D-RR inherits favorable characteristics of RR for both smooth strongly convex and smooth nonconvex objective functions. In particular, for smooth strongly convex objective functions, D-RR achieves $\mathcal{O}(1/T^2)$ rate of convergence (where $T$ counts epoch number) in terms of the squared distance between the iterate and the global minimizer. When the objective function is assumed to be smooth nonconvex, we show that D-RR drives the squared norm of gradient to $0$ at a rate of $\mathcal{O}(1/T^{2/3})$. These convergence results match those of centralized RR (up to constant factors) and outperform the distributed stochast
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#21333;&#20010;&#21644;&#22810;&#20010;&#26234;&#33021;&#20307;&#33258;&#20027;&#39550;&#39542;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#31639;&#27861;&#20197;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#22312;&#22810;&#20195;&#29702;&#39550;&#39542;&#29615;&#22659;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2112.11947</link><description>&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#22478;&#24066;&#39550;&#39542;&#29615;&#22659;&#20013;&#35780;&#20272;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#33258;&#20027;&#31574;&#30053;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Robustness of Deep Reinforcement Learning for Autonomous Policies in a Multi-agent Urban Driving Environment. (arXiv:2112.11947v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.11947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#21333;&#20010;&#21644;&#22810;&#20010;&#26234;&#33021;&#20307;&#33258;&#20027;&#39550;&#39542;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#31639;&#27861;&#20197;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#22312;&#22810;&#20195;&#29702;&#39550;&#39542;&#29615;&#22659;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#34987;&#24191;&#27867;&#29992;&#20110;&#22312;&#27169;&#25311;&#39550;&#39542;&#29615;&#22659;&#20013;&#35757;&#32451;&#33258;&#20027;&#36710;&#31574;&#30053;&#12290;&#30001;&#20110;&#21508;&#31181;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#22823;&#37327;&#21487;&#29992;&#24615;&#65292;&#24182;&#32570;&#20047;&#23545;&#23427;&#20204;&#22312;&#19981;&#21516;&#39550;&#39542;&#22330;&#26223;&#19979;&#30340;&#31995;&#32479;&#27604;&#36739;&#65292;&#25105;&#20204;&#19981;&#30830;&#23450;&#21738;&#20123;&#31639;&#27861;&#26356;&#26377;&#25928;&#22320;&#29992;&#20110;&#21333;&#36710;&#21644;&#22810;&#36710;&#39550;&#39542;&#29615;&#22659;&#20013;&#33258;&#20027;&#27773;&#36710;&#36719;&#20214;&#30340;&#35757;&#32451;&#12290;&#20026;&#35780;&#20272;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#22522;&#20110;&#35270;&#35273;&#30340;&#33258;&#20027;&#39550;&#39542;&#20013;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#24320;&#25918;&#19988;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#35780;&#20272;&#21644;&#27604;&#36739;&#20998;&#26512;&#21333;&#20010;&#21644;&#22810;&#20010;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#12290;&#21033;&#29992;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31163;&#25955;&#21644;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#27604;&#36739;&#30740;&#31350;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#31639;&#27861;&#65292;&#23558;&#31163;&#25955;&#21644;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#22312;&#22810;&#20195;&#29702;&#39550;&#39542;&#29615;&#22659;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning is actively used for training autonomous car policies in a simulated driving environment. Due to the large availability of various reinforcement learning algorithms and the lack of their systematic comparison across different driving scenarios, we are unsure of which ones are more effective for training autonomous car software in single-agent as well as multi-agent driving environments. A benchmarking framework for the comparison of deep reinforcement learning in a vision-based autonomous driving will open up the possibilities for training better autonomous car driving policies. To address these challenges, we provide an open and reusable benchmarking framework for systematic evaluation and comparative analysis of deep reinforcement learning algorithms for autonomous driving in a single- and multi-agent environment. Using the framework, we perform a comparative study of discrete and continuous action space deep reinforcement learning algorithms. We also prop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20960;&#20010;AL&#21477;&#23376;&#26597;&#35810;&#35780;&#20272;&#20989;&#25968;&#65292;&#20851;&#27880;&#28508;&#22312;&#27491;&#38754;&#26631;&#35760;&#65292;&#24182;&#20351;&#29992;&#26356;&#22909;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#27491;&#24120;&#21270;&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;NER&#27880;&#37322;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2111.03837</link><description>&lt;p&gt;
&#38598;&#20013;&#20851;&#27880;&#28508;&#22312;&#21629;&#21517;&#23454;&#20307;&#30340;&#20027;&#21160;&#26631;&#27880;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
Focusing on Potential Named Entities During Active Label Acquisition. (arXiv:2111.03837v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.03837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20960;&#20010;AL&#21477;&#23376;&#26597;&#35810;&#35780;&#20272;&#20989;&#25968;&#65292;&#20851;&#27880;&#28508;&#22312;&#27491;&#38754;&#26631;&#35760;&#65292;&#24182;&#20351;&#29992;&#26356;&#22909;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#27491;&#24120;&#21270;&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;NER&#27880;&#37322;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(NER)&#26088;&#22312;&#35782;&#21035;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#21629;&#21517;&#23454;&#20307;&#30340;&#25552;&#21450;&#24182;&#23558;&#20854;&#20998;&#31867;&#21040;&#39044;&#23450;&#20041;&#30340;&#21629;&#21517;&#23454;&#20307;&#31867;&#21035;&#20013;&#12290;&#34429;&#28982;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26377;&#21161;&#20110;&#22312;NER&#20013;&#23454;&#29616;&#33391;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20294;&#35768;&#22810;&#29305;&#23450;&#39046;&#22495;&#30340;NER&#24212;&#29992;&#20173;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#12290;&#20027;&#21160;&#23398;&#20064;(AL)&#26159;&#35299;&#20915;&#26631;&#31614;&#33719;&#21462;&#38382;&#39064;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#24050;&#29992;&#20110;NER&#20219;&#21153;&#65292;&#20197;&#26368;&#23567;&#21270;&#27880;&#37322;&#25104;&#26412;&#32780;&#19981;&#29306;&#29298;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26631;&#35760;&#30340;&#20005;&#37325;&#19981;&#22343;&#21248;&#31867;&#20998;&#24067;&#24341;&#20837;&#20102;&#35774;&#35745;&#26377;&#25928;&#30340;NER&#20027;&#21160;&#23398;&#20064;&#26597;&#35810;&#26041;&#27861;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;AL&#21477;&#23376;&#26597;&#35810;&#35780;&#20272;&#20989;&#25968;&#65292;&#26356;&#22810;&#20851;&#27880;&#28508;&#22312;&#30340;&#27491;&#38754;&#26631;&#35760;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#21477;&#23376;&#21644;&#26631;&#35760;&#25104;&#26412;&#35780;&#20272;&#31574;&#30053;&#26469;&#35780;&#20272;&#36825;&#20123;&#25552;&#35758;&#30340;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26356;&#22909;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#27491;&#24120;&#21270;&#26041;&#27861;&#65292;&#20197;&#24809;&#32602;&#36807;&#38271;&#25110;&#36807;&#30701;&#30340;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named entity recognition (NER) aims to identify mentions of named entities in an unstructured text and classify them into predefined named entity classes. While deep learning-based pre-trained language models help to achieve good predictive performances in NER, many domain-specific NER applications still call for a substantial amount of labeled data. Active learning (AL), a general framework for the label acquisition problem, has been used for NER tasks to minimize the annotation cost without sacrificing model performance. However, the heavily imbalanced class distribution of tokens introduces challenges in designing effective AL querying methods for NER. We propose several AL sentence query evaluation functions that pay more attention to potential positive tokens, and evaluate these proposed functions with both sentence-based and token-based cost evaluation strategies. We also propose a better data-driven normalization approach to penalize sentences that are too long or too short. Our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24212;&#29992;&#20110;&#20174;&#21307;&#30103;&#20445;&#20581;&#25110;&#25945;&#32946;&#39046;&#22495;&#25910;&#38598;&#30340;&#35266;&#27979;&#25968;&#25454;&#12290;&#25105;&#20204;&#32771;&#34385;&#22312;&#37096;&#20998;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#20013;&#36827;&#34892;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65292;&#20197;&#35299;&#20915;&#35266;&#23519;&#21040;&#30340;&#34892;&#21160;&#21487;&#33021;&#21463;&#21040;&#26410;&#35266;&#23519;&#21040;&#30340;&#22240;&#32032;&#24433;&#21709;&#65292;&#23548;&#33268;&#20272;&#35745;&#20540;&#20986;&#29616;&#20559;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2110.15332</link><description>&lt;p&gt;
&#36817;&#31471;&#24378;&#21270;&#23398;&#20064;&#65306;&#37096;&#20998;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#39640;&#25928;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Proximal Reinforcement Learning: Efficient Off-Policy Evaluation in Partially Observed Markov Decision Processes. (arXiv:2110.15332v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.15332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24212;&#29992;&#20110;&#20174;&#21307;&#30103;&#20445;&#20581;&#25110;&#25945;&#32946;&#39046;&#22495;&#25910;&#38598;&#30340;&#35266;&#27979;&#25968;&#25454;&#12290;&#25105;&#20204;&#32771;&#34385;&#22312;&#37096;&#20998;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#20013;&#36827;&#34892;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65292;&#20197;&#35299;&#20915;&#35266;&#23519;&#21040;&#30340;&#34892;&#21160;&#21487;&#33021;&#21463;&#21040;&#26410;&#35266;&#23519;&#21040;&#30340;&#22240;&#32032;&#24433;&#21709;&#65292;&#23548;&#33268;&#20272;&#35745;&#20540;&#20986;&#29616;&#20559;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20174;&#21307;&#30103;&#20445;&#20581;&#25110;&#25945;&#32946;&#39046;&#22495;&#25910;&#38598;&#30340;&#35266;&#23519;&#25968;&#25454;&#24212;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26102;&#65292;&#19968;&#20010;&#26222;&#36941;&#30340;&#20851;&#27880;&#28857;&#26159;&#65292;&#35266;&#23519;&#21040;&#30340;&#34892;&#21160;&#21487;&#33021;&#21463;&#21040;&#26410;&#35266;&#23519;&#21040;&#30340;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#24341;&#36215;&#28151;&#28102;&#24182;&#23548;&#33268;&#22312;&#20551;&#35774;&#23436;&#32654;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#24471;&#20986;&#30340;&#20272;&#35745;&#20540;&#20986;&#29616;&#20559;&#24046;&#12290;&#26412;&#25991;&#32771;&#34385;&#22312;&#37096;&#20998;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#20013;&#36827;&#34892;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#22312;&#32473;&#23450;&#21482;&#30001;&#19981;&#21516;&#19988;&#26410;&#30693;&#30340;&#31574;&#30053;&#29983;&#25104;&#30340;&#20855;&#26377;&#37096;&#20998;&#29366;&#24577;&#35266;&#27979;&#30340;&#36712;&#36857;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;POMDP&#20013;&#32473;&#23450;&#30446;&#26631;&#31574;&#30053;&#30340;&#20540;&#65292;&#35813;&#31574;&#30053;&#21487;&#33021;&#20381;&#36182;&#20110;&#26410;&#35266;&#23519;&#21040;&#30340;&#29366;&#24577;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#20004;&#20010;&#38382;&#39064;&#65306;&#20160;&#20040;&#26465;&#20214;&#20801;&#35768;&#25105;&#20204;&#20174;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#20013;&#35782;&#21035;&#30446;&#26631;&#31574;&#30053;&#20540;&#65292;&#24182;&#19988;&#22312;&#35782;&#21035;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#26368;&#22909;&#22320;&#20272;&#35745;&#23427;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#36817;&#31471;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#25193;&#23637;&#21040;&#25105;&#20204;&#30340;POMDP&#35774;&#32622;&#20013;&#65292;&#25552;&#20379;&#20102;&#35768;&#22810;&#22330;&#26223;&#65292;&#20854;&#20013;&#36890;&#36807;&#25152;&#35859;&#30340;&#26725;&#25509;&#20989;&#25968;&#30340;&#23384;&#22312;&#23454;&#29616;&#20102;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
In applications of offline reinforcement learning to observational data, such as in healthcare or education, a general concern is that observed actions might be affected by unobserved factors, inducing confounding and biasing estimates derived under the assumption of a perfect Markov decision process (MDP) model. Here we tackle this by considering off-policy evaluation in a partially observed MDP (POMDP). Specifically, we consider estimating the value of a given target policy in a POMDP given trajectories with only partial state observations generated by a different and unknown policy that may depend on the unobserved state. We tackle two questions: what conditions allow us to identify the target policy value from the observed data and, given identification, how to best estimate it. To answer these, we extend the framework of proximal causal inference to our POMDP setting, providing a variety of settings where identification is made possible by the existence of so-called bridge functio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32806;&#21512;&#35757;&#32451;&#33539;&#24335;FedSim&#65292;&#23558;&#19968;&#23545;&#22810;&#38142;&#25509;&#38598;&#25104;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;VFL&#26041;&#27861;&#24573;&#30053;&#8220;&#35760;&#24405;&#38142;&#25509;&#8221;&#36807;&#31243;&#30340;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2106.06312</link><description>&lt;p&gt;
&#19968;&#31181;&#21033;&#29992;&#35760;&#24405;&#30456;&#20284;&#24615;&#36827;&#34892;&#23454;&#38469;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#30340;&#32806;&#21512;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
A Coupled Design of Exploiting Record Similarity for Practical Vertical Federated Learning. (arXiv:2106.06312v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.06312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32806;&#21512;&#35757;&#32451;&#33539;&#24335;FedSim&#65292;&#23558;&#19968;&#23545;&#22810;&#38142;&#25509;&#38598;&#25104;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;VFL&#26041;&#27861;&#24573;&#30053;&#8220;&#35760;&#24405;&#38142;&#25509;&#8221;&#36807;&#31243;&#30340;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#23398;&#20064;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#19981;&#20844;&#24320;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#36328;&#19981;&#21516;&#26041;&#21442;&#19982;&#26041;&#30340;&#21327;&#20316;&#23398;&#20064;&#12290;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65288;VFL&#65289;&#26159;&#20854;&#20013;&#19968;&#31181;&#24212;&#29992;&#24191;&#27867;&#30340;&#20855;&#20307;&#23454;&#29616;&#65292;&#20854;&#20013;&#21442;&#19982;&#26041;&#20849;&#20139;&#30456;&#21516;&#30340;&#26679;&#26412;&#65292;&#20294;&#21482;&#25345;&#26377;&#37096;&#20998;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;VFL&#30740;&#31350;&#37117;&#24573;&#30053;&#20102;"&#35760;&#24405;&#38142;&#25509;"&#36807;&#31243;&#12290;&#23427;&#20204;&#35774;&#35745;&#30340;&#31639;&#27861;&#35201;&#20040;&#20551;&#35774;&#26469;&#33258;&#19981;&#21516;&#26041;&#30340;&#25968;&#25454;&#21487;&#20197;&#34987;&#20934;&#30830;&#22320;&#38142;&#25509;&#65292;&#35201;&#20040;&#20165;&#23558;&#27599;&#20010;&#35760;&#24405;&#38142;&#25509;&#21040;&#20854;&#26368;&#30456;&#20284;&#30340;&#30456;&#37051;&#35760;&#24405;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#25429;&#25417;&#20854;&#20182;&#19981;&#22826;&#30456;&#20284;&#35760;&#24405;&#30340;&#20851;&#38190;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#19981;&#21512;&#29702;&#30340;&#38142;&#25509;&#19981;&#33021;&#36890;&#36807;&#35757;&#32451;&#36827;&#34892;&#32416;&#27491;&#65292;&#22240;&#20026;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#27809;&#26377;&#25552;&#20379;&#20851;&#20110;&#38142;&#25509;&#30340;&#21453;&#39304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#32806;&#21512;&#35757;&#32451;&#33539;&#24335;FedSim&#65292;&#23558;&#19968;&#23545;&#22810;&#38142;&#25509;&#38598;&#25104;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#12290;&#38500;&#20102;&#20026;&#20855;&#26377;&#27169;&#31946;&#26631;&#35782;&#31526;&#30340;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#31243;&#24207;&#25552;&#20379;VFL&#22806;&#65292;FedSim&#36824;&#23454;&#29616;&#26356;&#22909;&#30340;&#27599;&#20010;
&lt;/p&gt;
&lt;p&gt;
Federated learning is a learning paradigm to enable collaborative learning across different parties without revealing raw data. Notably, vertical federated learning (VFL), where parties share the same set of samples but only hold partial features, has a wide range of real-world applications. However, most existing studies in VFL disregard the "record linkage" process. They design algorithms either assuming the data from different parties can be exactly linked or simply linking each record with its most similar neighboring record. These approaches may fail to capture the key features from other less similar records. Moreover, such improper linkage cannot be corrected by training since existing approaches provide no feedback on linkage during training. In this paper, we design a novel coupled training paradigm, FedSim, that integrates one-to-many linkage into the training process. Besides enabling VFL in many real-world applications with fuzzy identifiers, FedSim also achieves better per
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#21450;&#21487;&#33021;&#30340;&#21407;&#22240;&#65292;&#21457;&#29616;&#28145;&#24230;&#32593;&#32476;&#20855;&#26377;&#24402;&#32435;&#20559;&#35265;&#65292;&#26356;&#20542;&#21521;&#20110;&#23547;&#25214;&#20302;&#26377;&#25928;&#31209;&#23884;&#20837;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#35777;&#26126;&#20102;&#35813;&#20559;&#22909;&#22312;&#26377;&#38480;&#23485;&#24230;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#27169;&#22411;&#19978;&#30340;&#23454;&#29992;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2103.10427</link><description>&lt;p&gt;
&#28145;&#24230;&#32593;&#32476;&#20013;&#30340;&#20302;&#31209;&#31616;&#21333;&#24615;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
The Low-Rank Simplicity Bias in Deep Networks. (arXiv:2103.10427v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.10427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#21450;&#21487;&#33021;&#30340;&#21407;&#22240;&#65292;&#21457;&#29616;&#28145;&#24230;&#32593;&#32476;&#20855;&#26377;&#24402;&#32435;&#20559;&#35265;&#65292;&#26356;&#20542;&#21521;&#20110;&#23547;&#25214;&#20302;&#26377;&#25928;&#31209;&#23884;&#20837;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#35777;&#26126;&#20102;&#35813;&#20559;&#22909;&#22312;&#26377;&#38480;&#23485;&#24230;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#27169;&#22411;&#19978;&#30340;&#23454;&#29992;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#26412;&#25991;&#25506;&#31350;&#21644;&#25299;&#23637;&#20102;&#26356;&#28145;&#23618;&#32593;&#32476;&#20855;&#26377;&#24402;&#32435;&#20559;&#35265;&#65292;&#20197;&#23547;&#25214;&#20302;&#26377;&#25928;&#31209;&#23884;&#20837;&#35299;&#20915;&#26041;&#26696;&#30340;&#20551;&#35774;&#65292;&#36890;&#36807;&#23454;&#35777;&#35777;&#26126;&#20102;&#35813;&#20559;&#22909;&#22312;&#26377;&#38480;&#23485;&#24230;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#27169;&#22411;&#19978;&#30340;&#23454;&#29992;&#24615;&#24182;&#19988;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#36890;&#36807;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#32447;&#24615;&#36807;&#21442;&#25968;&#21270;&#26469;&#23454;&#29616;&#28145;&#24230;&#38750;&#32447;&#24615;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern deep neural networks are highly over-parameterized compared to the data on which they are trained, yet they often generalize remarkably well. A flurry of recent work has asked: why do deep networks not overfit to their training data? In this work, we make a series of empirical observations that investigate and extend the hypothesis that deeper networks are inductively biased to find solutions with lower effective rank embeddings. We conjecture that this bias exists because the volume of functions that maps to low effective rank embedding increases with depth. We show empirically that our claim holds true on finite width linear and non-linear models on practical learning paradigms and show that on natural data, these are often the solutions that generalize well. We then show that the simplicity bias exists at both initialization and after training and is resilient to hyper-parameters and learning methods. We further demonstrate how linear over-parameterization of deep non-linear 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;Q&#32593;&#32476;&#23398;&#20064;&#21551;&#21457;&#24335;&#20989;&#25968;&#65292;&#36890;&#36807;&#21482;&#36827;&#34892;&#19968;&#27425;&#21069;&#21521;&#20256;&#36882;&#35745;&#31639;&#30456;&#37051;&#33410;&#28857;&#30340;&#36716;&#31227;&#25104;&#26412;&#21644;&#21551;&#21457;&#24335;&#20540;&#20043;&#21644;&#65292;&#24182;&#22312;&#19981;&#26174;&#24335;&#29983;&#25104;&#36825;&#20123;&#23376;&#33410;&#28857;&#30340;&#24773;&#20917;&#19979;&#25351;&#23548;&#25628;&#32034;&#30340;Q*&#25628;&#32034;&#31639;&#27861;&#65292;&#20197;&#22823;&#24133;&#20943;&#23569;&#35745;&#31639;&#26102;&#38388;&#12290;&#22312;&#39764;&#26041;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#35299;&#20915;&#20855;&#26377;&#22823;&#21160;&#20316;&#31354;&#38388;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2102.04518</link><description>&lt;p&gt;
&#19981;&#25193;&#23637;&#30340;A*&#25628;&#32034;&#65306;&#29992;&#28145;&#24230;Q&#32593;&#32476;&#23398;&#20064;&#21551;&#21457;&#24335;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
A* Search Without Expansions: Learning Heuristic Functions with Deep Q-Networks. (arXiv:2102.04518v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.04518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;Q&#32593;&#32476;&#23398;&#20064;&#21551;&#21457;&#24335;&#20989;&#25968;&#65292;&#36890;&#36807;&#21482;&#36827;&#34892;&#19968;&#27425;&#21069;&#21521;&#20256;&#36882;&#35745;&#31639;&#30456;&#37051;&#33410;&#28857;&#30340;&#36716;&#31227;&#25104;&#26412;&#21644;&#21551;&#21457;&#24335;&#20540;&#20043;&#21644;&#65292;&#24182;&#22312;&#19981;&#26174;&#24335;&#29983;&#25104;&#36825;&#20123;&#23376;&#33410;&#28857;&#30340;&#24773;&#20917;&#19979;&#25351;&#23548;&#25628;&#32034;&#30340;Q*&#25628;&#32034;&#31639;&#27861;&#65292;&#20197;&#22823;&#24133;&#20943;&#23569;&#35745;&#31639;&#26102;&#38388;&#12290;&#22312;&#39764;&#26041;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#35299;&#20915;&#20855;&#26377;&#22823;&#21160;&#20316;&#31354;&#38388;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22320;&#20351;&#29992; A* &#25628;&#32034;&#35299;&#20915;&#20855;&#26377;&#22823;&#21160;&#20316;&#31354;&#38388;&#30340;&#38382;&#39064;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#20960;&#21313;&#24180;&#26469;&#19968;&#30452;&#38750;&#24120;&#37325;&#35201;&#12290;&#36825;&#26159;&#22240;&#20026; A* &#25628;&#32034;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#38543;&#30528;&#21160;&#20316;&#31354;&#38388;&#30340;&#22823;&#23567;&#21576;&#32447;&#24615;&#22686;&#38271;&#12290;&#24403; A* &#25628;&#32034;&#20351;&#29992;&#35745;&#31639;&#20195;&#20215;&#39640;&#26114;&#30340;&#20989;&#25968;&#36924;&#36817;&#22120;&#65288;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#23398;&#20064;&#21551;&#21457;&#24335;&#20989;&#25968;&#26102;&#65292;&#36825;&#31181;&#36127;&#25285;&#21464;&#24471;&#26356;&#21152;&#26126;&#26174;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; Q* &#25628;&#32034;&#65292;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230; Q &#32593;&#32476;&#24341;&#23548;&#25628;&#32034;&#30340;&#25628;&#32034;&#31639;&#27861;&#65292;&#20197;&#21033;&#29992;&#19968;&#20010;&#20107;&#23454;&#65292;&#21363;&#22312;&#19981;&#26174;&#24335;&#29983;&#25104;&#36825;&#20123;&#23376;&#33410;&#28857;&#30340;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#33410;&#28857;&#30340;&#23376;&#33410;&#28857;&#30340;&#36716;&#31227;&#25104;&#26412;&#21644;&#21551;&#21457;&#24335;&#20540;&#20043;&#21644;&#21487;&#20197;&#36890;&#36807;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#35745;&#31639;&#12290;&#36825;&#26174;&#30528;&#38477;&#20302;&#20102;&#35745;&#31639;&#26102;&#38388;&#65292;&#24182;&#19988;&#27599;&#27425;&#36845;&#20195;&#21482;&#38656;&#35201;&#29983;&#25104;&#19968;&#20010;&#33410;&#28857;&#12290;&#25105;&#20204;&#20351;&#29992; Q* &#25628;&#32034;&#26469;&#35299;&#20915;&#39764;&#26041;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#20204;&#34920;&#31034;&#20026;&#19968;&#20010;&#21253;&#21547; 1872 &#20010;&#20803;&#21160;&#20316;&#30340;&#22823;&#21160;&#20316;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently solving problems with large action spaces using A* search has been of importance to the artificial intelligence community for decades. This is because the computation and memory requirements of A* search grow linearly with the size of the action space. This burden becomes even more apparent when A* search uses a heuristic function learned by computationally expensive function approximators, such as deep neural networks. To address this problem, we introduce Q* search, a search algorithm that uses deep Q-networks to guide search in order to take advantage of the fact that the sum of the transition costs and heuristic values of the children of a node can be computed with a single forward pass through a deep Q-network without explicitly generating those children. This significantly reduces computation time and requires only one node to be generated per iteration. We use Q* search to solve the Rubik's cube when formulated with a large action space that includes 1872 meta-action
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36127;&#23545;&#29031;&#12289;&#20195;&#29702;&#21464;&#37327;&#21644;&#24037;&#20855;&#21464;&#37327;&#30340;&#26680;&#26041;&#27861;&#65292;&#20197;&#35782;&#21035;&#27835;&#30103;&#25928;&#26524;&#24182;&#23398;&#20064;&#38750;&#21442;&#25968;&#27835;&#30103;&#25928;&#26524;&#12290; &#20316;&#32773;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#19968;&#33268;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#20272;&#35745;&#20102;&#39321;&#28895;&#21560;&#28895;&#30340;&#21058;&#37327;&#21453;&#24212;&#26354;&#32447;&#12290;</title><link>http://arxiv.org/abs/2012.10315</link><description>&lt;p&gt;
&#26080;&#27861;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#21464;&#37327;&#30340;&#26680;&#26041;&#27861;&#65306;&#36127;&#23545;&#29031;&#12289;&#20195;&#29702;&#21464;&#37327;&#21644;&#24037;&#20855;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
Kernel Methods for Unobserved Confounding: Negative Controls, Proxies, and Instruments. (arXiv:2012.10315v5 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.10315
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36127;&#23545;&#29031;&#12289;&#20195;&#29702;&#21464;&#37327;&#21644;&#24037;&#20855;&#21464;&#37327;&#30340;&#26680;&#26041;&#27861;&#65292;&#20197;&#35782;&#21035;&#27835;&#30103;&#25928;&#26524;&#24182;&#23398;&#20064;&#38750;&#21442;&#25968;&#27835;&#30103;&#25928;&#26524;&#12290; &#20316;&#32773;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#19968;&#33268;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#20272;&#35745;&#20102;&#39321;&#28895;&#21560;&#28895;&#30340;&#21058;&#37327;&#21453;&#24212;&#26354;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36127;&#23545;&#29031;&#26159;&#19968;&#31181;&#22312;&#23384;&#22312;&#26410;&#27979;&#37327;&#28151;&#28102;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#27835;&#30103;&#19982;&#32467;&#26524;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#31574;&#30053;&#12290;&#22914;&#26524;&#26377;&#20004;&#20010;&#36741;&#21161;&#21464;&#37327;&#21487;&#29992;&#65306;&#19968;&#20010;&#36127;&#23545;&#29031;&#27835;&#30103;&#65288;&#23545;&#23454;&#38469;&#32467;&#26524;&#27809;&#26377;&#24433;&#21709;&#65289;&#21644;&#19968;&#20010;&#36127;&#23545;&#29031;&#32467;&#26524;&#65288;&#19981;&#21463;&#23454;&#38469;&#27835;&#30103;&#24433;&#21709;&#65289;&#65292;&#21017;&#20173;&#28982;&#21487;&#20197;&#35782;&#21035;&#27835;&#30103;&#25928;&#26524;&#12290; &#36825;&#20123;&#36741;&#21161;&#21464;&#37327;&#20063;&#21487;&#20197;&#35270;&#20026;&#20256;&#32479;&#25511;&#21046;&#21464;&#37327;&#38598;&#30340;&#20195;&#29702;&#21464;&#37327;&#65292;&#24182;&#19988;&#23427;&#20204;&#31867;&#20284;&#20110;&#24037;&#20855;&#21464;&#37327;&#12290;&#25105;&#25552;&#20986;&#20102;&#19968;&#26063;&#22522;&#20110;&#26680;&#23725;&#22238;&#24402;&#30340;&#31639;&#27861;&#65292;&#22312;&#36127;&#23545;&#29031;&#19979;&#23398;&#20064;&#38750;&#21442;&#25968;&#27835;&#30103;&#25928;&#26524;&#12290;&#31034;&#20363;&#21253;&#25324;&#21058;&#37327;&#21453;&#24212;&#26354;&#32447;&#12289;&#20855;&#26377;&#20998;&#24067;&#20559;&#31227;&#30340;&#21058;&#37327;&#21453;&#24212;&#26354;&#32447;&#21644;&#24322;&#36136;&#24615;&#27835;&#30103;&#25928;&#26524;&#12290; &#25968;&#25454;&#21487;&#20197;&#26159;&#31163;&#25955;&#12289;&#36830;&#32493;&#21644;&#20302;&#32500;&#12289;&#39640;&#32500;&#25110;&#26080;&#38480;&#32500;&#12290;&#25105;&#35777;&#26126;&#20102;&#22343;&#21248;&#19968;&#33268;&#24615;&#24182;&#25552;&#20379;&#20102;&#26377;&#38480;&#26679;&#26412;&#25910;&#25947;&#29575;&#12290; &#25105;&#20272;&#35745;&#20102;&#39321;&#28895;&#21560;&#28895;&#21058;&#37327;&#21453;&#24212;&#26354;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Negative control is a strategy for learning the causal relationship between treatment and outcome in the presence of unmeasured confounding. The treatment effect can nonetheless be identified if two auxiliary variables are available: a negative control treatment (which has no effect on the actual outcome), and a negative control outcome (which is not affected by the actual treatment). These auxiliary variables can also be viewed as proxies for a traditional set of control variables, and they bear resemblance to instrumental variables. I propose a family of algorithms based on kernel ridge regression for learning nonparametric treatment effects with negative controls. Examples include dose response curves, dose response curves with distribution shift, and heterogeneous treatment effects. Data may be discrete or continuous, and low, high, or infinite dimensional. I prove uniform consistency and provide finite sample rates of convergence. I estimate the dose response curve of cigarette sm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#24120;&#36890;&#29992;&#30340;&#26465;&#20214;&#30697;&#38382;&#39064;&#20272;&#35745;&#22120;&#31867; - &#21464;&#20998;&#30697;&#26041;&#27861;&#65292;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#25511;&#21046;&#26080;&#38480;&#25968;&#37327;&#30340;&#30697;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;&#26680;&#26041;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#20010;VMM&#20272;&#35745;&#22120;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2012.09422</link><description>&lt;p&gt;
&#26465;&#20214;&#30697;&#38382;&#39064;&#30340;&#21464;&#20998;&#30697;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
The Variational Method of Moments. (arXiv:2012.09422v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.09422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#24120;&#36890;&#29992;&#30340;&#26465;&#20214;&#30697;&#38382;&#39064;&#20272;&#35745;&#22120;&#31867; - &#21464;&#20998;&#30697;&#26041;&#27861;&#65292;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#25511;&#21046;&#26080;&#38480;&#25968;&#37327;&#30340;&#30697;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;&#26680;&#26041;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#20010;VMM&#20272;&#35745;&#22120;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#30697;&#38382;&#39064;&#26159;&#25551;&#36848;&#32467;&#26500;&#24615;&#22240;&#26524;&#21442;&#25968;&#30340;&#26377;&#21147;&#24418;&#24335;&#21270;&#24037;&#20855;&#12290;&#19968;&#20010;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#26377;&#38480;&#32452;&#30340;&#36793;&#38469;&#30697;&#26465;&#20214;&#65292;&#24182;&#24212;&#29992;&#26368;&#20248;&#21152;&#26435;&#24191;&#20041;&#30697;&#27861;&#65288;OWGMM&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20043;&#20026;&#21464;&#20998;&#30697;&#26041;&#27861;&#65288;VMM&#65289;&#30340;&#38750;&#24120;&#36890;&#29992;&#30340;&#26465;&#20214;&#30697;&#38382;&#39064;&#20272;&#35745;&#22120;&#31867;&#65292;&#24182;&#33258;&#28982;&#22320;&#20351;&#25105;&#20204;&#33021;&#22815;&#25511;&#21046;&#26080;&#38480;&#25968;&#37327;&#30340;&#30697;&#12290;&#20316;&#32773;&#23545;&#22810;&#20010;VMM&#20272;&#35745;&#22120;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#21253;&#25324;&#22522;&#20110;&#26680;&#26041;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#20272;&#35745;&#22120;&#65292;&#24182;&#25552;&#20379;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#19968;&#33268;&#20272;&#35745;&#30495;&#23454;&#22240;&#26524;&#21442;&#25968;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
The conditional moment problem is a powerful formulation for describing structural causal parameters in terms of observables, a prominent example being instrumental variable regression. A standard approach reduces the problem to a finite set of marginal moment conditions and applies the optimally weighted generalized method of moments (OWGMM), but this requires we know a finite set of identifying moments, can still be inefficient even if identifying, or can be theoretically efficient but practically unwieldy if we use a growing sieve of moment conditions. Motivated by a variational minimax reformulation of OWGMM, we define a very general class of estimators for the conditional moment problem, which we term the variational method of moments (VMM) and which naturally enables controlling infinitely-many moments. We provide a detailed theoretical analysis of multiple VMM estimators, including ones based on kernel methods and neural nets, and provide conditions under which these are consist
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#29983;&#25104;&#27169;&#22411;DiffStru&#65292;&#36890;&#36807;&#23398;&#20064;&#32806;&#21512;&#20302;&#32500;&#28508;&#22312;&#22240;&#32032;&#65292;&#22312;&#37096;&#20998;&#35266;&#27979;&#31038;&#20132;&#32593;&#32476;&#20013;&#32852;&#21512;&#25512;&#26029;&#26410;&#35266;&#27979;&#21040;&#30340;&#25193;&#25955;&#21644;&#32467;&#26500;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2010.01400</link><description>&lt;p&gt;
&#21033;&#29992;&#32806;&#21512;&#30697;&#38453;&#20998;&#35299;&#22312;&#37096;&#20998;&#35266;&#27979;&#31038;&#20132;&#32593;&#32476;&#20013;&#32852;&#21512;&#25512;&#26029;&#25193;&#25955;&#21644;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Joint Inference of Diffusion and Structure in Partially Observed Social Networks Using Coupled Matrix Factorization. (arXiv:2010.01400v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.01400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#29983;&#25104;&#27169;&#22411;DiffStru&#65292;&#36890;&#36807;&#23398;&#20064;&#32806;&#21512;&#20302;&#32500;&#28508;&#22312;&#22240;&#32032;&#65292;&#22312;&#37096;&#20998;&#35266;&#27979;&#31038;&#20132;&#32593;&#32476;&#20013;&#32852;&#21512;&#25512;&#26029;&#26410;&#35266;&#27979;&#21040;&#30340;&#25193;&#25955;&#21644;&#32467;&#26500;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#20013;&#65292;&#23436;&#25972;&#25968;&#25454;&#24448;&#24448;&#38590;&#20197;&#33719;&#21462;&#65292;&#22240;&#27492;&#32570;&#22833;&#25968;&#25454;&#30340;&#38382;&#39064;&#26159;&#20998;&#26512;&#21644;&#24314;&#27169;&#30495;&#23454;&#19990;&#30028;&#31038;&#20132;&#32593;&#32476;&#30340;&#19968;&#20010;&#20851;&#38190;&#21644;&#19981;&#21487;&#36991;&#20813;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20174;&#37096;&#20998;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#27169;&#22411;&#65292;&#25512;&#26029;&#26410;&#35266;&#27979;&#21040;&#30340;&#25193;&#25955;&#21644;&#32467;&#26500;&#32593;&#32476;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#29983;&#25104;&#27169;&#22411;DiffStru&#65292;&#36890;&#36807;&#23398;&#20064;&#32806;&#21512;&#20302;&#32500;&#28508;&#22312;&#22240;&#32032;&#65292;&#21033;&#29992;&#33410;&#28857;&#38142;&#25509;&#21644;&#32423;&#32852;&#36807;&#31243;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#38500;&#20102;&#25512;&#26029;&#26410;&#35265;&#25968;&#25454;&#22806;&#65292;&#28508;&#22312;&#22240;&#32032;&#65288;&#22914;&#31038;&#21306;&#26816;&#27979;&#65289;&#36824;&#21487;&#20197;&#24110;&#21161;&#32593;&#32476;&#20998;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Access to complete data in large-scale networks is often infeasible. Therefore, the problem of missing data is a crucial and unavoidable issue in the analysis and modeling of real-world social networks. However, most of the research on different aspects of social networks does not consider this limitation. One effective way to solve this problem is to recover the missing data as a pre-processing step. In this paper, a model is learned from partially observed data to infer unobserved diffusion and structure networks. To jointly discover omitted diffusion activities and hidden network structures, we develop a probabilistic generative model called "DiffStru." The interrelations among links of nodes and cascade processes are utilized in the proposed method via learning coupled with low-dimensional latent factors. Besides inferring unseen data, latent factors such as community detection may also aid in network classification problems. We tested different missing data scenarios on simulated 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RELL&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#21644;&#30693;&#35782;&#20445;&#25345;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20197;&#21327;&#21516;&#38598;&#25104;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#29420;&#31435;&#23398;&#20064;&#30340;&#34920;&#31034;&#65292;&#22312;&#20934;&#32447;&#24615;&#22797;&#26434;&#24230;&#19979;&#23454;&#29616;&#20102;&#21069;&#21521;&#21644;&#21518;&#21521;&#20256;&#36882;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;RELL&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#26174;&#30528;&#25913;&#21892;&#21453;&#21521;&#20256;&#36882;&#12290;</title><link>http://arxiv.org/abs/2004.12908</link><description>&lt;p&gt;
&#20195;&#34920;&#24615;&#38598;&#25104;&#22312;&#20934;&#32447;&#24615;&#22797;&#26434;&#24230;&#19979;&#23454;&#29616;&#21327;&#21516;&#29983;&#21629;&#21608;&#26399;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Ensembling for Synergistic Lifelong Learning with Quasilinear Complexity. (arXiv:2004.12908v16 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2004.12908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RELL&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#21644;&#30693;&#35782;&#20445;&#25345;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20197;&#21327;&#21516;&#38598;&#25104;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#29420;&#31435;&#23398;&#20064;&#30340;&#34920;&#31034;&#65292;&#22312;&#20934;&#32447;&#24615;&#22797;&#26434;&#24230;&#19979;&#23454;&#29616;&#20102;&#21069;&#21521;&#21644;&#21518;&#21521;&#20256;&#36882;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;RELL&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#26174;&#30528;&#25913;&#21892;&#21453;&#21521;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32456;&#36523;&#23398;&#20064;&#20013;&#65292;&#25968;&#25454;&#19981;&#20165;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#24403;&#21069;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#36824;&#21487;&#20197;&#29992;&#20110;&#20043;&#21069;&#21644;&#23578;&#26410;&#36935;&#21040;&#30340;&#20219;&#21153;&#12290;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#21017;&#20174;&#31354;&#30333;&#29366;&#24577;&#24320;&#22987;&#65292;&#20165;&#38024;&#23545;&#21333;&#20010;&#20219;&#21153;&#20351;&#29992;&#25968;&#25454;&#12290;&#34429;&#28982;&#20256;&#32479;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#25552;&#39640;&#26410;&#26469;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#21518;&#23545;&#26087;&#20219;&#21153;&#30340;&#24615;&#33021;&#19979;&#38477;&#65288;&#31216;&#20026;&#36951;&#24536;&#65289;&#12290;&#36817;&#26399;&#38024;&#23545;&#36830;&#32493;&#25110;&#32456;&#36523;&#23398;&#20064;&#30340;&#35768;&#22810;&#26041;&#27861;&#37117;&#35797;&#22270;&#22312;&#32473;&#23450;&#26032;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#23545;&#26087;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#20165;&#21162;&#21147;&#36991;&#20813;&#24536;&#35760;&#23558;&#30446;&#26631;&#23450;&#24471;&#36807;&#20302;&#12290;&#32456;&#36523;&#23398;&#20064;&#30340;&#30446;&#26631;&#19981;&#20165;&#24212;&#35813;&#26159;&#25552;&#39640;&#26410;&#26469;&#20219;&#21153;&#65288;&#21069;&#21521;&#20256;&#36882;&#65289;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#36824;&#24212;&#35813;&#26159;&#29992;&#20219;&#20309;&#26032;&#25968;&#25454;&#25552;&#39640;&#36807;&#21435;&#20219;&#21153;&#65288;&#21453;&#21521;&#20256;&#36882;&#65289;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#25105;&#20204;&#21487;&#20197;&#21327;&#21516;&#38598;&#25104;&#20998;&#21035;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#29420;&#31435;&#23398;&#20064;&#30340;&#34920;&#31034;&#65292;&#20197;&#23454;&#29616;&#20934;&#32447;&#24615;&#22797;&#26434;&#24230;&#19979;&#30340;&#21069;&#21521;&#21644;&#21518;&#21521;&#20256;&#36882;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#32456;&#36523;&#23398;&#20064;&#20013;&#30340;&#34920;&#31034;&#38598;&#25104;&#65288;RELL&#65289;&#8221;&#65292;&#23427;&#38598;&#25104;&#20102;&#30693;&#35782;&#33976;&#39311;&#21644;&#30693;&#35782;&#20445;&#25345;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20197;&#21033;&#29992;&#19981;&#21516;&#34920;&#31034;&#20013;&#21253;&#21547;&#30340;&#20114;&#34917;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;RELL&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#37117;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26174;&#30528;&#26356;&#22909;&#30340;&#21453;&#21521;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;
In lifelong learning, data are used to improve performance not only on the current task, but also on previously encountered, and as yet unencountered tasks. In contrast, classical machine learning, which we define as, starts from a blank slate, or tabula rasa and uses data only for the single task at hand. While typical transfer learning algorithms can improve performance on future tasks, their performance on prior tasks degrades upon learning new tasks (called forgetting). Many recent approaches for continual or lifelong learning have attempted to maintain performance on old tasks given new tasks. But striving to avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning should be not only to improve performance on future tasks (forward transfer) but also on past tasks (backward transfer) with any new data. Our key insight is that we can synergistically ensemble representations -- that were learned independently on disparate tasks -- to enable both forward and bac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;dropout&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#21450;&#25910;&#25947;&#36895;&#24230;&#30340;&#30740;&#31350;&#65292;&#32473;&#20986;&#20102;&#27010;&#29575;&#35770;&#35777;&#26126;&#65292;&#35777;&#26126;&#20854;&#26435;&#37325;&#23558;&#25910;&#25947;&#20110;&#27491;&#24120;&#24494;&#20998;&#26041;&#31243;&#31995;&#32479;&#30340;&#25237;&#24433;&#21807;&#19968;&#31283;&#24577;&#28857;&#65292;&#21516;&#26102;&#32473;&#20986;&#20102;&#949;-&#23450;&#24577;&#28857;&#30340;&#36890;&#29992;&#26679;&#26412;&#22797;&#26434;&#24230;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2002.02247</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;Dropout&#31639;&#27861;&#30340;&#20960;&#20046;&#24517;&#28982;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Almost Sure Convergence of Dropout Algorithms for Neural Networks. (arXiv:2002.02247v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2002.02247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;dropout&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#21450;&#25910;&#25947;&#36895;&#24230;&#30340;&#30740;&#31350;&#65292;&#32473;&#20986;&#20102;&#27010;&#29575;&#35770;&#35777;&#26126;&#65292;&#35777;&#26126;&#20854;&#26435;&#37325;&#23558;&#25910;&#25947;&#20110;&#27491;&#24120;&#24494;&#20998;&#26041;&#31243;&#31995;&#32479;&#30340;&#25237;&#24433;&#21807;&#19968;&#31283;&#24577;&#28857;&#65292;&#21516;&#26102;&#32473;&#20986;&#20102;&#949;-&#23450;&#24577;&#28857;&#30340;&#36890;&#29992;&#26679;&#26412;&#22797;&#26434;&#24230;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21463;Dropout&#65288;Hinton&#31561;&#65292;2012&#65289;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#38543;&#26426;&#35757;&#32451;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;&#20026;&#20102;&#36991;&#20813;&#35757;&#32451;&#26399;&#38388;&#30340;&#36807;&#24230;&#25311;&#21512;&#65292;&#23454;&#36341;&#20013;&#30340;dropout&#31639;&#27861;&#23454;&#38469;&#19978;&#26159;&#36890;&#36807;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#27599;&#27425;&#36845;&#20195;&#26399;&#38388;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#30697;&#38453;&#36880;&#20803;&#32032;&#19982;&#29420;&#31435;&#32472;&#21046;&#35813;&#20989;&#25968;&#30340;{0,1} -&#20540;&#30697;&#38453;&#30456;&#20056;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#29702;&#35770;&#35777;&#26126;&#65292;&#38024;&#23545;&#20855;&#26377;&#21487;&#24494;&#12289;&#22810;&#39033;&#24335;&#26377;&#30028;&#28608;&#27963;&#20989;&#25968;&#30340;&#20840;&#36830;&#36890;&#31070;&#32463;&#32593;&#32476;&#65292;&#22914;&#26524;&#25105;&#20204;&#22312;&#20351;&#29992;dropout&#31639;&#27861;&#26102;&#23558;&#26435;&#37325;&#25237;&#24433;&#21040;&#32039;&#33268;&#38598;&#19978;&#65292;&#21017;NN&#30340;&#26435;&#37325;&#23558;&#25910;&#25947;&#20110;&#27491;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODEs&#65289;&#30340;&#25237;&#24433;&#31995;&#32479;&#30340;&#21807;&#19968;&#23450;&#24120;&#28857;&#12290;&#22312;&#27492;&#36890;&#29992;&#25910;&#25947;&#24615;&#20445;&#35777;&#20043;&#21518;&#65292;&#25105;&#20204;&#32487;&#32493;&#30740;&#31350;dropout&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#25214;&#21040;&#949;-&#23450;&#24577;&#28857;&#30340;&#36890;&#29992;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the convergence and convergence rate of stochastic training algorithms for Neural Networks (NNs) that have been inspired by Dropout (Hinton et al., 2012). With the goal of avoiding overfitting during training of NNs, dropout algorithms consist in practice of multiplying the weight matrices of a NN componentwise by independently drawn random matrices with $\{0, 1 \}$-valued entries during each iteration of Stochastic Gradient Descent (SGD). This paper presents a probability theoretical proof that for fully-connected NNs with differentiable, polynomially bounded activation functions, if we project the weights onto a compact set when using a dropout algorithm, then the weights of the NN converge to a unique stationary point of a projected system of Ordinary Differential Equations (ODEs). After this general convergence guarantee, we go on to investigate the convergence rate of dropout. Firstly, we obtain generic sample complexity bounds for finding $\epsilon$-stationary poin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#24773;&#32490;&#20998;&#26512;&#12289;&#30524;&#21160;&#21644;&#22836;&#37096;&#36816;&#21160;&#36827;&#34892;&#23398;&#29983;&#21442;&#19982;&#24230;&#26816;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#33021;&#22815;&#20351;&#29992;&#31508;&#35760;&#26412;&#30005;&#33041;&#20869;&#32622;&#25668;&#20687;&#22836;&#23454;&#26102;&#30417;&#27979;&#23398;&#29983;&#30340;&#19987;&#27880;&#24230;&#65292;&#36890;&#36807;&#32467;&#21512;&#30524;&#30555;&#21644;&#22836;&#37096;&#30340;&#36816;&#21160;&#20449;&#24687;&#20197;&#21450;&#38754;&#37096;&#34920;&#24773;&#26469;&#35782;&#21035;&#23398;&#29983;&#22788;&#20110;&#19977;&#31181;&#19981;&#21516;&#30340;&#21442;&#19982;&#24230;&#29366;&#24577;&#65292;&#24182;&#22312;&#20856;&#22411;&#30340;&#22312;&#32447;&#23398;&#20064;&#24773;&#22659;&#19979;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#33021;&#22815;&#27491;&#30830;&#22320;&#35782;&#21035;&#23398;&#29983;&#30340;&#21442;&#19982;&#24230;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/1909.12913</link><description>&lt;p&gt;
&#21033;&#29992;&#24773;&#32490;&#20998;&#26512;&#12289;&#30524;&#21160;&#21644;&#22836;&#37096;&#36816;&#21160;&#36827;&#34892;&#23398;&#29983;&#21442;&#19982;&#24230;&#26816;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Student Engagement Detection Using Emotion Analysis, Eye Tracking and Head Movement with Machine Learning. (arXiv:1909.12913v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1909.12913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#24773;&#32490;&#20998;&#26512;&#12289;&#30524;&#21160;&#21644;&#22836;&#37096;&#36816;&#21160;&#36827;&#34892;&#23398;&#29983;&#21442;&#19982;&#24230;&#26816;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#33021;&#22815;&#20351;&#29992;&#31508;&#35760;&#26412;&#30005;&#33041;&#20869;&#32622;&#25668;&#20687;&#22836;&#23454;&#26102;&#30417;&#27979;&#23398;&#29983;&#30340;&#19987;&#27880;&#24230;&#65292;&#36890;&#36807;&#32467;&#21512;&#30524;&#30555;&#21644;&#22836;&#37096;&#30340;&#36816;&#21160;&#20449;&#24687;&#20197;&#21450;&#38754;&#37096;&#34920;&#24773;&#26469;&#35782;&#21035;&#23398;&#29983;&#22788;&#20110;&#19977;&#31181;&#19981;&#21516;&#30340;&#21442;&#19982;&#24230;&#29366;&#24577;&#65292;&#24182;&#22312;&#20856;&#22411;&#30340;&#22312;&#32447;&#23398;&#20064;&#24773;&#22659;&#19979;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#33021;&#22815;&#27491;&#30830;&#22320;&#35782;&#21035;&#23398;&#29983;&#30340;&#21442;&#19982;&#24230;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36828;&#31243;&#23398;&#20064;&#21644;&#22312;&#32447;&#23398;&#20064;&#30340;&#19981;&#26029;&#22686;&#22810;&#65292;&#24314;&#31435;&#19968;&#20010;&#33021;&#22815;&#30830;&#23450;&#23398;&#29983;&#21442;&#19982;&#24230;&#30340;&#31995;&#32479;&#23545;&#20110;&#25945;&#24072;&#12289;&#30740;&#31350;&#20154;&#21592;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#26469;&#26816;&#27979;&#23398;&#29983;&#21442;&#19982;&#24230;&#12290;&#23427;&#20165;&#20351;&#29992;&#31508;&#35760;&#26412;&#30005;&#33041;&#20869;&#32622;&#30340;&#20856;&#22411;&#32593;&#32476;&#25668;&#20687;&#22836;&#25552;&#20379;&#30340;&#20449;&#24687;&#65292;&#24182;&#34987;&#35774;&#35745;&#20026;&#23454;&#26102;&#24037;&#20316;&#12290;&#25105;&#20204;&#32467;&#21512;&#20102;&#30524;&#30555;&#21644;&#22836;&#37096;&#30340;&#36816;&#21160;&#20449;&#24687;&#20197;&#21450;&#38754;&#37096;&#34920;&#24773;&#26469;&#20135;&#29983;&#24102;&#26377;&#19977;&#31181;&#21442;&#19982;&#24230;&#20998;&#31867;&#30340;&#19987;&#27880;&#24230;&#25351;&#25968;&#65306;&#8220;&#38750;&#24120;&#21442;&#19982;&#24230;&#8221;&#65292;&#8220;&#21517;&#20041;&#19978;&#21442;&#19982;&#24230;&#8221;&#21644;&#8220;&#27809;&#26377;&#21442;&#19982;&#24230;&#8221;&#12290;&#35813;&#31995;&#32479;&#22312;&#20856;&#22411;&#30340;&#22312;&#32447;&#23398;&#20064;&#24773;&#24418;&#19979;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#33021;&#22815;&#27491;&#30830;&#35782;&#21035;&#23398;&#29983;&#22788;&#20110;&#8220;&#38750;&#24120;&#21442;&#19982;&#24230;&#8221;&#65292;&#8220;&#21517;&#20041;&#19978;&#21442;&#19982;&#24230;&#8221;&#21644;&#8220;&#27809;&#26377;&#21442;&#19982;&#24230;&#8221;&#30340;&#26102;&#26399;&#12290;&#27492;&#22806;&#65292;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#25104;&#32489;&#26368;&#22909;&#30340;&#23398;&#29983;&#20063;&#20855;&#26377;&#26356;&#39640;&#30340;&#19987;&#27880;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increase of distance learning, in general, and e-learning, in particular, having a system capable of determining the engagement of students is of primordial importance, and one of the biggest challenges, both for teachers, researchers and policy makers. Here, we present a system to detect the engagement level of the students. It uses only information provided by the typical built-in web-camera present in a laptop computer, and was designed to work in real time. We combine information about the movements of the eyes and head, and facial emotions to produce a concentration index with three classes of engagement: "very engaged", "nominally engaged" and "not engaged at all". The system was tested in a typical e-learning scenario, and the results show that it correctly identifies each period of time where students were "very engaged", "nominally engaged" and "not engaged at all". Additionally, the results also show that the students with best scores also have higher concentration i
&lt;/p&gt;</description></item></channel></rss>