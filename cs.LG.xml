<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Vabs-Net&#36827;&#34892;&#22810;&#32423;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#24403;&#21069;&#22823;&#22810;&#25968;&#22522;&#20110;&#32467;&#26500;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20165;&#20851;&#27880;&#27531;&#22522;&#27700;&#24179;&#65292;&#20294;&#24573;&#30053;&#20102;&#20391;&#38142;&#21407;&#23376;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#24341;&#20837;&#20102;&#36328;&#24230;&#25513;&#30721;&#65292;&#20197;&#22312;&#19977;&#32500;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#20013;&#21516;&#26102;&#24314;&#27169;&#27531;&#22522;&#21644;&#21407;&#23376;&#27700;&#24179;&#30340;&#20449;&#24687;&#65292;&#24182;&#25913;&#21892;&#20102;&#27531;&#22522;&#34920;&#31034;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01481</link><description>&lt;p&gt;
&#29992;Vabs-Net&#36827;&#34892;&#22810;&#32423;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Multi-level protein pre-training with Vabs-Net
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01481
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Vabs-Net&#36827;&#34892;&#22810;&#32423;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#24403;&#21069;&#22823;&#22810;&#25968;&#22522;&#20110;&#32467;&#26500;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20165;&#20851;&#27880;&#27531;&#22522;&#27700;&#24179;&#65292;&#20294;&#24573;&#30053;&#20102;&#20391;&#38142;&#21407;&#23376;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#24341;&#20837;&#20102;&#36328;&#24230;&#25513;&#30721;&#65292;&#20197;&#22312;&#19977;&#32500;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#20013;&#21516;&#26102;&#24314;&#27169;&#27531;&#22522;&#21644;&#21407;&#23376;&#27700;&#24179;&#30340;&#20449;&#24687;&#65292;&#24182;&#25913;&#21892;&#20102;&#27531;&#22522;&#34920;&#31034;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#19977;&#32500;&#32467;&#26500;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#27169;&#22411;&#30340;&#21457;&#23637;&#36805;&#29467;&#65292;&#30456;&#36739;&#20110;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20110;&#32467;&#26500;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#27531;&#22522;&#27700;&#24179;&#65292;&#21363;&#945;&#30899;&#21407;&#23376;&#65292;&#32780;&#24573;&#30053;&#20102;&#20854;&#20182;&#21407;&#23376;&#65292;&#22914;&#20391;&#38142;&#21407;&#23376;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#27531;&#22522;&#21644;&#21407;&#23376;&#27700;&#24179;&#19978;&#23545;&#34507;&#30333;&#36136;&#36827;&#34892;&#24314;&#27169;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;&#20391;&#38142;&#21407;&#23376;&#23545;&#20110;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#20998;&#23376;&#23545;&#25509;&#65289;&#20063;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#39044;&#35757;&#32451;&#20013;&#22825;&#30495;&#22320;&#32452;&#21512;&#27531;&#22522;&#21644;&#21407;&#23376;&#20449;&#24687;&#36890;&#24120;&#20250;&#22833;&#36133;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20449;&#24687;&#27844;&#28431;&#26159;&#21253;&#21547;&#21407;&#23376;&#32467;&#26500;&#30340;&#36755;&#20837;&#23548;&#33268;&#27531;&#22522;&#32423;&#39044;&#35757;&#32451;&#20219;&#21153;&#21464;&#24471;&#29712;&#30862;&#24182;&#23548;&#33268;&#27531;&#22522;&#34920;&#31034;&#19981;&#22815;&#20805;&#20998;&#30340;&#19968;&#20010;&#20851;&#38190;&#21407;&#22240;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#24230;&#25513;&#30721;&#39044;&#35757;&#32451;&#31574;&#30053;&#30340;&#19977;&#32500;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been a surge in the development of 3D structure-based pre-trained protein models, representing a significant advancement over pre-trained protein language models in various downstream tasks. However, most existing structure-based pre-trained models primarily focus on the residue level, i.e., alpha carbon atoms, while ignoring other atoms like side chain atoms. We argue that modeling proteins at both residue and atom levels is important since the side chain atoms can also be crucial for numerous downstream tasks, for example, molecular docking. Nevertheless, we find that naively combining residue and atom information during pre-training typically fails. We identify a key reason is the information leakage caused by the inclusion of atom structure in the input, which renders residue-level pre-training tasks trivial and results in insufficiently expressive residue representations. To address this issue, we introduce a span mask pre-training strategy on 3D protein
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#20174;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#38750;&#21487;&#20998;&#35299;&#24615;&#33021;&#24230;&#37327;&#30340;&#22810;&#31867;&#23398;&#20064;&#31639;&#27861;&#12290;&#36825;&#20123;&#31639;&#27861;&#20998;&#21035;&#36866;&#29992;&#20110;&#21333;&#35843;&#20984;&#24615;&#21644;&#32447;&#24615;&#27604;&#29575;&#20004;&#31867;&#24615;&#33021;&#24230;&#37327;&#65292;&#24182;&#22522;&#20110;&#31867;&#26465;&#20214;&#22122;&#22768;&#27169;&#22411;&#36827;&#34892;&#22122;&#22768;&#26657;&#27491;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01055</link><description>&lt;p&gt;
&#20174;&#26377;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#38750;&#21487;&#20998;&#35299;&#24615;&#33021;&#24230;&#37327;&#30340;&#22810;&#31867;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multiclass Learning from Noisy Labels for Non-decomposable Performance Measures
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#20174;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#38750;&#21487;&#20998;&#35299;&#24615;&#33021;&#24230;&#37327;&#30340;&#22810;&#31867;&#23398;&#20064;&#31639;&#27861;&#12290;&#36825;&#20123;&#31639;&#27861;&#20998;&#21035;&#36866;&#29992;&#20110;&#21333;&#35843;&#20984;&#24615;&#21644;&#32447;&#24615;&#27604;&#29575;&#20004;&#31867;&#24615;&#33021;&#24230;&#37327;&#65292;&#24182;&#22522;&#20110;&#31867;&#26465;&#20214;&#22122;&#22768;&#27169;&#22411;&#36827;&#34892;&#22122;&#22768;&#26657;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23398;&#20064;&#20174;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#25968;&#25454;&#20013;&#24471;&#21040;&#33391;&#22909;&#20998;&#31867;&#22120;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#20851;&#20110;&#20174;&#26377;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#30340;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#26631;&#20934;&#30340;&#22522;&#20110;&#25439;&#22833;&#30340;&#24615;&#33021;&#24230;&#37327;&#19978;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#38656;&#35201;&#20351;&#29992;&#38750;&#21487;&#20998;&#35299;&#24615;&#33021;&#24230;&#37327;&#65292;&#36825;&#20123;&#24230;&#37327;&#19981;&#33021;&#34920;&#31034;&#20026;&#21333;&#20010;&#31034;&#20363;&#19978;&#30340;&#25439;&#22833;&#30340;&#26399;&#26395;&#25110;&#24635;&#21644;&#65307;&#20854;&#20013;&#21253;&#25324;&#31867;&#19981;&#24179;&#34913;&#35774;&#32622;&#20013;&#30340;H-mean&#65292;Q-mean&#21644;G-mean&#65292;&#20197;&#21450;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;Micro F1&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#20004;&#31867;&#24191;&#27867;&#30340;&#22810;&#31867;&#38750;&#21487;&#20998;&#35299;&#24615;&#33021;&#24230;&#37327;&#65292;&#21363;&#21333;&#35843;&#20984;&#24615;&#21644;&#32447;&#24615;&#27604;&#29575;&#65292;&#23427;&#20204;&#21253;&#25324;&#19978;&#36848;&#25152;&#26377;&#31034;&#20363;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22522;&#20110;Narasimhan&#31561;&#20154;&#30340;Frank-Wolfe&#21644;Bisection&#31639;&#27861;(2015)&#12290;&#22312;&#36825;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#22312;&#24191;&#27867;&#30740;&#31350;&#30340;&#31867;&#26465;&#20214;&#22122;&#22768;&#27169;&#22411;&#23478;&#26063;&#19979;&#24320;&#21457;&#20102;&#31639;&#27861;&#30340;&#22122;&#22768;&#26657;&#27491;&#29256;&#26412;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#36951;&#25022;(&#36229;&#39069;&#39118;&#38505;)&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been much interest in recent years in learning good classifiers from data with noisy labels. Most work on learning from noisy labels has focused on standard loss-based performance measures. However, many machine learning problems require using non-decomposable performance measures which cannot be expressed as the expectation or sum of a loss on individual examples; these include for example the H-mean, Q-mean and G-mean in class imbalance settings, and the Micro $F_1$ in information retrieval. In this paper, we design algorithms to learn from noisy labels for two broad classes of multiclass non-decomposable performance measures, namely, monotonic convex and ratio-of-linear, which encompass all the above examples. Our work builds on the Frank-Wolfe and Bisection based methods of Narasimhan et al. (2015). In both cases, we develop noise-corrected versions of the algorithms under the widely studied family of class-conditional noise models. We provide regret (excess risk) bounds 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diffusion Meets DAgger (DMD)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;eye-in-hand&#27169;&#20223;&#23398;&#20064;&#65292;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21019;&#24314;&#26032;&#26679;&#26412;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#34920;&#29616;&#65292;&#24182;&#20943;&#23569;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.17768</link><description>&lt;p&gt;
&#25193;&#25955;&#36935;&#35265;DAgger: &#36229;&#32423;&#30524;&#22312;&#25163;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Diffusion Meets DAgger: Supercharging Eye-in-hand Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17768
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diffusion Meets DAgger (DMD)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;eye-in-hand&#27169;&#20223;&#23398;&#20064;&#65292;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21019;&#24314;&#26032;&#26679;&#26412;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#34920;&#29616;&#65292;&#24182;&#20943;&#23569;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;Diffusion Meets DAgger (DMD)&#65292;&#29992;&#20110;eye-in-hand&#27169;&#20223;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#21019;&#24314;&#26032;&#26679;&#26412;&#65292;&#20351;&#24471;&#23398;&#20064;&#31574;&#30053;&#22312;&#36935;&#21040;&#26410;&#20986;&#29616;&#22312;&#19987;&#23478;&#28436;&#31034;&#20013;&#30340;&#29366;&#24577;&#26102;&#20855;&#26377;&#40065;&#26834;&#24615;&#34920;&#29616;&#65292;&#20943;&#23569;&#20102;&#25968;&#25454;&#37319;&#38598;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17768v1 Announce Type: cross  Abstract: A common failure mode for policies trained with imitation is compounding execution errors at test time. When the learned policy encounters states that were not present in the expert demonstrations, the policy fails, leading to degenerate behavior. The Dataset Aggregation, or DAgger approach to this problem simply collects more data to cover these failure states. However, in practice, this is often prohibitively expensive. In this work, we propose Diffusion Meets DAgger (DMD), a method to reap the benefits of DAgger without the cost for eye-in-hand imitation learning problems. Instead of collecting new samples to cover out-of-distribution states, DMD uses recent advances in diffusion models to create these samples with diffusion models. This leads to robust performance from few demonstrations. In experiments conducted for non-prehensile pushing on a Franka Research 3, we show that DMD can achieve a success rate of 80% with as few as 8 e
&lt;/p&gt;</description></item><item><title>&#23454;&#29616;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#20351;&#21830;&#21697;&#31227;&#21160;&#25805;&#20316;&#22120;&#25104;&#21151;&#22312;&#20197;&#21069;&#26410;&#35265;&#30340;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#25171;&#24320;&#27249;&#26588;&#21644;&#25277;&#23625;&#65292;&#24863;&#30693;&#35823;&#24046;&#26159;&#20027;&#35201;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.17767</link><description>&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20351;&#29992;&#21830;&#21697;&#31227;&#21160;&#25805;&#20316;&#22120;&#25171;&#24320;&#27249;&#26588;&#21644;&#25277;&#23625;
&lt;/p&gt;
&lt;p&gt;
Opening Cabinets and Drawers in the Real World using a Commodity Mobile Manipulator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17767
&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#20351;&#21830;&#21697;&#31227;&#21160;&#25805;&#20316;&#22120;&#25104;&#21151;&#22312;&#20197;&#21069;&#26410;&#35265;&#30340;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#25171;&#24320;&#27249;&#26588;&#21644;&#25277;&#23625;&#65292;&#24863;&#30693;&#35823;&#24046;&#26159;&#20027;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#20351;&#21830;&#21697;&#31227;&#21160;&#25805;&#20316;&#22120;&#65288;Stretch RE2&#65289;&#33021;&#22815;&#22312;&#22810;&#26679;&#30340;&#20197;&#21069;&#26410;&#35265;&#30340;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#25289;&#24320;&#27249;&#26588;&#21644;&#25277;&#23625;&#12290;&#25105;&#20204;&#22312;31&#20010;&#19981;&#21516;&#30340;&#29289;&#20307;&#21644;13&#20010;&#19981;&#21516;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;4&#22825;&#30340;&#23454;&#38469;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#38646;&#20987;&#25171;&#19979;&#65292;&#23545;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#26032;&#39062;&#30340;&#27249;&#26588;&#21644;&#25277;&#23625;&#30340;&#25171;&#24320;&#29575;&#36798;&#21040;61%&#12290;&#23545;&#22833;&#36133;&#27169;&#24335;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#24863;&#30693;&#35823;&#24046;&#26159;&#25105;&#20204;&#31995;&#32479;&#38754;&#20020;&#30340;&#26368;&#37325;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17767v1 Announce Type: cross  Abstract: Pulling open cabinets and drawers presents many difficult technical challenges in perception (inferring articulation parameters for objects from onboard sensors), planning (producing motion plans that conform to tight task constraints), and control (making and maintaining contact while applying forces on the environment). In this work, we build an end-to-end system that enables a commodity mobile manipulator (Stretch RE2) to pull open cabinets and drawers in diverse previously unseen real world environments. We conduct 4 days of real world testing of this system spanning 31 different objects from across 13 different real world environments. Our system achieves a success rate of 61% on opening novel cabinets and drawers in unseen environments zero-shot. An analysis of the failure modes suggests that errors in perception are the most significant challenge for our system. We will open source code and models for others to replicate and bui
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;1&#27604;&#29305;LLM&#21464;&#20307;&#65292;&#36890;&#36807;&#24341;&#20837;&#19977;&#36827;&#21046;&#21442;&#25968;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#25104;&#26412;&#25928;&#30410;&#65292;&#23450;&#20041;&#20102;&#26032;&#30340;&#35757;&#32451;&#35268;&#24459;&#65292;&#20026;&#35774;&#35745;&#19987;&#38376;&#30828;&#20214;&#20248;&#21270;&#30340;1&#27604;&#29305;LLMs&#25171;&#24320;&#20102;&#22823;&#38376;</title><link>https://arxiv.org/abs/2402.17764</link><description>&lt;p&gt;
1&#27604;&#29305;LLM&#30340;&#26102;&#20195;&#65306;&#25152;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22343;&#20026;1.58&#27604;&#29305;
&lt;/p&gt;
&lt;p&gt;
The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17764
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;1&#27604;&#29305;LLM&#21464;&#20307;&#65292;&#36890;&#36807;&#24341;&#20837;&#19977;&#36827;&#21046;&#21442;&#25968;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#25104;&#26412;&#25928;&#30410;&#65292;&#23450;&#20041;&#20102;&#26032;&#30340;&#35757;&#32451;&#35268;&#24459;&#65292;&#20026;&#35774;&#35745;&#19987;&#38376;&#30828;&#20214;&#20248;&#21270;&#30340;1&#27604;&#29305;LLMs&#25171;&#24320;&#20102;&#22823;&#38376;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#65292;&#22914;BitNet&#65292;&#27491;&#22312;&#20026;&#19968;&#20010;&#26032;&#26102;&#20195;&#30340;1&#27604;&#29305;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38138;&#24179;&#36947;&#36335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;1&#27604;&#29305;LLM&#21464;&#20307;&#65292;&#21363;BitNet b1.58&#65292;&#20854;&#20013;LLM&#30340;&#27599;&#20010;&#21333;&#20010;&#21442;&#25968;&#65288;&#25110;&#26435;&#37325;&#65289;&#22343;&#20026;&#19977;&#36827;&#21046;{-1, 0, 1}&#12290;&#23427;&#22312;&#22256;&#24785;&#24230;&#21644;&#26368;&#32456;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#19982;&#30456;&#21516;&#27169;&#22411;&#22823;&#23567;&#21644;&#35757;&#32451;&#26631;&#35760;&#30340;&#20840;&#31934;&#24230;&#65288;&#21363;FP16&#25110;BF16&#65289;Transformer LLM&#30456;&#21305;&#37197;&#65292;&#21516;&#26102;&#22312;&#24310;&#36831;&#12289;&#20869;&#23384;&#12289;&#21534;&#21520;&#37327;&#21644;&#33021;&#32791;&#26041;&#38754;&#26174;&#30528;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;1.58&#27604;&#29305;&#30340;LLM&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#30340;&#32553;&#25918;&#23450;&#24459;&#21644;&#35757;&#32451;&#26032;&#19968;&#20195;&#26082;&#39640;&#24615;&#33021;&#21448;&#20855;&#25104;&#26412;&#25928;&#30410;&#30340;LLMs&#30340;&#37197;&#26041;&#12290;&#27492;&#22806;&#65292;&#23427;&#23454;&#29616;&#20102;&#19968;&#20010;&#26032;&#30340;&#35745;&#31639;&#33539;&#24335;&#65292;&#24182;&#20026;&#35774;&#35745;&#19987;&#38376;&#38024;&#23545;1&#27604;&#29305;LLMs&#20248;&#21270;&#30340;&#29305;&#23450;&#30828;&#20214;&#25950;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17764v1 Announce Type: new  Abstract: Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption. More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective. Furthermore, it enables a new computation paradigm and opens the door for designing specific hardware optimized for 1-bit LLMs.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20986;&#29616;&#20102;&#22823;&#37327;&#28608;&#27963;&#29616;&#35937;&#65292;&#23427;&#20204;&#20855;&#26377;&#38750;&#24120;&#22823;&#30340;&#20540;&#24182;&#19988;&#22312;&#27169;&#22411;&#20013;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.17762</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22823;&#37327;&#28608;&#27963;
&lt;/p&gt;
&lt;p&gt;
Massive Activations in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17762
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20986;&#29616;&#20102;&#22823;&#37327;&#28608;&#27963;&#29616;&#35937;&#65292;&#23427;&#20204;&#20855;&#26377;&#38750;&#24120;&#22823;&#30340;&#20540;&#24182;&#19988;&#22312;&#27169;&#22411;&#20013;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35266;&#23519;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#19968;&#20010;&#32463;&#39564;&#29616;&#35937;&#8212;&#8212;&#24456;&#23569;&#30340;&#28608;&#27963;&#23637;&#29616;&#20986;&#27604;&#20854;&#20182;&#28608;&#27963;&#26126;&#26174;&#26356;&#22823;&#30340;&#20540;&#65288;&#20363;&#22914;&#65292;&#22823;&#20986; 100,000 &#20493;&#65289;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#22823;&#37327;&#28608;&#27963;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#37327;&#28608;&#27963;&#22312;&#21508;&#31181;LLMs&#20013;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#24182;&#23545;&#20854;&#20301;&#32622;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#30340;&#20540;&#22522;&#26412;&#19978;&#19981;&#21463;&#36755;&#20837;&#24433;&#21709;&#65292;&#24182;&#19988;&#22312;LLMs&#20013;&#36215;&#21040;&#19981;&#21487;&#25110;&#32570;&#30340;&#20559;&#32622;&#39033;&#20316;&#29992;&#12290;&#31532;&#19977;&#65292;&#36825;&#20123;&#22823;&#37327;&#28608;&#27963;&#23548;&#33268;&#20851;&#27880;&#27010;&#29575;&#38598;&#20013;&#20110;&#20854;&#23545;&#24212;&#30340;&#26631;&#35760;&#65292;&#24182;&#36827;&#19968;&#27493;&#25104;&#20026;&#33258;&#27880;&#24847;&#36755;&#20986;&#20013;&#30340;&#38544;&#24335;&#20559;&#32622;&#39033;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#35270;&#35273;Transformer&#20013;&#30340;&#22823;&#37327;&#28608;&#27963;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17762v1 Announce Type: new  Abstract: We observe an empirical phenomenon in Large Language Models (LLMs) -- very few activations exhibit significantly larger values than others (e.g., 100,000 times larger). We call them massive activations. First, we demonstrate the widespread existence of massive activations across various LLMs and characterize their locations. Second, we find their values largely stay constant regardless of the input, and they function as indispensable bias terms in LLMs. Third, these massive activations lead to the concentration of attention probabilities to their corresponding tokens, and further, implicit bias terms in the self-attention output. Last, we also study massive activations in Vision Transformers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#37327;&#23376;&#24555;&#36895;&#26435;&#37325;&#32534;&#31243;&#22120;&#65288;QFWP&#65289;&#20316;&#20026;&#35299;&#20915;&#37327;&#23376;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;QRNNs&#65289;&#27169;&#22411;&#35757;&#32451;&#26102;&#38388;&#24310;&#38271;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.17760</link><description>&lt;p&gt;
&#23398;&#20064;&#20351;&#29992;&#24555;&#36895;&#26435;&#37325;&#32534;&#31243;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Learning to Program Variational Quantum Circuits with Fast Weights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#37327;&#23376;&#24555;&#36895;&#26435;&#37325;&#32534;&#31243;&#22120;&#65288;QFWP&#65289;&#20316;&#20026;&#35299;&#20915;&#37327;&#23376;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;QRNNs&#65289;&#27169;&#22411;&#35757;&#32451;&#26102;&#38388;&#24310;&#38271;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Quantum Machine Learning (QML)&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#20027;&#35201;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#39034;&#24207;&#25511;&#21046;&#20219;&#21153;&#21644;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#12290; &#29305;&#21035;&#26159;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21644;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#31561;&#39046;&#22495;&#65292;&#24050;&#32463;&#23637;&#31034;&#20102;&#32463;&#39564;&#37327;&#23376;&#20248;&#21183;&#12290; &#37327;&#23376;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;QRNNs&#65289;&#26159;&#19968;&#20010;&#37325;&#22823;&#36827;&#23637;&#65292;&#19987;&#38376;&#20026;&#23384;&#20648;&#23494;&#38598;&#22411;&#20219;&#21153;&#35774;&#35745;&#65292;&#21253;&#25324;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#21644;&#38750;&#32447;&#24615;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290; &#28982;&#32780;&#65292;&#22522;&#20110;QRNN&#30340;&#27169;&#22411;&#38754;&#20020;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#30001;&#20110;&#38656;&#35201;&#20351;&#29992;&#36890;&#36807;&#26102;&#38388;&#21453;&#21521;&#20256;&#25773;&#65288;BPTT&#65289;&#35745;&#31639;&#37327;&#23376;&#26799;&#24230;&#32780;&#20135;&#29983;&#30340;&#35757;&#32451;&#26102;&#38388;&#24310;&#38271;&#30340;&#38382;&#39064;&#12290; &#24403;&#22312;&#37327;&#23376;&#35774;&#22791;&#19978;&#25191;&#34892;&#23436;&#25972;&#27169;&#22411;&#26102;&#65292;&#30001;&#20110;&#21442;&#25968;&#31227;&#20301;&#35268;&#21017;&#24102;&#26469;&#30340;&#30005;&#36335;&#35780;&#20272;&#38656;&#27714;&#24040;&#22823;&#65292;&#36825;&#20010;&#22256;&#22659;&#36827;&#19968;&#27493;&#21152;&#21095;&#12290; &#26412;&#25991;&#23558;&#37327;&#23376;&#24555;&#36895;&#26435;&#37325;&#31243;&#24207;&#21592;&#65288;QFWP&#65289;&#24341;&#20837;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17760v1 Announce Type: cross  Abstract: Quantum Machine Learning (QML) has surfaced as a pioneering framework addressing sequential control tasks and time-series modeling. It has demonstrated empirical quantum advantages notably within domains such as Reinforcement Learning (RL) and time-series prediction. A significant advancement lies in Quantum Recurrent Neural Networks (QRNNs), specifically tailored for memory-intensive tasks encompassing partially observable environments and non-linear time-series prediction. Nevertheless, QRNN-based models encounter challenges, notably prolonged training duration stemming from the necessity to compute quantum gradients using backpropagation-through-time (BPTT). This predicament exacerbates when executing the complete model on quantum devices, primarily due to the substantial demand for circuit evaluation arising from the parameter-shift rule. This paper introduces the Quantum Fast Weight Programmers (QFWP) as a solution to the temporal
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#23545;&#40784;&#38160;&#24230;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#21333;&#25351;&#25968;&#27169;&#22411;&#30340;&#24120;&#25968;&#36817;&#20284;&#23398;&#20064;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#20998;&#24067;&#21644;&#36830;&#25509;&#20989;&#25968;&#65292;&#26159;&#39318;&#20010;&#36866;&#29992;&#20110;&#39640;&#26031;&#25968;&#25454;&#21644;&#20219;&#20309;&#38750;&#24179;&#20961;&#36830;&#25509;&#20989;&#25968;&#31867;&#30340;&#31283;&#20581;&#23398;&#20064;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.17756</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#40784;&#38160;&#24230;&#31283;&#20581;&#23398;&#20064;&#21333;&#25351;&#25968;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Robustly Learning Single-Index Models via Alignment Sharpness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17756
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#23545;&#40784;&#38160;&#24230;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#21333;&#25351;&#25968;&#27169;&#22411;&#30340;&#24120;&#25968;&#36817;&#20284;&#23398;&#20064;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#20998;&#24067;&#21644;&#36830;&#25509;&#20989;&#25968;&#65292;&#26159;&#39318;&#20010;&#36866;&#29992;&#20110;&#39640;&#26031;&#25968;&#25454;&#21644;&#20219;&#20309;&#38750;&#24179;&#20961;&#36830;&#25509;&#20989;&#25968;&#31867;&#30340;&#31283;&#20581;&#23398;&#20064;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#23545;&#40784;&#27169;&#22411;&#19979;&#20197;$L_2^2$&#25439;&#22833;&#23398;&#20064;&#21333;&#25351;&#25968;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#22312;&#23545;&#40784;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#26368;&#20248;&#25439;&#22833;&#30340;&#24120;&#25968;&#36817;&#20284;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#20998;&#24067;&#65288;&#21253;&#25324;&#23545;&#25968;&#20985;&#20998;&#24067;&#65289;&#21644;&#24191;&#27867;&#30340;&#21333;&#35843;&#21644;Lipschitz&#36830;&#25509;&#20989;&#25968;&#30340;&#31867;&#12290;&#36825;&#26159;&#39318;&#20010;&#39640;&#25928;&#30340;&#24120;&#25968;&#36817;&#20284;&#23545;&#40784;&#23398;&#20064;&#22120;&#65292;&#21363;&#20351;&#23545;&#20110;&#39640;&#26031;&#25968;&#25454;&#21644;&#20219;&#20309;&#38750;&#24179;&#20961;&#30340;&#36830;&#25509;&#20989;&#25968;&#31867;&#12290;&#20197;&#21069;&#38024;&#23545;&#26410;&#30693;&#36830;&#25509;&#20989;&#25968;&#24773;&#20917;&#30340;&#24037;&#20316;&#35201;&#20040;&#36866;&#29992;&#20110;&#21487;&#23454;&#29616;&#35774;&#32622;&#65292;&#35201;&#20040;&#26080;&#27861;&#36798;&#21040;&#24120;&#25968;&#36817;&#20284;&#12290;&#20351;&#25105;&#20204;&#30340;&#31639;&#27861;&#21644;&#20998;&#26512;&#25104;&#20026;&#21487;&#33021;&#30340;&#20027;&#35201;&#25216;&#26415;&#35201;&#32032;&#26159;&#25105;&#20204;&#31216;&#20043;&#20026;&#23545;&#40784;&#38160;&#24230;&#30340;&#26032;&#39062;&#20248;&#21270;&#23616;&#37096;&#35823;&#24046;&#30028;&#30340;&#27010;&#24565;&#65292;&#36825;&#21487;&#33021;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17756v1 Announce Type: new  Abstract: We study the problem of learning Single-Index Models under the $L_2^2$ loss in the agnostic model. We give an efficient learning algorithm, achieving a constant factor approximation to the optimal loss, that succeeds under a range of distributions (including log-concave distributions) and a broad class of monotone and Lipschitz link functions. This is the first efficient constant factor approximate agnostic learner, even for Gaussian data and for any nontrivial class of link functions. Prior work for the case of unknown link function either works in the realizable setting or does not attain constant factor approximation. The main technical ingredient enabling our algorithm and analysis is a novel notion of a local error bound in optimization that we term alignment sharpness and that may be of broader interest.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#26426;&#22120;-&#20154;&#27969;&#31243;&#65292;&#22522;&#20110;LLM&#20195;&#29702;&#26550;&#26500;&#24182;&#23558;&#20854;&#23545;&#35805;&#22522;&#20110;&#20154;&#29289;&#35282;&#33394;&#21644;&#26102;&#38388;&#20107;&#20214;&#22270;&#36827;&#34892;&#22522;&#30784;&#65292;&#25104;&#21151;&#21019;&#24314;&#20102;LoCoMo&#25968;&#25454;&#38598;&#65292;&#20026;&#38750;&#24120;&#38271;&#26399;&#23545;&#35805;&#30340;&#30740;&#31350;&#22635;&#34917;&#20102;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2402.17753</link><description>&lt;p&gt;
&#35780;&#20272;LLM&#20195;&#29702;&#30340;&#38750;&#24120;&#38271;&#26399;&#23545;&#35805;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Evaluating Very Long-Term Conversational Memory of LLM Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17753
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#26426;&#22120;-&#20154;&#27969;&#31243;&#65292;&#22522;&#20110;LLM&#20195;&#29702;&#26550;&#26500;&#24182;&#23558;&#20854;&#23545;&#35805;&#22522;&#20110;&#20154;&#29289;&#35282;&#33394;&#21644;&#26102;&#38388;&#20107;&#20214;&#22270;&#36827;&#34892;&#22522;&#30784;&#65292;&#25104;&#21151;&#21019;&#24314;&#20102;LoCoMo&#25968;&#25454;&#38598;&#65292;&#20026;&#38750;&#24120;&#38271;&#26399;&#23545;&#35805;&#30340;&#30740;&#31350;&#22635;&#34917;&#20102;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#26041;&#38754;&#30340;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35780;&#20272;&#27169;&#22411;&#21709;&#24212;&#65292;&#20854;&#19978;&#19979;&#25991;&#36328;&#24230;&#19981;&#36229;&#36807;&#20116;&#20010;&#32842;&#22825;&#20250;&#35805;&#12290;&#23613;&#31649;&#38271;&#19978;&#19979;&#25991;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#26377;&#25152;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#22312;&#38750;&#24120;&#38271;&#26399;&#23545;&#35805;&#20013;&#30340;&#26377;&#25928;&#24615;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26426;&#22120;-&#20154;&#30340;&#27969;&#31243;&#65292;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#26550;&#26500;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#38750;&#24120;&#38271;&#26399;&#23545;&#35805;&#65292;&#24182;&#23558;&#20854;&#23545;&#35805;&#22522;&#20110;&#20154;&#29289;&#35282;&#33394;&#21644;&#26102;&#38388;&#20107;&#20214;&#22270;&#36827;&#34892;&#22522;&#30784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36171;&#20104;&#27599;&#20010;&#20195;&#29702;&#20998;&#20139;&#21644;&#23545;&#22270;&#20687;&#20570;&#20986;&#21453;&#24212;&#30340;&#33021;&#21147;&#12290;&#29983;&#25104;&#30340;&#23545;&#35805;&#32463;&#20154;&#31867;&#27880;&#37322;&#21592;&#39564;&#35777;&#21644;&#32534;&#36753;&#65292;&#20197;&#30830;&#20445;&#38271;&#26399;&#19968;&#33268;&#24615;&#21644;&#19982;&#20107;&#20214;&#22270;&#30340;&#22522;&#30784;&#30456;&#32852;&#31995;&#12290;&#20351;&#29992;&#27492;&#27969;&#31243;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;LoCoMo&#65292;&#19968;&#20010;&#38750;&#24120;&#38271;&#26399;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#21253;&#21547;300&#36718;&#21644;&#24179;&#22343;9K&#20196;&#29260;&#65292;&#26368;&#22810;&#36798;&#21040;35&#20010;&#20250;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17753v1 Announce Type: cross  Abstract: Existing works on long-term open-domain dialogues focus on evaluating model responses within contexts spanning no more than five chat sessions. Despite advancements in long-context large language models (LLMs) and retrieval augmented generation (RAG) techniques, their efficacy in very long-term dialogues remains unexplored. To address this research gap, we introduce a machine-human pipeline to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs. Moreover, we equip each agent with the capability of sharing and reacting to images. The generated conversations are verified and edited by human annotators for long-range consistency and grounding to the event graphs. Using this pipeline, we collect LoCoMo, a dataset of very long-term conversations, each encompassing 300 turns and 9K tokens on avg., over up to 35 sessions. Based on LoCoM
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#24182;&#28436;&#31034;&#20102;&#19968;&#31181;&#21033;&#29992;&#20219;&#24847;&#21487;&#32534;&#31243;&#27874;&#20256;&#25773;&#30340;2D&#27874;&#23548;&#22120;&#20214;&#65292;&#36890;&#36807;&#32452;&#21512;&#20809;&#30005;&#23548;&#22686;&#30410;&#21644;&#30005;&#20809;&#25928;&#24212;&#23454;&#29616;&#23545;&#26495;&#22359;&#30340;&#25240;&#23556;&#29575;&#36827;&#34892;&#22823;&#35268;&#27169;&#24182;&#34892;&#35843;&#21046;</title><link>https://arxiv.org/abs/2402.17750</link><description>&lt;p&gt;
&#21033;&#29992;&#20219;&#24847;&#21487;&#32534;&#31243;&#27874;&#20256;&#25773;&#26469;&#25193;&#23637;&#33455;&#29255;&#20809;&#23376;&#31070;&#32463;&#22788;&#29702;&#22120;&#30340;&#35268;&#27169;
&lt;/p&gt;
&lt;p&gt;
Scaling on-chip photonic neural processors using arbitrarily programmable wave propagation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17750
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#24182;&#28436;&#31034;&#20102;&#19968;&#31181;&#21033;&#29992;&#20219;&#24847;&#21487;&#32534;&#31243;&#27874;&#20256;&#25773;&#30340;2D&#27874;&#23548;&#22120;&#20214;&#65292;&#36890;&#36807;&#32452;&#21512;&#20809;&#30005;&#23548;&#22686;&#30410;&#21644;&#30005;&#20809;&#25928;&#24212;&#23454;&#29616;&#23545;&#26495;&#22359;&#30340;&#25240;&#23556;&#29575;&#36827;&#34892;&#22823;&#35268;&#27169;&#24182;&#34892;&#35843;&#21046;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#33455;&#29255;&#20809;&#23376;&#22788;&#29702;&#22120;&#22312;&#36895;&#24230;&#21644;&#33021;&#37327;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#20294;&#23578;&#26410;&#36798;&#21040;&#33021;&#22815;&#32988;&#36807;&#30005;&#23376;&#22788;&#29702;&#22120;&#30340;&#35268;&#27169;&#12290;&#35774;&#35745;&#33455;&#29255;&#20809;&#23376;&#23398;&#30340;&#20027;&#23548;&#33539;&#24335;&#26159;&#21046;&#20316;&#30001;&#30456;&#23545;&#31528;&#37325;&#30340;&#31163;&#25955;&#20803;&#20214;&#26500;&#25104;&#30340;&#32593;&#32476;&#65292;&#36825;&#20123;&#20803;&#20214;&#36890;&#36807;&#19968;&#32500;&#27874;&#23548;&#36830;&#25509;&#12290;&#19968;&#20010;&#26356;&#32039;&#20945;&#30340;&#26367;&#20195;&#26041;&#26696;&#26159;&#36991;&#20813;&#26126;&#30830;&#23450;&#20041;&#20219;&#20309;&#20803;&#20214;&#65292;&#32780;&#26159;&#36890;&#36807;&#22312;&#20004;&#20010;&#32500;&#24230;&#20013;&#33258;&#30001;&#20256;&#25773;&#30340;&#27874;&#30452;&#25509;&#22609;&#36896;&#20809;&#23376;&#22788;&#29702;&#22120;&#30340;&#36830;&#32493;&#34924;&#24213;&#26469;&#25191;&#34892;&#35745;&#31639;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#23637;&#31034;&#20102;&#19968;&#31181;&#21487;&#20197;&#24555;&#36895;&#37325;&#26032;&#32534;&#31243;&#31354;&#38388;&#25240;&#23556;&#29575;$n(x,z)$&#30340;&#35774;&#22791;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#35774;&#22791;&#20013;&#27874;&#20256;&#25773;&#30340;&#20219;&#24847;&#25511;&#21046;&#12290;&#25105;&#20204;&#30340;&#35774;&#22791;&#65292;&#19968;&#32500;&#21487;&#32534;&#31243;&#27874;&#23548;&#65292;&#23558;&#20809;&#30005;&#23548;&#22686;&#30410;&#19982;&#30005;&#20809;&#25928;&#24212;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#26495;&#22359;&#30340;&#25240;&#23556;&#29575;&#30340;&#24182;&#34892;&#35843;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17750v1 Announce Type: cross  Abstract: On-chip photonic processors for neural networks have potential benefits in both speed and energy efficiency but have not yet reached the scale at which they can outperform electronic processors. The dominant paradigm for designing on-chip photonics is to make networks of relatively bulky discrete components connected by one-dimensional waveguides. A far more compact alternative is to avoid explicitly defining any components and instead sculpt the continuous substrate of the photonic processor to directly perform the computation using waves freely propagating in two dimensions. We propose and demonstrate a device whose refractive index as a function of space, $n(x,z)$, can be rapidly reprogrammed, allowing arbitrary control over the wave propagation in the device. Our device, a 2D-programmable waveguide, combines photoconductive gain with the electro-optic effect to achieve massively parallel modulation of the refractive index of a slab
&lt;/p&gt;</description></item><item><title>RLHF&#22312;&#32771;&#34385;&#37096;&#20998;&#35266;&#23519;&#24615;&#26102;&#21487;&#33021;&#23548;&#33268;&#31574;&#30053;&#27450;&#39575;&#24615;&#22320;&#22840;&#22823;&#24615;&#33021;&#25110;&#36807;&#24230;&#36777;&#25252;&#34892;&#20026;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#23398;&#26465;&#20214;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#35686;&#21578;&#19981;&#35201;&#30450;&#30446;&#24212;&#29992;RLHF&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#24773;&#20917;&#19979;&#12290;</title><link>https://arxiv.org/abs/2402.17747</link><description>&lt;p&gt;
&#24403;&#20320;&#30340;AI&#27450;&#39575;&#20320;&#65306;&#22312;&#22870;&#21169;&#23398;&#20064;&#20013;&#20154;&#31867;&#35780;&#20272;&#32773;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
When Your AI Deceives You: Challenges with Partial Observability of Human Evaluators in Reward Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17747
&lt;/p&gt;
&lt;p&gt;
RLHF&#22312;&#32771;&#34385;&#37096;&#20998;&#35266;&#23519;&#24615;&#26102;&#21487;&#33021;&#23548;&#33268;&#31574;&#30053;&#27450;&#39575;&#24615;&#22320;&#22840;&#22823;&#24615;&#33021;&#25110;&#36807;&#24230;&#36777;&#25252;&#34892;&#20026;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#23398;&#26465;&#20214;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#35686;&#21578;&#19981;&#35201;&#30450;&#30446;&#24212;&#29992;RLHF&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#30340;&#36807;&#21435;&#20998;&#26512;&#20551;&#35774;&#20154;&#31867;&#23436;&#20840;&#35266;&#23519;&#21040;&#29615;&#22659;&#12290;&#24403;&#20154;&#31867;&#21453;&#39304;&#20165;&#22522;&#20110;&#37096;&#20998;&#35266;&#23519;&#26102;&#20250;&#21457;&#29983;&#20160;&#20040;&#65311;&#25105;&#20204;&#23545;&#20004;&#31181;&#22833;&#36133;&#24773;&#20917;&#36827;&#34892;&#20102;&#27491;&#24335;&#23450;&#20041;&#65306;&#27450;&#39575;&#21644;&#36807;&#24230;&#36777;&#25252;&#12290;&#36890;&#36807;&#23558;&#20154;&#31867;&#24314;&#27169;&#20026;&#23545;&#36712;&#36857;&#20449;&#24565;&#30340;Boltzmann-&#29702;&#24615;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;RLHF&#20445;&#35777;&#20250;&#23548;&#33268;&#31574;&#30053;&#27450;&#39575;&#24615;&#22320;&#22840;&#22823;&#20854;&#24615;&#33021;&#12289;&#20026;&#20102;&#30041;&#19979;&#21360;&#35937;&#32780;&#36807;&#24230;&#36777;&#25252;&#25110;&#32773;&#20004;&#32773;&#20860;&#32780;&#26377;&#20043;&#30340;&#26465;&#20214;&#12290;&#20026;&#20102;&#24110;&#21161;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25968;&#23398;&#22320;&#21051;&#30011;&#20102;&#29615;&#22659;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#22914;&#20309;&#36716;&#21270;&#20026;&#65288;&#32570;&#20047;&#65289;&#23398;&#21040;&#30340;&#22238;&#25253;&#20989;&#25968;&#20013;&#30340;&#27169;&#31946;&#24615;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#32771;&#34385;&#29615;&#22659;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#20351;&#24471;&#22312;&#29702;&#35770;&#19978;&#21487;&#33021;&#24674;&#22797;&#22238;&#25253;&#20989;&#25968;&#21644;&#26368;&#20248;&#31574;&#30053;&#65292;&#32780;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#65292;&#23384;&#22312;&#19981;&#21487;&#20943;&#23569;&#30340;&#27169;&#31946;&#24615;&#12290;&#25105;&#20204;&#35686;&#21578;&#19981;&#35201;&#30450;&#30446;&#24212;&#29992;RLHF&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17747v1 Announce Type: cross  Abstract: Past analyses of reinforcement learning from human feedback (RLHF) assume that the human fully observes the environment. What happens when human feedback is based only on partial observations? We formally define two failure cases: deception and overjustification. Modeling the human as Boltzmann-rational w.r.t. a belief over trajectories, we prove conditions under which RLHF is guaranteed to result in policies that deceptively inflate their performance, overjustify their behavior to make an impression, or both. To help address these issues, we mathematically characterize how partial observability of the environment translates into (lack of) ambiguity in the learned return function. In some cases, accounting for partial observability makes it theoretically possible to recover the return function and thus the optimal policy, while in other cases, there is irreducible ambiguity. We caution against blindly applying RLHF in partially observa
&lt;/p&gt;</description></item><item><title>reBandit&#26159;&#19968;&#31181;&#22312;&#32447;RL&#31639;&#27861;&#65292;&#21033;&#29992;&#38543;&#26426;&#25928;&#24212;&#21644;&#36125;&#21494;&#26031;&#20808;&#39564;&#24555;&#36895;&#39640;&#25928;&#22320;&#23398;&#20064;&#65292;&#22312;&#31227;&#21160;&#20581;&#24247;&#29615;&#22659;&#20013;&#36890;&#36807;&#20010;&#24615;&#21270;&#24178;&#39044;&#26469;&#20943;&#23569;&#26032;&#20852;&#25104;&#24180;&#20154;&#30340;&#22823;&#40635;&#20351;&#29992;</title><link>https://arxiv.org/abs/2402.17739</link><description>&lt;p&gt;
reBandit&#65306;&#22522;&#20110;&#38543;&#26426;&#25928;&#24212;&#30340;&#22312;&#32447;RL&#31639;&#27861;&#29992;&#20110;&#20943;&#23569;&#22823;&#40635;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
reBandit: Random Effects based Online RL algorithm for Reducing Cannabis Use
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17739
&lt;/p&gt;
&lt;p&gt;
reBandit&#26159;&#19968;&#31181;&#22312;&#32447;RL&#31639;&#27861;&#65292;&#21033;&#29992;&#38543;&#26426;&#25928;&#24212;&#21644;&#36125;&#21494;&#26031;&#20808;&#39564;&#24555;&#36895;&#39640;&#25928;&#22320;&#23398;&#20064;&#65292;&#22312;&#31227;&#21160;&#20581;&#24247;&#29615;&#22659;&#20013;&#36890;&#36807;&#20010;&#24615;&#21270;&#24178;&#39044;&#26469;&#20943;&#23569;&#26032;&#20852;&#25104;&#24180;&#20154;&#30340;&#22823;&#40635;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#40635;&#20351;&#29992;&#21450;&#30456;&#20851;&#30340;&#22823;&#40635;&#20351;&#29992;&#38556;&#30861;&#65288;CUD&#65289;&#30340;&#19981;&#26029;&#22686;&#21152;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#22823;&#30340;&#20844;&#20849;&#21355;&#29983;&#25361;&#25112;&#12290;&#23588;&#20854;&#26159;&#22312;&#26032;&#20852;&#25104;&#24180;&#20154;&#65288;18-25&#23681;&#65289;&#20013;&#65292;&#23384;&#22312;&#26126;&#26174;&#30340;&#27835;&#30103;&#32570;&#21475;&#65292;&#22240;&#27492;&#35299;&#20915;&#22823;&#40635;&#20351;&#29992;&#21644;CUD&#20173;&#28982;&#26159;2030&#24180;&#32852;&#21512;&#22269;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#65288;SDG&#65289;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#30446;&#26631;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;reBandit&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#31227;&#21160;&#20581;&#24247;&#30740;&#31350;&#20013;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#20010;&#24615;&#21270;&#31227;&#21160;&#20581;&#24247;&#24178;&#39044;&#26469;&#20943;&#23569;&#26032;&#20852;&#25104;&#24180;&#20154;&#30340;&#22823;&#40635;&#20351;&#29992;&#12290;reBandit&#21033;&#29992;&#38543;&#26426;&#25928;&#24212;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#36125;&#21494;&#26031;&#20808;&#39564;&#20197;&#22312;&#22024;&#26434;&#30340;&#31227;&#21160;&#20581;&#24247;&#29615;&#22659;&#20013;&#24555;&#36895;&#32780;&#26377;&#25928;&#22320;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;reBandit&#37319;&#29992;&#32463;&#39564;&#36125;&#21494;&#26031;&#21644;&#20248;&#21270;&#25216;&#26415;&#26469;&#22312;&#32447;&#33258;&#20027;&#26356;&#26032;&#20854;&#36229;&#21442;&#25968;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#21033;&#29992;&#25968;&#25454;&#26500;&#24314;&#20102;&#19968;&#20010;&#27169;&#25311;&#27979;&#35797;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17739v1 Announce Type: new  Abstract: The escalating prevalence of cannabis use, and associated cannabis-use disorder (CUD), poses a significant public health challenge globally. With a notably wide treatment gap, especially among emerging adults (EAs; ages 18-25), addressing cannabis use and CUD remains a pivotal objective within the 2030 United Nations Agenda for Sustainable Development Goals (SDG). In this work, we develop an online reinforcement learning (RL) algorithm called reBandit which will be utilized in a mobile health study to deliver personalized mobile health interventions aimed at reducing cannabis use among EAs. reBandit utilizes random effects and informative Bayesian priors to learn quickly and efficiently in noisy mobile health environments. Moreover, reBandit employs Empirical Bayes and optimization techniques to autonomously update its hyper-parameters online. To evaluate the performance of our algorithm, we construct a simulation testbed using data from
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38024;&#23545;&#26410;&#30693;&#22270;&#30340;&#22270;&#25628;&#32034;&#38382;&#39064;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#39318;&#27425;&#22312;&#26410;&#30693;&#21152;&#26435;&#22270;&#19978;&#24314;&#31435;&#20102;&#24418;&#24335;&#20445;&#35777;&#65292;&#24182;&#35774;&#35745;&#31639;&#27861;&#22312;&#39044;&#27979;&#35823;&#24046;&#19978;&#20855;&#26377;&#26368;&#20248;&#25110;&#20960;&#20046;&#26368;&#20339;&#20381;&#23384;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.17736</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#22270;&#25628;&#32034;&#38382;&#39064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning-Based Algorithms for Graph Searching Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38024;&#23545;&#26410;&#30693;&#22270;&#30340;&#22270;&#25628;&#32034;&#38382;&#39064;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#39318;&#27425;&#22312;&#26410;&#30693;&#21152;&#26435;&#22270;&#19978;&#24314;&#31435;&#20102;&#24418;&#24335;&#20445;&#35777;&#65292;&#24182;&#35774;&#35745;&#31639;&#27861;&#22312;&#39044;&#27979;&#35823;&#24046;&#19978;&#20855;&#26377;&#26368;&#20248;&#25110;&#20960;&#20046;&#26368;&#20339;&#20381;&#23384;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;Banerjee&#31561;&#20154;&#65288;2022&#24180;&#65289;&#26368;&#36817;&#25552;&#20986;&#30340;&#20855;&#26377;&#39044;&#27979;&#30340;&#22270;&#25628;&#32034;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#19968;&#20010;&#20174;&#26576;&#20010;&#39030;&#28857;$r$&#20986;&#21457;&#30340;&#20195;&#29702;&#32773;&#24517;&#39035;&#22312;&#26368;&#23567;&#21270;&#24635;&#34892;&#31243;&#30340;&#21516;&#26102;&#36941;&#21382;&#19968;&#20010;&#65288;&#28508;&#22312;&#26410;&#30693;&#30340;&#65289;&#22270;$G$&#20197;&#25214;&#21040;&#38544;&#34255;&#30340;&#30446;&#26631;&#33410;&#28857;$g$&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#35774;&#32622;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#65292;&#22312;&#20219;&#24847;&#33410;&#28857;$v$&#22788;&#65292;&#20195;&#29702;&#32773;&#20250;&#25509;&#25910;&#21040;&#21040;$g$&#30340;&#36317;&#31163;&#30340;&#22122;&#22768;&#20272;&#35745;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#38024;&#23545;&#26410;&#30693;&#22270;&#30340;&#36825;&#31181;&#25628;&#32034;&#20219;&#21153;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#26410;&#30693;&#21152;&#26435;&#22270;&#19978;&#24314;&#31435;&#20102;&#31532;&#19968;&#27425;&#24418;&#24335;&#20445;&#35777;&#65292;&#24182;&#25552;&#20379;&#20102;&#26174;&#31034;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#39044;&#27979;&#35823;&#24046;&#19978;&#20855;&#26377;&#26368;&#20248;&#25110;&#20960;&#20046;&#26368;&#20339;&#20381;&#23384;&#20851;&#31995;&#30340;&#19979;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#35777;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#38500;&#20102;&#23545;&#25239;&#24615;&#35823;&#24046;&#20855;&#26377;&#40065;&#26834;&#24615;&#22806;&#65292;&#36824;&#22312;&#35823;&#24046;&#26159;&#38543;&#26426;&#30340;&#20856;&#22411;&#23454;&#20363;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;Banerjee&#31561;&#20154;&#31639;&#27861;&#30340;&#26367;&#20195;&#31616;&#21270;&#24615;&#33021;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17736v1 Announce Type: cross  Abstract: We consider the problem of graph searching with prediction recently introduced by Banerjee et al. (2022). In this problem, an agent, starting at some vertex $r$ has to traverse a (potentially unknown) graph $G$ to find a hidden goal node $g$ while minimizing the total distance travelled. We study a setting in which at any node $v$, the agent receives a noisy estimate of the distance from $v$ to $g$. We design algorithms for this search task on unknown graphs. We establish the first formal guarantees on unknown weighted graphs and provide lower bounds showing that the algorithms we propose have optimal or nearly-optimal dependence on the prediction error. Further, we perform numerical experiments demonstrating that in addition to being robust to adversarial error, our algorithms perform well in typical instances in which the error is stochastic. Finally, we provide alternative simpler performance bounds on the algorithms of Banerjee et 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25209;&#22788;&#29702;&#32422;&#26463;&#19979;&#30340;&#38750;&#21442;&#25968;&#19978;&#19979;&#25991;&#33218;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BaSEDB&#30340;&#26041;&#26696;&#65292;&#22312;&#21160;&#24577;&#20998;&#21106;&#21327;&#21464;&#37327;&#31354;&#38388;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#26368;&#20248;&#30340;&#21518;&#24724;&#12290;</title><link>https://arxiv.org/abs/2402.17732</link><description>&lt;p&gt;
&#25209;&#22788;&#29702;&#38750;&#21442;&#25968;&#19978;&#19979;&#25991;&#33218;
&lt;/p&gt;
&lt;p&gt;
Batched Nonparametric Contextual Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17732
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25209;&#22788;&#29702;&#32422;&#26463;&#19979;&#30340;&#38750;&#21442;&#25968;&#19978;&#19979;&#25991;&#33218;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BaSEDB&#30340;&#26041;&#26696;&#65292;&#22312;&#21160;&#24577;&#20998;&#21106;&#21327;&#21464;&#37327;&#31354;&#38388;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#26368;&#20248;&#30340;&#21518;&#24724;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#25209;&#22788;&#29702;&#32422;&#26463;&#19979;&#30340;&#38750;&#21442;&#25968;&#19978;&#19979;&#25991;&#33218;&#38382;&#39064;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#27599;&#20010;&#21160;&#20316;&#30340;&#26399;&#26395;&#22870;&#21169;&#34987;&#24314;&#27169;&#20026;&#21327;&#21464;&#37327;&#30340;&#24179;&#28369;&#20989;&#25968;&#65292;&#24182;&#19988;&#31574;&#30053;&#26356;&#26032;&#26159;&#22312;&#27599;&#20010;Observations&#25209;&#27425;&#32467;&#26463;&#26102;&#36827;&#34892;&#30340;&#12290;&#25105;&#20204;&#20026;&#36825;&#31181;&#35774;&#32622;&#24314;&#31435;&#20102;&#19968;&#20010;&#26368;&#23567;&#21270;&#21518;&#24724;&#30340;&#19979;&#38480;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Batched Successive Elimination with Dynamic Binning&#65288;BaSEDB&#65289;&#30340;&#26041;&#26696;&#65292;&#21487;&#20197;&#23454;&#29616;&#26368;&#20248;&#30340;&#21518;&#24724;&#65288;&#36798;&#21040;&#23545;&#25968;&#22240;&#23376;&#65289;&#12290;&#23454;&#36136;&#19978;&#65292;BaSEDB&#21160;&#24577;&#22320;&#23558;&#21327;&#21464;&#37327;&#31354;&#38388;&#20998;&#21106;&#25104;&#26356;&#23567;&#30340;&#31665;&#23376;&#65292;&#24182;&#20180;&#32454;&#35843;&#25972;&#23427;&#20204;&#30340;&#23485;&#24230;&#20197;&#31526;&#21512;&#25209;&#27425;&#22823;&#23567;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#25209;&#22788;&#29702;&#32422;&#26463;&#19979;&#38745;&#24577;&#20998;&#31665;&#30340;&#38750;&#26368;&#20248;&#24615;&#65292;&#31361;&#20986;&#20102;&#21160;&#24577;&#20998;&#31665;&#30340;&#24517;&#35201;&#24615;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23436;&#20840;&#22312;&#32447;&#35774;&#32622;&#20013;&#65292;&#20960;&#20046;&#24658;&#23450;&#25968;&#37327;&#30340;&#31574;&#30053;&#26356;&#26032;&#21487;&#20197;&#36798;&#21040;&#26368;&#20339;&#21518;&#24724;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17732v1 Announce Type: cross  Abstract: We study nonparametric contextual bandits under batch constraints, where the expected reward for each action is modeled as a smooth function of covariates, and the policy updates are made at the end of each batch of observations. We establish a minimax regret lower bound for this setting and propose Batched Successive Elimination with Dynamic Binning (BaSEDB) that achieves optimal regret (up to logarithmic factors). In essence, BaSEDB dynamically splits the covariate space into smaller bins, carefully aligning their widths with the batch size. We also show the suboptimality of static binning under batch constraints, highlighting the necessity of dynamic binning. Additionally, our results suggest that a nearly constant number of policy updates can attain optimal regret in the fully online setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#26694;&#26550;&#26469;&#25506;&#32034;CTMCs&#65292;&#24378;&#35843;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#38271;&#24230;&#21644;&#28151;&#21512;&#21442;&#25968;&#23545;&#38382;&#39064;&#24773;&#22659;&#30340;&#24433;&#21709;&#65292;&#36825;&#38656;&#35201;&#29305;&#23450;&#30340;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.17730</link><description>&lt;p&gt;
&#39532;&#23572;&#31185;&#22827;&#21147;&#23398;&#65306;&#23398;&#20064;&#36830;&#32493;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#38142;&#28151;&#21512;&#30340;&#26041;&#27861;&#21644;&#26032;&#39062;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Markovletics: Methods and A Novel Application for Learning Continuous-Time Markov Chain Mixtures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#26694;&#26550;&#26469;&#25506;&#32034;CTMCs&#65292;&#24378;&#35843;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#38271;&#24230;&#21644;&#28151;&#21512;&#21442;&#25968;&#23545;&#38382;&#39064;&#24773;&#22659;&#30340;&#24433;&#21709;&#65292;&#36825;&#38656;&#35201;&#29305;&#23450;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#25968;&#25454;&#33258;&#28982;&#20135;&#29983;&#20110;&#25968;&#23383;&#24179;&#21488;&#19978;&#29992;&#25143;&#30340;&#21442;&#19982;&#65292;&#27604;&#22914;&#31038;&#20132;&#23186;&#20307;&#12289;&#38899;&#20048;&#27969;&#23186;&#20307;&#26381;&#21153;&#21644;&#32593;&#39029;&#23548;&#33322;&#65292;&#36890;&#36807;&#36830;&#32493;&#20449;&#24687;&#27969;&#20307;&#29616;&#20102;&#29992;&#25143;&#20559;&#22909;&#21644;&#34892;&#20026;&#30340;&#28436;&#21270;&#12290;&#22312;&#38543;&#26426;&#36807;&#31243;&#20013;&#19968;&#20010;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#26159;&#23398;&#20064;&#36830;&#32493;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#38142;&#28151;&#21512;&#65288;CTMCs&#65289;&#12290;&#34429;&#28982;&#22312;&#23398;&#20064;&#20855;&#26377;&#24674;&#22797;&#20445;&#35777;&#30340;&#31163;&#25955;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#38142;&#28151;&#21512;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#36830;&#32493;&#24773;&#26223;&#25581;&#31034;&#20102;&#29420;&#29305;&#23578;&#26410;&#25506;&#32034;&#30340;&#25361;&#25112;&#12290;CTMC&#28151;&#21512;&#30340;&#21560;&#24341;&#21147;&#22312;&#20110;&#23427;&#20204;&#26377;&#28508;&#21147;&#23545;&#21508;&#20010;&#39046;&#22495;&#26222;&#36941;&#23384;&#22312;&#30340;&#22797;&#26434;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#36807;&#31243;&#36827;&#34892;&#24314;&#27169;&#65292;&#21253;&#25324;&#31038;&#20132;&#23186;&#20307;&#12289;&#37329;&#34701;&#21644;&#29983;&#29289;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17730v1 Announce Type: new  Abstract: Sequential data naturally arises from user engagement on digital platforms like social media, music streaming services, and web navigation, encapsulating evolving user preferences and behaviors through continuous information streams. A notable unresolved query in stochastic processes is learning mixtures of continuous-time Markov chains (CTMCs). While there is progress in learning mixtures of discrete-time Markov chains with recovery guarantees [GKV16,ST23,KTT2023], the continuous scenario uncovers unique unexplored challenges. The intrigue in CTMC mixtures stems from their potential to model intricate continuous-time stochastic processes prevalent in various fields including social media, finance, and biology.   In this study, we introduce a novel framework for exploring CTMCs, emphasizing the influence of observed trails' length and mixture parameters on problem regimes, which demands specific algorithms. Through thorough experimentati
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#23545;&#19968;&#33324;Bregman&#25955;&#24230;&#25903;&#25345;&#30340;&#38750;&#20984;SMD&#26032;&#25910;&#25947;&#20998;&#26512;&#65292;&#20811;&#26381;&#20102;&#20808;&#21069;&#30340;&#38480;&#21046;&#65292;&#24182;&#22312;&#20840;&#23616;&#25910;&#25947;&#24615;&#21644;&#39640;&#27010;&#29575;&#25910;&#25947;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.17722</link><description>&lt;p&gt;
&#25511;&#21046;&#38750;&#20984;&#38543;&#26426;&#38236;&#20687;&#19979;&#38477;&#19982;&#19968;&#33324;Bregman&#25955;&#24230;
&lt;/p&gt;
&lt;p&gt;
Taming Nonconvex Stochastic Mirror Descent with General Bregman Divergence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17722
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#23545;&#19968;&#33324;Bregman&#25955;&#24230;&#25903;&#25345;&#30340;&#38750;&#20984;SMD&#26032;&#25910;&#25947;&#20998;&#26512;&#65292;&#20811;&#26381;&#20102;&#20808;&#21069;&#30340;&#38480;&#21046;&#65292;&#24182;&#22312;&#20840;&#23616;&#25910;&#25947;&#24615;&#21644;&#39640;&#27010;&#29575;&#25910;&#25947;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#25506;&#35752;&#20102;&#22312;&#24403;&#20195;&#38750;&#20984;&#20248;&#21270;&#35774;&#32622;&#20013;&#38543;&#26426;&#38236;&#20687;&#19979;&#38477;&#65288;SMD&#65289;&#30340;&#25910;&#25947;&#24615;&#12290;&#29616;&#26377;&#20851;&#20110;&#26080;&#25209;&#22788;&#29702;&#38750;&#20984;SMD&#30340;&#32467;&#26524;&#38480;&#21046;&#20102;&#36873;&#25321;&#19981;&#21516;iable&#19981;&#21487;&#38388;&#26029;&#26799;&#24230;&#30340;&#36317;&#31163;&#29983;&#25104;&#20989;&#25968;&#65288;DGF&#65289;&#65292;&#20174;&#32780;&#25490;&#38500;&#20102;&#37325;&#35201;&#35774;&#32622;&#65292;&#22914;Shannon&#29109;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25903;&#25345;&#19968;&#33324;DGF&#30340;&#38750;&#20984;SMD&#30340;&#26032;&#25910;&#25947;&#20998;&#26512;&#65292;&#20811;&#26381;&#20102;&#19978;&#36848;&#38480;&#21046;&#65292;&#20165;&#20381;&#36182;&#20110;&#26631;&#20934;&#20551;&#35774;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#25910;&#25947;&#24615;&#26159;&#38024;&#23545;Bregman&#21069;&#21521;-&#21518;&#21521;&#21253;&#32476;&#24314;&#31435;&#30340;&#65292;&#36825;&#27604;&#24120;&#29992;&#30340;&#26799;&#24230;&#26144;&#23556;&#30340;&#24179;&#26041;&#33539;&#25968;&#26356;&#24378;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#25193;&#23637;&#21040;&#22312;&#27425;&#39640;&#26031;&#22122;&#22768;&#19979;&#20445;&#35777;&#39640;&#27010;&#29575;&#25910;&#25947;&#65292;&#24182;&#22312;&#24191;&#20041;Bregman Proximal Polyak-{\L}ojasiewicz&#26465;&#20214;&#19979;&#20445;&#35777;&#20840;&#23616;&#25910;&#25947;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17722v1 Announce Type: cross  Abstract: This paper revisits the convergence of Stochastic Mirror Descent (SMD) in the contemporary nonconvex optimization setting. Existing results for batch-free nonconvex SMD restrict the choice of the distance generating function (DGF) to be differentiable with Lipschitz continuous gradients, thereby excluding important setups such as Shannon entropy. In this work, we present a new convergence analysis of nonconvex SMD supporting general DGF, that overcomes the above limitations and relies solely on the standard assumptions. Moreover, our convergence is established with respect to the Bregman Forward-Backward envelope, which is a stronger measure than the commonly used squared norm of gradient mapping. We further extend our results to guarantee high probability convergence under sub-Gaussian noise and global convergence under the generalized Bregman Proximal Polyak-{\L}ojasiewicz condition. Additionally, we illustrate the advantages of our 
&lt;/p&gt;</description></item><item><title>SMART&#31639;&#27861;&#22312;&#23454;&#29616;&#23454;&#20363;&#26368;&#20248;&#22312;&#32447;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#31361;&#30772;&#65292;&#20855;&#26377;&#27604;&#20256;&#32479;&#8220;&#20004;&#20840;&#20854;&#32654;&#8221;&#30028;&#38480;&#26356;&#24378;&#22823;&#30340;&#20445;&#35777;&#65292;&#33021;&#22815;&#22312;&#27599;&#20010;&#36755;&#20837;&#24207;&#21015;&#19978;&#23454;&#29616;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.17720</link><description>&lt;p&gt;
SMART&#31639;&#27861;&#22312;&#23454;&#20363;&#26368;&#20248;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The SMART approach to instance-optimal online learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17720
&lt;/p&gt;
&lt;p&gt;
SMART&#31639;&#27861;&#22312;&#23454;&#29616;&#23454;&#20363;&#26368;&#20248;&#22312;&#32447;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#31361;&#30772;&#65292;&#20855;&#26377;&#27604;&#20256;&#32479;&#8220;&#20004;&#20840;&#20854;&#32654;&#8221;&#30028;&#38480;&#26356;&#24378;&#22823;&#30340;&#20445;&#35777;&#65292;&#33021;&#22815;&#22312;&#27599;&#20010;&#36755;&#20837;&#24207;&#21015;&#19978;&#23454;&#29616;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861; - Switching via Monotone Adapted Regret Traces (SMART) - &#35813;&#31639;&#27861;&#33021;&#22815;&#36866;&#24212;&#25968;&#25454;&#24182;&#23454;&#29616;&#23454;&#20363;&#26368;&#20248;&#30340;&#36951;&#25022;&#65292;&#21363;&#30456;&#23545;&#20110;&#38543;&#26426;&#36319;&#38543;&#32773;&#65288;FTL&#65289;&#31574;&#30053;&#30340;&#24615;&#33021;&#20197;&#21450;&#20219;&#20309;&#20854;&#20182;&#36755;&#20837;&#31574;&#30053;&#30340;&#26368;&#22351;&#24773;&#20917;&#20445;&#35777;&#32780;&#35328;&#65292;&#22312;&#27599;&#20010;&#36755;&#20837;&#24207;&#21015;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;SMART&#31574;&#30053;&#23545;&#20219;&#20309;&#36755;&#20837;&#24207;&#21015;&#30340;&#36951;&#25022;&#37117;&#22312;&#19968;&#20010;&#20493;&#20056;&#22240;&#23376;$e/(e-1) \approx 1.58$&#20043;&#20869;&#65292;&#36825;&#20010;&#20493;&#20056;&#22240;&#23376;&#26159;FTL&#31574;&#30053;&#22312;&#35813;&#24207;&#21015;&#19978;&#33719;&#24471;&#30340;&#36951;&#25022;&#21644;&#30001;&#32473;&#23450;&#30340;&#26368;&#22351;&#24773;&#20917;&#31574;&#30053;&#20445;&#35777;&#30340;&#36951;&#25022;&#19978;&#38480;&#20013;&#36739;&#23567;&#20540;&#30340;&#20493;&#25968;&#12290;&#36825;&#24847;&#21619;&#30528;&#25105;&#20204;&#25552;&#20379;&#30340;&#20445;&#35777;&#27604;&#20856;&#22411;&#30340;&#8220;&#20004;&#20840;&#20854;&#32654;&#8221;&#30340;&#30028;&#38480;&#35201;&#24378;&#22823;&#65292;&#22240;&#20026;&#35813;&#20445;&#35777;&#36866;&#29992;&#20110;&#27599;&#20010;&#36755;&#20837;&#24207;&#21015;&#65292;&#26080;&#35770;&#23427;&#26159;&#22914;&#20309;&#29983;&#25104;&#30340;&#12290;SMART&#31639;&#27861;&#26131;&#20110;&#23454;&#26045;&#65292;&#22240;&#20026;&#23427;&#20174;FTL&#24320;&#22987;&#65292;&#24182;&#19988;&#22312;&#26102;&#38388;&#33539;&#22260;&#20869;&#33267;&#22810;&#20999;&#25442;&#19968;&#27425;&#21040;&#26368;&#22351;&#24773;&#20917;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17720v1 Announce Type: new  Abstract: We devise an online learning algorithm -- titled Switching via Monotone Adapted Regret Traces (SMART) -- that adapts to the data and achieves regret that is instance optimal, i.e., simultaneously competitive on every input sequence compared to the performance of the follow-the-leader (FTL) policy and the worst case guarantee of any other input policy. We show that the regret of the SMART policy on any input sequence is within a multiplicative factor $e/(e-1) \approx 1.58$ of the smaller of: 1) the regret obtained by FTL on the sequence, and 2) the upper bound on regret guaranteed by the given worst-case policy. This implies a strictly stronger guarantee than typical `best-of-both-worlds' bounds as the guarantee holds for every input sequence regardless of how it is generated. SMART is simple to implement as it begins by playing FTL and switches at most once during the time horizon to the worst-case algorithm. Our approach and results fol
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#20110;&#22686;&#26448;&#21046;&#36896;&#30340;&#25968;&#23383;&#23402;&#29983;&#26694;&#26550;&#65292;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#23454;&#29616;&#23454;&#26102;&#39044;&#27979;&#25511;&#21046;&#65292;&#35299;&#20915;DED&#36807;&#31243;&#20013;&#30340;&#28909;&#31649;&#29702;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.17718</link><description>&lt;p&gt;
&#38754;&#21521;&#22686;&#26448;&#21046;&#36896;&#30340;&#25968;&#23383;&#23402;&#29983;&#26694;&#26550;&#65306;&#26426;&#22120;&#23398;&#20064;&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#36807;&#31243;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Towards a Digital Twin Framework in Additive Manufacturing: Machine Learning and Bayesian Optimization for Time Series Process Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17718
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#20110;&#22686;&#26448;&#21046;&#36896;&#30340;&#25968;&#23383;&#23402;&#29983;&#26694;&#26550;&#65292;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#23454;&#29616;&#23454;&#26102;&#39044;&#27979;&#25511;&#21046;&#65292;&#35299;&#20915;DED&#36807;&#31243;&#20013;&#30340;&#28909;&#31649;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Laser-directed-energy deposition (DED)&#25552;&#20379;&#22686;&#26448;&#21046;&#36896;(AM)&#20013;&#21019;&#36896;&#22797;&#26434;&#20960;&#20309;&#21644;&#26448;&#26009;&#20998;&#32423;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#20998;&#23618;&#21046;&#36896;&#65292;&#26448;&#26009;&#30340;&#19981;&#19968;&#33268;&#24615;&#21644;&#38646;&#20214;&#30340;&#21487;&#21464;&#24615;&#31561;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#12290;DED&#36807;&#31243;&#20013;&#28909;&#31215;&#32047;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#24433;&#21709;&#26448;&#26009;&#30340;&#24494;&#35266;&#32467;&#26500;&#21644;&#24615;&#36136;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23383;&#23402;&#29983;(DT)&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#26102;&#39044;&#27979;&#25511;&#21046;DED&#36807;&#31243;&#21442;&#25968;&#65292;&#20197;&#28385;&#36275;&#29305;&#23450;&#30340;&#35774;&#35745;&#30446;&#26631;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;(LSTM)&#30340;&#26426;&#22120;&#23398;&#20064;&#19982;&#36125;&#21494;&#26031;&#25512;&#26029;&#24320;&#21457;&#20102;&#19968;&#20010;&#20195;&#29992;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;DED&#38646;&#20214;&#20013;&#30340;&#28201;&#24230;&#12290;&#36825;&#20010;&#27169;&#22411;&#33021;&#22815;&#23454;&#26102;&#39044;&#27979;&#26410;&#26469;&#30340;&#28201;&#24230;&#29366;&#24577;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#36125;&#21494;&#26031;&#20248;&#21270;(BO)&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17718v1 Announce Type: new  Abstract: Laser-directed-energy deposition (DED) offers advantages in additive manufacturing (AM) for creating intricate geometries and material grading. Yet, challenges like material inconsistency and part variability remain, mainly due to its layer-wise fabrication. A key issue is heat accumulation during DED, which affects the material microstructure and properties. While closed-loop control methods for heat management are common in DED research, few integrate real-time monitoring, physics-based modeling, and control in a unified framework. Our work presents a digital twin (DT) framework for real-time predictive control of DED process parameters to meet specific design objectives. We develop a surrogate model using Long Short-Term Memory (LSTM)-based machine learning with Bayesian Inference to predict temperatures in DED parts. This model predicts future temperature states in real time. We also introduce Bayesian Optimization (BO) for Time Seri
&lt;/p&gt;</description></item><item><title>&#20174;&#20248;&#21270;&#30340;&#35282;&#24230;&#38416;&#26126;&#31070;&#32463;&#32593;&#32476;&#20108;&#20540;&#21270;&#20013;&#30340;&#35757;&#32451;&#25216;&#24039;&#65292;&#25552;&#20986;&#20102;ProxConnect++&#65288;PC++&#65289;&#36825;&#19968;&#27867;&#21270;&#27169;&#22411;&#65292;&#23558;&#29616;&#26377;&#20108;&#20540;&#21270;&#25216;&#26415;&#35270;&#20026;&#20854;&#29305;&#20363;</title><link>https://arxiv.org/abs/2402.17710</link><description>&lt;p&gt;
&#36890;&#36807;&#21069;&#21521;&#21644;&#21518;&#21521;&#36817;&#31471;&#37327;&#21270;&#22120;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#20108;&#20540;&#21270;
&lt;/p&gt;
&lt;p&gt;
Understanding Neural Network Binarization with Forward and Backward Proximal Quantizers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17710
&lt;/p&gt;
&lt;p&gt;
&#20174;&#20248;&#21270;&#30340;&#35282;&#24230;&#38416;&#26126;&#31070;&#32463;&#32593;&#32476;&#20108;&#20540;&#21270;&#20013;&#30340;&#35757;&#32451;&#25216;&#24039;&#65292;&#25552;&#20986;&#20102;ProxConnect++&#65288;PC++&#65289;&#36825;&#19968;&#27867;&#21270;&#27169;&#22411;&#65292;&#23558;&#29616;&#26377;&#20108;&#20540;&#21270;&#25216;&#26415;&#35270;&#20026;&#20854;&#29305;&#20363;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#32593;&#32476;&#20108;&#20540;&#21270;&#20013;&#65292;BinaryConnect&#65288;BC&#65289;&#21450;&#20854;&#21464;&#20307;&#34987;&#35748;&#20026;&#26159;&#26631;&#20934;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#21069;&#21521;&#20256;&#25773;&#20013;&#24212;&#29992;&#31526;&#21495;&#20989;&#25968;&#65292;&#23427;&#20204;&#30340;&#26799;&#24230;&#34987;&#21453;&#21521;&#20256;&#25773;&#26469;&#26356;&#26032;&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;&#24403;&#23450;&#20041;&#26102;&#31526;&#21495;&#20989;&#25968;&#30340;&#23548;&#25968;&#20026;&#38646;&#65292;&#36825;&#20250;&#23548;&#33268;&#35757;&#32451;&#20572;&#28382;&#12290;&#22240;&#27492;&#65292;BC&#30340;&#23454;&#29616;&#65288;&#20363;&#22914;BNN&#65289;&#36890;&#24120;&#20351;&#29992;&#24658;&#31561;&#25110;&#20854;&#20182;&#36817;&#20284;&#26799;&#24230;&#26367;&#20195;&#31526;&#21495;&#20989;&#25968;&#22312;&#21453;&#21521;&#35745;&#31639;&#20013;&#30340;&#23548;&#25968;&#12290;&#34429;&#28982;&#36825;&#31181;&#20570;&#27861;&#22312;&#32463;&#39564;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20027;&#35201;&#26159;&#19968;&#31181;&#21551;&#21457;&#24335;&#25110;&#8220;&#35757;&#32451;&#25216;&#24039;&#8221;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20174;&#20248;&#21270;&#30340;&#35282;&#24230;&#38416;&#26126;&#36825;&#20123;&#35757;&#32451;&#25216;&#24039;&#12290;&#22522;&#20110;&#29616;&#26377;&#30340;ProxConnect&#65288;PC&#65292;BC&#30340;&#19968;&#31181;&#27867;&#21270;&#65289;&#29702;&#35770;&#65292;&#25105;&#20204;&#65288;1&#65289;&#20026;PC&#37197;&#22791;&#20102;&#19981;&#21516;&#30340;&#21069;&#21521;-&#21518;&#21521;&#37327;&#21270;&#22120;&#65292;&#33719;&#24471;&#20102;&#21253;&#25324;&#29616;&#26377;&#20108;&#20540;&#21270;&#25216;&#26415;&#22312;&#20869;&#30340;ProxConnect++&#65288;PC++&#65289;&#29305;&#20363;&#65307;&#65288;2&#65289;&#25512;&#23548;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17710v1 Announce Type: new  Abstract: In neural network binarization, BinaryConnect (BC) and its variants are considered the standard. These methods apply the sign function in their forward pass and their respective gradients are backpropagated to update the weights. However, the derivative of the sign function is zero whenever defined, which consequently freezes training. Therefore, implementations of BC (e.g., BNN) usually replace the derivative of sign in the backward computation with identity or other approximate gradient alternatives. Although such practice works well empirically, it is largely a heuristic or ''training trick.'' We aim at shedding some light on these training tricks from the optimization perspective. Building from existing theory on ProxConnect (PC, a generalization of BC), we (1) equip PC with different forward-backward quantizers and obtain ProxConnect++ (PC++) that includes existing binarization techniques as special cases; (2) derive a principled wa
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#22312;&#26426;&#26500;&#20043;&#38388;&#21327;&#20316;&#23398;&#20064;HTE&#20272;&#35745;&#37327;&#30340;&#26032;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#21363;&#20351;&#22312;&#23458;&#25143;&#20043;&#38388;&#23384;&#22312;&#22810;&#26679;&#24178;&#39044;&#21644;&#21463;&#35797;&#32773;&#32676;&#20307;&#24773;&#20917;&#19979;&#20849;&#21516;&#23398;&#20064;&#29305;&#24449;&#34920;&#31034;&#24182;&#31169;&#19979;&#23398;&#20064;&#29305;&#23450;&#30340;&#39044;&#27979;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.17705</link><description>&lt;p&gt;
&#29992;&#20110;&#20272;&#35745;&#24322;&#36136;&#27835;&#30103;&#25928;&#26524;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning for Estimating Heterogeneous Treatment Effects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17705
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#22312;&#26426;&#26500;&#20043;&#38388;&#21327;&#20316;&#23398;&#20064;HTE&#20272;&#35745;&#37327;&#30340;&#26032;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#21363;&#20351;&#22312;&#23458;&#25143;&#20043;&#38388;&#23384;&#22312;&#22810;&#26679;&#24178;&#39044;&#21644;&#21463;&#35797;&#32773;&#32676;&#20307;&#24773;&#20917;&#19979;&#20849;&#21516;&#23398;&#20064;&#29305;&#24449;&#34920;&#31034;&#24182;&#31169;&#19979;&#23398;&#20064;&#29305;&#23450;&#30340;&#39044;&#27979;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#20110;&#20272;&#35745;&#24322;&#36136;&#27835;&#30103;&#25928;&#26524;&#65288;HTE&#65289;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20419;&#36827;&#20102;&#36328;&#21508;&#31181;&#39046;&#22495;&#65288;&#22914;&#21307;&#30103;&#20445;&#20581;&#12289;&#25919;&#31574;&#21046;&#23450;&#12289;&#25945;&#32946;&#31561;&#65289;&#30340;&#22823;&#35268;&#27169;&#20010;&#24615;&#21270;&#20915;&#31574;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;HTE&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#27599;&#31181;&#22788;&#29702;&#26041;&#27861;&#22823;&#37327;&#25968;&#25454;&#30340;&#35775;&#38382;&#65292;&#32780;&#24178;&#39044;&#30340;&#39640;&#25104;&#26412;&#20351;&#24471;&#20026;&#27599;&#31181;&#24178;&#39044;&#38598;&#20013;&#25910;&#38598;&#36825;&#20040;&#22810;&#25968;&#25454;&#25104;&#20026;&#19968;&#20010;&#33392;&#24040;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38556;&#30861;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#22312;&#26426;&#26500;&#20043;&#38388;&#21327;&#20316;&#23398;&#20064;HTE&#20272;&#35745;&#37327;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#21363;&#20351;&#22312;&#23458;&#25143;&#20043;&#38388;&#23384;&#22312;&#22810;&#26679;&#30340;&#24178;&#39044;&#21644;&#21463;&#35797;&#32773;&#32676;&#20307;&#65292;&#20063;&#21487;&#20197;&#20849;&#21516;&#23398;&#20064;&#19968;&#20010;&#20849;&#21516;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#21516;&#26102;&#22312;&#21508;&#20010;&#26426;&#26500;&#20043;&#38388;&#21516;&#26102;&#24182;&#31169;&#19979;&#23398;&#20064;&#20851;&#20110;&#19981;&#21516;&#24178;&#39044;&#24773;&#20917;&#19979;&#32467;&#26524;&#30340;&#29305;&#23450;&#39044;&#27979;&#21151;&#33021;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21644;&#30456;&#20851;&#31639;&#27861;&#22522;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17705v1 Announce Type: new  Abstract: Machine learning methods for estimating heterogeneous treatment effects (HTE) facilitate large-scale personalized decision-making across various domains such as healthcare, policy making, education, and more. Current machine learning approaches for HTE require access to substantial amounts of data per treatment, and the high costs associated with interventions makes centrally collecting so much data for each intervention a formidable challenge. To overcome this obstacle, in this work, we propose a novel framework for collaborative learning of HTE estimators across institutions via Federated Learning. We show that even under a diversity of interventions and subject populations across clients, one can jointly learn a common feature representation, while concurrently and privately learning the specific predictive functions for outcomes under distinct interventions across institutions. Our framework and the associated algorithm are based on 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#36716;&#31227;&#23398;&#20064;&#30340;&#20195;&#29702;&#27169;&#22411;&#19982;&#36125;&#21494;&#26031;&#20248;&#21270;&#30456;&#32467;&#21512;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22312;&#20248;&#21270;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#20449;&#24687;&#26469;&#20943;&#23569;&#23454;&#39564;&#30340;&#24635;&#25968;&#65292;&#24182;&#19988;&#28436;&#31034;&#20102;&#22312;&#35774;&#35745;&#29992;&#20110;&#25193;&#22686;&#22522;&#22240;&#35786;&#26029;&#27979;&#23450;&#30340;DNA&#31454;&#20105;&#23545;&#25163;&#26102;&#23454;&#39564;&#25968;&#37327;&#30340;&#20943;&#23569;&#12290;</title><link>https://arxiv.org/abs/2402.17704</link><description>&lt;p&gt;
&#23558;&#36125;&#21494;&#26031;&#20248;&#21270;&#24212;&#29992;&#20110;&#36716;&#31227;&#23398;&#20064;&#20197;&#35774;&#35745;&#29992;&#20110;&#35786;&#26029;&#27979;&#23450;&#30340;&#31454;&#20105;&#23545;&#25163;DNA&#20998;&#23376;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning Bayesian Optimization to Design Competitor DNA Molecules for Use in Diagnostic Assays
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17704
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#36716;&#31227;&#23398;&#20064;&#30340;&#20195;&#29702;&#27169;&#22411;&#19982;&#36125;&#21494;&#26031;&#20248;&#21270;&#30456;&#32467;&#21512;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22312;&#20248;&#21270;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#20449;&#24687;&#26469;&#20943;&#23569;&#23454;&#39564;&#30340;&#24635;&#25968;&#65292;&#24182;&#19988;&#28436;&#31034;&#20102;&#22312;&#35774;&#35745;&#29992;&#20110;&#25193;&#22686;&#22522;&#22240;&#35786;&#26029;&#27979;&#23450;&#30340;DNA&#31454;&#20105;&#23545;&#25163;&#26102;&#23454;&#39564;&#25968;&#37327;&#30340;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24037;&#31243;&#29983;&#29289;&#20998;&#23376;&#35774;&#22791;&#30340;&#20852;&#36215;&#65292;&#23450;&#21046;&#29983;&#29289;&#24207;&#21015;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#12290;&#36890;&#24120;&#65292;&#20026;&#20102;&#29305;&#23450;&#24212;&#29992;&#38656;&#35201;&#21046;&#20316;&#35768;&#22810;&#31867;&#20284;&#30340;&#29983;&#29289;&#24207;&#21015;&#65292;&#36825;&#24847;&#21619;&#30528;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#29978;&#33267;&#26114;&#36149;&#30340;&#23454;&#39564;&#26469;&#20248;&#21270;&#36825;&#20123;&#24207;&#21015;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36716;&#31227;&#23398;&#20064;&#35774;&#35745;&#23454;&#39564;&#24037;&#20316;&#27969;&#31243;&#65292;&#20351;&#36825;&#31181;&#24320;&#21457;&#21464;&#24471;&#21487;&#34892;&#12290;&#36890;&#36807;&#23558;&#36716;&#31227;&#23398;&#20064;&#20195;&#29702;&#27169;&#22411;&#19982;&#36125;&#21494;&#26031;&#20248;&#21270;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22312;&#20248;&#21270;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#20449;&#24687;&#26469;&#20943;&#23569;&#23454;&#39564;&#30340;&#24635;&#25968;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#20351;&#29992;&#29992;&#20110;&#25193;&#22686;&#22522;&#22240;&#35786;&#26029;&#27979;&#23450;&#20013;&#20351;&#29992;&#30340;DNA&#31454;&#20105;&#23545;&#25163;&#24320;&#21457;&#25968;&#25454;&#26469;&#20943;&#23569;&#23454;&#39564;&#25968;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#20132;&#21449;&#39564;&#35777;&#26469;&#27604;&#36739;&#19981;&#21516;&#36716;&#31227;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#28982;&#21518;&#27604;&#36739;&#36825;&#20123;&#27169;&#22411;&#22312;&#21333;&#19968;&#30446;&#26631;&#21644;&#24809;&#32602;&#20248;&#21270;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17704v1 Announce Type: cross  Abstract: With the rise in engineered biomolecular devices, there is an increased need for tailor-made biological sequences. Often, many similar biological sequences need to be made for a specific application meaning numerous, sometimes prohibitively expensive, lab experiments are necessary for their optimization. This paper presents a transfer learning design of experiments workflow to make this development feasible. By combining a transfer learning surrogate model with Bayesian optimization, we show how the total number of experiments can be reduced by sharing information between optimization tasks. We demonstrate the reduction in the number of experiments using data from the development of DNA competitors for use in an amplification-based diagnostic assay. We use cross-validation to compare the predictive accuracy of different transfer learning models, and then compare the performance of the models for both single objective and penalized opti
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#35889;&#22270;&#26102;&#22495;&#38899;&#39057;&#20998;&#31163;&#32593;&#32476;&#65288;HS-TasNet&#65289;&#65292;&#29992;&#20110;&#23454;&#29616;&#23454;&#26102;&#20302;&#24310;&#36831;&#38899;&#20048;&#28304;&#20998;&#31163;&#65292;&#22312;MusDB&#27979;&#35797;&#38598;&#19978;&#36798;&#21040;&#20102;&#36739;&#39640;&#30340;&#20449;&#21495;-&#22833;&#30495;&#27604;&#12290;</title><link>https://arxiv.org/abs/2402.17701</link><description>&lt;p&gt;
&#20351;&#29992;&#28151;&#21512;&#35889;&#22270;-TasNet&#36827;&#34892;&#23454;&#26102;&#20302;&#24310;&#36831;&#38899;&#20048;&#28304;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Real-time Low-latency Music Source Separation using Hybrid Spectrogram-TasNet
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17701
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#35889;&#22270;&#26102;&#22495;&#38899;&#39057;&#20998;&#31163;&#32593;&#32476;&#65288;HS-TasNet&#65289;&#65292;&#29992;&#20110;&#23454;&#29616;&#23454;&#26102;&#20302;&#24310;&#36831;&#38899;&#20048;&#28304;&#20998;&#31163;&#65292;&#22312;MusDB&#27979;&#35797;&#38598;&#19978;&#36798;&#21040;&#20102;&#36739;&#39640;&#30340;&#20449;&#21495;-&#22833;&#30495;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#38899;&#20048;&#35299;&#28151;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22914;&#20309;&#35843;&#25972;&#36825;&#20123;&#31070;&#32463;&#32593;&#32476;&#20197;&#36866;&#29992;&#20110;&#23454;&#26102;&#20302;&#24310;&#36831;&#24212;&#29992;&#20960;&#20046;&#27809;&#26377;&#24471;&#21040;&#20851;&#27880;&#65292;&#36825;&#23545;&#21161;&#21548;&#22120;&#12289;&#28151;&#38899;&#38899;&#39057;&#27969;&#21644;&#29616;&#22330;&#34920;&#28436;&#21487;&#33021;&#24456;&#26377;&#24110;&#21161;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#24403;&#21069;&#25991;&#29486;&#20013;&#30340;&#35299;&#28151;&#27169;&#22411;&#35843;&#25972;&#20026;&#36825;&#31181;&#29992;&#20363;&#25152;&#28041;&#21450;&#30340;&#21508;&#31181;&#25361;&#25112;&#12290;&#38543;&#21518;&#65292;&#21463;&#28151;&#21512;Demucs&#26550;&#26500;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28151;&#21512;&#35889;&#22270;&#26102;&#22495;&#38899;&#39057;&#20998;&#31163;&#32593;&#32476;HS-TasNet&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#39057;&#35889;&#21644;&#27874;&#24418;&#22495;&#30340;&#20248;&#21183;&#12290;&#22312;23&#27627;&#31186;&#30340;&#24310;&#36831;&#19979;&#65292;HS-TasNet&#22312;MusDB&#27979;&#35797;&#38598;&#19978;&#33719;&#24471;&#20102;4.65&#30340;&#24635;&#20449;&#21495;-&#22833;&#30495;&#27604;&#65288;SDR&#65289;&#65292;&#24182;&#22312;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#19979;&#22686;&#21152;&#21040;5.55&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#20102;&#23454;&#26102;&#20302;&#24310;&#36831;&#38899;&#20048;&#24212;&#29992;&#20013;&#39640;&#25928;&#35299;&#28151;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17701v1 Announce Type: cross  Abstract: There have been significant advances in deep learning for music demixing in recent years. However, there has been little attention given to how these neural networks can be adapted for real-time low-latency applications, which could be helpful for hearing aids, remixing audio streams and live shows. In this paper, we investigate the various challenges involved in adapting current demixing models in the literature for this use case. Subsequently, inspired by the Hybrid Demucs architecture, we propose the Hybrid Spectrogram Time-domain Audio Separation Network HS-TasNet, which utilises the advantages of spectral and waveform domains. For a latency of 23 ms, the HS-TasNet obtains an overall signal-to-distortion ratio (SDR) of 4.65 on the MusDB test set, and increases to 5.55 with additional training data. These results demonstrate the potential of efficient demixing for real-time low-latency music applications.
&lt;/p&gt;</description></item><item><title>RAVEL&#25968;&#25454;&#38598;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;MDAS&#65292;&#35813;&#26041;&#27861;&#22312;&#35299;&#24320;&#35821;&#35328;&#27169;&#22411;&#34920;&#31034;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25104;&#26524;&#65292;&#24378;&#35843;&#20102;&#36328;&#28608;&#27963;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17700</link><description>&lt;p&gt;
RAVEL: &#22312;&#35299;&#24320;&#35821;&#35328;&#27169;&#22411;&#34920;&#31034;&#26041;&#38754;&#35780;&#20272;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17700
&lt;/p&gt;
&lt;p&gt;
RAVEL&#25968;&#25454;&#38598;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;MDAS&#65292;&#35813;&#26041;&#27861;&#22312;&#35299;&#24320;&#35821;&#35328;&#27169;&#22411;&#34920;&#31034;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25104;&#26524;&#65292;&#24378;&#35843;&#20102;&#36328;&#28608;&#27963;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#21035;&#31070;&#32463;&#20803;&#21442;&#19982;&#22810;&#20010;&#39640;&#32423;&#27010;&#24565;&#30340;&#34920;&#31034;&#12290;&#19981;&#21516;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#33021;&#25104;&#21151;&#35299;&#24320;&#36825;&#20123;&#35282;&#33394;&#65311;&#20026;&#20102;&#24110;&#21161;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;RAVEL&#65288;Resolving Attribute-Value Entanglements in Language Models&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#22810;&#31181;&#29616;&#26377;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#36827;&#34892;&#32039;&#23494;&#25511;&#21046;&#30340;&#23450;&#37327;&#27604;&#36739;&#12290;&#25105;&#20204;&#21033;&#29992;&#30001;&#27492;&#20135;&#29983;&#30340;&#27010;&#24565;&#26694;&#26550;&#26469;&#23450;&#20041;&#26032;&#30340;Multi-task Distributed Alignment Search&#65288;MDAS&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25214;&#21040;&#28385;&#36275;&#22810;&#20010;&#22240;&#26524;&#26631;&#20934;&#30340;&#20998;&#24067;&#24335;&#34920;&#31034;&#12290;&#20197;Llama2-7B&#20316;&#20026;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#65292;MDAS&#22312;RAVEL&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25104;&#26524;&#65292;&#23637;&#31034;&#20102;&#36229;&#36234;&#31070;&#32463;&#20803;&#32423;&#21035;&#20998;&#26512;&#20197;&#35782;&#21035;&#36328;&#28608;&#27963;&#30340;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#22312;https://github.com/explanare/ravel&#19978;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17700v1 Announce Type: new  Abstract: Individual neurons participate in the representation of multiple high-level concepts. To what extent can different interpretability methods successfully disentangle these roles? To help address this question, we introduce RAVEL (Resolving Attribute-Value Entanglements in Language Models), a dataset that enables tightly controlled, quantitative comparisons between a variety of existing interpretability methods. We use the resulting conceptual framework to define the new method of Multi-task Distributed Alignment Search (MDAS), which allows us to find distributed representations satisfying multiple causal criteria. With Llama2-7B as the target language model, MDAS achieves state-of-the-art results on RAVEL, demonstrating the importance of going beyond neuron-level analyses to identify features distributed across activations. We release our benchmark at https://github.com/explanare/ravel.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#21160;&#24490;&#29615;&#35843;&#24230;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#31163;&#25955;&#37319;&#26679;&#26041;&#27861;&#65292;&#26377;&#25928;&#24212;&#23545;&#39640;&#24230;&#22810;&#27169;&#24577;&#30340;&#31163;&#25955;&#20998;&#24067;&#65292;&#21253;&#25324;&#24490;&#29615;&#27493;&#38271;&#35843;&#24230;&#12289;&#24490;&#29615;&#24179;&#34913;&#35843;&#24230;&#21644;&#33258;&#21160;&#35843;&#25972;&#36229;&#21442;&#25968;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.17699</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#21160;&#24490;&#29615;&#35843;&#24230;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#31163;&#25955;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Gradient-based Discrete Sampling with Automatic Cyclical Scheduling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17699
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#21160;&#24490;&#29615;&#35843;&#24230;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#31163;&#25955;&#37319;&#26679;&#26041;&#27861;&#65292;&#26377;&#25928;&#24212;&#23545;&#39640;&#24230;&#22810;&#27169;&#24577;&#30340;&#31163;&#25955;&#20998;&#24067;&#65292;&#21253;&#25324;&#24490;&#29615;&#27493;&#38271;&#35843;&#24230;&#12289;&#24490;&#29615;&#24179;&#34913;&#35843;&#24230;&#21644;&#33258;&#21160;&#35843;&#25972;&#36229;&#21442;&#25968;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#25955;&#20998;&#24067;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#32500;&#28145;&#24230;&#27169;&#22411;&#20013;&#65292;&#36890;&#24120;&#30001;&#20110;&#22266;&#26377;&#30340;&#19981;&#36830;&#32493;&#24615;&#32780;&#21576;&#29616;&#39640;&#24230;&#22810;&#27169;&#24577;&#12290;&#34429;&#28982;&#22522;&#20110;&#26799;&#24230;&#30340;&#31163;&#25955;&#37319;&#26679;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#30001;&#20110;&#26799;&#24230;&#20449;&#24687;&#65292;&#23427;&#23481;&#26131;&#38519;&#20837;&#23616;&#37096;&#27169;&#24335;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#24490;&#29615;&#35843;&#24230;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;&#22810;&#27169;&#24577;&#31163;&#25955;&#20998;&#24067;&#36827;&#34892;&#39640;&#25928;&#20934;&#30830;&#30340;&#37319;&#26679;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#37096;&#20998;&#65306;&#65288;1&#65289;&#24490;&#29615;&#27493;&#38271;&#35843;&#24230;&#65292;&#20854;&#20013;&#22823;&#27493;&#38271;&#21457;&#29616;&#26032;&#27169;&#24335;&#65292;&#23567;&#27493;&#38271;&#21033;&#29992;&#27599;&#20010;&#27169;&#24335;&#65307;&#65288;2&#65289;&#24490;&#29615;&#24179;&#34913;&#35843;&#24230;&#65292;&#30830;&#20445;&#32473;&#23450;&#27493;&#38271;&#30340;&#8220;&#24179;&#34913;&#8221;&#25552;&#26696;&#21644;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#39640;&#25928;&#29575;&#65307;&#20197;&#21450;&#65288;3&#65289;&#33258;&#21160;&#35843;&#25972;&#26041;&#26696;&#65292;&#29992;&#20110;&#35843;&#25972;&#24490;&#29615;&#35843;&#24230;&#20013;&#30340;&#36229;&#21442;&#25968;&#65292;&#23454;&#29616;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#33258;&#36866;&#24212;&#24615;&#19988;&#38656;&#26368;&#23567;&#35843;&#25972;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#21644;&#25512;&#26029;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17699v1 Announce Type: new  Abstract: Discrete distributions, particularly in high-dimensional deep models, are often highly multimodal due to inherent discontinuities. While gradient-based discrete sampling has proven effective, it is susceptible to becoming trapped in local modes due to the gradient information. To tackle this challenge, we propose an automatic cyclical scheduling, designed for efficient and accurate sampling in multimodal discrete distributions. Our method contains three key components: (1) a cyclical step size schedule where large steps discover new modes and small steps exploit each mode; (2) a cyclical balancing schedule, ensuring ``balanced" proposals for given step sizes and high efficiency of the Markov chain; and (3) an automatic tuning scheme for adjusting the hyperparameters in the cyclical schedules, allowing adaptability across diverse datasets with minimal tuning. We prove the non-asymptotic convergence and inference guarantee for our method i
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#31639;&#23376;&#25512;&#26029;&#23398;&#20064;&#30340;&#38477;&#38454;&#27169;&#22411;&#22312;&#36807;&#31243;&#24037;&#31243;&#20013;&#24314;&#27169;&#21160;&#24577;&#31995;&#32479;&#65292;&#20026;&#23454;&#29616;&#24555;&#36895;&#21487;&#38752;&#30340;&#25968;&#23383;&#23402;&#29983;&#26550;&#26500;&#36808;&#20986;&#37325;&#35201;&#19968;&#27493;&#12290;</title><link>https://arxiv.org/abs/2402.17698</link><description>&lt;p&gt;
&#22312;&#36807;&#31243;&#24037;&#31243;&#20013;&#20351;&#29992;&#31639;&#23376;&#25512;&#26029;&#23398;&#20064;&#38477;&#38454;&#20108;&#27425;-&#32447;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning reduced-order Quadratic-Linear models in Process Engineering using Operator Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17698
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31639;&#23376;&#25512;&#26029;&#23398;&#20064;&#30340;&#38477;&#38454;&#27169;&#22411;&#22312;&#36807;&#31243;&#24037;&#31243;&#20013;&#24314;&#27169;&#21160;&#24577;&#31995;&#32479;&#65292;&#20026;&#23454;&#29616;&#24555;&#36895;&#21487;&#38752;&#30340;&#25968;&#23383;&#23402;&#29983;&#26550;&#26500;&#36808;&#20986;&#37325;&#35201;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;&#36807;&#31243;&#24037;&#31243;&#20013;&#39640;&#25928;&#24314;&#27169;&#21160;&#24577;&#31995;&#32479;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#20351;&#29992;&#38477;&#38454;&#27169;&#22411;&#23398;&#20064;&#65292;&#20855;&#20307;&#26469;&#35828;&#26159;&#31639;&#23376;&#25512;&#26029;&#12290;&#36825;&#26159;&#19968;&#31181;&#38750;&#20405;&#20837;&#24335;&#12289;&#25968;&#25454;&#39537;&#21160;&#30340;&#20174;&#26102;&#22495;&#25968;&#25454;&#23398;&#20064;&#21160;&#24577;&#31995;&#32479;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24212;&#29992;&#26159;&#20108;&#27687;&#21270;&#30899;&#30002;&#28919;&#21270;&#21453;&#24212;&#65292;&#36825;&#26159;&#30005;&#21147;&#36716;&#21270;&#25216;&#26415;&#26694;&#26550;&#20013;&#30340;&#37325;&#35201;&#21453;&#24212;&#65292;&#20197;&#23637;&#31034;&#20854;&#28508;&#21147;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#29992;&#31639;&#23376;&#25512;&#26029;&#26500;&#24314;&#30340;&#38477;&#38454;&#27169;&#22411;&#33021;&#22815;&#25552;&#20379;&#19968;&#20010;&#31616;&#21270;&#20294;&#20934;&#30830;&#30340;&#26367;&#20195;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#26631;&#24535;&#30528;&#23454;&#29616;&#24555;&#36895;&#21487;&#38752;&#25968;&#23383;&#23402;&#29983;&#26550;&#26500;&#30340;&#37325;&#35201;&#37324;&#31243;&#30865;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17698v1 Announce Type: cross  Abstract: In this work, we address the challenge of efficiently modeling dynamical systems in process engineering. We use reduced-order model learning, specifically operator inference. This is a non-intrusive, data-driven method for learning dynamical systems from time-domain data. The application in our study is carbon dioxide methanation, an important reaction within the Power-to-X framework, to demonstrate its potential. The numerical results show the ability of the reduced-order models constructed with operator inference to provide a reduced yet accurate surrogate solution. This represents an important milestone towards the implementation of fast and reliable digital twin architectures.
&lt;/p&gt;</description></item><item><title>&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#35745;&#31639;&#26426;&#36741;&#21161;&#35774;&#35745;&#39046;&#22495;&#20855;&#26377;&#21464;&#38761;&#24615;&#21147;&#37327;&#65292;&#21487;&#20197;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;CAD&#35774;&#35745;&#24072;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#33410;&#30465;&#26102;&#38388;&#21644;&#31934;&#21147;&#65292;&#25552;&#39640;&#20915;&#31574;&#25928;&#29575;&#65292;&#24182;&#21019;&#36896;&#20986;&#20855;&#26377;&#21019;&#26032;&#24615;&#21644;&#23454;&#29992;&#24615;&#30340;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.17695</link><description>&lt;p&gt;
&#35745;&#31639;&#26426;&#36741;&#21161;&#35774;&#35745;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Geometric Deep Learning for Computer-Aided Design: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17695
&lt;/p&gt;
&lt;p&gt;
&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#35745;&#31639;&#26426;&#36741;&#21161;&#35774;&#35745;&#39046;&#22495;&#20855;&#26377;&#21464;&#38761;&#24615;&#21147;&#37327;&#65292;&#21487;&#20197;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;CAD&#35774;&#35745;&#24072;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#33410;&#30465;&#26102;&#38388;&#21644;&#31934;&#21147;&#65292;&#25552;&#39640;&#20915;&#31574;&#25928;&#29575;&#65292;&#24182;&#21019;&#36896;&#20986;&#20855;&#26377;&#21019;&#26032;&#24615;&#21644;&#23454;&#29992;&#24615;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24050;&#25104;&#20026;&#35745;&#31639;&#26426;&#36741;&#21161;&#35774;&#35745;&#65288;CAD&#65289;&#39046;&#22495;&#30340;&#19968;&#32929;&#21464;&#38761;&#21147;&#37327;&#65292;&#24182;&#26377;&#21487;&#33021;&#24443;&#24213;&#25913;&#21464;&#35774;&#35745;&#24072;&#21644;&#24037;&#31243;&#24072;&#22788;&#29702;&#21644;&#22686;&#24378;&#35774;&#35745;&#36807;&#31243;&#30340;&#26041;&#24335;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;CAD&#35774;&#35745;&#24072;&#21487;&#20197;&#20248;&#21270;&#20182;&#20204;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#33410;&#30465;&#26102;&#38388;&#21644;&#31934;&#21147;&#65292;&#20570;&#20986;&#26356;&#20026;&#26126;&#26234;&#30340;&#20915;&#31574;&#65292;&#21019;&#36896;&#26082;&#21019;&#26032;&#21448;&#23454;&#29992;&#30340;&#35774;&#35745;&#12290;&#22788;&#29702;&#20197;&#20960;&#20309;&#25968;&#25454;&#34920;&#31034;&#30340;CAD&#35774;&#35745;&#24182;&#20998;&#26512;&#20854;&#32534;&#30721;&#29305;&#24449;&#30340;&#33021;&#21147;&#20351;&#24471;&#33021;&#22815;&#35782;&#21035;&#19981;&#21516;CAD&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20284;&#20043;&#22788;&#65292;&#25552;&#20986;&#26367;&#20195;&#35774;&#35745;&#21644;&#22686;&#24378;&#26041;&#26696;&#65292;&#29978;&#33267;&#29983;&#25104;&#26032;&#30340;&#35774;&#35745;&#26367;&#20195;&#26041;&#26696;&#12290;&#36825;&#20221;&#35843;&#26597;&#20840;&#38754;&#20171;&#32461;&#20102;&#35745;&#31639;&#26426;&#36741;&#21161;&#35774;&#35745;&#20013;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#31867;&#21035;&#65292;&#21253;&#25324;&#30456;&#20284;&#24615;&#20998;&#26512;&#21644;&#26816;&#32034;&#12289;2D&#21644;3D CAD&#27169;&#22411;&#21512;&#25104;&#65292;&#20197;&#21450;CAD&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17695v1 Announce Type: cross  Abstract: Geometric Deep Learning techniques have become a transformative force in the field of Computer-Aided Design (CAD), and have the potential to revolutionize how designers and engineers approach and enhance the design process. By harnessing the power of machine learning-based methods, CAD designers can optimize their workflows, save time and effort while making better informed decisions, and create designs that are both innovative and practical. The ability to process the CAD designs represented by geometric data and to analyze their encoded features enables the identification of similarities among diverse CAD models, the proposition of alternative designs and enhancements, and even the generation of novel design alternatives. This survey offers a comprehensive overview of learning-based methods in computer-aided design across various categories, including similarity analysis and retrieval, 2D and 3D CAD model synthesis, and CAD generatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;AI&#30340;&#28436;&#36827;&#36712;&#36857;&#65292;&#20174;&#22522;&#30784;&#21407;&#29702;&#36861;&#28335;&#21040;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#38416;&#26126;&#20102;AI&#22312;&#22609;&#36896;&#36710;&#36742;&#33258;&#20027;&#20915;&#31574;&#33021;&#21147;&#20013;&#30340;&#22522;&#30784;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.17690</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65306;&#20154;&#24037;&#26234;&#33021;&#21644;&#23398;&#20064;&#31639;&#27861;&#30340;&#28436;&#36827;
&lt;/p&gt;
&lt;p&gt;
Autonomous Vehicles: Evolution of Artificial Intelligence and Learning Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;AI&#30340;&#28436;&#36827;&#36712;&#36857;&#65292;&#20174;&#22522;&#30784;&#21407;&#29702;&#36861;&#28335;&#21040;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#38416;&#26126;&#20102;AI&#22312;&#22609;&#36896;&#36710;&#36742;&#33258;&#20027;&#20915;&#31574;&#33021;&#21147;&#20013;&#30340;&#22522;&#30784;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#20986;&#29616;&#26631;&#24535;&#30528;&#20132;&#36890;&#36816;&#36755;&#39046;&#22495;&#36814;&#26469;&#20102;&#19968;&#20010;&#21464;&#38761;&#26102;&#20195;&#65292;&#36890;&#36807;&#23574;&#31471;&#25216;&#26415;&#37325;&#22609;&#20102;&#31227;&#21160;&#24615;&#30340;&#26684;&#23616;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#23398;&#20064;&#31639;&#27861;&#30340;&#25972;&#21512;&#26159;&#36825;&#19968;&#36827;&#21270;&#30340;&#26680;&#24515;&#65292;&#23558;&#36710;&#36742;&#25512;&#21521;&#21069;&#25152;&#26410;&#26377;&#30340;&#33258;&#20027;&#39046;&#22495;&#12290;&#26412;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;AI&#30340;&#28436;&#36827;&#36712;&#36857;&#65292;&#20174;&#22522;&#30784;&#21407;&#29702;&#36861;&#28335;&#21040;&#26368;&#26032;&#36827;&#23637;&#12290;&#20174;&#24403;&#21069;&#26223;&#35266;&#27010;&#36848;&#24320;&#22987;&#65292;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;AI&#22312;&#22609;&#36896;&#36710;&#36742;&#33258;&#20027;&#20915;&#31574;&#33021;&#21147;&#20013;&#30340;&#22522;&#30784;&#20316;&#29992;&#12290;&#38416;&#26126;&#20102;AI&#39537;&#21160;&#30340;&#36710;&#36742;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#20013;&#28041;&#21450;&#30340;&#27493;&#39588;&#65292;&#35299;&#20915;&#20102;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;AI&#39537;&#21160;&#36719;&#20214;&#24320;&#21457;&#20013;&#30340;&#20262;&#29702;&#32771;&#34385;&#21644;&#20559;&#35265;&#38382;&#39064;&#12290;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#20851;&#20110;AI/&#23398;&#20064;&#30340;&#20351;&#29992;&#21644;&#31867;&#22411;&#30340;&#32479;&#35745;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17690v1 Announce Type: cross  Abstract: The advent of autonomous vehicles has heralded a transformative era in transportation, reshaping the landscape of mobility through cutting-edge technologies. Central to this evolu- tion is the integration of Artificial Intelligence (AI) and learning algorithms, propelling vehicles into realms of unprecedented autonomy. This paper provides a comprehensive exploration of the evolutionary trajectory of AI within autonomous vehicles, tracing the journey from foundational principles to the most recent advancements. Commencing with a current landscape overview, the paper delves into the fundamental role of AI in shaping the autonomous decision-making capabilities of vehicles. It elucidates the steps involved in the AI-powered development life cycle in vehicles, addressing ethical considerations and bias in AI-driven software development for autonomous vehicles. The study presents statis- tical insights into the usage and types of AI/learning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#20351;&#29992;&#20808;&#21069;&#25552;&#21462;&#30340;&#29992;&#25143;&#20449;&#24687;&#23545;&#36710;&#36733;&#26080;&#32447;&#29615;&#22659;&#20013;&#30340;QoS&#36827;&#34892;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;ML&#26641;&#38598;&#25104;&#26041;&#27861;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.17689</link><description>&lt;p&gt;
&#36890;&#36807;&#20808;&#39564;&#29992;&#25143;&#20449;&#24687;&#22312;&#26080;&#32447;&#36710;&#36733;&#29615;&#22659;&#20013;&#39044;&#27979;QoS
&lt;/p&gt;
&lt;p&gt;
QoS prediction in radio vehicular environments via prior user information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#20351;&#29992;&#20808;&#21069;&#25552;&#21462;&#30340;&#29992;&#25143;&#20449;&#24687;&#23545;&#36710;&#36733;&#26080;&#32447;&#29615;&#22659;&#20013;&#30340;QoS&#36827;&#34892;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;ML&#26641;&#38598;&#25104;&#26041;&#27861;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#30340;&#26080;&#32447;&#36890;&#20449;&#22312;&#27773;&#36710;&#34892;&#19994;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#23427;&#26377;&#21161;&#20110;&#22686;&#24378;&#24403;&#21069;&#29992;&#20363;&#24182;&#23454;&#29616;&#26032;&#30340;&#29992;&#20363;&#65292;&#22914;&#36830;&#25509;&#30340;&#33258;&#21160;&#39550;&#39542;&#12289;&#32534;&#38431;&#34892;&#39542;&#12289;&#21512;&#20316;&#25805;&#32437;&#12289;&#36828;&#31243;&#39550;&#39542;&#21644;&#26234;&#33021;&#23548;&#33322;&#12290;&#36825;&#20123;&#20197;&#21450;&#20854;&#20182;&#29992;&#20363;&#36890;&#24120;&#20381;&#36182;&#20110;&#29305;&#23450;&#30340;&#36890;&#20449;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#27700;&#24179;&#12290;&#26368;&#36817;&#65292;&#39044;&#27979;&#24615;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#39046;&#22495;&#21463;&#21040;&#20102;&#26497;&#22823;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25552;&#21069;&#36275;&#22815;&#20934;&#30830;&#22320;&#39044;&#27979;&#36890;&#20449;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#22312;&#21487;&#38752;&#22320;&#39044;&#27979;QoS&#26041;&#38754;&#26159;&#19968;&#39033;&#38750;&#24120;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20351;&#29992;&#20174;&#34562;&#31389;&#27979;&#35797;&#32593;&#32476;&#25910;&#38598;&#30340;&#25968;&#25454;&#26469;&#39044;&#27979;QoS&#30340;ML&#26641;&#38598;&#25104;&#26041;&#27861;&#65292;&#33539;&#22260;&#20026;&#20960;&#20998;&#38047;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#26080;&#32447;&#29615;&#22659;&#29305;&#24449;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#29305;&#24449;&#26469;&#25552;&#39640;ML&#24615;&#33021;&#65292;&#24182;&#36827;&#19968;&#27493;&#25903;&#25345;ML&#22312;&#21830;&#19994;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17689v1 Announce Type: cross  Abstract: Reliable wireless communications play an important role in the automotive industry as it helps to enhance current use cases and enable new ones such as connected autonomous driving, platooning, cooperative maneuvering, teleoperated driving, and smart navigation. These and other use cases often rely on specific quality of service (QoS) levels for communication. Recently, the area of predictive quality of service (QoS) has received a great deal of attention as a key enabler to forecast communication quality well enough in advance. However, predicting QoS in a reliable manner is a notoriously difficult task. In this paper, we evaluate ML tree-ensemble methods to predict QoS in the range of minutes with data collected from a cellular test network. We discuss radio environment characteristics and we showcase how these can be used to improve ML performance and further support the uptake of ML in commercial networks. Specifically, we use the 
&lt;/p&gt;</description></item><item><title>&#38598;&#25104;&#27169;&#22411;&#22312;&#26816;&#27979;&#24322;&#24120;&#20540;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65292;&#20854;&#27425;&#26159;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#30740;&#31350;&#21457;&#29616;&#19968;&#31181;&#32467;&#26500;&#25351;&#26631;&#19982;&#22823;&#35823;&#24046;&#30456;&#20851;&#65292;&#26377;&#21161;&#20110;&#24555;&#36895;&#20998;&#31867;&#26032;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.17686</link><description>&lt;p&gt;
&#21453;&#24212;&#24615;&#26426;&#22120;&#23398;&#20064;&#21183;&#33021;&#34920;&#38754;&#30340;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Outlier-Detection for Reactive Machine Learned Potential Energy Surfaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17686
&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#27169;&#22411;&#22312;&#26816;&#27979;&#24322;&#24120;&#20540;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65292;&#20854;&#27425;&#26159;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#30740;&#31350;&#21457;&#29616;&#19968;&#31181;&#32467;&#26500;&#25351;&#26631;&#19982;&#22823;&#35823;&#24046;&#30456;&#20851;&#65292;&#26377;&#21161;&#20110;&#24555;&#36895;&#20998;&#31867;&#26032;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24212;&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#26469;&#26816;&#27979;&#20855;&#26377;&#22823;&#26399;&#26395;&#35823;&#24046;&#65288;&#24322;&#24120;&#20540;&#65289;&#30340;&#26679;&#26412;&#65292;&#24212;&#29992;&#20110;&#21453;&#24212;&#24615;&#20998;&#23376;&#21183;&#33021;&#34920;&#38754;&#65288;PES&#65289;&#12290;&#19977;&#31181;&#26041;&#27861; - &#38598;&#25104;&#27169;&#22411;&#12289;&#28145;&#21051;&#35777;&#25454;&#22238;&#24402;&#65288;DER&#65289;&#21644;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289; - &#34987;&#24212;&#29992;&#20110;${\it syn-}$Criegee&#21644;&#20057;&#28911;&#32671;&#22522;&#36807;&#27687;&#21270;&#29289;&#20043;&#38388;&#30340;&#27682;&#36716;&#31227;&#21453;&#24212;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#38598;&#25104;&#27169;&#22411;&#20026;&#26816;&#27979;&#24322;&#24120;&#20540;&#25552;&#20379;&#20102;&#26368;&#20339;&#32467;&#26524;&#65292;&#20854;&#27425;&#26159;GMM&#12290;&#20363;&#22914;&#65292;&#20174;&#20855;&#26377;&#26368;&#22823;&#19981;&#30830;&#23450;&#24615;&#30340;1000&#20010;&#32467;&#26500;&#20013;&#65292;&#22914;&#26524;&#23547;&#25214;&#20855;&#26377;&#22823;&#35823;&#24046;&#30340;25&#25110;1000&#20010;&#32467;&#26500;&#65292;&#24322;&#24120;&#20540;&#26816;&#27979;&#36136;&#37327;&#20998;&#21035;&#20026;&#32422;90\%&#21644;&#32422;50\%&#12290;&#30456;&#21453;&#65292;DER&#30340;&#32479;&#35745;&#20551;&#35774;&#30340;&#23616;&#38480;&#24615;&#20005;&#37325;&#24433;&#21709;&#20102;&#20854;&#39044;&#27979;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#30340;&#25351;&#26631;&#19982;&#22823;&#24179;&#22343;&#35823;&#24046;&#30456;&#20851;&#65292;&#36825;&#26377;&#21161;&#20110;&#24555;&#36895;&#23558;&#26032;&#32467;&#26500;&#20998;&#31867;&#20026;&#37027;&#20123;&#33021;&#22815;&#25552;&#20379;&#26368;&#22823;&#31283;&#23450;&#24615;&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17686v1 Announce Type: cross  Abstract: Uncertainty quantification (UQ) to detect samples with large expected errors (outliers) is applied to reactive molecular potential energy surfaces (PESs). Three methods - Ensembles, Deep Evidential Regression (DER), and Gaussian Mixture Models (GMM) - were applied to the H-transfer reaction between ${\it syn-}$Criegee and vinyl hydroxyperoxide. The results indicate that ensemble models provide the best results for detecting outliers, followed by GMM. For example, from a pool of 1000 structures with the largest uncertainty, the detection quality for outliers is $\sim 90$ \% and $\sim 50$ \%, respectively, if 25 or 1000 structures with large errors are sought. On the contrary, the limitations of the statistical assumptions of DER greatly impacted its prediction capabilities. Finally, a structure-based indicator was found to be correlated with large average error, which may help to rapidly classify new structures into those that provide a
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#22238;&#39038;&#20102;&#22686;&#24378;&#22522;&#30784;&#27169;&#22411;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#30528;&#37325;&#20110;&#22235;&#31181;&#20851;&#38190;&#26041;&#27861;&#35770;&#65292;&#20026;&#26500;&#24314;&#23433;&#20840;&#21487;&#38752;&#30340;FMs&#21644;&#20419;&#36827;&#31283;&#23450;&#19968;&#33268;&#30340;ICL&#29615;&#22659;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.17671</link><description>&lt;p&gt;
&#21152;&#24378;&#19978;&#19979;&#25991;&#23398;&#20064;&#20197;&#30830;&#20445;&#21487;&#38752;&#24615;&#65306;&#23545;&#22522;&#30784;&#27169;&#22411;&#30340;&#31616;&#35201;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
Securing Reliability: A Brief Overview on Enhancing In-Context Learning for Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17671
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#22238;&#39038;&#20102;&#22686;&#24378;&#22522;&#30784;&#27169;&#22411;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#30528;&#37325;&#20110;&#22235;&#31181;&#20851;&#38190;&#26041;&#27861;&#35770;&#65292;&#20026;&#26500;&#24314;&#23433;&#20840;&#21487;&#38752;&#30340;FMs&#21644;&#20419;&#36827;&#31283;&#23450;&#19968;&#33268;&#30340;ICL&#29615;&#22659;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#32487;&#32493;&#22609;&#36896;&#20154;&#24037;&#26234;&#33021;&#30340;&#26684;&#23616;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33539;&#24335;&#34028;&#21187;&#21457;&#23637;&#65292;&#20294;&#20063;&#36935;&#21040;&#20102;&#27602;&#24615;&#12289;&#24187;&#35273;&#12289;&#24046;&#24322;&#12289;&#23545;&#25239;&#24615;&#33030;&#24369;&#24615;&#21644;&#19981;&#19968;&#33268;&#24615;&#31561;&#38382;&#39064;&#12290;&#30830;&#20445;FMs&#30340;&#21487;&#38752;&#24615;&#21644;&#36131;&#20219;&#24615;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#29983;&#24577;&#31995;&#32479;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#31687;&#31616;&#26126;&#27010;&#36848;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22686;&#24378;FMs&#22312;ICL&#26694;&#26550;&#20869;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#22235;&#31181;&#20851;&#38190;&#26041;&#27861;&#35770;&#65292;&#27599;&#31181;&#26041;&#27861;&#35770;&#37117;&#26377;&#20854;&#30456;&#24212;&#30340;&#23376;&#30446;&#26631;&#12290;&#25105;&#20204;&#30495;&#35802;&#24076;&#26395;&#26412;&#25991;&#33021;&#20026;&#33268;&#21147;&#20110;&#26500;&#24314;&#23433;&#20840;&#21487;&#38752;FMs&#24182;&#20419;&#36827;&#31283;&#23450;&#19968;&#33268;&#30340;ICL&#29615;&#22659;&#12289;&#20174;&#32780;&#37322;&#25918;&#20854;&#24040;&#22823;&#28508;&#21147;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17671v1 Announce Type: new  Abstract: As foundation models (FMs) continue to shape the landscape of AI, the in-context learning (ICL) paradigm thrives but also encounters issues such as toxicity, hallucination, disparity, adversarial vulnerability, and inconsistency. Ensuring the reliability and responsibility of FMs is crucial for the sustainable development of the AI ecosystem. In this concise overview, we investigate recent advancements in enhancing the reliability and trustworthiness of FMs within ICL frameworks, focusing on four key methodologies, each with its corresponding subgoals. We sincerely hope this paper can provide valuable insights for researchers and practitioners endeavoring to build safe and dependable FMs and foster a stable and consistent ICL environment, thereby unlocking their vast potential.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20302;&#22320;&#29699;&#36712;&#36947;&#21355;&#26143;&#26143;&#24231;&#20013;&#30340;&#36335;&#30001;&#65292;&#36890;&#36807;&#31163;&#32447;&#23398;&#20064;&#26368;&#20339;&#36335;&#24452;&#65292;&#24182;&#22312;&#22312;&#32447;&#38454;&#27573;&#36827;&#34892;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#36335;&#30001;&#12290;</title><link>https://arxiv.org/abs/2402.17666</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#20998;&#24067;&#24335;&#21355;&#26143;&#36335;&#30001;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Deep Reinforcement Learning for Distributed Satellite Routing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20302;&#22320;&#29699;&#36712;&#36947;&#21355;&#26143;&#26143;&#24231;&#20013;&#30340;&#36335;&#30001;&#65292;&#36890;&#36807;&#31163;&#32447;&#23398;&#20064;&#26368;&#20339;&#36335;&#24452;&#65292;&#24182;&#22312;&#22312;&#32447;&#38454;&#27573;&#36827;&#34892;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#36335;&#30001;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#20302;&#22320;&#29699;&#36712;&#36947;&#21355;&#26143;&#26143;&#24231;&#65288;LSatCs&#65289;&#20013;&#36335;&#30001;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;MA-DRL&#65289;&#26041;&#27861;&#12290;&#27599;&#20010;&#21355;&#26143;&#26159;&#19968;&#20010;&#29420;&#31435;&#30340;&#20915;&#31574;&#21046;&#23450;&#26234;&#33021;&#20307;&#65292;&#20855;&#26377;&#23545;&#29615;&#22659;&#30340;&#37096;&#20998;&#30693;&#35782;&#65292;&#24182;&#21463;&#21040;&#38468;&#36817;&#26234;&#33021;&#20307;&#30340;&#21453;&#39304;&#25903;&#25345;&#12290;&#22312;&#25105;&#20204;&#20043;&#21069;&#20171;&#32461;&#30340;Q-routing&#35299;&#20915;&#26041;&#26696;&#30340;&#22522;&#30784;&#19978;&#65292;&#26412;&#25991;&#30340;&#36129;&#29486;&#26159;&#23558;&#20854;&#25193;&#23637;&#20026;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#32593;&#32476;&#21644;&#20132;&#36890;&#21464;&#21270;&#65292;&#24182;&#22522;&#20110;&#20004;&#20010;&#38454;&#27573;&#65306;&#65288;1&#65289;&#19968;&#20010;&#20381;&#36182;&#20840;&#23616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#23398;&#20064;&#22312;&#27599;&#20010;&#21487;&#33021;&#20301;&#32622;&#21644;&#25317;&#22581;&#32423;&#21035;&#19978;&#30340;&#26368;&#20339;&#36335;&#24452;&#30340;&#31163;&#32447;&#25506;&#32034;&#23398;&#20064;&#38454;&#27573;&#65307;&#65288;2&#65289;&#19968;&#20010;&#24102;&#26377;&#26412;&#22320;&#12289;&#26426;&#36733;&#12289;&#39044;&#35757;&#32451;DNN&#30340;&#22312;&#32447;&#24320;&#21457;&#38454;&#27573;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;MA-DRL&#33021;&#22815;&#26377;&#25928;&#22320;&#22312;&#31163;&#32447;&#23398;&#20064;&#26368;&#20339;&#36335;&#30001;&#65292;&#28982;&#21518;&#21152;&#36733;&#20197;&#36827;&#34892;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#36335;&#30001;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17666v1 Announce Type: new  Abstract: This paper introduces a Multi-Agent Deep Reinforcement Learning (MA-DRL) approach for routing in Low Earth Orbit Satellite Constellations (LSatCs). Each satellite is an independent decision-making agent with a partial knowledge of the environment, and supported by feedback received from the nearby agents. Building on our previous work that introduced a Q-routing solution, the contribution of this paper is to extend it to a deep learning framework able to quickly adapt to the network and traffic changes, and based on two phases: (1) An offline exploration learning phase that relies on a global Deep Neural Network (DNN) to learn the optimal paths at each possible position and congestion level; (2) An online exploitation phase with local, on-board, pre-trained DNNs. Results show that MA-DRL efficiently learns optimal routes offline that are then loaded for an efficient distributed routing online.
&lt;/p&gt;</description></item><item><title>TorchMD-Net 2.0&#26159;&#22312;&#31070;&#32463;&#32593;&#32476;&#21183;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#30340;&#37325;&#35201;&#36827;&#23637;&#65292;&#36890;&#36807;&#24341;&#20837;TensorNet&#31561;&#23574;&#31471;&#32467;&#26500;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#20351;&#24471;&#22312;&#35745;&#31639;&#33021;&#37327;&#21644;&#21147;&#26102;&#33719;&#24471;&#20102;2&#21040;10&#20493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.17660</link><description>&lt;p&gt;
TorchMD-Net 2.0: &#20998;&#23376;&#27169;&#25311;&#20013;&#30340;&#24555;&#36895;&#31070;&#32463;&#32593;&#32476;&#21183;
&lt;/p&gt;
&lt;p&gt;
TorchMD-Net 2.0: Fast Neural Network Potentials for Molecular Simulations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17660
&lt;/p&gt;
&lt;p&gt;
TorchMD-Net 2.0&#26159;&#22312;&#31070;&#32463;&#32593;&#32476;&#21183;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#30340;&#37325;&#35201;&#36827;&#23637;&#65292;&#36890;&#36807;&#24341;&#20837;TensorNet&#31561;&#23574;&#31471;&#32467;&#26500;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#20351;&#24471;&#22312;&#35745;&#31639;&#33021;&#37327;&#21644;&#21147;&#26102;&#33719;&#24471;&#20102;2&#21040;10&#20493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17660v1 &#22768;&#26126;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#22312;&#20998;&#23376;&#27169;&#25311;&#20013;&#23454;&#29616;&#35745;&#31639;&#36895;&#24230;&#12289;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#36890;&#29992;&#36866;&#29992;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#19968;&#30452;&#26159;&#19968;&#20010;&#25345;&#20037;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TorchMD-Net&#36719;&#20214;&#30340;&#37325;&#22823;&#36827;&#23637;&#65292;&#36825;&#26159;&#20174;&#20256;&#32479;&#21147;&#22330;&#36716;&#21521;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21183;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;TorchMD-Net&#28436;&#21464;&#25104;&#19968;&#20010;&#26356;&#20840;&#38754;&#21644;&#26356;&#22810;&#26679;&#21270;&#30340;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;TensorNet&#31561;&#23574;&#31471;&#20307;&#31995;&#32467;&#26500;&#12290;&#36890;&#36807;&#27169;&#22359;&#21270;&#35774;&#35745;&#26041;&#27861;&#23454;&#29616;&#20102;&#36825;&#31181;&#36716;&#21464;&#65292;&#40723;&#21169;&#31185;&#23398;&#30028;&#20869;&#37096;&#30340;&#23450;&#21046;&#24212;&#29992;&#12290;&#26368;&#26174;&#30528;&#30340;&#22686;&#24378;&#26159;&#22312;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#26174;&#33879;&#25913;&#36827;&#65292;&#22312;TensorNet&#27169;&#22411;&#30340;&#33021;&#37327;&#21644;&#21147;&#35745;&#31639;&#20013;&#23454;&#29616;&#20102;&#38750;&#24120;&#26174;&#33879;&#30340;&#21152;&#36895;&#65292;&#24615;&#33021;&#25552;&#21319;&#33539;&#22260;&#20174;&#21069;&#20960;&#20010;&#29256;&#26412;&#30340;2&#20493;&#21040;10&#20493;&#12290;&#20854;&#20182;&#22686;&#24378;&#21151;&#33021;&#21253;&#25324;&#39640;&#24230;&#20248;&#21270;&#30340;&#37051;&#23621;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17660v1 Announce Type: new  Abstract: Achieving a balance between computational speed, prediction accuracy, and universal applicability in molecular simulations has been a persistent challenge. This paper presents substantial advancements in the TorchMD-Net software, a pivotal step forward in the shift from conventional force fields to neural network-based potentials. The evolution of TorchMD-Net into a more comprehensive and versatile framework is highlighted, incorporating cutting-edge architectures such as TensorNet. This transformation is achieved through a modular design approach, encouraging customized applications within the scientific community. The most notable enhancement is a significant improvement in computational efficiency, achieving a very remarkable acceleration in the computation of energy and forces for TensorNet models, with performance gains ranging from 2-fold to 10-fold over previous iterations. Other enhancements include highly optimized neighbor sear
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#22810;&#23383;&#27573;&#26657;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#26681;&#25454;&#26679;&#26412;&#32479;&#35745;&#25512;&#23548;&#30340;&#32622;&#20449;&#27700;&#24179;&#33258;&#36866;&#24212;&#35843;&#25972;&#26657;&#20934;&#24378;&#24230;&#65292;&#20197;&#35299;&#20915;&#26657;&#20934;&#36807;&#31243;&#20013;&#23384;&#22312;&#30340;&#20559;&#24046;&#25918;&#22823;&#21644;&#22312;&#32447;&#24178;&#25200;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.17655</link><description>&lt;p&gt;
&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#22810;&#23383;&#27573;&#27169;&#22411;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Confidence-Aware Multi-Field Model Calibration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#22810;&#23383;&#27573;&#26657;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#26681;&#25454;&#26679;&#26412;&#32479;&#35745;&#25512;&#23548;&#30340;&#32622;&#20449;&#27700;&#24179;&#33258;&#36866;&#24212;&#35843;&#25972;&#26657;&#20934;&#24378;&#24230;&#65292;&#20197;&#35299;&#20915;&#26657;&#20934;&#36807;&#31243;&#20013;&#23384;&#22312;&#30340;&#20559;&#24046;&#25918;&#22823;&#21644;&#22312;&#32447;&#24178;&#25200;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#29992;&#25143;&#21453;&#39304;&#27010;&#29575;&#65288;&#22914;&#28857;&#20987;&#21644;&#36716;&#25442;&#65289;&#23545;&#20110;&#24191;&#21578;&#25490;&#21517;&#21644;&#31454;&#20215;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#20998;&#24067;&#30340;&#36716;&#31227;&#21644;&#22266;&#26377;&#27169;&#22411;&#20559;&#24046;&#65292;&#39044;&#27979;&#27010;&#29575;&#19982;&#30495;&#23454;&#21487;&#33021;&#24615;&#20043;&#38388;&#32463;&#24120;&#23384;&#22312;&#19981;&#24076;&#26395;&#30340;&#19981;&#19968;&#33268;&#12290;&#26657;&#20934;&#26088;&#22312;&#36890;&#36807;&#21518;&#22788;&#29702;&#27169;&#22411;&#39044;&#27979;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#32780;&#22522;&#20110;&#23383;&#27573;&#30340;&#26657;&#20934;&#21487;&#20197;&#35843;&#25972;&#19981;&#21516;&#29305;&#24449;&#23383;&#27573;&#20540;&#19978;&#30340;&#27169;&#22411;&#36755;&#20986;&#65292;&#20197;&#28385;&#36275;&#32454;&#31890;&#24230;&#30340;&#24191;&#21578;&#38656;&#27714;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#23545;&#24212;&#20110;&#26576;&#20123;&#23383;&#27573;&#20540;&#30340;&#35266;&#23519;&#26679;&#26412;&#21487;&#33021;&#22826;&#26377;&#38480;&#65292;&#26080;&#27861;&#36827;&#34892;&#26377;&#20449;&#24515;&#30340;&#26657;&#20934;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20559;&#24046;&#25918;&#22823;&#21644;&#22312;&#32447;&#24178;&#25200;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#22810;&#23383;&#27573;&#26657;&#20934;&#26041;&#27861;&#65292;&#26681;&#25454;&#26679;&#26412;&#32479;&#35745;&#25512;&#23548;&#30340;&#32622;&#20449;&#27700;&#24179;&#33258;&#36866;&#24212;&#35843;&#25972;&#26657;&#20934;&#24378;&#24230;&#12290;&#23427;&#36824;&#21033;&#29992;&#22810;&#20010;&#29305;&#24449;&#23383;&#27573;&#36827;&#34892;&#32852;&#21512;&#27169;&#22411;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17655v1 Announce Type: new  Abstract: Accurately predicting the probabilities of user feedback, such as clicks and conversions, is critical for ad ranking and bidding. However, there often exist unwanted mismatches between predicted probabilities and true likelihoods due to the shift of data distributions and intrinsic model biases. Calibration aims to address this issue by post-processing model predictions, and field-aware calibration can adjust model output on different feature field values to satisfy fine-grained advertising demands. Unfortunately, the observed samples corresponding to certain field values can be too limited to make confident calibrations, which may yield bias amplification and online disturbance. In this paper, we propose a confidence-aware multi-field calibration method, which adaptively adjusts the calibration intensity based on the confidence levels derived from sample statistics. It also utilizes multiple feature fields for joint model calibration wi
&lt;/p&gt;</description></item><item><title>&#21464;&#20998;&#23398;&#20064;&#22312;&#22823;&#22411;&#28145;&#24230;&#32593;&#32476;&#20013;&#23637;&#29616;&#20986;&#38750;&#24120;&#22909;&#30340;&#25928;&#26524;&#65292;IVON&#20248;&#21270;&#22120;&#22312;&#35757;&#32451;&#22823;&#22411;&#32593;&#32476;&#26102;&#20960;&#20046;&#33021;&#19982;Adam&#30456;&#23218;&#32654;&#29978;&#33267;&#32988;&#36807;&#23427;&#65292;&#19988;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#26356;&#20934;&#30830;&#65292;&#23545;&#27169;&#22411;&#24494;&#35843;&#12289;&#27867;&#21270;&#35823;&#24046;&#39044;&#27979;&#21644;&#25968;&#25454;&#25935;&#24863;&#24615;&#20272;&#35745;&#22343;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.17641</link><description>&lt;p&gt;
&#21464;&#20998;&#23398;&#20064;&#23545;&#22823;&#22411;&#28145;&#24230;&#32593;&#32476;&#26377;&#25928;
&lt;/p&gt;
&lt;p&gt;
Variational Learning is Effective for Large Deep Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17641
&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#23398;&#20064;&#22312;&#22823;&#22411;&#28145;&#24230;&#32593;&#32476;&#20013;&#23637;&#29616;&#20986;&#38750;&#24120;&#22909;&#30340;&#25928;&#26524;&#65292;IVON&#20248;&#21270;&#22120;&#22312;&#35757;&#32451;&#22823;&#22411;&#32593;&#32476;&#26102;&#20960;&#20046;&#33021;&#19982;Adam&#30456;&#23218;&#32654;&#29978;&#33267;&#32988;&#36807;&#23427;&#65292;&#19988;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#26356;&#20934;&#30830;&#65292;&#23545;&#27169;&#22411;&#24494;&#35843;&#12289;&#27867;&#21270;&#35823;&#24046;&#39044;&#27979;&#21644;&#25968;&#25454;&#25935;&#24863;&#24615;&#20272;&#35745;&#22343;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#22823;&#37327;&#23454;&#35777;&#35777;&#25454;&#65292;&#21453;&#39539;&#20102;&#21464;&#20998;&#23398;&#20064;&#23545;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#26080;&#25928;&#30340;&#26222;&#36941;&#30475;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#21517;&#20026;Improved Variational Online Newton (IVON)&#30340;&#20248;&#21270;&#22120;&#65292;&#22312;&#35757;&#32451;&#22823;&#22411;&#32593;&#32476;&#65288;&#22914;GPT-2&#21644;ResNets&#65289;&#26102;&#22987;&#32456;&#33021;&#22815;&#19982;Adam&#30456;&#21305;&#37197;&#25110;&#32988;&#36807;&#23427;&#12290;IVON&#30340;&#35745;&#31639;&#25104;&#26412;&#20960;&#20046;&#19982;Adam&#30456;&#21516;&#65292;&#20294;&#20854;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#26356;&#22909;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;IVON&#30340;&#20960;&#31181;&#26032;&#29992;&#20363;&#65292;&#20854;&#20013;&#25105;&#20204;&#25913;&#36827;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#21644;&#27169;&#22411;&#21512;&#24182;&#65292;&#22312;&#20934;&#30830;&#39044;&#27979;&#27867;&#21270;&#35823;&#24046;&#21644;&#24544;&#23454;&#20272;&#35745;&#23545;&#25968;&#25454;&#30340;&#25935;&#24863;&#24615;&#26041;&#38754;&#12290;&#25105;&#20204;&#25214;&#21040;&#20102;&#22823;&#37327;&#25903;&#25345;&#21464;&#20998;&#23398;&#20064;&#26377;&#25928;&#24615;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17641v1 Announce Type: cross  Abstract: We give extensive empirical evidence against the common belief that variational learning is ineffective for large neural networks. We show that an optimizer called Improved Variational Online Newton (IVON) consistently matches or outperforms Adam for training large networks such as GPT-2 and ResNets from scratch. IVON's computational costs are nearly identical to Adam but its predictive uncertainty is better. We show several new use cases of IVON where we improve fine-tuning and model merging in Large Language Models, accurately predict generalization error, and faithfully estimate sensitivity to data. We find overwhelming evidence in support of effectiveness of variational learning.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#22823;&#37327;&#26399;&#21002;&#25991;&#31456;&#65292;&#24635;&#32467;&#20102;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#22312;&#24494;&#29983;&#29289;&#32452;&#23398;&#20013;&#30340;&#29616;&#26377;&#23454;&#36341;&#65292;&#25506;&#35752;&#20102;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#36991;&#20813;&#24120;&#35265;&#23454;&#39564;&#35774;&#35745;&#32570;&#38519;&#30340;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2402.17621</link><description>&lt;p&gt;
&#29992;&#20110;&#24494;&#29983;&#29289;&#32452;&#23398;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#65306;&#24357;&#21512;&#24403;&#21069;&#21644;&#26368;&#20339;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Supervised machine learning for microbiomics: bridging the gap between current and best practices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17621
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#22823;&#37327;&#26399;&#21002;&#25991;&#31456;&#65292;&#24635;&#32467;&#20102;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#22312;&#24494;&#29983;&#29289;&#32452;&#23398;&#20013;&#30340;&#29616;&#26377;&#23454;&#36341;&#65292;&#25506;&#35752;&#20102;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#36991;&#20813;&#24120;&#35265;&#23454;&#39564;&#35774;&#35745;&#32570;&#38519;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#23558;&#21152;&#36895;&#20020;&#24202;&#24494;&#29983;&#29289;&#32452;&#23398;&#21019;&#26032;&#65292;&#22914;&#30142;&#30149;&#35786;&#26029;&#21644;&#39044;&#21518;&#12290;&#36825;&#23558;&#38656;&#35201;&#39640;&#36136;&#37327;&#12289;&#21487;&#37325;&#29616;&#12289;&#21487;&#35299;&#37322;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#20854;&#39044;&#27979;&#33021;&#21147;&#36798;&#21040;&#25110;&#36229;&#36807;&#30417;&#31649;&#26426;&#26500;&#23545;&#20020;&#24202;&#24037;&#20855;&#35774;&#23450;&#30340;&#39640;&#38376;&#27099;&#12290;&#25105;&#20204;&#36890;&#36807;&#28145;&#20837;&#20998;&#26512;2021-2022&#24180;&#21457;&#34920;&#30340;100&#31687;&#21516;&#34892;&#35780;&#35758;&#30340;&#26399;&#21002;&#25991;&#31456;&#65292;&#25429;&#25417;&#20102;&#24403;&#21069;&#23558;&#30417;&#30563;ML&#24212;&#29992;&#20110;&#24494;&#29983;&#29289;&#32452;&#23398;&#25968;&#25454;&#30340;&#23454;&#36341;&#30340;&#19968;&#20010;&#24555;&#29031;&#12290;&#25105;&#20204;&#37319;&#29992;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#24341;&#23548;&#35752;&#35770;&#21508;&#31181;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#21253;&#25324;&#20851;&#38190;&#32771;&#34385;&#22240;&#32032;&#65292;&#22914;&#22914;&#20309;&#20943;&#36731;&#23567;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#24433;&#21709;&#21516;&#26102;&#36991;&#20813;&#25968;&#25454;&#27844;&#28431;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20379;&#20851;&#20110;&#22914;&#20309;&#36991;&#20813;&#21487;&#33021;&#25439;&#23475;&#27169;&#22411;&#24615;&#33021;&#12289;&#21487;&#20449;&#24230;&#21644;&#21487;&#37325;&#22797;&#24615;&#30340;&#24120;&#35265;&#23454;&#39564;&#35774;&#35745;&#32570;&#38519;&#30340;&#25351;&#21335;&#12290;&#35752;&#35770;&#38468;&#26377;&#19968;&#20010;&#20114;&#21160;&#22312;&#32447;&#25945;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17621v1 Announce Type: cross  Abstract: Machine learning (ML) is set to accelerate innovations in clinical microbiomics, such as in disease diagnostics and prognostics. This will require high-quality, reproducible, interpretable workflows whose predictive capabilities meet or exceed the high thresholds set for clinical tools by regulatory agencies. Here, we capture a snapshot of current practices in the application of supervised ML to microbiomics data, through an in-depth analysis of 100 peer-reviewed journal articles published in 2021-2022. We apply a data-driven approach to steer discussion of the merits of varied approaches to experimental design, including key considerations such as how to mitigate the effects of small dataset size while avoiding data leakage. We further provide guidance on how to avoid common experimental design pitfalls that can hurt model performance, trustworthiness, and reproducibility. Discussion is accompanied by an interactive online tutorial th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25299;&#25169;&#24863;&#30693;&#30340;&#21452;&#21521;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;TBGAT&#65289;&#65292;&#22312;&#35299;&#20915;&#36710;&#38388;&#20316;&#19994;&#35843;&#24230;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#23884;&#20837;&#24182;&#21457;&#22270;&#24182;&#21033;&#29992;&#21452;&#21521;&#35270;&#22270;&#23884;&#20837;&#12289;&#22270;&#27880;&#24847;&#21147;&#32858;&#21512;&#31561;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#23545;&#25299;&#25169;&#32467;&#26500;&#30340;&#26356;&#22909;&#24314;&#27169;&#21644;&#21033;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.17606</link><description>&lt;p&gt;
&#20351;&#29992;&#21452;&#21521;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#23398;&#20064;&#25299;&#25169;&#34920;&#31034;&#35299;&#20915;&#36710;&#38388;&#20316;&#19994;&#35843;&#24230;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Learning Topological Representations with Bidirectional Graph Attention Network for Solving Job Shop Scheduling Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25299;&#25169;&#24863;&#30693;&#30340;&#21452;&#21521;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;TBGAT&#65289;&#65292;&#22312;&#35299;&#20915;&#36710;&#38388;&#20316;&#19994;&#35843;&#24230;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#23884;&#20837;&#24182;&#21457;&#22270;&#24182;&#21033;&#29992;&#21452;&#21521;&#35270;&#22270;&#23884;&#20837;&#12289;&#22270;&#27880;&#24847;&#21147;&#32858;&#21512;&#31561;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#23545;&#25299;&#25169;&#32467;&#26500;&#30340;&#26356;&#22909;&#24314;&#27169;&#21644;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#38024;&#23545;&#26080;&#21521;&#22270;&#30340;&#29616;&#25104;GNN&#27169;&#22411;&#35299;&#20915;&#36710;&#38388;&#20316;&#19994;&#35843;&#24230;&#38382;&#39064;&#65288;JSSP&#65289;&#65292;&#24182;&#24573;&#30053;&#20102;&#24182;&#21457;&#22270;&#65288;DGs&#65289;&#30340;&#20016;&#23500;&#32780;&#26377;&#24847;&#20041;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25299;&#25169;&#24863;&#30693;&#30340;&#21452;&#21521;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;TBGAT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26032;&#39062;GNN&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#26412;&#22320;&#25628;&#32034;&#26694;&#26550;&#20013;&#23884;&#20837;DG&#20197;&#35299;&#20915;JSSP&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TBGAT&#20998;&#21035;&#20174;&#27491;&#21521;&#21644;&#21453;&#21521;&#35270;&#22270;&#23884;&#20837;DG&#65292;&#28040;&#24687;&#36890;&#36807;&#36981;&#24490;&#19981;&#21516;&#35270;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#20256;&#25773;&#65292;&#24182;&#36890;&#36807;&#22270;&#27880;&#24847;&#21147;&#36827;&#34892;&#27719;&#24635;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#30340;&#26032;&#25805;&#20316;&#31526;&#65292;&#29992;&#20110;&#35745;&#31639;DG&#30340;&#21069;&#21521;&#21644;&#21518;&#21521;&#25299;&#25169;&#25490;&#24207;&#65292;&#36825;&#20123;&#29305;&#24449;&#29992;&#20110;&#34920;&#24449;&#25299;&#25169;&#32467;&#26500;&#24182;&#34987;&#25105;&#20204;&#30340;&#27169;&#22411;&#21033;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#23454;&#39564;&#19978;&#23637;&#31034;&#20102;TBGAT&#30340;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17606v1 Announce Type: cross  Abstract: Existing learning-based methods for solving job shop scheduling problem (JSSP) usually use off-the-shelf GNN models tailored to undirected graphs and neglect the rich and meaningful topological structures of disjunctive graphs (DGs). This paper proposes the topology-aware bidirectional graph attention network (TBGAT), a novel GNN architecture based on the attention mechanism, to embed the DG for solving JSSP in a local search framework. Specifically, TBGAT embeds the DG from a forward and a backward view, respectively, where the messages are propagated by following the different topologies of the views and aggregated via graph attention. Then, we propose a novel operator based on the message-passing mechanism to calculate the forward and backward topological sorts of the DG, which are the features for characterizing the topological structures and exploited by our model. In addition, we theoretically and experimentally show that TBGAT h
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24369;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#30561;&#30496;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#22522;&#20110;&#20256;&#32479;&#30561;&#30496;&#26816;&#27979;&#31639;&#27861;&#29983;&#25104;&#30340;&#24369;&#26631;&#31614;&#38598;&#65292;&#37319;&#29992;&#26032;&#39062;&#30340;&#32479;&#35745;&#27169;&#22411;&#26469;&#26368;&#23567;&#21270;&#36719;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;Brier&#20998;&#25968;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.17601</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#24369;&#26631;&#31614;&#38598;&#24314;&#27169;&#25512;&#36827;&#30561;&#30496;&#26816;&#27979;&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Advancing sleep detection by modelling weak label sets: A novel weakly supervised learning approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17601
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24369;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#30561;&#30496;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#22522;&#20110;&#20256;&#32479;&#30561;&#30496;&#26816;&#27979;&#31639;&#27861;&#29983;&#25104;&#30340;&#24369;&#26631;&#31614;&#38598;&#65292;&#37319;&#29992;&#26032;&#39062;&#30340;&#32479;&#35745;&#27169;&#22411;&#26469;&#26368;&#23567;&#21270;&#36719;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;Brier&#20998;&#25968;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30561;&#30496;&#21644;&#27963;&#21160;&#27169;&#24335;&#30340;&#29702;&#35299;&#22312;&#36523;&#24515;&#20581;&#24247;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#24369;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#30561;&#30496;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#21487;&#38752;&#22320;&#22522;&#20934;&#26631;&#31614;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#32452;&#24369;&#26631;&#31614;&#65292;&#36825;&#20123;&#26631;&#31614;&#26159;&#36890;&#36807;&#20256;&#32479;&#30561;&#30496;&#26816;&#27979;&#31639;&#27861;&#29983;&#25104;&#30340;&#39044;&#27979;&#23548;&#20986;&#30340;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24191;&#20041;&#38750;&#32447;&#24615;&#32479;&#35745;&#27169;&#22411;&#65292;&#20854;&#20013;&#24369;&#30561;&#30496;&#26631;&#31614;&#30340;&#25968;&#37327;&#34987;&#24314;&#27169;&#20026;&#20108;&#39033;&#20998;&#24067;&#30340;&#32467;&#26524;&#12290;&#20108;&#39033;&#20998;&#24067;&#20013;&#30340;&#30561;&#30496;&#27010;&#29575;&#19982;&#22522;&#20110;&#35273;&#37266;&#21058;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#32467;&#26524;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#22823;&#21270;&#27169;&#22411;&#30340;&#20284;&#28982;&#20989;&#25968;&#31561;&#20215;&#20110;&#26368;&#23567;&#21270;&#36719;&#20132;&#21449;&#29109;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;Brier&#20998;&#25968;&#20316;&#20026;&#24369;&#26631;&#31614;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#20351;&#29992;&#12290;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#25928;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17601v1 Announce Type: new  Abstract: Understanding sleep and activity patterns plays a crucial role in physical and mental health. This study introduces a novel approach for sleep detection using weakly supervised learning for scenarios where reliable ground truth labels are unavailable. The proposed method relies on a set of weak labels, derived from the predictions generated by conventional sleep detection algorithms. Introducing a novel approach, we suggest a novel generalised non-linear statistical model in which the number of weak sleep labels is modelled as outcome of a binomial distribution. The probability of sleep in the binomial distribution is linked to the outcomes of neural networks trained to detect sleep based on actigraphy. We show that maximizing the likelihood function of the model, is equivalent to minimizing the soft cross-entropy loss. Additionally, we explored the use of the Brier score as a loss function for weak labels. The efficacy of the suggested 
&lt;/p&gt;</description></item><item><title>DAGnosis&#20351;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;(DAGs)&#26469;&#35299;&#20915;&#25968;&#25454;&#19968;&#33268;&#24615;&#26816;&#27979;&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65292;&#24182;&#33021;&#22815;&#20934;&#30830;&#23450;&#20301;&#20026;&#20309;&#26679;&#26412;&#20250;&#34987;&#26631;&#35760;&#20026;&#19981;&#19968;&#33268;&#12290;</title><link>https://arxiv.org/abs/2402.17599</link><description>&lt;p&gt;
DAGnosis&#65306;&#20351;&#29992;&#32467;&#26500;&#36827;&#34892;&#25968;&#25454;&#19981;&#19968;&#33268;&#24615;&#30340;&#23616;&#37096;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
DAGnosis: Localized Identification of Data Inconsistencies using Structures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17599
&lt;/p&gt;
&lt;p&gt;
DAGnosis&#20351;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;(DAGs)&#26469;&#35299;&#20915;&#25968;&#25454;&#19968;&#33268;&#24615;&#26816;&#27979;&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65292;&#24182;&#33021;&#22815;&#20934;&#30830;&#23450;&#20301;&#20026;&#20309;&#26679;&#26412;&#20250;&#34987;&#26631;&#35760;&#20026;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37096;&#32626;&#26102;&#35782;&#21035;&#21644;&#36866;&#24403;&#22788;&#29702;&#25968;&#25454;&#20013;&#30340;&#19981;&#19968;&#33268;&#24615;&#23545;&#21487;&#38752;&#22320;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#26399;&#30340;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;&#33021;&#22815;&#35782;&#21035;&#19982;&#35757;&#32451;&#38598;&#30456;&#20851;&#30340;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#20294;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65306;&#65288;1&#65289;&#22312;&#29305;&#24449;&#23637;&#29616;&#32479;&#35745;&#29420;&#31435;&#24615;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#20026;&#23427;&#20204;&#20351;&#29992;&#21387;&#32553;&#34920;&#31034;&#65307;&#65288;2&#65289;&#32570;&#20047;&#23616;&#37096;&#21270;&#65292;&#26080;&#27861;&#20934;&#30830;&#23450;&#20301;&#26679;&#26412;&#20026;&#20309;&#34987;&#26631;&#35760;&#20026;&#19981;&#19968;&#33268;&#65292;&#36825;&#23545;&#25351;&#23548;&#26410;&#26469;&#25968;&#25454;&#25910;&#38598;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#20351;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAGs&#65289;&#26469;&#32534;&#30721;&#35757;&#32451;&#38598;&#30340;&#29305;&#24449;&#27010;&#29575;&#20998;&#24067;&#21644;&#29420;&#31435;&#24615;&#20316;&#20026;&#32467;&#26500;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#22522;&#26412;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;DAGnosis&#65292;&#21033;&#29992;&#36825;&#20123;&#32467;&#26500;&#20132;&#20114;&#24102;&#26469;&#26377;&#20215;&#20540;&#30340;&#12289;&#28145;&#21051;&#30340;&#25968;&#25454;&#20013;&#24515;&#32467;&#35770;&#12290;DAGnosis&#35299;&#38145;&#20102;&#22312;DAG&#19978;&#23450;&#20301;&#19981;&#19968;&#33268;&#24615;&#21407;&#22240;&#30340;&#33021;&#21147;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17599v1 Announce Type: cross  Abstract: Identification and appropriate handling of inconsistencies in data at deployment time is crucial to reliably use machine learning models. While recent data-centric methods are able to identify such inconsistencies with respect to the training set, they suffer from two key limitations: (1) suboptimality in settings where features exhibit statistical independencies, due to their usage of compressive representations and (2) lack of localization to pin-point why a sample might be flagged as inconsistent, which is important to guide future data collection. We solve these two fundamental limitations using directed acyclic graphs (DAGs) to encode the training set's features probability distribution and independencies as a structure. Our method, called DAGnosis, leverages these structural interactions to bring valuable and insightful data-centric conclusions. DAGnosis unlocks the localization of the causes of inconsistencies on a DAG, an aspec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#26356;&#29616;&#23454;&#30340;&#31070;&#32463;&#32593;&#32476;&#32972;&#26223;&#19979;&#25506;&#35752;&#20102;&#38544;&#24335;&#27491;&#21017;&#21270;&#29616;&#35937;&#65292;&#36890;&#36807;&#30740;&#31350;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#19968;&#33324;&#31867;&#21035;&#65292;&#20005;&#26684;&#35777;&#26126;&#20102;&#22312;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#35774;&#32622;&#20013;&#36825;&#20123;&#32593;&#32476;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#29616;&#35937;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#36895;&#29575;&#20445;&#35777;&#65292;&#30830;&#20445;&#26799;&#24230;&#30340;&#25351;&#25968;&#32423;&#24555;&#36895;&#25910;&#25947;&#12290;</title><link>https://arxiv.org/abs/2402.17595</link><description>&lt;p&gt;
&#36890;&#36807;&#35889;&#31070;&#32463;&#32593;&#32476;&#21644;&#38750;&#32447;&#24615;&#30697;&#38453;&#24863;&#30693;&#23454;&#29616;&#38544;&#24335;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Implicit Regularization via Spectral Neural Networks and Non-linear Matrix Sensing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#26356;&#29616;&#23454;&#30340;&#31070;&#32463;&#32593;&#32476;&#32972;&#26223;&#19979;&#25506;&#35752;&#20102;&#38544;&#24335;&#27491;&#21017;&#21270;&#29616;&#35937;&#65292;&#36890;&#36807;&#30740;&#31350;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#19968;&#33324;&#31867;&#21035;&#65292;&#20005;&#26684;&#35777;&#26126;&#20102;&#22312;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#35774;&#32622;&#20013;&#36825;&#20123;&#32593;&#32476;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#29616;&#35937;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#36895;&#29575;&#20445;&#35777;&#65292;&#30830;&#20445;&#26799;&#24230;&#30340;&#25351;&#25968;&#32423;&#24555;&#36895;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#27491;&#21017;&#21270;&#29616;&#35937;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20852;&#36259;&#65292;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#20986;&#33394;&#27867;&#21270;&#33021;&#21147;&#30340;&#19968;&#20010;&#22522;&#26412;&#26041;&#38754;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#23427;&#24847;&#21619;&#30528;&#22312;&#35768;&#22810;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#21363;&#20351;&#25439;&#22833;&#20989;&#25968;&#20013;&#27809;&#26377;&#20219;&#20309;&#26174;&#24335;&#27491;&#21017;&#21270;&#22120;&#65292;&#26799;&#24230;&#19979;&#38477;&#21160;&#24577;&#20063;&#20250;&#25910;&#25947;&#21040;&#19968;&#20010;&#27491;&#21017;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#35299;&#12290;&#28982;&#32780;&#65292;&#24050;&#30693;&#30340;&#35797;&#22270;&#20174;&#29702;&#35770;&#19978;&#35299;&#37322;&#36825;&#19968;&#29616;&#35937;&#30340;&#32467;&#26524;&#20027;&#35201;&#38598;&#20013;&#22312;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#30340;&#35774;&#32622;&#19978;&#65292;&#32447;&#24615;&#32467;&#26500;&#30340;&#31616;&#21333;&#24615;&#23545;&#29616;&#26377;&#35770;&#25454;&#29305;&#21035;&#20851;&#38190;&#12290;&#26412;&#25991;&#22312;&#26356;&#29616;&#23454;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#19968;&#33324;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#32972;&#26223;&#19979;&#25506;&#35752;&#20102;&#36825;&#19968;&#38382;&#39064;&#65292;&#24182;&#22312;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#30340;&#35774;&#32622;&#20013;&#20005;&#26684;&#35777;&#26126;&#20102;&#36825;&#20123;&#32593;&#32476;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#29616;&#35937;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#36895;&#29575;&#20445;&#35777;&#65292;&#30830;&#20445;&#26799;&#24230;&#30340;&#25351;&#25968;&#32423;&#24555;&#36895;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17595v1 Announce Type: cross  Abstract: The phenomenon of implicit regularization has attracted interest in recent years as a fundamental aspect of the remarkable generalizing ability of neural networks. In a nutshell, it entails that gradient descent dynamics in many neural nets, even without any explicit regularizer in the loss function, converges to the solution of a regularized learning problem. However, known results attempting to theoretically explain this phenomenon focus overwhelmingly on the setting of linear neural nets, and the simplicity of the linear structure is particularly crucial to existing arguments. In this paper, we explore this problem in the context of more realistic neural networks with a general class of non-linear activation functions, and rigorously demonstrate the implicit regularization phenomenon for such networks in the setting of matrix sensing problems, together with rigorous rate guarantees that ensure exponentially fast convergence of gradi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FaultProfIT&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#22823;&#35268;&#27169;&#20113;&#31995;&#32479;&#20013;&#30340;&#25925;&#38556;&#27169;&#24335;&#20998;&#26512;&#65292;&#22635;&#34917;&#20102;&#25163;&#21160;&#26631;&#35760;&#30340;&#32570;&#38519;&#12290;</title><link>https://arxiv.org/abs/2402.17583</link><description>&lt;p&gt;
FaultProfIT: &#22823;&#35268;&#27169;&#20113;&#31995;&#32479;&#20013;&#25925;&#38556;&#31080;&#25454;&#30340;&#20998;&#23618;&#25925;&#38556;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
FaultProfIT: Hierarchical Fault Profiling of Incident Tickets in Large-scale Cloud Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17583
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FaultProfIT&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#22823;&#35268;&#27169;&#20113;&#31995;&#32479;&#20013;&#30340;&#25925;&#38556;&#27169;&#24335;&#20998;&#26512;&#65292;&#22635;&#34917;&#20102;&#25163;&#21160;&#26631;&#35760;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#21518;&#20998;&#26512;&#22312;&#20113;&#31995;&#32479;&#20013;&#30340;&#20107;&#20214;&#31649;&#29702;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#23427;&#20026;&#25913;&#36827;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#31283;&#20581;&#24615;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#22312;CloudA&#65292;&#25925;&#38556;&#27169;&#24335;&#20998;&#26512;&#26159;&#22312;&#20107;&#21518;&#38454;&#27573;&#25191;&#34892;&#30340;&#65292;&#28041;&#21450;&#23558;&#20107;&#20214;&#25925;&#38556;&#20998;&#31867;&#20026;&#29420;&#29305;&#31867;&#21035;&#65292;&#31216;&#20026;&#25925;&#38556;&#27169;&#24335;&#12290;&#36890;&#36807;&#27719;&#24635;&#21644;&#20998;&#26512;&#36825;&#20123;&#25925;&#38556;&#27169;&#24335;&#65292;&#24037;&#31243;&#24072;&#21487;&#20197;&#35782;&#21035;&#24120;&#35265;&#25925;&#38556;&#12289;&#33030;&#24369;&#32452;&#20214;&#21644;&#26032;&#20986;&#29616;&#30340;&#25925;&#38556;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#36807;&#31243;&#30446;&#21069;&#26159;&#36890;&#36807;&#25163;&#21160;&#26631;&#35760;&#36827;&#34892;&#30340;&#65292;&#23384;&#22312;&#22266;&#26377;&#32570;&#38519;&#12290;&#19968;&#26041;&#38754;&#65292;&#20107;&#20214;&#25968;&#37327;&#24222;&#22823;&#24847;&#21619;&#30528;&#21482;&#20998;&#26512;&#20102;&#26368;&#20005;&#37325;&#30340;&#20107;&#20214;&#65292;&#23548;&#33268;&#23545;&#25925;&#38556;&#27169;&#24335;&#30340;&#20559;&#26012;&#27010;&#36848;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#38656;&#35201;&#24191;&#27867;&#30340;&#39046;&#22495;&#30693;&#35782;&#65292;&#36825;&#23548;&#33268;&#38169;&#35823;&#21644;&#19981;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#21517;&#20026;FaultProfIT&#65292;&#29992;&#20110;&#22788;&#29702;&#25925;&#38556;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17583v1 Announce Type: cross  Abstract: Postmortem analysis is essential in the management of incidents within cloud systems, which provides valuable insights to improve system's reliability and robustness. At CloudA, fault pattern profiling is performed during the postmortem phase, which involves the classification of incidents' faults into unique categories, referred to as fault pattern. By aggregating and analyzing these fault patterns, engineers can discern common faults, vulnerable components and emerging fault trends. However, this process is currently conducted by manual labeling, which has inherent drawbacks. On the one hand, the sheer volume of incidents means only the most severe ones are analyzed, causing a skewed overview of fault patterns. On the other hand, the complexity of the task demands extensive domain knowledge, which leads to errors and inconsistencies. To address these limitations, we propose an automated approach, named FaultProfIT, for Fault pattern 
&lt;/p&gt;</description></item><item><title>&#36229;&#32500;&#35745;&#31639;&#20316;&#20026;&#19968;&#31181;&#26032;&#20852;&#25216;&#26415;&#65292;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#20855;&#26377;&#39640;&#25928;&#12289;&#21487;&#35299;&#37322;&#21644;&#33021;&#22815;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.17572</link><description>&lt;p&gt;
&#36229;&#32500;&#35745;&#31639;&#65306;&#29983;&#29289;&#25968;&#25454;&#30340;&#24555;&#36895;&#12289;&#31283;&#20581;&#21644;&#21487;&#35299;&#37322;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Hyperdimensional computing: a fast, robust and interpretable paradigm for biological data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17572
&lt;/p&gt;
&lt;p&gt;
&#36229;&#32500;&#35745;&#31639;&#20316;&#20026;&#19968;&#31181;&#26032;&#20852;&#25216;&#26415;&#65292;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#20855;&#26377;&#39640;&#25928;&#12289;&#21487;&#35299;&#37322;&#21644;&#33021;&#22815;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#20449;&#24687;&#23398;&#30340;&#36827;&#27493;&#20027;&#35201;&#24402;&#22240;&#20110;&#22788;&#29702;&#22810;&#26679;&#29983;&#29289;&#25968;&#25454;&#28304;&#30340;&#26032;&#31639;&#27861;&#12290;&#39640;&#32423;&#30340;&#24207;&#21015;&#27604;&#23545;&#31639;&#27861;&#24050;&#32463;&#22312;&#20998;&#26512;&#29983;&#29289;&#24207;&#21015;&#26041;&#38754;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#65292;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#22823;&#24133;&#25913;&#21464;&#20102;&#29983;&#29289;&#20449;&#24687;&#23398;&#65292;&#35299;&#20915;&#20102;&#24207;&#21015;&#12289;&#32467;&#26500;&#21644;&#21151;&#33021;&#20998;&#26512;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38750;&#24120;&#20381;&#36182;&#25968;&#25454;&#65292;&#35745;&#31639;&#23494;&#38598;&#19988;&#38590;&#20197;&#35299;&#37322;&#12290;&#36229;&#32500;&#35745;&#31639;&#65288;HDC&#65289;&#26368;&#36817;&#20316;&#20026;&#19968;&#31181;&#26377;&#36259;&#30340;&#26367;&#20195;&#26041;&#26696;&#20986;&#29616;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#39640;&#32500;&#24230;&#30340;&#38543;&#26426;&#21521;&#37327;&#21487;&#20197;&#34920;&#31034;&#35832;&#22914;&#24207;&#21015;&#21516;&#19968;&#24615;&#25110;&#31995;&#32479;&#21457;&#32946;&#31561;&#27010;&#24565;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#39640;&#32500;&#31354;&#38388;&#30340;&#29305;&#27530;&#23646;&#24615;&#65292;&#21487;&#20197;&#20351;&#29992;&#31616;&#21333;&#36816;&#31639;&#31526;&#32452;&#21512;&#36825;&#20123;&#21521;&#37327;&#36827;&#34892;&#23398;&#20064;&#12289;&#25512;&#29702;&#25110;&#26597;&#35810;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23457;&#26597;&#24182;&#25506;&#35752;&#20102;HDC&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#30340;&#28508;&#21147;&#65292;&#24378;&#35843;&#20102;&#20854;&#25928;&#29575;&#12289;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#22788;&#29702;&#22810;&#27169;&#24577;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17572v1 Announce Type: new  Abstract: Advances in bioinformatics are primarily due to new algorithms for processing diverse biological data sources. While sophisticated alignment algorithms have been pivotal in analyzing biological sequences, deep learning has substantially transformed bioinformatics, addressing sequence, structure, and functional analyses. However, these methods are incredibly data-hungry, compute-intensive and hard to interpret. Hyperdimensional computing (HDC) has recently emerged as an intriguing alternative. The key idea is that random vectors of high dimensionality can represent concepts such as sequence identity or phylogeny. These vectors can then be combined using simple operators for learning, reasoning or querying by exploiting the peculiar properties of high-dimensional spaces. Our work reviews and explores the potential of HDC for bioinformatics, emphasizing its efficiency, interpretability, and adeptness in handling multimodal and structured da
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#21464;&#20998;&#21463;&#24178;&#25200;&#22122;&#22768;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26694;&#26550;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#22788;&#29702;&#24322;&#26041;&#24046;&#26041;&#24046;&#21644;&#31163;&#32676;&#22122;&#22768;&#65292;&#24212;&#29992;&#20110;&#22320;&#30913;&#25200;&#21160;&#39044;&#27979;&#65292;&#24182;&#23637;&#31034;&#20102;&#26356;&#30701;&#30340;&#39044;&#27979;&#38388;&#38548;&#21644;&#31867;&#20284;&#30340;&#35206;&#30422;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.17570</link><description>&lt;p&gt;
&#31232;&#30095;&#21464;&#20998;&#21463;&#24178;&#25200;&#22122;&#22768;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#29992;&#20110;&#22320;&#30913;&#25200;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Sparse Variational Contaminated Noise Gaussian Process Regression for Forecasting Geomagnetic Perturbations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#21464;&#20998;&#21463;&#24178;&#25200;&#22122;&#22768;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26694;&#26550;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#22788;&#29702;&#24322;&#26041;&#24046;&#26041;&#24046;&#21644;&#31163;&#32676;&#22122;&#22768;&#65292;&#24212;&#29992;&#20110;&#22320;&#30913;&#25200;&#21160;&#39044;&#27979;&#65292;&#24182;&#23637;&#31034;&#20102;&#26356;&#30701;&#30340;&#39044;&#27979;&#38388;&#38548;&#21644;&#31867;&#20284;&#30340;&#35206;&#30422;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#24050;&#25104;&#20026;&#22788;&#29702;&#22797;&#26434;&#21327;&#26041;&#24046;&#32467;&#26500;&#25968;&#25454;&#38598;&#30340;&#22522;&#20110;&#26680;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;GP&#26694;&#26550;&#25193;&#23637;&#65292;&#20351;&#29992;&#21463;&#24178;&#25200;&#30340;&#27491;&#24577;&#20284;&#28982;&#20989;&#25968;&#26356;&#22909;&#22320;&#32771;&#34385;&#24322;&#26041;&#24046;&#26041;&#24046;&#21644;&#31163;&#32676;&#22122;&#22768;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#31232;&#30095;&#21464;&#20998;&#39640;&#26031;&#36807;&#31243;&#65288;SVGP&#65289;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#25512;&#26029;&#31639;&#27861;&#65292;&#29992;&#20110;&#25311;&#21512;&#20855;&#26377;&#21463;&#24178;&#25200;&#27491;&#24577;&#22122;&#22768;&#30340;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#27169;&#22411;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#32771;&#23519;&#20102;&#22320;&#30913;&#22320;&#38754;&#25200;&#21160;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#27169;&#22411;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#20154;&#24037;&#23494;&#38598;&#30340;&#31070;&#32463;&#32593;&#32476;&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#20102;&#26356;&#30701;&#30340;&#39044;&#27979;&#38388;&#38548;&#65292;&#20294;&#20855;&#26377;&#30456;&#20284;&#30340;&#35206;&#30422;&#33539;&#22260;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17570v1 Announce Type: new  Abstract: Gaussian Processes (GP) have become popular machine learning methods for kernel based learning on datasets with complicated covariance structures. In this paper, we present a novel extension to the GP framework using a contaminated normal likelihood function to better account for heteroscedastic variance and outlier noise. We propose a scalable inference algorithm based on the Sparse Variational Gaussian Process (SVGP) method for fitting sparse Gaussian process regression models with contaminated normal noise on large datasets. We examine an application to geomagnetic ground perturbations, where the state-of-art prediction model is based on neural networks. We show that our approach yields shorter predictions intervals for similar coverage and accuracy when compared to an artificial dense neural network baseline.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#65292;&#20351;&#20915;&#31574;&#32773;&#33021;&#22815;&#26681;&#25454;&#20854;&#21487;&#38752;&#24615;&#26469;&#25509;&#21463;&#25110;&#25298;&#32477;&#39044;&#27979;&#32467;&#26524;</title><link>https://arxiv.org/abs/2402.17554</link><description>&lt;p&gt;
&#35780;&#20272;&#39044;&#27979;&#21487;&#38752;&#24615;&#20197;&#22521;&#20859;&#20154;&#24037;&#26234;&#33021;&#30340;&#20449;&#20219;&#12290;&#22810;&#21457;&#24615;&#30828;&#21270;&#30151;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Predictive Reliability to Foster Trust in Artificial Intelligence. A case study in Multiple Sclerosis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17554
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#65292;&#20351;&#20915;&#31574;&#32773;&#33021;&#22815;&#26681;&#25454;&#20854;&#21487;&#38752;&#24615;&#26469;&#25509;&#21463;&#25110;&#25298;&#32477;&#39044;&#27979;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20851;&#38190;&#32972;&#26223;&#65288;&#22914;&#21307;&#23398;&#65289;&#20013;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#38656;&#35201;&#23454;&#26045;&#23433;&#20840;&#25514;&#26045;&#65292;&#20197;&#38477;&#20302;&#22312;&#39044;&#27979;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#36896;&#25104;&#30340;&#20260;&#23475;&#39118;&#38505;&#12290;&#24403;ML&#39044;&#27979;&#29992;&#20110;&#25351;&#23548;&#20020;&#24202;&#20915;&#31574;&#26102;&#65292;&#21457;&#29616;ML&#22833;&#36133;&#33267;&#20851;&#37325;&#35201;&#12290;ML&#39044;&#27979;&#21487;&#38752;&#24615;&#34913;&#37327;&#20102;ML&#39044;&#27979;&#22312;&#26032;&#31034;&#20363;&#19978;&#30340;&#20449;&#20219;&#24230;&#65292;&#20174;&#32780;&#20351;&#20915;&#31574;&#32773;&#33021;&#22815;&#26681;&#25454;&#20854;&#21487;&#38752;&#24615;&#25509;&#21463;&#25110;&#25298;&#32477;&#12290;&#20026;&#20102;&#35780;&#20272;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#20004;&#20010;&#21407;&#21017;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#35780;&#20272;&#35201;&#20998;&#31867;&#30340;&#31034;&#20363;&#26159;&#21542;&#26469;&#33258;&#35757;&#32451;&#38598;&#30340;&#30456;&#21516;&#20998;&#24067;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;AE&#65289;&#37325;&#24314;&#20855;&#26377;&#20302;&#35823;&#24046;&#30340;&#35757;&#32451;&#38598;&#30340;&#33021;&#21147;&#12290;&#22914;&#26524;AE&#23558;&#31034;&#20363;&#37325;&#26500;&#20026;&#39640;&#35823;&#24046;&#65292;&#21017;&#35748;&#20026;&#35813;&#31034;&#20363;&#26159;&#8220;&#20998;&#24067;&#22806;&#8221;&#65288;OOD&#65289;&#12290;&#20854;&#27425;&#65292;&#35780;&#20272;ML&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#26159;&#21542;&#33391;&#22909;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17554v1 Announce Type: new  Abstract: Applying Artificial Intelligence (AI) and Machine Learning (ML) in critical contexts, such as medicine, requires the implementation of safety measures to reduce risks of harm in case of prediction errors. Spotting ML failures is of paramount importance when ML predictions are used to drive clinical decisions. ML predictive reliability measures the degree of trust of a ML prediction on a new instance, thus allowing decision-makers to accept or reject it based on its reliability. To assess reliability, we propose a method that implements two principles. First, our approach evaluates whether an instance to be classified is coming from the same distribution of the training set. To do this, we leverage Autoencoders (AEs) ability to reconstruct the training set with low error. An instance is considered Out-of-Distribution (OOD) if the AE reconstructs it with a high error. Second, it is evaluated whether the ML classifier has good performances 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#21487;&#35843;&#21464;&#25442;&#30340;&#23398;&#20064;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21069;&#21518;&#28388;&#27874;&#22120;&#25552;&#39640;&#23631;&#24149;&#20869;&#23481;&#21387;&#32553;&#25928;&#29575;&#24182;&#20943;&#23569;&#32534;&#30721;&#20266;&#24433;&#65292;&#27604;&#22522;&#32447;&#27169;&#22411;&#33410;&#30465;10%&#27604;&#29305;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.17544</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#35843;&#21464;&#25442;&#23558;&#23398;&#20064;&#30340;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;&#36866;&#24212;&#23631;&#24149;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Adapting Learned Image Codecs to Screen Content via Adjustable Transformations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17544
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#21487;&#35843;&#21464;&#25442;&#30340;&#23398;&#20064;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21069;&#21518;&#28388;&#27874;&#22120;&#25552;&#39640;&#23631;&#24149;&#20869;&#23481;&#21387;&#32553;&#25928;&#29575;&#24182;&#20943;&#23569;&#32534;&#30721;&#20266;&#24433;&#65292;&#27604;&#22522;&#32447;&#27169;&#22411;&#33410;&#30465;10%&#27604;&#29305;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23398;&#20064;&#30340;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;&#65288;LICs&#65289;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#23427;&#20204;&#23545;&#20110;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#30340;&#20302;&#32534;&#30721;&#25928;&#29575;&#25104;&#20026;&#26576;&#20123;&#24212;&#29992;&#30340;&#29942;&#39048;&#12290;&#20026;&#20102;&#25552;&#39640;LICs&#22312;&#23631;&#24149;&#20869;&#23481;&#65288;SC&#65289;&#22270;&#20687;&#19978;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#21448;&#19981;&#30772;&#22351;&#21521;&#21518;&#20860;&#23481;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#24341;&#20837;&#21442;&#25968;&#21270;&#21644;&#21487;&#36870;&#32447;&#24615;&#21464;&#25442;&#21040;&#32534;&#30721;&#31649;&#32447;&#20013;&#65292;&#32780;&#19981;&#25913;&#21464;&#22522;&#32447;&#32534;&#35299;&#30721;&#22120;&#30340;&#25805;&#20316;&#27969;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#25105;&#20204;&#35774;&#32622;&#20013;&#30340;&#21069;&#32622;&#28388;&#27874;&#22120;&#21644;&#21518;&#32622;&#28388;&#27874;&#22120;&#65292;&#20197;&#22686;&#21152;&#32534;&#30721;&#25928;&#29575;&#24182;&#24110;&#21161;&#24674;&#22797;&#32534;&#30721;&#20266;&#24433;&#12290;&#25105;&#20204;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#35299;&#20915;&#26041;&#26696;&#22312;SC&#21387;&#32553;&#19978;&#30456;&#27604;&#22522;&#32447;LICs&#23454;&#29616;&#39640;&#36798;10%&#30340;&#27604;&#29305;&#29575;&#33410;&#30465;&#65292;&#21516;&#26102;&#21482;&#24341;&#20837;1%&#30340;&#39069;&#22806;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17544v1 Announce Type: cross  Abstract: As learned image codecs (LICs) become more prevalent, their low coding efficiency for out-of-distribution data becomes a bottleneck for some applications. To improve the performance of LICs for screen content (SC) images without breaking backwards compatibility, we propose to introduce parameterized and invertible linear transformations into the coding pipeline without changing the underlying baseline codec's operation flow. We design two neural networks to act as prefilters and postfilters in our setup to increase the coding efficiency and help with the recovery from coding artifacts. Our end-to-end trained solution achieves up to 10% bitrate savings on SC compression compared to the baseline LICs while introducing only 1% extra parameters.
&lt;/p&gt;</description></item><item><title>Transition-aware weighted Denoising Score Matching&#65288;TDSM&#65289;&#26159;&#29992;&#20110;&#35757;&#32451;&#24102;&#26377;&#22024;&#26434;&#26631;&#31614;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;&#24471;&#20998;&#32593;&#32476;&#21644;&#36807;&#28193;&#27010;&#29575;&#26469;&#25552;&#39640;&#29983;&#25104;&#26679;&#26412;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.17517</link><description>&lt;p&gt;
&#26631;&#31614;&#22122;&#22768;&#40065;&#26834;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Label-Noise Robust Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17517
&lt;/p&gt;
&lt;p&gt;
Transition-aware weighted Denoising Score Matching&#65288;TDSM&#65289;&#26159;&#29992;&#20110;&#35757;&#32451;&#24102;&#26377;&#22024;&#26434;&#26631;&#31614;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;&#24471;&#20998;&#32593;&#32476;&#21644;&#36807;&#28193;&#27010;&#29575;&#26469;&#25552;&#39640;&#29983;&#25104;&#26679;&#26412;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#22312;&#21508;&#31181;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#35757;&#32451;&#23427;&#20204;&#38656;&#35201;&#21253;&#21547;&#26377;&#22122;&#22768;&#30340;&#26465;&#20214;&#36755;&#20837;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#21363;&#22024;&#26434;&#26631;&#31614;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#35757;&#32451;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;Transition-aware weighted Denoising Score Matching (TDSM)&#65292;&#36825;&#26159;&#25193;&#25955;&#27169;&#22411;&#39046;&#22495;&#20013;&#30340;&#39318;&#27425;&#30740;&#31350;&#12290;TDSM&#30446;&#26631;&#21253;&#21547;&#24471;&#20998;&#32593;&#32476;&#30340;&#21152;&#26435;&#21644;&#65292;&#32467;&#21512;&#20102;&#23454;&#20363;&#32423;&#21644;&#26102;&#38388;&#30456;&#20851;&#30340;&#26631;&#31614;&#36716;&#31227;&#27010;&#29575;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36807;&#28193;&#24863;&#30693;&#30340;&#26435;&#37325;&#20272;&#35745;&#22120;&#65292;&#21033;&#29992;&#20102;&#19968;&#20010;&#19982;&#25193;&#25955;&#36807;&#31243;&#26126;&#26174;&#23450;&#21046;&#21270;&#30340;&#26102;&#38388;&#30456;&#20851;&#22024;&#26434;&#26631;&#31614;&#20998;&#31867;&#22120;&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#22024;&#26434;&#26631;&#31614;&#35774;&#32622;&#19978;&#30340;&#23454;&#39564;&#65292;TDSM&#25552;&#39640;&#20102;&#29983;&#25104;&#30340;&#26679;&#26412;&#36136;&#37327;&#19982;&#32473;&#23450;&#26465;&#20214;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17517v1 Announce Type: new  Abstract: Conditional diffusion models have shown remarkable performance in various generative tasks, but training them requires large-scale datasets that often contain noise in conditional inputs, a.k.a. noisy labels. This noise leads to condition mismatch and quality degradation of generated data. This paper proposes Transition-aware weighted Denoising Score Matching (TDSM) for training conditional diffusion models with noisy labels, which is the first study in the line of diffusion models. The TDSM objective contains a weighted sum of score networks, incorporating instance-wise and time-dependent label transition probabilities. We introduce a transition-aware weight estimator, which leverages a time-dependent noisy-label classifier distinctively customized to the diffusion process. Through experiments across various datasets and noisy label settings, TDSM improves the quality of generated samples aligned with given conditions. Furthermore, our 
&lt;/p&gt;</description></item><item><title>QUCE&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#36335;&#24452;&#19981;&#30830;&#23450;&#24615;&#26469;&#37327;&#21270;&#21644;&#32531;&#35299;&#22522;&#20110;&#36335;&#24452;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#25913;&#21892;&#23545;&#25239;&#24615;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.17516</link><description>&lt;p&gt;
QUCE: &#20943;&#23569;&#21644;&#37327;&#21270;&#22522;&#20110;&#36335;&#24452;&#30340;&#19981;&#30830;&#23450;&#24615;&#20197;&#29983;&#25104;&#23545;&#25239;&#24615;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
QUCE: The Minimisation and Quantification of Path-Based Uncertainty for Generative Counterfactual Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17516
&lt;/p&gt;
&lt;p&gt;
QUCE&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#36335;&#24452;&#19981;&#30830;&#23450;&#24615;&#26469;&#37327;&#21270;&#21644;&#32531;&#35299;&#22522;&#20110;&#36335;&#24452;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#25913;&#21892;&#23545;&#25239;&#24615;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17516v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#23398;&#31185; &#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#26368;&#31361;&#20986;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;DNNs&#30340;&#26377;&#25928;&#24615;&#38543;&#30528;&#26368;&#36817;&#35745;&#31639;&#33021;&#21147;&#30340;&#22686;&#21152;&#32780;&#28608;&#22686;&#65292;&#20351;&#24471;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#25193;&#23637;&#21040;&#22788;&#29702;&#22823;&#25968;&#25454;&#20013;&#30340;&#37325;&#35201;&#22797;&#26434;&#24615;&#20197;&#24212;&#23545;&#39044;&#27979;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;DNN&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#25552;&#39640;&#65292;&#21487;&#35299;&#37322;&#24615;&#38477;&#20302;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#35832;&#22914;&#23545;&#25239;&#26799;&#24230;&#25972;&#21512;&#65288;AGI&#65289;&#36825;&#26679;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#21033;&#29992;DNN&#25552;&#20379;&#30340;&#22522;&#20110;&#36335;&#24452;&#30340;&#26799;&#24230;&#26469;&#38416;&#26126;&#23427;&#20204;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#24403;&#26799;&#24230;&#22312;&#36234;&#30028;&#36335;&#24452;&#36941;&#21382;&#26399;&#38388;&#34920;&#29616;&#20986;&#19981;&#35268;&#21017;&#24615;&#26102;&#65292;&#22522;&#20110;&#36335;&#24452;&#30340;&#35299;&#37322;&#22120;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#21463;&#21040;&#25439;&#23475;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Quantified Uncertainty Counterfactual Explanations&#65288;QUCE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#20943;&#23569;&#36335;&#24452;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#36234;&#30028;&#36941;&#21382;&#12290; QUCE&#19981;&#20165;&#22312;&#25552;&#20986;&#35299;&#37322;&#26102;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17516v1 Announce Type: cross  Abstract: Deep Neural Networks (DNNs) stand out as one of the most prominent approaches within the Machine Learning (ML) domain. The efficacy of DNNs has surged alongside recent increases in computational capacity, allowing these approaches to scale to significant complexities for addressing predictive challenges in big data. However, as the complexity of DNN models rises, interpretability diminishes. In response to this challenge, explainable models such as Adversarial Gradient Integration (AGI) leverage path-based gradients provided by DNNs to elucidate their decisions. Yet the performance of path-based explainers can be compromised when gradients exhibit irregularities during out-of-distribution path traversal. In this context, we introduce Quantified Uncertainty Counterfactual Explanations (QUCE), a method designed to mitigate out-of-distribution traversal by minimizing path uncertainty. QUCE not only quantifies uncertainty when presenting e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28909;&#21147;&#23398;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#26469;&#25552;&#39640;&#29289;&#29702;&#31995;&#32479;&#27979;&#37327;&#20998;&#36776;&#29575;&#24182;&#39044;&#27979;&#26102;&#38388;&#28436;&#21270;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#23545;&#25239;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#32467;&#26500;&#20445;&#25345;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#21487;&#26377;&#25928;&#35299;&#20915;&#36229;&#20998;&#36776;&#29575;&#38382;&#39064;&#65292;&#24182;&#30830;&#20445;&#28385;&#36275;&#28909;&#21147;&#23398;&#23450;&#24459;&#12290;</title><link>https://arxiv.org/abs/2402.17506</link><description>&lt;p&gt;
&#28909;&#21147;&#23398;&#23548;&#21521;&#31232;&#32570;&#26102;&#38388;&#21160;&#24577;&#25968;&#25454;&#30340;&#36229;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
Thermodynamics-informed super-resolution of scarce temporal dynamics data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17506
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28909;&#21147;&#23398;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#26469;&#25552;&#39640;&#29289;&#29702;&#31995;&#32479;&#27979;&#37327;&#20998;&#36776;&#29575;&#24182;&#39044;&#27979;&#26102;&#38388;&#28436;&#21270;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#23545;&#25239;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#32467;&#26500;&#20445;&#25345;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#21487;&#26377;&#25928;&#35299;&#20915;&#36229;&#20998;&#36776;&#29575;&#38382;&#39064;&#65292;&#24182;&#30830;&#20445;&#28385;&#36275;&#28909;&#21147;&#23398;&#23450;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20102;&#28909;&#21147;&#23398;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#26469;&#25552;&#39640;&#29289;&#29702;&#31995;&#32479;&#27979;&#37327;&#20998;&#36776;&#29575;&#24182;&#38543;&#21518;&#39044;&#27979;&#20854;&#26102;&#38388;&#28436;&#21270;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#23545;&#25239;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#23558;&#23436;&#25972;&#27169;&#22411;&#30340;&#32500;&#24230;&#38477;&#20302;&#21040;&#19968;&#32452;&#28508;&#21464;&#37327;&#65292;&#36825;&#20123;&#28508;&#21464;&#37327;&#34987;&#24378;&#21046;&#21305;&#37197;&#20808;&#39564;&#65292;&#20363;&#22914;&#27491;&#24577;&#20998;&#24067;&#12290;&#23545;&#25239;&#33258;&#21160;&#32534;&#30721;&#22120;&#34987;&#35270;&#20026;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#20204;&#21487;&#20197;&#34987;&#35757;&#32451;&#20197;&#20174;&#20302;&#20998;&#36776;&#29575;&#36755;&#20837;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#26679;&#26412;&#65292;&#20063;&#23601;&#26159;&#21487;&#20197;&#35299;&#20915;&#25152;&#35859;&#30340;&#36229;&#20998;&#36776;&#29575;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#31532;&#20108;&#20010;&#31070;&#32463;&#32593;&#32476;&#34987;&#35757;&#32451;&#20197;&#23398;&#20064;&#28508;&#21464;&#37327;&#30340;&#29289;&#29702;&#32467;&#26500;&#24182;&#39044;&#27979;&#20854;&#26102;&#38388;&#28436;&#21270;&#12290;&#36825;&#20010;&#31070;&#32463;&#32593;&#32476;&#34987;&#31216;&#20026;&#20445;&#25345;&#32467;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#23427;&#23398;&#20064;&#31995;&#32479;&#30340;metriplectic&#32467;&#26500;&#65292;&#24182;&#24212;&#29992;&#29289;&#29702;&#20559;&#24046;&#20197;&#30830;&#20445;&#28909;&#21147;&#23398;&#30340;&#31532;&#19968;&#21644;&#31532;&#20108;&#23450;&#24459;&#34987;&#36981;&#23432;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17506v1 Announce Type: cross  Abstract: We present a method to increase the resolution of measurements of a physical system and subsequently predict its time evolution using thermodynamics-aware neural networks. Our method uses adversarial autoencoders, which reduce the dimensionality of the full order model to a set of latent variables that are enforced to match a prior, for example a normal distribution. Adversarial autoencoders are seen as generative models, and they can be trained to generate high-resolution samples from low-resoution inputs, meaning they can address the so-called super-resolution problem. Then, a second neural network is trained to learn the physical structure of the latent variables and predict their temporal evolution. This neural network is known as an structure-preserving neural network. It learns the metriplectic-structure of the system and applies a physical bias to ensure that the first and second principles of thermodynamics are fulfilled. The i
&lt;/p&gt;</description></item><item><title>&#23558;&#21307;&#30103;&#20445;&#20581;&#35270;&#20026;&#24207;&#21015;&#24314;&#27169;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#24739;&#32773;&#19982;&#21307;&#30103;&#25552;&#20379;&#32773;&#20043;&#38388;&#30340;&#20132;&#20114;&#34920;&#31034;&#20026;&#20107;&#20214;&#27969;&#65292;&#23454;&#29616;&#23545;&#26410;&#26469;&#20107;&#20214;&#65288;&#22914;&#35786;&#26029;&#21644;&#27835;&#30103;&#36873;&#25321;&#65289;&#36827;&#34892;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.17501</link><description>&lt;p&gt;
&#37325;&#30151;&#30417;&#25252;&#20316;&#20026;&#19968;&#20010;&#22823;&#22411;&#24207;&#21015;&#24314;&#27169;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Intensive Care as One Big Sequence Modeling Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17501
&lt;/p&gt;
&lt;p&gt;
&#23558;&#21307;&#30103;&#20445;&#20581;&#35270;&#20026;&#24207;&#21015;&#24314;&#27169;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#24739;&#32773;&#19982;&#21307;&#30103;&#25552;&#20379;&#32773;&#20043;&#38388;&#30340;&#20132;&#20114;&#34920;&#31034;&#20026;&#20107;&#20214;&#27969;&#65292;&#23454;&#29616;&#23545;&#26410;&#26469;&#20107;&#20214;&#65288;&#22914;&#35786;&#26029;&#21644;&#27835;&#30103;&#36873;&#25321;&#65289;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#36890;&#24120;&#28041;&#21450;&#29421;&#31364;&#30340;&#33258;&#21253;&#21547;&#20219;&#21153;&#65292;&#22914;&#33043;&#27602;&#30151;&#39044;&#27979;&#25110;&#40635;&#37257;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#29992;&#27169;&#22411;&#65288;&#20027;&#35201;&#31034;&#20363;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#20855;&#26377;&#36229;&#36234;&#29305;&#23450;&#20219;&#21153;&#26041;&#27861;&#30340;&#28508;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#38544;&#24335;&#36801;&#31227;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#20445;&#20581;&#22522;&#30784;&#27169;&#22411;&#30340;&#35757;&#32451;&#20197;&#21450;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;Transformer&#26550;&#26500;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20445;&#20581;&#20316;&#20026;&#24207;&#21015;&#24314;&#27169;&#30340;&#33539;&#24335;&#65292;&#20854;&#20013;&#24739;&#32773;&#21644;&#21307;&#30103;&#25552;&#20379;&#32773;&#20043;&#38388;&#30340;&#20132;&#20114;&#34987;&#34920;&#31034;&#20026;&#20107;&#20214;&#27969;&#65292;&#35786;&#26029;&#21644;&#27835;&#30103;&#36873;&#25321;&#31561;&#20219;&#21153;&#34987;&#24314;&#27169;&#20026;&#23545;&#27969;&#20013;&#26410;&#26469;&#20107;&#20214;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#22312;&#23454;&#39564;&#20013;&#25506;&#32034;&#36825;&#19968;&#33539;&#24335;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;MIMIC-SEQ&#65292;&#36825;&#26159;&#19968;&#20010;&#24207;&#21015;&#24314;&#27169;&#22522;&#20934;&#65292;&#36890;&#36807;&#23558;&#26469;&#33258;MIMIC-IV&#25968;&#25454;&#38598;&#30340;&#24322;&#26500;&#20020;&#24202;&#35760;&#24405;&#36716;&#25442;&#20026;&#19968;&#31181;&#32479;&#19968;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17501v1 Announce Type: cross  Abstract: Reinforcement Learning in Healthcare is typically concerned with narrow self-contained tasks such as sepsis prediction or anesthesia control. However, previous research has demonstrated the potential of generalist models (the prime example being Large Language Models) to outperform task-specific approaches due to their capability for implicit transfer learning. To enable training of foundation models for Healthcare as well as leverage the capabilities of state of the art Transformer architectures, we propose the paradigm of Healthcare as Sequence Modeling, in which interaction between the patient and the healthcare provider is represented as an event stream and tasks like diagnosis and treatment selection are modeled as prediction of future events in the stream. To explore this paradigm experimentally we develop MIMIC-SEQ, a sequence modeling benchmark derived by translating heterogenous clinical records from MIMIC-IV dataset into a un
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#65292;&#23545;&#20110;&#25391;&#33633;&#31995;&#32479;&#30340;&#21516;&#27493;&#31283;&#23450;&#24615;&#39044;&#27979;&#65292;&#32593;&#32476;&#29305;&#24449;&#26080;&#27861;&#21487;&#38752;&#22320;&#36827;&#34892;&#39044;&#27979;&#65292;&#21482;&#26377;&#36890;&#36807;&#32467;&#21512;&#25152;&#26377;&#32593;&#32476;&#29305;&#24449;&#21644;&#33410;&#28857;&#26426;&#22120;&#23398;&#20064;&#25165;&#33021;&#21305;&#25932;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.17500</link><description>&lt;p&gt;
&#39044;&#27979;&#22797;&#26434;&#25391;&#33633;&#22120;&#32593;&#32476;&#20013;&#30340;&#19981;&#31283;&#23450;&#24615;: &#32593;&#32476;&#29305;&#24449;&#19982;&#26426;&#22120;&#23398;&#20064;&#30340;&#23616;&#38480;&#24615;&#19982;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Predicting Instability in Complex Oscillator Networks: Limitations and Potentials of Network Measures and Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17500
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#65292;&#23545;&#20110;&#25391;&#33633;&#31995;&#32479;&#30340;&#21516;&#27493;&#31283;&#23450;&#24615;&#39044;&#27979;&#65292;&#32593;&#32476;&#29305;&#24449;&#26080;&#27861;&#21487;&#38752;&#22320;&#36827;&#34892;&#39044;&#27979;&#65292;&#21482;&#26377;&#36890;&#36807;&#32467;&#21512;&#25152;&#26377;&#32593;&#32476;&#29305;&#24449;&#21644;&#33410;&#28857;&#26426;&#22120;&#23398;&#20064;&#25165;&#33021;&#21305;&#25932;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#31185;&#23398;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#31995;&#32479;&#30340;&#21151;&#33021;&#29305;&#24615;&#22914;&#20309;&#20174;&#20854;&#32467;&#26500;&#20013;&#20135;&#29983;&#12290;&#23545;&#20110;&#32593;&#32476;&#21160;&#21147;&#31995;&#32479;&#65292;&#32467;&#26500;&#36890;&#24120;&#29992;&#32593;&#32476;&#29305;&#24449;&#26469;&#37327;&#21270;&#12290;&#23545;&#20110;&#25391;&#33633;&#31995;&#32479;&#32780;&#35328;&#65292;&#19968;&#20010;&#29702;&#35770;&#19978;&#21644;&#23454;&#38469;&#19978;&#24863;&#20852;&#36259;&#30340;&#21151;&#33021;&#29305;&#24615;&#26159;&#21516;&#27493;&#31283;&#23450;&#24615;&#23545;&#23616;&#37096;&#25200;&#21160;&#30340;&#21709;&#24212;&#12290;&#26368;&#36817;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#34987;&#35777;&#26126;&#25104;&#21151;&#22320;&#39044;&#27979;&#36825;&#31181;&#31283;&#23450;&#24615;; &#19982;&#27492;&#21516;&#26102;&#65292;&#32593;&#32476;&#29305;&#24449;&#21364;&#38590;&#20197;&#25551;&#32472;&#20986;&#28165;&#26224;&#30340;&#22270;&#26223;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;46&#20010;&#30456;&#20851;&#30340;&#32593;&#32476;&#29305;&#24449;&#65292;&#21457;&#29616;&#27809;&#26377;&#20219;&#20309;&#23567;&#30340;&#23376;&#38598;&#33021;&#22815;&#21487;&#38752;&#22320;&#39044;&#27979;&#31283;&#23450;&#24615;&#12290;GNNs&#30340;&#24615;&#33021;&#21482;&#26377;&#36890;&#36807;&#32467;&#21512;&#25152;&#26377;&#32593;&#32476;&#29305;&#24449;&#21644;&#33410;&#28857;&#26426;&#22120;&#23398;&#20064;&#25165;&#33021;&#21305;&#25932;&#12290;&#28982;&#32780;&#65292;&#19982;GNNs&#19981;&#21516;&#65292;&#36825;&#31181;&#26041;&#27861;&#26080;&#27861;&#20174;&#32593;&#32476;&#38598;&#21512;&#25512;&#24191;&#21040;&#20960;&#31181;&#30495;&#23454;&#30340;&#30005;&#21147;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#12290;&#36825;&#34920;&#26126;&#32593;&#32476;&#29305;&#24449;&#19982;&#21151;&#33021;&#30340;&#30456;&#20851;&#24615;&#21487;&#33021;&#20855;&#26377;&#35823;&#23548;&#24615;&#65292;&#32780;GNNs
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17500v1 Announce Type: cross  Abstract: A central question of network science is how functional properties of systems arise from their structure. For networked dynamical systems, structure is typically quantified with network measures. A functional property that is of theoretical and practical interest for oscillatory systems is the stability of synchrony to localized perturbations. Recently, Graph Neural Networks (GNNs) have been shown to predict this stability successfully; at the same time, network measures have struggled to paint a clear picture. Here we collect 46 relevant network measures and find that no small subset can reliably predict stability. The performance of GNNs can only be matched by combining all network measures and nodewise machine learning. However, unlike GNNs, this approach fails to extrapolate from network ensembles to several real power grid topologies. This suggests that correlations of network measures and function may be misleading, and that GNNs
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31526;&#21495;&#22238;&#24402;&#33719;&#24471;&#20102;&#31616;&#21333;&#30340;&#35299;&#26512;&#36924;&#36817;&#65292;&#37325;&#26032;&#20248;&#21270;&#20102;halofit&#30340;&#31995;&#25968;&#20197;&#25311;&#21512;&#21508;&#31181;&#23431;&#23449;&#23398;&#21644;&#32418;&#31227;&#33539;&#22260;&#65292;&#21033;&#29992;&#31526;&#21495;&#22238;&#24402;&#25506;&#32034;&#20102;&#29992;&#20110;&#25311;&#21512;&#27531;&#24046;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#31354;&#38388;&#65292;&#25152;&#26377;&#26041;&#27861;&#22343;&#32463;&#36807;$N$&#20307;&#27169;&#25311;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.17492</link><description>&lt;p&gt;
syren-halofit: &#19968;&#31181;&#24555;&#36895;&#12289;&#21487;&#35299;&#37322;&#12289;&#39640;&#31934;&#24230;&#30340;$\Lambda$CDM&#38750;&#32447;&#24615;&#29289;&#36136;&#21151;&#29575;&#35889;&#20844;&#24335;
&lt;/p&gt;
&lt;p&gt;
syren-halofit: A fast, interpretable, high-precision formula for the $\Lambda$CDM nonlinear matter power spectrum
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17492
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31526;&#21495;&#22238;&#24402;&#33719;&#24471;&#20102;&#31616;&#21333;&#30340;&#35299;&#26512;&#36924;&#36817;&#65292;&#37325;&#26032;&#20248;&#21270;&#20102;halofit&#30340;&#31995;&#25968;&#20197;&#25311;&#21512;&#21508;&#31181;&#23431;&#23449;&#23398;&#21644;&#32418;&#31227;&#33539;&#22260;&#65292;&#21033;&#29992;&#31526;&#21495;&#22238;&#24402;&#25506;&#32034;&#20102;&#29992;&#20110;&#25311;&#21512;&#27531;&#24046;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#31354;&#38388;&#65292;&#25152;&#26377;&#26041;&#27861;&#22343;&#32463;&#36807;$N$&#20307;&#27169;&#25311;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23431;&#23449;&#23398;&#20013;&#65292;&#24555;&#36895;&#20934;&#30830;&#22320;&#35780;&#20272;&#38750;&#32447;&#24615;&#29289;&#36136;&#21151;&#29575;&#35889;$P(k)$&#20851;&#20110;&#23431;&#23449;&#23398;&#21442;&#25968;&#21644;&#32418;&#31227;&#30340;&#20989;&#25968;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#31526;&#21495;&#22238;&#24402;&#33719;&#24471;&#20102;&#20851;&#20110;halofit&#27169;&#22411;&#25152;&#38656;&#30340;&#38750;&#32447;&#24615;&#23610;&#24230;$k_\sigma$&#12289;&#26377;&#25928;&#35889;&#25351;&#25968;$n_{\rm eff}$&#21644;&#26354;&#29575;$C$&#30340;&#31616;&#21333;&#35299;&#26512;&#36924;&#36817;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37325;&#26032;&#20248;&#21270;halofit&#30340;&#31995;&#25968;&#20197;&#36866;&#24212;&#24191;&#27867;&#30340;&#23431;&#23449;&#23398;&#21644;&#32418;&#31227;&#33539;&#22260;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#20877;&#27425;&#21033;&#29992;&#31526;&#21495;&#22238;&#24402;&#26469;&#25506;&#32034;&#29992;&#20110;&#25311;&#21512;$P(k)$&#19982;halofit&#20248;&#21270;&#39044;&#27979;&#20043;&#38388;&#27531;&#24046;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#31354;&#38388;&#12290;&#25152;&#26377;&#26041;&#27861;&#37117;&#32463;&#36807;&#19982;$N$&#20307;&#27169;&#25311;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17492v1 Announce Type: cross  Abstract: Rapid and accurate evaluation of the nonlinear matter power spectrum, $P(k)$, as a function of cosmological parameters and redshift is of fundamental importance in cosmology. Analytic approximations provide an interpretable solution, yet current approximations are neither fast nor accurate relative to black-box numerical emulators. We use symbolic regression to obtain simple analytic approximations to the nonlinear scale, $k_\sigma$, the effective spectral index, $n_{\rm eff}$, and the curvature, $C$, which are required for the halofit model. We then re-optimise the coefficients of halofit to fit a wide range of cosmologies and redshifts. We then again exploit symbolic regression to explore the space of analytic expressions to fit the residuals between $P(k)$ and the optimised predictions of halofit. All methods are validated against $N$-body simulations. Our symbolic expressions for $k_\sigma$, $n_{\rm eff}$ and $C$ have root mean squ
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23545;JPEG-AI&#39564;&#35777;&#27169;&#22411;&#20013;&#30340;&#27604;&#29305;&#29575;&#21305;&#37197;&#31639;&#27861;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#21644;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.17487</link><description>&lt;p&gt;
JPEG-AI&#39564;&#35777;&#27169;&#22411;&#20013;&#27604;&#29305;&#29575;&#21305;&#37197;&#31639;&#27861;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Bit Rate Matching Algorithm Optimization in JPEG-AI Verification Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17487
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23545;JPEG-AI&#39564;&#35777;&#27169;&#22411;&#20013;&#30340;&#27604;&#29305;&#29575;&#21305;&#37197;&#31639;&#27861;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;&#22270;&#20687;&#21387;&#32553;&#30740;&#31350;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#19982;&#20256;&#32479;&#21387;&#32553;&#26694;&#26550;&#30456;&#27604;&#26377;&#30528;&#26356;&#32039;&#20945;&#30340;&#27604;&#29305;&#34920;&#31034;&#65292;&#24182;&#22312;&#24182;&#34892;&#35774;&#22791;&#19978;&#23454;&#29616;&#26356;&#24555;&#30340;&#32534;&#30721;&#36895;&#24230;&#12290;&#36825;&#20123;&#29305;&#24615;&#24341;&#36215;&#20102;&#31185;&#23398;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#20851;&#27880;&#65292;&#23548;&#33268;&#20102;JPEG-AI&#26631;&#20934;&#21270;&#27963;&#21160;&#30340;&#23637;&#24320;&#12290;JPEG-AI&#26631;&#20934;&#21270;&#36807;&#31243;&#30340;&#39564;&#35777;&#27169;&#22411;&#24050;&#32463;&#22312;&#24320;&#21457;&#20013;&#65292;&#24182;&#19988;&#24050;&#36229;&#36234;&#20808;&#36827;&#30340;VVC intra&#32534;&#35299;&#30721;&#22120;&#12290;&#20026;&#20102;&#29983;&#25104;&#24102;&#26377;&#25152;&#38656;&#27599;&#20687;&#32032;&#20301;&#25968;&#30340;&#37325;&#24314;&#22270;&#20687;&#65292;&#24182;&#35780;&#20272;JPEG-AI&#39564;&#35777;&#27169;&#22411;&#21644;VVC intra&#30340;BD-&#29575;&#24615;&#33021;&#65292;&#37319;&#29992;&#20102;&#27604;&#29305;&#29575;&#21305;&#37197;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;JPEG-AI&#39564;&#35777;&#27169;&#22411;&#22788;&#20110;&#27604;&#29305;&#29575;&#21305;&#37197;&#36807;&#31243;&#20013;&#32463;&#21382;&#20102;&#26174;&#30528;&#30340;&#20943;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17487v1 Announce Type: cross  Abstract: The research on neural network (NN) based image compression has shown superior performance compared to classical compression frameworks. Unlike the hand-engineered transforms in the classical frameworks, NN-based models learn the non-linear transforms providing more compact bit representations, and achieve faster coding speed on parallel devices over their classical counterparts. Those properties evoked the attention of both scientific and industrial communities, resulting in the standardization activity JPEG-AI. The verification model for the standardization process of JPEG-AI is already in development and has surpassed the advanced VVC intra codec. To generate reconstructed images with the desired bits per pixel and assess the BD-rate performance of both the JPEG-AI verification model and VVC intra, bit rate matching is employed. However, the current state of the JPEG-AI verification model experiences significant slowdowns during bit
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RAGFormer&#30340;&#26032;&#26694;&#26550;&#65292;&#21516;&#26102;&#23558;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#23884;&#20837;&#30446;&#26631;&#33410;&#28857;&#65292;&#20197;&#25913;&#36827;&#27450;&#35784;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.17472</link><description>&lt;p&gt;
&#36890;&#36807;&#34701;&#21512;&#20840;&#23616;&#21644;&#23616;&#37096;&#20851;&#31995;&#20132;&#20114;&#36827;&#34892;&#27450;&#35784;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Fraud Detection with Binding Global and Local Relational Interaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17472
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RAGFormer&#30340;&#26032;&#26694;&#26550;&#65292;&#21516;&#26102;&#23558;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#23884;&#20837;&#30446;&#26631;&#33410;&#28857;&#65292;&#20197;&#25913;&#36827;&#27450;&#35784;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#27450;&#35784;&#26816;&#27979;&#20855;&#26377;&#26377;&#25928;&#24615;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#22312;&#25972;&#20307;&#35270;&#35282;&#20013;&#32534;&#30721;&#33410;&#28857;&#20132;&#20114;&#21644;&#32858;&#21512;&#29305;&#24449;&#12290;&#26368;&#36817;&#65292;&#20855;&#26377;&#20986;&#33394;&#24207;&#21015;&#32534;&#30721;&#33021;&#21147;&#30340;Transformer&#32593;&#32476;&#22312;&#25991;&#29486;&#20013;&#20063;&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;GNN&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;GNN&#21644;&#22522;&#20110;Transformer&#30340;&#32593;&#32476;&#21482;&#32534;&#30721;&#25972;&#20010;&#22270;&#30340;&#19968;&#20010;&#35270;&#35282;&#65292;&#32780;GNN&#32534;&#30721;&#20840;&#23616;&#29305;&#24449;&#65292;Transformer&#32593;&#32476;&#32534;&#30721;&#23616;&#37096;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#24573;&#35270;&#20102;&#20351;&#29992;&#21333;&#29420;&#32593;&#32476;&#32534;&#30721;&#24322;&#26500;&#22270;&#30340;&#20840;&#23616;&#20132;&#20114;&#29305;&#24449;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;Relation-Aware GNN with transFormer&#65288;RAGFormer&#65289;&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#23558;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#21516;&#26102;&#23884;&#20837;&#30446;&#26631;&#33410;&#28857;&#20013;&#12290;&#36825;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#32593;&#32476;&#24212;&#29992;&#20102;&#19968;&#20010;&#20462;&#25913;&#21518;&#30340;GAGA&#27169;&#22359;&#65292;&#20854;&#20013;&#27599;&#20010;Transformer&#23618;&#21518;&#38754;&#37117;&#36319;&#30528;&#19968;&#20010;&#36328;&#20851;&#31995;&#32858;&#21512;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17472v1 Announce Type: cross  Abstract: Graph Neural Network has been proved to be effective for fraud detection for its capability to encode node interaction and aggregate features in a holistic view. Recently, Transformer network with great sequence encoding ability, has also outperformed other GNN-based methods in literatures. However, both GNN-based and Transformer-based networks only encode one perspective of the whole graph, while GNN encodes global features and Transformer network encodes local ones. Furthermore, previous works ignored encoding global interaction features of the heterogeneous graph with separate networks, thus leading to suboptimal performance. In this work, we present a novel framework called Relation-Aware GNN with transFormer (RAGFormer) which simultaneously embeds local and global features into a target node. The simple yet effective network applies a modified GAGA module where each transformer layer is followed by a cross-relation aggregation lay
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#34920;&#26126;&#65292;JPEG-AI&#26631;&#20934;&#21270;&#20013;&#30340;&#28789;&#27963;&#20301;&#20998;&#24067;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#22270;&#20687;&#21387;&#32553;&#24615;&#33021;&#65292;&#24182;&#36229;&#36807;&#32463;&#20856;&#32534;&#35299;&#30721;&#22120;VVC intra&#12290;</title><link>https://arxiv.org/abs/2402.17470</link><description>&lt;p&gt;
&#20301;&#20998;&#24067;&#30740;&#31350;&#19982;JPEG-AI&#26631;&#20934;&#21270;&#20013;&#31354;&#38388;&#36136;&#37327;&#22270;&#30340;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Bit Distribution Study and Implementation of Spatial Quality Map in the JPEG-AI Standardization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17470
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;JPEG-AI&#26631;&#20934;&#21270;&#20013;&#30340;&#28789;&#27963;&#20301;&#20998;&#24067;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#22270;&#20687;&#21387;&#32553;&#24615;&#33021;&#65292;&#24182;&#36229;&#36807;&#32463;&#20856;&#32534;&#35299;&#30721;&#22120;VVC intra&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#23545;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#20687;&#21387;&#32553;&#32534;&#35299;&#30721;&#22120;&#26377;&#24456;&#39640;&#30340;&#38656;&#27714;&#12290;&#36825;&#20123;&#32534;&#35299;&#30721;&#22120;&#37319;&#29992;&#38750;&#32447;&#24615;&#21464;&#25442;&#26469;&#21019;&#24314;&#32039;&#20945;&#30340;&#20301;&#34920;&#31034;&#65292;&#24182;&#22312;&#35774;&#22791;&#19978;&#27604;&#32463;&#20856;&#26694;&#26550;&#20013;&#20351;&#29992;&#30340;&#25163;&#24037;&#21046;&#20316;&#30340;&#21464;&#25442;&#23454;&#29616;&#26356;&#24555;&#30340;&#32534;&#30721;&#36895;&#24230;&#12290;&#31185;&#23398;&#30028;&#21644;&#24037;&#19994;&#30028;&#23545;&#36825;&#20123;&#29305;&#24615;&#38750;&#24120;&#24863;&#20852;&#36259;&#65292;&#36825;&#23548;&#33268;&#20102;JPEG-AI&#30340;&#26631;&#20934;&#21270;&#24037;&#20316;&#12290;JPEG-AI&#39564;&#35777;&#27169;&#22411;&#24050;&#21457;&#24067;&#65292;&#30446;&#21069;&#27491;&#22312;&#36827;&#34892;&#26631;&#20934;&#21270;&#24320;&#21457;&#12290;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#21487;&#20197;&#22312;&#22522;&#26412;&#25805;&#20316;&#28857;&#19978;&#27604;&#32463;&#20856;&#32534;&#35299;&#30721;&#22120;VVC intra&#25552;&#39640;10%&#20197;&#19978;&#30340;BD&#29575;&#12290;&#30740;&#31350;&#20154;&#21592;&#23558;&#36825;&#19968;&#25104;&#21151;&#24402;&#22240;&#20110;&#31354;&#38388;&#22495;&#20013;&#30340;&#28789;&#27963;&#20301;&#20998;&#24067;&#65292;&#19982;VVC intra&#30340;&#38170;&#28857;&#30456;&#21453;&#65292;&#21518;&#32773;&#29983;&#25104;&#19968;&#20010;&#24658;&#23450;&#36136;&#37327;&#28857;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;VVC intra&#36890;&#36807;&#23454;&#29616;&#26356;&#20855;&#36866;&#24212;&#24615;&#30340;&#20301;&#20998;&#24067;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17470v1 Announce Type: cross  Abstract: Currently, there is a high demand for neural network-based image compression codecs. These codecs employ non-linear transforms to create compact bit representations and facilitate faster coding speeds on devices compared to the hand-crafted transforms used in classical frameworks. The scientific and industrial communities are highly interested in these properties, leading to the standardization effort of JPEG-AI. The JPEG-AI verification model has been released and is currently under development for standardization. Utilizing neural networks, it can outperform the classic codec VVC intra by over 10% BD-rate operating at base operation point. Researchers attribute this success to the flexible bit distribution in the spatial domain, in contrast to VVC intra's anchor that is generated with a constant quality point. However, our study reveals that VVC intra displays a more adaptable bit distribution structure through the implementation of 
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#36895;&#29575;&#36801;&#31227;&#29616;&#35937;&#21487;&#20197;&#24402;&#22240;&#20110;&#22312;&#956;P&#21644;&#20854;&#28145;&#24230;&#24310;&#20280;&#19979;&#65292;&#35757;&#32451;&#25439;&#22833;Hessian&#30697;&#38453;&#30340;&#26368;&#22823;&#29305;&#24449;&#20540;&#65288;&#21363;&#38160;&#24230;&#65289;&#22312;&#36739;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#22522;&#26412;&#29420;&#31435;&#20110;&#32593;&#32476;&#30340;&#23485;&#24230;&#21644;&#28145;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.17457</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#23398;&#20064;&#36895;&#29575;&#20855;&#26377;&#36801;&#31227;&#24615;&#65311;&#35843;&#21644;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#20248;&#21270;&#21644;&#23610;&#24230;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Why do Learning Rates Transfer? Reconciling Optimization and Scaling Limits for Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17457
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#36895;&#29575;&#36801;&#31227;&#29616;&#35937;&#21487;&#20197;&#24402;&#22240;&#20110;&#22312;&#956;P&#21644;&#20854;&#28145;&#24230;&#24310;&#20280;&#19979;&#65292;&#35757;&#32451;&#25439;&#22833;Hessian&#30697;&#38453;&#30340;&#26368;&#22823;&#29305;&#24449;&#20540;&#65288;&#21363;&#38160;&#24230;&#65289;&#22312;&#36739;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#22522;&#26412;&#29420;&#31435;&#20110;&#32593;&#32476;&#30340;&#23485;&#24230;&#21644;&#28145;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#35777;&#25454;&#34920;&#26126;&#65292;&#22914;&#26524;&#31070;&#32463;&#32593;&#32476;&#30340;&#23485;&#24230;&#21644;&#28145;&#24230;&#26397;&#30528;&#25152;&#35859;&#30340;&#20016;&#23500;&#29305;&#24449;&#23398;&#20064;&#26497;&#38480;&#65288;&#956;P&#21450;&#20854;&#28145;&#24230;&#25193;&#23637;&#65289;&#36827;&#34892;&#32553;&#25918;&#65292;&#37027;&#20040;&#26576;&#20123;&#36229;&#21442;&#25968; - &#20363;&#22914;&#23398;&#20064;&#36895;&#29575; - &#23601;&#20250;&#20174;&#23567;&#27169;&#22411;&#36716;&#31227;&#21040;&#38750;&#24120;&#22823;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#25104;&#26412;&#12290;&#20174;&#20248;&#21270;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#29616;&#35937;&#20196;&#20154;&#22256;&#24785;&#65292;&#22240;&#20026;&#23427;&#24847;&#21619;&#30528;&#25439;&#22833;&#26223;&#35266;&#22312;&#38750;&#24120;&#19981;&#21516;&#30340;&#27169;&#22411;&#23610;&#23544;&#20043;&#38388;&#26159;&#38750;&#24120;&#19968;&#33268;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25214;&#21040;&#35777;&#25454;&#25903;&#25345;&#23398;&#20064;&#36895;&#29575;&#36801;&#31227;&#21487;&#20197;&#24402;&#22240;&#20110;&#20107;&#23454;&#65306;&#22312;&#956;P&#21450;&#20854;&#28145;&#24230;&#25193;&#23637;&#19979;&#65292;&#35757;&#32451;&#25439;&#22833;Hessian&#30340;&#26368;&#22823;&#29305;&#24449;&#20540;&#65288;&#21363;&#38160;&#24230;&#65289;&#22312;&#36739;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#29420;&#31435;&#20110;&#32593;&#32476;&#30340;&#23485;&#24230;&#21644;&#28145;&#24230;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#23637;&#31034;&#22312;&#31070;&#32463;&#20999;&#32447;&#26680;&#65288;NTK&#65289;&#20307;&#31995;&#19979;&#65292;&#38160;&#24230;&#22312;&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17457v1 Announce Type: new  Abstract: Recently, there has been growing evidence that if the width and depth of a neural network are scaled toward the so-called rich feature learning limit ($\mu$P and its depth extension), then some hyperparameters - such as the learning rate - exhibit transfer from small to very large models, thus reducing the cost of hyperparameter tuning. From an optimization perspective, this phenomenon is puzzling, as it implies that the loss landscape is remarkably consistent across very different model sizes. In this work, we find empirical evidence that learning rate transfer can be attributed to the fact that under $\mu$P and its depth extension, the largest eigenvalue of the training loss Hessian (i.e. the sharpness) is largely independent of the width and depth of the network for a sustained period of training time. On the other hand, we show that under the neural tangent kernel (NTK) regime, the sharpness exhibits very different dynamics at differ
&lt;/p&gt;</description></item><item><title>DS-Agent&#26159;&#19968;&#20010;&#33258;&#21160;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#21644;&#26696;&#20363;&#25512;&#29702;&#65292;&#33021;&#22815;&#22312;&#25968;&#25454;&#31185;&#23398;&#20219;&#21153;&#20013;&#28789;&#27963;&#21033;&#29992;&#19987;&#23478;&#30693;&#35782;&#24182;&#36890;&#36807;&#21453;&#39304;&#26426;&#21046;&#25345;&#32493;&#25913;&#21892;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.17453</link><description>&lt;p&gt;
DS-Agent&#65306;&#36890;&#36807;&#36171;&#20104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26696;&#20363;&#25512;&#29702;&#33021;&#21147;&#23454;&#29616;&#33258;&#21160;&#21270;&#25968;&#25454;&#31185;&#23398;
&lt;/p&gt;
&lt;p&gt;
DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17453
&lt;/p&gt;
&lt;p&gt;
DS-Agent&#26159;&#19968;&#20010;&#33258;&#21160;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#21644;&#26696;&#20363;&#25512;&#29702;&#65292;&#33021;&#22815;&#22312;&#25968;&#25454;&#31185;&#23398;&#20219;&#21153;&#20013;&#28789;&#27963;&#21033;&#29992;&#19987;&#23478;&#30693;&#35782;&#24182;&#36890;&#36807;&#21453;&#39304;&#26426;&#21046;&#25345;&#32493;&#25913;&#21892;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#29702;&#30340;&#28508;&#21147;&#65292;&#20197;&#33258;&#21160;&#21270;&#25968;&#25454;&#31185;&#23398;&#20219;&#21153;&#65292;&#30446;&#26631;&#26159;&#29702;&#35299;&#20219;&#21153;&#35201;&#27714;&#65292;&#28982;&#21518;&#26500;&#24314;&#21644;&#35757;&#32451;&#26368;&#21512;&#36866;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;LLM&#20195;&#29702;&#21462;&#24471;&#20102;&#24191;&#27867;&#25104;&#21151;&#65292;&#20294;&#22312;&#36825;&#31181;&#24773;&#26223;&#19979;&#29983;&#25104;&#19981;&#21512;&#29702;&#30340;&#23454;&#39564;&#35745;&#21010;&#21463;&#21040;&#38459;&#30861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DS-Agent&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;LLM&#20195;&#29702;&#21644;&#26696;&#20363;&#25512;&#29702;&#65288;CBR&#65289;&#30340;&#26032;&#39062;&#33258;&#21160;&#21270;&#26694;&#26550;&#12290;&#22312;&#24320;&#21457;&#38454;&#27573;&#65292;DS-Agent&#36981;&#24490;CBR&#26694;&#26550;&#26469;&#26500;&#24314;&#33258;&#21160;&#36845;&#20195;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#28789;&#27963;&#21033;&#29992;&#26469;&#33258;Kaggle&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#21453;&#39304;&#26426;&#21046;&#20419;&#36827;&#19968;&#33268;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;DS-Agent&#23454;&#29616;&#20102;&#19968;&#20010;&#20302;&#36164;&#28304;&#37096;&#32626;&#38454;&#27573;&#65292;&#37319;&#29992;&#31616;&#21270;&#30340;CBR&#33539;&#20363;&#26469;&#36866;&#24212;&#24320;&#21457;&#38454;&#27573;&#25104;&#21151;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#36827;&#34892;&#30452;&#25509;&#20195;&#30721;&#29983;&#25104;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17453v1 Announce Type: new  Abstract: In this work, we investigate the potential of large language models (LLMs) based agents to automate data science tasks, with the goal of comprehending task requirements, then building and training the best-fit machine learning models. Despite their widespread success, existing LLM agents are hindered by generating unreasonable experiment plans within this scenario. To this end, we present DS-Agent, a novel automatic framework that harnesses LLM agent and case-based reasoning (CBR). In the development stage, DS-Agent follows the CBR framework to structure an automatic iteration pipeline, which can flexibly capitalize on the expert knowledge from Kaggle, and facilitate consistent performance improvement through the feedback mechanism. Moreover, DS-Agent implements a low-resource deployment stage with a simplified CBR paradigm to adapt past successful solutions from the development stage for direct code generation, significantly reducing th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31934;&#30830;&#23450;&#20301;&#21021;&#22987;&#21270;&#21644;&#26368;&#22823;&#23398;&#20064;&#36895;&#29575;&#23545;&#32593;&#32476;&#32467;&#26500;&#30340;&#20381;&#36182;&#24615;&#65292;&#26412;&#25991;&#21487;&#20197;&#23558;&#21021;&#22987;&#21270;&#21644;&#23398;&#20064;&#36895;&#29575;&#25512;&#24191;&#21040;MLP&#21644;CNN&#20013;&#65292;&#20197;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.17440</link><description>&lt;p&gt;
&#22522;&#20110;&#21407;&#21017;&#30340;&#26550;&#26500;&#24863;&#30693;&#36229;&#21442;&#25968;&#32553;&#25918;
&lt;/p&gt;
&lt;p&gt;
Principled Architecture-aware Scaling of Hyperparameters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17440
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31934;&#30830;&#23450;&#20301;&#21021;&#22987;&#21270;&#21644;&#26368;&#22823;&#23398;&#20064;&#36895;&#29575;&#23545;&#32593;&#32476;&#32467;&#26500;&#30340;&#20381;&#36182;&#24615;&#65292;&#26412;&#25991;&#21487;&#20197;&#23558;&#21021;&#22987;&#21270;&#21644;&#23398;&#20064;&#36895;&#29575;&#25512;&#24191;&#21040;MLP&#21644;CNN&#20013;&#65292;&#20197;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#39640;&#36136;&#37327;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#36873;&#25321;&#21512;&#36866;&#30340;&#36229;&#21442;&#25968;&#65292;&#36825;&#26159;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#19988;&#26114;&#36149;&#30340;&#36807;&#31243;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#35797;&#22270;&#33258;&#21160;&#20248;&#21270;&#25110;&#35774;&#35745;&#36229;&#21442;&#25968;&#30340;&#21407;&#21017;&#65292;&#20197;&#20415;&#23427;&#20204;&#33021;&#22815;&#25512;&#24191;&#21040;&#22810;&#26679;&#30340;&#26410;&#30693;&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#35774;&#35745;&#25110;&#20248;&#21270;&#26041;&#27861;&#23545;&#32593;&#32476;&#32467;&#26500;&#30340;&#36873;&#25321;&#19968;&#26080;&#25152;&#30693;&#65292;&#22240;&#27492;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#30053;&#20102;&#31070;&#32463;&#26550;&#26500;&#23545;&#36229;&#21442;&#25968;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20934;&#30830;&#34920;&#24449;&#20102;&#21021;&#22987;&#21270;&#21644;&#26368;&#22823;&#23398;&#20064;&#36895;&#29575;&#23545;&#32593;&#32476;&#26550;&#26500;&#30340;&#20381;&#36182;&#24615;&#65292;&#21253;&#25324;&#32593;&#32476;&#28145;&#24230;&#12289;&#23485;&#24230;&#12289;&#21367;&#31215;&#26680;&#22823;&#23567;&#21644;&#36830;&#25509;&#27169;&#24335;&#12290;&#36890;&#36807;&#36861;&#27714;&#20351;&#27599;&#20010;&#21442;&#25968;&#22312;&#39044;&#28608;&#27963;&#20013;&#20855;&#26377;&#30456;&#21516;&#30340;&#22343;&#26041;&#21464;&#21270;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#21021;&#22987;&#21270;&#21644;&#23398;&#20064;&#36895;&#29575;&#25512;&#24191;&#21040;&#20855;&#26377;&#22797;&#26434;&#22270;&#25299;&#25169;&#32467;&#26500;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17440v1 Announce Type: new  Abstract: Training a high-quality deep neural network requires choosing suitable hyperparameters, which is a non-trivial and expensive process. Current works try to automatically optimize or design principles of hyperparameters, such that they can generalize to diverse unseen scenarios. However, most designs or optimization methods are agnostic to the choice of network structures, and thus largely ignore the impact of neural architectures on hyperparameters. In this work, we precisely characterize the dependence of initializations and maximal learning rates on the network architecture, which includes the network depth, width, convolutional kernel size, and connectivity patterns. By pursuing every parameter to be maximally updated with the same mean squared change in pre-activations, we can generalize our initialization and learning rates across MLPs (multi-layer perception) and CNNs (convolutional neural network) with sophisticated graph topologie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;KANDY&#22522;&#20934;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#22522;&#20110;&#22350;&#19969;&#26031;&#22522;&#27169;&#24335;&#30340;&#23398;&#20064;&#21644;&#25512;&#29702;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#25345;&#32493;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#25361;&#25112;&#65292;&#24182;&#30528;&#37325;&#30740;&#31350;&#31526;&#21495;&#32452;&#25104;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17431</link><description>&lt;p&gt;
KANDY&#22522;&#20934;&#65306;&#20351;&#29992;&#22350;&#19969;&#26031;&#22522;&#27169;&#24335;&#36827;&#34892;&#22686;&#37327;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#21644;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
The KANDY Benchmark: Incremental Neuro-Symbolic Learning and Reasoning with Kandinsky Patterns
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;KANDY&#22522;&#20934;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#22522;&#20110;&#22350;&#19969;&#26031;&#22522;&#27169;&#24335;&#30340;&#23398;&#20064;&#21644;&#25512;&#29702;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#25345;&#32493;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#25361;&#25112;&#65292;&#24182;&#30528;&#37325;&#30740;&#31350;&#31526;&#21495;&#32452;&#25104;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17431v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#20154;&#24037;&#26234;&#33021;&#19981;&#26029;&#23547;&#27714;&#26032;&#30340;&#25361;&#25112;&#21644;&#22522;&#20934;&#65292;&#20197;&#26377;&#25928;&#34913;&#37327;&#24615;&#33021;&#24182;&#25512;&#21160;&#26368;&#26032;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;KANDY&#65292;&#19968;&#20010;&#21487;&#29992;&#20110;&#29983;&#25104;&#21463;&#22350;&#19969;&#26031;&#22522;&#27169;&#24335;&#21551;&#21457;&#30340;&#21508;&#31181;&#23398;&#20064;&#21644;&#25512;&#29702;&#20219;&#21153;&#30340;&#22522;&#20934;&#26694;&#26550;&#12290;&#36890;&#36807;&#21019;&#24314;&#19968;&#31995;&#21015;&#20855;&#26377;&#36882;&#22686;&#22797;&#26434;&#24615;&#21644;&#31232;&#30095;&#30417;&#30563;&#30340;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#35838;&#31243;&#65292;KANDY&#21487;&#29992;&#20110;&#23454;&#29616;&#25345;&#32493;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#22522;&#20934;&#65292;&#24182;&#19987;&#27880;&#20110;&#31526;&#21495;&#32452;&#25104;&#24615;&#12290;&#22522;&#26412;&#20107;&#23454;&#20013;&#36824;&#25552;&#20379;&#20102;&#20998;&#31867;&#35268;&#21017;&#65292;&#20197;&#20415;&#20998;&#26512;&#21487;&#35299;&#37322;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#38500;&#20102;&#22522;&#20934;&#29983;&#25104;&#31649;&#36947;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#20004;&#20010;&#35838;&#31243;&#65292;&#19968;&#20010;&#26356;&#23481;&#26131;&#19968;&#20010;&#26356;&#38590;&#65292;&#25105;&#20204;&#25552;&#35758;&#36825;&#20123;&#20316;&#20026;&#30740;&#31350;&#31038;&#21306;&#30340;&#26032;&#25361;&#25112;&#12290;&#36890;&#36807;&#24443;&#24213;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#27169;&#22411;&#21644;&#32431;&#31526;&#21495;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17431v1 Announce Type: new  Abstract: Artificial intelligence is continuously seeking novel challenges and benchmarks to effectively measure performance and to advance the state-of-the-art. In this paper we introduce KANDY, a benchmarking framework that can be used to generate a variety of learning and reasoning tasks inspired by Kandinsky patterns. By creating curricula of binary classification tasks with increasing complexity and with sparse supervisions, KANDY can be used to implement benchmarks for continual and semi-supervised learning, with a specific focus on symbol compositionality. Classification rules are also provided in the ground truth to enable analysis of interpretable solutions. Together with the benchmark generation pipeline, we release two curricula, an easier and a harder one, that we propose as new challenges for the research community. With a thorough experimental evaluation, we show how both state-of-the-art neural models and purely symbolic approaches 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#31471;&#21040;&#31471;&#22320;&#24378;&#21270;&#23398;&#20064;&#40657;&#30418;&#20248;&#21270;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#34920;&#36798;&#33021;&#21147;&#24378;&#30340;&#24207;&#21015;&#27169;&#22411;&#21644;&#21518;&#24724;-&#21069;&#36827;&#20196;&#29260;&#26469;&#33719;&#21462;&#20219;&#21153;&#20449;&#24687;&#24182;&#20570;&#20986;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2402.17423</link><description>&lt;p&gt;
&#21152;&#24378;&#19978;&#19979;&#25991;&#40657;&#30418;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Reinforced In-Context Black-Box Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17423
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#31471;&#21040;&#31471;&#22320;&#24378;&#21270;&#23398;&#20064;&#40657;&#30418;&#20248;&#21270;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#34920;&#36798;&#33021;&#21147;&#24378;&#30340;&#24207;&#21015;&#27169;&#22411;&#21644;&#21518;&#24724;-&#21069;&#36827;&#20196;&#29260;&#26469;&#33719;&#21462;&#20219;&#21153;&#20449;&#24687;&#24182;&#20570;&#20986;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#30418;&#20248;&#21270;&#65288;BBO&#65289;&#24050;&#32463;&#22312;&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#21462;&#24471;&#25104;&#21151;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#20803;&#23398;&#20064;BBO&#31639;&#27861;&#30340;&#29305;&#23450;&#32452;&#20214;&#65292;&#20197;&#21152;&#24555;&#20248;&#21270;&#36895;&#24230;&#24182;&#25670;&#33073;&#32321;&#29712;&#30340;&#25163;&#24037;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#20316;&#20026;&#25193;&#23637;&#65292;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#25972;&#20010;&#31639;&#27861;&#38656;&#35201;&#19987;&#23478;&#26368;&#23569;&#30340;&#24037;&#20316;&#37327;&#65292;&#24182;&#19988;&#21487;&#20197;&#25552;&#20379;&#26368;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RIBBO&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#24378;&#21270;&#23398;&#20064;BBO&#31639;&#27861;&#12290;RIBBO&#21033;&#29992;&#34920;&#36798;&#33021;&#21147;&#24378;&#30340;&#24207;&#21015;&#27169;&#22411;&#26469;&#23398;&#20064;&#22810;&#20010;&#34892;&#20026;&#31639;&#27861;&#21644;&#20219;&#21153;&#20135;&#29983;&#30340;&#20248;&#21270;&#21382;&#21490;&#65292;&#21033;&#29992;&#22823;&#22411;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#26469;&#25552;&#21462;&#20219;&#21153;&#20449;&#24687;&#24182;&#30456;&#24212;&#22320;&#20570;&#20986;&#20915;&#31574;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#36890;&#36807;&#22686;&#21152;&#21518;&#24724;-&#21069;&#36827;&#20196;&#29260;&#26469;&#22686;&#24378;&#20248;&#21270;&#21382;&#21490;&#65292;&#36825;&#20123;&#20196;&#29260;&#26088;&#22312;&#22522;&#20110;&#32047;&#31215;&#34920;&#29616;&#26469;&#34920;&#31034;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17423v1 Announce Type: cross  Abstract: Black-Box Optimization (BBO) has found successful applications in many fields of science and engineering. Recently, there has been a growing interest in meta-learning particular components of BBO algorithms to speed up optimization and get rid of tedious hand-crafted heuristics. As an extension, learning the entire algorithm from data requires the least labor from experts and can provide the most flexibility. In this paper, we propose RIBBO, a method to reinforce-learn a BBO algorithm from offline data in an end-to-end fashion. RIBBO employs expressive sequence models to learn the optimization histories produced by multiple behavior algorithms and tasks, leveraging the in-context learning ability of large models to extract task information and make decisions accordingly. Central to our method is to augment the optimization histories with regret-to-go tokens, which are designed to represent the performance of an algorithm based on cumul
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;MRI&#37325;&#24314;&#30340;&#20613;&#37324;&#21494;&#22495;&#25554;&#20540;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#20687;&#31354;&#38388;&#24418;&#24335;&#20027;&#20041;&#65292;&#24182;&#20998;&#26512;&#20102;&#22312;CNN&#25512;&#26029;&#36807;&#31243;&#20013;&#22122;&#22768;&#20256;&#25773;&#30340;&#20272;&#35745;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.17410</link><description>&lt;p&gt;
&#20613;&#37324;&#21494;&#22495;&#25554;&#20540;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#20687;&#31354;&#38388;&#24418;&#24335;&#20027;&#20041;&#29992;&#20110;&#22122;&#22768;&#20256;&#25773;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A novel image space formalism of Fourier domain interpolation neural networks for noise propagation analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17410
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;MRI&#37325;&#24314;&#30340;&#20613;&#37324;&#21494;&#22495;&#25554;&#20540;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#20687;&#31354;&#38388;&#24418;&#24335;&#20027;&#20041;&#65292;&#24182;&#20998;&#26512;&#20102;&#22312;CNN&#25512;&#26029;&#36807;&#31243;&#20013;&#22122;&#22768;&#20256;&#25773;&#30340;&#20272;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26088;&#22312;&#20026;MRI&#37325;&#24314;&#20013;&#30340;&#22270;&#20687;&#22495;&#25554;&#20540;&#24320;&#21457;&#22810;&#23618;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#30340;&#22270;&#20687;&#31354;&#38388;&#24418;&#24335;&#20027;&#20041;&#65292;&#24182;&#22312;CNN&#25512;&#26029;&#36807;&#31243;&#20013;&#23545;&#22122;&#22768;&#20256;&#25773;&#36827;&#34892;&#20998;&#26512;&#12290;&#36890;&#36807;&#20351;&#29992;&#22797;&#20540;&#25972;&#27969;&#32447;&#24615;&#21333;&#20803;&#22312;&#20613;&#37324;&#21494;&#22495;&#65288;&#20063;&#31216;&#20026;k&#31354;&#38388;&#65289;&#20013;&#30340;&#38750;&#32447;&#24615;&#28608;&#27963;&#65292;&#23558;&#20854;&#34920;&#31034;&#20026;&#19982;&#28608;&#27963;&#25513;&#27169;&#30340;&#36880;&#20803;&#32032;&#20056;&#27861;&#12290;&#36825;&#31181;&#25805;&#20316;&#22312;&#22270;&#20687;&#31354;&#38388;&#20013;&#36716;&#25442;&#20026;&#21367;&#31215;&#12290;&#22312;k&#31354;&#38388;&#32593;&#32476;&#35757;&#32451;&#21518;&#65292;&#36825;&#31181;&#26041;&#27861;&#20026;&#30456;&#23545;&#20110;&#21035;&#21517;&#32447;&#22280;&#22270;&#20687;&#30340;&#37325;&#24314;&#22270;&#20687;&#30340;&#23548;&#25968;&#25552;&#20379;&#20102;&#19968;&#20010;&#20195;&#25968;&#34920;&#36798;&#24335;&#65292;&#36825;&#20123;&#21035;&#21517;&#32447;&#22280;&#22270;&#20687;&#20316;&#20026;&#22270;&#20687;&#31354;&#38388;&#20013;&#32593;&#32476;&#30340;&#36755;&#20837;&#24352;&#37327;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#20272;&#35745;&#32593;&#32476;&#25512;&#26029;&#20013;&#30340;&#26041;&#24046;&#65292;&#24182;&#29992;&#20110;&#25551;&#36848;&#22122;&#22768;&#29305;&#24615;&#12290;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#27169;&#25311;&#21644;&#22522;&#20110;&#33258;&#21160;&#24494;&#20998;&#30340;&#25968;&#20540;&#26041;&#27861;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17410v1 Announce Type: cross  Abstract: Purpose: To develop an image space formalism of multi-layer convolutional neural networks (CNNs) for Fourier domain interpolation in MRI reconstructions and analytically estimate noise propagation during CNN inference. Theory and Methods: Nonlinear activations in the Fourier domain (also known as k-space) using complex-valued Rectifier Linear Units are expressed as elementwise multiplication with activation masks. This operation is transformed into a convolution in the image space. After network training in k-space, this approach provides an algebraic expression for the derivative of the reconstructed image with respect to the aliased coil images, which serve as the input tensors to the network in the image space. This allows the variance in the network inference to be estimated analytically and to be used to describe noise characteristics. Monte-Carlo simulations and numerical approaches based on auto-differentiation were used for val
&lt;/p&gt;</description></item><item><title>LSPT&#26159;&#19968;&#31181;&#38761;&#21629;&#24615;&#30340;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38271;&#26399;&#38376;&#25511;&#25552;&#31034;&#65292;&#24039;&#22937;&#22320;&#21033;&#29992;&#38271;&#36317;&#31163;&#20808;&#21069;&#22359;&#20316;&#20026;&#25552;&#31034;&#30340;&#28508;&#22312;&#26469;&#28304;&#65292;&#20943;&#36731;&#20102;&#36951;&#24536;&#21442;&#25968;&#30340;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2402.17406</link><description>&lt;p&gt;
LSPT&#65306;&#29992;&#20110;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#30340;&#38271;&#26399;&#31354;&#38388;&#25552;&#31034;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
LSPT: Long-term Spatial Prompt Tuning for Visual Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17406
&lt;/p&gt;
&lt;p&gt;
LSPT&#26159;&#19968;&#31181;&#38761;&#21629;&#24615;&#30340;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38271;&#26399;&#38376;&#25511;&#25552;&#31034;&#65292;&#24039;&#22937;&#22320;&#21033;&#29992;&#38271;&#36317;&#31163;&#20808;&#21069;&#22359;&#20316;&#20026;&#25552;&#31034;&#30340;&#28508;&#22312;&#26469;&#28304;&#65292;&#20943;&#36731;&#20102;&#36951;&#24536;&#21442;&#25968;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25552;&#31034;&#35843;&#25972;&#65288;VPT&#65289;&#25216;&#26415;&#22240;&#20854;&#36890;&#36807;&#19987;&#29992;&#30340;&#21487;&#23398;&#20064;&#20196;&#29260;&#65288;&#31216;&#20026;&#25552;&#31034;&#65289;&#23558;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;Transformer&#65288;ViT&#65289;&#35843;&#25972;&#21040;&#19979;&#28216;&#35270;&#35273;&#20219;&#21153;&#32780;&#38395;&#21517;&#12290;&#22312;&#33258;&#30417;&#30563;&#35270;&#35273;Transformer&#20013;&#20351;&#29992;&#30340;&#24403;&#20195;VPT&#26041;&#27861;&#36890;&#24120;&#40664;&#35748;&#24341;&#20837;&#26469;&#28304;&#33258;&#27169;&#22411;&#20808;&#21069;&#22359;&#30340;&#26032;&#21487;&#23398;&#20064;&#25552;&#31034;&#25110;&#38376;&#25511;&#25552;&#31034;&#20196;&#29260;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#32570;&#22833;&#26159;&#26410;&#21033;&#29992;&#38271;&#36317;&#31163;&#20808;&#21069;&#22359;&#20316;&#20026;&#27599;&#20010;&#33258;&#30417;&#30563;ViT&#20869;&#25552;&#31034;&#30340;&#28508;&#21147;&#26469;&#28304;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#37325;&#35201;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38271;&#26399;&#31354;&#38388;&#25552;&#31034;&#35843;&#25972;&#65288;LSPT&#65289;- &#19968;&#31181;&#38761;&#21629;&#24615;&#30340;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290; LSPT&#20174;&#20154;&#31867;&#22823;&#33041;&#30340;&#22797;&#26434;&#24615;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#24039;&#22937;&#22320;&#32467;&#21512;&#20102;&#38271;&#26399;&#38376;&#25511;&#25552;&#31034;&#12290;&#36825;&#20010;&#29305;&#24615;&#20316;&#20026;&#26102;&#38388;&#32534;&#30721;&#65292;&#20943;&#36731;&#20102;&#36951;&#24536;&#21442;&#25968;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17406v1 Announce Type: cross  Abstract: Visual Prompt Tuning (VPT) techniques have gained prominence for their capacity to adapt pre-trained Vision Transformers (ViTs) to downstream visual tasks using specialized learnable tokens termed as prompts. Contemporary VPT methodologies, especially when employed with self-supervised vision transformers, often default to the introduction of new learnable prompts or gated prompt tokens predominantly sourced from the model's previous block. A pivotal oversight in such approaches is their failure to harness the potential of long-range previous blocks as sources of prompts within each self-supervised ViT. To bridge this crucial gap, we introduce Long-term Spatial Prompt Tuning (LSPT) - a revolutionary approach to visual representation learning. Drawing inspiration from the intricacies of the human brain, LSPT ingeniously incorporates long-term gated prompts. This feature serves as temporal coding, curbing the risk of forgetting parameter
&lt;/p&gt;</description></item><item><title>Beacon&#26159;&#19968;&#20010;&#24320;&#28304;&#22522;&#20934;&#24211;&#65292;&#29992;&#20110;&#27969;&#25511;&#21046;&#65292;&#21253;&#21547;7&#20010;&#36731;&#37327;&#32423;&#30340;1D&#21644;2D&#38382;&#39064;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#23545;&#25968;&#20540;&#27969;&#20307;&#21160;&#21147;&#23398;&#29615;&#22659;&#30340;&#36866;&#24212;&#24615;&#21644;&#21487;&#37325;&#29616;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17402</link><description>&lt;p&gt;
Beacon&#65292;&#19968;&#20010;&#29992;&#20110;&#27969;&#25511;&#21046;&#30340;&#36731;&#37327;&#32423;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#24211;
&lt;/p&gt;
&lt;p&gt;
Beacon, a lightweight deep reinforcement learning benchmark library for flow control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17402
&lt;/p&gt;
&lt;p&gt;
Beacon&#26159;&#19968;&#20010;&#24320;&#28304;&#22522;&#20934;&#24211;&#65292;&#29992;&#20110;&#27969;&#25511;&#21046;&#65292;&#21253;&#21547;7&#20010;&#36731;&#37327;&#32423;&#30340;1D&#21644;2D&#38382;&#39064;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#23545;&#25968;&#20540;&#27969;&#20307;&#21160;&#21147;&#23398;&#29615;&#22659;&#30340;&#36866;&#24212;&#24615;&#21644;&#21487;&#37325;&#29616;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#27969;&#25511;&#21046;&#38382;&#39064;&#20013;&#30340;&#26085;&#30410;&#22686;&#22810;&#30340;&#24212;&#29992;&#23548;&#33268;&#20102;&#19968;&#20010;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#19987;&#27880;&#20110;&#23558;&#29616;&#26377;&#31639;&#27861;&#19982;&#25968;&#20540;&#27969;&#20307;&#21160;&#21147;&#23398;&#29615;&#22659;&#30340;&#25511;&#21046;&#32806;&#21512;&#21644;&#35843;&#25972;&#12290;&#23613;&#31649;&#36825;&#20010;&#39046;&#22495;&#20173;&#22788;&#20110;&#33804;&#33469;&#38454;&#27573;&#65292;&#20294;&#22312;&#30701;&#26102;&#38388;&#20869;&#21462;&#24471;&#20102;&#22810;&#27425;&#25104;&#21151;&#65292;&#20854;&#24555;&#36895;&#21457;&#23637;&#36895;&#24230;&#32943;&#23450;&#37096;&#20998;&#24402;&#21151;&#20110;&#25512;&#21160;&#31038;&#21306;&#25193;&#22823;&#30340;&#24320;&#28304;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#20173;&#32570;&#20047;&#19968;&#20010;&#20849;&#21516;&#22522;&#30784;&#65292;&#26469;&#30830;&#20445;&#32467;&#26524;&#30340;&#21487;&#37325;&#29616;&#24615;&#65292;&#24182;&#25552;&#20379;&#36866;&#24403;&#30340;&#19987;&#38376;&#22522;&#20934;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Beacon&#65292;&#19968;&#20010;&#24320;&#28304;&#22522;&#20934;&#24211;&#65292;&#30001;&#19971;&#20010;&#36731;&#37327;&#32423;1D&#21644;2D&#27969;&#25511;&#21046;&#38382;&#39064;&#32452;&#25104;&#65292;&#20855;&#26377;&#19981;&#21516;&#30340;&#29305;&#24449;&#12289;&#34892;&#21160;&#21644;&#35266;&#23519;&#31354;&#38388;&#29305;&#24449;&#20197;&#21450;CPU&#38656;&#27714;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25551;&#36848;&#20102;&#19971;&#20010;&#32771;&#34385;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#21442;&#32771;&#25511;&#21046;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17402v1 Announce Type: cross  Abstract: Recently, the increasing use of deep reinforcement learning for flow control problems has led to a new area of research, focused on the coupling and the adaptation of the existing algorithms to the control of numerical fluid dynamics environments. Although still in its infancy, the field has seen multiple successes in a short time span, and its fast development pace can certainly be partly imparted to the open-source effort that drives the expansion of the community. Yet, this emerging domain still misses a common ground to (i) ensure the reproducibility of the results, and (ii) offer a proper ad-hoc benchmarking basis. To this end, we propose Beacon, an open-source benchmark library composed of seven lightweight 1D and 2D flow control problems with various characteristics, action and observation space characteristics, and CPU requirements. In this contribution, the seven considered problems are described, and reference control solutio
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#25552;&#20986;&#20102;Quantum-SMOTE&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#26059;&#36716;&#35282;&#24230;&#12289;&#23569;&#25968;&#31867;&#30334;&#20998;&#27604;&#21644;&#20998;&#35010;&#22240;&#23376;&#31561;&#36229;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#23545;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#26356;&#22909;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.17398</link><description>&lt;p&gt;
&#37327;&#23376;&#26041;&#27861;&#30740;&#31350;&#21512;&#25104;&#23569;&#25968;&#31867;&#36807;&#37319;&#26679;&#25216;&#26415;&#65288;SMOTE&#65289;
&lt;/p&gt;
&lt;p&gt;
A Quantum Approach to Synthetic Minority Oversampling Technique (SMOTE)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17398
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#25552;&#20986;&#20102;Quantum-SMOTE&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#26059;&#36716;&#35282;&#24230;&#12289;&#23569;&#25968;&#31867;&#30334;&#20998;&#27604;&#21644;&#20998;&#35010;&#22240;&#23376;&#31561;&#36229;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#23545;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#26356;&#22909;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;Quantum-SMOTE&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#26469;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#26032;&#39062;&#35299;&#20915;&#26041;&#26696;&#12290;Quantum-SMOTE&#21463;&#21040;&#21512;&#25104;&#23569;&#25968;&#31867;&#36807;&#37319;&#26679;&#25216;&#26415;&#65288;SMOTE&#65289;&#30340;&#21551;&#21457;&#65292;&#21033;&#29992;&#37327;&#23376;&#36807;&#31243;&#22914;&#20132;&#25442;&#27979;&#35797;&#21644;&#37327;&#23376;&#26059;&#36716;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#28857;&#12290;&#35813;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;SMOTE&#31639;&#27861;&#20351;&#29992;K-&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#21644;&#27431;&#27663;&#36317;&#31163;&#30340;&#26041;&#24335;&#26377;&#25152;&#19981;&#21516;&#65292;&#33021;&#22815;&#20174;&#23569;&#25968;&#31867;&#25968;&#25454;&#28857;&#29983;&#25104;&#21512;&#25104;&#23454;&#20363;&#32780;&#26080;&#38656;&#20381;&#36182;&#20110;&#37051;&#36817;&#24615;&#12290;&#31639;&#27861;&#36890;&#36807;&#24341;&#20837;&#26059;&#36716;&#35282;&#24230;&#12289;&#23569;&#25968;&#31867;&#30334;&#20998;&#27604;&#21644;&#20998;&#35010;&#22240;&#23376;&#31561;&#36229;&#21442;&#25968;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25511;&#21046;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#29305;&#23450;&#25968;&#25454;&#38598;&#38656;&#27714;&#30340;&#23450;&#21046;&#12290;&#35813;&#26041;&#27861;&#22312;TelecomChurn&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#19982;&#20004;&#31181;&#20027;&#35201;&#30340;&#20998;&#31867;&#31639;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17398v1 Announce Type: cross  Abstract: The paper proposes the Quantum-SMOTE method, a novel solution that uses quantum computing techniques to solve the prevalent problem of class imbalance in machine learning datasets. Quantum-SMOTE, inspired by the Synthetic Minority Oversampling Technique (SMOTE), generates synthetic data points using quantum processes such as swap tests and quantum rotation. The process varies from the conventional SMOTE algorithm's usage of K-Nearest Neighbors (KNN) and Euclidean distances, enabling synthetic instances to be generated from minority class data points without relying on neighbor proximity. The algorithm asserts greater control over the synthetic data generation process by introducing hyperparameters such as rotation angle, minority percentage, and splitting factor, which allow for customization to specific dataset requirements. The approach is tested on a public dataset of TelecomChurn and evaluated alongside two prominent classification
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#40065;&#26834;&#19968;&#33268;&#23545;&#25239;&#35757;&#32451;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#26356;&#26032;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#21644;&#31995;&#32479;&#23433;&#20840;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.17390</link><description>&lt;p&gt;
&#38024;&#23545;&#23433;&#20840;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26356;&#26032;&#30340;&#40065;&#26834;&#19968;&#33268;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Robustness-Congruent Adversarial Training for Secure Machine Learning Model Updates
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17390
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#40065;&#26834;&#19968;&#33268;&#23545;&#25239;&#35757;&#32451;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#26356;&#26032;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#21644;&#31995;&#32479;&#23433;&#20840;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#23450;&#26399;&#26356;&#26032;&#20197;&#25552;&#39640;&#20854;&#24179;&#22343;&#20934;&#30830;&#24230;&#65292;&#21033;&#29992;&#26032;&#39062;&#30340;&#26550;&#26500;&#21644;&#39069;&#22806;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#26032;&#26356;&#26032;&#30340;&#27169;&#22411;&#21487;&#33021;&#20250;&#29359;&#20197;&#21069;&#27169;&#22411;&#26410;&#26366;&#29359;&#36807;&#30340;&#38169;&#35823;&#12290;&#36825;&#31181;&#35823;&#20998;&#31867;&#34987;&#31216;&#20026;&#36127;&#32763;&#36716;&#65292;&#24182;&#34987;&#29992;&#25143;&#20307;&#39564;&#20026;&#24615;&#33021;&#30340;&#36864;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#38382;&#39064;&#20063;&#24433;&#21709;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#40065;&#26834;&#24615;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#23433;&#20840;&#27169;&#22411;&#26356;&#26032;&#23454;&#36341;&#30340;&#21457;&#23637;&#12290;&#29305;&#21035;&#26159;&#65292;&#24403;&#26356;&#26032;&#27169;&#22411;&#20197;&#25552;&#39640;&#20854;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#26102;&#65292;&#19968;&#20123;&#20808;&#21069;&#26080;&#25928;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#21487;&#33021;&#20250;&#34987;&#38169;&#35823;&#20998;&#31867;&#65292;&#23548;&#33268;&#31995;&#32479;&#23433;&#20840;&#24615;&#30340;&#35748;&#30693;&#36864;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#40065;&#26834;&#19968;&#33268;&#23545;&#25239;&#35757;&#32451;&#30340;&#26032;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23427;&#28041;&#21450;&#20351;&#29992;&#23545;&#25239;&#35757;&#32451;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#21516;&#26102;&#32422;&#26463;&#20854;&#22312;&#23545;&#25239;&#24615;&#31034;&#20363;&#19978;&#20445;&#25345;&#26356;&#39640;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17390v1 Announce Type: new  Abstract: Machine-learning models demand for periodic updates to improve their average accuracy, exploiting novel architectures and additional data. However, a newly-updated model may commit mistakes that the previous model did not make. Such misclassifications are referred to as negative flips, and experienced by users as a regression of performance. In this work, we show that this problem also affects robustness to adversarial examples, thereby hindering the development of secure model update practices. In particular, when updating a model to improve its adversarial robustness, some previously-ineffective adversarial examples may become misclassified, causing a regression in the perceived security of the system. We propose a novel technique, named robustness-congruent adversarial training, to address this issue. It amounts to fine-tuning a model with adversarial training, while constraining it to retain higher robustness on the adversarial examp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#29992;&#20110;&#35774;&#35745;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#36890;&#36807;&#23547;&#25214;&#26356;&#21512;&#36866;&#30340;&#26102;&#38388;&#27493;&#38271;&#21152;&#36895;&#25193;&#25955;&#37319;&#26679;&#12290;</title><link>https://arxiv.org/abs/2402.17376</link><description>&lt;p&gt;
&#20248;&#21270;&#26102;&#38388;&#27493;&#38271;&#21152;&#36895;&#25193;&#25955;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Accelerating Diffusion Sampling with Optimized Time Steps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17376
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#29992;&#20110;&#35774;&#35745;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#36890;&#36807;&#23547;&#25214;&#26356;&#21512;&#36866;&#30340;&#26102;&#38388;&#27493;&#38271;&#21152;&#36895;&#25193;&#25955;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#22312;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21512;&#25104;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#37319;&#26679;&#27493;&#39588;&#65292;&#20854;&#37319;&#26679;&#25928;&#29575;&#20173;&#26377;&#24453;&#25552;&#39640;&#12290;&#36817;&#26399;&#39640;&#38454;&#25968;&#20540;ODE&#27714;&#35299;&#22120;&#22312;DPMs&#20013;&#30340;&#24212;&#29992;&#20351;&#24471;&#29992;&#26356;&#23569;&#30340;&#37319;&#26679;&#27493;&#39588;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#25104;&#20026;&#21487;&#33021;&#12290;&#23613;&#31649;&#36825;&#26159;&#19968;&#39033;&#37325;&#22823;&#36827;&#23637;&#65292;&#22823;&#22810;&#25968;&#37319;&#26679;&#26041;&#27861;&#20173;&#28982;&#37319;&#29992;&#22343;&#21248;&#26102;&#38388;&#27493;&#38271;&#65292;&#32780;&#22312;&#37319;&#26679;&#27493;&#39588;&#36739;&#23569;&#26102;&#24182;&#19981;&#26159;&#26368;&#20339;&#36873;&#25321;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#35813;&#20248;&#21270;&#38382;&#39064;&#26088;&#22312;&#20026;DPMs&#30340;&#29305;&#23450;&#25968;&#20540;ODE&#27714;&#35299;&#22120;&#23547;&#25214;&#26356;&#21512;&#36866;&#30340;&#26102;&#38388;&#27493;&#38271;&#12290;&#27492;&#20248;&#21270;&#38382;&#39064;&#26088;&#22312;&#26368;&#23567;&#21270;&#22320;&#23454;&#29616;&#22320;&#30495;&#23454;&#35299;&#19982;&#19982;&#25968;&#20540;&#27714;&#35299;&#22120;&#23545;&#24212;&#30340;&#36817;&#20284;&#35299;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#21463;&#38480;&#20449;&#36182;&#22495;&#26041;&#27861;&#36827;&#34892;&#39640;&#25928;&#27714;&#35299;&#65292;&#26102;&#38388;&#23569;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17376v1 Announce Type: cross  Abstract: Diffusion probabilistic models (DPMs) have shown remarkable performance in high-resolution image synthesis, but their sampling efficiency is still to be desired due to the typically large number of sampling steps. Recent advancements in high-order numerical ODE solvers for DPMs have enabled the generation of high-quality images with much fewer sampling steps. While this is a significant development, most sampling methods still employ uniform time steps, which is not optimal when using a small number of steps. To address this issue, we propose a general framework for designing an optimization problem that seeks more appropriate time steps for a specific numerical ODE solver for DPMs. This optimization problem aims to minimize the distance between the ground-truth solution to the ODE and an approximate solution corresponding to the numerical solver. It can be efficiently solved using the constrained trust region method, taking less than 
&lt;/p&gt;</description></item><item><title>&#35745;&#31639;&#26041;&#27861;&#36873;&#25321;&#20250;&#26174;&#33879;&#24433;&#21709;&#36830;&#32493;&#26102;&#38388;&#25511;&#21046;&#20013;&#31215;&#20998;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#34920;&#29616;</title><link>https://arxiv.org/abs/2402.17375</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#25511;&#21046;&#20013;&#31215;&#20998;&#24378;&#21270;&#23398;&#20064;&#20013;&#35745;&#31639;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of Computation in Integral Reinforcement Learning for Continuous-Time Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17375
&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26041;&#27861;&#36873;&#25321;&#20250;&#26174;&#33879;&#24433;&#21709;&#36830;&#32493;&#26102;&#38388;&#25511;&#21046;&#20013;&#31215;&#20998;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31215;&#20998;&#24378;&#21270;&#23398;&#20064;(IntRL)&#22312;&#25919;&#31574;&#35780;&#20272;(PEV)&#38454;&#27573;&#38656;&#35201;&#31934;&#30830;&#35745;&#31639;&#25928;&#29992;&#20989;&#25968;&#30340;&#31215;&#20998;&#12290;&#36825;&#26159;&#36890;&#36807;&#31215;&#20998;&#35268;&#21017;&#23454;&#29616;&#30340;&#65292;&#21363;&#26469;&#33258;&#31163;&#25955;&#26102;&#38388;&#20013;&#33719;&#24471;&#30340;&#29366;&#24577;&#26679;&#26412;&#35780;&#20272;&#30340;&#25928;&#29992;&#20989;&#25968;&#30340;&#21152;&#26435;&#24635;&#21644;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#19968;&#20010;&#20851;&#38190;&#20294;&#34987;&#24573;&#35270;&#30340;&#29616;&#35937;&#65306;&#35745;&#31639;&#26041;&#27861;&#30340;&#36873;&#25321;--&#22312;&#26412;&#20363;&#20013;&#26159;&#31215;&#20998;&#35268;&#21017;--&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#25511;&#21046;&#24615;&#33021;&#12290;&#36825;&#31181;&#24433;&#21709;&#21487;&#36861;&#28335;&#21040;&#24341;&#20837;&#20110;PEV&#38454;&#27573;&#30340;&#35745;&#31639;&#38169;&#35823;&#21487;&#33021;&#24433;&#21709;&#25919;&#31574;&#36845;&#20195;&#30340;&#25910;&#25947;&#34892;&#20026;&#65292;&#36827;&#32780;&#24433;&#21709;&#25152;&#23398;&#25511;&#21046;&#22120;&#12290;&#20026;&#20102;&#38416;&#26126;&#35745;&#31639;&#22914;&#20309;&#24433;&#21709;&#25511;&#21046;&#65292;&#25105;&#20204;&#23558;IntRL&#30340;&#25919;&#31574;&#36845;&#20195;&#19982;&#24212;&#29992;&#20110;&#21704;&#23494;&#39039;-&#38597;&#21487;&#27604;-&#36125;&#23572;&#26364;&#26041;&#31243;&#30340;&#29275;&#39039;&#27861;&#36827;&#34892;&#20102;&#31867;&#27604;&#12290;&#22312;&#36825;&#31181;&#20809;&#19979;&#65292;PEV&#20013;&#30340;&#35745;&#31639;&#35823;&#24046;&#34920;&#29616;&#20026;&#29275;&#39039;&#27861;&#30340;&#27599;&#27425;&#36845;&#20195;&#20013;&#30340;&#39069;&#22806;&#35823;&#24046;&#39033;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17375v1 Announce Type: cross  Abstract: Integral reinforcement learning (IntRL) demands the precise computation of the utility function's integral at its policy evaluation (PEV) stage. This is achieved through quadrature rules, which are weighted sums of utility functions evaluated from state samples obtained in discrete time. Our research reveals a critical yet underexplored phenomenon: the choice of the computational method -- in this case, the quadrature rule -- can significantly impact control performance. This impact is traced back to the fact that computational errors introduced in the PEV stage can affect the policy iteration's convergence behavior, which in turn affects the learned controller. To elucidate how computation impacts control, we draw a parallel between IntRL's policy iteration and Newton's method applied to the Hamilton-Jacobi-Bellman equation. In this light, computational error in PEV manifests as an extra error term in each iteration of Newton's method
&lt;/p&gt;</description></item><item><title>CGGM&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#31232;&#30095;&#24615;&#29983;&#25104;&#37051;&#25509;&#30697;&#38453;&#65292;&#35299;&#20915;&#20102;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#33410;&#28857;&#24322;&#24120;&#26816;&#27979;&#20013;&#33410;&#28857;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.17363</link><description>&lt;p&gt;
CGGM&#65306;&#19968;&#31181;&#20855;&#26377;&#33258;&#36866;&#24212;&#31232;&#30095;&#24615;&#30340;&#26465;&#20214;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#33410;&#28857;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
CGGM: A conditional graph generation model with adaptive sparsity for node anomaly detection in IoT networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17363
&lt;/p&gt;
&lt;p&gt;
CGGM&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#31232;&#30095;&#24615;&#29983;&#25104;&#37051;&#25509;&#30697;&#38453;&#65292;&#35299;&#20915;&#20102;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#33410;&#28857;&#24322;&#24120;&#26816;&#27979;&#20013;&#33410;&#28857;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22270;&#34987;&#24191;&#27867;&#29992;&#20110;&#26816;&#27979;&#29289;&#32852;&#32593;&#20013;&#33410;&#28857;&#30340;&#24322;&#24120;&#34892;&#20026;&#12290;&#29983;&#25104;&#27169;&#22411;&#36890;&#24120;&#29992;&#20110;&#35299;&#20915;&#21160;&#24577;&#22270;&#20013;&#33410;&#28857;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#38754;&#20020;&#30340;&#32422;&#26463;&#21253;&#25324;&#37051;&#25509;&#20851;&#31995;&#30340;&#21333;&#35843;&#24615;&#65292;&#20026;&#33410;&#28857;&#26500;&#24314;&#22810;&#32500;&#29305;&#24449;&#30340;&#22256;&#38590;&#65292;&#20197;&#21450;&#32570;&#20047;&#31471;&#21040;&#31471;&#29983;&#25104;&#22810;&#31867;&#33410;&#28857;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CGGM&#30340;&#26032;&#39062;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#29983;&#25104;&#23569;&#25968;&#31867;&#21035;&#20013;&#26356;&#22810;&#33410;&#28857;&#12290;&#36890;&#36807;&#33258;&#36866;&#24212;&#31232;&#30095;&#24615;&#29983;&#25104;&#37051;&#25509;&#30697;&#38453;&#30340;&#26426;&#21046;&#22686;&#24378;&#20102;&#20854;&#32467;&#26500;&#30340;&#28789;&#27963;&#24615;&#12290;&#29305;&#24449;&#29983;&#25104;&#27169;&#22359;&#21517;&#20026;&#22810;&#32500;&#29305;&#24449;&#29983;&#25104;&#22120;&#65288;MFG&#65289;&#65292;&#21487;&#29983;&#25104;&#21253;&#25324;&#25299;&#25169;&#20449;&#24687;&#22312;&#20869;&#30340;&#33410;&#28857;&#29305;&#24449;&#12290;&#26631;&#31614;&#34987;&#36716;&#25442;&#20026;&#23884;&#20837;&#21521;&#37327;&#65292;&#29992;&#20316;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17363v1 Announce Type: cross  Abstract: Dynamic graphs are extensively employed for detecting anomalous behavior in nodes within the Internet of Things (IoT). Generative models are often used to address the issue of imbalanced node categories in dynamic graphs. Nevertheless, the constraints it faces include the monotonicity of adjacency relationships, the difficulty in constructing multi-dimensional features for nodes, and the lack of a method for end-to-end generation of multiple categories of nodes. This paper presents a novel graph generation model, called CGGM, designed specifically to generate a larger number of nodes belonging to the minority class. The mechanism for generating an adjacency matrix, through adaptive sparsity, enhances flexibility in its structure. The feature generation module, called multidimensional features generator (MFG) to generate node features along with topological information. Labels are transformed into embedding vectors, serving as condition
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#25439;&#22833;&#20989;&#25968;&#26799;&#24230;&#32479;&#35745;&#21644;&#26679;&#26412;&#28857;&#27604;&#20363;&#65292;&#30740;&#31350;&#20102;PINNs&#23545;&#32463;&#27982;&#32764;&#38750;&#31283;&#23450;&#27969;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#21508;&#20010;&#36755;&#20837;&#31354;&#38388;&#23376;&#22495;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.17346</link><description>&lt;p&gt;
&#36890;&#36807;&#36755;&#20837;&#23376;&#22495;&#32423;&#21035;&#25439;&#22833;&#20989;&#25968;&#26799;&#24230;&#35299;&#26512;&#29702;&#35299; PINNs &#23545;&#32463;&#27982;&#32764;&#38750;&#31283;&#23450;&#27969;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Understanding the training of PINNs for unsteady flow past a plunging foil through the lens of input subdomain level loss function gradients
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17346
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#25439;&#22833;&#20989;&#25968;&#26799;&#24230;&#32479;&#35745;&#21644;&#26679;&#26412;&#28857;&#27604;&#20363;&#65292;&#30740;&#31350;&#20102;PINNs&#23545;&#32463;&#27982;&#32764;&#38750;&#31283;&#23450;&#27969;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#21508;&#20010;&#36755;&#20837;&#31354;&#38388;&#23376;&#22495;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21463;&#28024;&#20837;&#36793;&#30028;&#26041;&#27861;&#21551;&#21457;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#65292;&#21253;&#25324;&#31227;&#21160;&#36793;&#30028;&#21551;&#29992;&#30340; PINNs&#65288;MB-PINNs&#65289;&#65292;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#32463;&#27982;&#32764;&#38750;&#31283;&#23450;&#27969;&#32463;&#36807;&#36816;&#21160;&#20307;&#26102;&#20934;&#30830;&#37325;&#24314;&#36895;&#24230;&#21644;&#24674;&#22797;&#21387;&#21147;&#20316;&#20026;&#38544;&#34255;&#21464;&#37327;&#30340;&#33021;&#21147;&#12290;&#32771;&#34385;&#21040;&#32463;&#27982;&#32764;&#30340;&#27969;&#21160;&#65292;MB-PINNs &#22312;&#20840;&#23616;&#29289;&#29702;&#25439;&#22833;&#25918;&#26494;&#21644;&#29289;&#29702;&#23398;&#22522;&#30784;&#19979;&#37319;&#26679;&#26041;&#27861;&#30340;&#24433;&#21709;&#19979;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#36890;&#36807;&#29289;&#29702;&#25439;&#22833;&#25918;&#26494;&#21644;&#22522;&#20110;&#29289;&#29702;&#30340;&#19979;&#37319;&#26679;&#26041;&#27861;&#65292;&#35843;&#26597;&#21738;&#20010;&#36755;&#20837;&#31354;&#38388;&#23376;&#22495;&#23545;&#35757;&#32451;&#26377;&#36129;&#29486;&#12290;&#22312; MB-PINNs &#35757;&#32451;&#30340;&#32972;&#26223;&#19979;&#65292;&#23450;&#20041;&#20102;&#19977;&#20010;&#31354;&#38388;&#21306;&#22495;&#65306;&#36816;&#21160;&#20307;&#12289;&#23614;&#36857;&#21644;&#22806;&#37096;&#21306;&#22495;&#12290;&#20026;&#20102;&#37327;&#21270;&#21738;&#20010;&#31354;&#38388;&#21306;&#22495;&#39537;&#21160;&#20102;&#35757;&#32451;&#65292;&#20174;&#20998;&#21306;&#25439;&#22833;&#20998;&#37327;&#26799;&#24230;&#32479;&#35745;&#21644;&#27599;&#20010;&#21306;&#22495;&#20013;&#26679;&#26412;&#28857;&#30340;&#27604;&#20363;&#35745;&#31639;&#20102;&#20004;&#20010;&#26032;&#30340;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17346v1 Announce Type: cross  Abstract: Recently immersed boundary method-inspired physics-informed neural networks (PINNs) including the moving boundary-enabled PINNs (MB-PINNs) have shown the ability to accurately reconstruct velocity and recover pressure as a hidden variable for unsteady flow past moving bodies. Considering flow past a plunging foil, MB-PINNs were trained with global physics loss relaxation and also in conjunction with a physics-based undersampling method, obtaining good accuracy. The purpose of this study was to investigate which input spatial subdomain contributes to the training under the effect of physics loss relaxation and physics-based undersampling. In the context of MB-PINNs training, three spatial zones: the moving body, wake, and outer zones were defined. To quantify which spatial zone drives the training, two novel metrics are computed from the zonal loss component gradient statistics and the proportion of sample points in each zone. Results c
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LocalGCL&#30340;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#25513;&#30721;&#24314;&#27169;&#34917;&#20805;&#22320;&#25429;&#25417;&#23616;&#37096;&#22270;&#20449;&#24687;&#65292;&#20248;&#20110;&#20256;&#32479;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.17345</link><description>&lt;p&gt;
LocalGCL&#65306;&#38754;&#21521;&#22270;&#30340;&#23616;&#37096;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LocalGCL: Local-aware Contrastive Learning for Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17345
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LocalGCL&#30340;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#25513;&#30721;&#24314;&#27169;&#34917;&#20805;&#22320;&#25429;&#25417;&#23616;&#37096;&#22270;&#20449;&#24687;&#65292;&#20248;&#20110;&#20256;&#32479;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;GRL&#65289;&#26368;&#36817;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#65292;&#23427;&#23558;&#22270;&#19982;&#25299;&#25169;&#32467;&#26500;&#32534;&#30721;&#20026;&#20302;&#32500;&#23884;&#20837;&#12290;&#21516;&#26102;&#65292;&#25163;&#21160;&#27880;&#37322;&#22270;&#26631;&#31614;&#30340;&#32791;&#26102;&#21644;&#25104;&#26412;&#39640;&#26114;&#20419;&#20351;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#20316;&#20026;SSL&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#36890;&#36807;&#21306;&#20998;&#27491;&#36127;&#26679;&#26412;&#26469;&#23398;&#20064;&#20855;&#26377;&#21306;&#20998;&#24615;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#22270;&#25968;&#25454;&#26102;&#65292;&#23427;&#36807;&#20998;&#24378;&#35843;&#20840;&#23616;&#27169;&#24335;&#32780;&#24573;&#35270;&#20102;&#23616;&#37096;&#32467;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#21363;&#38754;&#21521;&#22270;&#30340;&#23616;&#37096;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#65288;\methname&#65289;&#65292;&#19982;&#26222;&#36890;&#23545;&#27604;&#23398;&#20064;&#30456;&#27604;&#65292;&#23427;&#36890;&#36807;&#22522;&#20110;&#25513;&#30721;&#30340;&#24314;&#27169;&#34917;&#20805;&#22320;&#25429;&#25417;&#23616;&#37096;&#22270;&#20449;&#24687;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;\methname &#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17345v1 Announce Type: cross  Abstract: Graph representation learning (GRL) makes considerable progress recently, which encodes graphs with topological structures into low-dimensional embeddings. Meanwhile, the time-consuming and costly process of annotating graph labels manually prompts the growth of self-supervised learning (SSL) techniques. As a dominant approach of SSL, Contrastive learning (CL) learns discriminative representations by differentiating between positive and negative samples. However, when applied to graph data, it overemphasizes global patterns while neglecting local structures. To tackle the above issue, we propose \underline{Local}-aware \underline{G}raph \underline{C}ontrastive \underline{L}earning (\textbf{\methnametrim}), a self-supervised learning framework that supplementarily captures local graph information with masking-based modeling compared with vanilla contrastive learning. Extensive experiments validate the superiority of \methname against st
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#19987;&#23478;&#23545;&#26410;&#34987;&#27979;&#37327;&#30340;&#25277;&#35937;&#23646;&#24615;&#30340;&#20559;&#22909;&#32435;&#20837;&#21040;&#26367;&#20195;&#24314;&#27169;&#20013;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.17343</link><description>&lt;p&gt;
&#36890;&#36807;&#20248;&#20808;&#24314;&#27169;&#25277;&#35937;&#23646;&#24615;&#22686;&#24378;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Enhanced Bayesian Optimization via Preferential Modeling of Abstract Properties
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#19987;&#23478;&#23545;&#26410;&#34987;&#27979;&#37327;&#30340;&#25277;&#35937;&#23646;&#24615;&#30340;&#20559;&#22909;&#32435;&#20837;&#21040;&#26367;&#20195;&#24314;&#27169;&#20013;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#39564;&#35774;&#35745;&#20248;&#21270;&#26159;&#35774;&#35745;&#21644;&#21457;&#29616;&#26032;&#20135;&#21697;&#21644;&#27969;&#31243;&#30340;&#20851;&#38190;&#39537;&#21160;&#22240;&#32032;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#26159;&#20248;&#21270;&#26114;&#36149;&#21644;&#40657;&#30418;&#23454;&#39564;&#35774;&#35745;&#36807;&#31243;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#23558;&#19987;&#23478;&#23545;&#26410;&#34987;&#27979;&#37327;&#30340;&#25277;&#35937;&#23646;&#24615;&#30340;&#20559;&#22909;&#32435;&#20837;&#21040;&#26367;&#20195;&#24314;&#27169;&#20013;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#21319;BO&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31574;&#30053;&#65292;&#21487;&#20197;&#22788;&#29702;&#20219;&#20309;&#19981;&#27491;&#30830;/&#35823;&#23548;&#24615;&#30340;&#19987;&#23478;&#20559;&#35265;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25910;&#25947;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17343v1 Announce Type: new  Abstract: Experimental (design) optimization is a key driver in designing and discovering new products and processes. Bayesian Optimization (BO) is an effective tool for optimizing expensive and black-box experimental design processes. While Bayesian optimization is a principled data-driven approach to experimental optimization, it learns everything from scratch and could greatly benefit from the expertise of its human (domain) experts who often reason about systems at different abstraction levels using physical properties that are not necessarily directly measured (or measurable). In this paper, we propose a human-AI collaborative Bayesian framework to incorporate expert preferences about unmeasured abstract properties into the surrogate modeling to further boost the performance of BO. We provide an efficient strategy that can also handle any incorrect/misleading expert bias in preferential judgments. We discuss the convergence behavior of our pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29615;&#22659;&#26080;&#32447;&#20449;&#21495;&#36827;&#34892;&#23460;&#22806;&#29615;&#22659;&#37325;&#24314;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23545;RF&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#22635;&#34917;&#20102;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2402.17336</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#22312;&#26080;&#32447;&#30005;&#20256;&#25773;&#36335;&#24452;&#19978;&#37325;&#24314;&#23460;&#22806;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
Outdoor Environment Reconstruction with Deep Learning on Radio Propagation Paths
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29615;&#22659;&#26080;&#32447;&#20449;&#21495;&#36827;&#34892;&#23460;&#22806;&#29615;&#22659;&#37325;&#24314;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23545;RF&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#22635;&#34917;&#20102;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#23460;&#22806;&#29615;&#22659;&#37325;&#24314;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#35270;&#35273;&#25216;&#26415;&#65292;&#22914;&#25668;&#24433;&#27979;&#37327;&#21644;&#28608;&#20809;&#38647;&#36798;&#65292;&#38754;&#20020;&#30528;&#35832;&#22914;&#35206;&#30422;&#33539;&#22260;&#21463;&#38480;&#12289;&#23481;&#26131;&#21463;&#29615;&#22659;&#26465;&#20214;&#24433;&#21709;&#12289;&#35745;&#31639;&#21644;&#33021;&#28304;&#38656;&#27714;&#39640;&#31561;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29615;&#22659;&#20013;&#30340;&#29615;&#22659;&#26080;&#32447;&#20449;&#21495;&#36827;&#34892;&#23460;&#22806;&#29615;&#22659;&#37325;&#24314;&#12290;&#36890;&#36807;&#20998;&#26512;&#23556;&#39057;&#65288;RF&#65289;&#25968;&#25454;&#65292;&#26412;&#25991;&#26088;&#22312;&#25512;&#26029;&#29615;&#22659;&#29305;&#24449;&#24182;&#25968;&#23383;&#37325;&#24314;&#23460;&#22806;&#29615;&#22659;&#12290;&#30740;&#31350;&#22312;&#21512;&#25104;RF&#25968;&#25454;&#38598;WAIR-D&#19978;&#36873;&#23450;&#30340;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65292;&#21162;&#21147;&#22635;&#34917;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17336v1 Announce Type: cross  Abstract: Conventional methods for outdoor environment reconstruction rely predominantly on vision-based techniques like photogrammetry and LiDAR, facing limitations such as constrained coverage, susceptibility to environmental conditions, and high computational and energy demands. These challenges are particularly pronounced in applications like augmented reality navigation, especially when integrated with wearable devices featuring constrained computational resources and energy budgets. In response, this paper proposes a novel approach harnessing ambient wireless signals for outdoor environment reconstruction. By analyzing radio frequency (RF) data, the paper aims to deduce the environmental characteristics and digitally reconstruct the outdoor surroundings. Investigating the efficacy of selected deep learning (DL) techniques on the synthetic RF dataset WAIR-D, the study endeavors to address the research gap in this domain. Two DL-driven appro
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#32858;&#31867;&#21644;&#25935;&#24863;&#24230;&#37319;&#26679;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#36873;&#25321;&#20195;&#34920;&#24615;&#25968;&#25454;&#23376;&#38598;&#26469;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#24494;&#35843;&#22522;&#30784;&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.17327</link><description>&lt;p&gt;
&#22522;&#20110;&#32858;&#31867;&#25935;&#24863;&#24230;&#37319;&#26679;&#30340;&#25968;&#25454;&#39640;&#25928;&#23398;&#20064;&#65306;&#22522;&#30784;&#27169;&#22411;&#21450;&#20854;&#24310;&#20280;
&lt;/p&gt;
&lt;p&gt;
Data-Efficient Learning via Clustering-Based Sensitivity Sampling: Foundation Models and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17327
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#32858;&#31867;&#21644;&#25935;&#24863;&#24230;&#37319;&#26679;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#36873;&#25321;&#20195;&#34920;&#24615;&#25968;&#25454;&#23376;&#38598;&#26469;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#24494;&#35843;&#22522;&#30784;&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#25968;&#25454;&#36873;&#25321;&#38382;&#39064;&#65292;&#20854;&#30446;&#30340;&#26159;&#36873;&#25321;&#19968;&#20010;&#23567;&#30340;&#20195;&#34920;&#24615;&#25968;&#25454;&#23376;&#38598;&#65292;&#21487;&#20197;&#29992;&#26469;&#39640;&#25928;&#22320;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;$k$-means&#32858;&#31867;&#21644;&#25935;&#24863;&#24230;&#37319;&#26679;&#30340;&#26032;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#12290;&#20551;&#35774;&#25105;&#20204;&#21487;&#20197;&#35775;&#38382;&#25968;&#25454;&#30340;&#23884;&#20837;&#34920;&#31034;&#65292;&#20854;&#20013;&#27169;&#22411;&#25439;&#22833;&#26159;H\"older&#36830;&#32493;&#30340;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36873;&#25321;&#19968;&#32452;&#8220;&#20856;&#22411;&#8221;&#30340;$k + 1/\varepsilon^2$&#20010;&#20803;&#32032;&#65292;&#36825;&#20123;&#20803;&#32032;&#30340;&#24179;&#22343;&#25439;&#22833;&#19982;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#24179;&#22343;&#25439;&#22833;&#23545;&#24212;&#65292;&#20056;&#20197;&#19968;&#20010;$(1\pm\varepsilon)$&#30340;&#22240;&#23376;&#24182;&#21152;&#19978;&#19968;&#20010;$\varepsilon \lambda \Phi_k$&#65292;&#20854;&#20013;$\Phi_k$&#20195;&#34920;&#36755;&#20837;&#23884;&#20837;&#30340;$k$-means&#25104;&#26412;&#65292;$\lambda$&#26159;H\"older&#24120;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24494;&#35843;&#22522;&#30784;&#27169;&#22411;&#19978;&#30340;&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#34920;&#26126;&#23427;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17327v1 Announce Type: new  Abstract: We study the data selection problem, whose aim is to select a small representative subset of data that can be used to efficiently train a machine learning model. We present a new data selection approach based on $k$-means clustering and sensitivity sampling. Assuming access to an embedding representation of the data with respect to which the model loss is H\"older continuous, our approach provably allows selecting a set of ``typical'' $k + 1/\varepsilon^2$ elements whose average loss corresponds to the average loss of the whole dataset, up to a multiplicative $(1\pm\varepsilon)$ factor and an additive $\varepsilon \lambda \Phi_k$, where $\Phi_k$ represents the $k$-means cost for the input embeddings and $\lambda$ is the H\"older constant.   We furthermore demonstrate the performance and scalability of our approach on fine-tuning foundation models and show that it outperforms state-of-the-art methods. We also show how it can be applied on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#26412;&#22320;&#23398;&#20064;&#26041;&#27861;AugLocal&#65292;&#36890;&#36807;&#26500;&#24314;&#36741;&#21161;&#32593;&#32476;&#26469;&#22686;&#24378;&#21508;&#38544;&#34255;&#23618;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#26412;&#22320;&#23398;&#20064;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#20013;&#19982;BP&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#30340;&#31934;&#24230;&#24046;&#36317;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.17318</link><description>&lt;p&gt;
&#36890;&#36807;&#22686;&#24378;&#30340;&#36741;&#21161;&#32593;&#32476;&#25193;&#23637;&#30417;&#30563;&#26412;&#22320;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Scaling Supervised Local Learning with Augmented Auxiliary Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#26412;&#22320;&#23398;&#20064;&#26041;&#27861;AugLocal&#65292;&#36890;&#36807;&#26500;&#24314;&#36741;&#21161;&#32593;&#32476;&#26469;&#22686;&#24378;&#21508;&#38544;&#34255;&#23618;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#26412;&#22320;&#23398;&#20064;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#20013;&#19982;BP&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#30340;&#31934;&#24230;&#24046;&#36317;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#20351;&#29992;&#20840;&#23616;&#35823;&#24046;&#20449;&#21495;&#36827;&#34892;&#31471;&#21040;&#31471;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#26082;&#19981;&#31526;&#21512;&#29983;&#29289;&#23398;&#23454;&#38469;&#65292;&#20063;&#23384;&#22312;&#26356;&#26032;&#38145;&#23450;&#38382;&#39064;&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#37327;&#20869;&#23384;&#28040;&#32791;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#26412;&#22320;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;AugLocal&#12290;AugLocal&#36890;&#36807;&#20174;&#20854;&#21518;&#32493;&#32593;&#32476;&#23618;&#20013;&#22343;&#21248;&#36873;&#25321;&#19968;&#23567;&#37096;&#20998;&#23618;&#26469;&#26500;&#24314;&#27599;&#20010;&#38544;&#34255;&#23618;&#30340;&#36741;&#21161;&#32593;&#32476;&#65292;&#20197;&#22686;&#24378;&#23427;&#20204;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17318v1 Announce Type: cross  Abstract: Deep neural networks are typically trained using global error signals that backpropagate (BP) end-to-end, which is not only biologically implausible but also suffers from the update locking problem and requires huge memory consumption. Local learning, which updates each layer independently with a gradient-isolated auxiliary network, offers a promising alternative to address the above problems. However, existing local learning methods are confronted with a large accuracy gap with the BP counterpart, particularly for large-scale networks. This is due to the weak coupling between local layers and their subsequent network layers, as there is no gradient communication across layers. To tackle this issue, we put forward an augmented local learning method, dubbed AugLocal. AugLocal constructs each hidden layer's auxiliary network by uniformly selecting a small subset of layers from its subsequent network layers to enhance their synergy. We al
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#37197;&#20934;&#26469;&#22686;&#24378;&#21512;&#25104;&#25968;&#25454;&#65292;&#25105;&#20204;&#25104;&#21151;&#35757;&#32451;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#21512;&#21367;&#31215;&#31639;&#27861;&#21644;transformers&#25216;&#26415;&#22635;&#34917;&#20102;&#30693;&#35782;&#24046;&#36317;&#65292;&#21462;&#24471;&#20102;0.9005&#30340;dice&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.17317</link><description>&lt;p&gt;
&#22914;&#20309;&#36194;&#24471;BraTS 2023&#25104;&#24180;&#33014;&#36136;&#30244;&#25361;&#25112;&#65311;&#20551;&#35013;&#32780;&#24050;&#65281;&#22686;&#24378;&#30340;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#21644;&#27169;&#22411;&#38598;&#25104;&#29992;&#20110;&#33041;&#32959;&#30244;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
How we won BraTS 2023 Adult Glioma challenge? Just faking it! Enhanced Synthetic Data Augmentation and Model Ensemble for brain tumour segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17317
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#37197;&#20934;&#26469;&#22686;&#24378;&#21512;&#25104;&#25968;&#25454;&#65292;&#25105;&#20204;&#25104;&#21151;&#35757;&#32451;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#21512;&#21367;&#31215;&#31639;&#27861;&#21644;transformers&#25216;&#26415;&#22635;&#34917;&#20102;&#30693;&#35782;&#24046;&#36317;&#65292;&#21462;&#24471;&#20102;0.9005&#30340;dice&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26159;&#39045;&#20869;&#32959;&#30244;&#20998;&#21106;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#20294;&#36825;&#38656;&#35201;&#22823;&#37327;&#39640;&#36136;&#37327;&#25968;&#25454;&#65292;&#23588;&#20854;&#22312;&#21307;&#23398;&#39046;&#22495;&#38590;&#20197;&#33719;&#24471;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#36807;&#20351;&#29992;&#38750;&#20256;&#32479;&#30340;&#25968;&#25454;&#22686;&#24378;&#26426;&#21046;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#37197;&#20934;&#34987;&#29992;&#26469;&#22823;&#37327;&#22686;&#21152;&#21487;&#29992;&#26679;&#26412;&#25968;&#65292;&#29992;&#20110;&#35757;&#32451;&#19977;&#20010;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20998;&#21035;&#29992;&#20110;&#39045;&#20869;&#32959;&#30244;&#20998;&#21106;&#30340;BraTS2023&#25361;&#25112;&#30340;&#31532;&#19968;&#20010;&#20219;&#21153;&#12290;&#31532;&#19968;&#20010;&#27169;&#22411;&#26159;&#26631;&#20934;nnU-Net&#65292;&#31532;&#20108;&#20010;&#26159;Swin UNETR&#65292;&#31532;&#19977;&#20010;&#26159;BraTS 2021&#25361;&#25112;&#30340;&#33719;&#32988;&#26041;&#26696;&#12290;&#25972;&#20010;&#27969;&#31243;&#22522;&#20110;nnU-Net&#23454;&#29616;&#65292;&#38500;&#20102;&#21512;&#25104;&#25968;&#25454;&#30340;&#29983;&#25104;&#12290;&#21367;&#31215;&#31639;&#27861;&#21644;transformers&#30340;&#20351;&#29992;&#33021;&#22815;&#22635;&#34917;&#24444;&#27492;&#30340;&#30693;&#35782;&#24046;&#36317;&#12290;&#20351;&#29992;&#26032;&#25351;&#26631;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#36798;&#21040;&#20102;0.9005&#30340;dice&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17317v1 Announce Type: cross  Abstract: Deep Learning is the state-of-the-art technology for segmenting brain tumours. However, this requires a lot of high-quality data, which is difficult to obtain, especially in the medical field. Therefore, our solutions address this problem by using unconventional mechanisms for data augmentation. Generative adversarial networks and registration are used to massively increase the amount of available samples for training three different deep learning models for brain tumour segmentation, the first task of the BraTS2023 challenge. The first model is the standard nnU-Net, the second is the Swin UNETR and the third is the winning solution of the BraTS 2021 Challenge. The entire pipeline is built on the nnU-Net implementation, except for the generation of the synthetic data. The use of convolutional algorithms and transformers is able to fill each other's knowledge gaps. Using the new metric, our best solution achieves the dice results 0.9005
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#20102;&#37327;&#23376;&#35745;&#31639;&#26426;&#22312;&#20272;&#35745;&#25345;&#20037;&#22270;&#20043;&#38388;&#36317;&#31163;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;Wasserstein&#36317;&#31163;&#21644;$d^{c}_{p}$&#36317;&#31163;&#30340;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;</title><link>https://arxiv.org/abs/2402.17295</link><description>&lt;p&gt;
&#25345;&#20037;&#22270;&#30340;&#37327;&#23376;&#36317;&#31163;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Quantum Distance Approximation for Persistence Diagrams
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17295
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#20102;&#37327;&#23376;&#35745;&#31639;&#26426;&#22312;&#20272;&#35745;&#25345;&#20037;&#22270;&#20043;&#38388;&#36317;&#31163;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;Wasserstein&#36317;&#31163;&#21644;$d^{c}_{p}$&#36317;&#31163;&#30340;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#23545;&#20110;&#22312;&#35768;&#22810;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#20998;&#31867;&#21644;&#32858;&#31867;&#20219;&#21153;&#21487;&#33021;&#20250;&#24456;&#26377;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#25552;&#20379;&#24635;&#32467;&#20851;&#20110;&#28508;&#22312;&#22797;&#26434;&#21644;&#39640;&#32500;&#25968;&#25454;&#38598;&#24418;&#29366;&#30340;&#37325;&#35201;&#20449;&#24687;&#30340;&#20108;&#32500;&#25345;&#20037;&#22270;&#12290;&#25345;&#20037;&#22270;&#30340;&#31354;&#38388;&#21487;&#20197;&#36171;&#20104;&#21508;&#31181;&#24230;&#37327;&#65292;&#27604;&#22914;Wasserstein&#36317;&#31163;&#65292;&#20854;&#20855;&#26377;&#32479;&#35745;&#32467;&#26500;&#65292;&#24182;&#20801;&#35768;&#23558;&#36825;&#20123;&#24635;&#32467;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#35745;&#31639;&#20004;&#20010;&#25345;&#20037;&#22270;&#20043;&#38388;&#30340;&#36317;&#31163;&#28041;&#21450;&#25214;&#21040;&#20004;&#20010;&#22270;&#30340;&#28857;&#20043;&#38388;&#30340;&#26368;&#20339;&#21305;&#37197;&#26041;&#24335;&#65292;&#23545;&#20110;&#20256;&#32479;&#35745;&#31639;&#26426;&#26469;&#35828;&#21487;&#33021;&#24182;&#19981;&#24635;&#26159;&#19968;&#39033;&#23481;&#26131;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#37327;&#23376;&#35745;&#31639;&#26426;&#35780;&#20272;&#25345;&#20037;&#22270;&#20043;&#38388;&#36317;&#31163;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;Wasserstein&#36317;&#31163;&#21644;$d^{c}_{p}$&#36317;&#31163;&#30340;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17295v1 Announce Type: cross  Abstract: Topological Data Analysis methods can be useful for classification and clustering tasks in many different fields as they can provide two dimensional persistence diagrams that summarize important information about the shape of potentially complex and high dimensional data sets. The space of persistence diagrams can be endowed with various metrics such as the Wasserstein distance which admit a statistical structure and allow to use these summaries for machine learning algorithms. However, computing the distance between two persistence diagrams involves finding an optimal way to match the points of the two diagrams and may not always be an easy task for classical computers. In this work we explore the potential of quantum computers to estimate the distance between persistence diagrams, in particular we propose variational quantum algorithms for the Wasserstein distance as well as the $d^{c}_{p}$ distance. Our implementation is a weighted 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#26032;&#39062;&#24615;&#30340;&#22522;&#20110;&#26680;&#30340;&#29109;&#26032;&#39062;&#24615; (KEN) &#20998;&#25968;</title><link>https://arxiv.org/abs/2402.17287</link><description>&lt;p&gt;
&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#29983;&#25104;&#27169;&#22411;&#29109;&#20540;&#26032;&#39062;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Interpretable Evaluation of Entropy-based Novelty of Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17287
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#26032;&#39062;&#24615;&#30340;&#22522;&#20110;&#26680;&#30340;&#29109;&#26032;&#39062;&#24615; (KEN) &#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#26694;&#26550;&#21644;&#26550;&#26500;&#30340;&#24040;&#22823;&#21457;&#23637;&#38656;&#35201;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#22411;&#30456;&#23545;&#20110;&#21442;&#32771;&#25968;&#25454;&#38598;&#25110;&#22522;&#32447;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#39062;&#24615;&#12290; &#34429;&#28982;&#26368;&#36817;&#30340;&#25991;&#29486;&#24050;&#24191;&#27867;&#30740;&#31350;&#20102;&#29983;&#25104;&#27169;&#22411;&#30340;&#36136;&#37327;&#12289;&#22810;&#26679;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#35780;&#20272;&#65292;&#20294;&#19982;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#30340;&#27169;&#22411;&#26032;&#39062;&#24615;&#35780;&#20272;&#22312;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20013;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#22810;&#27169;&#24577;&#29983;&#25104;&#27169;&#22411;&#19979;&#30340;&#26032;&#39062;&#24615;&#35780;&#20272;&#65292;&#24182;&#23581;&#35797;&#22238;&#31572;&#20197;&#19979;&#38382;&#39064;&#65306;&#32473;&#23450;&#29983;&#25104;&#27169;&#22411; $\mathcal{G}$ &#30340;&#26679;&#26412;&#21644;&#21442;&#32771;&#25968;&#25454;&#38598; $\mathcal{S}$&#65292;&#25105;&#20204;&#22914;&#20309;&#21457;&#29616;&#24182;&#35745;&#31639; $\mathcal{G}$ &#27604; $\mathcal{S}$ &#20013;&#26356;&#39057;&#32321;&#22320;&#34920;&#36798;&#30340;&#27169;&#24335;&#12290; &#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#35889;&#26041;&#27861;&#26469;&#25551;&#36848;&#36825;&#19968;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#26680;&#30340;&#29109;&#26032;&#39062;&#24615; (KEN) &#20998;&#25968;&#26469;&#37327;&#21270;&#22522;&#20110;&#27169;&#24335;&#30340;&#26032;&#39062;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17287v1 Announce Type: new  Abstract: The massive developments of generative model frameworks and architectures require principled methods for the evaluation of a model's novelty compared to a reference dataset or baseline generative models. While the recent literature has extensively studied the evaluation of the quality, diversity, and generalizability of generative models, the assessment of a model's novelty compared to a baseline model has not been adequately studied in the machine learning community. In this work, we focus on the novelty assessment under multi-modal generative models and attempt to answer the following question: Given the samples of a generative model $\mathcal{G}$ and a reference dataset $\mathcal{S}$, how can we discover and count the modes expressed by $\mathcal{G}$ more frequently than in $\mathcal{S}$. We introduce a spectral approach to the described task and propose the Kernel-based Entropic Novelty (KEN) score to quantify the mode-based novelty 
&lt;/p&gt;</description></item><item><title>&#35843;&#26597;&#20102;&#22810;&#26234;&#33021;&#20307;&#12289;&#20154;&#26234;&#33021;&#20307;&#21644;&#20154;&#24037;&#26234;&#33021;&#26234;&#33021;&#20307;&#22312;&#31038;&#20250;&#22256;&#22659;&#21512;&#20316;&#20013;&#30340;&#19977;&#20010;&#20851;&#38190;&#39046;&#22495;&#65292;&#35752;&#35770;&#20102;&#21512;&#20316;&#30340;&#21160;&#26426;&#12289;&#31574;&#30053;&#12289;&#20154;&#31867;&#20559;&#35265;&#65292;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.17270</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#12289;&#20154;&#26234;&#33021;&#20307;&#21450;&#20854;&#36827;&#23637;&#65306;&#21512;&#20316;&#22312;&#31038;&#20250;&#22256;&#22659;&#20013;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent, Human-Agent and Beyond: A Survey on Cooperation in Social Dilemmas
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17270
&lt;/p&gt;
&lt;p&gt;
&#35843;&#26597;&#20102;&#22810;&#26234;&#33021;&#20307;&#12289;&#20154;&#26234;&#33021;&#20307;&#21644;&#20154;&#24037;&#26234;&#33021;&#26234;&#33021;&#20307;&#22312;&#31038;&#20250;&#22256;&#22659;&#21512;&#20316;&#20013;&#30340;&#19977;&#20010;&#20851;&#38190;&#39046;&#22495;&#65292;&#35752;&#35770;&#20102;&#21512;&#20316;&#30340;&#21160;&#26426;&#12289;&#31574;&#30053;&#12289;&#20154;&#31867;&#20559;&#35265;&#65292;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20250;&#22256;&#22659;&#20013;&#30740;&#31350;&#21512;&#20316;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#21508;&#31181;&#23398;&#31185;&#30340;&#22522;&#26412;&#35838;&#39064;&#65292;&#21253;&#25324;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#12290;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#26174;&#33879;&#37325;&#22609;&#20102;&#36825;&#19968;&#39046;&#22495;&#65292;&#20026;&#29702;&#35299;&#21644;&#22686;&#24378;&#21512;&#20316;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#26412;&#35843;&#26597;&#32771;&#23519;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#31038;&#20250;&#22256;&#22659;&#21512;&#20316;&#20132;&#27719;&#22788;&#30340;&#19977;&#20010;&#20851;&#38190;&#39046;&#22495;&#12290;&#39318;&#20808;&#65292;&#30528;&#37325;&#20110;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;&#25903;&#25345;&#29702;&#24615;&#26234;&#33021;&#20307;&#20043;&#38388;&#21512;&#20316;&#30340;&#20869;&#22312;&#21644;&#22806;&#22312;&#21160;&#26426;&#65292;&#20197;&#21450;&#29992;&#20110;&#21046;&#23450;&#26377;&#25928;&#31574;&#30053;&#23545;&#25239;&#19981;&#21516;&#23545;&#25163;&#30340;&#26041;&#27861;&#12290;&#20854;&#27425;&#65292;&#25506;&#35752;&#20102;&#20154;&#26234;&#33021;&#20307;&#21512;&#20316;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24403;&#21069;&#29992;&#20110;&#19982;&#20154;&#31867;&#21512;&#20316;&#30340;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#65292;&#20197;&#21450;&#20154;&#31867;&#23545;&#20154;&#24037;&#26234;&#33021;&#26234;&#33021;&#20307;&#30340;&#20559;&#35265;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#26234;&#33021;&#20307;&#22686;&#24378;&#20154;&#31867;&#21512;&#20316;&#30340;&#26032;&#20852;&#39046;&#22495;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#65292;&#20363;&#22914; u
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17270v1 Announce Type: new  Abstract: The study of cooperation within social dilemmas has long been a fundamental topic across various disciplines, including computer science and social science. Recent advancements in Artificial Intelligence (AI) have significantly reshaped this field, offering fresh insights into understanding and enhancing cooperation. This survey examines three key areas at the intersection of AI and cooperation in social dilemmas. First, focusing on multi-agent cooperation, we review the intrinsic and external motivations that support cooperation among rational agents, and the methods employed to develop effective strategies against diverse opponents. Second, looking into human-agent cooperation, we discuss the current AI algorithms for cooperating with humans and the human biases towards AI agents. Third, we review the emergent field of leveraging AI agents to enhance cooperation among humans. We conclude by discussing future research avenues, such as u
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;&#21644;&#35838;&#31243;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#25552;&#21319;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#22312;&#22788;&#29702;&#24773;&#24863;&#21464;&#21270;&#21644;&#25968;&#25454;&#19981;&#24179;&#34913;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.17269</link><description>&lt;p&gt;
&#35838;&#31243;&#23398;&#20064;&#36935;&#35265;&#26377;&#21521;&#26080;&#29615;&#22270;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Curriculum Learning Meets Directed Acyclic Graph for Multimodal Emotion Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17269
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;&#21644;&#35838;&#31243;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#25552;&#21319;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#22312;&#22788;&#29702;&#24773;&#24863;&#21464;&#21270;&#21644;&#25968;&#25454;&#19981;&#24179;&#34913;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;MultiDAG+CL&#65292;&#29992;&#20110;&#20250;&#35805;&#20013;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#65288;ERC&#65289;&#65292;&#23427;&#21033;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20869;&#38598;&#25104;&#25991;&#26412;&#12289;&#22768;&#38899;&#21644;&#35270;&#35273;&#29305;&#24449;&#12290;&#27169;&#22411;&#36890;&#36807;&#35838;&#31243;&#23398;&#20064;&#65288;CL&#65289;&#36827;&#34892;&#22686;&#24378;&#65292;&#20197;&#24212;&#23545;&#24773;&#24863;&#21464;&#21270;&#21644;&#25968;&#25454;&#19981;&#24179;&#34913;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#35838;&#31243;&#23398;&#20064;&#36890;&#36807;&#36880;&#28176;&#20197;&#26377;&#24847;&#20041;&#30340;&#39034;&#24207;&#21576;&#29616;&#35757;&#32451;&#26679;&#26412;&#26469;&#20419;&#36827;&#23398;&#20064;&#36807;&#31243;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#22788;&#29702;&#24773;&#32490;&#21464;&#21270;&#21644;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#24615;&#33021;&#12290;&#22312;IEMOCAP&#21644;MELD&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MultiDAG+CL&#27169;&#22411;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17269v1 Announce Type: new  Abstract: Emotion recognition in conversation (ERC) is a crucial task in natural language processing and affective computing. This paper proposes MultiDAG+CL, a novel approach for Multimodal Emotion Recognition in Conversation (ERC) that employs Directed Acyclic Graph (DAG) to integrate textual, acoustic, and visual features within a unified framework. The model is enhanced by Curriculum Learning (CL) to address challenges related to emotional shifts and data imbalance. Curriculum learning facilitates the learning process by gradually presenting training samples in a meaningful order, thereby improving the model's performance in handling emotional variations and data imbalance. Experimental results on the IEMOCAP and MELD datasets demonstrate that the MultiDAG+CL models outperform baseline models.
&lt;/p&gt;</description></item><item><title>RIME&#26159;&#19968;&#31181;&#38024;&#23545;&#22024;&#26434;&#20559;&#22909;&#30340;&#20581;&#22766;PbRL&#31639;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#36807;&#28388;&#21435;&#22122;&#20559;&#22909;&#21644;&#28909;&#21551;&#21160;&#22870;&#21169;&#27169;&#22411;&#65292;&#26497;&#22823;&#22686;&#24378;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;PbRL&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17257</link><description>&lt;p&gt;
RIME: &#20855;&#26377;&#22024;&#26434;&#20559;&#22909;&#30340;&#20581;&#22766;&#20559;&#22909;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RIME: Robust Preference-based Reinforcement Learning with Noisy Preferences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17257
&lt;/p&gt;
&lt;p&gt;
RIME&#26159;&#19968;&#31181;&#38024;&#23545;&#22024;&#26434;&#20559;&#22909;&#30340;&#20581;&#22766;PbRL&#31639;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#36807;&#28388;&#21435;&#22122;&#20559;&#22909;&#21644;&#28909;&#21551;&#21160;&#22870;&#21169;&#27169;&#22411;&#65292;&#26497;&#22823;&#22686;&#24378;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;PbRL&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#22909;&#24378;&#21270;&#23398;&#20064;&#65288;PbRL&#65289;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#20559;&#22909;&#20316;&#20026;&#22870;&#21169;&#20449;&#21495;&#65292;&#36991;&#20813;&#20102;&#23545;&#22870;&#21169;&#35774;&#35745;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;PbRL&#31639;&#27861;&#36807;&#24230;&#20381;&#36182;&#26469;&#33258;&#39046;&#22495;&#19987;&#23478;&#30340;&#39640;&#36136;&#37327;&#21453;&#39304;&#65292;&#23548;&#33268;&#32570;&#20047;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RIME&#65292;&#19968;&#31181;&#38024;&#23545;&#22024;&#26434;&#20559;&#22909;&#30340;&#20581;&#22766;PbRL&#31639;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#20174;&#22024;&#26434;&#20559;&#22909;&#20013;&#23398;&#20064;&#22870;&#21169;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#22522;&#20110;&#26679;&#26412;&#36873;&#25321;&#30340;&#37492;&#21035;&#22120;&#65292;&#21160;&#24577;&#36807;&#28388;&#21435;&#22122;&#20559;&#22909;&#20197;&#36827;&#34892;&#20581;&#22766;&#35757;&#32451;&#12290;&#20026;&#20102;&#20943;&#36731;&#36873;&#25321;&#19981;&#27491;&#30830;&#36896;&#25104;&#30340;&#32047;&#31215;&#35823;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#28909;&#21551;&#21160;&#22870;&#21169;&#27169;&#22411;&#65292;&#27492;&#22806;&#36824;&#33021;&#22635;&#34917;PbRL&#20013;&#20174;&#39044;&#35757;&#32451;&#21040;&#22312;&#32447;&#35757;&#32451;&#36807;&#28193;&#26102;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#25105;&#20204;&#22312;&#26426;&#22120;&#20154;&#25805;&#32437;&#21644;&#36816;&#21160;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;RIME&#26174;&#33879;&#25552;&#21319;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;PbRL&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;&#28040;&#34701;&#30740;&#31350;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#28909;&#21551;&#21160;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17257v1 Announce Type: cross  Abstract: Preference-based Reinforcement Learning (PbRL) avoids the need for reward engineering by harnessing human preferences as the reward signal. However, current PbRL algorithms over-reliance on high-quality feedback from domain experts, which results in a lack of robustness. In this paper, we present RIME, a robust PbRL algorithm for effective reward learning from noisy preferences. Our method incorporates a sample selection-based discriminator to dynamically filter denoised preferences for robust training. To mitigate the accumulated error caused by incorrect selection, we propose to warm start the reward model, which additionally bridges the performance gap during transition from pre-training to online training in PbRL. Our experiments on robotic manipulation and locomotion tasks demonstrate that RIME significantly enhances the robustness of the current state-of-the-art PbRL method. Ablation studies further demonstrate that the warm star
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#21644;&#38543;&#26426;&#26862;&#26519;&#30340;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#39044;&#27979;&#23618;&#20013;&#35835;&#21462;&#22270;&#20687;&#12289;&#21512;&#25104;&#35821;&#38899;&#20197;&#21450;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.17249</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35821;&#38899;&#21644;&#35270;&#35273;&#21512;&#25104;&#22312;&#22810;&#23618;&#33258;&#36866;&#24212;&#26694;&#26550;&#20013;&#25913;&#36827;&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Learning-Based Speech and Vision Synthesis to Improve Phishing Attack Detection through a Multi-layer Adaptive Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17249
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#21644;&#38543;&#26426;&#26862;&#26519;&#30340;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#39044;&#27979;&#23618;&#20013;&#35835;&#21462;&#22270;&#20687;&#12289;&#21512;&#25104;&#35821;&#38899;&#20197;&#21450;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25915;&#20987;&#32773;&#19981;&#26029;&#28436;&#36827;&#30340;&#26041;&#24335;&#25345;&#32493;&#25913;&#36827;&#20854;&#27450;&#39575;&#25216;&#26415;&#65292;&#20197;&#32469;&#36807;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#32593;&#32476;&#38035;&#40060;&#26816;&#27979;&#26041;&#27861;&#65292;&#32473;&#34892;&#19994;&#21644;&#23398;&#26415;&#30740;&#31350;&#20154;&#21592;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#24403;&#21069;&#26041;&#27861;&#26080;&#27861;&#26816;&#27979;&#22797;&#26434;&#30340;&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#12290;&#22240;&#27492;&#65292;&#30001;&#20110;&#25915;&#20987;&#32773;&#37319;&#29992;&#30340;&#31574;&#30053;&#26085;&#30410;&#22797;&#26434;&#19988;&#26032;&#31574;&#30053;&#19981;&#26029;&#34987;&#24320;&#21457;&#20197;&#36867;&#36991;&#26816;&#27979;&#65292;&#24403;&#21069;&#21453;&#32593;&#32476;&#38035;&#40060;&#26041;&#27861;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#22797;&#26434;&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35843;&#25972;&#30340;&#26694;&#26550;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#21644;&#38543;&#26426;&#26862;&#26519;&#32467;&#21512;&#36215;&#26469;&#65292;&#20174;&#28145;&#24230;&#20266;&#36896;&#35270;&#39057;&#20013;&#35835;&#21462;&#22270;&#20687;&#65292;&#21512;&#25104;&#35821;&#38899;&#65292;&#20197;&#21450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#24182;&#22312;&#21508;&#31181;&#39044;&#27979;&#23618;&#20013;&#26174;&#33879;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17249v1 Announce Type: cross  Abstract: The ever-evolving ways attacker continues to im prove their phishing techniques to bypass existing state-of-the-art phishing detection methods pose a mountain of challenges to researchers in both industry and academia research due to the inability of current approaches to detect complex phishing attack. Thus, current anti-phishing methods remain vulnerable to complex phishing because of the increasingly sophistication tactics adopted by attacker coupled with the rate at which new tactics are being developed to evade detection. In this research, we proposed an adaptable framework that combines Deep learning and Randon Forest to read images, synthesize speech from deep-fake videos, and natural language processing at various predictions layered to significantly increase the performance of machine learning models for phishing attack detection.
&lt;/p&gt;</description></item><item><title>SDR-Former&#26159;&#19968;&#31181;&#38024;&#23545;&#32925;&#33039;&#30149;&#21464;&#20998;&#31867;&#32780;&#35774;&#35745;&#30340;&#26032;&#22411;Transformer&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#20849;&#28966;&#31070;&#32463;&#32593;&#32476;&#21644;&#28151;&#21512;&#21452;&#20998;&#36776;&#29575;Transformer&#65292;&#33021;&#22815;&#22312;3D&#22810;&#30456;CT&#21644;MR&#25104;&#20687;&#20013;&#22788;&#29702;&#22810;&#30456;&#36755;&#20837;&#65292;&#25552;&#21319;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.17246</link><description>&lt;p&gt;
SDR-Former&#65306;&#29992;&#20110;3D&#22810;&#30456;&#25104;&#20687;&#30340;&#20849;&#28966;&#21452;&#20998;&#36776;&#29575;Transformer&#36827;&#34892;&#32925;&#33039;&#30149;&#21464;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
SDR-Former: A Siamese Dual-Resolution Transformer for Liver Lesion Classification Using 3D Multi-Phase Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17246
&lt;/p&gt;
&lt;p&gt;
SDR-Former&#26159;&#19968;&#31181;&#38024;&#23545;&#32925;&#33039;&#30149;&#21464;&#20998;&#31867;&#32780;&#35774;&#35745;&#30340;&#26032;&#22411;Transformer&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#20849;&#28966;&#31070;&#32463;&#32593;&#32476;&#21644;&#28151;&#21512;&#21452;&#20998;&#36776;&#29575;Transformer&#65292;&#33021;&#22815;&#22312;3D&#22810;&#30456;CT&#21644;MR&#25104;&#20687;&#20013;&#22788;&#29702;&#22810;&#30456;&#36755;&#20837;&#65292;&#25552;&#21319;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17246v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495;&#25688;&#35201;&#65306;&#22810;&#30456;CT&#21644;MR&#25195;&#25551;&#20013;&#32925;&#33039;&#30149;&#21464;&#30340;&#33258;&#21160;&#20998;&#31867;&#20855;&#26377;&#20020;&#24202;&#24847;&#20041;&#65292;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Siamese Dual-Resolution Transformer&#65288;SDR-Former&#65289;&#26694;&#26550;&#65292;&#19987;&#20026;&#22788;&#29702;3D&#22810;&#30456;CT&#21644;MR&#25104;&#20687;&#20013;&#30340;&#32925;&#33039;&#30149;&#21464;&#20998;&#31867;&#38382;&#39064;&#32780;&#35774;&#35745;&#65292;&#20855;&#26377;&#19981;&#21516;&#30456;&#20301;&#35745;&#25968;&#12290;&#25152;&#25552;&#20986;&#30340;SDR-Former&#21033;&#29992;&#20102;&#31616;&#21270;&#30340;&#20849;&#28966;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#26469;&#22788;&#29702;&#22810;&#30456;&#25104;&#20687;&#36755;&#20837;&#65292;&#20855;&#26377;&#31283;&#20581;&#30340;&#29305;&#24449;&#34920;&#31034;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#12290;SNN&#30340;&#26435;&#37325;&#20849;&#20139;&#29305;&#24615;&#36890;&#36807;&#28151;&#21512;&#21452;&#20998;&#36776;&#29575;Transformer&#65288;DR-Former&#65289;&#36827;&#19968;&#27493;&#20016;&#23500;&#65292;&#21253;&#25324;&#29992;&#20110;&#22788;&#29702;&#39640;&#20998;&#36776;&#29575;&#21644;&#20302;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;3D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#23450;&#21046;&#21270;3D Transformer&#12290;&#36825;&#31181;&#28151;&#21512;&#23376;&#26550;&#26500;&#25797;&#38271;&#25429;&#25417;&#35814;&#32454;&#30340;&#23616;&#37096;&#29305;&#24449;&#21644;&#29702;&#35299;&#20840;&#23616;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#21319;SNN&#30340;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17246v1 Announce Type: cross  Abstract: Automated classification of liver lesions in multi-phase CT and MR scans is of clinical significance but challenging. This study proposes a novel Siamese Dual-Resolution Transformer (SDR-Former) framework, specifically designed for liver lesion classification in 3D multi-phase CT and MR imaging with varying phase counts. The proposed SDR-Former utilizes a streamlined Siamese Neural Network (SNN) to process multi-phase imaging inputs, possessing robust feature representations while maintaining computational efficiency. The weight-sharing feature of the SNN is further enriched by a hybrid Dual-Resolution Transformer (DR-Former), comprising a 3D Convolutional Neural Network (CNN) and a tailored 3D Transformer for processing high- and low-resolution images, respectively. This hybrid sub-architecture excels in capturing detailed local features and understanding global contextual information, thereby, boosting the SNN's feature extraction ca
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#36127;&#37319;&#26679;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#23545;&#36127;&#37319;&#26679;&#30340;&#21382;&#21490;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#35752;&#65292;&#23558;&#24403;&#21069;&#30340;&#36127;&#37319;&#26679;&#26041;&#27861;&#20998;&#31867;&#20026;&#38745;&#24577;&#12289;&#22256;&#38590;&#12289;&#22522;&#20110;GAN&#12289;&#22522;&#20110;&#36741;&#21161;&#21644;&#25209;&#20869;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.17238</link><description>&lt;p&gt;
&#36127;&#37319;&#26679;&#37325;&#35201;&#21527;&#65311;&#23545;&#20854;&#29702;&#35770;&#21644;&#24212;&#29992;&#30340;&#32508;&#36848;&#19982;&#27934;&#23519;
&lt;/p&gt;
&lt;p&gt;
Does Negative Sampling Matter? A Review with Insights into its Theory and Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17238
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#36127;&#37319;&#26679;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#23545;&#36127;&#37319;&#26679;&#30340;&#21382;&#21490;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#35752;&#65292;&#23558;&#24403;&#21069;&#30340;&#36127;&#37319;&#26679;&#26041;&#27861;&#20998;&#31867;&#20026;&#38745;&#24577;&#12289;&#22256;&#38590;&#12289;&#22522;&#20110;GAN&#12289;&#22522;&#20110;&#36741;&#21161;&#21644;&#25209;&#20869;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36127;&#37319;&#26679;&#36805;&#36895;&#25104;&#20026;&#30740;&#31350;&#30340;&#28966;&#28857;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#28085;&#30422;&#26426;&#22120;&#23398;&#20064;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#25968;&#25454;&#25366;&#25496;&#21644;&#25512;&#33616;&#31995;&#32479;&#12290;&#36825;&#31181;&#26085;&#30410;&#22686;&#38271;&#30340;&#20852;&#36259;&#24341;&#21457;&#20102;&#20960;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#36127;&#37319;&#26679;&#30495;&#30340;&#24456;&#37325;&#35201;&#21527;&#65311;&#26159;&#21542;&#23384;&#22312;&#19968;&#20010;&#33021;&#22815;&#25972;&#21512;&#25152;&#26377;&#29616;&#26377;&#36127;&#37319;&#26679;&#26041;&#27861;&#30340;&#36890;&#29992;&#26694;&#26550;&#65311;&#23427;&#22312;&#21738;&#20123;&#39046;&#22495;&#34987;&#24212;&#29992;&#65311;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#36127;&#37319;&#26679;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#28145;&#20837;&#25506;&#35752;&#20102;&#36127;&#37319;&#26679;&#30340;&#21382;&#21490;&#65292;&#25105;&#20204;&#36890;&#36807;&#20116;&#20010;&#28436;&#21270;&#36335;&#24452;&#36861;&#28335;&#20102;&#36127;&#37319;&#26679;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#21078;&#26512;&#24182;&#20998;&#31867;&#20102;&#36873;&#25321;&#36127;&#26679;&#26412;&#20505;&#36873;&#30340;&#31574;&#30053;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#20840;&#23616;&#12289;&#23616;&#37096;&#12289;&#23567;&#25209;&#37327;&#12289;&#36339;&#36291;&#21644;&#22522;&#20110;&#20869;&#23384;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#23558;&#24403;&#21069;&#30340;&#36127;&#37319;&#26679;&#26041;&#27861;&#20998;&#31867;&#20026;&#38745;&#24577;&#12289;&#22256;&#38590;&#12289;&#22522;&#20110;GAN&#12289;&#22522;&#20110;&#36741;&#21161;&#21644;&#25209;&#20869;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17238v1 Announce Type: new  Abstract: Negative sampling has swiftly risen to prominence as a focal point of research, with wide-ranging applications spanning machine learning, computer vision, natural language processing, data mining, and recommender systems. This growing interest raises several critical questions: Does negative sampling really matter? Is there a general framework that can incorporate all existing negative sampling methods? In what fields is it applied? Addressing these questions, we propose a general framework that leverages negative sampling. Delving into the history of negative sampling, we trace the development of negative sampling through five evolutionary paths. We dissect and categorize the strategies used to select negative sample candidates, detailing global, local, mini-batch, hop, and memory-based approaches. Our review categorizes current negative sampling methods into five types: static, hard, GAN-based, Auxiliary-based, and In-batch methods, pr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#32508;&#36848;&#20102;&#20010;&#24615;&#21270;&#25945;&#32946;&#20013;&#25968;&#25454;&#25366;&#25496;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#30528;&#37325;&#20110;&#25945;&#32946;&#25512;&#33616;&#12289;&#35748;&#30693;&#35786;&#26029;&#12289;&#30693;&#35782;&#36861;&#36394;&#21644;&#23398;&#20064;&#20998;&#26512;&#22235;&#20010;&#20027;&#35201;&#22330;&#26223;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.17236</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#25945;&#32946;&#20013;&#30340;&#25968;&#25454;&#25366;&#25496;&#32508;&#36848;&#65306;&#24403;&#21069;&#36235;&#21183;&#19982;&#26410;&#26469;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
A Review of Data Mining in Personalized Education: Current Trends and Future Prospects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17236
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#32508;&#36848;&#20102;&#20010;&#24615;&#21270;&#25945;&#32946;&#20013;&#25968;&#25454;&#25366;&#25496;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#30528;&#37325;&#20110;&#25945;&#32946;&#25512;&#33616;&#12289;&#35748;&#30693;&#35786;&#26029;&#12289;&#30693;&#35782;&#36861;&#36394;&#21644;&#23398;&#20064;&#20998;&#26512;&#22235;&#20010;&#20027;&#35201;&#22330;&#26223;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#25945;&#32946;&#65292;&#26681;&#25454;&#20010;&#20154;&#23398;&#29983;&#38656;&#27714;&#23450;&#21046;&#65292;&#21033;&#29992;&#25945;&#32946;&#25216;&#26415;&#21644;&#20154;&#24037;&#26234;&#33021;&#22312;&#25968;&#23383;&#21270;&#26102;&#20195;&#22686;&#24378;&#23398;&#20064;&#25928;&#26524;&#12290;&#20154;&#24037;&#26234;&#33021;&#22312;&#25945;&#32946;&#24179;&#21488;&#20013;&#30340;&#25972;&#21512;&#20026;&#23398;&#26415;&#34920;&#29616;&#12289;&#23398;&#20064;&#20559;&#22909;&#21644;&#34892;&#20026;&#25552;&#20379;&#20102;&#27934;&#23519;&#65292;&#20248;&#21270;&#20102;&#20010;&#24615;&#21270;&#23398;&#20064;&#36807;&#31243;&#12290;&#30001;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#39537;&#21160;&#65292;&#19981;&#20165;&#20351;&#23398;&#29983;&#21463;&#30410;&#65292;&#21516;&#26102;&#20026;&#25945;&#32946;&#32773;&#21644;&#26426;&#26500;&#25552;&#20379;&#20102;&#24037;&#20855;&#65292;&#20197;&#25171;&#36896;&#23450;&#21046;&#21270;&#23398;&#20064;&#20307;&#39564;&#12290;&#35813;&#35770;&#25991;&#30528;&#37325;&#20110;&#22235;&#20010;&#20027;&#35201;&#22330;&#26223;&#65306;&#25945;&#32946;&#25512;&#33616;&#12289;&#35748;&#30693;&#35786;&#26029;&#12289;&#30693;&#35782;&#36861;&#36394;&#21644;&#23398;&#20064;&#20998;&#26512;&#65292;&#20197;&#20840;&#38754;&#22238;&#39038;&#20010;&#24615;&#21270;&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#26412;&#25991;&#20026;&#27599;&#20010;&#39046;&#22495;&#25552;&#20379;&#20102;&#32467;&#26500;&#21270;&#20998;&#31867;&#65292;&#25972;&#29702;&#20102;&#24120;&#29992;&#25968;&#25454;&#38598;&#65292;&#24182;&#30830;&#23450;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#24378;&#35843;&#20102;&#25968;&#25454;&#25366;&#25496;&#22312;&#22686;&#24378;&#20010;&#24615;&#21270;&#25945;&#32946;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17236v1 Announce Type: cross  Abstract: Personalized education, tailored to individual student needs, leverages educational technology and artificial intelligence (AI) in the digital age to enhance learning effectiveness. The integration of AI in educational platforms provides insights into academic performance, learning preferences, and behaviors, optimizing the personal learning process. Driven by data mining techniques, it not only benefits students but also provides educators and institutions with tools to craft customized learning experiences. To offer a comprehensive review of recent advancements in personalized educational data mining, this paper focuses on four primary scenarios: educational recommendation, cognitive diagnosis, knowledge tracing, and learning analysis. This paper presents a structured taxonomy for each area, compiles commonly used datasets, and identifies future research directions, emphasizing the role of data mining in enhancing personalized educat
&lt;/p&gt;</description></item><item><title>&#38543;&#26426;&#26799;&#24230;&#36172;&#21338;&#26426;&#31639;&#27861;&#20197;&#24120;&#25968;&#27493;&#38271;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#20248;&#31574;&#30053;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#22122;&#22768;&#25511;&#21046;&#65292;&#21516;&#26102;&#33258;&#21160;&#23454;&#29616;&#24369;&#25506;&#32034;&#65292;&#30830;&#20445;&#27599;&#20010;&#21160;&#20316;&#34987;&#26080;&#38480;&#27425;&#37319;&#26679;&#12290;</title><link>https://arxiv.org/abs/2402.17235</link><description>&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#22312;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#21462;&#24471;&#25104;&#21151;
&lt;/p&gt;
&lt;p&gt;
Stochastic Gradient Succeeds for Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17235
&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#36172;&#21338;&#26426;&#31639;&#27861;&#20197;&#24120;&#25968;&#27493;&#38271;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#20248;&#31574;&#30053;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#22122;&#22768;&#25511;&#21046;&#65292;&#21516;&#26102;&#33258;&#21160;&#23454;&#29616;&#24369;&#25506;&#32034;&#65292;&#30830;&#20445;&#27599;&#20010;&#21160;&#20316;&#34987;&#26080;&#38480;&#27425;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;\emph{&#38543;&#26426;&#26799;&#24230;}&#36172;&#21338;&#26426;&#31639;&#27861;&#20197;$O(1/t)$&#30340;&#36895;&#24230;&#25910;&#25947;&#21040;&#19968;&#20010;\emph{&#20840;&#23616;&#26368;&#20248;}&#31574;&#30053;&#65292;&#21363;&#20351;&#37319;&#29992;\emph{&#24658;&#23450;}&#27493;&#38271;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23613;&#31649;&#38543;&#26426;&#26799;&#24230;&#36172;&#21338;&#26426;&#31639;&#27861;&#26159;&#19968;&#20010;&#24050;&#30693;&#36866;&#29992;&#20110;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#21476;&#32769;&#31639;&#27861;&#65292;&#20294;&#20854;&#20840;&#23616;&#25910;&#25947;&#24615;&#20197;&#21069;&#23578;&#26410;&#34987;&#35777;&#23454;&#12290;&#36890;&#36807;&#24314;&#31435;&#20004;&#20010;&#26032;&#39062;&#30340;&#25216;&#26415;&#21457;&#29616;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;&#36825;&#19968;&#26032;&#32467;&#26524;&#65306;&#39318;&#20808;&#65292;&#26799;&#24230;&#36172;&#21338;&#26426;&#31639;&#27861;&#20013;&#38543;&#26426;&#26356;&#26032;&#30340;&#22122;&#22768;&#28385;&#36275;&#24378;&#8220;&#22686;&#38271;&#26465;&#20214;&#8221;&#23646;&#24615;&#65292;&#21363;&#24403;&#36827;&#23637;&#21464;&#23567;&#26102;&#65292;&#26041;&#24046;&#20250;&#20943;&#23567;&#65292;&#36825;&#24847;&#21619;&#30528;&#36890;&#36807;&#20943;&#23567;&#27493;&#38271;&#26469;&#25511;&#21046;&#39069;&#22806;&#22122;&#22768;&#26159;&#19981;&#24517;&#35201;&#30340;&#65307;&#20854;&#27425;&#65292;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#26356;&#26032;&#33258;&#21160;&#23454;&#29616;&#20102;&#19968;&#31181;&#24418;&#24335;&#30340;&#8220;&#24369;&#25506;&#32034;&#8221;&#65292;&#22240;&#20026;&#23427;&#20204;&#38459;&#27490;&#34892;&#21160;&#27010;&#29575;&#20197;&#27604;$O(1/t)$&#26356;&#24555;&#30340;&#36895;&#24230;&#34928;&#20943;&#65292;&#20174;&#32780;&#30830;&#20445;&#27599;&#20010;&#21160;&#20316;&#34987;&#26080;&#38480;&#27425;&#37319;&#26679;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17235v1 Announce Type: new  Abstract: We show that the \emph{stochastic gradient} bandit algorithm converges to a \emph{globally optimal} policy at an $O(1/t)$ rate, even with a \emph{constant} step size. Remarkably, global convergence of the stochastic gradient bandit algorithm has not been previously established, even though it is an old algorithm known to be applicable to bandits. The new result is achieved by establishing two novel technical findings: first, the noise of the stochastic updates in the gradient bandit algorithm satisfies a strong ``growth condition'' property, where the variance diminishes whenever progress becomes small, implying that additional noise control via diminishing step sizes is unnecessary; second, a form of ``weak exploration'' is automatically achieved through the stochastic gradient updates, since they prevent the action probabilities from decaying faster than $O(1/t)$, thus ensuring that every action is sampled infinitely often with probabi
&lt;/p&gt;</description></item><item><title>&#28151;&#21512;&#27169;&#22411;&#23558;&#22522;&#20110;ODE&#30340;&#26426;&#26800;&#21160;&#21147;&#23398;&#19982;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#32467;&#21512;&#65292;&#22312;&#35299;&#37322;&#24615;&#21644;&#22240;&#26524;&#22522;&#30784;&#30340;&#21516;&#26102;&#65292;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#23545;&#27835;&#30103;&#25928;&#26524;&#36827;&#34892;&#25490;&#21517;&#65292;&#20174;&#32780;&#35299;&#20915;&#28789;&#27963;&#24615;&#22686;&#21152;&#24102;&#26469;&#30340;&#22240;&#26524;&#22522;&#30784;&#20002;&#22833;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.17233</link><description>&lt;p&gt;
&#28151;&#21512;&#26041;&#22359;&#31070;&#32463;ODE&#22240;&#26524;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Hybrid Square Neural ODE Causal Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17233
&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#27169;&#22411;&#23558;&#22522;&#20110;ODE&#30340;&#26426;&#26800;&#21160;&#21147;&#23398;&#19982;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#32467;&#21512;&#65292;&#22312;&#35299;&#37322;&#24615;&#21644;&#22240;&#26524;&#22522;&#30784;&#30340;&#21516;&#26102;&#65292;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#23545;&#27835;&#30103;&#25928;&#26524;&#36827;&#34892;&#25490;&#21517;&#65292;&#20174;&#32780;&#35299;&#20915;&#28789;&#27963;&#24615;&#22686;&#21152;&#24102;&#26469;&#30340;&#22240;&#26524;&#22522;&#30784;&#20002;&#22833;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#27169;&#22411;&#23558;&#22522;&#20110;ODE&#30340;&#26426;&#26800;&#21160;&#21147;&#23398;&#19982;&#28789;&#27963;&#19988;&#34920;&#36798;&#21147;&#24378;&#30340;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#32467;&#21512;&#36215;&#26469;&#12290;&#36825;&#31181;&#27169;&#22411;&#22312;&#31185;&#23398;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#29305;&#21035;&#26159;&#22312;ODE-based&#24314;&#27169;&#25552;&#20379;&#37325;&#35201;&#35299;&#37322;&#24615;&#21644;&#32463;&#36807;&#39564;&#35777;&#30340;&#22240;&#26524;&#22522;&#30784;&#65288;&#20363;&#22914;&#65292;&#29992;&#20110;&#21453;&#20107;&#23454;&#25512;&#29702;&#65289;&#30340;&#39046;&#22495;&#12290;&#23558;&#26426;&#26800;&#27169;&#22411;&#32435;&#20837;&#20063;&#20026;&#26631;&#20934;&#40657;&#31665;&#24314;&#27169;&#26041;&#27861;&#25552;&#20379;&#20102;&#24402;&#32435;&#20559;&#24046;&#65292;&#36825;&#22312;&#20174;&#23567;&#22411;&#25968;&#25454;&#38598;&#25110;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#22797;&#26434;&#31995;&#32479;&#20013;&#23398;&#20064;&#26102;&#33267;&#20851;&#37325;&#35201;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#38543;&#30528;&#28151;&#21512;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#28789;&#27963;&#65292;&#26426;&#26800;&#27169;&#22411;&#25552;&#20379;&#30340;&#22240;&#26524;&#22522;&#30784;&#24456;&#24555;&#20250;&#20002;&#22833;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#21478;&#19968;&#20010;&#24120;&#35265;&#30340;&#39046;&#22495;&#30693;&#35782;&#26469;&#28304;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65306;&#23545;&#19968;&#32452;&#24178;&#39044;&#30340;&#27835;&#30103;&#25928;&#26524;&#36827;&#34892;&#25490;&#21517;&#65292;&#21363;&#20351;&#20934;&#30830;&#30340;&#27835;&#30103;&#25928;&#26524;&#19981;&#30693;&#36947;&#12290;&#25105;&#20204;&#22312;&#22240;&#26524;&#25439;&#22833;&#20013;&#32534;&#30721;&#36825;&#20123;&#20449;&#24687;&#65292;&#23558;&#20854;&#19982;&#26631;&#20934;&#39044;&#27979;&#25439;&#22833;&#30456;&#32467;&#21512;&#65292;&#24471;&#20986;&#28151;&#21512;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17233v1 Announce Type: new  Abstract: Hybrid models combine mechanistic ODE-based dynamics with flexible and expressive neural network components. Such models have grown rapidly in popularity, especially in scientific domains where such ODE-based modeling offers important interpretability and validated causal grounding (e.g., for counterfactual reasoning). The incorporation of mechanistic models also provides inductive bias in standard blackbox modeling approaches, critical when learning from small datasets or partially observed, complex systems. Unfortunately, as hybrid models become more flexible, the causal grounding provided by the mechanistic model can quickly be lost. We address this problem by leveraging another common source of domain knowledge: ranking of treatment effects for a set of interventions, even if the precise treatment effect is unknown. We encode this information in a causal loss that we combine with the standard predictive loss to arrive at a hybrid los
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#21452;&#23610;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#35299;&#20915;&#20855;&#26377;&#23567;&#21442;&#25968;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#30452;&#25509;&#23558;&#23567;&#21442;&#25968;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#65292;&#20174;&#32780;&#31616;&#21270;&#35299;&#20915;&#36807;&#31243;&#65292;&#24182;&#33021;&#22815;&#21512;&#29702;&#20934;&#30830;&#22320;&#25429;&#25417;&#30001;&#23567;&#21442;&#25968;&#24341;&#36215;&#30340;&#35299;&#20013;&#22823;&#23548;&#25968;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.17232</link><description>&lt;p&gt;
&#20855;&#26377;&#23567;&#21442;&#25968;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#21452;&#23610;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Two-scale Neural Networks for Partial Differential Equations with Small Parameters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17232
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#21452;&#23610;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#35299;&#20915;&#20855;&#26377;&#23567;&#21442;&#25968;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#30452;&#25509;&#23558;&#23567;&#21442;&#25968;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#65292;&#20174;&#32780;&#31616;&#21270;&#35299;&#20915;&#36807;&#31243;&#65292;&#24182;&#33021;&#22815;&#21512;&#29702;&#20934;&#30830;&#22320;&#25429;&#25417;&#30001;&#23567;&#21442;&#25968;&#24341;&#36215;&#30340;&#35299;&#20013;&#22823;&#23548;&#25968;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#35299;&#20915;&#20855;&#26377;&#23567;&#21442;&#25968;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#30340;&#21452;&#23610;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#12290;&#25105;&#20204;&#30452;&#25509;&#23558;&#23567;&#21442;&#25968;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#20013;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#24471;&#20197;&#31616;&#21333;&#26041;&#24335;&#35299;&#20915;&#20855;&#26377;&#23567;&#21442;&#25968;&#30340;PDE&#25104;&#20026;&#21487;&#33021;&#65292;&#32780;&#26080;&#38656;&#28155;&#21152;&#20613;&#37324;&#21494;&#29305;&#24449;&#25110;&#20854;&#20182;&#35745;&#31639;&#32321;&#29712;&#30340;&#25130;&#26029;&#21442;&#25968;&#25628;&#32034;&#12290;&#22810;&#20010;&#25968;&#20540;&#20363;&#23376;&#23637;&#31034;&#20102;&#22312;&#35299;&#20915;&#30001;&#23567;&#21442;&#25968;&#24341;&#36215;&#30340;&#35299;&#20013;&#22823;&#23548;&#25968;&#29305;&#24449;&#26102;&#30340;&#21512;&#29702;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17232v1 Announce Type: cross  Abstract: We propose a two-scale neural network method for solving partial differential equations (PDEs) with small parameters using physics-informed neural networks (PINNs). We directly incorporate the small parameters into the architecture of neural networks. The proposed method enables solving PDEs with small parameters in a simple fashion, without adding Fourier features or other computationally taxing searches of truncation parameters. Various numerical examples demonstrate reasonable accuracy in capturing features of large derivatives in the solutions caused by small parameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#20013;&#30340;&#20844;&#24179;&#27867;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#29305;&#24449;&#12289;&#25439;&#22833;&#21644;&#20248;&#21270;&#26041;&#38754;&#65292;&#21033;&#29992;&#35299;&#32806;&#23398;&#20064;&#26469;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.17229</link><description>&lt;p&gt;
&#22312;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#20013;&#20445;&#25345;&#20844;&#24179;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Preserving Fairness Generalization in Deepfake Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#20013;&#30340;&#20844;&#24179;&#27867;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#29305;&#24449;&#12289;&#25439;&#22833;&#21644;&#20248;&#21270;&#26041;&#38754;&#65292;&#21033;&#29992;&#35299;&#32806;&#23398;&#20064;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#36817;&#24180;&#26469;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#26377;&#25928;&#30340;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#27169;&#22411;&#65292;&#28982;&#32780;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#23548;&#33268;&#22312;&#20154;&#21475;&#32479;&#35745;&#23398;&#32676;&#20307;&#20013;&#65288;&#22914;&#31181;&#26063;&#21644;&#24615;&#21035;&#65289;&#20986;&#29616;&#19981;&#20844;&#24179;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#29305;&#23450;&#32676;&#20307;&#38754;&#20020;&#19981;&#20844;&#24179;&#30340;&#23450;&#20301;&#25110;&#34987;&#25490;&#38500;&#22312;&#26816;&#27979;&#20043;&#22806;&#65292;&#28508;&#22312;&#22320;&#20801;&#35768;&#34987;&#38169;&#35823;&#20998;&#31867;&#30340;&#28145;&#24230;&#20266;&#36896;&#31713;&#25913;&#20844;&#20247;&#33286;&#35770;&#24182;&#30772;&#22351;&#23545;&#27169;&#22411;&#30340;&#20449;&#20219;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#26041;&#27861;&#26159;&#25552;&#20379;&#20844;&#24179;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290; &#23427;&#23545;&#20110;&#20869;&#22495;&#35780;&#20272;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#20844;&#24179;&#24615;&#33021;&#65292;&#20294;&#22312;&#36328;&#22495;&#27979;&#35797;&#20013;&#26080;&#27861;&#20445;&#25345;&#20844;&#24179;&#24615;&#12290;&#36825;&#31361;&#26174;&#20102;&#22312;&#25171;&#20987;&#28145;&#24230;&#20266;&#36896;&#20013;&#20844;&#24179;&#27867;&#21270;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#29305;&#24449;&#12289;&#25439;&#22833;&#21644;&#20248;&#21270;&#26041;&#38754;&#65292;&#35299;&#20915;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#20013;&#30340;&#20844;&#24179;&#27867;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#35299;&#32806;&#23398;&#20064;&#26469;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17229v1 Announce Type: cross  Abstract: Although effective deepfake detection models have been developed in recent years, recent studies have revealed that these models can result in unfair performance disparities among demographic groups, such as race and gender. This can lead to particular groups facing unfair targeting or exclusion from detection, potentially allowing misclassified deepfakes to manipulate public opinion and undermine trust in the model. The existing method for addressing this problem is providing a fair loss function. It shows good fairness performance for intra-domain evaluation but does not maintain fairness for cross-domain testing. This highlights the significance of fairness generalization in the fight against deepfakes. In this work, we propose the first method to address the fairness generalization problem in deepfake detection by simultaneously considering features, loss, and optimization aspects. Our method employs disentanglement learning to ext
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#26041;&#24046;&#25511;&#21046;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#65288;VCAS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25968;&#25454;&#32500;&#24230;&#21644;&#26631;&#35760;&#32500;&#24230;&#20013;&#36827;&#34892;&#37325;&#35201;&#24615;&#37319;&#26679;&#65292;&#25511;&#21046;&#37319;&#26679;&#27604;&#29575;&#26469;&#21152;&#36895;&#21453;&#21521;&#20256;&#25773;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17227</link><description>&lt;p&gt;
&#20855;&#26377;&#26041;&#24046;&#25511;&#21046;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#30340;&#39640;&#25928;&#21453;&#21521;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Efficient Backpropagation with Variance-Controlled Adaptive Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17227
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#26041;&#24046;&#25511;&#21046;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#65288;VCAS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25968;&#25454;&#32500;&#24230;&#21644;&#26631;&#35760;&#32500;&#24230;&#20013;&#36827;&#34892;&#37325;&#35201;&#24615;&#37319;&#26679;&#65292;&#25511;&#21046;&#37319;&#26679;&#27604;&#29575;&#26469;&#21152;&#36895;&#21453;&#21521;&#20256;&#25773;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#37319;&#26679;&#30340;&#31639;&#27861;&#22312;&#21069;&#21521;&#20256;&#25773;&#21644;/&#25110;&#21453;&#21521;&#20256;&#25773;&#20013;&#28040;&#38500;&#8220;&#19981;&#37325;&#35201;&#8221;&#30340;&#35745;&#31639;&#65292;&#20026;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25552;&#20379;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#37319;&#26679;&#24341;&#20837;&#20102;&#35757;&#32451;&#30340;&#36817;&#20284;&#65292;&#36825;&#20123;&#31639;&#27861;&#21487;&#33021;&#26080;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#22987;&#32456;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26088;&#22312;&#21152;&#36895;&#21453;&#21521;&#20256;&#25773;&#30340;&#26041;&#24046;&#25511;&#21046;&#33258;&#36866;&#24212;&#37319;&#26679;&#65288;VCAS&#65289;&#26041;&#27861;&#12290;VCAS&#36890;&#36807;&#25968;&#25454;&#32500;&#24230;&#20013;&#30340;&#32454;&#31890;&#24230;&#36880;&#23618;&#37325;&#35201;&#24615;&#37319;&#26679;&#26469;&#35745;&#31639;&#26080;&#20559;&#38543;&#26426;&#26799;&#24230;&#20197;&#36827;&#34892;&#28608;&#27963;&#26799;&#24230;&#35745;&#31639;&#65292;&#24182;&#36890;&#36807;&#26631;&#35760;&#32500;&#24230;&#20013;&#30340;&#26464;&#26438;&#20998;&#25968;&#37319;&#26679;&#26469;&#36827;&#34892;&#26435;&#37325;&#26799;&#24230;&#35745;&#31639;&#12290;&#20026;&#20102;&#20445;&#25345;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36890;&#36807;&#23398;&#20064;&#26679;&#26412;&#27604;&#29575;&#32852;&#21512;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#26469;&#25511;&#21046;&#39069;&#22806;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#22312;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#39046;&#22495;&#30340;&#22810;&#20010;&#24494;&#35843;&#21644;&#39044;&#35757;&#32451;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;VCAS&#12290;&#22312;&#25152;&#26377;&#20219;&#21153;&#19978;&#65292;VCAS&#33021;&#22815;&#20445;&#25345;&#21407;&#22987;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17227v1 Announce Type: new  Abstract: Sampling-based algorithms, which eliminate ''unimportant'' computations during forward and/or back propagation (BP), offer potential solutions to accelerate neural network training. However, since sampling introduces approximations to training, such algorithms may not consistently maintain accuracy across various tasks. In this work, we introduce a variance-controlled adaptive sampling (VCAS) method designed to accelerate BP. VCAS computes an unbiased stochastic gradient with fine-grained layerwise importance sampling in data dimension for activation gradient calculation and leverage score sampling in token dimension for weight gradient calculation. To preserve accuracy, we control the additional variance by learning the sample ratio jointly with model parameters during training. We assessed VCAS on multiple fine-tuning and pre-training tasks in both vision and natural language domains. On all the tasks, VCAS can preserve the original tr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26102;&#38388;&#36923;&#36753;&#35268;&#33539;&#26465;&#20214;&#21270;&#20915;&#31574;&#36716;&#25442;&#22120;&#65288;SDT&#65289;&#26694;&#26550;&#65292;&#32467;&#21512;&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#65288;STL&#65289;&#21644;&#20915;&#31574;&#36716;&#25442;&#22120;&#65288;DT&#65289;&#30340;&#33021;&#21147;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#22312;&#31163;&#32447;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#23398;&#20064;&#20986;&#26356;&#22909;&#30340;&#23433;&#20840;&#39640;&#22870;&#21169;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.17217</link><description>&lt;p&gt;
&#31163;&#32447;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26102;&#38388;&#36923;&#36753;&#35268;&#33539;&#26465;&#20214;&#21270;&#20915;&#31574;&#36716;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Temporal Logic Specification-Conditioned Decision Transformer for Offline Safe Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17217
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26102;&#38388;&#36923;&#36753;&#35268;&#33539;&#26465;&#20214;&#21270;&#20915;&#31574;&#36716;&#25442;&#22120;&#65288;SDT&#65289;&#26694;&#26550;&#65292;&#32467;&#21512;&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#65288;STL&#65289;&#21644;&#20915;&#31574;&#36716;&#25442;&#22120;&#65288;DT&#65289;&#30340;&#33021;&#21147;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#22312;&#31163;&#32447;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#23398;&#20064;&#20986;&#26356;&#22909;&#30340;&#23433;&#20840;&#39640;&#22870;&#21169;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#20174;&#22266;&#23450;&#25968;&#25454;&#38598;&#35757;&#32451;&#19968;&#20010;&#28385;&#36275;&#32422;&#26463;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21363;&#26102;&#38388;&#36923;&#36753;&#35268;&#33539;&#26465;&#20214;&#21270;&#20915;&#31574;&#36716;&#25442;&#22120;&#65288;SDT&#65289;&#65292;&#23427;&#21033;&#29992;&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#65288;STL&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#26469;&#25351;&#23450;&#20195;&#29702;&#24212;&#35813;&#36981;&#24490;&#30340;&#22797;&#26434;&#26102;&#38388;&#35268;&#21017;&#65292;&#20197;&#21450;&#20915;&#31574;&#36716;&#25442;&#22120;&#65288;DT&#65289;&#30340;&#39034;&#24207;&#24314;&#27169;&#33021;&#21147;&#12290;&#23545;DSRL&#22522;&#20934;&#27979;&#35797;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;SDT&#22312;&#23398;&#20064;&#23433;&#20840;&#39640;&#22870;&#21169;&#31574;&#30053;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17217v1 Announce Type: cross  Abstract: Offline safe reinforcement learning (RL) aims to train a constraint satisfaction policy from a fixed dataset. Current state-of-the-art approaches are based on supervised learning with a conditioned policy. However, these approaches fall short in real-world applications that involve complex tasks with rich temporal and logical structures. In this paper, we propose temporal logic Specification-conditioned Decision Transformer (SDT), a novel framework that harnesses the expressive power of signal temporal logic (STL) to specify complex temporal rules that an agent should follow and the sequential modeling capability of Decision Transformer (DT). Empirical evaluations on the DSRL benchmarks demonstrate the better capacity of SDT in learning safe and high-reward policies compared with existing approaches. In addition, SDT shows good alignment with respect to different desired degrees of satisfaction of the STL specification that it is condi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#25216;&#26415;&#35299;&#20915;&#20113;&#35745;&#31639;&#36164;&#28304;&#35843;&#24230;&#19982;&#31649;&#29702;&#20013;&#22797;&#26434;&#38382;&#39064;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.17216</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#22312;&#20113;&#35745;&#31639;&#36164;&#28304;&#35843;&#24230;&#19982;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Application of Machine Learning Optimization in Cloud Computing Resource Scheduling and Management
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#25216;&#26415;&#35299;&#20915;&#20113;&#35745;&#31639;&#36164;&#28304;&#35843;&#24230;&#19982;&#31649;&#29702;&#20013;&#22797;&#26434;&#38382;&#39064;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20113;&#35745;&#31639;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#20113;&#35745;&#31639;&#26159;&#25351;&#38598;&#20013;&#24335;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#29992;&#25143;&#36890;&#36807;&#35775;&#38382;&#38598;&#20013;&#24335;&#36164;&#28304;&#23436;&#25104;&#35745;&#31639;&#65292;&#20113;&#35745;&#31639;&#20013;&#24515;&#23558;&#31243;&#24207;&#22788;&#29702;&#32467;&#26524;&#36820;&#22238;&#32473;&#29992;&#25143;&#12290;&#20113;&#35745;&#31639;&#19981;&#20165;&#36866;&#29992;&#20110;&#20010;&#20154;&#29992;&#25143;&#65292;&#20063;&#36866;&#29992;&#20110;&#20225;&#19994;&#29992;&#25143;&#12290;&#36141;&#20080;&#20113;&#26381;&#21153;&#22120;&#21518;&#65292;&#29992;&#25143;&#26080;&#38656;&#36141;&#20080;&#22823;&#37327;&#35745;&#31639;&#26426;&#65292;&#33410;&#32422;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;&#26681;&#25454;&#20013;&#22269;&#32463;&#27982;&#26032;&#38395;&#32593;&#32476;&#30340;&#25253;&#21578;&#65292;&#20013;&#22269;&#30340;&#20113;&#35745;&#31639;&#35268;&#27169;&#24050;&#36798;&#21040;2091&#20159;&#20803;&#20154;&#27665;&#24065;&#12290;&#30446;&#21069;&#65292;&#20013;&#22269;&#36739;&#20026;&#25104;&#29087;&#30340;&#20113;&#26381;&#21153;&#25552;&#20379;&#21830;&#26377;&#38463;&#37324;&#20113;&#12289;&#30334;&#24230;&#20113;&#12289;&#21326;&#20026;&#20113;&#31561;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#25216;&#26415;&#35299;&#20915;&#20113;&#35745;&#31639;&#36164;&#28304;&#35843;&#24230;&#19982;&#31649;&#29702;&#20013;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17216v1 Announce Type: cross  Abstract: In recent years, cloud computing has been widely used. Cloud computing refers to the centralized computing resources, users through the access to the centralized resources to complete the calculation, the cloud computing center will return the results of the program processing to the user. Cloud computing is not only for individual users, but also for enterprise users. By purchasing a cloud server, users do not have to buy a large number of computers, saving computing costs. According to a report by China Economic News Network, the scale of cloud computing in China has reached 209.1 billion yuan. At present, the more mature cloud service providers in China are Ali Cloud, Baidu Cloud, Huawei Cloud and so on. Therefore, this paper proposes an innovative approach to solve complex problems in cloud computing resource scheduling and management using machine learning optimization techniques. Through in-depth study of challenges such as low r
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29305;&#24449;&#30697;&#38453;&#26041;&#27861;&#26469;&#35299;&#20915;&#22810;&#32500;&#38750;&#32467;&#26500;&#31232;&#30095;&#24674;&#22797;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#32467;&#26524;&#23637;&#31034;&#20102;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.17215</link><description>&lt;p&gt;
&#36890;&#36807;&#29305;&#24449;&#30697;&#38453;&#36827;&#34892;&#22810;&#32500;&#38750;&#32467;&#26500;&#31232;&#30095;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Multidimensional unstructured sparse recovery via eigenmatrix
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17215
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29305;&#24449;&#30697;&#38453;&#26041;&#27861;&#26469;&#35299;&#20915;&#22810;&#32500;&#38750;&#32467;&#26500;&#31232;&#30095;&#24674;&#22797;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#32467;&#26524;&#23637;&#31034;&#20102;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32771;&#34385;&#20102;&#22810;&#32500;&#38750;&#32467;&#26500;&#31232;&#30095;&#24674;&#22797;&#38382;&#39064;&#65292;&#20363;&#22914;&#20613;&#31435;&#21494;&#21453;&#28436;&#21644;&#31232;&#30095;&#21435;&#21367;&#31215;&#12290;&#29305;&#24449;&#30697;&#38453;&#26159;&#19968;&#31181;&#20855;&#26377;&#26399;&#26395;&#36817;&#20284;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30340;&#25968;&#25454;&#39537;&#21160;&#26500;&#36896;&#65292;&#29992;&#20110;&#35299;&#20915;&#19968;&#32500;&#38382;&#39064;&#12290;&#26412;&#25991;&#23558;&#29305;&#24449;&#30697;&#38453;&#26041;&#27861;&#25193;&#23637;&#21040;&#22810;&#32500;&#38382;&#39064;&#12290;&#36890;&#36807;&#25968;&#20540;&#32467;&#26524;&#23637;&#31034;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17215v1 Announce Type: cross  Abstract: This note considers the multidimensional unstructured sparse recovery problems. Examples include Fourier inversion and sparse deconvolution. The eigenmatrix is a data-driven construction with desired approximate eigenvalues and eigenvectors proposed for the one-dimensional problems. This note extends the eigenmatrix approach to multidimensional problems. Numerical results are provided to demonstrate the performance of the proposed method.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#25361;&#25112;&#65292;&#29992;&#20110;&#27979;&#35797;&#31070;&#32463;&#27169;&#22411;&#30340;STEM&#25216;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#22522;&#30784;&#25216;&#33021;&#21644;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#38656;&#35201;&#29702;&#35299;STEM&#30340;&#22810;&#27169;&#24335;&#35270;&#35273;&#35821;&#35328;&#20449;&#24687;&#65292;&#24182;&#23637;&#31034;&#20102;&#26368;&#26032;&#27169;&#22411;&#23545;&#20110;&#20302;&#24180;&#32423;&#25216;&#33021;&#30340;&#26377;&#38480;&#25484;&#25569;&#12290;</title><link>https://arxiv.org/abs/2402.17205</link><description>&lt;p&gt;
&#27979;&#37327;&#31070;&#32463;&#27169;&#22411;&#30340;&#35270;&#35273;&#35821;&#35328;STEM&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Measuring Vision-Language STEM Skills of Neural Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17205
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#25361;&#25112;&#65292;&#29992;&#20110;&#27979;&#35797;&#31070;&#32463;&#27169;&#22411;&#30340;STEM&#25216;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#22522;&#30784;&#25216;&#33021;&#21644;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#38656;&#35201;&#29702;&#35299;STEM&#30340;&#22810;&#27169;&#24335;&#35270;&#35273;&#35821;&#35328;&#20449;&#24687;&#65292;&#24182;&#23637;&#31034;&#20102;&#26368;&#26032;&#27169;&#22411;&#23545;&#20110;&#20302;&#24180;&#32423;&#25216;&#33021;&#30340;&#26377;&#38480;&#25484;&#25569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#25361;&#25112;&#65292;&#29992;&#20110;&#27979;&#35797;&#31070;&#32463;&#27169;&#22411;&#30340;STEM&#25216;&#33021;&#12290;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#38382;&#39064;&#36890;&#24120;&#38656;&#35201;&#32467;&#21512;STEM&#65288;&#31185;&#23398;&#12289;&#25216;&#26415;&#12289;&#24037;&#31243;&#21644;&#25968;&#23398;&#65289;&#30693;&#35782;&#26469;&#35299;&#20915;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#38656;&#35201;&#29702;&#35299;STEM&#30340;&#22810;&#27169;&#24335;&#35270;&#35273;&#35821;&#35328;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#26159;&#25361;&#25112;&#24615;&#38382;&#39064;&#20013;&#26368;&#22823;&#12289;&#26368;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#20043;&#19968;&#12290;&#23427;&#21253;&#25324;448&#39033;&#25216;&#33021;&#21644;1,073,146&#20010;&#36328;&#36234;&#25152;&#26377;STEM&#31185;&#30446;&#30340;&#38382;&#39064;&#12290;&#19982;&#36890;&#24120;&#20391;&#37325;&#20110;&#26816;&#39564;&#19987;&#23478;&#27700;&#24179;&#33021;&#21147;&#30340;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;&#22522;&#30784;&#25216;&#33021;&#21644;&#26681;&#25454;K-12&#35838;&#31243;&#35774;&#35745;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#23558;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;CLIP&#21644;GPT-3.5-Turbo&#65292;&#28155;&#21152;&#21040;&#25105;&#20204;&#30340;&#22522;&#20934;&#20013;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#26368;&#36817;&#30340;&#27169;&#22411;&#36827;&#23637;&#21482;&#26377;&#21161;&#20110;&#25484;&#25569;&#25968;&#25454;&#38598;&#20013;&#38750;&#24120;&#26377;&#38480;&#25968;&#37327;&#30340;&#20302;&#24180;&#32423;&#25216;&#33021;&#65288;&#19977;&#24180;&#32423;&#20013;&#30340;2.5%&#65289;&#12290;&#20107;&#23454;&#19978;&#65292;&#36825;&#20123;&#27169;&#22411;&#20173;&#36828;&#27809;&#26377;&#23436;&#20840;&#25484;&#25569;&#23398;&#21069;&#25945;&#32946;&#38454;&#27573;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17205v1 Announce Type: cross  Abstract: We introduce a new challenge to test the STEM skills of neural models. The problems in the real world often require solutions, combining knowledge from STEM (science, technology, engineering, and math). Unlike existing datasets, our dataset requires the understanding of multimodal vision-language information of STEM. Our dataset features one of the largest and most comprehensive datasets for the challenge. It includes 448 skills and 1,073,146 questions spanning all STEM subjects. Compared to existing datasets that often focus on examining expert-level ability, our dataset includes fundamental skills and questions designed based on the K-12 curriculum. We also add state-of-the-art foundation models such as CLIP and GPT-3.5-Turbo to our benchmark. Results show that the recent model advances only help master a very limited number of lower grade-level skills (2.5% in the third grade) in our dataset. In fact, these models are still well bel
&lt;/p&gt;</description></item><item><title>FedBRB&#26041;&#27861;&#25552;&#20986;&#20102;&#22522;&#20110;&#22359;&#27010;&#24565;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#20351;&#29992;&#23567;&#22411;&#26412;&#22320;&#27169;&#22411;&#35757;&#32451;&#22823;&#22411;&#20840;&#23616;&#27169;&#22411;&#30340;&#25152;&#26377;&#22359;&#65292;&#24182;&#22312;&#35774;&#22791;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#20013;&#26377;&#25928;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.17202</link><description>&lt;p&gt;
FedBRB&#65306;&#35299;&#20915;&#35774;&#22791;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#20013;&#23567;&#35268;&#27169;&#21040;&#22823;&#35268;&#27169;&#22330;&#26223;&#30340;&#26377;&#25928;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
FedBRB: An Effective Solution to the Small-to-Large Scenario in Device-Heterogeneity Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17202
&lt;/p&gt;
&lt;p&gt;
FedBRB&#26041;&#27861;&#25552;&#20986;&#20102;&#22522;&#20110;&#22359;&#27010;&#24565;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#20351;&#29992;&#23567;&#22411;&#26412;&#22320;&#27169;&#22411;&#35757;&#32451;&#22823;&#22411;&#20840;&#23616;&#27169;&#22411;&#30340;&#25152;&#26377;&#22359;&#65292;&#24182;&#22312;&#35774;&#22791;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#20013;&#26377;&#25928;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#27169;&#22411;&#30340;&#25104;&#21151;&#35777;&#26126;&#20102;&#25193;&#22823;&#27169;&#22411;&#35268;&#27169;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#24341;&#21457;&#20102;&#20174;&#32852;&#37030;&#23398;&#20064;&#30340;&#35282;&#24230;&#25506;&#32034;&#22823;&#35268;&#27169;&#27169;&#22411;&#21327;&#20316;&#35757;&#32451;&#30340;&#20852;&#36259;&#12290;&#30001;&#20110;&#35745;&#31639;&#32422;&#26463;&#65292;&#35768;&#22810;&#26426;&#26500;&#38590;&#20197;&#22312;&#26412;&#22320;&#35757;&#32451;&#22823;&#35268;&#27169;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#20165;&#20351;&#29992;&#36739;&#23567;&#30340;&#26412;&#22320;&#27169;&#22411;&#35757;&#32451;&#26356;&#22823;&#30340;&#20840;&#23616;&#27169;&#22411;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#24773;&#26223;&#65292;&#21363;\textbf{&#23567;&#35268;&#27169;&#21040;&#22823;&#35268;&#27169;&#22330;&#26223;}&#12290;&#23613;&#31649;&#26368;&#36817;&#35774;&#22791;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#24320;&#22987;&#25506;&#32034;&#36825;&#19968;&#39046;&#22495;&#65292;&#20294;&#23427;&#20204;&#22312;&#23436;&#20840;&#28085;&#30422;&#20840;&#23616;&#27169;&#22411;&#30340;&#21442;&#25968;&#31354;&#38388;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#22359;&#27010;&#24565;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;\textbf{FedBRB}&#65288;&#22522;&#20110;&#22359;&#27010;&#24565;&#30340;\underline{B}locking&#21644;&#21152;&#26435;\underline{R}olling&#19982;\underline{B}roadcasting&#65289;&#30340;&#26041;&#27861;&#12290;FedBRB&#21487;&#20197;&#20351;&#29992;&#23567;&#22411;&#26412;&#22320;&#27169;&#22411;&#35757;&#32451;&#22823;&#22411;&#20840;&#23616;&#27169;&#22411;&#30340;&#25152;&#26377;&#22359;&#65292;&#24182;&#23558;&#35757;&#32451;&#22909;&#30340;&#21442;&#25968;&#24191;&#25773;&#21040;&#25972;&#20010;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17202v1 Announce Type: new  Abstract: Recently, the success of large models has demonstrated the importance of scaling up model size. This has spurred interest in exploring collaborative training of large-scale models from federated learning perspective. Due to computational constraints, many institutions struggle to train a large-scale model locally. Thus, training a larger global model using only smaller local models has become an important scenario (i.e., the \textbf{small-to-large scenario}). Although recent device-heterogeneity federated learning approaches have started to explore this area, they face limitations in fully covering the parameter space of the global model. In this paper, we propose a method called \textbf{FedBRB} (\underline{B}lock-wise \underline{R}olling and weighted \underline{B}roadcast) based on the block concept. FedBRB can uses small local models to train all blocks of the large global model, and broadcasts the trained parameters to the entire spac
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SYMHnet&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21512;&#20316;&#23398;&#20064;&#22826;&#38451;&#39118;&#21644;&#34892;&#26143;&#38388;&#30913;&#22330;&#21442;&#25968;&#30340;&#27169;&#24335;&#65292;&#21487;&#20197;&#30701;&#26399;&#39044;&#27979;SYM-H&#25351;&#25968;&#65292;&#24182;&#37327;&#21270;&#39044;&#27979;&#20013;&#30340;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#21644;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;</title><link>https://arxiv.org/abs/2402.17196</link><description>&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#19982;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#39044;&#27979;SYM-H&#25351;&#25968;
&lt;/p&gt;
&lt;p&gt;
Prediction of the SYM-H Index Using a Bayesian Deep Learning Method with Uncertainty Quantification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17196
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SYMHnet&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21512;&#20316;&#23398;&#20064;&#22826;&#38451;&#39118;&#21644;&#34892;&#26143;&#38388;&#30913;&#22330;&#21442;&#25968;&#30340;&#27169;&#24335;&#65292;&#21487;&#20197;&#30701;&#26399;&#39044;&#27979;SYM-H&#25351;&#25968;&#65292;&#24182;&#37327;&#21270;&#39044;&#27979;&#20013;&#30340;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#21644;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21517;&#20026;SYMHnet&#65292;&#23427;&#37319;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21452;&#21521;&#38271;&#30701;&#26102;&#35760;&#24518;&#32593;&#32476;&#65292;&#21512;&#20316;&#23398;&#20064;&#26469;&#33258;&#22826;&#38451;&#39118;&#21644;&#34892;&#26143;&#38388;&#30913;&#22330;&#21442;&#25968;&#30340;&#27169;&#24335;&#65292;&#29992;&#20110;&#22522;&#20110;1&#20998;&#38047;&#21644;5&#20998;&#38047;&#20998;&#36776;&#29575;&#25968;&#25454;&#30340;SYM-H&#25351;&#25968;&#30340;&#30701;&#26399;&#39044;&#27979;&#12290;SYMHnet&#23558;NASA&#22826;&#31354;&#31185;&#23398;&#25968;&#25454;&#21327;&#35843;&#23384;&#26723;&#25552;&#20379;&#30340;&#21442;&#25968;&#20540;&#30340;&#26102;&#38388;&#24207;&#21015;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#39044;&#27979;&#22312;&#32473;&#23450;&#26102;&#38388;&#28857;t&#26102;&#65292;&#26102;&#38388;&#28857;t + w&#23567;&#26102;&#30340;SYM-H&#25351;&#25968;&#20540;&#65292;&#20854;&#20013;w&#20026;1&#25110;2&#12290;&#36890;&#36807;&#23558;&#36125;&#21494;&#26031;&#25512;&#26029;&#32435;&#20837;&#23398;&#20064;&#26694;&#26550;&#65292;SYMHnet&#22312;&#39044;&#27979;&#26410;&#26469;SYM-H&#25351;&#25968;&#26102;&#21487;&#20197;&#37327;&#21270;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#21644;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SYMHnet&#22312;&#24179;&#38745;&#26102;&#26399;&#21644;&#39118;&#26292;&#26102;&#26399;&#65292;&#26080;&#35770;&#26159;1&#20998;&#38047;&#36824;&#26159;5&#20998;&#38047;&#20998;&#36776;&#29575;&#25968;&#25454;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;SYMHnet&#36890;&#24120;&#34920;&#29616;&#20986;&#33394;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17196v1 Announce Type: cross  Abstract: We propose a novel deep learning framework, named SYMHnet, which employs a graph neural network and a bidirectional long short-term memory network to cooperatively learn patterns from solar wind and interplanetary magnetic field parameters for short-term forecasts of the SYM-H index based on 1-minute and 5-minute resolution data. SYMHnet takes, as input, the time series of the parameters' values provided by NASA's Space Science Data Coordinated Archive and predicts, as output, the SYM-H index value at time point t + w hours for a given time point t where w is 1 or 2. By incorporating Bayesian inference into the learning framework, SYMHnet can quantify both aleatoric (data) uncertainty and epistemic (model) uncertainty when predicting future SYM-H indices. Experimental results show that SYMHnet works well at quiet time and storm time, for both 1-minute and 5-minute resolution data. The results also show that SYMHnet generally performs b
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#19981;&#21516;&#25193;&#23637;&#22240;&#32032;&#22914;&#20309;&#24433;&#21709;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#24615;&#33021;&#65292;&#35748;&#20026;LLM&#24494;&#35843;&#36981;&#24490;&#30528;&#19968;&#31181;&#29305;&#27530;&#30340;&#25193;&#23637;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.17193</link><description>&lt;p&gt;
&#24403;&#25193;&#23637;&#36935;&#21040;LLM&#24494;&#35843;: &#25968;&#25454;&#12289;&#27169;&#22411;&#21644;&#24494;&#35843;&#26041;&#27861;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17193
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#19981;&#21516;&#25193;&#23637;&#22240;&#32032;&#22914;&#20309;&#24433;&#21709;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#24615;&#33021;&#65292;&#35748;&#20026;LLM&#24494;&#35843;&#36981;&#24490;&#30528;&#19968;&#31181;&#29305;&#27530;&#30340;&#25193;&#23637;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#37319;&#29992;&#24494;&#35843;&#26469;&#37322;&#25918;&#20854;&#22312;&#19979;&#28216;&#24212;&#29992;&#20013;&#30340;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#23545;&#19981;&#21516;&#24494;&#35843;&#26041;&#27861;&#30340;&#24402;&#32435;&#20559;&#24046;&#65288;&#29305;&#21035;&#26159;&#25193;&#23637;&#23646;&#24615;&#65289;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31995;&#32479;&#23454;&#39564;&#65292;&#30740;&#31350;&#19981;&#21516;&#25193;&#23637;&#22240;&#32032;&#65288;&#21253;&#25324;LLM&#27169;&#22411;&#22823;&#23567;&#12289;&#39044;&#35757;&#32451;&#25968;&#25454;&#22823;&#23567;&#12289;&#26032;&#24494;&#35843;&#21442;&#25968;&#22823;&#23567;&#21644;&#24494;&#35843;&#25968;&#25454;&#22823;&#23567;&#65289;&#22914;&#20309;&#24433;&#21709;&#24494;&#35843;&#24615;&#33021;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#24494;&#35843;&#31867;&#22411;--&#20840;&#27169;&#22411;&#35843;&#25972;&#65288;FMT&#65289;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PET&#65292;&#21253;&#25324;&#25552;&#31034;&#35843;&#25972;&#21644;LoRA&#65289;&#65292;&#24182;&#25506;&#32034;&#23427;&#20204;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#30340;&#25193;&#23637;&#34892;&#20026;&#65292;&#20854;&#20013;LLM&#27169;&#22411;&#22823;&#23567;&#36828;&#36828;&#36229;&#36807;&#24494;&#35843;&#25968;&#25454;&#22823;&#23567;&#12290;&#22522;&#20110;1B&#21040;16B&#30340;&#20004;&#32452;&#39044;&#35757;&#32451;&#21452;&#35821;LLMs&#21644;&#23545;&#21452;&#35821;&#26426;&#22120;&#32763;&#35793;&#21644;&#22810;&#35821;&#35328;&#25688;&#35201;&#22522;&#20934;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;LLM&#24494;&#35843;&#36981;&#24490;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17193v1 Announce Type: new  Abstract: While large language models (LLMs) often adopt finetuning to unlock their capabilities for downstream applications, our understanding on the inductive biases (especially the scaling properties) of different finetuning methods is still limited. To fill this gap, we conduct systematic experiments studying whether and how different scaling factors, including LLM model size, pretraining data size, new finetuning parameter size and finetuning data size, affect the finetuning performance. We consider two types of finetuning -- full-model tuning (FMT) and parameter efficient tuning (PET, including prompt tuning and LoRA), and explore their scaling behaviors in the data-limited regime where the LLM model size substantially outweighs the finetuning data size. Based on two sets of pretrained bilingual LLMs from 1B to 16B and experiments on bilingual machine translation and multilingual summarization benchmarks, we find that 1) LLM finetuning follo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#30340;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#31639;&#27861;&#65292;&#23454;&#29616;&#20010;&#20154;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#21644;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.17191</link><description>&lt;p&gt;
AI&#39537;&#21160;&#30340;&#21311;&#21517;&#21270;&#65306;&#22312;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#21516;&#26102;&#20445;&#25252;&#20010;&#20154;&#25968;&#25454;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
AI-Driven Anonymization: Protecting Personal Data Privacy While Leveraging Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17191
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#30340;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#31639;&#27861;&#65292;&#23454;&#29616;&#20010;&#20154;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#21644;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#26174;&#33879;&#25913;&#21464;&#20102;&#20154;&#20204;&#30340;&#29983;&#27963;&#12290;&#28982;&#32780;&#65292;&#23427;&#20063;&#23545;&#38544;&#31169;&#21644;&#23433;&#20840;&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#65292;&#35768;&#22810;&#20010;&#20154;&#20449;&#24687;&#34987;&#20844;&#24320;&#65292;&#24182;&#26377;&#29359;&#32618;&#25915;&#20987;&#21644;&#31363;&#21462;&#30340;&#25253;&#36947;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23454;&#29616;&#20010;&#20154;&#20449;&#24687;&#30340;&#26234;&#33021;&#20445;&#25252;&#24050;&#32463;&#25104;&#20026;&#39318;&#35201;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#20154;&#24037;&#26234;&#33021;&#21033;&#29992;&#20808;&#36827;&#31639;&#27861;&#21644;&#25216;&#26415;&#26377;&#25928;&#21152;&#23494;&#21644;&#21311;&#21517;&#21270;&#20010;&#20154;&#25968;&#25454;&#65292;&#23454;&#29616;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#20998;&#26512;&#21644;&#21033;&#29992;&#21516;&#26102;&#32500;&#25252;&#38544;&#31169;&#12290;&#26412;&#25991;&#30528;&#30524;&#20110;&#20010;&#20154;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#21644;&#21311;&#21517;&#21270;&#25512;&#24191;&#20316;&#20026;&#20854;&#26680;&#24515;&#30740;&#31350;&#30446;&#26631;&#12290;&#23427;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#31639;&#27861;&#23454;&#29616;&#20102;&#20010;&#20154;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#21644;&#26816;&#27979;&#12290;&#35813;&#35770;&#25991;&#36824;&#35299;&#20915;&#20102;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#29616;&#26377;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17191v1 Announce Type: cross  Abstract: The development of artificial intelligence has significantly transformed people's lives. However, it has also posed a significant threat to privacy and security, with numerous instances of personal information being exposed online and reports of criminal attacks and theft. Consequently, the need to achieve intelligent protection of personal information through machine learning algorithms has become a paramount concern. Artificial intelligence leverages advanced algorithms and technologies to effectively encrypt and anonymize personal data, enabling valuable data analysis and utilization while safeguarding privacy. This paper focuses on personal data privacy protection and the promotion of anonymity as its core research objectives. It achieves personal data privacy protection and detection through the use of machine learning's differential privacy protection algorithm. The paper also addresses existing challenges in machine learning rel
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#21521;&#37327;&#37327;&#21270;&#25216;&#26415;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#23398;&#20064;&#36807;&#31243;&#23558;&#23436;&#25972;&#21644;&#19981;&#23436;&#25972;&#30340;&#27969;&#20307;&#25968;&#25454;&#31354;&#38388;&#26144;&#23556;&#21040;&#31163;&#25955;&#20540;&#36739;&#20302;&#32500;&#24230;&#30340;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.17185</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#23545;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#36827;&#34892;&#20462;&#34917;
&lt;/p&gt;
&lt;p&gt;
Inpainting Computational Fluid Dynamics with Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17185
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21521;&#37327;&#37327;&#21270;&#25216;&#26415;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#23398;&#20064;&#36807;&#31243;&#23558;&#23436;&#25972;&#21644;&#19981;&#23436;&#25972;&#30340;&#27969;&#20307;&#25968;&#25454;&#31354;&#38388;&#26144;&#23556;&#21040;&#31163;&#25955;&#20540;&#36739;&#20302;&#32500;&#24230;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#20307;&#25968;&#25454;&#30340;&#23436;&#25104;&#26159;&#19968;&#20010;&#20855;&#26377;&#28508;&#22312;&#30410;&#22788;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#23545;&#23454;&#39564;&#21644;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#37117;&#26377;&#39640;&#28508;&#21147;&#12290;&#26377;&#25928;&#30340;&#27969;&#20307;&#25968;&#25454;&#23436;&#25104;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#27969;&#20307;&#21160;&#21147;&#23398;&#23454;&#39564;&#20013;&#25152;&#38656;&#30340;&#20256;&#24863;&#22120;&#25968;&#37327;&#65292;&#24182;&#20801;&#35768;&#22312;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#65288;CFD&#65289;&#27169;&#25311;&#20013;&#20351;&#29992;&#26356;&#31895;&#31961;&#21644;&#26356;&#33258;&#36866;&#24212;&#30340;&#32593;&#26684;&#12290;&#28982;&#32780;&#65292;&#27969;&#20307;&#25968;&#25454;&#23436;&#25104;&#38382;&#39064;&#30340;&#36870;&#38382;&#39064;&#24615;&#36136;&#20351;&#24471;&#20174;&#29702;&#35770;&#19978;&#33719;&#24471;&#35299;&#20915;&#26041;&#26696;&#38750;&#24120;&#22256;&#38590;&#65292;&#24182;&#23545;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65288;&#20363;&#22914;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65289;&#20135;&#29983;&#39640;&#25968;&#20540;&#19981;&#30830;&#23450;&#24615;&#21644;&#19981;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21033;&#29992;&#21521;&#37327;&#37327;&#21270;&#25216;&#26415;&#36890;&#36807;&#20004;&#38454;&#27573;&#23398;&#20064;&#36807;&#31243;&#23558;&#23436;&#25972;&#21644;&#19981;&#23436;&#25972;&#30340;&#27969;&#20307;&#25968;&#25454;&#31354;&#38388;&#26144;&#23556;&#21040;&#31163;&#25955;&#20540;&#36739;&#20302;&#32500;&#24230;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312; Kolmogorov &#27969;&#25968;&#25454;&#65288;&#38647;&#35834;&#25968;&#65306;100&#65289;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17185v1 Announce Type: new  Abstract: Fluid data completion is a research problem with high potential benefit for both experimental and computational fluid dynamics. An effective fluid data completion method reduces the required number of sensors in a fluid dynamics experiment, and allows a coarser and more adaptive mesh for a Computational Fluid Dynamics (CFD) simulation. However, the ill-posed nature of the fluid data completion problem makes it prohibitively difficult to obtain a theoretical solution and presents high numerical uncertainty and instability for a data-driven approach (e.g., a neural network model). To address these challenges, we leverage recent advancements in computer vision, employing the vector quantization technique to map both complete and incomplete fluid data spaces onto discrete-valued lower-dimensional representations via a two-stage learning procedure. We demonstrated the effectiveness of our approach on Kolmogorov flow data (Reynolds number: 100
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21452;&#31354;&#38388;&#20248;&#21270;&#65288;DSO&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#28508;&#22312;&#31354;&#38388;&#37319;&#26679;&#21644;&#25968;&#25454;&#31354;&#38388;&#36873;&#25321;&#65292;&#20351;&#29992;Latent Prompt Transformer (LPT)&#29983;&#25104;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20998;&#23376;&#35774;&#35745;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.17179</link><description>&lt;p&gt;
&#21452;&#31354;&#38388;&#20248;&#21270;&#65306;&#36890;&#36807;&#28508;&#22312;&#25552;&#31034;&#21464;&#25442;&#22120;&#25913;&#36827;&#20998;&#23376;&#24207;&#21015;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Dual-Space Optimization: Improved Molecule Sequence Design by Latent Prompt Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17179
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21452;&#31354;&#38388;&#20248;&#21270;&#65288;DSO&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#28508;&#22312;&#31354;&#38388;&#37319;&#26679;&#21644;&#25968;&#25454;&#31354;&#38388;&#36873;&#25321;&#65292;&#20351;&#29992;Latent Prompt Transformer (LPT)&#29983;&#25104;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20998;&#23376;&#35774;&#35745;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20855;&#26377;&#29702;&#24819;&#24615;&#36136;&#65288;&#22914;&#33647;&#29289;&#26679;&#24615;&#21644;&#23545;&#34507;&#30333;&#38774;&#28857;&#30340;&#39640;&#32467;&#21512;&#20146;&#21644;&#21147;&#65289;&#30340;&#20998;&#23376;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21452;&#31354;&#38388;&#20248;&#21270;&#65288;DSO&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25972;&#21512;&#20102;&#28508;&#22312;&#31354;&#38388;&#37319;&#26679;&#21644;&#25968;&#25454;&#31354;&#38388;&#36873;&#25321;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;DSO&#36890;&#36807;&#36845;&#20195;&#26356;&#26032;&#28508;&#22312;&#31354;&#38388;&#29983;&#25104;&#27169;&#22411;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#36880;&#27493;&#23558;&#29983;&#25104;&#27169;&#22411;&#21644;&#21512;&#25104;&#25968;&#25454;&#31227;&#21521;&#25152;&#38656;&#24615;&#36136;&#25968;&#20540;&#30340;&#21306;&#22495;&#12290;&#25105;&#20204;&#30340;&#29983;&#25104;&#27169;&#22411;&#37319;&#29992;&#28508;&#22312;&#25552;&#31034;&#21464;&#25442;&#22120;&#65288;LPT&#65289;&#30340;&#24418;&#24335;&#65292;&#20854;&#20013;&#28508;&#22312;&#21521;&#37327;&#20805;&#24403;&#22240;&#26524;&#21464;&#25442;&#22120;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#21333;&#30446;&#26631;&#12289;&#22810;&#30446;&#26631;&#21644;&#32422;&#26463;&#20998;&#23376;&#35774;&#35745;&#20219;&#21153;&#20013;&#26641;&#31435;&#20102;&#26032;&#30340;&#24615;&#33021;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17179v1 Announce Type: new  Abstract: Designing molecules with desirable properties, such as drug-likeliness and high binding affinities towards protein targets, is a challenging problem. In this paper, we propose the Dual-Space Optimization (DSO) method that integrates latent space sampling and data space selection to solve this problem. DSO iteratively updates a latent space generative model and a synthetic dataset in an optimization process that gradually shifts the generative model and the synthetic data towards regions of desired property values. Our generative model takes the form of a Latent Prompt Transformer (LPT) where the latent vector serves as the prompt of a causal transformer. Our extensive experiments demonstrate effectiveness of the proposed method, which sets new performance benchmarks across single-objective, multi-objective and constrained molecule design tasks.
&lt;/p&gt;</description></item><item><title>Sora&#26159;&#19968;&#31181;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#23637;&#31034;&#20986;&#22312;&#27169;&#25311;&#29289;&#29702;&#19990;&#30028;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#21644;&#25361;&#25112;&#65292;&#26410;&#26469;&#21457;&#23637;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.17177</link><description>&lt;p&gt;
Sora: &#22823;&#22411;&#35270;&#35273;&#27169;&#22411;&#32972;&#26223;&#12289;&#25216;&#26415;&#12289;&#23616;&#38480;&#24615;&#21644;&#26426;&#36935;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17177
&lt;/p&gt;
&lt;p&gt;
Sora&#26159;&#19968;&#31181;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#23637;&#31034;&#20986;&#22312;&#27169;&#25311;&#29289;&#29702;&#19990;&#30028;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#21644;&#25361;&#25112;&#65292;&#26410;&#26469;&#21457;&#23637;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Sora&#26159;&#30001;OpenAI&#20110;2024&#24180;2&#26376;&#21457;&#24067;&#30340;&#19968;&#31181;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#65292;&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25351;&#20196;&#29983;&#25104;&#36924;&#30495;&#25110;&#24819;&#35937;&#30340;&#22330;&#26223;&#35270;&#39057;&#65292;&#24182;&#22312;&#27169;&#25311;&#29289;&#29702;&#19990;&#30028;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#26412;&#25991;&#22522;&#20110;&#20844;&#24320;&#30340;&#25216;&#26415;&#25253;&#21578;&#21644;&#36870;&#21521;&#24037;&#31243;&#65292;&#23545;&#36825;&#20010;&#27169;&#22411;&#30340;&#32972;&#26223;&#12289;&#30456;&#20851;&#25216;&#26415;&#12289;&#24212;&#29992;&#12289;&#23578;&#23384;&#30340;&#25361;&#25112;&#20197;&#21450;&#25991;&#26412;&#21040;&#35270;&#39057;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#26410;&#26469;&#26041;&#21521;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#12290;&#39318;&#20808;&#25105;&#20204;&#36861;&#28335;&#20102;Sora&#30340;&#21457;&#23637;&#21382;&#31243;&#65292;&#24182;&#35843;&#26597;&#20102;&#29992;&#20110;&#26500;&#24314;&#36825;&#20010;"&#19990;&#30028;&#27169;&#25311;&#22120;"&#30340;&#22522;&#30784;&#25216;&#26415;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35814;&#32454;&#25551;&#36848;&#20102;Sora&#22312;&#20174;&#30005;&#24433;&#21046;&#20316;&#21644;&#25945;&#32946;&#21040;&#33829;&#38144;&#31561;&#22810;&#20010;&#34892;&#19994;&#20013;&#30340;&#24212;&#29992;&#21644;&#28508;&#22312;&#24433;&#21709;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#38656;&#35201;&#35299;&#20915;&#30340;&#20027;&#35201;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#65292;&#20197;&#20415;&#24191;&#27867;&#37096;&#32626;Sora&#65292;&#22914;&#30830;&#20445;&#23433;&#20840;&#21644;&#26080;&#20559;&#35265;&#30340;&#35270;&#39057;&#29983;&#25104;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;Sora&#20197;&#21450;&#35270;&#39057;&#29983;&#25104;&#25216;&#26415;&#26410;&#26469;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17177v1 Announce Type: cross  Abstract: Sora is a text-to-video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model's background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models. We first trace Sora's development and investigate the underlying technologies used to build this "world simulator". Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film-making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of Sora and video gene
&lt;/p&gt;</description></item><item><title>DeepDRK&#26159;&#19968;&#31181;&#20998;&#24067;&#26080;&#20851;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#29983;&#25104;&#27169;&#22411;&#20197;&#23454;&#29616;&#8220;&#20132;&#25442;&#23646;&#24615;&#8221;&#65292;&#24182;&#25552;&#20986;&#26032;&#39062;&#26377;&#25928;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#21462;&#24471;&#20102;&#22312;FDR&#21644;&#33021;&#21147;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.17176</link><description>&lt;p&gt;
DeepDRK:&#28145;&#24230;&#20381;&#36182;&#27491;&#21017;&#21270; Knockoff &#29992;&#20110;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
DeepDRK: Deep Dependency Regularized Knockoff for Feature Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17176
&lt;/p&gt;
&lt;p&gt;
DeepDRK&#26159;&#19968;&#31181;&#20998;&#24067;&#26080;&#20851;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#29983;&#25104;&#27169;&#22411;&#20197;&#23454;&#29616;&#8220;&#20132;&#25442;&#23646;&#24615;&#8221;&#65292;&#24182;&#25552;&#20986;&#26032;&#39062;&#26377;&#25928;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#21462;&#24471;&#20102;&#22312;FDR&#21644;&#33021;&#21147;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17176v1 &#20844;&#21578;&#31867;&#22411;:&#26032; &#25688;&#35201;: Model-X knockoff&#65292;&#22312;&#21508;&#31181;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#20013;&#65292;&#30001;&#20110;&#20854;&#23545;&#20551;&#21457;&#29616;&#29575;&#65288;FDR&#65289;&#25511;&#21046;&#30340;&#20445;&#35777;&#32780;&#26368;&#36817;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#22312;&#21442;&#25968;&#35774;&#35745;&#20013;&#24341;&#20837;&#21518;&#65292;knockoff&#34987;&#21457;&#23637;&#20026;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29983;&#25104;&#24314;&#27169;&#26469;&#22788;&#29702;&#20219;&#24847;&#25968;&#25454;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#30446;&#21069;&#28145;&#24230;Model-X knockoff&#26694;&#26550;&#30340;&#23454;&#29616;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;knockoffs&#25152;&#38656;&#30340;&#8220;&#20132;&#25442;&#23646;&#24615;&#8221;&#32463;&#24120;&#22312;&#26679;&#26412;&#32423;&#21035;&#36935;&#21040;&#25361;&#25112;&#65292;&#23548;&#33268;&#36873;&#25321;&#33021;&#21147;&#19979;&#38477;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#8220;&#28145;&#24230;&#20381;&#36182;&#27491;&#21017;&#21270;Knockoff&#65288;DeepDRK&#65289;&#8221;&#65292;&#36825;&#26159;&#19968;&#31181;&#19981;&#20381;&#36182;&#20998;&#24067;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;FDR&#21644;&#33021;&#21147;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#22312;DeepDRK&#20013;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#26356;&#22909;&#22320;&#23454;&#29616;&#8220;&#20132;&#25442;&#23646;&#24615;&#8221;&#12290;&#36824;&#25552;&#20986;&#20102;&#26032;&#39062;&#26377;&#25928;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#20197;&#33719;&#24471;&#26356;&#39640;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17176v1 Announce Type: new  Abstract: Model-X knockoff, among various feature selection methods, received much attention recently due to its guarantee on false discovery rate (FDR) control. Subsequent to its introduction in parametric design, knockoff is advanced to handle arbitrary data distributions using deep learning-based generative modeling. However, we observed that current implementations of the deep Model-X knockoff framework exhibit limitations. Notably, the "swap property" that knockoffs necessitate frequently encounter challenges on sample level, leading to a diminished selection power. To overcome, we develop "Deep Dependency Regularized Knockoff (DeepDRK)", a distribution-free deep learning method that strikes a balance between FDR and power. In DeepDRK, a generative model grounded in a transformer architecture is introduced to better achieve the "swap property". Novel efficient regularization techniques are also proposed to reach higher power. Our model outper
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#23398;&#20064;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#21644;&#28436;&#21464;&#31995;&#32479;&#30340;&#26377;&#25928;&#21160;&#24577;&#21152;&#36895;&#27169;&#25311;&#65292;&#20026;&#20934;&#30830;&#39044;&#27979;&#22797;&#26434;&#31995;&#32479;&#30340;&#32479;&#35745;&#24615;&#36136;&#25552;&#20379;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17157</link><description>&lt;p&gt;
&#29992;&#20110;&#39044;&#27979;&#22797;&#26434;&#31995;&#32479;&#21160;&#24577;&#30340;&#29983;&#25104;&#24335;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generative Learning for Forecasting the Dynamics of Complex Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17157
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#23398;&#20064;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#21644;&#28436;&#21464;&#31995;&#32479;&#30340;&#26377;&#25928;&#21160;&#24577;&#21152;&#36895;&#27169;&#25311;&#65292;&#20026;&#20934;&#30830;&#39044;&#27979;&#22797;&#26434;&#31995;&#32479;&#30340;&#32479;&#35745;&#24615;&#36136;&#25552;&#20379;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#36890;&#36807;&#23398;&#20064;&#21644;&#28436;&#21464;&#20854;&#26377;&#25928;&#21160;&#24577;&#21152;&#36895;&#22797;&#26434;&#31995;&#32479;&#27169;&#25311;&#30340;&#29983;&#25104;&#24335;&#27169;&#22411;&#12290;&#22312;&#25552;&#20986;&#30340;&#26377;&#25928;&#21160;&#24577;&#29983;&#25104;&#24335;&#23398;&#20064;&#65288;G-LED&#65289;&#20013;&#65292;&#23558;&#39640;&#32500;&#25968;&#25454;&#30340;&#23454;&#20363;&#36827;&#34892;&#38477;&#37319;&#26679;&#21040;&#19968;&#20010;&#32463;&#36807;&#33258;&#22238;&#24402;&#27880;&#24847;&#26426;&#21046;&#28436;&#21270;&#30340;&#20302;&#32500;&#27969;&#24418;&#20013;&#12290;&#21453;&#36807;&#26469;&#65292;&#36125;&#21494;&#26031;&#25193;&#25955;&#27169;&#22411;&#23558;&#36825;&#20010;&#20302;&#32500;&#27969;&#24418;&#26144;&#23556;&#21040;&#20854;&#30456;&#24212;&#30340;&#39640;&#32500;&#31354;&#38388;&#65292;&#25429;&#25417;&#31995;&#32479;&#21160;&#24577;&#30340;&#32479;&#35745;&#20449;&#24687;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;G-LED&#22312;&#20960;&#20010;&#22522;&#20934;&#31995;&#32479;&#30340;&#27169;&#25311;&#20013;&#30340;&#33021;&#21147;&#21644;&#32570;&#38519;&#65292;&#21253;&#25324;Kuramoto-Sivashinsky&#65288;KS&#65289;&#26041;&#31243;&#65292;&#21453;&#21521;&#33033;&#20914;&#21518;&#26041;&#39640;&#38647;&#35834;&#25968;&#27969;&#20307;&#30340;&#20108;&#32500;&#27969;&#21160;&#65292;&#20197;&#21450;&#19977;&#32500;&#28237;&#27969;&#36890;&#36947;&#27969;&#30340;&#27169;&#25311;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#29983;&#25104;&#24335;&#23398;&#20064;&#20026;&#20934;&#30830;&#39044;&#27979;&#22797;&#26434;&#31995;&#32479;&#30340;&#32479;&#35745;&#24615;&#36136;&#24320;&#36767;&#20102;&#26032;&#30340;&#21069;&#27839;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17157v1 Announce Type: new  Abstract: We introduce generative models for accelerating simulations of complex systems through learning and evolving their effective dynamics. In the proposed Generative Learning of Effective Dynamics (G-LED), instances of high dimensional data are down sampled to a lower dimensional manifold that is evolved through an auto-regressive attention mechanism. In turn, Bayesian diffusion models, that map this low-dimensional manifold onto its corresponding high-dimensional space, capture the statistics of the system dynamics. We demonstrate the capabilities and drawbacks of G-LED in simulations of several benchmark systems, including the Kuramoto-Sivashinsky (KS) equation, two-dimensional high Reynolds number flow over a backward-facing step, and simulations of three-dimensional turbulent channel flow. The results demonstrate that generative learning offers new frontiers for the accurate forecasting of the statistical properties of complex systems at
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TaxDiff&#30340;&#20998;&#31867;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#29983;&#29289;&#29289;&#31181;&#20449;&#24687;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#29992;&#20110;&#21487;&#25511;&#29983;&#25104;&#32467;&#26500;&#31283;&#23450;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#12290;</title><link>https://arxiv.org/abs/2402.17156</link><description>&lt;p&gt;
TaxDiff&#65306;&#29992;&#20110;&#34507;&#30333;&#36136;&#24207;&#21015;&#29983;&#25104;&#30340;&#20998;&#31867;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TaxDiff: Taxonomic-Guided Diffusion Model for Protein Sequence Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17156
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TaxDiff&#30340;&#20998;&#31867;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#29983;&#29289;&#29289;&#31181;&#20449;&#24687;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#29992;&#20110;&#21487;&#25511;&#29983;&#25104;&#32467;&#26500;&#31283;&#23450;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20855;&#26377;&#29305;&#23450;&#29983;&#29289;&#21151;&#33021;&#21644;&#32467;&#26500;&#31283;&#23450;&#24615;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#22312;&#29983;&#29289;&#23398;&#21644;&#21270;&#23398;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#21487;&#38752;&#34507;&#30333;&#36136;&#35774;&#35745;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#27169;&#22411;&#20165;&#38480;&#20110;&#26080;&#26465;&#20214;&#29983;&#25104;&#34507;&#30333;&#36136;&#24207;&#21015;&#65292;&#32570;&#20047;&#23545;&#29983;&#29289;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#30340;&#21487;&#25511;&#29983;&#25104;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TaxDiff&#65292;&#19968;&#31181;&#29992;&#20110;&#21487;&#25511;&#34507;&#30333;&#36136;&#24207;&#21015;&#29983;&#25104;&#30340;&#20998;&#31867;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#65292;&#23427;&#23558;&#29983;&#29289;&#29289;&#31181;&#20449;&#24687;&#19982;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#20197;&#22312;&#24207;&#21015;&#31354;&#38388;&#20869;&#29983;&#25104;&#32467;&#26500;&#31283;&#23450;&#30340;&#34507;&#30333;&#36136;&#12290;&#20855;&#20307;&#22320;&#65292;&#20998;&#31867;&#25511;&#21046;&#20449;&#24687;&#34987;&#25554;&#20837;&#21040;&#21464;&#21387;&#22120;&#22359;&#30340;&#27599;&#19968;&#23618;&#65292;&#20197;&#23454;&#29616;&#32454;&#31890;&#24230;&#25511;&#21046;&#12290;&#20840;&#23616;&#21644;&#23616;&#37096;&#20851;&#27880;&#30340;&#32467;&#21512;&#30830;&#20445;&#20102;&#20998;&#31867;&#29305;&#23450;&#34507;&#30333;&#36136;&#30340;&#24207;&#21015;&#19968;&#33268;&#24615;&#21644;&#32467;&#26500;&#21487;&#25240;&#21472;&#24615;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17156v1 Announce Type: cross  Abstract: Designing protein sequences with specific biological functions and structural stability is crucial in biology and chemistry. Generative models already demonstrated their capabilities for reliable protein design. However, previous models are limited to the unconditional generation of protein sequences and lack the controllable generation ability that is vital to biological tasks. In this work, we propose TaxDiff, a taxonomic-guided diffusion model for controllable protein sequence generation that combines biological species information with the generative capabilities of diffusion models to generate structurally stable proteins within the sequence space. Specifically, taxonomic control information is inserted into each layer of the transformer block to achieve fine-grained control. The combination of global and local attention ensures the sequence consistency and structural foldability of taxonomic-specific proteins. Extensive experimen
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;HSTU&#26550;&#26500;&#65292;&#29992;&#20110;&#39640;&#22522;&#25968;&#12289;&#38750;&#24179;&#31283;&#27969;&#25512;&#33616;&#25968;&#25454;&#65292;&#24615;&#33021;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#39640;&#36798;65.8&#65285;&#30340;NDCG&#65292;&#24182;&#19988;&#27604;&#22522;&#20110;FlashAttention2&#30340;Transformer&#22312;8192&#38271;&#24230;&#24207;&#21015;&#19978;&#24555;5.3&#20493;&#21040;15.2&#20493;&#12290;</title><link>https://arxiv.org/abs/2402.17152</link><description>&lt;p&gt;
&#34892;&#21160;&#32988;&#36807;&#35328;&#36766;&#65306;&#29992;&#20110;&#29983;&#25104;&#25512;&#33616;&#30340;&#21315;&#20159;&#21442;&#25968;&#39034;&#24207;&#36716;&#23548;&#22120;
&lt;/p&gt;
&lt;p&gt;
Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17152
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;HSTU&#26550;&#26500;&#65292;&#29992;&#20110;&#39640;&#22522;&#25968;&#12289;&#38750;&#24179;&#31283;&#27969;&#25512;&#33616;&#25968;&#25454;&#65292;&#24615;&#33021;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#39640;&#36798;65.8&#65285;&#30340;NDCG&#65292;&#24182;&#19988;&#27604;&#22522;&#20110;FlashAttention2&#30340;Transformer&#22312;8192&#38271;&#24230;&#24207;&#21015;&#19978;&#24555;5.3&#20493;&#21040;15.2&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#30340;&#29305;&#28857;&#26159;&#20381;&#36182;&#20110;&#39640;&#22522;&#25968;&#12289;&#24322;&#26500;&#29305;&#24449;&#65292;&#24182;&#19988;&#38656;&#35201;&#27599;&#22825;&#22788;&#29702;&#25968;&#21313;&#20159;&#29992;&#25143;&#34892;&#20026;&#12290;&#23613;&#31649;&#22312;&#25104;&#21315;&#19978;&#19975;&#20010;&#29305;&#24449;&#19978;&#35757;&#32451;&#20102;&#22823;&#37327;&#25968;&#25454;&#65292;&#20294;&#22823;&#22810;&#25968;&#34892;&#19994;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411;(DLRMs)&#22312;&#35745;&#31639;&#26041;&#38754;&#26080;&#27861;&#25193;&#23637;&#12290;&#21463;&#21040;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#25104;&#21151;&#30340;Transformer&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#22522;&#26412;&#35774;&#35745;&#36873;&#25321;&#12290;&#25105;&#20204;&#23558;&#25512;&#33616;&#38382;&#39064;&#37325;&#26032;&#26500;&#24314;&#20026;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#20013;&#30340;&#39034;&#24207;&#36716;&#23548;&#20219;&#21153;&#65288;&#8220;&#29983;&#25104;&#25512;&#33616;&#32773;&#8221;&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#22522;&#25968;&#12289;&#38750;&#24179;&#31283;&#27969;&#25512;&#33616;&#25968;&#25454;&#35774;&#35745;&#30340;&#26032;&#26550;&#26500;HSTU&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17152v1 Announce Type: new  Abstract: Large-scale recommendation systems are characterized by their reliance on high cardinality, heterogeneous features and the need to handle tens of billions of user actions on a daily basis. Despite being trained on huge volume of data with thousands of features, most Deep Learning Recommendation Models (DLRMs) in industry fail to scale with compute.   Inspired by success achieved by Transformers in language and vision domains, we revisit fundamental design choices in recommendation systems. We reformulate recommendation problems as sequential transduction tasks within a generative modeling framework (``Generative Recommenders''), and propose a new architecture, HSTU, designed for high cardinality, non-stationary streaming recommendation data.   HSTU outperforms baselines over synthetic and public datasets by up to 65.8\% in NDCG, and is 5.3x to 15.2x faster than FlashAttention2-based Transformers on 8192 length sequences. HSTU-based Gener
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30697;&#38453;&#20056;&#31215;&#24577;&#20316;&#20026;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#29983;&#25104;&#22810;&#20010;&#26102;&#38388;&#28857;&#22788;&#22522;&#30784;&#36164;&#20135;&#20215;&#26684;&#30340;&#32852;&#21512;&#20998;&#24067;&#30340;&#24577;&#65292;&#24182;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#22312;Heston&#27169;&#22411;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17148</link><description>&lt;p&gt;
&#20351;&#29992;&#24352;&#37327;&#32593;&#32476;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#29983;&#25104;&#26399;&#26435;&#23450;&#20215;&#30340;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Time series generation for option pricing on quantum computers using tensor network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17148
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30697;&#38453;&#20056;&#31215;&#24577;&#20316;&#20026;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#29983;&#25104;&#22810;&#20010;&#26102;&#38388;&#28857;&#22788;&#22522;&#30784;&#36164;&#20135;&#20215;&#26684;&#30340;&#32852;&#21512;&#20998;&#24067;&#30340;&#24577;&#65292;&#24182;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#22312;Heston&#27169;&#22411;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#65292;&#29305;&#21035;&#26159;&#26399;&#26435;&#23450;&#20215;&#65292;&#26159;&#19968;&#20010;&#26377;&#26395;&#20174;&#37327;&#23376;&#35745;&#31639;&#20013;&#21463;&#30410;&#30340;&#34892;&#19994;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#29992;&#20110;&#26399;&#26435;&#23450;&#20215;&#30340;&#37327;&#23376;&#31639;&#27861;&#65292;&#20294;&#20154;&#20204;&#24076;&#26395;&#22312;&#31639;&#27861;&#20013;&#35774;&#35745;&#20986;&#26356;&#39640;&#25928;&#30340;&#23454;&#29616;&#26041;&#24335;&#65292;&#20854;&#20013;&#20043;&#19968;&#26159;&#20934;&#22791;&#32534;&#30721;&#22522;&#30784;&#36164;&#20135;&#20215;&#26684;&#27010;&#29575;&#20998;&#24067;&#30340;&#37327;&#23376;&#24577;&#12290;&#29305;&#21035;&#26159;&#22312;&#23450;&#20215;&#20381;&#36182;&#36335;&#24452;&#30340;&#26399;&#26435;&#26102;&#65292;&#25105;&#20204;&#38656;&#35201;&#29983;&#25104;&#19968;&#20010;&#32534;&#30721;&#22810;&#20010;&#26102;&#38388;&#28857;&#22788;&#22522;&#30784;&#36164;&#20135;&#20215;&#26684;&#30340;&#32852;&#21512;&#20998;&#24067;&#30340;&#24577;&#65292;&#36825;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30697;&#38453;&#20056;&#31215;&#24577;&#65288;MPS&#65289;&#20316;&#20026;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20197;Heston&#27169;&#22411;&#20026;&#30446;&#26631;&#65292;&#25105;&#20204;&#36827;&#34892;&#25968;&#20540;&#23454;&#39564;&#20197;&#22312;&#27169;&#22411;&#20013;&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;MPS&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;Heston&#27169;&#22411;&#20013;&#30340;&#36335;&#24452;&#65292;&#31361;&#26174;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17148v1 Announce Type: cross  Abstract: Finance, especially option pricing, is a promising industrial field that might benefit from quantum computing. While quantum algorithms for option pricing have been proposed, it is desired to devise more efficient implementations of costly operations in the algorithms, one of which is preparing a quantum state that encodes a probability distribution of the underlying asset price. In particular, in pricing a path-dependent option, we need to generate a state encoding a joint distribution of the underlying asset price at multiple time points, which is more demanding. To address these issues, we propose a novel approach using Matrix Product State (MPS) as a generative model for time series generation. To validate our approach, taking the Heston model as a target, we conduct numerical experiments to generate time series in the model. Our findings demonstrate the capability of the MPS model to generate paths in the Heston model, highlightin
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#30340;&#33021;&#25928;&#35843;&#24230;&#31639;&#27861;&#22312;&#33021;&#37327;&#26368;&#23567;&#21270;&#19982;&#25130;&#27490;&#26102;&#38388;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#25913;&#36827;&#30340;&#31454;&#20105;&#27604;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.17143</link><description>&lt;p&gt;
&#24102;&#26377;&#39044;&#27979;&#30340;&#33021;&#25928;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Energy-Efficient Scheduling with Predictions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17143
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#30340;&#33021;&#25928;&#35843;&#24230;&#31639;&#27861;&#22312;&#33021;&#37327;&#26368;&#23567;&#21270;&#19982;&#25130;&#27490;&#26102;&#38388;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#25913;&#36827;&#30340;&#31454;&#20105;&#27604;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35843;&#24230;&#31995;&#32479;&#30340;&#19968;&#20010;&#37325;&#35201;&#30446;&#26631;&#26159;&#39640;&#25928;&#31649;&#29702;&#33021;&#28304;&#20351;&#29992;&#12290;&#22312;&#33021;&#25928;&#35843;&#24230;&#20013;&#65292;&#25805;&#20316;&#31995;&#32479;&#25511;&#21046;&#30528;&#26426;&#22120;&#22788;&#29702;&#20316;&#19994;&#30340;&#36895;&#24230;&#65292;&#20197;&#26368;&#23567;&#21270;&#33021;&#37327;&#28040;&#32791;&#24182;&#20248;&#21270;&#25152;&#24471;&#35843;&#24230;&#30340;&#26381;&#21153;&#36136;&#37327;&#25104;&#26412;&#12290;&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#20851;&#20110;&#26410;&#26469;&#35831;&#27714;&#30340;&#39044;&#27979;&#36890;&#24120;&#21487;&#20197;&#20174;&#21382;&#21490;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#65292;&#26368;&#36817;&#19968;&#31995;&#21015;&#20851;&#20110;&#23398;&#20064;&#22686;&#24378;&#31639;&#27861;&#30340;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#39044;&#27979;&#26469;&#23454;&#29616;&#25913;&#36827;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;&#29305;&#21035;&#26159;&#65292;&#38024;&#23545;&#33021;&#25928;&#35843;&#24230;&#65292;Bamas et. al. [BamasMRS20] &#21644; Antoniadis et. al. [antoniadis2021novel] &#35774;&#35745;&#20102;&#24102;&#26377;&#39044;&#27979;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#33021;&#37327;&#26368;&#23567;&#21270;&#19982;&#25130;&#27490;&#26102;&#38388;&#38382;&#39064;&#65292;&#24182;&#22312;&#39044;&#27979;&#35823;&#24046;&#36739;&#23567;&#26102;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#31454;&#20105;&#27604;&#29575;&#65292;&#21363;&#20351;&#22312;&#39044;&#27979;&#35823;&#24046;&#20219;&#24847;&#22823;&#26102;&#20063;&#33021;&#20445;&#25345;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17143v1 Announce Type: cross  Abstract: An important goal of modern scheduling systems is to efficiently manage power usage. In energy-efficient scheduling, the operating system controls the speed at which a machine is processing jobs with the dual objective of minimizing energy consumption and optimizing the quality of service cost of the resulting schedule. Since machine-learned predictions about future requests can often be learned from historical data, a recent line of work on learning-augmented algorithms aims to achieve improved performance guarantees by leveraging predictions. In particular, for energy-efficient scheduling, Bamas et. al. [BamasMRS20] and Antoniadis et. al. [antoniadis2021novel] designed algorithms with predictions for the energy minimization with deadlines problem and achieved an improved competitive ratio when the prediction error is small while also maintaining worst-case bounds even when the prediction error is arbitrarily large.   In this paper, w
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21151;&#33021;&#22870;&#21169;&#32534;&#30721;&#23454;&#29616;&#30340;&#26080;&#30417;&#30563;&#38646;&#26679;&#26412;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#22312;&#21508;&#31181;&#27169;&#25311;&#26426;&#22120;&#20154;&#22522;&#20934;&#27979;&#35797;&#20013;&#35757;&#32451;&#20195;&#29702;&#24182;&#25104;&#21151;&#35299;&#20915;&#26032;&#20219;&#21153;&#65292;&#30456;&#27604;&#20197;&#24448;&#30340;&#38646;&#26679;&#26412;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#31168;&#12290;</title><link>https://arxiv.org/abs/2402.17135</link><description>&lt;p&gt;
&#36890;&#36807;&#21151;&#33021;&#22870;&#21169;&#32534;&#30721;&#23454;&#29616;&#30340;&#26080;&#30417;&#30563;&#38646;&#26679;&#26412;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Zero-Shot Reinforcement Learning via Functional Reward Encodings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17135
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21151;&#33021;&#22870;&#21169;&#32534;&#30721;&#23454;&#29616;&#30340;&#26080;&#30417;&#30563;&#38646;&#26679;&#26412;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#22312;&#21508;&#31181;&#27169;&#25311;&#26426;&#22120;&#20154;&#22522;&#20934;&#27979;&#35797;&#20013;&#35757;&#32451;&#20195;&#29702;&#24182;&#25104;&#21151;&#35299;&#20915;&#26032;&#20219;&#21153;&#65292;&#30456;&#27604;&#20197;&#24448;&#30340;&#38646;&#26679;&#26412;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21151;&#33021;&#22870;&#21169;&#32534;&#30721;&#65288;FRE&#65289;&#30340;&#36890;&#29992;&#12289;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#24819;&#27861;&#26159;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;transformer&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#23545;&#20219;&#24847;&#20219;&#21153;&#30340;&#29366;&#24577;-&#22870;&#21169;&#26679;&#26412;&#36827;&#34892;&#32534;&#30721;&#65292;&#20174;&#32780;&#23398;&#20064;&#20219;&#24847;&#20219;&#21153;&#30340;&#21151;&#33021;&#34920;&#31034;&#12290;&#36825;&#31181;&#21151;&#33021;&#32534;&#30721;&#19981;&#20165;&#20351;&#24471;&#33021;&#22815;&#20174;&#21508;&#31181;&#36890;&#29992;&#26080;&#30417;&#30563;&#22870;&#21169;&#20989;&#25968;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#32780;&#19988;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#35299;&#20915;&#20219;&#20309;&#26032;&#30340;&#19979;&#28216;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#23569;&#37327;&#22870;&#21169;&#27880;&#37322;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#26174;&#31034;&#65292;&#38024;&#23545;&#22810;&#26679;&#30340;&#38543;&#26426;&#26080;&#30417;&#30563;&#22870;&#21169;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#30340;FRE&#20195;&#29702;&#33021;&#22815;&#25512;&#24191;&#21040;&#35299;&#20915;&#19968;&#31995;&#21015;&#27169;&#25311;&#26426;&#22120;&#20154;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#26032;&#20219;&#21153;&#65292;&#36890;&#24120;&#20248;&#20110;&#20808;&#21069;&#30340;&#38646;&#26679;&#26412;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17135v1 Announce Type: cross  Abstract: Can we pre-train a generalist agent from a large amount of unlabeled offline trajectories such that it can be immediately adapted to any new downstream tasks in a zero-shot manner? In this work, we present a functional reward encoding (FRE) as a general, scalable solution to this zero-shot RL problem. Our main idea is to learn functional representations of any arbitrary tasks by encoding their state-reward samples using a transformer-based variational auto-encoder. This functional encoding not only enables the pre-training of an agent from a wide diversity of general unsupervised reward functions, but also provides a way to solve any new downstream tasks in a zero-shot manner, given a small number of reward-annotated samples. We empirically show that FRE agents trained on diverse random unsupervised reward functions can generalize to solve novel tasks in a range of simulated robotic benchmarks, often outperforming previous zero-shot RL
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;&#21152;&#26435;&#28966;&#28857;&#21487;&#24494;MCC&#65292;&#29992;&#20110;&#25913;&#21892;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#39044;&#27979;&#21754;&#20083;&#21160;&#29289;&#34507;&#30333;&#36136;&#20013;&#30340;O-GlcNAcylation&#20301;&#28857;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;</title><link>https://arxiv.org/abs/2402.17131</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#21644;RNN&#22312;&#32463;&#36807;&#35757;&#32451;&#30340;&#26032;&#25439;&#22833;&#20989;&#25968;&#19979;&#39044;&#27979;&#21754;&#20083;&#21160;&#29289;&#34507;&#30333;&#36136;&#20013;&#30340;O-GlcNAcylation&#20301;&#28857;
&lt;/p&gt;
&lt;p&gt;
Predicting O-GlcNAcylation Sites in Mammalian Proteins with Transformers and RNNs Trained with a New Loss Function
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;&#21152;&#26435;&#28966;&#28857;&#21487;&#24494;MCC&#65292;&#29992;&#20110;&#25913;&#21892;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#39044;&#27979;&#21754;&#20083;&#21160;&#29289;&#34507;&#30333;&#36136;&#20013;&#30340;O-GlcNAcylation&#20301;&#28857;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31958;&#22522;&#21270;&#26159;&#19968;&#31181;&#34507;&#30333;&#36136;&#20462;&#39280;&#65292;&#22312;&#21151;&#33021;&#21644;&#32467;&#26500;&#19978;&#36215;&#30528;&#22810;&#31181;&#37325;&#35201;&#20316;&#29992;&#12290;O-GlcNAcylation&#26159;&#31958;&#22522;&#21270;&#30340;&#19968;&#31181;&#20122;&#22411;&#65292;&#26377;&#28508;&#21147;&#25104;&#20026;&#27835;&#30103;&#30340;&#37325;&#35201;&#38774;&#28857;&#65292;&#20294;&#22312;2023&#24180;&#20043;&#21069;&#23578;&#26410;&#26377;&#21487;&#38752;&#39044;&#27979;O-GlcNAcylation&#20301;&#28857;&#30340;&#26041;&#27861;&#65307;2021&#24180;&#30340;&#19968;&#31687;&#35780;&#35770;&#27491;&#30830;&#25351;&#20986;&#24050;&#21457;&#34920;&#30340;&#27169;&#22411;&#19981;&#36275;&#65292;&#24182;&#19988;&#26410;&#33021;&#27867;&#21270;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#27169;&#22411;&#24050;&#19981;&#20877;&#21487;&#29992;&#12290;2023&#24180;&#65292;&#19968;&#31687;&#20855;&#26377;F$_1$&#20998;&#25968;36.17%&#21644;MCC&#20998;&#25968;34.57%&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#30340;&#26174;&#30528;&#26356;&#22909;&#30340;RNN&#27169;&#22411;&#34987;&#21457;&#34920;&#12290;&#26412;&#25991;&#39318;&#27425;&#35797;&#22270;&#36890;&#36807;Transformer&#32534;&#30721;&#22120;&#25552;&#39640;&#36825;&#20123;&#25351;&#26631;&#12290;&#23613;&#31649;Transformer&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20854;&#24615;&#33021;&#20173;&#19981;&#21450;&#20808;&#21069;&#21457;&#34920;&#30340;RNN&#12290;&#28982;&#21518;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;&#21152;&#26435;&#28966;&#28857;&#21487;&#24494;MCC&#65292;&#20197;&#25552;&#39640;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17131v1 Announce Type: new  Abstract: Glycosylation, a protein modification, has multiple essential functional and structural roles. O-GlcNAcylation, a subtype of glycosylation, has the potential to be an important target for therapeutics, but methods to reliably predict O-GlcNAcylation sites had not been available until 2023; a 2021 review correctly noted that published models were insufficient and failed to generalize. Moreover, many are no longer usable. In 2023, a considerably better RNN model with an F$_1$ score of 36.17% and an MCC of 34.57% on a large dataset was published. This article first sought to improve these metrics using transformer encoders. While transformers displayed high performance on this dataset, their performance was inferior to that of the previously published RNN. We then created a new loss function, which we call the weighted focal differentiable MCC, to improve the performance of classification models. RNN models trained with this new function di
&lt;/p&gt;</description></item><item><title>LCEN&#31639;&#27861;&#26159;&#19968;&#31181;&#29992;&#20110;&#21019;&#24314;&#38750;&#32447;&#24615;&#12289;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26032;&#22411;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#12289;&#26356;&#31232;&#30095;&#22320;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17120</link><description>&lt;p&gt;
LCEN&#65306;&#19968;&#31181;&#26032;&#22411;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#65292;&#29992;&#20110;&#38750;&#32447;&#24615;&#30340;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LCEN: A Novel Feature Selection Algorithm for Nonlinear, Interpretable Machine Learning Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17120
&lt;/p&gt;
&lt;p&gt;
LCEN&#31639;&#27861;&#26159;&#19968;&#31181;&#29992;&#20110;&#21019;&#24314;&#38750;&#32447;&#24615;&#12289;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26032;&#22411;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#12289;&#26356;&#31232;&#30095;&#22320;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#26550;&#26500;&#30456;&#23545;&#20110;&#40657;&#30418;&#26550;&#26500;&#20855;&#26377;&#20248;&#21183;&#65292;&#22312;&#20851;&#38190;&#39046;&#22495;&#22914;&#33322;&#31354;&#25110;&#21307;&#23398;&#20013;&#65292;&#21487;&#35299;&#37322;&#24615;&#23545;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#26368;&#31616;&#21333;&#12289;&#26368;&#24120;&#29992;&#30340;&#21487;&#35299;&#37322;&#26550;&#26500;&#65288;&#22914;LASSO&#25110;EN&#65289;&#20165;&#38480;&#20110;&#32447;&#24615;&#39044;&#27979;&#65292;&#24182;&#19988;&#29305;&#24449;&#36873;&#25321;&#33021;&#21147;&#36739;&#24046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LASSO-Clip-EN&#65288;LCEN&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#21019;&#24314;&#38750;&#32447;&#24615;&#12289;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;LCEN&#22312;&#22810;&#31181;&#20154;&#24037;&#21644;&#23454;&#35777;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#29983;&#25104;&#27604;&#20854;&#20182;&#24120;&#29992;&#26550;&#26500;&#26356;&#20934;&#30830;&#12289;&#26356;&#31232;&#30095;&#30340;&#27169;&#22411;&#12290;&#36825;&#20123;&#23454;&#39564;&#34920;&#26126;&#65292;LCEN&#23545;&#25968;&#25454;&#38598;&#21644;&#24314;&#27169;&#20013;&#36890;&#24120;&#23384;&#22312;&#30340;&#35768;&#22810;&#38382;&#39064;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#21253;&#25324;&#22122;&#22768;&#12289;&#22810;&#37325;&#20849;&#32447;&#24615;&#12289;&#25968;&#25454;&#31232;&#32570;&#21644;&#36229;&#21442;&#25968;&#26041;&#24046;&#12290;LCEN&#36824;&#33021;&#22815;&#20174;&#23454;&#35777;&#25968;&#25454;&#20013;&#37325;&#26032;&#21457;&#29616;&#22810;&#20010;&#29289;&#29702;&#23450;&#24459;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17120v1 Announce Type: new  Abstract: Interpretable architectures can have advantages over black-box architectures, and interpretability is essential for the application of machine learning in critical settings, such as aviation or medicine. However, the simplest, most commonly used interpretable architectures (such as LASSO or EN) are limited to linear predictions and have poor feature selection capabilities. In this work, we introduce the LASSO-Clip-EN (LCEN) algorithm for the creation of nonlinear, interpretable machine learning models. LCEN is tested on a wide variety of artificial and empirical datasets, creating more accurate, sparser models than other commonly used architectures. These experiments reveal that LCEN is robust against many issues typically present in datasets and modeling, including noise, multicollinearity, data scarcity, and hyperparameter variance. LCEN is also able to rediscover multiple physical laws from empirical data and, for processes with no kn
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Sinkhorn Knowledge Distillation&#65288;SinKD&#65289;&#26469;&#35299;&#20915;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;&#20013;&#25955;&#24230;&#24230;&#37327;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#30830;&#20445;&#23545;&#25945;&#24072;&#21644;&#23398;&#29983;&#20998;&#24067;&#20043;&#38388;&#24046;&#24322;&#30340;&#20934;&#30830;&#35780;&#20272;</title><link>https://arxiv.org/abs/2402.17110</link><description>&lt;p&gt;
Sinkhorn Distance Minimization for Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
Sinkhorn Distance Minimization for Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17110
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Sinkhorn Knowledge Distillation&#65288;SinKD&#65289;&#26469;&#35299;&#20915;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;&#20013;&#25955;&#24230;&#24230;&#37327;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#30830;&#20445;&#23545;&#25945;&#24072;&#21644;&#23398;&#29983;&#20998;&#24067;&#20043;&#38388;&#24046;&#24322;&#30340;&#20934;&#30830;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#34987;&#24191;&#27867;&#37319;&#29992;&#26469;&#21387;&#32553;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#29616;&#26377;&#30340;KD&#26041;&#27861;&#30740;&#31350;&#21253;&#25324;Kullback-Leibler&#65288;KL&#65289;&#12289;&#21453;&#21521;Kullback-Leibler&#65288;RKL&#65289;&#21644;Jensen-Shannon&#65288;JS&#65289;&#25955;&#24230;&#22312;&#20869;&#30340;&#21508;&#31181;&#25955;&#24230;&#24230;&#37327;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#20551;&#35774;&#21644;&#23450;&#20041;&#20013;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#24230;&#37327;&#22312;&#25945;&#24072;&#21644;&#23398;&#29983;&#20043;&#38388;&#23384;&#22312;&#23569;&#37327;&#20998;&#24067;&#37325;&#21472;&#26102;&#26410;&#33021;&#25552;&#20379;&#26377;&#25928;&#30340;&#30417;&#30563;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#21069;&#36848;&#30340;KL&#12289;RKL&#21644;JS&#25955;&#24230;&#20998;&#21035;&#23384;&#22312;&#27169;&#24335;&#24179;&#22343;&#12289;&#27169;&#24335;&#22349;&#22604;&#21644;&#27169;&#24335;&#20302;&#20272;&#30340;&#38382;&#39064;&#65292;&#36825;&#24694;&#21270;&#20102;&#22522;&#20110;logits&#30340;&#22810;&#26679;NLP&#20219;&#21153;&#30340;KD&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;Sinkhorn&#36317;&#31163;&#30340;Sinkhorn&#30693;&#35782;&#33976;&#39311;&#65288;SinKD&#65289;&#65292;&#20197;&#30830;&#20445;&#23545;&#25945;&#24072;&#21644;&#23398;&#29983;&#20998;&#24067;&#20043;&#38388;&#24046;&#24322;&#30340;&#32454;&#33268;&#21644;&#20934;&#30830;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;Sinkhorn&#24230;&#37327;&#30340;&#23646;&#24615;&#65292;&#25105;&#20204;&#21487;&#20197;&#25670;&#33073;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17110v1 Announce Type: new  Abstract: Knowledge distillation (KD) has been widely adopted to compress large language models (LLMs). Existing KD methods investigate various divergence measures including the Kullback-Leibler (KL), reverse Kullback-Leibler (RKL), and Jensen-Shannon (JS) divergences. However, due to limitations inherent in their assumptions and definitions, these measures fail to deliver effective supervision when few distribution overlap exists between the teacher and the student. In this paper, we show that the aforementioned KL, RKL, and JS divergences respectively suffer from issues of mode-averaging, mode-collapsing, and mode-underestimation, which deteriorates logits-based KD for diverse NLP tasks. We propose the Sinkhorn Knowledge Distillation (SinKD) that exploits the Sinkhorn distance to ensure a nuanced and precise assessment of the disparity between teacher and student distributions. Besides, profit by properties of the Sinkhorn metric, we can get rid
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#22312;&#36873;&#25321;&#20195;&#29702;&#20154;&#36827;&#34892;&#21512;&#21516;&#26102;&#20135;&#29983;&#30340;&#21338;&#24328;&#65292;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#32431;&#31574;&#30053;&#22343;&#34913;&#20197;&#21450;&#23545;&#20110;&#20219;&#20309;&#20985;&#21512;&#21516;&#30340;&#25919;&#31574;&#21518;&#24724;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.17108</link><description>&lt;p&gt;
&#22810;&#20010;&#38750;&#36817;&#35270;&#20195;&#29702;&#30340;&#37325;&#22797;&#21512;&#21516;&#65306;&#25919;&#31574;&#21518;&#24724;&#21644;&#26377;&#38480;&#36131;&#20219;
&lt;/p&gt;
&lt;p&gt;
Repeated Contracting with Multiple Non-Myopic Agents: Policy Regret and Limited Liability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17108
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#22312;&#36873;&#25321;&#20195;&#29702;&#20154;&#36827;&#34892;&#21512;&#21516;&#26102;&#20135;&#29983;&#30340;&#21338;&#24328;&#65292;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#32431;&#31574;&#30053;&#22343;&#34913;&#20197;&#21450;&#23545;&#20110;&#20219;&#20309;&#20985;&#21512;&#21516;&#30340;&#25919;&#31574;&#21518;&#24724;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#37325;&#22797;&#21512;&#21516;&#35774;&#32622;&#65292;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#22996;&#25176;&#20154;&#22312;&#27599;&#19968;&#36718;&#20013;&#36873;&#25321;$k$&#21517;&#20195;&#29702;&#12290;&#20195;&#29702;&#19981;&#26159;&#36817;&#35270;&#30340;&#65292;&#25152;&#20197;&#22996;&#25176;&#20154;&#30340;&#26426;&#21046;&#24341;&#21457;&#20102;&#19968;&#22330;&#22312;&#20195;&#29702;&#20043;&#38388;&#36827;&#34892;&#30340;$T$&#36718;&#24191;&#27867;&#24418;&#24335;&#30340;&#21338;&#24328;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;&#20960;&#20010;&#26088;&#22312;&#29702;&#35299;&#21512;&#21516;&#29702;&#35770;&#20013;&#19968;&#20010;&#23569;&#26377;&#25506;&#35752;&#30340;&#26041;&#38754;&#30340;&#32467;&#26524; - &#21363;&#22312;&#36873;&#25321;&#19982;&#20043;&#31614;&#35746;&#21512;&#21516;&#30340;&#20195;&#29702;&#26102;&#24341;&#21457;&#30340;&#21338;&#24328;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#22330;&#21338;&#24328;&#20801;&#35768;&#20195;&#29702;&#20043;&#38388;&#23384;&#22312;&#19968;&#20010;&#32431;&#31574;&#30053;&#30340;\emph{&#38750;&#21709;&#24212;}&#22343;&#34913;&#8212;&#8212;&#38750;&#27491;&#24335;&#22320;&#35828;&#65292;&#36825;&#26159;&#19968;&#20010;&#22343;&#34913;&#29366;&#24577;&#65292;&#20195;&#29702;&#30340;&#34892;&#21160;&#21462;&#20915;&#20110;&#23454;&#29616;&#29366;&#24577;&#30340;&#21382;&#21490;&#65292;&#20294;&#19981;&#21462;&#20915;&#20110;&#24444;&#27492;&#34892;&#21160;&#30340;&#21382;&#21490;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#21246;&#32467;&#21644;&#23041;&#32961;&#30340;&#22797;&#26434;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35777;&#26126;&#22914;&#26524;&#22996;&#25176;&#20154;&#20351;&#29992;&#19968;&#20010;\emph{&#21333;&#35843;}&#25671;&#33218;&#31639;&#27861;&#26469;&#36873;&#25321;&#20195;&#29702;&#65292;&#37027;&#20040;&#23545;&#20110;&#20219;&#20309;&#20985;&#21512;&#21516;&#65292;&#22312;&#36825;&#26679;&#19968;&#20010;&#22343;&#34913;&#29366;&#24577;&#19979;&#65292;&#22996;&#25176;&#20154;&#23545;&#20107;&#21518;&#36873;&#25321;&#19982;&#26368;&#20339;&#20195;&#29702;&#31614;&#35746;&#21512;&#21516;&#27809;&#26377;&#21518;&#24724;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17108v1 Announce Type: cross  Abstract: We study a repeated contracting setting in which a Principal adaptively chooses amongst $k$ Agents at each of $T$ rounds. The Agents are non-myopic, and so a mechanism for the Principal induces a $T$-round extensive form game amongst the Agents. We give several results aimed at understanding an under-explored aspect of contract theory -- the game induced when choosing an Agent to contract with. First, we show that this game admits a pure-strategy \emph{non-responsive} equilibrium amongst the Agents -- informally an equilibrium in which the Agent's actions depend on the history of realized states of nature, but not on the history of each other's actions, and so avoids the complexities of collusion and threats. Next, we show that if the Principal selects Agents using a \emph{monotone} bandit algorithm, then for any concave contract, in any such equilibrium, the Principal obtains no regret to contracting with the best Agent in hindsight -
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#38598;&#29305;&#24615;&#37327;&#36523;&#23450;&#21046;&#30340;&#36817;&#20284;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#26354;&#32447;&#35745;&#31639;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#20943;&#36731;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#30340;&#35745;&#31639;&#36127;&#25285;&#24182;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#32479;&#35745;&#20445;&#35777;</title><link>https://arxiv.org/abs/2402.17106</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#20844;&#24179;&#24615;&#65306;&#22312;&#24744;&#30340;&#25968;&#25454;&#19978;&#23454;&#29616;&#20855;&#26377;&#25928;&#29992;&#20445;&#35777;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Dataset Fairness: Achievable Fairness on Your Data With Utility Guarantees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17106
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#38598;&#29305;&#24615;&#37327;&#36523;&#23450;&#21046;&#30340;&#36817;&#20284;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#26354;&#32447;&#35745;&#31639;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#20943;&#36731;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#30340;&#35745;&#31639;&#36127;&#25285;&#24182;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#32479;&#35745;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#20013;&#65292;&#35757;&#32451;&#33021;&#22815;&#26368;&#23567;&#21270;&#19981;&#21516;&#25935;&#24863;&#32676;&#20307;&#20043;&#38388;&#24046;&#24322;&#30340;&#27169;&#22411;&#36890;&#24120;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#19979;&#38477;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#12290;&#36825;&#31181;&#26435;&#34913;&#30340;&#20005;&#37325;&#31243;&#24230;&#22522;&#26412;&#21462;&#20915;&#20110;&#25968;&#25454;&#38598;&#30340;&#29305;&#24615;&#65292;&#22914;&#25968;&#25454;&#38598;&#30340;&#19981;&#22343;&#34913;&#25110;&#20559;&#35265;&#12290;&#22240;&#27492;&#65292;&#22312;&#25968;&#25454;&#38598;&#20043;&#38388;&#20351;&#29992;&#32479;&#19968;&#30340;&#20844;&#24179;&#24615;&#35201;&#27714;&#20173;&#28982;&#20540;&#24471;&#24576;&#30097;&#65292;&#24182;&#19988;&#24448;&#24448;&#20250;&#23548;&#33268;&#25928;&#29992;&#26497;&#20302;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21333;&#20010;&#25968;&#25454;&#38598;&#37327;&#36523;&#23450;&#21046;&#30340;&#36817;&#20284;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#26354;&#32447;&#30340;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25903;&#25345;&#20005;&#26684;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;&#36890;&#36807;&#21033;&#29992;You-Only-Train-Once&#65288;YOTO&#65289;&#26694;&#26550;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20943;&#36731;&#20102;&#22312;&#36924;&#36817;&#26435;&#34913;&#26354;&#32447;&#26102;&#38656;&#35201;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#35813;&#26354;&#32447;&#21608;&#22260;&#24341;&#20837;&#32622;&#20449;&#21306;&#38388;&#26469;&#37327;&#21270;&#25105;&#20204;&#36817;&#20284;&#20540;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17106v1 Announce Type: cross  Abstract: In machine learning fairness, training models which minimize disparity across different sensitive groups often leads to diminished accuracy, a phenomenon known as the fairness-accuracy trade-off. The severity of this trade-off fundamentally depends on dataset characteristics such as dataset imbalances or biases. Therefore using a uniform fairness requirement across datasets remains questionable and can often lead to models with substantially low utility. To address this, we present a computationally efficient approach to approximate the fairness-accuracy trade-off curve tailored to individual datasets, backed by rigorous statistical guarantees. By utilizing the You-Only-Train-Once (YOTO) framework, our approach mitigates the computational burden of having to train multiple models when approximating the trade-off curve. Moreover, we quantify the uncertainty in our approximation by introducing confidence intervals around this curve, offe
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22522;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#20449;&#21495;&#20998;&#31867;&#22120;&#23545;&#29289;&#29702;&#20449;&#21495;&#30340;&#23545;&#25239;&#25200;&#21160;&#30340;&#33030;&#24369;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;PDE&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#26500;&#36896;&#24178;&#25200;&#20449;&#21495;&#65292;&#25104;&#21151;&#23454;&#29616;&#23545;&#26816;&#27979;&#22120;&#30340;&#35823;&#20998;&#31867;&#65292;&#23545;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#25552;&#20986;&#20102;&#39640;&#25928;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#21487;&#20197;&#35745;&#31639;&#20986;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23545;&#25239;&#25200;&#21160;&#12290;</title><link>https://arxiv.org/abs/2402.17104</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#21495;&#30340;&#23545;&#25239;&#25200;&#21160;
&lt;/p&gt;
&lt;p&gt;
Adversarial Perturbations of Physical Signals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17104
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22522;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#20449;&#21495;&#20998;&#31867;&#22120;&#23545;&#29289;&#29702;&#20449;&#21495;&#30340;&#23545;&#25239;&#25200;&#21160;&#30340;&#33030;&#24369;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;PDE&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#26500;&#36896;&#24178;&#25200;&#20449;&#21495;&#65292;&#25104;&#21151;&#23454;&#29616;&#23545;&#26816;&#27979;&#22120;&#30340;&#35823;&#20998;&#31867;&#65292;&#23545;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#25552;&#20986;&#20102;&#39640;&#25928;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#21487;&#20197;&#35745;&#31639;&#20986;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23545;&#25239;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#20449;&#21495;&#20998;&#31867;&#22120;&#23545;&#20854;&#36755;&#20837;&#30340;&#23545;&#25239;&#25200;&#21160;&#30340;&#33030;&#24369;&#24615;&#65292;&#20854;&#20013;&#20449;&#21495;&#21644;&#25200;&#21160;&#21463;&#29289;&#29702;&#32422;&#26463;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#36825;&#26679;&#19968;&#20010;&#24773;&#26223;&#65306;&#19968;&#20010;&#28304;&#21644;&#24178;&#25200;&#22120;&#21457;&#20986;&#20449;&#21495;&#20316;&#20026;&#27874;&#20256;&#25773;&#21040;&#26816;&#27979;&#22120;&#65292;&#26816;&#27979;&#22120;&#35797;&#22270;&#36890;&#36807;&#20998;&#26512;&#25509;&#25910;&#21040;&#30340;&#20449;&#21495;&#30340;&#39057;&#35889;&#22270;&#26469;&#20998;&#31867;&#28304;&#65292;&#20351;&#29992;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#27714;&#35299;PDE&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#36896;&#24178;&#25200;&#20449;&#21495;&#65292;&#21363;&#20351;&#23545;&#25509;&#25910;&#21040;&#30340;&#20449;&#21495;&#30340;&#39057;&#35889;&#22270;&#30340;&#25200;&#21160;&#20960;&#20046;&#19981;&#21487;&#23519;&#35273;&#65292;&#20063;&#20250;&#23548;&#33268;&#26816;&#27979;&#22120;&#23545;&#28304;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#12290;&#34429;&#28982;&#36825;&#31867;&#38382;&#39064;&#21487;&#33021;&#21253;&#21547;&#25968;&#30334;&#19975;&#20010;&#20915;&#31574;&#21464;&#37327;&#65292;&#20294;&#25105;&#20204;&#24341;&#20837;&#20102;&#26377;&#25928;&#27714;&#35299;&#23427;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#21487;&#20197;&#20026;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#29289;&#29702;&#32422;&#26463;&#19979;&#35745;&#31639;&#20986;&#26377;&#25928;&#19988;&#29289;&#29702;&#21487;&#23454;&#29616;&#30340;&#23545;&#25239;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17104v1 Announce Type: new  Abstract: We investigate the vulnerability of computer-vision-based signal classifiers to adversarial perturbations of their inputs, where the signals and perturbations are subject to physical constraints. We consider a scenario in which a source and interferer emit signals that propagate as waves to a detector, which attempts to classify the source by analyzing the spectrogram of the signal it receives using a pre-trained neural network. By solving PDE-constrained optimization problems, we construct interfering signals that cause the detector to misclassify the source even though the perturbations to the spectrogram of the received signal are nearly imperceptible. Though such problems can have millions of decision variables, we introduce methods to solve them efficiently. Our experiments demonstrate that one can compute effective and physically realizable adversarial perturbations for a variety of machine learning models under various physical co
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#20004;&#21442;&#25968;&#27169;&#22411;&#21644;&#26799;&#24230;&#27969;&#23398;&#20064;&#39640;&#32500;&#30446;&#26631;&#30340;&#29702;&#35770;&#21487;&#33021;&#24615;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#23384;&#22312;&#22823;&#37327;&#19981;&#21487;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#19988;&#36825;&#20123;&#30446;&#26631;&#30340;&#38598;&#21512;&#19981;&#23494;&#38598;&#65292;&#20855;&#26377;&#19968;&#23450;&#25299;&#25169;&#24615;&#36136;&#30340;&#23376;&#38598;&#20013;&#20063;&#23384;&#22312;&#19981;&#21487;&#23398;&#20064;&#30446;&#26631;&#12290;&#26368;&#32456;&#65292;&#21457;&#29616;&#20351;&#29992;&#23618;&#27425;&#36807;&#31243;&#26500;&#24314;&#30340;&#20027;&#35201;&#23450;&#29702;&#27169;&#22411;&#22312;&#25968;&#23398;&#34920;&#36798;&#19978;&#24182;&#38750;&#30001;&#21333;&#19968;&#21021;&#31561;&#20989;&#25968;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.17089</link><description>&lt;p&gt;
&#36890;&#36807;&#20004;&#21442;&#25968;&#27169;&#22411;&#21644;&#26799;&#24230;&#27969;&#23398;&#20064;&#39640;&#32500;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Learning high-dimensional targets by two-parameter models and gradient flow
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17089
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#20004;&#21442;&#25968;&#27169;&#22411;&#21644;&#26799;&#24230;&#27969;&#23398;&#20064;&#39640;&#32500;&#30446;&#26631;&#30340;&#29702;&#35770;&#21487;&#33021;&#24615;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#23384;&#22312;&#22823;&#37327;&#19981;&#21487;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#19988;&#36825;&#20123;&#30446;&#26631;&#30340;&#38598;&#21512;&#19981;&#23494;&#38598;&#65292;&#20855;&#26377;&#19968;&#23450;&#25299;&#25169;&#24615;&#36136;&#30340;&#23376;&#38598;&#20013;&#20063;&#23384;&#22312;&#19981;&#21487;&#23398;&#20064;&#30446;&#26631;&#12290;&#26368;&#32456;&#65292;&#21457;&#29616;&#20351;&#29992;&#23618;&#27425;&#36807;&#31243;&#26500;&#24314;&#30340;&#20027;&#35201;&#23450;&#29702;&#27169;&#22411;&#22312;&#25968;&#23398;&#34920;&#36798;&#19978;&#24182;&#38750;&#30001;&#21333;&#19968;&#21021;&#31561;&#20989;&#25968;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#24403;$W&lt;d$&#26102;&#65292;&#36890;&#36807;&#26799;&#24230;&#27969;&#65288;GF&#65289;&#20197;$W$&#21442;&#25968;&#27169;&#22411;&#23398;&#20064;$d$&#32500;&#30446;&#26631;&#30340;&#29702;&#35770;&#21487;&#33021;&#24615;&#65292;&#24517;&#28982;&#23384;&#22312;GF-&#19981;&#21487;&#23398;&#20064;&#30446;&#26631;&#30340;&#22823;&#23376;&#38598;&#12290;&#29305;&#21035;&#26159;&#65292;&#21487;&#23398;&#20064;&#30446;&#26631;&#30340;&#38598;&#21512;&#22312;$\mathbb R^d$&#20013;&#19981;&#26159;&#23494;&#38598;&#30340;&#65292;&#20219;&#20309;&#24418;&#21516;$W$&#32500;&#29699;&#38754;&#30340;$\mathbb R^d$&#23376;&#38598;&#21253;&#21547;&#19981;&#21487;&#23398;&#20064;&#30446;&#26631;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#20960;&#20046;&#20445;&#35777;&#20108;&#21442;&#25968;&#23398;&#20064;&#30340;&#20027;&#35201;&#23450;&#29702;&#20013;&#65292;&#25152;&#36848;&#27169;&#22411;&#26159;&#36890;&#36807;&#23618;&#27425;&#36807;&#31243;&#26500;&#24314;&#30340;&#65292;&#22240;&#27492;&#19981;&#33021;&#29992;&#21333;&#20010;&#21021;&#31561;&#20989;&#25968;&#34920;&#36798;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#38480;&#21046;&#22312;&#26412;&#36136;&#19978;&#26159;&#24517;&#35201;&#30340;&#65292;&#22240;&#20026;&#36825;&#31181;&#21487;&#23398;&#20064;&#24615;&#23545;&#20110;&#35768;&#22810;&#21021;&#31561;&#20989;&#25968;&#31867;&#30340;&#21487;&#23398;&#20064;&#24615;&#26159;&#34987;&#25490;&#38500;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17089v1 Announce Type: cross  Abstract: We explore the theoretical possibility of learning $d$-dimensional targets with $W$-parameter models by gradient flow (GF) when $W&lt;d$ there is necessarily a large subset of GF-non-learnable targets. In particular, the set of learnable targets is not dense in $\mathbb R^d$, and any subset of $\mathbb R^d$ homeomorphic to the $W$-dimensional sphere contains non-learnable targets. Finally, we observe that the model in our main theorem on almost guaranteed two-parameter learning is constructed using a hierarchical procedure and as a result is not expressible by a single elementary function. We show that this limitation is essential in the sense that such learnability can be ruled out for a large class of elementary functions.
&lt;/p&gt;</description></item><item><title>&#20174;&#36125;&#21494;&#26031;&#32593;&#32476;&#35745;&#31639;&#20986;&#30340;&#25968;&#25454;&#38598;&#30340;&#20284;&#28982;&#24615;&#20027;&#35201;&#30001;&#32463;&#39564;&#32593;&#32476;&#30340;&#20284;&#28982;&#24615;&#30340;&#20840;&#23616;&#26368;&#22823;&#20540;&#25152;&#20027;&#23548;&#65292;&#24182;&#19988;&#20165;&#24403;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#21442;&#25968;&#19982;&#32463;&#39564;&#27169;&#22411;&#30340;&#21442;&#25968;&#19968;&#33268;&#26102;&#65292;&#36825;&#26679;&#30340;&#26368;&#22823;&#20540;&#25165;&#20250;&#34987;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.17087</link><description>&lt;p&gt;
&#20851;&#20110;&#20855;&#26377;&#28508;&#22312;&#26681;&#21464;&#37327;&#30340;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#27880;&#35299;
&lt;/p&gt;
&lt;p&gt;
A Note on Bayesian Networks with Latent Root Variables
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17087
&lt;/p&gt;
&lt;p&gt;
&#20174;&#36125;&#21494;&#26031;&#32593;&#32476;&#35745;&#31639;&#20986;&#30340;&#25968;&#25454;&#38598;&#30340;&#20284;&#28982;&#24615;&#20027;&#35201;&#30001;&#32463;&#39564;&#32593;&#32476;&#30340;&#20284;&#28982;&#24615;&#30340;&#20840;&#23616;&#26368;&#22823;&#20540;&#25152;&#20027;&#23548;&#65292;&#24182;&#19988;&#20165;&#24403;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#21442;&#25968;&#19982;&#32463;&#39564;&#27169;&#22411;&#30340;&#21442;&#25968;&#19968;&#33268;&#26102;&#65292;&#36825;&#26679;&#30340;&#26368;&#22823;&#20540;&#25165;&#20250;&#34987;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#34920;&#24449;&#20102;&#20174;&#20855;&#26377;&#28508;&#22312;&#26681;&#33410;&#28857;&#30340;&#36125;&#21494;&#26031;&#32593;&#32476;&#35745;&#31639;&#20986;&#30340;&#20284;&#28982;&#20989;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#21097;&#20313;&#30340;&#26174;&#24615;&#21464;&#37327;&#30340;&#36793;&#32536;&#20998;&#24067;&#20063;&#20250;&#20687;&#19968;&#20010;&#36125;&#21494;&#26031;&#32593;&#32476;&#19968;&#26679;&#20998;&#35299;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#32463;&#39564;&#21270;&#12290;&#19968;&#32452;&#35266;&#27979;&#21040;&#30340;&#26174;&#24615;&#21464;&#37327;&#30340;&#25968;&#25454;&#38598;&#20351;&#25105;&#20204;&#33021;&#22815;&#37327;&#21270;&#32463;&#39564;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;(i)&#20174;&#21407;&#22987;&#36125;&#21494;&#26031;&#32593;&#32476;&#35745;&#31639;&#20986;&#36825;&#26679;&#19968;&#20010;&#25968;&#25454;&#38598;&#30340;&#20284;&#28982;&#24615;&#30001;&#32463;&#39564;&#21270;&#27169;&#22411;&#30340;&#20284;&#28982;&#24615;&#30340;&#20840;&#23616;&#26368;&#22823;&#20540;&#25152;&#20027;&#23548;&#65307;&#20197;&#21450;(ii)&#36825;&#26679;&#19968;&#20010;&#26368;&#22823;&#20540;&#20165;&#22312;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#21442;&#25968;&#19982;&#32463;&#39564;&#27169;&#22411;&#30340;&#21442;&#25968;&#19968;&#33268;&#26102;&#25165;&#20250;&#36798;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17087v1 Announce Type: cross  Abstract: We characterise the likelihood function computed from a Bayesian network with latent variables as root nodes. We show that the marginal distribution over the remaining, manifest, variables also factorises as a Bayesian network, which we call empirical. A dataset of observations of the manifest variables allows us to quantify the parameters of the empirical Bayesian net. We prove that (i) the likelihood of such a dataset from the original Bayesian network is dominated by the global maximum of the likelihood from the empirical one; and that (ii) such a maximum is attained if and only if the parameters of the Bayesian network are consistent with those of the empirical model.
&lt;/p&gt;</description></item><item><title>PSB&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#24207;&#21015;&#36755;&#20837;&#36827;&#34892;&#26102;&#31354;&#24182;&#34892;&#21270;&#30340;&#27133;&#23398;&#20064;&#26550;&#26500;&#65292;&#36890;&#36807;&#24182;&#34892;&#22788;&#29702;&#25152;&#26377;&#26102;&#38388;&#27493;&#39588;&#20013;&#30340;&#23545;&#35937;&#20013;&#24515;&#34920;&#31034;&#65292;&#21033;&#29992;&#22266;&#23450;&#23618;&#25968;&#21644;&#22240;&#26524;&#20851;&#27880;&#21147;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.17077</link><description>&lt;p&gt;
&#24182;&#34892;&#21270;&#26102;&#31354;&#32465;&#23450;
&lt;/p&gt;
&lt;p&gt;
Parallelized Spatiotemporal Binding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17077
&lt;/p&gt;
&lt;p&gt;
PSB&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#24207;&#21015;&#36755;&#20837;&#36827;&#34892;&#26102;&#31354;&#24182;&#34892;&#21270;&#30340;&#27133;&#23398;&#20064;&#26550;&#26500;&#65292;&#36890;&#36807;&#24182;&#34892;&#22788;&#29702;&#25152;&#26377;&#26102;&#38388;&#27493;&#39588;&#20013;&#30340;&#23545;&#35937;&#20013;&#24515;&#34920;&#31034;&#65292;&#21033;&#29992;&#22266;&#23450;&#23618;&#25968;&#21644;&#22240;&#26524;&#20851;&#27880;&#21147;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29616;&#20195;&#26368;&#20339;&#23454;&#36341;&#25552;&#20513;&#25903;&#25345;&#36828;&#31243;&#20132;&#20114;&#30340;&#21487;&#25193;&#23637;&#20307;&#31995;&#32467;&#26500;&#65292;&#20294;&#38754;&#21521;&#23545;&#35937;&#30340;&#27169;&#22411;&#20173;&#26410;&#23436;&#20840;&#37319;&#29992;&#36825;&#20123;&#20307;&#31995;&#32467;&#26500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30001;&#20110;&#29616;&#26377;&#30340;&#38754;&#21521;&#23545;&#35937;&#27169;&#22411;&#20381;&#36182;&#20110;&#22522;&#20110;RNN&#30340;&#23454;&#29616;&#65292;&#23548;&#33268;&#20854;&#31283;&#23450;&#24615;&#21644;&#23481;&#37327;&#36739;&#24046;&#65292;&#24182;&#19988;&#22312;&#38271;&#24207;&#21015;&#19978;&#35757;&#32451;&#32531;&#24930;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#24182;&#34892;&#21270;&#26102;&#31354;&#32465;&#23450;&#22120;&#25110;PSB&#65292;&#36825;&#26159;&#38754;&#21521;&#24207;&#21015;&#36755;&#20837;&#30340;&#31532;&#19968;&#20010;&#26102;&#38388;&#24182;&#34892;&#21487;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;RNN&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;PSB&#21516;&#26102;&#20026;&#25152;&#26377;&#26102;&#38388;&#27493;&#29983;&#25104;&#23545;&#35937;&#20013;&#24515;&#21270;&#34920;&#31034;&#65292;&#31216;&#20026;slots&#12290;&#36890;&#36807;&#22312;&#20855;&#26377;&#22240;&#26524;&#20851;&#27880;&#21147;&#30340;&#22266;&#23450;&#23618;&#25968;&#19978;&#31934;&#32454;&#21270;&#21021;&#22987;slots&#65292;&#25105;&#20204;&#24471;&#20197;&#22312;&#25152;&#26377;&#26102;&#38388;&#27493;&#39588;&#19978;&#23454;&#29616;&#24182;&#34892;&#12290;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#25105;&#20204;&#20307;&#31995;&#32467;&#26500;&#25152;&#24341;&#20837;&#30340;&#24182;&#34892;&#24615;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#25928;&#29575;&#19978;&#26174;&#31034;&#20986;&#26174;&#33879;&#25552;&#21319;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23545;PSB&#36827;&#34892;&#20102;&#24191;&#27867;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17077v1 Announce Type: new  Abstract: While modern best practices advocate for scalable architectures that support long-range interactions, object-centric models are yet to fully embrace these architectures. In particular, existing object-centric models for handling sequential inputs, due to their reliance on RNN-based implementation, show poor stability and capacity and are slow to train on long sequences. We introduce Parallelizable Spatiotemporal Binder or PSB, the first temporally-parallelizable slot learning architecture for sequential inputs. Unlike conventional RNN-based approaches, PSB produces object-centric representations, known as slots, for all time-steps in parallel. This is achieved by refining the initial slots across all time-steps through a fixed number of layers equipped with causal attention. By capitalizing on the parallelism induced by our architecture, the proposed model exhibits a significant boost in efficiency. In experiments, we test PSB extensivel
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36229;&#39640;&#32500;&#35745;&#31639;&#36827;&#34892;&#21333;&#27425;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#39640;&#32500;&#31354;&#38388;&#24182;&#21033;&#29992;HD&#36816;&#31639;&#31526;&#36827;&#34892;&#20449;&#24687;&#32858;&#21512;&#65292;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#35745;&#31639;&#26114;&#36149;&#30340;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2402.17073</link><description>&lt;p&gt;
&#20351;&#29992;&#36229;&#39640;&#32500;&#35745;&#31639;&#36827;&#34892;&#21333;&#27425;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
One-Shot Graph Representation Learning Using Hyperdimensional Computing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17073
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36229;&#39640;&#32500;&#35745;&#31639;&#36827;&#34892;&#21333;&#27425;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#39640;&#32500;&#31354;&#38388;&#24182;&#21033;&#29992;HD&#36816;&#31639;&#31526;&#36827;&#34892;&#20449;&#24687;&#32858;&#21512;&#65292;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#35745;&#31639;&#26114;&#36149;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#31616;&#21333;&#12289;&#24555;&#36895;&#12289;&#39640;&#25928;&#30340;&#21322;&#30417;&#30563;&#22270;&#23398;&#20064;&#26041;&#27861;&#12290;&#25152;&#25552;&#26041;&#27861;&#21033;&#29992;&#36229;&#39640;&#32500;&#35745;&#31639;&#65292;&#23558;&#25968;&#25454;&#26679;&#26412;&#20351;&#29992;&#38543;&#26426;&#25237;&#24433;&#32534;&#30721;&#21040;&#39640;&#32500;&#31354;&#38388;&#65288;&#31616;&#31216;HD&#31354;&#38388;&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#33410;&#28857;&#34920;&#31034;&#30340;&#21333;&#23556;&#24615;&#36136;&#30340;&#36229;&#39640;&#32500;&#22270;&#23398;&#20064;&#65288;HDGL&#65289;&#31639;&#27861;&#12290;HDGL&#23558;&#33410;&#28857;&#29305;&#24449;&#26144;&#23556;&#21040;HD&#31354;&#38388;&#65292;&#28982;&#21518;&#20351;&#29992;HD&#36816;&#31639;&#31526;&#65288;&#22914;&#25414;&#32465;&#21644;&#32465;&#23450;&#65289;&#26469;&#32858;&#21512;&#27599;&#20010;&#33410;&#28857;&#30340;&#23616;&#37096;&#37051;&#22495;&#20449;&#24687;&#12290;&#23545;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;HDGL&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#35745;&#31639;&#26114;&#36149;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17073v1 Announce Type: cross  Abstract: We present a novel, simple, fast, and efficient approach for semi-supervised learning on graphs. The proposed approach takes advantage of hyper-dimensional computing which encodes data samples using random projections into a high dimensional space (HD space for short). Specifically, we propose a Hyper-dimensional Graph Learning (HDGL) algorithm that leverages the injectivity property of the node representations of a family of graph neural networks. HDGL maps node features to the HD space and then uses HD operators such as bundling and binding to aggregate information from the local neighborhood of each node. Results of experiments with widely used benchmark data sets show that HDGL achieves predictive performance that is competitive with the state-of-the-art deep learning methods, without the need for computationally expensive training.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#36739;&#20302;&#20998;&#36776;&#29575;&#36827;&#34892;&#26080;&#26465;&#20214;&#35757;&#32451;&#65292;&#20801;&#35768;&#38271;&#23614;&#31867;&#21035;&#20174;&#20449;&#24687;&#26356;&#20016;&#23500;&#30340;&#31867;&#21035;&#20013;&#20849;&#20139;&#30693;&#35782;&#65292;&#20197;&#25913;&#21892;&#38271;&#23614;&#25968;&#25454;&#19979;&#31867;&#21035;&#26465;&#20214;GANs&#30340;&#35757;&#32451;</title><link>https://arxiv.org/abs/2402.17065</link><description>&lt;p&gt;
&#39535;&#26381;&#31867;&#21035;&#26465;&#20214;GAN&#20013;&#30340;&#38271;&#23614;&#38382;&#39064;&#65306;&#36890;&#36807;&#22312;&#36739;&#20302;&#20998;&#36776;&#29575;&#36827;&#34892;&#26080;&#26465;&#20214;&#35757;&#32451;&#36827;&#34892;&#30693;&#35782;&#20849;&#20139;
&lt;/p&gt;
&lt;p&gt;
Taming the Tail in Class-Conditional GANs: Knowledge Sharing via Unconditional Training at Lower Resolutions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17065
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#36739;&#20302;&#20998;&#36776;&#29575;&#36827;&#34892;&#26080;&#26465;&#20214;&#35757;&#32451;&#65292;&#20801;&#35768;&#38271;&#23614;&#31867;&#21035;&#20174;&#20449;&#24687;&#26356;&#20016;&#23500;&#30340;&#31867;&#21035;&#20013;&#20849;&#20139;&#30693;&#35782;&#65292;&#20197;&#25913;&#21892;&#38271;&#23614;&#25968;&#25454;&#19979;&#31867;&#21035;&#26465;&#20214;GANs&#30340;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23545;&#20110;&#20351;&#29992;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#35757;&#32451;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#20174;&#38271;&#23614;&#35757;&#32451;&#20998;&#24067;&#29983;&#25104;&#22270;&#20687;&#30340;&#25216;&#26415;&#20173;&#28982;&#30456;&#24403;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#23384;&#22312;&#19981;&#24179;&#34913;&#30340;&#22810;&#31867;&#21035;&#35757;&#32451;&#25968;&#25454;&#26102;&#65292;GANs&#20542;&#21521;&#20110;&#20559;&#29233;&#26679;&#26412;&#26356;&#22810;&#30340;&#31867;&#21035;&#65292;&#23548;&#33268;&#23614;&#37096;&#31867;&#21035;&#30340;&#29983;&#25104;&#20302;&#36136;&#37327;&#19988;&#26679;&#26412;&#19981;&#22815;&#22810;&#26679;&#21270;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25913;&#36827;&#20351;&#29992;&#38271;&#23614;&#25968;&#25454;&#35757;&#32451;&#31867;&#21035;&#26465;&#20214;GANs&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#30693;&#35782;&#20849;&#20139;&#26041;&#27861;&#65292;&#20801;&#35768;&#23614;&#37096;&#31867;&#21035;&#20174;&#35757;&#32451;&#25968;&#25454;&#26356;&#20016;&#23500;&#30340;&#31867;&#21035;&#20013;&#20511;&#37492;&#20016;&#23500;&#30340;&#20449;&#24687;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#31867;&#21035;&#26465;&#20214;GAN&#26550;&#26500;&#36827;&#34892;&#20102;&#20462;&#25913;&#65292;&#20197;&#30830;&#20445;&#29983;&#25104;&#22120;&#30340;&#36739;&#20302;&#20998;&#36776;&#29575;&#23618;&#23436;&#20840;&#26080;&#26465;&#20214;&#22320;&#36827;&#34892;&#35757;&#32451;&#65292;&#21516;&#26102;&#23558;&#31867;&#21035;&#26465;&#20214;&#29983;&#25104;&#20445;&#30041;&#32473;&#36739;&#39640;&#20998;&#36776;&#29575;&#23618;&#12290;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17065v1 Announce Type: cross  Abstract: Despite the extensive research on training generative adversarial networks (GANs) with limited training data, learning to generate images from long-tailed training distributions remains fairly unexplored. In the presence of imbalanced multi-class training data, GANs tend to favor classes with more samples, leading to the generation of low-quality and less diverse samples in tail classes. In this study, we aim to improve the training of class-conditional GANs with long-tailed data. We propose a straightforward yet effective method for knowledge sharing, allowing tail classes to borrow from the rich information from classes with more abundant training data. More concretely, we propose modifications to existing class-conditional GAN architectures to ensure that the lower-resolution layers of the generator are trained entirely unconditionally while reserving class-conditional generation for the higher-resolution layers. Experiments on seve
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#39640;&#32500;&#35774;&#35745;&#31354;&#38388;&#30340;&#26032;&#22411;&#38477;&#38454;&#27169;&#22411;&#22810;&#20445;&#30495;&#24230;&#26041;&#27861;&#65292;&#38598;&#25104;&#20102;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#29992;&#20110;&#27969;&#24418;&#23545;&#40784;&#21644;&#32500;&#24230;&#20943;&#23569;&#12290;</title><link>https://arxiv.org/abs/2402.17061</link><description>&lt;p&gt;
&#19968;&#20010;&#36866;&#29992;&#20110;&#39640;&#32500;&#36755;&#20837;&#30340;&#38477;&#38454;&#27169;&#22411;&#22810;&#20445;&#30495;&#24230;&#26041;&#27861;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Multi-Fidelity Methodology for Reduced Order Models with High-Dimensional Inputs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17061
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#39640;&#32500;&#35774;&#35745;&#31354;&#38388;&#30340;&#26032;&#22411;&#38477;&#38454;&#27169;&#22411;&#22810;&#20445;&#30495;&#24230;&#26041;&#27861;&#65292;&#38598;&#25104;&#20102;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#29992;&#20110;&#27969;&#24418;&#23545;&#40784;&#21644;&#32500;&#24230;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33322;&#31354;&#33322;&#22825;&#35774;&#35745;&#30340;&#26089;&#26399;&#38454;&#27573;&#65292;&#38477;&#38454;&#27169;&#22411;&#65288;ROMs&#65289;&#23545;&#20110;&#22312;&#22810;&#27425;&#35780;&#20272;&#30028;&#38754;&#29289;&#29702;&#20016;&#23500;&#30340;&#22330;&#20449;&#24687;&#30340;&#35768;&#22810;&#26597;&#35810;&#24773;&#26223;&#20013;&#26368;&#23567;&#21270;&#35745;&#31639;&#25104;&#26412;&#33267;&#20851;&#37325;&#35201;&#12290;&#33322;&#31354;&#33322;&#22825;&#35774;&#35745;&#30340;&#22797;&#26434;&#24615;&#35201;&#27714;&#20351;&#29992;&#39640;&#32500;&#35774;&#35745;&#31354;&#38388;&#26469;&#20934;&#30830;&#25429;&#33719;&#35814;&#32454;&#29305;&#24449;&#21644;&#35774;&#35745;&#21464;&#24322;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31354;&#38388;&#24341;&#20837;&#20102;&#37325;&#35201;&#25361;&#25112;&#65292;&#21253;&#25324;&#32500;&#24230;&#30340;&#35781;&#21650;&#65292;&#36825;&#26159;&#30001;&#20110;&#39640;&#32500;&#36755;&#20837;&#21644;&#36755;&#20986;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#21644;&#35745;&#31639;&#21162;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#22797;&#26434;&#24615;&#65292;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#32500;&#32972;&#26223;&#35774;&#35745;&#30340;&#26032;&#39062;&#30340;&#22810;&#20445;&#30495;&#24230;&#12289;&#21442;&#25968;&#21270;&#21644;&#38750;&#20405;&#20837;&#24335;ROM&#26694;&#26550;&#12290;&#23427;&#38598;&#25104;&#20102;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#29992;&#20110;&#27969;&#24418;&#23545;&#40784;&#21644;&#32500;&#24230;&#20943;&#23569;&#65292;&#37319;&#29992;Proper Orthogonal Decomposition&#65288;POD&#65289;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#20027;&#21160;&#23376;&#31354;&#38388;&#19982;&#22810;&#20445;&#30495;&#24230;&#22238;&#24402;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17061v1 Announce Type: new  Abstract: In the early stages of aerospace design, reduced order models (ROMs) are crucial for minimizing computational costs associated with using physics-rich field information in many-query scenarios requiring multiple evaluations. The intricacy of aerospace design demands the use of high-dimensional design spaces to capture detailed features and design variability accurately. However, these spaces introduce significant challenges, including the curse of dimensionality, which stems from both high-dimensional inputs and outputs necessitating substantial training data and computational effort. To address these complexities, this study introduces a novel multi-fidelity, parametric, and non-intrusive ROM framework designed for high-dimensional contexts. It integrates machine learning techniques for manifold alignment and dimension reduction employing Proper Orthogonal Decomposition (POD) and Model-based Active Subspace with multi-fidelity regressio
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#20102;&#36807;&#21435;10&#24180;&#38024;&#23545;&#19981;&#21516;&#31867;&#22411;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#30340;&#21508;&#31181;&#26368;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#30528;&#37325;&#27604;&#36739;&#20102;&#26368;&#26032;&#30340;&#24037;&#20316;&#12290;</title><link>https://arxiv.org/abs/2402.17045</link><description>&lt;p&gt;
&#23545;&#21508;&#31867;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#30340;&#26368;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24615;&#33021;&#30340;&#30740;&#31350;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
An Investigation into the Performances of the State-of-the-art Machine Learning Approaches for Various Cyber-attack Detection: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17045
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#36807;&#21435;10&#24180;&#38024;&#23545;&#19981;&#21516;&#31867;&#22411;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#30340;&#21508;&#31181;&#26368;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#30528;&#37325;&#27604;&#36739;&#20102;&#26368;&#26032;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20445;&#25252;&#35745;&#31639;&#26426;&#21644;&#20449;&#24687;&#31995;&#32479;&#20813;&#21463;&#25915;&#20987;&#32773;&#21033;&#29992;&#31995;&#32479;&#20013;&#30340;&#28431;&#27934;&#36827;&#34892;&#32593;&#32476;&#29359;&#32618;&#30340;&#20405;&#23475;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#29992;&#20110;&#23454;&#26102;&#26816;&#27979;&#28431;&#27934;&#20197;&#25552;&#39640;&#20449;&#24687;&#31995;&#32479;&#23433;&#20840;&#24615;&#30340;&#26041;&#27861;&#12290;&#22312;&#25152;&#26377;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#26159;&#23454;&#29616;&#31995;&#32479;&#23433;&#20840;&#30340;&#25928;&#26524;&#26368;&#22909;&#30340;&#26041;&#27861;&#65292;&#20854;&#33021;&#21147;&#33539;&#22260;&#20174;&#26089;&#26399;&#26816;&#27979;&#36719;&#20214;&#28431;&#27934;&#21040;&#23454;&#26102;&#26816;&#27979;&#31995;&#32479;&#20013;&#27491;&#22312;&#36827;&#34892;&#30340;&#22949;&#21327;&#12290;&#30001;&#20110;&#23384;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#32593;&#32476;&#25915;&#20987;&#65292;&#27599;&#31181;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37117;&#20381;&#36182;&#20110;&#19981;&#21516;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#36825;&#20063;&#24433;&#21709;&#20102;&#23427;&#20204;&#23545;&#29305;&#23450;&#31867;&#22411;&#32593;&#32476;&#25915;&#20987;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36807;&#21435;10&#24180;&#38024;&#23545;&#19981;&#21516;&#31867;&#22411;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#30340;&#27599;&#19968;&#20010;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#37325;&#28857;&#25918;&#22312;&#26368;&#36817;&#30340;&#24037;&#20316;&#19978;&#20197;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17045v1 Announce Type: cross  Abstract: To secure computers and information systems from attackers taking advantage of vulnerabilities in the system to commit cybercrime, several methods have been proposed for real-time detection of vulnerabilities to improve security around information systems. Of all the proposed methods, machine learning had been the most effective method in securing a system with capabilities ranging from early detection of software vulnerabilities to real-time detection of ongoing compromise in a system. As there are different types of cyberattacks, each of the existing state-of-the-art machine learning models depends on different algorithms for training which also impact their suitability for detection of a particular type of cyberattack. In this research, we analyzed each of the current state-of-theart machine learning models for different types of cyberattack detection from the past 10 years with a major emphasis on the most recent works for comparat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#35797;&#22270;&#35299;&#20915;&#20174;&#35797;&#39564;&#32467;&#26524;&#25512;&#24191;&#21040;&#30446;&#26631;&#31181;&#32676;&#30340;&#22806;&#37096;&#26377;&#25928;&#24615;&#25361;&#25112;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;</title><link>https://arxiv.org/abs/2402.17042</link><description>&lt;p&gt;
&#36890;&#21521;&#20174;&#35797;&#39564;&#25512;&#24191;&#25512;&#29702;&#21040;&#30446;&#26631;&#31181;&#32676;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Towards Generalizing Inferences from Trials to Target Populations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#35797;&#22270;&#35299;&#20915;&#20174;&#35797;&#39564;&#32467;&#26524;&#25512;&#24191;&#21040;&#30446;&#26631;&#31181;&#32676;&#30340;&#22806;&#37096;&#26377;&#25928;&#24615;&#25361;&#25112;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#65288;RCTs&#65289;&#22312;&#20135;&#29983;&#20869;&#37096;&#26377;&#25928;&#20272;&#35745;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#32780;&#23545;&#25193;&#23637;&#36825;&#20123;&#21457;&#29616;&#20197;&#33719;&#24471;&#22806;&#37096;&#26377;&#25928;&#20272;&#35745;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#20419;&#36827;&#26356;&#24191;&#27867;&#30340;&#31185;&#23398;&#25506;&#31350;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24212;&#23545;&#36825;&#20123;&#22806;&#37096;&#26377;&#25928;&#24615;&#25361;&#25112;&#30340;&#21069;&#27839;&#65292;&#27010;&#25324;&#20102;2023&#24180;&#31179;&#23395;&#22312;&#24067;&#26391;&#22823;&#23398;&#35745;&#31639;&#19982;&#23454;&#39564;&#25968;&#23398;&#30740;&#31350;&#25152;&#65288;ICERM&#65289;&#20030;&#34892;&#30340;&#19968;&#27425;&#36328;&#23398;&#31185;&#30740;&#35752;&#20250;&#30340;&#31934;&#21326;&#12290;&#35813;&#30740;&#35752;&#20250;&#27719;&#38598;&#20102;&#26469;&#33258;&#31038;&#20250;&#31185;&#23398;&#12289;&#21307;&#23398;&#12289;&#20844;&#20849;&#21355;&#29983;&#12289;&#32479;&#35745;&#23398;&#12289;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#25945;&#32946;&#31561;&#21508;&#20010;&#39046;&#22495;&#30340;&#19987;&#23478;&#65292;&#20197;&#35299;&#20915;&#27599;&#20010;&#23398;&#31185;&#22312;&#25512;&#26029;&#23454;&#39564;&#32467;&#26524;&#26041;&#38754;&#38754;&#20020;&#30340;&#29420;&#29305;&#38556;&#30861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#20010;&#20851;&#38190;&#36129;&#29486;&#65306;&#25105;&#20204;&#25972;&#21512;&#27491;&#22312;&#36827;&#34892;&#30340;&#21162;&#21147;&#65292;&#31361;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17042v1 Announce Type: cross  Abstract: Randomized Controlled Trials (RCTs) are pivotal in generating internally valid estimates with minimal assumptions, serving as a cornerstone for researchers dedicated to advancing causal inference methods. However, extending these findings beyond the experimental cohort to achieve externally valid estimates is crucial for broader scientific inquiry. This paper delves into the forefront of addressing these external validity challenges, encapsulating the essence of a multidisciplinary workshop held at the Institute for Computational and Experimental Research in Mathematics (ICERM), Brown University, in Fall 2023. The workshop congregated experts from diverse fields including social science, medicine, public health, statistics, computer science, and education, to tackle the unique obstacles each discipline faces in extrapolating experimental findings. Our study presents three key contributions: we integrate ongoing efforts, highlighting me
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36845;&#20195;INLA&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#20013;&#25512;&#26029;&#29366;&#24577;&#21644;&#21442;&#25968;&#65292;&#33021;&#22815;&#20445;&#30041;&#21487;&#35299;&#37322;&#24615;&#24182;&#19988;&#36866;&#29992;&#20110;&#20219;&#24847;&#38750;&#32447;&#24615;&#31995;&#32479;&#12290;</title><link>https://arxiv.org/abs/2402.17036</link><description>&lt;p&gt;
&#36845;&#20195;INLA&#29992;&#20110;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#29366;&#24577;&#21644;&#21442;&#25968;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Iterated INLA for State and Parameter Estimation in Nonlinear Dynamical Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17036
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36845;&#20195;INLA&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#20013;&#25512;&#26029;&#29366;&#24577;&#21644;&#21442;&#25968;&#65292;&#33021;&#22815;&#20445;&#30041;&#21487;&#35299;&#37322;&#24615;&#24182;&#19988;&#36866;&#29992;&#20110;&#20219;&#24847;&#38750;&#32447;&#24615;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#21516;&#21270;&#65288;DA&#65289;&#26041;&#27861;&#20351;&#29992;&#28304;&#33258;&#24494;&#20998;&#26041;&#31243;&#30340;&#20808;&#39564;&#26465;&#20214;&#26469;&#31283;&#20581;&#22320;&#23545;&#25968;&#25454;&#36827;&#34892;&#25554;&#20540;&#21644;&#22806;&#25512;&#12290;&#27969;&#34892;&#30340;&#25216;&#26415;&#65292;&#22914;&#22788;&#29702;&#39640;&#32500;&#38750;&#32447;&#24615;PDE&#20808;&#39564;&#26465;&#20214;&#30340;&#38598;&#21512;&#26041;&#27861;&#65292;&#20027;&#35201;&#20851;&#27880;&#29366;&#24577;&#20272;&#35745;&#65292;&#20294;&#21487;&#33021;&#20250;&#22312;&#23398;&#20064;&#21442;&#25968;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#21487;&#20197;&#33258;&#28982;&#22320;&#23398;&#20064;&#29366;&#24577;&#21644;&#21442;&#25968;&#65292;&#20294;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#65292;&#25110;&#32773;&#20135;&#29983;&#38590;&#20197;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#21463;&#31354;&#38388;&#32479;&#35745;&#20013;&#38598;&#25104;&#23884;&#22871;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#65288;INLA&#65289;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36845;&#20195;&#32447;&#24615;&#21270;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;DA&#26367;&#20195;&#26041;&#27861;&#12290;&#36825;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#20135;&#29983;&#19968;&#20010;&#39640;&#26031;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#65292;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;INLA&#26469;&#25512;&#26029;&#29366;&#24577;&#21644;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#20219;&#24847;&#38750;&#32447;&#24615;&#31995;&#32479;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#36827;&#19968;&#27493;&#34987;&#35777;&#26126;&#21487;&#20197;o
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17036v1 Announce Type: cross  Abstract: Data assimilation (DA) methods use priors arising from differential equations to robustly interpolate and extrapolate data. Popular techniques such as ensemble methods that handle high-dimensional, nonlinear PDE priors focus mostly on state estimation, however can have difficulty learning the parameters accurately. On the other hand, machine learning based approaches can naturally learn the state and parameters, but their applicability can be limited, or produce uncertainties that are hard to interpret. Inspired by the Integrated Nested Laplace Approximation (INLA) method in spatial statistics, we propose an alternative approach to DA based on iteratively linearising the dynamical model. This produces a Gaussian Markov random field at each iteration, enabling one to use INLA to infer the state and parameters. Our approach can be used for arbitrary nonlinear systems, while retaining interpretability, and is furthermore demonstrated to o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;REFACTOR&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20174;&#35777;&#26126;&#20013;&#25552;&#21462;&#23450;&#29702;&#65292;&#26032;&#23450;&#29702;&#30340;&#24341;&#20837;&#24110;&#21161;&#32553;&#30701;&#35777;&#26126;&#38271;&#24230;&#24182;&#25552;&#39640;&#35777;&#26126;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.17032</link><description>&lt;p&gt;
&#20174;&#35777;&#26126;&#20013;&#25552;&#21462;&#23450;&#29702;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
REFACTOR: Learning to Extract Theorems from Proofs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17032
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;REFACTOR&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20174;&#35777;&#26126;&#20013;&#25552;&#21462;&#23450;&#29702;&#65292;&#26032;&#23450;&#29702;&#30340;&#24341;&#20837;&#24110;&#21161;&#32553;&#30701;&#35777;&#26126;&#38271;&#24230;&#24182;&#25552;&#39640;&#35777;&#26126;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#25968;&#23398;&#23478;&#36890;&#24120;&#25797;&#38271;&#35782;&#21035;&#27169;&#22359;&#21270;&#21644;&#21487;&#37325;&#29992;&#30340;&#23450;&#29702;&#65292;&#36825;&#20123;&#23450;&#29702;&#20351;&#22797;&#26434;&#30340;&#25968;&#23398;&#32467;&#26524;&#26131;&#20110;&#33719;&#24471;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#23450;&#29702;&#20174;&#35777;&#26126;&#20013;&#25552;&#21462;&#22120;&#65288;REFACTOR&#65289;&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#20223;&#36825;&#31181;&#24418;&#24335;&#25968;&#23398;&#23450;&#29702;&#35777;&#26126;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19968;&#32452;&#26410;&#35265;&#35777;&#26126;&#19978;&#65292;REFACTOR&#33021;&#22815;&#25552;&#21462;&#20986;&#20154;&#31867;&#22312;&#20889;&#35777;&#26126;&#26102;&#20250;&#20351;&#29992;&#30340;19.6%&#30340;&#23450;&#29702;&#12290;&#24403;&#23558;&#27169;&#22411;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;Metamath&#24211;&#26102;&#65292;REFACTOR&#25552;&#21462;&#20986;&#20102;16&#20010;&#26032;&#23450;&#29702;&#12290;&#36890;&#36807;&#26032;&#25552;&#21462;&#30340;&#23450;&#29702;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;MetaMath&#25968;&#25454;&#24211;&#20013;&#30340;&#29616;&#26377;&#35777;&#26126;&#21487;&#20197;&#34987;&#37325;&#26500;&#12290;&#32463;&#37325;&#26500;&#21518;&#65292;&#36825;&#20123;&#26032;&#23450;&#29702;&#34987;&#38750;&#24120;&#39057;&#32321;&#22320;&#20351;&#29992;&#65292;&#24179;&#22343;&#20351;&#29992;&#27425;&#25968;&#20026;733.5&#27425;&#65292;&#24182;&#26377;&#21161;&#20110;&#32553;&#30701;&#35777;&#26126;&#38271;&#24230;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#32463;&#36807;&#26032;&#23450;&#29702;&#37325;&#26500;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#35777;&#26126;&#32773;&#35777;&#26126;&#20102;&#26356;&#22810;&#27979;&#35797;&#23450;&#29702;&#65292;&#24182;&#36890;&#36807;&#39057;&#32321;&#21033;&#29992;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17032v1 Announce Type: new  Abstract: Human mathematicians are often good at recognizing modular and reusable theorems that make complex mathematical results within reach. In this paper, we propose a novel method called theoREm-from-prooF extrACTOR (REFACTOR) for training neural networks to mimic this ability in formal mathematical theorem proving. We show on a set of unseen proofs, REFACTOR is able to extract 19.6% of the theorems that humans would use to write the proofs. When applying the model to the existing Metamath library, REFACTOR extracted 16 new theorems. With newly extracted theorems, we show that the existing proofs in the MetaMath database can be refactored. The new theorems are used very frequently after refactoring, with an average usage of 733.5 times, and help shorten the proof lengths. Lastly, we demonstrate that the prover trained on the new-theorem refactored dataset proves more test theorems and outperforms state-of-the-art baselines by frequently lever
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#31070;&#32463;&#27169;&#22411;&#20013;&#24341;&#20837;&#19981;&#21516;iable&#21644;&#23436;&#20840;&#21367;&#31215;&#30340;&#21069;&#31471;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#36339;&#36291;&#36830;&#25509;&#65292;&#25104;&#21151;&#23454;&#29616;&#23545;&#26799;&#24230;&#25915;&#20987;&#30340;&#26174;&#33879;&#38887;&#24615;&#65292;&#24182;&#36890;&#36807;&#23558;&#27169;&#22411;&#32452;&#21512;&#25104;&#38543;&#26426;&#38598;&#21512;&#65292;&#26377;&#25928;&#23545;&#25239;&#40657;&#30418;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2402.17018</link><description>&lt;p&gt;
&#36890;&#36807;&#23436;&#20840;&#21367;&#31215;&#21644;&#21487;&#24494;&#30340;&#21069;&#31471;&#19982;&#36339;&#36291;&#36830;&#25509;&#23545;&#26799;&#24230;&#25915;&#20987;&#34920;&#29616;&#20986;&#26174;&#33879;&#38887;&#24615;&#30340;&#32784;&#20154;&#23547;&#21619;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
A Curious Case of Remarkable Resilience to Gradient Attacks via Fully Convolutional and Differentiable Front End with a Skip Connection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17018
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#31070;&#32463;&#27169;&#22411;&#20013;&#24341;&#20837;&#19981;&#21516;iable&#21644;&#23436;&#20840;&#21367;&#31215;&#30340;&#21069;&#31471;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#36339;&#36291;&#36830;&#25509;&#65292;&#25104;&#21151;&#23454;&#29616;&#23545;&#26799;&#24230;&#25915;&#20987;&#30340;&#26174;&#33879;&#38887;&#24615;&#65292;&#24182;&#36890;&#36807;&#23558;&#27169;&#22411;&#32452;&#21512;&#25104;&#38543;&#26426;&#38598;&#21512;&#65292;&#26377;&#25928;&#23545;&#25239;&#40657;&#30418;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#27979;&#35797;&#20102;&#36890;&#36807;&#22312;&#19968;&#20010;&#20923;&#32467;&#30340;&#20998;&#31867;&#22120;&#20043;&#21069;&#22686;&#21152;&#19968;&#20010;&#21487;&#24494;&#19988;&#23436;&#20840;&#21367;&#31215;&#30340;&#27169;&#22411;&#65292;&#24182;&#20855;&#26377;&#36339;&#36291;&#36830;&#25509;&#30340;&#21069;&#31471;&#22686;&#24378;&#31070;&#32463;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#36739;&#23567;&#30340;&#23398;&#20064;&#29575;&#36827;&#34892;&#22823;&#32422;&#19968;&#20010;epoch&#30340;&#35757;&#32451;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20123;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20445;&#25345;&#39592;&#24178;&#20998;&#31867;&#22120;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#23545;&#21253;&#25324;AutoAttack&#36719;&#20214;&#21253;&#20013;&#30340;APGD&#21644;FAB-T&#25915;&#20987;&#22312;&#20869;&#30340;&#26799;&#24230;&#25915;&#20987;&#20855;&#26377;&#24322;&#24120;&#30340;&#25269;&#25239;&#21147;&#65292;&#36825;&#24402;&#22240;&#20110;&#26799;&#24230;&#25513;&#30422;&#12290;&#26799;&#24230;&#25513;&#30422;&#29616;&#35937;&#24182;&#19981;&#26032;&#40092;&#65292;&#20294;&#23545;&#20110;&#36825;&#20123;&#27809;&#26377;&#26799;&#24230;&#30772;&#22351;&#37096;&#20998;&#65288;&#22914;JPEG&#21387;&#32553;&#25110;&#39044;&#35745;&#23548;&#33268;&#26799;&#24230;&#20943;&#23567;&#30340;&#37096;&#20998;&#65289;&#30340;&#23436;&#20840;&#21487;&#24494;&#27169;&#22411;&#26469;&#35828;&#65292;&#25513;&#30422;&#30340;&#31243;&#24230;&#30456;&#24403;&#26174;&#33879;&#12290;&#23613;&#31649;&#40657;&#30418;&#25915;&#20987;&#23545;&#26799;&#24230;&#25513;&#30422;&#21487;&#33021;&#37096;&#20998;&#26377;&#25928;&#65292;&#20294;&#36890;&#36807;&#23558;&#27169;&#22411;&#32452;&#21512;&#25104;&#38543;&#26426;&#38598;&#21512;&#65292;&#21487;&#20197;&#36731;&#26494;&#20987;&#36133;&#23427;&#20204;&#12290;&#25105;&#20204;&#20272;&#35745;&#36825;&#26679;&#30340;&#38598;&#21512;&#22312;CIFAR10&#21644;CIF&#31561;&#19978;&#23454;&#29616;&#20102;&#20960;&#20046;SOTA&#32423;&#21035;&#30340;AutoAttack&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17018v1 Announce Type: cross  Abstract: We tested front-end enhanced neural models where a frozen classifier was prepended by a differentiable and fully convolutional model with a skip connection. By training them using a small learning rate for about one epoch, we obtained models that retained the accuracy of the backbone classifier while being unusually resistant to gradient attacks including APGD and FAB-T attacks from the AutoAttack package, which we attributed to gradient masking. The gradient masking phenomenon is not new, but the degree of masking was quite remarkable for fully differentiable models that did not have gradient-shattering components such as JPEG compression or components that are expected to cause diminishing gradients.   Though black box attacks can be partially effective against gradient masking, they are easily defeated by combining models into randomized ensembles. We estimate that such ensembles achieve near-SOTA AutoAttack accuracy on CIFAR10, CIF
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#22312;&#29790;&#22763;&#21496;&#27861;&#39044;&#27979;&#20013;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#21033;&#29992;&#20102;&#21807;&#19968;&#21487;&#29992;&#30340;&#22810;&#35821;&#35328;LJP&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#26368;&#26032;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;BERT-based LJP&#27169;&#22411;&#36827;&#34892;&#20102;&#21487;&#35299;&#37322;&#24615;&#33021;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.17013</link><description>&lt;p&gt;
&#22312;&#29790;&#22763;&#21496;&#27861;&#39044;&#27979;&#20013;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#21644;&#20844;&#24179;&#24615;&#65306;&#22312;&#19968;&#20010;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Towards Explainability and Fairness in Swiss Judgement Prediction: Benchmarking on a Multilingual Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#22312;&#29790;&#22763;&#21496;&#27861;&#39044;&#27979;&#20013;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#21033;&#29992;&#20102;&#21807;&#19968;&#21487;&#29992;&#30340;&#22810;&#35821;&#35328;LJP&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#26368;&#26032;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;BERT-based LJP&#27169;&#22411;&#36827;&#34892;&#20102;&#21487;&#35299;&#37322;&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17013v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#23545;&#27861;&#24459;&#35009;&#20915;&#39044;&#27979;&#65288;LJP&#65289;&#31995;&#32479;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#36827;&#34892;&#35780;&#20272;&#22312;&#26500;&#24314;&#20540;&#24471;&#20449;&#36182;&#21644;&#36879;&#26126;&#31995;&#32479;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#32771;&#34385;&#21040;&#36825;&#20123;&#31995;&#32479;&#20381;&#36182;&#21487;&#33021;&#32570;&#20047;&#27861;&#24459;&#30456;&#20851;&#24615;&#25110;&#28041;&#21450;&#25935;&#24863;&#23646;&#24615;&#30340;&#22240;&#32032;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;LJP&#27169;&#22411;&#20013;&#21487;&#35299;&#37322;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#39046;&#22495;&#65292;&#21033;&#29992;&#29790;&#22763;&#35009;&#20915;&#39044;&#27979;&#65288;SJP&#65289;&#36825;&#19968;&#21807;&#19968;&#21487;&#29992;&#30340;&#22810;&#35821;&#35328;LJP&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25972;&#29702;&#20102;&#19968;&#20010;&#21253;&#25324;108&#20010;&#26696;&#20363;&#30340;&#25903;&#25345;&#21644;&#21453;&#23545;&#27861;&#24459;&#19987;&#23478;&#35009;&#20915;&#30340;&#29702;&#30001;&#30340;&#20840;&#38754;&#25910;&#38598;&#65292;&#22312;&#24503;&#35821;&#12289;&#27861;&#35821;&#21644;&#24847;&#22823;&#21033;&#35821;&#20013;&#25552;&#20379;&#12290;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#36974;&#25377;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#26368;&#26032;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;BERT-based LJP&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#34920;&#29616;&#65292;&#20197;&#21450;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#31561;&#25216;&#26415;&#24320;&#21457;&#30340;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#23637;&#31034;&#20102;&#39044;&#27979;&#24615;&#33021;&#30340;&#25552;&#39640;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17013v1 Announce Type: cross  Abstract: The assessment of explainability in Legal Judgement Prediction (LJP) systems is of paramount importance in building trustworthy and transparent systems, particularly considering the reliance of these systems on factors that may lack legal relevance or involve sensitive attributes. This study delves into the realm of explainability and fairness in LJP models, utilizing Swiss Judgement Prediction (SJP), the only available multilingual LJP dataset. We curate a comprehensive collection of rationales that `support' and `oppose' judgement from legal experts for 108 cases in German, French, and Italian. By employing an occlusion-based explainability approach, we evaluate the explainability performance of state-of-the-art monolingual and multilingual BERT-based LJP models, as well as models developed with techniques such as data augmentation and cross-lingual transfer, which demonstrated prediction performance improvement. Notably, our finding
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20102;&#38544;&#31169;&#25915;&#20987;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#39318;&#20010;&#33021;&#21516;&#26102;&#23454;&#29616;&#39640;&#30495;&#27491;&#29575;&#21644;&#20302;&#35823;&#20998;&#31867;&#29575;&#30340;&#39044;&#35757;&#32451;LLMs&#20250;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIAs&#65289;&#65292;&#20197;&#21450;&#23637;&#31034;&#20102;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#21487;&#20197;&#20174;&#24494;&#35843;LLM&#20013;&#25552;&#21462;&#36229;&#36807;50%&#30340;&#24494;&#35843;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.17012</link><description>&lt;p&gt;
Pandora's White-Box&#65306;&#24320;&#25918;LLMs&#20013;&#35757;&#32451;&#25968;&#25454;&#27844;&#28431;&#30340;&#22686;&#21152;
&lt;/p&gt;
&lt;p&gt;
Pandora's White-Box: Increased Training Data Leakage in Open LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20102;&#38544;&#31169;&#25915;&#20987;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#39318;&#20010;&#33021;&#21516;&#26102;&#23454;&#29616;&#39640;&#30495;&#27491;&#29575;&#21644;&#20302;&#35823;&#20998;&#31867;&#29575;&#30340;&#39044;&#35757;&#32451;LLMs&#20250;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIAs&#65289;&#65292;&#20197;&#21450;&#23637;&#31034;&#20102;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#21487;&#20197;&#20174;&#24494;&#35843;LLM&#20013;&#25552;&#21462;&#36229;&#36807;50%&#30340;&#24494;&#35843;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36973;&#21463;&#30340;&#38544;&#31169;&#25915;&#20987;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#20854;&#20013;&#23545;&#25163;&#21487;&#20197;&#35775;&#38382;&#27169;&#22411;&#26435;&#37325;&#12289;&#26799;&#24230;&#25110;&#25439;&#22833;&#65292;&#35797;&#22270;&#21033;&#29992;&#23427;&#20204;&#26469;&#20102;&#35299;&#24213;&#23618;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#38024;&#23545;&#39044;&#35757;&#32451;LLMs&#30340;&#31532;&#19968;&#20010;&#20250;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIAs&#65289;&#65292;&#33021;&#22815;&#21516;&#26102;&#23454;&#29616;&#39640;TPR&#21644;&#20302;FPR&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#21487;&#20197;&#20174;&#24494;&#35843;LLM&#20013;&#25552;&#21462;&#36229;&#36807;50%&#30340;&#24494;&#35843;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#23545;&#24213;&#23618;&#27169;&#22411;&#30340;&#19981;&#21516;&#35775;&#38382;&#31243;&#24230;&#12289;&#35821;&#35328;&#27169;&#22411;&#30340;&#23450;&#21046;&#21270;&#20197;&#21450;&#25915;&#20987;&#32773;&#21487;&#20197;&#20351;&#29992;&#30340;&#36164;&#28304;&#12290;&#22312;&#39044;&#35757;&#32451;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#30333;&#30418;MIAs&#65306;&#22522;&#20110;&#26799;&#24230;&#33539;&#25968;&#30340;&#25915;&#20987;&#12289;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#21644;&#21333;&#27493;&#25439;&#22833;&#27604;&#25915;&#20987;&#12290;&#25152;&#26377;&#36825;&#20123;&#37117;&#20248;&#20110;&#29616;&#26377;&#30340;&#40657;&#30418;&#22522;&#32447;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;.....
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17012v1 Announce Type: cross  Abstract: In this paper we undertake a systematic study of privacy attacks against open source Large Language Models (LLMs), where an adversary has access to either the model weights, gradients, or losses, and tries to exploit them to learn something about the underlying training data. Our headline results are the first membership inference attacks (MIAs) against pre-trained LLMs that are able to simultaneously achieve high TPRs and low FPRs, and a pipeline showing that over $50\%$ (!) of the fine-tuning dataset can be extracted from a fine-tuned LLM in natural settings. We consider varying degrees of access to the underlying model, customization of the language model, and resources available to the attacker. In the pre-trained setting, we propose three new white-box MIAs: an attack based on the gradient norm, a supervised neural network classifier, and a single step loss ratio attack. All outperform existing black-box baselines, and our supervi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31639;&#27861;&#20934;&#30830;&#24615;&#20316;&#20026;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#37096;&#32626;&#22312;&#32447;RL&#31639;&#27861;&#30340;&#20851;&#38190;&#35201;&#27714;&#65292;&#24378;&#35843;&#20102;&#23545;&#21442;&#19982;&#32773;&#20445;&#25252;&#21644;&#25968;&#25454;&#31185;&#23398;&#25928;&#29992;&#30340;&#20445;&#30041;&#36131;&#20219;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#36827;&#34892;&#39044;&#37096;&#32626;&#35268;&#21010;&#21644;&#23454;&#26102;&#30417;&#27979;&#20197;&#30830;&#20445;&#31639;&#27861;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17003</link><description>&lt;p&gt;
&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#30417;&#27979;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Monitoring Fidelity of Online Reinforcement Learning Algorithms in Clinical Trials
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17003
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31639;&#27861;&#20934;&#30830;&#24615;&#20316;&#20026;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#37096;&#32626;&#22312;&#32447;RL&#31639;&#27861;&#30340;&#20851;&#38190;&#35201;&#27714;&#65292;&#24378;&#35843;&#20102;&#23545;&#21442;&#19982;&#32773;&#20445;&#25252;&#21644;&#25968;&#25454;&#31185;&#23398;&#25928;&#29992;&#30340;&#20445;&#30041;&#36131;&#20219;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#36827;&#34892;&#39044;&#37096;&#32626;&#35268;&#21010;&#21644;&#23454;&#26102;&#30417;&#27979;&#20197;&#30830;&#20445;&#31639;&#27861;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#20026;&#20010;&#24615;&#21270;&#20020;&#24202;&#35797;&#39564;&#20013;&#21442;&#19982;&#32773;&#30340;&#27835;&#30103;&#25552;&#20379;&#20102;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#39118;&#38505;&#21307;&#30103;&#39046;&#22495;&#37096;&#32626;&#22312;&#32447;&#33258;&#20027;&#31639;&#27861;&#20351;&#24471;&#36136;&#37327;&#25511;&#21046;&#21644;&#25968;&#25454;&#36136;&#37327;&#29305;&#21035;&#38590;&#20197;&#23454;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20316;&#20026;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#37096;&#32626;&#22312;&#32447;RL&#31639;&#27861;&#30340;&#20851;&#38190;&#35201;&#27714;&#30340;&#31639;&#27861;&#20934;&#30830;&#24615;&#12290;&#23427;&#24378;&#35843;&#20102;&#31639;&#27861;&#23545;&#65288;1&#65289;&#20445;&#25252;&#21442;&#19982;&#32773;&#21644;&#65288;2&#65289;&#20445;&#30041;&#25968;&#25454;&#22312;&#35797;&#39564;&#21518;&#20998;&#26512;&#20013;&#30340;&#31185;&#23398;&#25928;&#29992;&#30340;&#36131;&#20219;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#37096;&#32626;&#21069;&#35268;&#21010;&#21644;&#23454;&#26102;&#30417;&#27979;&#30340;&#26694;&#26550;&#65292;&#20197;&#21327;&#21161;&#31639;&#27861;&#24320;&#21457;&#32773;&#21644;&#20020;&#24202;&#30740;&#31350;&#20154;&#21592;&#30830;&#20445;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35828;&#26126;&#25105;&#20204;&#26694;&#26550;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#26469;&#33258;Oralytics&#20020;&#24202;&#35797;&#39564;&#30340;&#30495;&#23454;&#26696;&#20363;&#12290;&#33258;2023&#24180;&#26149;&#23395;&#20197;&#26469;&#65292;&#36825;&#39033;&#35797;&#39564;&#25104;&#21151;&#22320;&#37096;&#32626;&#20102;&#19968;&#31181;&#33258;&#20027;&#30340;&#22312;&#32447;RL&#31639;&#27861;&#26469;&#36827;&#34892;&#20010;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17003v1 Announce Type: cross  Abstract: Online reinforcement learning (RL) algorithms offer great potential for personalizing treatment for participants in clinical trials. However, deploying an online, autonomous algorithm in the high-stakes healthcare setting makes quality control and data quality especially difficult to achieve. This paper proposes algorithm fidelity as a critical requirement for deploying online RL algorithms in clinical trials. It emphasizes the responsibility of the algorithm to (1) safeguard participants and (2) preserve the scientific utility of the data for post-trial analyses. We also present a framework for pre-deployment planning and real-time monitoring to help algorithm developers and clinical researchers ensure algorithm fidelity. To illustrate our framework's practical application, we present real-world examples from the Oralytics clinical trial. Since Spring 2023, this trial successfully deployed an autonomous, online RL algorithm to persona
&lt;/p&gt;</description></item><item><title>HyperCube&#32593;&#32476;&#36890;&#36807;&#29420;&#29305;&#30340;&#22240;&#24335;&#20998;&#35299;&#26550;&#26500;&#21644;&#27491;&#21017;&#21270;&#22120;&#65292;&#25104;&#21151;&#23398;&#20064;&#20102;&#23545;&#31216;&#32676;&#30340;&#25805;&#20316;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#24674;&#22797;&#23436;&#25972;&#25805;&#20316;&#34920;&#65292;&#24182;&#24418;&#25104;&#24191;&#20041;&#20613;&#37324;&#21494;&#22522;&#36827;&#34892;&#32676;&#21367;&#31215;&#12290;</title><link>https://arxiv.org/abs/2402.17002</link><description>&lt;p&gt;
&#36890;&#36807;&#38544;&#21547;&#27491;&#20132;&#20559;&#32622;&#21457;&#29616;&#23545;&#31216;&#32676;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Discovering Symmetry Group Structures via Implicit Orthogonality Bias
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17002
&lt;/p&gt;
&lt;p&gt;
HyperCube&#32593;&#32476;&#36890;&#36807;&#29420;&#29305;&#30340;&#22240;&#24335;&#20998;&#35299;&#26550;&#26500;&#21644;&#27491;&#21017;&#21270;&#22120;&#65292;&#25104;&#21151;&#23398;&#20064;&#20102;&#23545;&#31216;&#32676;&#30340;&#25805;&#20316;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#24674;&#22797;&#23436;&#25972;&#25805;&#20316;&#34920;&#65292;&#24182;&#24418;&#25104;&#24191;&#20041;&#20613;&#37324;&#21494;&#22522;&#36827;&#34892;&#32676;&#21367;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;HyperCube&#32593;&#32476;&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#21457;&#29616;&#25968;&#25454;&#20013;&#23545;&#31216;&#32676;&#32467;&#26500;&#30340;&#26032;&#26041;&#27861;&#12290;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#29420;&#29305;&#30340;&#22240;&#24335;&#20998;&#35299;&#26550;&#26500;&#65292;&#32467;&#21512;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#22120;&#65292;&#21521;&#23398;&#20064;&#27491;&#20132;&#34920;&#31034;&#28748;&#36755;&#20102;&#24378;&#22823;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#36825;&#21033;&#29992;&#20102;&#34920;&#31034;&#29702;&#35770;&#30340;&#19968;&#20010;&#22522;&#26412;&#23450;&#29702;&#65292;&#21363;&#25152;&#26377;&#32039;&#33268;/&#26377;&#38480;&#32676;&#37117;&#21487;&#20197;&#30001;&#27491;&#20132;&#30697;&#38453;&#34920;&#31034;&#12290;HyperCube&#33021;&#22815;&#39640;&#25928;&#22320;&#20174;&#37096;&#20998;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#36890;&#29992;&#32676;&#25805;&#20316;&#65292;&#25104;&#21151;&#24674;&#22797;&#23436;&#25972;&#30340;&#25805;&#20316;&#34920;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25152;&#23398;&#20064;&#20986;&#30340;&#22240;&#32032;&#30452;&#25509;&#23545;&#24212;&#20110;&#24213;&#23618;&#32676;&#30340;&#31934;&#30830;&#30697;&#38453;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#22240;&#32032;&#25429;&#25417;&#21040;&#20102;&#32676;&#30340;&#23436;&#25972;&#19981;&#21487;&#32422;&#34920;&#31034;&#38598;&#21512;&#65292;&#24418;&#25104;&#20102;&#25191;&#34892;&#32676;&#21367;&#31215;&#30340;&#24191;&#20041;&#20613;&#37324;&#21494;&#22522;&#12290;&#22312;&#23545;&#32676;&#21644;&#38750;&#32676;&#31526;&#21495;&#25805;&#20316;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;HyperCube&#23637;&#31034;&#20102;10
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17002v1 Announce Type: new  Abstract: We introduce the HyperCube network, a novel approach for autonomously discovering symmetry group structures within data. The key innovation is a unique factorization architecture coupled with a novel regularizer that instills a powerful inductive bias towards learning orthogonal representations. This leverages a fundamental theorem of representation theory that all compact/finite groups can be represented by orthogonal matrices. HyperCube efficiently learns general group operations from partially observed data, successfully recovering complete operation tables. Remarkably, the learned factors correspond directly to exact matrix representations of the underlying group. Moreover, these factors capture the group's complete set of irreducible representations, forming the generalized Fourier basis for performing group convolutions. In extensive experiments with both group and non-group symbolic operations, HyperCube demonstrates a dramatic 10
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#32447;&#24615;&#25506;&#38024;&#65292;&#23558;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#39044;&#35757;&#32451;&#38899;&#39057;&#27169;&#22411;&#20013;&#30340;&#22768;&#38899;&#34920;&#31034;&#32852;&#31995;&#22312;&#19968;&#36215;&#65292;&#30740;&#31350;&#21457;&#29616;&#23613;&#31649;&#20165;&#22312;&#21407;&#22987;&#25991;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#19968;&#20123;&#23545;&#35937;&#30340;&#22768;&#38899;&#30693;&#35782;&#26377;&#30528;&#22522;&#20110;&#23454;&#36136;&#30340;&#32534;&#30721;&#12290;</title><link>https://arxiv.org/abs/2402.16998</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21548;&#21040;&#20102;&#20160;&#20040;&#65311;&#25506;&#31350;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21548;&#35273;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
What Do Language Models Hear? Probing for Auditory Representations in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16998
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#32447;&#24615;&#25506;&#38024;&#65292;&#23558;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#39044;&#35757;&#32451;&#38899;&#39057;&#27169;&#22411;&#20013;&#30340;&#22768;&#38899;&#34920;&#31034;&#32852;&#31995;&#22312;&#19968;&#36215;&#65292;&#30740;&#31350;&#21457;&#29616;&#23613;&#31649;&#20165;&#22312;&#21407;&#22987;&#25991;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#19968;&#20123;&#23545;&#35937;&#30340;&#22768;&#38899;&#30693;&#35782;&#26377;&#30528;&#22522;&#20110;&#23454;&#36136;&#30340;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#23545;&#29289;&#20307;&#30340;&#22768;&#38899;&#20855;&#26377;&#21547;&#20041;&#28145;&#21051;&#19988;&#22522;&#20110;&#23454;&#36136;&#30340;&#34920;&#24449;&#12290;&#25105;&#20204;&#23398;&#20064;&#20102;&#19968;&#20010;&#32447;&#24615;&#25506;&#38024;&#65292;&#36890;&#36807;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#38899;&#39057;&#27169;&#22411;&#32473;&#20986;&#19968;&#20010;&#23545;&#35937;&#30340;&#22768;&#38899;&#34920;&#31034;&#65292;&#20174;&#32780;&#22312;&#32473;&#23450;&#19982;&#35813;&#23545;&#35937;&#30456;&#20851;&#30340;&#38899;&#39057;&#29255;&#27573;&#30340;&#24773;&#20917;&#19979;&#26816;&#32034;&#20986;&#35813;&#23545;&#35937;&#30340;&#27491;&#30830;&#25991;&#26412;&#34920;&#31034;&#12290;&#36825;&#20010;&#25506;&#38024;&#26159;&#36890;&#36807;&#23545;&#27604;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#25512;&#21160;&#23545;&#35937;&#30340;&#35821;&#35328;&#34920;&#31034;&#21644;&#22768;&#38899;&#34920;&#31034;&#24444;&#27492;&#25509;&#36817;&#12290;&#22312;&#35757;&#32451;&#20043;&#21518;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#25506;&#38024;&#23545;&#20110;&#19968;&#20123;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#35265;&#36807;&#30340;&#23545;&#35937;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#38899;&#39057;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#25506;&#38024;&#30340;&#27867;&#21270;&#33021;&#21147;&#36229;&#36807;&#20102;&#38543;&#26426;&#29468;&#27979;&#30340;&#27700;&#24179;&#65292;&#36825;&#34920;&#26126;&#23613;&#31649;&#20165;&#22312;&#21407;&#22987;&#25991;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#19968;&#20123;&#23545;&#35937;&#30340;&#22768;&#38899;&#30693;&#35782;&#20855;&#26377;&#22522;&#20110;&#23454;&#36136;&#30340;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16998v1 Announce Type: cross  Abstract: This work explores whether language models encode meaningfully grounded representations of sounds of objects. We learn a linear probe that retrieves the correct text representation of an object given a snippet of audio related to that object, where the sound representation is given by a pretrained audio model. This probe is trained via a contrastive loss that pushes the language representations and sound representations of an object to be close to one another. After training, the probe is tested on its ability to generalize to objects that were not seen during training. Across different language models and audio models, we find that the probe generalization is above chance in many cases, indicating that despite being trained only on raw text, language models encode grounded knowledge of sounds for some objects.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20174;&#39045;&#20869;&#33041;&#30005;&#22270;&#25968;&#25454;&#35299;&#30721;&#34987;&#21160;&#21548;&#21462;&#30340;&#35328;&#35821;&#65292;&#26088;&#22312;&#25512;&#21160;&#33041;&#26426;&#25509;&#21475;&#25216;&#26415;&#30340;&#35328;&#35821;&#21512;&#25104;&#24212;&#29992;&#65292;&#24182;&#25552;&#20379;&#23545;&#35328;&#35821;&#30693;&#35273;&#35748;&#30693;&#36807;&#31243;&#30340;&#39069;&#22806;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2402.16996</link><description>&lt;p&gt;
&#25506;&#32034;&#34987;&#21160;&#21548;&#21462;&#35328;&#35821;&#26102;&#22823;&#33041;&#27963;&#21160;&#30340;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Towards Decoding Brain Activity During Passive Listening of Speech
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20174;&#39045;&#20869;&#33041;&#30005;&#22270;&#25968;&#25454;&#35299;&#30721;&#34987;&#21160;&#21548;&#21462;&#30340;&#35328;&#35821;&#65292;&#26088;&#22312;&#25512;&#21160;&#33041;&#26426;&#25509;&#21475;&#25216;&#26415;&#30340;&#35328;&#35821;&#21512;&#25104;&#24212;&#29992;&#65292;&#24182;&#25552;&#20379;&#23545;&#35328;&#35821;&#30693;&#35273;&#35748;&#30693;&#36807;&#31243;&#30340;&#39069;&#22806;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#25506;&#31350;&#35328;&#35821;&#30693;&#35273;&#30340;&#22797;&#26434;&#26426;&#21046;&#65292;&#24182;&#26368;&#32456;&#35299;&#30721;&#22312;&#21548;&#21462;&#35328;&#35821;&#26102;&#22823;&#33041;&#20013;&#20135;&#29983;&#30340;&#30005;&#20449;&#21495;&#21464;&#21270;&#12290;&#25105;&#20204;&#23581;&#35797;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20174;&#39045;&#20869;&#33041;&#30005;&#22270;&#65288;iEEG&#65289;&#25968;&#25454;&#20013;&#35299;&#30721;&#25152;&#21548;&#21462;&#30340;&#35328;&#35821;&#12290;&#26088;&#22312;&#25512;&#21160;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#25216;&#26415;&#22312;&#35328;&#35821;&#21512;&#25104;&#26041;&#38754;&#30340;&#21457;&#23637;&#65292;&#24182;&#24076;&#26395;&#20026;&#35328;&#35821;&#30693;&#35273;&#30340;&#35748;&#30693;&#36807;&#31243;&#25552;&#20379;&#39069;&#22806;&#35270;&#35282;&#12290;&#36825;&#31181;&#26041;&#27861;&#20559;&#31163;&#20102;&#20256;&#32479;&#23545;&#35328;&#35821;&#20135;&#29983;&#30340;&#20851;&#27880;&#65292;&#36716;&#32780;&#36873;&#25321;&#30740;&#31350;&#25152;&#24863;&#30693;&#30340;&#35328;&#35821;&#30340;&#31070;&#32463;&#34920;&#31034;&#12290;&#36825;&#19968;&#35270;&#35282;&#25171;&#24320;&#20102;&#22797;&#26434;&#30340;&#35270;&#37326;&#65292;&#26377;&#21487;&#33021;&#35753;&#25105;&#20204;&#30740;&#31350;&#26356;&#22797;&#26434;&#30340;&#31070;&#32463;&#27169;&#24335;&#12290;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#30740;&#31350;&#26088;&#22312;&#24314;&#31435;&#36825;&#20123;&#22797;&#26434;&#30340;&#31070;&#32463;&#27963;&#21160;&#19982;&#30456;&#24212;&#35328;&#35821;&#22768;&#38899;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861; diverges from the conventional focus on speech production and instead chooses to investigate neural representations of perceived speech. &#35813;&#30740;&#31350;new angle opened up a complex perspective, potentially allowing us to study more sophisticated neural patterns.
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16996v1 Announce Type: cross  Abstract: The aim of the study is to investigate the complex mechanisms of speech perception and ultimately decode the electrical changes in the brain accruing while listening to speech. We attempt to decode heard speech from intracranial electroencephalographic (iEEG) data using deep learning methods. The goal is to aid the advancement of brain-computer interface (BCI) technology for speech synthesis, and, hopefully, to provide an additional perspective on the cognitive processes of speech perception. This approach diverges from the conventional focus on speech production and instead chooses to investigate neural representations of perceived speech. This angle opened up a complex perspective, potentially allowing us to study more sophisticated neural patterns. Leveraging the power of deep learning models, the research aimed to establish a connection between these intricate neural activities and the corresponding speech sounds. Despite the appro
&lt;/p&gt;</description></item><item><title>GEM3D&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#12289;&#25299;&#25169;&#24863;&#30693;&#30340;&#19977;&#32500;&#24418;&#29366;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#31070;&#32463;&#39592;&#26550;&#32534;&#30721;&#20102;&#25299;&#25169;&#21644;&#20960;&#20309;&#20449;&#24687;&#65292;&#36890;&#36807;&#39592;&#26550;&#39537;&#21160;&#30340;&#31070;&#32463;&#38544;&#24335;&#20844;&#24335;&#29983;&#25104;&#20934;&#30830;&#21644;&#22810;&#26679;&#21270;&#30340;&#34920;&#38754;&#12290;</title><link>https://arxiv.org/abs/2402.16994</link><description>&lt;p&gt;
GEM3D&#65306;&#19977;&#32500;&#24418;&#29366;&#21512;&#25104;&#30340;&#29983;&#25104;&#23186;&#20307;&#25277;&#35937;
&lt;/p&gt;
&lt;p&gt;
GEM3D: GEnerative Medial Abstractions for 3D Shape Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16994
&lt;/p&gt;
&lt;p&gt;
GEM3D&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#12289;&#25299;&#25169;&#24863;&#30693;&#30340;&#19977;&#32500;&#24418;&#29366;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#31070;&#32463;&#39592;&#26550;&#32534;&#30721;&#20102;&#25299;&#25169;&#21644;&#20960;&#20309;&#20449;&#24687;&#65292;&#36890;&#36807;&#39592;&#26550;&#39537;&#21160;&#30340;&#31070;&#32463;&#38544;&#24335;&#20844;&#24335;&#29983;&#25104;&#20934;&#30830;&#21644;&#22810;&#26679;&#21270;&#30340;&#34920;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;GEM3D&#8212;&#8212;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#12289;&#25299;&#25169;&#24863;&#30693;&#30340;&#19977;&#32500;&#24418;&#29366;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#37096;&#20998;&#26159;&#22522;&#20110;&#31070;&#32463;&#39592;&#26550;&#30340;&#34920;&#31034;&#65292;&#32534;&#30721;&#20102;&#20851;&#20110;&#24418;&#29366;&#25299;&#25169;&#21644;&#20960;&#20309;&#30340;&#20449;&#24687;&#12290;&#36890;&#36807;&#19968;&#20010;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#29983;&#25104;&#36981;&#24490;&#20013;&#36724;&#21464;&#25442;&#65288;MAT&#65289;&#30340;&#22522;&#20110;&#39592;&#26550;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#36890;&#36807;&#19968;&#20010;&#39592;&#26550;&#39537;&#21160;&#30340;&#31070;&#32463;&#38544;&#24335;&#20844;&#24335;&#29983;&#25104;&#34920;&#38754;&#12290;&#31070;&#32463;&#38544;&#24335;&#32771;&#34385;&#20102;&#22312;&#29983;&#25104;&#30340;&#39592;&#26550;&#34920;&#31034;&#20013;&#23384;&#20648;&#30340;&#25299;&#25169;&#21644;&#20960;&#20309;&#20449;&#24687;&#65292;&#20135;&#29983;&#30340;&#34920;&#38754;&#19982;&#20043;&#21069;&#30340;&#31070;&#32463;&#22330;&#20844;&#24335;&#30456;&#27604;&#26356;&#21152;&#25299;&#25169;&#21644;&#20960;&#20309;&#20934;&#30830;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24418;&#29366;&#21512;&#25104;&#21644;&#28857;&#20113;&#37325;&#24314;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#23450;&#24615;&#21644;&#23450;&#37327;&#35780;&#20272;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27604;&#20197;&#21069;&#26356;&#20026;&#20934;&#30830;&#21644;&#22810;&#26679;&#21270;&#30340;&#24418;&#29366;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16994v1 Announce Type: cross  Abstract: We introduce GEM3D -- a new deep, topology-aware generative model of 3D shapes. The key ingredient of our method is a neural skeleton-based representation encoding information on both shape topology and geometry. Through a denoising diffusion probabilistic model, our method first generates skeleton-based representations following the Medial Axis Transform (MAT), then generates surfaces through a skeleton-driven neural implicit formulation. The neural implicit takes into account the topological and geometric information stored in the generated skeleton representations to yield surfaces that are more topologically and geometrically accurate compared to previous neural field formulations. We discuss applications of our method in shape synthesis and point cloud reconstruction tasks, and evaluate our method both qualitatively and quantitatively. We demonstrate significantly more faithful surface reconstruction and diverse shape generation r
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#27169;&#22411;&#22312;&#30740;&#31350;&#25968;&#25454;&#30340;&#20998;&#23618;&#29983;&#25104;&#27169;&#22411;&#20013;&#23637;&#31034;&#20986;&#20102;&#22312;&#38408;&#20540;&#26102;&#38388;&#21457;&#29983;&#30456;&#21464;&#30340;&#29305;&#24615;&#65292;&#36825;&#24433;&#21709;&#20102;&#39640;&#32423;&#29305;&#24449;&#21644;&#20302;&#32423;&#29305;&#24449;&#30340;&#37325;&#24314;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.16991</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#30456;&#21464;&#25581;&#31034;&#20102;&#25968;&#25454;&#30340;&#20998;&#23618;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
A Phase Transition in Diffusion Models Reveals the Hierarchical Nature of Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16991
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#30740;&#31350;&#25968;&#25454;&#30340;&#20998;&#23618;&#29983;&#25104;&#27169;&#22411;&#20013;&#23637;&#31034;&#20986;&#20102;&#22312;&#38408;&#20540;&#26102;&#38388;&#21457;&#29983;&#30456;&#21464;&#30340;&#29305;&#24615;&#65292;&#36825;&#24433;&#21709;&#20102;&#39640;&#32423;&#29305;&#24449;&#21644;&#20302;&#32423;&#29305;&#24449;&#30340;&#37325;&#24314;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#30495;&#23454;&#25968;&#25454;&#30340;&#32467;&#26500;&#22312;&#25512;&#21160;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#33258;&#28982;&#25968;&#25454;&#65292;&#22914;&#22270;&#20687;&#65292;&#34987;&#35748;&#20026;&#26159;&#30001;&#20197;&#23618;&#27425;&#21644;&#32452;&#21512;&#26041;&#24335;&#32452;&#32455;&#30340;&#29305;&#24449;&#32452;&#25104;&#30340;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#25429;&#25417;&#21040;&#36825;&#20123;&#29305;&#24449;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#26174;&#31034;&#65292;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#26263;&#31034;&#20102;&#23427;&#20204;&#25429;&#25417;&#21040;&#36825;&#31181;&#28508;&#22312;&#32467;&#26500;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25968;&#25454;&#30340;&#20998;&#23618;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#36825;&#19968;&#29616;&#35937;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#26102;&#38388;$t$&#21518;&#20316;&#29992;&#30340;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#21463;&#21040;&#26576;&#20010;&#38408;&#20540;&#26102;&#38388;&#22788;&#30340;&#30456;&#21464;&#25511;&#21046;&#65292;&#27492;&#26102;&#37325;&#24314;&#39640;&#32423;&#29305;&#24449;&#65288;&#22914;&#22270;&#20687;&#30340;&#31867;&#21035;&#65289;&#30340;&#27010;&#29575;&#31361;&#28982;&#19979;&#38477;&#12290;&#30456;&#21453;&#65292;&#20302;&#32423;&#29305;&#24449;&#65288;&#22914;&#22270;&#20687;&#30340;&#20855;&#20307;&#32454;&#33410;&#65289;&#30340;&#37325;&#24314;&#22312;&#25972;&#20010;&#25193;&#25955;&#36807;&#31243;&#20013;&#24179;&#31283;&#28436;&#21464;&#12290;&#36825;&#19968;&#32467;&#26524;&#26263;&#31034;&#65292;&#22312;&#36229;&#20986;&#36716;&#21464;&#26102;&#38388;&#30340;&#26102;&#21051;&#65292;&#31867;&#21035;&#24050;&#21464;&#21270;&#65292;&#20294;&#26159;&#22522;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16991v1 Announce Type: cross  Abstract: Understanding the structure of real data is paramount in advancing modern deep-learning methodologies. Natural data such as images are believed to be composed of features organised in a hierarchical and combinatorial manner, which neural networks capture during learning. Recent advancements show that diffusion models can generate high-quality images, hinting at their ability to capture this underlying structure. We study this phenomenon in a hierarchical generative model of data. We find that the backward diffusion process acting after a time $t$ is governed by a phase transition at some threshold time, where the probability of reconstructing high-level features, like the class of an image, suddenly drops. Instead, the reconstruction of low-level features, such as specific details of an image, evolves smoothly across the whole diffusion process. This result implies that at times beyond the transition, the class has changed but the gene
&lt;/p&gt;</description></item><item><title>inGRASS&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#26080;&#21521;&#22270;&#22686;&#37327;&#35889;&#31232;&#30095;&#21270;&#30340;&#26032;&#31639;&#27861;&#65292;&#20854;&#20855;&#26377;&#39640;&#24230;&#21487;&#25193;&#23637;&#24615;&#21644;&#24182;&#34892;&#21451;&#22909;&#24615;&#65292;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#20302;&#38459;&#25239;&#30452;&#24452;&#20998;&#35299;&#26041;&#26696;&#65292;&#33021;&#22815;&#39640;&#25928;&#35782;&#21035;&#20851;&#38190;&#36793;&#21644;&#26816;&#27979;&#22810;&#20313;&#36793;&#12290;</title><link>https://arxiv.org/abs/2402.16990</link><description>&lt;p&gt;
inGRASS: &#36890;&#36807;&#20302;&#38459;&#25239;&#30452;&#24452;&#20998;&#35299;&#23454;&#29616;&#22686;&#37327;&#22270;&#35889;&#31232;&#30095;&#21270;
&lt;/p&gt;
&lt;p&gt;
inGRASS: Incremental Graph Spectral Sparsification via Low-Resistance-Diameter Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16990
&lt;/p&gt;
&lt;p&gt;
inGRASS&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#26080;&#21521;&#22270;&#22686;&#37327;&#35889;&#31232;&#30095;&#21270;&#30340;&#26032;&#31639;&#27861;&#65292;&#20854;&#20855;&#26377;&#39640;&#24230;&#21487;&#25193;&#23637;&#24615;&#21644;&#24182;&#34892;&#21451;&#22909;&#24615;&#65292;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#20302;&#38459;&#25239;&#30452;&#24452;&#20998;&#35299;&#26041;&#26696;&#65292;&#33021;&#22815;&#39640;&#25928;&#35782;&#21035;&#20851;&#38190;&#36793;&#21644;&#26816;&#27979;&#22810;&#20313;&#36793;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;inGRASS&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#23545;&#22823;&#22411;&#26080;&#21521;&#22270;&#36827;&#34892;&#22686;&#37327;&#35889;&#31232;&#30095;&#21270;&#30340;&#26032;&#31639;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;inGRASS&#31639;&#27861;&#20855;&#26377;&#39640;&#24230;&#21487;&#25193;&#23637;&#24615;&#21644;&#24182;&#34892;&#21451;&#22909;&#24615;&#65292;&#35774;&#32622;&#38454;&#27573;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#20960;&#20046;&#26159;&#32447;&#24615;&#30340;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#23545;&#20855;&#26377;N&#20010;&#33410;&#28857;&#30340;&#21407;&#22987;&#22270;&#36827;&#34892;&#22686;&#37327;&#26356;&#25913;&#26102;&#65292;&#20197;$O(\log N)$&#30340;&#26102;&#38388;&#26356;&#26032;&#35889;&#31232;&#30095;&#22120;&#12290;&#22312;inGRASS&#30340;&#35774;&#32622;&#38454;&#27573;&#20013;&#65292;&#19968;&#20010;&#20851;&#38190;&#32452;&#20214;&#26159;&#24341;&#20837;&#20102;&#22810;&#32423;&#38459;&#25239;&#23884;&#20837;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#25928;&#35782;&#21035;&#35889;&#20851;&#38190;&#36793;&#24182;&#26377;&#25928;&#26816;&#27979;&#22810;&#20313;&#36793;&#65292;&#36825;&#26159;&#36890;&#36807;&#23558;&#21021;&#22987;&#31232;&#30095;&#22120;&#20998;&#35299;&#20026;&#35768;&#22810;&#33410;&#28857;&#32676;&#38598;&#24182;&#21033;&#29992;&#20302;&#38459;&#25239;&#30452;&#24452;&#20998;&#35299;&#65288;LRD&#65289;&#26041;&#26696;&#26469;&#23454;&#29616;&#30340;&#12290;inGRASS&#30340;&#26356;&#26032;&#38454;&#27573;&#21033;&#29992;&#20302;&#32500;&#33410;&#28857;&#23884;&#20837;&#21521;&#37327;&#65292;&#26377;&#25928;&#20272;&#35745;&#27599;&#20010;&#26032;&#28155;&#21152;&#36793;&#30340;&#37325;&#35201;&#24615;&#21644;&#21807;&#19968;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16990v1 Announce Type: cross  Abstract: This work presents inGRASS, a novel algorithm designed for incremental spectral sparsification of large undirected graphs. The proposed inGRASS algorithm is highly scalable and parallel-friendly, having a nearly-linear time complexity for the setup phase and the ability to update the spectral sparsifier in $O(\log N)$ time for each incremental change made to the original graph with $N$ nodes. A key component in the setup phase of inGRASS is a multilevel resistance embedding framework introduced for efficiently identifying spectrally-critical edges and effectively detecting redundant ones, which is achieved by decomposing the initial sparsifier into many node clusters with bounded effective-resistance diameters leveraging a low-resistance-diameter decomposition (LRD) scheme. The update phase of inGRASS exploits low-dimensional node embedding vectors for efficiently estimating the importance and uniqueness of each newly added edge. As de
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#22312;&#20869;&#23481;&#23457;&#26680;&#20013;&#24341;&#20837;&#20102;&#39044;&#27979;&#22810;&#26679;&#24615;&#30340;&#25361;&#25112;&#65292;&#21487;&#33021;&#23548;&#33268;&#20869;&#23481;&#34987;&#27494;&#26029;&#20998;&#31867;&#24182;&#38480;&#21046;&#35328;&#35770;&#33258;&#30001;&#12290;</title><link>https://arxiv.org/abs/2402.16979</link><description>&lt;p&gt;
&#20869;&#23481;&#23457;&#26680;&#20013;&#30340;&#31639;&#27861;&#27494;&#26029;&#24615;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Arbitrariness in Content Moderation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16979
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#20869;&#23481;&#23457;&#26680;&#20013;&#24341;&#20837;&#20102;&#39044;&#27979;&#22810;&#26679;&#24615;&#30340;&#25361;&#25112;&#65292;&#21487;&#33021;&#23548;&#33268;&#20869;&#23481;&#34987;&#27494;&#26029;&#20998;&#31867;&#24182;&#38480;&#21046;&#35328;&#35770;&#33258;&#30001;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22312;&#32447;&#20869;&#23481;&#23457;&#26680;&#12290;&#23613;&#31649;&#30456;&#23545;&#20110;&#20154;&#24037;&#23457;&#26680;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#20294;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#22312;&#20869;&#23481;&#23457;&#26680;&#20013;&#24341;&#20837;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#20854;&#20013;&#20043;&#19968;&#26159;&#39044;&#27979;&#30340;&#22810;&#26679;&#24615;&#65306;&#38024;&#23545;&#20869;&#23481;&#20998;&#31867;&#30340;&#22810;&#20010;&#31454;&#20105;&#27169;&#22411;&#21487;&#33021;&#22312;&#24179;&#22343;&#34920;&#29616;&#19978;&#21516;&#26679;&#20986;&#33394;&#65292;&#20294;&#23545;&#21516;&#19968;&#20869;&#23481;&#32473;&#20986;&#20102;&#20914;&#31361;&#30340;&#39044;&#27979;&#12290;&#36825;&#31181;&#22810;&#26679;&#24615;&#21487;&#33021;&#28304;&#33258;&#27169;&#22411;&#24320;&#21457;&#36807;&#31243;&#20013;&#30475;&#20284;&#26080;&#23475;&#30340;&#36873;&#25321;&#65292;&#27604;&#22914;&#21442;&#25968;&#21021;&#22987;&#21270;&#30340;&#38543;&#26426;&#31181;&#23376;&#36873;&#25321;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20869;&#23481;&#23457;&#26680;&#24037;&#20855;&#22914;&#20309;&#21487;&#20197;&#27494;&#26029;&#22320;&#23558;&#26679;&#26412;&#20998;&#31867;&#20026;&#26377;&#27602;&#30340;&#65292;&#23548;&#33268;&#35328;&#35770;&#21463;&#21040;&#27494;&#26029;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#20174;&#12298;&#22269;&#38469;&#20844;&#27665;&#21644;&#25919;&#27835;&#26435;&#21033;&#20844;&#32422;&#12299;&#25152;&#35268;&#23450;&#30340;&#20154;&#26435;&#35282;&#24230;&#65292;&#29305;&#21035;&#26159;&#35328;&#35770;&#33258;&#30001;&#12289;&#38750;&#27495;&#35270;&#21644;&#31243;&#24207;&#20844;&#27491;&#65292;&#35752;&#35770;&#20102;&#36825;&#20123;&#21457;&#29616;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#65288;i&#65289;&#26368;&#20808;&#36827;&#30340;&#23457;&#26680;&#24037;&#20855;&#20043;&#38388;&#39044;&#27979;&#22810;&#26679;&#24615;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16979v1 Announce Type: cross  Abstract: Machine learning (ML) is widely used to moderate online content. Despite its scalability relative to human moderation, the use of ML introduces unique challenges to content moderation. One such challenge is predictive multiplicity: multiple competing models for content classification may perform equally well on average, yet assign conflicting predictions to the same content. This multiplicity can result from seemingly innocuous choices during model development, such as random seed selection for parameter initialization. We experimentally demonstrate how content moderation tools can arbitrarily classify samples as toxic, leading to arbitrary restrictions on speech. We discuss these findings in terms of human rights set out by the International Covenant on Civil and Political Rights (ICCPR), namely freedom of expression, non-discrimination, and procedural justice. We analyze (i) the extent of predictive multiplicity among state-of-the-ar
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#31934;&#30830;&#30340;Bregman&#36817;&#31471;&#28857;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#19981;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#20445;&#25345;&#31639;&#27861;&#25910;&#25947;&#24615;&#21644;&#31283;&#23450;&#24615;&#30340;&#21516;&#26102;&#65292;&#23545;&#20110;&#22823;&#27491;&#21017;&#21270;&#21442;&#25968;&#21644;&#23567;&#27491;&#21017;&#21270;&#21442;&#25968;&#22343;&#26377;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.16978</link><description>&lt;p&gt;
&#19968;&#31181;&#19981;&#31934;&#30830;&#30340;Bregman&#36817;&#31471;&#28857;&#26041;&#27861;&#21450;&#20854;&#21152;&#36895;&#29256;&#26412;&#29992;&#20110;&#19981;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
An inexact Bregman proximal point method and its acceleration version for unbalanced optimal transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16978
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#31934;&#30830;&#30340;Bregman&#36817;&#31471;&#28857;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#19981;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#20445;&#25345;&#31639;&#27861;&#25910;&#25947;&#24615;&#21644;&#31283;&#23450;&#24615;&#30340;&#21516;&#26102;&#65292;&#23545;&#20110;&#22823;&#27491;&#21017;&#21270;&#21442;&#25968;&#21644;&#23567;&#27491;&#21017;&#21270;&#21442;&#25968;&#22343;&#26377;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#65288;UOT&#65289;&#38382;&#39064;&#22312;&#35745;&#31639;&#29983;&#29289;&#23398;&#12289;&#35745;&#31639;&#25104;&#20687;&#21644;&#28145;&#24230;&#23398;&#20064;&#20013;&#21457;&#25381;&#30528;&#26085;&#30410;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#30001;&#20110;&#20854;&#20415;&#21033;&#24615;&#21644;&#33391;&#22909;&#30340;&#25910;&#25947;&#29305;&#24615;&#65292;&#32553;&#25918;&#31639;&#27861;&#34987;&#24191;&#27867;&#29992;&#20110;&#35299;&#20915;UOT&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36739;&#22823;&#30340;&#27491;&#21017;&#21270;&#21442;&#25968;&#65292;&#36825;&#31181;&#31639;&#27861;&#30340;&#31934;&#24230;&#36739;&#20302;&#65292;&#32780;&#30001;&#20110;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#36739;&#23567;&#30340;&#27491;&#21017;&#21270;&#21442;&#25968;&#24456;&#23481;&#26131;&#23548;&#33268;&#25968;&#20540;&#28322;&#20986;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#19981;&#31934;&#30830;&#30340;Bregman&#36817;&#31471;&#28857;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#35813;&#31639;&#27861;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#20351;&#29992;&#32553;&#25918;&#31639;&#27861;&#26469;&#36817;&#20284;&#36817;&#31471;&#31639;&#23376;&#12290;&#35813;&#31639;&#27861;&#65288;1&#65289;&#25910;&#25947;&#20110;UOT&#30340;&#30495;&#23454;&#35299;&#65292;&#65288;2&#65289;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#21644;&#31283;&#20581;&#30340;&#27491;&#21017;&#21270;&#21442;&#25968;&#36873;&#25321;&#65292;&#65288;3&#65289;&#32531;&#35299;&#20102;&#25968;&#20540;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#65288;4&#65289;&#22312;&#20855;&#20307;&#23454;&#36341;&#20013;&#21487;&#20197;&#23454;&#29616;&#19982;&#32553;&#25918;&#31639;&#27861;&#30456;&#24403;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16978v1 Announce Type: cross  Abstract: The Unbalanced Optimal Transport (UOT) problem plays increasingly important roles in computational biology, computational imaging and deep learning. Scaling algorithm is widely used to solve UOT due to its convenience and good convergence properties. However, this algorithm has lower accuracy for large regularization parameters, and due to stability issues, small regularization parameters can easily lead to numerical overflow. We address this challenge by developing an inexact Bregman proximal point method for solving UOT. This algorithm approximates the proximal operator using the Scaling algorithm at each iteration. The algorithm (1) converges to the true solution of UOT, (2) has theoretical guarantees and robust regularization parameter selection, (3) mitigates numerical stability issues, and (4) can achieve comparable computational complexity to the Scaling algorithm in specific practice. Building upon this, we develop an accelerat
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#36890;&#36807;&#24067;&#23616;&#23398;&#20064;&#23454;&#29616;&#20102;&#23558;&#19977;&#32500;&#22330;&#26223;&#20998;&#35299;&#20026;&#21508;&#20010;&#21333;&#29420;&#23545;&#35937;&#65292;&#20174;&#32780;&#22312;&#25991;&#26412;&#21040;&#19977;&#32500;&#20869;&#23481;&#21019;&#24314;&#26041;&#38754;&#24102;&#26469;&#20102;&#26032;&#30340;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.16936</link><description>&lt;p&gt;
&#20855;&#26377;&#24067;&#23616;&#23398;&#20064;&#30340;&#21435;&#21367;&#31215;&#19977;&#32500;&#22330;&#26223;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Disentangled 3D Scene Generation with Layout Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16936
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#36890;&#36807;&#24067;&#23616;&#23398;&#20064;&#23454;&#29616;&#20102;&#23558;&#19977;&#32500;&#22330;&#26223;&#20998;&#35299;&#20026;&#21508;&#20010;&#21333;&#29420;&#23545;&#35937;&#65292;&#20174;&#32780;&#22312;&#25991;&#26412;&#21040;&#19977;&#32500;&#20869;&#23481;&#21019;&#24314;&#26041;&#38754;&#24102;&#26469;&#20102;&#26032;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#29983;&#25104;&#34987;&#20998;&#35299;&#25104;&#20854;&#32452;&#20214;&#23545;&#35937;&#30340;&#19977;&#32500;&#22330;&#26223;&#12290;&#36825;&#31181;&#20998;&#35299;&#26159;&#26080;&#30417;&#30563;&#30340;&#65292;&#20165;&#20381;&#36182;&#20110;&#19968;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#26159;&#65292;&#36890;&#36807;&#25214;&#21040;&#19968;&#20010;&#19977;&#32500;&#22330;&#26223;&#30340;&#37096;&#20998;&#65292;&#22312;&#31354;&#38388;&#37325;&#26032;&#24067;&#32622;&#26102;&#20173;&#28982;&#20135;&#29983;&#30456;&#21516;&#22330;&#26223;&#30340;&#26377;&#25928;&#37197;&#32622;&#65292;&#21487;&#20197;&#21457;&#29616;&#23545;&#35937;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#22836;&#24320;&#22987;&#32852;&#21512;&#20248;&#21270;&#22810;&#20010; NeRF &#27169;&#22411; - &#27599;&#20010;&#27169;&#22411;&#20195;&#34920;&#20854;&#33258;&#24049;&#30340;&#23545;&#35937; - &#20197;&#21450;&#23558;&#36825;&#20123;&#23545;&#35937;&#32452;&#25104;&#22330;&#26223;&#30340;&#19968;&#32452;&#24067;&#23616;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#40723;&#21169;&#36825;&#20123;&#21512;&#25104;&#30340;&#22330;&#26223;&#26681;&#25454;&#22270;&#20687;&#29983;&#25104;&#22120;&#20445;&#25345;&#22312;&#20998;&#24067;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#23613;&#31649;&#26041;&#27861;&#31616;&#21333;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#29983;&#25104;&#20102;&#20998;&#35299;&#20026;&#21333;&#29420;&#23545;&#35937;&#30340;&#19977;&#32500;&#22330;&#26223;&#65292;&#23454;&#29616;&#20102;&#25991;&#26412;&#21040;&#19977;&#32500;&#20869;&#23481;&#21019;&#24314;&#20013;&#30340;&#26032;&#21151;&#33021;&#12290;&#26377;&#20851;&#32467;&#26524;&#21644;&#20132;&#20114;&#24335;&#28436;&#31034;&#65292;&#35831;&#26597;&#30475;&#25105;&#20204;&#30340;&#39033;&#30446;&#39029;&#38754; https://dave.ml/layoutlearning/
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16936v1 Announce Type: cross  Abstract: We introduce a method to generate 3D scenes that are disentangled into their component objects. This disentanglement is unsupervised, relying only on the knowledge of a large pretrained text-to-image model. Our key insight is that objects can be discovered by finding parts of a 3D scene that, when rearranged spatially, still produce valid configurations of the same scene. Concretely, our method jointly optimizes multiple NeRFs from scratch - each representing its own object - along with a set of layouts that composite these objects into scenes. We then encourage these composited scenes to be in-distribution according to the image generator. We show that despite its simplicity, our approach successfully generates 3D scenes decomposed into individual objects, enabling new capabilities in text-to-3D content creation. For results and an interactive demo, see our project page at https://dave.ml/layoutlearning/
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;FedReview&#26426;&#21046;&#65292;&#36890;&#36807;&#38543;&#26426;&#20998;&#37197;&#35780;&#23457;&#21592;&#23458;&#25143;&#31471;&#26469;&#35782;&#21035;&#21644;&#25298;&#32477;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#28508;&#22312;&#27602;&#21270;&#26356;&#26032;&#65292;&#24182;&#37319;&#29992;&#22810;&#25968;&#34920;&#20915;&#26426;&#21046;&#26469;&#25972;&#21512;&#25490;&#21517;&#24182;&#31227;&#38500;&#36825;&#20123;&#26356;&#26032;&#12290;</title><link>https://arxiv.org/abs/2402.16934</link><description>&lt;p&gt;
FedReview: &#19968;&#31181;&#29992;&#20110;&#25298;&#32477;&#27602;&#21270;&#26356;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#23457;&#26597;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
FedReview: A Review Mechanism for Rejecting Poisoned Updates in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16934
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;FedReview&#26426;&#21046;&#65292;&#36890;&#36807;&#38543;&#26426;&#20998;&#37197;&#35780;&#23457;&#21592;&#23458;&#25143;&#31471;&#26469;&#35782;&#21035;&#21644;&#25298;&#32477;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#28508;&#22312;&#27602;&#21270;&#26356;&#26032;&#65292;&#24182;&#37319;&#29992;&#22810;&#25968;&#34920;&#20915;&#26426;&#21046;&#26469;&#25972;&#21512;&#25490;&#21517;&#24182;&#31227;&#38500;&#36825;&#20123;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Federated learning&#26368;&#36817;&#24050;&#32463;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#26041;&#27861;&#65292;&#22312;&#19981;&#35775;&#38382;&#29992;&#25143;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#19968;&#20010;&#39640;&#24615;&#33021;&#27169;&#22411;&#12290;&#23613;&#31649;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294;&#32852;&#37030;&#23398;&#20064;&#32473;&#24694;&#24847;&#29992;&#25143;&#25552;&#20379;&#20102;&#26426;&#20250;&#36890;&#36807;&#21521;&#26381;&#21153;&#22120;&#19978;&#20256;&#27602;&#21270;&#27169;&#22411;&#26356;&#26032;&#26469;&#25805;&#32437;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedReview&#30340;&#23457;&#26597;&#26426;&#21046;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#25298;&#32477;&#32852;&#37030;&#23398;&#20064;&#20013;&#28508;&#22312;&#30340;&#27602;&#21270;&#26356;&#26032;&#12290;&#22312;&#25105;&#20204;&#30340;&#26426;&#21046;&#19979;&#65292;&#26381;&#21153;&#22120;&#27599;&#36718;&#38543;&#26426;&#20998;&#37197;&#23376;&#38598;&#23458;&#25143;&#31471;&#20316;&#20026;&#35780;&#23457;&#21592;&#65292;&#22312;&#20854;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#27169;&#22411;&#26356;&#26032;&#12290;&#35780;&#23457;&#21592;&#26681;&#25454;&#35780;&#20215;&#32467;&#26524;&#23545;&#27169;&#22411;&#26356;&#26032;&#36827;&#34892;&#25490;&#21517;&#65292;&#32479;&#35745;&#30456;&#23545;&#20302;&#36136;&#37327;&#30340;&#26356;&#26032;&#25968;&#37327;&#20316;&#20026;&#20272;&#35745;&#30340;&#27602;&#21270;&#26356;&#26032;&#25968;&#37327;&#12290;&#22522;&#20110;&#23457;&#26597;&#25253;&#21578;&#65292;&#26381;&#21153;&#22120;&#37319;&#29992;&#22810;&#25968;&#34920;&#20915;&#26426;&#21046;&#25972;&#21512;&#25490;&#21517;&#24182;&#22312;&#27169;&#22411;&#32858;&#21512;&#36807;&#31243;&#20013;&#21435;&#38500;&#28508;&#22312;&#30340;&#27602;&#21270;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16934v1 Announce Type: cross  Abstract: Federated learning has recently emerged as a decentralized approach to learn a high-performance model without access to user data. Despite its effectiveness, federated learning gives malicious users opportunities to manipulate the model by uploading poisoned model updates to the server. In this paper, we propose a review mechanism called FedReview to identify and decline the potential poisoned updates in federated learning. Under our mechanism, the server randomly assigns a subset of clients as reviewers to evaluate the model updates on their training datasets in each round. The reviewers rank the model updates based on the evaluation results and count the number of the updates with relatively low quality as the estimated number of poisoned updates. Based on review reports, the server employs a majority voting mechanism to integrate the rankings and remove the potential poisoned updates in the model aggregation process. Extensive evalu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Cobweb4V&#30340;&#26032;&#39062;&#35270;&#35273;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#31867;&#31867;&#20284;&#23398;&#20064;&#31995;&#32479;&#65292;&#36991;&#20813;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#25928;&#24212;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#26469;&#23454;&#29616;&#26377;&#25928;&#23398;&#20064;&#25104;&#26524;&#65292;&#24182;&#20445;&#25345;&#31283;&#23450;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.16933</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#31867;&#27010;&#24565;&#24418;&#25104;&#36991;&#20813;&#35270;&#35273;&#20998;&#31867;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Avoiding Catastrophic Forgetting in Visual Classification Using Human Concept Formation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16933
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Cobweb4V&#30340;&#26032;&#39062;&#35270;&#35273;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#31867;&#31867;&#20284;&#23398;&#20064;&#31995;&#32479;&#65292;&#36991;&#20813;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#25928;&#24212;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#26469;&#23454;&#29616;&#26377;&#25928;&#23398;&#20064;&#25104;&#26524;&#65292;&#24182;&#20445;&#25345;&#31283;&#23450;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#65292;&#28982;&#32780;&#65292;&#24403;&#25353;&#39034;&#24207;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#65292;&#23427;&#20204;&#32463;&#24120;&#38754;&#20020;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Cobweb4V&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#35273;&#20998;&#31867;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;Cobweb&#65292;&#36825;&#26159;&#19968;&#31181;&#20154;&#31867;&#31867;&#20284;&#30340;&#23398;&#20064;&#31995;&#32479;&#65292;&#21463;&#21040;&#20154;&#31867;&#38543;&#26102;&#38388;&#36880;&#28176;&#23398;&#20064;&#26032;&#27010;&#24565;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;Cobweb4V&#22312;&#23398;&#20064;&#35270;&#35273;&#27010;&#24565;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;&#23398;&#20064;&#25104;&#26524;&#65292;&#38543;&#26102;&#38388;&#20445;&#25345;&#31283;&#23450;&#30340;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#20102;&#20196;&#20154;&#31216;&#36190;&#30340;&#28176;&#36817;&#34892;&#20026;&#65292;&#36991;&#20813;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#25928;&#24212;&#12290;&#36825;&#20123;&#29305;&#24449;&#19982;&#20154;&#31867;&#35748;&#30693;&#20013;&#30340;&#23398;&#20064;&#31574;&#30053;&#19968;&#33268;&#65292;&#23558;Cobweb4V&#23450;&#20301;&#20026;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16933v1 Announce Type: cross  Abstract: Deep neural networks have excelled in machine learning, particularly in vision tasks, however, they often suffer from catastrophic forgetting when learning new tasks sequentially. In this work, we propose Cobweb4V, a novel visual classification approach that builds on Cobweb, a human like learning system that is inspired by the way humans incrementally learn new concepts over time. In this research, we conduct a comprehensive evaluation, showcasing the proficiency of Cobweb4V in learning visual concepts, requiring less data to achieve effective learning outcomes compared to traditional methods, maintaining stable performance over time, and achieving commendable asymptotic behavior, without catastrophic forgetting effects. These characteristics align with learning strategies in human cognition, positioning Cobweb4V as a promising alternative to neural network approaches.
&lt;/p&gt;</description></item><item><title>TrustMol&#25552;&#20986;&#20102;&#19968;&#31181;&#20540;&#24471;&#20449;&#36182;&#30340;&#36870;&#21521;&#20998;&#23376;&#35774;&#35745;&#26041;&#27861;&#65292;&#21033;&#29992;&#26032;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#32593;&#32476;&#21644;&#28508;&#22312;&#23646;&#24615;&#23545;&#33719;&#21462;&#26041;&#27861;&#26469;&#26377;&#25928;&#23548;&#33322;&#20998;&#23376;&#30340;&#28508;&#22312;&#20248;&#21270;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16930</link><description>&lt;p&gt;
TrustMol: &#36890;&#36807;&#19982;&#20998;&#23376;&#21160;&#21147;&#23398;&#23545;&#40784;&#23454;&#29616;&#20540;&#24471;&#20449;&#36182;&#30340;&#36870;&#21521;&#20998;&#23376;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
TrustMol: Trustworthy Inverse Molecular Design via Alignment with Molecular Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16930
&lt;/p&gt;
&lt;p&gt;
TrustMol&#25552;&#20986;&#20102;&#19968;&#31181;&#20540;&#24471;&#20449;&#36182;&#30340;&#36870;&#21521;&#20998;&#23376;&#35774;&#35745;&#26041;&#27861;&#65292;&#21033;&#29992;&#26032;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#32593;&#32476;&#21644;&#28508;&#22312;&#23646;&#24615;&#23545;&#33719;&#21462;&#26041;&#27861;&#26469;&#26377;&#25928;&#23548;&#33322;&#20998;&#23376;&#30340;&#28508;&#22312;&#20248;&#21270;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36817;&#24180;&#26469;&#23545;&#20855;&#26377;&#26399;&#26395;&#23646;&#24615;&#30340;&#20998;&#23376;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#29983;&#25104;&#65288;&#20063;&#34987;&#31216;&#20026;&#36870;&#21521;&#20998;&#23376;&#35774;&#35745;IMD&#65289;&#30340;&#26174;&#33879;&#20851;&#27880;&#65292;&#22312;&#31934;&#30830;&#24230;&#21644;&#35299;&#20915;&#26041;&#26696;&#22810;&#26679;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;IMD&#26041;&#27861;&#22312;&#35299;&#20915;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22312;&#21487;&#38752;&#24615;&#26041;&#38754;&#20173;&#28982;&#28382;&#21518;&#12290;&#36825;&#20123;&#26041;&#27861;&#35774;&#35745;&#36807;&#31243;&#30340;&#26681;&#26412;&#38382;&#39064;&#26159;&#36234;&#26469;&#36234;&#38544;&#21547;&#21644;&#38388;&#25509;&#65292;&#32780;&#19988;&#36825;&#19968;&#36807;&#31243;&#20063;&#19982;&#26412;&#26426;&#30340;&#21069;&#21521;&#36807;&#31243;&#65288;NFP&#65289;&#38548;&#31163;&#65292;&#21518;&#32773;&#26159;&#27169;&#25311;&#20998;&#23376;&#21160;&#21147;&#23398;&#30340;&#22320;&#38754;&#30495;&#23454;&#20989;&#25968;&#12290;&#22522;&#20110;&#36825;&#19968;&#24605;&#36335;&#65292;&#25105;&#20204;&#25552;&#20986;TrustMol&#65292;&#36825;&#26159;&#19968;&#31181;&#20540;&#24471;&#20449;&#36182;&#30340;IMD&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;TrustMol&#20381;&#36182;&#20110;&#19968;&#32452;&#25216;&#26415;&#21019;&#26032;&#65292;&#21253;&#25324;&#19968;&#20010;&#26032;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28508;&#22312;&#23646;&#24615;&#23545;&#33719;&#21462;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#22320;&#23548;&#33322;&#20998;&#23376;&#28508;&#22312;&#20248;&#21270;&#30340;&#22797;&#26434;&#24615;&#65292;&#36825;&#20010;&#36807;&#31243;&#30475;&#20284;&#30452;&#35266;&#20294;&#30001;&#20110;&#39640;&#39057;&#29575;.
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16930v1 Announce Type: cross  Abstract: Data-driven generation of molecules with desired properties, also known as inverse molecular design (IMD), has attracted significant attention in recent years. Despite the significant progress in the accuracy and diversity of solutions, existing IMD methods lag behind in terms of trustworthiness. The root issue is that the design process of these methods is increasingly more implicit and indirect, and this process is also isolated from the native forward process (NFP), the ground-truth function that models the molecular dynamics. Following this insight, we propose TrustMol, an IMD method built to be trustworthy. For this purpose, TrustMol relies on a set of technical novelties including a new variational autoencoder network. Moreover, we propose a latent-property pairs acquisition method to effectively navigate the complexities of molecular latent optimization, a process that seems intuitive yet challenging due to the high-frequency an
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#21518;&#38376;&#26816;&#27979;&#38382;&#39064;&#30340;&#27491;&#24335;&#32479;&#35745;&#23450;&#20041;&#65292;&#24182;&#24471;&#20986;&#20102;&#21518;&#38376;&#26816;&#27979;&#30340;&#19981;&#21487;&#33021;&#24615;&#19982;&#21487;&#23454;&#29616;&#24615;&#32467;&#26524;&#65292;&#25351;&#20986;&#20102;&#36890;&#29992;&#21518;&#38376;&#26816;&#27979;&#30340;&#23616;&#38480;&#24615;&#65292;&#24378;&#35843;&#21518;&#38376;&#26816;&#27979;&#26041;&#27861;&#38656;&#35201;&#32771;&#34385;&#25932;&#23545;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2402.16926</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21518;&#38376;&#26816;&#27979;&#30340;&#21487;&#34892;&#24615;&#38382;&#39064;&#20316;&#20026;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#30340;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
On the (In)feasibility of ML Backdoor Detection as an Hypothesis Testing Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16926
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#21518;&#38376;&#26816;&#27979;&#38382;&#39064;&#30340;&#27491;&#24335;&#32479;&#35745;&#23450;&#20041;&#65292;&#24182;&#24471;&#20986;&#20102;&#21518;&#38376;&#26816;&#27979;&#30340;&#19981;&#21487;&#33021;&#24615;&#19982;&#21487;&#23454;&#29616;&#24615;&#32467;&#26524;&#65292;&#25351;&#20986;&#20102;&#36890;&#29992;&#21518;&#38376;&#26816;&#27979;&#30340;&#23616;&#38480;&#24615;&#65292;&#24378;&#35843;&#21518;&#38376;&#26816;&#27979;&#26041;&#27861;&#38656;&#35201;&#32771;&#34385;&#25932;&#23545;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#24335;&#30340;&#32479;&#35745;&#23398;&#23450;&#20041;&#65292;&#29992;&#20110;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#21518;&#38376;&#26816;&#27979;&#38382;&#39064;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#20998;&#26512;&#36825;&#20123;&#38382;&#39064;&#30340;&#21487;&#34892;&#24615;&#65292;&#25552;&#20379;&#20102;&#23545;&#25105;&#20204;&#23450;&#20041;&#30340;&#23454;&#29992;&#24615;&#21644;&#36866;&#29992;&#24615;&#30340;&#35777;&#25454;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#23545;&#21518;&#38376;&#26816;&#27979;&#30340;&#19981;&#21487;&#33021;&#24615;&#32467;&#26524;&#21644;&#21487;&#23454;&#29616;&#24615;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#27809;&#26377;&#20813;&#36153;&#21320;&#39184;&#23450;&#29702;&#65292;&#35777;&#26126;&#20102;&#36890;&#29992;&#65288;&#23545;&#25932;&#26041;&#19981;&#30693;&#24773;&#65289;&#21518;&#38376;&#26816;&#27979;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#38500;&#38750;Alphabet&#22823;&#23567;&#38750;&#24120;&#23567;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026;&#65292;&#21518;&#38376;&#26816;&#27979;&#26041;&#27861;&#38656;&#35201;&#26126;&#30830;&#22320;&#25110;&#38544;&#24335;&#22320;&#32771;&#34385;&#25932;&#23545;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#24182;&#19981;&#24847;&#21619;&#30528;&#21518;&#38376;&#26816;&#27979;&#22312;&#29305;&#23450;&#22330;&#26223;&#19979;&#19981;&#33021;&#36215;&#20316;&#29992;&#65292;&#22240;&#20026;&#31185;&#23398;&#25991;&#29486;&#20013;&#25104;&#21151;&#30340;&#21518;&#38376;&#26816;&#27979;&#26041;&#27861;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#23450;&#20041;&#19982;&#22823;&#27010;&#36817;&#20284;&#27491;&#30830;&#65288;PAC&#65289;&#23398;&#20064;&#19982;&#22806;&#20998;&#24067;&#26816;&#27979;&#38382;&#39064;&#32852;&#31995;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16926v1 Announce Type: cross  Abstract: We introduce a formal statistical definition for the problem of backdoor detection in machine learning systems and use it to analyze the feasibility of such problems, providing evidence for the utility and applicability of our definition. The main contributions of this work are an impossibility result and an achievability result for backdoor detection. We show a no-free-lunch theorem, proving that universal (adversary-unaware) backdoor detection is impossible, except for very small alphabet sizes. Thus, we argue, that backdoor detection methods need to be either explicitly, or implicitly adversary-aware. However, our work does not imply that backdoor detection cannot work in specific scenarios, as evidenced by successful backdoor detection methods in the scientific literature. Furthermore, we connect our definition to the probably approximately correct (PAC) learnability of the out-of-distribution detection problem.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#24102;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#22270;&#30528;&#33394;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#24378;&#32467;&#26500;&#21487;&#25511;&#24615;&#26465;&#20214;&#19979;&#26368;&#23567;&#21270;&#25511;&#21046;&#36755;&#20837;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.16925</link><description>&lt;p&gt;
&#20351;&#29992;&#24102;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#32467;&#26500;&#21487;&#25511;&#24615;&#24378;&#21270;&#23398;&#20064;&#26368;&#23567;&#21270;&#25511;&#21046;&#36755;&#20837;
&lt;/p&gt;
&lt;p&gt;
Minimize Control Inputs for Strong Structural Controllability Using Reinforcement Learning with Graph Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16925
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#24102;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#22270;&#30528;&#33394;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#24378;&#32467;&#26500;&#21487;&#25511;&#24615;&#26465;&#20214;&#19979;&#26368;&#23567;&#21270;&#25511;&#21046;&#36755;&#20837;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#32467;&#26500;&#21487;&#25511;&#24615;(SSC)&#20445;&#35777;&#20855;&#26377;&#32447;&#24615;&#19981;&#21464;&#21160;&#21147;&#23398;&#30340;&#32593;&#32476;&#31995;&#32479;&#23545;&#20110;&#25152;&#26377;&#21442;&#25968;&#30340;&#25968;&#20540;&#23454;&#29616;&#37117;&#26159;&#21487;&#25511;&#30340;&#12290;&#24403;&#21069;&#30740;&#31350;&#24050;&#32463;&#20026;&#38646;/&#38750;&#38646;&#25110;&#38646;/&#38750;&#38646;/&#20219;&#24847;&#32467;&#26500;&#30340;SSC&#24314;&#31435;&#20102;&#20195;&#25968;&#21644;&#22270;&#35770;&#26465;&#20214;&#12290;&#19968;&#20010;&#30456;&#20851;&#30340;&#23454;&#38469;&#38382;&#39064;&#26159;&#22914;&#20309;&#29992;&#26368;&#23569;&#30340;&#36755;&#20837;&#20449;&#21495;&#23436;&#20840;&#25511;&#21046;&#31995;&#32479;&#65292;&#24182;&#30830;&#23450;&#24517;&#39035;&#26045;&#21152;&#20449;&#21495;&#30340;&#33410;&#28857;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20010;&#20248;&#21270;&#38382;&#39064;&#26159;NP&#38590;&#30340;&#65292;&#24456;&#38590;&#25214;&#21040;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26681;&#25454;&#38646;/&#38750;&#38646;&#21644;&#38646;/&#38750;&#38646;/&#20219;&#24847;&#32467;&#26500;&#30340;SSC&#30340;&#22270;&#35770;&#26465;&#20214;&#65292;&#23558;&#22270;&#30528;&#33394;&#36807;&#31243;&#26500;&#24314;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#12290;&#25105;&#20204;&#20351;&#29992;&#24102;&#26377;&#26377;&#21521;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;Actor-critic&#26041;&#27861;&#26469;&#34920;&#31034;&#22270;&#30340;&#30528;&#33394;&#20449;&#24687;&#20197;&#20248;&#21270;MDP&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#20010;&#31038;&#20132;&#24433;&#21709;&#32593;&#32476;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16925v1 Announce Type: cross  Abstract: Strong structural controllability (SSC) guarantees networked system with linear-invariant dynamics controllable for all numerical realizations of parameters. Current research has established algebraic and graph-theoretic conditions of SSC for zero/nonzero or zero/nonzero/arbitrary structure. One relevant practical problem is how to fully control the system with the minimal number of input signals and identify which nodes must be imposed signals. Previous work shows that this optimization problem is NP-hard and it is difficult to find the solution. To solve this problem, we formulate the graph coloring process as a Markov decision process (MDP) according to the graph-theoretical condition of SSC for both zero/nonzero and zero/nonzero/arbitrary structure. We use Actor-critic method with Directed graph neural network which represents the color information of graph to optimize MDP. Our method is validated in a social influence network with
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20307;&#31995;&#32467;&#26500;&#25628;&#32034;&#30340;&#26032;&#22411;&#20010;&#24615;&#21270;&#32852;&#37030;&#25351;&#20196;&#35843;&#20248;&#65288;PerFIT&#65289;&#26694;&#26550;&#65292;&#20801;&#35768;&#27599;&#20010;&#23458;&#25143;&#31471;&#36890;&#36807;&#25193;&#23637;&#20840;&#23616;&#27169;&#22411;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#31354;&#38388;&#26469;&#25628;&#32034;&#20010;&#24615;&#21270;&#20307;&#31995;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;FIT&#38754;&#20020;&#30340;&#25968;&#25454;&#21644;&#36164;&#28304;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.16919</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Personalized Federated Instruction Tuning via Neural Architecture Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16919
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20307;&#31995;&#32467;&#26500;&#25628;&#32034;&#30340;&#26032;&#22411;&#20010;&#24615;&#21270;&#32852;&#37030;&#25351;&#20196;&#35843;&#20248;&#65288;PerFIT&#65289;&#26694;&#26550;&#65292;&#20801;&#35768;&#27599;&#20010;&#23458;&#25143;&#31471;&#36890;&#36807;&#25193;&#23637;&#20840;&#23616;&#27169;&#22411;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#31354;&#38388;&#26469;&#25628;&#32034;&#20010;&#24615;&#21270;&#20307;&#31995;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;FIT&#38754;&#20020;&#30340;&#25968;&#25454;&#21644;&#36164;&#28304;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Federated Instruction Tuning (FIT)&#24050;&#32463;&#34920;&#26126;&#22312;&#19981;&#20849;&#20139;&#31169;&#20154;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#23454;&#29616;&#24222;&#22823;&#25968;&#25454;&#25152;&#26377;&#32773;&#20043;&#38388;&#30340;&#21327;&#20316;&#27169;&#22411;&#25351;&#20196;&#35843;&#20248;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20173;&#28982;&#38754;&#20020;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#21363;&#25968;&#25454;&#21644;&#36164;&#28304;&#30340;&#24322;&#36136;&#24615;&#12290;&#30001;&#20110;&#25968;&#25454;&#25152;&#26377;&#32773;&#20043;&#38388;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#20559;&#22909;&#21508;&#19981;&#30456;&#21516;&#65292;FIT&#26080;&#27861;&#36866;&#24212;&#20010;&#20154;&#25152;&#26377;&#32773;&#30340;&#20010;&#24615;&#21270;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#20855;&#26377;&#20248;&#36234;&#35745;&#31639;&#33021;&#21147;&#30340;&#23458;&#25143;&#31471;&#21463;&#21040;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#20445;&#25345;&#19982;&#36739;&#24369;&#23458;&#25143;&#31471;&#30456;&#21516;&#30340;&#24494;&#35843;&#26550;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20307;&#31995;&#32467;&#26500;&#25628;&#32034;&#30340;&#26032;&#22411;&#20010;&#24615;&#21270;&#32852;&#37030;&#25351;&#20196;&#35843;&#20248;&#65288;PerFIT&#65289;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;PerFIT&#20801;&#35768;&#27599;&#20010;&#23458;&#25143;&#31471;&#36890;&#36807;&#25193;&#23637;&#20840;&#23616;&#27169;&#22411;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#31354;&#38388;&#26469;&#25628;&#32034;&#20010;&#24615;&#21270;&#20307;&#31995;&#32467;&#26500;&#65292;&#28982;&#21518;&#23558;&#21442;&#25968;&#20462;&#21098;&#21040;&#21407;&#22987;&#29366;&#24577;&#12290;&#35813;&#36807;&#31243;&#20801;&#35768;&#22312;&#25193;&#23637;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20010;&#24615;&#21270;&#25351;&#20196;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16919v1 Announce Type: new  Abstract: Federated Instruction Tuning (FIT) has shown the ability to achieve collaborative model instruction tuning among massive data owners without sharing private data. However, it still faces two key challenges, i.e., data and resource heterogeneity. Due to the varying data distribution and preferences among data owners, FIT cannot adapt to the personalized data of individual owners. Moreover, clients with superior computational abilities are constrained since they need to maintain the same fine-tuning architecture as the weaker clients. To address these issues, we propose a novel Personalized Federated Instruction Tuning (PerFIT) framework based on architecture search. Specifically, PerFIT allows each client to search for a personalized architecture by expanding the trainable parameter space of the global model followed by pruning the parameters to the original state. This procedure allows personalized instruction fine-tuning within expanded
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#20110;&#22312;&#27169;&#22359;&#20043;&#38388;&#20256;&#36882;&#30693;&#35782;&#30340;&#36890;&#29992;&#27169;&#22359;&#21040;&#27169;&#22359;&#30693;&#35782;&#33976;&#39311;&#65288;m2mKD&#65289;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#27169;&#22359;&#21270;Transformer&#35757;&#32451;&#20013;&#30340;&#20248;&#21270;&#22256;&#38590;&#21644;&#21442;&#25968;&#25968;&#37327;&#24222;&#22823;&#31561;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.16918</link><description>&lt;p&gt;
m2mKD&#65306;&#27169;&#22359;&#38388;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;&#27169;&#22359;&#21270;Transformer
&lt;/p&gt;
&lt;p&gt;
m2mKD: Module-to-Module Knowledge Distillation for Modular Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16918
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#20110;&#22312;&#27169;&#22359;&#20043;&#38388;&#20256;&#36882;&#30693;&#35782;&#30340;&#36890;&#29992;&#27169;&#22359;&#21040;&#27169;&#22359;&#30693;&#35782;&#33976;&#39311;&#65288;m2mKD&#65289;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#27169;&#22359;&#21270;Transformer&#35757;&#32451;&#20013;&#30340;&#20248;&#21270;&#22256;&#38590;&#21644;&#21442;&#25968;&#25968;&#37327;&#24222;&#22823;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22359;&#21270;&#31070;&#32463;&#32467;&#26500;&#22240;&#20854;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#23545;&#26032;&#39046;&#22495;&#30340;&#39640;&#25928;&#36866;&#24212;&#33021;&#21147;&#32780;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#27169;&#22359;&#21270;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22312;&#26089;&#26399;&#38454;&#27573;&#65292;&#30001;&#20110;&#22266;&#26377;&#30340;&#31232;&#30095;&#36830;&#25509;&#23548;&#33268;&#30340;&#20248;&#21270;&#22256;&#38590;&#65292;&#23384;&#22312;&#25361;&#25112;&#12290;&#21033;&#29992;&#26469;&#33258;&#25972;&#20307;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#22914;&#30693;&#35782;&#33976;&#39311;&#31561;&#25216;&#26415;&#65292;&#21487;&#33021;&#26377;&#21161;&#20110;&#35757;&#32451;&#27169;&#22359;&#21270;&#27169;&#22411;&#65292;&#24182;&#20351;&#23427;&#20204;&#33021;&#22815;&#25972;&#21512;&#26469;&#33258;&#22312;&#22810;&#20010;&#26469;&#28304;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#24182;&#19981;&#38024;&#23545;&#27169;&#22359;&#21270;&#27169;&#22411;&#35774;&#35745;&#65292;&#30452;&#25509;&#24212;&#29992;&#26102;&#21487;&#33021;&#22833;&#36133;&#65292;&#36825;&#26159;&#30001;&#20110;&#29420;&#29305;&#30340;&#26550;&#26500;&#21644;&#22823;&#37327;&#28041;&#21450;&#30340;&#21442;&#25968;&#12290;&#21463;&#21040;&#36825;&#20123;&#25361;&#25112;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#27169;&#22359;&#20043;&#38388;&#20256;&#36882;&#30693;&#35782;&#30340;&#36890;&#29992;&#27169;&#22359;&#21040;&#27169;&#22359;&#30693;&#35782;&#33976;&#39311;&#65288;m2mKD&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16918v1 Announce Type: new  Abstract: Modular neural architectures are gaining increasing attention due to their powerful capability for generalization and sample-efficient adaptation to new domains. However, training modular models, particularly in the early stages, poses challenges due to the optimization difficulties arising from their intrinsic sparse connectivity. Leveraging the knowledge from monolithic models, using techniques such as knowledge distillation, is likely to facilitate the training of modular models and enable them to integrate knowledge from multiple models pretrained on diverse sources. Nevertheless, conventional knowledge distillation approaches are not tailored to modular models and can fail when directly applied due to the unique architectures and the enormous number of parameters involved. Motivated by these challenges, we propose a general module-to-module knowledge distillation (m2mKD) method for transferring knowledge between modules. Our approac
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#21512;GPS&#21644;&#36335;&#30001;&#24314;&#27169;&#30340;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#25216;&#26415;&#23454;&#29616;&#65292;&#21033;&#29992;&#20004;&#20010;&#32534;&#30721;&#22120;&#20998;&#21035;&#25429;&#33719;&#36335;&#30001;&#21644;GPS&#36712;&#36857;&#30340;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#20849;&#20139;&#30340;&#21464;&#21387;&#22120;&#36827;&#34892;&#27169;&#24577;&#38388;&#20449;&#24687;&#20132;&#20114;&#12290;</title><link>https://arxiv.org/abs/2402.16915</link><description>&lt;p&gt;
&#36229;&#36234;&#36335;&#30001;&#65306;&#32852;&#21512;GPS&#21644;&#36335;&#30001;&#24314;&#27169;&#20197;&#20248;&#21270;&#36712;&#36857;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
More Than Routing: Joint GPS and Route Modeling for Refine Trajectory Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16915
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#21512;GPS&#21644;&#36335;&#30001;&#24314;&#27169;&#30340;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#25216;&#26415;&#23454;&#29616;&#65292;&#21033;&#29992;&#20004;&#20010;&#32534;&#30721;&#22120;&#20998;&#21035;&#25429;&#33719;&#36335;&#30001;&#21644;GPS&#36712;&#36857;&#30340;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#20849;&#20139;&#30340;&#21464;&#21387;&#22120;&#36827;&#34892;&#27169;&#24577;&#38388;&#20449;&#24687;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36712;&#36857;&#34920;&#31034;&#23398;&#20064;&#22312;&#25903;&#25345;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20256;&#32479;&#26041;&#27861;&#20026;&#20102;&#36807;&#28388;GPS&#36712;&#36857;&#20013;&#30340;&#22122;&#22768;&#24448;&#24448;&#20391;&#37325;&#20110;&#22522;&#20110;&#36335;&#30001;&#30340;&#26041;&#27861;&#29992;&#20110;&#31616;&#21270;&#36712;&#36857;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24573;&#30053;&#20102;GPS&#25968;&#25454;&#20013;&#21253;&#21547;&#30340;&#36816;&#21160;&#32454;&#33410;&#65292;&#38480;&#21046;&#20102;&#36712;&#36857;&#34920;&#31034;&#23398;&#20064;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33258;&#30417;&#30563;&#25216;&#26415;&#30340;&#32852;&#21512;GPS&#21644;&#36335;&#30001;&#24314;&#27169;&#30340;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21363;JGRM&#12290;&#25105;&#20204;&#23558;GPS&#36712;&#36857;&#21644;&#36335;&#30001;&#35270;&#20026;&#21333;&#20010;&#31227;&#21160;&#35266;&#23519;&#30340;&#20004;&#31181;&#27169;&#24335;&#65292;&#24182;&#36890;&#36807;&#27169;&#24577;&#38388;&#20449;&#24687;&#20132;&#20114;&#26469;&#34701;&#21512;&#20449;&#24687;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#20010;&#32534;&#30721;&#22120;&#65292;&#20998;&#21035;&#29992;&#20110;&#25429;&#33719;&#36335;&#30001;&#21644;GPS&#36712;&#36857;&#30340;&#34920;&#31034;&#12290;&#26469;&#33258;&#36825;&#20004;&#31181;&#27169;&#24577;&#30340;&#34920;&#31034;&#34987;&#36865;&#20837;&#19968;&#20010;&#20849;&#20139;&#30340;&#21464;&#21387;&#22120;&#36827;&#34892;&#27169;&#24577;&#38388;&#20449;&#24687;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16915v1 Announce Type: cross  Abstract: Trajectory representation learning plays a pivotal role in supporting various downstream tasks. Traditional methods in order to filter the noise in GPS trajectories tend to focus on routing-based methods used to simplify the trajectories. However, this approach ignores the motion details contained in the GPS data, limiting the representation capability of trajectory representation learning. To fill this gap, we propose a novel representation learning framework that Joint GPS and Route Modelling based on self-supervised technology, namely JGRM. We consider GPS trajectory and route as the two modes of a single movement observation and fuse information through inter-modal information interaction. Specifically, we develop two encoders, each tailored to capture representations of route and GPS trajectories respectively. The representations from the two modalities are fed into a shared transformer for inter-modal information interaction. Eve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PDETime&#65292;&#19968;&#31181;&#21463;&#31070;&#32463;PDE&#35299;&#31639;&#22120;&#21407;&#21017;&#21551;&#21457;&#30340;&#26032;&#22411;LMTF&#27169;&#22411;&#65292;&#20174;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35270;&#35282;&#37325;&#26032;&#24605;&#32771;&#20102;&#38271;&#26399;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.16913</link><description>&lt;p&gt;
PDETime&#65306;&#20174;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35270;&#35282;&#37325;&#26032;&#24605;&#32771;&#38271;&#26399;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
PDETime: Rethinking Long-Term Multivariate Time Series Forecasting from the perspective of partial differential equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PDETime&#65292;&#19968;&#31181;&#21463;&#31070;&#32463;PDE&#35299;&#31639;&#22120;&#21407;&#21017;&#21551;&#21457;&#30340;&#26032;&#22411;LMTF&#27169;&#22411;&#65292;&#20174;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35270;&#35282;&#37325;&#26032;&#24605;&#32771;&#20102;&#38271;&#26399;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#23548;&#33268;&#20102;&#21508;&#31181;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#29992;&#20110;&#38271;&#26399;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;LMTF&#65289;&#65292;&#20854;&#20013;&#35768;&#22810;&#27169;&#22411;&#34920;&#29616;&#20986;&#33394;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#20851;&#27880;&#28857;&#38598;&#20013;&#22312;&#22522;&#20110;&#21382;&#21490;&#20215;&#20540;&#30340;&#27169;&#22411;&#19978;&#65292;&#36825;&#20123;&#27169;&#22411;&#20381;&#36182;&#20110;&#36807;&#21435;&#30340;&#35266;&#27979;&#32467;&#26524;&#26469;&#39044;&#27979;&#26410;&#26469;&#30340;&#24207;&#21015;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#26032;&#30340;&#36235;&#21183;&#24050;&#32463;&#20986;&#29616;&#65292;&#21363;&#22522;&#20110;&#26102;&#38388;&#32034;&#24341;&#30340;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#23545;&#26102;&#38388;&#24207;&#21015;&#28508;&#22312;&#36830;&#32493;&#21160;&#24577;&#30340;&#26356;&#32454;&#33268;&#29702;&#35299;&#12290;&#19982;&#32858;&#21512;&#31354;&#38388;&#22495;&#25110;&#26102;&#38388;&#22495;&#20449;&#24687;&#30340;&#36825;&#20004;&#31181;&#27169;&#22411;&#19981;&#21516;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#35270;&#20026;&#20174;&#36830;&#32493;&#21160;&#21147;&#31995;&#32479;&#23450;&#26399;&#25277;&#26679;&#30340;&#26102;&#31354;&#25968;&#25454;&#65292;&#21487;&#36890;&#36807;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#34920;&#31034;&#65292;&#20854;&#20013;&#31354;&#38388;&#22495;&#20026;&#22266;&#23450;&#12290;&#22522;&#20110;&#36825;&#19968;&#35270;&#35282;&#65292;&#25105;&#20204;&#25552;&#20986;PDETime&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#31070;&#32463;PDE&#35299;&#31639;&#22120;&#21407;&#21017;&#21551;&#21457;&#30340;&#26032;&#22411;LMTF&#27169;&#22411;&#65292;&#36981;&#24490;&#32534;&#30721;-&#38598;&#25104;-
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16913v1 Announce Type: new  Abstract: Recent advancements in deep learning have led to the development of various models for long-term multivariate time-series forecasting (LMTF), many of which have shown promising results. Generally, the focus has been on historical-value-based models, which rely on past observations to predict future series. Notably, a new trend has emerged with time-index-based models, offering a more nuanced understanding of the continuous dynamics underlying time series. Unlike these two types of models that aggregate the information of spatial domains or temporal domains, in this paper, we consider multivariate time series as spatiotemporal data regularly sampled from a continuous dynamical system, which can be represented by partial differential equations (PDEs), with the spatial domain being fixed. Building on this perspective, we present PDETime, a novel LMTF model inspired by the principles of Neural PDE solvers, following the encoding-integration-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#40065;&#26834;&#24615;&#22522;&#20934;&#65292;&#35780;&#20272;&#20102;&#22810;&#20010;&#20915;&#31574;&#26641;&#38598;&#25104;&#27169;&#22411;&#22312;&#20225;&#19994;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#26032;&#30340;&#25968;&#25454;&#38598;NewCICIDS&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20294;&#23545;&#20110;&#26368;&#26032;&#30340;&#32593;&#32476;&#25915;&#20987;&#65292;RF&#21644;LGBM&#30340;&#40065;&#26834;&#24615;&#36739;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.16912</link><description>&lt;p&gt;
&#20225;&#19994;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
An Adversarial Robustness Benchmark for Enterprise Network Intrusion Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#40065;&#26834;&#24615;&#22522;&#20934;&#65292;&#35780;&#20272;&#20102;&#22810;&#20010;&#20915;&#31574;&#26641;&#38598;&#25104;&#27169;&#22411;&#22312;&#20225;&#19994;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#26032;&#30340;&#25968;&#25454;&#38598;NewCICIDS&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20294;&#23545;&#20110;&#26368;&#26032;&#30340;&#32593;&#32476;&#25915;&#20987;&#65292;RF&#21644;LGBM&#30340;&#40065;&#26834;&#24615;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#32593;&#32476;&#25915;&#20987;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#23545;&#20110;&#21508;&#20010;&#35268;&#27169;&#30340;&#20225;&#19994;&#37117;&#24517;&#39035;&#26159;&#19968;&#20010;&#20248;&#20808;&#32771;&#34385;&#30340;&#20107;&#39033;&#12290;&#20026;&#20102;&#21487;&#38752;&#22320;&#27604;&#36739;&#19981;&#21516;ML&#27169;&#22411;&#22312;&#20225;&#19994;&#35745;&#31639;&#26426;&#32593;&#32476;&#20013;&#29992;&#20110;&#26816;&#27979;&#32593;&#32476;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#23427;&#20204;&#24517;&#39035;&#22312;&#26631;&#20934;&#21270;&#26465;&#20214;&#19979;&#36827;&#34892;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#26465;&#29702;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#22522;&#20934;&#65292;&#21033;&#29992;&#26631;&#20934;&#25968;&#25454;&#38598;&#29983;&#25104;&#26377;&#38480;&#21046;&#30340;&#23545;&#25239;&#31034;&#20363;&#65292;&#35780;&#20272;&#20102;&#22810;&#20010;&#20915;&#31574;&#26641;&#38598;&#25104;&#65288;RF&#12289;XGB&#12289;LGBM&#21644;EBM&#65289;&#30340;&#40065;&#26834;&#24615;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#21407;&#22987;&#30340;CICIDS2017&#25968;&#25454;&#38598;&#12289;&#32463;&#36807;&#26657;&#27491;&#30340;NewCICIDS&#29256;&#26412;&#20197;&#21450;&#21253;&#21547;&#20102;&#26356;&#22810;&#26368;&#26032;&#32593;&#32476;&#27969;&#37327;&#30340;HIKARI&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;NewCICIDS&#23548;&#33268;&#20102;&#24615;&#33021;&#26356;&#22909;&#30340;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;XGB&#21644;EBM&#65292;&#32780;RF&#21644;LGBM&#23545;HIKARI&#30340;&#26368;&#26032;&#32593;&#32476;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#36739;&#24046;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#27169;&#22411;&#23545;&#23545;&#25239;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16912v1 Announce Type: cross  Abstract: As cyber-attacks become more sophisticated, improving the robustness of Machine Learning (ML) models must be a priority for enterprises of all sizes. To reliably compare the robustness of different ML models for cyber-attack detection in enterprise computer networks, they must be evaluated in standardized conditions. This work presents a methodical adversarial robustness benchmark of multiple decision tree ensembles with constrained adversarial examples generated from standard datasets. The robustness of regularly and adversarially trained RF, XGB, LGBM, and EBM models was evaluated on the original CICIDS2017 dataset, a corrected version of it designated as NewCICIDS, and the HIKARI dataset, which contains more recent network traffic. NewCICIDS led to models with a better performance, especially XGB and EBM, but RF and LGBM were less robust against the more recent cyber-attacks of HIKARI. Overall, the robustness of the models to advers
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#38754;&#20020;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#20302;&#36755;&#20986;&#21487;&#35299;&#37322;&#24615;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#34701;&#21512;&#20102;&#36125;&#21494;&#26031;&#26041;&#27861;&#21644;&#27491;&#21017;&#21270;&#27969;&#65292;&#22312;&#21442;&#25968;&#21518;&#39564;&#35282;&#24230;&#23454;&#29616;&#20010;&#24615;&#21270;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#23545;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#23545;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#36229;&#20998;&#24067;&#26816;&#27979;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.16911</link><description>&lt;p&gt;
&#22522;&#20110;&#21518;&#39564;&#24494;&#35843;&#30340;&#21487;&#20449;&#20010;&#24615;&#21270;&#36125;&#21494;&#26031;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Trustworthy Personalized Bayesian Federated Learning via Posterior Fine-Tune
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16911
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#38754;&#20020;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#20302;&#36755;&#20986;&#21487;&#35299;&#37322;&#24615;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#34701;&#21512;&#20102;&#36125;&#21494;&#26031;&#26041;&#27861;&#21644;&#27491;&#21017;&#21270;&#27969;&#65292;&#22312;&#21442;&#25968;&#21518;&#39564;&#35282;&#24230;&#23454;&#29616;&#20010;&#24615;&#21270;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#23545;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#23545;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#36229;&#20998;&#24067;&#26816;&#27979;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#20302;&#36755;&#20986;&#21487;&#35299;&#37322;&#24615;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#26159;&#32852;&#37030;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#38754;&#20020;&#30340;&#26368;&#22823;&#25361;&#25112;&#12290;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#19981;&#21516;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#19981;&#20877;&#35797;&#22270;&#35757;&#32451;&#21333;&#20010;&#27169;&#22411;&#65292;&#32780;&#26159;&#20026;&#27599;&#20010;&#23458;&#25143;&#23450;&#21046;&#29420;&#29305;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23558;&#36125;&#21494;&#26031;&#26041;&#27861;&#34701;&#20837;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#22686;&#24378;&#20102;&#31639;&#27861;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#27491;&#21017;&#21270;&#27969;&#26469;&#23454;&#29616;&#20174;&#21442;&#25968;&#21518;&#39564;&#35282;&#24230;&#30340;&#20010;&#24615;&#21270;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#27491;&#21017;&#21270;&#27969;&#23545;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#36229;&#20998;&#24067;&#26816;&#27979;&#30340;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16911v1 Announce Type: new  Abstract: Performance degradation owing to data heterogeneity and low output interpretability are the most significant challenges faced by federated learning in practical applications. Personalized federated learning diverges from traditional approaches, as it no longer seeks to train a single model, but instead tailors a unique personalized model for each client. However, previous work focused only on personalization from the perspective of neural network parameters and lack of robustness and interpretability. In this work, we establish a novel framework for personalized federated learning, incorporating Bayesian methodology which enhances the algorithm's ability to quantify uncertainty. Furthermore, we introduce normalizing flow to achieve personalization from the parameter posterior perspective and theoretically analyze the impact of normalizing flow on out-of-distribution (OOD) detection for Bayesian neural networks. Finally, we evaluated our 
&lt;/p&gt;</description></item><item><title>&#20307;&#32946;&#27963;&#21160;&#23545;&#24576;&#23381;&#26399;&#38388;&#29983;&#27963;&#36136;&#37327;&#30340;&#24433;&#21709;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#29616;&#26377;&#30740;&#31350;&#26041;&#27861;&#23384;&#22312;&#20559;&#24046;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#32771;&#23519;&#27492;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.16909</link><description>&lt;p&gt;
&#24576;&#23381;&#26399;&#38388;&#20307;&#32946;&#27963;&#21160;&#23545;&#29983;&#27963;&#36136;&#37327;&#30340;&#24433;&#21709;&#65306;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Impact of Physical Activity on Quality of Life During Pregnancy: A Causal ML Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16909
&lt;/p&gt;
&lt;p&gt;
&#20307;&#32946;&#27963;&#21160;&#23545;&#24576;&#23381;&#26399;&#38388;&#29983;&#27963;&#36136;&#37327;&#30340;&#24433;&#21709;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#29616;&#26377;&#30740;&#31350;&#26041;&#27861;&#23384;&#22312;&#20559;&#24046;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#32771;&#23519;&#27492;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#27963;&#36136;&#37327;&#65288;QoL&#65289;&#30340;&#27010;&#24565;&#28041;&#21450;&#23545;&#20010;&#20307;&#31119;&#31049;&#30340;&#25972;&#20307;&#35780;&#20272;&#65292;&#21253;&#25324;&#24515;&#29702;&#21644;&#31038;&#20250;&#26041;&#38754;&#12290;&#24576;&#23381;&#30340;&#22919;&#22899;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#32933;&#32982;&#21644;&#21387;&#21147;&#36739;&#22823;&#30340;&#22919;&#22899;&#65292;&#24120;&#24120;&#20307;&#39564;&#21040;&#36739;&#20302;&#30340;&#29983;&#27963;&#36136;&#37327;&#12290;&#20307;&#32946;&#27963;&#21160;&#65288;PA&#65289;&#26174;&#31034;&#20986;&#22686;&#24378;&#29983;&#27963;&#36136;&#37327;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36229;&#37325;&#21644;&#32933;&#32982;&#30340;&#23381;&#22919;&#24456;&#23569;&#36798;&#21040;&#25512;&#33616;&#30340;&#20307;&#32946;&#27963;&#21160;&#27700;&#24179;&#12290;&#30740;&#31350;&#24050;&#32463;&#20351;&#29992;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#26041;&#27861;&#35843;&#26597;&#20102;&#24576;&#23381;&#26399;&#38388;&#20307;&#32946;&#27963;&#21160;&#19982;&#29983;&#27963;&#36136;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36825;&#20123;&#26041;&#27861;&#26088;&#22312;&#21457;&#29616;&#21464;&#37327;&#20043;&#38388;&#30340;&#20598;&#28982;&#30456;&#20851;&#24615;&#65292;&#32780;&#19981;&#26159;&#22240;&#26524;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#20307;&#32946;&#27963;&#21160;&#21442;&#25968;&#65292;&#24573;&#30053;&#20102;&#20351;&#29992;&#19981;&#21516;&#22240;&#32032;&#65288;&#22914;&#27597;&#20146;&#30340;&#21307;&#23398;&#21490;&#65289;&#21644;&#19978;&#19979;&#25991;&#25968;&#25454;&#65292;&#23548;&#33268;&#20272;&#35745;&#26377;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#20272;&#35745;&#32570;&#20047;&#23545;&#21487;&#33021;&#24433;&#21709;&#23427;&#20204;&#30340;&#20013;&#20171;&#21464;&#37327;&#21644;&#21453;&#20107;&#23454;&#24773;&#26223;&#30340;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#35843;&#26597;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16909v1 Announce Type: new  Abstract: The concept of Quality of Life (QoL) refers to a holistic measurement of an individual's well-being, incorporating psychological and social aspects. Pregnant women, especially those with obesity and stress, often experience lower QoL. Physical activity (PA) has shown the potential to enhance the QoL. However, pregnant women who are overweight and obese rarely meet the recommended level of PA. Studies have investigated the relationship between PA and QoL during pregnancy using correlation-based approaches. These methods aim to discover spurious correlations between variables rather than causal relationships. Besides, the existing methods mainly rely on physical activity parameters and neglect the use of different factors such as maternal (medical) history and context data, leading to biased estimates. Furthermore, the estimations lack an understanding of mediators and counterfactual scenarios that might affect them. In this paper, we inve
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#24518;&#38459;&#22120;&#24320;&#21457;&#38543;&#26426;&#36923;&#36753;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#33391;&#22909;&#35843;&#33410;&#27010;&#29575;&#21644;&#30456;&#20851;&#24615;&#30340;&#38543;&#26426;&#25968;&#23383;&#32534;&#30721;&#21644;&#22788;&#29702;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#32039;&#20945;&#30340;&#38543;&#26426;Roberts&#20132;&#21449;&#31639;&#23376;&#29992;&#20110;&#36793;&#32536;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.16908</link><description>&lt;p&gt;
&#21033;&#29992;&#24518;&#38459;&#22120;&#21551;&#29992;&#30340;&#38543;&#26426;&#36923;&#36753;&#36827;&#34892;&#26412;&#22320;&#38543;&#26426;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Local stochastic computing using memristor-enabled stochastic logics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16908
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#24518;&#38459;&#22120;&#24320;&#21457;&#38543;&#26426;&#36923;&#36753;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#33391;&#22909;&#35843;&#33410;&#27010;&#29575;&#21644;&#30456;&#20851;&#24615;&#30340;&#38543;&#26426;&#25968;&#23383;&#32534;&#30721;&#21644;&#22788;&#29702;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#32039;&#20945;&#30340;&#38543;&#26426;Roberts&#20132;&#21449;&#31639;&#23376;&#29992;&#20110;&#36793;&#32536;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#35745;&#31639;&#25552;&#20379;&#20102;&#19968;&#31181;&#27010;&#29575;&#26041;&#27861;&#26469;&#35299;&#20915;&#21508;&#20010;&#39046;&#22495;&#20013;&#30001;&#20110;&#19981;&#30830;&#23450;&#24615;&#21644;&#22122;&#22768;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#38543;&#26426;&#35745;&#31639;&#38754;&#20020;&#30528;&#21457;&#23637;&#21487;&#38752;&#30340;&#38543;&#26426;&#36923;&#36753;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#24518;&#38459;&#22120;&#24320;&#21457;&#38543;&#26426;&#36923;&#36753;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#24518;&#38459;&#22120;&#38598;&#25104;&#21040;&#36923;&#36753;&#30005;&#36335;&#20013;&#35774;&#35745;&#38543;&#26426;&#36923;&#36753;&#65292;&#20854;&#20013;&#24518;&#38459;&#22120;&#20999;&#25442;&#20013;&#22266;&#26377;&#30340;&#38543;&#26426;&#24615;&#34987;&#21033;&#29992;&#26469;&#23454;&#29616;&#20855;&#26377;&#33391;&#22909;&#35843;&#33410;&#27010;&#29575;&#21644;&#30456;&#20851;&#24615;&#30340;&#38543;&#26426;&#25968;&#23383;&#32534;&#30721;&#21644;&#22788;&#29702;&#12290;&#20316;&#20026;&#38543;&#26426;&#36923;&#36753;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#36793;&#32536;&#26816;&#27979;&#30340;&#32039;&#20945;&#22411;&#38543;&#26426;Roberts&#20132;&#21449;&#31639;&#23376;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#35813;&#31639;&#23376;&#23637;&#31034;&#20986;&#20986;&#33394;&#30340;&#36718;&#24275;&#21644;&#32441;&#29702;&#25552;&#21462;&#33021;&#21147;&#65292;&#21363;&#20351;&#23384;&#22312;50%&#30340;&#22122;&#38899;&#65292;&#30001;&#20110;&#20854;&#27010;&#29575;&#24615;&#36136;&#21644;&#32039;&#20945;&#30340;&#35774;&#35745;&#65292;&#35813;&#31639;&#23376;&#33021;&#22815;&#33410;&#30465;95%&#30340;&#33021;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16908v1 Announce Type: cross  Abstract: Stochastic computing offers a probabilistic approach to address challenges posed by problems with uncertainty and noise in various fields, particularly machine learning. The realization of stochastic computing, however, faces the limitation of developing reliable stochastic logics. Here, we present stochastic logics development using memristors. Specifically, we integrate memristors into logic circuits to design the stochastic logics, wherein the inherent stochasticity in memristor switching is harnessed to enable stochastic number encoding and processing with well-regulated probabilities and correlations. As a practical application of the stochastic logics, we design a compact stochastic Roberts cross operator for edge detection. Remarkably, the operator demonstrates exceptional contour and texture extractions, even in the presence of 50% noise, and owning to the probabilistic nature and compact design, the operator can consume 95% le
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#33539;&#24335;&#65292;&#36890;&#36807;&#36873;&#25321;&#19982;&#27979;&#37327;&#26631;&#35782;&#19968;&#33268;&#30340;&#26679;&#26412;&#65292;&#20197;&#21450;&#20174;&#19982;&#27979;&#37327;&#20449;&#21495;&#30456;&#32467;&#21512;&#30340;&#21021;&#22987;&#21270;&#24320;&#22987;&#24674;&#22797;&#36807;&#31243;&#65292;&#23454;&#29616;&#36755;&#20986;&#31283;&#23450;&#24615;&#21644;&#22686;&#24378;&#12290;</title><link>https://arxiv.org/abs/2402.16907</link><description>&lt;p&gt;
&#29992;&#20110;&#22270;&#20687;&#24674;&#22797;&#30340;&#25193;&#25955;&#21518;&#39564;&#36817;&#20284;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Diffusion Posterior Proximal Sampling for Image Restoration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#33539;&#24335;&#65292;&#36890;&#36807;&#36873;&#25321;&#19982;&#27979;&#37327;&#26631;&#35782;&#19968;&#33268;&#30340;&#26679;&#26412;&#65292;&#20197;&#21450;&#20174;&#19982;&#27979;&#37327;&#20449;&#21495;&#30456;&#32467;&#21512;&#30340;&#21021;&#22987;&#21270;&#24320;&#22987;&#24674;&#22797;&#36807;&#31243;&#65292;&#23454;&#29616;&#36755;&#20986;&#31283;&#23450;&#24615;&#21644;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#26679;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#21151;&#25928;&#12290;&#29616;&#26377;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#31639;&#27861;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#21033;&#29992;&#25968;&#25454;&#20808;&#39564;&#65292;&#20294;&#20173;&#20445;&#30041;&#20102;&#32487;&#25215;&#33258;&#26080;&#26465;&#20214;&#29983;&#25104;&#33539;&#24335;&#30340;&#20803;&#32032;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#33539;&#24335;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36873;&#25321;&#22312;&#27599;&#20010;&#29983;&#25104;&#27493;&#39588;&#20013;&#19982;&#27979;&#37327;&#26631;&#35782;&#19968;&#33268;&#30340;&#26679;&#26412;&#65292;&#21033;&#29992;&#37319;&#26679;&#36873;&#25321;&#20316;&#20026;&#36755;&#20986;&#31283;&#23450;&#24615;&#21644;&#22686;&#24378;&#30340;&#36884;&#24452;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#19968;&#20010;&#19982;&#27979;&#37327;&#20449;&#21495;&#30456;&#32467;&#21512;&#30340;&#21021;&#22987;&#21270;&#24320;&#22987;&#24674;&#22797;&#36807;&#31243;&#65292;&#25552;&#20379;&#20102;&#38468;&#21152;&#20449;&#24687;&#20197;&#26356;&#22909;&#22320;&#23545;&#40784;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16907v1 Announce Type: cross  Abstract: Diffusion models have demonstrated remarkable efficacy in generating high-quality samples. Existing diffusion-based image restoration algorithms exploit pre-trained diffusion models to leverage data priors, yet they still preserve elements inherited from the unconditional generation paradigm. These strategies initiate the denoising process with pure white noise and incorporate random noise at each generative step, leading to over-smoothed results. In this paper, we introduce a refined paradigm for diffusion-based image restoration. Specifically, we opt for a sample consistent with the measurement identity at each generative step, exploiting the sampling selection as an avenue for output stability and enhancement. Besides, we start the restoration process with an initialization combined with the measurement signal, providing supplementary information to better align the generative process. Extensive experimental results and analyses val
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24418;&#24335;&#36923;&#36753;&#20026;&#22522;&#30784;&#30340;&#31243;&#24207;&#21512;&#25104;&#21644;LLM&#20869;&#23481;&#29983;&#25104;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26102;&#38388;&#27969;&#36923;&#36753;&#65288;TSL&#65289;&#23545;&#29983;&#25104;&#24335;&#20195;&#29702;&#26045;&#21152;&#26102;&#38388;&#32422;&#26463;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20195;&#29702;&#34892;&#20026;&#30340;&#20445;&#35777;&#27700;&#24179;&#12289;&#31995;&#32479;&#30340;&#35299;&#37322;&#24615;&#21644;&#20195;&#29702;&#30340;&#27169;&#22359;&#21270;&#26500;&#24314;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.16905</link><description>&lt;p&gt;
&#21033;&#29992;&#21453;&#24212;&#21512;&#25104;&#23545;&#29983;&#25104;&#24335;&#20195;&#29702;&#34892;&#20026;&#26045;&#21152;&#26102;&#38388;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Enforcing Temporal Constraints on Generative Agent Behavior with Reactive Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16905
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24418;&#24335;&#36923;&#36753;&#20026;&#22522;&#30784;&#30340;&#31243;&#24207;&#21512;&#25104;&#21644;LLM&#20869;&#23481;&#29983;&#25104;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26102;&#38388;&#27969;&#36923;&#36753;&#65288;TSL&#65289;&#23545;&#29983;&#25104;&#24335;&#20195;&#29702;&#26045;&#21152;&#26102;&#38388;&#32422;&#26463;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20195;&#29702;&#34892;&#20026;&#30340;&#20445;&#35777;&#27700;&#24179;&#12289;&#31995;&#32479;&#30340;&#35299;&#37322;&#24615;&#21644;&#20195;&#29702;&#30340;&#27169;&#22359;&#21270;&#26500;&#24314;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#27969;&#34892;&#24341;&#21457;&#20102;&#23545;&#21019;&#24314;&#20132;&#20114;&#20195;&#29702;&#26032;&#26041;&#27861;&#30340;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#22312;&#20114;&#21160;&#36807;&#31243;&#20013;&#31649;&#29702;&#36825;&#20123;&#20195;&#29702;&#30340;&#26102;&#38388;&#34892;&#20026;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#24418;&#24335;&#36923;&#36753;&#20026;&#22522;&#30784;&#30340;&#31243;&#24207;&#21512;&#25104;&#19982;LLM&#20869;&#23481;&#29983;&#25104;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#21019;&#24314;&#36981;&#23432;&#26102;&#38388;&#32422;&#26463;&#30340;&#29983;&#25104;&#24335;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#26102;&#38388;&#27969;&#36923;&#36753;&#65288;Temporal Stream Logic&#65292;TSL&#65289;&#29983;&#25104;&#19968;&#20010;&#33258;&#21160;&#26426;&#65292;&#23545;&#20195;&#29702;&#26045;&#21152;&#26102;&#38388;&#32467;&#26500;&#65292;&#24182;&#23558;&#27599;&#20010;&#21160;&#20316;&#30340;&#32454;&#33410;&#30041;&#32473;LLM&#12290;&#36890;&#36807;&#20351;&#29992;TSL&#65292;&#25105;&#20204;&#33021;&#22815;&#22686;&#24378;&#29983;&#25104;&#20195;&#29702;&#65292;&#20351;&#29992;&#25143;&#22312;&#34892;&#20026;&#19978;&#26377;&#26356;&#39640;&#30340;&#20445;&#35777;&#27700;&#24179;&#65292;&#31995;&#32479;&#26356;&#26131;&#35299;&#37322;&#65292;&#24182;&#19988;&#26356;&#33021;&#20197;&#27169;&#22359;&#21270;&#26041;&#24335;&#26500;&#24314;&#20195;&#29702;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16905v1 Announce Type: new  Abstract: The surge in popularity of Large Language Models (LLMs) has opened doors for new approaches to the creation of interactive agents. However, managing the temporal behavior of such agents over the course of an interaction remains challenging. The stateful, long-term horizon and quantitative reasoning required for coherent agent behavior does not fit well into the LLM paradigm. We propose a combination of formal logic-based program synthesis and LLM content generation to create generative agents that adhere to temporal constraints. Our approach uses Temporal Stream Logic (TSL) to generate an automaton that enforces a temporal structure on an agent and leaves the details of each action for a moment in time to an LLM. By using TSL, we are able to augment the generative agent where users have a higher level of guarantees on behavior, better interpretability of the system, and more ability to build agents in a modular way. We evaluate our appro
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#28151;&#21512;&#36951;&#20256;&#31639;&#27861;&#65288;LGSTO&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#38754;&#21521;&#23454;&#26102;&#29289;&#32852;&#32593;&#20256;&#24863;&#31995;&#32479;&#30340;&#36873;&#25321;&#24615;&#20219;&#21153;&#21368;&#36733;&#38382;&#39064;&#65292;&#20197;&#22312;&#26102;&#38388;&#21644;&#33021;&#37327;&#32422;&#26463;&#19979;&#26368;&#22823;&#21270;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16904</link><description>&lt;p&gt;
&#38754;&#21521;&#26368;&#22823;&#25512;&#29702;&#20934;&#30830;&#24615;&#21644;&#33021;&#25928;&#23454;&#26102;&#29289;&#32852;&#32593;&#20256;&#24863;&#31995;&#32479;&#30340;&#36873;&#25321;&#24615;&#20219;&#21153;&#21368;&#36733;
&lt;/p&gt;
&lt;p&gt;
Selective Task offloading for Maximum Inference Accuracy and Energy efficient Real-Time IoT Sensing Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#28151;&#21512;&#36951;&#20256;&#31639;&#27861;&#65288;LGSTO&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#38754;&#21521;&#23454;&#26102;&#29289;&#32852;&#32593;&#20256;&#24863;&#31995;&#32479;&#30340;&#36873;&#25321;&#24615;&#20219;&#21153;&#21368;&#36733;&#38382;&#39064;&#65292;&#20197;&#22312;&#26102;&#38388;&#21644;&#33021;&#37327;&#32422;&#26463;&#19979;&#26368;&#22823;&#21270;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#22411;&#25512;&#29702;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#20419;&#36827;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#36793;&#32536;&#37096;&#32626;&#12290;&#28982;&#32780;&#65292;&#36793;&#32536;&#35774;&#22791;&#26377;&#38480;&#30340;&#36164;&#28304;&#29305;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23454;&#26102;&#24212;&#29992;&#31243;&#24207;&#65292;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#37096;&#32626;&#22810;&#20010;&#25512;&#29702;&#27169;&#22411;&#65288;&#25110;&#32773;&#19968;&#20010;&#23610;&#23544;&#21487;&#35843;&#30340;&#27169;&#22411;&#65289;&#21464;&#21270;&#22823;&#23567;&#65292;&#22240;&#27492;&#20934;&#30830;&#24615;&#21644;&#21151;&#32791;&#65292;&#20197;&#21450;&#36793;&#32536;&#26381;&#21153;&#22120;&#25512;&#29702;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#20379;&#19968;&#20010;&#21160;&#24577;&#31995;&#32479;&#65292;&#22312;&#20854;&#20013;&#26681;&#25454;&#24403;&#21069;&#36164;&#28304;&#26465;&#20214;&#25191;&#34892;&#23545;&#25512;&#29702;&#27169;&#22411;&#21040;&#25512;&#29702;&#20219;&#21153;&#30340;&#20998;&#37197;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#23558;&#25512;&#29702;&#27169;&#22411;&#36873;&#25321;&#24615;&#20998;&#37197;&#32473;&#20219;&#21153;&#25110;&#23558;&#20854;&#21368;&#36733;&#21040;&#36793;&#32536;&#26381;&#21153;&#22120;&#20197;&#22312;&#26102;&#38388;&#21644;&#33021;&#37327;&#32422;&#26463;&#19979;&#26368;&#22823;&#21270;&#25512;&#29702;&#20934;&#30830;&#24615;&#30340;&#38382;&#39064;&#12290;&#35813;&#38382;&#39064;&#34987;&#35777;&#26126;&#26159;&#26080;&#30028;&#22810;&#32500;&#32972;&#21253;&#38382;&#39064;&#30340;&#19968;&#20010;&#23454;&#20363;&#65292;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#24378; NP-&#38590;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#28151;&#21512;&#36951;&#20256;&#31639;&#27861;&#65288;LGSTO&#65289;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16904v1 Announce Type: cross  Abstract: The recent advancements in small-size inference models facilitated AI deployment on the edge. However, the limited resource nature of edge devices poses new challenges especially for real-time applications. Deploying multiple inference models (or a single tunable model) varying in size and therefore accuracy and power consumption, in addition to an edge server inference model, can offer a dynamic system in which the allocation of inference models to inference jobs is performed according to the current resource conditions. Therefore, in this work, we tackle the problem of selectively allocating inference models to jobs or offloading them to the edge server to maximize inference accuracy under time and energy constraints. This problem is shown to be an instance of the unbounded multidimensional knapsack problem which is considered a strongly NP-hard problem. We propose a lightweight hybrid genetic algorithm (LGSTO) to solve this problem.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#36731;&#28145;&#24230;&#25805;&#20316;&#32593;&#32476;&#65288;DeepONets&#65289;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#36991;&#20813;&#20102;&#20351;&#29992;&#20559;&#24494;&#20998;&#26041;&#31243;&#31215;&#20998;&#31574;&#30053;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#35745;&#31639;&#25104;&#26412;</title><link>https://arxiv.org/abs/2402.16903</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#26696;&#29992;&#20110;&#28145;&#24230;&#25805;&#20316;&#32593;&#32476;&#30340;&#20195;&#29702;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
A novel data generation scheme for surrogate modelling with deep operator networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16903
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#36731;&#28145;&#24230;&#25805;&#20316;&#32593;&#32476;&#65288;DeepONets&#65289;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#36991;&#20813;&#20102;&#20351;&#29992;&#20559;&#24494;&#20998;&#26041;&#31243;&#31215;&#20998;&#31574;&#30053;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#35745;&#31639;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16903v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#22522;&#20110;&#25805;&#20316;&#31526;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#22914;DeepONets&#24050;&#25104;&#20026;&#29289;&#29702;&#31995;&#32479;&#20195;&#29702;&#24314;&#27169;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#24037;&#20855;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#20026;&#20102;&#36827;&#34892;&#25805;&#20316;&#31526;&#20195;&#29702;&#24314;&#27169;&#65292;&#35757;&#32451;&#25968;&#25454;&#26159;&#36890;&#36807;&#20351;&#29992;&#26377;&#38480;&#20803;&#27861;&#31561;&#25216;&#26415;&#35299;&#20915;PDEs&#29983;&#25104;&#30340;&#12290;&#25968;&#25454;&#29983;&#25104;&#30340;&#35745;&#31639;&#23494;&#38598;&#22411;&#29305;&#24615;&#26159;&#37096;&#32626;&#36825;&#20123;&#20195;&#29702;&#27169;&#22411;&#21040;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#26368;&#22823;&#29942;&#39048;&#20043;&#19968;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;DeepONets&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#25152;&#24102;&#26469;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#19982;&#29616;&#26377;&#25991;&#29486;&#19981;&#21516;&#65292;&#25152;&#25552;&#20986;&#30340;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#19981;&#20351;&#29992;&#20219;&#20309;&#20559;&#24494;&#20998;&#26041;&#31243;&#31215;&#20998;&#31574;&#30053;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#29992;&#20110;DeepONet&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#31574;&#30053;&#20013;&#65292;&#39318;&#20808;&#38543;&#26426;&#29983;&#25104;&#36755;&#20986;&#23383;&#27573;&#65292;&#28385;&#36275;&#36793;&#30028;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16903v1 Announce Type: new  Abstract: Operator-based neural network architectures such as DeepONets have emerged as a promising tool for the surrogate modeling of physical systems. In general, towards operator surrogate modeling, the training data is generated by solving the PDEs using techniques such as Finite Element Method (FEM). The computationally intensive nature of data generation is one of the biggest bottleneck in deploying these surrogate models for practical applications. In this study, we propose a novel methodology to alleviate the computational burden associated with training data generation for DeepONets. Unlike existing literature, the proposed framework for data generation does not use any partial differential equation integration strategy, thereby significantly reducing the computational cost associated with generating training dataset for DeepONet. In the proposed strategy, first, the output field is generated randomly, satisfying the boundary conditions u
&lt;/p&gt;</description></item><item><title>PRoLoRA&#26159;&#19968;&#20010;&#26032;&#30340;&#37096;&#20998;&#26059;&#36716;&#22686;&#24378;&#20302;&#31209;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24191;&#25773;&#20943;&#23569;&#12289;&#26059;&#36716;&#22686;&#24378;&#12289;&#37096;&#20998;&#20849;&#20139;&#32454;&#21270;&#21644;&#20462;&#27491;&#21021;&#22987;&#21270;&#31574;&#30053;&#31561;&#22235;&#20010;&#32452;&#20214;&#65292;&#23454;&#29616;&#20102;&#23545;LoRA&#30340;&#20248;&#21183;&#25552;&#21319;&#65292;&#36991;&#20813;&#20102;&#20854;&#20182;&#21442;&#25968;&#20849;&#20139;&#26041;&#27861;&#30340;&#32570;&#28857;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#21442;&#25968;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16902</link><description>&lt;p&gt;
PRoLoRA: &#37096;&#20998;&#26059;&#36716;&#22686;&#24378;&#26356;&#39640;&#25928;LoRA
&lt;/p&gt;
&lt;p&gt;
PRoLoRA: Partial Rotation Empowers More Parameter-Efficient LoRA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16902
&lt;/p&gt;
&lt;p&gt;
PRoLoRA&#26159;&#19968;&#20010;&#26032;&#30340;&#37096;&#20998;&#26059;&#36716;&#22686;&#24378;&#20302;&#31209;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24191;&#25773;&#20943;&#23569;&#12289;&#26059;&#36716;&#22686;&#24378;&#12289;&#37096;&#20998;&#20849;&#20139;&#32454;&#21270;&#21644;&#20462;&#27491;&#21021;&#22987;&#21270;&#31574;&#30053;&#31561;&#22235;&#20010;&#32452;&#20214;&#65292;&#23454;&#29616;&#20102;&#23545;LoRA&#30340;&#20248;&#21183;&#25552;&#21319;&#65292;&#36991;&#20813;&#20102;&#20854;&#20182;&#21442;&#25968;&#20849;&#20139;&#26041;&#27861;&#30340;&#32570;&#28857;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#21442;&#25968;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#25193;&#23637;&#65292;&#21516;&#26102;&#26381;&#21153;&#22810;&#20010;LoRAs&#21464;&#24471;&#26085;&#30410;&#19981;&#20999;&#23454;&#38469;&#65292;&#23548;&#33268;&#25104;&#26412;&#19981;&#21487;&#25215;&#21463;&#65292;&#38656;&#35201;&#26356;&#20855;&#21442;&#25968;&#25928;&#29575;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#37096;&#20998;&#26059;&#36716;&#22686;&#24378;&#20302;&#31209;&#36866;&#24212;&#65288;PRoLoRA&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#30001;&#22235;&#20010;&#20851;&#38190;&#32452;&#20214;&#32452;&#25104;&#30340;&#23618;&#20869;&#20849;&#20139;&#26426;&#21046;&#65306;&#24191;&#25773;&#20943;&#23569;&#12289;&#26059;&#36716;&#22686;&#24378;&#12289;&#37096;&#20998;&#20849;&#20139;&#32454;&#21270;&#21644;&#20462;&#27491;&#30340;&#21021;&#22987;&#21270;&#31574;&#30053;&#12290;&#20316;&#20026;LoRA&#30340;&#36229;&#38598;&#65292;PRoLoRA&#20445;&#30041;&#20102;&#20854;&#20248;&#21183;&#65292;&#24182;&#36890;&#36807;&#20855;&#26377;&#21331;&#36234;&#30340;&#27169;&#22411;&#23481;&#37327;&#12289;&#23454;&#29992;&#21487;&#34892;&#24615;&#21644;&#24191;&#27867;&#36866;&#29992;&#24615;&#65292;&#26377;&#25928;&#22320;&#36991;&#24320;&#20102;&#20854;&#20182;&#21442;&#25968;&#20849;&#20139;&#26041;&#27861;&#30340;&#32570;&#28857;&#12290;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#65292;PRoLoRA&#22312;&#29305;&#23450;&#21442;&#25968;&#39044;&#31639;&#21644;&#24615;&#33021;&#30446;&#26631;&#24773;&#26223;&#19979;&#65292;&#20855;&#26377;&#26174;&#30528;&#26356;&#39640;&#30340;&#21442;&#25968;&#25928;&#29575;&#65292;&#24182;&#19988;&#21487;&#20197;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;LLMs&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;PRoLoRA&#22312;&#21487;&#35757;&#32451;&#21442;&#25968;&#20943;&#23569;&#19968;&#20493;&#30340;&#24773;&#20917;&#19979;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16902v1 Announce Type: new  Abstract: With the rapid scaling of large language models (LLMs), serving numerous LoRAs concurrently has become increasingly impractical, leading to unaffordable costs and necessitating more parameter-efficient finetuning methods. In this work, we introduce Partially Rotation-enhanced Low-Rank Adaptation (PRoLoRA), an intra-layer sharing mechanism comprising four essential components: broadcast reduction, rotation enhancement, partially-sharing refinement, and rectified initialization strategy. As a superset of LoRA, PRoLoRA pertains its advantages, and effectively circumvent the drawbacks of peer parameter-sharing methods with superior model capacity, practical feasibility, and broad applicability. Empirical experiments demonstrate the remarkably higher parameter efficiency of PRoLoRA in both specific parameter budget and performance target scenarios, and its scalability to larger LLMs. Notably, with one time less trainable parameters, PRoLoRA s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#34507;&#30333;&#36136;&#30340;&#22522;&#22240;&#34920;&#31034;&#20316;&#20026;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#21644;&#32467;&#26500;&#30456;&#20851;&#30340;&#26631;&#35760;&#22120;&#65292;&#36890;&#36807;Masked Gene Modeling&#65288;MGM&#65289;&#21644;Triple Enhanced Metagenomic Contrastive Learning&#65288;TEM-CL&#65289;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23439;&#22522;&#22240;&#32452;&#35821;&#35328;&#27169;&#22411;FGBERT&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#22522;&#22240;&#24207;&#21015;&#19982;&#21151;&#33021;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.16901</link><description>&lt;p&gt;
FGBERT&#65306;&#22522;&#20110;&#21151;&#33021;&#39537;&#21160;&#30340;&#23439;&#22522;&#22240;&#32452;&#39044;&#35757;&#32451;&#22522;&#22240;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FGBERT: Function-Driven Pre-trained Gene Language Model for Metagenomics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16901
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#34507;&#30333;&#36136;&#30340;&#22522;&#22240;&#34920;&#31034;&#20316;&#20026;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#21644;&#32467;&#26500;&#30456;&#20851;&#30340;&#26631;&#35760;&#22120;&#65292;&#36890;&#36807;Masked Gene Modeling&#65288;MGM&#65289;&#21644;Triple Enhanced Metagenomic Contrastive Learning&#65288;TEM-CL&#65289;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23439;&#22522;&#22240;&#32452;&#35821;&#35328;&#27169;&#22411;FGBERT&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#22522;&#22240;&#24207;&#21015;&#19982;&#21151;&#33021;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Metagenomic data, comprising mixed multi-species genomes, are prevalent in diverse environments like oceans and soils, significantly impacting human health and ecological functions. However, current research relies on K-mer representations, limiting the capture of structurally relevant gene contexts. To address these limitations and further our understanding of complex relationships between metagenomic sequences and their functions, we introduce a protein-based gene representation as a context-aware and structure-relevant tokenizer. Our approach includes Masked Gene Modeling (MGM) for gene group-level pre-training, providing insights into inter-gene contextual information, and Triple Enhanced Metagenomic Contrastive Learning (TEM-CL) for gene-level pre-training to model gene sequence-function relationships. MGM and TEM-CL constitute our novel metagenomic language model FGBERT, pre-trained on 100 million metagenomic sequences.
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16901v1 Announce Type: cross  Abstract: Metagenomic data, comprising mixed multi-species genomes, are prevalent in diverse environments like oceans and soils, significantly impacting human health and ecological functions. However, current research relies on K-mer representations, limiting the capture of structurally relevant gene contexts. To address these limitations and further our understanding of complex relationships between metagenomic sequences and their functions, we introduce a protein-based gene representation as a context-aware and structure-relevant tokenizer. Our approach includes Masked Gene Modeling (MGM) for gene group-level pre-training, providing insights into inter-gene contextual information, and Triple Enhanced Metagenomic Contrastive Learning (TEM-CL) for gene-level pre-training to model gene sequence-function relationships. MGM and TEM-CL constitute our novel metagenomic language model {\NAME}, pre-trained on 100 million metagenomic sequences. We demon
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#36830;&#32493;&#26102;&#38388;&#25511;&#21046;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#30452;&#25509;&#20998;&#26512;Bellman&#26368;&#20248;&#25439;&#22833;\emph{&#20808;&#39564;}&#27867;&#21270;&#35823;&#24046;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#26377;&#30028;&#24615;&#20551;&#35774;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#31639;&#23376;&#30340;&#20998;&#35299;&#26041;&#27861;&#23454;&#29616;&#20102;&#25439;&#22833;&#20989;&#25968;&#30340;&#36716;&#25442;&#12290;</title><link>https://arxiv.org/abs/2402.16899</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#24378;&#21270;&#23398;&#20064;&#20013;&#28145;&#24230;&#27531;&#24046;&#32593;&#32476;&#30340;\emph{&#20808;&#39564;&#20272;&#35745;}
&lt;/p&gt;
&lt;p&gt;
A prior Estimates for Deep Residual Network in Continuous-time Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#36830;&#32493;&#26102;&#38388;&#25511;&#21046;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#30452;&#25509;&#20998;&#26512;Bellman&#26368;&#20248;&#25439;&#22833;\emph{&#20808;&#39564;}&#27867;&#21270;&#35823;&#24046;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#26377;&#30028;&#24615;&#20551;&#35774;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#31639;&#23376;&#30340;&#20998;&#35299;&#26041;&#27861;&#23454;&#29616;&#20102;&#25439;&#22833;&#20989;&#25968;&#30340;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#35768;&#22810;&#22823;&#35268;&#27169;&#23454;&#38469;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24615;&#33021;&#20998;&#26512;&#24573;&#30053;&#20102;&#36830;&#32493;&#26102;&#38388;&#25511;&#21046;&#38382;&#39064;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#26080;&#27861;&#30452;&#25509;&#20272;&#35745;Bellman&#26368;&#20248;&#25439;&#22833;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#24182;&#19988;&#38656;&#35201;&#19968;&#20010;&#26377;&#30028;&#24615;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#36830;&#32493;&#26102;&#38388;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25152;&#26377;&#28385;&#36275;&#21322;&#32676;&#21644;Lipschitz&#24615;&#36136;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#22312;&#35813;&#26041;&#27861;&#19979;&#65292;&#25105;&#20204;&#33021;&#22815;&#30452;&#25509;&#20998;&#26512;Bellman&#26368;&#20248;&#25439;&#22833;&#30340;\emph{&#20808;&#39564;}&#27867;&#21270;&#35823;&#24046;&#12290;&#35813;&#26041;&#27861;&#30340;&#26680;&#24515;&#22312;&#20110;&#25439;&#22833;&#20989;&#25968;&#30340;&#20004;&#27425;&#36716;&#25442;&#12290;&#20026;&#20102;&#23436;&#25104;&#36716;&#25442;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#22823;&#31639;&#23376;&#30340;&#20998;&#35299;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#20998;&#26512;&#26041;&#27861;&#19981;&#38656;&#35201;&#26377;&#30028;&#24615;&#20551;&#35774;&#12290;&#26368;&#32456;&#25105;&#20204;&#32500;&#24471;&#21040;&#20102;&#19968;&#20010;&#27809;&#26377;&#8220;&#32500;&#24230;&#35781;&#21650;&#8221;&#30340;\emph{&#20808;&#39564;}&#27867;&#21270;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16899v1 Announce Type: cross  Abstract: Deep reinforcement learning excels in numerous large-scale practical applications. However, existing performance analyses ignores the unique characteristics of continuous-time control problems, is unable to directly estimate the generalization error of the Bellman optimal loss and require a boundedness assumption. Our work focuses on continuous-time control problems and proposes a method that is applicable to all such problems where the transition function satisfies semi-group and Lipschitz properties. Under this method, we can directly analyze the \emph{a priori} generalization error of the Bellman optimal loss. The core of this method lies in two transformations of the loss function. To complete the transformation, we propose a decomposition method for the maximum operator. Additionally, this analysis method does not require a boundedness assumption. Finally, we obtain an \emph{a priori} generalization error without the curse of dime
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;MIM-Reasoner&#65292;&#32467;&#21512;&#24378;&#21270;&#23398;&#20064;&#21644;&#27010;&#29575;&#22270;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#32473;&#23450;&#22810;&#37325;&#32593;&#32476;&#20869;&#37096;&#21644;&#23618;&#38388;&#30340;&#22797;&#26434;&#20256;&#25773;&#36807;&#31243;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;MIM&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.16898</link><description>&lt;p&gt;
MIM-Reasoner: &#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#22810;&#37325;&#24433;&#21709;&#26368;&#22823;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MIM-Reasoner: Learning with Theoretical Guarantees for Multiplex Influence Maximization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16898
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;MIM-Reasoner&#65292;&#32467;&#21512;&#24378;&#21270;&#23398;&#20064;&#21644;&#27010;&#29575;&#22270;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#32473;&#23450;&#22810;&#37325;&#32593;&#32476;&#20869;&#37096;&#21644;&#23618;&#38388;&#30340;&#22797;&#26434;&#20256;&#25773;&#36807;&#31243;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;MIM&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#37325;&#24433;&#21709;&#26368;&#22823;&#21270;&#65288;MIM&#65289;&#35201;&#27714;&#25105;&#20204;&#35782;&#21035;&#19968;&#32452;&#31181;&#23376;&#29992;&#25143;&#65292;&#20197;&#26368;&#22823;&#21270;&#22810;&#37325;&#32593;&#32476;&#20013;&#21463;&#24433;&#21709;&#29992;&#25143;&#30340;&#39044;&#26399;&#25968;&#37327;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MIM-Reasoner&#65292;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#27010;&#29575;&#22270;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#25429;&#25417;&#32473;&#23450;&#22810;&#37325;&#32593;&#32476;&#20869;&#37096;&#21644;&#23618;&#38388;&#30340;&#22797;&#26434;&#20256;&#25773;&#36807;&#31243;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;MIM&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16898v1 Announce Type: cross  Abstract: Multiplex influence maximization (MIM) asks us to identify a set of seed users such as to maximize the expected number of influenced users in a multiplex network. MIM has been one of central research topics, especially in nowadays social networking landscape where users participate in multiple online social networks (OSNs) and their influences can propagate among several OSNs simultaneously. Although there exist a couple combinatorial algorithms to MIM, learning-based solutions have been desired due to its generalization ability to heterogeneous networks and their diversified propagation characteristics. In this paper, we introduce MIM-Reasoner, coupling reinforcement learning with probabilistic graphical model, which effectively captures the complex propagation process within and between layers of a given multiplex network, thereby tackling the most challenging problem in MIM. We establish a theoretical guarantee for MIM-Reasoner as w
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21487;&#38752;&#30340;&#20914;&#31361;&#22810;&#35270;&#35282;&#23398;&#20064;&#65288;RCML&#65289;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;Evidential Conflictive Multi-view Learning (ECML)&#26041;&#27861;&#26469;&#22788;&#29702;&#20855;&#26377;&#20914;&#31361;&#20449;&#24687;&#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.16897</link><description>&lt;p&gt;
&#21487;&#38752;&#30340;&#20914;&#31361;&#22810;&#35270;&#35282;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reliable Conflictive Multi-View Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16897
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21487;&#38752;&#30340;&#20914;&#31361;&#22810;&#35270;&#35282;&#23398;&#20064;&#65288;RCML&#65289;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;Evidential Conflictive Multi-view Learning (ECML)&#26041;&#27861;&#26469;&#22788;&#29702;&#20855;&#26377;&#20914;&#31361;&#20449;&#24687;&#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#23398;&#20064;&#26088;&#22312;&#32467;&#21512;&#22810;&#20010;&#29305;&#24449;&#65292;&#20197;&#23454;&#29616;&#23545;&#25968;&#25454;&#30340;&#26356;&#20840;&#38754;&#25551;&#36848;&#12290;&#20043;&#21069;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#37117;&#20551;&#35774;&#22810;&#20010;&#35270;&#22270;&#26159;&#20005;&#26684;&#23545;&#40784;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#21487;&#33021;&#21253;&#21547;&#20302;&#36136;&#37327;&#30340;&#20914;&#31361;&#23454;&#20363;&#65292;&#21363;&#22312;&#19981;&#21516;&#35270;&#22270;&#20013;&#26174;&#31034;&#20914;&#31361;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21487;&#38752;&#30340;&#20914;&#31361;&#22810;&#35270;&#35282;&#23398;&#20064;&#65288;RCML&#65289;&#38382;&#39064;&#65292;&#35201;&#27714;&#27169;&#22411;&#20026;&#20914;&#31361;&#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#25552;&#20379;&#20915;&#31574;&#32467;&#26524;&#21644;&#38468;&#21152;&#30340;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#20026;&#36825;&#20010;&#38382;&#39064;&#24320;&#21457;&#20102;&#19968;&#31181;Evidential Conflictive Multi-view Learning (ECML)&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16897v1 Announce Type: cross  Abstract: Multi-view learning aims to combine multiple features to achieve more comprehensive descriptions of data. Most previous works assume that multiple views are strictly aligned. However, real-world multi-view data may contain low-quality conflictive instances, which show conflictive information in different views. Previous methods for this problem mainly focus on eliminating the conflictive data instances by removing them or replacing conflictive views. Nevertheless, real-world applications usually require making decisions for conflictive instances rather than only eliminating them. To solve this, we point out a new Reliable Conflictive Multi-view Learning (RCML) problem, which requires the model to provide decision results and attached reliabilities for conflictive multi-view data. We develop an Evidential Conflictive Multi-view Learning (ECML) method for this problem. ECML first learns view-specific evidence, which could be termed as th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#26408;&#39532;&#31614;&#21517;&#30340;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#26408;&#39532;&#31614;&#21517;&#26080;&#27861;&#25512;&#24191;&#21040;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#30740;&#31350;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.16896</link><description>&lt;p&gt;
&#20851;&#20110;&#20195;&#30721;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26408;&#39532;&#31614;&#21517;
&lt;/p&gt;
&lt;p&gt;
On Trojan Signatures in Large Language Models of Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#26408;&#39532;&#31614;&#21517;&#30340;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#26408;&#39532;&#31614;&#21517;&#26080;&#27861;&#25512;&#24191;&#21040;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#30740;&#31350;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26408;&#39532;&#31614;&#21517;&#26159;&#30001;Fields&#31561;&#20154;(2021)&#25551;&#36848;&#30340;&#65292;&#26159;&#34987;&#24863;&#26579;&#31867;&#21035;&#21442;&#25968;&#65288;&#26435;&#37325;&#65289;&#19982;&#26410;&#34987;&#24863;&#26579;&#31867;&#21035;&#21442;&#25968;&#20043;&#38388;&#20998;&#24067;&#30340;&#26174;&#33879;&#24046;&#24322;&#65292;&#21487;&#20197;&#29992;&#20110;&#26816;&#27979;&#34987;&#24863;&#26579;&#30340;&#27169;&#22411;&#12290;Fields&#31561;&#20154;(2021)&#21457;&#29616;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26408;&#39532;&#31614;&#21517;&#65292;&#27604;&#22914;Resnet&#12289;WideResnet&#12289;Densenet&#21644;VGG&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#28304;&#20195;&#30721;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#20998;&#31867;&#23618;&#21442;&#25968;&#30340;&#36825;&#31181;&#31614;&#21517;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26408;&#39532;&#31614;&#21517;&#19981;&#33021;&#27867;&#21270;&#21040;&#20195;&#30721;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#26356;&#26126;&#30830;&#30340;&#35774;&#32622;&#19979;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#20013;&#27602;&#65288;&#29992;&#39044;&#35757;&#32451;&#30340;&#26435;&#37325;&#20923;&#32467;&#36827;&#34892;&#24494;&#35843;&#65289;&#65292;&#34987;&#24863;&#26579;&#30340;&#20195;&#30721;&#27169;&#22411;&#20063;&#20173;&#28982;&#22266;&#25191;&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;&#20108;&#36827;&#21046;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#20102;&#20061;&#20010;&#24863;&#26579;&#27169;&#22411;&#30340;&#20998;&#26512;&#65306;&#20811;&#38534;&#21644;&#32570;&#38519;&#26816;&#27979;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26816;&#26597;&#22522;&#20110;&#26435;&#37325;&#30340;&#26408;&#39532;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16896v1 Announce Type: cross  Abstract: Trojan signatures, as described by Fields et al. (2021), are noticeable differences in the distribution of the trojaned class parameters (weights) and the non-trojaned class parameters of the trojaned model, that can be used to detect the trojaned model. Fields et al. (2021) found trojan signatures in computer vision classification tasks with image models, such as, Resnet, WideResnet, Densenet, and VGG. In this paper, we investigate such signatures in the classifier layer parameters of large language models of source code.   Our results suggest that trojan signatures could not generalize to LLMs of code. We found that trojaned code models are stubborn, even when the models were poisoned under more explicit settings (finetuned with pre-trained weights frozen). We analyzed nine trojaned models for two binary classification tasks: clone and defect detection. To the best of our knowledge, this is the first work to examine weight-based troj
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#35299;&#20915;&#36328;&#38382;&#39064;&#27867;&#21270;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#36890;&#36807;&#23558;VRPs&#23450;&#20041;&#20026;&#20849;&#20139;&#22522;&#30784;&#23646;&#24615;&#30340;&#19981;&#21516;&#32452;&#21512;&#65292;&#24182;&#36890;&#36807;&#23646;&#24615;&#32452;&#21512;&#21516;&#26102;&#35299;&#20915;&#23427;&#20204;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#27867;&#21270;&#30340;&#36335;&#24452;&#38382;&#39064;&#35299;&#20915;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.16891</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#29992;&#20110;&#20855;&#26377;&#36328;&#38382;&#39064;&#38646;&#26679;&#26412;&#27867;&#21270;&#30340;&#36335;&#24452;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Learning for Routing Problem with Cross-Problem Zero-Shot Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#35299;&#20915;&#36328;&#38382;&#39064;&#27867;&#21270;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#36890;&#36807;&#23558;VRPs&#23450;&#20041;&#20026;&#20849;&#20139;&#22522;&#30784;&#23646;&#24615;&#30340;&#19981;&#21516;&#32452;&#21512;&#65292;&#24182;&#36890;&#36807;&#23646;&#24615;&#32452;&#21512;&#21516;&#26102;&#35299;&#20915;&#23427;&#20204;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#27867;&#21270;&#30340;&#36335;&#24452;&#38382;&#39064;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;VRPs&#65289;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#37117;&#33021;&#25214;&#21040;&#65292;&#24050;&#32463;&#25104;&#20026;&#20960;&#21313;&#24180;&#30340;&#37325;&#35201;&#30740;&#31350;&#35838;&#39064;&#12290;&#26368;&#36817;&#65292;&#21033;&#29992;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#22411;&#26469;&#35299;&#20915;VRPs&#30340;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#65288;NCO&#65289;&#26041;&#27861;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;NCO&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#20026;&#27599;&#20010;&#36335;&#24452;&#38382;&#39064;&#26500;&#24314;&#19968;&#20010;&#27169;&#22411;&#65292;&#36825;&#26174;&#33879;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#20855;&#26377;&#19981;&#21516;&#23646;&#24615;&#30340;&#30495;&#23454;&#24037;&#19994;&#38382;&#39064;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#35299;&#20915;&#36328;&#38382;&#39064;&#27867;&#21270;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;VRPs&#23450;&#20041;&#20026;&#19968;&#32452;&#20849;&#20139;&#30340;&#22522;&#30784;&#23646;&#24615;&#30340;&#19981;&#21516;&#32452;&#21512;&#65292;&#24182;&#36890;&#36807;&#23646;&#24615;&#32452;&#21512;&#21516;&#26102;&#36890;&#36807;&#21333;&#19968;&#27169;&#22411;&#35299;&#20915;&#23427;&#20204;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#33021;&#22815;&#25104;&#21151;&#35299;&#20915;&#20855;&#26377;&#26410;&#35265;&#23646;&#24615;&#32452;&#21512;&#30340;VRPs&#65292;&#23454;&#29616;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16891v1 Announce Type: cross  Abstract: Vehicle routing problems (VRPs), which can be found in numerous real-world applications, have been an important research topic for several decades. Recently, the neural combinatorial optimization (NCO) approach that leverages a learning-based model to solve VRPs without manual algorithm design has gained substantial attention. However, current NCO methods typically require building one model for each routing problem, which significantly hinders their practical application for real-world industry problems with diverse attributes. In this work, we make the first attempt to tackle the crucial challenge of cross-problem generalization. In particular, we formulate VRPs as different combinations of a set of shared underlying attributes and solve them simultaneously via a single model through attribute composition. In this way, our proposed model can successfully solve VRPs with unseen attribute combinations in a zero-shot generalization mann
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20877;&#29983;&#25104;&#35782;&#21035;&#27169;&#22411;&#25968;&#25454;&#25152;&#26377;&#26435;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#25968;&#23383;&#27700;&#21360;&#25216;&#26415;&#21487;&#33021;&#30772;&#22351;&#36755;&#20986;&#36136;&#37327;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.16889</link><description>&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#20855;&#26377;&#33258;&#36523;&#27700;&#21360;&#65306;&#36890;&#36807;&#20877;&#29983;&#25104;&#22768;&#26126;&#27169;&#22411;&#35748;&#35777;
&lt;/p&gt;
&lt;p&gt;
Generative Models are Self-Watermarked: Declaring Model Authentication through Re-Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16889
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20877;&#29983;&#25104;&#35782;&#21035;&#27169;&#22411;&#25968;&#25454;&#25152;&#26377;&#26435;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#25968;&#23383;&#27700;&#21360;&#25216;&#26415;&#21487;&#33021;&#30772;&#22351;&#36755;&#20986;&#36136;&#37327;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#21644;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#20869;&#23481;&#19981;&#26029;&#22686;&#21152;&#65292;&#20445;&#25252;&#29983;&#25104;&#27169;&#22411;&#30340;&#30693;&#35782;&#20135;&#26435;&#24050;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#39564;&#35777;&#25968;&#25454;&#25152;&#26377;&#26435;&#22312;&#26410;&#32463;&#25480;&#26435;&#37325;&#22797;&#20351;&#29992;&#29983;&#25104;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#38754;&#20020;&#30528;&#24040;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#33268;&#21147;&#20110;&#26816;&#27979;&#21363;&#20351;&#26159;&#20010;&#21035;&#26679;&#26412;&#30340;&#25968;&#25454;&#37325;&#22797;&#20351;&#29992;&#12290;&#20256;&#32479;&#19978;&#65292;&#25968;&#23383;&#27700;&#21360;&#25216;&#26415;&#34987;&#21033;&#29992;&#26469;&#26816;&#27979;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#19982;&#23558;&#38468;&#21152;&#20449;&#24687;&#23884;&#20837;&#27169;&#22411;&#25110;&#29983;&#25104;&#20869;&#23481;&#20197;&#20316;&#20026;&#35302;&#21457;&#22120;&#30340;&#27700;&#21360;&#25216;&#26415;&#19981;&#21516;&#65292;&#28508;&#22312;&#25439;&#23475;&#36755;&#20986;&#36136;&#37327;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#37325;&#26032;&#29983;&#25104;&#35782;&#21035;&#22312;&#36755;&#20986;&#20013;&#22266;&#26377;&#23384;&#22312;&#30340;&#28508;&#22312;&#25351;&#32441;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#39564;&#35777;&#36807;&#31243;&#65292;&#36890;&#36807;&#20877;&#29983;&#25104;&#26469;&#24402;&#23646;&#25968;&#25454;&#25152;&#26377;&#26435;&#65292;&#36827;&#19968;&#27493;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16889v1 Announce Type: cross  Abstract: As machine- and AI-generated content proliferates, protecting the intellectual property of generative models has become imperative, yet verifying data ownership poses formidable challenges, particularly in cases of unauthorized reuse of generated data. The challenge of verifying data ownership is further amplified by using Machine Learning as a Service (MLaaS), which often functions as a black-box system.   Our work is dedicated to detecting data reuse from even an individual sample. Traditionally, watermarking has been leveraged to detect AI-generated content. However, unlike watermarking techniques that embed additional information as triggers into models or generated content, potentially compromising output quality, our approach identifies latent fingerprints inherently present within the outputs through re-generation. We propose an explainable verification procedure that attributes data ownership through re-generation, and further 
&lt;/p&gt;</description></item><item><title>&#37325;&#24314;&#28151;&#27788;&#21560;&#24341;&#23376;&#26102;&#65292;&#38750;&#32806;&#21512;&#33410;&#28857;&#30340;&#27700;&#24211;&#27604;&#22797;&#26434;&#27700;&#24211;&#26356;&#21487;&#38752;&#20135;&#29983;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#19988;&#36739;&#23567;&#30340;&#35889;&#21322;&#24452;&#26377;&#21161;&#20110;&#25913;&#36827;&#26367;&#20195;&#31995;&#32479;&#30340;&#21560;&#24341;&#23376;&#37325;&#24314;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.16888</link><description>&lt;p&gt;
&#21033;&#29992;&#23567;&#27700;&#24211;&#37325;&#24314;&#28151;&#27788;&#21560;&#24341;&#23376;-&#25299;&#25169;&#32467;&#26500;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Chaotic attractor reconstruction using small reservoirs - the influence of topology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16888
&lt;/p&gt;
&lt;p&gt;
&#37325;&#24314;&#28151;&#27788;&#21560;&#24341;&#23376;&#26102;&#65292;&#38750;&#32806;&#21512;&#33410;&#28857;&#30340;&#27700;&#24211;&#27604;&#22797;&#26434;&#27700;&#24211;&#26356;&#21487;&#38752;&#20135;&#29983;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#19988;&#36739;&#23567;&#30340;&#35889;&#21322;&#24452;&#26377;&#21161;&#20110;&#25913;&#36827;&#26367;&#20195;&#31995;&#32479;&#30340;&#21560;&#24341;&#23376;&#37325;&#24314;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38656;&#35201;&#22522;&#20110;&#27979;&#37327;&#25968;&#25454;&#23545;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#39044;&#27979;&#65292;&#22312;&#24191;&#27867;&#30340;&#24212;&#29992;&#20013;&#26159;&#24517;&#38656;&#30340;&#65292;&#24182;&#19988;&#24050;&#32463;&#25104;&#20026;&#24191;&#27867;&#30740;&#31350;&#30340;&#35838;&#39064;&#12290;&#20854;&#20013;&#19968;&#20010;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#26159;&#23545;&#28151;&#27788;&#21160;&#21147;&#23398;&#29983;&#25104;&#30340;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#39044;&#27979;&#12290;&#36817;&#24180;&#26469;&#65292;&#20648;&#23618;&#35745;&#31639;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#39044;&#27979;&#28151;&#27788;&#21160;&#21147;&#23398;&#24182;&#37325;&#24314;&#28151;&#27788;&#21560;&#24341;&#23376;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26397;&#30528;&#26356;&#23567;&#12289;&#26356;&#20302;&#22797;&#26434;&#24230;&#30340;&#27700;&#24211;&#36808;&#20986;&#20102;&#27493;&#20240;&#65292;&#30446;&#26631;&#26159;&#25552;&#39640;&#30828;&#20214;&#23454;&#29616;&#24615;&#33021;&#65292;&#24182;&#26356;&#21487;&#38752;&#22320;&#29983;&#20135;&#20986;&#36275;&#22815;&#28385;&#24847;&#30340;&#26367;&#20195;&#27169;&#22411;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#19968;&#20010;&#38750;&#32806;&#21512;&#33410;&#28857;&#30340;&#27700;&#24211;&#27604;&#22797;&#26434;&#27700;&#24211;&#25299;&#25169;&#26356;&#21487;&#38752;&#22320;&#20135;&#29983;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#38750;&#32806;&#21512;&#27700;&#24211;&#25913;&#36827;&#30340;&#21560;&#24341;&#23376;&#37325;&#24314;&#19982;&#25152;&#24471;&#26367;&#20195;&#31995;&#32479;&#30340;&#36739;&#23567;&#35889;&#21322;&#24452;&#32852;&#31995;&#36215;&#26469;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#33410;&#28857;&#24230;&#22312;&#30830;&#23450;&#27700;&#24211;&#30340;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16888v1 Announce Type: new  Abstract: Forecasting timeseries based upon measured data is needed in a wide range of applications and has been the subject of extensive research. A particularly challenging task is the forecasting of timeseries generated by chaotic dynamics. In recent years reservoir computing has been shown to be an effective method of forecasting chaotic dynamics and reconstructing chaotic attractors from data. In this work strides are made toward smaller and lower complexity reservoirs with the goal of improved hardware implementability and more reliable production of adequate surrogate models. We show that a reservoir of uncoupled nodes more reliably produces long term timeseries predictions than complex reservoir topologies. We then link the improved attractor reconstruction of the uncoupled reservoir with smaller spectral radii of the resulting surrogate systems. These results indicate that, the node degree plays an important role in determining whether th
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#19982;&#20016;&#23500;&#30495;&#23454;&#32593;&#32476;&#25968;&#25454;&#30340;&#23384;&#22312;&#24320;&#21551;&#20102;&#22797;&#26434;&#32593;&#32476;&#31185;&#23398;&#30740;&#31350;&#30340;&#26032;&#26102;&#20195;&#65292;&#26377;&#26395;&#20811;&#26381;&#29616;&#23384;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.16887</link><description>&lt;p&gt;
&#22797;&#26434;&#32593;&#32476;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#28508;&#21147;&#12289;&#26041;&#27861;&#35770;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence for Complex Network: Potential, Methodology and Application
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16887
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#19982;&#20016;&#23500;&#30495;&#23454;&#32593;&#32476;&#25968;&#25454;&#30340;&#23384;&#22312;&#24320;&#21551;&#20102;&#22797;&#26434;&#32593;&#32476;&#31185;&#23398;&#30740;&#31350;&#30340;&#26032;&#26102;&#20195;&#65292;&#26377;&#26395;&#20811;&#26381;&#29616;&#23384;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#32593;&#32476;&#23384;&#22312;&#20110;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#31995;&#32479;&#20013;&#65292;&#20174;&#33258;&#28982;&#29615;&#22659;&#21040;&#20154;&#31867;&#31038;&#20250;&#12290;&#36825;&#20123;&#32593;&#32476;&#30340;&#26412;&#36136;&#22312;&#20110;&#23427;&#20204;&#33021;&#22815;&#20174;&#24494;&#35266;&#28151;&#20081;-&#20854;&#20013;&#32593;&#32476;&#25299;&#25169;&#21644;&#33410;&#28857;&#21160;&#24577;&#20132;&#32455;-&#36716;&#21464;&#21644;&#28436;&#21270;&#20026;&#20855;&#26377;&#29305;&#23450;&#38598;&#20307;&#34892;&#20026;&#30340;&#23439;&#35266;&#31209;&#24207;&#12290;&#22312;&#36807;&#21435;&#30340;&#20108;&#21313;&#24180;&#37324;&#65292;&#22797;&#26434;&#32593;&#32476;&#31185;&#23398;&#26174;&#33879;&#22686;&#24378;&#20102;&#25105;&#20204;&#23545;&#30495;&#23454;&#19990;&#30028;&#32593;&#32476;&#28508;&#22312;&#26426;&#21046;&#12289;&#32467;&#26500;&#21644;&#21160;&#24577;&#30340;&#29702;&#35299;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#22312;&#25506;&#32034;&#26356;&#21152;&#30495;&#23454;&#31995;&#32479;&#21644;&#25552;&#21319;&#23454;&#38469;&#24212;&#29992;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#30528;&#30456;&#24403;&#22823;&#30340;&#25361;&#25112;&#12290;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#20986;&#29616;&#65292;&#20197;&#21450;&#20016;&#23500;&#22810;&#26679;&#30340;&#30495;&#23454;&#19990;&#30028;&#32593;&#32476;&#25968;&#25454;&#30340;&#23384;&#22312;&#65292;&#24320;&#21551;&#20102;&#22797;&#26434;&#32593;&#32476;&#31185;&#23398;&#30740;&#31350;&#30340;&#26032;&#26102;&#20195;&#12290;&#26412;&#35843;&#26597;&#26088;&#22312;&#31995;&#32479;&#22320;&#25506;&#35752;&#20154;&#24037;&#26234;&#33021;&#22312;&#20811;&#26381;&#22797;&#26434;&#32593;&#32476;&#31185;&#23398;&#30740;&#31350;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#26041;&#38754;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16887v1 Announce Type: cross  Abstract: Complex networks pervade various real-world systems, from the natural environment to human societies. The essence of these networks is in their ability to transition and evolve from microscopic disorder-where network topology and node dynamics intertwine-to a macroscopic order characterized by certain collective behaviors. Over the past two decades, complex network science has significantly enhanced our understanding of the statistical mechanics, structures, and dynamics underlying real-world networks. Despite these advancements, there remain considerable challenges in exploring more realistic systems and enhancing practical applications. The emergence of artificial intelligence (AI) technologies, coupled with the abundance of diverse real-world network data, has heralded a new era in complex network science research. This survey aims to systematically address the potential advantages of AI in overcoming the lingering challenges of com
&lt;/p&gt;</description></item><item><title>&#21521;&#37327;&#25968;&#25454;&#24211;&#21644;&#23884;&#20837;&#27169;&#22411;&#30340;&#24212;&#29992;&#20026;&#25991;&#26412;&#20998;&#31867;&#22120;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#26041;&#24335;&#26469;&#34920;&#36798;&#25968;&#25454;&#27169;&#24335;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#39046;&#22495;&#20013;&#24320;&#22987;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.16886</link><description>&lt;p&gt;
&#20351;&#29992;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#21644;&#21521;&#37327;&#25968;&#25454;&#24211;&#20316;&#20026;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#30740;&#31350;&#65292;&#20197;&#21307;&#30103;&#25968;&#25454;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Using text embedding models and vector databases as text classifiers with the example of medical data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16886
&lt;/p&gt;
&lt;p&gt;
&#21521;&#37327;&#25968;&#25454;&#24211;&#21644;&#23884;&#20837;&#27169;&#22411;&#30340;&#24212;&#29992;&#20026;&#25991;&#26412;&#20998;&#31867;&#22120;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#26041;&#24335;&#26469;&#34920;&#36798;&#25968;&#25454;&#27169;&#24335;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#39046;&#22495;&#20013;&#24320;&#22987;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#26159;&#20196;&#20154;&#20852;&#22859;&#30340;&#65292;&#24182;&#24050;&#22312;&#35768;&#22810;&#39046;&#22495;&#25214;&#21040;&#24212;&#29992;&#65292;&#20294;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#21307;&#23398;&#39046;&#22495;&#30340;&#26631;&#20934;&#35201;&#27714;&#38750;&#24120;&#39640;&#12290;&#19982;LLMs&#37197;&#21512;&#20351;&#29992;&#65292;&#21521;&#37327;&#23884;&#20837;&#27169;&#22411;&#21644;&#21521;&#37327;&#25968;&#25454;&#24211;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#24335;&#26469;&#34920;&#36798;&#21508;&#31181;&#25968;&#25454;&#27169;&#24335;&#65292;&#36825;&#20123;&#25968;&#25454;&#27169;&#24335;&#23481;&#26131;&#34987;&#20856;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#29702;&#35299;&#12290;&#38500;&#20102;&#26041;&#20415;&#22320;&#21521;&#36825;&#20123;&#21521;&#37327;&#25968;&#25454;&#24211;&#28155;&#21152;&#20449;&#24687;&#12289;&#30693;&#35782;&#21644;&#25968;&#25454;&#22806;&#65292;&#23427;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20196;&#20154;&#20449;&#26381;&#30340;&#29702;&#30001;&#65292;&#21363;&#23558;&#20854;&#24212;&#29992;&#20110;&#36890;&#24120;&#30001;&#20154;&#31867;&#23436;&#25104;&#30340;&#26816;&#32034;&#20449;&#24687;&#20219;&#21153;&#30340;&#21508;&#31181;&#39046;&#22495;&#12290;Google&#30340;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#19968;&#20010;&#28165;&#26224;&#30340;&#26367;&#20195;&#27169;&#22411;Med-PaLM&#65292;&#19987;&#38376;&#26088;&#22312;&#19982;&#20020;&#24202;&#21307;&#24072;&#30340;&#21307;&#23398;&#30693;&#35782;&#27700;&#24179;&#21305;&#37197;&#12290;&#22312;&#35757;&#32451;&#20998;&#31867;&#22120;&#21644;&#24320;&#21457;&#27169;&#22411;&#26102;&#65292;&#20445;&#25345;&#20107;&#23454;&#21644;&#20943;&#23569;&#20559;&#35265;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21521;&#37327;&#25968;&#25454;&#24211;&#21644;&#23884;&#20837;&#27169;&#22411;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16886v1 Announce Type: cross  Abstract: The advent of Large Language Models (LLMs) is promising and has found application in numerous fields, but as it often is with the medical field, the bar is typically quite high [5]. In tandem with LLMs, vector embedding models and vector databases provide a robust way of expressing numerous modes of data that are easily digestible by typical machine learning models. Along with the ease of adding information, knowledge, and data to these vector databases, they provide a compelling reason to apply them in numerous fields where the task of retrieving information is typically done by humans. Researchers at Google have developed a clear alternative model, Med-PaLM [6] specifically designed to match a clinician's level of accuracy when it comes to medical knowledge. When training classifiers, and developing models, it is imperative to maintain factuality and reduce bias [4]. Here, we explore the use of vector databases and embedding models a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#24213;&#29289;&#33539;&#22260;&#23545;&#27604;&#23398;&#20064;&#65292;&#20197;&#23398;&#20064;&#36866;&#21512;&#21270;&#23398;&#21453;&#24212;&#24615;&#30340;&#21407;&#23376;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.16882</link><description>&lt;p&gt;
&#24213;&#29289;&#33539;&#22260;&#23545;&#27604;&#23398;&#20064;&#65306;&#37325;&#26032;&#21033;&#29992;&#20154;&#31867;&#20559;&#35265;&#23398;&#20064;&#21407;&#23376;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Substrate Scope Contrastive Learning: Repurposing Human Bias to Learn Atomic Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16882
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#24213;&#29289;&#33539;&#22260;&#23545;&#27604;&#23398;&#20064;&#65292;&#20197;&#23398;&#20064;&#36866;&#21512;&#21270;&#23398;&#21453;&#24212;&#24615;&#30340;&#21407;&#23376;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20998;&#23376;&#34920;&#31034;&#26159;&#20998;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#23545;&#24314;&#27169;&#25104;&#21151;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#65292;&#23588;&#20854;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#12290;&#24191;&#20041;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#27010;&#24565;&#25512;&#21160;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#34507;&#30333;&#36136;&#24037;&#31243;&#31561;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#31867;&#20284;&#30340;&#26041;&#27861;&#22312;&#23567;&#26377;&#26426;&#20998;&#23376;&#26041;&#38754;&#24182;&#26410;&#21462;&#24471;&#31867;&#20284;&#30340;&#25104;&#21151;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#21363;&#24213;&#29289;&#33539;&#22260;&#23545;&#27604;&#23398;&#20064;&#65292;&#23427;&#23398;&#20064;&#36866;&#21512;&#21270;&#23398;&#21453;&#24212;&#24615;&#30340;&#21407;&#23376;&#34920;&#31034;&#12290;&#36825;&#31181;&#26041;&#27861;&#20197;&#24050;&#21457;&#34920;&#30340;&#24213;&#29289;&#33539;&#22260;&#34920;&#20013;&#24213;&#29289;&#30340;&#20998;&#32452;&#21644;&#20135;&#29289;&#25910;&#29575;&#20316;&#20026;&#21270;&#23398;&#21453;&#24212;&#24615;&#30456;&#20284;&#24615;&#25110;&#19981;&#30456;&#20284;&#24615;&#30340;&#34913;&#37327;&#12290;&#25105;&#20204;&#20851;&#27880; CAS Content Collection &#20013;&#30340; 20,798 &#20010;&#33459;&#39321;&#21348;&#20195;&#28867;&#65292;&#28085;&#30422;&#25968;&#21315;&#31687;&#20986;&#29256;&#29289;&#65292;&#20197;&#23398;&#20064;&#33459;&#39321;&#21348;&#20195;&#28867;&#30340;&#21453;&#24212;&#24615;&#34920;&#31034;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16882v1 Announce Type: cross  Abstract: Learning molecular representation is a critical step in molecular machine learning that significantly influences modeling success, particularly in data-scarce situations. The concept of broadly pre-training neural networks has advanced fields such as computer vision, natural language processing, and protein engineering. However, similar approaches for small organic molecules have not achieved comparable success. In this work, we introduce a novel pre-training strategy, substrate scope contrastive learning, which learns atomic representations tailored to chemical reactivity. This method considers the grouping of substrates and their yields in published substrate scope tables as a measure of their similarity or dissimilarity in terms of chemical reactivity. We focus on 20,798 aryl halides in the CAS Content Collection spanning thousands of publications to learn a representation of aryl halide reactivity. We validate our pre-training appr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BESA&#30340;&#26032;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20462;&#21098;&#25216;&#26415;&#65292;&#36890;&#36807;&#24212;&#29992;&#20998;&#22359;&#37325;&#26500;&#25439;&#22833;&#65292;&#19982;&#20256;&#32479;&#30340;&#36880;&#23618;&#20462;&#21098;&#25216;&#26415;&#19981;&#21516;&#65292;BESA&#20855;&#26377;&#20248;&#21183;</title><link>https://arxiv.org/abs/2402.16880</link><description>&lt;p&gt;
BESA: &#20351;&#29992;&#20998;&#22359;&#21442;&#25968;&#39640;&#25928;&#31232;&#30095;&#20998;&#37197;&#20462;&#21098;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16880
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BESA&#30340;&#26032;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20462;&#21098;&#25216;&#26415;&#65292;&#36890;&#36807;&#24212;&#29992;&#20998;&#22359;&#37325;&#26500;&#25439;&#22833;&#65292;&#19982;&#20256;&#32479;&#30340;&#36880;&#23618;&#20462;&#21098;&#25216;&#26415;&#19981;&#21516;&#65292;BESA&#20855;&#26377;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#25688;&#35201;&#12289;&#25991;&#26412;&#38382;&#31572;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#24615;&#33021;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#65292;&#20294;&#30001;&#20110;&#22823;&#37327;&#21442;&#25968;&#36896;&#25104;&#30340;&#35745;&#31639;&#21344;&#29992;&#21487;&#33021;&#26159;&#31105;&#38178;&#30340;&#12290;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#65288;&#22914;SparseGPT&#21644;Wanda&#65289;&#23581;&#35797;&#36890;&#36807;&#26435;&#37325;&#20462;&#21098;&#32531;&#35299;&#27492;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#36880;&#23618;&#26041;&#27861;&#20250;&#23548;&#33268;&#27169;&#22411;&#36755;&#20986;&#26174;&#33879;&#25200;&#21160;&#65292;&#24182;&#38656;&#35201;&#32454;&#33268;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#22914;&#20462;&#21098;&#36895;&#29575;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#25972;&#20307;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#20026;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#20462;&#21098;&#25216;&#26415;&#65292;&#31216;&#20026;&#20998;&#22359;&#21442;&#25968;&#39640;&#25928;&#31232;&#30095;&#20998;&#37197;&#65288;BESA&#65289;&#65292;&#36890;&#36807;&#24212;&#29992;&#20998;&#22359;&#37325;&#26500;&#25439;&#22833;&#12290;&#19982;&#20856;&#22411;&#30340;&#36880;&#23618;&#20462;&#21098;&#25216;&#26415;&#30456;&#27604;&#65292;BESA&#20855;&#26377;&#20004;&#20010;&#29420;&#29305;&#30340;&#29305;&#28857;&#65306;i&#65289;&#23427;&#23450;&#20301;&#20110;&#25972;&#20307;&#20462;&#21098;&#35823;&#24046;&#30456;&#23545;&#20110;&#27599;&#20010;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16880v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated outstanding performance in various tasks, such as text summarization, text question-answering, and etc. While their performance is impressive, the computational footprint due to their vast number of parameters can be prohibitive. Existing solutions such as SparseGPT and Wanda attempt to alleviate this issue through weight pruning. However, their layer-wise approach results in significant perturbation to the model's output and requires meticulous hyperparameter tuning, such as the pruning rate, which can adversely affect overall model performance. To address this, this paper introduces a novel LLM pruning technique dubbed blockwise parameter-efficient sparsity allocation (BESA) by applying a blockwise reconstruction loss. In contrast to the typical layer-wise pruning techniques, BESA is characterized by two distinctive attributes: i) it targets the overall pruning error with respect to indi
&lt;/p&gt;</description></item><item><title>EvoGPT-f&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#36827;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#20116;&#20010;&#24418;&#24335;&#25968;&#23398;&#35821;&#26009;&#24211;&#36827;&#34892;&#24046;&#24322;&#26426;&#22120;&#21487;&#23398;&#20064;&#24615;&#30340;&#31995;&#32479;&#37327;&#21270;&#20998;&#26512;&#65292;&#20026;&#24418;&#24335;&#25968;&#23398;&#35821;&#35328;&#30340;&#22522;&#20934;&#27979;&#35797;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.16878</link><description>&lt;p&gt;
EvoGPT-f: &#19968;&#31181;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#24418;&#24335;&#25968;&#23398;&#35821;&#35328;&#30340;&#36827;&#21270;GPT&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EvoGPT-f: An Evolutionary GPT Framework for Benchmarking Formal Math Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16878
&lt;/p&gt;
&lt;p&gt;
EvoGPT-f&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#36827;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#20116;&#20010;&#24418;&#24335;&#25968;&#23398;&#35821;&#26009;&#24211;&#36827;&#34892;&#24046;&#24322;&#26426;&#22120;&#21487;&#23398;&#20064;&#24615;&#30340;&#31995;&#32479;&#37327;&#21270;&#20998;&#26512;&#65292;&#20026;&#24418;&#24335;&#25968;&#23398;&#35821;&#35328;&#30340;&#22522;&#20934;&#27979;&#35797;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16878v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#24418;&#24335;&#25968;&#23398;&#26159;&#23558;&#25968;&#23398;&#36716;&#21270;&#20026;&#32534;&#31243;&#35821;&#35328;&#30340;&#23398;&#31185;&#65292;&#22312;&#36825;&#31181;&#32534;&#31243;&#35821;&#35328;&#20013;&#65292;&#20219;&#20309;&#38472;&#36848;&#37117;&#21487;&#20197;&#34987;&#35745;&#31639;&#26426;&#26126;&#30830;&#22320;&#26816;&#26597;&#12290;&#25968;&#23398;&#23478;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#33457;&#36153;&#20102;&#25968;&#21313;&#24180;&#36827;&#34892;&#33392;&#33510;&#30340;&#24418;&#24335;&#21270;&#24037;&#20316;&#65292;&#24320;&#21457;&#20102;&#35832;&#22914;Coq&#12289;HOL&#21644;Lean&#31561;&#35821;&#35328;&#12290;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#24050;&#32463;&#27719;&#38598;&#21040;&#36825;&#20123;&#24418;&#24335;&#21270;&#25968;&#23398;&#35821;&#26009;&#24211;&#19978;&#65292;&#24182;&#20135;&#29983;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#24110;&#21161;&#20132;&#20114;&#24335;&#21644;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35770;&#25991;&#20027;&#35201;&#38598;&#20013;&#22312;&#19968;&#20010;&#26041;&#27861;&#12289;&#19968;&#20010;&#35777;&#26126;&#20219;&#21153;&#12289;&#19968;&#20010;&#35821;&#35328;&#19978;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;EvoGPT-f: &#19968;&#31181;&#26032;&#39062;&#30340;&#36827;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#39318;&#27425;&#31995;&#32479;&#37327;&#21270;&#20998;&#26512;&#20116;&#20010;&#24418;&#24335;&#25968;&#23398;&#35821;&#26009;&#24211;(Lean 3&#12289;Lean 4&#12289;Coq&#12289;HOL 4&#12289;HOL Light)&#30340;&#24046;&#24322;&#26426;&#22120;&#21487;&#23398;&#20064;&#24615;&#65292;&#20351;&#29992;&#22235;&#31181;&#35760;&#21495;&#21270;&#26041;&#27861;(&#23383;&#31526;&#12289;&#21333;&#35789;&#32423;&#12289;&#23383;&#33410;&#23545;&#32534;&#30721;&#21644;StarCoder&#35760;&#21495;&#21270;&#22120;)&#12290;&#26412;&#25991;&#24182;&#26410;&#32467;&#26463;&#20851;&#20110;&#8220;&#26368;&#20339;&#8221;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16878v1 Announce Type: new  Abstract: Formal mathematics is the discipline of translating mathematics into a programming language in which any statement can be unequivocally checked by a computer. Mathematicians and computer scientists have spent decades of painstaking formalization efforts developing languages such as Coq, HOL, and Lean. Machine learning research has converged on these formal math corpora and given rise to an assortment of methodologies to aid in interactive and automated theorem proving. However, these papers have primarily focused on one method, for one proof task, in one language. This paper introduces EvoGPT-f: a novel evolutionary framework for the first systematic quantitative analysis of the differential machine learnability of five formal math corpora (Lean 3, Lean 4, Coq, HOL 4, HOL Light) using four tokenization methods (character, word-level, Byte Pair Encoding and StarCoder tokenizer). This paper does not put to rest the question of the "best" o
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#29983;&#25104;&#33021;&#21147;&#26469;&#21512;&#25104;&#20551;&#35774;&#32451;&#20064;&#65292;&#20197;&#24357;&#21512;&#23398;&#20064;&#32773;&#38656;&#27714;&#19982;&#32451;&#20064;&#20869;&#23481;&#20043;&#38388;&#30340;&#35821;&#20041;&#40511;&#27807;&#65292;&#25552;&#39640;&#20010;&#24615;&#21270;&#35821;&#35328;&#23398;&#20064;&#32451;&#20064;&#26816;&#32034;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.16877</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#20010;&#24615;&#21270;&#35821;&#35328;&#23398;&#20064;&#32451;&#20064;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Augmented Exercise Retrieval for Personalized Language Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16877
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#29983;&#25104;&#33021;&#21147;&#26469;&#21512;&#25104;&#20551;&#35774;&#32451;&#20064;&#65292;&#20197;&#24357;&#21512;&#23398;&#20064;&#32773;&#38656;&#27714;&#19982;&#32451;&#20064;&#20869;&#23481;&#20043;&#38388;&#30340;&#35821;&#20041;&#40511;&#27807;&#65292;&#25552;&#39640;&#20010;&#24615;&#21270;&#35821;&#35328;&#23398;&#20064;&#32451;&#20064;&#26816;&#32034;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#35821;&#35328;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#38646;&#26679;&#26412;&#32451;&#20064;&#26816;&#32034;&#38382;&#39064;&#65292;&#20197;&#36171;&#20104;&#23398;&#20064;&#32773;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#26126;&#30830;&#35831;&#27714;&#20010;&#24615;&#21270;&#32451;&#20064;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#25910;&#38598;&#33258;&#35821;&#35328;&#23398;&#20064;&#32773;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#30690;&#37327;&#30456;&#20284;&#24615;&#26041;&#27861;&#24456;&#38590;&#25429;&#25417;&#32451;&#20064;&#20869;&#23481;&#19982;&#23398;&#20064;&#32773;&#29992;&#20110;&#34920;&#36798;&#20182;&#20204;&#24819;&#35201;&#23398;&#20064;&#20869;&#23481;&#30340;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#36890;&#36807;&#22522;&#20110;&#23398;&#20064;&#32773;&#36755;&#20837;&#21512;&#25104;&#20551;&#35774;&#32451;&#20064;&#65292;&#28982;&#21518;&#29992;&#20110;&#25628;&#32034;&#30456;&#20851;&#32451;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;mHyER&#20811;&#26381;&#20102;&#19977;&#20010;&#25361;&#25112;&#65306;&#65288;1&#65289;&#32570;&#20047;&#29992;&#20110;&#35757;&#32451;&#30340;&#30456;&#20851;&#24615;&#26631;&#31614;&#65292;&#65288;2&#65289;&#21463;&#38480;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16877v1 Announce Type: cross  Abstract: We study the problem of zero-shot exercise retrieval in the context of online language learning, to give learners the ability to explicitly request personalized exercises via natural language. Using real-world data collected from language learners, we observe that vector similarity approaches poorly capture the relationship between exercise content and the language that learners use to express what they want to learn. This semantic gap between queries and content dramatically reduces the effectiveness of general-purpose retrieval models pretrained on large scale information retrieval datasets like MS MARCO. We leverage the generative capabilities of large language models to bridge the gap by synthesizing hypothetical exercises based on the learner's input, which are then used to search for relevant exercises. Our approach, which we call mHyER, overcomes three challenges: (1) lack of relevance labels for training, (2) unrestricted learn
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#30340;&#27010;&#29575;&#31163;&#25955;&#28508;&#21464;&#37327;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30524;&#37096;&#30142;&#30149;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#27969;&#32593;&#32476;&#26469;&#23398;&#20064;&#30524;&#24213;&#22270;&#20687;&#20013;&#30524;&#37096;&#30142;&#30149;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.16865</link><description>&lt;p&gt;
&#36890;&#36807;&#23558;&#21487;&#23398;&#20064;&#30340;&#27010;&#29575;&#31163;&#25955;&#28508;&#21464;&#37327;&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#25552;&#39640;&#30524;&#37096;&#30142;&#30149;&#26816;&#27979;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improve Robustness of Eye Disease Detection by including Learnable Probabilistic Discrete Latent Variables into Machine Learning Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16865
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#30340;&#27010;&#29575;&#31163;&#25955;&#28508;&#21464;&#37327;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30524;&#37096;&#30142;&#30149;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#27969;&#32593;&#32476;&#26469;&#23398;&#20064;&#30524;&#24213;&#22270;&#20687;&#20013;&#30524;&#37096;&#30142;&#30149;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30524;&#37096;&#30142;&#30149;&#20174;&#31958;&#23615;&#30149;&#24615;&#35270;&#32593;&#33180;&#30149;&#21464;&#21040;&#38738;&#20809;&#30524;&#31561;&#65292;&#30001;&#20110;&#20854;&#39640;&#21457;&#30149;&#29575;&#21644;&#21487;&#33021;&#23548;&#33268;&#35270;&#21147;&#25439;&#23475;&#65292;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#20844;&#20849;&#21355;&#29983;&#25361;&#25112;&#12290;&#21450;&#26089;&#21644;&#20934;&#30830;&#30340;&#35786;&#26029;&#23545;&#20110;&#26377;&#25928;&#27835;&#30103;&#21644;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#20998;&#26512;&#21307;&#23398;&#22270;&#20687;&#65288;&#21253;&#25324;&#30524;&#37096;&#22270;&#20687;&#65289;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#65292;&#36825;&#23545;&#20020;&#24202;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;GFlowOut&#30340;&#26032;&#39062;&#24212;&#29992;&#65292;&#21033;&#29992;&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#30340;&#27010;&#29575;&#26694;&#26550;&#26469;&#23398;&#20064;&#20851;&#20110;&#36749;&#23398;&#25513;&#30721;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#29992;&#20110;&#20351;&#29992;&#30524;&#24213;&#22270;&#20687;&#23545;&#30524;&#37096;&#30142;&#30149;&#36827;&#34892;&#20998;&#31867;&#21644;&#20998;&#26512;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31283;&#20581;&#19988;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20197;ResNet18&#21644;ViT&#27169;&#22411;&#20026;&#20027;&#24178;&#30340;GFlowOut&#26469;&#35782;&#21035;&#21508;&#31181;&#30524;&#37096;&#29366;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16865v1 Announce Type: cross  Abstract: Ocular diseases, ranging from diabetic retinopathy to glaucoma, present a significant public health challenge due to their prevalence and potential for causing vision impairment. Early and accurate diagnosis is crucial for effective treatment and management.In recent years, deep learning models have emerged as powerful tools for analysing medical images, including ocular imaging . However, challenges persist in model interpretability and uncertainty estimation, which are critical for clinical decision-making. This study introduces a novel application of GFlowOut, leveraging the probabilistic framework of Generative Flow Networks (GFlowNets) to learn the posterior distribution over dropout masks, for the classification and analysis of ocular diseases using eye fundus images. We develop a robust and generalizable method that utilizes GFlowOut integrated with ResNet18 and ViT models as backbone in identifying various ocular conditions. Th
&lt;/p&gt;</description></item><item><title>&#22312;AI&#36741;&#21161;&#30340;6G&#32593;&#32476;&#20013;&#65292;&#23454;&#29616;&#20102;&#35821;&#20041;&#12289;&#23454;&#29992;&#21644;&#30446;&#26631;&#23548;&#21521;&#36890;&#20449;&#31574;&#30053;&#30340;&#25972;&#21512;&#65292;&#36890;&#36807;&#25552;&#20986;&#25968;&#23398;&#27169;&#22411;&#35299;&#20915;&#20102;&#35821;&#35328;&#19981;&#21305;&#37197;&#23548;&#33268;&#30340;&#20986;&#38169;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.16858</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#20041;&#26377;&#25928;&#24615;&#36890;&#36947;&#38169;&#35823;&#30340;&#23454;&#29992;&#30446;&#26631;&#23548;&#21521;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Pragmatic Goal-Oriented Communications under Semantic-Effectiveness Channel Errors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16858
&lt;/p&gt;
&lt;p&gt;
&#22312;AI&#36741;&#21161;&#30340;6G&#32593;&#32476;&#20013;&#65292;&#23454;&#29616;&#20102;&#35821;&#20041;&#12289;&#23454;&#29992;&#21644;&#30446;&#26631;&#23548;&#21521;&#36890;&#20449;&#31574;&#30053;&#30340;&#25972;&#21512;&#65292;&#36890;&#36807;&#25552;&#20986;&#25968;&#23398;&#27169;&#22411;&#35299;&#20915;&#20102;&#35821;&#35328;&#19981;&#21305;&#37197;&#23548;&#33268;&#30340;&#20986;&#38169;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21363;&#23558;&#21040;&#26469;&#30340;AI&#36741;&#21161;&#30340;6G&#32593;&#32476;&#20013;&#65292;&#25972;&#21512;&#35821;&#20041;&#12289;&#23454;&#29992;&#21644;&#30446;&#26631;&#23548;&#21521;&#36890;&#20449;&#31574;&#30053;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31181;&#25972;&#21512;&#23558;&#23454;&#29616;&#20165;&#28041;&#21450;&#30456;&#20851;&#20219;&#21153;&#25968;&#25454;&#30340;&#24863;&#30693;&#12289;&#20256;&#36755;&#21644;&#22788;&#29702;&#65292;&#30830;&#20445;&#20256;&#36798;&#30340;&#20449;&#24687;&#20855;&#26377;&#21487;&#29702;&#35299;&#30340;&#12289;&#23454;&#29992;&#30340;&#35821;&#20041;&#37325;&#35201;&#24615;&#65292;&#19982;&#30446;&#26631;&#21644;&#38656;&#27714;&#30456;&#19968;&#33268;&#12290;&#22312;&#27492;&#32972;&#26223;&#19979;&#65292;&#38500;&#20102;&#20856;&#22411;&#26080;&#32447;&#36890;&#20449;&#21160;&#24577;&#24341;&#36215;&#30340;&#38169;&#35823;&#22806;&#65292;&#30001;&#20110;&#35821;&#20041;&#22788;&#29702;&#33021;&#21147;&#30340;&#38480;&#21046;&#12289;&#21457;&#23556;&#26041;&#24847;&#22270;&#21644;&#25509;&#25910;&#26041;&#35299;&#37322;&#20043;&#38388;&#30340;&#24847;&#20041;&#24046;&#24322;&#20197;&#21450;&#21457;&#23556;&#26041;&#21644;&#25509;&#25910;&#26041;&#20043;&#38388;&#30340;&#35821;&#35328;&#21644;&#30693;&#35782;&#34920;&#24449;&#24046;&#24322;&#65292;&#36824;&#21487;&#33021;&#20986;&#29616;&#21457;&#23556;&#26041;&#24847;&#22270;&#21644;&#25509;&#25910;&#26041;&#35299;&#37322;&#20043;&#38388;&#30340;&#28508;&#22312;&#22833;&#30495;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26377;&#20004;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#23427;&#25552;&#20986;&#24182;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#28304;&#20110;&#35821;&#35328;&#19981;&#21305;&#37197;&#30340;&#38169;&#35823;&#65292;&#21253;&#25324;&#35821;&#20041;&#21644;&#26377;&#25928;&#24615;&#32423;&#21035;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16858v1 Announce Type: cross  Abstract: In forthcoming AI-assisted 6G networks, integrating semantic, pragmatic, and goal-oriented communication strategies becomes imperative. This integration will enable sensing, transmission, and processing of exclusively pertinent task data, ensuring conveyed information possesses understandable, pragmatic semantic significance, aligning with destination needs and goals. Without doubt, no communication is error free. Within this context, besides errors stemming from typical wireless communication dynamics, potential distortions between transmitter-intended and receiver-interpreted meanings can emerge due to limitations in semantic processing capabilities, as well as language and knowledge representation disparities between transmitters and receivers. The main contribution of this paper is two-fold. First, it proposes and details a novel mathematical modeling of errors stemming from language mismatches at both semantic and effectiveness le
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#19982;&#21367;&#31215;&#32593;&#32476;&#20998;&#23618;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#20998;&#23376;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#37325;&#24314;&#24050;&#30693;&#20998;&#23376;&#26102;&#33719;&#24471;95%&#30340;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#33021;&#22815;&#23454;&#29616;&#22312;SMILES&#23383;&#31526;&#20018;&#21644;&#20854;&#23398;&#20064;&#34920;&#31034;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;</title><link>https://arxiv.org/abs/2402.16854</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20998;&#23618;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#29983;&#25104;&#20998;&#23376;
&lt;/p&gt;
&lt;p&gt;
Attention Based Molecule Generation via Hierarchical Variational Autoencoder
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16854
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#19982;&#21367;&#31215;&#32593;&#32476;&#20998;&#23618;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#20998;&#23376;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#37325;&#24314;&#24050;&#30693;&#20998;&#23376;&#26102;&#33719;&#24471;95%&#30340;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#33021;&#22815;&#23454;&#29616;&#22312;SMILES&#23383;&#31526;&#20018;&#21644;&#20854;&#23398;&#20064;&#34920;&#31034;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#29983;&#25104;&#36825;&#19968;&#20219;&#21153;&#22312;&#35745;&#31639;&#19978;&#33021;&#34920;&#31034;&#20998;&#23376;&#30340;&#22797;&#26434;&#26041;&#24335;&#20013;&#21464;&#24471;&#21313;&#20998;&#22256;&#38590;&#12290;&#22312;&#20998;&#23376;&#29983;&#25104;&#27169;&#22411;&#20013;&#24120;&#29992;&#30340;&#25216;&#26415;&#26159;&#20351;&#29992;&#24102;&#26377;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;SMILES&#23383;&#31526;&#20018;&#65292;&#20869;&#32622;&#21040;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#20013;&#65292;&#20294;&#36825;&#20123;&#25216;&#26415;&#23384;&#22312;&#35768;&#22810;&#38382;&#39064;&#65306;&#26799;&#24230;&#28040;&#22833;&#65292;&#38271;&#36317;&#31163;&#36951;&#24536;&#21644;&#26080;&#25928;&#20998;&#23376;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20197;&#20998;&#23618;&#26041;&#24335;&#23558;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#19982;&#21367;&#31215;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#33021;&#22815;&#20174;SMILES&#23383;&#31526;&#20018;&#20013;&#25552;&#21462;&#33258;&#22238;&#24402;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#20449;&#21495;&#21644;&#38271;&#36317;&#31163;&#20381;&#36182;&#24615;&#12290;&#36825;&#20351;&#24471;&#22312;&#37325;&#24314;&#24050;&#30693;&#20998;&#23376;&#26102;&#29983;&#25104;&#30340;&#20998;&#23376;&#20855;&#26377;&#38750;&#24120;&#39640;&#30340;&#26377;&#25928;&#24615;&#65292;&#36798;&#21040;&#20102;95%&#30340;&#27700;&#24179;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#27979;&#35797;&#38598;&#21644;&#37325;&#24314;&#20998;&#23376;&#20043;&#38388;&#30340;&#24179;&#22343;Tanimoto&#30456;&#20284;&#24230;&#20026;0.6&#65292;&#36825;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;SMILES&#23383;&#31526;&#20018;&#21644;&#23427;&#20204;&#30340;&#23398;&#20064;&#34920;&#31034;&#20043;&#38388;&#36827;&#34892;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16854v1 Announce Type: cross  Abstract: Molecule generation is a task made very difficult by the complex ways in which we represent molecules computationally. A common technique used in molecular generative modeling is to use SMILES strings with recurrent neural networks built into variational autoencoders - but these suffer from a myriad of issues: vanishing gradients, long-range forgetting, and invalid molecules. In this work, we show that by combining recurrent neural networks with convolutional networks in a hierarchical manner, we are able to both extract autoregressive information from SMILES strings while maintaining signal and long-range dependencies. This allows for generations with very high validity rates on the order of 95% when reconstructing known molecules. We also observe an average Tanimoto similarity of .6 between test set and reconstructed molecules, which suggests our method is able to map between SMILES strings and their learned representations in a more
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#24494;&#35843;&#36807;&#31243;&#20013;&#23384;&#22312;&#30528;&#20302;&#31209;&#36866;&#37197;&#22120;&#30697;&#38453;&#37325;&#35201;&#24615;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#26356;&#26032;&#21442;&#25968;&#30697;&#38453;&#26102;&#65292;$B$&#21644;$A$&#30697;&#38453;&#20855;&#26377;&#19981;&#21516;&#21151;&#33021;&#65292;&#24494;&#35843;$B$&#27604;&#24494;&#35843;$A$&#26356;&#21152;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.16842</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#20302;&#31209;&#36866;&#37197;&#22120;&#30340;&#19981;&#23545;&#31216;&#24615;
&lt;/p&gt;
&lt;p&gt;
Asymmetry in Low-Rank Adapters of Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#24494;&#35843;&#36807;&#31243;&#20013;&#23384;&#22312;&#30528;&#20302;&#31209;&#36866;&#37197;&#22120;&#30697;&#38453;&#37325;&#35201;&#24615;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#26356;&#26032;&#21442;&#25968;&#30697;&#38453;&#26102;&#65292;$B$&#21644;$A$&#30697;&#38453;&#20855;&#26377;&#19981;&#21516;&#21151;&#33021;&#65292;&#24494;&#35843;$B$&#27604;&#24494;&#35843;$A$&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#36890;&#36807;&#26356;&#26032;&#21442;&#25968;&#30340;&#23376;&#38598;&#26469;&#20248;&#21270;&#22823;&#22411;&#12289;&#39044;&#20808;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#65307;&#22312;&#36825;&#19968;&#31867;&#20013;&#65292;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#29305;&#21035;&#26377;&#25928;&#12290;&#21463;&#21040;&#35843;&#26597;LoRA&#30697;&#38453;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#19981;&#21516;&#20316;&#29992;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#34920;&#24449;&#21644;&#21033;&#29992;&#20102;&#20302;&#31209;&#36866;&#37197;&#22120;&#30697;&#38453;&#37325;&#35201;&#24615;&#30340;&#24847;&#22806;&#19981;&#23545;&#31216;&#24615;&#12290;&#20855;&#20307;&#22320;&#65292;&#22312;&#36890;&#36807;&#28155;&#21152;&#20056;&#31215;$BA$&#26469;&#26356;&#26032;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#30697;&#38453;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;$B$&#21644;$A$&#30697;&#38453;&#20855;&#26377;&#19981;&#21516;&#30340;&#21151;&#33021;&#65306;$A$&#20174;&#36755;&#20837;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#32780;$B$&#21033;&#29992;&#36825;&#20123;&#29305;&#24449;&#21019;&#24314;&#26399;&#26395;&#30340;&#36755;&#20986;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#35777;&#26126;&#24494;&#35843;$B$&#22266;&#26377;&#22320;&#27604;&#24494;&#35843;$A$&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#19968;&#20010;&#38543;&#26426;&#26410;&#32463;&#35757;&#32451;&#30340;$A$&#24212;&#35813;&#34920;&#29616;&#20960;&#20046;&#19982;&#32463;&#36807;&#24494;&#35843;&#30340;$A$&#19968;&#26679;&#22909;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#20449;&#24687;&#35770;&#35270;&#35282;&#38480;&#21046;&#20102;&#20302;&#31209;&#36866;&#37197;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16842v2 Announce Type: replace  Abstract: Parameter-efficient fine-tuning optimizes large, pre-trained foundation models by updating a subset of parameters; in this class, Low-Rank Adaptation (LoRA) is particularly effective. Inspired by an effort to investigate the different roles of LoRA matrices during fine-tuning, this paper characterizes and leverages unexpected asymmetry in the importance of low-rank adapter matrices. Specifically, when updating the parameter matrices of a neural network by adding a product $BA$, we observe that the $B$ and $A$ matrices have distinct functions: $A$ extracts features from the input, while $B$ uses these features to create the desired output. Based on this observation, we demonstrate that fine-tuning $B$ is inherently more effective than fine-tuning $A$, and that a random untrained $A$ should perform nearly as well as a fine-tuned one. Using an information-theoretic lens, we also bound the generalization of low-rank adapters, showing tha
&lt;/p&gt;</description></item><item><title>&#23558;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#32479;&#19968;&#25551;&#36848;&#20026;&#35745;&#31639;&#22270;&#65292;&#25552;&#20986;&#26032;&#39062;&#30340;&#33258;&#21160;&#22270;&#20248;&#21270;&#22120;&#26469;&#25913;&#36827;&#33410;&#28857;&#21644;&#36793;&#65292;&#23454;&#29616;&#20102;&#20195;&#29702;&#20043;&#38388;&#30340;&#33258;&#21160;&#21327;&#20316;&#21644;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.16823</link><description>&lt;p&gt;
&#20316;&#20026;&#21487;&#20248;&#21270;&#22270;&#30340;&#35821;&#35328;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Language Agents as Optimizable Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16823
&lt;/p&gt;
&lt;p&gt;
&#23558;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#32479;&#19968;&#25551;&#36848;&#20026;&#35745;&#31639;&#22270;&#65292;&#25552;&#20986;&#26032;&#39062;&#30340;&#33258;&#21160;&#22270;&#20248;&#21270;&#22120;&#26469;&#25913;&#36827;&#33410;&#28857;&#21644;&#36793;&#65292;&#23454;&#29616;&#20102;&#20195;&#29702;&#20043;&#38388;&#30340;&#33258;&#21160;&#21327;&#20316;&#21644;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#31181;&#20154;&#31867;&#35774;&#35745;&#30340;&#25552;&#21319;&#25216;&#26415;&#34987;&#25552;&#20986;&#65292;&#29992;&#20110;&#25913;&#36827;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38382;&#39064;&#27714;&#35299;&#22120;&#65292;&#20135;&#29983;&#20102;&#35768;&#22810;&#19981;&#21516;&#30340;&#20195;&#30721;&#24211;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;LLM&#20195;&#29702;&#25551;&#36848;&#20026;&#35745;&#31639;&#22270;&#26469;&#32479;&#19968;&#36825;&#20123;&#26041;&#27861;&#12290;&#33410;&#28857;&#23454;&#29616;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#25110;&#26597;&#35810;LLMs&#30340;&#21151;&#33021;&#65292;&#24182;&#19988;&#36793;&#25551;&#36848;&#25805;&#20316;&#20043;&#38388;&#30340;&#20449;&#24687;&#27969;&#21160;&#12290;&#22270;&#24418;&#21487;&#20197;&#36882;&#24402;&#22320;&#32452;&#21512;&#25104;&#20195;&#34920;&#19981;&#21516;&#20195;&#29702;&#20043;&#38388;&#21327;&#20316;&#23618;&#27425;&#30340;&#26356;&#22823;&#32452;&#21512;&#22270;&#65288;&#20854;&#20013;&#36793;&#36830;&#25509;&#19981;&#21516;&#20195;&#29702;&#30340;&#25805;&#20316;&#65289;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#33258;&#21160;&#22270;&#20248;&#21270;&#22120;&#65288;1&#65289;&#20248;&#21270;&#33410;&#28857;&#32423;LLM&#25552;&#31034;&#65288;&#33410;&#28857;&#20248;&#21270;&#65289;&#24182;&#65288;2&#65289;&#36890;&#36807;&#25913;&#21464;&#22270;&#36830;&#25509;&#24615;&#26469;&#25913;&#21892;&#20195;&#29702;&#21327;&#35843;&#65288;&#36793;&#32536;&#20248;&#21270;&#65289;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#29992;&#20110;&#39640;&#25928;&#24320;&#21457;&#12289;&#38598;&#25104;&#21644;&#33258;&#21160;&#25913;&#36827;&#21508;&#31181;LLM&#20195;&#29702;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/metauto-ai/gptswarm&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16823v1 Announce Type: cross  Abstract: Various human-designed prompt engineering techniques have been proposed to improve problem solvers based on Large Language Models (LLMs), yielding many disparate code bases. We unify these approaches by describing LLM-based agents as computational graphs. The nodes implement functions to process multimodal data or query LLMs, and the edges describe the information flow between operations. Graphs can be recursively combined into larger composite graphs representing hierarchies of inter-agent collaboration (where edges connect operations of different agents). Our novel automatic graph optimizers (1) refine node-level LLM prompts (node optimization) and (2) improve agent orchestration by changing graph connectivity (edge optimization). Experiments demonstrate that our framework can be used to efficiently develop, integrate, and automatically improve various LLM agents. The code can be found at https://github.com/metauto-ai/gptswarm.
&lt;/p&gt;</description></item><item><title>Nemotron-4 15B&#26159;&#19968;&#20010;150&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#22810;&#35821;&#35328;&#33021;&#21147;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#36229;&#36807;&#20854;&#20182;&#35268;&#27169;&#30456;&#20284;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.16819</link><description>&lt;p&gt;
Nemotron-4 15B&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Nemotron-4 15B Technical Report
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16819
&lt;/p&gt;
&lt;p&gt;
Nemotron-4 15B&#26159;&#19968;&#20010;150&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#22810;&#35821;&#35328;&#33021;&#21147;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#36229;&#36807;&#20854;&#20182;&#35268;&#27169;&#30456;&#20284;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Nemotron-4 15B&#65292;&#36825;&#26159;&#19968;&#20010;&#25317;&#26377;150&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#20102;8000&#19975;&#20159;&#20010;&#25991;&#26412;&#26631;&#35760;&#12290;Nemotron-4 15B&#22312;&#33521;&#35821;&#12289;&#22810;&#35821;&#35328;&#21644;&#32534;&#30721;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65306;&#22312;7&#20010;&#19979;&#28216;&#35780;&#20272;&#39046;&#22495;&#20013;&#65292;&#23427;&#22312;4&#20010;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;&#20854;&#20313;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#34920;&#29616;&#65292;&#36229;&#36807;&#20102;&#25152;&#26377;&#29616;&#26377;&#35268;&#27169;&#30456;&#20284;&#30340;&#24320;&#25918;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;Nemotron-4 15B&#23637;&#29616;&#20986;&#20102;&#25152;&#26377;&#35268;&#27169;&#30456;&#20284;&#27169;&#22411;&#20013;&#26368;&#24378;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#65292;&#29978;&#33267;&#22312;&#22810;&#35821;&#35328;&#20219;&#21153;&#19978;&#20248;&#20110;&#22235;&#20493;&#20197;&#19978;&#30340;&#22823;&#22411;&#27169;&#22411;&#65292;&#20197;&#21450;&#19987;&#38376;&#29992;&#20110;&#22810;&#35821;&#35328;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16819v1 Announce Type: new  Abstract: We introduce Nemotron-4 15B, a 15-billion-parameter large multilingual language model trained on 8 trillion text tokens. Nemotron-4 15B demonstrates strong performance when assessed on English, multilingual, and coding tasks: it outperforms all existing similarly-sized open models on 4 out of 7 downstream evaluation areas and achieves competitive performance to the leading open models in the remaining ones. Specifically, Nemotron-4 15B exhibits the best multilingual capabilities of all similarly-sized models, even outperforming models over four times larger and those explicitly specialized for multilingual tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#36870;&#21521;&#24037;&#31243;&#22312;&#22797;&#26434;&#27169;&#22359;&#21270;&#31639;&#26415;&#20013;&#35266;&#23519;&#20102;Transformer&#20869;&#37096;&#30005;&#36335;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#21457;&#29616;&#20943;&#27861;&#22312;Transformer&#19978;&#36896;&#25104;&#20102;&#24378;&#28872;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#20056;&#27861;&#38656;&#35201;&#20313;&#24358;&#20559;&#32622;&#20998;&#37327;&#65292;&#22810;&#39033;&#24335;&#21472;&#21152;&#20102;&#22522;&#26412;&#31639;&#26415;&#27169;&#24335;&#65292;&#20294;&#22312;&#25361;&#25112;&#24615;&#24773;&#20917;&#19979;&#24182;&#19981;&#28165;&#26224;&#65292;Grokking&#29978;&#33267;&#21487;&#20197;&#22312;&#20855;&#26377;&#22522;&#26412;&#23545;&#31216;&#21644;&#20132;&#26367;&#34920;&#36798;&#24335;&#30340;&#39640;&#27425;&#20844;&#24335;&#20013;&#36731;&#26494;&#21457;&#29983;&#12290;</title><link>https://arxiv.org/abs/2402.16726</link><description>&lt;p&gt;
&#22312;&#22797;&#26434;&#27169;&#22359;&#21270;&#31639;&#26415;&#20013;&#35299;&#37322;&#29702;&#35299;&#30340;Transformer
&lt;/p&gt;
&lt;p&gt;
Interpreting Grokked Transformers in Complex Modular Arithmetic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#36870;&#21521;&#24037;&#31243;&#22312;&#22797;&#26434;&#27169;&#22359;&#21270;&#31639;&#26415;&#20013;&#35266;&#23519;&#20102;Transformer&#20869;&#37096;&#30005;&#36335;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#21457;&#29616;&#20943;&#27861;&#22312;Transformer&#19978;&#36896;&#25104;&#20102;&#24378;&#28872;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#20056;&#27861;&#38656;&#35201;&#20313;&#24358;&#20559;&#32622;&#20998;&#37327;&#65292;&#22810;&#39033;&#24335;&#21472;&#21152;&#20102;&#22522;&#26412;&#31639;&#26415;&#27169;&#24335;&#65292;&#20294;&#22312;&#25361;&#25112;&#24615;&#24773;&#20917;&#19979;&#24182;&#19981;&#28165;&#26224;&#65292;Grokking&#29978;&#33267;&#21487;&#20197;&#22312;&#20855;&#26377;&#22522;&#26412;&#23545;&#31216;&#21644;&#20132;&#26367;&#34920;&#36798;&#24335;&#30340;&#39640;&#27425;&#20844;&#24335;&#20013;&#36731;&#26494;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Grokking&#19968;&#30452;&#26159;&#35299;&#24320;&#24310;&#36831;&#27867;&#21270;&#20043;&#35868;&#30340;&#31215;&#26497;&#25506;&#32034;&#12290;&#22312;&#24050;&#35299;&#23494;&#27169;&#22411;&#20013;&#35782;&#21035;&#21487;&#35299;&#37322;&#30340;&#31639;&#27861;&#26159;&#29702;&#35299;&#20854;&#26426;&#21046;&#30340;&#26263;&#31034;&#24615;&#32447;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#38500;&#20102;&#26368;&#31616;&#21333;&#21644;&#24191;&#20026;&#30740;&#31350;&#30340;&#27169;&#22359;&#21270;&#21152;&#27861;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#36870;&#21521;&#24037;&#31243;&#35266;&#23519;&#20102;&#36890;&#36807;Grokking&#22312;&#22797;&#26434;&#27169;&#22359;&#21270;&#31639;&#26415;&#20013;&#23398;&#21040;&#30340;&#20869;&#37096;&#30005;&#36335;&#65292;&#31361;&#20986;&#26174;&#31034;&#20102;&#23427;&#20204;&#21160;&#21147;&#23398;&#19978;&#30340;&#37325;&#22823;&#24046;&#24322;&#65306;&#20943;&#27861;&#23545;Transformer&#20135;&#29983;&#24378;&#28872;&#30340;&#19981;&#23545;&#31216;&#24615;&#65307;&#20056;&#27861;&#22312;&#20613;&#31435;&#21494;&#22495;&#30340;&#25152;&#26377;&#39057;&#29575;&#19978;&#38656;&#35201;&#20313;&#24358;&#20559;&#32622;&#20998;&#37327;&#65307;&#22810;&#39033;&#24335;&#36890;&#24120;&#23548;&#33268;&#22522;&#26412;&#31639;&#26415;&#27169;&#24335;&#30340;&#21472;&#21152;&#65292;&#20294;&#22312;&#25361;&#25112;&#24615;&#24773;&#20917;&#19979;&#28165;&#26224;&#30340;&#27169;&#24335;&#24182;&#19981;&#26174;&#29616;&#65307;&#21363;&#20351;&#22312;&#20855;&#26377;&#22522;&#26412;&#23545;&#31216;&#21644;&#20132;&#26367;&#34920;&#36798;&#24335;&#30340;&#39640;&#27425;&#20844;&#24335;&#20013;&#65292;Grokking&#20063;&#24456;&#23481;&#26131;&#21457;&#29983;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#27169;&#22359;&#21270;&#31639;&#26415;&#30340;&#26032;&#39062;&#36827;&#23637;&#24230;&#37327;&#65307;&#20613;&#31435;&#21494;&#39057;&#29575;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16726v2 Announce Type: replace-cross  Abstract: Grokking has been actively explored to reveal the mystery of delayed generalization. Identifying interpretable algorithms inside the grokked models is a suggestive hint to understanding its mechanism. In this work, beyond the simplest and well-studied modular addition, we observe the internal circuits learned through grokking in complex modular arithmetic via interpretable reverse engineering, which highlights the significant difference in their dynamics: subtraction poses a strong asymmetry on Transformer; multiplication requires cosine-biased components at all the frequencies in a Fourier domain; polynomials often result in the superposition of the patterns from elementary arithmetic, but clear patterns do not emerge in challenging cases; grokking can easily occur even in higher-degree formulas with basic symmetric and alternating expressions. We also introduce the novel progress measure for modular arithmetic; Fourier Freque
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#35270;&#35273;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#19981;&#30830;&#23450;&#24615;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#19981;&#30830;&#23450;&#24615;&#30340;&#38646;-shot&#36716;&#31227;&#24182;&#21152;&#36895;&#20102;&#35757;&#32451;&#65292;&#33021;&#27867;&#21270;&#21040;&#26410;&#30693;&#25968;&#25454;&#38598;&#65292;&#20351;&#24471;&#23433;&#20840;&#26816;&#32034;&#21644;&#23545;&#19981;&#30830;&#23450;&#24615;&#25935;&#24863;&#30340;&#25968;&#25454;&#38598;&#21487;&#35270;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.16569</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Pretrained Visual Uncertainties
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16569
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#35270;&#35273;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#19981;&#30830;&#23450;&#24615;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#19981;&#30830;&#23450;&#24615;&#30340;&#38646;-shot&#36716;&#31227;&#24182;&#21152;&#36895;&#20102;&#35757;&#32451;&#65292;&#33021;&#27867;&#21270;&#21040;&#26410;&#30693;&#25968;&#25454;&#38598;&#65292;&#20351;&#24471;&#23433;&#20840;&#26816;&#32034;&#21644;&#23545;&#19981;&#30830;&#23450;&#24615;&#25935;&#24863;&#30340;&#25968;&#25454;&#38598;&#21487;&#35270;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23545;&#20110;&#21487;&#20449;&#20219;&#30340;&#26426;&#22120;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#36890;&#24120;&#19981;&#30830;&#23450;&#24615;&#24517;&#39035;&#38024;&#23545;&#27599;&#20010;&#20219;&#21153;&#37325;&#26032;&#23398;&#20064;&#12290;&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#35270;&#35273;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#19981;&#30830;&#23450;&#24615;&#27169;&#22359;&#12290;&#31867;&#20284;&#20110;&#26631;&#20934;&#30340;&#39044;&#35757;&#32451;&#65292;&#36825;&#20351;&#24471;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#30340;&#19981;&#30830;&#23450;&#24615;&#21487;&#20197;&#38646;-shot&#36716;&#31227;&#21040;&#19987;&#38376;&#30340;&#19979;&#28216;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#35299;&#20915;&#20197;&#21069;&#19981;&#30830;&#23450;&#24615;&#27169;&#22359;&#20013;&#30340;&#26799;&#24230;&#20914;&#31361;&#24182;&#23558;&#35757;&#32451;&#21152;&#36895;&#39640;&#36798;180&#20493;&#26469;&#23454;&#29616;&#22312;ImageNet-21k&#19978;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#21457;&#29616;&#39044;&#35757;&#32451;&#30340;&#19981;&#30830;&#23450;&#24615;&#21487;&#20197;&#27867;&#21270;&#21040;&#26410;&#30693;&#25968;&#25454;&#38598;&#12290;&#22312;&#23457;&#26597;&#23398;&#20064;&#21040;&#30340;&#19981;&#30830;&#23450;&#24615;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#20854;&#25429;&#33719;&#20102;&#19982;&#35748;&#30693;&#25104;&#20998;&#20998;&#31163;&#30340;&#38750;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20351;&#24471;&#23433;&#20840;&#26816;&#32034;&#21644;&#23545;&#19981;&#30830;&#23450;&#24615;&#25935;&#24863;&#30340;&#25968;&#25454;&#38598;&#21487;&#35270;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;&#20026;&#20102;&#40723;&#21169;&#23558;&#24212;&#29992;&#25299;&#23637;&#21040;&#26356;&#22810;&#38382;&#39064;&#21644;&#39046;&#22495;&#65292;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#25152;&#26377;&#39044;&#35757;&#32451;&#30340;&#26816;&#26597;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16569v2 Announce Type: replace-cross  Abstract: Accurate uncertainty estimation is vital to trustworthy machine learning, yet uncertainties typically have to be learned for each task anew. This work introduces the first pretrained uncertainty modules for vision models. Similar to standard pretraining this enables the zero-shot transfer of uncertainties learned on a large pretraining dataset to specialized downstream datasets. We enable our large-scale pretraining on ImageNet-21k by solving a gradient conflict in previous uncertainty modules and accelerating the training by up to 180x. We find that the pretrained uncertainties generalize to unseen datasets. In scrutinizing the learned uncertainties, we find that they capture aleatoric uncertainty, disentangled from epistemic components. We demonstrate that this enables safe retrieval and uncertainty-aware dataset visualization. To encourage applications to further problems and domains, we release all pretrained checkpoints an
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#39304;&#39640;&#25928;&#30340;&#22312;&#32447;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31243;&#24207;</title><link>https://arxiv.org/abs/2402.16359</link><description>&lt;p&gt;
&#21453;&#39304;&#39640;&#25928;&#22312;&#32447;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Feedback Efficient Online Fine-Tuning of Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16359
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#39304;&#39640;&#25928;&#30340;&#22312;&#32447;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#24314;&#27169;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#21253;&#25324;&#22270;&#20687;&#65292;&#34507;&#30333;&#36136;&#21644;&#23567;&#20998;&#23376;&#30340;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#27169;&#25311;&#26368;&#22823;&#21270;&#26576;&#20123;&#23646;&#24615;&#30340;&#20998;&#24067;&#30340;&#37096;&#20998;&#65306;&#20363;&#22914;&#65292;&#25105;&#20204;&#21487;&#33021;&#24076;&#26395;&#29983;&#25104;&#20855;&#26377;&#39640;&#23457;&#32654;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#25110;&#20855;&#26377;&#39640;&#29983;&#29289;&#27963;&#24615;&#30340;&#20998;&#23376;&#12290;&#33258;&#28982;&#22320;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#36825;&#35270;&#20026;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#38382;&#39064;&#65292;&#20854;&#30446;&#26631;&#26159;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#20197;&#26368;&#22823;&#21270;&#19982;&#26576;&#20123;&#23646;&#24615;&#23545;&#24212;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#21363;&#20351;&#21487;&#20197;&#35775;&#38382;&#22320;&#38754;&#30495;&#23454;&#22870;&#21169;&#20989;&#25968;&#30340;&#22312;&#32447;&#26597;&#35810;&#65292;&#26377;&#25928;&#22320;&#21457;&#29616;&#39640;&#22870;&#21169;&#26679;&#26412;&#20063;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65306;&#23427;&#20204;&#22312;&#21021;&#22987;&#20998;&#24067;&#20013;&#30340;&#27010;&#29575;&#21487;&#33021;&#24456;&#20302;&#65292;&#24182;&#19988;&#21487;&#33021;&#23384;&#22312;&#35768;&#22810;&#19981;&#21487;&#34892;&#30340;&#26679;&#26412;&#65292;&#29978;&#33267;&#27809;&#26377;&#23450;&#20041;&#33391;&#22909;&#30340;&#22870;&#21169;&#65288;&#20363;&#22914;&#65292;&#19981;&#33258;&#28982;&#30340;&#22270;&#20687;&#25110;&#29289;&#29702;&#19978;&#19981;&#21487;&#33021;&#30340;&#20998;&#23376;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#31243;&#24207;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#21457;&#29616;&#39640;&#22870;&#21169;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16359v1 Announce Type: cross  Abstract: Diffusion models excel at modeling complex data distributions, including those of images, proteins, and small molecules. However, in many cases, our goal is to model parts of the distribution that maximize certain properties: for example, we may want to generate images with high aesthetic quality, or molecules with high bioactivity. It is natural to frame this as a reinforcement learning (RL) problem, in which the objective is to fine-tune a diffusion model to maximize a reward function that corresponds to some property. Even with access to online queries of the ground-truth reward function, efficiently discovering high-reward samples can be challenging: they might have a low probability in the initial distribution, and there might be many infeasible samples that do not even have a well-defined reward (e.g., unnatural images or physically impossible molecules). In this work, we propose a novel reinforcement learning procedure that effi
&lt;/p&gt;</description></item><item><title>CoDream&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#36755;&#20837;&#25968;&#25454;&#31354;&#38388;&#20013;&#21327;&#20316;&#20248;&#21270;&#25968;&#25454;&#26469;&#20132;&#25442;&#30693;&#35782;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#20043;&#38388;&#30340;&#21512;&#20316;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#26550;&#26500;&#26080;&#20851;&#12289;&#36890;&#20449;&#19981;&#21463;&#27169;&#22411;&#22823;&#23567;&#24433;&#21709;&#12289;&#20860;&#23481;&#23433;&#20840;&#32858;&#21512;&#30340;&#20248;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.15968</link><description>&lt;p&gt;
CoDream&#65306;&#20351;&#29992;&#24322;&#26500;&#27169;&#22411;&#20132;&#25442;&#26790;&#24819;&#32780;&#19981;&#26159;&#27169;&#22411;&#36827;&#34892;&#32852;&#21512;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
CoDream: Exchanging dreams instead of models for federated aggregation with heterogeneous models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15968
&lt;/p&gt;
&lt;p&gt;
CoDream&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#36755;&#20837;&#25968;&#25454;&#31354;&#38388;&#20013;&#21327;&#20316;&#20248;&#21270;&#25968;&#25454;&#26469;&#20132;&#25442;&#30693;&#35782;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#20043;&#38388;&#30340;&#21512;&#20316;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#26550;&#26500;&#26080;&#20851;&#12289;&#36890;&#20449;&#19981;&#21463;&#27169;&#22411;&#22823;&#23567;&#24433;&#21709;&#12289;&#20860;&#23481;&#23433;&#20840;&#32858;&#21512;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36890;&#36807;&#32858;&#21512;&#27169;&#22411;&#21442;&#25968;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20998;&#25955;&#25968;&#25454;&#19978;&#30340;&#21327;&#20316;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#32858;&#21512;&#27169;&#22411;&#20135;&#29983;&#30340;&#8220;&#30693;&#35782;&#8221;&#65292;&#32780;&#19981;&#26159;&#27169;&#22411;&#21442;&#25968;&#26469;&#25193;&#23637;&#36825;&#19968;&#27010;&#24565;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; \codream &#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#23458;&#25143;&#31471;&#36890;&#36807;&#22312;&#36755;&#20837;&#25968;&#25454;&#31354;&#38388;&#20013;&#20351;&#29992;&#32852;&#21512;&#20248;&#21270;&#26469;&#21327;&#20316;&#20248;&#21270;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#25968;&#25454;&#65292;&#31867;&#20284;&#20110;&#22312;FL&#20013;&#20248;&#21270;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#32852;&#21512;&#20248;&#21270;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#26377;&#25928;&#25429;&#33719;&#20840;&#23616;&#25968;&#25454;&#20998;&#24067;&#30340;&#29305;&#24615;&#12290;&#22312;&#25968;&#25454;&#31354;&#38388;&#20849;&#20139;&#30693;&#35782;&#20855;&#26377;&#35768;&#22810;&#22909;&#22788;&#65306;&#65288;1&#65289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#21327;&#20316;&#23398;&#20064;&#65292;&#21363;&#19981;&#21516;&#23458;&#25143;&#31471;&#21487;&#20197;&#20855;&#26377;&#19981;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#65307;&#65288;2&#65289;&#36890;&#20449;&#19981;&#21463;&#27169;&#22411;&#22823;&#23567;&#24433;&#21709;&#65292;&#28040;&#38500;&#20102;&#27169;&#22411;&#21442;&#25968;&#30340;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#65307;&#65288;3&#65289;&#19982;&#23433;&#20840;&#32858;&#21512;&#20860;&#23481;&#65292;&#22240;&#27492;&#21487;&#39044;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15968v1 Announce Type: cross  Abstract: Federated Learning (FL) enables collaborative optimization of machine learning models across decentralized data by aggregating model parameters. Our approach extends this concept by aggregating "knowledge" derived from models, instead of model parameters. We present a novel framework called \codream, where clients collaboratively optimize randomly initialized data using federated optimization in the input data space, similar to how randomly initialized model parameters are optimized in FL. Our key insight is that jointly optimizing this data can effectively capture the properties of the global data distribution. Sharing knowledge in data space offers numerous benefits: (1) model-agnostic collaborative learning, i.e., different clients can have different model architectures; (2) communication that is independent of the model size, eliminating scalability concerns with model parameters; (3) compatibility with secure aggregation, thus pre
&lt;/p&gt;</description></item><item><title>&#28145;&#20837;&#30740;&#31350;&#19977;&#23618;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#20957;&#32858;&#29616;&#35937;&#21644;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#33258;&#21457;&#20943;&#23569;&#31070;&#32463;&#32593;&#32476;&#22797;&#26434;&#24615;&#30340;&#26426;&#21046;&#65292;&#25552;&#20986;&#26377;&#25928;&#21160;&#21147;&#23398;&#30340;&#29190;&#28856;&#24615;&#36136;&#21644;&#20957;&#32858;&#21457;&#29983;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#20123;&#21457;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.15958</link><description>&lt;p&gt;
&#35770;&#19977;&#23618;&#31070;&#32463;&#32593;&#32476;&#21160;&#21147;&#23398;&#65306;&#21021;&#22987;&#20957;&#32858;
&lt;/p&gt;
&lt;p&gt;
On the dynamics of three-layer neural networks: initial condensation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15958
&lt;/p&gt;
&lt;p&gt;
&#28145;&#20837;&#30740;&#31350;&#19977;&#23618;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#20957;&#32858;&#29616;&#35937;&#21644;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#33258;&#21457;&#20943;&#23569;&#31070;&#32463;&#32593;&#32476;&#22797;&#26434;&#24615;&#30340;&#26426;&#21046;&#65292;&#25552;&#20986;&#26377;&#25928;&#21160;&#21147;&#23398;&#30340;&#29190;&#28856;&#24615;&#36136;&#21644;&#20957;&#32858;&#21457;&#29983;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#20123;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#39564;&#21644;&#29702;&#35770;&#30740;&#31350;&#26174;&#31034;&#65292;&#24403;&#21021;&#22987;&#21270;&#20026;&#23567;&#20540;&#26102;&#65292;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#26435;&#37325;&#20250;&#25910;&#25947;&#21040;&#23396;&#31435;&#30340;&#26041;&#21521;&#12290;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#20957;&#32858;&#65292;&#34920;&#26126;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24448;&#24448;&#20250;&#33258;&#21457;&#22320;&#20943;&#23569;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#12290; &#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#19977;&#23618;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#20986;&#29616;&#30340;&#20957;&#32858;&#29616;&#35937;&#32972;&#21518;&#30340;&#26426;&#21046;&#65292;&#24182;&#23558;&#20854;&#19982;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#36827;&#34892;&#21306;&#20998;&#12290; &#36890;&#36807;&#20005;&#26684;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#26377;&#25928;&#21160;&#21147;&#23398;&#30340;&#29190;&#28856;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#21457;&#29983;&#20957;&#32858;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#36825;&#20123;&#21457;&#29616;&#24471;&#21040;&#20102;&#23454;&#39564;&#32467;&#26524;&#30340;&#35777;&#23454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#20957;&#32858;&#19982;&#28145;&#24230;&#30697;&#38453;&#22240;&#23376;&#20998;&#35299;&#20013;&#35266;&#23519;&#21040;&#30340;&#20302;&#31209;&#20559;&#24046;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15958v1 Announce Type: new  Abstract: Empirical and theoretical works show that the input weights of two-layer neural networks, when initialized with small values, converge towards isolated orientations. This phenomenon, referred to as condensation, indicates that the gradient descent methods tend to spontaneously reduce the complexity of neural networks during the training process. In this work, we elucidate the mechanisms behind the condensation phenomena occurring in the training of three-layer neural networks and distinguish it from the training of two-layer neural networks. Through rigorous theoretical analysis, we establish the blow-up property of effective dynamics and present a sufficient condition for the occurrence of condensation, findings that are substantiated by experimental results. Additionally, we explore the association between condensation and the low-rank bias observed in deep matrix factorization.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphEdit&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23398;&#20064;&#22797;&#26434;&#30340;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#30340;&#33410;&#28857;&#20851;&#31995;&#65292;&#36890;&#36807;&#22312;&#22270;&#32467;&#26500;&#19978;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#65292;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15183</link><description>&lt;p&gt;
GraphEdit&#65306;&#29992;&#20110;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GraphEdit: Large Language Models for Graph Structure Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphEdit&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23398;&#20064;&#22797;&#26434;&#30340;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#30340;&#33410;&#28857;&#20851;&#31995;&#65292;&#36890;&#36807;&#22312;&#22270;&#32467;&#26500;&#19978;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#65292;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32467;&#26500;&#23398;&#20064;&#65288;GSL&#65289;&#33268;&#21147;&#20110;&#36890;&#36807;&#29983;&#25104;&#26032;&#39062;&#30340;&#22270;&#32467;&#26500;&#26469;&#25429;&#25417;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#22266;&#26377;&#20381;&#36182;&#24615;&#21644;&#30456;&#20114;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphEdit&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23398;&#20064;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#22797;&#26434;&#30340;&#33410;&#28857;&#20851;&#31995;&#12290;&#36890;&#36807;&#22312;&#22270;&#32467;&#26500;&#19978;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#65292;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#26088;&#22312;&#20811;&#26381;&#26174;&#24335;&#22270;&#32467;&#26500;&#20449;&#24687;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#39640;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15183v1 Announce Type: cross  Abstract: Graph Structure Learning (GSL) focuses on capturing intrinsic dependencies and interactions among nodes in graph-structured data by generating novel graph structures. Graph Neural Networks (GNNs) have emerged as promising GSL solutions, utilizing recursive message passing to encode node-wise inter-dependencies. However, many existing GSL methods heavily depend on explicit graph structural information as supervision signals, leaving them susceptible to challenges such as data noise and sparsity. In this work, we propose GraphEdit, an approach that leverages large language models (LLMs) to learn complex node relationships in graph-structured data. By enhancing the reasoning capabilities of LLMs through instruction-tuning over graph structures, we aim to overcome the limitations associated with explicit graph structural information and enhance the reliability of graph structure learning. Our approach not only effectively denoises noisy co
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#23436;&#21892;&#21644;&#26684;&#24335;&#21270;&#25913;&#36827;LMs&#23545;&#25239;&#36234;&#29425;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#22312;&#38750;&#23433;&#20840;&#23545;&#40784;&#30340;LMs&#20013;&#20063;&#20855;&#26377;&#20986;&#33394;&#30340;&#23433;&#20840;&#24615;&#65292;&#21516;&#26102;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.15180</link><description>&lt;p&gt;
&#25171;&#30772;Breakout: &#29992;&#33258;&#25105;&#23436;&#21892;&#37325;&#26032;&#23450;&#20041;LM&#23545;&#25239;&#36234;&#29425;&#25915;&#20987;&#30340;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks with Self-Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15180
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#23436;&#21892;&#21644;&#26684;&#24335;&#21270;&#25913;&#36827;LMs&#23545;&#25239;&#36234;&#29425;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#22312;&#38750;&#23433;&#20840;&#23545;&#40784;&#30340;LMs&#20013;&#20063;&#20855;&#26377;&#20986;&#33394;&#30340;&#23433;&#20840;&#24615;&#65292;&#21516;&#26102;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35686;&#21578;&#65306;&#26412;&#25991;&#21253;&#21547;&#21487;&#33021;&#24341;&#36215;&#19981;&#24555;&#30340;&#20882;&#29359;&#24615;&#35789;&#35821;&#12290;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#23481;&#26131;&#34987;&#21033;&#29992;&#36827;&#34892;&#24694;&#24847;&#28389;&#29992;&#12290;&#23545;LM&#36827;&#34892;&#23433;&#20840;&#23545;&#40784;&#30340;&#35757;&#32451;&#38750;&#24120;&#22797;&#26434;&#65292;&#20351;&#24471;&#38590;&#20197;&#31435;&#21363;&#24212;&#23545;&#24555;&#36895;&#21457;&#23637;&#30340;&#25915;&#20987;&#65292;&#22914;&#36234;&#29425;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26684;&#24335;&#33258;&#25105;&#23436;&#21892;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#22312;&#38750;&#23433;&#20840;&#23545;&#40784;&#30340;LMs&#20013;&#20063;&#33021;&#23454;&#29616;&#20986;&#33394;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20960;&#31181;&#38450;&#24481;&#22522;&#32447;&#36827;&#34892;&#35780;&#20272;&#65292;&#34920;&#26126;&#36825;&#26159;&#38024;&#23545;&#36234;&#29425;&#25915;&#20987;&#26368;&#23433;&#20840;&#30340;&#26080;&#35757;&#32451;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#33258;&#25105;&#23436;&#21892;&#36807;&#31243;&#25928;&#29575;&#30340;&#26684;&#24335;&#21270;&#26041;&#27861;&#65292;&#21516;&#26102;&#22312;&#36739;&#23569;&#36845;&#20195;&#20013;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#38750;&#23433;&#20840;&#23545;&#40784;&#30340;LM&#22312;&#23433;&#20840;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#23433;&#20840;&#23545;&#40784;&#30340;LM&#65292;&#22240;&#20026;&#23427;&#20204;&#32473;&#20986;&#26356;&#26377;&#29992;&#19988;&#26356;&#23433;&#20840;&#30340;&#22238;&#22797;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#33021;&#22815;&#22312;&#36739;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#23454;&#29616;&#26356;&#23569;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15180v1 Announce Type: cross  Abstract: Caution: This paper includes offensive words that could potentially cause unpleasantness. Language models (LMs) are vulnerable to exploitation for adversarial misuse. Training LMs for safety alignment is extensive and makes it hard to respond to fast-developing attacks immediately, such as jailbreaks. We propose self-refine with formatting that achieves outstanding safety even in non-safety-aligned LMs and evaluate our method alongside several defense baselines, demonstrating that it is the safest training-free method against jailbreak attacks. Additionally, we proposed a formatting method that improves the efficiency of the self-refine process while reducing attack success rates in fewer iterations. We've also observed that non-safety-aligned LMs outperform safety-aligned LMs in safety tasks by giving more helpful and safe responses. In conclusion, our findings can achieve less safety risk with fewer computational costs, allowing non-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25506;&#35752;&#20102;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#26426;&#22120;&#36951;&#24536;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#24314;&#31435;&#20102;&#20840;&#38754;&#30340;&#36951;&#24536;&#26694;&#26550;&#21450;&#39640;&#25928;&#30340;&#36951;&#24536;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#25913;&#36827;&#36229;&#21442;&#25968;&#40065;&#26834;&#24615;&#20197;&#21450;&#39640;&#25928;&#35843;&#25972;&#36229;&#21442;&#25968;&#30340;&#25351;&#21335;&#12290;</title><link>https://arxiv.org/abs/2402.15159</link><description>&lt;p&gt;
&#38754;&#21521;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Machine Unlearning of Pre-trained Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15159
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25506;&#35752;&#20102;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#26426;&#22120;&#36951;&#24536;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#24314;&#31435;&#20102;&#20840;&#38754;&#30340;&#36951;&#24536;&#26694;&#26550;&#21450;&#39640;&#25928;&#30340;&#36951;&#24536;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#25913;&#36827;&#36229;&#21442;&#25968;&#40065;&#26834;&#24615;&#20197;&#21450;&#39640;&#25928;&#35843;&#25972;&#36229;&#21442;&#25968;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32972;&#26223;&#19979;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#20197;&#26426;&#22120;&#36951;&#24536;&#20316;&#20026;&#19968;&#20010;&#20851;&#38190;&#35299;&#20915;&#26041;&#26696;&#65292;&#37325;&#28857;&#20851;&#27880;&#39044;&#35757;&#32451;&#27169;&#22411;&#8212;&#8212;&#19968;&#20010;&#26126;&#26174;&#32570;&#20047;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#22312;&#39044;&#35757;&#32451;LLMs&#20013;&#21246;&#21202;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26426;&#22120;&#36951;&#24536;&#26694;&#26550;&#65292;&#21253;&#25324;&#23545;&#19971;&#31181;&#19981;&#21516;&#36951;&#24536;&#26041;&#27861;&#30340;&#25209;&#21028;&#24615;&#20998;&#26512;&#12290;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;arXiv&#12289;&#20070;&#31821;&#21644;GitHub&#30340;&#31574;&#21010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20005;&#26684;&#35780;&#20272;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26377;&#21147;&#30340;&#26426;&#22120;&#36951;&#24536;&#24615;&#33021;&#22522;&#20934;&#65292;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;&#27604;&#37325;&#26032;&#35757;&#32451;&#39640;&#20986; $10^5$ &#20493;&#20197;&#19978;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20998;&#24067;&#25968;&#25454;&#19978;&#23558;&#26799;&#24230;&#19978;&#21319;&#19982;&#26799;&#24230;&#19979;&#38477;&#32467;&#21512;&#21487;&#20197;&#25913;&#21892;&#36229;&#21442;&#25968;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#22312;&#36951;&#24536;&#36807;&#31243;&#20013;&#36827;&#34892;&#39640;&#25928;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#35814;&#32454;&#25351;&#21335;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25512;&#21160;&#20102;&#26377;&#20851;&#20262;&#29702;&#20154;&#24037;&#26234;&#33021;&#23454;&#36341;&#30340;&#35752;&#35770;&#65292;&#25552;&#20379;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15159v1 Announce Type: cross  Abstract: This study investigates the concept of the `right to be forgotten' within the context of large language models (LLMs). We explore machine unlearning as a pivotal solution, with a focus on pre-trained models--a notably under-researched area. Our research delineates a comprehensive framework for machine unlearning in pre-trained LLMs, encompassing a critical analysis of seven diverse unlearning methods. Through rigorous evaluation using curated datasets from arXiv, books, and GitHub, we establish a robust benchmark for unlearning performance, demonstrating that these methods are over $10^5$ times more computationally efficient than retraining. Our results show that integrating gradient ascent with gradient descent on in-distribution data improves hyperparameter robustness. We also provide detailed guidelines for efficient hyperparameter tuning in the unlearning process. Our findings advance the discourse on ethical AI practices, offering
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Continual Optimal Policy Regularization (COPR) &#26041;&#27861;&#65292;&#36890;&#36807;&#20511;&#37492;&#26368;&#20248;&#31574;&#30053;&#29702;&#35770;&#65292;&#21033;&#29992;&#37319;&#26679;&#20998;&#24067;&#20316;&#20026;&#31034;&#33539;&#21644;&#27491;&#21017;&#21270;&#32422;&#26463;&#65292;&#20197;&#21160;&#24577;&#22320;&#23545;&#24403;&#21069;&#31574;&#30053;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20174;&#32780;&#20351;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#22312;&#25345;&#32493;&#23398;&#20064;&#24773;&#22659;&#19979;&#26356;&#21152;&#31283;&#20581;</title><link>https://arxiv.org/abs/2402.14228</link><description>&lt;p&gt;
COPR:&#36890;&#36807;&#26368;&#20248;&#31574;&#30053;&#27491;&#21017;&#21270;&#23454;&#29616;&#25345;&#32493;&#20154;&#31867;&#20559;&#22909;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
COPR: Continual Human Preference Learning via Optimal Policy Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14228
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Continual Optimal Policy Regularization (COPR) &#26041;&#27861;&#65292;&#36890;&#36807;&#20511;&#37492;&#26368;&#20248;&#31574;&#30053;&#29702;&#35770;&#65292;&#21033;&#29992;&#37319;&#26679;&#20998;&#24067;&#20316;&#20026;&#31034;&#33539;&#21644;&#27491;&#21017;&#21270;&#32422;&#26463;&#65292;&#20197;&#21160;&#24577;&#22320;&#23545;&#24403;&#21069;&#31574;&#30053;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20174;&#32780;&#20351;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#22312;&#25345;&#32493;&#23398;&#20064;&#24773;&#22659;&#19979;&#26356;&#21152;&#31283;&#20581;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14228v1 &#20844;&#21578;&#31867;&#22411;:&#36328;&#30028; &#25688;&#35201;: &#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;RLHF&#65289;&#36890;&#24120;&#29992;&#20110;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#12290;&#37492;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#19981;&#26029;&#21464;&#21270;&#65292;&#25345;&#32493;&#23545;&#40784;&#30456;&#23545;&#20110;&#20256;&#32479;&#38745;&#24577;&#23545;&#40784;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#21644;&#23454;&#38469;&#12290;&#28982;&#32780;&#65292;&#20351;RLHF&#19982;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#20860;&#23481;&#30001;&#20110;&#20854;&#22797;&#26434;&#36807;&#31243;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21516;&#26102;&#65292;&#30452;&#25509;&#23398;&#20064;&#26032;&#30340;&#20154;&#31867;&#20559;&#22909;&#21487;&#33021;&#23548;&#33268;&#21382;&#21490;&#20559;&#22909;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#65292;&#23548;&#33268;&#26080;&#21161;&#25110;&#26377;&#23475;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Continual Optimal Policy Regularization (COPR) &#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20511;&#37492;&#20102;&#26368;&#20248;&#31574;&#30053;&#29702;&#35770;&#12290;COPR&#21033;&#29992;&#37319;&#26679;&#20998;&#24067;&#20316;&#20026;&#31034;&#33539;&#21644;&#27491;&#21017;&#21270;&#32422;&#26463;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#12290;&#23427;&#37319;&#29992;Lagrange&#23545;&#20598;&#65288;LD&#65289;&#26041;&#27861;&#26681;&#25454;&#21382;&#21490;&#19978;&#30340;&#26368;&#20248;&#31574;&#30053;&#21160;&#24577;&#22320;&#27491;&#21017;&#21270;&#24403;&#21069;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14228v1 Announce Type: cross  Abstract: Reinforcement Learning from Human Feedback (RLHF) is commonly utilized to improve the alignment of Large Language Models (LLMs) with human preferences. Given the evolving nature of human preferences, continual alignment becomes more crucial and practical in comparison to traditional static alignment. Nevertheless, making RLHF compatible with Continual Learning (CL) is challenging due to its complex process. Meanwhile, directly learning new human preferences may lead to Catastrophic Forgetting (CF) of historical preferences, resulting in helpless or harmful outputs. To overcome these challenges, we propose the Continual Optimal Policy Regularization (COPR) method, which draws inspiration from the optimal policy theory. COPR utilizes a sampling distribution as a demonstration and regularization constraints for CL. It adopts the Lagrangian Duality (LD) method to dynamically regularize the current policy based on the historically optimal p
&lt;/p&gt;</description></item><item><title>&#23558;&#22522;&#30784;&#27169;&#22411;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#19987;&#29992;&#24212;&#29992;&#27169;&#22411;&#20013;&#23384;&#22312;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#21019;&#24314;&#25945;&#23398;&#22996;&#21592;&#20250;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.14035</link><description>&lt;p&gt;
&#22996;&#21592;&#20250;&#30340;&#26234;&#24935;&#65306;&#20174;&#22522;&#30784;&#27169;&#22411;&#21040;&#19987;&#29992;&#24212;&#29992;&#27169;&#22411;&#30340;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Wisdom of Committee: Distilling from Foundation Model to SpecializedApplication Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14035
&lt;/p&gt;
&lt;p&gt;
&#23558;&#22522;&#30784;&#27169;&#22411;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#19987;&#29992;&#24212;&#29992;&#27169;&#22411;&#20013;&#23384;&#22312;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#21019;&#24314;&#25945;&#23398;&#22996;&#21592;&#20250;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22522;&#30784;&#27169;&#22411;&#30340;&#36827;&#23637;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#19982;&#27492;&#21516;&#26102;&#65292;&#20026;&#29305;&#23450;&#24212;&#29992;&#65292;&#20174;&#19994;&#32773;&#20204;&#19968;&#30452;&#22312;&#24320;&#21457;&#19987;&#38376;&#30340;&#24212;&#29992;&#27169;&#22411;&#12290;&#20026;&#20102;&#20139;&#21463;&#36825;&#20004;&#31181;&#27169;&#22411;&#30340;&#22909;&#22788;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#36335;&#24452;&#26159;&#23558;&#22522;&#30784;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#19987;&#29992;&#24212;&#29992;&#27169;&#22411;&#20013;&#65292;&#21518;&#32773;&#36890;&#24120;&#26356;&#26377;&#25928;&#22320;&#25552;&#20379;&#26381;&#21153;&#12290;&#30693;&#35782;&#33976;&#39311;&#30340;&#25216;&#26415;&#21487;&#20197;&#22312;&#36825;&#37324;&#24212;&#29992;&#65292;&#20854;&#20013;&#24212;&#29992;&#27169;&#22411;&#23398;&#20250;&#27169;&#20223;&#22522;&#30784;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#19987;&#29992;&#24212;&#29992;&#27169;&#22411;&#21644;&#22522;&#30784;&#27169;&#22411;&#22312;&#23481;&#37327;&#19978;&#23384;&#22312;&#23454;&#36136;&#24615;&#24046;&#36317;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#26550;&#26500;&#65292;&#20351;&#29992;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#19981;&#21516;&#36755;&#20837;&#29305;&#24449;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#20998;&#24067;&#19978;&#36827;&#34892;&#20248;&#21270;&#12290;&#27169;&#22411;&#29305;&#24449;&#19978;&#30340;&#36825;&#20123;&#24046;&#24322;&#23548;&#33268;&#20102;&#33976;&#39311;&#26041;&#27861;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21019;&#24314;&#19968;&#20010;&#25945;&#23398;&#22996;&#21592;&#20250;&#65292;&#21253;&#25324;&#22522;&#30784;&#27169;&#22411;&#21644;&#19987;&#29992;&#24212;&#29992;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14035v1 Announce Type: cross  Abstract: Recent advancements in foundation models have yielded impressive performance across a wide range of tasks. Meanwhile, for specific applications, practitioners have been developing specialized application models. To enjoy the benefits of both kinds of models, one natural path is to transfer the knowledge in foundation models into specialized application models, which are generally more efficient for serving. Techniques from knowledge distillation may be applied here, where the application model learns to mimic the foundation model. However, specialized application models and foundation models have substantial gaps in capacity, employing distinct architectures, using different input features from different modalities, and being optimized on different distributions. These differences in model characteristics lead to significant challenges for distillation methods. In this work, we propose creating a teaching committee comprising both foun
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;AttackGNN&#65292;&#38024;&#23545;&#30828;&#20214;&#23433;&#20840;&#20013;&#20351;&#29992;&#30340;&#22522;&#20110;GNN&#30340;&#25216;&#26415;&#36827;&#34892;&#20102;&#39318;&#27425;&#32418;&#38431;&#25915;&#20987;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#29983;&#25104;&#23545;&#25239;&#24615;&#31034;&#20363;&#30005;&#36335;&#12290;</title><link>https://arxiv.org/abs/2402.13946</link><description>&lt;p&gt;
AttackGNN: &#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#30828;&#20214;&#23433;&#20840;&#20013;&#23545;GNN&#36827;&#34892;&#32418;&#38431;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
AttackGNN: Red-Teaming GNNs in Hardware Security Using Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13946
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;AttackGNN&#65292;&#38024;&#23545;&#30828;&#20214;&#23433;&#20840;&#20013;&#20351;&#29992;&#30340;&#22522;&#20110;GNN&#30340;&#25216;&#26415;&#36827;&#34892;&#20102;&#39318;&#27425;&#32418;&#38431;&#25915;&#20987;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#29983;&#25104;&#23545;&#25239;&#24615;&#31034;&#20363;&#30005;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#35299;&#20915;&#19968;&#20123;&#20851;&#38190;&#30340;&#30828;&#20214;&#23433;&#20840;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#20102;&#26497;&#22823;&#30340;&#28508;&#21147;&#12290;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#26032;&#39062;&#25216;&#26415;&#65292;&#29992;&#20110;&#26816;&#27979;&#30693;&#35782;&#20135;&#26435;&#65288;IP&#65289;&#30423;&#29256;&#12289;&#26816;&#27979;&#30828;&#20214;&#29305;&#27931;&#20234;&#26408;&#39532;&#65288;HTs&#65289;&#21644;&#21453;&#21521;&#24037;&#31243;&#30005;&#36335;&#31561;&#38382;&#39064;&#12290;&#36825;&#20123;&#25216;&#26415;&#34920;&#29616;&#20986;&#33394;&#65292;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;AttackGNN&#65292;&#36825;&#26159;&#38024;&#23545;&#30828;&#20214;&#23433;&#20840;&#20013;&#22522;&#20110;GNN&#30340;&#25216;&#26415;&#30340;&#31532;&#19968;&#20010;&#32418;&#38431;&#25915;&#20987;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#65292;&#29992;&#20110;&#29983;&#25104;&#38024;&#23545;GNN&#25216;&#26415;&#30340;&#23545;&#25239;&#31034;&#20363;&#65292;&#21363;&#30005;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13946v1 Announce Type: new  Abstract: Machine learning has shown great promise in addressing several critical hardware security problems. In particular, researchers have developed novel graph neural network (GNN)-based techniques for detecting intellectual property (IP) piracy, detecting hardware Trojans (HTs), and reverse engineering circuits, to name a few. These techniques have demonstrated outstanding accuracy and have received much attention in the community. However, since these techniques are used for security applications, it is imperative to evaluate them thoroughly and ensure they are robust and do not compromise the security of integrated circuits.   In this work, we propose AttackGNN, the first red-team attack on GNN-based techniques in hardware security. To this end, we devise a novel reinforcement learning (RL) agent that generates adversarial examples, i.e., circuits, against the GNN-based techniques. We overcome three challenges related to effectiveness, scal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.13516</link><description>&lt;p&gt;
ProSparse: &#24341;&#20837;&#21644;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#28608;&#27963;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Activation sparsity&#25351;&#30340;&#26159;&#28608;&#27963;&#36755;&#20986;&#20013;&#23384;&#22312;&#35768;&#22810;&#24369;&#36129;&#29486;&#20803;&#32032;&#12290;&#20316;&#20026;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#27169;&#22411;&#30340;&#26222;&#36941;&#23646;&#24615;&#65292;&#24050;&#34987;&#35777;&#26126;&#26159;&#25552;&#39640;&#27169;&#22411;&#25512;&#29702;&#25928;&#29575;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#37319;&#29992;&#20102;&#27809;&#26377;&#20869;&#22312;&#28608;&#27963;&#31232;&#30095;&#24615;&#30340;&#28608;&#27963;&#20989;&#25968;&#65288;&#20363;&#22914;GELU&#21644;Swish&#65289;&#12290;&#19968;&#20123;&#26368;&#36817;&#30340;&#21162;&#21147;&#23581;&#35797;&#24341;&#20837;ReLU&#25110;&#20854;&#21464;&#20307;&#20316;&#20026;&#26367;&#20195;&#28608;&#27963;&#20989;&#25968;&#65292;&#20197;&#24110;&#21161;LLMs&#23454;&#29616;&#28608;&#27963;&#31232;&#30095;&#24615;&#21644;&#25512;&#29702;&#21152;&#36895;&#65292;&#20294;&#24456;&#23569;&#33021;&#21516;&#26102;&#33719;&#24471;&#39640;&#31232;&#30095;&#24230;&#21644;&#21487;&#27604;&#36739;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;LLMs&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23558;LLMs&#30340;&#28608;&#27963;&#20989;&#25968;&#26367;&#25442;&#20026;ReLU&#21518;&#65292;ProSparse&#37319;&#29992;&#28176;&#36827;&#31232;&#30095;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13516v1 Announce Type: cross  Abstract: Activation sparsity refers to the existence of considerable weakly-contributed elements among activation outputs. As a prevalent property of the models using the ReLU activation function, it has been proven a promising paradigm to boost model inference efficiency. Nevertheless, most large language models (LLMs) adopt activation functions without intrinsic activation sparsity (e.g., GELU and Swish). Some recent efforts have explored introducing ReLU or its variants as the substitutive activation function to help LLMs achieve activation sparsity and inference acceleration, but few can simultaneously obtain high sparsity and comparable model performance. This paper introduces an effective sparsification method named "ProSparse" to push LLMs for higher activation sparsity without decreasing model performance. Specifically, after substituting the activation function of LLMs with ReLU, ProSparse adopts progressive sparsity regularization wit
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#32852;&#37030;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#26088;&#22312;&#36866;&#24212;&#20219;&#24847;&#22240;&#26524;&#27169;&#22411;&#21644;&#24322;&#26500;&#25968;&#25454;&#65292;&#36890;&#36807;&#20351;&#29992;&#26367;&#20195;&#21464;&#37327;&#21644;&#32852;&#37030;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#26469;&#35299;&#20915;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#24182;&#24314;&#31435;&#20102;&#32852;&#37030;&#29420;&#31435;&#21464;&#21270;&#21407;&#21017;&#29992;&#20110;&#30830;&#23450;&#22240;&#26524;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.13241</link><description>&lt;p&gt;
&#26469;&#33258;&#24322;&#26500;&#25968;&#25454;&#30340;&#32852;&#37030;&#22240;&#26524;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Federated Causal Discovery from Heterogeneous Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13241
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#32852;&#37030;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#26088;&#22312;&#36866;&#24212;&#20219;&#24847;&#22240;&#26524;&#27169;&#22411;&#21644;&#24322;&#26500;&#25968;&#25454;&#65292;&#36890;&#36807;&#20351;&#29992;&#26367;&#20195;&#21464;&#37327;&#21644;&#32852;&#37030;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#26469;&#35299;&#20915;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#24182;&#24314;&#31435;&#20102;&#32852;&#37030;&#29420;&#31435;&#21464;&#21270;&#21407;&#21017;&#29992;&#20110;&#30830;&#23450;&#22240;&#26524;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#20381;&#36182;&#20110;&#38598;&#20013;&#24335;&#25968;&#25454;&#65292;&#36825;&#19982;&#35768;&#22810;&#23454;&#38469;&#24773;&#20917;&#19979;&#25968;&#25454;&#30340;&#20998;&#25955;&#24615;&#36136;&#19981;&#19968;&#33268;&#12290;&#36825;&#31181;&#24046;&#24322;&#25512;&#21160;&#20102;&#32852;&#37030;&#22240;&#26524;&#21457;&#29616;&#65288;FCD&#65289;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;FCD&#26041;&#27861;&#21487;&#33021;&#21463;&#21040;&#20854;&#23545;&#21487;&#35782;&#21035;&#21151;&#33021;&#22240;&#26524;&#27169;&#22411;&#25110; homogeneous&#25968;&#25454;&#20998;&#24067;&#30340;&#28508;&#22312;&#38480;&#21046;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23581;&#35797;&#36866;&#24212;&#20219;&#24847;&#22240;&#26524;&#27169;&#22411;&#21644;&#24322;&#26500;&#25968;&#25454;&#30340;&#26032;&#22411;FCD&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#19982;&#23458;&#25143;&#31471;&#32034;&#24341;&#23545;&#24212;&#30340;&#26367;&#20195;&#21464;&#37327;&#65292;&#20197;&#35299;&#20915;&#19981;&#21516;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#12290;&#28982;&#21518;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#22240;&#26524;&#39592;&#26550;&#21457;&#29616;&#30340;&#32852;&#37030;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#65288;FCIT&#65289;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#29992;&#20110;&#30830;&#23450;&#22240;&#26524;&#26041;&#21521;&#30340;&#32852;&#37030;&#29420;&#31435;&#21464;&#21270;&#21407;&#21017;&#65288;FICP&#65289;&#12290;&#36825;&#20123;&#26041;&#27861;&#28041;&#21450;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13241v1 Announce Type: cross  Abstract: Conventional causal discovery methods rely on centralized data, which is inconsistent with the decentralized nature of data in many real-world situations. This discrepancy has motivated the development of federated causal discovery (FCD) approaches. However, existing FCD methods may be limited by their potentially restrictive assumptions of identifiable functional causal models or homogeneous data distributions, narrowing their applicability in diverse scenarios. In this paper, we propose a novel FCD method attempting to accommodate arbitrary causal models and heterogeneous data. We first utilize a surrogate variable corresponding to the client index to account for the data heterogeneity across different clients. We then develop a federated conditional independence test (FCIT) for causal skeleton discovery and establish a federated independent change principle (FICP) to determine causal directions. These approaches involve constructing
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SMORE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20256;&#24863;&#22120;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#39046;&#22495;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#21033;&#29992;&#39640;&#32500;&#35745;&#31639;&#30340;&#39640;&#25928;&#21644;&#24182;&#34892;&#25805;&#20316;&#65292;&#21160;&#24577;&#23450;&#21046;&#27979;&#35797;&#27169;&#22411;&#20197;&#20943;&#36731;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#24102;&#26469;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2402.13233</link><description>&lt;p&gt;
&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#39640;&#32500;&#22495;&#33258;&#36866;&#24212;&#31639;&#27861;&#29992;&#20110;&#22810;&#20256;&#24863;&#22120;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
SMORE: Similarity-based Hyperdimensional Domain Adaptation for Multi-Sensor Time Series Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13233
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SMORE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20256;&#24863;&#22120;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#39046;&#22495;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#21033;&#29992;&#39640;&#32500;&#35745;&#31639;&#30340;&#39640;&#25928;&#21644;&#24182;&#34892;&#25805;&#20316;&#65292;&#21160;&#24577;&#23450;&#21046;&#27979;&#35797;&#27169;&#22411;&#20197;&#20943;&#36731;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#24102;&#26469;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29289;&#32852;&#32593;(IoT)&#30340;&#23454;&#38469;&#24212;&#29992;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;(ML)&#31639;&#27861;&#20998;&#26512;&#30001;&#30456;&#20114;&#36830;&#25509;&#30340;&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#26102;&#38388;&#24207;&#21015;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#37096;&#32626;&#22312;&#19982;&#35757;&#32451;&#25968;&#25454;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#19978;&#30340;&#27169;&#22411;&#26102;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;ML&#20013;&#22266;&#26377;&#30340;&#25361;&#25112;&#8212;&#8212;&#20998;&#24067;&#20559;&#31227;&#20250;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#38656;&#35201;&#25429;&#33719;&#22810;&#20256;&#24863;&#22120;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#22797;&#26434;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#24448;&#24448;&#36229;&#36807;&#20102;&#20170;&#22825;&#36793;&#32536;&#35774;&#22791;&#30340;&#33021;&#21147;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SMORE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#36164;&#28304;&#39640;&#25928;&#30340;&#22810;&#20256;&#24863;&#22120;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#39046;&#22495;&#33258;&#36866;&#24212;(DA)&#31639;&#27861;&#65292;&#21033;&#29992;&#20102;&#36229;&#39640;&#32500;&#35745;&#31639;&#30340;&#39640;&#25928;&#21644;&#24182;&#34892;&#25805;&#20316;&#12290; SMORE&#21160;&#24577;&#22320;&#23450;&#21046;&#27979;&#35797;&#27169;&#22411;&#65292;&#26126;&#30830;&#32771;&#34385;&#27599;&#20010;&#26679;&#26412;&#30340;&#39046;&#22495;&#19978;&#19979;&#25991;&#65292;&#20197;&#20943;&#36731;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13233v1 Announce Type: new  Abstract: Many real-world applications of the Internet of Things (IoT) employ machine learning (ML) algorithms to analyze time series information collected by interconnected sensors. However, distribution shift, a fundamental challenge in data-driven ML, arises when a model is deployed on a data distribution different from the training data and can substantially degrade model performance. Additionally, increasingly sophisticated deep neural networks (DNNs) are required to capture intricate spatial and temporal dependencies in multi-sensor time series data, often exceeding the capabilities of today's edge devices. In this paper, we propose SMORE, a novel resource-efficient domain adaptation (DA) algorithm for multi-sensor time series classification, leveraging the efficient and parallel operations of hyperdimensional computing. SMORE dynamically customizes test-time models with explicit consideration of the domain context of each sample to mitigate
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#30340;DLO&#23454;&#20363;&#20998;&#21106;&#25216;&#26415;&#65292;&#32467;&#21512;&#20102;CLIPSeg&#27169;&#22411;&#30340;&#25991;&#26412;&#26465;&#20214;&#35821;&#20041;&#20998;&#21106;&#33021;&#21147;&#21644;Segment Anything Model&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#24863;&#30693;&#21487;&#21464;&#24418;&#30452;&#32447;&#23545;&#35937;&#22914;&#30005;&#32447;&#12289;&#30005;&#32518;&#21644;&#26580;&#24615;&#31649;&#36947;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#24615;&#33021;&#36229;&#36234;&#20102;&#30446;&#21069;&#30340;&#25216;&#26415;&#27700;&#24179;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;DLO&#29305;&#23450;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.11996</link><description>&lt;p&gt;
ISCUTE&#65306;&#20351;&#29992;&#25991;&#26412;&#23884;&#20837;&#36827;&#34892;&#30005;&#32518;&#23454;&#20363;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
ISCUTE: Instance Segmentation of Cables Using Text Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11996
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#30340;DLO&#23454;&#20363;&#20998;&#21106;&#25216;&#26415;&#65292;&#32467;&#21512;&#20102;CLIPSeg&#27169;&#22411;&#30340;&#25991;&#26412;&#26465;&#20214;&#35821;&#20041;&#20998;&#21106;&#33021;&#21147;&#21644;Segment Anything Model&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#24863;&#30693;&#21487;&#21464;&#24418;&#30452;&#32447;&#23545;&#35937;&#22914;&#30005;&#32447;&#12289;&#30005;&#32518;&#21644;&#26580;&#24615;&#31649;&#36947;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#24615;&#33021;&#36229;&#36234;&#20102;&#30446;&#21069;&#30340;&#25216;&#26415;&#27700;&#24179;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;DLO&#29305;&#23450;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#25216;&#26415;&#21644;&#33258;&#21160;&#21270;&#39046;&#22495;&#65292;&#20256;&#32479;&#30340;&#23545;&#35937;&#35782;&#21035;&#21644;&#23454;&#20363;&#20998;&#21106;&#26041;&#27861;&#22312;&#24863;&#30693;&#31867;&#20284;&#30005;&#32447;&#12289;&#30005;&#32518;&#21644;&#26580;&#24615;&#31649;&#36947;&#31561;&#21487;&#21464;&#24418;&#30452;&#32447;&#23545;&#35937;&#65288;DLOs&#65289;&#26041;&#38754;&#38754;&#20020;&#30528;&#24040;&#22823;&#25361;&#25112;&#12290;&#36825;&#19968;&#25361;&#25112;&#20027;&#35201;&#28304;&#20110;&#32570;&#20047;&#24418;&#29366;&#12289;&#39068;&#33394;&#21644;&#32441;&#29702;&#31561;&#26126;&#26174;&#23646;&#24615;&#65292;&#36825;&#38656;&#35201;&#37327;&#36523;&#23450;&#21046;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#23454;&#29616;&#31934;&#30830;&#35782;&#21035;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#30340;&#12289;&#29992;&#25143;&#21451;&#22909;&#30340;DLO&#23454;&#20363;&#20998;&#21106;&#25216;&#26415;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;CLIPSeg&#27169;&#22411;&#30340;&#25991;&#26412;&#26465;&#20214;&#35821;&#20041;&#20998;&#21106;&#33021;&#21147;&#21644;Segment Anything Model (SAM) &#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;DLO&#23454;&#20363;&#20998;&#21106;&#26041;&#38754;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;$91.21\%$&#30340;&#24179;&#22343;&#20132;&#24182;&#27604;&#65288;mIoU&#65289;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#20016;&#23500;&#22810;&#26679;&#30340;&#29992;&#20110;&#23454;&#20363;&#20998;&#21106;&#30340;DLO&#29305;&#23450;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11996v1 Announce Type: cross  Abstract: In the field of robotics and automation, conventional object recognition and instance segmentation methods face a formidable challenge when it comes to perceiving Deformable Linear Objects (DLOs) like wires, cables, and flexible tubes. This challenge arises primarily from the lack of distinct attributes such as shape, color, and texture, which calls for tailored solutions to achieve precise identification. In this work, we propose a foundation model-based DLO instance segmentation technique that is text-promptable and user-friendly. Specifically, our approach combines the text-conditioned semantic segmentation capabilities of CLIPSeg model with the zero-shot generalization capabilities of Segment Anything Model (SAM). We show that our method exceeds SOTA performance on DLO instance segmentation, achieving a mIoU of $91.21\%$. We also introduce a rich and diverse DLO-specific dataset for instance segmentation.
&lt;/p&gt;</description></item><item><title>&#21457;&#29616;&#28145;&#23618;ReLU&#32593;&#32476;&#34920;&#29616;&#20986;&#36807;&#24230;&#27867;&#21270;&#29616;&#35937;&#65292;&#21033;&#29992;&#36825;&#19968;&#29305;&#24615;&#35774;&#35745;&#20102;&#8220;&#29983;&#25104;&#19975;&#33457;&#31570;&#32593;&#32476;&#8221;&#65292;&#36890;&#36807;&#36882;&#24402;&#26144;&#23556;&#38543;&#26426;&#36755;&#20837;&#22122;&#22768;&#29983;&#25104;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.11793</link><description>&lt;p&gt;
&#29983;&#25104;&#19975;&#33457;&#31570;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Generative Kaleidoscopic Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11793
&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#28145;&#23618;ReLU&#32593;&#32476;&#34920;&#29616;&#20986;&#36807;&#24230;&#27867;&#21270;&#29616;&#35937;&#65292;&#21033;&#29992;&#36825;&#19968;&#29305;&#24615;&#35774;&#35745;&#20102;&#8220;&#29983;&#25104;&#19975;&#33457;&#31570;&#32593;&#32476;&#8221;&#65292;&#36890;&#36807;&#36882;&#24402;&#26144;&#23556;&#38543;&#26426;&#36755;&#20837;&#22122;&#22768;&#29983;&#25104;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#28145;&#23618;ReLU&#32593;&#32476;&#65288;&#25110;&#22810;&#23618;&#24863;&#30693;&#22120;&#26550;&#26500;&#65289;&#34920;&#29616;&#20986;&#8220;&#36807;&#24230;&#27867;&#21270;&#8221;&#29616;&#35937;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#37027;&#20123;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#27809;&#26377;&#30475;&#21040;&#30340;&#36755;&#20837;&#30340;&#36755;&#20986;&#20540;&#34987;&#26144;&#23556;&#21040;&#20102;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#35266;&#23519;&#21040;&#30340;&#36755;&#20986;&#33539;&#22260;&#38468;&#36817;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#22810;&#23618;&#24863;&#30693;&#22120;&#23398;&#20064;&#20102;&#19968;&#23545;&#22810;&#30340;&#26144;&#23556;&#65292;&#36825;&#31181;&#25928;&#24212;&#22312;&#22686;&#21152;&#23618;&#25968;&#25110;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#28145;&#24230;&#26102;&#26356;&#20026;&#26126;&#26174;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#28145;&#23618;ReLU&#32593;&#32476;&#30340;&#36825;&#19968;&#29305;&#24615;&#26469;&#35774;&#35745;&#19968;&#20010;&#25968;&#25454;&#38598;&#19975;&#33457;&#31570;&#65292;&#31216;&#20026;&#8220;&#29983;&#25104;&#19975;&#33457;&#31570;&#32593;&#32476;&#8221;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#22914;&#26524;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;&#22810;&#23618;&#24863;&#30693;&#22120;&#23558;&#36755;&#20837; $x\in\mathbb{R}^D$ &#26144;&#23556;&#21040;&#33258;&#36523; $f_\mathcal{N}(x)\rightarrow x$&#65292;&#37027;&#20040;&#8220;&#19975;&#33457;&#31570;&#37319;&#26679;&#8221;&#36807;&#31243;&#23558;&#20174;&#38543;&#26426;&#36755;&#20837;&#22122;&#22768; $z\in\mathbb{R}^D$ &#24320;&#22987;&#65292;&#24182;&#36882;&#24402;&#22320;&#24212;&#29992; $f_\mathcal{N}(\cdots f_\mathcal{N}(z)\cdots )$&#12290;&#32463;&#36807;&#29123;&#28903;&#26399;&#21518;&#65292;&#25105;&#20204;&#24320;&#22987;&#35266;&#23519;&#26469;&#33258;&#36755;&#20837;&#20998;&#24067;&#30340;&#26679;&#26412;&#65292;&#25105;&#20204;&#21457;&#29616;&#26356;&#28145;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11793v1 Announce Type: cross  Abstract: We discovered that the Deep ReLU networks (or Multilayer Perceptron architecture) demonstrate an 'over-generalization' phenomenon. That is, the output values for the inputs that were not seen during training are mapped close to the output range that were observed during the learning process. In other words, the MLP learns a many-to-one mapping and this effect is more prominent as we increase the number of layers or depth of the MLP. We utilize this property of Deep ReLU networks to design a dataset kaleidoscope, termed as 'Generative Kaleidoscopic Networks'. Briefly, if we learn a MLP to map from input $x\in\mathbb{R}^D$ to itself $f_\mathcal{N}(x)\rightarrow x$, the 'Kaleidoscopic sampling' procedure starts with a random input noise $z\in\mathbb{R}^D$ and recursively applies $f_\mathcal{N}(\cdots f_\mathcal{N}(z)\cdots )$. After a burn-in period duration, we start observing samples from the input distribution and we found that deeper 
&lt;/p&gt;</description></item><item><title>PolypNextLSTM&#26159;&#19968;&#20010;&#36731;&#37327;&#19988;&#24555;&#36895;&#30340;&#24687;&#32905;&#35270;&#39057;&#20998;&#21106;&#32593;&#32476;&#65292;&#20351;&#29992;ConvNext&#21644;ConvLSTM&#65292;&#26368;&#22823;&#30340;&#21019;&#26032;&#22312;&#20110;&#21442;&#25968;&#26368;&#23569;&#19988;&#36895;&#24230;&#26368;&#24555;&#65292;&#24615;&#33021;&#36229;&#36234;&#20102;&#20116;&#31181;&#20808;&#36827;&#30340;&#22522;&#20110;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.11585</link><description>&lt;p&gt;
PolypNextLSTM&#65306;&#20351;&#29992;ConvNext&#21644;ConvLSTM&#30340;&#36731;&#37327;&#32423;&#24555;&#36895;&#24687;&#32905;&#35270;&#39057;&#20998;&#21106;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
PolypNextLSTM: A lightweight and fast polyp video segmentation network using ConvNext and ConvLSTM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11585
&lt;/p&gt;
&lt;p&gt;
PolypNextLSTM&#26159;&#19968;&#20010;&#36731;&#37327;&#19988;&#24555;&#36895;&#30340;&#24687;&#32905;&#35270;&#39057;&#20998;&#21106;&#32593;&#32476;&#65292;&#20351;&#29992;ConvNext&#21644;ConvLSTM&#65292;&#26368;&#22823;&#30340;&#21019;&#26032;&#22312;&#20110;&#21442;&#25968;&#26368;&#23569;&#19988;&#36895;&#24230;&#26368;&#24555;&#65292;&#24615;&#33021;&#36229;&#36234;&#20102;&#20116;&#31181;&#20808;&#36827;&#30340;&#22522;&#20110;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#29992;&#20110;&#24687;&#32905;&#20998;&#21106;&#30340;&#21333;&#22270;&#20687;UNet&#26550;&#26500;&#32570;&#20047;&#20020;&#24202;&#21307;&#29983;&#22312;&#35786;&#26029;&#24687;&#32905;&#26102;&#20174;&#35270;&#39057;&#25968;&#25454;&#20013;&#33719;&#24471;&#30340;&#26102;&#38388;&#27934;&#23519;&#12290;&#20026;&#20102;&#26356;&#24544;&#23454;&#22320;&#21453;&#26144;&#20020;&#24202;&#23454;&#36341;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;PolypNextLSTM&#21033;&#29992;&#22522;&#20110;&#35270;&#39057;&#30340;&#28145;&#24230;&#23398;&#20064;&#65292;&#21033;&#29992;&#26102;&#38388;&#20449;&#24687;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#20998;&#21106;&#24615;&#33021;&#65292;&#21442;&#25968;&#24320;&#38144;&#26368;&#23567;&#65292;&#21487;&#33021;&#36866;&#29992;&#20110;&#36793;&#32536;&#35774;&#22791;&#12290;PolypNextLSTM&#37319;&#29992;&#31867;&#20284;UNet&#30340;&#32467;&#26500;&#65292;ConvNext-Tiny&#20316;&#20026;&#20854;&#20027;&#24178;&#65292;&#31574;&#30053;&#24615;&#22320;&#30465;&#30053;&#26368;&#21518;&#20004;&#23618;&#20197;&#20943;&#23569;&#21442;&#25968;&#24320;&#38144;&#12290;&#25105;&#20204;&#30340;&#26102;&#38388;&#34701;&#21512;&#27169;&#22359;&#65292;&#19968;&#20010;&#21367;&#31215;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;ConvLSTM&#65289;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#26102;&#38388;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21019;&#26032;&#22312;&#20110;PolypNextLSTM&#65292;&#22312;&#21442;&#25968;&#19978;&#26368;&#30246;&#19988;&#36895;&#24230;&#26368;&#24555;&#65292;&#36229;&#36234;&#20102;&#20116;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;SUN-SEG&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#36328;&#36234;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11585v1 Announce Type: cross  Abstract: Commonly employed in polyp segmentation, single image UNet architectures lack the temporal insight clinicians gain from video data in diagnosing polyps. To mirror clinical practices more faithfully, our proposed solution, PolypNextLSTM, leverages video-based deep learning, harnessing temporal information for superior segmentation performance with the least parameter overhead, making it possibly suitable for edge devices. PolypNextLSTM employs a UNet-like structure with ConvNext-Tiny as its backbone, strategically omitting the last two layers to reduce parameter overhead. Our temporal fusion module, a Convolutional Long Short Term Memory (ConvLSTM), effectively exploits temporal features. Our primary novelty lies in PolypNextLSTM, which stands out as the leanest in parameters and the fastest model, surpassing the performance of five state-of-the-art image and video-based deep learning models. The evaluation of the SUN-SEG dataset spans 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26469;&#25913;&#36827;&#32763;&#35793;&#36136;&#37327;&#30340;&#25104;&#26412;&#25928;&#30410;&#20559;&#22909;&#23398;&#20064;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#20248;&#21270;&#22870;&#21169;&#27169;&#22411;&#26469;&#21306;&#20998;&#20154;&#31867;&#21644;&#26426;&#22120;&#32763;&#35793;&#65292;&#20174;&#32780;&#25351;&#23548;&#25913;&#36827;&#26426;&#22120;&#32763;&#35793;&#12290;</title><link>https://arxiv.org/abs/2402.11525</link><description>&lt;p&gt;
&#29992;RLHF&#25512;&#36827;&#32763;&#35793;&#20559;&#22909;&#24314;&#27169;&#65306;&#36808;&#21521;&#25104;&#26412;&#25928;&#30410;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Advancing Translation Preference Modeling with RLHF: A Step Towards Cost-Effective Solution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11525
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26469;&#25913;&#36827;&#32763;&#35793;&#36136;&#37327;&#30340;&#25104;&#26412;&#25928;&#30410;&#20559;&#22909;&#23398;&#20064;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#20248;&#21270;&#22870;&#21169;&#27169;&#22411;&#26469;&#21306;&#20998;&#20154;&#31867;&#21644;&#26426;&#22120;&#32763;&#35793;&#65292;&#20174;&#32780;&#25351;&#23548;&#25913;&#36827;&#26426;&#22120;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11525v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#30495;&#23454;&#24615;&#12289;&#34920;&#36798;&#21147;&#21644;&#20248;&#38597;&#26159;&#26426;&#22120;&#32763;&#35793;&#20013;&#19981;&#26029;&#36861;&#27714;&#30340;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#24230;&#37327;&#26631;&#20934;&#22914;BLEU&#24182;&#19981;&#20005;&#26684;&#31526;&#21512;&#20154;&#31867;&#23545;&#32763;&#35793;&#36136;&#37327;&#30340;&#20559;&#22909;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26469;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;&#25910;&#38598;&#20154;&#31867;&#23545;&#32763;&#35793;&#20043;&#38388;&#30340;&#27604;&#36739;&#30340;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#24182;&#19981;&#23481;&#26131;&#65292;&#23588;&#20854;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#30340;&#20559;&#22909;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#21306;&#20998;&#20154;&#31867;&#21644;&#26426;&#22120;&#32763;&#35793;&#26469;&#20248;&#21270;&#22870;&#21169;&#27169;&#22411;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#22870;&#21169;&#27169;&#22411;&#23398;&#20064;&#26426;&#22120;&#32763;&#35793;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#24182;&#25351;&#23548;&#38543;&#21518;&#23545;&#26426;&#22120;&#32763;&#35793;&#30340;&#25913;&#36827;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RLHF&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#21319;&#32763;&#35793;&#36136;&#37327;&#65292;&#36825;&#31181;&#25913;&#36827;&#20063;&#26377;&#30410;&#20110;&#20854;&#20182;&#32763;&#35793;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11525v1 Announce Type: new  Abstract: Faithfulness, expressiveness, and elegance is the constant pursuit in machine translation. However, traditional metrics like \textit{BLEU} do not strictly align with human preference of translation quality. In this paper, we explore leveraging reinforcement learning with human feedback (\textit{RLHF}) to improve translation quality. It is non-trivial to collect a large high-quality dataset of human comparisons between translations, especially for low-resource languages. To address this issue, we propose a cost-effective preference learning strategy, optimizing reward models by distinguishing between human and machine translations. In this manner, the reward model learns the deficiencies of machine translation compared to human and guides subsequent improvements in machine translation. Experimental results demonstrate that \textit{RLHF} can effectively enhance translation quality and this improvement benefits other translation directions 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#20139;&#20102;&#20851;&#20110;&#22312;&#24037;&#19994;&#29615;&#22659;&#20013;&#20351;&#29992;AED&#31995;&#32479;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#36825;&#31181;&#29615;&#22659;&#20013;&#36866;&#24403;&#30340;&#30446;&#26631;&#21644;&#31995;&#32479;&#35268;&#26684;&#30340;&#35270;&#35282;&#65292;&#26368;&#32456;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#21453;&#20107;&#23454;&#25512;&#26029;&#30340;AED&#26694;&#26550;&#24182;&#22312;&#21830;&#19994;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;</title><link>https://arxiv.org/abs/2402.10870</link><description>&lt;p&gt;
&#19977;&#30028;&#20043;&#26368;&#65306;&#23454;&#36341;&#20013;&#30340;&#25968;&#23383;&#33829;&#38144;&#33258;&#36866;&#24212;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Best of Three Worlds: Adaptive Experimentation for Digital Marketing in Practice
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#20139;&#20102;&#20851;&#20110;&#22312;&#24037;&#19994;&#29615;&#22659;&#20013;&#20351;&#29992;AED&#31995;&#32479;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#36825;&#31181;&#29615;&#22659;&#20013;&#36866;&#24403;&#30340;&#30446;&#26631;&#21644;&#31995;&#32479;&#35268;&#26684;&#30340;&#35270;&#35282;&#65292;&#26368;&#32456;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#21453;&#20107;&#23454;&#25512;&#26029;&#30340;AED&#26694;&#26550;&#24182;&#22312;&#21830;&#19994;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#23454;&#39564;&#35774;&#35745;&#65288;AED&#65289;&#26041;&#27861;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#24037;&#19994;&#30028;&#29992;&#20316;&#19968;&#31181;&#24037;&#20855;&#65292;&#20197;&#25552;&#39640;&#27979;&#35797;&#21534;&#21520;&#37327;&#25110;&#20943;&#23569;&#19982;&#20256;&#32479;A/B/N&#27979;&#35797;&#26041;&#27861;&#30456;&#27604;&#30340;&#23454;&#39564;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#34892;&#20026;&#21644;&#20445;&#35777;&#22312;&#29702;&#24819;&#21270;&#30340;&#31283;&#24577;&#35774;&#32622;&#20043;&#22806;&#24182;&#19981;&#20026;&#20154;&#29087;&#30693;&#12290;&#26412;&#25991;&#20998;&#20139;&#20102;&#26377;&#20851;&#22312;&#24037;&#19994;&#29615;&#22659;&#20013;&#22825;&#30495;&#22320;&#20351;&#29992;AED&#31995;&#32479;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#22312;&#36825;&#31181;&#29615;&#22659;&#20013;&#36866;&#24403;&#30340;&#30446;&#26631;&#21644;&#31995;&#32479;&#35268;&#26684;&#30340;&#35270;&#35282;&#12290;&#25105;&#20204;&#26681;&#25454;&#36825;&#20123;&#32463;&#39564;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#21453;&#20107;&#23454;&#25512;&#26029;&#30340;AED&#26694;&#26550;&#65292;&#24182;&#22312;&#21830;&#19994;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10870v1 Announce Type: new  Abstract: Adaptive experimental design (AED) methods are increasingly being used in industry as a tool to boost testing throughput or reduce experimentation cost relative to traditional A/B/N testing methods. However, the behavior and guarantees of such methods are not well-understood beyond idealized stationary settings. This paper shares lessons learned regarding the challenges of naively using AED systems in industrial settings where non-stationarity is prevalent, while also providing perspectives on the proper objectives and system specifications in such settings. We developed an AED framework for counterfactual inference based on these experiences, and tested it in a commercial environment.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;Johnson-Lindenstrauss&#65288;JL&#65289;&#24341;&#29702;&#30340;&#31616;&#21333;&#32479;&#19968;&#20998;&#26512;&#65292;&#31616;&#21270;&#21644;&#32479;&#19968;&#20102;&#21508;&#31181;&#26500;&#36896;&#65292;&#21253;&#25324;&#29699;&#24418;&#12289;&#39640;&#26031;&#12289;&#20108;&#36827;&#21046;&#30828;&#24065;&#21644;&#27425;&#39640;&#26031;&#27169;&#22411;&#65292;&#36890;&#36807;&#21019;&#26032;&#24615;&#22320;&#23558;Hanson-Wright&#19981;&#31561;&#24335;&#25299;&#23637;&#21040;&#39640;&#32500;&#24230;&#65292;&#26631;&#24535;&#30528;&#23545;&#25968;&#25454;&#22266;&#26377;&#20960;&#20309;&#30340;&#20445;&#25345;&#21462;&#24471;&#37325;&#22823;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.10232</link><description>&lt;p&gt;
Johnson-Lindenstrauss&#30340;&#31616;&#21333;&#32479;&#19968;&#20998;&#26512;&#21450;&#20854;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Simple, unified analysis of Johnson-Lindenstrauss with applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10232
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;Johnson-Lindenstrauss&#65288;JL&#65289;&#24341;&#29702;&#30340;&#31616;&#21333;&#32479;&#19968;&#20998;&#26512;&#65292;&#31616;&#21270;&#21644;&#32479;&#19968;&#20102;&#21508;&#31181;&#26500;&#36896;&#65292;&#21253;&#25324;&#29699;&#24418;&#12289;&#39640;&#26031;&#12289;&#20108;&#36827;&#21046;&#30828;&#24065;&#21644;&#27425;&#39640;&#26031;&#27169;&#22411;&#65292;&#36890;&#36807;&#21019;&#26032;&#24615;&#22320;&#23558;Hanson-Wright&#19981;&#31561;&#24335;&#25299;&#23637;&#21040;&#39640;&#32500;&#24230;&#65292;&#26631;&#24535;&#30528;&#23545;&#25968;&#25454;&#22266;&#26377;&#20960;&#20309;&#30340;&#20445;&#25345;&#21462;&#24471;&#37325;&#22823;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Johnson-Lindenstrauss&#65288;JL&#65289;&#24341;&#29702;&#30340;&#31616;&#21333;&#32479;&#19968;&#20998;&#26512;&#65292;&#36825;&#26159;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#33267;&#20851;&#37325;&#35201;&#30340;&#38477;&#32500;&#39046;&#22495;&#20013;&#30340;&#22522;&#30707;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#31616;&#21270;&#20102;&#29702;&#35299;&#65292;&#36824;&#23558;&#21508;&#31181;&#26500;&#36896;&#32479;&#19968;&#21040;JL&#26694;&#26550;&#19979;&#65292;&#21253;&#25324;&#29699;&#24418;&#12289;&#39640;&#26031;&#12289;&#20108;&#36827;&#21046;&#30828;&#24065;&#21644;&#27425;&#39640;&#26031;&#27169;&#22411;&#12290;&#36825;&#31181;&#31616;&#21270;&#21644;&#32479;&#19968;&#22312;&#20445;&#25345;&#25968;&#25454;&#22266;&#26377;&#20960;&#20309;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#23545;&#20174;&#27969;&#31639;&#27861;&#21040;&#24378;&#21270;&#23398;&#20064;&#31561;&#21508;&#31181;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;&#36825;&#20010;&#31616;&#21270;&#26694;&#26550;&#20869;&#25552;&#20986;&#20102;&#29699;&#24418;&#26500;&#36896;&#26377;&#25928;&#24615;&#30340;&#31532;&#19968;&#20010;&#20005;&#26684;&#35777;&#26126;&#12290;&#25105;&#20204;&#36129;&#29486;&#30340;&#26680;&#24515;&#26159;&#23558;Hanson-Wright&#19981;&#31561;&#24335;&#25299;&#23637;&#21040;&#39640;&#32500;&#24230;&#65292;&#20855;&#26377;&#26126;&#30830;&#30340;&#24120;&#25968;&#65292;&#36825;&#26631;&#24535;&#30528;&#25991;&#29486;&#20013;&#36136;&#30340;&#39134;&#36291;&#12290;&#36890;&#36807;&#36816;&#29992;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#27010;&#29575;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10232v1 Announce Type: new  Abstract: In this work, we present a simple and unified analysis of the Johnson-Lindenstrauss (JL) lemma, a cornerstone in the field of dimensionality reduction critical for managing high-dimensional data. Our approach not only simplifies the understanding but also unifies various constructions under the JL framework, including spherical, Gaussian, binary coin, and sub-Gaussian models. This simplification and unification make significant strides in preserving the intrinsic geometry of data, essential across diverse applications from streaming algorithms to reinforcement learning. Notably, we deliver the first rigorous proof of the spherical construction's effectiveness within this simplified framework. At the heart of our contribution is an innovative extension of the Hanson-Wright inequality to high dimensions, complete with explicit constants, marking a substantial leap in the literature. By employing simple yet powerful probabilistic tools and 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#31995;&#32479;&#24615;&#22238;&#39038;&#20840;&#38754;&#20998;&#26512;&#20102;&#25968;&#25454;&#21040;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30740;&#31350;&#30340;&#29616;&#29366;&#65292;&#25552;&#20986;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#35299;&#20915;&#20102;&#30456;&#20851;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.08496</link><description>&lt;p&gt;
&#25968;&#25454;&#21040;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30740;&#31350;&#30340;&#31995;&#32479;&#24615;&#22238;&#39038;
&lt;/p&gt;
&lt;p&gt;
A Systematic Review of Data-to-Text NLG
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08496
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#31995;&#32479;&#24615;&#22238;&#39038;&#20840;&#38754;&#20998;&#26512;&#20102;&#25968;&#25454;&#21040;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30740;&#31350;&#30340;&#29616;&#29366;&#65292;&#25552;&#20986;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#35299;&#20915;&#20102;&#30456;&#20851;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#31995;&#32479;&#24615;&#22238;&#39038;&#26088;&#22312;&#20840;&#38754;&#20998;&#26512;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#30740;&#31350;&#30340;&#29616;&#29366;&#65292;&#37325;&#28857;&#26159;&#30830;&#23450;&#30740;&#31350;&#31354;&#30333;&#65292;&#25552;&#20379;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#35299;&#20915;&#22238;&#39038;&#20013;&#21457;&#29616;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#23545;&#25991;&#29486;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#26816;&#26597;&#65292;&#21253;&#25324;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#12289;&#24212;&#29992;&#12289;&#22810;&#35821;&#35328;&#24615;&#21644;&#24187;&#35273;&#32531;&#35299;&#25514;&#26045;&#12290;&#25105;&#20204;&#30340;&#22238;&#39038;&#20026;&#36825;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This systematic review aims to provide a comprehensive analysis of the state of data-to-text generation research, focusing on identifying research gaps, offering future directions, and addressing challenges found during the review. We thoroughly examined the literature, including approaches, datasets, evaluation metrics, applications, multilingualism, and hallucination mitigation measures. Our review provides a roadmap for future research in this rapidly evolving field.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#36890;&#20449;&#22797;&#26434;&#24615;&#35777;&#26126;&#20102;Transformer&#23618;&#22312;&#22788;&#29702;&#20989;&#25968;&#32452;&#21512;&#20219;&#21153;&#26102;&#30340;&#23616;&#38480;&#24615;&#65292;&#25351;&#20986;&#23545;&#20110;&#22823;&#22411;&#23450;&#20041;&#22495;&#21644;&#26576;&#20123;&#25968;&#23398;&#20219;&#21153;&#65292;Transformers&#21487;&#33021;&#26080;&#27861;&#35299;&#20915;&#12290;</title><link>https://arxiv.org/abs/2402.08164</link><description>&lt;p&gt;
&#20851;&#20110;Transformer&#26550;&#26500;&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
On Limitations of the Transformer Architecture
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#36890;&#20449;&#22797;&#26434;&#24615;&#35777;&#26126;&#20102;Transformer&#23618;&#22312;&#22788;&#29702;&#20989;&#25968;&#32452;&#21512;&#20219;&#21153;&#26102;&#30340;&#23616;&#38480;&#24615;&#65292;&#25351;&#20986;&#23545;&#20110;&#22823;&#22411;&#23450;&#20041;&#22495;&#21644;&#26576;&#20123;&#25968;&#23398;&#20219;&#21153;&#65292;Transformers&#21487;&#33021;&#26080;&#27861;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#24187;&#35273;&#30340;&#26681;&#26412;&#21407;&#22240;&#26159;&#20160;&#20040;&#65311;&#25105;&#20204;&#20351;&#29992;&#36890;&#20449;&#22797;&#26434;&#24615;&#26469;&#35777;&#26126;&#65292;&#22914;&#26524;&#20989;&#25968;&#30340;&#23450;&#20041;&#22495;&#36275;&#22815;&#22823;&#65292;Transformer&#23618;&#26080;&#27861;&#32452;&#21512;&#20989;&#25968;&#65288;&#20363;&#22914;&#65292;&#22312;&#23478;&#35889;&#20013;&#26597;&#25214;&#19968;&#20010;&#20154;&#30340;&#31062;&#29238;&#65289;&#65307;&#25105;&#20204;&#36890;&#36807;&#31034;&#20363;&#26174;&#31034;&#65292;&#24403;&#23450;&#20041;&#22495;&#30456;&#24403;&#23567;&#30340;&#26102;&#20505;&#65292;&#36825;&#31181;&#33021;&#21147;&#30340;&#32570;&#20047;&#24050;&#32463;&#22312;&#32463;&#39564;&#19978;&#23384;&#22312;&#12290;&#25105;&#20204;&#36824;&#25351;&#20986;&#65292;&#35768;&#22810;&#22312;&#25152;&#35859;&#30340;&#32452;&#21512;&#20219;&#21153;&#20013;&#30340;&#25968;&#23398;&#20219;&#21153;&#65292;&#35748;&#20026;&#23427;&#20204;&#23545;LLMs&#26469;&#35828;&#24456;&#38590;&#35299;&#20915;&#65292;&#23545;&#20110;&#36275;&#22815;&#22823;&#30340;&#23454;&#20363;&#26469;&#35828;&#65292;&#19988;&#20551;&#35774;&#35745;&#31639;&#22797;&#26434;&#24615;&#39046;&#22495;&#30340;&#26576;&#20123;&#34987;&#24191;&#27867;&#25509;&#21463;&#30340;&#29468;&#24819;&#26159;&#27491;&#30830;&#30340;&#65292;Transformers&#20063;&#19981;&#22826;&#21487;&#33021;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
What are the root causes of hallucinations in large language models (LLMs)? We use Communication Complexity to prove that the Transformer layer is incapable of composing functions (e.g., identify a grandparent of a person in a genealogy) if the domains of the functions are large enough; we show through examples that this inability is already empirically present when the domains are quite small. We also point out that several mathematical tasks that are at the core of the so-called compositional tasks thought to be hard for LLMs are unlikely to be solvable by Transformers, for large enough instances and assuming that certain well accepted conjectures in the field of Computational Complexity are true.
&lt;/p&gt;</description></item><item><title>TriAug&#26159;&#19968;&#20010;&#29992;&#20110;&#20083;&#33146;&#36229;&#22768;&#22270;&#20687;&#30340;&#24322;&#24120;&#26679;&#26412;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#19977;&#20803;&#29366;&#24577;&#22686;&#24378;&#21644;&#24179;&#34913;&#30340;&#29699;&#24418;&#25439;&#22833;&#26469;&#25552;&#39640;&#31034;&#36394;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#24322;&#24120;&#26679;&#26412;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07452</link><description>&lt;p&gt;
TriAug&#65306;&#29992;&#20110;&#36229;&#22768;&#20083;&#33146;&#30149;&#21464;&#19981;&#24179;&#34913;&#20998;&#31867;&#30340;&#24322;&#24120;&#26679;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
TriAug: Out-of-Distribution Detection for Robust Classification of Imbalanced Breast Lesion in Ultrasound
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07452
&lt;/p&gt;
&lt;p&gt;
TriAug&#26159;&#19968;&#20010;&#29992;&#20110;&#20083;&#33146;&#36229;&#22768;&#22270;&#20687;&#30340;&#24322;&#24120;&#26679;&#26412;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#19977;&#20803;&#29366;&#24577;&#22686;&#24378;&#21644;&#24179;&#34913;&#30340;&#29699;&#24418;&#25439;&#22833;&#26469;&#25552;&#39640;&#31034;&#36394;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#24322;&#24120;&#26679;&#26412;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#30340;&#30142;&#30149;&#65292;&#22914;&#20083;&#33146;&#30149;&#21464;&#30340;&#32452;&#32455;&#20122;&#22411;&#65292;&#20855;&#26377;&#20005;&#37325;&#19981;&#21516;&#30340;&#21457;&#30149;&#29575;&#12290;&#21363;&#20351;&#36890;&#36807;&#22823;&#37327;&#30340;&#31034;&#36394;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#27169;&#22411;&#22312;&#20020;&#24202;&#23454;&#38469;&#20013;&#36890;&#24120;&#36935;&#21040;&#23646;&#20110;&#26410;&#35265;&#31867;&#21035;&#30340;&#24322;&#24120;&#26679;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;&#20083;&#33146;&#36229;&#22768;&#22270;&#20687;&#30340;&#38271;&#23614;&#24322;&#24120;&#26679;&#26412;&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#37197;&#22791;&#20102;&#19968;&#31181;&#19977;&#20803;&#29366;&#24577;&#22686;&#24378;&#65288;TriAug&#65289;&#65292;&#23427;&#21487;&#20197;&#25552;&#39640;&#31034;&#36394;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#33391;&#22909;&#30340;&#24322;&#24120;&#26679;&#26412;&#26816;&#27979;&#24615;&#33021;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#24179;&#34913;&#30340;&#29699;&#24418;&#25439;&#22833;&#26469;&#22788;&#29702;&#31867;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Different diseases, such as histological subtypes of breast lesions, have severely varying incidence rates. Even trained with substantial amount of in-distribution (ID) data, models often encounter out-of-distribution (OOD) samples belonging to unseen classes in clinical reality. To address this, we propose a novel framework built upon a long-tailed OOD detection task for breast ultrasound images. It is equipped with a triplet state augmentation (TriAug) which improves ID classification accuracy while maintaining a promising OOD detection performance. Meanwhile, we designed a balanced sphere loss to handle the class imbalanced problem.
&lt;/p&gt;</description></item><item><title>HarmBench&#26159;&#19968;&#20010;&#20026;&#33258;&#21160;&#32418;&#38431;&#21644;&#24378;&#22823;&#25298;&#32477;&#35774;&#35745;&#30340;&#26631;&#20934;&#21270;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;18&#31181;&#32418;&#38431;&#26041;&#27861;&#21644;33&#20010;&#30446;&#26631;LLM&#21644;&#38450;&#24481;&#30340;&#27604;&#36739;&#65292;&#24471;&#20986;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;LLM&#22312;&#21508;&#31181;&#25915;&#20987;&#19979;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04249</link><description>&lt;p&gt;
HarmBench&#65306;&#29992;&#20110;&#33258;&#21160;&#32418;&#38431;&#21644;&#24378;&#22823;&#25298;&#32477;&#30340;&#26631;&#20934;&#21270;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04249
&lt;/p&gt;
&lt;p&gt;
HarmBench&#26159;&#19968;&#20010;&#20026;&#33258;&#21160;&#32418;&#38431;&#21644;&#24378;&#22823;&#25298;&#32477;&#35774;&#35745;&#30340;&#26631;&#20934;&#21270;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;18&#31181;&#32418;&#38431;&#26041;&#27861;&#21644;33&#20010;&#30446;&#26631;LLM&#21644;&#38450;&#24481;&#30340;&#27604;&#36739;&#65292;&#24471;&#20986;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;LLM&#22312;&#21508;&#31181;&#25915;&#20987;&#19979;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#32418;&#38431;&#20855;&#26377;&#21457;&#29616;&#21644;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24694;&#24847;&#20351;&#29992;&#30340;&#39118;&#38505;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#28982;&#32780;&#35813;&#39046;&#22495;&#32570;&#20047;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#35780;&#20272;&#26694;&#26550;&#26469;&#20005;&#26684;&#35780;&#20272;&#26032;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;HarmBench&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#32418;&#38431;&#30340;&#26631;&#20934;&#21270;&#35780;&#20272;&#26694;&#26550;&#12290;&#25105;&#20204;&#22312;&#32418;&#38431;&#35780;&#20272;&#20013;&#30830;&#23450;&#20102;&#20960;&#20010;&#20197;&#21069;&#26410;&#32771;&#34385;&#30340;&#26377;&#21560;&#24341;&#21147;&#30340;&#29305;&#24615;&#65292;&#24182;&#31995;&#32479;&#22320;&#35774;&#35745;&#20102;HarmBench&#20197;&#28385;&#36275;&#36825;&#20123;&#26631;&#20934;&#12290;&#20351;&#29992;HarmBench&#65292;&#25105;&#20204;&#23545;18&#31181;&#32418;&#38431;&#26041;&#27861;&#21644;33&#20010;&#30446;&#26631;LLM&#21644;&#38450;&#24481;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#27604;&#36739;&#65292;&#24471;&#21040;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;LLM&#22312;&#21508;&#31181;&#25915;&#20987;&#19979;&#30340;&#31283;&#20581;&#24615;&#65292;&#23637;&#31034;&#20102;HarmBench&#22914;&#20309;&#20419;&#36827;&#25915;&#20987;&#21644;&#38450;&#24481;&#30340;&#20849;&#21516;&#24320;&#21457;&#12290;&#25105;&#20204;&#22312;https://github.com/centerforaisafety/HarmBench&#19978;&#24320;&#28304;&#20102;HarmBench&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models (LLMs), yet the field lacks a standardized evaluation framework to rigorously assess new methods. To address this issue, we introduce HarmBench, a standardized evaluation framework for automated red teaming. We identify several desirable properties previously unaccounted for in red teaming evaluations and systematically design HarmBench to meet these criteria. Using HarmBench, we conduct a large-scale comparison of 18 red teaming methods and 33 target LLMs and defenses, yielding novel insights. We also introduce a highly efficient adversarial training method that greatly enhances LLM robustness across a wide range of attacks, demonstrating how HarmBench enables codevelopment of attacks and defenses. We open source HarmBench at https://github.com/centerforaisafety/HarmBench.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#33258;&#30001;&#25991;&#26412;&#20013;&#35782;&#21035;&#20316;&#20026;&#20195;&#29702;&#30340;&#27169;&#22411;&#21644;&#20923;&#32467;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#30340;&#23567;&#22411;&#32447;&#24615;&#25506;&#27979;&#22120;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#26356;&#22823;&#27169;&#22411;&#20196;&#29260;&#32423;&#21035;&#19978;&#30340;&#33258;&#20449;&#24230;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#22312;&#30456;&#21516;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#38750;&#24179;&#20961;&#30340;&#20934;&#30830;&#24230;&#65292;&#36825;&#35777;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.03563</link><description>&lt;p&gt;
&#29992;&#35821;&#35328;&#27169;&#22411;&#21306;&#20998;&#21487;&#30693;&#19982;&#19981;&#21487;&#30693;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Distinguishing the Knowable from the Unknowable with Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03563
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#33258;&#30001;&#25991;&#26412;&#20013;&#35782;&#21035;&#20316;&#20026;&#20195;&#29702;&#30340;&#27169;&#22411;&#21644;&#20923;&#32467;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#30340;&#23567;&#22411;&#32447;&#24615;&#25506;&#27979;&#22120;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#26356;&#22823;&#27169;&#22411;&#20196;&#29260;&#32423;&#21035;&#19978;&#30340;&#33258;&#20449;&#24230;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#22312;&#30456;&#21516;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#38750;&#24179;&#20961;&#30340;&#20934;&#30830;&#24230;&#65292;&#36825;&#35777;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#33258;&#30001;&#25991;&#26412;&#36755;&#20986;&#20013;&#65292;&#26159;&#21542;&#21487;&#20197;&#37492;&#21035;&#20986;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#65288;&#21453;&#26144;&#32570;&#20047;&#30693;&#35782;&#30340;&#19981;&#30830;&#23450;&#24615;&#65289;&#21644;&#20598;&#28982;&#19981;&#30830;&#23450;&#24615;&#65288;&#21453;&#26144;&#22522;&#30784;&#20998;&#24067;&#20013;&#30340;&#29109;&#65289;&#12290;&#22312;&#27809;&#26377;&#30495;&#23454;&#27010;&#29575;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#20010;&#35774;&#32622;&#65292;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#20026;&#20102;&#65288;&#36817;&#20284;&#22320;&#65289;&#20998;&#35299;&#32473;&#23450;LLM&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#19968;&#20010;&#26126;&#26174;&#26356;&#22823;&#30340;&#27169;&#22411;&#20805;&#24403;&#22320;&#38754;&#30495;&#30456;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22522;&#20110;&#20923;&#32467;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#30340;&#23567;&#22411;&#32447;&#24615;&#25506;&#27979;&#22120;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#22312;&#20196;&#29260;&#32423;&#21035;&#19978;&#26356;&#22823;&#27169;&#22411;&#23558;&#26356;&#33258;&#20449;&#30340;&#24773;&#20917;&#65292;&#24182;&#19988;&#22312;&#19968;&#20010;&#25991;&#26412;&#39046;&#22495;&#19978;&#35757;&#32451;&#30340;&#25506;&#27979;&#22120;&#21487;&#20197;&#27867;&#21270;&#21040;&#20854;&#20182;&#39046;&#22495;&#12290;&#36827;&#19968;&#27493;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#22312;&#30456;&#21516;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#38750;&#24179;&#20961;&#30340;&#20934;&#30830;&#24230;&#12290;&#32508;&#21512;&#32771;&#34385;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#35299;&#37322;&#36825;&#20123;&#32467;&#26524;&#20316;&#20026;LLMs&#20869;&#37096;&#33258;&#28982;&#22320;&#21253;&#21547;&#20102;&#19981;&#21516;&#31867;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#34920;&#31034;&#65292;&#36825;&#21487;&#33021;&#26377;&#21161;&#20110;&#21046;&#23450;&#26356;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the feasibility of identifying epistemic uncertainty (reflecting a lack of knowledge), as opposed to aleatoric uncertainty (reflecting entropy in the underlying distribution), in the outputs of large language models (LLMs) over free-form text. In the absence of ground-truth probabilities, we explore a setting where, in order to (approximately) disentangle a given LLM's uncertainty, a significantly larger model stands in as a proxy for the ground truth. We show that small linear probes trained on the embeddings of frozen, pretrained models accurately predict when larger models will be more confident at the token level and that probes trained on one text domain generalize to others. Going further, we propose a fully unsupervised method that achieves non-trivial accuracy on the same task. Taken together, we interpret these results as evidence that LLMs naturally contain internal representations of different types of uncertainty that could potentially be leveraged to devise more i
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#36234;&#29425;&#65292;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#21335;&#36981;&#24490;&#24773;&#20917;&#12290;&#31995;&#32479;&#36890;&#36807;&#25910;&#38598;&#29616;&#26377;&#36234;&#29425;&#24182;&#23558;&#20854;&#32452;&#32455;&#25104;&#30693;&#35782;&#22270;&#26469;&#29983;&#25104;&#26032;&#30340;&#36234;&#29425;&#65292;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03299</link><description>&lt;p&gt;
GUARD: &#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#36234;&#29425;&#26469;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#25351;&#21335;&#30340;&#21512;&#35268;&#24615;
&lt;/p&gt;
&lt;p&gt;
GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#36234;&#29425;&#65292;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#21335;&#36981;&#24490;&#24773;&#20917;&#12290;&#31995;&#32479;&#36890;&#36807;&#25910;&#38598;&#29616;&#26377;&#36234;&#29425;&#24182;&#23558;&#20854;&#32452;&#32455;&#25104;&#30693;&#35782;&#22270;&#26469;&#29983;&#25104;&#26032;&#30340;&#36234;&#29425;&#65292;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#32469;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23433;&#20840;&#36807;&#28388;&#21644;&#26377;&#23475;&#22238;&#24212;&#30340;"&#36234;&#29425;"&#24050;&#32463;&#40723;&#21169;&#31038;&#21306;&#37319;&#21462;&#23433;&#20840;&#25514;&#26045;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#30340;&#23433;&#20840;&#25514;&#26045;&#26159;&#22312;&#21457;&#24067;&#20043;&#21069;&#29992;&#36234;&#29425;&#20027;&#21160;&#27979;&#35797;LLM&#12290;&#22240;&#27492;&#65292;&#36825;&#26679;&#30340;&#27979;&#35797;&#23558;&#38656;&#35201;&#19968;&#31181;&#33021;&#22815;&#22823;&#35268;&#27169;&#19988;&#39640;&#25928;&#22320;&#29983;&#25104;&#36234;&#29425;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#22312;&#36861;&#38543;&#19968;&#31181;&#26032;&#39062;&#32780;&#30452;&#35266;&#30340;&#31574;&#30053;&#19979;&#65292;&#20197;&#20154;&#31867;&#29983;&#25104;&#30340;&#26041;&#24335;&#26469;&#29983;&#25104;&#36234;&#29425;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35282;&#33394;&#25198;&#28436;&#31995;&#32479;&#65292;&#23558;&#22235;&#31181;&#19981;&#21516;&#35282;&#33394;&#20998;&#37197;&#32473;&#29992;&#25143;LLM&#65292;&#20197;&#20415;&#21327;&#20316;&#29983;&#25104;&#26032;&#30340;&#36234;&#29425;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25910;&#38598;&#29616;&#26377;&#30340;&#36234;&#29425;&#65292;&#24182;&#36890;&#36807;&#21477;&#23376;&#36880;&#21477;&#36827;&#34892;&#32858;&#31867;&#39057;&#29575;&#21644;&#35821;&#20041;&#27169;&#24335;&#30340;&#21010;&#20998;&#65292;&#23558;&#23427;&#20204;&#20998;&#25104;&#19981;&#21516;&#30340;&#29420;&#31435;&#29305;&#24449;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#29305;&#24449;&#32452;&#32455;&#25104;&#19968;&#20010;&#30693;&#35782;&#22270;&#65292;&#20351;&#20854;&#26356;&#26131;&#20110;&#35775;&#38382;&#21644;&#26816;&#32034;&#12290;&#25105;&#20204;&#30340;&#35282;&#33394;&#31995;&#32479;&#23558;&#21033;&#29992;&#36825;&#20010;&#30693;&#35782;&#22270;&#26469;&#29983;&#25104;&#26032;&#30340;&#36234;&#29425;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The discovery of "jailbreaks" to bypass safety filters of Large Language Models (LLMs) and harmful responses have encouraged the community to implement safety measures. One major safety measure is to proactively test the LLMs with jailbreaks prior to the release. Therefore, such testing will require a method that can generate jailbreaks massively and efficiently. In this paper, we follow a novel yet intuitive strategy to generate jailbreaks in the style of the human generation. We propose a role-playing system that assigns four different roles to the user LLMs to collaborate on new jailbreaks. Furthermore, we collect existing jailbreaks and split them into different independent characteristics using clustering frequency and semantic patterns sentence by sentence. We organize these characteristics into a knowledge graph, making them more accessible and easier to retrieve. Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;AI&#27494;&#22120;&#30340;&#27010;&#24565;&#12289;&#37096;&#32626;&#12289;&#26816;&#27979;&#21644;&#28508;&#22312;&#23545;&#31574;&#65292;&#24378;&#35843;&#20102;&#22312;&#20449;&#24687;&#39046;&#22495;&#20869;&#22522;&#20110;AI&#30340;&#24515;&#29702;&#25805;&#32437;&#30340;&#28508;&#21147;&#65292;&#20197;&#21450;&#20854;&#23545;&#20840;&#29699;&#20010;&#20154;&#12289;&#32452;&#32455;&#21644;&#31038;&#20250;&#30340;&#23041;&#32961;&#12290;</title><link>https://arxiv.org/abs/2402.01663</link><description>&lt;p&gt;
&#26432;&#25163;&#32423;&#24212;&#29992;&#65306;&#20302;&#36895;&#22823;&#35268;&#27169;AI&#27494;&#22120;
&lt;/p&gt;
&lt;p&gt;
Killer Apps: Low-Speed, Large-Scale AI Weapons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;AI&#27494;&#22120;&#30340;&#27010;&#24565;&#12289;&#37096;&#32626;&#12289;&#26816;&#27979;&#21644;&#28508;&#22312;&#23545;&#31574;&#65292;&#24378;&#35843;&#20102;&#22312;&#20449;&#24687;&#39046;&#22495;&#20869;&#22522;&#20110;AI&#30340;&#24515;&#29702;&#25805;&#32437;&#30340;&#28508;&#21147;&#65292;&#20197;&#21450;&#20854;&#23545;&#20840;&#29699;&#20010;&#20154;&#12289;&#32452;&#32455;&#21644;&#31038;&#20250;&#30340;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#19981;&#26029;&#36827;&#27493;&#65292;&#29305;&#21035;&#26159;&#30001;OpenAI&#12289;Meta&#21644;Anthropic&#31561;&#32452;&#32455;&#24320;&#21457;&#30340;&#23574;&#31471;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#32473;&#25112;&#20105;&#21644;&#23433;&#20840;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;&#30446;&#21069;&#20851;&#27880;&#30340;&#20027;&#35201;&#26159;AI&#22312;&#27494;&#22120;&#31995;&#32479;&#20013;&#30340;&#25972;&#21512;&#20197;&#21450;&#22312;&#21160;&#33021;&#20914;&#31361;&#20013;&#24555;&#36895;&#20915;&#31574;&#20013;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#21516;&#26679;&#37325;&#35201;&#20294;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#19968;&#20010;&#26041;&#38754;&#26159;&#22312;&#20449;&#24687;&#39046;&#22495;&#20013;&#22522;&#20110;AI&#30340;&#24515;&#29702;&#25805;&#32437;&#22312;&#20114;&#32852;&#32593;&#35268;&#27169;&#20869;&#30340;&#28508;&#21147;&#12290;&#36825;&#20123;&#33021;&#21147;&#21487;&#33021;&#23545;&#20840;&#29699;&#20010;&#20154;&#12289;&#32452;&#32455;&#21644;&#31038;&#20250;&#36896;&#25104;&#37325;&#22823;&#23041;&#32961;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;AI&#27494;&#22120;&#30340;&#27010;&#24565;&#12289;&#37096;&#32626;&#12289;&#26816;&#27979;&#21644;&#28508;&#22312;&#23545;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accelerating advancements in Artificial Intelligence (AI) and Machine Learning (ML), highlighted by the development of cutting-edge Generative Pre-trained Transformer (GPT) models by organizations such as OpenAI, Meta, and Anthropic, present new challenges and opportunities in warfare and security. Much of the current focus is on AI's integration within weapons systems and its role in rapid decision-making in kinetic conflict. However, an equally important but often overlooked aspect is the potential of AI-based psychological manipulation at internet scales within the information domain. These capabilities could pose significant threats to individuals, organizations, and societies globally. This paper explores the concept of AI weapons, their deployment, detection, and potential countermeasures.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#30340;&#31639;&#27861;&#31867;&#65292;&#29992;&#20110;&#24178;&#39044;&#33539;&#22260;&#20869;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65292;&#28085;&#30422;&#20102;&#32447;&#24615;&#21644;&#19968;&#33324;&#36716;&#21270;&#12290;&#31639;&#27861;&#20445;&#35777;&#20102;&#21487;&#35782;&#21035;&#24615;&#21644;&#23454;&#29616;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#21019;&#36896;&#24615;&#22320;&#23558;&#24471;&#20998;&#20989;&#25968;&#19982;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.00849</link><description>&lt;p&gt;
&#22522;&#20110;&#24471;&#20998;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65306;&#32447;&#24615;&#21644;&#19968;&#33324;&#30340;&#36716;&#21270;
&lt;/p&gt;
&lt;p&gt;
Score-based Causal Representation Learning: Linear and General Transformations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00849
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#30340;&#31639;&#27861;&#31867;&#65292;&#29992;&#20110;&#24178;&#39044;&#33539;&#22260;&#20869;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65292;&#28085;&#30422;&#20102;&#32447;&#24615;&#21644;&#19968;&#33324;&#36716;&#21270;&#12290;&#31639;&#27861;&#20445;&#35777;&#20102;&#21487;&#35782;&#21035;&#24615;&#21644;&#23454;&#29616;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#21019;&#36896;&#24615;&#22320;&#23558;&#24471;&#20998;&#20989;&#25968;&#19982;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#38024;&#23545;&#19968;&#33324;&#38750;&#21442;&#25968;&#28508;&#22312;&#22240;&#26524;&#27169;&#22411;&#21644;&#23558;&#28508;&#22312;&#21464;&#37327;&#26144;&#23556;&#21040;&#35266;&#27979;&#21464;&#37327;&#30340;&#26410;&#30693;&#36716;&#21270;&#65292;&#30740;&#31350;&#20102;&#22522;&#20110;&#24178;&#39044;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65288;CRL&#65289;&#12290;&#30740;&#31350;&#20102;&#32447;&#24615;&#21644;&#19968;&#33324;&#30340;&#36716;&#21270;&#12290;&#36825;&#31687;&#35770;&#25991;&#21516;&#26102;&#35752;&#35770;&#20102;&#21487;&#35782;&#21035;&#24615;&#21644;&#23454;&#29616;&#24615;&#20004;&#20010;&#26041;&#38754;&#12290;&#21487;&#35782;&#21035;&#24615;&#26159;&#25351;&#30830;&#23450;&#31639;&#27861;&#19981;&#30456;&#20851;&#30340;&#26465;&#20214;&#65292;&#20197;&#30830;&#20445;&#24674;&#22797;&#30495;&#23454;&#30340;&#28508;&#22312;&#22240;&#26524;&#21464;&#37327;&#21644;&#28508;&#22312;&#22240;&#26524;&#22270;&#12290;&#23454;&#29616;&#24615;&#26159;&#25351;&#31639;&#27861;&#26041;&#38754;&#65292;&#35299;&#20915;&#35774;&#35745;&#31639;&#27861;&#26469;&#23454;&#29616;&#21487;&#35782;&#21035;&#20445;&#35777;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#24471;&#20998;&#20989;&#25968;&#65288;&#21363;&#23494;&#24230;&#20989;&#25968;&#23545;&#25968;&#30340;&#26799;&#24230;&#65289;&#19982;CRL&#20043;&#38388;&#24314;&#31435;&#26032;&#32852;&#31995;&#65292;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#24471;&#20998;&#20026;&#22522;&#30784;&#30340;&#31639;&#27861;&#31867;&#65292;&#30830;&#20445;&#20102;&#21487;&#35782;&#21035;&#24615;&#21644;&#23454;&#29616;&#24615;&#12290;&#39318;&#20808;&#65292;&#26412;&#25991;&#19987;&#27880;&#20110;&#32447;&#24615;&#36716;&#21270;&#65292;&#24182;&#23637;&#31034;&#20102;&#27599;&#20010;n&#20010;&#38543;&#26426;&#30828;&#24178;&#39044;&#19979;&#35813;&#36716;&#21270;&#30340;&#22240;&#26524;&#34920;&#31034;&#21487;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses intervention-based causal representation learning (CRL) under a general nonparametric latent causal model and an unknown transformation that maps the latent variables to the observed variables. Linear and general transformations are investigated. The paper addresses both the \emph{identifiability} and \emph{achievability} aspects. Identifiability refers to determining algorithm-agnostic conditions that ensure recovering the true latent causal variables and the latent causal graph underlying them. Achievability refers to the algorithmic aspects and addresses designing algorithms that achieve identifiability guarantees. By drawing novel connections between \emph{score functions} (i.e., the gradients of the logarithm of density functions) and CRL, this paper designs a \emph{score-based class of algorithms} that ensures both identifiability and achievability. First, the paper focuses on \emph{linear} transformations and shows that one stochastic hard intervention per n
&lt;/p&gt;</description></item><item><title>&#26412;&#25945;&#31243;&#35814;&#32454;&#12289;&#31616;&#21270;&#21644;&#28165;&#26224;&#22320;&#35299;&#37322;&#20102;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#28165;&#26224;&#30340;&#22270;&#24418;&#35828;&#26126;&#65292;&#22635;&#34917;&#20102;&#32570;&#20047;&#32479;&#19968;&#25968;&#23398;&#26694;&#26550;&#30340;&#29616;&#23384;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2401.03797</link><description>&lt;p&gt;
&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#35299;&#21078;&#23398;
&lt;/p&gt;
&lt;p&gt;
Anatomy of Neural Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.03797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25945;&#31243;&#35814;&#32454;&#12289;&#31616;&#21270;&#21644;&#28165;&#26224;&#22320;&#35299;&#37322;&#20102;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#28165;&#26224;&#30340;&#22270;&#24418;&#35828;&#26126;&#65292;&#22635;&#34917;&#20102;&#32570;&#20047;&#32479;&#19968;&#25968;&#23398;&#26694;&#26550;&#30340;&#29616;&#23384;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#36801;&#31227;&#23398;&#20064;&#39046;&#22495;&#22312;&#26368;&#36817;&#20960;&#24180;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#12290;&#21464;&#21387;&#22120;&#24050;&#32463;&#25104;&#20026;&#36825;&#20123;&#36827;&#23637;&#30340;&#26680;&#24515;&#65292;&#22522;&#20110;&#26368;&#21069;&#27839;&#30340;&#21464;&#21387;&#22120;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#24191;&#27867;&#30340;&#24212;&#29992;&#20013;&#23548;&#33268;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#23613;&#31649;&#28041;&#21450;&#31070;&#32463;LMs&#30340;&#30740;&#31350;&#20316;&#21697;&#25968;&#37327;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#20294;&#20854;&#20013;&#32477;&#22823;&#22810;&#25968;&#26159;&#39640;&#32423;&#30340;&#65292;&#31163;&#23454;&#38469;&#25805;&#20316;&#39047;&#36828;&#12290;&#22240;&#27492;&#65292;&#22312;&#32570;&#20047;&#35299;&#37322;&#20027;&#35201;&#31867;&#22411;&#31070;&#32463;LMs&#30340;&#32479;&#19968;&#25968;&#23398;&#26694;&#26550;&#30340;&#21069;&#25552;&#19979;&#65292;&#23545;&#36825;&#19968;&#39046;&#22495;&#30340;&#25991;&#29486;&#28145;&#20837;&#20102;&#35299;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#26412;&#25945;&#31243;&#20013;&#35299;&#20915;&#20102;&#19978;&#36848;&#38382;&#39064;&#65292;&#26088;&#22312;&#22312;&#35814;&#32454;&#12289;&#31616;&#21270;&#21644;&#28165;&#26224;&#30340;&#25968;&#23398;&#26694;&#26550;&#20013;&#35299;&#37322;&#31070;&#32463;LMs&#65292;&#24182;&#38468;&#26377;&#28165;&#26224;&#30340;&#22270;&#24418;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.03797v2 Announce Type: replace  Abstract: The fields of generative AI and transfer learning have experienced remarkable advancements in recent years especially in the domain of Natural Language Processing (NLP). Transformers have been at the heart of these advancements where the cutting-edge transformer-based Language Models (LMs) have led to new state-of-the-art results in a wide spectrum of applications. While the number of research works involving neural LMs is exponentially increasing, their vast majority are high-level and far from self-contained. Consequently, a deep understanding of the literature in this area is a tough task especially in the absence of a unified mathematical framework explaining the main types of neural LMs. We address the aforementioned problem in this tutorial where the objective is to explain neural LMs in a detailed, simplified and unambiguous mathematical framework accompanied by clear graphical illustrations. Concrete examples on widely used m
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;Cascade Speculative Drafting&#65288;CS Drafting&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#22402;&#30452;&#32423;&#32852;&#28040;&#38500;&#31070;&#32463;&#27169;&#22411;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#65292;&#36890;&#36807;&#27700;&#24179;&#32423;&#32852;&#20248;&#21270;&#33609;&#31295;&#20013;&#30340;&#26102;&#38388;&#20998;&#37197;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;LLM&#25512;&#29702;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2312.11462</link><description>&lt;p&gt;
&#29992;&#20110;&#26356;&#24555;&#30340;LLM&#25512;&#29702;&#30340;&#32423;&#32852;&#25512;&#27979;&#33609;&#22270;
&lt;/p&gt;
&lt;p&gt;
Cascade Speculative Drafting for Even Faster LLM Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11462
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;Cascade Speculative Drafting&#65288;CS Drafting&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#22402;&#30452;&#32423;&#32852;&#28040;&#38500;&#31070;&#32463;&#27169;&#22411;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#65292;&#36890;&#36807;&#27700;&#24179;&#32423;&#32852;&#20248;&#21270;&#33609;&#31295;&#20013;&#30340;&#26102;&#38388;&#20998;&#37197;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;LLM&#25512;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#29702;&#25928;&#29575;&#30340;&#32423;&#32852;&#25512;&#27979;&#33609;&#22270;&#65292;&#36890;&#36807;&#36739;&#23567;&#30340;&#27169;&#22411;&#29983;&#25104;&#33609;&#31295;&#26469;&#36816;&#20316;&#12290;&#36739;&#22823;&#30340;&#30446;&#26631;&#27169;&#22411;&#28982;&#21518;&#26597;&#30475;&#36825;&#20010;&#33609;&#31295;&#20197;&#19982;&#20854;&#36755;&#20986;&#23545;&#40784;&#65292;&#30446;&#26631;&#27169;&#22411;&#30340;&#20219;&#20309;&#25509;&#21463;&#37117;&#23558;&#20943;&#23569;&#30446;&#26631;&#27169;&#22411;&#36816;&#34892;&#30340;&#25968;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#22312;&#32423;&#32852;&#25512;&#27979;&#30340;&#33609;&#22270;&#36807;&#31243;&#20013;&#21253;&#25324;&#32531;&#24930;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#65292;&#24182;&#20026;&#29983;&#25104;&#30340;&#26631;&#35760;&#20998;&#37197;&#30456;&#21516;&#30340;&#26102;&#38388;&#65292;&#32780;&#19981;&#32771;&#34385;&#23427;&#20204;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#20123;&#20302;&#25928;&#24615;&#20849;&#21516;&#23548;&#33268;&#32423;&#32852;&#25512;&#27979;&#30340;&#24615;&#33021;&#19981;&#20339;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25913;&#21892;LLM&#25512;&#29702;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32423;&#32852;&#25512;&#27979;&#33609;&#22270;&#65288;CS Drafting&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#25972;&#21512;&#20102;&#20004;&#31181;&#32423;&#32852;&#31867;&#22411;&#30340;&#25512;&#27979;&#25191;&#34892;&#31639;&#27861;&#12290;&#22402;&#30452;&#32423;&#32852;&#20174;&#31070;&#32463;&#27169;&#22411;&#20013;&#28040;&#38500;&#33258;&#22238;&#24402;&#29983;&#25104;&#65292;&#32780;&#27700;&#24179;&#32423;&#32852;&#20248;&#21270;&#20102;&#33609;&#31295;&#20013;&#30340;&#26102;&#38388;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11462v3 Announce Type: replace-cross  Abstract: Introduced to enhance the efficiency of large language model (LLM) inference, speculative decoding operates by having a smaller model generate a draft. A larger target model then reviews this draft to align with its output, and any acceptance by the target model results in a reduction of the number of the target model runs, ultimately improving efficiency. However, the drafting process in speculative decoding includes slow autoregressive generation and allocates equal time to generating tokens, irrespective of their importance. These inefficiencies collectively contribute to the suboptimal performance of speculative decoding. To further improve LLM inference, we introduce Cascade Speculative Drafting (CS Drafting), a speculative execution algorithm that incorporates two types of cascades. The Vertical Cascade eliminates autoregressive generation from neural models, while the Horizontal Cascade optimizes time allocation in draft
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#20102;&#22240;&#26524;&#20844;&#24179;&#24615;&#23545;&#26410;&#35266;&#23519;&#21040;&#28151;&#26434;&#30340;&#25935;&#24863;&#24615;&#65292;&#25512;&#23548;&#20986;&#22240;&#26524;&#20844;&#24179;&#24615;&#25351;&#26631;&#30340;&#30028;&#38480;&#65292;&#25552;&#20986;&#31070;&#32463;&#26694;&#26550;&#29992;&#20110;&#23398;&#20064;&#20844;&#24179;&#39044;&#27979;&#65292;&#23637;&#31034;&#20102;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;</title><link>https://arxiv.org/abs/2311.18460</link><description>&lt;p&gt;
&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#26434;&#19979;&#30340;&#22240;&#26524;&#20844;&#24179;&#24615;&#65306;&#19968;&#31181;&#31070;&#32463;&#25935;&#24863;&#24615;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Causal Fairness under Unobserved Confounding: A Neural Sensitivity Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18460
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#22240;&#26524;&#20844;&#24179;&#24615;&#23545;&#26410;&#35266;&#23519;&#21040;&#28151;&#26434;&#30340;&#25935;&#24863;&#24615;&#65292;&#25512;&#23548;&#20986;&#22240;&#26524;&#20844;&#24179;&#24615;&#25351;&#26631;&#30340;&#30028;&#38480;&#65292;&#25552;&#20986;&#31070;&#32463;&#26694;&#26550;&#29992;&#20110;&#23398;&#20064;&#20844;&#24179;&#39044;&#27979;&#65292;&#23637;&#31034;&#20102;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#20013;&#30340;&#20844;&#24179;&#24615;&#30001;&#20110;&#27861;&#24459;&#12289;&#36947;&#24503;&#21644;&#31038;&#20250;&#21407;&#22240;&#22312;&#23454;&#36341;&#20013;&#34987;&#24191;&#27867;&#35201;&#27714;&#12290;&#29616;&#26377;&#24037;&#20316;&#36890;&#24120;&#38598;&#20013;&#22312;&#27809;&#26377;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#26434;&#30340;&#35774;&#32622;&#19978;&#65292;&#23613;&#31649;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#26434;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#36829;&#21453;&#22240;&#26524;&#20844;&#24179;&#24615;&#65292;&#20174;&#32780;&#20135;&#29983;&#19981;&#20844;&#24179;&#30340;&#39044;&#27979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22240;&#26524;&#20844;&#24179;&#24615;&#23545;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#26434;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19981;&#21516;&#26469;&#28304;&#30340;&#26410;&#35266;&#23519;&#21040;&#28151;&#26434;&#19979;&#22240;&#26524;&#20844;&#24179;&#24615;&#25351;&#26631;&#30340;&#30028;&#38480;&#12290;&#36825;&#20351;&#20174;&#19994;&#32773;&#33021;&#22815;&#26816;&#26597;&#20854;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#22312;&#20844;&#24179;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#26434;&#30340;&#25935;&#24863;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#20844;&#24179;&#39044;&#27979;&#30340;&#26032;&#22411;&#31070;&#32463;&#26694;&#26550;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20379;&#23545;&#22240;&#26524;&#20844;&#24179;&#24615;&#21487;&#33021;&#30001;&#20110;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#26434;&#32780;&#21463;&#21040;&#36829;&#21453;&#30340;&#31243;&#24230;&#30340;&#26368;&#22351;&#24773;&#20917;&#20445;&#35777;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18460v2 Announce Type: replace-cross  Abstract: Fairness for machine learning predictions is widely required in practice for legal, ethical, and societal reasons. Existing work typically focuses on settings without unobserved confounding, even though unobserved confounding can lead to severe violations of causal fairness and, thus, unfair predictions. In this work, we analyze the sensitivity of causal fairness to unobserved confounding. Our contributions are three-fold. First, we derive bounds for causal fairness metrics under different sources of unobserved confounding. This enables practitioners to examine the sensitivity of their machine learning models to unobserved confounding in fairness-critical applications. Second, we propose a novel neural framework for learning fair predictions, which allows us to offer worst-case guarantees of the extent to which causal fairness can be violated due to unobserved confounding. Third, we demonstrate the effectiveness of our framewor
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#30340;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#65288;TEA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#36716;&#25442;&#35757;&#32451;&#21518;&#30340;&#20998;&#31867;&#22120;&#20026;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65292;&#22686;&#24378;&#27169;&#22411;&#23545;&#30446;&#26631;&#25968;&#25454;&#20998;&#24067;&#30340;&#24863;&#30693;&#65292;&#20174;&#32780;&#35299;&#20915;&#21327;&#21464;&#37327;&#36716;&#31227;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.14402</link><description>&lt;p&gt;
TEA: &#27979;&#35797;&#26102;&#38388;&#33021;&#37327;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
TEA: Test-time Energy Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14402
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#30340;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#65288;TEA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#36716;&#25442;&#35757;&#32451;&#21518;&#30340;&#20998;&#31867;&#22120;&#20026;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65292;&#22686;&#24378;&#27169;&#22411;&#23545;&#30446;&#26631;&#25968;&#25454;&#20998;&#24067;&#30340;&#24863;&#30693;&#65292;&#20174;&#32780;&#35299;&#20915;&#21327;&#21464;&#37327;&#36716;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#65288;TTA&#65289;&#26088;&#22312;&#22312;&#27979;&#35797;&#25968;&#25454;&#20559;&#31163;&#35757;&#32451;&#20998;&#24067;&#26102;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20854;&#29420;&#29305;&#20248;&#21183;&#22312;&#20110;&#19981;&#38656;&#35201;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#21644;&#22788;&#29702;&#36807;&#31243;&#65292;&#23588;&#20854;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#23588;&#20026;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;TTA&#26041;&#27861;&#26410;&#33021;&#35299;&#20915;&#26681;&#26412;&#38382;&#39064;&#65306;&#21327;&#21464;&#37327;&#36716;&#31227;&#65292;&#21363;&#38477;&#20302;&#30340;&#27867;&#21270;&#33021;&#21147;&#21487;&#20197;&#24402;&#22240;&#20110;&#27169;&#22411;&#20381;&#36182;&#35757;&#32451;&#25968;&#25454;&#30340;&#36793;&#38469;&#20998;&#24067;&#65292;&#36825;&#21487;&#33021;&#24433;&#21709;&#27169;&#22411;&#26657;&#20934;&#24182;&#24341;&#20837;&#30830;&#35748;&#20559;&#35265;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33021;&#37327;&#30340;&#35270;&#35282;&#65292;&#22686;&#24378;&#27169;&#22411;&#23545;&#30446;&#26631;&#25968;&#25454;&#20998;&#24067;&#30340;&#24863;&#30693;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#25110;&#22788;&#29702;&#36807;&#31243;&#12290;&#22522;&#20110;&#36825;&#19968;&#35270;&#35282;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; $\textbf{T}$est-time $\textbf{E}$nergy $\textbf{A}$daptation&#65288;$\textbf{TEA}$&#65289;&#65292;&#23558;&#32463;&#36807;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#36716;&#25442;&#20026;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65292;&#24182;&#20351;&#27169;&#22411;&#30340;&#20998;&#31867;&#36793;&#32536;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.14402v2 Announce Type: replace  Abstract: Test-time adaptation (TTA) aims to improve model generalizability when test data diverges from training distribution, offering the distinct advantage of not requiring access to training data and processes, especially valuable in the context of large pre-trained models. However, current TTA methods fail to address the fundamental issue: covariate shift, i.e., the decreased generalizability can be attributed to the model's reliance on the marginal distribution of the training data, which may impair model calibration and introduce confirmation bias. To address this, we propose a novel energy-based perspective, enhancing the model's perception of target data distributions without requiring access to training data or processes. Building on this perspective, we introduce $\textbf{T}$est-time $\textbf{E}$nergy $\textbf{A}$daptation ($\textbf{TEA}$), which transforms the trained classifier into an energy-based model and aligns the model's di
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;EarnMore&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;RL&#26041;&#27861;&#65292;&#21487;&#20197;&#20801;&#35768;RL&#20195;&#29702;&#19982;&#21487;&#23450;&#21046;&#32929;&#31080;&#27744;&#65288;CSPs&#65289;&#20132;&#20114;&#65292;&#32780;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2311.10801</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#23631;&#34109;&#32929;&#31080;&#34920;&#31034;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;&#21487;&#23450;&#21046;&#32929;&#31080;&#27744;&#20013;&#36827;&#34892;&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Maskable Stock Representation for Portfolio Management in Customizable Stock Pools
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10801
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;EarnMore&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;RL&#26041;&#27861;&#65292;&#21487;&#20197;&#20801;&#35768;RL&#20195;&#29702;&#19982;&#21487;&#23450;&#21046;&#32929;&#31080;&#27744;&#65288;CSPs&#65289;&#20132;&#20114;&#65292;&#32780;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;&#65288;PM&#65289;&#26159;&#19968;&#39033;&#22522;&#26412;&#30340;&#37329;&#34701;&#20132;&#26131;&#20219;&#21153;&#65292;&#25506;&#32034;&#23450;&#26399;&#23558;&#36164;&#37329;&#37325;&#26032;&#37197;&#32622;&#21040;&#19981;&#21516;&#32929;&#31080;&#20013;&#20197;&#36861;&#27714;&#38271;&#26399;&#21033;&#28070;&#12290;&#26368;&#36817;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26174;&#31034;&#20986;&#20854;&#28508;&#21147;&#65292;&#36890;&#36807;&#19982;&#37329;&#34701;&#24066;&#22330;&#20114;&#21160;&#26469;&#35757;&#32451;&#20855;&#26377;&#30408;&#21033;&#33021;&#21147;&#30340;PM&#20195;&#29702;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#22266;&#23450;&#32929;&#31080;&#27744;&#19978;&#65292;&#36825;&#19982;&#25237;&#36164;&#32773;&#30340;&#23454;&#38469;&#38656;&#27714;&#19981;&#19968;&#33268;&#12290;&#20026;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;EarnMore&#65292;&#19968;&#31181;&#26032;&#30340;RL&#26041;&#27861;&#65292;&#21487;&#20197;&#20801;&#35768;RL&#20195;&#29702;&#19982;&#21487;&#23450;&#21046;&#32929;&#31080;&#27744;&#65288;CSPs&#65289;&#20132;&#20114;&#65292;&#32780;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10801v3 Announce Type: replace-cross  Abstract: Portfolio management (PM) is a fundamental financial trading task, which explores the optimal periodical reallocation of capitals into different stocks to pursue long-term profits. Reinforcement learning (RL) has recently shown its potential to train profitable agents for PM through interacting with financial markets. However, existing work mostly focuses on fixed stock pools, which is inconsistent with investors' practical demand. Specifically, the target stock pool of different investors varies dramatically due to their discrepancy on market states and individual investors may temporally adjust stocks they desire to trade (e.g., adding one popular stocks), which lead to customizable stock pools (CSPs). Existing RL methods require to retrain RL agents even with a tiny change of the stock pool, which leads to high computational cost and unstable performance. To tackle this challenge, we propose EarnMore, a rEinforcement leARNin
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#21355;&#26143;&#21644;&#27979;&#31449;&#25968;&#25454;&#36827;&#34892;&#25554;&#20540;&#65292;&#36890;&#36807;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#26469;&#25552;&#39640;&#38477;&#27700;&#25968;&#25454;&#38598;&#30340;&#20998;&#36776;&#29575;&#12290;</title><link>https://arxiv.org/abs/2311.07511</link><description>&lt;p&gt;
&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#21355;&#26143;&#38477;&#27700;&#25554;&#20540;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Uncertainty estimation in satellite precipitation interpolation with machine learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07511
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#21355;&#26143;&#21644;&#27979;&#31449;&#25968;&#25454;&#36827;&#34892;&#25554;&#20540;&#65292;&#36890;&#36807;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#26469;&#25552;&#39640;&#38477;&#27700;&#25968;&#25454;&#38598;&#30340;&#20998;&#36776;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#24182;&#21355;&#26143;&#21644;&#27979;&#31449;&#25968;&#25454;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20135;&#29983;&#39640;&#20998;&#36776;&#29575;&#38477;&#27700;&#25968;&#25454;&#38598;&#65292;&#20294;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24448;&#24448;&#32570;&#22833;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#27604;&#20845;&#31181;&#31639;&#27861;&#65292;&#22823;&#37096;&#20998;&#26159;&#38024;&#23545;&#36825;&#19968;&#20219;&#21153;&#32780;&#35774;&#35745;&#30340;&#26032;&#31639;&#27861;&#65292;&#26469;&#37327;&#21270;&#31354;&#38388;&#25554;&#20540;&#20013;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#36830;&#32493;&#32654;&#22269;&#30340;15&#24180;&#26376;&#24230;&#25968;&#25454;&#19978;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20998;&#20301;&#25968;&#22238;&#24402;&#65288;QR&#65289;&#12289;&#20998;&#20301;&#25968;&#22238;&#24402;&#26862;&#26519;&#65288;QRF&#65289;&#12289;&#24191;&#20041;&#38543;&#26426;&#26862;&#26519;&#65288;GRF&#65289;&#12289;&#26799;&#24230;&#25552;&#21319;&#26426;&#65288;GBM&#65289;&#12289;&#36731;&#26799;&#24230;&#25552;&#21319;&#26426;&#65288;LightGBM&#65289;&#21644;&#20998;&#20301;&#25968;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;QRNN&#65289;&#12290;&#23427;&#20204;&#33021;&#22815;&#22312;&#20061;&#20010;&#20998;&#20301;&#27700;&#24179;&#65288;0.025&#12289;0.050&#12289;0.100&#12289;0.250&#12289;0.500&#12289;0.750&#12289;0.900&#12289;0.950&#12289;0.975&#65289;&#19978;&#21457;&#24067;&#39044;&#27979;&#38477;&#27700;&#20998;&#20301;&#25968;&#65292;&#20197;&#36817;&#20284;&#23436;&#25972;&#27010;&#29575;&#20998;&#24067;&#65292;&#35780;&#20272;&#26102;&#37319;&#29992;&#20998;&#20301;&#25968;&#35780;&#20998;&#20989;&#25968;&#21644;&#20998;&#20301;&#25968;&#35780;&#20998;&#35268;&#21017;&#12290;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#26512;&#25581;&#31034;&#20102;&#21355;&#26143;&#38477;&#27700;&#65288;PERSIA
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07511v2 Announce Type: replace-cross  Abstract: Merging satellite and gauge data with machine learning produces high-resolution precipitation datasets, but uncertainty estimates are often missing. We address this gap by benchmarking six algorithms, mostly novel for this task, for quantifying predictive uncertainty in spatial interpolation. On 15 years of monthly data over the contiguous United States (CONUS), we compared quantile regression (QR), quantile regression forests (QRF), generalized random forests (GRF), gradient boosting machines (GBM), light gradient boosting machines (LightGBM), and quantile regression neural networks (QRNN). Their ability to issue predictive precipitation quantiles at nine quantile levels (0.025, 0.050, 0.100, 0.250, 0.500, 0.750, 0.900, 0.950, 0.975), approximating the full probability distribution, was evaluated using quantile scoring functions and the quantile scoring rule. Feature importance analysis revealed satellite precipitation (PERSIA
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37329;&#23383;&#22612;&#24418;&#38544;&#34255;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;PHMM&#65289;&#65292;&#33021;&#22815;&#25429;&#25417;&#22810;&#20010;&#22810;&#27493;&#38543;&#26426;&#29366;&#24577;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#32467;&#26500;&#65292;&#22312;&#24615;&#33021;&#19978;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2310.14341</link><description>&lt;p&gt;
&#37329;&#23383;&#22612;&#24418;&#38544;&#34255;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Pyramidal Hidden Markov Model For Multivariate Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37329;&#23383;&#22612;&#24418;&#38544;&#34255;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;PHMM&#65289;&#65292;&#33021;&#22815;&#25429;&#25417;&#22810;&#20010;&#22810;&#27493;&#38543;&#26426;&#29366;&#24577;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#32467;&#26500;&#65292;&#22312;&#24615;&#33021;&#19978;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#34255;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;HMM&#65289;&#21487;&#20197;&#26681;&#25454;&#24403;&#21069;&#21644;&#20808;&#21069;&#20540;&#39044;&#27979;&#26102;&#38388;&#24207;&#21015;&#30340;&#26410;&#26469;&#20540;&#65292;&#20351;&#20854;&#25104;&#20026;&#22788;&#29702;&#21508;&#31181;&#31867;&#22411;&#26102;&#38388;&#24207;&#21015;&#30340;&#24378;&#22823;&#31639;&#27861;&#12290;&#35768;&#22810;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#20808;&#36827;&#25216;&#26415;&#25913;&#36827;HMM&#30340;&#26041;&#27861;&#65292;&#23548;&#33268;&#20102;&#20960;&#31181;HMM&#30340;&#21464;&#20307;&#30340;&#21457;&#23637;&#12290;&#23613;&#31649;&#36825;&#20123;&#30740;&#31350;&#34920;&#26126;HMM&#19982;&#20854;&#20182;&#20808;&#36827;&#31639;&#27861;&#30456;&#27604;&#20855;&#26377;&#22686;&#24378;&#30340;&#31454;&#20105;&#21147;&#65292;&#20294;&#24456;&#23569;&#26377;&#20154;&#24847;&#35782;&#21040;&#23558;&#22810;&#27493;&#38543;&#26426;&#29366;&#24577;&#32435;&#20837;&#20854;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#21644;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#22810;&#20010;&#22810;&#27493;&#38543;&#26426;&#29366;&#24577;&#30340;&#37329;&#23383;&#22612;&#24418;&#38544;&#34255;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;PHMM&#65289;&#12290;&#26368;&#21021;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#27493;HMM&#29992;&#20110;&#25552;&#21462;&#30701;&#22810;&#27493;&#38543;&#26426;&#29366;&#24577;&#12290;&#25509;&#19979;&#26469;&#65292;&#22522;&#20110;PHMM&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#32467;&#26500;&#65292;&#21033;&#29992;&#31867;&#20284;&#37329;&#23383;&#22612;&#22534;&#21472;&#30340;&#26041;&#24335;&#33258;&#36866;&#24212;&#22320;&#35782;&#21035;&#38271;&#22810;&#27493;&#38543;&#26426;&#29366;&#24577;&#12290;&#36890;&#36807;&#37319;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.14341v2 Announce Type: replace  Abstract: The Hidden Markov Model (HMM) can predict the future value of a time series based on its current and previous values, making it a powerful algorithm for handling various types of time series. Numerous studies have explored the improvement of HMM using advanced techniques, leading to the development of several variations of HMM. Despite these studies indicating the increased competitiveness of HMM compared to other advanced algorithms, few have recognized the significance and impact of incorporating multistep stochastic states into its performance. In this work, we propose a Pyramidal Hidden Markov Model (PHMM) that can capture multiple multistep stochastic states. Initially, a multistep HMM is designed for extracting short multistep stochastic states. Next, a novel time series forecasting structure is proposed based on PHMM, which utilizes pyramid-like stacking to adaptively identify long multistep stochastic states. By employing the
&lt;/p&gt;</description></item><item><title>&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24341;&#20837;&#20102;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#65292;&#32467;&#21512;&#26368;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#36827;&#23637;&#65292;&#30830;&#20445;&#20102;&#24739;&#32773;&#25968;&#25454;&#38544;&#31169;&#21644;&#23433;&#20840;&#65292;&#20026;&#20248;&#21270;&#21307;&#30103;AI&#31995;&#32479;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.09650</link><description>&lt;p&gt;
&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Multimodal Federated Learning in Healthcare: a Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.09650
&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24341;&#20837;&#20102;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#65292;&#32467;&#21512;&#26368;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#36827;&#23637;&#65292;&#30830;&#20445;&#20102;&#24739;&#32773;&#25968;&#25454;&#38544;&#31169;&#21644;&#23433;&#20840;&#65292;&#20026;&#20248;&#21270;&#21307;&#30103;AI&#31995;&#32479;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#36171;&#20104;&#20102;&#21307;&#23398;&#39046;&#22495;&#20934;&#30830;&#32780;&#31283;&#20581;&#30340;AI&#31995;&#32479;&#30340;&#21457;&#23637;&#65292;&#23588;&#20854;&#26159;&#22312;&#20013;&#24515;&#21270;&#25968;&#25454;&#24211;&#31995;&#32479;&#20869;&#12290;&#21516;&#26102;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20063;&#24471;&#21040;&#20102;&#36827;&#23637;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#25968;&#25454;&#26080;&#38656;&#25972;&#21512;&#30340;&#21435;&#20013;&#24515;&#21270;&#26426;&#21046;&#65292;&#22686;&#24378;&#20102;&#23545;&#25935;&#24863;&#21307;&#30103;&#25968;&#25454;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#12290;&#36825;&#20004;&#20010;&#27010;&#24565;&#30340;&#25972;&#21512;&#25903;&#25345;&#20102;&#21307;&#30103;&#20445;&#20581;&#20013;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#25345;&#32493;&#36827;&#23637;&#65292;&#21516;&#26102;&#30830;&#20445;&#20102;&#24739;&#32773;&#35760;&#24405;&#22312;&#26412;&#22320;&#25968;&#25454;&#25345;&#26377;&#26426;&#26500;&#20869;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#12290;&#26412;&#25991;&#31616;&#35201;&#27010;&#36848;&#20102;FL&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#27010;&#36848;&#20102;&#21307;&#30103;&#39046;&#22495;&#20869;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#65288;MMFL&#65289;&#30340;&#26368;&#26032;&#25216;&#26415;&#26041;&#27861;&#12290;&#25991;&#31456;&#20840;&#38754;&#23457;&#35270;&#20102;&#35813;&#39046;&#22495;&#20013;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.09650v2 Announce Type: replace-cross  Abstract: Recent advancements in multimodal machine learning have empowered the development of accurate and robust AI systems in the medical domain, especially within centralized database systems. Simultaneously, Federated Learning (FL) has progressed, providing a decentralized mechanism where data need not be consolidated, thereby enhancing the privacy and security of sensitive healthcare data. The integration of these two concepts supports the ongoing progress of multimodal learning in healthcare while ensuring the security and privacy of patient records within local data-holding agencies. This paper offers a concise overview of the significance of FL in healthcare and outlines the current state-of-the-art approaches to Multimodal Federated Learning (MMFL) within the healthcare domain. It comprehensively examines the existing challenges in the field, shedding light on the limitations of present models. Finally, the paper outlines poten
&lt;/p&gt;</description></item><item><title>&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#21518;&#32493;&#38382;&#39064;&#26102;&#24120;&#24120;&#25671;&#25670;&#19981;&#23450;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#21518;&#32493;&#38382;&#39064;&#26426;&#21046;&#21644;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#24320;&#21457;&#20986;Unwavering-FQ&#26694;&#26550;&#26469;&#25945;&#23548;&#27169;&#22411;&#20445;&#25345;&#26368;&#21021;&#30340;&#27491;&#30830;&#21028;&#26029;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.02174</link><description>&lt;p&gt;
&#35753;&#24490;&#29615;&#30340;&#35810;&#38382;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21028;&#26029;&#20013;&#30340;&#25671;&#25670;
&lt;/p&gt;
&lt;p&gt;
Ask Again, Then Fail: Large Language Models' Vacillations in Judgement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02174
&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#21518;&#32493;&#38382;&#39064;&#26102;&#24120;&#24120;&#25671;&#25670;&#19981;&#23450;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#21518;&#32493;&#38382;&#39064;&#26426;&#21046;&#21644;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#24320;&#21457;&#20986;Unwavering-FQ&#26694;&#26550;&#26469;&#25945;&#23548;&#27169;&#22411;&#20445;&#25345;&#26368;&#21021;&#30340;&#27491;&#30830;&#21028;&#26029;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35266;&#23519;&#21040;&#30446;&#21069;&#30340;&#20250;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#21518;&#32493;&#38382;&#39064;&#26102;&#24448;&#24448;&#22312;&#20854;&#21028;&#26029;&#19978;&#25671;&#25670;&#19981;&#23450;&#65292;&#21363;&#20351;&#21407;&#22987;&#21028;&#26029;&#26159;&#27491;&#30830;&#30340;&#12290;&#36825;&#31181;&#25671;&#25670;&#23545;&#20110;&#29983;&#25104;&#21487;&#38752;&#22238;&#22797;&#21644;&#24314;&#31435;&#29992;&#25143;&#20449;&#20219;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21518;&#32493;&#38382;&#39064;&#26426;&#21046;&#20197;&#21450;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#30830;&#35748;&#20102;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#26222;&#36941;&#23384;&#22312;&#36825;&#31181;&#24773;&#20917;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21508;&#31181;&#25552;&#31034;&#31574;&#30053;&#29992;&#20110;&#38381;&#28304;&#27169;&#22411;&#65307;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#35757;&#32451;&#30340;&#26694;&#26550;Unwavering-FQ&#65292;&#36890;&#36807;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#20559;&#22909;&#25968;&#25454;&#26469;&#25945;&#23548;&#35821;&#35328;&#27169;&#22411;&#20445;&#25345;&#20854;&#26368;&#21021;&#30340;&#27491;&#30830;&#21028;&#26029;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#20854;&#22686;&#24378;&#27169;&#22411;&#36890;&#29992;&#33021;&#21147;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02174v2 Announce Type: replace-cross  Abstract: We observe that current conversational language models often waver in their judgements when faced with follow-up questions, even if the original judgement was correct. This wavering presents a significant challenge for generating reliable responses and building user trust. To comprehensively assess this issue, we introduce a Follow-up Questioning Mechanism along with two metrics to quantify this inconsistency, confirming its widespread presence in current language models. To mitigate this issue, we explore various prompting strategies for closed-source models; moreover, we develop a training-based framework Unwavering-FQ that teaches language models to maintain their originally correct judgements through synthesized high-quality preference data. Our experimental results confirm the effectiveness of our framework and its ability to enhance the general capabilities of models (https://github.com/NUSTM/LLMs-Waver-In-Judgements).
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;Atlas&#24341;&#23548;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#38382;&#39064;</title><link>https://arxiv.org/abs/2307.00676</link><description>&lt;p&gt;
&#20851;&#27880;Atlas&#65306;Atlas&#24341;&#23548;&#30340;&#29992;&#20110;&#40065;&#26834;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Pay Attention to the Atlas: Atlas-Guided Test-Time Adaptation Method for Robust 3D Medical Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.00676
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;Atlas&#24341;&#23548;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#22312;&#27979;&#35797;&#30446;&#26631;&#25968;&#25454;&#19982;&#35757;&#32451;&#65288;&#28304;&#65289;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#26102;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#65292;&#23588;&#20854;&#22312;&#21307;&#23398;&#25104;&#20687;&#24212;&#29992;&#20013;&#65292;&#19981;&#21516;&#20020;&#24202;&#31449;&#28857;&#21644;&#25195;&#25551;&#20202;&#20043;&#38388;&#30340;&#25104;&#20687;&#21327;&#35758;&#21464;&#21270;&#23548;&#33268;&#19981;&#21516;&#30340;&#25104;&#20687;&#22806;&#35266;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Atlas&#24341;&#23548;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#65288;TTA&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#30340;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#31216;&#20026;AdaAtlas&#12290;AdaAtlas&#20165;&#38656;&#35201;&#19968;&#20010;&#26410;&#26631;&#35760;&#30340;&#21333;&#20010;&#27979;&#35797;&#26679;&#26412;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#22522;&#20110;Atlas&#30340;&#25439;&#22833;&#26469;&#35843;&#25972;&#20998;&#21106;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.00676v2 Announce Type: replace-cross  Abstract: Convolutional neural networks (CNNs) often suffer from poor performance when tested on target data that differs from the training (source) data distribution, particularly in medical imaging applications where variations in imaging protocols across different clinical sites and scanners lead to different imaging appearances. However, re-accessing source training data for unsupervised domain adaptation or labeling additional test data for model fine-tuning can be difficult due to privacy issues and high labeling costs, respectively. To solve this problem, we propose a novel atlas-guided test-time adaptation (TTA) method for robust 3D medical image segmentation, called AdaAtlas. AdaAtlas only takes one single unlabeled test sample as input and adapts the segmentation network by minimizing an atlas-based loss. Specifically, the network is adapted so that its prediction after registration is aligned with the learned atlas in the atla
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#39640;&#25928;Contextformer&#65292;&#37319;&#29992;&#20102;&#26102;&#31354;&#36890;&#36947;&#31383;&#21475;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#23398;&#20064;&#22270;&#20687;&#21387;&#32553;&#20013;&#24555;&#36895;&#19978;&#19979;&#25991;&#24314;&#27169;&#65292;&#24182;&#24341;&#20837;&#20102;&#20248;&#21270;&#25216;&#26415;&#26469;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;</title><link>https://arxiv.org/abs/2306.14287</link><description>&lt;p&gt;
&#39640;&#25928;Contextformer&#65306;&#29992;&#20110;&#23398;&#20064;&#22270;&#20687;&#21387;&#32553;&#20013;&#24555;&#36895;&#19978;&#19979;&#25991;&#24314;&#27169;&#30340;&#26102;&#31354;&#36890;&#36947;&#31383;&#21475;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Efficient Contextformer: Spatio-Channel Window Attention for Fast Context Modeling in Learned Image Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.14287
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#39640;&#25928;Contextformer&#65292;&#37319;&#29992;&#20102;&#26102;&#31354;&#36890;&#36947;&#31383;&#21475;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#23398;&#20064;&#22270;&#20687;&#21387;&#32553;&#20013;&#24555;&#36895;&#19978;&#19979;&#25991;&#24314;&#27169;&#65292;&#24182;&#24341;&#20837;&#20102;&#20248;&#21270;&#25216;&#26415;&#26469;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29109;&#20272;&#35745;&#23545;&#20110;&#23398;&#20064;&#22270;&#20687;&#21387;&#32553;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#22522;&#20110;Transformer&#30340;&#29109;&#27169;&#22411;&#23545;&#20110;&#23454;&#29616;&#39640;&#21387;&#32553;&#27604;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#26159;&#38656;&#35201;&#20184;&#20986;&#24040;&#22823;&#30340;&#35745;&#31639;&#20195;&#20215;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39640;&#25928;Contextformer&#65288;eContextformer&#65289;- &#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#22270;&#20687;&#21387;&#32553;&#30340;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#22522;&#20110;Transformer&#30340;&#33258;&#22238;&#24402;&#19978;&#19979;&#25991;&#27169;&#22411;&#12290;eContextformer&#26377;&#25928;&#22320;&#34701;&#21512;&#20102;&#22522;&#20110;&#22359;&#12289;&#26827;&#30424;&#26684;&#21644;&#36890;&#36947;&#30340;&#20998;&#32452;&#25216;&#26415;&#65292;&#29992;&#20110;&#24182;&#34892;&#19978;&#19979;&#25991;&#24314;&#27169;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#31227;&#20301;&#31383;&#21475;&#26102;&#31354;&#36890;&#36947;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#26356;&#22909;&#30340;&#35757;&#32451;&#31574;&#30053;&#21644;&#26550;&#26500;&#35774;&#35745;&#65292;&#24182;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#22797;&#26434;&#24615;&#20248;&#21270;&#12290;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#20248;&#21270;&#25216;&#26415;&#21160;&#24577;&#35843;&#25972;&#27880;&#24847;&#21147;&#33539;&#22260;&#24182;&#32531;&#23384;&#20808;&#21069;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#65292;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#27169;&#22411;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.14287v2 Announce Type: replace-cross  Abstract: Entropy estimation is essential for the performance of learned image compression. It has been demonstrated that a transformer-based entropy model is of critical importance for achieving a high compression ratio, however, at the expense of a significant computational effort. In this work, we introduce the Efficient Contextformer (eContextformer) - a computationally efficient transformer-based autoregressive context model for learned image compression. The eContextformer efficiently fuses the patch-wise, checkered, and channel-wise grouping techniques for parallel context modeling, and introduces a shifted window spatio-channel attention mechanism. We explore better training strategies and architectural designs and introduce additional complexity optimizations. During decoding, the proposed optimization techniques dynamically scale the attention span and cache the previous attention computations, drastically reducing the model an
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MaskSub&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36974;&#32617;&#23376;&#27169;&#22411;&#21644;&#25918;&#26494;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#24378;&#21270;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#36974;&#32617;&#22686;&#24378;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2306.11339</link><description>&lt;p&gt;
&#36974;&#32617;&#25968;&#25454;&#22686;&#24378;&#29992;&#20110;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Masking Augmentation for Supervised Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.11339
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MaskSub&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36974;&#32617;&#23376;&#27169;&#22411;&#21644;&#25918;&#26494;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#24378;&#21270;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#36974;&#32617;&#22686;&#24378;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#38543;&#26426;&#36974;&#32617;&#36827;&#34892;&#39044;&#35757;&#32451;&#24050;&#32463;&#25104;&#20026;&#35757;&#32451;&#25216;&#26415;&#20013;&#30340;&#26032;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#30417;&#30563;&#23398;&#20064;&#22312;&#37319;&#29992;&#36974;&#32617;&#22686;&#24378;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#19981;&#31283;&#23450;&#30340;&#35757;&#32451;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28041;&#21450;&#36974;&#32617;&#22686;&#24378;&#30340;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;Masked Sub-model (MaskSub)&#12290;MaskSub&#30001;&#20027;&#27169;&#22411;&#21644;&#23376;&#27169;&#22411;&#32452;&#25104;&#65307;&#21069;&#32773;&#20139;&#21463;&#20256;&#32479;&#35757;&#32451;&#26041;&#27861;&#65292;&#32780;&#21518;&#32773;&#21033;&#29992;&#24378;&#22823;&#30340;&#36974;&#32617;&#22686;&#24378;&#26469;&#35757;&#32451;&#12290;MaskSub&#36890;&#36807;&#32531;&#35299;&#31867;&#20284;&#20110;&#33258;&#33976;&#39311;&#25439;&#22833;&#30340;&#25918;&#26494;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;MaskSub&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#35757;&#32451;&#25439;&#22833;&#30340;&#25910;&#25947;&#36895;&#24230;&#29978;&#33267;&#27604;&#24120;&#35268;&#35757;&#32451;&#26356;&#24555;&#65292;&#36825;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#21161;&#20110;&#35757;&#32451;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;MaskSub&#22312;&#21508;&#31181;&#35757;&#32451;&#26041;&#27861;&#21644;&#27169;&#22411;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;DeiT-III&#65292;MAE&#24494;&#35843;&#65292;CLIP&#24494;&#35843;&#65292;ResNet&#21644;Swin T&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.11339v2 Announce Type: replace-cross  Abstract: Pre-training using random masking has emerged as a novel trend in training techniques. However, supervised learning faces a challenge in adopting masking augmentations, primarily due to unstable training. In this paper, we propose a novel way to involve masking augmentations dubbed Masked Sub-model (MaskSub). MaskSub consists of the main-model and sub-model; while the former enjoys conventional training recipes, the latter leverages the benefit of strong masking augmentations in training. MaskSub addresses the challenge by mitigating adverse effects through a relaxed loss function similar to a self-distillation loss. Our analysis shows that MaskSub improves performance, with the training loss converging even faster than regular training, which suggests our method facilitates training. We further validate MaskSub across diverse training recipes and models, including DeiT-III, MAE fine-tuning, CLIP fine-tuning, ResNet, and Swin T
&lt;/p&gt;</description></item><item><title>&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;Atari 2600&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;OCAtari&#25193;&#23637;&#20102;Atari Learning Environments&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23545;&#28216;&#25103;&#20013;&#22522;&#20110;&#23545;&#35937;&#30340;&#29366;&#24577;&#36827;&#34892;&#36164;&#28304;&#39640;&#25928;&#25552;&#21462;&#65292;&#24182;&#20801;&#35768;&#23545;&#35937;&#21457;&#29616;&#12289;&#23545;&#35937;&#34920;&#24449;&#23398;&#20064;&#20197;&#21450;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2306.08649</link><description>&lt;p&gt;
OCAtari: &#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;Atari 2600&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
OCAtari: Object-Centric Atari 2600 Reinforcement Learning Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.08649
&lt;/p&gt;
&lt;p&gt;
&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;Atari 2600&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;OCAtari&#25193;&#23637;&#20102;Atari Learning Environments&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23545;&#28216;&#25103;&#20013;&#22522;&#20110;&#23545;&#35937;&#30340;&#29366;&#24577;&#36827;&#34892;&#36164;&#28304;&#39640;&#25928;&#25552;&#21462;&#65292;&#24182;&#20801;&#35768;&#23545;&#35937;&#21457;&#29616;&#12289;&#23545;&#35937;&#34920;&#24449;&#23398;&#20064;&#20197;&#21450;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#31185;&#23398;&#21644;&#24515;&#29702;&#23398;&#34920;&#26126;&#65292;&#22797;&#26434;&#22330;&#26223;&#30340;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#34920;&#24449;&#26159;&#23454;&#29616;&#20174;&#20302;&#32423;&#24863;&#30693;&#29305;&#24449;&#26377;&#25928;&#25277;&#35937;&#25512;&#29702;&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21482;&#20381;&#36182;&#20110;&#20687;&#32032;&#32423;&#34920;&#31034;&#65292;&#26080;&#27861;&#25429;&#25417;&#33258;&#28982;&#22330;&#26223;&#30340;&#32452;&#21512;&#29305;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#20801;&#35768;&#25105;&#20204;&#22788;&#29702;&#21644;&#35780;&#20272;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#26041;&#27861;&#30340;&#29615;&#22659;&#21644;&#25968;&#25454;&#38598;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;OCAtari&#26469;&#25193;&#23637;Atari&#23398;&#20064;&#29615;&#22659;&#65292;&#36825;&#26159;&#28145;&#24230;RL&#26041;&#27861;&#26368;&#24120;&#29992;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;OCAtari&#23545;&#36825;&#20123;&#28216;&#25103;&#36827;&#34892;&#20102;&#36164;&#28304;&#39640;&#25928;&#30340;&#23545;&#35937;&#20013;&#24515;&#29366;&#24577;&#25552;&#21462;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#23545;&#35937;&#21457;&#29616;&#12289;&#23545;&#35937;&#34920;&#24449;&#23398;&#20064;&#20197;&#21450;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;RL&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;OCAtari&#30340;&#26816;&#27979;&#33021;&#21147;&#21644;&#36164;&#28304;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;&#21487;&#22312;github.com/k4ntz/OC_Atari&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.08649v2 Announce Type: replace-cross  Abstract: Cognitive science and psychology suggest that object-centric representations of complex scenes are a promising step towards enabling efficient abstract reasoning from low-level perceptual features. Yet, most deep reinforcement learning approaches only rely on pixel-based representations that do not capture the compositional properties of natural scenes. For this, we need environments and datasets that allow us to work and evaluate object-centric approaches. In our work, we extend the Atari Learning Environments, the most-used evaluation framework for deep RL approaches, by introducing OCAtari, that performs resource-efficient extractions of the object-centric states for these games. Our framework allows for object discovery, object representation learning, as well as object-centric RL. We evaluate OCAtari's detection capabilities and resource efficiency. Our source code is available at github.com/k4ntz/OC_Atari.
&lt;/p&gt;</description></item><item><title>&#23454;&#26102;&#36882;&#24402;&#23398;&#20064;&#65288;RTRL&#65289;&#20855;&#26377;&#19968;&#23450;&#27010;&#24565;&#20248;&#21183;&#65292;&#19981;&#38656;&#35201;&#32531;&#23384;&#36807;&#21435;&#30340;&#28608;&#27963;&#29366;&#24577;&#21644;&#25130;&#26029;&#19978;&#19979;&#25991;&#65292;&#25903;&#25345;&#22312;&#32447;&#23398;&#20064;&#65292;&#22312;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#20013;&#25506;&#32034;&#20102;&#20854;&#23454;&#38469;&#28508;&#21147;&#65292;&#24182;&#22312;DMLab-30&#12289;ProcGen&#21644;Atari-2600&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#22312;DMLab&#23384;&#20648;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#19982;&#20248;&#20110;IMPALA&#21644;R2D2&#22522;&#32447;&#30456;&#23218;&#32654;&#30340;&#31454;&#20105;&#21147;&#65292;&#20026;&#20102;&#24212;&#23545;&#22797;&#26434;&#20219;&#21153;&#65292;&#30740;&#31350;&#37325;&#28857;&#25918;&#22312;&#20102;&#26576;&#20123;&#26041;&#38754;</title><link>https://arxiv.org/abs/2305.19044</link><description>&lt;p&gt;
&#25506;&#32034;&#23454;&#26102;&#36882;&#24402;&#23398;&#20064;&#30340;&#28508;&#21147;&#19982;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Exploring the Promise and Limits of Real-Time Recurrent Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.19044
&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#36882;&#24402;&#23398;&#20064;&#65288;RTRL&#65289;&#20855;&#26377;&#19968;&#23450;&#27010;&#24565;&#20248;&#21183;&#65292;&#19981;&#38656;&#35201;&#32531;&#23384;&#36807;&#21435;&#30340;&#28608;&#27963;&#29366;&#24577;&#21644;&#25130;&#26029;&#19978;&#19979;&#25991;&#65292;&#25903;&#25345;&#22312;&#32447;&#23398;&#20064;&#65292;&#22312;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#20013;&#25506;&#32034;&#20102;&#20854;&#23454;&#38469;&#28508;&#21147;&#65292;&#24182;&#22312;DMLab-30&#12289;ProcGen&#21644;Atari-2600&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#22312;DMLab&#23384;&#20648;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#19982;&#20248;&#20110;IMPALA&#21644;R2D2&#22522;&#32447;&#30456;&#23218;&#32654;&#30340;&#31454;&#20105;&#21147;&#65292;&#20026;&#20102;&#24212;&#23545;&#22797;&#26434;&#20219;&#21153;&#65292;&#30740;&#31350;&#37325;&#28857;&#25918;&#22312;&#20102;&#26576;&#20123;&#26041;&#38754;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#36882;&#24402;&#23398;&#20064;&#65288;RTRL&#65289;&#29992;&#20110;&#24207;&#21015;&#22788;&#29702;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#30456;&#27604;&#20110;&#26102;&#38388;&#21453;&#21521;&#20256;&#25773;&#65288;BPTT&#65289;&#20855;&#26377;&#19968;&#23450;&#30340;&#27010;&#24565;&#20248;&#21183;&#12290;RTRL&#26082;&#19981;&#38656;&#35201;&#32531;&#23384;&#36807;&#21435;&#30340;&#28608;&#27963;&#29366;&#24577;&#65292;&#20063;&#19981;&#38656;&#35201;&#25130;&#26029;&#19978;&#19979;&#25991;&#65292;&#32780;&#19988;&#25903;&#25345;&#22312;&#32447;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;RTRL&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#20351;&#20854;&#23454;&#38469;&#24212;&#29992;&#22256;&#38590;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#20851;&#20110;RTRL&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#36817;&#20284;&#29702;&#35770;&#19978;&#65292;&#32780;&#23454;&#39564;&#36890;&#24120;&#23616;&#38480;&#20110;&#35786;&#26029;&#35774;&#32622;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#22312;&#26356;&#29616;&#23454;&#30340;&#29615;&#22659;&#20013;&#25506;&#32034;&#20102;RTRL&#30340;&#23454;&#38469;&#28508;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#32467;&#21512;&#20102;RTRL&#21644;&#31574;&#30053;&#26799;&#24230;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#65292;&#24182;&#22312;DMLab-30&#12289;ProcGen&#21644;Atari-2600&#29615;&#22659;&#30340;&#20960;&#20010;&#23376;&#38598;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#22312;DMLab&#23384;&#20648;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#23569;&#20110;1.2 B&#30340;&#29615;&#22659;&#24103;&#19978;&#35757;&#32451;&#65292;&#19982;&#25110;&#20248;&#20110;&#22312;10 B&#24103;&#19978;&#35757;&#32451;&#30340;&#33879;&#21517;IMPALA&#21644;R2D2&#22522;&#32447;&#12290;&#20026;&#20102;&#25193;&#23637;&#21040;&#36825;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#26576;&#20123;&#26041;&#38754;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.19044v2 Announce Type: replace  Abstract: Real-time recurrent learning (RTRL) for sequence-processing recurrent neural networks (RNNs) offers certain conceptual advantages over backpropagation through time (BPTT). RTRL requires neither caching past activations nor truncating context, and enables online learning. However, RTRL's time and space complexity make it impractical. To overcome this problem, most recent work on RTRL focuses on approximation theories, while experiments are often limited to diagnostic settings. Here we explore the practical promise of RTRL in more realistic settings. We study actor-critic methods that combine RTRL and policy gradients, and test them in several subsets of DMLab-30, ProcGen, and Atari-2600 environments. On DMLab memory tasks, our system trained on fewer than 1.2 B environmental frames is competitive with or outperforms well-known IMPALA and R2D2 baselines trained on 10 B frames. To scale to such challenging tasks, we focus on certain wel
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#22686;&#24378;&#26159;&#19968;&#31181;&#21033;&#29992;dropout&#25110;PCA&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#36716;&#25442;&#30446;&#26631;&#23618;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#25913;&#21892;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#22312;Transformers&#12289;ResNets&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#31561;&#22522;&#30784;&#27169;&#22411;&#19978;&#65292;&#36890;&#36807;&#28145;&#24230;&#22686;&#24378;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#22312;&#30417;&#30563;&#38382;&#39064;&#19978;&#25928;&#26524;&#30456;&#21453;&#12290;</title><link>https://arxiv.org/abs/2303.14537</link><description>&lt;p&gt;
&#28145;&#24230;&#22686;&#24378;&#65306;&#22312;&#28608;&#27963;&#31354;&#38388;&#20013;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Deep Augmentation: Self-Supervised Learning with Transformations in Activation Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.14537
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#22686;&#24378;&#26159;&#19968;&#31181;&#21033;&#29992;dropout&#25110;PCA&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#36716;&#25442;&#30446;&#26631;&#23618;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#25913;&#21892;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#22312;Transformers&#12289;ResNets&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#31561;&#22522;&#30784;&#27169;&#22411;&#19978;&#65292;&#36890;&#36807;&#28145;&#24230;&#22686;&#24378;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#22312;&#30417;&#30563;&#38382;&#39064;&#19978;&#25928;&#26524;&#30456;&#21453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28145;&#24230;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36749;&#23398;&#25110;PCA&#26469;&#36716;&#25442;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#30446;&#26631;&#23618;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22270;&#23398;&#20064;&#20013;&#30340;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#26469;&#23637;&#31034;&#28145;&#24230;&#22686;&#24378;&#12290; &#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#23545;&#27604;&#23398;&#20064;&#30340;&#22522;&#30784;&#27169;&#22411;&#20013;&#65292;&#22914;Transformers&#12289;ResNets&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#19978;&#28145;&#24230;&#22686;&#24378;&#33021;&#22815;&#24102;&#26469;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#22312;&#30456;&#24212;&#30340;&#30417;&#30563;&#38382;&#39064;&#19978;&#35266;&#23519;&#21040;&#30456;&#21453;&#30340;&#25928;&#26524;&#12290; &#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#28145;&#24230;&#22686;&#24378;&#20943;&#36731;&#20102;&#23618;&#20043;&#38388;&#30340;&#30456;&#20114;&#36866;&#24212;&#65292;&#21363;"&#23849;&#28291;"&#24418;&#24335;&#30340;&#38382;&#39064;&#12290; &#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#21046;&#23450;&#20102;&#19968;&#31181;&#36873;&#25321;&#30446;&#26631;&#23618;&#30340;&#26041;&#27861;&#65307;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#29992;&#28145;&#24230;&#22686;&#24378;&#23450;&#20301;&#26356;&#28145;&#23618;&#27425;&#30340;&#23618;&#35201;&#20248;&#20110;&#22686;&#24378;&#36755;&#20837;&#25968;&#25454;&#12290; &#36825;&#31181;&#26041;&#27861;&#30340;&#31616;&#21333;&#32593;&#32476;&#21644;&#27169;&#24577;&#26080;&#20851;&#24615;&#20351;&#20854;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.14537v2 Announce Type: replace-cross  Abstract: We introduce Deep Augmentation, an approach to implicit data augmentation using dropout or PCA to transform a targeted layer within a neural network to improve performance and generalization. We demonstrate Deep Augmentation through extensive experiments on contrastive learning tasks in NLP, computer vision, and graph learning. We observe substantial performance gains with Transformers, ResNets, and Graph Neural Networks as the underlying models in contrastive learning, but observe inverse effects on the corresponding supervised problems. Our analysis suggests that Deep Augmentation alleviates co-adaption between layers, a form of "collapse." We use this observation to formulate a method for selecting which layer to target; in particular, our experimentation reveals that targeting deeper layers with Deep Augmentation outperforms augmenting the input data. The simple network- and modality-agnostic nature of this approach enables
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#20013;&#30340;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#20449;&#39640;&#25928;&#30340;&#31639;&#27861;FedBiOAcc&#65292;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#36890;&#20449;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#19982;&#23458;&#25143;&#31471;&#25968;&#37327;&#32447;&#24615;&#21152;&#36895;&#12290;</title><link>https://arxiv.org/abs/2302.06701</link><description>&lt;p&gt;
&#20855;&#26377;&#26412;&#22320;&#21644;&#20840;&#23616;&#19979;&#23618;&#38382;&#39064;&#30340;&#36890;&#20449;&#39640;&#25928;&#32852;&#37030;&#21452;&#23618;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Communication-Efficient Federated Bilevel Optimization with Local and Global Lower Level Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.06701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#20013;&#30340;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#20449;&#39640;&#25928;&#30340;&#31639;&#27861;FedBiOAcc&#65292;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#36890;&#20449;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#19982;&#23458;&#25143;&#31471;&#25968;&#37327;&#32447;&#24615;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#23618;&#20248;&#21270;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#26032;&#20852;&#26377;&#25928;&#31639;&#27861;&#19981;&#26029;&#28044;&#29616;&#12290;&#28982;&#32780;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#24212;&#29992;&#21452;&#23618;&#20248;&#21270;&#20173;&#30456;&#23545;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#65292;&#20197;&#21450;&#32852;&#37030;&#23398;&#20064;&#22266;&#26377;&#25361;&#25112;&#23545;&#21452;&#23618;&#31639;&#27861;&#25910;&#25947;&#30340;&#24433;&#21709;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedBiOAcc&#30340;&#36890;&#20449;&#39640;&#25928;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#21033;&#29992;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#23545;&#36229;&#26799;&#24230;&#30340;&#39640;&#25928;&#20272;&#35745;&#65292;&#24182;&#21033;&#29992;&#22522;&#20110;&#21160;&#37327;&#30340;&#26041;&#24046;&#20943;&#23569;&#21152;&#36895;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;FedBiOAcc&#23454;&#29616;&#20102;&#36890;&#20449;&#22797;&#26434;&#24230;$O(\epsilon^{-1})$&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;$O(\epsilon^{-1.5})$&#65292;&#24182;&#19988;&#38543;&#30528;&#23458;&#25143;&#31471;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#32447;&#24615;&#21152;&#36895;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#32852;&#37030;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#30340;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#65292;&#21363;&#30001;&#23458;&#25143;&#31471;&#26412;&#22320;&#31649;&#29702;&#19979;&#23618;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.06701v2 Announce Type: replace  Abstract: Bilevel Optimization has witnessed notable progress recently with new emerging efficient algorithms. However, its application in the Federated Learning setting remains relatively underexplored, and the impact of Federated Learning's inherent challenges on the convergence of bilevel algorithms remain obscure. In this work, we investigate Federated Bilevel Optimization problems and propose a communication-efficient algorithm, named FedBiOAcc. The algorithm leverages an efficient estimation of the hyper-gradient in the distributed setting and utilizes the momentum-based variance-reduction acceleration. Remarkably, FedBiOAcc achieves a communication complexity $O(\epsilon^{-1})$, a sample complexity $O(\epsilon^{-1.5})$ and the linear speed up with respect to the number of clients. We also analyze a special case of the Federated Bilevel Optimization problems, where lower level problems are locally managed by clients. We prove that FedBiO
&lt;/p&gt;</description></item><item><title>&#22122;&#38899;&#22914;&#20309;&#24433;&#21709;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2302.05059</link><description>&lt;p&gt;
&#22122;&#38899;&#23545;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Effects of noise on the overparametrization of quantum neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.05059
&lt;/p&gt;
&lt;p&gt;
&#22122;&#38899;&#22914;&#20309;&#24433;&#21709;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#21442;&#25968;&#21270;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#20196;&#20154;&#24778;&#35766;&#21644;&#33261;&#21517;&#26157;&#33879;&#30340;&#29616;&#35937;&#20043;&#19968;&#12290;&#26368;&#36817;&#65292;&#26377;&#22810;&#27425;&#23581;&#35797;&#30740;&#31350;&#22312;&#27809;&#26377;&#30828;&#20214;&#22122;&#38899;&#30340;&#24773;&#20917;&#19979;&#65292;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNNs&#65289;&#26159;&#21542;&#20197;&#21450;&#22914;&#20309;&#34987;&#36807;&#24230;&#21442;&#25968;&#21270;&#12290;&#29305;&#21035;&#26159;&#65292;&#24050;&#32463;&#25552;&#20986;&#65292;&#22914;&#26524;QNN&#20855;&#26377;&#36275;&#22815;&#30340;&#21442;&#25968;&#26469;&#25506;&#32034;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#25152;&#26377;&#21487;&#29992;&#26041;&#21521;&#65292;&#21017;&#21487;&#20197;&#23558;&#20854;&#23450;&#20041;&#20026;&#36807;&#24230;&#21442;&#25968;&#21270;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#22914;&#26524;QNN&#30340;&#36755;&#20986;&#29366;&#24577;&#30340;&#37327;&#23376;&#36153;&#33293;&#23572;&#20449;&#24687;&#30697;&#38453;&#65288;QFIM&#65289;&#30340;&#31209;&#24471;&#20197;&#39281;&#21644;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22122;&#38899;&#30340;&#23384;&#22312;&#22914;&#20309;&#24433;&#21709;&#36807;&#24230;&#21442;&#25968;&#21270;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22122;&#38899;&#21487;&#20197;&#8220;&#25171;&#24320;&#8221;&#20808;&#21069;&#20026;&#38646;&#30340;QFIM&#29305;&#24449;&#20540;&#12290;&#36825;&#20351;&#24471;&#21442;&#25968;&#21270;&#29366;&#24577;&#21487;&#20197;&#25506;&#32034;&#21407;&#26412;&#26080;&#27861;&#35775;&#38382;&#30340;&#26041;&#21521;&#65292;&#20174;&#32780;&#28508;&#22312;&#22320;&#23558;&#19968;&#20010;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;QNN&#36716;&#21270;&#20026;&#19968;&#20010;&#27424;&#21442;&#25968;&#21270;&#30340;QNN&#12290;&#23545;&#20110;&#23567;&#22122;&#38899;&#27700;&#24179;&#65292;QNN&#26159;&#20934;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#65292;&#22240;&#20026;&#23384;&#22312;&#22823;&#30340;&#29305;&#24449;&#20540;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.05059v2 Announce Type: replace-cross  Abstract: Overparametrization is one of the most surprising and notorious phenomena in machine learning. Recently, there have been several efforts to study if, and how, Quantum Neural Networks (QNNs) acting in the absence of hardware noise can be overparametrized. In particular, it has been proposed that a QNN can be defined as overparametrized if it has enough parameters to explore all available directions in state space. That is, if the rank of the Quantum Fisher Information Matrix (QFIM) for the QNN's output state is saturated. Here, we explore how the presence of noise affects the overparametrization phenomenon. Our results show that noise can "turn on" previously-zero eigenvalues of the QFIM. This enables the parametrized state to explore directions that were otherwise inaccessible, thus potentially turning an overparametrized QNN into an underparametrized one. For small noise levels, the QNN is quasi-overparametrized, as large eige
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#29983;&#25104;&#24314;&#27169;&#22797;&#26434;&#21160;&#21147;&#23398;&#65292;&#24341;&#23548;&#37325;&#24314;&#27169;&#22411;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2212.07892</link><description>&lt;p&gt;
&#25972;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#29992;&#20110;&#22797;&#26434;&#21160;&#21147;&#23398;&#30340;&#32852;&#21512;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Integrating Multimodal Data for Joint Generative Modeling of Complex Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.07892
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#29983;&#25104;&#24314;&#27169;&#22797;&#26434;&#21160;&#21147;&#23398;&#65292;&#24341;&#23548;&#37325;&#24314;&#27169;&#22411;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#31185;&#23398;&#20013;&#24863;&#20852;&#36259;&#30340;&#31995;&#32479;&#26412;&#36136;&#19978;&#26159;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#12290;&#36890;&#24120;&#65292;&#25105;&#20204;&#36890;&#36807;&#26102;&#38388;&#24207;&#21015;&#27979;&#37327;&#26469;&#35775;&#38382;&#36825;&#20123;&#31995;&#32479;&#12290;&#36825;&#20123;&#26102;&#38388;&#24207;&#21015;&#21487;&#33021;&#30001;&#31163;&#25955;&#38543;&#26426;&#21464;&#37327;&#32780;&#38750;&#36830;&#32493;&#27979;&#37327;&#32452;&#25104;&#65292;&#25110;&#32773;&#21487;&#33021;&#30001;&#21516;&#26102;&#35266;&#23519;&#21040;&#30340;&#22810;&#20010;&#25968;&#25454;&#27169;&#24577;&#30340;&#27979;&#37327;&#32452;&#25104;&#12290;&#22312;&#31070;&#32463;&#31185;&#23398;&#20013;&#65292;&#25105;&#20204;&#21487;&#33021;&#38500;&#20102;&#33033;&#20914;&#35745;&#25968;&#21644;&#36830;&#32493;&#29983;&#29702;&#35760;&#24405;&#22806;&#65292;&#36824;&#26377;&#34892;&#20026;&#26631;&#31614;&#12290;&#34429;&#28982;&#29616;&#22312;&#20851;&#20110;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#21160;&#24577;&#31995;&#32479;&#37325;&#24314;&#30340;&#25991;&#29486;&#27491;&#22312;&#34028;&#21187;&#21457;&#23637;&#65292;&#20294;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#25104;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#20960;&#20046;&#27809;&#26377;&#34987;&#32771;&#34385;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#39640;&#25928;&#28789;&#27963;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#31232;&#30095;&#25945;&#24072;&#20449;&#21495;&#65292;&#25351;&#23548;&#37325;&#24314;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#21033;&#29992;&#20102;DSR&#35757;&#32451;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.07892v2 Announce Type: replace  Abstract: Many, if not most, systems of interest in science are naturally described as nonlinear dynamical systems. Empirically, we commonly access these systems through time series measurements. Often such time series may consist of discrete random variables rather than continuous measurements, or may be composed of measurements from multiple data modalities observed simultaneously. For instance, in neuroscience we may have behavioral labels in addition to spike counts and continuous physiological recordings. While by now there is a burgeoning literature on deep learning for dynamical systems reconstruction (DSR), multimodal data integration has hardly been considered in this context. Here we provide such an efficient and flexible algorithmic framework that rests on a multimodal variational autoencoder for generating a sparse teacher signal that guides training of a reconstruction model, exploiting recent advances in DSR training techniques. 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#27969;&#30340;&#35821;&#38899;&#36716;&#25442;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#25991;&#26412;&#21040;&#35821;&#38899;&#65292;&#25552;&#21319;&#20102;&#21457;&#38899;&#36136;&#37327;&#65292;&#24182;&#22312;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2210.17264</link><description>&lt;p&gt;
&#20855;&#26377;&#22522;&#20110;&#27969;&#30340;&#35821;&#38899;&#36716;&#25442;&#30340;&#36328;&#35821;&#35328;&#25991;&#26412;&#21040;&#35821;&#38899;&#26041;&#27861;&#20197;&#25913;&#21892;&#21457;&#38899;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual Text-To-Speech with Flow-based Voice Conversion for Improved Pronunciation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.17264
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#27969;&#30340;&#35821;&#38899;&#36716;&#25442;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#25991;&#26412;&#21040;&#35821;&#38899;&#65292;&#25552;&#21319;&#20102;&#21457;&#38899;&#36136;&#37327;&#65292;&#24182;&#22312;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31471;&#21040;&#31471;&#36328;&#35821;&#35328;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20445;&#30041;&#30446;&#26631;&#35821;&#35328;&#30340;&#21457;&#38899;&#65292;&#32780;&#19981;&#32771;&#34385;&#21407;&#22987;&#35828;&#35805;&#32773;&#30340;&#35821;&#35328;&#12290;&#25152;&#20351;&#29992;&#30340;&#27169;&#22411;&#22522;&#20110;&#38750;&#27880;&#24847;&#21147;Tacotron&#26550;&#26500;&#65292;&#20854;&#20013;&#35299;&#30721;&#22120;&#24050;&#32463;&#34987;&#26367;&#25442;&#20026;&#19968;&#20010;&#21463;&#35762;&#35805;&#32773;&#36523;&#20221;&#26465;&#20214;&#30340;&#24402;&#19968;&#21270;&#27969;&#32593;&#32476;&#65292;&#20801;&#35768;&#36890;&#36807;&#30456;&#21516;&#27169;&#22411;&#25191;&#34892;TTS&#21644;&#22768;&#38899;&#36716;&#25442;&#65288;VC&#65289;&#65292;&#22240;&#20026;&#22266;&#26377;&#30340;&#35821;&#35328;&#20869;&#23481;&#21644;&#35828;&#35805;&#32773;&#36523;&#20221;&#35299;&#32806;&#12290;&#22312;&#36328;&#35821;&#35328;&#35774;&#32622;&#20013;&#20351;&#29992;&#26102;&#65292;&#39318;&#20808;&#20351;&#29992;&#30446;&#26631;&#35821;&#35328;&#30340;&#26412;&#22320;&#21457;&#38899;&#32773;&#20135;&#29983;&#22768;&#23398;&#29305;&#24449;&#65292;&#28982;&#21518;&#36890;&#36807;&#30456;&#21516;&#27169;&#22411;&#24212;&#29992;&#22768;&#38899;&#36716;&#25442;&#65292;&#20197;&#23558;&#36825;&#20123;&#29305;&#24449;&#36716;&#25442;&#20026;&#30446;&#26631;&#35828;&#35805;&#32773;&#30340;&#22768;&#38899;&#12290;&#25105;&#20204;&#36890;&#36807;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#39564;&#35777;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#22522;&#20934;&#36328;&#35821;&#35328;&#21512;&#25104;&#30456;&#27604;&#20855;&#26377;&#19968;&#23450;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.17264v2 Announce Type: replace-cross  Abstract: This paper presents a method for end-to-end cross-lingual text-to-speech (TTS) which aims to preserve the target language's pronunciation regardless of the original speaker's language. The model used is based on a non-attentive Tacotron architecture, where the decoder has been replaced with a normalizing flow network conditioned on the speaker identity, allowing both TTS and voice conversion (VC) to be performed by the same model due to the inherent linguistic content and speaker identity disentanglement. When used in a cross-lingual setting, acoustic features are initially produced with a native speaker of the target language and then voice conversion is applied by the same model in order to convert these features to the target speaker's voice. We verify through objective and subjective evaluations that our method can have benefits compared to baseline cross-lingual synthesis. By including speakers averaging 7.5 minutes of spe
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#24265;&#20215;&#12289;&#27169;&#22359;&#21270;&#12289;&#31283;&#20581;&#19988;&#21487;&#25193;&#23637;&#30340;DManus&#24179;&#21488;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#20154;&#20013;&#28789;&#24039;&#25163;&#30340;&#39640;&#25104;&#26412;&#12289;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#20379;&#22823;&#35268;&#27169;&#25968;&#25454;&#37319;&#38598;&#65292;&#20854;&#24102;&#26377;&#35206;&#30422;&#25972;&#20010;&#25163;&#25484;&#21644;&#25351;&#23574;&#34920;&#38754;&#30340;ReSkin&#24863;&#24212;&#22120;&#12290;</title><link>https://arxiv.org/abs/2210.15658</link><description>&lt;p&gt;
&#25152;&#26377;&#24863;&#21463;&#65306;&#24102;&#26377;&#22823;&#38754;&#31215;&#35302;&#35273;&#20256;&#24863;&#30340;&#28789;&#24039;&#25163;
&lt;/p&gt;
&lt;p&gt;
All the Feels: A dexterous hand with large-area tactile sensing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.15658
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#24265;&#20215;&#12289;&#27169;&#22359;&#21270;&#12289;&#31283;&#20581;&#19988;&#21487;&#25193;&#23637;&#30340;DManus&#24179;&#21488;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#20154;&#20013;&#28789;&#24039;&#25163;&#30340;&#39640;&#25104;&#26412;&#12289;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#20379;&#22823;&#35268;&#27169;&#25968;&#25454;&#37319;&#38598;&#65292;&#20854;&#24102;&#26377;&#35206;&#30422;&#25972;&#20010;&#25163;&#25484;&#21644;&#25351;&#23574;&#34920;&#38754;&#30340;ReSkin&#24863;&#24212;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26114;&#30340;&#25104;&#26412;&#21644;&#32570;&#20047;&#21487;&#38752;&#24615;&#22952;&#30861;&#20102;&#28789;&#24039;&#25163;&#22312;&#26426;&#22120;&#20154;&#25216;&#26415;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#33021;&#22815;&#24863;&#30693;&#25972;&#20010;&#25163;&#21306;&#22495;&#30340;&#21487;&#38752;&#35302;&#35273;&#20256;&#24863;&#22120;&#38459;&#30861;&#20102;&#20016;&#23500;&#30340;&#20302;&#32423;&#21453;&#39304;&#65292;&#36825;&#23558;&#25913;&#21892;&#23545;&#28789;&#24039;&#25805;&#20316;&#25216;&#33021;&#30340;&#23398;&#20064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24265;&#20215;&#12289;&#27169;&#22359;&#21270;&#12289;&#31283;&#20581;&#19988;&#21487;&#25193;&#23637;&#30340;&#24179;&#21488; - DManus&#65292;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#21516;&#26102;&#28385;&#36275;&#28145;&#24230;&#26426;&#22120;&#20154;&#23398;&#20064;&#33539;&#24335;&#25152;&#38656;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#37319;&#38598;&#33021;&#21147;&#12290;&#23545;&#20154;&#31867;&#25805;&#20316;&#30340;&#30740;&#31350;&#25351;&#20986;&#65292;&#22312;&#25191;&#34892;&#26085;&#24120;&#28789;&#24039;&#20219;&#21153;&#26102;&#65292;&#20302;&#32423;&#35302;&#35273;&#21453;&#39304;&#30340;&#37325;&#35201;&#24615;&#12290;DManus&#37197;&#26377;ReSkin&#24863;&#24212;&#22120;&#65292;&#35206;&#30422;&#25972;&#20010;&#25163;&#25484;&#21644;&#25351;&#23574;&#30340;&#34920;&#38754;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23436;&#20840;&#38598;&#25104;&#31995;&#32479;&#22312;&#35302;&#35273;&#24863;&#30693;&#20219;&#21153; - &#31665;&#24335;&#25342;&#21462;&#21644;&#20998;&#31867;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#20195;&#30721;&#12289;&#25991;&#26723;&#12289;&#35774;&#35745;&#25991;&#20214;&#12289;&#35814;&#32454;&#32452;&#35013;&#35828;&#26126;&#25991;&#26723;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.15658v3 Announce Type: replace-cross  Abstract: High cost and lack of reliability has precluded the widespread adoption of dexterous hands in robotics. Furthermore, the lack of a viable tactile sensor capable of sensing over the entire area of the hand impedes the rich, low-level feedback that would improve learning of dexterous manipulation skills. This paper introduces an inexpensive, modular, robust, and scalable platform -- the DManus -- aimed at resolving these challenges while satisfying the large-scale data collection capabilities demanded by deep robot learning paradigms. Studies on human manipulation point to the criticality of low-level tactile feedback in performing everyday dexterous tasks. The DManus comes with ReSkin sensing on the entire surface of the palm as well as the fingertips. We demonstrate effectiveness of the fully integrated system in a tactile aware task -- bin picking and sorting. Code, documentation, design files, detailed assembly instructions, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#22238;&#24402;&#30340;&#26641;&#23548;&#21521;&#29305;&#24449;&#36873;&#25321;&#21644;&#36923;&#36753;&#32858;&#21512;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#21892;&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#24314;&#27169;&#21644;&#21033;&#29992;&#30142;&#30149;&#20998;&#31867;&#30340;&#33258;&#28982;&#20998;&#23618;&#32467;&#26500;</title><link>https://arxiv.org/abs/2206.09107</link><description>&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#20013;&#30340;&#26641;&#23548;&#21521;&#32597;&#35265;&#29305;&#24449;&#36873;&#25321;&#21644;&#36923;&#36753;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Tree-Guided Rare Feature Selection and Logic Aggregation with Electronic Health Records Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.09107
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#22238;&#24402;&#30340;&#26641;&#23548;&#21521;&#29305;&#24449;&#36873;&#25321;&#21644;&#36923;&#36753;&#32858;&#21512;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#21892;&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#24314;&#27169;&#21644;&#21033;&#29992;&#30142;&#30149;&#20998;&#31867;&#30340;&#33258;&#28982;&#20998;&#23618;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#20855;&#26377;&#22823;&#37327;&#32597;&#35265;&#20108;&#36827;&#21046;&#29305;&#24449;&#30340;&#32479;&#35745;&#23398;&#20064;&#22312;&#20998;&#26512;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#26102;&#24456;&#24120;&#35265;&#65292;&#23588;&#20854;&#26159;&#22312;&#21033;&#29992;&#20808;&#21069;&#30340;&#21307;&#23398;&#35786;&#26029;&#21644;&#31243;&#24207;&#23545;&#30142;&#30149;&#21457;&#29983;&#36827;&#34892;&#24314;&#27169;&#26102;&#12290;&#20026;&#20102;&#25913;&#21892;&#22522;&#20110;EHR&#30340;&#24314;&#27169;&#24182;&#21033;&#29992;&#30142;&#30149;&#20998;&#31867;&#30340;&#33258;&#28982;&#20998;&#23618;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#22238;&#24402;&#30340;&#26641;&#23548;&#21521;&#29305;&#24449;&#36873;&#25321;&#21644;&#36923;&#36753;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#31232;&#30095;&#36861;&#27714;&#21644;&#8220;&#25110;&#8221;&#36923;&#36753;&#36816;&#31639;&#31526;&#30340;&#32858;&#21512;&#20419;&#36827;&#22120;&#23454;&#29616;&#20102;&#38477;&#32500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2206.09107v2 Announce Type: replace  Abstract: Statistical learning with a large number of rare binary features is commonly encountered in analyzing electronic health records (EHR) data, especially in the modeling of disease onset with prior medical diagnoses and procedures. Dealing with the resulting highly sparse and large-scale binary feature matrix is notoriously challenging as conventional methods may suffer from a lack of power in testing and inconsistency in model fitting while machine learning methods may suffer from the inability of producing interpretable results or clinically-meaningful risk factors. To improve EHR-based modeling and utilize the natural hierarchical structure of disease classification, we propose a tree-guided feature selection and logic aggregation approach for large-scale regression with rare binary features, in which dimension reduction is achieved through not only a sparsity pursuit but also an aggregation promoter with the logic operator of ``or''
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#19968;&#32452;&#65288;&#25277;&#35937;&#30340;&#65289;&#23376;&#30446;&#26631;&#19978;&#36827;&#34892;&#32422;&#26463;&#21644;&#23398;&#20064;&#26412;&#22320;&#12289;&#23376;&#30446;&#26631;&#26465;&#20214;&#30340;&#27169;&#22411;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#30446;&#26631;&#31354;&#38388;&#35268;&#21010;&#65288;GSP&#65289;&#26041;&#27861;&#26356;&#20855;&#35745;&#31639;&#25928;&#29575;&#65292;&#33258;&#28982;&#22320;&#32467;&#21512;&#20102;&#26102;&#38388;&#25277;&#35937;&#65292;&#36991;&#20813;&#20102;&#23398;&#20064;&#36716;&#25442;&#21160;&#21147;&#23398;&#12290;</title><link>https://arxiv.org/abs/2206.02902</link><description>&lt;p&gt;
&#20855;&#26377;&#23376;&#30446;&#26631;&#27169;&#22411;&#30340;&#30446;&#26631;&#31354;&#38388;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Goal-Space Planning with Subgoal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.02902
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#19968;&#32452;&#65288;&#25277;&#35937;&#30340;&#65289;&#23376;&#30446;&#26631;&#19978;&#36827;&#34892;&#32422;&#26463;&#21644;&#23398;&#20064;&#26412;&#22320;&#12289;&#23376;&#30446;&#26631;&#26465;&#20214;&#30340;&#27169;&#22411;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#30446;&#26631;&#31354;&#38388;&#35268;&#21010;&#65288;GSP&#65289;&#26041;&#27861;&#26356;&#20855;&#35745;&#31639;&#25928;&#29575;&#65292;&#33258;&#28982;&#22320;&#32467;&#21512;&#20102;&#26102;&#38388;&#25277;&#35937;&#65292;&#36991;&#20813;&#20102;&#23398;&#20064;&#36716;&#25442;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#32972;&#26223;&#35268;&#21010;&#65306;&#28151;&#21512;&#65288;&#36817;&#20284;&#30340;&#65289;&#21160;&#24577;&#35268;&#21010;&#26356;&#26032;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#26356;&#26032;&#65292;&#31867;&#20284;&#20110;Dyna&#26550;&#26500;&#12290;&#21033;&#29992;&#23398;&#20064;&#30340;&#27169;&#22411;&#36827;&#34892;&#32972;&#26223;&#35268;&#21010;&#36890;&#24120;&#27604;&#27169;&#22411;&#26080;&#20851;&#30340;&#26367;&#20195;&#26041;&#27861;&#65288;&#22914;Double DQN&#65289;&#26356;&#24046;&#65292;&#23613;&#31649;&#21069;&#32773;&#20351;&#29992;&#20102;&#26174;&#33879;&#26356;&#22810;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#26681;&#26412;&#38382;&#39064;&#22312;&#20110;&#65292;&#23398;&#20064;&#30340;&#27169;&#22411;&#21487;&#33021;&#19981;&#20934;&#30830;&#65292;&#24182;&#19988;&#22312;&#36845;&#20195;&#22810;&#20010;&#27493;&#39588;&#26102;&#36890;&#24120;&#20250;&#20135;&#29983;&#26080;&#25928;&#29366;&#24577;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#32972;&#26223;&#35268;&#21010;&#38480;&#21046;&#22312;&#19968;&#32452;&#65288;&#25277;&#35937;&#30340;&#65289;&#23376;&#30446;&#26631;&#65292;&#24182;&#20165;&#23398;&#20064;&#26412;&#22320;&#12289;&#23376;&#30446;&#26631;&#26465;&#20214;&#30340;&#27169;&#22411;&#26469;&#36991;&#20813;&#36825;&#31181;&#38480;&#21046;&#12290;&#36825;&#31181;&#30446;&#26631;&#31354;&#38388;&#35268;&#21010;&#65288;GSP&#65289;&#26041;&#27861;&#26356;&#20855;&#35745;&#31639;&#25928;&#29575;&#65292;&#33258;&#28982;&#22320;&#32467;&#21512;&#20102;&#29992;&#20110;&#26356;&#24555;&#38271;&#26102;&#31243;&#35268;&#21010;&#30340;&#26102;&#38388;&#25277;&#35937;&#65292;&#24182;&#23436;&#20840;&#36991;&#20813;&#20102;&#23398;&#20064;&#36716;&#25442;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;GSP&#31639;&#27861;&#21487;&#20197;&#20256;&#25773;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
arXiv:2206.02902v5 Announce Type: replace-cross  Abstract: This paper investigates a new approach to model-based reinforcement learning using background planning: mixing (approximate) dynamic programming updates and model-free updates, similar to the Dyna architecture. Background planning with learned models is often worse than model-free alternatives, such as Double DQN, even though the former uses significantly more memory and computation. The fundamental problem is that learned models can be inaccurate and often generate invalid states, especially when iterated many steps. In this paper, we avoid this limitation by constraining background planning to a set of (abstract) subgoals and learning only local, subgoal-conditioned models. This goal-space planning (GSP) approach is more computationally efficient, naturally incorporates temporal abstraction for faster long-horizon planning and avoids learning the transition dynamics entirely. We show that our GSP algorithm can propagate value
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#28151;&#21512;&#25163;&#21183;&#35782;&#21035;&#31995;&#32479;&#65292;&#33021;&#22815;&#23398;&#20064;&#38745;&#24577;&#21644;&#21160;&#24577;&#25163;&#21183;&#65292;&#24182;&#36890;&#36807;&#25429;&#25417;&#25163;&#21183;&#34920;&#29616;&#39640;&#23792;&#30340;&#8220;&#24555;&#29031;&#8221;&#26469;&#34701;&#21512;&#38745;&#24577;&#23039;&#21183;&#21644;&#21160;&#24577;&#36816;&#21160;&#12290;</title><link>https://arxiv.org/abs/2205.15862</link><description>&lt;p&gt;
Snapture -- &#19968;&#31181;&#29992;&#20110;&#38745;&#24577;&#21644;&#21160;&#24577;&#25163;&#21183;&#35782;&#21035;&#30340;&#26032;&#22411;&#31070;&#32463;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Snapture -- A Novel Neural Architecture for Combined Static and Dynamic Hand Gesture Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.15862
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#28151;&#21512;&#25163;&#21183;&#35782;&#21035;&#31995;&#32479;&#65292;&#33021;&#22815;&#23398;&#20064;&#38745;&#24577;&#21644;&#21160;&#24577;&#25163;&#21183;&#65292;&#24182;&#36890;&#36807;&#25429;&#25417;&#25163;&#21183;&#34920;&#29616;&#39640;&#23792;&#30340;&#8220;&#24555;&#29031;&#8221;&#26469;&#34701;&#21512;&#38745;&#24577;&#23039;&#21183;&#21644;&#21160;&#24577;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#20154;&#39044;&#26399;&#26356;&#22810;&#22320;&#21442;&#19982;&#20154;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#65292;&#38656;&#35201;&#33021;&#22815;&#25552;&#20379;&#30452;&#35266;&#29992;&#25143;&#30028;&#38754;&#30340;&#26694;&#26550;&#12290;&#25163;&#21183;&#35782;&#21035;&#31995;&#32479;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#20132;&#27969;&#26041;&#24335;&#65292;&#22240;&#27492;&#26159;&#26080;&#32541;&#20154;&#26426;&#20132;&#20114;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#36817;&#24180;&#26469;&#65292;&#30001;&#28145;&#24230;&#23398;&#20064;&#39537;&#21160;&#30340;&#35745;&#31639;&#27169;&#22411;&#21457;&#29983;&#20102;&#24040;&#22823;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#36328;&#19981;&#21516;&#25163;&#21183;&#39046;&#22495;&#65288;&#22914;&#35937;&#24449;&#25163;&#21183;&#21644;&#20849;&#35821;&#25163;&#21183;&#65289;&#26041;&#38754;&#20173;&#23384;&#22312;&#19981;&#36275;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#25163;&#21183;&#35782;&#21035;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#23454;&#29616;&#20102;&#23545;&#38745;&#24577;&#21644;&#21160;&#24577;&#25163;&#21183;&#30340;&#23398;&#20064;&#65306;&#36890;&#36807;&#25429;&#25417;&#25163;&#21183;&#22312;&#20854;&#39640;&#23792;&#34920;&#29616;&#26102;&#30340;&#25152;&#35859;&#8220;&#24555;&#29031;&#8221;&#65292;&#25105;&#20204;&#23558;&#25163;&#21183;&#23039;&#21183;&#19982;&#21160;&#24577;&#36816;&#21160;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#25163;&#21183;&#36816;&#21160;&#36712;&#36857;&#30340;&#26041;&#27861;&#65292;&#20197;&#25581;&#31034;&#20854;&#21160;&#24577;&#29305;&#24449;&#65292;&#24182;&#20801;&#35768;&#35843;&#33410;&#19968;&#20010;&#38745;&#24577;&#36890;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2205.15862v2 Announce Type: replace-cross  Abstract: As robots are expected to get more involved in people's everyday lives, frameworks that enable intuitive user interfaces are in demand. Hand gesture recognition systems provide a natural way of communication and, thus, are an integral part of seamless Human-Robot Interaction (HRI). Recent years have witnessed an immense evolution of computational models powered by deep learning. However, state-of-the-art models fall short in expanding across different gesture domains, such as emblems and co-speech. In this paper, we propose a novel hybrid hand gesture recognition system. Our architecture enables learning both static and dynamic gestures: by capturing a so-called "snapshot" of the gesture performance at its peak, we integrate the hand pose along with the dynamic movement. Moreover, we present a method for analyzing the motion profile of a gesture to uncover its dynamic characteristics and which allows regulating a static channel
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22810;&#25968;&#25237;&#31080;&#30340;&#26041;&#24335;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32593;&#32476;&#23433;&#20840;&#20027;&#39064;&#20998;&#31867;&#24037;&#20855;&#65292;&#30456;&#27604;&#20110;21&#20010;&#21333;&#29420;&#27169;&#22411;&#65292;&#35813;&#24037;&#20855;&#22312;&#26816;&#27979;&#32593;&#32476;&#23433;&#20840;&#30456;&#20851;&#25991;&#26412;&#26102;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#20551;&#38451;&#24615;&#21644;&#20551;&#38452;&#24615;&#29575;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#25968;&#21313;&#19975;&#20221;&#25991;&#26723;&#12290;</title><link>https://arxiv.org/abs/2109.02473</link><description>&lt;p&gt;
&#19968;&#31181;&#24378;&#22823;&#30340;&#32593;&#32476;&#23433;&#20840;&#20027;&#39064;&#20998;&#31867;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
A Robust Cybersecurity Topic Classification Tool
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2109.02473
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22810;&#25968;&#25237;&#31080;&#30340;&#26041;&#24335;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32593;&#32476;&#23433;&#20840;&#20027;&#39064;&#20998;&#31867;&#24037;&#20855;&#65292;&#30456;&#27604;&#20110;21&#20010;&#21333;&#29420;&#27169;&#22411;&#65292;&#35813;&#24037;&#20855;&#22312;&#26816;&#27979;&#32593;&#32476;&#23433;&#20840;&#30456;&#20851;&#25991;&#26412;&#26102;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#20551;&#38451;&#24615;&#21644;&#20551;&#38452;&#24615;&#29575;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#25968;&#21313;&#19975;&#20221;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#19977;&#20010;&#20114;&#32852;&#32593;&#25991;&#26412;&#20449;&#24687;&#26469;&#28304;&#65288;Reddit&#65292;Stackexchange&#65292;Arxiv&#65289;&#30340;&#29992;&#25143;&#23450;&#20041;&#30340;&#26631;&#31614;&#65292;&#35757;&#32451;&#20102;21&#31181;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#33258;&#28982;&#25991;&#26412;&#20013;&#30340;&#32593;&#32476;&#23433;&#20840;&#35752;&#35770;&#30340;&#20027;&#39064;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#27599;&#20010;&#27169;&#22411;&#22312;&#20132;&#21449;&#39564;&#35777;&#23454;&#39564;&#20013;&#30340;&#20551;&#38451;&#24615;&#21644;&#20551;&#38452;&#24615;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32593;&#32476;&#23433;&#20840;&#20027;&#39064;&#20998;&#31867;&#65288;CTC&#65289;&#24037;&#20855;&#65292;&#35813;&#24037;&#20855;&#23558;21&#20010;&#35757;&#32451;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22810;&#25968;&#25237;&#31080;&#20316;&#20026;&#26816;&#27979;&#32593;&#32476;&#23433;&#20840;&#30456;&#20851;&#25991;&#26412;&#30340;&#20915;&#31574;&#26426;&#21046;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;CTC&#24037;&#20855;&#30340;&#22810;&#25968;&#25237;&#31080;&#26426;&#21046;&#24179;&#22343;&#25552;&#20379;&#20102;&#27604;21&#20010;&#21333;&#29420;&#27169;&#22411;&#26356;&#20302;&#30340;&#20551;&#38452;&#24615;&#21644;&#20551;&#38451;&#24615;&#29575;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CTC&#24037;&#20855;&#21487;&#25193;&#23637;&#21040;&#25968;&#21313;&#19975;&#20221;&#25991;&#26723;&#65292;&#32780;&#20854;&#22681;&#38047;&#26102;&#38388;&#22823;&#32422;&#20026;&#20960;&#23567;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2109.02473v4 Announce Type: replace-cross  Abstract: In this research, we use user defined labels from three internet text sources (Reddit, Stackexchange, Arxiv) to train 21 different machine learning models for the topic classification task of detecting cybersecurity discussions in natural text. We analyze the false positive and false negative rates of each of the 21 model's in a cross validation experiment. Then we present a Cybersecurity Topic Classification (CTC) tool, which takes the majority vote of the 21 trained machine learning models as the decision mechanism for detecting cybersecurity related text. We also show that the majority vote mechanism of the CTC tool provides lower false negative and false positive rates on average than any of the 21 individual models. We show that the CTC tool is scalable to the hundreds of thousands of documents with a wall clock time on the order of hours.
&lt;/p&gt;</description></item><item><title>OneLog&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21333;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23383;&#31526;&#32423;&#21035;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36719;&#20214;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2104.07324</link><description>&lt;p&gt;
OneLog: &#38754;&#21521;&#36719;&#20214;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
OneLog: Towards End-to-End Training in Software Log Anomaly Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2104.07324
&lt;/p&gt;
&lt;p&gt;
OneLog&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21333;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23383;&#31526;&#32423;&#21035;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36719;&#20214;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;&#32447;&#26381;&#21153;&#12289;&#29289;&#32852;&#32593;&#35774;&#22791;&#21644;&#38754;&#21521;DevOps&#30340;&#36719;&#20214;&#24320;&#21457;&#30340;&#22686;&#38271;&#65292;&#36719;&#20214;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;OneLog&#65292;&#23427;&#21033;&#29992;&#21333;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20195;&#26367;&#22810;&#20010;&#29420;&#31435;&#32452;&#20214;&#12290;OneLog&#22312;&#23383;&#31526;&#32423;&#21035;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#32771;&#34385;&#25968;&#23383;&#12289;&#25968;&#23383;&#21644;&#26631;&#28857;&#31526;&#21495;&#65292;&#24182;&#32467;&#21512;&#20027;&#35201;&#30340;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#65292;&#32780;&#36825;&#20123;&#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#34987;&#31227;&#38500;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;&#22522;&#20110;&#28040;&#24687;&#21644;&#24207;&#21015;&#30340;&#25968;&#25454;&#38598;&#65288;HDFS&#12289;Hadoop&#12289;BGL&#12289;Thunderbird&#12289;Spirit&#21644;Liberty&#65289;&#20013;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#21333;&#39033;&#30446;&#12289;&#22810;&#39033;&#30446;&#21644;&#36328;&#39033;&#30446;&#35774;&#32622;&#19979;&#23581;&#35797;&#20102;Onelog&#12290;Onelog&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#20013;&#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;Onelog&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21516;&#26102;&#21033;&#29992;&#22810;&#20010;&#39033;&#30446;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2104.07324v2 Announce Type: replace-cross  Abstract: With the growth of online services, IoT devices, and DevOps-oriented software development, software log anomaly detection is becoming increasingly important. Prior works mainly follow a traditional four-staged architecture (Preprocessor, Parser, Vectorizer, and Classifier). This paper proposes OneLog, which utilizes a single Deep Neural Network (DNN) instead of multiple separate components. OneLog harnesses Convolutional Neural Networks (CNN) at the character level to take digits, numbers, and punctuations, which were removed in prior works, into account alongside the main natural language text. We evaluate our approach in six message- and sequence-based data sets: HDFS, Hadoop, BGL, Thunderbird, Spirit, and Liberty. We experiment with Onelog with single-, multi-, and cross-project setups. Onelog offers state-of-the-art performance in our datasets. Onelog can utilize multi-project datasets simultaneously during training, which 
&lt;/p&gt;</description></item><item><title>DoubleML&#26159;&#22312;R&#20013;&#23454;&#29616;&#30340;&#21452;&#37325;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#20272;&#35745;&#22240;&#26524;&#27169;&#22411;&#21442;&#25968;&#30340;&#21151;&#33021;&#65292;&#21253;&#25324;&#22312;&#37096;&#20998;&#32447;&#24615;&#21644;&#20132;&#20114;&#22238;&#24402;&#27169;&#22411;&#20013;&#36827;&#34892;&#25512;&#26029;&#12290;&#23545;&#35937;&#23548;&#21521;&#30340;&#23454;&#29616;&#20351;&#24471;&#27169;&#22411;&#35268;&#33539;&#20855;&#26377;&#24456;&#39640;&#30340;&#28789;&#27963;&#24615;&#24182;&#26131;&#20110;&#25193;&#23637;&#12290;</title><link>https://arxiv.org/abs/2103.09603</link><description>&lt;p&gt;
DoubleML -- &#21452;&#37325;&#26426;&#22120;&#23398;&#20064;&#30340;&#38754;&#21521;&#23545;&#35937;&#23454;&#29616;&#22312;R&#20013;
&lt;/p&gt;
&lt;p&gt;
DoubleML -- An Object-Oriented Implementation of Double Machine Learning in R
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2103.09603
&lt;/p&gt;
&lt;p&gt;
DoubleML&#26159;&#22312;R&#20013;&#23454;&#29616;&#30340;&#21452;&#37325;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#20272;&#35745;&#22240;&#26524;&#27169;&#22411;&#21442;&#25968;&#30340;&#21151;&#33021;&#65292;&#21253;&#25324;&#22312;&#37096;&#20998;&#32447;&#24615;&#21644;&#20132;&#20114;&#22238;&#24402;&#27169;&#22411;&#20013;&#36827;&#34892;&#25512;&#26029;&#12290;&#23545;&#35937;&#23548;&#21521;&#30340;&#23454;&#29616;&#20351;&#24471;&#27169;&#22411;&#35268;&#33539;&#20855;&#26377;&#24456;&#39640;&#30340;&#28789;&#27963;&#24615;&#24182;&#26131;&#20110;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;R&#21253;DoubleML&#23454;&#29616;&#20102;Chernozhukov&#31561;&#20154;&#65288;2018&#65289;&#25552;&#20986;&#30340;&#21452;&#37325;/&#26080;&#20559;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#12290;&#23427;&#25552;&#20379;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20272;&#35745;&#22240;&#26524;&#27169;&#22411;&#21442;&#25968;&#30340;&#21151;&#33021;&#12290;&#21452;&#37325;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#35201;&#32032;&#65306;Neyman&#27491;&#20132;&#24615;&#12289;&#39640;&#36136;&#37327;&#30340;&#26426;&#22120;&#23398;&#20064;&#20272;&#35745;&#21644;&#26679;&#26412;&#25286;&#20998;&#12290;&#21487;&#20197;&#36890;&#36807;mlr3&#29983;&#24577;&#31995;&#32479;&#20013;&#25552;&#20379;&#30340;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#20272;&#35745;&#24178;&#25200;&#25104;&#20998;&#12290;DoubleML&#20351;&#24471;&#21487;&#20197;&#22312;&#21508;&#31181;&#22240;&#26524;&#27169;&#22411;&#20013;&#36827;&#34892;&#25512;&#26029;&#65292;&#21253;&#25324;&#37096;&#20998;&#32447;&#24615;&#21644;&#20132;&#20114;&#22238;&#24402;&#27169;&#22411;&#20197;&#21450;&#23427;&#20204;&#23545;&#24037;&#20855;&#21464;&#37327;&#20272;&#35745;&#30340;&#25193;&#23637;&#12290;DoubleML&#30340;&#38754;&#21521;&#23545;&#35937;&#23454;&#29616;&#20351;&#27169;&#22411;&#35268;&#33539;&#38750;&#24120;&#28789;&#27963;&#19988;&#26131;&#20110;&#25193;&#23637;&#12290;&#26412;&#25991;&#20316;&#20026;&#21452;&#37325;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#21644;R pac&#30340;&#20171;&#32461;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2103.09603v5 Announce Type: replace-cross  Abstract: The R package DoubleML implements the double/debiased machine learning framework of Chernozhukov et al. (2018). It provides functionalities to estimate parameters in causal models based on machine learning methods. The double machine learning framework consist of three key ingredients: Neyman orthogonality, high-quality machine learning estimation and sample splitting. Estimation of nuisance components can be performed by various state-of-the-art machine learning methods that are available in the mlr3 ecosystem. DoubleML makes it possible to perform inference in a variety of causal models, including partially linear and interactive regression models and their extensions to instrumental variable estimation. The object-oriented implementation of DoubleML enables a high flexibility for the model specification and makes it easily extendable. This paper serves as an introduction to the double machine learning framework and the R pac
&lt;/p&gt;</description></item><item><title>&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#26088;&#22312;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#29983;&#25104;&#20196;&#20154;&#28385;&#24847;&#30340;ML&#37197;&#32622;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;AutoML&#21407;&#29702;&#21644;&#23454;&#36341;&#30340;&#20840;&#38754;&#35843;&#30740;&#12290;</title><link>https://arxiv.org/abs/1810.13306</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65306;&#20174;&#21407;&#29702;&#21040;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Automated Machine Learning: From Principles to Practices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1810.13306
&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#26088;&#22312;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#29983;&#25104;&#20196;&#20154;&#28385;&#24847;&#30340;ML&#37197;&#32622;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;AutoML&#21407;&#29702;&#21644;&#23454;&#36341;&#30340;&#20840;&#38754;&#35843;&#30740;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#21457;&#23637;&#36805;&#36895;&#65292;&#20294;&#37197;&#32622;&#21644;&#36873;&#25321;&#21512;&#36866;&#30340;&#26041;&#27861;&#20197;&#36798;&#21040;&#25152;&#38656;&#24615;&#33021;&#27491;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#21644;&#20047;&#21619;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#24212;&#36816;&#32780;&#29983;&#65292;&#26088;&#22312;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#20026;&#32473;&#23450;&#20219;&#21153;&#29983;&#25104;&#20196;&#20154;&#28385;&#24847;&#30340;ML&#37197;&#32622;&#12290;&#26412;&#25991;&#23601;&#35813;&#20027;&#39064;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#30740;&#12290;&#25105;&#20204;&#20174;AutoML&#30340;&#27491;&#24335;&#23450;&#20041;&#24320;&#22987;&#65292;&#28982;&#21518;&#20171;&#32461;&#20854;&#21407;&#29702;&#65292;&#21253;&#25324;&#21452;&#23618;&#23398;&#20064;&#30446;&#26631;&#12289;&#23398;&#20064;&#31574;&#30053;&#21644;&#29702;&#35770;&#35299;&#37322;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#19977;&#20010;&#20027;&#35201;&#22240;&#32032;&#8212;&#8212;&#25628;&#32034;&#31354;&#38388;&#12289;&#25628;&#32034;&#31639;&#27861;&#21644;&#35780;&#20272;&#31574;&#30053;&#8212;&#8212;&#35774;&#31435;&#29616;&#26377;&#24037;&#20316;&#30340;&#20998;&#31867;&#27861;&#26469;&#24635;&#32467;AutoML&#23454;&#36341;&#12290;&#27599;&#20010;&#31867;&#21035;&#36824;&#20250;&#36890;&#36807;&#20195;&#34920;&#24615;&#26041;&#27861;&#36827;&#34892;&#35299;&#37322;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#37197;&#32622;ML&#31649;&#32447;&#31561;&#31034;&#20363;&#24212;&#29992;&#35828;&#26126;&#20102;AutoML&#30340;&#21407;&#29702;&#21644;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1810.13306v5 Announce Type: replace  Abstract: Machine learning (ML) methods have been developing rapidly, but configuring and selecting proper methods to achieve a desired performance is increasingly difficult and tedious. To address this challenge, automated machine learning (AutoML) has emerged, which aims to generate satisfactory ML configurations for given tasks in a data-driven way. In this paper, we provide a comprehensive survey on this topic. We begin with the formal definition of AutoML and then introduce its principles, including the bi-level learning objective, the learning strategy, and the theoretical interpretation. Then, we summarize the AutoML practices by setting up the taxonomy of existing works based on three main factors: the search space, the search algorithm, and the evaluation strategy. Each category is also explained with the representative methods. Then, we illustrate the principles and practices with exemplary applications from configuring ML pipeline, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#35268;&#21010;&#36793;&#30028;&#27861;&#65292;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#25214;&#21040;&#21487;&#35777;&#26126;&#26368;&#20248;&#31232;&#30095;&#29983;&#23384;&#26641;&#27169;&#22411;&#65292;&#23545;&#20110;&#28041;&#21450;&#20154;&#31867;&#20581;&#24247;&#30340;&#39640;&#39118;&#38505;&#38382;&#39064;&#30340;&#20998;&#26512;&#21644;&#20915;&#31574;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.15330</link><description>&lt;p&gt;
&#26368;&#20248;&#31232;&#30095;&#29983;&#23384;&#26641;
&lt;/p&gt;
&lt;p&gt;
Optimal Sparse Survival Trees. (arXiv:2401.15330v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#35268;&#21010;&#36793;&#30028;&#27861;&#65292;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#25214;&#21040;&#21487;&#35777;&#26126;&#26368;&#20248;&#31232;&#30095;&#29983;&#23384;&#26641;&#27169;&#22411;&#65292;&#23545;&#20110;&#28041;&#21450;&#20154;&#31867;&#20581;&#24247;&#30340;&#39640;&#39118;&#38505;&#38382;&#39064;&#30340;&#20998;&#26512;&#21644;&#20915;&#31574;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28041;&#21450;&#20154;&#31867;&#20581;&#24247;&#30340;&#39640;&#39118;&#38505;&#38382;&#39064;&#30340;&#20998;&#26512;&#21644;&#20915;&#31574;&#20013;&#65292;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#21307;&#29983;&#12289;&#21307;&#38498;&#12289;&#21046;&#33647;&#20844;&#21496;&#21644;&#29983;&#29289;&#25216;&#26415;&#20844;&#21496;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#20854;&#21560;&#24341;&#20154;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#25429;&#25417;&#22797;&#26434;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#22522;&#20110;&#26641;&#30340;&#26041;&#27861;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#29983;&#23384;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#29983;&#25104;&#29983;&#23384;&#26641;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#65288;&#25110;&#36138;&#23146;&#65289;&#31639;&#27861;&#65292;&#23384;&#22312;&#29983;&#25104;&#27425;&#20248;&#27169;&#22411;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#35268;&#21010;&#36793;&#30028;&#27861;&#65292;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#25214;&#21040;&#21487;&#35777;&#26126;&#26368;&#20248;&#31232;&#30095;&#29983;&#23384;&#26641;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability is crucial for doctors, hospitals, pharmaceutical companies and biotechnology corporations to analyze and make decisions for high stakes problems that involve human health. Tree-based methods have been widely adopted for \textit{survival analysis} due to their appealing interpretablility and their ability to capture complex relationships. However, most existing methods to produce survival trees rely on heuristic (or greedy) algorithms, which risk producing sub-optimal models. We present a dynamic-programming-with-bounds approach that finds provably-optimal sparse survival tree models, frequently in only a few seconds.
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;Ricci&#27969;&#24341;&#23548;&#30340;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#65292;&#23588;&#20854;&#26159;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#23398;&#20064;&#27969;&#24418;&#65292;&#24182;&#20351;&#29992;Ricci&#27969;&#20351;&#27969;&#24418;&#28508;&#31354;&#38388;&#36880;&#27493;&#36866;&#24212;&#21160;&#21147;&#23398;&#30340;&#21464;&#21270;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22909;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#20855;&#26377;&#21608;&#26399;&#24615;&#21644;&#38543;&#26426;&#24615;&#30340;PDE&#19978;&#30340;&#24212;&#29992;&#65292;&#24182;&#35780;&#20272;&#20102;&#22312;&#20998;&#24067;&#20869;&#21644;&#22806;&#25512;&#22330;&#26223;&#20013;&#30340;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2401.14591</link><description>&lt;p&gt;
&#21033;&#29992;Ricci&#27969;&#24341;&#23548;&#30340;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#26102;&#21464;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Ricci flow-guided autoencoders in learning time-dependent dynamics. (arXiv:2401.14591v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14591
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;Ricci&#27969;&#24341;&#23548;&#30340;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#65292;&#23588;&#20854;&#26159;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#23398;&#20064;&#27969;&#24418;&#65292;&#24182;&#20351;&#29992;Ricci&#27969;&#20351;&#27969;&#24418;&#28508;&#31354;&#38388;&#36880;&#27493;&#36866;&#24212;&#21160;&#21147;&#23398;&#30340;&#21464;&#21270;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22909;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#20855;&#26377;&#21608;&#26399;&#24615;&#21644;&#38543;&#26426;&#24615;&#30340;PDE&#19978;&#30340;&#24212;&#29992;&#65292;&#24182;&#35780;&#20272;&#20102;&#22312;&#20998;&#24067;&#20869;&#21644;&#22806;&#25512;&#22330;&#26223;&#20013;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#24418;&#30340;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#26102;&#38388;&#19978;&#30340;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#65292;&#23588;&#20854;&#26159;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#65292;&#20854;&#20013;&#27969;&#24418;&#28508;&#31354;&#38388;&#26681;&#25454;Ricci&#27969;&#21457;&#23637;&#12290;&#36825;&#21487;&#20197;&#36890;&#36807;&#22312;&#29289;&#29702;&#20449;&#24687;&#35774;&#32622;&#20013;&#27169;&#25311;Ricci&#27969;&#26469;&#23454;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#21305;&#37197;&#27969;&#24418;&#37327;&#65292;&#20197;&#20415;&#23454;&#29616;Ricci&#27969;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#27969;&#24418;&#26159;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#23398;&#20064;&#30340;&#65292;&#22240;&#27492;&#21487;&#20197;&#35782;&#21035;&#20986;&#29702;&#24819;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#21516;&#26102;&#28436;&#21464;&#20063;&#33021;&#22312;&#38745;&#24577;&#26041;&#27861;&#19978;&#24341;&#36215;&#26356;&#23485;&#23481;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20855;&#26377;&#21608;&#26399;&#24615;&#21644;&#38543;&#26426;&#24615;&#31561;&#29702;&#24819;&#29305;&#24449;&#30340;PDE&#65292;&#24182;&#22312;&#20998;&#24067;&#20869;&#21644;&#22806;&#25512;&#22330;&#26223;&#20013;&#36827;&#34892;&#35823;&#24046;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a manifold-based autoencoder method for learning nonlinear dynamics in time, notably partial differential equations (PDEs), in which the manifold latent space evolves according to Ricci flow. This can be accomplished by simulating Ricci flow in a physics-informed setting, and manifold quantities can be matched so that Ricci flow is empirically achieved. With our methodology, the manifold is learned as part of the training procedure, so ideal geometries may be discerned, while the evolution simultaneously induces a more accommodating latent representation over static methods. We present our method on a range of numerical experiments consisting of PDEs that encompass desirable characteristics such as periodicity and randomness, remarking error on in-distribution and extrapolation scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21021;&#27493;&#25506;&#32034;&#20102;&#22522;&#20110;GPT-4&#30340;ChatGPT&#22312;&#38754;&#37096;&#29983;&#29289;&#35782;&#21035;&#20013;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#20998;&#26512;&#20102;ChatGPT&#22312;&#38754;&#37096;&#39564;&#35777;&#12289;&#36719;&#29983;&#29289;&#29305;&#24449;&#20272;&#35745;&#21644;&#32467;&#26524;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;ChatGPT&#30340;&#24212;&#29992;&#26377;&#26395;&#25552;&#39640;&#33258;&#21160;&#20915;&#31574;&#22312;&#20154;&#31867;&#22330;&#26223;&#20013;&#30340;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.13641</link><description>&lt;p&gt;
ChatGPT&#22312;&#38754;&#37096;&#29983;&#29289;&#35782;&#21035;&#20013;&#30340;&#34920;&#29616;&#26377;&#22810;&#22909;&#65311;&#23545;&#35782;&#21035;&#12289;&#36719;&#29983;&#29289;&#29305;&#24449;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#21021;&#27493;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
How Good is ChatGPT at Face Biometrics? A First Look into Recognition, Soft Biometrics, and Explainability. (arXiv:2401.13641v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21021;&#27493;&#25506;&#32034;&#20102;&#22522;&#20110;GPT-4&#30340;ChatGPT&#22312;&#38754;&#37096;&#29983;&#29289;&#35782;&#21035;&#20013;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#20998;&#26512;&#20102;ChatGPT&#22312;&#38754;&#37096;&#39564;&#35777;&#12289;&#36719;&#29983;&#29289;&#29305;&#24449;&#20272;&#35745;&#21644;&#32467;&#26524;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;ChatGPT&#30340;&#24212;&#29992;&#26377;&#26395;&#25552;&#39640;&#33258;&#21160;&#20915;&#31574;&#22312;&#20154;&#31867;&#22330;&#26223;&#20013;&#30340;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35832;&#22914;OpenAI&#24320;&#21457;&#30340;GPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#29616;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#65292;&#20026;&#25105;&#20204;&#30340;&#31038;&#20250;&#24341;&#20837;&#20102;&#24555;&#36895;&#21464;&#38761;&#12290;ChatGPT&#30340;&#21457;&#24067;&#36827;&#19968;&#27493;&#21152;&#24378;&#20102;&#36825;&#19968;&#24433;&#21709;&#65292;&#23427;&#20351;&#20219;&#20309;&#20154;&#37117;&#33021;&#20197;&#31616;&#21333;&#30340;&#23545;&#35805;&#26041;&#24335;&#19982;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#39046;&#22495;&#32463;&#39564;&#12290;&#22240;&#27492;&#65292;ChatGPT&#24050;&#34987;&#36805;&#36895;&#24212;&#29992;&#20110;&#35768;&#22810;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#22914;&#20195;&#30721;&#21644;&#27468;&#26354;&#21019;&#20316;&#12289;&#25945;&#32946;&#12289;&#34394;&#25311;&#21161;&#25163;&#31561;&#65292;&#23637;&#31034;&#20102;&#23545;&#20110;&#26410;&#32463;&#36807;&#35757;&#32451;&#30340;&#20219;&#21153;&#32780;&#35328;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65288;&#38646;&#26679;&#26412;&#23398;&#20064;&#65289;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#22522;&#20110;&#26368;&#26032;&#30340;GPT-4&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#30340;ChatGPT&#22312;&#38754;&#37096;&#29983;&#29289;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;ChatGPT&#22312;&#38754;&#37096;&#39564;&#35777;&#12289;&#36719;&#29983;&#29289;&#29305;&#24449;&#20272;&#35745;&#21644;&#32467;&#26524;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;ChatGPT&#23545;&#20110;&#36827;&#19968;&#27493;&#22686;&#21152;&#20154;&#31867;&#22330;&#26223;&#20013;&#33258;&#21160;&#20915;&#31574;&#30340;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24230;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#23454;&#39564;&#34987;&#36827;&#34892;&#20197;&#35780;&#20272;ChatGPT&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) such as GPT developed by OpenAI, have already shown astonishing results, introducing quick changes in our society. This has been intensified by the release of ChatGPT which allows anyone to interact in a simple conversational way with LLMs, without any experience in the field needed. As a result, ChatGPT has been rapidly applied to many different tasks such as code- and song-writer, education, virtual assistants, etc., showing impressive results for tasks for which it was not trained (zero-shot learning).  The present study aims to explore the ability of ChatGPT, based on the recent GPT-4 multimodal LLM, for the task of face biometrics. In particular, we analyze the ability of ChatGPT to perform tasks such as face verification, soft-biometrics estimation, and explainability of the results. ChatGPT could be very valuable to further increase the explainability and transparency of the automatic decisions in human scenarios. Experiments are carried out in order
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#20165;&#26377;&#30340;&#23569;&#37327;&#24494;&#23567;&#22810;&#35821;&#35328;&#24179;&#34892;&#25968;&#25454;&#26469;&#22686;&#24378;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#32763;&#35793;&#33021;&#21147;&#65292;&#23454;&#29616;&#22823;&#24133;&#24230;&#30340;&#38750;&#33521;&#25991;&#25972;&#20307;&#25913;&#36827;&#65292;&#24182;&#20445;&#25345;&#33521;&#25991;&#26041;&#21521;&#19978;&#30340;&#24615;&#33021;&#33021;&#21147;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#38543;&#26426;&#25277;&#21462;&#30340;&#23569;&#37327;&#26041;&#21521;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20063;&#21487;&#20197;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#25972;&#20307;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.12413</link><description>&lt;p&gt;
100&#20010;&#26679;&#26412;&#21487;&#20197;&#36208;&#22810;&#36828;&#65311;&#36890;&#36807;&#24494;&#23567;&#30340;&#22810;&#35821;&#35328;&#24179;&#34892;&#25968;&#25454;&#35299;&#38145;&#20840;&#38754;&#30340;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
How Far Can 100 Samples Go? Unlocking Overall Zero-Shot Multilingual Translation via Tiny Multi-Parallel Data. (arXiv:2401.12413v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#20165;&#26377;&#30340;&#23569;&#37327;&#24494;&#23567;&#22810;&#35821;&#35328;&#24179;&#34892;&#25968;&#25454;&#26469;&#22686;&#24378;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#32763;&#35793;&#33021;&#21147;&#65292;&#23454;&#29616;&#22823;&#24133;&#24230;&#30340;&#38750;&#33521;&#25991;&#25972;&#20307;&#25913;&#36827;&#65292;&#24182;&#20445;&#25345;&#33521;&#25991;&#26041;&#21521;&#19978;&#30340;&#24615;&#33021;&#33021;&#21147;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#38543;&#26426;&#25277;&#21462;&#30340;&#23569;&#37327;&#26041;&#21521;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20063;&#21487;&#20197;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#25972;&#20307;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#32763;&#35793;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#26088;&#22312;&#22312;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#65288;MMT&#65289;&#20013;&#32763;&#35793;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#23545;&#12290;&#19968;&#31181;&#24120;&#35265;&#20294;&#36164;&#28304;&#28040;&#32791;&#36739;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#23613;&#21487;&#33021;&#25366;&#25496;&#26356;&#22810;&#30340;&#32763;&#35793;&#26041;&#21521;&#24182;&#28155;&#21152;&#21040;&#24179;&#34892;&#35821;&#26009;&#24211;&#20013;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#36890;&#36807;&#20351;&#29992;&#20165;&#26377;&#30340;&#23569;&#37327;&#24494;&#23567;&#22810;&#35821;&#35328;&#24179;&#34892;&#25968;&#25454;&#26469;&#20248;&#21270;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;&#22312;EC30&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20165;&#20351;&#29992;100&#20010;&#22810;&#35821;&#35328;&#24179;&#34892;&#26679;&#26412;&#23601;&#33021;&#22815;&#23454;&#29616;+21.7 ChrF&#38750;&#33521;&#25991;&#25972;&#20307;&#25913;&#36827;&#65288;870&#20010;&#26041;&#21521;&#65289;&#65292;&#21516;&#26102;&#20445;&#25345;&#22312;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#26041;&#21521;&#19978;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#24494;&#35843;&#25968;&#25454;&#30340;&#35268;&#27169;&#25928;&#24212;&#21644;&#20854;&#36716;&#31227;&#33021;&#21147;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#22312;&#19968;&#20010;&#23567;&#30340;&#12289;&#38543;&#26426;&#25277;&#21462;&#30340;&#26041;&#21521;&#38598;&#65288;10%&#65289;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20063;&#21487;&#20197;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#25972;&#20307;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25152;&#24471;&#21040;&#30340;&#38750;&#33521;&#25991;&#24615;&#33021;&#19982;&#33521;&#25991;&#24615;&#33021;&#38750;&#24120;&#25509;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot translation is an open problem, aiming to translate between language pairs unseen during training in Multilingual Machine Translation (MMT). A common, albeit resource-consuming, solution is to mine as many translation directions as possible to add to the parallel corpus. In this paper, we show that the zero-shot capability of an English-centric model can be easily enhanced by fine-tuning with a very small amount of multi-parallel data. For example, on the EC30 dataset, we show that up to +21.7 ChrF non-English overall improvements (870 directions) can be achieved by using only 100 multi-parallel samples, meanwhile preserving capability in English-centric directions. We further study the size effect of fine-tuning data and its transfer capabilities. Surprisingly, our empirical analysis shows that comparable overall improvements can be achieved even through fine-tuning in a small, randomly sampled direction set (10\%). Also, the resulting non-English performance is quite close 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#28857;&#21464;&#25442;&#22120;&#32467;&#21512;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#21994;&#21644;&#22055;&#21878;&#27861;&#26579;&#33394;&#30340;&#20840;&#20999;&#29255;&#22270;&#20687;&#20013;&#39044;&#27979;&#20083;&#33146;&#30284;HER2&#29366;&#24577;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#21160;&#24577;&#26631;&#31614;&#20998;&#24067;&#31574;&#30053;&#21644;&#36741;&#21161;&#20998;&#31867;&#22120;&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26631;&#31614;&#19981;&#24179;&#34913;&#21644;&#21033;&#29992;&#23616;&#37096;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#38271;&#31243;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.06454</link><description>&lt;p&gt;
&#20351;&#29992;&#28857;&#21464;&#25442;&#22120;&#32467;&#21512;&#32852;&#37030;&#23398;&#20064;&#20174;&#21994;&#21644;&#22055;&#21878;&#27861;&#27927;&#20840;&#20999;&#29255;&#22270;&#20687;&#20013;&#39044;&#27979;&#20083;&#33146;&#30284;HER2&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
Point Transformer with Federated Learning for Predicting Breast Cancer HER2 Status from Hematoxylin and Eosin-Stained Whole Slide Images. (arXiv:2312.06454v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.06454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#28857;&#21464;&#25442;&#22120;&#32467;&#21512;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#21994;&#21644;&#22055;&#21878;&#27861;&#26579;&#33394;&#30340;&#20840;&#20999;&#29255;&#22270;&#20687;&#20013;&#39044;&#27979;&#20083;&#33146;&#30284;HER2&#29366;&#24577;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#21160;&#24577;&#26631;&#31614;&#20998;&#24067;&#31574;&#30053;&#21644;&#36741;&#21161;&#20998;&#31867;&#22120;&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26631;&#31614;&#19981;&#24179;&#34913;&#21644;&#21033;&#29992;&#23616;&#37096;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#38271;&#31243;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#20174;&#24191;&#27867;&#21487;&#24471;&#30340;&#21994;&#21644;&#22055;&#21878;&#27861;&#26579;&#33394;&#20840;&#20999;&#29255;&#22270;&#20687;&#20013;&#39044;&#27979;&#20154;&#31867;&#34920;&#30382;&#29983;&#38271;&#22240;&#23376;&#21463;&#20307;2&#65288;HER2&#65289;&#29366;&#24577;&#21487;&#20197;&#38477;&#20302;&#25216;&#26415;&#25104;&#26412;&#65292;&#21152;&#24555;&#27835;&#30103;&#36873;&#25321;&#36895;&#24230;&#12290;&#20934;&#30830;&#39044;&#27979;HER2&#38656;&#35201;&#22823;&#37327;&#30340;&#22810;&#22320;&#28857;&#20840;&#20999;&#29255;&#22270;&#20687;&#12290;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#22312;&#19981;&#20256;&#36755;&#21315;&#20806;&#23383;&#33410;&#22823;&#23567;&#30340;&#20840;&#20999;&#29255;&#22270;&#20687;&#21644;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#65292;&#21327;&#21516;&#35757;&#32451;&#36825;&#20123;&#20840;&#20999;&#29255;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;&#23398;&#20064;&#22312;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#20013;&#22810;&#22320;&#28857;&#20840;&#20999;&#29255;&#22270;&#20687;&#30340;&#26631;&#31614;&#19981;&#24179;&#34913;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#19981;&#33021;&#21516;&#26102;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#20013;&#31449;&#28857;&#31471;&#29305;&#24449;&#34920;&#31034;&#20013;&#30340;&#23616;&#37096;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28857;&#21464;&#25442;&#22120;&#32467;&#21512;&#32852;&#37030;&#23398;&#20064;&#20174;&#21994;&#21644;&#22055;&#21878;&#27861;&#26579;&#33394;&#20840;&#20999;&#29255;&#22270;&#20687;&#20013;&#39044;&#27979;&#22810;&#22320;&#28857;HER2&#29366;&#24577;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#39033;&#26032;&#35774;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#26631;&#31614;&#20998;&#24067;&#31574;&#30053;&#21644;&#19968;&#20010;&#36741;&#21161;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Directly predicting human epidermal growth factor receptor 2 (HER2) status from widely available hematoxylin and eosin (HE)-stained whole slide images (WSIs) can reduce technical costs and expedite treatment selection. Accurately predicting HER2 requires large collections of multi-site WSIs. Federated learning enables collaborative training of these WSIs without gigabyte-size WSIs transportation and data privacy concerns. However, federated learning encounters challenges in addressing label imbalance in multi-site WSIs from the real world. Moreover, existing WSI classification methods cannot simultaneously exploit local context information and long-range dependencies in the site-end feature representation of federated learning. To address these issues, we present a point transformer with federated learning for multi-site HER2 status prediction from HE-stained WSIs. Our approach incorporates two novel designs. We propose a dynamic label distribution strategy and an auxiliary classifier,
&lt;/p&gt;</description></item><item><title>&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#36890;&#36807;&#22810;&#38454;&#27573;&#21327;&#20316;&#33976;&#39311;&#30340;&#26041;&#24335;&#24212;&#29992;&#20110;&#21322;&#30417;&#30563;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.08640</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#38454;&#27573;&#21327;&#20316;&#30693;&#35782;&#33976;&#39311;&#22312;&#21322;&#30417;&#30563;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multistage Collaborative Knowledge Distillation from Large Language Models for Semi-Supervised Sequence Generation. (arXiv:2311.08640v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.08640
&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#36890;&#36807;&#22810;&#38454;&#27573;&#21327;&#20316;&#33976;&#39311;&#30340;&#26041;&#24335;&#24212;&#29992;&#20110;&#21322;&#30417;&#30563;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21322;&#30417;&#30563;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#65292;&#22312;&#36825;&#31181;&#20219;&#21153;&#20013;&#65292;&#26631;&#35760;&#25968;&#25454;&#22826;&#23569;&#20197;&#33267;&#20110;&#26080;&#27861;&#26377;&#25928;&#22320;&#24494;&#35843;&#27169;&#22411;&#65292;&#21516;&#26102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#20013;&#36827;&#34892;&#23569;&#26679;&#26412;&#25552;&#31034;&#30340;&#24615;&#33021;&#20063;&#19981;&#22815;&#29702;&#24819;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#19968;&#20123;&#26114;&#36149;&#19988;&#23545;&#39044;&#35757;&#32451;&#30340; LLM &#19981;&#29087;&#24713;&#30340;&#20219;&#21153;&#65292;&#22914;&#35299;&#26512;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#20174;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340; LLM &#33976;&#39311;&#20986;&#30340;&#23398;&#29983;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#36890;&#24120;&#27604;&#20854;&#25945;&#24072;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861; - &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#38454;&#27573;&#21327;&#20316;&#30693;&#35782;&#33976;&#39311; (MCKD) - &#29992;&#20110;&#36825;&#20123;&#20219;&#21153;&#12290;MCKD &#39318;&#20808;&#36827;&#34892;&#23569;&#26679;&#26412;&#25552;&#31034;&#65292;&#35753;LLM&#20026;&#26080;&#26631;&#31614;&#25968;&#25454;&#29983;&#25104;&#20266;&#26631;&#31614;&#12290;&#22312;&#27599;&#20010;&#20013;&#38388;&#30693;&#35782;&#33976;&#39311; (KD) &#38454;&#27573;&#65292;&#20351;&#29992;&#20266;&#26631;&#31614;&#25968;&#25454;&#30340;&#19981;&#37325;&#21472;&#20998;&#21306;&#26469;&#35757;&#32451;&#19968;&#23545;&#26032;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#27599;&#20010;&#23398;&#29983;&#27169;&#22411;&#20026;&#20854;&#26410;&#35265;&#20998;&#21306;&#29983;&#25104;&#26032;&#30340;&#21644;&#25913;&#36827;&#30340;&#20266;&#26631;&#31614;&#65292;&#22312;&#19979;&#19968;&#20010;&#33976;&#39311;&#38454;&#27573;&#20013;&#20351;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study semi-supervised sequence generation tasks where labeled data are too scarce to effectively finetune a model and at the same time few-shot prompting of a large language model (LLM) has suboptimal performance. This happens when a task, such as parsing, is expensive to annotate and also unfamiliar to a pretrained LLM. In this paper, we present a discovery that student models distilled from an in-context learned LLM can often generalize better than their teacher on such tasks. Leveraging this finding, we present a new method -multistage collaborative knowledge distillation from an LLM (MCKD) -- for such tasks. MCKD first few-shot prompts an LLM to produce pseudolabels for unlabeled data. At each intermediate knowledge distillation (KD) stage, a new pair of students is trained on disjoint partitions of the pseudolabeled data. Each student then produces new and improved pseudolabels for its unseen partition to be used in the next stage of distillation. We demonstrate the advantage
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26465;&#20214;&#38750;&#32447;&#24615;&#33258;&#21160;&#32534;&#30721;&#22120;(CVAE)&#36827;&#34892;&#36712;&#36857;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(VAE)&#20013;&#30340;&#38750;&#32447;&#24615;&#37319;&#26679;&#36807;&#31243;&#21644;&#20854;&#20182;&#25913;&#36827;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#25216;&#26415;&#65292;&#20026;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#30340;&#36712;&#36857;&#39044;&#27979;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.19944</link><description>&lt;p&gt;
&#26465;&#20214;&#38750;&#32447;&#24615;&#33258;&#21160;&#32534;&#30721;&#22120;&#29992;&#20110;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Conditional Unscented Autoencoders for Trajectory Prediction. (arXiv:2310.19944v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26465;&#20214;&#38750;&#32447;&#24615;&#33258;&#21160;&#32534;&#30721;&#22120;(CVAE)&#36827;&#34892;&#36712;&#36857;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(VAE)&#20013;&#30340;&#38750;&#32447;&#24615;&#37319;&#26679;&#36807;&#31243;&#21644;&#20854;&#20182;&#25913;&#36827;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#25216;&#26415;&#65292;&#20026;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#30340;&#36712;&#36857;&#39044;&#27979;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(CVAE)&#26159;&#33258;&#21160;&#39550;&#39542;&#36712;&#36857;&#39044;&#27979;&#20013;&#26368;&#24120;&#29992;&#30340;&#27169;&#22411;&#20043;&#19968;&#12290;&#23427;&#23558;&#39550;&#39542;&#29615;&#22659;&#21644;&#30495;&#23454;&#26410;&#26469;&#20851;&#31995;&#24314;&#31435;&#22312;&#27010;&#29575;&#38544;&#31354;&#38388;&#20013;&#65292;&#24182;&#21033;&#29992;&#27492;&#31354;&#38388;&#29983;&#25104;&#39044;&#27979;&#12290;&#26412;&#25991;&#23545;CVAE&#30340;&#20851;&#38190;&#32452;&#20214;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(VAE)&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21457;&#29616;&#21464;&#21270;&#37319;&#26679;&#36807;&#31243;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#38750;&#32447;&#24615;&#37319;&#26679;&#33021;&#22815;&#26356;&#36866;&#21512;&#20110;&#36712;&#36857;&#39044;&#27979;&#65292;&#32780;&#38543;&#26426;&#37319;&#26679;&#21487;&#33021;&#24102;&#26469;&#28508;&#22312;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20854;&#20182;&#25913;&#36827;&#65292;&#21253;&#25324;&#26356;&#32467;&#26500;&#21270;&#30340;&#28151;&#21512;&#38544;&#31354;&#38388;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#39062;&#12289;&#21487;&#33021;&#26356;&#20855;&#34920;&#36798;&#21147;&#30340;CVAE&#25512;&#29702;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;INTERACTION&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The \ac{CVAE} is one of the most widely-used models in trajectory prediction for \ac{AD}. It captures the interplay between a driving context and its ground-truth future into a probabilistic latent space and uses it to produce predictions. In this paper, we challenge key components of the CVAE. We leverage recent advances in the space of the VAE, the foundation of the CVAE, which show that a simple change in the sampling procedure can greatly benefit performance. We find that unscented sampling, which draws samples from any learned distribution in a deterministic manner, can naturally be better suited to trajectory prediction than potentially dangerous random sampling. We go further and offer additional improvements, including a more structured mixture latent space, as well as a novel, potentially more expressive way to do inference with CVAEs. We show wide applicability of our models by evaluating them on the INTERACTION prediction dataset, outperforming the state of the art, as well 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22312;&#32447;&#33258;&#21161;&#27861;&#29992;&#20110;&#22788;&#29702;&#22823;&#35268;&#27169;&#30340;&#26102;&#38388;&#24207;&#21015;&#21644;&#30456;&#20851;&#25968;&#25454;&#27969;&#65292;&#36890;&#36807;&#32771;&#34385;&#25968;&#25454;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#33258;&#21161;&#27861;&#22312;&#22797;&#26434;&#25968;&#25454;&#20381;&#36182;&#24773;&#20917;&#19979;&#30340;&#24212;&#29992;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2310.19683</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#30340;&#22312;&#32447;&#33258;&#21161;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Online Bootstrap for Time Series. (arXiv:2310.19683v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22312;&#32447;&#33258;&#21161;&#27861;&#29992;&#20110;&#22788;&#29702;&#22823;&#35268;&#27169;&#30340;&#26102;&#38388;&#24207;&#21015;&#21644;&#30456;&#20851;&#25968;&#25454;&#27969;&#65292;&#36890;&#36807;&#32771;&#34385;&#25968;&#25454;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#33258;&#21161;&#27861;&#22312;&#22797;&#26434;&#25968;&#25454;&#20381;&#36182;&#24773;&#20917;&#19979;&#30340;&#24212;&#29992;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#22788;&#29702;&#22823;&#35268;&#27169;&#30340;&#30456;&#20851;&#25968;&#25454;&#27969;&#65288;&#22914;&#26102;&#38388;&#24207;&#21015;&#25110;&#31354;&#38388;&#30456;&#20851;&#35266;&#27979;&#25968;&#25454;&#65289;&#26102;&#65292;&#20256;&#32479;&#30340;&#33258;&#21161;&#27861;&#21463;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#22312;&#32447;&#25191;&#34892;&#30340;&#26032;&#22411;&#33258;&#21161;&#27861;&#65292;&#19987;&#38376;&#29992;&#20110;&#32771;&#34385;&#25968;&#25454;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20351;&#20854;&#29305;&#21035;&#36866;&#29992;&#20110;&#23454;&#26102;&#24212;&#29992;&#12290;&#36825;&#31181;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#33258;&#22238;&#24402;&#24207;&#21015;&#65292;&#20854;&#20013;&#21253;&#21547;&#36234;&#26469;&#36234;&#30456;&#20851;&#30340;&#37325;&#37319;&#26679;&#26435;&#37325;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#19968;&#33324;&#26465;&#20214;&#19979;&#25552;&#20986;&#30340;&#33258;&#21161;&#27861;&#30340;&#29702;&#35770;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#27169;&#25311;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#26174;&#31034;&#23427;&#22312;&#22797;&#26434;&#25968;&#25454;&#20381;&#36182;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22635;&#34917;&#20102;&#20256;&#32479;&#37325;&#37319;&#26679;&#25216;&#26415;&#19982;&#29616;&#20195;&#25968;&#25454;&#20998;&#26512;&#38656;&#27714;&#20043;&#38388;&#30340;&#40511;&#27807;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Resampling methods such as the bootstrap have proven invaluable in the field of machine learning. However, the applicability of traditional bootstrap methods is limited when dealing with large streams of dependent data, such as time series or spatially correlated observations. In this paper, we propose a novel bootstrap method that is designed to account for data dependencies and can be executed online, making it particularly suitable for real-time applications. This method is based on an autoregressive sequence of increasingly dependent resampling weights. We prove the theoretical validity of the proposed bootstrap scheme under general conditions. We demonstrate the effectiveness of our approach through extensive simulations and show that it provides reliable uncertainty quantification even in the presence of complex data dependencies. Our work bridges the gap between classical resampling techniques and the demands of modern data analysis, providing a valuable tool for researchers and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#37096;&#20998;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36716;&#21270;&#20026;&#27169;&#25311;&#36153;&#26364;-&#21345;&#20811;&#27169;&#22411;&#30340;&#39640;&#25928;&#37319;&#26679;&#35757;&#32451;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#21508;&#31181;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2310.19608</link><description>&lt;p&gt;
&#35770;&#36153;&#26364;-&#21345;&#20811;&#35757;&#32451;&#37096;&#20998;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
On Feynman--Kac training of partial Bayesian neural networks. (arXiv:2310.19608v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#37096;&#20998;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36716;&#21270;&#20026;&#27169;&#25311;&#36153;&#26364;-&#21345;&#20811;&#27169;&#22411;&#30340;&#39640;&#25928;&#37319;&#26679;&#35757;&#32451;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#21508;&#31181;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#37096;&#20998;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;(pBNNs)&#34987;&#35777;&#26126;&#19982;&#20840;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#20294;pBNNs&#22312;&#28508;&#21464;&#37327;&#31354;&#38388;&#20013;&#24448;&#24448;&#26159;&#22810;&#23792;&#30340;&#65292;&#22240;&#27492;&#29992;&#21442;&#25968;&#27169;&#22411;&#26469;&#36817;&#20284;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#21363;&#23558;pBNN&#30340;&#35757;&#32451;&#36716;&#21270;&#20026;&#27169;&#25311;&#36153;&#26364;-&#21345;&#20811;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#24207;&#36143;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#22120;&#30340;&#21464;&#31181;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#20197;&#21487;&#34892;&#30340;&#35745;&#31639;&#25104;&#26412;&#21516;&#26102;&#20272;&#35745;&#21442;&#25968;&#21644;&#35813;&#27169;&#22411;&#30340;&#28508;&#22312;&#21518;&#39564;&#20998;&#24067;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#35757;&#32451;&#26041;&#26696;&#22312;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, partial Bayesian neural networks (pBNNs), which only consider a subset of the parameters to be stochastic, were shown to perform competitively with full Bayesian neural networks. However, pBNNs are often multi-modal in the latent-variable space and thus challenging to approximate with parametric models. To address this problem, we propose an efficient sampling-based training strategy, wherein the training of a pBNN is formulated as simulating a Feynman--Kac model. We then describe variations of sequential Monte Carlo samplers that allow us to simultaneously estimate the parameters and the latent posterior distribution of this model at a tractable computational cost. We show on various synthetic and real-world datasets that our proposed training scheme outperforms the state of the art in terms of predictive performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;Hessian-Vector&#20056;&#31215;&#31995;&#21015;&#20026;&#22522;&#30784;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#24179;&#26041;&#26681;&#21644;&#27714;&#36870;&#25805;&#20316;&#23454;&#29616;&#20102;&#39640;&#25928;&#21487;&#20280;&#32553;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#24182;&#30456;&#23545;&#20110;&#20854;&#20182;&#19968;&#38454;&#21644;&#20108;&#38454;&#20248;&#21270;&#26041;&#27861;&#22312;&#36816;&#34892;&#26102;&#38388;&#21644;&#24615;&#33021;&#19978;&#20855;&#26377;&#21487;&#27604;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.14901</link><description>&lt;p&gt;
&#21487;&#34892;&#30340;&#26080;&#38797;&#29275;&#39039;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;Hessian-Vector&#20056;&#31215;&#31995;&#21015;
&lt;/p&gt;
&lt;p&gt;
Series of Hessian-Vector Products for Tractable Saddle-Free Newton Optimisation of Neural Networks. (arXiv:2310.14901v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;Hessian-Vector&#20056;&#31215;&#31995;&#21015;&#20026;&#22522;&#30784;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#24179;&#26041;&#26681;&#21644;&#27714;&#36870;&#25805;&#20316;&#23454;&#29616;&#20102;&#39640;&#25928;&#21487;&#20280;&#32553;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#24182;&#30456;&#23545;&#20110;&#20854;&#20182;&#19968;&#38454;&#21644;&#20108;&#38454;&#20248;&#21270;&#26041;&#27861;&#22312;&#36816;&#34892;&#26102;&#38388;&#21644;&#24615;&#33021;&#19978;&#20855;&#26377;&#21487;&#27604;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25311;&#29275;&#39039;&#26041;&#27861;&#22312;&#36830;&#32493;&#20248;&#21270;&#39046;&#22495;&#38750;&#24120;&#21463;&#27426;&#36814;&#65292;&#20294;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24212;&#29992;&#20173;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;Hessian&#30697;&#38453;&#30340;&#35268;&#27169;&#36807;&#22823;&#12290;&#36890;&#36807;&#20462;&#25913;Hessian&#30340;&#29305;&#24449;&#20540;&#26469;&#22788;&#29702;&#38750;&#20984;&#24615;&#65292;&#22914;&#26080;&#38797;&#29275;&#39039;&#26041;&#27861;&#65292;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#35745;&#31639;&#36127;&#25285;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#30340;&#20248;&#21270;&#31639;&#27861;-&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#20010;&#21487;&#20197;&#28176;&#36817;&#22320;&#20351;&#29992;&#31934;&#30830;&#65288;&#29305;&#24449;&#20540;&#20462;&#25913;&#21518;&#30340;&#65289;&#36870;Hessian&#30340;&#39640;&#25928;&#21487;&#20280;&#32553;&#20248;&#21270;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#38382;&#39064;&#34920;&#36848;&#20026;&#19968;&#20010;&#20027;&#35201;&#23545;Hessian&#36827;&#34892;&#24179;&#26041;&#26681;&#21644;&#27714;&#36870;&#30340;&#32423;&#25968;&#65292;&#28982;&#21518;&#29992;&#23427;&#26469;&#39044;&#22788;&#29702;&#26799;&#24230;&#21521;&#37327;&#65292;&#32780;&#26080;&#38656;&#26174;&#24335;&#35745;&#31639;&#25110;&#36827;&#34892;&#29305;&#24449;&#20998;&#35299;&#12290;&#23545;&#35813;&#26080;&#38480;&#32423;&#25968;&#30340;&#25130;&#26029;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#21487;&#20280;&#32553;&#20248;&#21270;&#31639;&#27861;&#65292;&#20854;&#36816;&#34892;&#26102;&#38388;&#21644;&#20248;&#21270;&#24615;&#33021;&#19982;&#20854;&#20182;&#19968;&#38454;&#21644;&#20108;&#38454;&#20248;&#21270;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their popularity in the field of continuous optimisation, second-order quasi-Newton methods are challenging to apply in machine learning, as the Hessian matrix is intractably large. This computational burden is exacerbated by the need to address non-convexity, for instance by modifying the Hessian's eigenvalues as in Saddle-Free Newton methods. We propose an optimisation algorithm which addresses both of these concerns - to our knowledge, the first efficiently-scalable optimisation algorithm to asymptotically use the exact (eigenvalue-modified) inverse Hessian. Our method frames the problem as a series which principally square-roots and inverts the squared Hessian, then uses it to precondition a gradient vector, all without explicitly computing or eigendecomposing the Hessian. A truncation of this infinite series provides a new optimisation algorithm which is scalable and comparable to other first- and second-order optimisation methods in both runtime and optimisation performan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#20219;&#21153;&#20013;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#12290;&#36890;&#36807;&#30740;&#31350;Fr\'echet inception&#36317;&#31163;&#65288;FID&#65289;&#65292;&#24182;&#32771;&#34385;&#19981;&#21516;&#32858;&#21512;&#20998;&#25968;&#65292;&#21457;&#29616;FID-all&#21644;FID-avg&#20998;&#25968;&#30340;&#27169;&#22411;&#25490;&#21517;&#21487;&#33021;&#19981;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2310.11714</link><description>&lt;p&gt;
&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20219;&#21153;&#20013;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
On the Evaluation of Generative Models in Distributed Learning Tasks. (arXiv:2310.11714v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#20219;&#21153;&#20013;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#12290;&#36890;&#36807;&#30740;&#31350;Fr\'echet inception&#36317;&#31163;&#65288;FID&#65289;&#65292;&#24182;&#32771;&#34385;&#19981;&#21516;&#32858;&#21512;&#20998;&#25968;&#65292;&#21457;&#29616;FID-all&#21644;FID-avg&#20998;&#25968;&#30340;&#27169;&#22411;&#25490;&#21517;&#21487;&#33021;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#29486;&#20013;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#20102;&#23545;&#21253;&#25324;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21644;&#25193;&#25955;&#27169;&#22411;&#22312;&#20869;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#20027;&#35201;&#38024;&#23545;&#21333;&#20010;&#23458;&#25143;&#31471;&#23384;&#20648;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#38598;&#20013;&#24335;&#23398;&#20064;&#38382;&#39064;&#65292;&#32780;&#29983;&#25104;&#27169;&#22411;&#30340;&#35768;&#22810;&#24212;&#29992;&#28041;&#21450;&#21040;&#20998;&#24067;&#24335;&#23398;&#20064;&#29615;&#22659;&#65292;&#20363;&#22914;&#32852;&#37030;&#23398;&#20064;&#22330;&#26223;&#65292;&#20854;&#20013;&#35757;&#32451;&#25968;&#25454;&#30001;&#22810;&#20010;&#23458;&#25143;&#31471;&#25910;&#38598;&#24182;&#20998;&#21457;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#20219;&#21153;&#20013;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20851;&#27880;Fr\'echet inception&#36317;&#31163;&#65288;FID&#65289;&#65292;&#24182;&#32771;&#34385;&#20197;&#19979;&#22522;&#20110;FID&#30340;&#32858;&#21512;&#20998;&#25968;&#65306;1&#65289;FID-avg&#20316;&#20026;&#23458;&#25143;&#31471;&#20010;&#20307;FID&#20998;&#25968;&#30340;&#24179;&#22343;&#20540;&#65292;2&#65289;FID-all&#20316;&#20026;&#35757;&#32451;&#27169;&#22411;&#19982;&#21253;&#21547;&#25152;&#26377;&#23458;&#25143;&#31471;&#25968;&#25454;&#30340;&#38598;&#20307;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;FID&#36317;&#31163;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26681;&#25454;FID-all&#21644;FID-avg&#20998;&#25968;&#30340;&#27169;&#22411;&#25490;&#21517;&#21487;&#33021;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evaluation of deep generative models including generative adversarial networks (GANs) and diffusion models has been extensively studied in the literature. While the existing evaluation methods mainly target a centralized learning problem with training data stored by a single client, many applications of generative models concern distributed learning settings, e.g. the federated learning scenario, where training data are collected by and distributed among several clients. In this paper, we study the evaluation of generative models in distributed learning tasks with heterogeneous data distributions. First, we focus on the Fr\'echet inception distance (FID) and consider the following FID-based aggregate scores over the clients: 1) FID-avg as the mean of clients' individual FID scores, 2) FID-all as the FID distance of the trained model to the collective dataset containing all clients' data. We prove that the model rankings according to the FID-all and FID-avg scores could be inconsist
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#28145;&#24230;&#32593;&#32476;&#20013;&#20351;&#29992;&#20154;&#31867;&#20559;&#22909;&#36827;&#34892;&#38750;&#21442;&#25968;&#31163;&#31574;&#30053;&#35780;&#20272;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#24182;&#24314;&#31435;&#20102;&#32479;&#35745;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.10556</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#38750;&#21442;&#25968;&#31163;&#31574;&#30053;&#35780;&#20272;&#22312;&#28145;&#24230;&#32593;&#32476;&#20013;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Sample Complexity of Preference-Based Nonparametric Off-Policy Evaluation with Deep Networks. (arXiv:2310.10556v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#28145;&#24230;&#32593;&#32476;&#20013;&#20351;&#29992;&#20154;&#31867;&#20559;&#22909;&#36827;&#34892;&#38750;&#21442;&#25968;&#31163;&#31574;&#30053;&#35780;&#20272;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#24182;&#24314;&#31435;&#20102;&#32479;&#35745;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#27969;&#34892;&#30340;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#12290;&#20107;&#23454;&#19978;&#65292;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#29616;&#22312;&#19982;&#32463;&#20856;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;&#22914;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#65289;&#19968;&#36215;&#20351;&#29992;&#65292;&#22312;&#20174;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#22870;&#21169;&#19978;&#35780;&#20272;&#20013;&#38388;&#31574;&#30053;&#65292;&#21363;&#31163;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#12290;&#35813;&#31639;&#27861;&#21253;&#25324;&#65288;i&#65289;&#20174;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#23398;&#20064;&#30446;&#26631;&#31574;&#30053;&#30340;&#32047;&#31215;&#22870;&#21169;&#12290;&#23613;&#31649;&#26377;&#24040;&#22823;&#30340;&#32463;&#39564;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;&#20351;&#29992;&#20559;&#22909;&#25968;&#25454;&#30340;OPE&#26041;&#27861;&#36890;&#24120;&#32570;&#20047;&#29702;&#35770;&#29702;&#35299;&#65292;&#24182;&#19988;&#20005;&#37325;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;OPE&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#20026;&#20854;&#24314;&#31435;&#20102;&#32479;&#35745;&#20445;&#35777;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25311;&#21512;Q&#35780;&#20272;&#26469;&#22788;&#29702;OPE&#12290;&#36890;&#36807;&#36866;&#24403;&#36873;&#25321;ReLU&#32593;&#32476;&#30340;&#22823;&#23567;&#65292;&#25105;&#20204;&#34920;&#26126;&#21487;&#20197;&#21033;&#29992;&#20219;&#20309;lo
&lt;/p&gt;
&lt;p&gt;
A recently popular approach to solving reinforcement learning is with data from human preferences. In fact, human preference data are now used with classic reinforcement learning algorithms such as actor-critic methods, which involve evaluating an intermediate policy over a reward learned from human preference data with distribution shift, known as off-policy evaluation (OPE). Such algorithm includes (i) learning reward function from human preference dataset, and (ii) learning expected cumulative reward of a target policy. Despite the huge empirical success, existing OPE methods with preference data often lack theoretical understanding and rely heavily on heuristics. In this paper, we study the sample efficiency of OPE with human preference and establish a statistical guarantee for it. Specifically, we approach OPE by learning the value function by fitted-Q-evaluation with a deep neural network. By appropriately selecting the size of a ReLU network, we show that one can leverage any lo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#29616;&#20195;&#22270;&#20687;&#21644;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#24230;&#37327;&#26041;&#27861;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#21457;&#29616;&#37096;&#20998;&#24230;&#37327;&#26041;&#27861;&#23545;&#23545;&#25239;&#25915;&#20987;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#25269;&#25239;&#21147;&#65292;&#20026;&#22522;&#20934;&#27979;&#35797;&#25552;&#20379;&#20102;&#26356;&#23433;&#20840;&#30340;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2310.06958</link><description>&lt;p&gt;
&#27604;&#36739;&#29616;&#20195;&#26080;&#21442;&#32771;&#22270;&#20687;&#21644;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#24230;&#37327;&#26041;&#27861;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Comparing the robustness of modern no-reference image- and video-quality metrics to adversarial attacks. (arXiv:2310.06958v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#29616;&#20195;&#22270;&#20687;&#21644;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#24230;&#37327;&#26041;&#27861;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#21457;&#29616;&#37096;&#20998;&#24230;&#37327;&#26041;&#27861;&#23545;&#23545;&#25239;&#25915;&#20987;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#25269;&#25239;&#21147;&#65292;&#20026;&#22522;&#20934;&#27979;&#35797;&#25552;&#20379;&#20102;&#26356;&#23433;&#20840;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#20687;&#21644;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#24230;&#37327;&#26041;&#27861;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20063;&#21464;&#24471;&#26356;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#36825;&#20123;&#25915;&#20987;&#21487;&#20197;&#25552;&#39640;&#24230;&#37327;&#20998;&#25968;&#20294;&#19981;&#25913;&#21892;&#35270;&#35273;&#36136;&#37327;&#12290;&#29616;&#26377;&#30340;&#36136;&#37327;&#24230;&#37327;&#22522;&#20934;&#23558;&#20854;&#24615;&#33021;&#19982;&#20027;&#35266;&#36136;&#37327;&#30456;&#20851;&#24615;&#21644;&#35745;&#31639;&#26102;&#38388;&#36827;&#34892;&#27604;&#36739;&#12290;&#28982;&#32780;&#65292;&#22270;&#20687;&#36136;&#37327;&#24230;&#37327;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#20063;&#26159;&#19968;&#20010;&#20540;&#24471;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#29616;&#20195;&#24230;&#37327;&#26041;&#27861;&#23545;&#19981;&#21516;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#24182;&#27604;&#36739;&#20102;&#36825;&#20123;&#25915;&#20987;&#23545;15&#20010;&#26080;&#21442;&#32771;&#22270;&#20687;/&#35270;&#39057;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#19968;&#20123;&#24230;&#37327;&#26041;&#27861;&#23545;&#23545;&#25239;&#25915;&#20987;&#34920;&#29616;&#20986;&#20102;&#36739;&#39640;&#30340;&#25269;&#25239;&#21147;&#65292;&#20351;&#23427;&#20204;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#20351;&#29992;&#27604;&#23481;&#26131;&#21463;&#25915;&#20987;&#30340;&#26041;&#27861;&#26356;&#23433;&#20840;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#25509;&#21463;&#30740;&#31350;&#20154;&#21592;&#25552;&#20132;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#20197;&#20351;&#20182;&#20204;&#30340;&#26041;&#27861;&#23545;&#25915;&#20987;&#26356;&#21152;&#40065;&#26834;&#65292;&#25110;&#32773;&#20026;&#20182;&#20204;&#23547;&#25214;&#31526;&#21512;&#38656;&#27714;&#30340;&#40065;&#26834;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays neural-network-based image- and video-quality metrics show better performance compared to traditional methods. However, they also became more vulnerable to adversarial attacks that increase metrics' scores without improving visual quality. The existing benchmarks of quality metrics compare their performance in terms of correlation with subjective quality and calculation time. However, the adversarial robustness of image-quality metrics is also an area worth researching. In this paper, we analyse modern metrics' robustness to different adversarial attacks. We adopted adversarial attacks from computer vision tasks and compared attacks' efficiency against 15 no-reference image/video-quality metrics. Some metrics showed high resistance to adversarial attacks which makes their usage in benchmarks safer than vulnerable metrics. The benchmark accepts new metrics submissions for researchers who want to make their metrics more robust to attacks or to find such metrics for their needs. 
&lt;/p&gt;</description></item><item><title>NECO&#26159;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#22349;&#22604;&#30340;&#26032;&#39062;&#30340;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20960;&#20309;&#23646;&#24615;&#21644;&#20027;&#25104;&#20998;&#31354;&#38388;&#35782;&#21035;OOD&#25968;&#25454;&#65292;&#22312;&#23567;&#35268;&#27169;&#21644;&#22823;&#35268;&#27169;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.06823</link><description>&lt;p&gt;
NECO: &#22522;&#20110;&#31070;&#32463;&#22349;&#22604;&#30340;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
NECO: NEural Collapse Based Out-of-distribution detection. (arXiv:2310.06823v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06823
&lt;/p&gt;
&lt;p&gt;
NECO&#26159;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#22349;&#22604;&#30340;&#26032;&#39062;&#30340;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20960;&#20309;&#23646;&#24615;&#21644;&#20027;&#25104;&#20998;&#31354;&#38388;&#35782;&#21035;OOD&#25968;&#25454;&#65292;&#22312;&#23567;&#35268;&#27169;&#21644;&#22823;&#35268;&#27169;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#27169;&#22411;&#36807;&#20110;&#33258;&#20449;&#24182;&#19988;&#27809;&#26377;&#24847;&#35782;&#21040;&#20854;&#35748;&#35782;&#35770;&#38480;&#21046;&#65292;&#26816;&#27979;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#25968;&#25454;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#25105;&#20204;&#20551;&#35774;&#8220;&#31070;&#32463;&#22349;&#22604;&#8221;&#65292;&#19968;&#31181;&#24433;&#21709;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#30340;&#29616;&#35937;&#65292;&#20063;&#20250;&#24433;&#21709;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#12290;&#20026;&#20102;&#20174;&#36825;&#31181;&#30456;&#20114;&#20316;&#29992;&#20013;&#21463;&#30410;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;NECO&#65292;&#19968;&#31181;&#29992;&#20110;OOD&#26816;&#27979;&#30340;&#26032;&#39062;&#30340;&#20107;&#21518;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#8220;&#31070;&#32463;&#22349;&#22604;&#8221;&#21644;&#20027;&#25104;&#20998;&#31354;&#38388;&#30340;&#20960;&#20309;&#23646;&#24615;&#26469;&#35782;&#21035;OOD&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;NECO&#22312;&#23567;&#35268;&#27169;&#21644;&#22823;&#35268;&#27169;OOD&#26816;&#27979;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#22312;&#19981;&#21516;&#30340;&#32593;&#32476;&#26550;&#26500;&#19978;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;OOD&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#12290;&#25105;&#20204;&#35745;&#21010;&#22312;&#21311;&#21517;&#26399;&#32467;&#26463;&#21518;&#21457;&#24067;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting out-of-distribution (OOD) data is a critical challenge in machine learning due to model overconfidence, often without awareness of their epistemological limits. We hypothesize that ``neural collapse'', a phenomenon affecting in-distribution data for models trained beyond loss convergence, also influences OOD data. To benefit from this interplay, we introduce NECO, a novel post-hoc method for OOD detection, which leverages the geometric properties of ``neural collapse'' and of principal component spaces to identify OOD data. Our extensive experiments demonstrate that NECO achieves state-of-the-art results on both small and large-scale OOD detection tasks while exhibiting strong generalization capabilities across different network architectures. Furthermore, we provide a theoretical explanation for the effectiveness of our method in OOD detection. We plan to release the code after the anonymity period.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35299;&#37322;&#20854;&#20182;&#39044;&#27979;&#27169;&#22411;&#26377;&#25928;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#22810;&#20010;&#25552;&#31034;&#31574;&#30053;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;&#23545;&#20110;LLMs&#22312;&#35299;&#37322;&#20854;&#20182;&#27169;&#22411;&#34892;&#20026;&#26041;&#38754;&#30340;&#32570;&#22833;&#12290;</title><link>http://arxiv.org/abs/2310.05797</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#20107;&#21518;&#35299;&#37322;&#22120;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Models Post Hoc Explainers?. (arXiv:2310.05797v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05797
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35299;&#37322;&#20854;&#20182;&#39044;&#27979;&#27169;&#22411;&#26377;&#25928;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#22810;&#20010;&#25552;&#31034;&#31574;&#30053;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;&#23545;&#20110;LLMs&#22312;&#35299;&#37322;&#20854;&#20182;&#27169;&#22411;&#34892;&#20026;&#26041;&#38754;&#30340;&#32570;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36234;&#26469;&#36234;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24212;&#29992;&#20013;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#21019;&#26032;&#65292;&#21363;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#20351;&#24471;LLM&#33021;&#22815;&#22312;&#25512;&#29702;&#38454;&#27573;&#36890;&#36807;&#22312;&#25552;&#31034;&#20013;&#25552;&#20379;&#23569;&#37327;&#31034;&#20363;&#26469;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#27169;&#22411;&#24494;&#35843;&#30340;&#38656;&#35201;&#12290;&#34429;&#28982;LLM&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#65292;&#20294;&#20854;&#22312;&#35299;&#37322;&#20854;&#20182;&#27169;&#22411;&#34892;&#20026;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#20173;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#23613;&#31649;&#23384;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#26032;&#35299;&#37322;&#25216;&#26415;&#65292;&#20294;&#24456;&#22810;&#25216;&#26415;&#35201;&#27714;&#23545;&#27169;&#22411;&#20855;&#26377;&#30333;&#30418;&#35775;&#38382;&#26435;&#38480;&#21644;/&#25110;&#35745;&#31639;&#25104;&#26412;&#36739;&#39640;&#65292;&#20984;&#26174;&#20102;&#19979;&#19968;&#20195;&#20107;&#21518;&#35299;&#37322;&#22120;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#30740;&#31350;LLM&#35299;&#37322;&#20854;&#20182;&#39044;&#27979;&#27169;&#22411;&#26377;&#25928;&#24615;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#22810;&#31181;&#25552;&#31034;&#31574;&#30053;&#30340;&#26032;&#39062;&#26694;&#26550;&#65306;i&#65289;&#22522;&#20110;&#25200;&#21160;&#30340;ICL&#65292;ii&#65289;&#22522;&#20110;&#39044;&#27979;&#30340;ICL&#65292;iii&#65289;&#22522;&#20110;&#25351;&#20196;&#30340;ICL&#65292;&#21644;iv&#65289;&#22522;&#20110;&#35299;&#37322;&#30340;ICL&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly used as powerful tools for a plethora of natural language processing (NLP) applications. A recent innovation, in-context learning (ICL), enables LLMs to learn new tasks by supplying a few examples in the prompt during inference time, thereby eliminating the need for model fine-tuning. While LLMs have been utilized in several applications, their applicability in explaining the behavior of other models remains relatively unexplored. Despite the growing number of new explanation techniques, many require white-box access to the model and/or are computationally expensive, highlighting a need for next-generation post hoc explainers. In this work, we present the first framework to study the effectiveness of LLMs in explaining other predictive models. More specifically, we propose a novel framework encompassing multiple prompting strategies: i) Perturbation-based ICL, ii) Prediction-based ICL, iii) Instruction-based ICL, and iv) Explanation-based I
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#20165;&#35299;&#30721;&#22120;&#20391;&#20449;&#24687;&#30340;&#20998;&#24067;&#24335;&#28145;&#24230;&#32852;&#21512;&#28304;&#20449;&#36947;&#32534;&#30721;&#26041;&#27861;&#65292;&#22312;&#20302;&#24310;&#36831;&#22270;&#20687;&#20256;&#36755;&#20013;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#20302;&#20449;&#36947;&#20449;&#22122;&#27604;&#21644;&#23567;&#24102;&#23485;&#27604;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/2310.04311</link><description>&lt;p&gt;
&#24102;&#26377;&#20165;&#35299;&#30721;&#22120;&#20391;&#20449;&#24687;&#30340;&#20998;&#24067;&#24335;&#28145;&#24230;&#32852;&#21512;&#28304;&#20449;&#36947;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Distributed Deep Joint Source-Channel Coding with Decoder-Only Side Information. (arXiv:2310.04311v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#20165;&#35299;&#30721;&#22120;&#20391;&#20449;&#24687;&#30340;&#20998;&#24067;&#24335;&#28145;&#24230;&#32852;&#21512;&#28304;&#20449;&#36947;&#32534;&#30721;&#26041;&#27861;&#65292;&#22312;&#20302;&#24310;&#36831;&#22270;&#20687;&#20256;&#36755;&#20013;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#20302;&#20449;&#36947;&#20449;&#22122;&#27604;&#21644;&#23567;&#24102;&#23485;&#27604;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#21482;&#26377;&#25509;&#25910;&#26041;&#26377;&#30456;&#20851;&#36741;&#21161;&#20449;&#24687;&#30340;&#22122;&#22768;&#26080;&#32447;&#20449;&#36947;&#19978;&#36827;&#34892;&#20302;&#24310;&#36831;&#22270;&#20687;&#20256;&#36755;&#65288;Wyner-Ziv&#24773;&#26223;&#65289;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#32852;&#21512;&#28304;&#20449;&#36947;&#32534;&#30721;&#65288;JSCC&#65289;&#26041;&#27861;&#24320;&#21457;&#23454;&#38469;&#26041;&#26696;&#65292;&#36825;&#22312;&#23454;&#38469;&#26377;&#38480;&#22359;&#38271;&#24230;&#30340;&#24773;&#20917;&#19979;&#24050;&#34987;&#35777;&#26126;&#20248;&#20110;&#20256;&#32479;&#30340;&#20998;&#31163;&#24335;&#26041;&#27861;&#65292;&#24182;&#22312;&#20449;&#36947;&#36136;&#37327;&#26041;&#38754;&#25552;&#20379;&#20102;&#20248;&#38597;&#30340;&#36864;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#22312;&#25509;&#25910;&#22120;&#31471;&#30340;&#22810;&#20010;&#38454;&#27573;&#23558;&#20165;&#35299;&#30721;&#22120;&#20391;&#20449;&#24687;&#34701;&#20837;&#20854;&#20013;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#25972;&#21512;&#20102;&#36741;&#21161;&#20449;&#24687;&#65292;&#22312;&#21508;&#31181;&#30072;&#21464;&#20934;&#21017;&#19979;&#65292;&#22312;&#25152;&#26377;&#20449;&#36947;&#22122;&#22768;&#27700;&#24179;&#19978;&#37117;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#20302;&#20449;&#36947;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#21644;&#23567;&#24102;&#23485;&#27604;&#65288;BR&#65289;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#28304;&#20195;&#30721;&#65292;&#20197;&#20415;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider low-latency image transmission over a noisy wireless channel when correlated side information is present only at the receiver side (the Wyner-Ziv scenario). In particular, we are interested in developing practical schemes using a data-driven joint source-channel coding (JSCC) approach, which has been previously shown to outperform conventional separation-based approaches in the practical finite blocklength regimes, and to provide graceful degradation with channel quality. We propose a novel neural network architecture that incorporates the decoder-only side information at multiple stages at the receiver side. Our results demonstrate that the proposed method succeeds in integrating the side information, yielding improved performance at all channel noise levels in terms of the various distortion criteria considered here, especially at low channel signal-to-noise ratios (SNRs) and small bandwidth ratios (BRs). We also provide the source code of the proposed method to enable fu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;&#31227;&#21160;&#30446;&#26631;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;iMOS&#65292;&#36890;&#36807;&#23545;&#24207;&#21015;&#20013;&#21482;&#26377;&#23569;&#37327;&#22270;&#20687;&#36827;&#34892;&#27880;&#37322;&#65292;&#21363;&#21487;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#20998;&#21106;&#25928;&#26524;</title><link>http://arxiv.org/abs/2309.17264</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;&#19968;&#33324;&#31227;&#21160;&#30446;&#26631;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Foundation Model for General Moving Object Segmentation in Medical Images. (arXiv:2309.17264v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;&#31227;&#21160;&#30446;&#26631;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;iMOS&#65292;&#36890;&#36807;&#23545;&#24207;&#21015;&#20013;&#21482;&#26377;&#23569;&#37327;&#22270;&#20687;&#36827;&#34892;&#27880;&#37322;&#65292;&#21363;&#21487;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#20998;&#21106;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26088;&#22312;&#25551;&#32472;&#24863;&#20852;&#36259;&#30340;&#35299;&#21078;&#25110;&#30149;&#29702;&#32467;&#26500;&#65292;&#22312;&#20020;&#24202;&#35786;&#26029;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26500;&#24314;&#39640;&#31934;&#24230;&#30340;&#28145;&#24230;&#20998;&#21106;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#39640;&#36136;&#37327;&#30340;&#27880;&#37322;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#21307;&#23398;&#27880;&#37322;&#38750;&#24120;&#32321;&#29712;&#32791;&#26102;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21307;&#23398;&#35270;&#39057;&#25110;3D&#20307;&#31215;&#65292;&#30001;&#20110;&#24040;&#22823;&#30340;&#26631;&#31614;&#31354;&#38388;&#21644;&#24046;&#30340;&#24103;&#38388;&#19968;&#33268;&#24615;&#12290;&#26368;&#36817;&#65292;&#22312;&#33258;&#28982;&#22270;&#20687;&#20013;&#65292;&#19968;&#20010;&#21517;&#20026;Moving Object Segmentation (MOS)&#30340;&#22522;&#26412;&#20219;&#21153;&#22312;&#25216;&#26415;&#19978;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#23427;&#30340;&#30446;&#26631;&#26159;&#22312;&#22270;&#20687;&#24207;&#21015;&#20013;&#20174;&#32972;&#26223;&#20013;&#25551;&#32472;&#31227;&#21160;&#29289;&#20307;&#65292;&#21482;&#38656;&#35201;&#26368;&#23567;&#30340;&#27880;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;MOS&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21517;&#20026;iMOS&#12290;&#23545;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;iMOS&#30340;&#26377;&#25928;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21482;&#38656;&#23545;&#24207;&#21015;&#20013;&#23569;&#37327;&#30340;&#22270;&#20687;&#36827;&#34892;&#27880;&#37322;&#65292;iMOS&#23601;&#21487;&#20197;&#23454;&#29616;&#20102;
&lt;/p&gt;
&lt;p&gt;
Medical image segmentation aims to delineate the anatomical or pathological structures of interest, playing a crucial role in clinical diagnosis. A substantial amount of high-quality annotated data is crucial for constructing high-precision deep segmentation models. However, medical annotation is highly cumbersome and time-consuming, especially for medical videos or 3D volumes, due to the huge labeling space and poor inter-frame consistency. Recently, a fundamental task named Moving Object Segmentation (MOS) has made significant advancements in natural images. Its objective is to delineate moving objects from the background within image sequences, requiring only minimal annotations. In this paper, we propose the first foundation model, named iMOS, for MOS in medical images. Extensive experiments on a large multi-modal medical dataset validate the effectiveness of the proposed iMOS. Specifically, with the annotation of only a small number of images in the sequence, iMOS can achieve sati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#22270;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;LLM&#21487;&#20197;&#20174;&#32467;&#26500;&#20449;&#24687;&#20013;&#21463;&#30410;&#65292;&#23588;&#20854;&#26159;&#22312;&#25991;&#26412;&#33410;&#28857;&#29305;&#24449;&#32570;&#20047;&#30340;&#24773;&#20917;&#19979;&#65292;&#32780;LLM&#30340;&#24615;&#33021;&#19982;&#25968;&#25454;&#27844;&#38706;&#27809;&#26377;&#26174;&#33879;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2309.16595</link><description>&lt;p&gt;
LLM&#33021;&#21542;&#26377;&#25928;&#21033;&#29992;&#32467;&#26500;&#20449;&#24687;&#36827;&#34892;&#22270;&#23398;&#20064;&#65306;&#20309;&#26102;&#20309;&#22320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Effectively Leverage Structural Information for Graph Learning: When and Why. (arXiv:2309.16595v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#22270;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;LLM&#21487;&#20197;&#20174;&#32467;&#26500;&#20449;&#24687;&#20013;&#21463;&#30410;&#65292;&#23588;&#20854;&#26159;&#22312;&#25991;&#26412;&#33410;&#28857;&#29305;&#24449;&#32570;&#20047;&#30340;&#24773;&#20917;&#19979;&#65292;&#32780;LLM&#30340;&#24615;&#33021;&#19982;&#25968;&#25454;&#27844;&#38706;&#27809;&#26377;&#26174;&#33879;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#29305;&#21035;&#26159;&#22270;&#25968;&#25454;&#65289;&#19978;&#30340;&#24212;&#29992;&#65292;&#36825;&#26159;LLM&#25991;&#29486;&#20013;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#30340;&#37325;&#35201;&#25968;&#25454;&#24418;&#24577;&#12290;&#25105;&#20204;&#26088;&#22312;&#20102;&#35299;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#20309;&#26102;&#20309;&#22320;&#24341;&#20837;&#22270;&#25968;&#25454;&#20013;&#30340;&#32467;&#26500;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;LLM&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#8220;&#20309;&#26102;&#8221;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#31181;&#32534;&#30721;&#32467;&#26500;&#20449;&#24687;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#35774;&#32622;&#20013;&#25991;&#26412;&#33410;&#28857;&#29305;&#24449;&#20016;&#23500;&#25110;&#31232;&#32570;&#12290;&#23545;&#20110;&#8220;&#20026;&#20160;&#20040;&#8221;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;LLM&#24615;&#33021;&#30340;&#20004;&#20010;&#28508;&#22312;&#22240;&#32032;&#65306;&#25968;&#25454;&#27844;&#38706;&#21644;&#21516;&#36136;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;i&#65289;LLM&#21487;&#20197;&#20174;&#32467;&#26500;&#20449;&#24687;&#20013;&#21463;&#30410;&#65292;&#23588;&#20854;&#26159;&#22312;&#25991;&#26412;&#33410;&#28857;&#29305;&#24449;&#32570;&#20047;&#30340;&#24773;&#20917;&#19979;&#65307;&#65288;ii&#65289;&#27809;&#26377;&#23454;&#36136;&#24615;&#30340;&#35777;&#25454;&#34920;&#26126;LLM&#24615;&#33021;&#19982;&#25968;&#25454;&#27844;&#38706;&#26377;&#26174;&#33879;&#30456;&#20851;&#65307;&#65288;iii&#65289;LLM&#22312;&#30446;&#26631;&#33410;&#28857;&#19978;&#30340;&#24615;&#33021;&#19982;&#27491;&#21521;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies Large Language Models (LLMs) for structured data--particularly graphs--a crucial data modality that remains underexplored in the LLM literature. We aim to understand when and why the incorporation of structural information inherent in graph data can improve the prediction performance of LLMs on node classification tasks. To address the ``when'' question, we examine a variety of prompting methods for encoding structural information, in settings where textual node features are either rich or scarce. For the ``why'' questions, we probe into two potential contributing factors to the LLM performance: data leakage and homophily. Our exploration of these questions reveals that (i) LLMs can benefit from structural information, especially when textual node features are scarce; (ii) there is no substantial evidence indicating that the performance of LLMs is significantly attributed to data leakage; and (iii) the performance of LLMs on a target node is strongly positively relat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#38543;&#26426;&#21160;&#24577;&#35268;&#21010;&#26041;&#31243;&#65292;&#29983;&#25104;&#30340;&#25511;&#21046;&#22120;&#33021;&#22815;&#20027;&#21160;&#23398;&#20064;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#30830;&#20445;&#23433;&#20840;&#24615;&#21644;&#23454;&#26102;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2309.10831</link><description>&lt;p&gt;
&#27963;&#21160;&#23398;&#20064;&#24378;&#21270;&#23398;&#20064;&#65306;&#19968;&#31181;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#26041;&#27861;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Actively Learning Reinforcement Learning: A Stochastic Optimal Control Approach. (arXiv:2309.10831v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#38543;&#26426;&#21160;&#24577;&#35268;&#21010;&#26041;&#31243;&#65292;&#29983;&#25104;&#30340;&#25511;&#21046;&#22120;&#33021;&#22815;&#20027;&#21160;&#23398;&#20064;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#30830;&#20445;&#23433;&#20840;&#24615;&#21644;&#23454;&#26102;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#24212;&#23545;&#20004;&#20010;&#38382;&#39064;&#65306;&#65288;i&#65289;&#24378;&#21270;&#23398;&#20064;&#22312;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#30340;&#33030;&#24369;&#24615;&#65292;&#22240;&#20026;&#21463;&#25511;&#23454;&#39564;&#23460;/&#20223;&#30495;&#21644;&#23454;&#38469;&#26465;&#20214;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#30340;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#38543;&#26426;&#21160;&#24577;&#35268;&#21010;&#26041;&#31243;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#22120;&#23545;&#20110;&#20960;&#31181;&#31867;&#22411;&#30340;&#32422;&#26463;&#26465;&#20214;&#26159;&#23433;&#20840;&#30340;&#65292;&#24182;&#19988;&#23427;&#21487;&#20197;&#20027;&#21160;&#23398;&#20064;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;&#19982;&#25506;&#32034;&#21644;&#21033;&#29992;&#19981;&#21516;&#65292;&#25506;&#27979;&#21644;&#23433;&#20840;&#24615;&#30001;&#25511;&#21046;&#22120;&#33258;&#36523;&#33258;&#21160;&#23454;&#29616;&#65292;&#23454;&#29616;&#20102;&#23454;&#26102;&#23398;&#20064;&#12290;&#19968;&#20010;&#20223;&#30495;&#31034;&#20363;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we provide framework to cope with two problems: (i) the fragility of reinforcement learning due to modeling uncertainties because of the mismatch between controlled laboratory/simulation and real-world conditions and (ii) the prohibitive computational cost of stochastic optimal control. We approach both problems by using reinforcement learning to solve the stochastic dynamic programming equation. The resulting reinforcement learning controller is safe with respect to several types of constraints constraints and it can actively learn about the modeling uncertainties. Unlike exploration and exploitation, probing and safety are employed automatically by the controller itself, resulting real-time learning. A simulation example demonstrates the efficacy of the proposed approach.
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#31181;&#23616;&#37096;&#24179;&#31283;&#22270;&#24418;&#36807;&#31243;&#27169;&#22411;&#65292;&#26088;&#22312;&#23558;&#23616;&#37096;&#24179;&#31283;&#27010;&#24565;&#25193;&#23637;&#21040;&#19981;&#35268;&#21017;&#30340;&#22270;&#22495;&#19978;&#12290;&#23427;&#36890;&#36807;&#23558;&#25972;&#20010;&#36807;&#31243;&#34920;&#31034;&#20026;&#19968;&#32452;&#32452;&#25104;&#37096;&#20998;&#36807;&#31243;&#30340;&#32452;&#21512;&#26469;&#34920;&#24449;&#23616;&#37096;&#24179;&#31283;&#24615;&#65292;&#20197;&#20351;&#36807;&#31243;&#22312;&#22270;&#19978;&#25353;&#29031;&#27599;&#20010;&#32452;&#25104;&#37096;&#20998;&#30340;&#35201;&#27714;&#21464;&#21270;&#24471;&#26356;&#21152;&#24179;&#28369;&#12290;</title><link>http://arxiv.org/abs/2309.01657</link><description>&lt;p&gt;
&#23616;&#37096;&#24179;&#31283;&#22270;&#24418;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Locally Stationary Graph Processes. (arXiv:2309.01657v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01657
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#31181;&#23616;&#37096;&#24179;&#31283;&#22270;&#24418;&#36807;&#31243;&#27169;&#22411;&#65292;&#26088;&#22312;&#23558;&#23616;&#37096;&#24179;&#31283;&#27010;&#24565;&#25193;&#23637;&#21040;&#19981;&#35268;&#21017;&#30340;&#22270;&#22495;&#19978;&#12290;&#23427;&#36890;&#36807;&#23558;&#25972;&#20010;&#36807;&#31243;&#34920;&#31034;&#20026;&#19968;&#32452;&#32452;&#25104;&#37096;&#20998;&#36807;&#31243;&#30340;&#32452;&#21512;&#26469;&#34920;&#24449;&#23616;&#37096;&#24179;&#31283;&#24615;&#65292;&#20197;&#20351;&#36807;&#31243;&#22312;&#22270;&#19978;&#25353;&#29031;&#27599;&#20010;&#32452;&#25104;&#37096;&#20998;&#30340;&#35201;&#27714;&#21464;&#21270;&#24471;&#26356;&#21152;&#24179;&#28369;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#35268;&#21017;&#30340;&#32593;&#32476;&#25299;&#25169;&#19978;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#21644;&#25512;&#29702;&#20013;&#65292;&#24120;&#24120;&#20250;&#20351;&#29992;&#24179;&#31283;&#22270;&#24418;&#36807;&#31243;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#21333;&#19968;&#30340;&#20840;&#23616;&#26377;&#25928;&#30340;&#24179;&#31283;&#36807;&#31243;&#27169;&#22411;&#34920;&#31034;&#22270;&#24418;&#20449;&#21495;&#65292;&#20294;&#22312;&#35768;&#22810;&#23454;&#38469;&#38382;&#39064;&#20013;&#65292;&#36807;&#31243;&#30340;&#29305;&#24615;&#21487;&#33021;&#20250;&#22312;&#22270;&#30340;&#19981;&#21516;&#21306;&#22495;&#21457;&#29983;&#23616;&#37096;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23616;&#37096;&#24179;&#31283;&#22270;&#24418;&#36807;&#31243;&#65288;LSGP&#65289;&#27169;&#22411;&#65292;&#26088;&#22312;&#23558;&#32463;&#20856;&#30340;&#23616;&#37096;&#24179;&#31283;&#27010;&#24565;&#25193;&#23637;&#21040;&#19981;&#35268;&#21017;&#30340;&#22270;&#22495;&#19978;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#25972;&#20010;&#36807;&#31243;&#34920;&#31034;&#20026;&#19968;&#32452;&#32452;&#25104;&#37096;&#20998;&#36807;&#31243;&#30340;&#32452;&#21512;&#26469;&#34920;&#24449;&#23616;&#37096;&#24179;&#31283;&#24615;&#65292;&#20197;&#20351;&#36807;&#31243;&#22312;&#22270;&#19978;&#25353;&#29031;&#27599;&#20010;&#32452;&#25104;&#37096;&#20998;&#30340;&#35201;&#27714;&#21464;&#21270;&#24471;&#26356;&#21152;&#24179;&#28369;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;LSGP&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#29992;WSS&#36807;&#31243;&#23545;LSGP&#36827;&#34892;&#23616;&#37096;&#36817;&#20284;&#12290;&#22312;&#20449;&#21495;&#20869;&#25554;&#38382;&#39064;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
Stationary graph process models are commonly used in the analysis and inference of data sets collected on irregular network topologies. While most of the existing methods represent graph signals with a single stationary process model that is globally valid on the entire graph, in many practical problems, the characteristics of the process may be subject to local variations in different regions of the graph. In this work, we propose a locally stationary graph process (LSGP) model that aims to extend the classical concept of local stationarity to irregular graph domains. We characterize local stationarity by expressing the overall process as the combination of a set of component processes such that the extent to which the process adheres to each component varies smoothly over the graph. We propose an algorithm for computing LSGP models from realizations of the process, and also study the approximation of LSGPs locally with WSS processes. Experiments on signal interpolation problems show 
&lt;/p&gt;</description></item><item><title>FedSoL&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21248;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#24179;&#34913;&#20840;&#23616;&#23545;&#40784;&#21644;&#26412;&#22320;&#19968;&#33324;&#24615;&#26469;&#25913;&#21892;FL&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.12532</link><description>&lt;p&gt;
FedSoL: &#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#20840;&#23616;&#23545;&#40784;&#21644;&#26412;&#22320;&#19968;&#33324;&#24615;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
FedSoL: Bridging Global Alignment and Local Generality in Federated Learning. (arXiv:2308.12532v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12532
&lt;/p&gt;
&lt;p&gt;
FedSoL&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21248;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#24179;&#34913;&#20840;&#23616;&#23545;&#40784;&#21644;&#26412;&#22320;&#19968;&#33324;&#24615;&#26469;&#25913;&#21892;FL&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(Federated Learning, FL)&#36890;&#36807;&#32858;&#21512;&#26469;&#33258;&#20010;&#20307;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#26469;&#26500;&#24314;&#20840;&#23616;&#27169;&#22411;&#12290;&#34429;&#28982;FL&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#24403;&#23458;&#25143;&#31471;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21248;&#26102;&#65292;&#24120;&#24120;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;FL&#31639;&#27861;&#36890;&#36807;&#24341;&#20837;&#21508;&#31181;&#36817;&#20284;&#32422;&#26463;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20123;&#32422;&#26463;&#26088;&#22312;&#36890;&#36807;&#38480;&#21046;&#23616;&#37096;&#23398;&#20064;&#19982;&#20840;&#23616;&#30446;&#26631;&#30340;&#20559;&#31163;&#26469;&#20419;&#36827;&#20840;&#23616;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26412;&#36136;&#19978;&#36890;&#36807;&#24178;&#25200;&#21407;&#22987;&#30340;&#23616;&#37096;&#30446;&#26631;&#32780;&#38480;&#21046;&#20102;&#23616;&#37096;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#26469;&#25913;&#21892;&#26412;&#22320;&#23398;&#20064;&#30340;&#19968;&#33324;&#24615;&#12290;&#36890;&#36807;&#22312;&#24179;&#28369;&#30340;&#25439;&#22833;&#31354;&#38388;&#20013;&#33719;&#24471;&#26412;&#22320;&#27169;&#22411;&#65292;&#36825;&#31181;&#26041;&#27861;&#20943;&#36731;&#20102;&#23458;&#25143;&#31471;&#19981;&#21516;&#26412;&#22320;&#30446;&#26631;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;&#28982;&#32780;&#65292;&#23427;&#19981;&#33021;&#30830;&#20445;&#31283;&#23450;&#30340;&#20840;&#23616;&#23545;&#40784;&#65292;&#22240;&#20026;&#26412;&#22320;&#23398;&#20064;&#19981;&#32771;&#34385;&#20840;&#23616;&#30446;&#26631;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#31283;&#23450;&#24615;(FedSoL)&#26041;&#27861;&#26469;&#22312;FL&#20013;&#35299;&#20915;&#20840;&#23616;&#23545;&#40784;&#21644;&#26412;&#22320;&#19968;&#33324;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) aggregates locally trained models from individual clients to construct a global model. While FL enables learning a model with data privacy, it often suffers from significant performance degradation when client data distributions are heterogeneous. Many previous FL algorithms have addressed this issue by introducing various proximal restrictions. These restrictions aim to encourage global alignment by constraining the deviation of local learning from the global objective. However, they inherently limit local learning by interfering with the original local objectives. Recently, an alternative approach has emerged to improve local learning generality. By obtaining local models within a smooth loss landscape, this approach mitigates conflicts among different local objectives of the clients. Yet, it does not ensure stable global alignment, as local learning does not take the global objective into account. In this study, we propose Federated Stability on Learning (Fed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#22411;&#21512;&#25104;&#22270;&#29983;&#25104;&#22120;DGGI&#65292;&#29992;&#20110;&#20934;&#30830;&#22320;&#27169;&#25311;&#20114;&#32852;&#32593;&#20013;&#33258;&#27835;&#31995;&#32479;&#20869;&#30340;&#22270;&#30340;&#23646;&#24615;&#65292;&#22914;&#20013;&#24515;&#24615;&#12289;&#32858;&#31867;&#24615;&#12289;&#21516;&#36136;&#24615;&#20197;&#21450;&#33410;&#28857;&#24230;&#37327;&#12290;&#35813;&#29983;&#25104;&#22120;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;&#20114;&#32852;&#32593;&#25299;&#25169;&#29983;&#25104;&#22120;&#12290;</title><link>http://arxiv.org/abs/2308.05254</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#33258;&#27835;&#31995;&#32479;&#22270;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Data-driven Intra-Autonomous Systems Graph Generator. (arXiv:2308.05254v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#22411;&#21512;&#25104;&#22270;&#29983;&#25104;&#22120;DGGI&#65292;&#29992;&#20110;&#20934;&#30830;&#22320;&#27169;&#25311;&#20114;&#32852;&#32593;&#20013;&#33258;&#27835;&#31995;&#32479;&#20869;&#30340;&#22270;&#30340;&#23646;&#24615;&#65292;&#22914;&#20013;&#24515;&#24615;&#12289;&#32858;&#31867;&#24615;&#12289;&#21516;&#36136;&#24615;&#20197;&#21450;&#33410;&#28857;&#24230;&#37327;&#12290;&#35813;&#29983;&#25104;&#22120;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;&#20114;&#32852;&#32593;&#25299;&#25169;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#22411;&#21512;&#25104;&#22270;&#29983;&#25104;&#22120;DGGI&#65292;&#29992;&#20110;&#34920;&#31034;&#20114;&#32852;&#32593;&#20013;&#33258;&#27835;&#31995;&#32479;&#65288;AS&#65289;&#20869;&#30340;&#22270;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26469;&#33258;Internet Topology Data Kit&#65288;ITDK&#65289;&#39033;&#30446;&#30340;&#30495;&#23454;&#33258;&#27835;&#31995;&#32479;&#22270;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;Internet Graphs&#65288;IGraphs&#65289;&#12290;&#21019;&#24314;IGraphs&#37319;&#29992;&#20102;Filtered Recurrent Multi-level&#65288;FRM&#65289;&#31639;&#27861;&#36827;&#34892;&#31038;&#21306;&#25552;&#21462;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DGGI&#29983;&#25104;&#30340;&#21512;&#25104;&#22270;&#21487;&#20197;&#20934;&#30830;&#22320;&#20877;&#29616;&#20013;&#24515;&#24615;&#12289;&#32858;&#31867;&#24615;&#12289;&#21516;&#36136;&#24615;&#21644;&#33410;&#28857;&#24230;&#37327;&#30340;&#29305;&#24615;&#12290;DGGI&#29983;&#25104;&#22120;&#20248;&#20110;&#29616;&#26377;&#30340;&#20114;&#32852;&#32593;&#25299;&#25169;&#29983;&#25104;&#22120;&#12290;&#24179;&#22343;&#32780;&#35328;&#65292;&#23545;&#20110;&#21516;&#36136;&#24615;&#12289;&#20013;&#20171;&#24230;&#12289;&#32858;&#31867;&#24615;&#21644;&#33410;&#28857;&#24230;&#37327;&#65292;DGGI&#22312;&#26368;&#22823;&#22343;&#21248;&#24046;&#24322;&#24230;&#65288;MMD&#65289;&#25351;&#26631;&#19978;&#20998;&#21035;&#25552;&#39640;&#20102;84.4%&#12289;95.1%&#12289;97.9%&#21644;94.7%&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel deep-learning based generator of synthetic graphs that represent intra-Autonomous System (AS) in the Internet, named Deep-generative graphs for the Internet (DGGI). It also presents a novel massive dataset of real intra-AS graphs extracted from the project Internet Topology Data Kit (ITDK), called Internet Graphs (IGraphs). To create IGraphs, the Filtered Recurrent Multi-level (FRM) algorithm for community extraction was developed. It is shown that DGGI creates synthetic graphs which accurately reproduce the properties of centrality, clustering, assortativity, and node degree. The DGGI generator overperforms existing Internet topology generators. On average, DGGI improves the Maximum Mean Discrepancy (MMD) metric 84.4%, 95.1%, 97.9%, and 94.7% for assortativity, betweenness, clustering, and node degree, respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FlexPredict&#30340;&#38543;&#26426;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#27169;&#22411;&#20013;&#21152;&#20837;&#20301;&#32622;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#39044;&#27979;&#25513;&#30422;&#30340;&#26631;&#35760;&#20301;&#32622;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#25513;&#30422;&#22270;&#20687;&#24314;&#27169;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.00566</link><description>&lt;p&gt;
&#22312;&#38543;&#26426;&#20301;&#32622;&#39044;&#27979;&#25513;&#30422;&#30340;&#26631;&#35760;&#25913;&#21892;&#20102;&#25513;&#30422;&#22270;&#20687;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Predicting masked tokens in stochastic locations improves masked image modeling. (arXiv:2308.00566v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FlexPredict&#30340;&#38543;&#26426;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#27169;&#22411;&#20013;&#21152;&#20837;&#20301;&#32622;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#39044;&#27979;&#25513;&#30422;&#30340;&#26631;&#35760;&#20301;&#32622;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#25513;&#30422;&#22270;&#20687;&#24314;&#27169;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#26500;&#24314;&#38656;&#35201;&#23398;&#20064;&#26377;&#29992;&#34920;&#31034;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#21487;&#20197;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#20027;&#35201;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#26159;&#25513;&#30422;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#65292;&#32780;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#23384;&#22312;&#30456;&#24212;&#30340;&#25513;&#30422;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#12290;&#28982;&#32780;&#65292;MIM&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#22312;&#20934;&#30830;&#20301;&#32622;&#19978;&#39044;&#27979;&#35821;&#20041;&#20869;&#23481;&#12290;&#20363;&#22914;&#65292;&#32473;&#23450;&#19968;&#24352;&#19981;&#23436;&#25972;&#30340;&#29399;&#30340;&#22270;&#29255;&#65292;&#25105;&#20204;&#21487;&#20197;&#29468;&#27979;&#26377;&#19968;&#20010;&#23614;&#24052;&#65292;&#20294;&#25105;&#20204;&#26080;&#27861;&#30830;&#23450;&#23427;&#30340;&#30830;&#20999;&#20301;&#32622;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FlexPredict&#65292;&#36825;&#26159;&#19968;&#20010;&#32771;&#34385;&#20301;&#32622;&#19981;&#30830;&#23450;&#24615;&#30340;&#38543;&#26426;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#26465;&#20214;&#21270;&#21040;&#38543;&#26426;&#25513;&#30422;&#30340;&#26631;&#35760;&#20301;&#32622;&#19978;&#65292;&#24341;&#23548;&#27169;&#22411;&#23398;&#20064;&#26356;&#21152;&#40065;&#26834;&#23545;&#20301;&#32622;&#19981;&#30830;&#23450;&#24615;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#36827;&#20102;&#22810;&#20010;&#20219;&#21153;&#30340;&#19979;&#28216;&#24615;&#33021;&#65292;&#20363;&#22914;&#19982;MIM&#22522;&#20934;&#30456;&#27604;&#65292;FlexPredict&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning is a promising paradigm in deep learning that enables learning from unlabeled data by constructing pretext tasks that require learning useful representations. In natural language processing, the dominant pretext task has been masked language modeling (MLM), while in computer vision there exists an equivalent called Masked Image Modeling (MIM). However, MIM is challenging because it requires predicting semantic content in accurate locations. E.g, given an incomplete picture of a dog, we can guess that there is a tail, but we cannot determine its exact location. In this work, we propose FlexPredict, a stochastic model that addresses this challenge by incorporating location uncertainty into the model. Specifically, we condition the model on stochastic masked token positions to guide the model toward learning features that are more robust to location uncertainties. Our approach improves downstream performance on a range of tasks, e.g, compared to MIM baselines, Fle
&lt;/p&gt;</description></item><item><title>&#22312;&#24433;&#20687;&#23548;&#24341;&#30340;&#24494;&#21019;&#21307;&#30103;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;X&#23556;&#32447;&#25237;&#24433;&#36827;&#34892;&#36752;&#23556;&#36879;&#26126;&#29289;&#20307;&#30340;&#23039;&#24577;&#20272;&#35745;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20248;&#21270;&#35270;&#22270;&#21512;&#25104;&#22312;&#23436;&#25104;&#27492;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.00214</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#35843;&#25972;&#23618;&#26512;&#25104;&#20687;&#65288;NeTT&#65289;&#21644;&#25513;&#34109;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;mNeRF&#65289;&#30340;&#31283;&#20581;&#21333;&#35270;&#38181;&#24418;X&#23556;&#32447;&#23039;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Robust Single-view Cone-beam X-ray Pose Estimation with Neural Tuned Tomography (NeTT) and Masked Neural Radiance Fields (mNeRF). (arXiv:2308.00214v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00214
&lt;/p&gt;
&lt;p&gt;
&#22312;&#24433;&#20687;&#23548;&#24341;&#30340;&#24494;&#21019;&#21307;&#30103;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;X&#23556;&#32447;&#25237;&#24433;&#36827;&#34892;&#36752;&#23556;&#36879;&#26126;&#29289;&#20307;&#30340;&#23039;&#24577;&#20272;&#35745;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20248;&#21270;&#35270;&#22270;&#21512;&#25104;&#22312;&#23436;&#25104;&#27492;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24433;&#20687;&#23548;&#24341;&#30340;&#24494;&#21019;&#21307;&#30103;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#20219;&#21153;&#21487;&#20197;&#30475;&#20316;&#26159;&#23039;&#24577;&#20272;&#35745;&#38382;&#39064;&#65292;&#20854;&#20013;&#21033;&#29992;X&#23556;&#32447;&#25237;&#24433;&#26469;&#36798;&#21040;3D&#31354;&#38388;&#20013;&#30340;&#30446;&#26631;&#12290;&#36817;&#26399;&#22312;&#21487;&#24494;&#20998;&#28210;&#26579;&#25216;&#26415;&#19978;&#30340;&#36827;&#23637;&#20351;&#24471;RGB&#30456;&#26426;&#35270;&#22270;&#21512;&#25104;&#21644;&#23039;&#24577;&#20272;&#35745;&#30340;&#24615;&#33021;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;&#22312;&#20043;&#21069;&#30340;&#24037;&#20316;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;X&#23556;&#32447;&#25237;&#24433;&#36827;&#34892;&#36752;&#23556;&#36879;&#26126;&#29289;&#20307;&#30340;&#23039;&#24577;&#20272;&#35745;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20248;&#21270;&#35270;&#22270;&#21512;&#25104;&#22312;&#23436;&#25104;&#27492;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#39318;&#20808;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31639;&#27861;&#65288;DiffDRR&#65289;&#65292;&#33021;&#22815;&#22312;TensorFlow&#20013;&#39640;&#25928;&#35745;&#31639;&#25968;&#23383;&#37325;&#24314;&#25918;&#23556;&#22270;&#20687;&#65288;DRRs&#65289;&#24182;&#21033;&#29992;&#33258;&#21160;&#24494;&#20998;&#12290;&#32467;&#21512;&#32463;&#20856;&#30340;CBCT&#37325;&#24314;&#31639;&#27861;&#65292;&#25105;&#20204;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#23039;&#24577;&#20272;&#35745;&#65292;&#20351;&#29992;&#19968;&#20010;&#25439;&#22833;&#20989;&#25968;&#26469;&#37327;&#21270;&#20174;&#38543;&#26426;&#21021;&#22987;&#21270;&#23039;&#24577;&#21512;&#25104;&#30340;DRR&#19982;&#30446;&#26631;&#22788;&#30495;&#23454;&#36879;&#35270;&#22270;&#20687;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many tasks performed in image-guided, mini-invasive, medical procedures can be cast as pose estimation problems, where an X-ray projection is utilized to reach a target in 3D space. Recent advances in the differentiable rendering of optically reflective materials have enabled state-of-the-art performance in RGB camera view synthesis and pose estimation. Expanding on these prior works, we introduce new methods for pose estimation of radiolucent objects using X-ray projections, and we demonstrate the critical role of optimal view synthesis in performing this task. We first develop an algorithm (DiffDRR) that efficiently computes Digitally Reconstructed Radiographs (DRRs) and leverages automatic differentiation within TensorFlow. In conjunction with classic CBCT reconstruction algorithms, we perform pose estimation by gradient descent using a loss function that quantifies the similarity of the DRR synthesized from a randomly initialized pose and the true fluoroscopic image at the target p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#26631;&#31614;&#31232;&#32570;&#30340;Learning-To-Rank&#38382;&#39064;&#20013;&#65292;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#27169;&#22411;&#26159;&#21542;&#33021;&#32988;&#36807;GBDTs&#21644;&#20854;&#20182;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;SimCLR-Rank&#26041;&#27861;&#36827;&#34892;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.00177</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#27169;&#22411;&#22312;&#26631;&#31614;&#31232;&#32570;&#30340;Learning-To-Rank&#20013;&#32988;&#36807;GBDTs
&lt;/p&gt;
&lt;p&gt;
Pretrained deep models outperform GBDTs in Learning-To-Rank under label scarcity. (arXiv:2308.00177v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#26631;&#31614;&#31232;&#32570;&#30340;Learning-To-Rank&#38382;&#39064;&#20013;&#65292;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#27169;&#22411;&#26159;&#21542;&#33021;&#32988;&#36807;GBDTs&#21644;&#20854;&#20182;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;SimCLR-Rank&#26041;&#27861;&#36827;&#34892;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#39046;&#22495;&#26159;&#26368;&#20808;&#36827;&#30340;&#65292;&#20294;&#23427;&#20204;&#22312;&#34920;&#26684;&#24418;&#24335;&#30340;Learning-To-Rank&#38382;&#39064;&#19978;&#23578;&#26410;&#19968;&#33268;&#22320;&#32988;&#36807;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;(GBDTs)&#12290;&#36817;&#26399;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#20219;&#21153;&#19978;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21462;&#24471;&#30340;&#24615;&#33021;&#25552;&#21319;&#20027;&#35201;&#20381;&#36182;&#20110;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#27604;&#26377;&#26631;&#31614;&#25968;&#25454;&#22810;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#36824;&#26410;&#24212;&#29992;&#20110;Learning-To-Rank&#38382;&#39064;&#65292;&#32780;&#35813;&#38382;&#39064;&#36890;&#24120;&#20135;&#29983;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26159;&#21542;&#33021;&#25552;&#39640;LTR&#24615;&#33021;&#65292;&#19982;GBDTs&#21644;&#20854;&#20182;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#12290;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;&#35774;&#35745;&#36873;&#25321;(&#21253;&#25324;SimCLR-Rank&#65292;&#36825;&#26159;&#25105;&#20204;&#38024;&#23545;&#25490;&#21517;&#38382;&#39064;&#20462;&#25913;&#30340;SimCLR&#26041;&#27861;)&#65292;&#25105;&#20204;&#20135;&#29983;&#20102;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#26377;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#19988;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#26174;&#33879;&#20248;&#20110;GBDTs(&#21644;&#20854;&#20182;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;)&#12290;
&lt;/p&gt;
&lt;p&gt;
While deep learning (DL) models are state-of-the-art in text and image domains, they have not yet consistently outperformed Gradient Boosted Decision Trees (GBDTs) on tabular Learning-To-Rank (LTR) problems. Most of the recent performance gains attained by DL models in text and image tasks have used unsupervised pretraining, which exploits orders of magnitude more unlabeled data than labeled data. To the best of our knowledge, unsupervised pretraining has not been applied to the LTR problem, which often produces vast amounts of unlabeled data. In this work, we study whether unsupervised pretraining can improve LTR performance over GBDTs and other non-pretrained models. Using simple design choices--including SimCLR-Rank, our ranking-specific modification of SimCLR (an unsupervised pretraining method for images)--we produce pretrained deep learning models that soundly outperform GBDTs (and other non-pretrained models) in the case where labeled data is vastly outnumbered by unlabeled data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#26080;&#38480;&#21046;&#23545;&#25239;&#26679;&#26412;&#30340;&#26041;&#27861;AdvDiff&#12290;&#36890;&#36807;&#35774;&#35745;&#20004;&#31181;&#26032;&#30340;&#23545;&#25239;&#24341;&#23548;&#25216;&#26415;&#65292;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#36870;&#29983;&#25104;&#36807;&#31243;&#20013;&#36827;&#34892;&#23545;&#25239;&#37319;&#26679;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2307.12499</link><description>&lt;p&gt;
AdvDiff:&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#26080;&#38480;&#21046;&#30340;&#23545;&#25239;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
AdvDiff: Generating Unrestricted Adversarial Examples using Diffusion Models. (arXiv:2307.12499v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#26080;&#38480;&#21046;&#23545;&#25239;&#26679;&#26412;&#30340;&#26041;&#27861;AdvDiff&#12290;&#36890;&#36807;&#35774;&#35745;&#20004;&#31181;&#26032;&#30340;&#23545;&#25239;&#24341;&#23548;&#25216;&#26415;&#65292;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#36870;&#29983;&#25104;&#36807;&#31243;&#20013;&#36827;&#34892;&#23545;&#25239;&#37319;&#26679;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#38480;&#21046;&#30340;&#23545;&#25239;&#25915;&#20987;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#23545;&#25239;&#38450;&#24481;&#25216;&#26415;&#26500;&#25104;&#20005;&#37325;&#23041;&#32961;&#12290;&#23427;&#20204;&#23545;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#36896;&#25104;&#20005;&#37325;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#32469;&#36807;&#38450;&#24481;&#26426;&#21046;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#25915;&#20987;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#36825;&#20123;&#32593;&#32476;&#22312;&#29702;&#35770;&#19978;&#26080;&#27861;&#35777;&#26126;&#65292;&#22240;&#27492;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65288;&#22914;ImageNet&#65289;&#19978;&#36890;&#36807;&#24341;&#20837;&#23545;&#25239;&#30446;&#26631;&#29983;&#25104;&#30340;&#20363;&#23376;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;AdvDiff&#65292;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#26080;&#38480;&#21046;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#26032;&#30340;&#23545;&#25239;&#24341;&#23548;&#25216;&#26415;&#65292;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#36870;&#29983;&#25104;&#36807;&#31243;&#20013;&#36827;&#34892;&#23545;&#25239;&#37319;&#26679;&#12290;&#36825;&#20004;&#31181;&#25216;&#26415;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#30446;&#26631;&#20998;&#31867;&#22120;&#26799;&#24230;&#38598;&#25104;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#23545;&#25239;&#26679;&#26412;&#38750;&#24120;&#26377;&#25928;&#21644;&#31283;&#23450;&#12290;&#22312;MNIST&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AdvDiff&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unrestricted adversarial attacks present a serious threat to deep learning models and adversarial defense techniques. They pose severe security problems for deep learning applications because they can effectively bypass defense mechanisms. However, previous attack methods often utilize Generative Adversarial Networks (GANs), which are not theoretically provable and thus generate unrealistic examples by incorporating adversarial objectives, especially for large-scale datasets like ImageNet. In this paper, we propose a new method, called AdvDiff, to generate unrestricted adversarial examples with diffusion models. We design two novel adversarial guidance techniques to conduct adversarial sampling in the reverse generation process of diffusion models. These two techniques are effective and stable to generate high-quality, realistic adversarial examples by integrating gradients of the target classifier interpretably. Experimental results on MNIST and ImageNet datasets demonstrate that AdvD
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#25968;&#38477;&#22122;&#31639;&#27861;&#65292;&#29992;&#20110;3D&#20998;&#23376;&#39044;&#35757;&#32451;&#12290;&#36890;&#36807;&#28151;&#21512;&#22122;&#22768;&#31574;&#30053;&#35299;&#20915;&#20102;&#26679;&#26412;&#35206;&#30422;&#29575;&#20302;&#21644;&#21508;&#21521;&#21516;&#24615;&#21147;&#22330;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#35299;&#32806;&#20004;&#31181;&#31867;&#22411;&#30340;&#22122;&#22768;&#20811;&#26381;&#20102;&#20256;&#32479;&#38477;&#22122;&#26041;&#27861;&#26080;&#27861;&#23398;&#20064;&#21147;&#22330;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10683</link><description>&lt;p&gt;
&#20998;&#25968;&#38477;&#22122;&#29992;&#20110;3D&#20998;&#23376;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Fractional Denoising for 3D Molecular Pre-training. (arXiv:2307.10683v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#25968;&#38477;&#22122;&#31639;&#27861;&#65292;&#29992;&#20110;3D&#20998;&#23376;&#39044;&#35757;&#32451;&#12290;&#36890;&#36807;&#28151;&#21512;&#22122;&#22768;&#31574;&#30053;&#35299;&#20915;&#20102;&#26679;&#26412;&#35206;&#30422;&#29575;&#20302;&#21644;&#21508;&#21521;&#21516;&#24615;&#21147;&#22330;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#35299;&#32806;&#20004;&#31181;&#31867;&#22411;&#30340;&#22122;&#22768;&#20811;&#26381;&#20102;&#20256;&#32479;&#38477;&#22122;&#26041;&#27861;&#26080;&#27861;&#23398;&#20064;&#21147;&#22330;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22352;&#26631;&#38477;&#22122;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;3D&#20998;&#23376;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#33647;&#29289;&#21457;&#29616;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#20854;&#30446;&#26631;&#31561;&#21516;&#20110;&#23398;&#20064;&#21147;&#22330;&#65292;&#24182;&#19988;&#23545;&#19979;&#28216;&#20219;&#21153;&#26377;&#24110;&#21161;&#12290;&#28982;&#32780;&#65292;&#22352;&#26631;&#38477;&#22122;&#23398;&#20064;&#26377;&#25928;&#21147;&#22330;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#65292;&#21363;&#26679;&#26412;&#35206;&#30422;&#29575;&#20302;&#21644;&#21508;&#21521;&#21516;&#24615;&#21147;&#22330;&#12290;&#26681;&#26412;&#21407;&#22240;&#22312;&#20110;&#29616;&#26377;&#38477;&#22122;&#26041;&#27861;&#25152;&#20551;&#35774;&#30340;&#20998;&#23376;&#20998;&#24067;&#19981;&#33021;&#25429;&#25417;&#20998;&#23376;&#30340;&#21508;&#21521;&#24322;&#24615;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#22122;&#22768;&#31574;&#30053;&#65292;&#21253;&#25324;&#20108;&#38754;&#35282;&#21644;&#22352;&#26631;&#30340;&#22122;&#22768;&#12290;&#28982;&#32780;&#65292;&#20197;&#20256;&#32479;&#26041;&#24335;&#38477;&#22122;&#36825;&#31181;&#28151;&#21512;&#22122;&#22768;&#19981;&#20877;&#31561;&#21516;&#20110;&#23398;&#20064;&#21147;&#22330;&#12290;&#36890;&#36807;&#29702;&#35770;&#25512;&#23548;&#65292;&#25105;&#20204;&#21457;&#29616;&#38382;&#39064;&#26159;&#30001;&#20110;&#36755;&#20837;&#26500;&#35937;&#23545;&#21327;&#26041;&#24046;&#30340;&#20381;&#36182;&#24615;&#25152;&#23548;&#33268;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#22122;&#22768;&#35299;&#32806;
&lt;/p&gt;
&lt;p&gt;
Coordinate denoising is a promising 3D molecular pre-training method, which has achieved remarkable performance in various downstream drug discovery tasks. Theoretically, the objective is equivalent to learning the force field, which is revealed helpful for downstream tasks. Nevertheless, there are two challenges for coordinate denoising to learn an effective force field, i.e. low coverage samples and isotropic force field. The underlying reason is that molecular distributions assumed by existing denoising methods fail to capture the anisotropic characteristic of molecules. To tackle these challenges, we propose a novel hybrid noise strategy, including noises on both dihedral angel and coordinate. However, denoising such hybrid noise in a traditional way is no more equivalent to learning the force field. Through theoretical deductions, we find that the problem is caused by the dependency of the input conformation for covariance. To this end, we propose to decouple the two types of nois
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#27169;&#22411;&#21152;&#36895;Benders&#20998;&#35299;&#26041;&#27861;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#30456;&#23545;&#20110;&#20854;&#20182;&#21152;&#36895;&#26041;&#26696;&#30340;30%&#26356;&#24555;&#30340;&#24179;&#22343;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.08816</link><description>&lt;p&gt;
&#21152;&#36895;Benders&#20998;&#35299;&#26041;&#27861;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#27169;&#22411;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Accelerating Benders Decomposition via Reinforcement Learning Surrogate Models. (arXiv:2307.08816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#27169;&#22411;&#21152;&#36895;Benders&#20998;&#35299;&#26041;&#27861;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#30456;&#23545;&#20110;&#20854;&#20182;&#21152;&#36895;&#26041;&#26696;&#30340;30%&#26356;&#24555;&#30340;&#24179;&#22343;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#20248;&#21270;&#35797;&#22270;&#22312;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#26368;&#20248;&#20915;&#31574;&#12290;&#36890;&#24120;&#65292;&#30001;&#20110;&#38656;&#35201;&#25429;&#25417;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#26223;&#25968;&#37327;&#20197;&#21450;&#29616;&#23454;&#35268;&#21010;&#38382;&#39064;&#30340;&#31163;&#25955;&#24615;&#36136;&#65292;&#36825;&#20123;&#38382;&#39064;&#30340;&#32463;&#20856;&#24418;&#24335;&#21464;&#24471;&#38590;&#20197;&#22788;&#29702;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#21487;&#34892;&#24615;&#38382;&#39064;&#65292;&#23454;&#36341;&#32773;&#20204;&#36716;&#21521;&#20998;&#35299;&#26041;&#27861;&#65292;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#26356;&#23567;&#12289;&#26356;&#26131;&#22788;&#29702;&#30340;&#23376;&#38382;&#39064;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#20998;&#35299;&#26041;&#27861;&#26159;Benders&#20998;&#35299;&#65288;BD&#65289;&#65292;&#23427;&#26681;&#25454;&#24773;&#26223;&#29420;&#31435;&#24615;&#23545;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#36827;&#34892;&#20998;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20195;&#29702;&#27169;&#22411;&#21152;&#36895;BD&#30340;&#26041;&#27861;&#65292;&#35813;&#20195;&#29702;&#27169;&#22411;&#21462;&#20195;&#20102;NP&#38590;&#30340;&#25972;&#25968;&#20027;&#38382;&#39064;&#12290;&#36890;&#36807;&#21152;&#36895;&#26041;&#27861;&#65292;&#19982;&#20854;&#20182;&#21152;&#36895;&#30340;BD&#23454;&#29616;&#30456;&#27604;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24179;&#22343;&#25910;&#25947;&#36895;&#24230;&#25552;&#39640;&#20102;30%&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20316;&#20026;&#26367;&#20195;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#23427;&#26469;&#35299;&#20915;&#38543;&#26426;&#24211;&#23384;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic optimization (SO) attempts to offer optimal decisions in the presence of uncertainty. Often, the classical formulation of these problems becomes intractable due to (a) the number of scenarios required to capture the uncertainty and (b) the discrete nature of real-world planning problems. To overcome these tractability issues, practitioners turn to decomposition methods that divide the problem into smaller, more tractable sub-problems. The focal decomposition method of this paper is Benders decomposition (BD), which decomposes stochastic optimization problems on the basis of scenario independence. In this paper we propose a method of accelerating BD with the aid of a surrogate model in place of an NP-hard integer master problem. Through the acceleration method we observe 30% faster average convergence when compared to other accelerated BD implementations. We introduce a reinforcement learning agent as a surrogate and demonstrate how it can be used to solve a stochastic invent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#31216;&#22810;&#39033;&#24335;&#20195;&#25968;&#30340;&#24037;&#20855;&#35777;&#26126;&#20102;&#23545;&#20110;&#20855;&#26377;&#20998;&#27573;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#19988;&#20307;&#31995;&#32467;&#26500;&#22823;&#23567;&#19981;&#21464;&#30340;GNNs&#65292;&#23384;&#22312;&#19968;&#23545;&#38750;&#21516;&#26500;&#26681;&#26641;&#22312;&#20219;&#24847;&#36845;&#20195;&#27425;&#25968;&#20869;&#26080;&#27861;&#34987;&#21306;&#20998;&#65292;&#19982;&#27492;&#21516;&#26102;&#65292;&#20855;&#26377;&#19981;&#21516;&#22823;&#23567;&#30340;GNNs&#21482;&#38656;&#20004;&#27425;&#36845;&#20195;&#21363;&#21487;&#21306;&#20998;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#22914;&#26524;&#20801;&#35768;&#38750;&#20998;&#27573;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#65292;&#21017;&#22312;&#20004;&#27425;&#36845;&#20195;&#20869;&#65292;&#21333;&#20010;&#31070;&#32463;&#20803;&#24863;&#30693;&#22120;&#21487;&#20197;&#21306;&#20998;&#20219;&#24847;&#19968;&#23545;&#38750;&#21516;&#26500;&#26641;&#30340;&#26681;&#33410;&#28857;&#12290;</title><link>http://arxiv.org/abs/2307.04661</link><description>&lt;p&gt;
&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#21147;&#21644;&#28608;&#27963;&#20989;&#25968;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the power of graph neural networks and the role of the activation function. (arXiv:2307.04661v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#31216;&#22810;&#39033;&#24335;&#20195;&#25968;&#30340;&#24037;&#20855;&#35777;&#26126;&#20102;&#23545;&#20110;&#20855;&#26377;&#20998;&#27573;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#19988;&#20307;&#31995;&#32467;&#26500;&#22823;&#23567;&#19981;&#21464;&#30340;GNNs&#65292;&#23384;&#22312;&#19968;&#23545;&#38750;&#21516;&#26500;&#26681;&#26641;&#22312;&#20219;&#24847;&#36845;&#20195;&#27425;&#25968;&#20869;&#26080;&#27861;&#34987;&#21306;&#20998;&#65292;&#19982;&#27492;&#21516;&#26102;&#65292;&#20855;&#26377;&#19981;&#21516;&#22823;&#23567;&#30340;GNNs&#21482;&#38656;&#20004;&#27425;&#36845;&#20195;&#21363;&#21487;&#21306;&#20998;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#22914;&#26524;&#20801;&#35768;&#38750;&#20998;&#27573;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#65292;&#21017;&#22312;&#20004;&#27425;&#36845;&#20195;&#20869;&#65292;&#21333;&#20010;&#31070;&#32463;&#20803;&#24863;&#30693;&#22120;&#21487;&#20197;&#21306;&#20998;&#20219;&#24847;&#19968;&#23545;&#38750;&#21516;&#26500;&#26641;&#30340;&#26681;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#34920;&#36798;&#33021;&#21147;&#30340;&#26032;&#32467;&#26524;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20219;&#20309;&#20855;&#26377;&#20998;&#27573;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#12289;&#20854;&#20307;&#31995;&#32467;&#26500;&#22823;&#23567;&#19981;&#38543;&#22270;&#36755;&#20837;&#22823;&#23567;&#22686;&#38271;&#30340;GNNs&#65292;&#23384;&#22312;&#19968;&#23545;&#28145;&#24230;&#20026;&#20108;&#30340;&#38750;&#21516;&#26500;&#26681;&#26641;&#65292;&#20351;&#24471;GNNs&#22312;&#20219;&#24847;&#36845;&#20195;&#27425;&#25968;&#20869;&#26080;&#27861;&#21306;&#20998;&#23427;&#20204;&#30340;&#26681;&#33410;&#28857;&#12290;&#35777;&#26126;&#20381;&#36182;&#20110;&#23545;&#31216;&#22810;&#39033;&#24335;&#20195;&#25968;&#30340;&#24037;&#20855;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#24050;&#32463;&#30693;&#36947;&#20855;&#26377;&#20998;&#27573;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#30340;&#26080;&#30028;GNNs&#65288;&#20854;&#22823;&#23567;&#20801;&#35768;&#38543;&#22270;&#22823;&#23567;&#25913;&#21464;&#65289;&#21482;&#38656;&#20004;&#27425;&#36845;&#20195;&#21363;&#21487;&#21306;&#20998;&#36825;&#20123;&#39030;&#28857;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23545;&#20110;&#26377;&#30028;&#22823;&#23567;&#21644;&#26080;&#30028;&#22823;&#23567;&#30340;GNNs&#20043;&#38388;&#23384;&#22312;&#20005;&#26684;&#30340;&#20998;&#31163;&#65292;&#22238;&#31572;&#20102; [Grohe, 2021] &#25552;&#20986;&#30340;&#19968;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35777;&#26126;&#22914;&#26524;&#20801;&#35768;&#38750;&#20998;&#27573;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#65292;&#21017;&#22312;&#20004;&#27425;&#36845;&#20195;&#20013;&#65292;&#21333;&#20010;&#31070;&#32463;&#20803;&#24863;&#30693;&#22120;&#21487;&#20197;&#21306;&#20998;&#20219;&#24847;&#19968;&#23545;&#38750;&#21516;&#26500;&#26641;&#30340;&#26681;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article we present new results about the expressivity of Graph Neural Networks (GNNs). We prove that for any GNN with piecewise polynomial activations, whose architecture size does not grow with the graph input sizes, there exists a pair of non-isomorphic rooted trees of depth two such that the GNN cannot distinguish their root vertex up to an arbitrary number of iterations. The proof relies on tools from the algebra of symmetric polynomials. In contrast, it was already known that unbounded GNNs (those whose size is allowed to change with the graph sizes) with piecewise polynomial activations can distinguish these vertices in only two iterations. Our results imply a strict separation between bounded and unbounded size GNNs, answering an open question formulated by [Grohe, 2021]. We next prove that if one allows activations that are not piecewise polynomial, then in two iterations a single neuron perceptron can distinguish the root vertices of any pair of nonisomorphic trees of 
&lt;/p&gt;</description></item><item><title>&#31639;&#23376;&#23398;&#20064;&#20013;&#23384;&#22312;&#32500;&#24230;&#35781;&#21650;&#65292;&#20294;&#23545;&#20110;&#30001;Hamilton-Jacobi&#26041;&#31243;&#23450;&#20041;&#30340;&#35299;&#31639;&#23376;&#21487;&#20197;&#20811;&#26381;&#32500;&#24230;&#35781;&#21650;&#12290;</title><link>http://arxiv.org/abs/2306.15924</link><description>&lt;p&gt;
&#36816;&#31639;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#35781;&#21650;
&lt;/p&gt;
&lt;p&gt;
The curse of dimensionality in operator learning. (arXiv:2306.15924v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15924
&lt;/p&gt;
&lt;p&gt;
&#31639;&#23376;&#23398;&#20064;&#20013;&#23384;&#22312;&#32500;&#24230;&#35781;&#21650;&#65292;&#20294;&#23545;&#20110;&#30001;Hamilton-Jacobi&#26041;&#31243;&#23450;&#20041;&#30340;&#35299;&#31639;&#23376;&#21487;&#20197;&#20811;&#26381;&#32500;&#24230;&#35781;&#21650;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31639;&#23376;&#26550;&#26500;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;&#26144;&#23556;&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#30340;&#31639;&#23376;&#65292;&#21487;&#20197;&#29992;&#20110;&#36890;&#36807;&#27169;&#25311;&#21152;&#36895;&#27169;&#22411;&#35780;&#20272;&#65292;&#25110;&#32773;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;&#36817;&#24180;&#26469;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#24341;&#21457;&#20102;&#31639;&#23376;&#23398;&#20064;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#12290;&#26412;&#25991;&#30340;&#31532;&#19968;&#39033;&#36129;&#29486;&#26159;&#35777;&#26126;&#20102;&#23545;&#20110;&#19968;&#33324;&#30340;&#21482;&#30001;&#20854; $C^r$ &#25110; Lipschitz &#27491;&#21017;&#24615;&#29305;&#24449;&#21270;&#30340;&#31639;&#23376;&#31867;&#65292;&#31639;&#23376;&#23398;&#20064;&#36973;&#21463;&#20102;&#32500;&#24230;&#35781;&#21650;&#65292;&#36825;&#37324;&#36890;&#36807;&#26080;&#31351;&#32500;&#36755;&#20837;&#21644;&#36755;&#20986;&#20989;&#25968;&#31354;&#38388;&#30340;&#34920;&#24449;&#26469;&#31934;&#30830;&#23450;&#20041;&#32500;&#24230;&#35781;&#21650;&#12290;&#35813;&#32467;&#26524;&#36866;&#29992;&#20110;&#21253;&#25324; PCA-Net&#12289;DeepONet &#21644; FNO &#22312;&#20869;&#30340;&#22810;&#31181;&#29616;&#26377;&#31070;&#32463;&#31639;&#23376;&#12290;&#26412;&#25991;&#30340;&#31532;&#20108;&#39033;&#36129;&#29486;&#26159;&#35777;&#26126;&#20102;&#23545;&#20110;&#30001;Hamilton-Jacobi&#26041;&#31243;&#23450;&#20041;&#30340;&#35299;&#31639;&#23376;&#65292;&#21487;&#20197;&#20811;&#26381;&#19968;&#33324;&#30340;&#32500;&#24230;&#35781;&#21650;&#65307;&#36825;&#26159;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#34920;&#31034;&#26041;&#27861;&#26469;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural operator architectures employ neural networks to approximate operators mapping between Banach spaces of functions; they may be used to accelerate model evaluations via emulation, or to discover models from data. Consequently, the methodology has received increasing attention over recent years, giving rise to the rapidly growing field of operator learning. The first contribution of this paper is to prove that for general classes of operators which are characterized only by their $C^r$- or Lipschitz-regularity, operator learning suffers from a curse of dimensionality, defined precisely here in terms of representations of the infinite-dimensional input and output function spaces. The result is applicable to a wide variety of existing neural operators, including PCA-Net, DeepONet and the FNO. The second contribution of the paper is to prove that the general curse of dimensionality can be overcome for solution operators defined by the Hamilton-Jacobi equation; this is achieved by lev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#23558;HRTF&#19978;&#37319;&#26679;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#20351;&#24102;HRTF&#30340;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#21644;&#22686;&#24378;&#29616;&#23454;&#65288;AR&#65289;&#29615;&#22659;&#26356;&#21152;&#36924;&#30495;&#12290;</title><link>http://arxiv.org/abs/2306.05812</link><description>&lt;p&gt;
&#20351;&#29992;gnomonic equiangular&#25237;&#24433;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#36827;&#34892;HRTF&#19978;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
HRTF upsampling with a generative adversarial network using a gnomonic equiangular projection. (arXiv:2306.05812v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#23558;HRTF&#19978;&#37319;&#26679;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#20351;&#24102;HRTF&#30340;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#21644;&#22686;&#24378;&#29616;&#23454;&#65288;AR&#65289;&#29615;&#22659;&#26356;&#21152;&#36924;&#30495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#30340;&#22836;&#37096;&#30456;&#20851;&#36716;&#31227;&#20989;&#25968;&#65288;HRTF&#65289;&#23545;&#20110;&#21019;&#24314;&#36924;&#30495;&#30340;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#21644;&#22686;&#24378;&#29616;&#23454;&#65288;AR&#65289;&#29615;&#22659;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22768;&#23398;&#27979;&#37327;&#39640;&#36136;&#37327;&#30340;HRTF&#38656;&#35201;&#26114;&#36149;&#30340;&#35774;&#22791;&#21644;&#22768;&#23398;&#23454;&#39564;&#23460;&#35774;&#32622;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#24182;&#20351;&#27979;&#37327;&#26356;&#21152;&#39640;&#25928;&#65292;&#36807;&#21435;&#24050;&#32463;&#21033;&#29992;&#20102;HRTF&#19978;&#37319;&#26679;&#65292;&#20854;&#20013;&#20174;&#20302;&#20998;&#36776;&#29575;&#30340;HRTF&#21019;&#24314;&#39640;&#20998;&#36776;&#29575;&#30340;HRTF&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#24212;&#29992;&#20110;HRTF&#19978;&#37319;&#26679;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;HRTF&#25968;&#25454;&#36716;&#25442;&#20026;&#19982;&#21367;&#31215;&#36229;&#20998;&#36776;&#29575;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;SRGAN&#65289;&#26041;&#20415;&#20351;&#29992;&#12290;&#36825;&#31181;&#26032;&#26041;&#27861;&#19982;&#20004;&#20010;&#22522;&#32447;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65306;&#37325;&#24515;&#19978;&#37319;&#26679;&#21644;HRTF&#36873;&#25321;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20351;&#29992;&#24863;&#30693;&#27169;&#22411;&#26102;&#65292;&#20174;log-spectral&#22833;&#30495;&#65288;LSD&#65289;&#21644;&#23450;&#20301;&#24615;&#33021;&#26041;&#38754;&#22343;&#20248;&#20110;&#20004;&#20010;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
An individualised head-related transfer function (HRTF) is essential for creating realistic virtual reality (VR) and augmented reality (AR) environments. However, acoustically measuring high-quality HRTFs requires expensive equipment and an acoustic lab setting. To overcome these limitations and to make this measurement more efficient HRTF upsampling has been exploited in the past where a high-resolution HRTF is created from a low-resolution one. This paper demonstrates how generative adversarial networks (GANs) can be applied to HRTF upsampling. We propose a novel approach that transforms the HRTF data for convenient use with a convolutional super-resolution generative adversarial network (SRGAN). This new approach is benchmarked against two baselines: barycentric upsampling and a HRTF selection approach. Experimental results show that the proposed method outperforms both baselines in terms of log-spectral distortion (LSD) and localisation performance using perceptual models when the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#39640;&#26031;&#21464;&#20998;&#36807;&#31243;&#21442;&#25968;&#21270;&#26041;&#27861;&#26469;&#26356;&#22909;&#22320;&#23398;&#20064;&#20855;&#26377;&#38750;&#32447;&#24615;&#25193;&#25955;&#36807;&#31243;&#30340;&#28508;&#22312;&#36807;&#31243;&#65292;&#27492;&#26041;&#27861;&#37319;&#29992;&#20855;&#26377;&#36830;&#32493;&#25351;&#25968;&#26063;&#25551;&#36848;&#30340;&#31639;&#27861;&#23454;&#29616;&#20984;&#20248;&#21270;&#65292;&#21487;&#20197;&#20195;&#26367;&#32531;&#24930;&#30340;&#20855;&#26377;&#22266;&#23450;&#28857;&#36845;&#20195;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.02066</link><description>&lt;p&gt;
&#21464;&#20998;&#39640;&#26031;&#36807;&#31243;&#25193;&#25955;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Variational Gaussian Process Diffusion Processes. (arXiv:2306.02066v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#39640;&#26031;&#21464;&#20998;&#36807;&#31243;&#21442;&#25968;&#21270;&#26041;&#27861;&#26469;&#26356;&#22909;&#22320;&#23398;&#20064;&#20855;&#26377;&#38750;&#32447;&#24615;&#25193;&#25955;&#36807;&#31243;&#30340;&#28508;&#22312;&#36807;&#31243;&#65292;&#27492;&#26041;&#27861;&#37319;&#29992;&#20855;&#26377;&#36830;&#32493;&#25351;&#25968;&#26063;&#25551;&#36848;&#30340;&#31639;&#27861;&#23454;&#29616;&#20984;&#20248;&#21270;&#65292;&#21487;&#20197;&#20195;&#26367;&#32531;&#24930;&#30340;&#20855;&#26377;&#22266;&#23450;&#28857;&#36845;&#20195;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#36807;&#31243;&#26159;&#19968;&#31867;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65292;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#34920;&#29616;&#20016;&#23500;&#30340;&#27169;&#22411;&#65292;&#33258;&#28982;&#22320;&#20986;&#29616;&#22312;&#21160;&#24577;&#24314;&#27169;&#20219;&#21153;&#20013;&#12290;&#27010;&#29575;&#25512;&#29702;&#21644;&#29983;&#25104;&#27169;&#22411;&#19979;&#20855;&#26377;&#38750;&#32447;&#24615;&#25193;&#25955;&#36807;&#31243;&#30340;&#28508;&#22312;&#36807;&#31243;&#30340;&#23398;&#20064;&#37117;&#26159;&#26840;&#25163;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#22312;&#21464;&#20998;&#25512;&#29702;&#30340;&#22522;&#30784;&#19978;&#26500;&#24314;&#39640;&#26031;&#36807;&#31243;&#25193;&#25955;&#36807;&#31243;&#30340;&#21442;&#25968;&#21270;&#65292;&#25351;&#20986;&#26041;&#27861;&#20013;&#30340;&#30149;&#24577;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#36830;&#32493;&#25351;&#25968;&#26063;&#25551;&#36848;&#30340;&#39640;&#26031;&#21464;&#20998;&#36807;&#31243;&#30340;&#26367;&#20195;&#21442;&#25968;&#21270;&#26041;&#27861;&#12290;&#36825;&#20351;&#25105;&#20204;&#21487;&#20197;&#29992;&#20984;&#20248;&#21270;&#30340;&#24555;&#36895;&#31639;&#27861;&#20195;&#26367;&#20855;&#26377;&#22266;&#23450;&#28857;&#36845;&#20195;&#30340;&#32531;&#24930;&#31639;&#27861;&#65292;&#36825;&#31181;&#31639;&#27861;&#31867;&#20284;&#20110;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#65292;&#21516;&#26102;&#25552;&#20379;&#26356;&#22909;&#30340;&#30446;&#26631;&#26469;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion processes are a class of stochastic differential equations (SDEs) providing a rich family of expressive models that arise naturally in dynamic modelling tasks. Probabilistic inference and learning under generative models with latent processes endowed with a non-linear diffusion process prior are intractable problems. We build upon work within variational inference approximating the posterior process as a linear diffusion process, point out pathologies in the approach, and propose an alternative parameterization of the Gaussian variational process using a continuous exponential family description. This allows us to trade a slow inference algorithm with fixed-point iterations for a fast algorithm for convex optimization akin to natural gradient descent, which also provides a better objective for the learning of model parameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30417;&#30563;&#24335;&#27880;&#24847;&#21147;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#20998;&#26512;&#36229;&#22768;&#22270;&#20687;&#65292;&#23454;&#29616;AS&#30340;&#31934;&#30830;&#31579;&#26597;&#19988;&#31934;&#24230;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.00003</link><description>&lt;p&gt;
&#36890;&#36807;&#30417;&#30563;&#24335;&#27880;&#24847;&#21147;&#22810;&#23454;&#20363;&#23398;&#20064;&#20174;&#22810;&#35270;&#35282;&#36229;&#22768;&#22270;&#20687;&#26816;&#27979;&#24515;&#33039;&#30149;
&lt;/p&gt;
&lt;p&gt;
Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning. (arXiv:2306.00003v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30417;&#30563;&#24335;&#27880;&#24847;&#21147;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#20998;&#26512;&#36229;&#22768;&#22270;&#20687;&#65292;&#23454;&#29616;AS&#30340;&#31934;&#30830;&#31579;&#26597;&#19988;&#31934;&#24230;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#33033;&#29923;&#29421;&#31364;(AS)&#26159;&#19968;&#31181;&#23548;&#33268;&#20005;&#37325;&#21457;&#30149;&#29575;&#21644;&#27515;&#20129;&#29575;&#30340;&#36864;&#34892;&#24615;&#29923;&#33180;&#30142;&#30149;&#12290;&#36825;&#31181;&#24773;&#20917;&#32463;&#24120;&#34987;&#20302;&#20272;&#21644;&#20302;&#27835;&#30103;&#12290;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#65292;AS&#26159;&#36890;&#36807;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;&#19987;&#23478;&#23457;&#26597;&#26469;&#35786;&#26029;&#30340;&#65292;&#36825;&#20250;&#20135;&#29983;&#25968;&#21313;&#20010;&#19979;&#32954;&#37319;&#26679;&#30340;&#36229;&#22768;&#22270;&#20687;&#12290;&#21482;&#26377;&#19968;&#20123;&#35270;&#22270;&#26174;&#31034;&#20027;&#21160;&#33033;&#29923;&#12290;&#20026;&#20102;&#33258;&#21160;&#21270;&#31579;&#26597;AS&#65292;&#28145;&#24230;&#32593;&#32476;&#24517;&#39035;&#23398;&#20064;&#27169;&#20223;&#20154;&#31867;&#19987;&#23478;&#35782;&#21035;&#20027;&#21160;&#33033;&#29923;&#35270;&#22270;&#30340;&#33021;&#21147;&#65292;&#28982;&#21518;&#27719;&#24635;&#36825;&#20123;&#30456;&#20851;&#22270;&#20687;&#20197;&#20135;&#29983;&#30740;&#31350;&#32423;&#35786;&#26029;&#12290;&#25105;&#20204;&#21457;&#29616;&#20808;&#21069;&#30340;AS&#26816;&#27979;&#26041;&#27861;&#30001;&#20110;&#20381;&#36182;&#20110;&#36328;&#22270;&#20687;&#30340;&#19981;&#28789;&#27963;&#24179;&#22343;&#20540;&#32780;&#23548;&#33268;&#31934;&#24230;&#19981;&#36275;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#65292;&#29616;&#25104;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22810;&#23454;&#20363;(MIL)&#23398;&#20064;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;MIL&#26041;&#27861;&#65292;&#21253;&#21547;&#20004;&#20010;&#20851;&#38190;&#26041;&#27861;&#21019;&#26032;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#30417;&#30563;&#24335;&#27880;&#24847;&#25216;&#26415;&#65292;&#24341;&#23548;&#23398;&#20064;&#30340;&#27880;&#24847;&#26426;&#21046;&#20559;&#29233;&#30456;&#20851;&#35270;&#22270;&#12290;&#20854;&#27425;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#25552;&#39640;&#20102;&#27599;&#20010;&#21333;&#29420;&#22270;&#20687;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#20010;&#30495;&#23454;&#30340;&#20020;&#24202;&#25968;&#25454;&#38598;&#65288;4569&#21517;&#24739;&#32773;&#65289;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22312;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20043;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aortic stenosis (AS) is a degenerative valve condition that causes substantial morbidity and mortality. This condition is under-diagnosed and under-treated. In clinical practice, AS is diagnosed with expert review of transthoracic echocardiography, which produces dozens of ultrasound images of the heart. Only some of these views show the aortic valve. To automate screening for AS, deep networks must learn to mimic a human expert's ability to identify views of the aortic valve then aggregate across these relevant images to produce a study-level diagnosis. We find previous approaches to AS detection yield insufficient accuracy due to relying on inflexible averages across images. We further find that off-the-shelf attention-based multiple instance learning (MIL) performs poorly. We contribute a new end-to-end MIL approach with two key methodological innovations. First, a supervised attention technique guides the learned attention mechanism to favor relevant views. Second, a novel self-sup
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#26080;&#27861;&#22788;&#29702;&#30340;&#32467;&#26500;&#21270;&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#65288;SLDAS&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#37051;&#22495;&#26500;&#24314;&#65288;DNC&#65289;&#30340;&#26032;&#22411;&#21033;&#29992;&#31574;&#30053;&#65292;&#36890;&#36807;&#21487;&#25193;&#23637;&#30340;&#37051;&#22495;&#25506;&#32034;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#39640;&#25928;&#22320;&#25506;&#32034;&#36830;&#32493;&#20195;&#29702;&#21160;&#20316;&#21608;&#22260;&#30340;&#31163;&#25955;&#37051;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.19891</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#30340;&#21160;&#24577;&#37051;&#22495;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Dynamic Neighborhood Construction for Structured Large Discrete Action Spaces. (arXiv:2305.19891v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#26080;&#27861;&#22788;&#29702;&#30340;&#32467;&#26500;&#21270;&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#65288;SLDAS&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#37051;&#22495;&#26500;&#24314;&#65288;DNC&#65289;&#30340;&#26032;&#22411;&#21033;&#29992;&#31574;&#30053;&#65292;&#36890;&#36807;&#21487;&#25193;&#23637;&#30340;&#37051;&#22495;&#25506;&#32034;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#39640;&#25928;&#22320;&#25506;&#32034;&#36830;&#32493;&#20195;&#29702;&#21160;&#20316;&#21608;&#22260;&#30340;&#31163;&#25955;&#37051;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#65288;LDAS&#65289;&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#22788;&#29702;&#22810;&#36798;&#20960;&#30334;&#19975;&#20010;&#21160;&#20316;&#30340;&#38750;&#32467;&#26500;&#21270;LDAS&#12290;&#28982;&#32780;&#65292;&#22312;&#29289;&#27969;&#12289;&#29983;&#20135;&#21644;&#36816;&#36755;&#31995;&#32479;&#31561;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;&#21160;&#20316;&#31354;&#38388;&#20855;&#26377;&#32452;&#21512;&#32467;&#26500;&#65292;&#20854;&#35268;&#27169;&#29978;&#33267;&#22312;&#23567;&#35268;&#27169;&#23454;&#20363;&#19978;&#20063;&#36229;&#36807;&#20102;&#25968;&#30334;&#19975;&#20010;&#21160;&#20316;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#36825;&#26679;&#30340;&#21160;&#20316;&#31354;&#38388;&#21576;&#29616;&#20986;&#19968;&#23450;&#30340;&#32467;&#26500;&#65292;&#20363;&#22914;&#31561;&#38388;&#36317;&#30340;&#31163;&#25955;&#36164;&#28304;&#21333;&#20301;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#22788;&#29702;&#24403;&#21069;&#22522;&#20934;&#27979;&#35797;&#26080;&#27861;&#22788;&#29702;&#30340;&#32467;&#26500;&#21270;LDAS&#65288;SLDAS&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#37051;&#22495;&#26500;&#24314;&#65288;DNC&#65289;&#30340;&#26032;&#22411;&#21033;&#29992;&#31574;&#30053;&#65292;&#29992;&#20110;SLDAS&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#37051;&#22495;&#25506;&#32034;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#21033;&#29992;&#36825;&#31181;&#31574;&#30053;&#65292;&#22312;&#20855;&#26377;&#39640;&#36798;$10^{73}$&#20010;&#21160;&#20316;&#30340;&#32467;&#26500;&#21270;&#21160;&#20316;&#31354;&#38388;&#20013;&#39640;&#25928;&#22320;&#25506;&#32034;&#36830;&#32493;&#20195;&#29702;&#21160;&#20316;&#21608;&#22260;&#30340;&#31163;&#25955;&#37051;&#22495;&#12290;&#25105;&#20204;&#36890;&#36807;&#19982;&#19977;&#20010;&#26631;&#26438;&#31639;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#26469;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large discrete action spaces (LDAS) remain a central challenge in reinforcement learning. Existing solution approaches can handle unstructured LDAS with up to a few million actions. However, many real-world applications in logistics, production, and transportation systems have combinatorial action spaces, whose size grows well beyond millions of actions, even on small instances. Fortunately, such action spaces exhibit structure, e.g., equally spaced discrete resource units. With this work, we focus on handling structured LDAS (SLDAS) with sizes that cannot be handled by current benchmarks: we propose Dynamic Neighborhood Construction (DNC), a novel exploitation paradigm for SLDAS. We present a scalable neighborhood exploration heuristic that utilizes this paradigm and efficiently explores the discrete neighborhood around the continuous proxy action in structured action spaces with up to $10^{73}$ actions. We demonstrate the performance of our method by benchmarking it against three sta
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SUFO&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#24494;&#35843;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#22810;&#31181;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#20449;&#20219;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.17588</link><description>&lt;p&gt;
&#35786;&#26029;&#21464;&#21387;&#22120;&#65306;&#25581;&#31034;&#20020;&#24202;&#20915;&#31574;&#20013;&#30340;&#29305;&#24449;&#31354;&#38388;&#12290; (arXiv:2305.17588v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making. (arXiv:2305.17588v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17588
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SUFO&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#24494;&#35843;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#22810;&#31181;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#20449;&#20219;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#31561;&#39640;&#39118;&#38505;&#39046;&#22495;&#65292;&#20026;&#20102;&#24314;&#31435;&#20449;&#20219;&#21644;&#30830;&#20445;&#23433;&#20840;&#65292;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#20351;&#29992;&#26377;&#38480;&#30340;&#20020;&#24202;&#35760;&#24405;&#23545;&#39044;&#35757;&#32451;&#30340;&#21464;&#21387;&#22120;&#36827;&#34892;&#24494;&#35843;&#20197;&#36741;&#21161;&#20020;&#24202;&#20915;&#31574;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;SUFO&#30340;&#31995;&#32479;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22686;&#24378;&#20102;&#24494;&#35843;&#30340;&#21464;&#21387;&#22120;&#29305;&#24449;&#31354;&#38388;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;SUFO&#21033;&#29992;&#19968;&#31995;&#21015;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#21253;&#25324;&#30417;&#30563;&#25506;&#32034;&#12289;&#26080;&#30417;&#30563;&#30456;&#20284;&#24615;&#20998;&#26512;&#12289;&#29305;&#24449;&#21160;&#24577;&#21644;&#24322;&#24120;&#20540;&#20998;&#26512;&#65292;&#26469;&#35299;&#20915;&#20851;&#20110;&#27169;&#22411;&#20449;&#20219;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#23545;&#30495;&#23454;&#19990;&#30028;&#30149;&#29702;&#20998;&#31867;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;MedNLI&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20116;&#20010;110M&#35268;&#27169;&#30340;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#20998;&#20026;&#36890;&#29992;&#39046;&#22495;&#65288;BERT, TNLR&#65289;&#12289;&#28151;&#21512;&#39046;&#22495;&#65288;BioBERT, Clinical BioBERT&#65289;&#21644;&#39046;&#22495;&#29305;&#23450;&#65288;PubMedBERT&#65289;&#32452;&#12290;&#25105;&#20204;&#30340;SUFO&#20998;&#26512;&#25581;&#31034;&#20102;&#65306;(1)
&lt;/p&gt;
&lt;p&gt;
Pre-trained transformers are often fine-tuned to aid clinical decision-making using limited clinical notes. Model interpretability is crucial, especially in high-stakes domains like medicine, to establish trust and ensure safety, which requires human engagement. We introduce SUFO, a systematic framework that enhances interpretability of fine-tuned transformer feature spaces. SUFO utilizes a range of analytic and visualization techniques, including Supervised probing, Unsupervised similarity analysis, Feature dynamics, and Outlier analysis to address key questions about model trust and interpretability. We conduct a case study investigating the impact of pre-training data where we focus on real-world pathology classification tasks, and validate our findings on MedNLI. We evaluate five 110M-sized pre-trained transformer models, categorized into general-domain (BERT, TNLR), mixed-domain (BioBERT, Clinical BioBERT), and domain-specific (PubMedBERT) groups. Our SUFO analyses reveal that: (1
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#23558;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21644;&#24352;&#37327;&#32593;&#32476;&#24212;&#29992;&#20110;&#32929;&#31080;&#25910;&#30410;&#39044;&#27979;&#65292;&#22312;&#26085;&#26412;&#32929;&#24066;&#20013;&#24352;&#37327;&#32593;&#32476;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#27169;&#22411;&#65292;&#24182;&#22312;&#26368;&#26032;&#24066;&#22330;&#29615;&#22659;&#19979;&#21576;&#29616;&#20986;&#21331;&#36234;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.12501</link><description>&lt;p&gt;
&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21644;&#24352;&#37327;&#32593;&#32476;&#22312;&#25130;&#38754;&#32929;&#31080;&#25910;&#30410;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The cross-sectional stock return predictions via quantum neural network and tensor network. (arXiv:2304.12501v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#23558;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21644;&#24352;&#37327;&#32593;&#32476;&#24212;&#29992;&#20110;&#32929;&#31080;&#25910;&#30410;&#39044;&#27979;&#65292;&#22312;&#26085;&#26412;&#32929;&#24066;&#20013;&#24352;&#37327;&#32593;&#32476;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#27169;&#22411;&#65292;&#24182;&#22312;&#26368;&#26032;&#24066;&#22330;&#29615;&#22659;&#19979;&#21576;&#29616;&#20986;&#21331;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#37327;&#23376;&#21644;&#37327;&#23376;&#21551;&#21457;&#24335;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#32929;&#31080;&#25910;&#30410;&#39044;&#27979;&#30340;&#24212;&#29992;&#12290;&#20854;&#20013;&#65292;&#25105;&#20204;&#23558;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;&#19968;&#31181;&#36866;&#29992;&#20110;&#22122;&#22768;&#20013;&#31561;&#35268;&#27169;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#31639;&#27861;&#65289;&#21644;&#24352;&#37327;&#32593;&#32476;&#65288;&#19968;&#31181;&#21463;&#37327;&#23376;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65289;&#30340;&#24615;&#33021;&#19982;&#20256;&#32479;&#27169;&#22411;&#22914;&#32447;&#24615;&#22238;&#24402;&#21644;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27604;&#36739;&#12290;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#27169;&#22411;&#39044;&#27979;&#30340;&#25237;&#36164;&#32452;&#21512;&#24182;&#27979;&#37327;&#25237;&#36164;&#32489;&#25928;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#26085;&#26412;&#32929;&#24066;&#20013;&#65292;&#24352;&#37327;&#32593;&#32476;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#22522;&#20934;&#27169;&#22411;&#65288;&#21253;&#25324;&#32447;&#24615;&#21644;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65289;&#12290;&#34429;&#28982;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#25972;&#20010;&#21608;&#26399;&#20869;&#20855;&#26377;&#38477;&#20302;&#39118;&#38505;&#35843;&#25972;&#36229;&#39069;&#25910;&#30410;&#30340;&#33021;&#21147;&#65292;&#20294;&#26368;&#26032;&#30340;&#24066;&#22330;&#29615;&#22659;&#19979;&#65292;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21644;&#24352;&#37327;&#32593;&#32476;&#27169;&#22411;&#22343;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we investigate the application of quantum and quantum-inspired machine learning algorithms to stock return predictions. Specifically, we evaluate performance of quantum neural network, an algorithm suited for noisy intermediate-scale quantum computers, and tensor network, a quantum-inspired machine learning algorithm, against classical models such as linear regression and neural networks. To evaluate their abilities, we construct portfolios based on their predictions and measure investment performances. The empirical study on the Japanese stock market shows the tensor network model achieves superior performance compared to classical benchmark models, including linear and neural network models. Though the quantum neural network model attains the lowered risk-adjusted excess return than the classical neural network models over the whole period, both the quantum neural network and tensor network models have superior performances in the latest market environment, which sugges
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#30340;&#22270;&#20687;&#34701;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#25972;&#21512;&#22810;&#27169;&#24577;&#21644;&#22810;&#23545;&#27604;&#24230;&#30340;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#31070;&#32463;&#24433;&#20687;&#20998;&#26512;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.15963</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#29983;&#25104;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#21644;&#22810;&#23545;&#27604;&#24230;&#22270;&#20687;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Multimodal and multicontrast image fusion via deep generative models. (arXiv:2303.15963v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15963
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#30340;&#22270;&#20687;&#34701;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#25972;&#21512;&#22810;&#27169;&#24577;&#21644;&#22810;&#23545;&#27604;&#24230;&#30340;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#31070;&#32463;&#24433;&#20687;&#20998;&#26512;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#20204;&#36880;&#28176;&#24847;&#35782;&#21040;&#20256;&#32479;&#30340;&#35786;&#26029;&#26631;&#31614;&#26080;&#27861;&#21487;&#38752;&#22320;&#25551;&#36848;&#22810;&#31181;&#20020;&#24202;&#34920;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#21464;&#24322;&#24615;&#65292;&#23588;&#20854;&#26159;&#24191;&#27867;&#30340;&#31070;&#32463;&#31934;&#31070;&#30142;&#30149;&#65288;&#20363;&#22914;&#25233;&#37057;&#30151;&#12289;&#28966;&#34385;&#30151;&#12289;&#34892;&#20026;&#34920;&#22411;&#65289;&#12290;&#24739;&#32773;&#30340;&#24322;&#36136;&#24615;&#21487;&#20197;&#36890;&#36807;&#23558;&#20010;&#20307;&#26681;&#25454;&#32463;&#39564;&#24471;&#20986;&#30340;&#20132;&#32455;&#36830;&#32493;&#30340;&#26032;&#31867;&#21035;&#20998;&#32452;&#26469;&#26356;&#22909;&#22320;&#25551;&#36848;&#65292;&#36825;&#20123;&#36830;&#32493;&#36328;&#36234;&#24182;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#31867;&#21035;&#36793;&#30028;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#25658;&#24102;&#30528;&#20851;&#20110;&#27599;&#20010;&#24739;&#32773;&#22823;&#33041;&#30340;&#23500;&#21547;&#26102;&#31354;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#20250;&#32463;&#36807;&#19968;&#31995;&#21015;&#20107;&#20808;&#26410;&#23398;&#20064;&#20026;&#27169;&#22411;&#30340;&#35757;&#32451;&#37096;&#20998;&#30340;&#36807;&#31243;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#22240;&#27492;&#27809;&#26377;&#38024;&#23545;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#36827;&#34892;&#20248;&#21270;&#12290;&#36825;&#26159;&#22240;&#20026;&#27599;&#20010;&#20010;&#20307;&#36890;&#24120;&#37117;&#26377;&#22810;&#20010;&#20840;&#33041;&#19977;&#32500;&#25104;&#20687;&#27169;&#24577;&#65292;&#24120;&#20276;&#38543;&#30528;&#28145;&#23618;&#30340;&#22522;&#22240;&#22411;&#21644;&#34920;&#22411;&#29305;&#24449;&#25551;&#36848;&#12290;&#20026;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#22270;&#20687;&#34701;&#21512;&#26041;&#27861;&#65292;&#20197;&#32452;&#21512;&#22810;&#27169;&#24577;&#21644;&#22810;&#23545;&#27604;&#24230;&#30340;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26469;&#23398;&#20064;&#19981;&#21516;&#25104;&#20687;&#27169;&#24577;&#30340;&#20849;&#21516;&#34920;&#31034;&#31354;&#38388;&#65292;&#21516;&#26102;&#24378;&#21046;&#25191;&#34892;&#27169;&#24577;&#29305;&#23450;&#21644;&#23545;&#27604;&#24230;&#29305;&#23450;&#30340;&#32534;&#30721;-&#35299;&#30721;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;OPENNeuro&#25968;&#25454;&#24211;&#30340;174&#21517;MDD&#24739;&#32773;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#32467;&#26500;T1&#21152;&#26435;MRI&#12289;&#25193;&#25955;&#24352;&#37327;&#25104;&#20687;&#65288;DTI&#65289;&#21644;&#38745;&#24687;&#21151;&#33021;MRI&#65288;rsfMRI&#65289;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#22810;&#27169;&#24577;&#21644;&#22810;&#23545;&#27604;&#24230;&#30340;&#25104;&#20687;&#25968;&#25454;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#20998;&#31867;&#34920;&#29616;&#24471;&#21040;&#20102;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, it has become progressively more evident that classic diagnostic labels are unable to reliably describe the complexity and variability of several clinical phenotypes. This is particularly true for a broad range of neuropsychiatric illnesses (e.g., depression, anxiety disorders, behavioral phenotypes). Patient heterogeneity can be better described by grouping individuals into novel categories based on empirically derived sections of intersecting continua that span across and beyond traditional categorical borders. In this context, neuroimaging data carry a wealth of spatiotemporally resolved information about each patient's brain. However, they are usually heavily collapsed a priori through procedures which are not learned as part of model training, and consequently not optimized for the downstream prediction task. This is because every individual participant usually comes with multiple whole-brain 3D imaging modalities often accompanied by a deep genotypic and phenotypic char
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#38544;&#31169;&#20445;&#25252;&#35757;&#32451;&#21307;&#23398;&#24433;&#20687;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#65292;&#24182;&#19982;&#38750;&#38544;&#31169;&#35757;&#32451;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#30740;&#31350;&#32467;&#26524;&#21487;&#20026;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#30340;&#24191;&#27867;&#24212;&#29992;&#25552;&#20379;&#37325;&#35201;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2302.01622</link><description>&lt;p&gt;
&#31169;&#23494;&#12289;&#20844;&#24179;&#19988;&#31934;&#30830;&#65306;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#35757;&#32451;&#22823;&#35268;&#27169;&#38544;&#31169;&#20445;&#25252;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Private, fair and accurate: Training large-scale, privacy-preserving AI models in medical imaging. (arXiv:2302.01622v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#38544;&#31169;&#20445;&#25252;&#35757;&#32451;&#21307;&#23398;&#24433;&#20687;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#65292;&#24182;&#19982;&#38750;&#38544;&#31169;&#35757;&#32451;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#30740;&#31350;&#32467;&#26524;&#21487;&#20026;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#30340;&#24191;&#27867;&#24212;&#29992;&#25552;&#20379;&#37325;&#35201;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#22810;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21307;&#23398;&#25968;&#25454;&#30340;&#39640;&#24230;&#25935;&#24863;&#24615;&#65292;&#38656;&#35201;&#37319;&#21462;&#29305;&#27530;&#25514;&#26045;&#30830;&#20445;&#20854;&#20445;&#25252;&#12290;&#20445;&#25252;&#38544;&#31169;&#30340;&#40644;&#37329;&#26631;&#20934;&#26159;&#24341;&#20837;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#26469;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;DP&#23545;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#26377;&#36127;&#38754;&#24433;&#21709;&#65292;&#36825;&#22312;&#21307;&#23398;&#20013;&#26159;&#19981;&#21487;&#25509;&#21463;&#30340;&#65292;&#24182;&#19988;&#26159;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#24191;&#27867;&#24212;&#29992;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#38544;&#31169;&#20445;&#25252;&#35757;&#32451;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#23545;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65292;&#19982;&#38750;&#38544;&#31169;&#35757;&#32451;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65306;&#65288;1&#65289;&#19968;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65288;N=193,311&#65289;&#30340;&#39640;&#36136;&#37327;&#20020;&#24202;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#65292;&#21644;&#65288;2&#65289;&#19968;&#20010;&#25968;&#25454;&#38598;&#65288;N=1,625&#65289;&#30340;3D&#33145;&#37096;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#22270;&#20687;&#65292;&#29992;&#20110;&#20998;&#31867;&#33008;&#33146;&#23548;&#31649;&#33146;&#30284;&#65288;PDAC&#65289;&#30340;&#23384;&#22312;&#12290;&#20004;&#20010;&#25968;&#25454;&#38598;&#22343;&#20026;&#22238;&#39038;&#24615;&#37319;&#38598;&#65292;&#24182;&#30001;&#32463;&#39564;&#20016;&#23500;&#30340;&#21307;&#23398;&#24433;&#20687;&#19987;&#23478;&#36827;&#34892;&#25163;&#21160;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) models are increasingly used in the medical domain. However, as medical data is highly sensitive, special precautions to ensure its protection are required. The gold standard for privacy preservation is the introduction of differential privacy (DP) to model training. Prior work indicates that DP has negative implications on model accuracy and fairness, which are unacceptable in medicine and represent a main barrier to the widespread use of privacy-preserving techniques. In this work, we evaluated the effect of privacy-preserving training of AI models regarding accuracy and fairness compared to non-private training. For this, we used two datasets: (1) A large dataset (N=193,311) of high quality clinical chest radiographs, and (2) a dataset (N=1,625) of 3D abdominal computed tomography (CT) images, with the task of classifying the presence of pancreatic ductal adenocarcinoma (PDAC). Both were retrospectively collected and manually labeled by experienced radio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#30456;&#23545;&#29109;&#27491;&#21017;&#21270;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#21442;&#32771;&#24230;&#37327;&#20026;sigma&#26377;&#38480;&#27979;&#24230;&#65292;&#35299;&#20026;&#21807;&#19968;&#30340;&#27010;&#29575;&#27979;&#24230;&#24182;&#23637;&#29616;&#20102;&#20960;&#20046;&#27491;&#30830;&#30340;&#20445;&#35777;&#12290;ERM-RER&#38382;&#39064;&#30340;&#35299;&#34987;&#31216;&#20026;Gibbs&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.06617</link><description>&lt;p&gt;
&#30456;&#23545;&#29109;&#27491;&#21017;&#21270;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Empirical Risk Minimization with Relative Entropy Regularization. (arXiv:2211.06617v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30456;&#23545;&#29109;&#27491;&#21017;&#21270;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#21442;&#32771;&#24230;&#37327;&#20026;sigma&#26377;&#38480;&#27979;&#24230;&#65292;&#35299;&#20026;&#21807;&#19968;&#30340;&#27010;&#29575;&#27979;&#24230;&#24182;&#23637;&#29616;&#20102;&#20960;&#20046;&#27491;&#30830;&#30340;&#20445;&#35777;&#12290;ERM-RER&#38382;&#39064;&#30340;&#35299;&#34987;&#31216;&#20026;Gibbs&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20551;&#23450;&#21442;&#32771;&#24230;&#37327;&#20026;sigma&#26377;&#38480;&#27979;&#24230;&#65288;measure&#65289;&#32780;&#38750;&#27010;&#29575;&#27979;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#30740;&#31350;&#20102;&#30456;&#23545;&#29109;&#27491;&#21017;&#21270;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM-RER&#65289;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#20551;&#35774;&#19979;&#65292;&#23384;&#22312;&#19968;&#20010;ERM-RER&#38382;&#39064;&#30340;&#27867;&#21270;&#65292;&#20801;&#35768;&#26356;&#22823;&#31243;&#24230;&#22320;&#28789;&#27963;&#22320;&#24182;&#20837;&#20808;&#39564;&#30693;&#35782;&#12290;&#22312;&#36825;&#20123;&#24615;&#36136;&#20013;&#65292;&#22914;&#26524;&#23384;&#22312;ERM-RER&#38382;&#39064;&#30340;&#35299;&#65292;&#21017;&#35813;&#35299;&#26159;&#21807;&#19968;&#30340;&#27010;&#29575;&#27979;&#24230;&#65292;&#36890;&#24120;&#19982;&#21442;&#32771;&#27979;&#24230;&#30456;&#20114;&#32477;&#23545;&#36830;&#32493;&#12290;&#36825;&#26679;&#30340;&#35299;&#23545;&#20110;ERM&#38382;&#39064;&#23637;&#29616;&#20102;&#20960;&#20046;&#27491;&#30830;&#30340;&#20445;&#35777;&#65292;&#32780;&#19981;&#38656;&#20851;&#24515;ERM&#38382;&#39064;&#26159;&#21542;&#26377;&#35299;&#12290;&#24403;&#20174;ERM-RER&#38382;&#39064;&#30340;&#35299;&#25277;&#21462;&#27169;&#22411;&#26102;&#65292;&#22266;&#23450;&#25968;&#25454;&#38598;&#26102;&#65292;&#32463;&#39564;&#39118;&#38505;&#34987;&#35777;&#26126;&#26159;&#19968;&#20010;&#20122;&#39640;&#26031;&#38543;&#26426;&#21464;&#37327;&#12290;ERM-RER&#38382;&#39064;&#30340;&#35299;&#65288;Gibbs&#31639;&#27861;&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The empirical risk minimization (ERM) problem with relative entropy regularization (ERM-RER) is investigated under the assumption that the reference measure is a {\sigma}-finite measure, and not necessarily a probability measure. Under this assumption, which leads to a generalization of the ERM-RER problem allowing a larger degree of flexibility for incorporating prior knowledge, numerous relevant properties are stated. Among these properties, the solution to this problem, if it exists, is shown to be a unique probability measure, often mutually absolutely continuous with the reference measure. Such a solution exhibits a probably-approximately-correct guarantee for the ERM problem independently of whether the latter possesses a solution. For a fixed dataset, the empirical risk is shown to be a sub-Gaussian random variable when the models are sampled from the solution to the ERM-RER problem. The generalization capabilities of the solution to the ERM-RER problem (the Gibbs algorithm) are
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24555;&#36895;&#30340;&#22810;&#36724;&#39640;&#26031;&#22270;&#24418;&#27169;&#22411;&#65292;&#29992;&#20110;&#26500;&#24314;&#31232;&#30095;&#22270;&#24418;&#34920;&#31034;&#12290;&#30456;&#27604;&#20808;&#21069;&#24037;&#20316;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#27599;&#20010;&#36724;&#19978;&#20165;&#20351;&#29992;&#19968;&#27425;&#29305;&#24449;&#20998;&#35299;&#65292;&#23454;&#29616;&#20102;&#25968;&#37327;&#32423;&#30340;&#21152;&#36895;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#24212;&#29992;&#20110;&#22823;&#22411;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#21333;&#32454;&#32990;&#22810;&#32452;&#23398;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2211.02920</link><description>&lt;p&gt;
GmGM: &#19968;&#31181;&#24555;&#36895;&#30340;&#22810;&#36724;&#39640;&#26031;&#22270;&#24418;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
GmGM: a Fast Multi-Axis Gaussian Graphical Model. (arXiv:2211.02920v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24555;&#36895;&#30340;&#22810;&#36724;&#39640;&#26031;&#22270;&#24418;&#27169;&#22411;&#65292;&#29992;&#20110;&#26500;&#24314;&#31232;&#30095;&#22270;&#24418;&#34920;&#31034;&#12290;&#30456;&#27604;&#20808;&#21069;&#24037;&#20316;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#27599;&#20010;&#36724;&#19978;&#20165;&#20351;&#29992;&#19968;&#27425;&#29305;&#24449;&#20998;&#35299;&#65292;&#23454;&#29616;&#20102;&#25968;&#37327;&#32423;&#30340;&#21152;&#36895;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#24212;&#29992;&#20110;&#22823;&#22411;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#21333;&#32454;&#32990;&#22810;&#32452;&#23398;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#26031;&#22810;&#22270;&#24418;&#27169;&#22411;&#65292;&#29992;&#20110;&#26500;&#24314;&#30697;&#38453;&#21644;&#24352;&#37327;&#21464;&#37327;&#25968;&#25454;&#30340;&#31232;&#30095;&#22270;&#24418;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#20849;&#20139;&#36724;&#30340;&#24352;&#37327;&#19978;&#30340;&#34920;&#31034;&#26469;&#25512;&#24191;&#35813;&#39046;&#22495;&#30340;&#20808;&#21069;&#24037;&#20316;&#65292;&#36825;&#23545;&#20110;&#20998;&#26512;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65288;&#22914;&#22810;&#32452;&#23398;&#20013;&#36935;&#21040;&#30340;&#25968;&#25454;&#38598;&#65289;&#26159;&#24517;&#35201;&#30340;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#27599;&#20010;&#36724;&#19978;&#20165;&#20351;&#29992;&#19968;&#27425;&#29305;&#24449;&#20998;&#35299;&#65292;&#30456;&#23545;&#20110;&#38750;&#24191;&#20041;&#24773;&#20917;&#19979;&#30340;&#20808;&#21069;&#24037;&#20316;&#23454;&#29616;&#20102;&#25968;&#37327;&#32423;&#30340;&#21152;&#36895;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#21253;&#25324;&#21333;&#32454;&#32990;&#22810;&#32452;&#23398;&#25968;&#25454;&#22312;&#20869;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#36825;&#22312;&#20043;&#21069;&#30340;&#26041;&#27861;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#20116;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the Gaussian multi-Graphical Model, a model to construct sparse graph representations of matrix- and tensor-variate data. We generalize prior work in this area by simultaneously learning this representation across several tensors that share axes, which is necessary to allow the analysis of multimodal datasets such as those encountered in multi-omics. Our algorithm uses only a single eigendecomposition per axis, achieving an order of magnitude speedup over prior work in the ungeneralized case. This allows the use of our methodology on large multi-modal datasets such as single-cell multi-omics data, which was challenging with previous approaches. We validate our model on synthetic data and five real-world datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#30340;&#20551;&#35774;&#26816;&#39564;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22797;&#21512;&#26816;&#39564;&#38382;&#39064;&#65292;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#27491;&#30830;&#30340;&#27169;&#22411;&#35268;&#33539;&#30340;&#38646;&#20551;&#35774;&#19979;&#65292;&#38750;&#21442;&#25968;&#22320;&#20272;&#35745;&#21442;&#25968;&#65288;&#25110;&#27169;&#25311;&#22120;&#65289;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2111.10275</link><description>&lt;p&gt;
&#24102;&#26377;&#26680;&#30340;&#22797;&#21512;&#36866;&#21512;&#24615;&#26816;&#39564;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Composite Goodness-of-fit Tests with Kernels. (arXiv:2111.10275v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.10275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#30340;&#20551;&#35774;&#26816;&#39564;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22797;&#21512;&#26816;&#39564;&#38382;&#39064;&#65292;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#27491;&#30830;&#30340;&#27169;&#22411;&#35268;&#33539;&#30340;&#38646;&#20551;&#35774;&#19979;&#65292;&#38750;&#21442;&#25968;&#22320;&#20272;&#35745;&#21442;&#25968;&#65288;&#25110;&#27169;&#25311;&#22120;&#65289;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#38169;&#35823;&#35828;&#26126;&#21487;&#33021;&#20250;&#23545;&#27010;&#29575;&#27169;&#22411;&#30340;&#23454;&#29616;&#36896;&#25104;&#37325;&#22823;&#25361;&#25112;&#65292;&#36825;&#20419;&#20351;&#24320;&#21457;&#20986;&#19968;&#20123;&#30452;&#25509;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#40065;&#26834;&#26041;&#27861;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#26356;&#20026;&#22797;&#26434;&#30340;&#26041;&#27861;&#26159;&#21542;&#38656;&#35201;&#21462;&#20915;&#20110;&#27169;&#22411;&#26159;&#21542;&#30495;&#30340;&#38169;&#35823;&#65292;&#30446;&#21069;&#32570;&#20047;&#36890;&#29992;&#30340;&#26041;&#27861;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26680;&#30340;&#20551;&#35774;&#26816;&#39564;&#26041;&#27861;&#65292;&#29992;&#20110;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22797;&#21512;&#26816;&#39564;&#38382;&#39064;&#65292;&#21363;&#25105;&#20204;&#26159;&#21542;&#24863;&#20852;&#36259;&#30340;&#25968;&#25454;&#26469;&#33258;&#26576;&#20123;&#21442;&#25968;&#27169;&#22411;&#26063;&#20013;&#30340;&#20219;&#20309;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#27979;&#35797;&#21033;&#29992;&#22522;&#20110;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#21644;&#26680;Stein&#24046;&#24322;&#30340;&#26368;&#23567;&#36317;&#31163;&#20272;&#35745;&#22120;&#12290;&#23427;&#20204;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#65292;&#21253;&#25324;&#24403;&#21442;&#25968;&#27169;&#22411;&#30340;&#23494;&#24230;&#24050;&#30693;&#38500;&#26631;&#20934;&#21270;&#24120;&#25968;&#22806;&#65292;&#25110;&#32773;&#22914;&#26524;&#27169;&#22411;&#37319;&#29992;&#27169;&#25311;&#22120;&#24418;&#24335;&#12290;&#20316;&#20026;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#27491;&#30830;&#30340;&#27169;&#22411;&#35268;&#33539;&#30340;&#38646;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#33021;&#22815;&#38750;&#21442;&#25968;&#22320;&#20272;&#35745;&#21442;&#25968;&#65288;&#25110;&#27169;&#25311;&#22120;&#65289;&#20998;&#24067;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#24314;&#31435;&#25105;&#20204;&#26041;&#27861;&#26377;&#25928;&#24615;&#30340;&#29702;&#35770;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#21644;&#24322;&#24120;&#26816;&#27979;&#24212;&#29992;&#26696;&#20363;&#28436;&#31034;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model misspecification can create significant challenges for the implementation of probabilistic models, and this has led to development of a range of robust methods which directly account for this issue. However, whether these more involved methods are required will depend on whether the model is really misspecified, and there is a lack of generally applicable methods to answer this question. In this paper, we propose one such method. More precisely, we propose kernel-based hypothesis tests for the challenging composite testing problem, where we are interested in whether the data comes from any distribution in some parametric family. Our tests make use of minimum distance estimators based on the maximum mean discrepancy and the kernel Stein discrepancy. They are widely applicable, including whenever the density of the parametric model is known up to normalisation constant, or if the model takes the form of a simulator. As our main result, we show that we are able to estimate the param
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26410;&#30693;&#27969;&#24418;&#19978;&#35299;&#26925;&#22278;&#22411;PDE&#38382;&#39064;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#25955;&#26144;&#23556;&#21644;&#28145;&#24230;&#23398;&#20064;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#26080;&#32593;&#26684;&#35745;&#31639;&#26694;&#26550;&#12290;&#36890;&#36807;&#23558;PDE&#27714;&#35299;&#36716;&#21270;&#20026;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#37319;&#29992;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#23567;&#20108;&#20056;&#22238;&#24402;&#26469;&#36817;&#20284;&#27714;&#35299;&#20195;&#25968;&#26041;&#31243;&#65292;&#24471;&#21040;&#20102;&#19968;&#33268;&#20272;&#35745;&#37327;&#65292;&#26368;&#32456;&#24471;&#21040;&#30340;&#25968;&#20540;&#26041;&#27861;&#22312;&#26497;&#38480;&#24773;&#20917;&#19979;&#26159;&#19968;&#33268;&#35299;&#12290;</title><link>http://arxiv.org/abs/2106.06682</link><description>&lt;p&gt;
&#29992;&#26426;&#22120;&#23398;&#20064;&#22312;&#26410;&#30693;&#27969;&#24418;&#19978;&#35299;PDE&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving PDEs on Unknown Manifolds with Machine Learning. (arXiv:2106.06682v3 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.06682
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26410;&#30693;&#27969;&#24418;&#19978;&#35299;&#26925;&#22278;&#22411;PDE&#38382;&#39064;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#25955;&#26144;&#23556;&#21644;&#28145;&#24230;&#23398;&#20064;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#26080;&#32593;&#26684;&#35745;&#31639;&#26694;&#26550;&#12290;&#36890;&#36807;&#23558;PDE&#27714;&#35299;&#36716;&#21270;&#20026;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#37319;&#29992;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#23567;&#20108;&#20056;&#22238;&#24402;&#26469;&#36817;&#20284;&#27714;&#35299;&#20195;&#25968;&#26041;&#31243;&#65292;&#24471;&#21040;&#20102;&#19968;&#33268;&#20272;&#35745;&#37327;&#65292;&#26368;&#32456;&#24471;&#21040;&#30340;&#25968;&#20540;&#26041;&#27861;&#22312;&#26497;&#38480;&#24773;&#20917;&#19979;&#26159;&#19968;&#33268;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#26144;&#23556;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#26080;&#32593;&#26684;&#35745;&#31639;&#26694;&#26550;&#21644;&#26426;&#22120;&#23398;&#20064;&#29702;&#35770;&#65292;&#29992;&#20110;&#22312;&#26410;&#30693;&#27969;&#24418;&#19978;&#35299;&#26925;&#22278;&#22411;PDE&#38382;&#39064;&#65292;&#20854;&#20013;&#27969;&#24418;&#29992;&#28857;&#20113;&#34920;&#31034;&#12290;PDE&#27714;&#35299;&#22120;&#34987;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#20197;&#35299;&#20915;&#19968;&#20010;&#26368;&#23567;&#20108;&#20056;&#22238;&#24402;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#36890;&#36807;&#36817;&#20284;&#34920;&#31034;&#19968;&#20010;PDE&#65288;&#20197;&#21450;&#36793;&#30028;&#26465;&#20214;&#65292;&#22914;&#26524;&#36866;&#29992;&#65289;&#30340;&#20195;&#25968;&#26041;&#31243;&#12290;&#36825;&#20010;&#20195;&#25968;&#26041;&#31243;&#28041;&#21450;&#21040;&#36890;&#36807;&#25193;&#25955;&#26144;&#23556;&#28176;&#36827;&#23637;&#24320;&#24471;&#21040;&#30340;&#22270;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#65292;&#23427;&#26159;&#19968;&#20010;&#20851;&#20110;&#20108;&#38454;&#26925;&#22278;&#24494;&#20998;&#31639;&#23376;&#30340;&#19968;&#33268;&#20272;&#35745;&#37327;&#12290;&#26368;&#32456;&#24471;&#21040;&#30340;&#25968;&#20540;&#26041;&#27861;&#26159;&#35299;&#20915;&#19968;&#20010;&#39640;&#24230;&#38750;&#20984;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#38480;&#21046;&#22312;&#31070;&#32463;&#32593;&#32476;&#20551;&#35774;&#31354;&#38388;&#30340;&#35299;&#31354;&#38388;&#20869;&#12290;&#22312;&#33391;&#23450;&#20041;&#30340;&#26925;&#22278;&#22411;PDE&#35774;&#32622;&#20013;&#65292;&#24403;&#20551;&#35774;&#31354;&#38388;&#21253;&#25324;&#26080;&#38480;&#23485;&#24230;&#25110;&#28145;&#24230;&#30340;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#32463;&#39564;&#25439;&#22833;&#20989;&#25968;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#26159;&#26497;&#38480;&#24773;&#20917;&#19979;&#30340;&#19968;&#33268;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a mesh-free computational framework and machine learning theory for solving elliptic PDEs on unknown manifolds, identified with point clouds, based on diffusion maps (DM) and deep learning. The PDE solver is formulated as a supervised learning task to solve a least-squares regression problem that imposes an algebraic equation approximating a PDE (and boundary conditions if applicable). This algebraic equation involves a graph-Laplacian type matrix obtained via DM asymptotic expansion, which is a consistent estimator of second-order elliptic differential operators. The resulting numerical method is to solve a highly non-convex empirical risk minimization problem subjected to a solution from a hypothesis space of neural networks (NNs). In a well-posed elliptic PDE setting, when the hypothesis space consists of neural networks with either infinite width or depth, we show that the global minimizer of the empirical loss function is a consistent solution in the limit of l
&lt;/p&gt;</description></item></channel></rss>